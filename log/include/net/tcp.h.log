commit cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2
Merge: 2e63f6ce7ed2 065fcfd49763
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 16:27:18 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
    
     1) Allow setting bluetooth L2CAP modes via socket option, from Luiz
        Augusto von Dentz.
    
     2) Add GSO partial support to igc, from Sasha Neftin.
    
     3) Several cleanups and improvements to r8169 from Heiner Kallweit.
    
     4) Add IF_OPER_TESTING link state and use it when ethtool triggers a
        device self-test. From Andrew Lunn.
    
     5) Start moving away from custom driver versions, use the globally
        defined kernel version instead, from Leon Romanovsky.
    
     6) Support GRO vis gro_cells in DSA layer, from Alexander Lobakin.
    
     7) Allow hard IRQ deferral during NAPI, from Eric Dumazet.
    
     8) Add sriov and vf support to hinic, from Luo bin.
    
     9) Support Media Redundancy Protocol (MRP) in the bridging code, from
        Horatiu Vultur.
    
    10) Support netmap in the nft_nat code, from Pablo Neira Ayuso.
    
    11) Allow UDPv6 encapsulation of ESP in the ipsec code, from Sabrina
        Dubroca. Also add ipv6 support for espintcp.
    
    12) Lots of ReST conversions of the networking documentation, from Mauro
        Carvalho Chehab.
    
    13) Support configuration of ethtool rxnfc flows in bcmgenet driver,
        from Doug Berger.
    
    14) Allow to dump cgroup id and filter by it in inet_diag code, from
        Dmitry Yakunin.
    
    15) Add infrastructure to export netlink attribute policies to
        userspace, from Johannes Berg.
    
    16) Several optimizations to sch_fq scheduler, from Eric Dumazet.
    
    17) Fallback to the default qdisc if qdisc init fails because otherwise
        a packet scheduler init failure will make a device inoperative. From
        Jesper Dangaard Brouer.
    
    18) Several RISCV bpf jit optimizations, from Luke Nelson.
    
    19) Correct the return type of the ->ndo_start_xmit() method in several
        drivers, it's netdev_tx_t but many drivers were using
        'int'. From Yunjian Wang.
    
    20) Add an ethtool interface for PHY master/slave config, from Oleksij
        Rempel.
    
    21) Add BPF iterators, from Yonghang Song.
    
    22) Add cable test infrastructure, including ethool interfaces, from
        Andrew Lunn. Marvell PHY driver is the first to support this
        facility.
    
    23) Remove zero-length arrays all over, from Gustavo A. R. Silva.
    
    24) Calculate and maintain an explicit frame size in XDP, from Jesper
        Dangaard Brouer.
    
    25) Add CAP_BPF, from Alexei Starovoitov.
    
    26) Support terse dumps in the packet scheduler, from Vlad Buslov.
    
    27) Support XDP_TX bulking in dpaa2 driver, from Ioana Ciornei.
    
    28) Add devm_register_netdev(), from Bartosz Golaszewski.
    
    29) Minimize qdisc resets, from Cong Wang.
    
    30) Get rid of kernel_getsockopt and kernel_setsockopt in order to
        eliminate set_fs/get_fs calls. From Christoph Hellwig.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2517 commits)
      selftests: net: ip_defrag: ignore EPERM
      net_failover: fixed rollback in net_failover_open()
      Revert "tipc: Fix potential tipc_aead refcnt leak in tipc_crypto_rcv"
      Revert "tipc: Fix potential tipc_node refcnt leak in tipc_rcv"
      vmxnet3: allow rx flow hash ops only when rss is enabled
      hinic: add set_channels ethtool_ops support
      selftests/bpf: Add a default $(CXX) value
      tools/bpf: Don't use $(COMPILE.c)
      bpf, selftests: Use bpf_probe_read_kernel
      s390/bpf: Use bcr 0,%0 as tail call nop filler
      s390/bpf: Maintain 8-byte stack alignment
      selftests/bpf: Fix verifier test
      selftests/bpf: Fix sample_cnt shared between two threads
      bpf, selftests: Adapt cls_redirect to call csum_level helper
      bpf: Add csum_level helper for fixing up csum levels
      bpf: Fix up bpf_skb_adjust_room helper's skb csum setting
      sfc: add missing annotation for efx_ef10_try_update_nic_stats_vf()
      crypto/chtls: IPv6 support for inline TLS
      Crypto/chcr: Fixes a coccinile check error
      Crypto/chcr: Fixes compilations warnings
      ...

commit 81e8c10dac62c427b25f6bbb07936806e4dd9e8a
Merge: 729ea4e06420 58ca0060ec4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 12:00:10 2020 -0700

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto updates from Herbert Xu:
     "API:
       - Introduce crypto_shash_tfm_digest() and use it wherever possible.
       - Fix use-after-free and race in crypto_spawn_alg.
       - Add support for parallel and batch requests to crypto_engine.
    
      Algorithms:
       - Update jitter RNG for SP800-90B compliance.
       - Always use jitter RNG as seed in drbg.
    
      Drivers:
       - Add Arm CryptoCell driver cctrng.
       - Add support for SEV-ES to the PSP driver in ccp"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (114 commits)
      crypto: hisilicon - fix driver compatibility issue with different versions of devices
      crypto: engine - do not requeue in case of fatal error
      crypto: cavium/nitrox - Fix a typo in a comment
      crypto: hisilicon/qm - change debugfs file name from qm_regs to regs
      crypto: hisilicon/qm - add DebugFS for xQC and xQE dump
      crypto: hisilicon/zip - add debugfs for Hisilicon ZIP
      crypto: hisilicon/hpre - add debugfs for Hisilicon HPRE
      crypto: hisilicon/sec2 - add debugfs for Hisilicon SEC
      crypto: hisilicon/qm - add debugfs to the QM state machine
      crypto: hisilicon/qm - add debugfs for QM
      crypto: stm32/crc32 - protect from concurrent accesses
      crypto: stm32/crc32 - don't sleep in runtime pm
      crypto: stm32/crc32 - fix multi-instance
      crypto: stm32/crc32 - fix run-time self test issue.
      crypto: stm32/crc32 - fix ext4 chksum BUG_ON()
      crypto: hisilicon/zip - Use temporary sqe when doing work
      crypto: hisilicon - add device error report through abnormal irq
      crypto: hisilicon - remove codes of directly report device errors through MSI
      crypto: hisilicon - QM memory management optimization
      crypto: hisilicon - unify initial value assignment into QM
      ...

commit bc183dec08f9cb177cf5206a010b7a9e7b22e567
Author: Florian Westphal <fw@strlen.de>
Date:   Fri May 29 00:01:52 2020 +0200

    tcp: tcp_init_buffer_space can be static
    
    As of commit 98fa6271cfcb
    ("tcp: refactor setting the initial congestion window") this is called
    only from tcp_input.c, so it can be static.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 66e4b8331850..bca761ffa25f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -662,7 +662,6 @@ void tcp_initialize_rcv_mss(struct sock *sk);
 int tcp_mtu_to_mss(struct sock *sk, int pmtu);
 int tcp_mss_to_mtu(struct sock *sk, int mss);
 void tcp_mtup_init(struct sock *sk);
-void tcp_init_buffer_space(struct sock *sk);
 
 static inline void tcp_bound_rto(const struct sock *sk)
 {

commit d29245692a44d71d5e2e0770463184a693696232
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 27 17:34:58 2020 -0700

    tcp: ipv6: support RFC 6069 (TCP-LD)
    
    Make tcp_ld_RTO_revert() helper available to IPv6, and
    implement RFC 6069 :
    
    Quoting this RFC :
    
    3. Connectivity Disruption Indication
    
       For Internet Protocol version 6 (IPv6) [RFC2460], the counterpart of
       the ICMP destination unreachable message of code 0 (net unreachable)
       and of code 1 (host unreachable) is the ICMPv6 destination
       unreachable message of code 0 (no route to destination) [RFC4443].
       As with IPv4, a router should generate an ICMPv6 destination
       unreachable message of code 0 in response to a packet that cannot be
       delivered to its destination address because it lacks a matching
       entry in its routing table.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b681338a8320..66e4b8331850 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -437,6 +437,7 @@ u16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,
 void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 void tcp_v4_mtu_reduced(struct sock *sk);
 void tcp_req_err(struct sock *sk, u32 seq, bool abort);
+void tcp_ld_RTO_revert(struct sock *sk, u32 seq);
 int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_create_openreq_child(const struct sock *sk,
 				      struct request_sock *req,

commit da07f52d3caf6c24c6dbffb5500f379d819e04bd
Merge: 93d43e58683e f85c1598ddfe
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 15 13:48:59 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Move the bpf verifier trace check into the new switch statement in
    HEAD.
    
    Resolve the overlapping changes in hinic, where bug fixes overlap
    the addition of VF support.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 24adbc1676af4e134e709ddc7f34cf2adc2131e4
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 12 06:54:30 2020 -0700

    tcp: fix SO_RCVLOWAT hangs with fat skbs
    
    We autotune rcvbuf whenever SO_RCVLOWAT is set to account for 100%
    overhead in tcp_set_rcvlowat()
    
    This works well when skb->len/skb->truesize ratio is bigger than 0.5
    
    But if we receive packets with small MSS, we can end up in a situation
    where not enough bytes are available in the receive queue to satisfy
    RCVLOWAT setting.
    As our sk_rcvbuf limit is hit, we send zero windows in ACK packets,
    preventing remote peer from sending more data.
    
    Even autotuning does not help, because it only triggers at the time
    user process drains the queue. If no EPOLLIN is generated, this
    can not happen.
    
    Note poll() has a similar issue, after commit
    c7004482e8dc ("tcp: Respect SO_RCVLOWAT in tcp_poll().")
    
    Fixes: 03f45c883c6f ("tcp: avoid extra wakeups for SO_RCVLOWAT users")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 64f84683feae..6f8e60c6fbc7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1420,6 +1420,19 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk, READ_ONCE(sk->sk_rcvbuf));
 }
 
+/* We provision sk_rcvbuf around 200% of sk_rcvlowat.
+ * If 87.5 % (7/8) of the space has been consumed, we want to override
+ * SO_RCVLOWAT constraint, since we are receiving skbs with too small
+ * len/truesize ratio.
+ */
+static inline bool tcp_rmem_pressure(const struct sock *sk)
+{
+	int rcvbuf = READ_ONCE(sk->sk_rcvbuf);
+	int threshold = rcvbuf - (rcvbuf >> 3);
+
+	return atomic_read(&sk->sk_rmem_alloc) > threshold;
+}
+
 extern void tcp_openreq_init_rwin(struct request_sock *req,
 				  const struct sock *sk_listener,
 				  const struct dst_entry *dst);

commit 2aaba014b55be46affcae78edff356c5e3389081
Author: Eric Biggers <ebiggers@google.com>
Date:   Sat May 2 11:24:26 2020 -0700

    crypto: lib/sha1 - remove unnecessary includes of linux/cryptohash.h
    
    <linux/cryptohash.h> sounds very generic and important, like it's the
    header to include if you're doing cryptographic hashing in the kernel.
    But actually it only includes the library implementation of the SHA-1
    compression function (not even the full SHA-1).  This should basically
    never be used anymore; SHA-1 is no longer considered secure, and there
    are much better ways to do cryptographic hashing in the kernel.
    
    Most files that include this header don't actually need it.  So in
    preparation for removing it, remove all these unneeded includes of it.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5fa9eacd965a..5948c8e4c9e1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -23,7 +23,6 @@
 #include <linux/cache.h>
 #include <linux/percpu.h>
 #include <linux/skbuff.h>
-#include <linux/cryptohash.h>
 #include <linux/kref.h>
 #include <linux/ktime.h>
 

commit 64082b67ba724c5a5acfff38d352068c4992d089
Author: Maciej Żenczykowski <maze@google.com>
Date:   Thu May 7 00:58:05 2020 -0700

    net: remove spurious declaration of tcp_default_init_rwnd()
    
    it doesn't actually exist...
    
    Test: builds and 'git grep tcp_default_init_rwnd' comes up empty
    Signed-off-by: Maciej Żenczykowski <maze@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dcf9a72eeaa6..64f84683feae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1376,7 +1376,6 @@ static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 	rx_opt->num_sacks = 0;
 }
 
-u32 tcp_default_init_rwnd(u32 mss);
 void tcp_cwnd_restart(struct sock *sk, s32 delta);
 
 static inline void tcp_slow_start_after_idle_check(struct sock *sk)

commit 8dc242ad661c2694a582541c2264ffc0e7c4d27d
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon May 4 11:27:49 2020 -0700

    tcp: refine tcp_pacing_delay() for very low pacing rates
    
    With the addition of horizon feature to sch_fq, we noticed some
    suboptimal behavior of extremely low pacing rate TCP flows, especially
    when TCP is not aware of a drop happening in lower stacks.
    
    Back in commit 3f80e08f40cd ("tcp: add tcp_reset_xmit_timer() helper"),
    tcp_pacing_delay() was added to estimate an extra delay to add to standard
    rto timers.
    
    This patch removes the skb argument from this helper and
    tcp_reset_xmit_timer() because it makes more sense to simply
    consider the time at which next packet is allowed to be sent,
    instead of the time of whatever packet has been sent.
    
    This avoids arming RTO timer too soon and removes
    spurious horizon drops.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1beed50522b1..43b87a8d4790 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1289,26 +1289,22 @@ static inline bool tcp_needs_internal_pacing(const struct sock *sk)
 	return smp_load_acquire(&sk->sk_pacing_status) == SK_PACING_NEEDED;
 }
 
-/* Return in jiffies the delay before one skb is sent.
- * If @skb is NULL, we look at EDT for next packet being sent on the socket.
+/* Estimates in how many jiffies next packet for this flow can be sent.
+ * Scheduling a retransmit timer too early would be silly.
  */
-static inline unsigned long tcp_pacing_delay(const struct sock *sk,
-					     const struct sk_buff *skb)
+static inline unsigned long tcp_pacing_delay(const struct sock *sk)
 {
-	s64 pacing_delay = skb ? skb->tstamp : tcp_sk(sk)->tcp_wstamp_ns;
+	s64 delay = tcp_sk(sk)->tcp_wstamp_ns - tcp_sk(sk)->tcp_clock_cache;
 
-	pacing_delay -= tcp_sk(sk)->tcp_clock_cache;
-
-	return pacing_delay > 0 ? nsecs_to_jiffies(pacing_delay) : 0;
+	return delay > 0 ? nsecs_to_jiffies(delay) : 0;
 }
 
 static inline void tcp_reset_xmit_timer(struct sock *sk,
 					const int what,
 					unsigned long when,
-					const unsigned long max_when,
-					const struct sk_buff *skb)
+					const unsigned long max_when)
 {
-	inet_csk_reset_xmit_timer(sk, what, when + tcp_pacing_delay(sk, skb),
+	inet_csk_reset_xmit_timer(sk, what, when + tcp_pacing_delay(sk),
 				  max_when);
 }
 
@@ -1336,8 +1332,7 @@ static inline void tcp_check_probe_timer(struct sock *sk)
 {
 	if (!tcp_sk(sk)->packets_out && !inet_csk(sk)->icsk_pending)
 		tcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
-				     tcp_probe0_base(sk), TCP_RTO_MAX,
-				     NULL);
+				     tcp_probe0_base(sk), TCP_RTO_MAX);
 }
 
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)

commit f0628c524fd188c3f9418e12478dfdfadacba815
Author: Cambda Zhu <cambda@linux.alibaba.com>
Date:   Fri Apr 24 16:06:16 2020 +0800

    net: Replace the limit of TCP_LINGER2 with TCP_FIN_TIMEOUT_MAX
    
    This patch changes the behavior of TCP_LINGER2 about its limit. The
    sysctl_tcp_fin_timeout used to be the limit of TCP_LINGER2 but now it's
    only the default value. A new macro named TCP_FIN_TIMEOUT_MAX is added
    as the limit of TCP_LINGER2, which is 2 minutes.
    
    Since TCP_LINGER2 used sysctl_tcp_fin_timeout as the default value
    and the limit in the past, the system administrator cannot set the
    default value for most of sockets and let some sockets have a greater
    timeout. It might be a mistake that let the sysctl to be the limit of
    the TCP_LINGER2. Maybe we can add a new sysctl to set the max of
    TCP_LINGER2, but FIN-WAIT-2 timeout is usually no need to be too long
    and 2 minutes are legal considering TCP specs.
    
    Changes in v3:
    - Remove the new socket option and change the TCP_LINGER2 behavior so
      that the timeout can be set to value between sysctl_tcp_fin_timeout
      and 2 minutes.
    
    Changes in v2:
    - Add int overflow check for the new socket option.
    
    Changes in v1:
    - Add a new socket option to set timeout greater than
      sysctl_tcp_fin_timeout.
    
    Signed-off-by: Cambda Zhu <cambda@linux.alibaba.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dcf9a72eeaa6..1beed50522b1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -126,6 +126,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 				  * to combine FIN-WAIT-2 timeout with
 				  * TIME-WAIT timer.
 				  */
+#define TCP_FIN_TIMEOUT_MAX (120 * HZ) /* max TCP_LINGER2 value (two minutes) */
 
 #define TCP_DELACK_MAX	((unsigned)(HZ/5))	/* maximal time to delay before sending an ACK */
 #if HZ >= 100

commit 9bacd256f1354883d3c1402655153367982bba49
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 17 07:10:23 2020 -0700

    tcp: cache line align MAX_TCP_HEADER
    
    TCP stack is dumb in how it cooks its output packets.
    
    Depending on MAX_HEADER value, we might chose a bad ending point
    for the headers.
    
    If we align the end of TCP headers to cache line boundary, we
    make sure to always use the smallest number of cache lines,
    which always help.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5fa9eacd965a..dcf9a72eeaa6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -51,7 +51,7 @@ extern struct inet_hashinfo tcp_hashinfo;
 extern struct percpu_counter tcp_orphan_count;
 void tcp_time_wait(struct sock *sk, int state, int timeo);
 
-#define MAX_TCP_HEADER	(128 + MAX_HEADER)
+#define MAX_TCP_HEADER	L1_CACHE_ALIGN(128 + MAX_HEADER)
 #define MAX_TCP_OPTION_SPACE 40
 #define TCP_MIN_SND_MSS		48
 #define TCP_MIN_GSO_SIZE	(TCP_MIN_SND_MSS - MAX_TCP_OPTION_SPACE)

commit c0fd336ea4ca82377fbb0954db09d765eb794339
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Fri Mar 20 10:34:26 2020 +0800

    bpf, tcp: Make tcp_bpf_recvmsg static
    
    After commit f747632b608f ("bpf: sockmap: Move generic sockmap
    hooks from BPF TCP"), tcp_bpf_recvmsg() is not used out of
    tcp_bpf.c, so make it static and remove it from tcp.h. Also move
    it to BPF_STREAM_PARSER #ifdef to fix unused function warnings.
    
    Fixes: f747632b608f ("bpf: sockmap: Move generic sockmap hooks from BPF TCP")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200320023426.60684-3-yuehaibing@huawei.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 43fa07a36fa6..5fa9eacd965a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2207,8 +2207,6 @@ static inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)
 #ifdef CONFIG_NET_SOCK_MSG
 int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
 			  int flags);
-int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
-		    int nonblock, int flags, int *addr_len);
 int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
 		      struct msghdr *msg, int len, int flags);
 #endif /* CONFIG_NET_SOCK_MSG */

commit f747632b608f90217a4e9ebb1deba8a37612aa32
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:36 2020 +0000

    bpf: sockmap: Move generic sockmap hooks from BPF TCP
    
    The init, close and unhash handlers from TCP sockmap are generic,
    and can be reused by UDP sockmap. Move the helpers into the sockmap code
    base and expose them. This requires tcp_bpf_get_proto and tcp_bpf_clone to
    be conditional on BPF_STREAM_PARSER.
    
    The moved functions are unmodified, except that sk_psock_unlink is
    renamed to sock_map_unlink to better match its behaviour.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-6-lmb@cloudflare.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ad3abeaa703e..43fa07a36fa6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2195,19 +2195,22 @@ void tcp_update_ulp(struct sock *sk, struct proto *p,
 struct sk_msg;
 struct sk_psock;
 
+#ifdef CONFIG_BPF_STREAM_PARSER
+struct proto *tcp_bpf_get_proto(struct sock *sk, struct sk_psock *psock);
+void tcp_bpf_clone(const struct sock *sk, struct sock *newsk);
+#else
+static inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)
+{
+}
+#endif /* CONFIG_BPF_STREAM_PARSER */
+
 #ifdef CONFIG_NET_SOCK_MSG
-int tcp_bpf_init(struct sock *sk);
 int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
 			  int flags);
 int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		    int nonblock, int flags, int *addr_len);
 int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
 		      struct msghdr *msg, int len, int flags);
-void tcp_bpf_clone(const struct sock *sk, struct sock *newsk);
-#else
-static inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)
-{
-}
 #endif /* CONFIG_NET_SOCK_MSG */
 
 /* Call BPF_SOCK_OPS program that returns an int. If the return value

commit 5da0040442312a2b696748f8240243ce543a4970
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:35 2020 +0000

    bpf: tcp: Guard declarations with CONFIG_NET_SOCK_MSG
    
    tcp_bpf.c is only included in the build if CONFIG_NET_SOCK_MSG is
    selected. The declaration should therefore be guarded as such.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-5-lmb@cloudflare.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ccf39d80b695..ad3abeaa703e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2195,6 +2195,7 @@ void tcp_update_ulp(struct sock *sk, struct proto *p,
 struct sk_msg;
 struct sk_psock;
 
+#ifdef CONFIG_NET_SOCK_MSG
 int tcp_bpf_init(struct sock *sk);
 int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
 			  int flags);
@@ -2202,13 +2203,12 @@ int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		    int nonblock, int flags, int *addr_len);
 int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
 		      struct msghdr *msg, int len, int flags);
-#ifdef CONFIG_NET_SOCK_MSG
 void tcp_bpf_clone(const struct sock *sk, struct sock *newsk);
 #else
 static inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)
 {
 }
-#endif
+#endif /* CONFIG_NET_SOCK_MSG */
 
 /* Call BPF_SOCK_OPS program that returns an int. If the return value
  * is < 0, then the BPF op failed (for example if the loaded BPF

commit d19da360ee0f3e6c1375391db1a724b66fd43312
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Mar 9 11:12:34 2020 +0000

    bpf: tcp: Move assertions into tcp_bpf_get_proto
    
    We need to ensure that sk->sk_prot uses certain callbacks, so that
    code that directly calls e.g. tcp_sendmsg in certain corner cases
    works. To avoid spurious asserts, we must to do this only if
    sk_psock_update_proto has not yet been called. The same invariants
    apply for tcp_bpf_check_v6_needs_rebuild, so move the call as well.
    
    Doing so allows us to merge tcp_bpf_init and tcp_bpf_reinit.
    
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309111243.6982-4-lmb@cloudflare.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 07f947cc80e6..ccf39d80b695 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2196,7 +2196,6 @@ struct sk_msg;
 struct sk_psock;
 
 int tcp_bpf_init(struct sock *sk);
-void tcp_bpf_reinit(struct sock *sk);
 int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
 			  int flags);
 int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,

commit e80251555f0befd1271e74b080bccf0ff0348bfc
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Tue Feb 18 17:10:15 2020 +0000

    tcp_bpf: Don't let child socket inherit parent protocol ops on copy
    
    Prepare for cloning listening sockets that have their protocol callbacks
    overridden by sk_msg. Child sockets must not inherit parent callbacks that
    access state stored in sk_user_data owned by the parent.
    
    Restore the child socket protocol callbacks before it gets hashed and any
    of the callbacks can get invoked.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200218171023.844439-4-jakub@cloudflare.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a5ea27df3c2b..07f947cc80e6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2203,6 +2203,13 @@ int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		    int nonblock, int flags, int *addr_len);
 int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
 		      struct msghdr *msg, int len, int flags);
+#ifdef CONFIG_NET_SOCK_MSG
+void tcp_bpf_clone(const struct sock *sk, struct sock *newsk);
+#else
+static inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)
+{
+}
+#endif
 
 /* Call BPF_SOCK_OPS program that returns an int. If the return value
  * is < 0, then the BPF op failed (for example if the loaded BPF

commit 954b3c4397792c8614aa4aaf25030ae87ece8307
Merge: c5d19a6ecfce 85cc12f85138
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jan 23 08:10:16 2020 +0100

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf-next 2020-01-22
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 92 non-merge commits during the last 16 day(s) which contain
    a total of 320 files changed, 7532 insertions(+), 1448 deletions(-).
    
    The main changes are:
    
    1) function by function verification and program extensions from Alexei.
    
    2) massive cleanup of selftests/bpf from Toke and Andrii.
    
    3) batched bpf map operations from Brian and Yonghong.
    
    4) tcp congestion control in bpf from Martin.
    
    5) bulking for non-map xdp_redirect form Toke.
    
    6) bpf_send_signal_thread helper from Yonghong.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b3f7e3f23a763ccaae7b52d88d2c91e66c80d406
Merge: 4ee9e6e027c0 7008ee121089
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Jan 19 22:10:04 2020 +0100

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/netdev/net

commit 33bfe20dd7117dd81fd896a53f743a233e1ad64f
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Jan 11 06:12:01 2020 +0000

    bpf: Sockmap/tls, push write_space updates through ulp updates
    
    When sockmap sock with TLS enabled is removed we cleanup bpf/psock state
    and call tcp_update_ulp() to push updates to TLS ULP on top. However, we
    don't push the write_space callback up and instead simply overwrite the
    op with the psock stored previous op. This may or may not be correct so
    to ensure we don't overwrite the TLS write space hook pass this field to
    the ULP and have it fixup the ctx.
    
    This completes a previous fix that pushed the ops through to the ULP
    but at the time missed doing this for write_space, presumably because
    write_space TLS hook was added around the same time.
    
    Fixes: 95fa145479fbc ("bpf: sockmap/tls, close can race with map free")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/bpf/20200111061206.8028-4-john.fastabend@gmail.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e460ea7f767b..e6f48384dc71 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2147,7 +2147,8 @@ struct tcp_ulp_ops {
 	/* initialize ulp */
 	int (*init)(struct sock *sk);
 	/* update ulp */
-	void (*update)(struct sock *sk, struct proto *p);
+	void (*update)(struct sock *sk, struct proto *p,
+		       void (*write_space)(struct sock *sk));
 	/* cleanup ulp */
 	void (*release)(struct sock *sk);
 	/* diagnostic */
@@ -2162,7 +2163,8 @@ void tcp_unregister_ulp(struct tcp_ulp_ops *type);
 int tcp_set_ulp(struct sock *sk, const char *name);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
-void tcp_update_ulp(struct sock *sk, struct proto *p);
+void tcp_update_ulp(struct sock *sk, struct proto *p,
+		    void (*write_space)(struct sock *sk));
 
 #define MODULE_ALIAS_TCP_ULP(name)				\
 	__MODULE_INFO(alias, alias_userspace, name);		\

commit 35b2c32116091ef87a15c604cac363da8322a288
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:21 2020 -0800

    tcp: Export TCP functions and ops struct
    
    MPTCP will make use of tcp_send_mss() and tcp_push() when sending
    data to specific TCP subflows.
    
    tcp_request_sock_ipvX_ops and ipvX_specific will be referenced
    during TCP subflow creation.
    
    Co-developed-by: Peter Krystad <peter.krystad@linux.intel.com>
    Signed-off-by: Peter Krystad <peter.krystad@linux.intel.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 13bc83fab454..5e4133d09b9d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -330,6 +330,9 @@ int tcp_sendpage_locked(struct sock *sk, struct page *page, int offset,
 			size_t size, int flags);
 ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,
 		 size_t size, int flags);
+int tcp_send_mss(struct sock *sk, int *size_goal, int flags);
+void tcp_push(struct sock *sk, int flags, int mss_now, int nonagle,
+	      int size_goal);
 void tcp_release_cb(struct sock *sk);
 void tcp_wfree(struct sk_buff *skb);
 void tcp_write_timer_handler(struct sock *sk);
@@ -2011,6 +2014,11 @@ struct tcp_request_sock_ops {
 			   enum tcp_synack_type synack_type);
 };
 
+extern const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops;
+#if IS_ENABLED(CONFIG_IPV6)
+extern const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops;
+#endif
+
 #ifdef CONFIG_SYN_COOKIES
 static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
 					 const struct sock *sk, struct sk_buff *skb,

commit 85712484110df308215077be6ee21c4e57d7dec2
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:20 2020 -0800

    tcp: coalesce/collapse must respect MPTCP extensions
    
    Coalesce and collapse of packets carrying MPTCP extensions is allowed
    when the newer packet has no extension or the extensions carried by both
    packets are equal.
    
    This allows merging of TSO packet trains and even cross-TSO packets, and
    does not require any additional action when moving data into existing
    SKBs.
    
    v3 -> v4:
     - allow collapsing, under mptcp_skb_can_collapse() constraint
    
    v5 -> v6:
     - clarify MPTCP skb extensions must always be cleared at allocation
       time
    
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ac52633e7061..13bc83fab454 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -39,6 +39,7 @@
 #include <net/tcp_states.h>
 #include <net/inet_ecn.h>
 #include <net/dst.h>
+#include <net/mptcp.h>
 
 #include <linux/seq_file.h>
 #include <linux/memcontrol.h>
@@ -978,6 +979,13 @@ static inline bool tcp_skb_can_collapse_to(const struct sk_buff *skb)
 	return likely(!TCP_SKB_CB(skb)->eor);
 }
 
+static inline bool tcp_skb_can_collapse(const struct sk_buff *to,
+					const struct sk_buff *from)
+{
+	return likely(tcp_skb_can_collapse_to(to) &&
+		      mptcp_skb_can_collapse(to, from));
+}
+
 /* Events passed to congestion control interface */
 enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */

commit 1323059301c8f36d933876233516245d882346a6
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:18 2020 -0800

    tcp, ulp: Add clone operation to tcp_ulp_ops
    
    If ULP is used on a listening socket, icsk_ulp_ops and icsk_ulp_data are
    copied when the listener is cloned. Sometimes the clone is immediately
    deleted, which will invoke the release op on the clone and likely
    corrupt the listening socket's icsk_ulp_data.
    
    The clone operation is invoked immediately after the clone is copied and
    gives the ULP type an opportunity to set up the clone socket and its
    icsk_ulp_data.
    
    The MPTCP ULP clone will silently fallback to plain TCP on allocation
    failure, so 'clone()' does not need to return an error code.
    
    v6 -> v7:
     - move and rename ulp clone helper to make it inline-friendly
    v5 -> v6:
     - clarified MPTCP clone usage in commit message
    
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 85f1d7ff6e8b..ac52633e7061 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2154,6 +2154,9 @@ struct tcp_ulp_ops {
 	/* diagnostic */
 	int (*get_info)(const struct sock *sk, struct sk_buff *skb);
 	size_t (*get_info_size)(const struct sock *sk);
+	/* clone ulp */
+	void (*clone)(const struct request_sock *req, struct sock *newsk,
+		      const gfp_t priority);
 
 	char		name[TCP_ULP_NAME_MAX];
 	struct module	*owner;

commit c74a39c861aeaf6b789b7abdbb3256f54c9fb365
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:17 2020 -0800

    tcp: Add MPTCP option number
    
    TCP option 30 is allocated for MPTCP by the IANA.
    
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7df37e2fddca..85f1d7ff6e8b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -182,6 +182,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_SACK             5       /* SACK Block */
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
+#define TCPOPT_MPTCP		30	/* Multipath TCP (RFC6824) */
 #define TCPOPT_FASTOPEN		34	/* Fast open (RFC7413) */
 #define TCPOPT_EXP		254	/* Experimental */
 /* Magic number to be after the option value for sharing TCP

commit 0baf26b0fcd74bbfcef53c5d5e8bad2b99c8d0d2
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:08 2020 -0800

    bpf: tcp: Support tcp_congestion_ops in bpf
    
    This patch makes "struct tcp_congestion_ops" to be the first user
    of BPF STRUCT_OPS.  It allows implementing a tcp_congestion_ops
    in bpf.
    
    The BPF implemented tcp_congestion_ops can be used like
    regular kernel tcp-cc through sysctl and setsockopt.  e.g.
    [root@arch-fb-vm1 bpf]# sysctl -a | egrep congestion
    net.ipv4.tcp_allowed_congestion_control = reno cubic bpf_cubic
    net.ipv4.tcp_available_congestion_control = reno bic cubic bpf_cubic
    net.ipv4.tcp_congestion_control = bpf_cubic
    
    There has been attempt to move the TCP CC to the user space
    (e.g. CCP in TCP).   The common arguments are faster turn around,
    get away from long-tail kernel versions in production...etc,
    which are legit points.
    
    BPF has been the continuous effort to join both kernel and
    userspace upsides together (e.g. XDP to gain the performance
    advantage without bypassing the kernel).  The recent BPF
    advancements (in particular BTF-aware verifier, BPF trampoline,
    BPF CO-RE...) made implementing kernel struct ops (e.g. tcp cc)
    possible in BPF.  It allows a faster turnaround for testing algorithm
    in the production while leveraging the existing (and continue growing)
    BPF feature/framework instead of building one specifically for
    userspace TCP CC.
    
    This patch allows write access to a few fields in tcp-sock
    (in bpf_tcp_ca_btf_struct_access()).
    
    The optional "get_info" is unsupported now.  It can be added
    later.  One possible way is to output the info with a btf-id
    to describe the content.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003508.3856115-1-kafai@fb.com

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7df37e2fddca..9dd975be7fdf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1007,6 +1007,7 @@ enum tcp_ca_ack_event_flags {
 #define TCP_CONG_NON_RESTRICTED 0x1
 /* Requires ECN/ECT set on all packets */
 #define TCP_CONG_NEEDS_ECN	0x2
+#define TCP_CONG_MASK	(TCP_CONG_NON_RESTRICTED | TCP_CONG_NEEDS_ECN)
 
 union tcp_cc_info;
 
@@ -1101,6 +1102,7 @@ u32 tcp_reno_undo_cwnd(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
+struct tcp_congestion_ops *tcp_ca_find(const char *name);
 struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
 u32 tcp_ca_get_key_by_name(struct net *net, const char *name, bool *ecn_ca);
 #ifdef CONFIG_INET

commit dea53bb80e07b9e1641b865493908c20cb8df2ac
Author: David Ahern <dsahern@gmail.com>
Date:   Mon Dec 30 14:14:28 2019 -0800

    tcp: Add l3index to tcp_md5sig_key and md5 functions
    
    Add l3index to tcp_md5sig_key to represent the L3 domain of a key, and
    add l3index to tcp_md5_do_add and tcp_md5_do_del to fill in the key.
    
    With the key now based on an l3index, add the new parameter to the
    lookup functions and consider the l3index when looking for a match.
    
    The l3index comes from the skb when processing ingress packets leveraging
    the helpers created for socket lookups, tcp_v4_sdif and inet_iif (and the
    v6 variants). When the sdif index is set it means the packet ingressed a
    device that is part of an L3 domain and inet_iif points to the VRF device.
    For egress, the L3 domain is determined from the socket binding and
    sk_bound_dev_if.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e460ea7f767b..7df37e2fddca 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1532,8 +1532,9 @@ struct tcp_md5sig_key {
 	struct hlist_node	node;
 	u8			keylen;
 	u8			family; /* AF_INET or AF_INET6 */
-	union tcp_md5_addr	addr;
 	u8			prefixlen;
+	union tcp_md5_addr	addr;
+	int			l3index; /* set if key added with L3 scope */
 	u8			key[TCP_MD5SIG_MAXKEYLEN];
 	struct rcu_head		rcu;
 };
@@ -1577,34 +1578,33 @@ struct tcp_md5sig_pool {
 int tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,
 			const struct sock *sk, const struct sk_buff *skb);
 int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
-		   int family, u8 prefixlen, const u8 *newkey, u8 newkeylen,
-		   gfp_t gfp);
+		   int family, u8 prefixlen, int l3index,
+		   const u8 *newkey, u8 newkeylen, gfp_t gfp);
 int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
-		   int family, u8 prefixlen);
+		   int family, u8 prefixlen, int l3index);
 struct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,
 					 const struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
 #include <linux/jump_label.h>
 extern struct static_key_false tcp_md5_needed;
-struct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk,
+struct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk, int l3index,
 					   const union tcp_md5_addr *addr,
 					   int family);
 static inline struct tcp_md5sig_key *
-tcp_md5_do_lookup(const struct sock *sk,
-		  const union tcp_md5_addr *addr,
-		  int family)
+tcp_md5_do_lookup(const struct sock *sk, int l3index,
+		  const union tcp_md5_addr *addr, int family)
 {
 	if (!static_branch_unlikely(&tcp_md5_needed))
 		return NULL;
-	return __tcp_md5_do_lookup(sk, addr, family);
+	return __tcp_md5_do_lookup(sk, l3index, addr, family);
 }
 
 #define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_key)
 #else
-static inline struct tcp_md5sig_key *tcp_md5_do_lookup(const struct sock *sk,
-					 const union tcp_md5_addr *addr,
-					 int family)
+static inline struct tcp_md5sig_key *
+tcp_md5_do_lookup(const struct sock *sk, int l3index,
+		  const union tcp_md5_addr *addr, int family)
 {
 	return NULL;
 }

commit ee2aabd3fc2eef4c1a0ebdadccc76fbff74b94fc
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 12:55:30 2019 -0800

    tcp: refine tcp_write_queue_empty() implementation
    
    Due to how tcp_sendmsg() is implemented, we can have an empty
    skb at the tail of the write queue.
    
    Most [1] tcp_write_queue_empty() callers want to know if there is
    anything to send (payload and/or FIN)
    
    Instead of checking if the sk_write_queue is empty, we need
    to test if tp->write_seq == tp->snd_nxt
    
    [1] tcp_send_fin() was the only caller that expected to
     see if an skb was in the write queue, I have changed the code
     to reuse the tcp_write_queue_tail() result.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 86b9a8766648..e460ea7f767b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1766,9 +1766,18 @@ static inline bool tcp_skb_is_last(const struct sock *sk,
 	return skb_queue_is_last(&sk->sk_write_queue, skb);
 }
 
+/**
+ * tcp_write_queue_empty - test if any payload (or FIN) is available in write queue
+ * @sk: socket
+ *
+ * Since the write queue can have a temporary empty skb in it,
+ * we must not use "return skb_queue_empty(&sk->sk_write_queue)"
+ */
 static inline bool tcp_write_queue_empty(const struct sock *sk)
 {
-	return skb_queue_empty(&sk->sk_write_queue);
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	return tp->write_seq == tp->snd_nxt;
 }
 
 static inline bool tcp_rtx_queue_empty(const struct sock *sk)

commit 721c8dafad26ccfa90ff659ee19755e3377b829d
Author: Guillaume Nault <gnault@redhat.com>
Date:   Fri Dec 6 12:38:49 2019 +0100

    tcp: Protect accesses to .ts_recent_stamp with {READ,WRITE}_ONCE()
    
    Syncookies borrow the ->rx_opt.ts_recent_stamp field to store the
    timestamp of the last synflood. Protect them with READ_ONCE() and
    WRITE_ONCE() since reads and writes aren't serialised.
    
    Use of .rx_opt.ts_recent_stamp for storing the synflood timestamp was
    introduced by a0f82f64e269 ("syncookies: remove last_synq_overflow from
    struct tcp_sock"). But unprotected accesses were already there when
    timestamp was stored in .last_synq_overflow.
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 43e04e14c41e..86b9a8766648 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -501,9 +501,9 @@ static inline void tcp_synq_overflow(const struct sock *sk)
 		}
 	}
 
-	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	last_overflow = READ_ONCE(tcp_sk(sk)->rx_opt.ts_recent_stamp);
 	if (!time_between32(now, last_overflow, last_overflow + HZ))
-		tcp_sk(sk)->rx_opt.ts_recent_stamp = now;
+		WRITE_ONCE(tcp_sk(sk)->rx_opt.ts_recent_stamp, now);
 }
 
 /* syncookies: no recent synqueue overflow on this listening socket? */
@@ -524,7 +524,7 @@ static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
 		}
 	}
 
-	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	last_overflow = READ_ONCE(tcp_sk(sk)->rx_opt.ts_recent_stamp);
 
 	/* If last_overflow <= jiffies <= last_overflow + TCP_SYNCOOKIE_VALID,
 	 * then we're under synflood. However, we have to use

commit cb44a08f8647fd2e8db5cc9ac27cd8355fa392d8
Author: Guillaume Nault <gnault@redhat.com>
Date:   Fri Dec 6 12:38:43 2019 +0100

    tcp: tighten acceptance of ACKs not matching a child socket
    
    When no synflood occurs, the synflood timestamp isn't updated.
    Therefore it can be so old that time_after32() can consider it to be
    in the future.
    
    That's a problem for tcp_synq_no_recent_overflow() as it may report
    that a recent overflow occurred while, in fact, it's just that jiffies
    has grown past 'last_overflow' + TCP_SYNCOOKIE_VALID + 2^31.
    
    Spurious detection of recent overflows lead to extra syncookie
    verification in cookie_v[46]_check(). At that point, the verification
    should fail and the packet dropped. But we should have dropped the
    packet earlier as we didn't even send a syncookie.
    
    Let's refine tcp_synq_no_recent_overflow() to report a recent overflow
    only if jiffies is within the
    [last_overflow, last_overflow + TCP_SYNCOOKIE_VALID] interval. This
    way, no spurious recent overflow is reported when jiffies wraps and
    'last_overflow' becomes in the future from the point of view of
    time_after32().
    
    However, if jiffies wraps and enters the
    [last_overflow, last_overflow + TCP_SYNCOOKIE_VALID] interval (with
    'last_overflow' being a stale synflood timestamp), then
    tcp_synq_no_recent_overflow() still erroneously reports an
    overflow. In such cases, we have to rely on syncookie verification
    to drop the packet. We unfortunately have no way to differentiate
    between a fresh and a stale syncookie timestamp.
    
    In practice, using last_overflow as lower bound is problematic.
    If the synflood timestamp is concurrently updated between the time
    we read jiffies and the moment we store the timestamp in
    'last_overflow', then 'now' becomes smaller than 'last_overflow' and
    tcp_synq_no_recent_overflow() returns true, potentially dropping a
    valid syncookie.
    
    Reading jiffies after loading the timestamp could fix the problem,
    but that'd require a memory barrier. Let's just accommodate for
    potential timestamp growth instead and extend the interval using
    'last_overflow - HZ' as lower bound.
    
    Signed-off-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7d734ba391fc..43e04e14c41e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -518,13 +518,23 @@ static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
 		reuse = rcu_dereference(sk->sk_reuseport_cb);
 		if (likely(reuse)) {
 			last_overflow = READ_ONCE(reuse->synq_overflow_ts);
-			return time_after32(now, last_overflow +
-					    TCP_SYNCOOKIE_VALID);
+			return !time_between32(now, last_overflow - HZ,
+					       last_overflow +
+					       TCP_SYNCOOKIE_VALID);
 		}
 	}
 
 	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
-	return time_after32(now, last_overflow + TCP_SYNCOOKIE_VALID);
+
+	/* If last_overflow <= jiffies <= last_overflow + TCP_SYNCOOKIE_VALID,
+	 * then we're under synflood. However, we have to use
+	 * 'last_overflow - HZ' as lower bound. That's because a concurrent
+	 * tcp_synq_overflow() could update .ts_recent_stamp after we read
+	 * jiffies but before we store .ts_recent_stamp into last_overflow,
+	 * which could lead to rejecting a valid syncookie.
+	 */
+	return !time_between32(now, last_overflow - HZ,
+			       last_overflow + TCP_SYNCOOKIE_VALID);
 }
 
 static inline u32 tcp_cookie_time(void)

commit 04d26e7b159a396372646a480f4caa166d1b6720
Author: Guillaume Nault <gnault@redhat.com>
Date:   Fri Dec 6 12:38:36 2019 +0100

    tcp: fix rejected syncookies due to stale timestamps
    
    If no synflood happens for a long enough period of time, then the
    synflood timestamp isn't refreshed and jiffies can advance so much
    that time_after32() can't accurately compare them any more.
    
    Therefore, we can end up in a situation where time_after32(now,
    last_overflow + HZ) returns false, just because these two values are
    too far apart. In that case, the synflood timestamp isn't updated as
    it should be, which can trick tcp_synq_no_recent_overflow() into
    rejecting valid syncookies.
    
    For example, let's consider the following scenario on a system
    with HZ=1000:
    
      * The synflood timestamp is 0, either because that's the timestamp
        of the last synflood or, more commonly, because we're working with
        a freshly created socket.
    
      * We receive a new SYN, which triggers synflood protection. Let's say
        that this happens when jiffies == 2147484649 (that is,
        'synflood timestamp' + HZ + 2^31 + 1).
    
      * Then tcp_synq_overflow() doesn't update the synflood timestamp,
        because time_after32(2147484649, 1000) returns false.
        With:
          - 2147484649: the value of jiffies, aka. 'now'.
          - 1000: the value of 'last_overflow' + HZ.
    
      * A bit later, we receive the ACK completing the 3WHS. But
        cookie_v[46]_check() rejects it because tcp_synq_no_recent_overflow()
        says that we're not under synflood. That's because
        time_after32(2147484649, 120000) returns false.
        With:
          - 2147484649: the value of jiffies, aka. 'now'.
          - 120000: the value of 'last_overflow' + TCP_SYNCOOKIE_VALID.
    
        Of course, in reality jiffies would have increased a bit, but this
        condition will last for the next 119 seconds, which is far enough
        to accommodate for jiffie's growth.
    
    Fix this by updating the overflow timestamp whenever jiffies isn't
    within the [last_overflow, last_overflow + HZ] range. That shouldn't
    have any performance impact since the update still happens at most once
    per second.
    
    Now we're guaranteed to have fresh timestamps while under synflood, so
    tcp_synq_no_recent_overflow() can safely use it with time_after32() in
    such situations.
    
    Stale timestamps can still make tcp_synq_no_recent_overflow() return
    the wrong verdict when not under synflood. This will be handled in the
    next patch.
    
    For 64 bits architectures, the problem was introduced with the
    conversion of ->tw_ts_recent_stamp to 32 bits integer by commit
    cca9bab1b72c ("tcp: use monotonic timestamps for PAWS").
    The problem has always been there on 32 bits architectures.
    
    Fixes: cca9bab1b72c ("tcp: use monotonic timestamps for PAWS")
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 36f195fb576a..7d734ba391fc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -494,14 +494,15 @@ static inline void tcp_synq_overflow(const struct sock *sk)
 		reuse = rcu_dereference(sk->sk_reuseport_cb);
 		if (likely(reuse)) {
 			last_overflow = READ_ONCE(reuse->synq_overflow_ts);
-			if (time_after32(now, last_overflow + HZ))
+			if (!time_between32(now, last_overflow,
+					    last_overflow + HZ))
 				WRITE_ONCE(reuse->synq_overflow_ts, now);
 			return;
 		}
 	}
 
 	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
-	if (time_after32(now, last_overflow + HZ))
+	if (!time_between32(now, last_overflow, last_overflow + HZ))
 		tcp_sk(sk)->rx_opt.ts_recent_stamp = now;
 }
 

commit 200ecef67b8d09d16ec55f91c92751dcc7a38d40
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Nov 7 11:51:18 2019 -0800

    tcp: Remove one extra ktime_get_ns() from cookie_init_timestamp
    
    tcp_make_synack() already uses tcp_clock_ns(), and can pass
    the value to cookie_init_timestamp() to avoid another call
    to ktime_get_ns() helper.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ab4eb5eb5d07..36f195fb576a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -537,7 +537,7 @@ static inline u32 tcp_cookie_time(void)
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mss);
-u64 cookie_init_timestamp(struct request_sock *req);
+u64 cookie_init_timestamp(struct request_sock *req, u64 now);
 bool cookie_timestamp_decode(const struct net *net,
 			     struct tcp_options_received *opt);
 bool cookie_ecn_ok(const struct tcp_options_received *opt,
@@ -757,10 +757,16 @@ static inline u32 tcp_time_stamp(const struct tcp_sock *tp)
 	return div_u64(tp->tcp_mstamp, USEC_PER_SEC / TCP_TS_HZ);
 }
 
+/* Convert a nsec timestamp into TCP TSval timestamp (ms based currently) */
+static inline u32 tcp_ns_to_ts(u64 ns)
+{
+	return div_u64(ns, NSEC_PER_SEC / TCP_TS_HZ);
+}
+
 /* Could use tcp_clock_us() / 1000, but this version uses a single divide */
 static inline u32 tcp_time_stamp_raw(void)
 {
-	return div_u64(tcp_clock_ns(), NSEC_PER_SEC / TCP_TS_HZ);
+	return tcp_ns_to_ts(tcp_clock_ns());
 }
 
 void tcp_mstamp_refresh(struct tcp_sock *tp);
@@ -772,7 +778,7 @@ static inline u32 tcp_stamp_us_delta(u64 t1, u64 t0)
 
 static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 {
-	return div_u64(skb->skb_mstamp_ns, NSEC_PER_SEC / TCP_TS_HZ);
+	return tcp_ns_to_ts(skb->skb_mstamp_ns);
 }
 
 /* provide the departure time in us unit */

commit ebb3b78db7bf842270a46fd4fe7cc45c78fa5ed6
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:44 2019 -0700

    tcp: annotate sk->sk_rcvbuf lockless reads
    
    For the sake of tcp_poll(), there are few places where we fetch
    sk->sk_rcvbuf while this field can change from IRQ or other cpu.
    
    We need to add READ_ONCE() annotations, and also make sure write
    sides use corresponding WRITE_ONCE() to avoid store-tearing.
    
    Note that other transports probably need similar fixes.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e1d08f69fd39..ab4eb5eb5d07 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1380,14 +1380,14 @@ static inline int tcp_win_from_space(const struct sock *sk, int space)
 /* Note: caller must be prepared to deal with negative returns */
 static inline int tcp_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk, sk->sk_rcvbuf -
+	return tcp_win_from_space(sk, READ_ONCE(sk->sk_rcvbuf) -
 				  READ_ONCE(sk->sk_backlog.len) -
 				  atomic_read(&sk->sk_rmem_alloc));
 }
 
 static inline int tcp_full_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk, sk->sk_rcvbuf);
+	return tcp_win_from_space(sk, READ_ONCE(sk->sk_rcvbuf));
 }
 
 extern void tcp_openreq_init_rwin(struct request_sock *req,

commit e0d694d638dba768b47be31c22e1a9b4f862f561
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:42 2019 -0700

    tcp: annotate tp->snd_nxt lockless reads
    
    There are few places where we fetch tp->snd_nxt while
    this field can change from IRQ or other cpu.
    
    We need to add READ_ONCE() annotations, and also make
    sure write sides use corresponding WRITE_ONCE() to avoid
    store-tearing.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8e7c3f6801a9..e1d08f69fd39 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1917,7 +1917,8 @@ static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 static inline bool tcp_stream_memory_free(const struct sock *sk, int wake)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
-	u32 notsent_bytes = READ_ONCE(tp->write_seq) - tp->snd_nxt;
+	u32 notsent_bytes = READ_ONCE(tp->write_seq) -
+			    READ_ONCE(tp->snd_nxt);
 
 	return (notsent_bytes << wake) < tcp_notsent_lowat(tp);
 }

commit 0f31746452e6793ad6271337438af8f4defb8940
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:41 2019 -0700

    tcp: annotate tp->write_seq lockless reads
    
    There are few places where we fetch tp->write_seq while
    this field can change from IRQ or other cpu.
    
    We need to add READ_ONCE() annotations, and also make
    sure write sides use corresponding WRITE_ONCE() to avoid
    store-tearing.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 35f6f7e0fdc2..8e7c3f6801a9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1917,7 +1917,7 @@ static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 static inline bool tcp_stream_memory_free(const struct sock *sk, int wake)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
-	u32 notsent_bytes = tp->write_seq - tp->snd_nxt;
+	u32 notsent_bytes = READ_ONCE(tp->write_seq) - tp->snd_nxt;
 
 	return (notsent_bytes << wake) < tcp_notsent_lowat(tp);
 }

commit 70c2655849a25431f31b505a07fe0c861e5e41fb
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 15:41:03 2019 -0700

    net: silence KCSAN warnings about sk->sk_backlog.len reads
    
    sk->sk_backlog.len can be written by BH handlers, and read
    from process contexts in a lockless way.
    
    Note the write side should also use WRITE_ONCE() or a variant.
    We need some agreement about the best way to do this.
    
    syzbot reported :
    
    BUG: KCSAN: data-race in tcp_add_backlog / tcp_grow_window.isra.0
    
    write to 0xffff88812665f32c of 4 bytes by interrupt on cpu 1:
     sk_add_backlog include/net/sock.h:934 [inline]
     tcp_add_backlog+0x4a0/0xcc0 net/ipv4/tcp_ipv4.c:1737
     tcp_v4_rcv+0x1aba/0x1bf0 net/ipv4/tcp_ipv4.c:1925
     ip_protocol_deliver_rcu+0x51/0x470 net/ipv4/ip_input.c:204
     ip_local_deliver_finish+0x110/0x140 net/ipv4/ip_input.c:231
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_local_deliver+0x133/0x210 net/ipv4/ip_input.c:252
     dst_input include/net/dst.h:442 [inline]
     ip_rcv_finish+0x121/0x160 net/ipv4/ip_input.c:413
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_rcv+0x18f/0x1a0 net/ipv4/ip_input.c:523
     __netif_receive_skb_one_core+0xa7/0xe0 net/core/dev.c:5004
     __netif_receive_skb+0x37/0xf0 net/core/dev.c:5118
     netif_receive_skb_internal+0x59/0x190 net/core/dev.c:5208
     napi_skb_finish net/core/dev.c:5671 [inline]
     napi_gro_receive+0x28f/0x330 net/core/dev.c:5704
     receive_buf+0x284/0x30b0 drivers/net/virtio_net.c:1061
     virtnet_receive drivers/net/virtio_net.c:1323 [inline]
     virtnet_poll+0x436/0x7d0 drivers/net/virtio_net.c:1428
     napi_poll net/core/dev.c:6352 [inline]
     net_rx_action+0x3ae/0xa50 net/core/dev.c:6418
    
    read to 0xffff88812665f32c of 4 bytes by task 7292 on cpu 0:
     tcp_space include/net/tcp.h:1373 [inline]
     tcp_grow_window.isra.0+0x6b/0x480 net/ipv4/tcp_input.c:413
     tcp_event_data_recv+0x68f/0x990 net/ipv4/tcp_input.c:717
     tcp_rcv_established+0xbfe/0xf50 net/ipv4/tcp_input.c:5618
     tcp_v4_do_rcv+0x381/0x4e0 net/ipv4/tcp_ipv4.c:1542
     sk_backlog_rcv include/net/sock.h:945 [inline]
     __release_sock+0x135/0x1e0 net/core/sock.c:2427
     release_sock+0x61/0x160 net/core/sock.c:2943
     tcp_recvmsg+0x63b/0x1a30 net/ipv4/tcp.c:2181
     inet_recvmsg+0xbb/0x250 net/ipv4/af_inet.c:838
     sock_recvmsg_nosec net/socket.c:871 [inline]
     sock_recvmsg net/socket.c:889 [inline]
     sock_recvmsg+0x92/0xb0 net/socket.c:885
     sock_read_iter+0x15f/0x1e0 net/socket.c:967
     call_read_iter include/linux/fs.h:1864 [inline]
     new_sync_read+0x389/0x4f0 fs/read_write.c:414
     __vfs_read+0xb1/0xc0 fs/read_write.c:427
     vfs_read fs/read_write.c:461 [inline]
     vfs_read+0x143/0x2c0 fs/read_write.c:446
    
    Reported by Kernel Concurrency Sanitizer on:
    CPU: 0 PID: 7292 Comm: syz-fuzzer Not tainted 5.3.0+ #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 88e63d64c698..35f6f7e0fdc2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1380,7 +1380,8 @@ static inline int tcp_win_from_space(const struct sock *sk, int space)
 /* Note: caller must be prepared to deal with negative returns */
 static inline int tcp_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk, sk->sk_rcvbuf - sk->sk_backlog.len -
+	return tcp_win_from_space(sk, sk->sk_rcvbuf -
+				  READ_ONCE(sk->sk_backlog.len) -
 				  atomic_read(&sk->sk_rmem_alloc));
 }
 

commit 1f142c17d19a5618d5a633195a46f2c8be9bf232
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 15:10:15 2019 -0700

    tcp: annotate lockless access to tcp_memory_pressure
    
    tcp_memory_pressure is read without holding any lock,
    and its value could be changed on other cpus.
    
    Use READ_ONCE() to annotate these lockless reads.
    
    The write side is already using atomic ops.
    
    Fixes: b8da51ebb1aa ("tcp: introduce tcp_under_memory_pressure()")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c9a3f9688223..88e63d64c698 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -258,7 +258,7 @@ static inline bool tcp_under_memory_pressure(const struct sock *sk)
 	    mem_cgroup_under_socket_pressure(sk->sk_memcg))
 		return true;
 
-	return tcp_memory_pressure;
+	return READ_ONCE(tcp_memory_pressure);
 }
 /*
  * The next routines deal with comparing 32 bit unsigned ints

commit 61723b393292f1e4ea27f8d123384d50b176c29d
Author: Davide Caratti <dcaratti@redhat.com>
Date:   Fri Aug 30 12:25:48 2019 +0200

    tcp: ulp: add functions to dump ulp-specific information
    
    currently, only getsockopt(TCP_ULP) can be invoked to know if a ULP is on
    top of a TCP socket. Extend idiag_get_aux() and idiag_get_aux_size(),
    introduced by commit b37e88407c1d ("inet_diag: allow protocols to provide
    additional data"), to report the ULP name and other information that can
    be made available by the ULP through optional functions.
    
    Users having CAP_NET_ADMIN privileges will then be able to retrieve this
    information through inet_diag_handler, if they specify INET_DIAG_INFO in
    the request.
    
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 77fe87f7a992..c9a3f9688223 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2122,6 +2122,9 @@ struct tcp_ulp_ops {
 	void (*update)(struct sock *sk, struct proto *p);
 	/* cleanup ulp */
 	void (*release)(struct sock *sk);
+	/* diagnostic */
+	int (*get_info)(const struct sock *sk, struct sk_buff *skb);
+	size_t (*get_info_size)(const struct sock *sk);
 
 	char		name[TCP_ULP_NAME_MAX];
 	struct module	*owner;

commit 708852dcac84d2b923f2e8c1327f6006f612416a
Merge: a9a96760165d 72ef80b5ee13
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Aug 13 16:24:57 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    The following pull-request contains BPF updates for your *net-next* tree.
    
    There is a small merge conflict in libbpf (Cc Andrii so he's in the loop
    as well):
    
            for (i = 1; i <= btf__get_nr_types(btf); i++) {
                    t = (struct btf_type *)btf__type_by_id(btf, i);
    
                    if (!has_datasec && btf_is_var(t)) {
                            /* replace VAR with INT */
                            t->info = BTF_INFO_ENC(BTF_KIND_INT, 0, 0);
      <<<<<<< HEAD
                            /*
                             * using size = 1 is the safest choice, 4 will be too
                             * big and cause kernel BTF validation failure if
                             * original variable took less than 4 bytes
                             */
                            t->size = 1;
                            *(int *)(t+1) = BTF_INT_ENC(0, 0, 8);
                    } else if (!has_datasec && kind == BTF_KIND_DATASEC) {
      =======
                            t->size = sizeof(int);
                            *(int *)(t + 1) = BTF_INT_ENC(0, 0, 32);
                    } else if (!has_datasec && btf_is_datasec(t)) {
      >>>>>>> 72ef80b5ee131e96172f19e74b4f98fa3404efe8
                            /* replace DATASEC with STRUCT */
    
    Conflict is between the two commits 1d4126c4e119 ("libbpf: sanitize VAR to
    conservative 1-byte INT") and b03bc6853c0e ("libbpf: convert libbpf code to
    use new btf helpers"), so we need to pick the sanitation fixup as well as
    use the new btf_is_datasec() helper and the whitespace cleanup. Looks like
    the following:
    
      [...]
                    if (!has_datasec && btf_is_var(t)) {
                            /* replace VAR with INT */
                            t->info = BTF_INFO_ENC(BTF_KIND_INT, 0, 0);
                            /*
                             * using size = 1 is the safest choice, 4 will be too
                             * big and cause kernel BTF validation failure if
                             * original variable took less than 4 bytes
                             */
                            t->size = 1;
                            *(int *)(t + 1) = BTF_INT_ENC(0, 0, 8);
                    } else if (!has_datasec && btf_is_datasec(t)) {
                            /* replace DATASEC with STRUCT */
      [...]
    
    The main changes are:
    
    1) Addition of core parts of compile once - run everywhere (co-re) effort,
       that is, relocation of fields offsets in libbpf as well as exposure of
       kernel's own BTF via sysfs and loading through libbpf, from Andrii.
    
       More info on co-re: http://vger.kernel.org/bpfconf2019.html#session-2
       and http://vger.kernel.org/lpc-bpf2018.html#session-2
    
    2) Enable passing input flags to the BPF flow dissector to customize parsing
       and allowing it to stop early similar to the C based one, from Stanislav.
    
    3) Add a BPF helper function that allows generating SYN cookies from XDP and
       tc BPF, from Petar.
    
    4) Add devmap hash-based map type for more flexibility in device lookup for
       redirects, from Toke.
    
    5) Improvements to XDP forwarding sample code now utilizing recently enabled
       devmap lookups, from Jesper.
    
    6) Add support for reporting the effective cgroup progs in bpftool, from Jakub
       and Takshak.
    
    7) Fix reading kernel config from bpftool via /proc/config.gz, from Peter.
    
    8) Fix AF_XDP umem pages mapping for 32 bit architectures, from Ivan.
    
    9) Follow-up to add two more BPF loop tests for the selftest suite, from Alexei.
    
    10) Add perf event output helper also for other skb-based program types, from Allan.
    
    11) Fix a co-re related compilation error in selftests, from Yonghong.
    ====================
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

commit 1555e6fdf062e2ae2514c67f46d475c7ebcce10e
Author: Josh Hunt <johunt@akamai.com>
Date:   Wed Aug 7 19:52:30 2019 -0400

    tcp: Update TCP_BASE_MSS comment
    
    TCP_BASE_MSS is used as the default initial MSS value when MTU probing is
    enabled. Update the comment to reflect this.
    
    Suggested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Josh Hunt <johunt@akamai.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 81e8ade1e6e4..9e9fbfaf052b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -64,7 +64,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* Minimal accepted MSS. It is (60+60+8) - (20+20). */
 #define TCP_MIN_MSS		88U
 
-/* The least MTU to use for probing */
+/* The initial MTU to use for probing */
 #define TCP_BASE_MSS		1024
 
 /* probing interval, default to 10 minutes as per RFC4821 */

commit 9349d600fb6a1ca0aaeb515523e1bb5409483d76
Author: Petar Penkov <ppenkov@google.com>
Date:   Mon Jul 29 09:59:14 2019 -0700

    tcp: add skb-less helpers to retrieve SYN cookie
    
    This patch allows generation of a SYN cookie before an SKB has been
    allocated, as is the case at XDP.
    
    Signed-off-by: Petar Penkov <ppenkov@google.com>
    Reviewed-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e5cf514ba118..fb7e153aecc5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -414,6 +414,16 @@ void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       int estab, struct tcp_fastopen_cookie *foc);
 const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
+/*
+ *	BPF SKB-less helpers
+ */
+u16 tcp_v4_get_syncookie(struct sock *sk, struct iphdr *iph,
+			 struct tcphdr *th, u32 *cookie);
+u16 tcp_v6_get_syncookie(struct sock *sk, struct ipv6hdr *iph,
+			 struct tcphdr *th, u32 *cookie);
+u16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,
+			  const struct tcp_request_sock_ops *af_ops,
+			  struct sock *sk, struct tcphdr *th);
 /*
  *	TCP v4 functions exported for the inet6 API
  */

commit 28ba934d28cf46c1ea92f6aff28e8d9ec52e194e
Merge: 47d858d0bdcd cb8ffde5694a
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jul 25 17:35:03 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf 2019-07-25
    
    The following pull-request contains BPF updates for your *net* tree.
    
    The main changes are:
    
    1) fix segfault in libbpf, from Andrii.
    
    2) fix gso_segs access, from Eric.
    
    3) tls/sockmap fixes, from Jakub and John.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 95fa145479fbc0a0c1fd3274ceb42ec03c042a4a
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Jul 19 10:29:22 2019 -0700

    bpf: sockmap/tls, close can race with map free
    
    When a map free is called and in parallel a socket is closed we
    have two paths that can potentially reset the socket prot ops, the
    bpf close() path and the map free path. This creates a problem
    with which prot ops should be used from the socket closed side.
    
    If the map_free side completes first then we want to call the
    original lowest level ops. However, if the tls path runs first
    we want to call the sockmap ops. Additionally there was no locking
    around prot updates in TLS code paths so the prot ops could
    be changed multiple times once from TLS path and again from sockmap
    side potentially leaving ops pointed at either TLS or sockmap
    when psock and/or tls context have already been destroyed.
    
    To fix this race first only update ops inside callback lock
    so that TLS, sockmap and lowest level all agree on prot state.
    Second and a ULP callback update() so that lower layers can
    inform the upper layer when they are being removed allowing the
    upper layer to reset prot ops.
    
    This gets us close to allowing sockmap and tls to be stacked
    in arbitrary order but will save that patch for *next trees.
    
    v4:
     - make sure we don't free things for device;
     - remove the checks which swap the callbacks back
       only if TLS is at the top.
    
    Reported-by: syzbot+06537213db7ba2745c4a@syzkaller.appspotmail.com
    Fixes: 02c558b2d5d6 ("bpf: sockmap, support for msg_peek in sk_msg with redirect ingress")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f42d300f0cfa..c82a23470081 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2103,6 +2103,8 @@ struct tcp_ulp_ops {
 
 	/* initialize ulp */
 	int (*init)(struct sock *sk);
+	/* update ulp */
+	void (*update)(struct sock *sk, struct proto *p);
 	/* cleanup ulp */
 	void (*release)(struct sock *sk);
 
@@ -2114,6 +2116,7 @@ void tcp_unregister_ulp(struct tcp_ulp_ops *type);
 int tcp_set_ulp(struct sock *sk, const char *name);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
+void tcp_update_ulp(struct sock *sk, struct proto *p);
 
 #define MODULE_ALIAS_TCP_ULP(name)				\
 	__MODULE_INFO(alias, alias_userspace, name);		\

commit b617158dc096709d8600c53b6052144d12b89fab
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jul 19 11:52:33 2019 -0700

    tcp: be more careful in tcp_fragment()
    
    Some applications set tiny SO_SNDBUF values and expect
    TCP to just work. Recent patches to address CVE-2019-11478
    broke them in case of losses, since retransmits might
    be prevented.
    
    We should allow these flows to make progress.
    
    This patch allows the first and last skb in retransmit queue
    to be split even if memory limits are hit.
    
    It also adds the some room due to the fact that tcp_sendmsg()
    and tcp_sendpage() might overshoot sk_wmem_queued by about one full
    TSO skb (64KB size). Note this allowance was already present
    in stable backports for kernels < 4.15
    
    Note for < 4.15 backports :
     tcp_rtx_queue_tail() will probably look like :
    
    static inline struct sk_buff *tcp_rtx_queue_tail(const struct sock *sk)
    {
            struct sk_buff *skb = tcp_send_head(sk);
    
            return skb ? tcp_write_queue_prev(sk, skb) : tcp_write_queue_tail(sk);
    }
    
    Fixes: f070ef2ac667 ("tcp: tcp_fragment() should apply sane memory limits")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Andrew Prout <aprout@ll.mit.edu>
    Tested-by: Andrew Prout <aprout@ll.mit.edu>
    Tested-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Tested-by: Michal Kubecek <mkubecek@suse.cz>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Christoph Paasch <cpaasch@apple.com>
    Cc: Jonathan Looney <jtl@netflix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f42d300f0cfa..e5cf514ba118 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1709,6 +1709,11 @@ static inline struct sk_buff *tcp_rtx_queue_head(const struct sock *sk)
 	return skb_rb_first(&sk->tcp_rtx_queue);
 }
 
+static inline struct sk_buff *tcp_rtx_queue_tail(const struct sock *sk)
+{
+	return skb_rb_last(&sk->tcp_rtx_queue);
+}
+
 static inline struct sk_buff *tcp_write_queue_head(const struct sock *sk)
 {
 	return skb_peek(&sk->sk_write_queue);

commit 8d650cdedaabb33e85e9b7c517c0c71fcecc1de9
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jul 18 19:28:14 2019 -0700

    tcp: fix tcp_set_congestion_control() use from bpf hook
    
    Neal reported incorrect use of ns_capable() from bpf hook.
    
    bpf_setsockopt(...TCP_CONGESTION...)
      -> tcp_set_congestion_control()
       -> ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN)
        -> ns_capable_common()
         -> current_cred()
          -> rcu_dereference_protected(current->cred, 1)
    
    Accessing 'current' in bpf context makes no sense, since packets
    are processed from softirq context.
    
    As Neal stated : The capability check in tcp_set_congestion_control()
    was written assuming a system call context, and then was reused from
    a BPF call site.
    
    The fix is to add a new parameter to tcp_set_congestion_control(),
    so that the ns_capable() call is only performed under the right
    context.
    
    Fixes: 91b5b21c7c16 ("bpf: Add support for changing congestion control")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Lawrence Brakmo <brakmo@fb.com>
    Reported-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cca3c59b98bf..f42d300f0cfa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1064,7 +1064,8 @@ void tcp_get_default_congestion_control(struct net *net, char *name);
 void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
-int tcp_set_congestion_control(struct sock *sk, const char *name, bool load, bool reinit);
+int tcp_set_congestion_control(struct sock *sk, const char *name, bool load,
+			       bool reinit, bool cap_net_admin);
 u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 

commit bef8e2639242e7f7214f1ab3b37ace1415a4f750
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Jul 8 14:57:21 2019 +0200

    bpf: avoid unused variable warning in tcp_bpf_rtt()
    
    When CONFIG_BPF is disabled, we get a warning for an unused
    variable:
    
    In file included from drivers/target/target_core_device.c:26:
    include/net/tcp.h:2226:19: error: unused variable 'tp' [-Werror,-Wunused-variable]
            struct tcp_sock *tp = tcp_sk(sk);
    
    The variable is only used in one place, so it can be
    replaced with its value there to avoid the warning.
    
    Fixes: 23729ff23186 ("bpf: add BPF_CGROUP_SOCK_OPS callback that is executed on every RTT")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e16d8a3fd3b4..cca3c59b98bf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2223,9 +2223,7 @@ static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
 
 static inline void tcp_bpf_rtt(struct sock *sk)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	if (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RTT_CB_FLAG))
+	if (BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk), BPF_SOCK_OPS_RTT_CB_FLAG))
 		tcp_call_bpf(sk, BPF_SOCK_OPS_RTT_CB, 0, NULL);
 }
 

commit 23729ff23186424e54b4d6678fcd526cdacef4d3
Author: Stanislav Fomichev <sdf@google.com>
Date:   Tue Jul 2 09:13:56 2019 -0700

    bpf: add BPF_CGROUP_SOCK_OPS callback that is executed on every RTT
    
    Performance impact should be minimal because it's under a new
    BPF_SOCK_OPS_RTT_CB_FLAG flag that has to be explicitly enabled.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Priyaranjan Jha <priyarjha@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9d36cc88d043..e16d8a3fd3b4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2221,6 +2221,14 @@ static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
 	return (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN, 0, NULL) == 1);
 }
 
+static inline void tcp_bpf_rtt(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (BPF_SOCK_OPS_TEST_FLAG(tp, BPF_SOCK_OPS_RTT_CB_FLAG))
+		tcp_call_bpf(sk, BPF_SOCK_OPS_RTT_CB, 0, NULL);
+}
+
 #if IS_ENABLED(CONFIG_SMC)
 extern struct static_key_false tcp_have_smc;
 #endif

commit 438ac88009bcb10f9ced07fbb4b32d5377ee936b
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Jun 19 23:46:28 2019 +0200

    net: fastopen: robustness and endianness fixes for SipHash
    
    Some changes to the TCP fastopen code to make it more robust
    against future changes in the choice of key/cookie size, etc.
    
    - Instead of keeping the SipHash key in an untyped u8[] buffer
      and casting it to the right type upon use, use the correct
      type directly. This ensures that the key will appear at the
      correct alignment if we ever change the way these data
      structures are allocated. (Currently, they are only allocated
      via kmalloc so they always appear at the correct alignment)
    
    - Use DIV_ROUND_UP when sizing the u64[] array to hold the
      cookie, so it is always of sufficient size, even if
      TCP_FASTOPEN_COOKIE_MAX is no longer a multiple of 8.
    
    - Drop the 'len' parameter from the tcp_fastopen_reset_cipher()
      function, which is no longer used.
    
    - Add endian swabbing when setting the keys and calculating the hash,
      to ensure that cookie values are the same for a given key and
      source/destination address pair regardless of the endianness of
      the server.
    
    Note that none of these are functional changes wrt the current
    state of the code, with the exception of the swabbing, which only
    affects big endian systems.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 573c9e9b0d72..9d36cc88d043 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -43,6 +43,7 @@
 #include <linux/seq_file.h>
 #include <linux/memcontrol.h>
 #include <linux/bpf-cgroup.h>
+#include <linux/siphash.h>
 
 extern struct inet_hashinfo tcp_hashinfo;
 
@@ -1612,8 +1613,7 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 void tcp_fastopen_destroy_cipher(struct sock *sk);
 void tcp_fastopen_ctx_destroy(struct net *net);
 int tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,
-			      void *primary_key, void *backup_key,
-			      unsigned int len);
+			      void *primary_key, void *backup_key);
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
@@ -1623,14 +1623,14 @@ void tcp_fastopen_init_key_once(struct net *net);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);
 bool tcp_fastopen_defer_connect(struct sock *sk, int *err);
-#define TCP_FASTOPEN_KEY_LENGTH 16
+#define TCP_FASTOPEN_KEY_LENGTH sizeof(siphash_key_t)
 #define TCP_FASTOPEN_KEY_MAX 2
 #define TCP_FASTOPEN_KEY_BUF_LENGTH \
 	(TCP_FASTOPEN_KEY_LENGTH * TCP_FASTOPEN_KEY_MAX)
 
 /* Fastopen key context */
 struct tcp_fastopen_context {
-	__u8		key[TCP_FASTOPEN_KEY_MAX][TCP_FASTOPEN_KEY_LENGTH];
+	siphash_key_t	key[TCP_FASTOPEN_KEY_MAX];
 	int		num;
 	struct rcu_head	rcu;
 };

commit 13091aa30535b719e269f20a7bc34002bf5afae5
Merge: f97252a8c33f 29f785ff76b6
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 17 19:48:13 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Honestly all the conflicts were simple overlapping changes,
    nothing really interesting to report.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c681edae33e86ff27be2d6cc717663d91df20b0e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 17 10:09:33 2019 +0200

    net: ipv4: move tcp_fastopen server side code to SipHash library
    
    Using a bare block cipher in non-crypto code is almost always a bad idea,
    not only for security reasons (and we've seen some examples of this in
    the kernel in the past), but also for performance reasons.
    
    In the TCP fastopen case, we call into the bare AES block cipher one or
    two times (depending on whether the connection is IPv4 or IPv6). On most
    systems, this results in a call chain such as
    
      crypto_cipher_encrypt_one(ctx, dst, src)
        crypto_cipher_crt(tfm)->cit_encrypt_one(crypto_cipher_tfm(tfm), ...);
          aesni_encrypt
            kernel_fpu_begin();
            aesni_enc(ctx, dst, src); // asm routine
            kernel_fpu_end();
    
    It is highly unlikely that the use of special AES instructions has a
    benefit in this case, especially since we are doing the above twice
    for IPv6 connections, instead of using a transform which can process
    the entire input in one go.
    
    We could switch to the cbcmac(aes) shash, which would at least get
    rid of the duplicated overhead in *some* cases (i.e., today, only
    arm64 has an accelerated implementation of cbcmac(aes), while x86 will
    end up using the generic cbcmac template wrapping the AES-NI cipher,
    which basically ends up doing exactly the above). However, in the given
    context, it makes more sense to use a light-weight MAC algorithm that
    is more suitable for the purpose at hand, such as SipHash.
    
    Since the output size of SipHash already matches our chosen value for
    TCP_FASTOPEN_COOKIE_SIZE, and given that it accepts arbitrary input
    sizes, this greatly simplifies the code as well.
    
    NOTE: Server farms backing a single server IP for load balancing purposes
          and sharing a single fastopen key will be adversely affected by
          this change unless all systems in the pool receive their kernel
          upgrades at the same time.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 96e0e53ff440..184930b02779 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1628,9 +1628,9 @@ bool tcp_fastopen_defer_connect(struct sock *sk, int *err);
 
 /* Fastopen key context */
 struct tcp_fastopen_context {
-	struct crypto_cipher	*tfm[TCP_FASTOPEN_KEY_MAX];
-	__u8			key[TCP_FASTOPEN_KEY_BUF_LENGTH];
-	struct rcu_head		rcu;
+	__u8		key[TCP_FASTOPEN_KEY_MAX][TCP_FASTOPEN_KEY_LENGTH];
+	int		num;
+	struct rcu_head	rcu;
 };
 
 extern unsigned int sysctl_tcp_fastopen_blackhole_timeout;
@@ -1665,9 +1665,7 @@ bool tcp_fastopen_cookie_match(const struct tcp_fastopen_cookie *foc,
 static inline
 int tcp_fastopen_context_len(const struct tcp_fastopen_context *ctx)
 {
-	if (ctx->tfm[1])
-		return 2;
-	return 1;
+	return ctx->num;
 }
 
 /* Latencies incurred by various limits for a sender. They are

commit 3b4929f65b0d8249f19a50245cd88ed1a2f78cff
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 17 17:17:22 2019 -0700

    tcp: limit payload size of sacked skbs
    
    Jonathan Looney reported that TCP can trigger the following crash
    in tcp_shifted_skb() :
    
            BUG_ON(tcp_skb_pcount(skb) < pcount);
    
    This can happen if the remote peer has advertized the smallest
    MSS that linux TCP accepts : 48
    
    An skb can hold 17 fragments, and each fragment can hold 32KB
    on x86, or 64KB on PowerPC.
    
    This means that the 16bit witdh of TCP_SKB_CB(skb)->tcp_gso_segs
    can overflow.
    
    Note that tcp_sendmsg() builds skbs with less than 64KB
    of payload, so this problem needs SACK to be enabled.
    SACK blocks allow TCP to coalesce multiple skbs in the retransmit
    queue, thus filling the 17 fragments to maximal capacity.
    
    CVE-2019-11477 -- u16 overflow of TCP_SKB_CB(skb)->tcp_gso_segs
    
    Fixes: 832d11c5cd07 ("tcp: Try to restore large SKBs while SACK processing")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Jonathan Looney <jtl@netflix.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Tyler Hicks <tyhicks@canonical.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Bruce Curtis <brucec@netflix.com>
    Cc: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ac2f53fbfa6b..582c0caa9811 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -51,6 +51,8 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 #define MAX_TCP_OPTION_SPACE 40
+#define TCP_MIN_SND_MSS		48
+#define TCP_MIN_GSO_SIZE	(TCP_MIN_SND_MSS - MAX_TCP_OPTION_SPACE)
 
 /*
  * Never offer a window over 32767 without using window scaling. Some

commit d6fb396cfaa71afc9f38d573b8ec6409fe3716de
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jun 13 21:22:35 2019 -0700

    ipv4: tcp: fix ACK/RST sent with a transmit delay
    
    If we want to set a EDT time for the skb we want to send
    via ip_send_unicast_reply(), we have to pass a new parameter
    and initialize ipc.sockc.transmit_time with it.
    
    This fixes the EDT time for ACK/RST packets sent on behalf of
    a TIME_WAIT socket.
    
    Fixes: a842fe1425cb ("tcp: add optional per socket transmit delay")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 49a178b8d5b2..96e0e53ff440 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2240,15 +2240,18 @@ static inline void tcp_add_tx_delay(struct sk_buff *skb,
 		skb->skb_mstamp_ns += (u64)tp->tcp_tx_delay * NSEC_PER_USEC;
 }
 
-static inline void tcp_set_tx_time(struct sk_buff *skb,
-				   const struct sock *sk)
+/* Compute Earliest Departure Time for some control packets
+ * like ACK or RST for TIME_WAIT or non ESTABLISHED sockets.
+ */
+static inline u64 tcp_transmit_time(const struct sock *sk)
 {
 	if (static_branch_unlikely(&tcp_tx_delay_enabled)) {
 		u32 delay = (sk->sk_state == TCP_TIME_WAIT) ?
 			tcp_twsk(sk)->tw_tx_delay : tcp_sk(sk)->tcp_tx_delay;
 
-		skb->skb_mstamp_ns = tcp_clock_ns() + (u64)delay * NSEC_PER_USEC;
+		return tcp_clock_ns() + (u64)delay * NSEC_PER_USEC;
 	}
+	return 0;
 }
 
 #endif	/* _TCP_H */

commit a842fe1425cb20f457abd3f8ef98b468f83ca98b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 12 11:57:25 2019 -0700

    tcp: add optional per socket transmit delay
    
    Adding delays to TCP flows is crucial for studying behavior
    of TCP stacks, including congestion control modules.
    
    Linux offers netem module, but it has unpractical constraints :
    - Need root access to change qdisc
    - Hard to setup on egress if combined with non trivial qdisc like FQ
    - Single delay for all flows.
    
    EDT (Earliest Departure Time) adoption in TCP stack allows us
    to enable a per socket delay at a very small cost.
    
    Networking tools can now establish thousands of flows, each of them
    with a different delay, simulating real world conditions.
    
    This requires FQ packet scheduler or a EDT-enabled NIC.
    
    This patchs adds TCP_TX_DELAY socket option, to set a delay in
    usec units.
    
      unsigned int tx_delay = 10000; /* 10 msec */
    
      setsockopt(fd, SOL_TCP, TCP_TX_DELAY, &tx_delay, sizeof(tx_delay));
    
    Note that FQ packet scheduler limits might need some tweaking :
    
    man tc-fq
    
    PARAMETERS
       limit
           Hard  limit  on  the  real  queue  size. When this limit is
           reached, new packets are dropped. If the value is  lowered,
           packets  are  dropped so that the new limit is met. Default
           is 10000 packets.
    
       flow_limit
           Hard limit on the maximum  number  of  packets  queued  per
           flow.  Default value is 100.
    
    Use of TCP_TX_DELAY option will increase number of skbs in FQ qdisc,
    so packets would be dropped if any of the previous limit is hit.
    
    Use of a jump label makes this support runtime-free, for hosts
    never using the option.
    
    Also note that TSQ (TCP Small Queues) limits are slightly changed
    with this patch : we need to account that skbs artificially delayed
    wont stop us providind more skbs to feed the pipe (netem uses
    skb_orphan_partial() for this purpose, but FQ can not use this trick)
    
    Because of that, using big delays might very well trigger
    old bugs in TSO auto defer logic and/or sndbuf limited detection.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 204328b88412..49a178b8d5b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2232,4 +2232,23 @@ void clean_acked_data_disable(struct inet_connection_sock *icsk);
 void clean_acked_data_flush(void);
 #endif
 
+DECLARE_STATIC_KEY_FALSE(tcp_tx_delay_enabled);
+static inline void tcp_add_tx_delay(struct sk_buff *skb,
+				    const struct tcp_sock *tp)
+{
+	if (static_branch_unlikely(&tcp_tx_delay_enabled))
+		skb->skb_mstamp_ns += (u64)tp->tcp_tx_delay * NSEC_PER_USEC;
+}
+
+static inline void tcp_set_tx_time(struct sk_buff *skb,
+				   const struct sock *sk)
+{
+	if (static_branch_unlikely(&tcp_tx_delay_enabled)) {
+		u32 delay = (sk->sk_state == TCP_TIME_WAIT) ?
+			tcp_twsk(sk)->tw_tx_delay : tcp_sk(sk)->tcp_tx_delay;
+
+		skb->skb_mstamp_ns = tcp_clock_ns() + (u64)delay * NSEC_PER_USEC;
+	}
+}
+
 #endif	/* _TCP_H */

commit a6cdeeb16bff89c8486324f53577db058cbe81ba
Merge: 96524ea4be04 1e1d92636954
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 7 11:00:14 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Some ISDN files that got removed in net-next had some changes
    done in mainline, take the removals.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9092a76d3cf8638467b09bbb4f409094349b2b53
Author: Jason Baron <jbaron@akamai.com>
Date:   Wed May 29 12:33:57 2019 -0400

    tcp: add backup TFO key infrastructure
    
    We would like to be able to rotate TFO keys while minimizing the number of
    client cookies that are rejected. Currently, we have only one key which can
    be used to generate and validate cookies, thus if we simply replace this
    key clients can easily have cookies rejected upon rotation.
    
    We propose having the ability to have both a primary key and a backup key.
    The primary key is used to generate as well as to validate cookies.
    The backup is only used to validate cookies. Thus, keys can be rotated as:
    
    1) generate new key
    2) add new key as the backup key
    3) swap the primary and backup key, thus setting the new key as the primary
    
    We don't simply set the new key as the primary key and move the old key to
    the backup slot because the ip may be behind a load balancer and we further
    allow for the fact that all machines behind the load balancer will not be
    updated simultaneously.
    
    We make use of this infrastructure in subsequent patches.
    
    Suggested-by: Igor Lubashev <ilubashe@akamai.com>
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 985aa5db570c..0083a14fb64f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1614,7 +1614,8 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 void tcp_fastopen_destroy_cipher(struct sock *sk);
 void tcp_fastopen_ctx_destroy(struct net *net);
 int tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,
-			      void *key, unsigned int len);
+			      void *primary_key, void *backup_key,
+			      unsigned int len);
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
@@ -1625,11 +1626,14 @@ bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);
 bool tcp_fastopen_defer_connect(struct sock *sk, int *err);
 #define TCP_FASTOPEN_KEY_LENGTH 16
+#define TCP_FASTOPEN_KEY_MAX 2
+#define TCP_FASTOPEN_KEY_BUF_LENGTH \
+	(TCP_FASTOPEN_KEY_LENGTH * TCP_FASTOPEN_KEY_MAX)
 
 /* Fastopen key context */
 struct tcp_fastopen_context {
-	struct crypto_cipher	*tfm;
-	__u8			key[TCP_FASTOPEN_KEY_LENGTH];
+	struct crypto_cipher	*tfm[TCP_FASTOPEN_KEY_MAX];
+	__u8			key[TCP_FASTOPEN_KEY_BUF_LENGTH];
 	struct rcu_head		rcu;
 };
 
@@ -1639,6 +1643,37 @@ bool tcp_fastopen_active_should_disable(struct sock *sk);
 void tcp_fastopen_active_disable_ofo_check(struct sock *sk);
 void tcp_fastopen_active_detect_blackhole(struct sock *sk, bool expired);
 
+/* Caller needs to wrap with rcu_read_(un)lock() */
+static inline
+struct tcp_fastopen_context *tcp_fastopen_get_ctx(const struct sock *sk)
+{
+	struct tcp_fastopen_context *ctx;
+
+	ctx = rcu_dereference(inet_csk(sk)->icsk_accept_queue.fastopenq.ctx);
+	if (!ctx)
+		ctx = rcu_dereference(sock_net(sk)->ipv4.tcp_fastopen_ctx);
+	return ctx;
+}
+
+static inline
+bool tcp_fastopen_cookie_match(const struct tcp_fastopen_cookie *foc,
+			       const struct tcp_fastopen_cookie *orig)
+{
+	if (orig->len == TCP_FASTOPEN_COOKIE_SIZE &&
+	    orig->len == foc->len &&
+	    !memcmp(orig->val, foc->val, foc->len))
+		return true;
+	return false;
+}
+
+static inline
+int tcp_fastopen_context_len(const struct tcp_fastopen_context *ctx)
+{
+	if (ctx->tfm[1])
+		return 2;
+	return 1;
+}
+
 /* Latencies incurred by various limits for a sender. They are
  * chronograph-like stats that are mutually exclusive.
  */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 985aa5db570c..ac2f53fbfa6b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * INET		An implementation of the TCP/IP protocol suite for the LINUX
  *		operating system.  INET is implemented using the  BSD Socket
@@ -9,11 +10,6 @@
  *
  * Authors:	Ross Biro
  *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
- *
- *		This program is free software; you can redistribute it and/or
- *		modify it under the terms of the GNU General Public License
- *		as published by the Free Software Foundation; either version
- *		2 of the License, or (at your option) any later version.
  */
 #ifndef _TCP_H
 #define _TCP_H

commit 494bc1d281b5a9f02a81249fa566d8c7e390c50c
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed May 8 16:46:14 2019 -0700

    net/tcp: use deferred jump label for TCP acked data hook
    
    User space can flip the clean_acked_data_enabled static branch
    on and off with TLS offload when CONFIG_TLS_DEVICE is enabled.
    jump_label.h suggests we use the delayed version in this case.
    
    Deferred branches now also don't take the branch mutex on
    decrement, so we avoid potential locking issues.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7cf1181630a3..985aa5db570c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2198,7 +2198,7 @@ extern struct static_key_false tcp_have_smc;
 void clean_acked_data_enable(struct inet_connection_sock *icsk,
 			     void (*cad)(struct sock *sk, u32 ack_seq));
 void clean_acked_data_disable(struct inet_connection_sock *icsk);
-
+void clean_acked_data_flush(void);
 #endif
 
 #endif	/* _TCP_H */

commit 0b13c9bb96f6edf03018030f07929a16b4dd77a6
Author: Daniel T. Lee <danieltimlee@gmail.com>
Date:   Sun Apr 21 00:50:42 2019 +0900

    include/net/tcp.h: whitespace cleanup at tcp_v4_check
    
    This patch makes trivial whitespace fix to the function
    tcp_v4_check at include/net/tcp.h file.
    
    It has stylistic issue, which is "space required after that ','"
    and it can be confirmed with ./scripts/checkpatch.pl tool.
    
        ERROR: space required after that ',' (ctx:VxV)
        #29: FILE: include/net/tcp.h:1317:
        +           return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
                                                  ^
    
    Signed-off-by: Daniel T. Lee <danieltimlee@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 68ee02523b87..7cf1181630a3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1314,7 +1314,7 @@ static inline void tcp_update_wl(struct tcp_sock *tp, u32 seq)
 static inline __sum16 tcp_v4_check(int len, __be32 saddr,
 				   __be32 daddr, __wsum base)
 {
-	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
+	return csum_tcpudp_magic(saddr, daddr, len, IPPROTO_TCP, base);
 }
 
 static inline bool tcp_checksum_complete(struct sk_buff *skb)

commit 921f9a0f2e8c326bfcdde7a59be0bac801a3d588
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 26 09:49:11 2019 -0800

    tcp: convert tcp_md5_needed to static_branch API
    
    We prefer static_branch_unlikely() over static_key_false() these days.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d66ac76a33d1..68ee02523b87 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1558,7 +1558,7 @@ struct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,
 
 #ifdef CONFIG_TCP_MD5SIG
 #include <linux/jump_label.h>
-extern struct static_key tcp_md5_needed;
+extern struct static_key_false tcp_md5_needed;
 struct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk,
 					   const union tcp_md5_addr *addr,
 					   int family);
@@ -1567,7 +1567,7 @@ tcp_md5_do_lookup(const struct sock *sk,
 		  const union tcp_md5_addr *addr,
 		  int family)
 {
-	if (!static_key_false(&tcp_md5_needed))
+	if (!static_branch_unlikely(&tcp_md5_needed))
 		return NULL;
 	return __tcp_md5_do_lookup(sk, addr, family);
 }

commit a43e052beacb2c0cecd0e807590b70fc4ff99dba
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 26 09:49:10 2019 -0800

    tcp: get rid of __tcp_add_write_queue_tail()
    
    This helper is only used from tcp_add_write_queue_tail(), and does
    not make the code more readable.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 813042c08dc5..d66ac76a33d1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1716,14 +1716,9 @@ static inline bool tcp_rtx_and_write_queues_empty(const struct sock *sk)
 	return tcp_rtx_queue_empty(sk) && tcp_write_queue_empty(sk);
 }
 
-static inline void __tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
-{
-	__skb_queue_tail(&sk->sk_write_queue, skb);
-}
-
 static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
 {
-	__tcp_add_write_queue_tail(sk, skb);
+	__skb_queue_tail(&sk->sk_write_queue, skb);
 
 	/* Queue it, remembering where we must start sending. */
 	if (sk->sk_write_queue.next == skb)

commit 6c7b4ee7f96d77a40e474f7541c4e543a669dbde
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 26 09:49:09 2019 -0800

    tcp: get rid of tcp_check_send_head()
    
    This helper is used only once, and its name is no longer relevant.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a6e0355921e1..813042c08dc5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1716,12 +1716,6 @@ static inline bool tcp_rtx_and_write_queues_empty(const struct sock *sk)
 	return tcp_rtx_queue_empty(sk) && tcp_write_queue_empty(sk);
 }
 
-static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)
-{
-	if (tcp_write_queue_empty(sk))
-		tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
-}
-
 static inline void __tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_queue_tail(&sk->sk_write_queue, skb);

commit f859a448470304135f7a1af0083b99e188873bb4
Author: Willem de Bruijn <willemb@google.com>
Date:   Fri Jan 25 11:17:23 2019 -0500

    tcp: allow zerocopy with fastopen
    
    Accept MSG_ZEROCOPY in all the TCP states that allow sendmsg. Remove
    the explicit check for ESTABLISHED and CLOSE_WAIT states.
    
    This requires correctly handling zerocopy state (uarg, sk_zckey) in
    all paths reachable from other TCP states. Such as the EPIPE case
    in sk_stream_wait_connect, which a sendmsg() in incorrect state will
    now hit. Most paths are already safe.
    
    Only extension needed is for TCP Fastopen active open. This can build
    an skb with data in tcp_send_syn_data. Pass the uarg along with other
    fastopen state, so that this skb also generates a zerocopy
    notification on release.
    
    Tested with active and passive tcp fastopen packetdrill scripts at
    https://github.com/wdebruij/packetdrill/commit/1747eef03d25a2404e8132817d0f1244fd6f129d
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5c950180d61b..a6e0355921e1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1608,6 +1608,7 @@ struct tcp_fastopen_request {
 	struct msghdr			*data;  /* data in MSG_FASTOPEN */
 	size_t				size;
 	int				copied;	/* queued in tcp_connect() */
+	struct ubuf_info		*uarg;
 };
 void tcp_free_fastopen_req(struct tcp_sock *tp);
 void tcp_fastopen_destroy_cipher(struct sock *sk);

commit 340a6f3d2d52a1b72f3454818e53293807f8f127
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Thu Jan 17 18:03:14 2019 +0800

    tcp: declare tcp_mmap() only when CONFIG_MMU is set
    
    Since tcp_mmap() is defined when CONFIG_MMU is set.
    
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e0a65c067662..5c950180d61b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -406,8 +406,10 @@ int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
 int tcp_set_rcvlowat(struct sock *sk, int val);
 void tcp_data_ready(struct sock *sk);
+#ifdef CONFIG_MMU
 int tcp_mmap(struct file *file, struct socket *sock,
 	     struct vm_area_struct *vma);
+#endif
 void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);

commit a74f0fa082b76c6a76cba5672f36218518bfdc09
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 4 07:58:17 2018 -0800

    tcp: reduce POLLOUT events caused by TCP_NOTSENT_LOWAT
    
    TCP_NOTSENT_LOWAT socket option or sysctl was added in linux-3.12
    as a step to enable bigger tcp sndbuf limits.
    
    It works reasonably well, but the following happens :
    
    Once the limit is reached, TCP stack generates
    an [E]POLLOUT event for every incoming ACK packet.
    
    This causes a high number of context switches.
    
    This patch implements the strategy David Miller added
    in sock_def_write_space() :
    
     - If TCP socket has a notsent_lowat constraint of X bytes,
       allow sendmsg() to fill up to X bytes, but send [E]POLLOUT
       only if number of notsent bytes is below X/2
    
    This considerably reduces TCP_NOTSENT_LOWAT overhead,
    while allowing to keep the pipe full.
    
    Tested:
     100 ms RTT netem testbed between A and B, 100 concurrent TCP_STREAM
    
    A:/# cat /proc/sys/net/ipv4/tcp_wmem
    4096    262144  64000000
    A:/# super_netperf 100 -H B -l 1000 -- -K bbr &
    
    A:/# grep TCP /proc/net/sockstat
    TCP: inuse 203 orphan 0 tw 19 alloc 414 mem 1364904 # This is about 54 MB of memory per flow :/
    
    A:/# vmstat 5 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     0  0      0 256220672  13532 694976    0    0    10     0   28   14  0  1 99  0  0
     2  0      0 256320016  13532 698480    0    0   512     0 715901 5927  0 10 90  0  0
     0  0      0 256197232  13532 700992    0    0   735    13 771161 5849  0 11 89  0  0
     1  0      0 256233824  13532 703320    0    0   512    23 719650 6635  0 11 89  0  0
     2  0      0 256226880  13532 705780    0    0   642     4 775650 6009  0 12 88  0  0
    
    A:/# echo 2097152 >/proc/sys/net/ipv4/tcp_notsent_lowat
    
    A:/# grep TCP /proc/net/sockstat
    TCP: inuse 203 orphan 0 tw 19 alloc 414 mem 86411 # 3.5 MB per flow
    
    A:/# vmstat 5 5  # check that context switches have not inflated too much.
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     2  0      0 260386512  13592 662148    0    0    10     0   17   14  0  1 99  0  0
     0  0      0 260519680  13592 604184    0    0   512    13 726843 12424  0 10 90  0  0
     1  1      0 260435424  13592 598360    0    0   512    25 764645 12925  0 10 90  0  0
     1  0      0 260855392  13592 578380    0    0   512     7 722943 13624  0 11 88  0  0
     1  0      0 260445008  13592 601176    0    0   614    34 772288 14317  0 10 90  0  0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0681afc62354..e0a65c067662 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1870,12 +1870,16 @@ static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 	return tp->notsent_lowat ?: net->ipv4.sysctl_tcp_notsent_lowat;
 }
 
-static inline bool tcp_stream_memory_free(const struct sock *sk)
+/* @wake is one when sk_stream_write_space() calls us.
+ * This sends EPOLLOUT only if notsent_bytes is half the limit.
+ * This mimics the strategy used in sock_def_write_space().
+ */
+static inline bool tcp_stream_memory_free(const struct sock *sk, int wake)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 	u32 notsent_bytes = tp->write_seq - tp->snd_nxt;
 
-	return notsent_bytes < tcp_notsent_lowat(tp);
+	return (notsent_bytes << wake) < tcp_notsent_lowat(tp);
 }
 
 #ifdef CONFIG_PROC_FS

commit 6015c71e656bb6895b416c31a8b7db457e45cecf
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 27 15:03:21 2018 -0800

    tcp: md5: add tcp_md5_needed jump label
    
    Most linux hosts never setup TCP MD5 keys. We can avoid a
    cache line miss (accessing tp->md5ig_info) on RX and TX
    using a jump label.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3b522259da7d..0681afc62354 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1555,9 +1555,21 @@ struct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,
 					 const struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
-struct tcp_md5sig_key *tcp_md5_do_lookup(const struct sock *sk,
-					 const union tcp_md5_addr *addr,
-					 int family);
+#include <linux/jump_label.h>
+extern struct static_key tcp_md5_needed;
+struct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk,
+					   const union tcp_md5_addr *addr,
+					   int family);
+static inline struct tcp_md5sig_key *
+tcp_md5_do_lookup(const struct sock *sk,
+		  const union tcp_md5_addr *addr,
+		  int family)
+{
+	if (!static_key_false(&tcp_md5_needed))
+		return NULL;
+	return __tcp_md5_do_lookup(sk, addr, family);
+}
+
 #define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_key)
 #else
 static inline struct tcp_md5sig_key *tcp_md5_do_lookup(const struct sock *sk,

commit 85bdf7db5b53cdcc7a901db12bcb3d0063e3866d
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 27 14:42:02 2018 -0800

    tcp: make tcp_space() aware of socket backlog
    
    Jean-Louis Dupond reported poor iscsi TCP receive performance
    that we tracked to backlog drops.
    
    Apparently we fail to send window updates reflecting the
    fact that we are under stress.
    
    Note that we might lack a proper window increase when
    backlog is fully processed, since __release_sock() clears
    sk->sk_backlog.len _after_ all skbs have been processed.
    
    This should not matter in practice. If we had a significant
    load through socket backlog, we are in a dangerous
    situation.
    
    Reported-by: Jean-Louis Dupond <jean-louis@dupond.be>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Tested-by: Jean-Louis Dupond<jean-louis@dupond.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0c61bf0a06da..3b522259da7d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1368,7 +1368,7 @@ static inline int tcp_win_from_space(const struct sock *sk, int space)
 /* Note: caller must be prepared to deal with negative returns */
 static inline int tcp_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk, sk->sk_rcvbuf -
+	return tcp_win_from_space(sk, sk->sk_rcvbuf - sk->sk_backlog.len -
 				  atomic_read(&sk->sk_rmem_alloc));
 }
 

commit ebeef4bcccf4681ba3de04fbf3a005f6054f7999
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 27 14:42:00 2018 -0800

    tcp: hint compiler about sack flows
    
    Tell the compiler that most TCP flows are using SACK these days.
    
    There is no need to add the unlikely() clause in tcp_is_reno(),
    the compiler is able to infer it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 63e37dd1c274..0c61bf0a06da 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1124,7 +1124,7 @@ void tcp_rate_check_app_limited(struct sock *sk);
  */
 static inline int tcp_is_sack(const struct tcp_sock *tp)
 {
-	return tp->rx_opt.sack_ok;
+	return likely(tp->rx_opt.sack_ok);
 }
 
 static inline bool tcp_is_reno(const struct tcp_sock *tp)

commit 213d7767af02a079e6d485daab30167d5d675a57
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Wed Nov 14 22:26:17 2018 +0800

    tcp: clean up STATE_TRACE
    
    Currently we can use bpf or tcp tracepoint to conveniently trace the tcp
    state transition at the run time.
    So we don't need to do this stuff at the compile time anymore.
    
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b84b694e8b3d..63e37dd1c274 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1323,20 +1323,8 @@ static inline bool tcp_checksum_complete(struct sk_buff *skb)
 
 bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);
 int tcp_filter(struct sock *sk, struct sk_buff *skb);
-
-#undef STATE_TRACE
-
-#ifdef STATE_TRACE
-static const char *statename[]={
-	"Unused","Established","Syn Sent","Syn Recv",
-	"Fin Wait 1","Fin Wait 2","Time Wait", "Close",
-	"Close Wait","Last ACK","Listen","Closing"
-};
-#endif
 void tcp_set_state(struct sock *sk, int state);
-
 void tcp_done(struct sock *sk);
-
 int tcp_abort(struct sock *sk, int err);
 
 static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)

commit 6ab6dfa6bb500f5cbb9b7a0f23a1613417ca2d12
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Mon Nov 12 16:17:00 2018 -0800

    net: get rid of __tcp_checksum_complete()
    
    __tcp_checksum_complete() is 100% same with __skb_checksum_complete()
    and there is no other caller except tcp_checksum_complete().
    So, just use __skb_checksum_complete() there.
    
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4743836bed2e..b84b694e8b3d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1315,15 +1315,10 @@ static inline __sum16 tcp_v4_check(int len, __be32 saddr,
 	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
 }
 
-static inline __sum16 __tcp_checksum_complete(struct sk_buff *skb)
-{
-	return __skb_checksum_complete(skb);
-}
-
 static inline bool tcp_checksum_complete(struct sk_buff *skb)
 {
 	return !skb_csum_unnecessary(skb) &&
-		__tcp_checksum_complete(skb);
+		__skb_checksum_complete(skb);
 }
 
 bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);

commit 32bbd8793f24b0d5beb1cdb33c45c75ad1140e4b
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Thu Nov 8 12:19:21 2018 +0100

    net: Convert protocol error handlers from void to int
    
    We'll need this to handle ICMP errors for tunnels without a sending socket
    (i.e. FoU and GUE). There, we might have to look up different types of IP
    tunnels, registered as network protocols, before we get a match, so we
    want this for the error handlers of IPPROTO_IPIP and IPPROTO_IPV6 in both
    inet_protos and inet6_protos. These error codes will be used in the next
    patch.
    
    For consistency, return sensible error codes in protocol error handlers
    whenever handlers can't handle errors because, even if valid, they don't
    match a protocol or any of its states.
    
    This has no effect on existing error handling paths.
    
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Reviewed-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a18914d20486..4743836bed2e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -313,7 +313,7 @@ extern struct proto tcp_prot;
 
 void tcp_tasklet_init(void);
 
-void tcp_v4_err(struct sk_buff *skb, u32);
+int tcp_v4_err(struct sk_buff *skb, u32);
 
 void tcp_shutdown(struct sock *sk, int how);
 

commit 3f80e08f40cdb308589a49077c87632fa4508b21
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Oct 23 11:54:16 2018 -0700

    tcp: add tcp_reset_xmit_timer() helper
    
    With EDT model, SRTT no longer is inflated by pacing delays.
    
    This means that RTO and some other xmit timers might be setup
    incorrectly. This is particularly visible with either :
    
    - Very small enforced pacing rates (SO_MAX_PACING_RATE)
    - Reduced rto (from the default 200 ms)
    
    This can lead to TCP flows aborts in the worst case,
    or spurious retransmits in other cases.
    
    For example, this session gets far more throughput
    than the requested 80kbit :
    
    $ netperf -H 127.0.0.2 -l 100 -- -q 10000
    MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 127.0.0.2 () port 0 AF_INET
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/sec
    
    540000 262144 262144    104.00      2.66
    
    With the fix :
    
    $ netperf -H 127.0.0.2 -l 100 -- -q 10000
    MIGRATED TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 127.0.0.2 () port 0 AF_INET
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/sec
    
    540000 262144 262144    104.00      0.12
    
    EDT allows for better control of rtx timers, since TCP has
    a better idea of the earliest departure time of each skb
    in the rtx queue. We only have to eventually add to the
    timer the difference of the EDT time with current time.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8a61c3e8c15d..a18914d20486 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1245,8 +1245,31 @@ static inline bool tcp_needs_internal_pacing(const struct sock *sk)
 	return smp_load_acquire(&sk->sk_pacing_status) == SK_PACING_NEEDED;
 }
 
+/* Return in jiffies the delay before one skb is sent.
+ * If @skb is NULL, we look at EDT for next packet being sent on the socket.
+ */
+static inline unsigned long tcp_pacing_delay(const struct sock *sk,
+					     const struct sk_buff *skb)
+{
+	s64 pacing_delay = skb ? skb->tstamp : tcp_sk(sk)->tcp_wstamp_ns;
+
+	pacing_delay -= tcp_sk(sk)->tcp_clock_cache;
+
+	return pacing_delay > 0 ? nsecs_to_jiffies(pacing_delay) : 0;
+}
+
+static inline void tcp_reset_xmit_timer(struct sock *sk,
+					const int what,
+					unsigned long when,
+					const unsigned long max_when,
+					const struct sk_buff *skb)
+{
+	inet_csk_reset_xmit_timer(sk, what, when + tcp_pacing_delay(sk, skb),
+				  max_when);
+}
+
 /* Something is really bad, we could not queue an additional packet,
- * because qdisc is full or receiver sent a 0 window.
+ * because qdisc is full or receiver sent a 0 window, or we are paced.
  * We do not want to add fuel to the fire, or abort too early,
  * so make sure the timer we arm now is at least 200ms in the future,
  * regardless of current icsk_rto value (as it could be ~2ms)
@@ -1268,8 +1291,9 @@ static inline unsigned long tcp_probe0_when(const struct sock *sk,
 static inline void tcp_check_probe_timer(struct sock *sk)
 {
 	if (!tcp_sk(sk)->packets_out && !inet_csk(sk)->icsk_pending)
-		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
-					  tcp_probe0_base(sk), TCP_RTO_MAX);
+		tcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
+				     tcp_probe0_base(sk), TCP_RTO_MAX,
+				     NULL);
 }
 
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)

commit c16ee04c9b305d57719344922c4d48379e206a79
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Oct 21 02:09:23 2018 +0200

    ulp: remove uid and user_visible members
    
    They are not used anymore and therefore should be removed.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 14fdd7ce9992..8a61c3e8c15d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2051,11 +2051,6 @@ enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);
 #define TCP_ULP_MAX		128
 #define TCP_ULP_BUF_MAX		(TCP_ULP_NAME_MAX*TCP_ULP_MAX)
 
-enum {
-	TCP_ULP_TLS,
-	TCP_ULP_BPF,
-};
-
 struct tcp_ulp_ops {
 	struct list_head	list;
 
@@ -2064,9 +2059,7 @@ struct tcp_ulp_ops {
 	/* cleanup ulp */
 	void (*release)(struct sock *sk);
 
-	int		uid;
 	char		name[TCP_ULP_NAME_MAX];
-	bool		user_visible;
 	struct module	*owner;
 };
 int tcp_register_ulp(struct tcp_ulp_ops *type);

commit 02c558b2d5d679fbbcaa5b9689484c7e0f8abb7b
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Oct 16 11:08:04 2018 -0700

    bpf: sockmap, support for msg_peek in sk_msg with redirect ingress
    
    This adds support for the MSG_PEEK flag when doing redirect to ingress
    and receiving on the sk_msg psock queue. Previously the flag was
    being ignored which could confuse applications if they expected the
    flag to work as normal.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3600ae0f25c3..14fdd7ce9992 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2089,7 +2089,7 @@ int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
 int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		    int nonblock, int flags, int *addr_len);
 int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
-		      struct msghdr *msg, int len);
+		      struct msghdr *msg, int len, int flags);
 
 /* Call BPF_SOCK_OPS program that returns an int. If the return value
  * is < 0, then the BPF op failed (for example if the loaded BPF

commit 604326b41a6fb9b4a78b6179335decee0365cd8c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:58 2018 +0200

    bpf, sockmap: convert to generic sk_msg interface
    
    Add a generic sk_msg layer, and convert current sockmap and later
    kTLS over to make use of it. While sk_buff handles network packet
    representation from netdevice up to socket, sk_msg handles data
    representation from application to socket layer.
    
    This means that sk_msg framework spans across ULP users in the
    kernel, and enables features such as introspection or filtering
    of data with the help of BPF programs that operate on this data
    structure.
    
    Latter becomes in particular useful for kTLS where data encryption
    is deferred into the kernel, and as such enabling the kernel to
    perform L7 introspection and policy based on BPF for TLS connections
    where the record is being encrypted after BPF has run and came to
    a verdict. In order to get there, first step is to transform open
    coding of scatter-gather list handling into a common core framework
    that subsystems can use.
    
    The code itself has been split and refactored into three bigger
    pieces: i) the generic sk_msg API which deals with managing the
    scatter gather ring, providing helpers for walking and mangling,
    transferring application data from user space into it, and preparing
    it for BPF pre/post-processing, ii) the plain sock map itself
    where sockets can be attached to or detached from; these bits
    are independent of i) which can now be used also without sock
    map, and iii) the integration with plain TCP as one protocol
    to be used for processing L7 application data (later this could
    e.g. also be extended to other protocols like UDP). The semantics
    are the same with the old sock map code and therefore no change
    of user facing behavior or APIs. While pursuing this work it
    also helped finding a number of bugs in the old sockmap code
    that we've fixed already in earlier commits. The test_sockmap
    kselftest suite passes through fine as well.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8f5cef67fd35..3600ae0f25c3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -858,6 +858,21 @@ static inline void bpf_compute_data_end_sk_skb(struct sk_buff *skb)
 	TCP_SKB_CB(skb)->bpf.data_end = skb->data + skb_headlen(skb);
 }
 
+static inline bool tcp_skb_bpf_ingress(const struct sk_buff *skb)
+{
+	return TCP_SKB_CB(skb)->bpf.flags & BPF_F_INGRESS;
+}
+
+static inline struct sock *tcp_skb_bpf_redirect_fetch(struct sk_buff *skb)
+{
+	return TCP_SKB_CB(skb)->bpf.sk_redir;
+}
+
+static inline void tcp_skb_bpf_redirect_clear(struct sk_buff *skb)
+{
+	TCP_SKB_CB(skb)->bpf.sk_redir = NULL;
+}
+
 #if IS_ENABLED(CONFIG_IPV6)
 /* This is the variant of inet6_iif() that must be used by TCP,
  * as TCP moves IP6CB into a different location in skb->cb[]
@@ -2064,6 +2079,18 @@ void tcp_cleanup_ulp(struct sock *sk);
 	__MODULE_INFO(alias, alias_userspace, name);		\
 	__MODULE_INFO(alias, alias_tcp_ulp, "tcp-ulp-" name)
 
+struct sk_msg;
+struct sk_psock;
+
+int tcp_bpf_init(struct sock *sk);
+void tcp_bpf_reinit(struct sock *sk);
+int tcp_bpf_sendmsg_redir(struct sock *sk, struct sk_msg *msg, u32 bytes,
+			  int flags);
+int tcp_bpf_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
+		    int nonblock, int flags, int *addr_len);
+int __tcp_bpf_recvmsg(struct sock *sk, struct sk_psock *psock,
+		      struct msghdr *msg, int len);
+
 /* Call BPF_SOCK_OPS program that returns an int. If the return value
  * is < 0, then the BPF op failed (for example if the loaded BPF
  * program does not support the chosen operation or there is no BPF

commit 1243a51f6c05ecbb2c5c9e02fdcc1e7a06f76f26
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:57 2018 +0200

    tcp, ulp: remove ulp bits from sockmap
    
    In order to prepare sockmap logic to be used in combination with kTLS
    we need to detangle it from ULP, and further split it in later commits
    into a generic API.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0d2929223c70..8f5cef67fd35 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2057,7 +2057,6 @@ struct tcp_ulp_ops {
 int tcp_register_ulp(struct tcp_ulp_ops *type);
 void tcp_unregister_ulp(struct tcp_ulp_ops *type);
 int tcp_set_ulp(struct sock *sk, const char *name);
-int tcp_set_ulp_id(struct sock *sk, const int ulp);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
 

commit fb420d5d91c1274d5966917725e71f27ed092a85
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 28 10:28:44 2018 -0700

    tcp/fq: move back to CLOCK_MONOTONIC
    
    In the recent TCP/EDT patch series, I switched TCP and sch_fq
    clocks from MONOTONIC to TAI, in order to meet the choice done
    earlier for sch_etf packet scheduler.
    
    But sure enough, this broke some setups were the TAI clock
    jumps forward (by almost 50 year...), as reported
    by Leonard Crestez.
    
    If we want to converge later, we'll probably need to add
    an skb field to differentiate the clock bases, or a socket option.
    
    In the meantime, an UDP application will need to use CLOCK_MONOTONIC
    base for its SCM_TXTIME timestamps if using fq packet scheduler.
    
    Fixes: 72b0094f9182 ("tcp: switch tcp_clock_ns() to CLOCK_TAI base")
    Fixes: 142537e41923 ("net_sched: sch_fq: switch to CLOCK_TAI")
    Fixes: fd2bca2aa789 ("tcp: switch internal pacing timer to CLOCK_TAI")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Leonard Crestez <leonard.crestez@nxp.com>
    Tested-by: Leonard Crestez <leonard.crestez@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ff15d8e0d525..0d2929223c70 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -732,7 +732,7 @@ void tcp_send_window_probe(struct sock *sk);
 
 static inline u64 tcp_clock_ns(void)
 {
-	return ktime_get_tai_ns();
+	return ktime_get_ns();
 }
 
 static inline u64 tcp_clock_us(void)

commit d3edd06ea8ea9e03de6567fda80b8be57e21a537
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 21 08:51:50 2018 -0700

    tcp: provide earliest departure time in skb->tstamp
    
    Switch internal TCP skb->skb_mstamp to skb->skb_mstamp_ns,
    from usec units to nsec units.
    
    Do not clear skb->tstamp before entering IP stacks in TX,
    so that qdisc or devices can implement pacing based on the
    earliest departure time instead of socket sk->sk_pacing_rate
    
    Packets are fed with tcp_wstamp_ns, and following patch
    will update tcp_wstamp_ns when both TCP and sch_fq switch to
    the earliest departure time mechanism.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 370198fdc65d..ff15d8e0d525 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -761,13 +761,13 @@ static inline u32 tcp_stamp_us_delta(u64 t1, u64 t0)
 
 static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 {
-	return div_u64(skb->skb_mstamp, USEC_PER_SEC / TCP_TS_HZ);
+	return div_u64(skb->skb_mstamp_ns, NSEC_PER_SEC / TCP_TS_HZ);
 }
 
 /* provide the departure time in us unit */
 static inline u64 tcp_skb_timestamp_us(const struct sk_buff *skb)
 {
-	return skb->skb_mstamp;
+	return div_u64(skb->skb_mstamp_ns, NSEC_PER_USEC);
 }
 
 
@@ -813,7 +813,7 @@ struct tcp_skb_cb {
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
 #define TCPCB_LOST		0x04	/* SKB is lost			*/
 #define TCPCB_TAGBITS		0x07	/* All tag bits			*/
-#define TCPCB_REPAIRED		0x10	/* SKB repaired (no skb_mstamp)	*/
+#define TCPCB_REPAIRED		0x10	/* SKB repaired (no skb_mstamp_ns)	*/
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS| \
 				TCPCB_REPAIRED)

commit 9799ccb0e984a5c1311b22a212e7ff96e8b736de
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 21 08:51:49 2018 -0700

    tcp: add tcp_wstamp_ns socket field
    
    TCP will soon provide earliest departure time on TX skbs.
    It needs to track this in a new variable.
    
    tcp_mstamp_refresh() needs to update this variable, and
    became too big to stay an inline.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0ca5ea10dc06..370198fdc65d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -752,17 +752,7 @@ static inline u32 tcp_time_stamp_raw(void)
 	return div_u64(tcp_clock_ns(), NSEC_PER_SEC / TCP_TS_HZ);
 }
 
-
-/* Refresh 1us clock of a TCP socket,
- * ensuring monotically increasing values.
- */
-static inline void tcp_mstamp_refresh(struct tcp_sock *tp)
-{
-	u64 val = tcp_clock_us();
-
-	if (val > tp->tcp_mstamp)
-		tp->tcp_mstamp = val;
-}
+void tcp_mstamp_refresh(struct tcp_sock *tp);
 
 static inline u32 tcp_stamp_us_delta(u64 t1, u64 t0)
 {

commit 2fd66ffba50716fc5ab481c48db643af3bda2276
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 21 08:51:47 2018 -0700

    tcp: introduce tcp_skb_timestamp_us() helper
    
    There are few places where TCP reads skb->skb_mstamp expecting
    a value in usec unit.
    
    skb->tstamp (aka skb->skb_mstamp) will soon store CLOCK_TAI nsec value.
    
    Add tcp_skb_timestamp_us() to provide proper conversion when needed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c6f0bc1dc678..0ca5ea10dc06 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -774,6 +774,12 @@ static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 	return div_u64(skb->skb_mstamp, USEC_PER_SEC / TCP_TS_HZ);
 }
 
+/* provide the departure time in us unit */
+static inline u64 tcp_skb_timestamp_us(const struct sk_buff *skb)
+{
+	return skb->skb_mstamp;
+}
+
 
 #define tcp_flag_byte(th) (((u_int8_t *)th)[13])
 
@@ -1940,7 +1946,7 @@ static inline s64 tcp_rto_delta_us(const struct sock *sk)
 {
 	const struct sk_buff *skb = tcp_rtx_queue_head(sk);
 	u32 rto = inet_csk(sk)->icsk_rto;
-	u64 rto_time_stamp_us = skb->skb_mstamp + jiffies_to_usecs(rto);
+	u64 rto_time_stamp_us = tcp_skb_timestamp_us(skb) + jiffies_to_usecs(rto);
 
 	return rto_time_stamp_us - tcp_sk(sk)->tcp_mstamp;
 }

commit 72b0094f918294e6cb8cf5c3b4520d928fbb1a57
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 21 08:51:46 2018 -0700

    tcp: switch tcp_clock_ns() to CLOCK_TAI base
    
    TCP pacing is either implemented in sch_fq or internally.
    We have the goal of being able to offload pacing on the NICS.
    
    TCP will soon provide per skb skb->tstamp as early departure time.
    
    Like ETF in commit 25db26a91364 ("net/sched: Introduce the ETF Qdisc")
    we chose CLOCK_T as the clock base, so that TCP and pacers can share
    a common clock, to get better RTT samples (without pacing artificially
    inflating these samples).
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 770917d0caa7..c6f0bc1dc678 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -732,7 +732,7 @@ void tcp_send_window_probe(struct sock *sk);
 
 static inline u64 tcp_clock_ns(void)
 {
-	return local_clock();
+	return ktime_get_tai_ns();
 }
 
 static inline u64 tcp_clock_us(void)

commit 037b0b86ecf5646f8eae777d8b52ff8b401692ec
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Aug 16 21:49:06 2018 +0200

    tcp, ulp: add alias for all ulp modules
    
    Lets not turn the TCP ULP lookup into an arbitrary module loader as
    we only intend to load ULP modules through this mechanism, not other
    unrelated kernel modules:
    
      [root@bar]# cat foo.c
      #include <sys/types.h>
      #include <sys/socket.h>
      #include <linux/tcp.h>
      #include <linux/in.h>
    
      int main(void)
      {
          int sock = socket(PF_INET, SOCK_STREAM, 0);
          setsockopt(sock, IPPROTO_TCP, TCP_ULP, "sctp", sizeof("sctp"));
          return 0;
      }
    
      [root@bar]# gcc foo.c -O2 -Wall
      [root@bar]# lsmod | grep sctp
      [root@bar]# ./a.out
      [root@bar]# lsmod | grep sctp
      sctp                 1077248  4
      libcrc32c              16384  3 nf_conntrack,nf_nat,sctp
      [root@bar]#
    
    Fix it by adding module alias to TCP ULP modules, so probing module
    via request_module() will be limited to tcp-ulp-[name]. The existing
    modules like kTLS will load fine given tcp-ulp-tls alias, but others
    will fail to load:
    
      [root@bar]# lsmod | grep sctp
      [root@bar]# ./a.out
      [root@bar]# lsmod | grep sctp
      [root@bar]#
    
    Sockmap is not affected from this since it's either built-in or not.
    
    Fixes: 734942cc4ea6 ("tcp: ULP infrastructure")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d196901c9dba..770917d0caa7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2065,6 +2065,10 @@ int tcp_set_ulp_id(struct sock *sk, const int ulp);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
 
+#define MODULE_ALIAS_TCP_ULP(name)				\
+	__MODULE_INFO(alias, alias_userspace, name);		\
+	__MODULE_INFO(alias, alias_tcp_ulp, "tcp-ulp-" name)
+
 /* Call BPF_SOCK_OPS program that returns an int. If the return value
  * is < 0, then the BPF op failed (for example if the loaded BPF
  * program does not support the chosen operation or there is no BPF

commit 40a1227ea845a37ab197dd1caffb60b047fa36b1
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:21 2018 -0700

    tcp: Avoid TCP syncookie rejected by SO_REUSEPORT socket
    
    Although the actual cookie check "__cookie_v[46]_check()" does
    not involve sk specific info, it checks whether the sk has recent
    synq overflow event in "tcp_synq_no_recent_overflow()".  The
    tcp_sk(sk)->rx_opt.ts_recent_stamp is updated every second
    when it has sent out a syncookie (through "tcp_synq_overflow()").
    
    The above per sk "recent synq overflow event timestamp" works well
    for non SO_REUSEPORT use case.  However, it may cause random
    connection request reject/discard when SO_REUSEPORT is used with
    syncookie because it fails the "tcp_synq_no_recent_overflow()"
    test.
    
    When SO_REUSEPORT is used, it usually has multiple listening
    socks serving TCP connection requests destinated to the same local IP:PORT.
    There are cases that the TCP-ACK-COOKIE may not be received
    by the same sk that sent out the syncookie.  For example,
    if reuse->socks[] began with {sk0, sk1},
    1) sk1 sent out syncookies and tcp_sk(sk1)->rx_opt.ts_recent_stamp
       was updated.
    2) the reuse->socks[] became {sk1, sk2} later.  e.g. sk0 was first closed
       and then sk2 was added.  Here, sk2 does not have ts_recent_stamp set.
       There are other ordering that will trigger the similar situation
       below but the idea is the same.
    3) When the TCP-ACK-COOKIE comes back, sk2 was selected.
       "tcp_synq_no_recent_overflow(sk2)" returns true. In this case,
       all syncookies sent by sk1 will be handled (and rejected)
       by sk2 while sk1 is still alive.
    
    The userspace may create and remove listening SO_REUSEPORT sockets
    as it sees fit.  E.g. Adding new thread (and SO_REUSEPORT sock) to handle
    incoming requests, old process stopping and new process starting...etc.
    With or without SO_ATTACH_REUSEPORT_[CB]BPF,
    the sockets leaving and joining a reuseport group makes picking
    the same sk to check the syncookie very difficult (if not impossible).
    
    The later patches will allow bpf prog more flexibility in deciding
    where a sk should be located in a bpf map and selecting a particular
    SO_REUSEPORT sock as it sees fit.  e.g. Without closing any sock,
    replace the whole bpf reuseport_array in one map_update() by using
    map-in-map.  Getting the syncookie check working smoothly across
    socks in the same "reuse->socks[]" is important.
    
    A partial solution is to set the newly added sk's ts_recent_stamp
    to the max ts_recent_stamp of a reuseport group but that will require
    to iterate through reuse->socks[]  OR
    pessimistically set it to "now - TCP_SYNCOOKIE_VALID" when a sk is
    joining a reuseport group.  However, neither of them will solve the
    existing sk getting moved around the reuse->socks[] and that
    sk may not have ts_recent_stamp updated, unlikely under continuous
    synflood but not impossible.
    
    This patch opts to treat the reuseport group as a whole when
    considering the last synq overflow timestamp since
    they are serving the same IP:PORT from the userspace
    (and BPF program) perspective.
    
    "synq_overflow_ts" is added to "struct sock_reuseport".
    The tcp_synq_overflow() and tcp_synq_no_recent_overflow()
    will update/check reuse->synq_overflow_ts if the sk is
    in a reuseport group.  Similar to the reuseport decision in
    __inet_lookup_listener(), both sk->sk_reuseport and
    sk->sk_reuseport_cb are tested for SO_REUSEPORT usage.
    Update on "synq_overflow_ts" happens at roughly once
    every second.
    
    A synflood test was done with a 16 rx-queues and 16 reuseport sockets.
    No meaningful performance change is observed.  Before and
    after the change is ~9Mpps in IPv4.
    
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d769dc20359b..d196901c9dba 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -36,6 +36,7 @@
 #include <net/inet_hashtables.h>
 #include <net/checksum.h>
 #include <net/request_sock.h>
+#include <net/sock_reuseport.h>
 #include <net/sock.h>
 #include <net/snmp.h>
 #include <net/ip.h>
@@ -473,9 +474,22 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
  */
 static inline void tcp_synq_overflow(const struct sock *sk)
 {
-	unsigned int last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned int last_overflow;
 	unsigned int now = jiffies;
 
+	if (sk->sk_reuseport) {
+		struct sock_reuseport *reuse;
+
+		reuse = rcu_dereference(sk->sk_reuseport_cb);
+		if (likely(reuse)) {
+			last_overflow = READ_ONCE(reuse->synq_overflow_ts);
+			if (time_after32(now, last_overflow + HZ))
+				WRITE_ONCE(reuse->synq_overflow_ts, now);
+			return;
+		}
+	}
+
+	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
 	if (time_after32(now, last_overflow + HZ))
 		tcp_sk(sk)->rx_opt.ts_recent_stamp = now;
 }
@@ -483,9 +497,21 @@ static inline void tcp_synq_overflow(const struct sock *sk)
 /* syncookies: no recent synqueue overflow on this listening socket? */
 static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
 {
-	unsigned int last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned int last_overflow;
 	unsigned int now = jiffies;
 
+	if (sk->sk_reuseport) {
+		struct sock_reuseport *reuse;
+
+		reuse = rcu_dereference(sk->sk_reuseport_cb);
+		if (likely(reuse)) {
+			last_overflow = READ_ONCE(reuse->synq_overflow_ts);
+			return time_after32(now, last_overflow +
+					    TCP_SYNCOOKIE_VALID);
+		}
+	}
+
+	last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
 	return time_after32(now, last_overflow + TCP_SYNCOOKIE_VALID);
 }
 

commit 19725496da5602b401eae389736ab00d1817e264
Merge: aea5f654e6b7 9981b4fb8684
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 24 19:21:58 2018 -0700

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/davem/net

commit 24b711edfc34bc45777a3f068812b7d1ed004a5d
Author: David Ahern <dsahern@gmail.com>
Date:   Thu Jul 19 12:41:18 2018 -0700

    net/ipv6: Fix linklocal to global address with VRF
    
    Example setup:
        host: ip -6 addr add dev eth1 2001:db8:104::4
               where eth1 is enslaved to a VRF
    
        switch: ip -6 ro add 2001:db8:104::4/128 dev br1
                where br1 only has an LLA
    
               ping6 2001:db8:104::4
               ssh   2001:db8:104::4
    
    (NOTE: UDP works fine if the PKTINFO has the address set to the global
    address and ifindex is set to the index of eth1 with a destination an
    LLA).
    
    For ICMP, icmp6_iif needs to be updated to check if skb->dev is an
    L3 master. If it is then return the ifindex from rt6i_idev similar
    to what is done for loopback.
    
    For TCP, restore the original tcp_v6_iif definition which is needed in
    most places and add a new tcp_v6_iif_l3_slave that considers the
    l3_slave variability. This latter check is only needed for socket
    lookups.
    
    Fixes: 9ff74384600a ("net: vrf: Handle ipv6 multicast and link-local addresses")
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 25116ec02087..cd3ecda9386a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -840,6 +840,11 @@ static inline void bpf_compute_data_end_sk_skb(struct sk_buff *skb)
  * as TCP moves IP6CB into a different location in skb->cb[]
  */
 static inline int tcp_v6_iif(const struct sk_buff *skb)
+{
+	return TCP_SKB_CB(skb)->header.h6.iif;
+}
+
+static inline int tcp_v6_iif_l3_slave(const struct sk_buff *skb)
 {
 	bool l3_slave = ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags);
 

commit c4c5551df136a7c4edd7c2f433d9a296b39826a2
Merge: 40999f11ce67 48e5aee81f32
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jul 20 14:45:10 2018 -0700

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/torvalds/linux
    
    All conflicts were trivial overlapping changes, so reasonably
    easy to resolve.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a0496ef2c23b3b180902dd185d0d63ccbc624cf8
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Jul 18 13:56:36 2018 -0700

    tcp: do not delay ACK in DCTCP upon CE status change
    
    Per DCTCP RFC8257 (Section 3.2) the ACK reflecting the CE status change
    has to be sent immediately so the sender can respond quickly:
    
    """ When receiving packets, the CE codepoint MUST be processed as follows:
    
       1.  If the CE codepoint is set and DCTCP.CE is false, set DCTCP.CE to
           true and send an immediate ACK.
    
       2.  If the CE codepoint is not set and DCTCP.CE is true, set DCTCP.CE
           to false and send an immediate ACK.
    """
    
    Previously DCTCP implementation may continue to delay the ACK. This
    patch fixes that to implement the RFC by forcing an immediate ACK.
    
    Tested with this packetdrill script provided by Larry Brakmo
    
    0.000 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
    0.000 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
    0.000 setsockopt(3, SOL_TCP, TCP_CONGESTION, "dctcp", 5) = 0
    0.000 bind(3, ..., ...) = 0
    0.000 listen(3, 1) = 0
    
    0.100 < [ect0] SEW 0:0(0) win 32792 <mss 1000,sackOK,nop,nop,nop,wscale 7>
    0.100 > SE. 0:0(0) ack 1 <mss 1460,nop,nop,sackOK,nop,wscale 8>
    0.110 < [ect0] . 1:1(0) ack 1 win 257
    0.200 accept(3, ..., ...) = 4
       +0 setsockopt(4, SOL_SOCKET, SO_DEBUG, [1], 4) = 0
    
    0.200 < [ect0] . 1:1001(1000) ack 1 win 257
    0.200 > [ect01] . 1:1(0) ack 1001
    
    0.200 write(4, ..., 1) = 1
    0.200 > [ect01] P. 1:2(1) ack 1001
    
    0.200 < [ect0] . 1001:2001(1000) ack 2 win 257
    +0.005 < [ce] . 2001:3001(1000) ack 2 win 257
    
    +0.000 > [ect01] . 2:2(0) ack 2001
    // Previously the ACK below would be delayed by 40ms
    +0.000 > [ect01] E. 2:2(0) ack 3001
    
    +0.500 < F. 9501:9501(0) ack 4 win 257
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a08de496d1b2..25116ec02087 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -342,6 +342,7 @@ ssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,
 			struct pipe_inode_info *pipe, size_t len,
 			unsigned int flags);
 
+void tcp_enter_quickack_mode(struct sock *sk, unsigned int max_quickacks);
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
 {

commit 27cde44a259c380a3c09066fc4b42de7dde9b1ad
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Jul 18 13:56:35 2018 -0700

    tcp: do not cancel delay-AcK on DCTCP special ACK
    
    Currently when a DCTCP receiver delays an ACK and receive a
    data packet with a different CE mark from the previous one's, it
    sends two immediate ACKs acking previous and latest sequences
    respectly (for ECN accounting).
    
    Previously sending the first ACK may mark off the delayed ACK timer
    (tcp_event_ack_sent). This may subsequently prevent sending the
    second ACK to acknowledge the latest sequence (tcp_ack_snd_check).
    The culprit is that tcp_send_ack() assumes it always acknowleges
    the latest sequence, which is not true for the first special ACK.
    
    The fix is to not make the assumption in tcp_send_ack and check the
    actual ack sequence before cancelling the delayed ACK. Further it's
    safer to pass the ack sequence number as a local variable into
    tcp_send_ack routine, instead of intercepting tp->rcv_nxt to avoid
    future bugs like this.
    
    Reported-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3482d13d655b..a08de496d1b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -539,6 +539,7 @@ void tcp_send_fin(struct sock *sk);
 void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 int tcp_send_synack(struct sock *);
 void tcp_push_one(struct sock *, unsigned int mss_now);
+void __tcp_send_ack(struct sock *sk, u32 rcv_nxt);
 void tcp_send_ack(struct sock *sk);
 void tcp_send_delayed_ack(struct sock *sk);
 void tcp_send_loss_probe(struct sock *sk);

commit a69258f7aa2623e0930212f09c586fd06674ad79
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 12 06:04:53 2018 -0700

    tcp: remove DELAYED ACK events in DCTCP
    
    After fixing the way DCTCP tracking delayed ACKs, the delayed-ACK
    related callbacks are no longer needed
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index af3ec72d5d41..3482d13d655b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -912,8 +912,6 @@ enum tcp_ca_event {
 	CA_EVENT_LOSS,		/* loss timeout */
 	CA_EVENT_ECN_NO_CE,	/* ECT set, but not CE marked */
 	CA_EVENT_ECN_IS_CE,	/* received CE marked IP packet */
-	CA_EVENT_DELAYED_ACK,	/* Delayed ack is sent */
-	CA_EVENT_NON_DELAYED_ACK,
 };
 
 /* Information about inbound ACK, passed to cong_ops->in_ack_event() */

commit cca9bab1b72cd2296097c75f59ef11ef80461279
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Jul 11 12:16:12 2018 +0200

    tcp: use monotonic timestamps for PAWS
    
    Using get_seconds() for timestamps is deprecated since it can lead
    to overflows on 32-bit systems. While the interface generally doesn't
    overflow until year 2106, the specific implementation of the TCP PAWS
    algorithm breaks in 2038 when the intermediate signed 32-bit timestamps
    overflow.
    
    A related problem is that the local timestamps in CLOCK_REALTIME form
    lead to unexpected behavior when settimeofday is called to set the system
    clock backwards or forwards by more than 24 days.
    
    While the first problem could be solved by using an overflow-safe method
    of comparing the timestamps, a nicer solution is to use a monotonic
    clocksource with ktime_get_seconds() that simply doesn't overflow (at
    least not until 136 years after boot) and that doesn't change during
    settimeofday().
    
    To make 32-bit and 64-bit architectures behave the same way here, and
    also save a few bytes in the tcp_options_received structure, I'm changing
    the type to a 32-bit integer, which is now safe on all architectures.
    
    Finally, the ts_recent_stamp field also (confusingly) gets used to store
    a jiffies value in tcp_synq_overflow()/tcp_synq_no_recent_overflow().
    This is currently safe, but changing the type to 32-bit requires
    some small changes there to keep it working.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f6cb20e6e524..582304955087 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -472,19 +472,20 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
  */
 static inline void tcp_synq_overflow(const struct sock *sk)
 {
-	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
-	unsigned long now = jiffies;
+	unsigned int last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned int now = jiffies;
 
-	if (time_after(now, last_overflow + HZ))
+	if (time_after32(now, last_overflow + HZ))
 		tcp_sk(sk)->rx_opt.ts_recent_stamp = now;
 }
 
 /* syncookies: no recent synqueue overflow on this listening socket? */
 static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
 {
-	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned int last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned int now = jiffies;
 
-	return time_after(jiffies, last_overflow + TCP_SYNCOOKIE_VALID);
+	return time_after32(now, last_overflow + TCP_SYNCOOKIE_VALID);
 }
 
 static inline u32 tcp_cookie_time(void)
@@ -1375,7 +1376,8 @@ static inline bool tcp_paws_check(const struct tcp_options_received *rx_opt,
 {
 	if ((s32)(rx_opt->ts_recent - rx_opt->rcv_tsval) <= paws_win)
 		return true;
-	if (unlikely(get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS))
+	if (unlikely(!time_before32(ktime_get_seconds(),
+				    rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)))
 		return true;
 	/*
 	 * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
@@ -1405,7 +1407,8 @@ static inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,
 
 	   However, we can relax time bounds for RST segments to MSL.
 	 */
-	if (rst && get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_MSL)
+	if (rst && !time_before32(ktime_get_seconds(),
+				  rx_opt->ts_recent_stamp + TCP_PAWS_MSL))
 		return false;
 	return true;
 }

commit 4929c9428a171145f82f81aae0c3c25ef7d82837
Author: Deepti Raghavan <deeptir@mit.edu>
Date:   Mon Jul 9 17:53:39 2018 +0000

    tcp: expose both send and receive intervals for rate sample
    
    Congestion control algorithms, which access the rate sample
    through the tcp_cong_control function, only have access to the maximum
    of the send and receive interval, for cases where the acknowledgment
    rate may be inaccurate due to ACK compression or decimation. Algorithms
    may want to use send rates and receive rates as separate signals.
    
    Signed-off-by: Deepti Raghavan <deeptir@mit.edu>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cce37694776e..f6cb20e6e524 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -954,6 +954,8 @@ struct rate_sample {
 	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
 	s32  delivered;		/* number of packets delivered over interval */
 	long interval_us;	/* time for tp->delivered to incr "delivered" */
+	u32 snd_interval_us;	/* snd interval for delivered packets */
+	u32 rcv_interval_us;	/* rcv interval for delivered packets */
 	long rtt_us;		/* RTT of last (S)ACKed packet (or -1) */
 	int  losses;		/* number of packets marked lost upon ACK */
 	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */

commit 0ea488ff8d23c93da383fcf424825c298b13b1fb
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Jul 5 08:50:15 2018 -0700

    bpf: sockmap, convert bpf_compute_data_pointers to bpf_*_sk_skb
    
    In commit
    
      'bpf: bpf_compute_data uses incorrect cb structure' (8108a7751512)
    
    we added the routine bpf_compute_data_end_sk_skb() to compute the
    correct data_end values, but this has since been lost. In kernel
    v4.14 this was correct and the above patch was applied in it
    entirety. Then when v4.14 was merged into v4.15-rc1 net-next tree
    we lost the piece that renamed bpf_compute_data_pointers to the
    new function bpf_compute_data_end_sk_skb. This was done here,
    
    e1ea2f9856b7 ("Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net")
    
    When it conflicted with the following rename patch,
    
    6aaae2b6c433 ("bpf: rename bpf_compute_data_end into bpf_compute_data_pointers")
    
    Finally, after a refactor I thought even the function
    bpf_compute_data_end_sk_skb() was no longer needed and it was
    erroneously removed.
    
    However, we never reverted the sk_skb_convert_ctx_access() usage of
    tcp_skb_cb which had been committed and survived the merge conflict.
    Here we fix this by adding back the helper and *_data_end_sk_skb()
    usage. Using the bpf_skc_data_end mapping is not correct because it
    expects a qdisc_skb_cb object but at the sock layer this is not the
    case. Even though it happens to work here because we don't overwrite
    any data in-use at the socket layer and the cb structure is cleared
    later this has potential to create some subtle issues. But, even
    more concretely the filter.c access check uses tcp_skb_cb.
    
    And by some act of chance though,
    
    struct bpf_skb_data_end {
            struct qdisc_skb_cb        qdisc_cb;             /*     0    28 */
    
            /* XXX 4 bytes hole, try to pack */
    
            void *                     data_meta;            /*    32     8 */
            void *                     data_end;             /*    40     8 */
    
            /* size: 48, cachelines: 1, members: 3 */
            /* sum members: 44, holes: 1, sum holes: 4 */
            /* last cacheline: 48 bytes */
    };
    
    and then tcp_skb_cb,
    
    struct tcp_skb_cb {
            [...]
                    struct {
                            __u32      flags;                /*    24     4 */
                            struct sock * sk_redir;          /*    32     8 */
                            void *     data_end;             /*    40     8 */
                    } bpf;                                   /*          24 */
            };
    
    So when we use offset_of() to track down the byte offset we get 40 in
    either case and everything continues to work. Fix this mess and use
    correct structures its unclear how long this might actually work for
    until someone moves the structs around.
    
    Reported-by: Martin KaFai Lau <kafai@fb.com>
    Fixes: e1ea2f9856b7 ("Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net")
    Fixes: 6aaae2b6c433 ("bpf: rename bpf_compute_data_end into bpf_compute_data_pointers")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 800582b5dd54..af3ec72d5d41 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -828,6 +828,10 @@ struct tcp_skb_cb {
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
+static inline void bpf_compute_data_end_sk_skb(struct sk_buff *skb)
+{
+	TCP_SKB_CB(skb)->bpf.data_end = skb->data + skb_headlen(skb);
+}
 
 #if IS_ENABLED(CONFIG_IPV6)
 /* This is the variant of inet6_iif() that must be used by TCP,

commit 5cd3da4ba2397ef07226ca2aa5094ed21ff8198f
Merge: f6779e4e53b6 d0fbad0aec1d
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 3 10:26:50 2018 +0900

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/davem/net
    
    Simple overlapping changes in stmmac driver.
    
    Adjust skb_gro_flush_final_remcsum function signature to make GRO list
    changes in net-next, as per Stephen Rothwell's example merge
    resolution.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a11e1d432b51f63ba698d044441284a661f01144
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 28 09:43:44 2018 -0700

    Revert changes to convert to ->poll_mask() and aio IOCB_CMD_POLL
    
    The poll() changes were not well thought out, and completely
    unexplained.  They also caused a huge performance regression, because
    "->poll()" was no longer a trivial file operation that just called down
    to the underlying file operations, but instead did at least two indirect
    calls.
    
    Indirect calls are sadly slow now with the Spectre mitigation, but the
    performance problem could at least be largely mitigated by changing the
    "->get_poll_head()" operation to just have a per-file-descriptor pointer
    to the poll head instead.  That gets rid of one of the new indirections.
    
    But that doesn't fix the new complexity that is completely unwarranted
    for the regular case.  The (undocumented) reason for the poll() changes
    was some alleged AIO poll race fixing, but we don't make the common case
    slower and more complex for some uncommon special case, so this all
    really needs way more explanations and most likely a fundamental
    redesign.
    
    [ This revert is a revert of about 30 different commits, not reverted
      individually because that would just be unnecessarily messy  - Linus ]
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0448e7c5d2b4..800582b5dd54 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -388,7 +388,8 @@ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);
 void tcp_init_transfer(struct sock *sk, int bpf_op);
-__poll_t tcp_poll_mask(struct socket *sock, __poll_t events);
+__poll_t tcp_poll(struct file *file, struct socket *sock,
+		      struct poll_table_struct *wait);
 int tcp_getsockopt(struct sock *sk, int level, int optname,
 		   char __user *optval, int __user *optlen);
 int tcp_setsockopt(struct sock *sk, int level, int optname,

commit d4546c2509b1e9cd082e3682dcec98472e37ee5a
Author: David Miller <davem@davemloft.net>
Date:   Sun Jun 24 14:13:49 2018 +0900

    net: Convert GRO SKB handling to list_head.
    
    Manage pending per-NAPI GRO packets via list_head.
    
    Return an SKB pointer from the GRO receive handlers.  When GRO receive
    handlers return non-NULL, it means that this SKB needs to be completed
    at this time and removed from the NAPI queue.
    
    Several operations are greatly simplified by this transformation,
    especially timing out the oldest SKB in the list when gro_count
    exceeds MAX_GRO_SKBS, and napi_gro_flush() which walks the queue
    in reverse order.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 822ee49ed0f9..402a88b0e8a8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1788,7 +1788,7 @@ void tcp_v4_destroy_sock(struct sock *sk);
 
 struct sk_buff *tcp_gso_segment(struct sk_buff *skb,
 				netdev_features_t features);
-struct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb);
+struct sk_buff *tcp_gro_receive(struct list_head *head, struct sk_buff *skb);
 int tcp_gro_complete(struct sk_buff *skb);
 
 void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr);

commit cadefe5f584abaac40dce72009e4de738cbff467
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 20 16:07:35 2018 -0400

    tcp_bbr: fix bbr pacing rate for internal pacing
    
    This commit makes BBR use only the MSS (without any headers) to
    calculate pacing rates when internal TCP-layer pacing is used.
    
    This is necessary to achieve the correct pacing behavior in this case,
    since tcp_internal_pacing() uses only the payload length to calculate
    pacing delays.
    
    Signed-off-by: Kevin Yang <yyd@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0448e7c5d2b4..822ee49ed0f9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1184,6 +1184,17 @@ static inline bool tcp_is_cwnd_limited(const struct sock *sk)
 	return tp->is_cwnd_limited;
 }
 
+/* BBR congestion control needs pacing.
+ * Same remark for SO_MAX_PACING_RATE.
+ * sch_fq packet scheduler is efficiently handling pacing,
+ * but is not always installed/used.
+ * Return true if TCP stack should pace packets itself.
+ */
+static inline bool tcp_needs_internal_pacing(const struct sock *sk)
+{
+	return smp_load_acquire(&sk->sk_pacing_status) == SK_PACING_NEEDED;
+}
+
 /* Something is really bad, we could not queue an additional packet,
  * because qdisc is full or receiver sent a 0 window.
  * We do not want to add fuel to the fire, or abort too early,

commit 1c8c5a9d38f607c0b6fd12c91cbe1a4418762a21
Merge: 285767604576 7170e6045a6a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 6 18:39:49 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add Maglev hashing scheduler to IPVS, from Inju Song.
    
     2) Lots of new TC subsystem tests from Roman Mashak.
    
     3) Add TCP zero copy receive and fix delayed acks and autotuning with
        SO_RCVLOWAT, from Eric Dumazet.
    
     4) Add XDP_REDIRECT support to mlx5 driver, from Jesper Dangaard
        Brouer.
    
     5) Add ttl inherit support to vxlan, from Hangbin Liu.
    
     6) Properly separate ipv6 routes into their logically independant
        components. fib6_info for the routing table, and fib6_nh for sets of
        nexthops, which thus can be shared. From David Ahern.
    
     7) Add bpf_xdp_adjust_tail helper, which can be used to generate ICMP
        messages from XDP programs. From Nikita V. Shirokov.
    
     8) Lots of long overdue cleanups to the r8169 driver, from Heiner
        Kallweit.
    
     9) Add BTF ("BPF Type Format"), from Martin KaFai Lau.
    
    10) Add traffic condition monitoring to iwlwifi, from Luca Coelho.
    
    11) Plumb extack down into fib_rules, from Roopa Prabhu.
    
    12) Add Flower classifier offload support to igb, from Vinicius Costa
        Gomes.
    
    13) Add UDP GSO support, from Willem de Bruijn.
    
    14) Add documentation for eBPF helpers, from Quentin Monnet.
    
    15) Add TLS tx offload to mlx5, from Ilya Lesokhin.
    
    16) Allow applications to be given the number of bytes available to read
        on a socket via a control message returned from recvmsg(), from
        Soheil Hassas Yeganeh.
    
    17) Add x86_32 eBPF JIT compiler, from Wang YanQing.
    
    18) Add AF_XDP sockets, with zerocopy support infrastructure as well.
        From Björn Töpel.
    
    19) Remove indirect load support from all of the BPF JITs and handle
        these operations in the verifier by translating them into native BPF
        instead. From Daniel Borkmann.
    
    20) Add GRO support to ipv6 gre tunnels, from Eran Ben Elisha.
    
    21) Allow XDP programs to do lookups in the main kernel routing tables
        for forwarding. From David Ahern.
    
    22) Allow drivers to store hardware state into an ELF section of kernel
        dump vmcore files, and use it in cxgb4. From Rahul Lakkireddy.
    
    23) Various RACK and loss detection improvements in TCP, from Yuchung
        Cheng.
    
    24) Add TCP SACK compression, from Eric Dumazet.
    
    25) Add User Mode Helper support and basic bpfilter infrastructure, from
        Alexei Starovoitov.
    
    26) Support ports and protocol values in RTM_GETROUTE, from Roopa
        Prabhu.
    
    27) Support bulking in ->ndo_xdp_xmit() API, from Jesper Dangaard
        Brouer.
    
    28) Add lots of forwarding selftests, from Petr Machata.
    
    29) Add generic network device failover driver, from Sridhar Samudrala.
    
    * ra.kernel.org:/pub/scm/linux/kernel/git/davem/net-next: (1959 commits)
      strparser: Add __strp_unpause and use it in ktls.
      rxrpc: Fix terminal retransmission connection ID to include the channel
      net: hns3: Optimize PF CMDQ interrupt switching process
      net: hns3: Fix for VF mailbox receiving unknown message
      net: hns3: Fix for VF mailbox cannot receiving PF response
      bnx2x: use the right constant
      Revert "net: sched: cls: Fix offloading when ingress dev is vxlan"
      net: dsa: b53: Fix for brcm tag issue in Cygnus SoC
      enic: fix UDP rss bits
      netdev-FAQ: clarify DaveM's position for stable backports
      rtnetlink: validate attributes in do_setlink()
      mlxsw: Add extack messages for port_{un, }split failures
      netdevsim: Add extack error message for devlink reload
      devlink: Add extack to reload and port_{un, }split operations
      net: metrics: add proper netlink validation
      ipmr: fix error path when ipmr_new_table fails
      ip6mr: only set ip6mr_table from setsockopt when ip6mr_new_table succeeds
      net: hns3: remove unused hclgevf_cfg_func_mta_filter
      netfilter: provide udp*_lib_lookup for nf_tproxy
      qed*: Utilize FW 8.37.2.0
      ...

commit 408afb8d7847faea115508ba154346e33edfc7d5
Merge: b058efc1acfd 1da92779e2e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 13:57:43 2018 -0700

    Merge branch 'work.aio-1' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull aio updates from Al Viro:
     "Majority of AIO stuff this cycle. aio-fsync and aio-poll, mostly.
    
      The only thing I'm holding back for a day or so is Adam's aio ioprio -
      his last-minute fixup is trivial (missing stub in !CONFIG_BLOCK case),
      but let it sit in -next for decency sake..."
    
    * 'work.aio-1' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (46 commits)
      aio: sanitize the limit checking in io_submit(2)
      aio: fold do_io_submit() into callers
      aio: shift copyin of iocb into io_submit_one()
      aio_read_events_ring(): make a bit more readable
      aio: all callers of aio_{read,write,fsync,poll} treat 0 and -EIOCBQUEUED the same way
      aio: take list removal to (some) callers of aio_complete()
      aio: add missing break for the IOCB_CMD_FDSYNC case
      random: convert to ->poll_mask
      timerfd: convert to ->poll_mask
      eventfd: switch to ->poll_mask
      pipe: convert to ->poll_mask
      crypto: af_alg: convert to ->poll_mask
      net/rxrpc: convert to ->poll_mask
      net/iucv: convert to ->poll_mask
      net/phonet: convert to ->poll_mask
      net/nfc: convert to ->poll_mask
      net/caif: convert to ->poll_mask
      net/bluetooth: convert to ->poll_mask
      net/sctp: convert to ->poll_mask
      net/tipc: convert to ->poll_mask
      ...

commit 3d97d88e8091f3501e016f6b4ce45a32c4b8f2f6
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Tue May 29 23:27:31 2018 +0800

    tcp: minor optimization around tcp_hdr() usage in receive path
    
    This is additional to the
    commit ea1627c20c34 ("tcp: minor optimizations around tcp_hdr() usage").
    At this point, skb->data is same with tcp_hdr() as tcp header has not
    been pulled yet. So use the less expensive one to get the tcp header.
    
    Remove the third parameter of tcp_rcv_established() and put it into
    the function body.
    
    Furthermore, the local variables are listed as a reverse christmas tree :)
    
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 952d842a604a..029a51b91742 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -334,8 +334,7 @@ void tcp_write_timer_handler(struct sock *sk);
 void tcp_delack_timer_handler(struct sock *sk);
 int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb);
-void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-			 const struct tcphdr *th);
+void tcp_rcv_established(struct sock *sk, struct sk_buff *skb);
 void tcp_rcv_space_adjust(struct sock *sk);
 int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
 void tcp_twsk_destructor(struct sock *sk);

commit 2c7d3dacebd4b432acc5011bf69a986f366d851d
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 9 15:27:02 2018 +0200

    net/tcp: convert to ->poll_mask
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9c9b3768b350..72b0ef0fe8cf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -388,8 +388,7 @@ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);
 void tcp_init_transfer(struct sock *sk, int bpf_op);
-__poll_t tcp_poll(struct file *file, struct socket *sock,
-		      struct poll_table_struct *wait);
+__poll_t tcp_poll_mask(struct socket *sock, __poll_t events);
 int tcp_getsockopt(struct sock *sk, int level, int optname,
 		   char __user *optval, int __user *optlen);
 int tcp_setsockopt(struct sock *sk, int level, int optname,

commit 5d9f4262b7ea41ca9981cc790e37cca6e37c789e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 17 14:47:26 2018 -0700

    tcp: add SACK compression
    
    When TCP receives an out-of-order packet, it immediately sends
    a SACK packet, generating network load but also forcing the
    receiver to send 1-MSS pathological packets, increasing its
    RTX queue length/depth, and thus processing time.
    
    Wifi networks suffer from this aggressive behavior, but generally
    speaking, all these SACK packets add fuel to the fire when networks
    are under congestion.
    
    This patch adds a high resolution timer and tp->compressed_ack counter.
    
    Instead of sending a SACK, we program this timer with a small delay,
    based on RTT and capped to 1 ms :
    
            delay = min ( 5 % of RTT, 1 ms)
    
    If subsequent SACKs need to be sent while the timer has not yet
    expired, we simply increment tp->compressed_ack.
    
    When timer expires, a SACK is sent with the latest information.
    Whenever an ACK is sent (if data is sent, or if in-order
    data is received) timer is canceled.
    
    Note that tcp_sack_new_ofo_skb() is able to force a SACK to be sent
    if the sack blocks need to be shuffled, even if the timer has not
    expired.
    
    A new SNMP counter is added in the following patch.
    
    Two other patches add sysctls to allow changing the 1,000,000 and 44
    values that this commit hard-coded.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Toke Høiland-Jørgensen <toke@toke.dk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 511bd0fde1dc..952d842a604a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -561,6 +561,9 @@ static inline void tcp_clear_xmit_timers(struct sock *sk)
 	if (hrtimer_try_to_cancel(&tcp_sk(sk)->pacing_timer) == 1)
 		__sock_put(sk);
 
+	if (hrtimer_try_to_cancel(&tcp_sk(sk)->compressed_ack_timer) == 1)
+		__sock_put(sk);
+
 	inet_csk_clear_xmit_timers(sk);
 }
 

commit cf0dd203728c20e76976b69cdf42d065c1e5cab3
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 17 14:47:24 2018 -0700

    tcp: use __sock_put() instead of sock_put() in tcp_clear_xmit_timers()
    
    Socket can not disappear under us.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6deb540297cc..511bd0fde1dc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -559,7 +559,7 @@ void tcp_init_xmit_timers(struct sock *);
 static inline void tcp_clear_xmit_timers(struct sock *sk)
 {
 	if (hrtimer_try_to_cancel(&tcp_sk(sk)->pacing_timer) == 1)
-		sock_put(sk);
+		__sock_put(sk);
 
 	inet_csk_clear_xmit_timers(sk);
 }

commit b8fef65a8a76c2f887c56bf8e9684ff04e8d3b9f
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 16 16:40:16 2018 -0700

    tcp: new helper tcp_rack_skb_timeout
    
    Create and export a new helper tcp_rack_skb_timeout and move tcp_is_rack
    to prepare the final RTO change.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2b5372ef2e0e..6deb540297cc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1879,6 +1879,8 @@ void tcp_init(void);
 /* tcp_recovery.c */
 void tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb);
 void tcp_newreno_mark_lost(struct sock *sk, bool snd_una_advanced);
+extern s32 tcp_rack_skb_timeout(struct tcp_sock *tp, struct sk_buff *skb,
+				u32 reo_wnd);
 extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     u64 xmit_time);

commit d716bfdb10b4250617783c94253e48b0e85adcb1
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 16 16:40:13 2018 -0700

    tcp: account lost retransmit after timeout
    
    The previous approach for the lost and retransmit bits was to
    wipe the slate clean: zero all the lost and retransmit bits,
    correspondingly zero the lost_out and retrans_out counters, and
    then add back the lost bits (and correspondingly increment lost_out).
    
    The new approach is to treat this very much like marking packets
    lost in fast recovery. We don’t wipe the slate clean. We just say
    that for all packets that were not yet marked sacked or lost, we now
    mark them as lost in exactly the same way we do for fast recovery.
    
    This fixes the lost retransmit accounting at RTO time and greatly
    simplifies the RTO code by sharing much of the logic with Fast
    Recovery.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9a3ce379b994..2b5372ef2e0e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1877,6 +1877,7 @@ void tcp_v4_init(void);
 void tcp_init(void);
 
 /* tcp_recovery.c */
+void tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb);
 void tcp_newreno_mark_lost(struct sock *sk, bool snd_una_advanced);
 extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,

commit 6ac06ecd3a5d1dd1aaea5c2a8f6d6e4c81d5de6a
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 16 16:40:12 2018 -0700

    tcp: simpler NewReno implementation
    
    This is a rewrite of NewReno loss recovery implementation that is
    simpler and standalone for readability and better performance by
    using less states.
    
    Note that NewReno refers to RFC6582 as a modification to the fast
    recovery algorithm. It is used only if the connection does not
    support SACK in Linux. It should not to be confused with the Reno
    (AIMD) congestion control.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 994f869d793c..9a3ce379b994 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1877,6 +1877,7 @@ void tcp_v4_init(void);
 void tcp_init(void);
 
 /* tcp_recovery.c */
+void tcp_newreno_mark_lost(struct sock *sk, bool snd_una_advanced);
 extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     u64 xmit_time);

commit 20b654dfe1beaca60ab51894ff405a049248433d
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 16 16:40:10 2018 -0700

    tcp: support DUPACK threshold in RACK
    
    This patch adds support for the classic DUPACK threshold rule
    (#DupThresh) in RACK.
    
    When the number of packets SACKed is greater or equal to the
    threshold, RACK sets the reordering window to zero which would
    immediately mark all the unsacked packets below the highest SACKed
    sequence lost. Since this approach is known to not work well with
    reordering, RACK only uses it if no reordering has been observed.
    
    The DUPACK threshold rule is a particularly useful extension to the
    fast recoveries triggered by RACK reordering timer. For example
    data-center transfers where the RTT is much smaller than a timer
    tick, or high RTT path where the default RTT/4 may take too long.
    
    Note that this patch differs slightly from RFC6675. RFC6675
    considers a packet lost when at least #DupThresh higher-sequence
    packets are SACKed.
    
    With RACK, for connections that have seen reordering, RACK
    continues to use a dynamically-adaptive time-based reordering
    window to detect losses. But for connections on which we have not
    yet seen reordering, this patch considers a packet lost when at
    least one higher sequence packet is SACKed and the total number
    of SACKed packets is at least DupThresh. For example, suppose a
    connection has not seen reordering, and sends 10 packets, and
    packets 3, 5, 7 are SACKed. RFC6675 considers packets 1 and 2
    lost. RACK considers packets 1, 2, 4, 6 lost.
    
    There is some small risk of spurious retransmits here due to
    reordering. However, this is mostly limited to the first flight of
    a connection on which the sender receives SACKs from reordering.
    And RFC 6675 and FACK loss detection have a similar risk on the
    first flight with reordering (it's just that the risk of spurious
    retransmits from reordering was slightly narrower for those older
    algorithms due to the margin of 3*MSS).
    
    Also the minimum reordering window is reduced from 1 msec to 0
    to recover quicker on short RTT transfers. Therefore RACK is more
    aggressive in marking packets lost during recovery to reduce the
    reordering window timeouts.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a08eab58ef70..994f869d793c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -245,6 +245,7 @@ extern long sysctl_tcp_mem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 #define TCP_RACK_STATIC_REO_WND  0x2 /* Use static RACK reo wnd */
+#define TCP_RACK_NO_DUPTHRESH    0x4 /* Do not use DUPACK threshold in RACK */
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit b9f672af148bf7a08a6031743156faffd58dbc7e
Merge: 8e725f7caafb e23afe5e7cba
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 16 22:47:11 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-05-17
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Provide a new BPF helper for doing a FIB and neighbor lookup
       in the kernel tables from an XDP or tc BPF program. The helper
       provides a fast-path for forwarding packets. The API supports
       IPv4, IPv6 and MPLS protocols, but currently IPv4 and IPv6 are
       implemented in this initial work, from David (Ahern).
    
    2) Just a tiny diff but huge feature enabled for nfp driver by
       extending the BPF offload beyond a pure host processing offload.
       Offloaded XDP programs are allowed to set the RX queue index and
       thus opening the door for defining a fully programmable RSS/n-tuple
       filter replacement. Once BPF decided on a queue already, the device
       data-path will skip the conventional RSS processing completely,
       from Jakub.
    
    3) The original sockmap implementation was array based similar to
       devmap. However unlike devmap where an ifindex has a 1:1 mapping
       into the map there are use cases with sockets that need to be
       referenced using longer keys. Hence, sockhash map is added reusing
       as much of the sockmap code as possible, from John.
    
    4) Introduce BTF ID. The ID is allocatd through an IDR similar as
       with BPF maps and progs. It also makes BTF accessible to user
       space via BPF_BTF_GET_FD_BY_ID and adds exposure of the BTF data
       through BPF_OBJ_GET_INFO_BY_FD, from Martin.
    
    5) Enable BPF stackmap with build_id also in NMI context. Due to the
       up_read() of current->mm->mmap_sem build_id cannot be parsed.
       This work defers the up_read() via a per-cpu irq_work so that
       at least limited support can be enabled, from Song.
    
    6) Various BPF JIT follow-up cleanups and fixups after the LD_ABS/LD_IND
       JIT conversion as well as implementation of an optimized 32/64 bit
       immediate load in the arm64 JIT that allows to reduce the number of
       emitted instructions; in case of tested real-world programs they
       were shrinking by three percent, from Daniel.
    
    7) Add ifindex parameter to the libbpf loader in order to enable
       BPF offload support. Right now only iproute2 can load offloaded
       BPF and this will also enable libbpf for direct integration into
       other applications, from David (Beckett).
    
    8) Convert the plain text documentation under Documentation/bpf/ into
       RST format since this is the appropriate standard the kernel is
       moving to for all documentation. Also add an overview README.rst,
       from Jesper.
    
    9) Add __printf verification attribute to the bpf_verifier_vlog()
       helper. Though it uses va_list we can still allow gcc to check
       the format string, from Mathieu.
    
    10) Fix a bash reference in the BPF selftest's Makefile. The '|& ...'
        is a bash 4.0+ feature which is not guaranteed to be available
        when calling out to shell, therefore use a more portable variant,
        from Joe.
    
    11) Fix a 64 bit division in xdp_umem_reg() by using div_u64()
        instead of relying on the gcc built-in, from Björn.
    
    12) Fix a sock hashmap kmalloc warning reported by syzbot when an
        overly large key size is used in hashmap then causing overflows
        in htab->elem_size. Reject bogus attr->key_size early in the
        sock_hash_alloc(), from Yonghong.
    
    13) Ensure in BPF selftests when urandom_read is being linked that
        --build-id is always enabled so that test_stacktrace_build_id[_nmi]
        won't be failing, from Alexei.
    
    14) Add bitsperlong.h as well as errno.h uapi headers into the tools
        header infrastructure which point to one of the arch specific
        uapi headers. This was needed in order to fix a build error on
        some systems for the BPF selftests, from Sirio.
    
    15) Allow for short options to be used in the xdp_monitor BPF sample
        code. And also a bpf.h tools uapi header sync in order to fix a
        selftest build failure. Both from Prashant.
    
    16) More formally clarify the meaning of ID in the direct packet access
        section of the BPF documentation, from Wang.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 37d849bb4294e22d5250264e82beaf4dd8a5403c
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 11 09:31:28 2018 +0200

    ipv{4,6}/tcp: simplify procfs registration
    
    Avoid most of the afinfo indirections and just call the proc helpers
    directly.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9c9b3768b350..51dc7a26a2fa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1747,27 +1747,22 @@ enum tcp_seq_states {
 	TCP_SEQ_STATE_ESTABLISHED,
 };
 
-int tcp_seq_open(struct inode *inode, struct file *file);
+void *tcp_seq_start(struct seq_file *seq, loff_t *pos);
+void *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos);
+void tcp_seq_stop(struct seq_file *seq, void *v);
 
 struct tcp_seq_afinfo {
-	char				*name;
 	sa_family_t			family;
-	const struct file_operations	*seq_fops;
-	struct seq_operations		seq_ops;
 };
 
 struct tcp_iter_state {
 	struct seq_net_private	p;
-	sa_family_t		family;
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;
 	int			bucket, offset, sbucket, num;
 	loff_t			last_pos;
 };
 
-int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);
-void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo);
-
 extern struct request_sock_ops tcp_request_sock_ops;
 extern struct request_sock_ops tcp6_request_sock_ops;
 

commit e5cd3abcb31a48d4ea91bd32f0618802ca5f3592
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon May 14 10:00:16 2018 -0700

    bpf: sockmap, refactor sockmap routines to work with hashmap
    
    This patch only refactors the existing sockmap code. This will allow
    much of the psock initialization code path and bpf helper codes to
    work for both sockmap bpf map types that are backed by an array, the
    currently supported type, and the new hash backed bpf map type
    sockhash.
    
    Most the fallout comes from three changes,
    
      - Pushing bpf programs into an independent structure so we
        can use it from the htab struct in the next patch.
      - Generalizing helpers to use void *key instead of the hardcoded
        u32.
      - Instead of passing map/key through the metadata we now do
        the lookup inline. This avoids storing the key in the metadata
        which will be useful when keys can be longer than 4 bytes. We
        rename the sk pointers to sk_redir at this point as well to
        avoid any confusion between the current sk pointer and the
        redirect pointer sk_redir.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cf803fe0fb86..059287374ba0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -814,9 +814,8 @@ struct tcp_skb_cb {
 #endif
 		} header;	/* For incoming skbs */
 		struct {
-			__u32 key;
 			__u32 flags;
-			struct bpf_map *map;
+			struct sock *sk_redir;
 			void *data_end;
 		} bpf;
 	};

commit 73a6bab5aa2a83cb7df85805e08bc03b4065aea7
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 10 14:59:43 2018 -0700

    tcp: switch pacing timer to softirq based hrtimer
    
    linux-4.16 got support for softirq based hrtimers.
    TCP can switch its pacing hrtimer to this variant, since this
    avoids going through a tasklet and some atomic operations.
    
    pacing timer logic looks like other (jiffies based) tcp timers.
    
    v2: use hrtimer_try_to_cancel() in tcp_clear_xmit_timers()
        to correctly release reference on socket if needed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cf803fe0fb86..3b1d617b0110 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -557,7 +557,9 @@ void tcp_fin(struct sock *sk);
 void tcp_init_xmit_timers(struct sock *);
 static inline void tcp_clear_xmit_timers(struct sock *sk)
 {
-	hrtimer_cancel(&tcp_sk(sk)->pacing_timer);
+	if (hrtimer_try_to_cancel(&tcp_sk(sk)->pacing_timer) == 1)
+		sock_put(sk);
+
 	inet_csk_clear_xmit_timers(sk);
 }
 

commit 6dac152355d9308c9e187bf1d38d98afefcaa315
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:10 2018 +0300

    tcp: Add clean acked data hook
    
    Called when a TCP segment is acknowledged.
    Could be used by application protocols who hold additional
    metadata associated with the stream data.
    
    This is required by TLS device offload to release
    metadata associated with acknowledged TLS records.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 833154e3df17..cf803fe0fb86 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2105,4 +2105,12 @@ static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
 #if IS_ENABLED(CONFIG_SMC)
 extern struct static_key_false tcp_have_smc;
 #endif
+
+#if IS_ENABLED(CONFIG_TLS_DEVICE)
+void clean_acked_data_enable(struct inet_connection_sock *icsk,
+			     void (*cad)(struct sock *sk, u32 ack_seq));
+void clean_acked_data_disable(struct inet_connection_sock *icsk);
+
+#endif
+
 #endif	/* _TCP_H */

commit 93ab6cc69162775201587cc9da00d5016dc890e2
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 16 10:33:38 2018 -0700

    tcp: implement mmap() for zero copy receive
    
    Some networks can make sure TCP payload can exactly fit 4KB pages,
    with well chosen MSS/MTU and architectures.
    
    Implement mmap() system call so that applications can avoid
    copying data without complex splice() games.
    
    Note that a successful mmap( X bytes) on TCP socket is consuming
    bytes, as if recvmsg() has been done. (tp->copied += X)
    
    Only PROT_READ mappings are accepted, as skb page frags
    are fundamentally shared and read only.
    
    If tcp_mmap() finds data that is not a full page, or a patch of
    urgent data, -EINVAL is returned, no bytes are consumed.
    
    Application must fallback to recvmsg() to read the problematic sequence.
    
    mmap() wont block,  regardless of socket being in blocking or
    non-blocking mode. If not enough bytes are in receive queue,
    mmap() would return -EAGAIN, or -EIO if socket is in a state
    where no other bytes can be added into receive queue.
    
    An application might use SO_RCVLOWAT, poll() and/or ioctl( FIONREAD)
    to efficiently use mmap()
    
    On the sender side, MSG_EOR might help to clearly separate unaligned
    headers and 4K-aligned chunks if necessary.
    
    Tested:
    
    mlx4 (cx-3) 40Gbit NIC, with tcp_mmap program provided in following patch.
    MTU set to 4168  (4096 TCP payload, 40 bytes IPv6 header, 32 bytes TCP header)
    
    Without mmap() (tcp_mmap -s)
    
    received 32768 MB (0 % mmap'ed) in 8.13342 s, 33.7961 Gbit,
      cpu usage user:0.034 sys:3.778, 116.333 usec per MB, 63062 c-switches
    received 32768 MB (0 % mmap'ed) in 8.14501 s, 33.748 Gbit,
      cpu usage user:0.029 sys:3.997, 122.864 usec per MB, 61903 c-switches
    received 32768 MB (0 % mmap'ed) in 8.11723 s, 33.8635 Gbit,
      cpu usage user:0.048 sys:3.964, 122.437 usec per MB, 62983 c-switches
    received 32768 MB (0 % mmap'ed) in 8.39189 s, 32.7552 Gbit,
      cpu usage user:0.038 sys:4.181, 128.754 usec per MB, 55834 c-switches
    
    With mmap() on receiver (tcp_mmap -s -z)
    
    received 32768 MB (100 % mmap'ed) in 8.03083 s, 34.2278 Gbit,
      cpu usage user:0.024 sys:1.466, 45.4712 usec per MB, 65479 c-switches
    received 32768 MB (100 % mmap'ed) in 7.98805 s, 34.4111 Gbit,
      cpu usage user:0.026 sys:1.401, 43.5486 usec per MB, 65447 c-switches
    received 32768 MB (100 % mmap'ed) in 7.98377 s, 34.4296 Gbit,
      cpu usage user:0.028 sys:1.452, 45.166 usec per MB, 65496 c-switches
    received 32768 MB (99.9969 % mmap'ed) in 8.01838 s, 34.281 Gbit,
      cpu usage user:0.02 sys:1.446, 44.7388 usec per MB, 65505 c-switches
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0ee85c47c185..833154e3df17 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -404,6 +404,8 @@ int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
 int tcp_set_rcvlowat(struct sock *sk, int val);
 void tcp_data_ready(struct sock *sk);
+int tcp_mmap(struct file *file, struct socket *sock,
+	     struct vm_area_struct *vma);
 void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);

commit 03f45c883c6f391ed4fff8292415b35bd1107519
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 16 10:33:37 2018 -0700

    tcp: avoid extra wakeups for SO_RCVLOWAT users
    
    SO_RCVLOWAT is properly handled in tcp_poll(), so that POLLIN is only
    generated when enough bytes are available in receive queue, after
    David change (commit c7004482e8dc "tcp: Respect SO_RCVLOWAT in tcp_poll().")
    
    But TCP still calls sk->sk_data_ready() for each chunk added in receive
    queue, meaning thread is awaken, and goes back to sleep shortly after.
    
    Tested:
    
    tcp_mmap test program, receiving 32768 MB of data with SO_RCVLOWAT set to 512KB
    
    -> Should get ~2 wakeups (c-switches) per MB, regardless of how many
    (tiny or big) packets were received.
    
    High speed (mostly full size GRO packets)
    
    received 32768 MB (100 % mmap'ed) in 8.03112 s, 34.2266 Gbit,
      cpu usage user:0.037 sys:1.404, 43.9758 usec per MB, 65497 c-switches
    
    received 32768 MB (99.9954 % mmap'ed) in 7.98453 s, 34.4263 Gbit,
      cpu usage user:0.03 sys:1.422, 44.3115 usec per MB, 65485 c-switches
    
    Low speed (sender is ratelimited and sends 1-MSS at a time, so GRO is not helping)
    
    received 22474.5 MB (100 % mmap'ed) in 6015.35 s, 0.0313414 Gbit,
      cpu usage user:0.05 sys:1.586, 72.7952 usec per MB, 44950 c-switches
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b2318242cad8..0ee85c47c185 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -403,6 +403,7 @@ void tcp_syn_ack_timeout(const struct request_sock *req);
 int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
 int tcp_set_rcvlowat(struct sock *sk, int val);
+void tcp_data_ready(struct sock *sk);
 void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);

commit d1361840f8c519eaee9a78ffe09e4f0a1b586846
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 16 10:33:35 2018 -0700

    tcp: fix SO_RCVLOWAT and RCVBUF autotuning
    
    Applications might use SO_RCVLOWAT on TCP socket hoping to receive
    one [E]POLLIN event only when a given amount of bytes are ready in socket
    receive queue.
    
    Problem is that receive autotuning is not aware of this constraint,
    meaning sk_rcvbuf might be too small to allow all bytes to be stored.
    
    Add a new (struct proto_ops)->set_rcvlowat method so that a protocol
    can override the default setsockopt(SO_RCVLOWAT) behavior.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9c9b3768b350..b2318242cad8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -402,6 +402,7 @@ void tcp_set_keepalive(struct sock *sk, int val);
 void tcp_syn_ack_timeout(const struct request_sock *req);
 int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
+int tcp_set_rcvlowat(struct sock *sk, int val);
 void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);

commit dcb8c9b4373a583451b1b8a3e916d33de273633d
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Feb 28 14:40:46 2018 -0800

    tcp_bbr: better deal with suboptimal GSO (II)
    
    This is second part of dealing with suboptimal device gso parameters.
    In first patch (350c9f484bde "tcp_bbr: better deal with suboptimal GSO")
    we dealt with devices having low gso_max_segs
    
    Some devices lower gso_max_size from 64KB to 16 KB (r8152 is an example)
    
    In order to probe an optimal cwnd, we want BBR being not sensitive
    to whatever GSO constraint a device can have.
    
    This patch removes tso_segs_goal() CC callback in favor of
    min_tso_segs() for CC wanting to override sysctl_tcp_min_tso_segs
    
    Next patch will remove bbr->tso_segs_goal since it does not have
    to be persistent.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 92b06c6e7732..9c9b3768b350 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -511,8 +511,6 @@ __u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mss);
 #endif
 /* tcp_output.c */
 
-u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,
-		     int min_tso_segs);
 void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			       int nonagle);
 int __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
@@ -981,8 +979,8 @@ struct tcp_congestion_ops {
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);
-	/* suggest number of segments for each skb to transmit (optional) */
-	u32 (*tso_segs_goal)(struct sock *sk);
+	/* override sysctl_tcp_min_tso_segs */
+	u32 (*min_tso_segs)(struct sock *sk);
 	/* returns the multiplier used in tcp_sndbuf_expand (optional) */
 	u32 (*sndbuf_expand)(struct sock *sk);
 	/* call when packets are delivered to update cwnd and pacing rate,

commit e0f9759f530bf789e984961dce79f525b151ecf3
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 13 06:14:12 2018 -0800

    tcp: try to keep packet if SYN_RCV race is lost
    
    배석진 reported that in some situations, packets for a given 5-tuple
    end up being processed by different CPUS.
    
    This involves RPS, and fragmentation.
    
    배석진 is seeing packet drops when a SYN_RECV request socket is
    moved into ESTABLISH state. Other states are protected by socket lock.
    
    This is caused by a CPU losing the race, and simply not caring enough.
    
    Since this seems to occur frequently, we can do better and perform
    a second lookup.
    
    Note that all needed memory barriers are already in the existing code,
    thanks to the spin_lock()/spin_unlock() pair in inet_ehash_insert()
    and reqsk_put(). The second lookup must find the new socket,
    unless it has already been accepted and closed by another cpu.
    
    Note that the fragmentation could be avoided in the first place by
    use of a correct TCP MSS option in the SYN{ACK} packet, but this
    does not mean we can not be more robust.
    
    Many thanks to 배석진 for a very detailed analysis.
    
    Reported-by: 배석진 <soukjin.bae@samsung.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e3fc667f9ac2..92b06c6e7732 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -374,7 +374,8 @@ enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
 					      struct sk_buff *skb,
 					      const struct tcphdr *th);
 struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
-			   struct request_sock *req, bool fastopen);
+			   struct request_sock *req, bool fastopen,
+			   bool *lost_race);
 int tcp_child_process(struct sock *parent, struct sock *child,
 		      struct sk_buff *skb);
 void tcp_enter_loss(struct sock *sk);

commit 1aa12bdf1bfb95db7e75bfecf0e39a65f4e8fbf8
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Feb 5 10:17:49 2018 -0800

    bpf: sockmap, add sock close() hook to remove socks
    
    The selftests test_maps program was leaving dangling BPF sockmap
    programs around because not all psock elements were removed from
    the map. The elements in turn hold a reference on the BPF program
    they are attached to causing BPF programs to stay open even after
    test_maps has completed.
    
    The original intent was that sk_state_change() would be called
    when TCP socks went through TCP_CLOSE state. However, because
    socks may be in SOCK_DEAD state or the sock may be a listening
    socket the event is not always triggered.
    
    To resolve this use the ULP infrastructure and register our own
    proto close() handler. This fixes the above case.
    
    Fixes: 174a79ff9515 ("bpf: sockmap with sk redirect support")
    Reported-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a58292d31e12..e3fc667f9ac2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1985,6 +1985,7 @@ enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);
 
 enum {
 	TCP_ULP_TLS,
+	TCP_ULP_BPF,
 };
 
 struct tcp_ulp_ops {
@@ -2003,6 +2004,7 @@ struct tcp_ulp_ops {
 int tcp_register_ulp(struct tcp_ulp_ops *type);
 void tcp_unregister_ulp(struct tcp_ulp_ops *type);
 int tcp_set_ulp(struct sock *sk, const char *name);
+int tcp_set_ulp_id(struct sock *sk, const int ulp);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
 

commit b11a632c442eef34a0afeba61fab923241f317e9
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Feb 5 10:17:43 2018 -0800

    net: add a UID to use for ULP socket assignment
    
    Create a UID field and enum that can be used to assign ULPs to
    sockets. This saves a set of string comparisons if the ULP id
    is known.
    
    For sockmap, which is added in the next patches, a ULP is used to
    hook into TCP sockets close state. In this case the ULP being added
    is done at map insert time and the ULP is known and done on the kernel
    side. In this case the named lookup is not needed. Because we don't
    want to expose psock internals to user space socket options a user
    visible flag is also added. For TLS this is set for BPF it will be
    cleared.
    
    Alos remove pr_notice, user gets an error code back and should check
    that rather than rely on logs.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 58278669cc55..a58292d31e12 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1983,6 +1983,10 @@ enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);
 #define TCP_ULP_MAX		128
 #define TCP_ULP_BUF_MAX		(TCP_ULP_NAME_MAX*TCP_ULP_MAX)
 
+enum {
+	TCP_ULP_TLS,
+};
+
 struct tcp_ulp_ops {
 	struct list_head	list;
 
@@ -1991,7 +1995,9 @@ struct tcp_ulp_ops {
 	/* cleanup ulp */
 	void (*release)(struct sock *sk);
 
+	int		uid;
 	char		name[TCP_ULP_NAME_MAX];
+	bool		user_visible;
 	struct module	*owner;
 };
 int tcp_register_ulp(struct tcp_ulp_ops *type);

commit b2fe5fa68642860e7de76167c3111623aa0d5de1
Merge: a103950e0dd2 a54667f6728c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 31 14:31:10 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Significantly shrink the core networking routing structures. Result
        of http://vger.kernel.org/~davem/seoul2017_netdev_keynote.pdf
    
     2) Add netdevsim driver for testing various offloads, from Jakub
        Kicinski.
    
     3) Support cross-chip FDB operations in DSA, from Vivien Didelot.
    
     4) Add a 2nd listener hash table for TCP, similar to what was done for
        UDP. From Martin KaFai Lau.
    
     5) Add eBPF based queue selection to tun, from Jason Wang.
    
     6) Lockless qdisc support, from John Fastabend.
    
     7) SCTP stream interleave support, from Xin Long.
    
     8) Smoother TCP receive autotuning, from Eric Dumazet.
    
     9) Lots of erspan tunneling enhancements, from William Tu.
    
    10) Add true function call support to BPF, from Alexei Starovoitov.
    
    11) Add explicit support for GRO HW offloading, from Michael Chan.
    
    12) Support extack generation in more netlink subsystems. From Alexander
        Aring, Quentin Monnet, and Jakub Kicinski.
    
    13) Add 1000BaseX, flow control, and EEE support to mvneta driver. From
        Russell King.
    
    14) Add flow table abstraction to netfilter, from Pablo Neira Ayuso.
    
    15) Many improvements and simplifications to the NFP driver bpf JIT,
        from Jakub Kicinski.
    
    16) Support for ipv6 non-equal cost multipath routing, from Ido
        Schimmel.
    
    17) Add resource abstration to devlink, from Arkadi Sharshevsky.
    
    18) Packet scheduler classifier shared filter block support, from Jiri
        Pirko.
    
    19) Avoid locking in act_csum, from Davide Caratti.
    
    20) devinet_ioctl() simplifications from Al viro.
    
    21) More TCP bpf improvements from Lawrence Brakmo.
    
    22) Add support for onlink ipv6 route flag, similar to ipv4, from David
        Ahern.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1925 commits)
      tls: Add support for encryption using async offload accelerator
      ip6mr: fix stale iterator
      net/sched: kconfig: Remove blank help texts
      openvswitch: meter: Use 64-bit arithmetic instead of 32-bit
      tcp_nv: fix potential integer overflow in tcpnv_acked
      r8169: fix RTL8168EP take too long to complete driver initialization.
      qmi_wwan: Add support for Quectel EP06
      rtnetlink: enable IFLA_IF_NETNSID for RTM_NEWLINK
      ipmr: Fix ptrdiff_t print formatting
      ibmvnic: Wait for device response when changing MAC
      qlcnic: fix deadlock bug
      tcp: release sk_frag.page in tcp_disconnect
      ipv4: Get the address of interface correctly.
      net_sched: gen_estimator: fix lockdep splat
      net: macb: Handle HRESP error
      net/mlx5e: IPoIB, Fix copy-paste bug in flow steering refactoring
      ipv6: addrconf: break critical section in addrconf_verify_rtnl()
      ipv6: change route cache aging logic
      i40e/i40evf: Update DESC_NEEDED value to reflect larger value
      bnxt_en: cleanup DIM work on device shutdown
      ...

commit 168fe32a072a4b8dc81a3aebf0e5e588d38e2955
Merge: 13ddd1667e7f c71d227fc413
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 17:58:07 2018 -0800

    Merge branch 'misc.poll' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull poll annotations from Al Viro:
     "This introduces a __bitwise type for POLL### bitmap, and propagates
      the annotations through the tree. Most of that stuff is as simple as
      'make ->poll() instances return __poll_t and do the same to local
      variables used to hold the future return value'.
    
      Some of the obvious brainos found in process are fixed (e.g. POLLIN
      misspelled as POLL_IN). At that point the amount of sparse warnings is
      low and most of them are for genuine bugs - e.g. ->poll() instance
      deciding to return -EINVAL instead of a bitmap. I hadn't touched those
      in this series - it's large enough as it is.
    
      Another problem it has caught was eventpoll() ABI mess; select.c and
      eventpoll.c assumed that corresponding POLL### and EPOLL### were
      equal. That's true for some, but not all of them - EPOLL### are
      arch-independent, but POLL### are not.
    
      The last commit in this series separates userland POLL### values from
      the (now arch-independent) kernel-side ones, converting between them
      in the few places where they are copied to/from userland. AFAICS, this
      is the least disruptive fix preserving poll(2) ABI and making epoll()
      work on all architectures.
    
      As it is, it's simply broken on sparc - try to give it EPOLLWRNORM and
      it will trigger only on what would've triggered EPOLLWRBAND on other
      architectures. EPOLLWRBAND and EPOLLRDHUP, OTOH, are never triggered
      at all on sparc. With this patch they should work consistently on all
      architectures"
    
    * 'misc.poll' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (37 commits)
      make kernel-side POLL... arch-independent
      eventpoll: no need to mask the result of epi_item_poll() again
      eventpoll: constify struct epoll_event pointers
      debugging printk in sg_poll() uses %x to print POLL... bitmap
      annotate poll(2) guts
      9p: untangle ->poll() mess
      ->si_band gets POLL... bitmap stored into a user-visible long field
      ring_buffer_poll_wait() return value used as return value of ->poll()
      the rest of drivers/*: annotate ->poll() instances
      media: annotate ->poll() instances
      fs: annotate ->poll() instances
      ipc, kernel, mm: annotate ->poll() instances
      net: annotate ->poll() instances
      apparmor: annotate ->poll() instances
      tomoyo: annotate ->poll() instances
      sound: annotate ->poll() instances
      acpi: annotate ->poll() instances
      crypto: annotate ->poll() instances
      block: annotate ->poll() instances
      x86: annotate ->poll() instances
      ...

commit de525be2ca2734865d29c4b67ddd29913b214906
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Thu Jan 25 16:14:09 2018 -0800

    bpf: Support passing args to sock_ops bpf function
    
    Adds support for passing up to 4 arguments to sock_ops bpf functions. It
    reusues the reply union, so the bpf_sock_ops structures are not
    increased in size.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6092eaff61cf..093e967a2960 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2006,7 +2006,7 @@ void tcp_cleanup_ulp(struct sock *sk);
  * program loaded).
  */
 #ifdef CONFIG_BPF
-static inline int tcp_call_bpf(struct sock *sk, int op)
+static inline int tcp_call_bpf(struct sock *sk, int op, u32 nargs, u32 *args)
 {
 	struct bpf_sock_ops_kern sock_ops;
 	int ret;
@@ -2019,6 +2019,8 @@ static inline int tcp_call_bpf(struct sock *sk, int op)
 
 	sock_ops.sk = sk;
 	sock_ops.op = op;
+	if (nargs > 0)
+		memcpy(sock_ops.args, args, nargs * sizeof(*args));
 
 	ret = BPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);
 	if (ret == 0)
@@ -2027,18 +2029,46 @@ static inline int tcp_call_bpf(struct sock *sk, int op)
 		ret = -1;
 	return ret;
 }
+
+static inline int tcp_call_bpf_2arg(struct sock *sk, int op, u32 arg1, u32 arg2)
+{
+	u32 args[2] = {arg1, arg2};
+
+	return tcp_call_bpf(sk, op, 2, args);
+}
+
+static inline int tcp_call_bpf_3arg(struct sock *sk, int op, u32 arg1, u32 arg2,
+				    u32 arg3)
+{
+	u32 args[3] = {arg1, arg2, arg3};
+
+	return tcp_call_bpf(sk, op, 3, args);
+}
+
 #else
-static inline int tcp_call_bpf(struct sock *sk, int op)
+static inline int tcp_call_bpf(struct sock *sk, int op, u32 nargs, u32 *args)
 {
 	return -EPERM;
 }
+
+static inline int tcp_call_bpf_2arg(struct sock *sk, int op, u32 arg1, u32 arg2)
+{
+	return -EPERM;
+}
+
+static inline int tcp_call_bpf_3arg(struct sock *sk, int op, u32 arg1, u32 arg2,
+				    u32 arg3)
+{
+	return -EPERM;
+}
+
 #endif
 
 static inline u32 tcp_timeout_init(struct sock *sk)
 {
 	int timeout;
 
-	timeout = tcp_call_bpf(sk, BPF_SOCK_OPS_TIMEOUT_INIT);
+	timeout = tcp_call_bpf(sk, BPF_SOCK_OPS_TIMEOUT_INIT, 0, NULL);
 
 	if (timeout <= 0)
 		timeout = TCP_TIMEOUT_INIT;
@@ -2049,7 +2079,7 @@ static inline u32 tcp_rwnd_init_bpf(struct sock *sk)
 {
 	int rwnd;
 
-	rwnd = tcp_call_bpf(sk, BPF_SOCK_OPS_RWND_INIT);
+	rwnd = tcp_call_bpf(sk, BPF_SOCK_OPS_RWND_INIT, 0, NULL);
 
 	if (rwnd < 0)
 		rwnd = 0;
@@ -2058,7 +2088,7 @@ static inline u32 tcp_rwnd_init_bpf(struct sock *sk)
 
 static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
 {
-	return (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN) == 1);
+	return (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN, 0, NULL) == 1);
 }
 
 #if IS_ENABLED(CONFIG_SMC)

commit b73042b8a28e2603ac178295ab96c876ba5a97a1
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Thu Jan 25 16:14:08 2018 -0800

    bpf: Add write access to tcp_sock and sock fields
    
    This patch adds a macro, SOCK_OPS_SET_FIELD, for writing to
    struct tcp_sock or struct sock fields. This required adding a new
    field "temp" to struct bpf_sock_ops_kern for temporary storage that
    is used by sock_ops_convert_ctx_access. It is used to store and recover
    the contents of a register, so the register can be used to store the
    address of the sk. Since we cannot overwrite the dst_reg because it
    contains the pointer to ctx, nor the src_reg since it contains the value
    we want to store, we need an extra register to contain the address
    of the sk.
    
    Also adds the macro SOCK_OPS_GET_OR_SET_FIELD that calls one of the
    GET or SET macros depending on the value of the TYPE field.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5a1d26a18599..6092eaff61cf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2011,7 +2011,7 @@ static inline int tcp_call_bpf(struct sock *sk, int op)
 	struct bpf_sock_ops_kern sock_ops;
 	int ret;
 
-	memset(&sock_ops, 0, sizeof(sock_ops));
+	memset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));
 	if (sk_fullsock(sk)) {
 		sock_ops.is_fullsock = 1;
 		sock_owned_by_me(sk);

commit e42866031ff03c89a5bdd2056c76dd6cb41c3d35
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Jan 17 12:11:01 2018 -0800

    tcp: avoid min RTT bloat by skipping RTT from delayed-ACK in BBR
    
    A persistent connection may send tiny amount of data (e.g. health-check)
    for a long period of time. BBR's windowed min RTT filter may only see
    RTT samples from delayed ACKs causing BBR to grossly over-estimate
    the path delay depending how much the ACK was delayed at the receiver.
    
    This patch skips RTT samples that are likely coming from delayed ACKs. Note
    that it is possible the sender never obtains a valid measure to set the
    min RTT. In this case BBR will continue to set cwnd to initial window
    which seems fine because the connection is thin stream.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6939e69d3c37..5a1d26a18599 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -953,6 +953,7 @@ struct rate_sample {
 	u32  prior_in_flight;	/* in flight before this ACK */
 	bool is_app_limited;	/* is sample from packet with bubble in pipe? */
 	bool is_retrans;	/* is sample from retransmission? */
+	bool is_ack_delayed;	/* is this (likely) a delayed ACK? */
 };
 
 struct tcp_congestion_ops {

commit 7268586baa530312041e597b518b5c6a05110df1
Author: Yuchung Cheng <ycheng@google.com>
Date:   Tue Dec 12 13:10:40 2017 -0800

    tcp: pause Fast Open globally after third consecutive timeout
    
    Prior to this patch, active Fast Open is paused on a specific
    destination IP address if the previous connections to the
    IP address have experienced recurring timeouts . But recent
    experiments by Microsoft (https://goo.gl/cykmn7) and Mozilla
    browsers indicate the isssue is often caused by broken middle-boxes
    sitting close to the client. Therefore it is much better user
    experience if Fast Open is disabled out-right globally to avoid
    experiencing further timeouts on connections toward other
    destinations.
    
    This patch changes the destination-IP disablement to global
    disablement if a connection experiencing recurring timeouts
    or aborts due to timeout.  Repeated incidents would still
    exponentially increase the pause time, starting from an hour.
    This is extremely conservative but an unfortunate compromise to
    minimize bad experience due to broken middle-boxes.
    
    Reported-by: Dragana Damjanovic <ddamjanovic@mozilla.com>
    Reported-by: Patrick McManus <mcmanus@ducksong.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Wei Wang <weiwan@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c3744e52cd1..6939e69d3c37 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1507,8 +1507,7 @@ int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
 
 /* From tcp_fastopen.c */
 void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
-			    struct tcp_fastopen_cookie *cookie, int *syn_loss,
-			    unsigned long *last_syn_loss);
+			    struct tcp_fastopen_cookie *cookie);
 void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
 			    struct tcp_fastopen_cookie *cookie, bool syn_lost,
 			    u16 try_exp);
@@ -1546,7 +1545,7 @@ extern unsigned int sysctl_tcp_fastopen_blackhole_timeout;
 void tcp_fastopen_active_disable(struct sock *sk);
 bool tcp_fastopen_active_should_disable(struct sock *sk);
 void tcp_fastopen_active_disable_ofo_check(struct sock *sk);
-void tcp_fastopen_active_timeout_reset(void);
+void tcp_fastopen_active_detect_blackhole(struct sock *sk, bool expired);
 
 /* Latencies incurred by various limits for a sender. They are
  * chronograph-like stats that are mutually exclusive.

commit 51e18a453f5f59a40c721d4aeab082b4e2e9fac6
Merge: 5e54b3c12027 f335195adf04
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Dec 9 22:09:55 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflict was two parallel additions of include files to sch_generic.c,
    no biggie.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 62cd277039a3413604f486f0ca87faec810d7bb7
Merge: 124da8f6118b 47adc5b07c6e
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 8 10:48:25 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf-next 2017-12-07
    
    The following pull-request contains BPF updates for your net-next tree.
    
    The main changes are:
    
    1) Detailed documentation of BPF development process from Daniel.
    
    2) Addition of is_fullsock, snd_cwnd and srtt_us fields to bpf_sock_ops
       from Lawrence.
    
    3) Minor follow up for bpf_skb_set_tunnel_key() from William.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d4761754b4fb2ef8d9a1e9d121c4bec84e1fe292
Author: Yousuk Seung <ysseung@google.com>
Date:   Thu Dec 7 13:41:34 2017 -0800

    tcp: invalidate rate samples during SACK reneging
    
    Mark tcp_sock during a SACK reneging event and invalidate rate samples
    while marked. Such rate samples may overestimate bw by including packets
    that were SACKed before reneging.
    
    < ack 6001 win 10000 sack 7001:38001
    < ack 7001 win 0 sack 8001:38001 // Reneg detected
    > seq 7001:8001 // RTO, SACK cleared.
    < ack 38001 win 10000
    
    In above example the rate sample taken after the last ack will count
    7001-38001 as delivered while the actual delivery rate likely could
    be much lower i.e. 7001-8001.
    
    This patch adds a new field tcp_sock.sack_reneg and marks it when we
    declare SACK reneging and entering TCP_CA_Loss, and unmarks it after
    the last rate sample was taken before moving back to TCP_CA_Open. This
    patch also invalidates rate samples taken while tcp_sock.is_sack_reneg
    is set.
    
    Fixes: b9f64820fb22 ("tcp: track data delivery rate for a TCP connection")
    Signed-off-by: Yousuk Seung <ysseung@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6998707e81f3..6da880d2f022 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1055,7 +1055,7 @@ void tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb);
 void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,
 			    struct rate_sample *rs);
 void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
-		  struct rate_sample *rs);
+		  bool is_sack_reneg, struct rate_sample *rs);
 void tcp_rate_check_app_limited(struct sock *sk);
 
 /* These functions determine how the current flow behaves in respect of SACK

commit f19397a5c65665d66e3866b42056f1f58b7a366b
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Dec 1 10:15:04 2017 -0800

    bpf: Add access to snd_cwnd and others in sock_ops
    
    Adds read access to snd_cwnd and srtt_us fields of tcp_sock. Since these
    fields are only valid if the socket associated with the sock_ops program
    call is a full socket, the field is_fullsock is also added to the
    bpf_sock_ops struct. If the socket is not a full socket, reading these
    fields returns 0.
    
    Note that in most cases it will not be necessary to check is_fullsock to
    know if there is a full socket. The context of the call, as specified by
    the 'op' field, can sometimes determine whether there is a full socket.
    
    The struct bpf_sock_ops has the following fields added:
    
      __u32 is_fullsock;      /* Some TCP fields are only valid if
                               * there is a full socket. If not, the
                               * fields read as zero.
                               */
      __u32 snd_cwnd;
      __u32 srtt_us;          /* Averaged RTT << 3 in usecs */
    
    There is a new macro, SOCK_OPS_GET_TCP32(NAME), to make it easier to add
    read access to more 32 bit tcp_sock fields.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4e09398009c1..89a656077884 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2012,10 +2012,12 @@ static inline int tcp_call_bpf(struct sock *sk, int op)
 	struct bpf_sock_ops_kern sock_ops;
 	int ret;
 
-	if (sk_fullsock(sk))
+	memset(&sock_ops, 0, sizeof(sock_ops));
+	if (sk_fullsock(sk)) {
+		sock_ops.is_fullsock = 1;
 		sock_owned_by_me(sk);
+	}
 
-	memset(&sock_ops, 0, sizeof(sock_ops));
 	sock_ops.sk = sk;
 	sock_ops.op = op;
 

commit b4d1605a8ea608fd7dc45b926a05d75d340bde4b
Author: David Ahern <dsahern@gmail.com>
Date:   Sun Dec 3 09:33:00 2017 -0800

    tcp: use IPCB instead of TCP_SKB_CB in inet_exact_dif_match()
    
    After this fix : ("tcp: add tcp_v4_fill_cb()/tcp_v4_restore_cb()"),
    socket lookups happen while skb->cb[] has not been mangled yet by TCP.
    
    Fixes: a04a480d4392 ("net: Require exact match for TCP socket lookups if dif is l3mdev")
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4e09398009c1..6998707e81f3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -844,12 +844,11 @@ static inline int tcp_v6_sdif(const struct sk_buff *skb)
 }
 #endif
 
-/* TCP_SKB_CB reference means this can not be used from early demux */
 static inline bool inet_exact_dif_match(struct net *net, struct sk_buff *skb)
 {
 #if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)
 	if (!net->ipv4.sysctl_tcp_l3mdev_accept &&
-	    skb && ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))
+	    skb && ipv4_l3mdev_skb(IPCB(skb)->flags))
 		return true;
 #endif
 	return false;

commit ade994f4f6c8c3ef4c3bfc2d02166262fb9d089c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Jul 3 00:01:49 2017 -0400

    net: annotate ->poll() instances
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4e09398009c1..a6343271c629 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -387,7 +387,7 @@ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);
 void tcp_init_transfer(struct sock *sk, int bpf_op);
-unsigned int tcp_poll(struct file *file, struct socket *sock,
+__poll_t tcp_poll(struct file *file, struct socket *sock,
 		      struct poll_table_struct *wait);
 int tcp_getsockopt(struct sock *sk, int level, int optname,
 		   char __user *optval, int __user *optlen);

commit ed66dfaf236c04d414de1d218441296e57fb2bd2
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Nov 17 21:06:14 2017 -0500

    tcp: when scheduling TLP, time of RTO should account for current ACK
    
    Fix the TLP scheduling logic so that when scheduling a TLP probe, we
    ensure that the estimated time at which an RTO would fire accounts for
    the fact that ACKs indicating forward progress should push back RTO
    times.
    
    After the following fix:
    
    df92c8394e6e ("tcp: fix xmit timer to only be reset if data ACKed/SACKed")
    
    we had an unintentional behavior change in the following kind of
    scenario: suppose the RTT variance has been very low recently. Then
    suppose we send out a flight of N packets and our RTT is 100ms:
    
    t=0: send a flight of N packets
    t=100ms: receive an ACK for N-1 packets
    
    The response before df92c8394e6e that was:
      -> schedule a TLP for now + RTO_interval
    
    The response after df92c8394e6e is:
      -> schedule a TLP for t=0 + RTO_interval
    
    Since RTO_interval = srtt + RTT_variance, this means that we have
    scheduled a TLP timer at a point in the future that only accounts for
    RTT_variance. If the RTT_variance term is small, this means that the
    timer fires soon.
    
    Before df92c8394e6e this would not happen, because in that code, when
    we receive an ACK for a prefix of flight, we did:
    
        1) Near the top of tcp_ack(), switch from TLP timer to RTO
           at write_queue_head->paket_tx_time + RTO_interval:
                if (icsk->icsk_pending == ICSK_TIME_LOSS_PROBE)
                       tcp_rearm_rto(sk);
    
        2) In tcp_clean_rtx_queue(), update the RTO to now + RTO_interval:
                if (flag & FLAG_ACKED) {
                       tcp_rearm_rto(sk);
    
        3) In tcp_ack() after tcp_fastretrans_alert() switch from RTO
           to TLP at now + RTO_interval:
                if (icsk->icsk_pending == ICSK_TIME_RETRANS)
                       tcp_schedule_loss_probe(sk);
    
    In df92c8394e6e we removed that 3-phase dance, and instead directly
    set the TLP timer once: we set the TLP timer in cases like this to
    write_queue_head->packet_tx_time + RTO_interval. So if the RTT
    variance is small, then this means that this is setting the TLP timer
    to fire quite soon. This means if the ACK for the tail of the flight
    takes longer than an RTT to arrive (often due to delayed ACKs), then
    the TLP timer fires too quickly.
    
    Fixes: df92c8394e6e ("tcp: fix xmit timer to only be reset if data ACKed/SACKed")
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 85ea578195d4..4e09398009c1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -539,7 +539,7 @@ void tcp_push_one(struct sock *, unsigned int mss_now);
 void tcp_send_ack(struct sock *sk);
 void tcp_send_delayed_ack(struct sock *sk);
 void tcp_send_loss_probe(struct sock *sk);
-bool tcp_schedule_loss_probe(struct sock *sk);
+bool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto);
 void tcp_skb_collapse_tstamp(struct sk_buff *skb,
 			     const struct sk_buff *next_skb);
 

commit 50895b9de1d3e0258e015e8e55128d835d9a9f19
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 14 21:02:19 2017 -0800

    tcp: highest_sack fix
    
    syzbot easily found a regression added in our latest patches [1]
    
    No longer set tp->highest_sack to the head of the send queue since
    this is not logical and error prone.
    
    Only sack processing should maintain the pointer to an skb from rtx queue.
    
    We might in the future only remember the sequence instead of a pointer to skb,
    since rb-tree should allow a fast lookup.
    
    [1]
    BUG: KASAN: use-after-free in tcp_highest_sack_seq include/net/tcp.h:1706 [inline]
    BUG: KASAN: use-after-free in tcp_ack+0x42bb/0x4fd0 net/ipv4/tcp_input.c:3537
    Read of size 4 at addr ffff8801c154faa8 by task syz-executor4/12860
    
    CPU: 0 PID: 12860 Comm: syz-executor4 Not tainted 4.14.0-next-20171113+ #41
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     __dump_stack lib/dump_stack.c:17 [inline]
     dump_stack+0x194/0x257 lib/dump_stack.c:53
     print_address_description+0x73/0x250 mm/kasan/report.c:252
     kasan_report_error mm/kasan/report.c:351 [inline]
     kasan_report+0x25b/0x340 mm/kasan/report.c:409
     __asan_report_load4_noabort+0x14/0x20 mm/kasan/report.c:429
     tcp_highest_sack_seq include/net/tcp.h:1706 [inline]
     tcp_ack+0x42bb/0x4fd0 net/ipv4/tcp_input.c:3537
     tcp_rcv_established+0x672/0x18a0 net/ipv4/tcp_input.c:5439
     tcp_v4_do_rcv+0x2ab/0x7d0 net/ipv4/tcp_ipv4.c:1468
     sk_backlog_rcv include/net/sock.h:909 [inline]
     __release_sock+0x124/0x360 net/core/sock.c:2264
     release_sock+0xa4/0x2a0 net/core/sock.c:2778
     tcp_sendmsg+0x3a/0x50 net/ipv4/tcp.c:1462
     inet_sendmsg+0x11f/0x5e0 net/ipv4/af_inet.c:763
     sock_sendmsg_nosec net/socket.c:632 [inline]
     sock_sendmsg+0xca/0x110 net/socket.c:642
     ___sys_sendmsg+0x75b/0x8a0 net/socket.c:2048
     __sys_sendmsg+0xe5/0x210 net/socket.c:2082
     SYSC_sendmsg net/socket.c:2093 [inline]
     SyS_sendmsg+0x2d/0x50 net/socket.c:2089
     entry_SYSCALL_64_fastpath+0x1f/0x96
    RIP: 0033:0x452879
    RSP: 002b:00007fc9761bfbe8 EFLAGS: 00000212 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 0000000000758020 RCX: 0000000000452879
    RDX: 0000000000000000 RSI: 0000000020917fc8 RDI: 0000000000000015
    RBP: 0000000000000086 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000212 R12: 00000000006ee3a0
    R13: 00000000ffffffff R14: 00007fc9761c06d4 R15: 0000000000000000
    
    Allocated by task 12860:
     save_stack+0x43/0xd0 mm/kasan/kasan.c:447
     set_track mm/kasan/kasan.c:459 [inline]
     kasan_kmalloc+0xad/0xe0 mm/kasan/kasan.c:551
     kasan_slab_alloc+0x12/0x20 mm/kasan/kasan.c:489
     kmem_cache_alloc_node+0x144/0x760 mm/slab.c:3638
     __alloc_skb+0xf1/0x780 net/core/skbuff.c:193
     alloc_skb_fclone include/linux/skbuff.h:1023 [inline]
     sk_stream_alloc_skb+0x11d/0x900 net/ipv4/tcp.c:870
     tcp_sendmsg_locked+0x1341/0x3b80 net/ipv4/tcp.c:1299
     tcp_sendmsg+0x2f/0x50 net/ipv4/tcp.c:1461
     inet_sendmsg+0x11f/0x5e0 net/ipv4/af_inet.c:763
     sock_sendmsg_nosec net/socket.c:632 [inline]
     sock_sendmsg+0xca/0x110 net/socket.c:642
     SYSC_sendto+0x358/0x5a0 net/socket.c:1749
     SyS_sendto+0x40/0x50 net/socket.c:1717
     entry_SYSCALL_64_fastpath+0x1f/0x96
    
    Freed by task 12860:
     save_stack+0x43/0xd0 mm/kasan/kasan.c:447
     set_track mm/kasan/kasan.c:459 [inline]
     kasan_slab_free+0x71/0xc0 mm/kasan/kasan.c:524
     __cache_free mm/slab.c:3492 [inline]
     kmem_cache_free+0x77/0x280 mm/slab.c:3750
     kfree_skbmem+0xdd/0x1d0 net/core/skbuff.c:603
     __kfree_skb+0x1d/0x20 net/core/skbuff.c:642
     sk_wmem_free_skb include/net/sock.h:1419 [inline]
     tcp_rtx_queue_unlink_and_free include/net/tcp.h:1682 [inline]
     tcp_clean_rtx_queue net/ipv4/tcp_input.c:3111 [inline]
     tcp_ack+0x1b17/0x4fd0 net/ipv4/tcp_input.c:3593
     tcp_rcv_established+0x672/0x18a0 net/ipv4/tcp_input.c:5439
     tcp_v4_do_rcv+0x2ab/0x7d0 net/ipv4/tcp_ipv4.c:1468
     sk_backlog_rcv include/net/sock.h:909 [inline]
     __release_sock+0x124/0x360 net/core/sock.c:2264
     release_sock+0xa4/0x2a0 net/core/sock.c:2778
     tcp_sendmsg+0x3a/0x50 net/ipv4/tcp.c:1462
     inet_sendmsg+0x11f/0x5e0 net/ipv4/af_inet.c:763
     sock_sendmsg_nosec net/socket.c:632 [inline]
     sock_sendmsg+0xca/0x110 net/socket.c:642
     ___sys_sendmsg+0x75b/0x8a0 net/socket.c:2048
     __sys_sendmsg+0xe5/0x210 net/socket.c:2082
     SYSC_sendmsg net/socket.c:2093 [inline]
     SyS_sendmsg+0x2d/0x50 net/socket.c:2089
     entry_SYSCALL_64_fastpath+0x1f/0x96
    
    The buggy address belongs to the object at ffff8801c154fa80
     which belongs to the cache skbuff_fclone_cache of size 456
    The buggy address is located 40 bytes inside of
     456-byte region [ffff8801c154fa80, ffff8801c154fc48)
    The buggy address belongs to the page:
    page:ffffea00070553c0 count:1 mapcount:0 mapping:ffff8801c154f080 index:0x0
    flags: 0x2fffc0000000100(slab)
    raw: 02fffc0000000100 ffff8801c154f080 0000000000000000 0000000100000006
    raw: ffffea00070a5a20 ffffea0006a18360 ffff8801d9ca0500 0000000000000000
    page dumped because: kasan: bad access detected
    
    Fixes: 737ff314563c ("tcp: use sequence distance to detect reordering")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 35cc7d0d3d47..85ea578195d4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1630,9 +1630,6 @@ static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unli
 {
 	if (tcp_write_queue_empty(sk))
 		tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
-
-	if (tcp_sk(sk)->highest_sack == skb_unlinked)
-		tcp_sk(sk)->highest_sack = NULL;
 }
 
 static inline void __tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
@@ -1645,12 +1642,8 @@ static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb
 	__tcp_add_write_queue_tail(sk, skb);
 
 	/* Queue it, remembering where we must start sending. */
-	if (sk->sk_write_queue.next == skb) {
+	if (sk->sk_write_queue.next == skb)
 		tcp_chrono_start(sk, TCP_CHRONO_BUSY);
-
-		if (tcp_sk(sk)->highest_sack == NULL)
-			tcp_sk(sk)->highest_sack = skb;
-	}
 }
 
 /* Insert new before skb on the write queue of sk.  */
@@ -1708,9 +1701,7 @@ static inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)
 
 static inline void tcp_advance_highest_sack(struct sock *sk, struct sk_buff *skb)
 {
-	struct sk_buff *next = skb_rb_next(skb);
-
-	tcp_sk(sk)->highest_sack = next ?: tcp_send_head(sk);
+	tcp_sk(sk)->highest_sack = skb_rb_next(skb);
 }
 
 static inline struct sk_buff *tcp_highest_sack(struct sock *sk)
@@ -1720,9 +1711,7 @@ static inline struct sk_buff *tcp_highest_sack(struct sock *sk)
 
 static inline void tcp_highest_sack_reset(struct sock *sk)
 {
-	struct sk_buff *skb = tcp_rtx_queue_head(sk);
-
-	tcp_sk(sk)->highest_sack = skb ?: tcp_send_head(sk);
+	tcp_sk(sk)->highest_sack = tcp_rtx_queue_head(sk);
 }
 
 /* Called when old skb is about to be deleted and replaced by new skb */

commit 6670e152447732ba90626f36dfc015a13fbf150e
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Tue Nov 14 08:25:49 2017 -0800

    tcp: Namespace-ify sysctl_tcp_default_congestion_control
    
    Make default TCP default congestion control to a per namespace
    value. This changes default congestion control to a pointer to congestion ops
    (rather than implicit as first element of available lsit).
    
    The congestion control setting of new namespaces is inherited
    from the current setting of the root namespace.
    
    Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ed71511e67a6..35cc7d0d3d47 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1002,8 +1002,8 @@ void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 void tcp_assign_congestion_control(struct sock *sk);
 void tcp_init_congestion_control(struct sock *sk);
 void tcp_cleanup_congestion_control(struct sock *sk);
-int tcp_set_default_congestion_control(const char *name);
-void tcp_get_default_congestion_control(char *name);
+int tcp_set_default_congestion_control(struct net *net, const char *name);
+void tcp_get_default_congestion_control(struct net *net, char *name);
 void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
@@ -1017,7 +1017,7 @@ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
 struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
-u32 tcp_ca_get_key_by_name(const char *name, bool *ecn_ca);
+u32 tcp_ca_get_key_by_name(struct net *net, const char *name, bool *ecn_ca);
 #ifdef CONFIG_INET
 char *tcp_ca_get_name_by_key(u32 key, char *buffer);
 #else

commit 713bafea92920103cd3d361657406cf04d0e22dd
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Nov 8 13:01:26 2017 -0800

    tcp: retire FACK loss detection
    
    FACK loss detection has been disabled by default and the
    successor RACK subsumed FACK and can handle reordering better.
    This patch removes FACK to simplify TCP loss recovery.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Soheil Hassas Yeganeh <soheil@google.com>
    Reviewed-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2f2c69ad31b2..ed71511e67a6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -384,7 +384,6 @@ void tcp_update_metrics(struct sock *sk);
 void tcp_init_metrics(struct sock *sk);
 void tcp_metrics_init(void);
 bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
-void tcp_disable_fack(struct tcp_sock *tp);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);
 void tcp_init_transfer(struct sock *sk, int bpf_op);
@@ -776,7 +775,7 @@ struct tcp_skb_cb {
 	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
-	__u8		sacked;		/* State flags for SACK/FACK.	*/
+	__u8		sacked;		/* State flags for SACK.	*/
 #define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
 #define TCPCB_LOST		0x04	/* SKB is lost			*/
@@ -1066,7 +1065,6 @@ void tcp_rate_check_app_limited(struct sock *sk);
  *
  * tcp_is_sack - SACK enabled
  * tcp_is_reno - No SACK
- * tcp_is_fack - FACK enabled, implies SACK enabled
  */
 static inline int tcp_is_sack(const struct tcp_sock *tp)
 {
@@ -1078,16 +1076,6 @@ static inline bool tcp_is_reno(const struct tcp_sock *tp)
 	return !tcp_is_sack(tp);
 }
 
-static inline bool tcp_is_fack(const struct tcp_sock *tp)
-{
-	return tp->rx_opt.sack_ok & TCP_FACK_ENABLED;
-}
-
-static inline void tcp_enable_fack(struct tcp_sock *tp)
-{
-	tp->rx_opt.sack_ok |= TCP_FACK_ENABLED;
-}
-
 static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
 {
 	return tp->sacked_out + tp->lost_out;

commit 356d1833b638bd465672aefeb71def3ab93fc17d
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 7 00:29:28 2017 -0800

    tcp: Namespace-ify sysctl_tcp_rmem and sysctl_tcp_wmem
    
    Note that when a new netns is created, it inherits its
    sysctl_tcp_rmem and sysctl_tcp_wmem from initial netns.
    
    This change is needed so that we can refine TCP rcvbuf autotuning,
    to take RTT into consideration.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Wei Wang <weiwan@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index babfd4da1515..2f2c69ad31b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -242,8 +242,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* sysctl variables for tcp */
 extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
-extern int sysctl_tcp_wmem[3];
-extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 #define TCP_RACK_STATIC_REO_WND  0x2 /* Use static RACK reo wnd */

commit 1f2556916d974cfb62b6af51660186b5f58bd869
Author: Priyaranjan Jha <priyarjha@google.com>
Date:   Fri Nov 3 16:38:48 2017 -0700

    tcp: higher throughput under reordering with adaptive RACK reordering wnd
    
    Currently TCP RACK loss detection does not work well if packets are
    being reordered beyond its static reordering window (min_rtt/4).Under
    such reordering it may falsely trigger loss recoveries and reduce TCP
    throughput significantly.
    
    This patch improves that by increasing and reducing the reordering
    window based on DSACK, which is now supported in major TCP implementations.
    It makes RACK's reo_wnd adaptive based on DSACK and no. of recoveries.
    
    - If DSACK is received, increment reo_wnd by min_rtt/4 (upper bounded
      by srtt), since there is possibility that spurious retransmission was
      due to reordering delay longer than reo_wnd.
    
    - Persist the current reo_wnd value for TCP_RACK_RECOVERY_THRESH (16)
      no. of successful recoveries (accounts for full DSACK-based loss
      recovery undo). After that, reset it to default (min_rtt/4).
    
    - At max, reo_wnd is incremented only once per rtt. So that the new
      DSACK on which we are reacting, is due to the spurious retx (approx)
      after the reo_wnd has been updated last time.
    
    - reo_wnd is tracked in terms of steps (of min_rtt/4), rather than
      absolute value to account for change in rtt.
    
    In our internal testing, we observed significant increase in throughput,
    in scenarios where reordering exceeds min_rtt/4 (previous static value).
    
    Signed-off-by: Priyaranjan Jha <priyarjha@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c2bf2a822b10..babfd4da1515 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -246,6 +246,7 @@ extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
+#define TCP_RACK_STATIC_REO_WND  0x2 /* Use static RACK reo wnd */
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -1901,6 +1902,7 @@ extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     u64 xmit_time);
 extern void tcp_rack_reo_timeout(struct sock *sk);
+extern void tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs);
 
 /* At how many usecs into the future should the RTO fire? */
 static inline s64 tcp_rto_delta_us(const struct sock *sk)

commit ed29668d1aa2c6f01e61dd616df13b5241cee7e0
Merge: 65c959a39b7e 3a99df9a3d14
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Nov 2 14:59:52 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Smooth Cong Wang's bug fix into 'net-next'.  Basically put
    the bulk of the tcf_block_put() logic from 'net' into
    tcf_block_put_ext(), but after the offload unbind.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2b7cda9c35d3b940eb9ce74b30bbd5eb30db493d
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 30 23:08:20 2017 -0700

    tcp: fix tcp_mtu_probe() vs highest_sack
    
    Based on SNMP values provided by Roman, Yuchung made the observation
    that some crashes in tcp_sacktag_walk() might be caused by MTU probing.
    
    Looking at tcp_mtu_probe(), I found that when a new skb was placed
    in front of the write queue, we were not updating tcp highest sack.
    
    If one skb is freed because all its content was copied to the new skb
    (for MTU probing), then tp->highest_sack could point to a now freed skb.
    
    Bad things would then happen, including infinite loops.
    
    This patch renames tcp_highest_sack_combine() and uses it
    from tcp_mtu_probe() to fix the bug.
    
    Note that I also removed one test against tp->sacked_out,
    since we want to replace tp->highest_sack regardless of whatever
    condition, since keeping a stale pointer to freed skb is a recipe
    for disaster.
    
    Fixes: a47e5a988a57 ("[TCP]: Convert highest_sack to sk_buff to allow direct access")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Reported-by: Roman Gushchin <guro@fb.com>
    Reported-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 33599d17522d..e6d0002a1b0b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1771,12 +1771,12 @@ static inline void tcp_highest_sack_reset(struct sock *sk)
 	tcp_sk(sk)->highest_sack = tcp_write_queue_head(sk);
 }
 
-/* Called when old skb is about to be deleted (to be combined with new skb) */
-static inline void tcp_highest_sack_combine(struct sock *sk,
+/* Called when old skb is about to be deleted and replaced by new skb */
+static inline void tcp_highest_sack_replace(struct sock *sk,
 					    struct sk_buff *old,
 					    struct sk_buff *new)
 {
-	if (tcp_sk(sk)->sacked_out && (old == tcp_sk(sk)->highest_sack))
+	if (old == tcp_highest_sack(sk))
 		tcp_sk(sk)->highest_sack = new;
 }
 

commit e1ea2f9856b765a2eaabb403a6751f70efc9ba4c
Merge: aad93c70b9a3 0b07194bb55e
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Oct 30 14:10:01 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several conflicts here.
    
    NFP driver bug fix adding nfp_netdev_is_nfp_repr() check to
    nfp_fl_output() needed some adjustments because the code block is in
    an else block now.
    
    Parallel additions to net/pkt_cls.h and net/sch_generic.h
    
    A bug fix in __tcp_retransmit_skb() conflicted with some of
    the rbtree changes in net-next.
    
    The tc action RCU callback fixes in 'net' had some overlap with some
    of the recent tcf_block reworking.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8108a77515126f6db4374e8593956e20430307c0
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Oct 27 09:45:34 2017 -0700

    bpf: bpf_compute_data uses incorrect cb structure
    
    SK_SKB program types use bpf_compute_data to store the end of the
    packet data. However, bpf_compute_data assumes the cb is stored in the
    qdisc layer format. But, for SK_SKB this is the wrong layer of the
    stack for this type.
    
    It happens to work (sort of!) because in most cases nothing happens
    to be overwritten today. This is very fragile and error prone.
    Fortunately, we have another hole in tcp_skb_cb we can use so lets
    put the data_end value there.
    
    Note, SK_SKB program types do not use data_meta, they are failed by
    sk_skb_is_valid_access().
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b1ef98ebce53..33599d17522d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -844,6 +844,7 @@ struct tcp_skb_cb {
 			__u32 key;
 			__u32 flags;
 			struct bpf_map *map;
+			void *data_end;
 		} bpf;
 	};
 };

commit 5b52a4c3acf5f4b4854d1c3ddc8be8770330a79c
Author: Alexei Starovoitov <ast@fb.com>
Date:   Fri Oct 27 10:01:39 2017 -0700

    tcp: remove unnecessary include
    
    two extra #include are not necessary in tcp.h
    Remove them.
    
    Fixes: 40304b2a1567 ("bpf: BPF support for sock_ops")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 092d606fcc16..aa1cc90fdc02 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -45,9 +45,6 @@
 
 #include <linux/seq_file.h>
 #include <linux/memcontrol.h>
-
-#include <linux/bpf.h>
-#include <linux/filter.h>
 #include <linux/bpf-cgroup.h>
 
 extern struct inet_hashinfo tcp_hashinfo;

commit c26e91f8b9b8e1fd252e07c1f60e50220cd7ebab
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:32 2017 -0700

    tcp: Namespace-ify sysctl_tcp_pacing_ca_ratio
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2a5f8261ca03..092d606fcc16 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,8 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_pacing_ca_ratio;
-
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
 extern unsigned long tcp_memory_pressure;

commit 23a7102a2d1068508fa2a0ce593a0df7f8fdc0ac
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:31 2017 -0700

    tcp: Namespace-ify sysctl_tcp_pacing_ss_ratio
    
    Also remove an obsolete comment about TCP pacing.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5869a822ecb1..2a5f8261ca03 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_pacing_ss_ratio;
 extern int sysctl_tcp_pacing_ca_ratio;
 
 extern atomic_long_t tcp_memory_allocated;

commit 4170ba6b589ced82da56c7e4f71cc84b2be036d6
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:30 2017 -0700

    tcp: Namespace-ify sysctl_tcp_invalid_ratelimit
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0268f1025d9d..5869a822ecb1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_invalid_ratelimit;
 extern int sysctl_tcp_pacing_ss_ratio;
 extern int sysctl_tcp_pacing_ca_ratio;
 

commit 790f00e19f65673c3c169dfc137c09a9236847d5
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:29 2017 -0700

    tcp: Namespace-ify sysctl_tcp_autocorking
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 56f50c9a3e6a..0268f1025d9d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_autocorking;
 extern int sysctl_tcp_invalid_ratelimit;
 extern int sysctl_tcp_pacing_ss_ratio;
 extern int sysctl_tcp_pacing_ca_ratio;

commit bd239704295c66196e6b77c5717ec4aec076ddd5
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:28 2017 -0700

    tcp: Namespace-ify sysctl_tcp_min_rtt_wlen
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0735303a6575..56f50c9a3e6a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_min_rtt_wlen;
 extern int sysctl_tcp_autocorking;
 extern int sysctl_tcp_invalid_ratelimit;
 extern int sysctl_tcp_pacing_ss_ratio;

commit 26e9596e5b8f11025b57b12e7265df649129ab00
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:27 2017 -0700

    tcp: Namespace-ify sysctl_tcp_min_tso_segs
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index afc23596e9aa..0735303a6575 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_min_rtt_wlen;
 extern int sysctl_tcp_autocorking;
 extern int sysctl_tcp_invalid_ratelimit;

commit b530b68148301d73775cd27cc136ce4dd5738ae8
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:26 2017 -0700

    tcp: Namespace-ify sysctl_tcp_challenge_ack_limit
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 33f9d30a6905..afc23596e9aa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_challenge_ack_limit;
 extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_min_rtt_wlen;
 extern int sysctl_tcp_autocorking;

commit 9184d8bb448a3d2c2d9f90f1e2f5de625292e769
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:25 2017 -0700

    tcp: Namespace-ify sysctl_tcp_limit_output_bytes
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e338e16178dd..33f9d30a6905 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -250,7 +250,6 @@ extern int sysctl_tcp_rmem[3];
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
-extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
 extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_min_rtt_wlen;

commit ceef9ab6be7234f9e49f79769e0da88d1dccfcc7
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:24 2017 -0700

    tcp: Namespace-ify sysctl_tcp_workaround_signed_windows
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ed0828dc82f1..e338e16178dd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_workaround_signed_windows;
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
@@ -1302,7 +1301,8 @@ static inline void tcp_slow_start_after_idle_check(struct sock *sk)
 }
 
 /* Determine a window scaling and initial window to offer. */
-void tcp_select_initial_window(int __space, __u32 mss, __u32 *rcv_wnd,
+void tcp_select_initial_window(const struct sock *sk, int __space,
+			       __u32 mss, __u32 *rcv_wnd,
 			       __u32 *window_clamp, int wscale_ok,
 			       __u8 *rcv_wscale, __u32 init_rcv_wnd);
 

commit d06a99045837d3f4d5431793c4c390b0daf2a08d
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:23 2017 -0700

    tcp: Namespace-ify sysctl_tcp_tso_win_divisor
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f954e74578ff..ed0828dc82f1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_workaround_signed_windows;
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */

commit 4540c0cf98b8892a642d2453eec20ae3eb5696fb
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:22 2017 -0700

    tcp: Namespace-ify sysctl_tcp_moderate_rcvbuf
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6ab7fa4154b2..f954e74578ff 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_workaround_signed_windows;
 

commit ec36e416f06f6a8659281053fdc46ce484ad2211
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 27 07:47:21 2017 -0700

    tcp: Namespace-ify sysctl_tcp_nometrics_save
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 18f047501f53..6ab7fa4154b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_workaround_signed_windows;

commit af9b69a7a6ca6b817e8d6f416e7aa5b2a5bf1d91
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:10 2017 -0700

    tcp: Namespace-ify sysctl_tcp_frto
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0dc27cd24899..18f047501f53 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_frto;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;

commit 94f0893e0c27219f4a726932618505aab6795973
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:09 2017 -0700

    tcp: Namespace-ify sysctl_tcp_adv_win_scale
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7aa3d65062a1..0dc27cd24899 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
@@ -1311,9 +1310,9 @@ void tcp_select_initial_window(int __space, __u32 mss, __u32 *rcv_wnd,
 			       __u32 *window_clamp, int wscale_ok,
 			       __u8 *rcv_wscale, __u32 init_rcv_wnd);
 
-static inline int tcp_win_from_space(int space)
+static inline int tcp_win_from_space(const struct sock *sk, int space)
 {
-	int tcp_adv_win_scale = sysctl_tcp_adv_win_scale;
+	int tcp_adv_win_scale = sock_net(sk)->ipv4.sysctl_tcp_adv_win_scale;
 
 	return tcp_adv_win_scale <= 0 ?
 		(space>>(-tcp_adv_win_scale)) :
@@ -1323,13 +1322,13 @@ static inline int tcp_win_from_space(int space)
 /* Note: caller must be prepared to deal with negative returns */
 static inline int tcp_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk->sk_rcvbuf -
+	return tcp_win_from_space(sk, sk->sk_rcvbuf -
 				  atomic_read(&sk->sk_rmem_alloc));
 }
 
 static inline int tcp_full_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk->sk_rcvbuf);
+	return tcp_win_from_space(sk, sk->sk_rcvbuf);
 }
 
 extern void tcp_openreq_init_rwin(struct request_sock *req,

commit 0c12654ac6d9004b9538b2a969b2b59e9a5ed831
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:08 2017 -0700

    tcp: Namespace-ify sysctl_tcp_app_win
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8b2ae3e8d79f..7aa3d65062a1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -247,7 +247,6 @@ extern int sysctl_tcp_max_orphans;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
-extern int sysctl_tcp_app_win;
 extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_nometrics_save;

commit 6496f6bde0c323fba5e8c5b5cbf3a7bf28dad7ed
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:07 2017 -0700

    tcp: Namespace-ify sysctl_tcp_dsack
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8cd286226a1e..8b2ae3e8d79f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -244,7 +244,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_max_orphans;
-extern int sysctl_tcp_dsack;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];

commit c6e218035913e14952b04ceecf1a543205106fdb
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:06 2017 -0700

    tcp: Namespace-ify sysctl_tcp_max_reordering
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index fc134ba74c7d..8cd286226a1e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -244,7 +244,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_max_orphans;
-extern int sysctl_tcp_max_reordering;
 extern int sysctl_tcp_dsack;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];

commit 773d4bb96ceca6829ae9928f6b002b93e2e62cdc
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:05 2017 -0700

    tcp: remove stale sysctl_tcp_reordering
    
    This extern is no longer used.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e7b15e9f6e28..fc134ba74c7d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -244,7 +244,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_max_orphans;
-extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_max_reordering;
 extern int sysctl_tcp_dsack;
 extern long sysctl_tcp_mem[3];

commit 0bc65a28ae2aeb14aab7f4a930e0d8cf4cad9dc4
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:04 2017 -0700

    tcp: Namespace-ify sysctl_tcp_fack
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7331281a2292..e7b15e9f6e28 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -244,7 +244,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_max_orphans;
-extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_max_reordering;
 extern int sysctl_tcp_dsack;

commit 65c9410cf55ecf32da1b720f563365d565d6289a
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:03 2017 -0700

    tcp: Namespace-ify sysctl_tcp_abort_on_overflow
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2aea2b3373b3..7331281a2292 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -243,7 +243,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;
 extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;

commit 625357aa175c688d219da43c8cfaa2e1629e0e1a
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:02 2017 -0700

    tcp: Namespace-ify sysctl_tcp_rfc1337
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cf3fac7008d7..2aea2b3373b3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -243,7 +243,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_rfc1337;
 extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;
 extern int sysctl_tcp_fack;

commit 3f4c7c6f6a9053493ce7dd8a0f17ed8eafc53893
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:01 2017 -0700

    tcp: Namespace-ify sysctl_tcp_stdurg
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 33cc86355b8f..cf3fac7008d7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -243,7 +243,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_stdurg;
 extern int sysctl_tcp_rfc1337;
 extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;

commit e0a1e5b519236dc1662ff25e42560dd1be9e3776
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:55:00 2017 -0700

    tcp: Namespace-ify sysctl_tcp_retrans_collapse
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cc2ab522eb5c..33cc86355b8f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -243,7 +243,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
 extern int sysctl_tcp_rfc1337;
 extern int sysctl_tcp_abort_on_overflow;

commit b510f0d23a47c3d1f074fe583e7867dc4918fe02
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:54:59 2017 -0700

    tcp: Namespace-ify sysctl_tcp_slow_start_after_idle
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 063a7a48b7fe..cc2ab522eb5c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -262,7 +262,6 @@ extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_workaround_signed_windows;
-extern int sysctl_tcp_slow_start_after_idle;
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
@@ -1308,7 +1307,7 @@ static inline void tcp_slow_start_after_idle_check(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 	s32 delta;
 
-	if (!sysctl_tcp_slow_start_after_idle || tp->packets_out ||
+	if (!sock_net(sk)->ipv4.sysctl_tcp_slow_start_after_idle || tp->packets_out ||
 	    ca_ops->cong_control)
 		return;
 	delta = tcp_jiffies32 - tp->lsndtime;

commit 2c04ac8ae0b61e0780a30b7069a11bb202b21f26
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:54:58 2017 -0700

    tcp: Namespace-ify sysctl_tcp_thin_linear_timeouts
    
    Note that sysctl_tcp_thin_dupack was not used, I deleted it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c7f51534fc44..063a7a48b7fe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -263,8 +263,6 @@ extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
-extern int sysctl_tcp_thin_linear_timeouts;
-extern int sysctl_tcp_thin_dupack;
 
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 

commit e20223f1962831d1b1c416d59d259879d0639d68
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:54:57 2017 -0700

    tcp: Namespace-ify sysctl_tcp_recovery
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a12b71d4118b..c7f51534fc44 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -265,7 +265,7 @@ extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
-extern int sysctl_tcp_recovery;
+
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 
 extern int sysctl_tcp_limit_output_bytes;

commit 2ae21cf527da0e5cf9d7ee14bd5b0909bb9d1a75
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 26 21:54:56 2017 -0700

    tcp: Namespace-ify sysctl_tcp_early_retrans
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 285bc82dea41..a12b71d4118b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -265,7 +265,6 @@ extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
-extern int sysctl_tcp_early_retrans;
 extern int sysctl_tcp_recovery;
 #define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
 

commit 60e2a7780793bae0debc275a9ccd57f7da0cf195
Author: Ursula Braun <ubraun@linux.vnet.ibm.com>
Date:   Wed Oct 25 11:01:45 2017 +0200

    tcp: TCP experimental option for SMC
    
    The SMC protocol [1] relies on the use of a new TCP experimental
    option [2, 3]. With this option, SMC capabilities are exchanged
    between peers during the TCP three way handshake. This patch adds
    support for this experimental option to TCP.
    
    References:
    [1] SMC-R Informational RFC: http://www.rfc-editor.org/info/rfc7609
    [2] Shared Use of TCP Experimental Options RFC 6994:
        https://tools.ietf.org/rfc/rfc6994.txt
    [3] IANA ExID SMCR:
    http://www.iana.org/assignments/tcp-parameters/tcp-parameters.xhtml#tcp-exids
    
    Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2392f74074e7..285bc82dea41 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -191,6 +191,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
  * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
  */
 #define TCPOPT_FASTOPEN_MAGIC	0xF989
+#define TCPOPT_SMC_MAGIC	0xE2D4C3D9
 
 /*
  *     TCP option lengths
@@ -203,6 +204,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_MD5SIG         18
 #define TCPOLEN_FASTOPEN_BASE  2
 #define TCPOLEN_EXP_FASTOPEN_BASE  4
+#define TCPOLEN_EXP_SMC_BASE   6
 
 /* But this is what stacks really send out. */
 #define TCPOLEN_TSTAMP_ALIGNED		12
@@ -213,6 +215,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERBLOCK		8
 #define TCPOLEN_MD5SIG_ALIGNED		20
 #define TCPOLEN_MSS_ALIGNED		4
+#define TCPOLEN_EXP_SMC_BASE_ALIGNED	8
 
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
@@ -2108,4 +2111,8 @@ static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
 {
 	return (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN) == 1);
 }
+
+#if IS_ENABLED(CONFIG_SMC)
+extern struct static_key_false tcp_have_smc;
+#endif
 #endif	/* _TCP_H */

commit 71c02379c762cb616c00fd5c4ed253fbf6bbe11b
Author: Christoph Paasch <cpaasch@apple.com>
Date:   Mon Oct 23 13:22:23 2017 -0700

    tcp: Configure TFO without cookie per socket and/or per route
    
    We already allow to enable TFO without a cookie by using the
    fastopen-sysctl and setting it to TFO_SERVER_COOKIE_NOT_REQD (or
    TFO_CLIENT_NO_COOKIE).
    This is safe to do in certain environments where we know that there
    isn't a malicous host (aka., data-centers) or when the
    application-protocol already provides an authentication mechanism in the
    first flight of data.
    
    A server however might be providing multiple services or talking to both
    sides (public Internet and data-center). So, this server would want to
    enable cookie-less TFO for certain services and/or for connections that
    go to the data-center.
    
    This patch exposes a socket-option and a per-route attribute to enable such
    fine-grained configurations.
    
    Signed-off-by: Christoph Paasch <cpaasch@apple.com>
    Reviewed-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2c13484704cb..2392f74074e7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1567,7 +1567,8 @@ int tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
-			      struct tcp_fastopen_cookie *foc);
+			      struct tcp_fastopen_cookie *foc,
+			      const struct dst_entry *dst);
 void tcp_fastopen_init_key_once(struct net *net);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);

commit f8ddadc4db6c7b7029b6d0e0d9af24f74ad27ca2
Merge: bdd091bab8c6 b5ac3beb5a9f
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Oct 22 13:36:53 2017 +0100

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    There were quite a few overlapping sets of changes here.
    
    Daniel's bug fix for off-by-ones in the new BPF branch instructions,
    along with the added allowances for "data_end > ptr + x" forms
    collided with the metadata additions.
    
    Along with those three changes came veritifer test cases, which in
    their final form I tried to group together properly.  If I had just
    trimmed GIT's conflict tags as-is, this would have split up the
    meta tests unnecessarily.
    
    In the socketmap code, a set of preemption disabling changes
    overlapped with the rename of bpf_compute_data_end() to
    bpf_compute_data_pointers().
    
    Changes were made to the mv88e6060.c driver set addr method
    which got removed in net-next.
    
    The hyperv transport socket layer had a locking change in 'net'
    which overlapped with a change of socket state macro usage
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1fba70e5b6bed53496ba1f1f16127f5be01b5fb6
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Oct 18 11:22:51 2017 -0700

    tcp: socket option to set TCP fast open key
    
    New socket option TCP_FASTOPEN_KEY to allow different keys per
    listener.  The listener by default uses the global key until the
    socket option is set.  The key is a 16 bytes long binary data. This
    option has no effect on regular non-listener TCP sockets.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3b3b9b968e2d..1efe8365cb28 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1555,9 +1555,10 @@ struct tcp_fastopen_request {
 	int				copied;	/* queued in tcp_connect() */
 };
 void tcp_free_fastopen_req(struct tcp_sock *tp);
-
+void tcp_fastopen_destroy_cipher(struct sock *sk);
 void tcp_fastopen_ctx_destroy(struct net *net);
-int tcp_fastopen_reset_cipher(struct net *net, void *key, unsigned int len);
+int tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,
+			      void *key, unsigned int len);
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,

commit 34f79502bbcfab659b8729da68b5e387f96eb4c1
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Oct 18 07:10:36 2017 -0700

    bpf: avoid preempt enable/disable in sockmap using tcp_skb_cb region
    
    SK_SKB BPF programs are run from the socket/tcp context but early in
    the stack before much of the TCP metadata is needed in tcp_skb_cb. So
    we can use some unused fields to place BPF metadata needed for SK_SKB
    programs when implementing the redirect function.
    
    This allows us to drop the preempt disable logic. It does however
    require an API change so sk_redirect_map() has been updated to
    additionally provide ctx_ptr to skb. Note, we do however continue to
    disable/enable preemption around actual BPF program running to account
    for map updates.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 89974c5286d8..b1ef98ebce53 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -840,6 +840,11 @@ struct tcp_skb_cb {
 			struct inet6_skb_parm	h6;
 #endif
 		} header;	/* For incoming skbs */
+		struct {
+			__u32 key;
+			__u32 flags;
+			struct bpf_map *map;
+		} bpf;
 	};
 };
 

commit 437d2762ba07f0fc639d5a09acb323fe4106a61f
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 11 20:45:40 2017 -0700

    tcp: remove obsolete helpers
    
    Remove three inline helpers that are no longer needed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 15163454174b..3b3b9b968e2d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1629,18 +1629,6 @@ static inline struct sk_buff *tcp_write_queue_tail(const struct sock *sk)
 	return skb_peek_tail(&sk->sk_write_queue);
 }
 
-static inline struct sk_buff *tcp_write_queue_next(const struct sock *sk,
-						   const struct sk_buff *skb)
-{
-	return skb_queue_next(&sk->sk_write_queue, skb);
-}
-
-static inline struct sk_buff *tcp_write_queue_prev(const struct sock *sk,
-						   const struct sk_buff *skb)
-{
-	return skb_queue_prev(&sk->sk_write_queue, skb);
-}
-
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
@@ -1697,11 +1685,6 @@ static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb
 	}
 }
 
-static inline void __tcp_add_write_queue_head(struct sock *sk, struct sk_buff *skb)
-{
-	__skb_queue_head(&sk->sk_write_queue, skb);
-}
-
 /* Insert new before skb on the write queue of sk.  */
 static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 						  struct sk_buff *skb,

commit 4a269818a7eb8577d32d8b2879099c689ddbd856
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 11 13:27:29 2017 -0700

    tcp: fix tcp_unlink_write_queue()
    
    Yury reported crash with this signature :
    
    [  554.034021] [<ffff80003ccd5a58>] 0xffff80003ccd5a58
    [  554.034156] [<ffff00000888fd34>] skb_release_all+0x14/0x30
    [  554.034288] [<ffff00000888fd64>] __kfree_skb+0x14/0x28
    [  554.034409] [<ffff0000088ece6c>] tcp_sendmsg_locked+0x4dc/0xcc8
    [  554.034541] [<ffff0000088ed68c>] tcp_sendmsg+0x34/0x58
    [  554.034659] [<ffff000008919fd4>] inet_sendmsg+0x2c/0xf8
    [  554.034783] [<ffff0000088842e8>] sock_sendmsg+0x18/0x30
    [  554.034928] [<ffff0000088861fc>] SyS_sendto+0x84/0xf8
    
    Problem is that skb->destructor contains garbage, and this is
    because I accidentally removed tcp_skb_tsorted_anchor_cleanup()
    from tcp_unlink_write_queue()
    
    This would trigger with a write(fd, <invalid_memory>, len) attempt,
    and we will add to packetdrill this capability to avoid future
    regressions.
    
    Fixes: 75c119afe14f ("tcp: implement rb-tree based retransmit queue")
    Reported-by: Yury Norov <ynorov@caviumnetworks.com>
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5a95e5886b55..15163454174b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1712,6 +1712,7 @@ static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 
 static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 {
+	tcp_skb_tsorted_anchor_cleanup(skb);
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 

commit 75c119afe14f74b4dd967d75ed9f57ab6c0ef045
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 5 22:21:27 2017 -0700

    tcp: implement rb-tree based retransmit queue
    
    Using a linear list to store all skbs in write queue has been okay
    for quite a while : O(N) is not too bad when N < 500.
    
    Things get messy when N is the order of 100,000 : Modern TCP stacks
    want 10Gbit+ of throughput even with 200 ms RTT flows.
    
    40 ns per cache line miss means a full scan can use 4 ms,
    blowing away CPU caches.
    
    SACK processing often can use various hints to avoid parsing
    whole retransmit queue. But with high packet losses and/or high
    reordering, hints no longer work.
    
    Sender has to process thousands of unfriendly SACK, accumulating
    a huge socket backlog, burning a cpu and massively dropping packets.
    
    Using an rb-tree for retransmit queue has been avoided for years
    because it added complexity and overhead, but now is the time
    to be more resistant and say no to quadratic behavior.
    
    1) RTX queue is no longer part of the write queue : already sent skbs
    are stored in one rb-tree.
    
    2) Since reaching the head of write queue no longer needs
    sk->sk_send_head, we added an union of sk_send_head and tcp_rtx_queue
    
    Tested:
    
     On receiver :
     netem on ingress : delay 150ms 200us loss 1
     GRO disabled to force stress and SACK storms.
    
    for f in `seq 1 10`
    do
     ./netperf -H lpaa6 -l30 -- -K bbr -o THROUGHPUT|tail -1
    done | awk '{print $0} {sum += $0} END {printf "%7u\n",sum}'
    
    Before patch :
    
    323.87
    351.48
    339.59
    338.62
    306.72
    204.07
    304.93
    291.88
    202.47
    176.88
       2840
    
    After patch:
    
    1700.83
    2207.98
    2070.17
    1544.26
    2114.76
    2124.89
    1693.14
    1080.91
    2216.82
    1299.94
      18053
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 744559b72784..5a95e5886b55 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -551,7 +551,13 @@ void tcp_xmit_retransmit_queue(struct sock *);
 void tcp_simple_retransmit(struct sock *);
 void tcp_enter_recovery(struct sock *sk, bool ece_ack);
 int tcp_trim_head(struct sock *, struct sk_buff *, u32);
-int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int, gfp_t);
+enum tcp_queue {
+	TCP_FRAG_IN_WRITE_QUEUE,
+	TCP_FRAG_IN_RTX_QUEUE,
+};
+int tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,
+		 struct sk_buff *skb, u32 len,
+		 unsigned int mss_now, gfp_t gfp);
 
 void tcp_send_probe0(struct sock *);
 void tcp_send_partial(struct sock *);
@@ -1608,6 +1614,11 @@ static inline void tcp_skb_tsorted_anchor_cleanup(struct sk_buff *skb)
 
 void tcp_write_queue_purge(struct sock *sk);
 
+static inline struct sk_buff *tcp_rtx_queue_head(const struct sock *sk)
+{
+	return skb_rb_first(&sk->tcp_rtx_queue);
+}
+
 static inline struct sk_buff *tcp_write_queue_head(const struct sock *sk)
 {
 	return skb_peek(&sk->sk_write_queue);
@@ -1630,18 +1641,12 @@ static inline struct sk_buff *tcp_write_queue_prev(const struct sock *sk,
 	return skb_queue_prev(&sk->sk_write_queue, skb);
 }
 
-#define tcp_for_write_queue(skb, sk)					\
-	skb_queue_walk(&(sk)->sk_write_queue, skb)
-
-#define tcp_for_write_queue_from(skb, sk)				\
-	skb_queue_walk_from(&(sk)->sk_write_queue, skb)
-
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
 static inline struct sk_buff *tcp_send_head(const struct sock *sk)
 {
-	return sk->sk_send_head;
+	return skb_peek(&sk->sk_write_queue);
 }
 
 static inline bool tcp_skb_is_last(const struct sock *sk,
@@ -1650,29 +1655,30 @@ static inline bool tcp_skb_is_last(const struct sock *sk,
 	return skb_queue_is_last(&sk->sk_write_queue, skb);
 }
 
-static inline void tcp_advance_send_head(struct sock *sk, const struct sk_buff *skb)
+static inline bool tcp_write_queue_empty(const struct sock *sk)
 {
-	if (tcp_skb_is_last(sk, skb))
-		sk->sk_send_head = NULL;
-	else
-		sk->sk_send_head = tcp_write_queue_next(sk, skb);
+	return skb_queue_empty(&sk->sk_write_queue);
+}
+
+static inline bool tcp_rtx_queue_empty(const struct sock *sk)
+{
+	return RB_EMPTY_ROOT(&sk->tcp_rtx_queue);
+}
+
+static inline bool tcp_rtx_and_write_queues_empty(const struct sock *sk)
+{
+	return tcp_rtx_queue_empty(sk) && tcp_write_queue_empty(sk);
 }
 
 static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)
 {
-	if (sk->sk_send_head == skb_unlinked) {
-		sk->sk_send_head = NULL;
+	if (tcp_write_queue_empty(sk))
 		tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
-	}
+
 	if (tcp_sk(sk)->highest_sack == skb_unlinked)
 		tcp_sk(sk)->highest_sack = NULL;
 }
 
-static inline void tcp_init_send_head(struct sock *sk)
-{
-	sk->sk_send_head = NULL;
-}
-
 static inline void __tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_queue_tail(&sk->sk_write_queue, skb);
@@ -1683,8 +1689,7 @@ static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb
 	__tcp_add_write_queue_tail(sk, skb);
 
 	/* Queue it, remembering where we must start sending. */
-	if (sk->sk_send_head == NULL) {
-		sk->sk_send_head = skb;
+	if (sk->sk_write_queue.next == skb) {
 		tcp_chrono_start(sk, TCP_CHRONO_BUSY);
 
 		if (tcp_sk(sk)->highest_sack == NULL)
@@ -1697,35 +1702,32 @@ static inline void __tcp_add_write_queue_head(struct sock *sk, struct sk_buff *s
 	__skb_queue_head(&sk->sk_write_queue, skb);
 }
 
-/* Insert buff after skb on the write queue of sk.  */
-static inline void tcp_insert_write_queue_after(struct sk_buff *skb,
-						struct sk_buff *buff,
-						struct sock *sk)
-{
-	__skb_queue_after(&sk->sk_write_queue, skb, buff);
-}
-
 /* Insert new before skb on the write queue of sk.  */
 static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 						  struct sk_buff *skb,
 						  struct sock *sk)
 {
 	__skb_queue_before(&sk->sk_write_queue, skb, new);
-
-	if (sk->sk_send_head == skb)
-		sk->sk_send_head = new;
 }
 
 static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 {
-	list_del(&skb->tcp_tsorted_anchor);
-	tcp_skb_tsorted_anchor_cleanup(skb);
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 
-static inline bool tcp_write_queue_empty(struct sock *sk)
+void tcp_rbtree_insert(struct rb_root *root, struct sk_buff *skb);
+
+static inline void tcp_rtx_queue_unlink(struct sk_buff *skb, struct sock *sk)
 {
-	return skb_queue_empty(&sk->sk_write_queue);
+	tcp_skb_tsorted_anchor_cleanup(skb);
+	rb_erase(&skb->rbnode, &sk->tcp_rtx_queue);
+}
+
+static inline void tcp_rtx_queue_unlink_and_free(struct sk_buff *skb, struct sock *sk)
+{
+	list_del(&skb->tcp_tsorted_anchor);
+	tcp_rtx_queue_unlink(skb, sk);
+	sk_wmem_free_skb(sk, skb);
 }
 
 static inline void tcp_push_pending_frames(struct sock *sk)
@@ -1754,8 +1756,9 @@ static inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)
 
 static inline void tcp_advance_highest_sack(struct sock *sk, struct sk_buff *skb)
 {
-	tcp_sk(sk)->highest_sack = tcp_skb_is_last(sk, skb) ? NULL :
-						tcp_write_queue_next(sk, skb);
+	struct sk_buff *next = skb_rb_next(skb);
+
+	tcp_sk(sk)->highest_sack = next ?: tcp_send_head(sk);
 }
 
 static inline struct sk_buff *tcp_highest_sack(struct sock *sk)
@@ -1765,7 +1768,9 @@ static inline struct sk_buff *tcp_highest_sack(struct sock *sk)
 
 static inline void tcp_highest_sack_reset(struct sock *sk)
 {
-	tcp_sk(sk)->highest_sack = tcp_write_queue_head(sk);
+	struct sk_buff *skb = tcp_rtx_queue_head(sk);
+
+	tcp_sk(sk)->highest_sack = skb ?: tcp_send_head(sk);
 }
 
 /* Called when old skb is about to be deleted (to be combined with new skb) */
@@ -1935,7 +1940,7 @@ extern void tcp_rack_reo_timeout(struct sock *sk);
 /* At how many usecs into the future should the RTO fire? */
 static inline s64 tcp_rto_delta_us(const struct sock *sk)
 {
-	const struct sk_buff *skb = tcp_write_queue_head(sk);
+	const struct sk_buff *skb = tcp_rtx_queue_head(sk);
 	u32 rto = inet_csk(sk)->icsk_rto;
 	u64 rto_time_stamp_us = skb->skb_mstamp + jiffies_to_usecs(rto);
 

commit ac3f09ba3e496bd7cc780ead05b1d1bb5f33aedb
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 5 22:21:22 2017 -0700

    tcp: uninline tcp_write_queue_purge()
    
    Since the upcoming rtx rbtree will add some extra code,
    it is time to not inline this fat function anymore.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3b16f353b539..744559b72784 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1606,20 +1606,7 @@ static inline void tcp_skb_tsorted_anchor_cleanup(struct sk_buff *skb)
 	skb->_skb_refdst = _save;		\
 }
 
-/* write queue abstraction */
-static inline void tcp_write_queue_purge(struct sock *sk)
-{
-	struct sk_buff *skb;
-
-	tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
-	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL) {
-		tcp_skb_tsorted_anchor_cleanup(skb);
-		sk_wmem_free_skb(sk, skb);
-	}
-	INIT_LIST_HEAD(&tcp_sk(sk)->tsorted_sent_queue);
-	sk_mem_reclaim(sk);
-	tcp_clear_all_retrans_hints(tcp_sk(sk));
-}
+void tcp_write_queue_purge(struct sock *sk);
 
 static inline struct sk_buff *tcp_write_queue_head(const struct sock *sk)
 {

commit e2080072ed2d98a55ae69d95dea60ff7a17cddd5
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 4 12:59:58 2017 -0700

    tcp: new list for sent but unacked skbs for RACK recovery
    
    This patch adds a new queue (list) that tracks the sent but not yet
    acked or SACKed skbs for a TCP connection. The list is chronologically
    ordered by skb->skb_mstamp (the head is the oldest sent skb).
    
    This list will be used to optimize TCP Rack recovery, which checks
    an skb's timestamp to judge if it has been lost and needs to be
    retransmitted. Since TCP write queue is ordered by sequence instead
    of sent time, RACK has to scan over the write queue to catch all
    eligible packets to detect lost retransmission, and iterates through
    SACKed skbs repeatedly.
    
    Special cares for rare events:
    1. TCP repair fakes skb transmission so the send queue needs adjusted
    2. SACK reneging would require re-inserting SACKed skbs into the
       send queue. For now I believe it's not worth the complexity to
       make RACK work perfectly on SACK reneging, so we do nothing here.
    3. Fast Open: currently for non-TFO, send-queue correctly queues
       the pure SYN packet. For TFO which queues a pure SYN and
       then a data packet, send-queue only queues the data packet but
       not the pure SYN due to the structure of TFO code. This is okay
       because the SYN receiver would never respond with a SACK on a
       missing SYN (i.e. SYN is never fast-retransmitted by SACK/RACK).
    
    In order to not grow sk_buff, we use an union for the new list and
    _skb_refdst/destructor fields. This is a bit complicated because
    we need to make sure _skb_refdst and destructor are properly zeroed
    before skb is cloned/copied at transmit, and before being freed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 426c2e986016..3b16f353b539 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1589,14 +1589,34 @@ enum tcp_chrono {
 void tcp_chrono_start(struct sock *sk, const enum tcp_chrono type);
 void tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type);
 
+/* This helper is needed, because skb->tcp_tsorted_anchor uses
+ * the same memory storage than skb->destructor/_skb_refdst
+ */
+static inline void tcp_skb_tsorted_anchor_cleanup(struct sk_buff *skb)
+{
+	skb->destructor = NULL;
+	skb->_skb_refdst = 0UL;
+}
+
+#define tcp_skb_tsorted_save(skb) {		\
+	unsigned long _save = skb->_skb_refdst;	\
+	skb->_skb_refdst = 0UL;
+
+#define tcp_skb_tsorted_restore(skb)		\
+	skb->_skb_refdst = _save;		\
+}
+
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
 {
 	struct sk_buff *skb;
 
 	tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
-	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
+	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL) {
+		tcp_skb_tsorted_anchor_cleanup(skb);
 		sk_wmem_free_skb(sk, skb);
+	}
+	INIT_LIST_HEAD(&tcp_sk(sk)->tsorted_sent_queue);
 	sk_mem_reclaim(sk);
 	tcp_clear_all_retrans_hints(tcp_sk(sk));
 }
@@ -1711,6 +1731,8 @@ static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 
 static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 {
+	list_del(&skb->tcp_tsorted_anchor);
+	tcp_skb_tsorted_anchor_cleanup(skb);
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 

commit 27204aaa9dc67b833b77179fdac556288ec3a4bf
Author: Wei Wang <weiwan@google.com>
Date:   Wed Oct 4 10:03:44 2017 -0700

    tcp: uniform the set up of sockets after successful connection
    
    Currently in the TCP code, the initialization sequence for cached
    metrics, congestion control, BPF, etc, after successful connection
    is very inconsistent. This introduces inconsistent bevhavior and is
    prone to bugs. The current call sequence is as follows:
    
    (1) for active case (tcp_finish_connect() case):
            tcp_mtup_init(sk);
            icsk->icsk_af_ops->rebuild_header(sk);
            tcp_init_metrics(sk);
            tcp_call_bpf(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB);
            tcp_init_congestion_control(sk);
            tcp_init_buffer_space(sk);
    
    (2) for passive case (tcp_rcv_state_process() TCP_SYN_RECV case):
            icsk->icsk_af_ops->rebuild_header(sk);
            tcp_call_bpf(sk, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB);
            tcp_init_congestion_control(sk);
            tcp_mtup_init(sk);
            tcp_init_buffer_space(sk);
            tcp_init_metrics(sk);
    
    (3) for TFO passive case (tcp_fastopen_create_child()):
            inet_csk(child)->icsk_af_ops->rebuild_header(child);
            tcp_init_congestion_control(child);
            tcp_mtup_init(child);
            tcp_init_metrics(child);
            tcp_call_bpf(child, BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB);
            tcp_init_buffer_space(child);
    
    This commit uniforms the above functions to have the following sequence:
            tcp_mtup_init(sk);
            icsk->icsk_af_ops->rebuild_header(sk);
            tcp_init_metrics(sk);
            tcp_call_bpf(sk, BPF_SOCK_OPS_ACTIVE/PASSIVE_ESTABLISHED_CB);
            tcp_init_congestion_control(sk);
            tcp_init_buffer_space(sk);
    This sequence is the same as the (1) active case. We pick this sequence
    because this order correctly allows BPF to override the settings
    including congestion control module and initial cwnd, etc from
    the route, and then allows the CC module to see those settings.
    
    Suggested-by: Neal Cardwell <ncardwell@google.com>
    Tested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7a3a8af56fd6..426c2e986016 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -416,6 +416,7 @@ bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 void tcp_disable_fack(struct tcp_sock *tp);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);
+void tcp_init_transfer(struct sock *sk, int bpf_op);
 unsigned int tcp_poll(struct file *file, struct socket *sock,
 		      struct poll_table_struct *wait);
 int tcp_getsockopt(struct sock *sk, int level, int optname,

commit 53954cf8c5d205624167a2bfd117cc0c1a5f3c6d
Merge: 4b54db137575 7a92616c0bac
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Oct 5 17:57:03 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Just simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 437138485656c41e32b8c63c0987cfa0348be0e6
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Wed Sep 27 11:35:42 2017 +0800

    ipv4: Namespaceify tcp_fastopen_key knob
    
    Different namespace application might require different tcp_fastopen_key
    independently of the host.
    
    David Miller pointed out there is a leak without releasing the context
    of tcp_fastopen_key during netns teardown. So add the release action in
    exit_batch path.
    
    Tested:
    1. Container namespace:
    # cat /proc/sys/net/ipv4/tcp_fastopen_key:
    2817fff2-f803cf97-eadfd1f3-78c0992b
    
    cookie key in tcp syn packets:
    Fast Open Cookie
        Kind: TCP Fast Open Cookie (34)
        Length: 10
        Fast Open Cookie: 1e5dd82a8c492ca9
    
    2. Host:
    # cat /proc/sys/net/ipv4/tcp_fastopen_key:
    107d7c5f-68eb2ac7-02fb06e6-ed341702
    
    cookie key in tcp syn packets:
    Fast Open Cookie
        Kind: TCP Fast Open Cookie (34)
        Length: 10
        Fast Open Cookie: e213c02bf0afbc8a
    
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d9376e2458e9..6d25d8305054 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1549,13 +1549,13 @@ struct tcp_fastopen_request {
 };
 void tcp_free_fastopen_req(struct tcp_sock *tp);
 
-extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
-int tcp_fastopen_reset_cipher(void *key, unsigned int len);
+void tcp_fastopen_ctx_destroy(struct net *net);
+int tcp_fastopen_reset_cipher(struct net *net, void *key, unsigned int len);
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
 			      struct tcp_fastopen_cookie *foc);
-void tcp_fastopen_init_key_once(void);
+void tcp_fastopen_init_key_once(struct net *net);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);
 bool tcp_fastopen_defer_connect(struct sock *sk, int *err);

commit dd000598a39b6937fcefdf143720ec9fb5250e72
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Wed Sep 27 11:35:41 2017 +0800

    ipv4: Remove the 'publish' logic in tcp_fastopen_init_key_once
    
    The 'publish' logic is not necessary after commit dfea2aa65424 ("tcp:
    Do not call tcp_fastopen_reset_cipher from interrupt context"), because
    in tcp_fastopen_cookie_gen，it wouldn't call tcp_fastopen_init_key_once.
    
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9e414a99034f..d9376e2458e9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1555,7 +1555,7 @@ void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
 			      struct tcp_fastopen_cookie *foc);
-void tcp_fastopen_init_key_once(bool publish);
+void tcp_fastopen_init_key_once(void);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);
 bool tcp_fastopen_defer_connect(struct sock *sk, int *err);

commit e1cfcbe82b4534bd0f99fef92a6d33843fd85e0e
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Wed Sep 27 11:35:40 2017 +0800

    ipv4: Namespaceify tcp_fastopen knob
    
    Different namespace application might require enable TCP Fast Open
    feature independently of the host.
    
    This patch series continues making more of the TCP Fast Open related
    sysctl knobs be per net-namespace.
    
    Reported-by: Luca BRUNO <lucab@debian.org>
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 770b608c8439..9e414a99034f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
 extern int sysctl_tcp_rfc1337;

commit 7487449c86c65202b3b725c4524cb48dd65e4e6f
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Thu Sep 28 15:51:36 2017 +0200

    IPv4: early demux can return an error code
    
    Currently no error is emitted, but this infrastructure will
    used by the next patch to allow source address validation
    for mcast sockets.
    Since early demux can do a route lookup and an ipv4 route
    lookup can return an error code this is consistent with the
    current ipv4 route infrastructure.
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3bc910a9bfc6..89974c5286d8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -345,7 +345,7 @@ void tcp_v4_err(struct sk_buff *skb, u32);
 
 void tcp_shutdown(struct sock *sk, int how);
 
-void tcp_v4_early_demux(struct sk_buff *skb);
+int tcp_v4_early_demux(struct sk_buff *skb);
 int tcp_v4_rcv(struct sk_buff *skb);
 
 int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);

commit 1f8d31d189cc6ce1e4b972959fda41e790bb92b8
Merge: 3fb5ec06578e cd4175b11685
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Sep 23 10:16:53 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit bffa72cf7f9df842f0016ba03586039296b4caaf
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 19 05:14:24 2017 -0700

    net: sk_buff rbnode reorg
    
    skb->rbnode shares space with skb->next, skb->prev and skb->tstamp
    
    Current uses (TCP receive ofo queue and netem) need to save/restore
    tstamp, while skb->dev is either NULL (TCP) or a constant for a given
    queue (netem).
    
    Since we plan using an RB tree for TCP retransmit queue to speedup SACK
    processing with large BDP, this patch exchanges skb->dev and
    skb->tstamp.
    
    This saves some overhead in both TCP and netem.
    
    v2: removes the swtstamp field from struct tcp_skb_cb
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Wei Wang <weiwan@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b510f284427a..49a8a46466f3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -797,12 +797,6 @@ struct tcp_skb_cb {
 			u16	tcp_gso_segs;
 			u16	tcp_gso_size;
 		};
-
-		/* Used to stash the receive timestamp while this skb is in the
-		 * out of order queue, as skb->tstamp is overwritten by the
-		 * rbnode.
-		 */
-		ktime_t		swtstamp;
 	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 

commit 4c7124413aa759b8ea0b90cd39177e525396e662
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 18 11:05:16 2017 -0700

    tcp: remove two unused functions
    
    remove tcp_may_send_now and tcp_snd_test that are no longer used
    
    Fixes: 840a3cbe8969 ("tcp: remove forward retransmit feature")
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b510f284427a..3bc910a9bfc6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -544,7 +544,6 @@ u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,
 		     int min_tso_segs);
 void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			       int nonagle);
-bool tcp_may_send_now(struct sock *sk);
 int __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
 int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
 void tcp_retransmit_timer(struct sock *sk);

commit 6026e043d09012c6269f9a96a808d52d9c498224
Merge: 4cc5b44b29a9 138e4ad67afd
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 1 17:42:05 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Three cases of simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 31770e34e43d6c8dee129bfee77e56c34e61f0e5
Author: Florian Westphal <fw@strlen.de>
Date:   Wed Aug 30 19:24:58 2017 +0200

    tcp: Revert "tcp: remove header prediction"
    
    This reverts commit 45f119bf936b1f9f546a0b139c5b56f9bb2bdc78.
    
    Eric Dumazet says:
      We found at Google a significant regression caused by
      45f119bf936b1f9f546a0b139c5b56f9bb2bdc78 tcp: remove header prediction
    
      In typical RPC  (TCP_RR), when a TCP socket receives data, we now call
      tcp_ack() while we used to not call it.
    
      This touches enough cache lines to cause a slowdown.
    
    so problem does not seem to be HP removal itself but the tcp_ack()
    call.  Therefore, it might be possible to remove HP after all, provided
    one finds a way to elide tcp_ack for most cases.
    
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c546d13ffbca..9c3db054e47f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -634,6 +634,29 @@ static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 	return usecs_to_jiffies((tp->srtt_us >> 3) + tp->rttvar_us);
 }
 
+static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
+{
+	tp->pred_flags = htonl((tp->tcp_header_len << 26) |
+			       ntohl(TCP_FLAG_ACK) |
+			       snd_wnd);
+}
+
+static inline void tcp_fast_path_on(struct tcp_sock *tp)
+{
+	__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);
+}
+
+static inline void tcp_fast_path_check(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (RB_EMPTY_ROOT(&tp->out_of_order_queue) &&
+	    tp->rcv_wnd &&
+	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
+	    !tp->urg_data)
+		tcp_fast_path_on(tp);
+}
+
 /* Compute the actual rto_min value */
 static inline u32 tcp_rto_min(struct sock *sk)
 {

commit c1d2b4c3e204e602c97680335d082b8d012d08cd
Author: Florian Westphal <fw@strlen.de>
Date:   Wed Aug 30 19:24:57 2017 +0200

    tcp: Revert "tcp: remove CA_ACK_SLOWPATH"
    
    This change was a followup to the header prediction removal,
    so first revert this as a prerequisite to back out hp removal.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c614ff135b66..c546d13ffbca 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -910,8 +910,9 @@ enum tcp_ca_event {
 
 /* Information about inbound ACK, passed to cong_ops->in_ack_event() */
 enum tcp_ca_ack_event_flags {
-	CA_ACK_WIN_UPDATE	= (1 << 0),	/* ACK updated window */
-	CA_ACK_ECE		= (1 << 1),	/* ECE bit is set on ack */
+	CA_ACK_SLOWPATH		= (1 << 0),	/* In slow path processing */
+	CA_ACK_WIN_UPDATE	= (1 << 1),	/* ACK updated window */
+	CA_ACK_ECE		= (1 << 2),	/* ECE bit is set on ack */
 };
 
 /*

commit ebfa00c5745660fe7f0a91eea88d4dff658486c4
Author: Sabrina Dubroca <sd@queasysnail.net>
Date:   Fri Aug 25 13:10:12 2017 +0200

    tcp: fix refcnt leak with ebpf congestion control
    
    There are a few bugs around refcnt handling in the new BPF congestion
    control setsockopt:
    
     - The new ca is assigned to icsk->icsk_ca_ops even in the case where we
       cannot get a reference on it. This would lead to a use after free,
       since that ca is going away soon.
    
     - Changing the congestion control case doesn't release the refcnt on
       the previous ca.
    
     - In the reinit case, we first leak a reference on the old ca, then we
       call tcp_reinit_congestion_control on the ca that we have just
       assigned, leading to deinitializing the wrong ca (->release of the
       new ca on the old ca's data) and releasing the refcount on the ca
       that we actually want to use.
    
    This is visible by building (for example) BIC as a module and setting
    net.ipv4.tcp_congestion_control=bic, and using tcp_cong_kern.c from
    samples/bpf.
    
    This patch fixes the refcount issues, and moves reinit back into tcp
    core to avoid passing a ca pointer back to BPF.
    
    Fixes: 91b5b21c7c16 ("bpf: Add support for changing congestion control")
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Acked-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ada65e767b28..f642a39f9eee 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1004,9 +1004,7 @@ void tcp_get_default_congestion_control(char *name);
 void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
-int tcp_set_congestion_control(struct sock *sk, const char *name, bool load);
-void tcp_reinit_congestion_control(struct sock *sk,
-				   const struct tcp_congestion_ops *ca);
+int tcp_set_congestion_control(struct sock *sk, const char *name, bool load, bool reinit);
 u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 

commit 98aaa913b4ed250324429f0a9e6d5f77a3b5276c
Author: Mike Maloney <maloney@google.com>
Date:   Tue Aug 22 17:08:48 2017 -0400

    tcp: Extend SOF_TIMESTAMPING_RX_SOFTWARE to TCP recvmsg
    
    When SOF_TIMESTAMPING_RX_SOFTWARE is enabled for tcp sockets, return the
    timestamp corresponding to the highest sequence number data returned.
    
    Previously the skb->tstamp is overwritten when a TCP packet is placed
    in the out of order queue.  While the packet is in the ooo queue, save the
    timestamp in the TCB_SKB_CB.  This space is shared with the gso_*
    options which are only used on the tx path, and a previously unused 4
    byte hole.
    
    When skbs are coalesced either in the sk_receive_queue or the
    out_of_order_queue always choose the timestamp of the appended skb to
    maintain the invariant of returning the timestamp of the last byte in
    the recvmsg buffer.
    
    Signed-off-by: Mike Maloney <maloney@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a995004ae946..c614ff135b66 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -774,6 +774,12 @@ struct tcp_skb_cb {
 			u16	tcp_gso_segs;
 			u16	tcp_gso_size;
 		};
+
+		/* Used to stash the receive timestamp while this skb is in the
+		 * out of order queue, as skb->tstamp is overwritten by the
+		 * rbnode.
+		 */
+		ktime_t		swtstamp;
 	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
@@ -790,7 +796,8 @@ struct tcp_skb_cb {
 	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
 	__u8		txstamp_ack:1,	/* Record TX timestamp for ack? */
 			eor:1,		/* Is skb MSG_EOR marked? */
-			unused:6;
+			has_rxtstamp:1,	/* SKB has a RX timestamp	*/
+			unused:5;
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 	union {
 		struct {

commit 111993692741a7044e6c01b428cecf1071de3d0b
Author: Tonghao Zhang <xiangxia.m.yue@gmail.com>
Date:   Mon Aug 21 23:33:49 2017 -0700

    tcp: Remove the unused parameter for tcp_try_fastopen.
    
    Signed-off-by: Tonghao Zhang <xiangxia.m.yue@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index afdab3781425..a995004ae946 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1533,8 +1533,7 @@ int tcp_fastopen_reset_cipher(void *key, unsigned int len);
 void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
-			      struct tcp_fastopen_cookie *foc,
-			      struct dst_entry *dst);
+			      struct tcp_fastopen_cookie *foc);
 void tcp_fastopen_init_key_once(bool publish);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);

commit 3118e6e19da7b8d76b2456b880c74a9aa3a2268b
Merge: feca7d8c135b 48fb6f4db940
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Aug 9 16:28:45 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The UDP offload conflict is dealt with by simply taking what is
    in net-next where we have removed all of the UFO handling code
    entirely.
    
    The TCP conflict was a case of local variables in a function
    being removed from both net and net-next.
    
    In netvsc we had an assignment right next to where a missing
    set of u64 stats sync object inits were added.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4297a0ef085729af98adab9131d128c576ed3044
Author: David Ahern <dsahern@gmail.com>
Date:   Mon Aug 7 08:44:21 2017 -0700

    net: ipv6: add second dif to inet6 socket lookups
    
    Add a second device index, sdif, to inet6 socket lookups. sdif is the
    index for ingress devices enslaved to an l3mdev. It allows the lookups
    to consider the enslaved device as well as the L3 domain when searching
    for a socket.
    
    TCP moves the data in the cb. Prior to tcp_v4_rcv (e.g., early demux) the
    ingress index is obtained from IPCB using inet_sdif and after tcp_v4_rcv
    tcp_v4_sdif is used.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2b89f1ab8552..999f3efe572b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -827,6 +827,16 @@ static inline int tcp_v6_iif(const struct sk_buff *skb)
 
 	return l3_slave ? skb->skb_iif : TCP_SKB_CB(skb)->header.h6.iif;
 }
+
+/* TCP_SKB_CB reference means this can not be used from early demux */
+static inline int tcp_v6_sdif(const struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)
+	if (skb && ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags))
+		return TCP_SKB_CB(skb)->header.h6.iif;
+#endif
+	return 0;
+}
 #endif
 
 /* TCP_SKB_CB reference means this can not be used from early demux */

commit 3fa6f616a7a4d0bdf4d877d530456d8a5c3b109b
Author: David Ahern <dsahern@gmail.com>
Date:   Mon Aug 7 08:44:17 2017 -0700

    net: ipv4: add second dif to inet socket lookups
    
    Add a second device index, sdif, to inet socket lookups. sdif is the
    index for ingress devices enslaved to an l3mdev. It allows the lookups
    to consider the enslaved device as well as the L3 domain when searching
    for a socket.
    
    TCP moves the data in the cb. Prior to tcp_v4_rcv (e.g., early demux) the
    ingress index is obtained from IPCB using inet_sdif and after the cb move
    in  tcp_v4_rcv the tcp_v4_sdif helper is used.
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5173fecde495..2b89f1ab8552 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -840,6 +840,16 @@ static inline bool inet_exact_dif_match(struct net *net, struct sk_buff *skb)
 	return false;
 }
 
+/* TCP_SKB_CB reference means this can not be used from early demux */
+static inline int tcp_v4_sdif(struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)
+	if (skb && ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))
+		return TCP_SKB_CB(skb)->header.h4.iif;
+#endif
+	return 0;
+}
+
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */

commit 91ed1e666a4ea2e260452a7d7d311ac5ae852cba
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Thu Aug 3 18:07:06 2017 +0200

    ip/options: explicitly provide net ns to __ip_options_echo()
    
    __ip_options_echo() uses the current network namespace, and
    currently retrives it via skb->dst->dev.
    
    This commit adds an explicit 'net' argument to __ip_options_echo()
    and update all the call sites to provide it, usually via a simpler
    sock_net().
    
    After this change, __ip_options_echo() no more needs to access
    skb->dst and we can drop a couple of hack to preserve such
    info in the rx path.
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bb1881b4ce48..5173fecde495 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1885,7 +1885,8 @@ extern void tcp_rack_reo_timeout(struct sock *sk);
 /*
  * Save and compile IPv4 options, return a pointer to it
  */
-static inline struct ip_options_rcu *tcp_v4_save_options(struct sk_buff *skb)
+static inline struct ip_options_rcu *tcp_v4_save_options(struct net *net,
+							 struct sk_buff *skb)
 {
 	const struct ip_options *opt = &TCP_SKB_CB(skb)->header.h4.opt;
 	struct ip_options_rcu *dopt = NULL;
@@ -1894,7 +1895,7 @@ static inline struct ip_options_rcu *tcp_v4_save_options(struct sk_buff *skb)
 		int opt_size = sizeof(*dopt) + opt->optlen;
 
 		dopt = kmalloc(opt_size, GFP_ATOMIC);
-		if (dopt && __ip_options_echo(&dopt->opt, skb, opt)) {
+		if (dopt && __ip_options_echo(net, &dopt->opt, skb, opt)) {
 			kfree(dopt);
 			dopt = NULL;
 		}

commit e1a10ef7fa876f8510aaec36ea5c0cf34baba410
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu Aug 3 09:19:52 2017 -0400

    tcp: introduce tcp_rto_delta_us() helper for xmit timer fix
    
    Pure refactor. This helper will be required in the xmit timer fix
    later in the patch series. (Because the TLP logic will want to make
    this calculation.)
    
    Fixes: 6ba8a3b19e76 ("tcp: Tail loss probe (TLP)")
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 70483296157f..ada65e767b28 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1916,6 +1916,16 @@ extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     u64 xmit_time);
 extern void tcp_rack_reo_timeout(struct sock *sk);
 
+/* At how many usecs into the future should the RTO fire? */
+static inline s64 tcp_rto_delta_us(const struct sock *sk)
+{
+	const struct sk_buff *skb = tcp_write_queue_head(sk);
+	u32 rto = inet_csk(sk)->icsk_rto;
+	u64 rto_time_stamp_us = skb->skb_mstamp + jiffies_to_usecs(rto);
+
+	return rto_time_stamp_us - tcp_sk(sk)->tcp_mstamp;
+}
+
 /*
  * Save and compile IPv4 options, return a pointer to it
  */

commit 306b13eb3cf9515a8214bbf5d69d811371d05792
Author: Tom Herbert <tom@quantonium.net>
Date:   Fri Jul 28 16:22:41 2017 -0700

    proto_ops: Add locked held versions of sendmsg and sendpage
    
    Add new proto_ops sendmsg_locked and sendpage_locked that can be
    called when the socket lock is already held. Correspondingly, add
    kernel_sendmsg_locked and kernel_sendpage_locked as front end
    functions.
    
    These functions will be used in zero proxy so that we can take
    the socket lock in a ULP sendmsg/sendpage and then directly call the
    backend transport proto_ops functions.
    
    Signed-off-by: Tom Herbert <tom@quantonium.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3ecb62811004..bb1881b4ce48 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -350,8 +350,11 @@ int tcp_v4_rcv(struct sk_buff *skb);
 
 int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
+int tcp_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t size);
 int tcp_sendpage(struct sock *sk, struct page *page, int offset, size_t size,
 		 int flags);
+int tcp_sendpage_locked(struct sock *sk, struct page *page, int offset,
+			size_t size, int flags);
 ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,
 		 size_t size, int flags);
 void tcp_release_cb(struct sock *sk);

commit 573aeb0492be3d0e5be9796a0c91abde794c1e36
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:22 2017 +0200

    tcp: remove CA_ACK_SLOWPATH
    
    re-indent tcp_ack, and remove CA_ACK_SLOWPATH; it is always set now.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8f11b82b5b5a..3ecb62811004 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -880,9 +880,8 @@ enum tcp_ca_event {
 
 /* Information about inbound ACK, passed to cong_ops->in_ack_event() */
 enum tcp_ca_ack_event_flags {
-	CA_ACK_SLOWPATH		= (1 << 0),	/* In slow path processing */
-	CA_ACK_WIN_UPDATE	= (1 << 1),	/* ACK updated window */
-	CA_ACK_ECE		= (1 << 2),	/* ECE bit is set on ack */
+	CA_ACK_WIN_UPDATE	= (1 << 0),	/* ACK updated window */
+	CA_ACK_ECE		= (1 << 1),	/* ECE bit is set on ack */
 };
 
 /*

commit 45f119bf936b1f9f546a0b139c5b56f9bb2bdc78
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:21 2017 +0200

    tcp: remove header prediction
    
    Like prequeue, I am not sure this is overly useful nowadays.
    
    If we receive a train of packets, GRO will aggregate them if the
    headers are the same (HP predates GRO by several years) so we don't
    get a per-packet benefit, only a per-aggregated-packet one.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8507c81fb0e9..8f11b82b5b5a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -631,29 +631,6 @@ static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 	return usecs_to_jiffies((tp->srtt_us >> 3) + tp->rttvar_us);
 }
 
-static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
-{
-	tp->pred_flags = htonl((tp->tcp_header_len << 26) |
-			       ntohl(TCP_FLAG_ACK) |
-			       snd_wnd);
-}
-
-static inline void tcp_fast_path_on(struct tcp_sock *tp)
-{
-	__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);
-}
-
-static inline void tcp_fast_path_check(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	if (RB_EMPTY_ROOT(&tp->out_of_order_queue) &&
-	    tp->rcv_wnd &&
-	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
-	    !tp->urg_data)
-		tcp_fast_path_on(tp);
-}
-
 /* Compute the actual rto_min value */
 static inline u32 tcp_rto_min(struct sock *sk)
 {

commit b6690b14386698ce2c19309abad3f17656bdfaea
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:20 2017 +0200

    tcp: remove low_latency sysctl
    
    Was only checked by the removed prequeue code.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 93f115cfc8f8..8507c81fb0e9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -256,7 +256,6 @@ extern int sysctl_tcp_rmem[3];
 extern int sysctl_tcp_app_win;
 extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_frto;
-extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;

commit e7942d0633c47c791ece6afa038be9cf977226de
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Jul 30 03:57:18 2017 +0200

    tcp: remove prequeue support
    
    prequeue is a tcp receive optimization that moves part of rx processing
    from bh to process context.
    
    This only works if the socket being processed belongs to a process that
    is blocked in recv on that socket.
    
    In practice, this doesn't happen anymore that often because nowadays
    servers tend to use an event driven (epoll) model.
    
    Even normal client applications (web browsers) commonly use many tcp
    connections in parallel.
    
    This has measureable impact only in netperf (which uses plain recv and
    thus allows prequeue use) from host to locally running vm (~4%), however,
    there were no changes when using netperf between two physical hosts with
    ixgbe interfaces.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 12d68335acd4..93f115cfc8f8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1244,17 +1244,6 @@ static inline bool tcp_checksum_complete(struct sk_buff *skb)
 		__tcp_checksum_complete(skb);
 }
 
-/* Prequeue for VJ style copy to user, combined with checksumming. */
-
-static inline void tcp_prequeue_init(struct tcp_sock *tp)
-{
-	tp->ucopy.task = NULL;
-	tp->ucopy.len = 0;
-	tp->ucopy.memory = 0;
-	skb_queue_head_init(&tp->ucopy.prequeue);
-}
-
-bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
 bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);
 int tcp_filter(struct sock *sk, struct sk_buff *skb);
 

commit e42e24c3cc072088756d84ef07b492ac2a3ae2e5
Author: Matvejchikov Ilya <matvejchikov@gmail.com>
Date:   Mon Jul 24 16:02:12 2017 +0400

    tcp: remove redundant argument from tcp_rcv_established()
    
    The last (4th) argument of tcp_rcv_established() is redundant as it
    always equals to skb->len and the skb itself is always passed as 2th
    agrument. There is no reason to have it.
    
    Signed-off-by: Ilya V. Matveychikov <matvejchikov@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4f056ea79df2..12d68335acd4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -362,7 +362,7 @@ void tcp_delack_timer_handler(struct sock *sk);
 int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb);
 void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-			 const struct tcphdr *th, unsigned int len);
+			 const struct tcphdr *th);
 void tcp_rcv_space_adjust(struct sock *sk);
 int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
 void tcp_twsk_destructor(struct sock *sk);

commit bb4d991a28cc86a2dfbeefeff32911ca9f779c18
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Jul 19 15:41:26 2017 -0700

    tcp: adjust tail loss probe timeout
    
    This patch adjusts the timeout formula to schedule the TCP loss probe
    (TLP). The previous formula uses 2*SRTT or 1.5*RTT + DelayACKMax if
    only one packet is in flight. It keeps a lower bound of 10 msec which
    is too large for short RTT connections (e.g. within a data-center).
    The new formula = 2*RTT + (inflight == 1 ? 200ms : 2ticks) which
    performs better for short and fast connections.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 70483296157f..4f056ea79df2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -139,6 +139,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #endif
 #define TCP_RTO_MAX	((unsigned)(120*HZ))
 #define TCP_RTO_MIN	((unsigned)(HZ/5))
+#define TCP_TIMEOUT_MIN	(2U) /* Min timeout for TCP timers in jiffies */
 #define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))	/* RFC6298 2.1 initial RTO value	*/
 #define TCP_TIMEOUT_FALLBACK ((unsigned)(3*HZ))	/* RFC 1122 initial RTO value, now
 						 * used as a fallback RTO for the
@@ -150,8 +151,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U)) /* Maximal interval between probes
 					                 * for local resources.
 					                 */
-#define TCP_REO_TIMEOUT_MIN	(2000) /* Min RACK reordering timeout in usec */
-
 #define TCP_KEEPALIVE_TIME	(120*60*HZ)	/* two hours */
 #define TCP_KEEPALIVE_PROBES	9		/* Max of 9 keepalive probes	*/
 #define TCP_KEEPALIVE_INTVL	(75*HZ)

commit 91b5b21c7c16899abb37f4a9e4388b4e9aae0b9d
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:49 2017 -0700

    bpf: Add support for changing congestion control
    
    Added support for changing congestion control for SOCK_OPS bpf
    programs through the setsockopt bpf helper function. It also adds
    a new SOCK_OPS op, BPF_SOCK_OPS_NEEDS_ECN, that is needed for
    congestion controls, like dctcp, that need to enable ECN in the
    SYN packets.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d6bb3948203d..70483296157f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1004,7 +1004,9 @@ void tcp_get_default_congestion_control(char *name);
 void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
-int tcp_set_congestion_control(struct sock *sk, const char *name);
+int tcp_set_congestion_control(struct sock *sk, const char *name, bool load);
+void tcp_reinit_congestion_control(struct sock *sk,
+				   const struct tcp_congestion_ops *ca);
 u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 
@@ -2078,4 +2080,9 @@ static inline u32 tcp_rwnd_init_bpf(struct sock *sk)
 		rwnd = 0;
 	return rwnd;
 }
+
+static inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)
+{
+	return (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN) == 1);
+}
 #endif	/* _TCP_H */

commit 13d3b1ebe28762c79e981931a41914fae5d04386
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:44 2017 -0700

    bpf: Support for setting initial receive window
    
    This patch adds suppport for setting the initial advertized window from
    within a BPF_SOCK_OPS program. This can be used to support larger
    initial cwnd values in environments where it is known to be safe.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 564af2dee236..d6bb3948203d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2068,4 +2068,14 @@ static inline u32 tcp_timeout_init(struct sock *sk)
 	return timeout;
 }
 
+static inline u32 tcp_rwnd_init_bpf(struct sock *sk)
+{
+	int rwnd;
+
+	rwnd = tcp_call_bpf(sk, BPF_SOCK_OPS_RWND_INIT);
+
+	if (rwnd < 0)
+		rwnd = 0;
+	return rwnd;
+}
 #endif	/* _TCP_H */

commit 8550f328f45db6d37981eb2041bc465810245c03
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:42 2017 -0700

    bpf: Support for per connection SYN/SYN-ACK RTOs
    
    This patch adds support for setting a per connection SYN and
    SYN_ACK RTOs from within a BPF_SOCK_OPS program. For example,
    to set small RTOs when it is known both hosts are within a
    datacenter.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e58500825006..564af2dee236 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2057,4 +2057,15 @@ static inline int tcp_call_bpf(struct sock *sk, int op)
 }
 #endif
 
+static inline u32 tcp_timeout_init(struct sock *sk)
+{
+	int timeout;
+
+	timeout = tcp_call_bpf(sk, BPF_SOCK_OPS_TIMEOUT_INIT);
+
+	if (timeout <= 0)
+		timeout = TCP_TIMEOUT_INIT;
+	return timeout;
+}
+
 #endif	/* _TCP_H */

commit 40304b2a1567fecc321f640ee4239556dd0f3ee0
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:40 2017 -0700

    bpf: BPF support for sock_ops
    
    Created a new BPF program type, BPF_PROG_TYPE_SOCK_OPS, and a corresponding
    struct that allows BPF programs of this type to access some of the
    socket's fields (such as IP addresses, ports, etc.). It uses the
    existing bpf cgroups infrastructure so the programs can be attached per
    cgroup with full inheritance support. The program will be called at
    appropriate times to set relevant connections parameters such as buffer
    sizes, SYN and SYN-ACK RTOs, etc., based on connection information such
    as IP addresses, port numbers, etc.
    
    Alghough there are already 3 mechanisms to set parameters (sysctls,
    route metrics and setsockopts), this new mechanism provides some
    distinct advantages. Unlike sysctls, it can set parameters per
    connection. In contrast to route metrics, it can also use port numbers
    and information provided by a user level program. In addition, it could
    set parameters probabilistically for evaluation purposes (i.e. do
    something different on 10% of the flows and compare results with the
    other 90% of the flows). Also, in cases where IPv6 addresses contain
    geographic information, the rules to make changes based on the distance
    (or RTT) between the hosts are much easier than route metric rules and
    can be global. Finally, unlike setsockopt, it oes not require
    application changes and it can be updated easily at any time.
    
    Although the bpf cgroup framework already contains a sock related
    program type (BPF_PROG_TYPE_CGROUP_SOCK), I created the new type
    (BPF_PROG_TYPE_SOCK_OPS) beccause the existing type expects to be called
    only once during the connections's lifetime. In contrast, the new
    program type will be called multiple times from different places in the
    network stack code.  For example, before sending SYN and SYN-ACKs to set
    an appropriate timeout, when the connection is established to set
    congestion control, etc. As a result it has "op" field to specify the
    type of operation requested.
    
    The purpose of this new program type is to simplify setting connection
    parameters, such as buffer sizes, TCP's SYN RTO, etc. For example, it is
    easy to use facebook's internal IPv6 addresses to determine if both hosts
    of a connection are in the same datacenter. Therefore, it is easy to
    write a BPF program to choose a small SYN RTO value when both hosts are
    in the same datacenter.
    
    This patch only contains the framework to support the new BPF program
    type, following patches add the functionality to set various connection
    parameters.
    
    This patch defines a new BPF program type: BPF_PROG_TYPE_SOCKET_OPS
    and a new bpf syscall command to load a new program of this type:
    BPF_PROG_LOAD_SOCKET_OPS.
    
    Two new corresponding structs (one for the kernel one for the user/BPF
    program):
    
    /* kernel version */
    struct bpf_sock_ops_kern {
            struct sock *sk;
            __u32  op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
    };
    
    /* user version
     * Some fields are in network byte order reflecting the sock struct
     * Use the bpf_ntohl helper macro in samples/bpf/bpf_endian.h to
     * convert them to host byte order.
     */
    struct bpf_sock_ops {
            __u32 op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
            __u32 family;
            __u32 remote_ip4;     /* In network byte order */
            __u32 local_ip4;      /* In network byte order */
            __u32 remote_ip6[4];  /* In network byte order */
            __u32 local_ip6[4];   /* In network byte order */
            __u32 remote_port;    /* In network byte order */
            __u32 local_port;     /* In host byte horder */
    };
    
    Currently there are two types of ops. The first type expects the BPF
    program to return a value which is then used by the caller (or a
    negative value to indicate the operation is not supported). The second
    type expects state changes to be done by the BPF program, for example
    through a setsockopt BPF helper function, and they ignore the return
    value.
    
    The reply fields of the bpf_sockt_ops struct are there in case a bpf
    program needs to return a value larger than an integer.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d0751b79d99c..e58500825006 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -46,6 +46,10 @@
 #include <linux/seq_file.h>
 #include <linux/memcontrol.h>
 
+#include <linux/bpf.h>
+#include <linux/filter.h>
+#include <linux/bpf-cgroup.h>
+
 extern struct inet_hashinfo tcp_hashinfo;
 
 extern struct percpu_counter tcp_orphan_count;
@@ -2021,4 +2025,36 @@ int tcp_set_ulp(struct sock *sk, const char *name);
 void tcp_get_available_ulp(char *buf, size_t len);
 void tcp_cleanup_ulp(struct sock *sk);
 
+/* Call BPF_SOCK_OPS program that returns an int. If the return value
+ * is < 0, then the BPF op failed (for example if the loaded BPF
+ * program does not support the chosen operation or there is no BPF
+ * program loaded).
+ */
+#ifdef CONFIG_BPF
+static inline int tcp_call_bpf(struct sock *sk, int op)
+{
+	struct bpf_sock_ops_kern sock_ops;
+	int ret;
+
+	if (sk_fullsock(sk))
+		sock_owned_by_me(sk);
+
+	memset(&sock_ops, 0, sizeof(sock_ops));
+	sock_ops.sk = sk;
+	sock_ops.op = op;
+
+	ret = BPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);
+	if (ret == 0)
+		ret = sock_ops.reply;
+	else
+		ret = -1;
+	return ret;
+}
+#else
+static inline int tcp_call_bpf(struct sock *sk, int op)
+{
+	return -EPERM;
+}
+#endif
+
 #endif	/* _TCP_H */

commit 8917a777be3ba566377be05117f71b93a5fd909d
Author: Ivan Delalande <colona@arista.com>
Date:   Thu Jun 15 18:07:07 2017 -0700

    tcp: md5: add TCP_MD5SIG_EXT socket option to set a key address prefix
    
    Replace first padding in the tcp_md5sig structure with a new flag field
    and address prefix length so it can be specified when configuring a new
    key for TCP MD5 signature. The tcpm_flags field will only be used if the
    socket option is TCP_MD5SIG_EXT to avoid breaking existing programs, and
    tcpm_prefixlen only when the TCP_MD5SIG_FLAG_PREFIX flag is set.
    
    Signed-off-by: Bob Gilligan <gilligan@arista.com>
    Signed-off-by: Eric Mowat <mowat@arista.com>
    Signed-off-by: Ivan Delalande <colona@arista.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8f4076d31669..d0751b79d99c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1848,6 +1848,7 @@ struct tcp_sock_af_ops {
 					 const struct sock *sk,
 					 const struct sk_buff *skb);
 	int		(*md5_parse)(struct sock *sk,
+				     int optname,
 				     char __user *optval,
 				     int optlen);
 #endif

commit 6797318e623da68dfbacd0cb5c246f5ecd2baf6e
Author: Ivan Delalande <colona@arista.com>
Date:   Thu Jun 15 18:07:06 2017 -0700

    tcp: md5: add an address prefix for key lookup
    
    This allows the keys used for TCP MD5 signature to be used for whole
    range of addresses, specified with a prefix length, instead of only one
    address as it currently is.
    
    Signed-off-by: Bob Gilligan <gilligan@arista.com>
    Signed-off-by: Eric Mowat <mowat@arista.com>
    Signed-off-by: Ivan Delalande <colona@arista.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e17ec286e8df..8f4076d31669 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1441,6 +1441,7 @@ struct tcp_md5sig_key {
 	u8			keylen;
 	u8			family; /* AF_INET or AF_INET6 */
 	union tcp_md5_addr	addr;
+	u8			prefixlen;
 	u8			key[TCP_MD5SIG_MAXKEYLEN];
 	struct rcu_head		rcu;
 };
@@ -1484,9 +1485,10 @@ struct tcp_md5sig_pool {
 int tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,
 			const struct sock *sk, const struct sk_buff *skb);
 int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
-		   int family, const u8 *newkey, u8 newkeylen, gfp_t gfp);
+		   int family, u8 prefixlen, const u8 *newkey, u8 newkeylen,
+		   gfp_t gfp);
 int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
-		   int family);
+		   int family, u8 prefixlen);
 struct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,
 					 const struct sock *addr_sk);
 

commit e3b5616a347603a521fe3ac46f3194a60900e3a7
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jun 14 11:37:26 2017 -0700

    tcp: export do_tcp_sendpages and tcp_rate_check_app_limited functions
    
    Export do_tcp_sendpages and tcp_rate_check_app_limited, since tls will need to
    sendpages while the socket is already locked.
    
    tcp_sendpage is exported, but requires the socket lock to not be held already.
    
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b439f46f149c..e17ec286e8df 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -350,6 +350,8 @@ int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tcp_sendpage(struct sock *sk, struct page *page, int offset, size_t size,
 		 int flags);
+ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,
+		 size_t size, int flags);
 void tcp_release_cb(struct sock *sk);
 void tcp_wfree(struct sk_buff *skb);
 void tcp_write_timer_handler(struct sock *sk);

commit 734942cc4ea6478eed125af258da1bdbb4afe578
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jun 14 11:37:14 2017 -0700

    tcp: ULP infrastructure
    
    Add the infrustructure for attaching Upper Layer Protocols (ULPs) over TCP
    sockets. Based on a similar infrastructure in tcp_cong.  The idea is that any
    ULP can add its own logic by changing the TCP proto_ops structure to its own
    methods.
    
    Example usage:
    
    setsockopt(sock, SOL_TCP, TCP_ULP, "tls", sizeof("tls"));
    
    modules will call:
    tcp_register_ulp(&tcp_tls_ulp_ops);
    
    to register/unregister their ulp, with an init function and name.
    
    A list of registered ulps will be returned by tcp_get_available_ulp, which is
    hooked up to /proc.  Example:
    
    $ cat /proc/sys/net/ipv4/tcp_available_ulp
    tls
    
    There is currently no functionality to remove or chain ULPs, but
    it should be possible to add these in the future if needed.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3ab677d11d02..b439f46f149c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1991,4 +1991,29 @@ static inline void tcp_listendrop(const struct sock *sk)
 
 enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);
 
+/*
+ * Interface for adding Upper Level Protocols over TCP
+ */
+
+#define TCP_ULP_NAME_MAX	16
+#define TCP_ULP_MAX		128
+#define TCP_ULP_BUF_MAX		(TCP_ULP_NAME_MAX*TCP_ULP_MAX)
+
+struct tcp_ulp_ops {
+	struct list_head	list;
+
+	/* initialize ulp */
+	int (*init)(struct sock *sk);
+	/* cleanup ulp */
+	void (*release)(struct sock *sk);
+
+	char		name[TCP_ULP_NAME_MAX];
+	struct module	*owner;
+};
+int tcp_register_ulp(struct tcp_ulp_ops *type);
+void tcp_unregister_ulp(struct tcp_ulp_ops *type);
+int tcp_set_ulp(struct sock *sk, const char *name);
+void tcp_get_available_ulp(char *buf, size_t len);
+void tcp_cleanup_ulp(struct sock *sk);
+
 #endif	/* _TCP_H */

commit 0604475119de5f80dc051a5db055c6a2a75bd542
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 13:29:12 2017 -0700

    tcp: add TCPMemoryPressuresChrono counter
    
    DRAM supply shortage and poor memory pressure tracking in TCP
    stack makes any change in SO_SNDBUF/SO_RCVBUF (or equivalent autotuning
    limits) and tcp_mem[] quite hazardous.
    
    TCPMemoryPressures SNMP counter is an indication of tcp_mem sysctl
    limits being hit, but only tracking number of transitions.
    
    If TCP stack behavior under stress was perfect :
    1) It would maintain memory usage close to the limit.
    2) Memory pressure state would be entered for short times.
    
    We certainly prefer 100 events lasting 10ms compared to one event
    lasting 200 seconds.
    
    This patch adds a new SNMP counter tracking cumulative duration of
    memory pressure events, given in ms units.
    
    $ cat /proc/sys/net/ipv4/tcp_mem
    3088    4117    6176
    $ grep TCP /proc/net/sockstat
    TCP: inuse 180 orphan 0 tw 2 alloc 234 mem 4140
    $ nstat -n ; sleep 10 ; nstat |grep Pressure
    TcpExtTCPMemoryPressures        1700
    TcpExtTCPMemoryPressuresChrono  5209
    
    v2: Used EXPORT_SYMBOL_GPL() instead of EXPORT_SYMBOL() as David
    instructed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index aec092560d9b..3ab677d11d02 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -276,7 +276,7 @@ extern int sysctl_tcp_pacing_ca_ratio;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
-extern int tcp_memory_pressure;
+extern unsigned long tcp_memory_pressure;
 
 /* optimized version of sk_under_memory_pressure() for TCP sockets */
 static inline bool tcp_under_memory_pressure(const struct sock *sk)
@@ -1320,6 +1320,7 @@ extern void tcp_openreq_init_rwin(struct request_sock *req,
 				  const struct dst_entry *dst);
 
 void tcp_enter_memory_pressure(struct sock *sk);
+void tcp_leave_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 {

commit 5d2ed0521ac98f1ae0243fe52b8ebf95e2abf791
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 10:34:39 2017 -0700

    tcp: Namespaceify sysctl_tcp_timestamps
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f41ed5bac493..aec092560d9b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -237,7 +237,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 
 /* sysctl variables for tcp */
-extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
@@ -1869,7 +1868,7 @@ struct tcp_request_sock_ops {
 	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req);
 	u32 (*init_seq)(const struct sk_buff *skb);
-	u32 (*init_ts_off)(const struct sk_buff *skb);
+	u32 (*init_ts_off)(const struct net *net, const struct sk_buff *skb);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   struct tcp_fastopen_cookie *foc,

commit 9bb37ef00e932eb4b989e855245468feb3980700
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 10:34:38 2017 -0700

    tcp: Namespaceify sysctl_tcp_window_scaling
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f9d2ce0ba676..f41ed5bac493 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -238,7 +238,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_timestamps;
-extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;

commit f930103421f6579719b8252285c94c1195f6e032
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 10:34:37 2017 -0700

    tcp: Namespaceify sysctl_tcp_sack
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0b0cfeefa05b..f9d2ce0ba676 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -239,7 +239,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* sysctl variables for tcp */
 extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
-extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
@@ -520,7 +519,8 @@ u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mss);
 u64 cookie_init_timestamp(struct request_sock *req);
-bool cookie_timestamp_decode(struct tcp_options_received *opt);
+bool cookie_timestamp_decode(const struct net *net,
+			     struct tcp_options_received *opt);
 bool cookie_ecn_ok(const struct tcp_options_received *opt,
 		   const struct net *net, const struct dst_entry *dst);
 

commit eed29f17f09ad7f400bc245f209acad6a8214fac
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 10:34:36 2017 -0700

    tcp: add a struct net parameter to tcp_parse_options()
    
    We want to move some TCP sysctls to net namespaces in the future.
    
    tcp_window_scaling, tcp_sack and tcp_timestamps being fetched
    from tcp_parse_options(), we need to pass an extra parameter.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 28b577a35786..0b0cfeefa05b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -427,7 +427,7 @@ void tcp_set_keepalive(struct sock *sk, int val);
 void tcp_syn_ack_timeout(const struct request_sock *req);
 int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
-void tcp_parse_options(const struct sk_buff *skb,
+void tcp_parse_options(const struct net *net, const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);
 const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);

commit 216fe8f021e33c36e3b27c49c9f1951f6b037d7f
Merge: 9747e2313838 b29794ec95c6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 6 22:20:08 2017 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Just some simple overlapping changes in marvell PHY driver
    and the DSA core code.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1e0ce2a1ee0d5fb334e82e80600f1e8e77df525c
Author: Anmol Sarma <me@anmolsarma.in>
Date:   Sat Jun 3 17:40:54 2017 +0530

    net: Update TCP congestion control documentation
    
    Update tcp.txt to fix mandatory congestion control ops and default
    CCA selection. Also, fix comment in tcp.h for undo_cwnd.
    
    Signed-off-by: Anmol Sarma <me@anmolsarma.in>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 38a7427ae902..be6223c586fa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -924,7 +924,7 @@ struct tcp_congestion_ops {
 	void (*cwnd_event)(struct sock *sk, enum tcp_ca_event ev);
 	/* call when ack arrives (optional) */
 	void (*in_ack_event)(struct sock *sk, u32 flags);
-	/* new value of cwnd after loss (optional) */
+	/* new value of cwnd after loss (required) */
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);

commit 9a568de4818dea9a05af141046bd3e589245ab83
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 14:00:14 2017 -0700

    tcp: switch TCP TS option (RFC 7323) to 1ms clock
    
    TCP Timestamps option is defined in RFC 7323
    
    Traditionally on linux, it has been tied to the internal
    'jiffies' variable, because it had been a cheap and good enough
    generator.
    
    For TCP flows on the Internet, 1 ms resolution would be much better
    than 4ms or 10ms (HZ=250 or HZ=100 respectively)
    
    For TCP flows in the DC, Google has used usec resolution for more
    than two years with great success [1]
    
    Receive size autotuning (DRS) is indeed more precise and converges
    faster to optimal window size.
    
    This patch converts tp->tcp_mstamp to a plain u64 value storing
    a 1 usec TCP clock.
    
    This choice will allow us to upstream the 1 usec TS option as
    discussed in IETF 97.
    
    [1] https://www.ietf.org/proceedings/97/slides/slides-97-tcpm-tcp-options-for-low-latency-00.pdf
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5b2932b8363f..82462db97183 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -519,7 +519,7 @@ static inline u32 tcp_cookie_time(void)
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mss);
-__u32 cookie_init_timestamp(struct request_sock *req);
+u64 cookie_init_timestamp(struct request_sock *req);
 bool cookie_timestamp_decode(struct tcp_options_received *opt);
 bool cookie_ecn_ok(const struct tcp_options_received *opt,
 		   const struct net *net, const struct dst_entry *dst);
@@ -706,14 +706,55 @@ void tcp_send_window_probe(struct sock *sk);
  */
 #define tcp_jiffies32 ((u32)jiffies)
 
-/* Generator for TCP TS option (RFC 7323)
- * Currently tied to 'jiffies' but will soon be driven by 1 ms clock.
+/*
+ * Deliver a 32bit value for TCP timestamp option (RFC 7323)
+ * It is no longer tied to jiffies, but to 1 ms clock.
+ * Note: double check if you want to use tcp_jiffies32 instead of this.
+ */
+#define TCP_TS_HZ	1000
+
+static inline u64 tcp_clock_ns(void)
+{
+	return local_clock();
+}
+
+static inline u64 tcp_clock_us(void)
+{
+	return div_u64(tcp_clock_ns(), NSEC_PER_USEC);
+}
+
+/* This should only be used in contexts where tp->tcp_mstamp is up to date */
+static inline u32 tcp_time_stamp(const struct tcp_sock *tp)
+{
+	return div_u64(tp->tcp_mstamp, USEC_PER_SEC / TCP_TS_HZ);
+}
+
+/* Could use tcp_clock_us() / 1000, but this version uses a single divide */
+static inline u32 tcp_time_stamp_raw(void)
+{
+	return div_u64(tcp_clock_ns(), NSEC_PER_SEC / TCP_TS_HZ);
+}
+
+
+/* Refresh 1us clock of a TCP socket,
+ * ensuring monotically increasing values.
  */
-#define tcp_time_stamp		((__u32)(jiffies))
+static inline void tcp_mstamp_refresh(struct tcp_sock *tp)
+{
+	u64 val = tcp_clock_us();
+
+	if (val > tp->tcp_mstamp)
+		tp->tcp_mstamp = val;
+}
+
+static inline u32 tcp_stamp_us_delta(u64 t1, u64 t0)
+{
+	return max_t(s64, t1 - t0, 0);
+}
 
 static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 {
-	return skb->skb_mstamp.stamp_jiffies;
+	return div_u64(skb->skb_mstamp, USEC_PER_SEC / TCP_TS_HZ);
 }
 
 
@@ -778,9 +819,9 @@ struct tcp_skb_cb {
 			/* pkts S/ACKed so far upon tx of skb, incl retrans: */
 			__u32 delivered;
 			/* start of send pipeline phase */
-			struct skb_mstamp first_tx_mstamp;
+			u64 first_tx_mstamp;
 			/* when we reached the "delivered" count */
-			struct skb_mstamp delivered_mstamp;
+			u64 delivered_mstamp;
 		} tx;   /* only used for outgoing skbs */
 		union {
 			struct inet_skb_parm	h4;
@@ -896,7 +937,7 @@ struct ack_sample {
  * A sample is invalid if "delivered" or "interval_us" is negative.
  */
 struct rate_sample {
-	struct	skb_mstamp prior_mstamp; /* starting timestamp for interval */
+	u64  prior_mstamp; /* starting timestamp for interval */
 	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
 	s32  delivered;		/* number of packets delivered over interval */
 	long interval_us;	/* time for tp->delivered to incr "delivered" */
@@ -1862,7 +1903,7 @@ void tcp_init(void);
 /* tcp_recovery.c */
 extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
-			     const struct skb_mstamp *xmit_time);
+			     u64 xmit_time);
 extern void tcp_rack_reo_timeout(struct sock *sk);
 
 /*

commit 70eabf0e1b8fe11519f793416655266605f700b9
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 14:00:07 2017 -0700

    tcp: use tcp_jiffies32 for rcv_tstamp and lrcvtime
    
    Use tcp_jiffies32 instead of tcp_time_stamp, since
    tcp_time_stamp will soon be only used for TCP TS option.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index feba4c0406e5..5b2932b8363f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1307,8 +1307,8 @@ static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)
 {
 	const struct inet_connection_sock *icsk = &tp->inet_conn;
 
-	return min_t(u32, tcp_time_stamp - icsk->icsk_ack.lrcvtime,
-			  tcp_time_stamp - tp->rcv_tstamp);
+	return min_t(u32, tcp_jiffies32 - icsk->icsk_ack.lrcvtime,
+			  tcp_jiffies32 - tp->rcv_tstamp);
 }
 
 static inline int tcp_fin_time(const struct sock *sk)

commit d635fbe27ebee0f4b845abe5e9620c9400785a5c
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 14:00:03 2017 -0700

    tcp: use tcp_jiffies32 to feed tp->lsndtime
    
    Use tcp_jiffies32 instead of tcp_time_stamp to feed
    tp->lsndtime.
    
    tcp_time_stamp will soon be a litle bit more expensive
    than simply reading 'jiffies'.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4b45be570821..feba4c0406e5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1245,7 +1245,7 @@ static inline void tcp_slow_start_after_idle_check(struct sock *sk)
 	if (!sysctl_tcp_slow_start_after_idle || tp->packets_out ||
 	    ca_ops->cong_control)
 		return;
-	delta = tcp_time_stamp - tp->lsndtime;
+	delta = tcp_jiffies32 - tp->lsndtime;
 	if (delta > inet_csk(sk)->icsk_rto)
 		tcp_cwnd_restart(sk, delta);
 }

commit ec66eda82d4b0c552bf40005d8f53b63b2b07de4
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 14:00:01 2017 -0700

    tcp: introduce tcp_jiffies32
    
    We abuse tcp_time_stamp for two different cases :
    
    1) base to generate TCP Timestamp options (RFC 7323)
    
    2) A 32bit version of jiffies since some TCP fields
       are 32bit wide to save memory.
    
    Since we want in the future to have 1ms TCP TS clock,
    regardless of HZ value, we want to cleanup things.
    
    tcp_jiffies32 is the truncated jiffies value,
    which will be used only in places where we want a 'host'
    timestamp.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b4dc93dae98c..4b45be570821 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -700,11 +700,14 @@ u32 __tcp_select_window(struct sock *sk);
 
 void tcp_send_window_probe(struct sock *sk);
 
-/* TCP timestamps are only 32-bits, this causes a slight
- * complication on 64-bit systems since we store a snapshot
- * of jiffies in the buffer control blocks below.  We decided
- * to use only the low 32-bits of jiffies and hide the ugly
- * casts with the following macro.
+/* TCP uses 32bit jiffies to save some space.
+ * Note that this is different from tcp_time_stamp, which
+ * historically has been the same until linux-4.13.
+ */
+#define tcp_jiffies32 ((u32)jiffies)
+
+/* Generator for TCP TS option (RFC 7323)
+ * Currently tied to 'jiffies' but will soon be driven by 1 ms clock.
  */
 #define tcp_time_stamp		((__u32)(jiffies))
 

commit 218af599fa635b107cfe10acf3249c4dfe5e4123
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 04:24:36 2017 -0700

    tcp: internal implementation for pacing
    
    BBR congestion control depends on pacing, and pacing is
    currently handled by sch_fq packet scheduler for performance reasons,
    and also because implemening pacing with FQ was convenient to truly
    avoid bursts.
    
    However there are many cases where this packet scheduler constraint
    is not practical.
    - Many linux hosts are not focusing on handling thousands of TCP
      flows in the most efficient way.
    - Some routers use fq_codel or other AQM, but still would like
      to use BBR for the few TCP flows they initiate/terminate.
    
    This patch implements an automatic fallback to internal pacing.
    
    Pacing is requested either by BBR or use of SO_MAX_PACING_RATE option.
    
    If sch_fq happens to be in the egress path, pacing is delegated to
    the qdisc, otherwise pacing is done by TCP itself.
    
    One advantage of pacing from TCP stack is to get more precise rtt
    estimations, and less work done from TX completion, since TCP Small
    queue limits are not generally hit. Setups with single TX queue but
    many cpus might even benefit from this.
    
    Note that unlike sch_fq, we do not take into account header sizes.
    Taking care of these headers would add additional complexity for
    no practical differences in behavior.
    
    Some performance numbers using 800 TCP_STREAM flows rate limited to
    ~48 Mbit per second on 40Gbit NIC.
    
    If MQ+pfifo_fast is used on the NIC :
    
    $ sar -n DEV 1 5 | grep eth
    14:48:44         eth0 725743.00 2932134.00  46776.76 4335184.68      0.00      0.00      1.00
    14:48:45         eth0 725349.00 2932112.00  46751.86 4335158.90      0.00      0.00      0.00
    14:48:46         eth0 725101.00 2931153.00  46735.07 4333748.63      0.00      0.00      0.00
    14:48:47         eth0 725099.00 2931161.00  46735.11 4333760.44      0.00      0.00      1.00
    14:48:48         eth0 725160.00 2931731.00  46738.88 4334606.07      0.00      0.00      0.00
    Average:         eth0 725290.40 2931658.20  46747.54 4334491.74      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     4  0      0 259825920  45644 2708324    0    0    21     2  247   98  0  0 100  0  0
     4  0      0 259823744  45644 2708356    0    0     0     0 2400825 159843  0 19 81  0  0
     0  0      0 259824208  45644 2708072    0    0     0     0 2407351 159929  0 19 81  0  0
     1  0      0 259824592  45644 2708128    0    0     0     0 2405183 160386  0 19 80  0  0
     1  0      0 259824272  45644 2707868    0    0     0    32 2396361 158037  0 19 81  0  0
    
    Now use MQ+FQ :
    
    lpaa23:~# echo fq >/proc/sys/net/core/default_qdisc
    lpaa23:~# tc qdisc replace dev eth0 root mq
    
    $ sar -n DEV 1 5 | grep eth
    14:49:57         eth0 678614.00 2727930.00  43739.13 4033279.14      0.00      0.00      0.00
    14:49:58         eth0 677620.00 2723971.00  43674.69 4027429.62      0.00      0.00      1.00
    14:49:59         eth0 676396.00 2719050.00  43596.83 4020125.02      0.00      0.00      0.00
    14:50:00         eth0 675197.00 2714173.00  43518.62 4012938.90      0.00      0.00      1.00
    14:50:01         eth0 676388.00 2719063.00  43595.47 4020171.64      0.00      0.00      0.00
    Average:         eth0 676843.00 2720837.40  43624.95 4022788.86      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     2  0      0 259832240  46008 2710912    0    0    21     2  223  192  0  1 99  0  0
     1  0      0 259832896  46008 2710744    0    0     0     0 1702206 198078  0 17 82  0  0
     0  0      0 259830272  46008 2710596    0    0     0     0 1696340 197756  1 17 83  0  0
     4  0      0 259829168  46024 2710584    0    0    16     0 1688472 197158  1 17 82  0  0
     3  0      0 259830224  46024 2710408    0    0     0     0 1692450 197212  0 18 82  0  0
    
    As expected, number of interrupts per second is very different.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Cc: Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 38a7427ae902..b4dc93dae98c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -574,6 +574,7 @@ void tcp_fin(struct sock *sk);
 void tcp_init_xmit_timers(struct sock *);
 static inline void tcp_clear_xmit_timers(struct sock *sk)
 {
+	hrtimer_cancel(&tcp_sk(sk)->pacing_timer);
 	inet_csk_clear_xmit_timers(sk);
 }
 
@@ -1945,4 +1946,6 @@ static inline void tcp_listendrop(const struct sock *sk)
 	__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);
 }
 
+enum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);
+
 #endif	/* _TCP_H */

commit 1b1fc3fddabfb8739ef2c8f04e05a9858b42c1f7
Author: Wei Wang <weiwan@google.com>
Date:   Fri May 5 12:53:23 2017 -0700

    tcp: make congestion control optionally skip slow start after idle
    
    Congestion control modules that want full control over congestion
    control behavior do not want the cwnd modifications controlled by
    the sysctl_tcp_slow_start_after_idle code path.
    So skip those code paths for CC modules that use the cong_control()
    API.
    As an example, those cwnd effects are not desired for the BBR congestion
    control algorithm.
    
    Fixes: c0402760f565 ("tcp: new CC hook to set sending rate with rate_sample in any CA state")
    Signed-off-by: Wei Wang <weiwan@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8c0e5a901d64..38a7427ae902 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1234,10 +1234,12 @@ void tcp_cwnd_restart(struct sock *sk, s32 delta);
 
 static inline void tcp_slow_start_after_idle_check(struct sock *sk)
 {
+	const struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;
 	struct tcp_sock *tp = tcp_sk(sk);
 	s32 delta;
 
-	if (!sysctl_tcp_slow_start_after_idle || tp->packets_out)
+	if (!sysctl_tcp_slow_start_after_idle || tp->packets_out ||
+	    ca_ops->cong_control)
 		return;
 	delta = tcp_time_stamp - tp->lsndtime;
 	if (delta > inet_csk(sk)->icsk_rto)

commit 84b114b98452c431299d99c135f751659e517acb
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 5 06:56:54 2017 -0700

    tcp: randomize timestamps on syncookies
    
    Whole point of randomization was to hide server uptime, but an attacker
    can simply start a syn flood and TCP generates 'old style' timestamps,
    directly revealing server jiffies value.
    
    Also, TSval sent by the server to a particular remote address vary
    depending on syncookies being sent or not, potentially triggering PAWS
    drops for innocent clients.
    
    Lets implement proper randomization, including for SYNcookies.
    
    Also we do not need to export sysctl_tcp_timestamps, since it is not
    used from a module.
    
    In v2, I added Florian feedback and contribution, adding tsoff to
    tcp_get_cookie_sock().
    
    v3 removed one unused variable in tcp_v4_connect() as Florian spotted.
    
    Fixes: 95a22caee396c ("tcp: randomize tcp timestamp offsets for each connection")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Florian Westphal <fw@strlen.de>
    Tested-by: Florian Westphal <fw@strlen.de>
    Cc: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 270e5cc43c99..8c0e5a901d64 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -470,7 +470,7 @@ void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 /* From syncookies.c */
 struct sock *tcp_get_cookie_sock(struct sock *sk, struct sk_buff *skb,
 				 struct request_sock *req,
-				 struct dst_entry *dst);
+				 struct dst_entry *dst, u32 tsoff);
 int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 		      u32 cookie);
 struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
@@ -1822,7 +1822,8 @@ struct tcp_request_sock_ops {
 #endif
 	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req);
-	__u32 (*init_seq_tsoff)(const struct sk_buff *skb, u32 *tsoff);
+	u32 (*init_seq)(const struct sk_buff *skb);
+	u32 (*init_ts_off)(const struct sk_buff *skb);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   struct tcp_fastopen_cookie *foc,

commit d2329f102d846214e449941289c7009e16be01a0
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 25 10:15:38 2017 -0700

    tcp: do not pass timestamp to tcp_rack_advance()
    
    No longer needed, since tp->tcp_mstamp holds the information.
    
    This is needed to remove sack_state.ack_time in a following patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d7aae25efc7f..270e5cc43c99 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1855,8 +1855,7 @@ void tcp_init(void);
 /* tcp_recovery.c */
 extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
-			     const struct skb_mstamp *xmit_time,
-			     const struct skb_mstamp *ack_time);
+			     const struct skb_mstamp *xmit_time);
 extern void tcp_rack_reo_timeout(struct sock *sk);
 
 /*

commit 88d5c65098e5d15f2cea81f90bb6ecc167e1aa3b
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 25 10:15:37 2017 -0700

    tcp: do not pass timestamp to tcp_rate_gen()
    
    No longer needed, since tp->tcp_mstamp holds the information.
    
    This is needed to remove sack_state.ack_time in a following patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8b4433c4aaa2..d7aae25efc7f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1004,7 +1004,7 @@ void tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb);
 void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,
 			    struct rate_sample *rs);
 void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
-		  struct skb_mstamp *now, struct rate_sample *rs);
+		  struct rate_sample *rs);
 void tcp_rate_check_app_limited(struct sock *sk);
 
 /* These functions determine how the current flow behaves in respect of SACK

commit 128eda86bebeacefb0fcc64cab0155aa76857c92
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 25 10:15:34 2017 -0700

    tcp: do not pass timestamp to tcp_rack_mark_lost()
    
    This is no longer used, since tcp_rack_detect_loss() takes
    the timestamp from tp->tcp_mstamp
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index da28bef1d82b..8b4433c4aaa2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1853,7 +1853,7 @@ void tcp_v4_init(void);
 void tcp_init(void);
 
 /* tcp_recovery.c */
-extern void tcp_rack_mark_lost(struct sock *sk, const struct skb_mstamp *now);
+extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     const struct skb_mstamp *xmit_time,
 			     const struct skb_mstamp *ack_time);

commit 46c2fa39877ed70415ee2b1acfb9129e956f6de4
Author: Wei Wang <weiwan@google.com>
Date:   Thu Apr 20 14:45:47 2017 -0700

    net/tcp_fastopen: Add snmp counter for blackhole detection
    
    This counter records the number of times the firewall blackhole issue is
    detected and active TFO is disabled.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c1abc2abbdcb..da28bef1d82b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1507,7 +1507,7 @@ struct tcp_fastopen_context {
 };
 
 extern unsigned int sysctl_tcp_fastopen_blackhole_timeout;
-void tcp_fastopen_active_disable(void);
+void tcp_fastopen_active_disable(struct sock *sk);
 bool tcp_fastopen_active_should_disable(struct sock *sk);
 void tcp_fastopen_active_disable_ofo_check(struct sock *sk);
 void tcp_fastopen_active_timeout_reset(void);

commit cf1ef3f0719b4dcb74810ed507e2a2540f9811b4
Author: Wei Wang <weiwan@google.com>
Date:   Thu Apr 20 14:45:46 2017 -0700

    net/tcp_fastopen: Disable active side TFO in certain scenarios
    
    Middlebox firewall issues can potentially cause server's data being
    blackholed after a successful 3WHS using TFO. Following are the related
    reports from Apple:
    https://www.nanog.org/sites/default/files/Paasch_Network_Support.pdf
    Slide 31 identifies an issue where the client ACK to the server's data
    sent during a TFO'd handshake is dropped.
    C ---> syn-data ---> S
    C <--- syn/ack ----- S
    C (accept & write)
    C <---- data ------- S
    C ----- ACK -> X     S
                    [retry and timeout]
    
    https://www.ietf.org/proceedings/94/slides/slides-94-tcpm-13.pdf
    Slide 5 shows a similar situation that the server's data gets dropped
    after 3WHS.
    C ---- syn-data ---> S
    C <--- syn/ack ----- S
    C ---- ack --------> S
    S (accept & write)
    C?  X <- data ------ S
                    [retry and timeout]
    
    This is the worst failure b/c the client can not detect such behavior to
    mitigate the situation (such as disabling TFO). Failing to proceed, the
    application (e.g., SSL library) may simply timeout and retry with TFO
    again, and the process repeats indefinitely.
    
    The proposed solution is to disable active TFO globally under the
    following circumstances:
    1. client side TFO socket detects out of order FIN
    2. client side TFO socket receives out of order RST
    
    We disable active side TFO globally for 1hr at first. Then if it
    happens again, we disable it for 2h, then 4h, 8h, ...
    And we reset the timeout to 1hr if a client side TFO sockets not opened
    on loopback has successfully received data segs from server.
    And we examine this condition during close().
    
    The rational behind it is that when such firewall issue happens,
    application running on the client should eventually close the socket as
    it is not able to get the data it is expecting. Or application running
    on the server should close the socket as it is not able to receive any
    response from client.
    In both cases, out of order FIN or RST will get received on the client
    given that the firewall will not block them as no data are in those
    frames.
    And we want to disable active TFO globally as it helps if the middle box
    is very close to the client and most of the connections are likely to
    fail.
    
    Also, add a debug sysctl:
      tcp_fastopen_blackhole_detect_timeout_sec:
        the initial timeout to use when firewall blackhole issue happens.
        This can be set and read.
        When setting it to 0, it means to disable the active disable logic.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cc6ae0a95201..c1abc2abbdcb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1506,6 +1506,12 @@ struct tcp_fastopen_context {
 	struct rcu_head		rcu;
 };
 
+extern unsigned int sysctl_tcp_fastopen_blackhole_timeout;
+void tcp_fastopen_active_disable(void);
+bool tcp_fastopen_active_should_disable(struct sock *sk);
+void tcp_fastopen_active_disable_ofo_check(struct sock *sk);
+void tcp_fastopen_active_timeout_reset(void);
+
 /* Latencies incurred by various limits for a sender. They are
  * chronograph-like stats that are mutually exclusive.
  */

commit 589c49cbf9674808fd4ac9b7c17155abc0686f86
Author: Gao Feng <fgao@ikuai8.com>
Date:   Tue Apr 4 21:09:48 2017 +0800

    net: tcp: Define the TCP_MAX_WSCALE instead of literal number 14
    
    Define one new macro TCP_MAX_WSCALE instead of literal number '14',
    and use U16_MAX instead of 65535 as the max value of TCP window.
    There is another minor change, use rounddown(space, mss) instead of
    (space / mss) * mss;
    
    Signed-off-by: Gao Feng <fgao@ikuai8.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 582e3772c0d9..cc6ae0a95201 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -78,6 +78,9 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* Maximal number of ACKs sent quickly to accelerate slow-start. */
 #define TCP_MAX_QUICKACKS	16U
 
+/* Maximal number of window scale according to RFC1323 */
+#define TCP_MAX_WSCALE		14U
+
 /* urg_data states */
 #define TCP_URG_VALID	0x0100
 #define TCP_URG_NOTYET	0x0200

commit c48367427a39ea0b85c7cf018fe4256627abfd9e
Author: Gao Feng <fgao@ikuai8.com>
Date:   Fri Mar 24 07:05:12 2017 +0800

    tcp: sysctl: Fix a race to avoid unexpected 0 window from space
    
    Because sysctl_tcp_adv_win_scale could be changed any time, so there
    is one race in tcp_win_from_space.
    For example,
    1.sysctl_tcp_adv_win_scale<=0 (sysctl_tcp_adv_win_scale is negative now)
    2.space>>(-sysctl_tcp_adv_win_scale) (sysctl_tcp_adv_win_scale is postive now)
    
    As a result, tcp_win_from_space returns 0. It is unexpected.
    
    Certainly if the compiler put the sysctl_tcp_adv_win_scale into one
    register firstly, then use the register directly, it would be ok.
    But we could not depend on the compiler behavior.
    
    Signed-off-by: Gao Feng <fgao@ikuai8.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e614ad4d613e..582e3772c0d9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1248,9 +1248,11 @@ void tcp_select_initial_window(int __space, __u32 mss, __u32 *rcv_wnd,
 
 static inline int tcp_win_from_space(int space)
 {
-	return sysctl_tcp_adv_win_scale<=0 ?
-		(space>>(-sysctl_tcp_adv_win_scale)) :
-		space - (space>>sysctl_tcp_adv_win_scale);
+	int tcp_adv_win_scale = sysctl_tcp_adv_win_scale;
+
+	return tcp_adv_win_scale <= 0 ?
+		(space>>(-tcp_adv_win_scale)) :
+		space - (space>>tcp_adv_win_scale);
 }
 
 /* Note: caller must be prepared to deal with negative returns */

commit 4396e46187ca5070219b81773c4e65088dac50cc
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Wed Mar 15 16:30:46 2017 -0400

    tcp: remove tcp_tw_recycle
    
    The tcp_tw_recycle was already broken for connections
    behind NAT, since the per-destination timestamp is not
    monotonically increasing for multiple machines behind
    a single destination address.
    
    After the randomization of TCP timestamp offsets
    in commit 8a5bd45f6616 (tcp: randomize tcp timestamp offsets
    for each connection), the tcp_tw_recycle is broken for all
    types of connections for the same reason: the timestamps
    received from a single machine is not monotonically increasing,
    anymore.
    
    Remove tcp_tw_recycle, since it is not functional. Also, remove
    the PAWSPassive SNMP counter since it is only used for
    tcp_tw_recycle, and simplify tcp_v4_route_req and tcp_v6_route_req
    since the strict argument is only set when tcp_tw_recycle is
    enabled.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Cc: Lutz Vieweg <lvml@5t9.de>
    Cc: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c81f3b958d44..e614ad4d613e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1810,8 +1810,7 @@ struct tcp_request_sock_ops {
 				 __u16 *mss);
 #endif
 	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
-				       const struct request_sock *req,
-				       bool *strict);
+				       const struct request_sock *req);
 	__u32 (*init_seq_tsoff)(const struct sk_buff *skb, u32 *tsoff);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,

commit d82bae12dc38d79a2b77473f5eb0612a3d69c55b
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Wed Mar 15 16:30:45 2017 -0400

    tcp: remove per-destination timestamp cache
    
    Commit 8a5bd45f6616 (tcp: randomize tcp timestamp offsets for each connection)
    randomizes TCP timestamps per connection. After this commit,
    there is no guarantee that the timestamps received from the
    same destination are monotonically increasing. As a result,
    the per-destination timestamp cache in TCP metrics (i.e., tcpm_ts
    in struct tcp_metrics_block) is broken and cannot be relied upon.
    
    Remove the per-destination timestamp cache and all related code
    paths.
    
    Note that this cache was already broken for caching timestamps of
    multiple machines behind a NAT sharing the same address.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Cc: Lutz Vieweg <lvml@5t9.de>
    Cc: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bede8f7fa742..c81f3b958d44 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -406,11 +406,7 @@ void tcp_clear_retrans(struct tcp_sock *tp);
 void tcp_update_metrics(struct sock *sk);
 void tcp_init_metrics(struct sock *sk);
 void tcp_metrics_init(void);
-bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
-			bool paws_check, bool timestamps);
-bool tcp_remember_stamp(struct sock *sk);
-bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
-void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
+bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 void tcp_disable_fack(struct tcp_sock *tp);
 void tcp_close(struct sock *sk, long timeout);
 void tcp_init_sock(struct sock *sk);

commit a30aad50c26cac63026e5dfcc2e055ae63fe6ef7
Author: Alexey Kodanev <alexey.kodanev@oracle.com>
Date:   Thu Mar 9 13:53:55 2017 +0300

    tcp: rename *_sequence_number() to *_seq_and_tsoff()
    
    The functions that are returning tcp sequence number also setup
    TS offset value, so rename them to better describe their purpose.
    
    No functional changes in this patch.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Alexey Kodanev <alexey.kodanev@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6ec4ea652f3f..bede8f7fa742 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1816,7 +1816,7 @@ struct tcp_request_sock_ops {
 	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req,
 				       bool *strict);
-	__u32 (*init_seq)(const struct sk_buff *skb, u32 *tsoff);
+	__u32 (*init_seq_tsoff)(const struct sk_buff *skb, u32 *tsoff);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   struct tcp_fastopen_cookie *foc,

commit 19f6d3f3c8422d65b5e3d2162e30ef07c6e21ea2
Author: Wei Wang <weiwan@google.com>
Date:   Mon Jan 23 10:59:22 2017 -0800

    net/tcp-fastopen: Add new API support
    
    This patch adds a new socket option, TCP_FASTOPEN_CONNECT, as an
    alternative way to perform Fast Open on the active side (client). Prior
    to this patch, a client needs to replace the connect() call with
    sendto(MSG_FASTOPEN). This can be cumbersome for applications who want
    to use Fast Open: these socket operations are often done in lower layer
    libraries used by many other applications. Changing these libraries
    and/or the socket call sequences are not trivial. A more convenient
    approach is to perform Fast Open by simply enabling a socket option when
    the socket is created w/o changing other socket calls sequence:
      s = socket()
        create a new socket
      setsockopt(s, IPPROTO_TCP, TCP_FASTOPEN_CONNECT …);
        newly introduced sockopt
        If set, new functionality described below will be used.
        Return ENOTSUPP if TFO is not supported or not enabled in the
        kernel.
    
      connect()
        With cookie present, return 0 immediately.
        With no cookie, initiate 3WHS with TFO cookie-request option and
        return -1 with errno = EINPROGRESS.
    
      write()/sendmsg()
        With cookie present, send out SYN with data and return the number of
        bytes buffered.
        With no cookie, and 3WHS not yet completed, return -1 with errno =
        EINPROGRESS.
        No MSG_FASTOPEN flag is needed.
    
      read()
        Return -1 with errno = EWOULDBLOCK/EAGAIN if connect() is called but
        write() is not called yet.
        Return -1 with errno = EWOULDBLOCK/EAGAIN if connection is
        established but no msg is received yet.
        Return number of bytes read if socket is established and there is
        msg received.
    
    The new API simplifies life for applications that always perform a write()
    immediately after a successful connect(). Such applications can now take
    advantage of Fast Open by merely making one new setsockopt() call at the time
    of creating the socket. Nothing else about the application's socket call
    sequence needs to change.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index de67541d7adf..6ec4ea652f3f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1495,6 +1495,7 @@ struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 void tcp_fastopen_init_key_once(bool publish);
 bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
 			     struct tcp_fastopen_cookie *cookie);
+bool tcp_fastopen_defer_connect(struct sock *sk, int *err);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 
 /* Fastopen key context */

commit 065263f40f0972d5f1cd294bb0242bd5aa5f06b2
Author: Wei Wang <weiwan@google.com>
Date:   Mon Jan 23 10:59:20 2017 -0800

    net/tcp-fastopen: refactor cookie check logic
    
    Refactor the cookie check logic in tcp_send_syn_data() into a function.
    This function will be called else where in later changes.
    
    Signed-off-by: Wei Wang <weiwan@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c55d65f74f7f..de67541d7adf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1493,6 +1493,8 @@ struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct tcp_fastopen_cookie *foc,
 			      struct dst_entry *dst);
 void tcp_fastopen_init_key_once(bool publish);
+bool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,
+			     struct tcp_fastopen_cookie *cookie);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 
 /* Fastopen key context */

commit bec41a11dd3dc8c54f766b4f494140ca92ba7c10
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:39 2017 -0800

    tcp: remove early retransmit
    
    This patch removes the support of RFC5827 early retransmit (i.e.,
    fast recovery on small inflight with <3 dupacks) because it is
    subsumed by the new RACK loss detection. More specifically when
    RACK receives DUPACKs, it'll arm a reordering timer to start fast
    recovery after a quarter of (min)RTT, hence it covers the early
    retransmit except RACK does not limit itself to specific inflight
    or dupack numbers.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 423438dd6fe9..c55d65f74f7f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -565,7 +565,6 @@ void tcp_skb_collapse_tstamp(struct sk_buff *skb,
 			     const struct sk_buff *next_skb);
 
 /* tcp_input.c */
-void tcp_resume_early_retransmit(struct sock *sk);
 void tcp_rearm_rto(struct sock *sk);
 void tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req);
 void tcp_reset(struct sock *sk);
@@ -1037,24 +1036,6 @@ static inline void tcp_enable_fack(struct tcp_sock *tp)
 	tp->rx_opt.sack_ok |= TCP_FACK_ENABLED;
 }
 
-/* TCP early-retransmit (ER) is similar to but more conservative than
- * the thin-dupack feature.  Enable ER only if thin-dupack is disabled.
- */
-static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
-{
-	struct net *net = sock_net((struct sock *)tp);
-
-	tp->do_early_retrans = sysctl_tcp_early_retrans &&
-		sysctl_tcp_early_retrans < 4 && !sysctl_tcp_thin_dupack &&
-		!(sysctl_tcp_recovery & TCP_RACK_LOSS_DETECTION) &&
-		net->ipv4.sysctl_tcp_reordering == 3;
-}
-
-static inline void tcp_disable_early_retrans(struct tcp_sock *tp)
-{
-	tp->do_early_retrans = 0;
-}
-
 static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
 {
 	return tp->sacked_out + tp->lost_out;

commit a0370b3f3f2cfb8b424b04c0545414abaa53f5ee
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:36 2017 -0800

    tcp: enable RACK loss detection to trigger recovery
    
    This patch changes two things:
    
    1. Start fast recovery with RACK in addition to other heuristics
       (e.g., DUPACK threshold, FACK). Prior to this change RACK
       is enabled to detect losses only after the recovery has
       started by other algorithms.
    
    2. Disable TCP early retransmit. RACK subsumes the early retransmit
       with the new reordering timer feature. A latter patch in this
       series removes the early retransmit code.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5fb1e75a32a9..423438dd6fe9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -262,6 +262,9 @@ extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
+extern int sysctl_tcp_recovery;
+#define TCP_RACK_LOSS_DETECTION  0x1 /* Use RACK to detect losses */
+
 extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
 extern int sysctl_tcp_min_tso_segs;
@@ -1043,6 +1046,7 @@ static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
 
 	tp->do_early_retrans = sysctl_tcp_early_retrans &&
 		sysctl_tcp_early_retrans < 4 && !sysctl_tcp_thin_dupack &&
+		!(sysctl_tcp_recovery & TCP_RACK_LOSS_DETECTION) &&
 		net->ipv4.sysctl_tcp_reordering == 3;
 }
 
@@ -1859,13 +1863,6 @@ void tcp_v4_init(void);
 void tcp_init(void);
 
 /* tcp_recovery.c */
-
-/* Flags to enable various loss recovery features. See below */
-extern int sysctl_tcp_recovery;
-
-/* Use TCP RACK to detect (some) tail and retransmit losses */
-#define TCP_RACK_LOST_RETRANS  0x1
-
 extern void tcp_rack_mark_lost(struct sock *sk, const struct skb_mstamp *now);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     const struct skb_mstamp *xmit_time,

commit 1d0833df594390876647c54c2c88069d29059665
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:34 2017 -0800

    tcp: use sequence to break TS ties for RACK loss detection
    
    The packets inside a jumbo skb (e.g., TSO) share the same skb
    timestamp, even though they are sent sequentially on the wire. Since
    RACK is based on time, it can not detect some packets inside the
    same skb are lost.  However, we can leverage the packet sequence
    numbers as extended timestamps to detect losses. Therefore, when
    RACK timestamp is identical to skb's timestamp (i.e., one of the
    packets of the skb is acked or sacked), we use the sequence numbers
    of the acked and unacked packets to break ties.
    
    We can use the same sequence logic to advance RACK xmit time as
    well to detect more losses and avoid timeout.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 64fcdeb3358b..5fb1e75a32a9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1867,7 +1867,7 @@ extern int sysctl_tcp_recovery;
 #define TCP_RACK_LOST_RETRANS  0x1
 
 extern void tcp_rack_mark_lost(struct sock *sk, const struct skb_mstamp *now);
-extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked,
+extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,
 			     const struct skb_mstamp *xmit_time,
 			     const struct skb_mstamp *ack_time);
 extern void tcp_rack_reo_timeout(struct sock *sk);

commit 57dde7f70de34d4251f291c9eac7ad920aaf56b2
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:33 2017 -0800

    tcp: add reordering timer in RACK loss detection
    
    This patch makes RACK install a reordering timer when it suspects
    some packets might be lost, but wants to delay the decision
    a little bit to accomodate reordering.
    
    It does not create a new timer but instead repurposes the existing
    RTO timer, because both are meant to retransmit packets.
    Specifically it arms a timer ICSK_TIME_REO_TIMEOUT when
    the RACK timing check fails. The wait time is set to
    
      RACK.RTT + RACK.reo_wnd - (NOW - Packet.xmit_time) + fudge
    
    This translates to expecting a packet (Packet) should take
    (RACK.RTT + RACK.reo_wnd + fudge) to deliver after it was sent.
    
    When there are multiple packets that need a timer, we use one timer
    with the maximum timeout. Therefore the timer conservatively uses
    the maximum window to expire N packets by one timeout, instead of
    N timeouts to expire N packets sent at different times.
    
    The fudge factor is 2 jiffies to ensure when the timer fires, all
    the suspected packets would exceed the deadline and be marked lost
    by tcp_rack_detect_loss(). It has to be at least 1 jiffy because the
    clock may tick between calling icsk_reset_xmit_timer(timeout) and
    actually hang the timer. The next jiffy is to lower-bound the timeout
    to 2 jiffies when reo_wnd is < 1ms.
    
    When the reordering timer fires (tcp_rack_reo_timeout): If we aren't
    in Recovery we'll enter fast recovery and force fast retransmit.
    This is very similar to the early retransmit (RFC5827) except RACK
    is not constrained to only enter recovery for small outstanding
    flights.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1439107658c2..64fcdeb3358b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -143,6 +143,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U)) /* Maximal interval between probes
 					                 * for local resources.
 					                 */
+#define TCP_REO_TIMEOUT_MIN	(2000) /* Min RACK reordering timeout in usec */
 
 #define TCP_KEEPALIVE_TIME	(120*60*HZ)	/* two hours */
 #define TCP_KEEPALIVE_PROBES	9		/* Max of 9 keepalive probes	*/
@@ -397,6 +398,7 @@ struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
 int tcp_child_process(struct sock *parent, struct sock *child,
 		      struct sk_buff *skb);
 void tcp_enter_loss(struct sock *sk);
+void tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int flag);
 void tcp_clear_retrans(struct tcp_sock *tp);
 void tcp_update_metrics(struct sock *sk);
 void tcp_init_metrics(struct sock *sk);
@@ -541,6 +543,7 @@ int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
 void tcp_retransmit_timer(struct sock *sk);
 void tcp_xmit_retransmit_queue(struct sock *);
 void tcp_simple_retransmit(struct sock *);
+void tcp_enter_recovery(struct sock *sk, bool ece_ack);
 int tcp_trim_head(struct sock *, struct sk_buff *, u32);
 int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int, gfp_t);
 
@@ -1867,6 +1870,7 @@ extern void tcp_rack_mark_lost(struct sock *sk, const struct skb_mstamp *now);
 extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked,
 			     const struct skb_mstamp *xmit_time,
 			     const struct skb_mstamp *ack_time);
+extern void tcp_rack_reo_timeout(struct sock *sk);
 
 /*
  * Save and compile IPv4 options, return a pointer to it

commit deed7be78f512d003c6290da0a781479b31b3d74
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:32 2017 -0800

    tcp: record most recent RTT in RACK loss detection
    
    Record the most recent RTT in RACK. It is often identical to the
    "ca_rtt_us" values in tcp_clean_rtx_queue. But when the packet has
    been retransmitted, RACK choses to believe the ACK is for the
    (latest) retransmitted packet if the RTT is over minimum RTT.
    
    This requires passing the arrival time of the most recent ACK to
    RACK routines. The timestamp is now recorded in the "ack_time"
    in tcp_sacktag_state during the ACK processing.
    
    This patch does not change the RACK algorithm itself. It only adds
    the RTT variable to prepare the next main patch.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 51183bba3835..1439107658c2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1863,9 +1863,10 @@ extern int sysctl_tcp_recovery;
 /* Use TCP RACK to detect (some) tail and retransmit losses */
 #define TCP_RACK_LOST_RETRANS  0x1
 
-extern void tcp_rack_mark_lost(struct sock *sk);
-extern void tcp_rack_advance(struct tcp_sock *tp,
-			     const struct skb_mstamp *xmit_time, u8 sacked);
+extern void tcp_rack_mark_lost(struct sock *sk, const struct skb_mstamp *now);
+extern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked,
+			     const struct skb_mstamp *xmit_time,
+			     const struct skb_mstamp *ack_time);
 
 /*
  * Save and compile IPv4 options, return a pointer to it

commit e636f8b0104d6622aaaed6aa5ef17dfbf165bc51
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jan 12 22:11:31 2017 -0800

    tcp: new helper for RACK to detect loss
    
    Create a new helper tcp_rack_detect_loss to prepare the upcoming
    RACK reordering timer patch.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1da0aa724929..51183bba3835 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1863,8 +1863,7 @@ extern int sysctl_tcp_recovery;
 /* Use TCP RACK to detect (some) tail and retransmit losses */
 #define TCP_RACK_LOST_RETRANS  0x1
 
-extern int tcp_rack_mark_lost(struct sock *sk);
-
+extern void tcp_rack_mark_lost(struct sock *sk);
 extern void tcp_rack_advance(struct tcp_sock *tp,
 			     const struct skb_mstamp *xmit_time, u8 sacked);
 

commit 1946e672c173559155a3e210fe95dbf8b7b8ddf7
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Wed Dec 28 17:52:32 2016 +0800

    ipv4: Namespaceify tcp_tw_recycle and tcp_max_tw_buckets knob
    
    Different namespace application might require fast recycling
    TIME-WAIT sockets independently of the host.
    
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6061963cca98..1da0aa724929 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -231,7 +231,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
  */
 #define	TFO_SERVER_WO_SOCKOPT1	0x400
 
-extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */
 extern int sysctl_tcp_timestamps;

commit 56ab6b93007e5000a8812985aec1833c4a6a9ce0
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Sun Dec 25 14:33:16 2016 +0800

    ipv4: Namespaceify tcp_tw_reuse knob
    
    Different namespaces might have different requirements to reuse
    TIME-WAIT sockets for new connections. This might be required in
    cases where different namespace applications are in place which
    require TIME_WAIT socket connections to be reduced independently
    of the host.
    
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 207147b4c6b2..6061963cca98 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -252,7 +252,6 @@ extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
 extern int sysctl_tcp_app_win;
 extern int sysctl_tcp_adv_win_scale;
-extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_nometrics_save;

commit 95a22caee396cef0bb2ca8fafdd82966a49367bb
Author: Florian Westphal <fw@strlen.de>
Date:   Thu Dec 1 11:32:06 2016 +0100

    tcp: randomize tcp timestamp offsets for each connection
    
    jiffies based timestamps allow for easy inference of number of devices
    behind NAT translators and also makes tracking of hosts simpler.
    
    commit ceaa1fef65a7c2e ("tcp: adding a per-socket timestamp offset")
    added the main infrastructure that is needed for per-connection ts
    randomization, in particular writing/reading the on-wire tcp header
    format takes the offset into account so rest of stack can use normal
    tcp_time_stamp (jiffies).
    
    So only two items are left:
     - add a tsoffset for request sockets
     - extend the tcp isn generator to also return another 32bit number
       in addition to the ISN.
    
    Re-use of ISN generator also means timestamps are still monotonically
    increasing for same connection quadruple, i.e. PAWS will still work.
    
    Includes fixes from Eric Dumazet.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3e097e39d4d2..207147b4c6b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1827,7 +1827,7 @@ struct tcp_request_sock_ops {
 	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req,
 				       bool *strict);
-	__u32 (*init_seq)(const struct sk_buff *skb);
+	__u32 (*init_seq)(const struct sk_buff *skb, u32 *tsoff);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   struct tcp_fastopen_cookie *foc,

commit 0f87230d1a6c253681550c6064715d06a32be73d
Author: Francis Yan <francisyyan@gmail.com>
Date:   Sun Nov 27 23:07:14 2016 -0800

    tcp: instrument how long TCP is busy sending
    
    This patch measures TCP busy time, which is defined as the period
    of time when sender has data (or FIN) to send. The time starts when
    data is buffered and stops when the write queue is flushed by ACKs
    or error events.
    
    Note the busy time does not include SYN time, unless data is
    included in SYN (i.e. Fast Open). It does include FIN time even
    if the FIN carries no payload. Excluding pure FIN is possible but
    would incur one additional test in the fast path, which may not
    be worth it.
    
    Signed-off-by: Francis Yan <francisyyan@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e5ff4083870d..3e097e39d4d2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1535,6 +1535,7 @@ static inline void tcp_write_queue_purge(struct sock *sk)
 {
 	struct sk_buff *skb;
 
+	tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
 	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
 		sk_wmem_free_skb(sk, skb);
 	sk_mem_reclaim(sk);
@@ -1593,8 +1594,10 @@ static inline void tcp_advance_send_head(struct sock *sk, const struct sk_buff *
 
 static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)
 {
-	if (sk->sk_send_head == skb_unlinked)
+	if (sk->sk_send_head == skb_unlinked) {
 		sk->sk_send_head = NULL;
+		tcp_chrono_stop(sk, TCP_CHRONO_BUSY);
+	}
 	if (tcp_sk(sk)->highest_sack == skb_unlinked)
 		tcp_sk(sk)->highest_sack = NULL;
 }
@@ -1616,6 +1619,7 @@ static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb
 	/* Queue it, remembering where we must start sending. */
 	if (sk->sk_send_head == NULL) {
 		sk->sk_send_head = skb;
+		tcp_chrono_start(sk, TCP_CHRONO_BUSY);
 
 		if (tcp_sk(sk)->highest_sack == NULL)
 			tcp_sk(sk)->highest_sack = skb;

commit 05b055e89121394058c75dc354e9a46e1e765579
Author: Francis Yan <francisyyan@gmail.com>
Date:   Sun Nov 27 23:07:13 2016 -0800

    tcp: instrument tcp sender limits chronographs
    
    This patch implements the skeleton of the TCP chronograph
    instrumentation on sender side limits:
    
            1) idle (unspec)
            2) busy sending data other than 3-4 below
            3) rwnd-limited
            4) sndbuf-limited
    
    The limits are enumerated 'tcp_chrono'. Since a connection in
    theory can idle forever, we do not track the actual length of this
    uninteresting idle period. For the rest we track how long the sender
    spends in each limit. At any point during the life time of a
    connection, the sender must be in one of the four states.
    
    If there are multiple conditions worthy of tracking in a chronograph
    then the highest priority enum takes precedence over
    the other conditions. So that if something "more interesting"
    starts happening, stop the previous chrono and start a new one.
    
    The time unit is jiffy(u32) in order to save space in tcp_sock.
    This implies application must sample the stats no longer than every
    49 days of 1ms jiffy.
    
    Signed-off-by: Francis Yan <francisyyan@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7de80739adab..e5ff4083870d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1516,6 +1516,20 @@ struct tcp_fastopen_context {
 	struct rcu_head		rcu;
 };
 
+/* Latencies incurred by various limits for a sender. They are
+ * chronograph-like stats that are mutually exclusive.
+ */
+enum tcp_chrono {
+	TCP_CHRONO_UNSPEC,
+	TCP_CHRONO_BUSY, /* Actively sending data (non-empty write queue) */
+	TCP_CHRONO_RWND_LIMITED, /* Stalled by insufficient receive window */
+	TCP_CHRONO_SNDBUF_LIMITED, /* Stalled by insufficient send buffer */
+	__TCP_CHRONO_MAX,
+};
+
+void tcp_chrono_start(struct sock *sk, const enum tcp_chrono type);
+void tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type);
+
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
 {

commit e97991832a4ea4a5f47d65f068a4c966a2eb5730
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Nov 21 14:18:38 2016 +0100

    tcp: make undo_cwnd mandatory for congestion modules
    
    The undo_cwnd fallback in the stack doubles cwnd based on ssthresh,
    which un-does reno halving behaviour.
    
    It seems more appropriate to let congctl algorithms pair .ssthresh
    and .undo_cwnd properly. Add a 'tcp_reno_undo_cwnd' function and wire it
    up for all congestion algorithms that used to rely on the fallback.
    
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 123979fe12bf..7de80739adab 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -958,6 +958,7 @@ u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 
 u32 tcp_reno_ssthresh(struct sock *sk);
+u32 tcp_reno_undo_cwnd(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 

commit ac6e780070e30e4c35bd395acfe9191e6268bdd3
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Nov 10 13:12:35 2016 -0800

    tcp: take care of truncations done by sk_filter()
    
    With syzkaller help, Marco Grassi found a bug in TCP stack,
    crashing in tcp_collapse()
    
    Root cause is that sk_filter() can truncate the incoming skb,
    but TCP stack was not really expecting this to happen.
    It probably was expecting a simple DROP or ACCEPT behavior.
    
    We first need to make sure no part of TCP header could be removed.
    Then we need to adjust TCP_SKB_CB(skb)->end_seq
    
    Many thanks to syzkaller team and Marco for giving us a reproducer.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Marco Grassi <marco.gra@gmail.com>
    Reported-by: Vladis Dronov <vdronov@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 304a8e17bc87..123979fe12bf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1220,6 +1220,7 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 
 bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
 bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);
+int tcp_filter(struct sock *sk, struct sk_buff *skb);
 
 #undef STATE_TRACE
 

commit da96786e26c3ae47316db2b92046b11268c4379c
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Wed Nov 2 12:08:25 2016 -0700

    net: tcp: check skb is non-NULL for exact match on lookups
    
    Andrey reported the following error report while running the syzkaller
    fuzzer:
    
    general protection fault: 0000 [#1] SMP KASAN
    Dumping ftrace buffer:
       (ftrace buffer empty)
    Modules linked in:
    CPU: 0 PID: 648 Comm: syz-executor Not tainted 4.9.0-rc3+ #333
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    task: ffff8800398c4480 task.stack: ffff88003b468000
    RIP: 0010:[<ffffffff83091106>]  [<     inline     >]
    inet_exact_dif_match include/net/tcp.h:808
    RIP: 0010:[<ffffffff83091106>]  [<ffffffff83091106>]
    __inet_lookup_listener+0xb6/0x500 net/ipv4/inet_hashtables.c:219
    RSP: 0018:ffff88003b46f270  EFLAGS: 00010202
    RAX: 0000000000000004 RBX: 0000000000004242 RCX: 0000000000000001
    RDX: 0000000000000000 RSI: ffffc90000e3c000 RDI: 0000000000000054
    RBP: ffff88003b46f2d8 R08: 0000000000004000 R09: ffffffff830910e7
    R10: 0000000000000000 R11: 000000000000000a R12: ffffffff867fa0c0
    R13: 0000000000004242 R14: 0000000000000003 R15: dffffc0000000000
    FS:  00007fb135881700(0000) GS:ffff88003ec00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000020cc3000 CR3: 000000006d56a000 CR4: 00000000000006f0
    Stack:
     0000000000000000 000000000601a8c0 0000000000000000 ffffffff00004242
     424200003b9083c2 ffff88003def4041 ffffffff84e7e040 0000000000000246
     ffff88003a0911c0 0000000000000000 ffff88003a091298 ffff88003b9083ae
    Call Trace:
     [<ffffffff831100f4>] tcp_v4_send_reset+0x584/0x1700 net/ipv4/tcp_ipv4.c:643
     [<ffffffff83115b1b>] tcp_v4_rcv+0x198b/0x2e50 net/ipv4/tcp_ipv4.c:1718
     [<ffffffff83069d22>] ip_local_deliver_finish+0x332/0xad0
    net/ipv4/ip_input.c:216
    ...
    
    MD5 has a code path that calls __inet_lookup_listener with a null skb,
    so inet{6}_exact_dif_match needs to check skb against null before pulling
    the flag.
    
    Fixes: a04a480d4392 ("net: Require exact match for TCP socket lookups if
           dif is l3mdev")
    Reported-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Tested-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5b82d4d94834..304a8e17bc87 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -805,7 +805,7 @@ static inline bool inet_exact_dif_match(struct net *net, struct sk_buff *skb)
 {
 #if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)
 	if (!net->ipv4.sysctl_tcp_l3mdev_accept &&
-	    ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))
+	    skb && ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))
 		return true;
 #endif
 	return false;

commit a04a480d4392ea6efd117be2de564117b2a009c0
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Sun Oct 16 20:02:52 2016 -0700

    net: Require exact match for TCP socket lookups if dif is l3mdev
    
    Currently, socket lookups for l3mdev (vrf) use cases can match a socket
    that is bound to a port but not a device (ie., a global socket). If the
    sysctl tcp_l3mdev_accept is not set this leads to ack packets going out
    based on the main table even though the packet came in from an L3 domain.
    The end result is that the connection does not establish creating
    confusion for users since the service is running and a socket shows in
    ss output. Fix by requiring an exact dif to sk_bound_dev_if match if the
    skb came through an interface enslaved to an l3mdev device and the
    tcp_l3mdev_accept is not set.
    
    skb's through an l3mdev interface are marked by setting a flag in
    inet{6}_skb_parm. The IPv6 variant is already set; this patch adds the
    flag for IPv4. Using an skb flag avoids a device lookup on the dif. The
    flag is set in the VRF driver using the IP{6}CB macros. For IPv4, the
    inet_skb_parm struct is moved in the cb per commit 971f10eca186, so the
    match function in the TCP stack needs to use TCP_SKB_CB. For IPv6, the
    move is done after the socket lookup, so IP6CB is used.
    
    The flags field in inet_skb_parm struct needs to be increased to add
    another flag. There is currently a 1-byte hole following the flags,
    so it can be expanded to u16 without increasing the size of the struct.
    
    Fixes: 193125dbd8eb ("net: Introduce VRF device driver")
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f83b7f220a65..5b82d4d94834 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -794,12 +794,23 @@ struct tcp_skb_cb {
  */
 static inline int tcp_v6_iif(const struct sk_buff *skb)
 {
-	bool l3_slave = skb_l3mdev_slave(TCP_SKB_CB(skb)->header.h6.flags);
+	bool l3_slave = ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags);
 
 	return l3_slave ? skb->skb_iif : TCP_SKB_CB(skb)->header.h6.iif;
 }
 #endif
 
+/* TCP_SKB_CB reference means this can not be used from early demux */
+static inline bool inet_exact_dif_match(struct net *net, struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)
+	if (!net->ipv4.sysctl_tcp_l3mdev_accept &&
+	    ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))
+		return true;
+#endif
+	return false;
+}
+
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */

commit c0402760f565ae066621ebf8720a32fba074d538
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 19 23:39:21 2016 -0400

    tcp: new CC hook to set sending rate with rate_sample in any CA state
    
    This commit introduces an optional new "omnipotent" hook,
    cong_control(), for congestion control modules. The cong_control()
    function is called at the end of processing an ACK (i.e., after
    updating sequence numbers, the SACK scoreboard, and loss
    detection). At that moment we have precise delivery rate information
    the congestion control module can use to control the sending behavior
    (using cwnd, TSO skb size, and pacing rate) in any CA state.
    
    This function can also be used by a congestion control that prefers
    not to use the default cwnd reduction approach (i.e., the PRR
    algorithm) during CA_Recovery to control the cwnd and sending rate
    during loss recovery.
    
    We take advantage of the fact that recent changes defer the
    retransmission or transmission of new data (e.g. by F-RTO) in recovery
    until the new tcp_cong_control() function is run.
    
    With this commit, we only run tcp_update_pacing_rate() if the
    congestion control is not using this new API. New congestion controls
    which use the new API do not want the TCP stack to run the default
    pacing rate calculation and overwrite whatever pacing rate they have
    chosen at initialization time.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1aa9628ae608..f83b7f220a65 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -919,6 +919,10 @@ struct tcp_congestion_ops {
 	u32 (*tso_segs_goal)(struct sock *sk);
 	/* returns the multiplier used in tcp_sndbuf_expand (optional) */
 	u32 (*sndbuf_expand)(struct sock *sk);
+	/* call when packets are delivered to update cwnd and pacing rate,
+	 * after all the ca_state processing. (optional)
+	 */
+	void (*cong_control)(struct sock *sk, const struct rate_sample *rs);
 	/* get info for inet_diag (optional) */
 	size_t (*get_info)(struct sock *sk, u32 ext, int *attr,
 			   union tcp_cc_info *info);

commit 77bfc174c38e558a3425d3b069aa2762b2fedfdd
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 19 23:39:20 2016 -0400

    tcp: allow congestion control to expand send buffer differently
    
    Currently the TCP send buffer expands to twice cwnd, in order to allow
    limited transmits in the CA_Recovery state. This assumes that cwnd
    does not increase in the CA_Recovery.
    
    For some congestion control algorithms, like the upcoming BBR module,
    if the losses in recovery do not indicate congestion then we may
    continue to raise cwnd multiplicatively in recovery. In such cases the
    current multiplier will falsely limit the sending rate, much as if it
    were limited by the application.
    
    This commit adds an optional congestion control callback to use a
    different multiplier to expand the TCP send buffer. For congestion
    control modules that do not specificy this callback, TCP continues to
    use the previous default of 2.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 349204130d84..1aa9628ae608 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -917,6 +917,8 @@ struct tcp_congestion_ops {
 	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);
 	/* suggest number of segments for each skb to transmit (optional) */
 	u32 (*tso_segs_goal)(struct sock *sk);
+	/* returns the multiplier used in tcp_sndbuf_expand (optional) */
+	u32 (*sndbuf_expand)(struct sock *sk);
 	/* get info for inet_diag (optional) */
 	size_t (*get_info)(struct sock *sk, u32 ext, int *attr,
 			   union tcp_cc_info *info);

commit 1b3878ca1551f3baab2c408d1e703b5ef785a1b2
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Sep 19 23:39:18 2016 -0400

    tcp: export tcp_tso_autosize() and parameterize minimum number of TSO segments
    
    To allow congestion control modules to use the default TSO auto-sizing
    algorithm as one of the ingredients in their own decision about TSO sizing:
    
    1) Export tcp_tso_autosize() so that CC modules can use it.
    
    2) Change tcp_tso_autosize() to allow callers to specify a minimum
       number of segments per TSO skb, in case the congestion control
       module has a different notion of the best floor for TSO skbs for
       the connection right now. For very low-rate paths or policed
       connections it can be appropriate to use smaller TSO skbs.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f8f581fd05f5..349204130d84 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -533,6 +533,8 @@ __u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mss);
 #endif
 /* tcp_output.c */
 
+u32 tcp_tso_autosize(const struct sock *sk, unsigned int mss_now,
+		     int min_tso_segs);
 void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			       int nonagle);
 bool tcp_may_send_now(struct sock *sk);

commit ed6e7268b930e0a9a65d895d368eac79a438d992
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Sep 19 23:39:17 2016 -0400

    tcp: allow congestion control module to request TSO skb segment count
    
    Add the tso_segs_goal() function in tcp_congestion_ops to allow the
    congestion control module to specify the number of segments that
    should be in a TSO skb sent by tcp_write_xmit() and
    tcp_xmit_retransmit_queue(). The congestion control module can either
    request a particular number of segments in TSO skb that we transmit,
    or return 0 if it doesn't care.
    
    This allows the upcoming BBR congestion control module to select small
    TSO skb sizes if the module detects that the bottleneck bandwidth is
    very low, or that the connection is policed to a low rate.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a69ed7f0030c..f8f581fd05f5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -913,6 +913,8 @@ struct tcp_congestion_ops {
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);
+	/* suggest number of segments for each skb to transmit (optional) */
+	u32 (*tso_segs_goal)(struct sock *sk);
 	/* get info for inet_diag (optional) */
 	size_t (*get_info)(struct sock *sk, u32 ext, int *attr,
 			   union tcp_cc_info *info);

commit d7722e8570fc0f1e003cee7cf37694041828918b
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Mon Sep 19 23:39:15 2016 -0400

    tcp: track application-limited rate samples
    
    This commit adds code to track whether the delivery rate represented
    by each rate_sample was limited by the application.
    
    Upon each transmit, we store in the is_app_limited field in the skb a
    boolean bit indicating whether there is a known "bubble in the pipe":
    a point in the rate sample interval where the sender was
    application-limited, and did not transmit even though the cwnd and
    pacing rate allowed it.
    
    This logic marks the flow app-limited on a write if *all* of the
    following are true:
    
      1) There is less than 1 MSS of unsent data in the write queue
         available to transmit.
    
      2) There is no packet in the sender's queues (e.g. in fq or the NIC
         tx queue).
    
      3) The connection is not limited by cwnd.
    
      4) There are no lost packets to retransmit.
    
    The tcp_rate_check_app_limited() code in tcp_rate.c determines whether
    the connection is application-limited at the moment. If the flow is
    application-limited, it sets the tp->app_limited field. If the flow is
    application-limited then that means there is effectively a "bubble" of
    silence in the pipe now, and this silence will be reflected in a lower
    bandwidth sample for any rate samples from now until we get an ACK
    indicating this bubble has exited the pipe: specifically, until we get
    an ACK for the next packet we transmit.
    
    When we send every skb we record in scb->tx.is_app_limited whether the
    resulting rate sample will be application-limited.
    
    The code in tcp_rate_gen() checks to see when it is safe to mark all
    known application-limited bubbles of silence as having exited the
    pipe. It does this by checking to see when the delivered count moves
    past the tp->app_limited marker. At this point it zeroes the
    tp->app_limited marker, as all known bubbles are out of the pipe.
    
    We make room for the tx.is_app_limited bit in the skb by borrowing a
    bit from the in_flight field used by NV to record the number of bytes
    in flight. The receive window in the TCP header is 16 bits, and the
    max receive window scaling shift factor is 14 (RFC 1323). So the max
    receive window offered by the TCP protocol is 2^(16+14) = 2^30. So we
    only need 30 bits for the tx.in_flight used by NV.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b261c892605a..a69ed7f0030c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -764,7 +764,9 @@ struct tcp_skb_cb {
 	union {
 		struct {
 			/* There is space for up to 24 bytes */
-			__u32 in_flight;/* Bytes in flight when packet sent */
+			__u32 in_flight:30,/* Bytes in flight at transmit */
+			      is_app_limited:1, /* cwnd not fully used? */
+			      unused:1;
 			/* pkts S/ACKed so far upon tx of skb, incl retrans: */
 			__u32 delivered;
 			/* start of send pipeline phase */
@@ -883,6 +885,7 @@ struct rate_sample {
 	int  losses;		/* number of packets marked lost upon ACK */
 	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */
 	u32  prior_in_flight;	/* in flight before this ACK */
+	bool is_app_limited;	/* is sample from packet with bubble in pipe? */
 	bool is_retrans;	/* is sample from retransmission? */
 };
 
@@ -978,6 +981,7 @@ void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,
 			    struct rate_sample *rs);
 void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
 		  struct skb_mstamp *now, struct rate_sample *rs);
+void tcp_rate_check_app_limited(struct sock *sk);
 
 /* These functions determine how the current flow behaves in respect of SACK
  * handling. SACK is negotiated with the peer, and therefore it can vary

commit b9f64820fb226a4e8ab10591f46cecd91ca56b30
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Sep 19 23:39:14 2016 -0400

    tcp: track data delivery rate for a TCP connection
    
    This patch generates data delivery rate (throughput) samples on a
    per-ACK basis. These rate samples can be used by congestion control
    modules, and specifically will be used by TCP BBR in later patches in
    this series.
    
    Key state:
    
    tp->delivered: Tracks the total number of data packets (original or not)
                   delivered so far. This is an already-existing field.
    
    tp->delivered_mstamp: the last time tp->delivered was updated.
    
    Algorithm:
    
    A rate sample is calculated as (d1 - d0)/(t1 - t0) on a per-ACK basis:
    
      d1: the current tp->delivered after processing the ACK
      t1: the current time after processing the ACK
    
      d0: the prior tp->delivered when the acked skb was transmitted
      t0: the prior tp->delivered_mstamp when the acked skb was transmitted
    
    When an skb is transmitted, we snapshot d0 and t0 in its control
    block in tcp_rate_skb_sent().
    
    When an ACK arrives, it may SACK and ACK some skbs. For each SACKed
    or ACKed skb, tcp_rate_skb_delivered() updates the rate_sample struct
    to reflect the latest (d0, t0).
    
    Finally, tcp_rate_gen() generates a rate sample by storing
    (d1 - d0) in rs->delivered and (t1 - t0) in rs->interval_us.
    
    One caveat: if an skb was sent with no packets in flight, then
    tp->delivered_mstamp may be either invalid (if the connection is
    starting) or outdated (if the connection was idle). In that case,
    we'll re-stamp tp->delivered_mstamp.
    
    At first glance it seems t0 should always be the time when an skb was
    transmitted, but actually this could over-estimate the rate due to
    phase mismatch between transmit and ACK events. To track the delivery
    rate, we ensure that if packets are in flight then t0 and and t1 are
    times at which packets were marked delivered.
    
    If the initial and final RTTs are different then one may be corrupted
    by some sort of noise. The noise we see most often is sending gaps
    caused by delayed, compressed, or stretched acks. This either affects
    both RTTs equally or artificially reduces the final RTT. We approach
    this by recording the info we need to compute the initial RTT
    (duration of the "send phase" of the window) when we recorded the
    associated inflight. Then, for a filter to avoid bandwidth
    overestimates, we generalize the per-sample bandwidth computation
    from:
    
        bw = delivered / ack_phase_rtt
    
    to the following:
    
        bw = delivered / max(send_phase_rtt, ack_phase_rtt)
    
    In large-scale experiments, this filtering approach incorporating
    send_phase_rtt is effective at avoiding bandwidth overestimates due to
    ACK compression or stretched ACKs.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2f1648af4d12..b261c892605a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -763,8 +763,14 @@ struct tcp_skb_cb {
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 	union {
 		struct {
-			/* There is space for up to 20 bytes */
+			/* There is space for up to 24 bytes */
 			__u32 in_flight;/* Bytes in flight when packet sent */
+			/* pkts S/ACKed so far upon tx of skb, incl retrans: */
+			__u32 delivered;
+			/* start of send pipeline phase */
+			struct skb_mstamp first_tx_mstamp;
+			/* when we reached the "delivered" count */
+			struct skb_mstamp delivered_mstamp;
 		} tx;   /* only used for outgoing skbs */
 		union {
 			struct inet_skb_parm	h4;
@@ -860,6 +866,26 @@ struct ack_sample {
 	u32 in_flight;
 };
 
+/* A rate sample measures the number of (original/retransmitted) data
+ * packets delivered "delivered" over an interval of time "interval_us".
+ * The tcp_rate.c code fills in the rate sample, and congestion
+ * control modules that define a cong_control function to run at the end
+ * of ACK processing can optionally chose to consult this sample when
+ * setting cwnd and pacing rate.
+ * A sample is invalid if "delivered" or "interval_us" is negative.
+ */
+struct rate_sample {
+	struct	skb_mstamp prior_mstamp; /* starting timestamp for interval */
+	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
+	s32  delivered;		/* number of packets delivered over interval */
+	long interval_us;	/* time for tp->delivered to incr "delivered" */
+	long rtt_us;		/* RTT of last (S)ACKed packet (or -1) */
+	int  losses;		/* number of packets marked lost upon ACK */
+	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */
+	u32  prior_in_flight;	/* in flight before this ACK */
+	bool is_retrans;	/* is sample from retransmission? */
+};
+
 struct tcp_congestion_ops {
 	struct list_head	list;
 	u32 key;
@@ -946,6 +972,13 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
+/* From tcp_rate.c */
+void tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb);
+void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,
+			    struct rate_sample *rs);
+void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
+		  struct skb_mstamp *now, struct rate_sample *rs);
+
 /* These functions determine how the current flow behaves in respect of SACK
  * handling. SACK is negotiated with the peer, and therefore it can vary
  * between different flows.

commit 6403389211e1f4d40ed963fe47a96fce1a3ba7a9
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Sep 19 23:39:10 2016 -0400

    tcp: use windowed min filter library for TCP min_rtt estimation
    
    Refactor the TCP min_rtt code to reuse the new win_minmax library in
    lib/win_minmax.c to simplify the TCP code.
    
    This is a pure refactor: the functionality is exactly the same. We
    just moved the windowed min code to make TCP easier to read and
    maintain, and to allow other parts of the kernel to use the windowed
    min/max filter code.
    
    Signed-off-by: Van Jacobson <vanj@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index fdfbedd61c67..2f1648af4d12 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -671,7 +671,7 @@ static inline bool tcp_ca_dst_locked(const struct dst_entry *dst)
 /* Minimum RTT in usec. ~0 means not available. */
 static inline u32 tcp_min_rtt(const struct tcp_sock *tp)
 {
-	return tp->rtt_min[0].rtt;
+	return minmax_get(&tp->rtt_min);
 }
 
 /* Compute the actual receive window we are currently advertising.

commit 9f5afeae51526b3ad7b7cb21ee8b145ce6ea7a7a
Author: Yaogong Wang <wygivan@google.com>
Date:   Wed Sep 7 14:49:28 2016 -0700

    tcp: use an RB tree for ooo receive queue
    
    Over the years, TCP BDP has increased by several orders of magnitude,
    and some people are considering to reach the 2 Gbytes limit.
    
    Even with current window scale limit of 14, ~1 Gbytes maps to ~740,000
    MSS.
    
    In presence of packet losses (or reorders), TCP stores incoming packets
    into an out of order queue, and number of skbs sitting there waiting for
    the missing packets to be received can be in the 10^5 range.
    
    Most packets are appended to the tail of this queue, and when
    packets can finally be transferred to receive queue, we scan the queue
    from its head.
    
    However, in presence of heavy losses, we might have to find an arbitrary
    point in this queue, involving a linear scan for every incoming packet,
    throwing away cpu caches.
    
    This patch converts it to a RB tree, to get bounded latencies.
    
    Yaogong wrote a preliminary patch about 2 years ago.
    Eric did the rebase, added ofo_last_skb cache, polishing and tests.
    
    Tested with network dropping between 1 and 10 % packets, with good
    success (about 30 % increase of throughput in stress tests)
    
    Next step would be to also use an RB tree for the write queue at sender
    side ;)
    
    Signed-off-by: Yaogong Wang <wygivan@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Acked-By: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d6ae36512429..fdfbedd61c67 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -640,7 +640,7 @@ static inline void tcp_fast_path_check(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (skb_queue_empty(&tp->out_of_order_queue) &&
+	if (RB_EMPTY_ROOT(&tp->out_of_order_queue) &&
 	    tp->rcv_wnd &&
 	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
 	    !tp->urg_data)

commit 6abdd5f5935fff978f950561f3c5175eb34dad73
Merge: 0b498a527783 e4e98c460ad3
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Aug 30 00:54:02 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    All three conflicts were cases of simple overlapping
    changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c9c3321257e1b95be9b375f811fb250162af8d39
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Aug 27 07:37:54 2016 -0700

    tcp: add tcp_add_backlog()
    
    When TCP operates in lossy environments (between 1 and 10 % packet
    losses), many SACK blocks can be exchanged, and I noticed we could
    drop them on busy senders, if these SACK blocks have to be queued
    into the socket backlog.
    
    While the main cause is the poor performance of RACK/SACK processing,
    we can try to avoid these drops of valuable information that can lead to
    spurious timeouts and retransmits.
    
    Cause of the drops is the skb->truesize overestimation caused by :
    
    - drivers allocating ~2048 (or more) bytes as a fragment to hold an
      Ethernet frame.
    
    - various pskb_may_pull() calls bringing the headers into skb->head
      might have pulled all the frame content, but skb->truesize could
      not be lowered, as the stack has no idea of each fragment truesize.
    
    The backlog drops are also more visible on bidirectional flows, since
    their sk_rmem_alloc can be quite big.
    
    Let's add some room for the backlog, as only the socket owner
    can selectively take action to lower memory needs, like collapsing
    receive queues or partial ofo pruning.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a5af6be3a572..dd99679e2e51 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1161,6 +1161,7 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 }
 
 bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
+bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);
 
 #undef STATE_TRACE
 

commit 3203558589a597e0a10a66b258fbc5a4a6659ed0
Author: Tom Herbert <tom@herbertland.com>
Date:   Sun Aug 28 14:43:18 2016 -0700

    tcp: Set read_sock and peek_len proto_ops
    
    In inet_stream_ops we set read_sock to tcp_read_sock and peek_len to
    tcp_peek_len (which is just a stub function that calls tcp_inq).
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d56666ad9249..a5af6be3a572 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1848,6 +1848,8 @@ static inline int tcp_inq(struct sock *sk)
 	return answ;
 }
 
+int tcp_peek_len(struct socket *sock);
+
 static inline void tcp_segs_in(struct tcp_sock *tp, const struct sk_buff *skb)
 {
 	u16 segs_in;

commit 0294b625ad5a6d1fb50632d67cf384862d8a4a46
Author: Tom Herbert <tom@herbertland.com>
Date:   Sun Aug 28 14:43:17 2016 -0700

    net: Add read_sock proto_op
    
    Add new function in proto_ops structure. This includes moving the
    typedef got sk_read_actor into net.h and removing the definition from
    tcp.h.
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 25d64f6de69e..d56666ad9249 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -603,8 +603,6 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 void tcp_get_info(struct sock *, struct tcp_info *);
 
 /* Read 'sendfile()'-style from a TCP socket */
-typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
-				unsigned int, size_t);
 int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 		  sk_read_actor_t recv_actor);
 

commit cebc5cbab48f5c58512e26aa1965284354227258
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Aug 22 17:17:54 2016 -0700

    net-tcp: retire TFO_SERVER_WO_SOCKOPT2 config
    
    TFO_SERVER_WO_SOCKOPT2 was intended for debugging purposes during
    Fast Open development. Remove this config option and also
    update/clean-up the documentation of the Fast Open sysctl.
    
    Reported-by: Piotr Jurkiewicz <piotr.jerzy.jurkiewicz@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c00e7d51bb18..25d64f6de69e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -227,10 +227,9 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define	TFO_SERVER_COOKIE_NOT_REQD	0x200
 
 /* Force enable TFO on all listeners, i.e., not requiring the
- * TCP_FASTOPEN socket option. SOCKOPT1/2 determine how to set max_qlen.
+ * TCP_FASTOPEN socket option.
  */
 #define	TFO_SERVER_WO_SOCKOPT1	0x400
-#define	TFO_SERVER_WO_SOCKOPT2	0x800
 
 extern struct inet_timewait_death_row tcp_death_row;
 

commit bb1fceca22492109be12640d49f5ea5a544c6bb4
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Aug 17 05:56:26 2016 -0700

    tcp: fix use after free in tcp_xmit_retransmit_queue()
    
    When tcp_sendmsg() allocates a fresh and empty skb, it puts it at the
    tail of the write queue using tcp_add_write_queue_tail()
    
    Then it attempts to copy user data into this fresh skb.
    
    If the copy fails, we undo the work and remove the fresh skb.
    
    Unfortunately, this undo lacks the change done to tp->highest_sack and
    we can leave a dangling pointer (to a freed skb)
    
    Later, tcp_xmit_retransmit_queue() can dereference this pointer and
    access freed memory. For regular kernels where memory is not unmapped,
    this might cause SACK bugs because tcp_highest_sack_seq() is buggy,
    returning garbage instead of tp->snd_nxt, but with various debug
    features like CONFIG_DEBUG_PAGEALLOC, this can crash the kernel.
    
    This bug was found by Marco Grassi thanks to syzkaller.
    
    Fixes: 6859d49475d4 ("[TCP]: Abstract tp->highest_sack accessing & point to next skb")
    Reported-by: Marco Grassi <marco.gra@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c00e7d51bb18..7717302cab91 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1523,6 +1523,8 @@ static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unli
 {
 	if (sk->sk_send_head == skb_unlinked)
 		sk->sk_send_head = NULL;
+	if (tcp_sk(sk)->highest_sack == skb_unlinked)
+		tcp_sk(sk)->highest_sack = NULL;
 }
 
 static inline void tcp_init_send_head(struct sock *sk)

commit 19689e38eca5d7b32755182d4e62efd7a5376c45
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jun 27 18:51:53 2016 +0200

    tcp: md5: use kmalloc() backed scratch areas
    
    Some arches have virtually mapped kernel stacks, or will soon have.
    
    tcp_md5_hash_header() uses an automatic variable to copy tcp header
    before mangling th->check and calling crypto function, which might
    be problematic on such arches.
    
    David says that using percpu storage is also problematic on non SMP
    builds.
    
    Just use kmalloc() to allocate scratch areas.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d825858fe4f1..c00e7d51bb18 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1384,7 +1384,7 @@ union tcp_md5sum_block {
 /* - pool: digest algorithm, hash description and scratch buffer */
 struct tcp_md5sig_pool {
 	struct ahash_request	*md5_req;
-	union tcp_md5sum_block	md5_blk;
+	void			*scratch;
 };
 
 /* - functions */
@@ -1420,7 +1420,6 @@ static inline void tcp_put_md5sig_pool(void)
 	local_bh_enable();
 }
 
-int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
 int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
 			  unsigned int header_len);
 int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,

commit 2631b79f6cb8b634fe41b77de9c4add0ec6b3cae
Author: Seymour, Shane M <shane.seymour@hpe.com>
Date:   Tue Jun 28 23:06:48 2016 +0000

    tcp: increase size at which tcp_bound_to_half_wnd bounds to > TCP_MSS_DEFAULT
    
    In previous commit 01f83d69844d307be2aa6fea88b0e8fe5cbdb2f4
    the following comments were added:
    
    "When peer uses tiny windows, there is no use in packetizing to sub-MSS
    pieces for the sake of SWS or making sure there are enough packets in
    the pipe for fast recovery."
    
    The test should be > TCP_MSS_DEFAULT not >= 512. This allows low end
    devices that send an MSS of 536 (TCP_MSS_DEFAULT) to see better network
    performance by sending it 536 bytes of data at a time instead of bounding
    to half window size (268). Other network stacks work this way, e.g. HP-UX.
    
    Signed-off-by: Shane Seymour <shane.seymour@hpe.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a79894b66726..d825858fe4f1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -589,7 +589,7 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 	 * On the other hand, for extremely large MSS devices, handling
 	 * smaller than MSS windows in this way does make sense.
 	 */
-	if (tp->max_window >= 512)
+	if (tp->max_window > TCP_MSS_DEFAULT)
 		cutoff = (tp->max_window >> 1);
 	else
 		cutoff = tp->max_window;

commit 6f094b9ec680209c5b7314feee983b2f4c910b1b
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Wed Jun 8 21:16:44 2016 -0700

    tcp: add in_flight to tcp_skb_cb
    
    Add in_flight (bytes in flight when packet was sent) field
    to tx component of tcp_skb_cb and make it available to
    congestion modules' pkts_acked() function through the
    ack_sample function argument.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0bcc70f4e1fb..a79894b66726 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -767,6 +767,7 @@ struct tcp_skb_cb {
 	union {
 		struct {
 			/* There is space for up to 20 bytes */
+			__u32 in_flight;/* Bytes in flight when packet sent */
 		} tx;   /* only used for outgoing skbs */
 		union {
 			struct inet_skb_parm	h4;
@@ -859,6 +860,7 @@ union tcp_cc_info;
 struct ack_sample {
 	u32 pkts_acked;
 	s32 rtt_us;
+	u32 in_flight;
 };
 
 struct tcp_congestion_ops {

commit 74b20582ac389ee9f18a6fcc0eef244658ce8de0
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Tue May 10 11:19:50 2016 -0700

    net: l3mdev: Add hook in ip and ipv6
    
    Currently the VRF driver uses the rx_handler to switch the skb device
    to the VRF device. Switching the dev prior to the ip / ipv6 layer
    means the VRF driver has to duplicate IP/IPv6 processing which adds
    overhead and makes features such as retaining the ingress device index
    more complicated than necessary.
    
    This patch moves the hook to the L3 layer just after the first NF_HOOK
    for PRE_ROUTING. This location makes exposing the original ingress device
    trivial (next patch) and allows adding other NF_HOOKs to the VRF driver
    in the future.
    
    dev_queue_xmit_nit is exported so that the VRF driver can cycle the skb
    with the switched device through the packet taps to maintain current
    behavior (tcpdump can be used on either the vrf device or the enslaved
    devices).
    
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c9ab561387c4..0bcc70f4e1fb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -786,7 +786,9 @@ struct tcp_skb_cb {
  */
 static inline int tcp_v6_iif(const struct sk_buff *skb)
 {
-	return TCP_SKB_CB(skb)->header.h6.iif;
+	bool l3_slave = skb_l3mdev_slave(TCP_SKB_CB(skb)->header.h6.flags);
+
+	return l3_slave ? skb->skb_iif : TCP_SKB_CB(skb)->header.h6.iif;
 }
 #endif
 

commit 756ee1729b2feb3a45767da29e338f70f2086ba3
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Wed May 11 10:02:13 2016 -0700

    tcp: replace cnt & rtt with struct in pkts_acked()
    
    Replace 2 arguments (cnt and rtt) in the congestion control modules'
    pkts_acked() function with a struct. This will allow adding more
    information without having to modify existing congestion control
    modules (tcp_nv in particular needs bytes in flight when packet
    was sent).
    
    As proposed by Neal Cardwell in his comments to the tcp_nv patch.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4775a1bba7f7..c9ab561387c4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -854,6 +854,11 @@ enum tcp_ca_ack_event_flags {
 
 union tcp_cc_info;
 
+struct ack_sample {
+	u32 pkts_acked;
+	s32 rtt_us;
+};
+
 struct tcp_congestion_ops {
 	struct list_head	list;
 	u32 key;
@@ -877,7 +882,7 @@ struct tcp_congestion_ops {
 	/* new value of cwnd after loss (optional) */
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
-	void (*pkts_acked)(struct sock *sk, u32 num_acked, s32 rtt_us);
+	void (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);
 	/* get info for inet_diag (optional) */
 	size_t (*get_info)(struct sock *sk, u32 ext, int *attr,
 			   union tcp_cc_info *info);

commit b75803d52a2ce1f6cbaf7ae0ae40a369210070cf
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri May 6 20:35:35 2016 -0700

    tcp: refactor struct tcp_skb_cb
    
    Refactor tcp_skb_cb to create two overlaping areas to store
    state for incoming or outgoing skbs based on comments by
    Neal Cardwell to tcp_nv patch:
    
       AFAICT this patch would not require an increase in the size of
       sk_buff cb[] if it were to take advantage of the fact that the
       tcp_skb_cb header.h4 and header.h6 fields are only used in the packet
       reception code path, and this in_flight field is only used on the
       transmit side.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 24ec80483805..4775a1bba7f7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -765,11 +765,16 @@ struct tcp_skb_cb {
 			unused:6;
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 	union {
-		struct inet_skb_parm	h4;
+		struct {
+			/* There is space for up to 20 bytes */
+		} tx;   /* only used for outgoing skbs */
+		union {
+			struct inet_skb_parm	h4;
 #if IS_ENABLED(CONFIG_IPV6)
-		struct inet6_skb_parm	h6;
+			struct inet6_skb_parm	h6;
 #endif
-	} header;	/* For incoming frames		*/
+		} header;	/* For incoming skbs */
+	};
 };
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))

commit c134ecb87817ce70fd62b2dc48bb079c44fc08df
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Apr 25 14:44:48 2016 -0700

    tcp: Make use of MSG_EOR in tcp_sendmsg
    
    This patch adds an eor bit to the TCP_SKB_CB.  When MSG_EOR
    is passed to tcp_sendmsg, the eor bit will be set at the skb
    containing the last byte of the userland's msg.  The eor bit
    will prevent data from appending to that skb in the future.
    
    The change in do_tcp_sendpages is to honor the eor set
    during the previous tcp_sendmsg(MSG_EOR) call.
    
    This patch handles the tcp_sendmsg case.  The followup patches
    will handle other skb coalescing and fragment cases.
    
    One potential use case is to use MSG_EOR with
    SOF_TIMESTAMPING_TX_ACK to get a more accurate
    TCP ack timestamping on application protocol with
    multiple outgoing response messages (e.g. HTTP2).
    
    Packetdrill script for testing:
    ~~~~~~
    +0 `sysctl -q -w net.ipv4.tcp_min_tso_segs=10`
    +0 `sysctl -q -w net.ipv4.tcp_no_metrics_save=1`
    +0 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
    +0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
    +0 bind(3, ..., ...) = 0
    +0 listen(3, 1) = 0
    
    0.100 < S 0:0(0) win 32792 <mss 1460,sackOK,nop,nop,nop,wscale 7>
    0.100 > S. 0:0(0) ack 1 <mss 1460,nop,nop,sackOK,nop,wscale 7>
    0.200 < . 1:1(0) ack 1 win 257
    0.200 accept(3, ..., ...) = 4
    +0 setsockopt(4, SOL_TCP, TCP_NODELAY, [1], 4) = 0
    
    0.200 write(4, ..., 14600) = 14600
    0.200 sendto(4, ..., 730, MSG_EOR, ..., ...) = 730
    0.200 sendto(4, ..., 730, MSG_EOR, ..., ...) = 730
    
    0.200 > .  1:7301(7300) ack 1
    0.200 > P. 7301:14601(7300) ack 1
    
    0.300 < . 1:1(0) ack 14601 win 257
    0.300 > P. 14601:15331(730) ack 1
    0.300 > P. 15331:16061(730) ack 1
    
    0.400 < . 1:1(0) ack 16061 win 257
    0.400 close(4) = 0
    0.400 > F. 16061:16061(0) ack 1
    0.400 < F. 1:1(0) ack 16062 win 257
    0.400 > . 16062:16062(0) ack 2
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 992f317c1abe..24ec80483805 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -761,7 +761,8 @@ struct tcp_skb_cb {
 
 	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
 	__u8		txstamp_ack:1,	/* Record TX timestamp for ack? */
-			unused:7;
+			eor:1,		/* Is skb MSG_EOR marked? */
+			unused:6;
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 	union {
 		struct inet_skb_parm	h4;
@@ -808,6 +809,11 @@ static inline int tcp_skb_mss(const struct sk_buff *skb)
 	return TCP_SKB_CB(skb)->tcp_gso_size;
 }
 
+static inline bool tcp_skb_can_collapse_to(const struct sk_buff *skb)
+{
+	return likely(!TCP_SKB_CB(skb)->eor);
+}
+
 /* Events passed to congestion control interface */
 enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */

commit 13415e46c5915e2dac089de516369005fbc045f9
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 27 16:44:43 2016 -0700

    net: snmp: kill STATS_BH macros
    
    There is nothing related to BH in SNMP counters anymore,
    since linux-3.0.
    
    Rename helpers to use __ prefix instead of _BH prefix,
    for contexts where preemption is disabled.
    
    This more closely matches convention used to update
    percpu variables.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ff8b4265cb2b..992f317c1abe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -332,7 +332,7 @@ bool tcp_check_oom(struct sock *sk, int shift);
 extern struct proto tcp_prot;
 
 #define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
-#define __TCP_INC_STATS(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
+#define __TCP_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.tcp_statistics, field)
 #define TCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mib.tcp_statistics, field)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 

commit 02a1d6e7a6bb025a77da77012190e1efc1970f1c
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 27 16:44:39 2016 -0700

    net: rename NET_{ADD|INC}_STATS_BH()
    
    Rename NET_INC_STATS_BH() to __NET_INC_STATS()
    and NET_ADD_STATS_BH() to __NET_ADD_STATS()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 939ebd5320a9..ff8b4265cb2b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1743,7 +1743,7 @@ static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
 					 __u16 *mss)
 {
 	tcp_synq_overflow(sk);
-	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);
+	__NET_INC_STATS(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);
 	return ops->cookie_init_seq(skb, mss);
 }
 #else
@@ -1852,7 +1852,7 @@ static inline void tcp_segs_in(struct tcp_sock *tp, const struct sk_buff *skb)
 static inline void tcp_listendrop(const struct sock *sk)
 {
 	atomic_inc(&((struct sock *)sk)->sk_drops);
-	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+	__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);
 }
 
 #endif	/* _TCP_H */

commit 90bbcc608369a1b46089b0f5aa22b8ea31ffa12e
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 27 16:44:32 2016 -0700

    net: tcp: rename TCP_INC_STATS_BH
    
    Rename TCP_INC_STATS_BH() to __TCP_INC_STATS()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cfe15f712164..939ebd5320a9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -332,7 +332,7 @@ bool tcp_check_oom(struct sock *sk, int shift);
 extern struct proto tcp_prot;
 
 #define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
-#define TCP_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
+#define __TCP_INC_STATS(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
 #define TCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mib.tcp_statistics, field)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 

commit 6aef70a851ac77967992340faaff33f44598f60a
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 27 16:44:27 2016 -0700

    net: snmp: kill various STATS_USER() helpers
    
    In the old days (before linux-3.0), SNMP counters were duplicated,
    one for user context, and one for BH context.
    
    After commit 8f0ea0fe3a03 ("snmp: reduce percpu needs by 50%")
    we have a single copy, and what really matters is preemption being
    enabled or disabled, since we use this_cpu_inc() or __this_cpu_inc()
    respectively.
    
    We therefore kill SNMP_INC_STATS_USER(), SNMP_ADD_STATS_USER(),
    NET_INC_STATS_USER(), NET_ADD_STATS_USER(), SCTP_INC_STATS_USER(),
    SNMP_INC_STATS64_USER(), SNMP_ADD_STATS64_USER(), TCP_ADD_STATS_USER(),
    UDP_INC_STATS_USER(), UDP6_INC_STATS_USER(), and XFRM_INC_STATS_USER()
    
    Following patches will rename __BH helpers to make clear their
    usage is not tied to BH being disabled.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7f2553da10d1..cfe15f712164 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -334,7 +334,6 @@ extern struct proto tcp_prot;
 #define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
 #define TCP_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
 #define TCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mib.tcp_statistics, field)
-#define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
 void tcp_tasklet_init(void);
@@ -1298,10 +1297,10 @@ bool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,
 static inline void tcp_mib_init(struct net *net)
 {
 	/* See RFC 2012 */
-	TCP_ADD_STATS_USER(net, TCP_MIB_RTOALGORITHM, 1);
-	TCP_ADD_STATS_USER(net, TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);
-	TCP_ADD_STATS_USER(net, TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);
-	TCP_ADD_STATS_USER(net, TCP_MIB_MAXCONN, -1);
+	TCP_ADD_STATS(net, TCP_MIB_RTOALGORITHM, 1);
+	TCP_ADD_STATS(net, TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);
+	TCP_ADD_STATS(net, TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);
+	TCP_ADD_STATS(net, TCP_MIB_MAXCONN, -1);
 }
 
 /* from STCP */

commit 10d3be569243def8d92ac3722395ef5a59c504e6
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Apr 21 10:55:23 2016 -0700

    tcp-tso: do not split TSO packets at retransmit time
    
    Linux TCP stack painfully segments all TSO/GSO packets before retransmits.
    
    This was fine back in the days when TSO/GSO were emerging, with their
    bugs, but we believe the dark age is over.
    
    Keeping big packets in write queues, but also in stack traversal
    has a lot of benefits.
     - Less memory overhead, because write queues have less skbs
     - Less cpu overhead at ACK processing.
     - Better SACK processing, as lot of studies mentioned how
       awful linux was at this ;)
     - Less cpu overhead to send the rtx packets
       (IP stack traversal, netfilter traversal, drivers...)
     - Better latencies in presence of losses.
     - Smaller spikes in fq like packet schedulers, as retransmits
       are not constrained by TCP Small Queues.
    
    1 % packet losses are common today, and at 100Gbit speeds, this
    translates to ~80,000 losses per second.
    Losses are often correlated, and we see many retransmit events
    leading to 1-MSS train of packets, at the time hosts are already
    under stress.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c0ef0544dfcf..7f2553da10d1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -538,8 +538,8 @@ __u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mss);
 void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 			       int nonagle);
 bool tcp_may_send_now(struct sock *sk);
-int __tcp_retransmit_skb(struct sock *, struct sk_buff *);
-int tcp_retransmit_skb(struct sock *, struct sk_buff *);
+int __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
+int tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);
 void tcp_retransmit_timer(struct sock *sk);
 void tcp_xmit_retransmit_queue(struct sock *);
 void tcp_simple_retransmit(struct sock *);

commit 1602f49b58abcb0d34a5f0a29d68e7c1769547aa
Merge: 22d37b6b0058 5f44abd041c5
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Apr 23 18:26:24 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts were two cases of simple overlapping changes,
    nothing serious.
    
    In the UDP case, we need to add a hlist_add_tail_rcu()
    to linux/rculist.h, because we've moved UDP socket handling
    away from using nulls lists.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cfea5a688eb37bcd1081255df9f9f777f4e61999
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue Apr 19 22:39:29 2016 -0700

    tcp: Merge tx_flags and tskey in tcp_shifted_skb
    
    After receiving sacks, tcp_shifted_skb() will collapse
    skbs if possible.  tx_flags and tskey also have to be
    merged.
    
    This patch reuses the tcp_skb_collapse_tstamp() to handle
    them.
    
    BPF Output Before:
    ~~~~~
    <no-output-due-to-missing-tstamp-event>
    
    BPF Output After:
    ~~~~~
    <...>-2024  [007] d.s.    88.644374: : ee_data:14599
    
    Packetdrill Script:
    ~~~~~
    +0 `sysctl -q -w net.ipv4.tcp_min_tso_segs=10`
    +0 `sysctl -q -w net.ipv4.tcp_no_metrics_save=1`
    +0 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
    +0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
    +0 bind(3, ..., ...) = 0
    +0 listen(3, 1) = 0
    
    0.100 < S 0:0(0) win 32792 <mss 1460,sackOK,nop,nop,nop,wscale 7>
    0.100 > S. 0:0(0) ack 1 <mss 1460,nop,nop,sackOK,nop,wscale 7>
    0.200 < . 1:1(0) ack 1 win 257
    0.200 accept(3, ..., ...) = 4
    +0 setsockopt(4, SOL_TCP, TCP_NODELAY, [1], 4) = 0
    
    0.200 write(4, ..., 1460) = 1460
    +0 setsockopt(4, SOL_SOCKET, 37, [2688], 4) = 0
    0.200 write(4, ..., 13140) = 13140
    
    0.200 > P. 1:1461(1460) ack 1
    0.200 > . 1461:8761(7300) ack 1
    0.200 > P. 8761:14601(5840) ack 1
    
    0.300 < . 1:1(0) ack 1 win 257 <sack 1461:14601,nop,nop>
    0.300 > P. 1:1461(1460) ack 1
    0.400 < . 1:1(0) ack 14601 win 257
    
    0.400 close(4) = 0
    0.400 > F. 14601:14601(0) ack 1
    0.500 < F. 1:1(0) ack 14602 win 257
    0.500 > . 14602:14602(0) ack 2
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Tested-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b91370f61be6..6db10228113f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -552,6 +552,8 @@ void tcp_send_ack(struct sock *sk);
 void tcp_send_delayed_ack(struct sock *sk);
 void tcp_send_loss_probe(struct sock *sk);
 bool tcp_schedule_loss_probe(struct sock *sk);
+void tcp_skb_collapse_tstamp(struct sk_buff *skb,
+			     const struct sk_buff *next_skb);
 
 /* tcp_input.c */
 void tcp_resume_early_retransmit(struct sock *sk);

commit b3d051477cf94e9d71d6acadb8a90de15237b9c1
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 13 22:05:39 2016 -0700

    tcp: do not mess with listener sk_wmem_alloc
    
    When removing sk_refcnt manipulation on synflood, I missed that
    using skb_set_owner_w() was racy, if sk->sk_wmem_alloc had already
    transitioned to 0.
    
    We should hold sk_refcnt instead, but this is a big deal under attack.
    (Doing so increase performance from 3.2 Mpps to 3.8 Mpps only)
    
    In this patch, I chose to not attach a socket to syncookies skb.
    
    Performance is now 5 Mpps instead of 3.2 Mpps.
    
    Following patch will remove last known false sharing in
    tcp_rcv_state_process()
    
    Fixes: 3b24d854cb35 ("tcp/dccp: do not touch listener sk_refcnt under synflood")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 74d3ed5eb219..fd40f8c64d5f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -452,10 +452,15 @@ struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
 int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
 int tcp_connect(struct sock *sk);
+enum tcp_synack_type {
+	TCP_SYNACK_NORMAL,
+	TCP_SYNACK_FASTOPEN,
+	TCP_SYNACK_COOKIE,
+};
 struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,
 				struct request_sock *req,
 				struct tcp_fastopen_cookie *foc,
-				bool attach_req);
+				enum tcp_synack_type synack_type);
 int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
@@ -1728,7 +1733,7 @@ struct tcp_request_sock_ops {
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   struct tcp_fastopen_cookie *foc,
-			   bool attach_req);
+			   enum tcp_synack_type synack_type);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit 9caad864151e525929d323de96cad382da49c3b2
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 1 08:52:20 2016 -0700

    tcp: increment sk_drops for listeners
    
    Goal: packets dropped by a listener are accounted for.
    
    This adds tcp_listendrop() helper, and clears sk_drops in sk_clone_lock()
    so that children do not inherit their parent drop count.
    
    Note that we no longer increment LINUX_MIB_LISTENDROPS counter when
    sending a SYNCOOKIE, since the SYN packet generated a SYNACK.
    We already have a separate LINUX_MIB_SYNCOOKIESSENT
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a23282996ca9..74d3ed5eb219 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1836,4 +1836,17 @@ static inline void tcp_segs_in(struct tcp_sock *tp, const struct sk_buff *skb)
 		tp->data_segs_in += segs_in;
 }
 
+/*
+ * TCP listen path runs lockless.
+ * We forced "struct sock" to be const qualified to make sure
+ * we don't modify one of its field by mistake.
+ * Here, we increment sk_drops which is an atomic_t, so we can safely
+ * make sock writable again.
+ */
+static inline void tcp_listendrop(const struct sock *sk)
+{
+	atomic_inc(&((struct sock *)sk)->sk_drops);
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_LISTENDROPS);
+}
+
 #endif	/* _TCP_H */

commit 6b084928baac562ed61866f540a96120e9c9ddb7
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Sat Apr 2 23:08:08 2016 -0400

    tcp: use one bit in TCP_SKB_CB to mark ACK timestamps
    
    Currently, to avoid a cache line miss for accessing skb_shinfo,
    tcp_ack_tstamp skips socket that do not have
    SOF_TIMESTAMPING_TX_ACK bit set in sk_tsflags. This is
    implemented based on an implicit assumption that the
    SOF_TIMESTAMPING_TX_ACK is set via socket options for the
    duration that ACK timestamps are needed.
    
    To implement per-write timestamps, this check should be
    removed and replaced with a per-packet alternative that
    quickly skips packets missing ACK timestamps marks without
    a cache-line miss.
    
    To enable per-packet marking without a cache line miss, use
    one bit in TCP_SKB_CB to mark a whether a SKB might need a
    ack tx timestamp or not. Further checks in tcp_ack_tstamp are not
    modified and work as before.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f8bb4a4ed3d1..a23282996ca9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -754,7 +754,8 @@ struct tcp_skb_cb {
 				TCPCB_REPAIRED)
 
 	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
-	/* 1 byte hole */
+	__u8		txstamp_ack:1,	/* Record TX timestamp for ack? */
+			unused:7;
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 	union {
 		struct inet_skb_parm	h4;

commit 2349262397b89a421adfd142aad2a7dd33710f26
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Mar 30 14:54:20 2016 -0700

    tcp: remove cwnd moderation after recovery
    
    For non-SACK connections, cwnd is lowered to inflight plus 3 packets
    when the recovery ends. This is an optional feature in the NewReno
    RFC 2582 to reduce the potential burst when cwnd is "re-opened"
    after recovery and inflight is low.
    
    This feature is questionably effective because of PRR: when
    the recovery ends (i.e., snd_una == high_seq) NewReno holds the
    CA_Recovery state for another round trip to prevent false fast
    retransmits. But if the inflight is low, PRR will overwrite the
    moderated cwnd in tcp_cwnd_reduction() later regardlessly. So if a
    receiver responds bogus ACKs (i.e., acking future data) to speed up
    transfer after recovery, it can only induce a burst up to a window
    worth of data packets by acking up to SND.NXT. A restart from (short)
    idle or receiving streched ACKs can both cause such bursts as well.
    
    On the other hand, if the recovery ends because the sender
    detects the losses were spurious (e.g., reordering). This feature
    unconditionally lowers a reverted cwnd even though nothing
    was lost.
    
    By principle loss recovery module should not update cwnd. Further
    pacing is much more effective to reduce burst. Hence this patch
    removes the cwnd moderation feature.
    
    v2 changes: revised commit message on bogus ACKs and burst, and
                missing signature
    
    Signed-off-by: Matt Mathis <mattmathis@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b91370f61be6..f8bb4a4ed3d1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1039,17 +1039,6 @@ static inline __u32 tcp_max_tso_deferred_mss(const struct tcp_sock *tp)
 	return 3;
 }
 
-/* Slow start with delack produces 3 packets of burst, so that
- * it is safe "de facto".  This will be the default - same as
- * the default reordering threshold - but if reordering increases,
- * we must be able to allow cwnd to burst at least this much in order
- * to not pull it back when holes are filled.
- */
-static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
-{
-	return tp->reordering;
-}
-
 /* Returns end sequence number of the receiver's advertised window */
 static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 {

commit 1200b6809dfd9d73bc4c7db76d288c35fa4b2ebe
Merge: 6b5f04b6cf8e fe30937b6535
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 10:05:34 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support more Realtek wireless chips, from Jes Sorenson.
    
       2) New BPF types for per-cpu hash and arrap maps, from Alexei
          Starovoitov.
    
       3) Make several TCP sysctls per-namespace, from Nikolay Borisov.
    
       4) Allow the use of SO_REUSEPORT in order to do per-thread processing
       of incoming TCP/UDP connections.  The muxing can be done using a
       BPF program which hashes the incoming packet.  From Craig Gallek.
    
       5) Add a multiplexer for TCP streams, to provide a messaged based
          interface.  BPF programs can be used to determine the message
          boundaries.  From Tom Herbert.
    
       6) Add 802.1AE MACSEC support, from Sabrina Dubroca.
    
       7) Avoid factorial complexity when taking down an inetdev interface
          with lots of configured addresses.  We were doing things like
          traversing the entire address less for each address removed, and
          flushing the entire netfilter conntrack table for every address as
          well.
    
       8) Add and use SKB bulk free infrastructure, from Jesper Brouer.
    
       9) Allow offloading u32 classifiers to hardware, and implement for
          ixgbe, from John Fastabend.
    
      10) Allow configuring IRQ coalescing parameters on a per-queue basis,
          from Kan Liang.
    
      11) Extend ethtool so that larger link mode masks can be supported.
          From David Decotigny.
    
      12) Introduce devlink, which can be used to configure port link types
          (ethernet vs Infiniband, etc.), port splitting, and switch device
          level attributes as a whole.  From Jiri Pirko.
    
      13) Hardware offload support for flower classifiers, from Amir Vadai.
    
      14) Add "Local Checksum Offload".  Basically, for a tunneled packet
          the checksum of the outer header is 'constant' (because with the
          checksum field filled into the inner protocol header, the payload
          of the outer frame checksums to 'zero'), and we can take advantage
          of that in various ways.  From Edward Cree"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1548 commits)
      bonding: fix bond_get_stats()
      net: bcmgenet: fix dma api length mismatch
      net/mlx4_core: Fix backward compatibility on VFs
      phy: mdio-thunder: Fix some Kconfig typos
      lan78xx: add ndo_get_stats64
      lan78xx: handle statistics counter rollover
      RDS: TCP: Remove unused constant
      RDS: TCP: Add sysctl tunables for sndbuf/rcvbuf on rds-tcp socket
      net: smc911x: convert pxa dma to dmaengine
      team: remove duplicate set of flag IFF_MULTICAST
      bonding: remove duplicate set of flag IFF_MULTICAST
      net: fix a comment typo
      ethernet: micrel: fix some error codes
      ip_tunnels, bpf: define IP_TUNNEL_OPTS_MAX and use it
      bpf, dst: add and use dst_tclassid helper
      bpf: make skb->tc_classid also readable
      net: mvneta: bm: clarify dependencies
      cls_bpf: reset class and reuse major in da
      ldmvsw: Checkpatch sunvnet.c and sunvnet_common.c
      ldmvsw: Add ldmvsw.c driver code
      ...

commit 70477371dc350746d10431d74f0f213a8d59924c
Merge: 09fd671ccb24 34074205bb9f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 11:22:54 2016 -0700

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto update from Herbert Xu:
     "Here is the crypto update for 4.6:
    
      API:
       - Convert remaining crypto_hash users to shash or ahash, also convert
         blkcipher/ablkcipher users to skcipher.
       - Remove crypto_hash interface.
       - Remove crypto_pcomp interface.
       - Add crypto engine for async cipher drivers.
       - Add akcipher documentation.
       - Add skcipher documentation.
    
      Algorithms:
       - Rename crypto/crc32 to avoid name clash with lib/crc32.
       - Fix bug in keywrap where we zero the wrong pointer.
    
      Drivers:
       - Support T5/M5, T7/M7 SPARC CPUs in n2 hwrng driver.
       - Add PIC32 hwrng driver.
       - Support BCM6368 in bcm63xx hwrng driver.
       - Pack structs for 32-bit compat users in qat.
       - Use crypto engine in omap-aes.
       - Add support for sama5d2x SoCs in atmel-sha.
       - Make atmel-sha available again.
       - Make sahara hashing available again.
       - Make ccp hashing available again.
       - Make sha1-mb available again.
       - Add support for multiple devices in ccp.
       - Improve DMA performance in caam.
       - Add hashing support to rockchip"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (116 commits)
      crypto: qat - remove redundant arbiter configuration
      crypto: ux500 - fix checks of error code returned by devm_ioremap_resource()
      crypto: atmel - fix checks of error code returned by devm_ioremap_resource()
      crypto: qat - Change the definition of icp_qat_uof_regtype
      hwrng: exynos - use __maybe_unused to hide pm functions
      crypto: ccp - Add abstraction for device-specific calls
      crypto: ccp - CCP versioning support
      crypto: ccp - Support for multiple CCPs
      crypto: ccp - Remove check for x86 family and model
      crypto: ccp - memset request context to zero during import
      lib/mpi: use "static inline" instead of "extern inline"
      lib/mpi: avoid assembler warning
      hwrng: bcm63xx - fix non device tree compatibility
      crypto: testmgr - allow rfc3686 aes-ctr variants in fips mode.
      crypto: qat - The AE id should be less than the maximal AE number
      lib/mpi: Endianness fix
      crypto: rockchip - add hash support for crypto engine in rk3288
      crypto: xts - fix compile errors
      crypto: doc - add skcipher API documentation
      crypto: doc - update AEAD AD handling
      ...

commit a44d6eacdaf56f74fad699af7f4925a5f5ac0e7f
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Mar 14 10:52:15 2016 -0700

    tcp: Add RFC4898 tcpEStatsPerfDataSegsOut/In
    
    Per RFC4898, they count segments sent/received
    containing a positive length data segment (that includes
    retransmission segments carrying data).  Unlike
    tcpi_segs_out/in, tcpi_data_segs_out/in excludes segments
    carrying no data (e.g. pure ack).
    
    The patch also updates the segs_in in tcp_fastopen_add_skb()
    so that segs_in >= data_segs_in property is kept.
    
    Together with retransmission data, tcpi_data_segs_out
    gives a better signal on the rxmit rate.
    
    v6: Rebase on the latest net-next
    
    v5: Eric pointed out that checking skb->len is still needed in
    tcp_fastopen_add_skb() because skb can carry a FIN without data.
    Hence, instead of open coding segs_in and data_segs_in, tcp_segs_in()
    helper is used.  Comment is added to the fastopen case to explain why
    segs_in has to be reset and tcp_segs_in() has to be called before
    __skb_pull().
    
    v4: Add comment to the changes in tcp_fastopen_add_skb()
    and also add remark on this case in the commit message.
    
    v3: Add const modifier to the skb parameter in tcp_segs_in()
    
    v2: Rework based on recent fix by Eric:
    commit a9d99ce28ed3 ("tcp: fix tcpi_segs_in after connection establishment")
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Chris Rapier <rapier@psc.edu>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Marcelo Ricardo Leitner <mleitner@redhat.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0302636af98c..c8dbd293daae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1840,4 +1840,14 @@ static inline int tcp_inq(struct sock *sk)
 	return answ;
 }
 
+static inline void tcp_segs_in(struct tcp_sock *tp, const struct sk_buff *skb)
+{
+	u16 segs_in;
+
+	segs_in = max_t(u16, 1, skb_shinfo(skb)->gso_segs);
+	tp->segs_in += segs_in;
+	if (skb->len > tcp_hdrlen(skb))
+		tp->data_segs_in += segs_in;
+}
+
 #endif	/* _TCP_H */

commit 473bd239b808a8af5241ce9996a16d283d88ddff
Author: Tom Herbert <tom@herbertland.com>
Date:   Mon Mar 7 14:11:05 2016 -0800

    tcp: Add tcp_inq to get available receive bytes on socket
    
    Create a common kernel function to get the number of bytes available
    on a TCP socket. This is based on code in INQ getsockopt and we now call
    the function for that getsockopt.
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e90db8546806..0302636af98c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1816,4 +1816,28 @@ static inline void skb_set_tcp_pure_ack(struct sk_buff *skb)
 	skb->truesize = 2;
 }
 
+static inline int tcp_inq(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int answ;
+
+	if ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
+		answ = 0;
+	} else if (sock_flag(sk, SOCK_URGINLINE) ||
+		   !tp->urg_data ||
+		   before(tp->urg_seq, tp->copied_seq) ||
+		   !before(tp->urg_seq, tp->rcv_nxt)) {
+
+		answ = tp->rcv_nxt - tp->copied_seq;
+
+		/* Subtract 1, if FIN was received */
+		if (answ && sock_flag(sk, SOCK_DONE))
+			answ--;
+	} else {
+		answ = tp->urg_seq - tp->copied_seq;
+	}
+
+	return answ;
+}
+
 #endif	/* _TCP_H */

commit b633353115e352d3c31c12d4c61978c810f05ea1
Merge: b1d95ae5c5bd dea08e604408
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Feb 23 00:09:14 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/phy/bcm7xxx.c
            drivers/net/phy/marvell.c
            drivers/net/vxlan.c
    
    All three conflicts were cases of simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9cf7490360bf2c46a16b7525f899e4970c5fc144
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 2 19:31:12 2016 -0800

    tcp: do not drop syn_recv on all icmp reports
    
    Petr Novopashenniy reported that ICMP redirects on SYN_RECV sockets
    were leading to RST.
    
    This is of course incorrect.
    
    A specific list of ICMP messages should be able to drop a SYN_RECV.
    
    For instance, a REDIRECT on SYN_RECV shall be ignored, as we do
    not hold a dst per SYN_RECV pseudo request.
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=111751
    Fixes: 079096f103fa ("tcp/dccp: install syn_recv requests into ehash table")
    Reported-by: Petr Novopashenniy <pety@rusnet.ru>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f6f8f032c73e..ae6468f5c9f3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -447,7 +447,7 @@ const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
 void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 void tcp_v4_mtu_reduced(struct sock *sk);
-void tcp_req_err(struct sock *sk, u32 seq);
+void tcp_req_err(struct sock *sk, u32 seq, bool abort);
 int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_create_openreq_child(const struct sock *sk,
 				      struct request_sock *req,

commit 4979f2d9f7262b9b180bc83de8d70f7a7721c085
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:57 2016 +0200

    ipv4: Namespaceify tcp_notsent_lowat sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3f160c2e6960..9b2cb0c8d876 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -267,7 +267,6 @@ extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
 extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
-extern unsigned int sysctl_tcp_notsent_lowat;
 extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_min_rtt_wlen;
 extern int sysctl_tcp_autocorking;
@@ -1682,7 +1681,8 @@ void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr);
 
 static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 {
-	return tp->notsent_lowat ?: sysctl_tcp_notsent_lowat;
+	struct net *net = sock_net((struct sock *)tp);
+	return tp->notsent_lowat ?: net->ipv4.sysctl_tcp_notsent_lowat;
 }
 
 static inline bool tcp_stream_memory_free(const struct sock *sk)

commit 1e579caa18b96f9eb18f4f5416658cd15f37c062
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:56 2016 +0200

    ipv4: Namespaceify tcp_fin_timeout sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 71f840b89c76..3f160c2e6960 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -239,7 +239,6 @@ extern struct inet_timewait_death_row tcp_death_row;
 extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
-extern int sysctl_tcp_fin_timeout;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
@@ -1249,7 +1248,7 @@ static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)
 
 static inline int tcp_fin_time(const struct sock *sk)
 {
-	int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;
+	int fin_timeout = tcp_sk(sk)->linger2 ? : sock_net(sk)->ipv4.sysctl_tcp_fin_timeout;
 	const int rto = inet_csk(sk)->icsk_rto;
 
 	if (fin_timeout < (rto << 2) - (rto >> 1))

commit c402d9beffb6141ab2e4d2ad8be71128803a28ca
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:55 2016 +0200

    ipv4: Namespaceify tcp_orphan_retries sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a786cfa6301b..71f840b89c76 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_orphan_retries;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;

commit c6214a97c86c660de4f7ddb8eed925192e646161
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:54 2016 +0200

    ipv4: Namespaceify tcp_retries2 sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index da96b9af3e5f..a786cfa6301b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_retries2;
 extern int sysctl_tcp_orphan_retries;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;

commit ae5c3f406cffe15ffd2aa544961b7cd027468d46
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:53 2016 +0200

    ipv4: Namespaceify tcp_retries1 sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7e9a147cabae..da96b9af3e5f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_retries1;
 extern int sysctl_tcp_retries2;
 extern int sysctl_tcp_orphan_retries;
 extern int sysctl_tcp_fastopen;

commit 1043e25ff96a1efc7bd34d11f5f32203a28a3bd7
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:52 2016 +0200

    ipv4: Namespaceify tcp reordering sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1fb23b70d237..7e9a147cabae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -961,9 +961,11 @@ static inline void tcp_enable_fack(struct tcp_sock *tp)
  */
 static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
 {
+	struct net *net = sock_net((struct sock *)tp);
+
 	tp->do_early_retrans = sysctl_tcp_early_retrans &&
 		sysctl_tcp_early_retrans < 4 && !sysctl_tcp_thin_dupack &&
-		sysctl_tcp_reordering == 3;
+		net->ipv4.sysctl_tcp_reordering == 3;
 }
 
 static inline void tcp_disable_early_retrans(struct tcp_sock *tp)

commit 12ed8244ed8b31b023ea6d2851fd8b15f2999e9b
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:51 2016 +0200

    ipv4: Namespaceify tcp syncookies sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 05659e860039..1fb23b70d237 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -243,7 +243,6 @@ extern int sysctl_tcp_fin_timeout;
 extern int sysctl_tcp_retries1;
 extern int sysctl_tcp_retries2;
 extern int sysctl_tcp_orphan_retries;
-extern int sysctl_tcp_syncookies;
 extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;

commit 7c083ecb3ba4583a625d5ff9655d1a819e374493
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:50 2016 +0200

    ipv4: Namespaceify tcp synack retries sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 825485c7cc1a..05659e860039 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_synack_retries;
 extern int sysctl_tcp_retries1;
 extern int sysctl_tcp_retries2;
 extern int sysctl_tcp_orphan_retries;

commit 6fa251663069e05daadd1666cbf3b658bf840ea4
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Wed Feb 3 09:46:49 2016 +0200

    ipv4: Namespaceify tcp syn retries sysctl knob
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 479d535609fd..825485c7cc1a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_syn_retries;
 extern int sysctl_tcp_synack_retries;
 extern int sysctl_tcp_retries1;
 extern int sysctl_tcp_retries2;

commit e3e17b773bfe45462b7f3fae20c550025975cb13
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 6 11:16:28 2016 -0800

    tcp: fastopen: call tcp_fin() if FIN present in SYNACK
    
    When we acknowledge a FIN, it is not enough to ack the sequence number
    and queue the skb into receive queue. We also have to call tcp_fin()
    to properly update socket state and send proper poll() notifications.
    
    It seems we also had the problem if we received a SYN packet with the
    FIN flag set, but it does not seem an urgent issue, as no known
    implementation can do that.
    
    Fixes: 61d2bcae99f6 ("tcp: fastopen: accept data/FIN present in SYNACK message")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 27f4c733116d..479d535609fd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -568,6 +568,7 @@ void tcp_rearm_rto(struct sock *sk);
 void tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req);
 void tcp_reset(struct sock *sk);
 void tcp_skb_mark_lost_uncond_verify(struct tcp_sock *tp, struct sk_buff *skb);
+void tcp_fin(struct sock *sk);
 
 /* tcp_timer.c */
 void tcp_init_xmit_timers(struct sock *);

commit 61d2bcae99f66a640b3dd9632180209143fb5512
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Feb 1 21:03:07 2016 -0800

    tcp: fastopen: accept data/FIN present in SYNACK message
    
    RFC 7413 (TCP Fast Open) 4.2.2 states that the SYNACK message
    MAY include data and/or FIN
    
    This patch adds support for the client side :
    
    If we receive a SYNACK with payload or FIN, queue the skb instead
    of ignoring it.
    
    Since we already support the same for SYN, we refactor the existing
    code and reuse it. Note we need to clone the skb, so this operation
    might fail under memory pressure.
    
    Sara Dickinson pointed out FreeBSD server Fast Open implementation
    was planned to generate such SYNACK in the future.
    
    The server side might be implemented on linux later.
    
    Reported-by: Sara Dickinson <sara@sinodun.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f6f8f032c73e..27f4c733116d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1437,6 +1437,7 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
+void tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
 			      struct request_sock *req,
 			      struct tcp_fastopen_cookie *foc,

commit 21603fc45bdef3696f45d80a1c9d730e06b925c5
Author: Jörg Thalheim <joerg@higgsboson.tk>
Date:   Wed Jan 27 22:54:06 2016 +0100

    tcp: Change reference to experimental CWND RFC.
    
    Signed-off-by: Jörg Thalheim <joerg@higgsboson.tk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8ea19977ea53..f6f8f032c73e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -216,7 +216,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* TCP thin-stream limits */
 #define TCP_THIN_LINEAR_RETRIES 6       /* After 6 linear retries, do exp. backoff */
 
-/* TCP initial congestion window as per draft-hkchu-tcpm-initcwnd-01 */
+/* TCP initial congestion window as per rfc6928 */
 #define TCP_INIT_CWND		10
 
 /* Bit Flags for sysctl_tcp_fastopen */

commit cf80e0e47e0e7a8994dfadefec0e1395c622817a
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Jan 24 21:20:23 2016 +0800

    tcp: Use ahash
    
    This patch replaces uses of the long obsolete hash interface with
    ahash.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Acked-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8ea19977ea53..2a5b3b8daee8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -27,7 +27,6 @@
 #include <linux/cache.h>
 #include <linux/percpu.h>
 #include <linux/skbuff.h>
-#include <linux/crypto.h>
 #include <linux/cryptohash.h>
 #include <linux/kref.h>
 #include <linux/ktime.h>
@@ -1325,9 +1324,6 @@ static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
 	tp->retransmit_skb_hint = NULL;
 }
 
-/* MD5 Signature */
-struct crypto_hash;
-
 union tcp_md5_addr {
 	struct in_addr  a4;
 #if IS_ENABLED(CONFIG_IPV6)
@@ -1376,7 +1372,7 @@ union tcp_md5sum_block {
 
 /* - pool: digest algorithm, hash description and scratch buffer */
 struct tcp_md5sig_pool {
-	struct hash_desc	md5_desc;
+	struct ahash_request	*md5_req;
 	union tcp_md5sum_block	md5_blk;
 };
 

commit baac50bbc3cdfd184ebf586b1704edbfcee866df
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:17 2016 -0800

    net: tcp_memcontrol: simplify linkage between socket and page counter
    
    There won't be any separate counters for socket memory consumed by
    protocols other than TCP in the future.  Remove the indirection and link
    sockets directly to their owning memory cgroup.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d9df80deba31..8ea19977ea53 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -289,8 +289,8 @@ extern int tcp_memory_pressure;
 /* optimized version of sk_under_memory_pressure() for TCP sockets */
 static inline bool tcp_under_memory_pressure(const struct sock *sk)
 {
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
-	    mem_cgroup_under_socket_pressure(sk->sk_cgrp))
+	if (mem_cgroup_sockets_enabled && sk->sk_memcg &&
+	    mem_cgroup_under_socket_pressure(sk->sk_memcg))
 		return true;
 
 	return tcp_memory_pressure;

commit e805605c721021879a1469bdae45c6f80bc985f4
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:14 2016 -0800

    net: tcp_memcontrol: sanitize tcp memory accounting callbacks
    
    There won't be a tcp control soft limit, so integrating the memcg code
    into the global skmem limiting scheme complicates things unnecessarily.
    Replace this with simple and clear charge and uncharge calls--hidden
    behind a jump label--to account skb memory.
    
    Note that this is not purely aesthetic: as a result of shoehorning the
    per-memcg code into the same memory accounting functions that handle the
    global level, the old code would compare the per-memcg consumption
    against the smaller of the per-memcg limit and the global limit.  This
    allowed the total consumption of multiple sockets to exceed the global
    limit, as long as the individual sockets stayed within bounds.  After
    this change, the code will always compare the per-memcg consumption to
    the per-memcg limit, and the global consumption to the global limit, and
    thus close this loophole.
    
    Without a soft limit, the per-memcg memory pressure state in sockets is
    generally questionable.  However, we did it until now, so we continue to
    enter it when the hard limit is hit, and packets are dropped, to let
    other sockets in the cgroup know that they shouldn't grow their transmit
    windows, either.  However, keep it simple in the new callback model and
    leave memory pressure lazily when the next packet is accepted (as
    opposed to doing it synchroneously when packets are processed).  When
    packets are dropped, network performance will already be in the toilet,
    so that should be a reasonable trade-off.
    
    As described above, consumption is now checked on the per-memcg level
    and the global level separately.  Likewise, memory pressure states are
    maintained on both the per-memcg level and the global level, and a
    socket is considered under pressure when either level asserts as much.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a80255f4ca33..d9df80deba31 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -289,8 +289,9 @@ extern int tcp_memory_pressure;
 /* optimized version of sk_under_memory_pressure() for TCP sockets */
 static inline bool tcp_under_memory_pressure(const struct sock *sk)
 {
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return !!sk->sk_cgrp->memory_pressure;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
+	    mem_cgroup_under_socket_pressure(sk->sk_cgrp))
+		return true;
 
 	return tcp_memory_pressure;
 }

commit b840d15d39128d08ed4486085e5507d2617b9ae1
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Thu Jan 7 16:38:45 2016 +0200

    ipv4: Namespecify the tcp_keepalive_intvl sysctl knob
    
    This is the final part required to namespaceify the tcp
    keep alive mechanism.
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0646521400bf..a80255f4ca33 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_keepalive_intvl;
 extern int sysctl_tcp_syn_retries;
 extern int sysctl_tcp_synack_retries;
 extern int sysctl_tcp_retries1;
@@ -1223,7 +1222,9 @@ void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 {
-	return tp->keepalive_intvl ? : sysctl_tcp_keepalive_intvl;
+	struct net *net = sock_net((struct sock *)tp);
+
+	return tp->keepalive_intvl ? : net->ipv4.sysctl_tcp_keepalive_intvl;
 }
 
 static inline int keepalive_time_when(const struct tcp_sock *tp)

commit 9bd6861bd4326e3afd3f14a9ec8a723771fb20bb
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Thu Jan 7 16:38:44 2016 +0200

    ipv4: Namespecify tcp_keepalive_probes sysctl knob
    
    This is required to have full tcp keepalive mechanism namespace
    support.
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cb4d4cf25744..0646521400bf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_keepalive_probes;
 extern int sysctl_tcp_keepalive_intvl;
 extern int sysctl_tcp_syn_retries;
 extern int sysctl_tcp_synack_retries;
@@ -1236,7 +1235,9 @@ static inline int keepalive_time_when(const struct tcp_sock *tp)
 
 static inline int keepalive_probes(const struct tcp_sock *tp)
 {
-	return tp->keepalive_probes ? : sysctl_tcp_keepalive_probes;
+	struct net *net = sock_net((struct sock *)tp);
+
+	return tp->keepalive_probes ? : net->ipv4.sysctl_tcp_keepalive_probes;
 }
 
 static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)

commit 13b287e8d1cad951634389f85b8c9b816bd3bb1e
Author: Nikolay Borisov <kernel@kyup.com>
Date:   Thu Jan 7 16:38:43 2016 +0200

    ipv4: Namespaceify tcp_keepalive_time sysctl knob
    
    Different net namespaces might have different requirements as to
    the keepalive time of tcp sockets. This might be required in cases
    where different firewall rules are in place which require tcp
    timeout sockets to be increased/decreased independently of the host.
    
    Signed-off-by: Nikolay Borisov <kernel@kyup.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f33fecf4e282..cb4d4cf25744 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -240,7 +240,6 @@ extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_keepalive_time;
 extern int sysctl_tcp_keepalive_probes;
 extern int sysctl_tcp_keepalive_intvl;
 extern int sysctl_tcp_syn_retries;
@@ -1230,7 +1229,9 @@ static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 
 static inline int keepalive_time_when(const struct tcp_sock *tp)
 {
-	return tp->keepalive_time ? : sysctl_tcp_keepalive_time;
+	struct net *net = sock_net((struct sock *)tp);
+
+	return tp->keepalive_time ? : net->ipv4.sysctl_tcp_keepalive_time;
 }
 
 static inline int keepalive_probes(const struct tcp_sock *tp)

commit b1f0a0e99c58fbd7ea053ca36ba623718272b618
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Dec 21 21:29:24 2015 +0100

    net: add inet_sk_transparent() helper
    
    Avoids cluttering tcp_v4_send_reset when followup patch extends
    it to deal with timewait sockets.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3077735b348d..f33fecf4e282 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1620,6 +1620,18 @@ static inline void tcp_highest_sack_combine(struct sock *sk,
 		tcp_sk(sk)->highest_sack = new;
 }
 
+/* This helper checks if socket has IP_TRANSPARENT set */
+static inline bool inet_sk_transparent(const struct sock *sk)
+{
+	switch (sk->sk_state) {
+	case TCP_TIME_WAIT:
+		return inet_twsk(sk)->tw_transparent;
+	case TCP_NEW_SYN_RECV:
+		return inet_rsk(inet_reqsk(sk))->no_srccheck;
+	}
+	return inet_sk(sk)->transparent;
+}
+
 /* Determines whether this is a thin stream (which may suffer from
  * increased latency). Used to trigger latency-reducing mechanisms.
  */

commit c1e64e298b8cad309091b95d8436a0255c84f54a
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Wed Dec 16 12:30:05 2015 +0900

    net: diag: Support destroying TCP sockets.
    
    This implements SOCK_DESTROY for TCP sockets. It causes all
    blocking calls on the socket to fail fast with ECONNABORTED and
    causes a protocol close of the socket. It informs the other end
    of the connection by sending a RST, i.e., initiating a TCP ABORT
    as per RFC 793. ECONNABORTED was chosen for consistency with
    FreeBSD.
    
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f80e74c5ad18..3077735b348d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1170,6 +1170,8 @@ void tcp_set_state(struct sock *sk, int state);
 
 void tcp_done(struct sock *sk);
 
+int tcp_abort(struct sock *sk, int err);
+
 static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 {
 	rx_opt->dsack = 0;

commit 5e0724d027f0548511a2165a209572d48fe7a4c8
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 22 08:20:46 2015 -0700

    tcp/dccp: fix hashdance race for passive sessions
    
    Multiple cpus can process duplicates of incoming ACK messages
    matching a SYN_RECV request socket. This is a rare event under
    normal operations, but definitely can happen.
    
    Only one must win the race, otherwise corruption would occur.
    
    To fix this without adding new atomic ops, we use logic in
    inet_ehash_nolisten() to detect the request was present in the same
    ehash bucket where we try to insert the new child.
    
    If request socket was not found, we have to undo the child creation.
    
    This actually removes a spin_lock()/spin_unlock() pair in
    reqsk_queue_unlink() for the fast path.
    
    Fixes: e994b2f0fb92 ("tcp: do not lock listener to process SYN packets")
    Fixes: 079096f103fa ("tcp/dccp: install syn_recv requests into ehash table")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 11e320412216..f80e74c5ad18 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -457,7 +457,9 @@ struct sock *tcp_create_openreq_child(const struct sock *sk,
 void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);
 struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
 				  struct request_sock *req,
-				  struct dst_entry *dst);
+				  struct dst_entry *dst,
+				  struct request_sock *req_unhash,
+				  bool *own_req);
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
 int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
 int tcp_connect(struct sock *sk);

commit 4f41b1c58a32537542f14c1150099131613a5e8a
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:47 2015 -0700

    tcp: use RACK to detect losses
    
    This patch implements the second half of RACK that uses the the most
    recent transmit time among all delivered packets to detect losses.
    
    tcp_rack_mark_lost() is called upon receiving a dubious ACK.
    It then checks if an not-yet-sacked packet was sent at least
    "reo_wnd" prior to the sent time of the most recently delivered.
    If so the packet is deemed lost.
    
    The "reo_wnd" reordering window starts with 1msec for fast loss
    detection and changes to min-RTT/4 when reordering is observed.
    We found 1msec accommodates well on tiny degree of reordering
    (<3 pkts) on faster links. We use min-RTT instead of SRTT because
    reordering is more of a path property but SRTT can be inflated by
    self-inflicated congestion. The factor of 4 is borrowed from the
    delayed early retransmit and seems to work reasonably well.
    
    Since RACK is still experimental, it is now used as a supplemental
    loss detection on top of existing algorithms. It is only effective
    after the fast recovery starts or after the timeout occurs. The
    fast recovery is still triggered by FACK and/or dupack threshold
    instead of RACK.
    
    We introduce a new sysctl net.ipv4.tcp_recovery for future
    experiments of loss recoveries. For now RACK can be disabled by
    setting it to 0.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c3a9fe057d3..11e320412216 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -567,6 +567,7 @@ void tcp_resume_early_retransmit(struct sock *sk);
 void tcp_rearm_rto(struct sock *sk);
 void tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req);
 void tcp_reset(struct sock *sk);
+void tcp_skb_mark_lost_uncond_verify(struct tcp_sock *tp, struct sk_buff *skb);
 
 /* tcp_timer.c */
 void tcp_init_xmit_timers(struct sock *);
@@ -1752,6 +1753,14 @@ void tcp_init(void);
 
 /* tcp_recovery.c */
 
+/* Flags to enable various loss recovery features. See below */
+extern int sysctl_tcp_recovery;
+
+/* Use TCP RACK to detect (some) tail and retransmit losses */
+#define TCP_RACK_LOST_RETRANS  0x1
+
+extern int tcp_rack_mark_lost(struct sock *sk);
+
 extern void tcp_rack_advance(struct tcp_sock *tp,
 			     const struct skb_mstamp *xmit_time, u8 sacked);
 

commit 659a8ad56f490279f0efee43a62ffa1ac914a4e0
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:46 2015 -0700

    tcp: track the packet timings in RACK
    
    This patch is the first half of the RACK loss recovery.
    
    RACK loss recovery uses the notion of time instead
    of packet sequence (FACK) or counts (dupthresh). It's inspired by the
    previous FACK heuristic in tcp_mark_lost_retrans(): when a limited
    transmit (new data packet) is sacked, then current retransmitted
    sequence below the newly sacked sequence must been lost,
    since at least one round trip time has elapsed.
    
    But it has several limitations:
    1) can't detect tail drops since it depends on limited transmit
    2) is disabled upon reordering (assumes no reordering)
    3) only enabled in fast recovery ut not timeout recovery
    
    RACK (Recently ACK) addresses these limitations with the notion
    of time instead: a packet P1 is lost if a later packet P2 is s/acked,
    as at least one round trip has passed.
    
    Since RACK cares about the time sequence instead of the data sequence
    of packets, it can detect tail drops when later retransmission is
    s/acked while FACK or dupthresh can't. For reordering RACK uses a
    dynamically adjusted reordering window ("reo_wnd") to reduce false
    positives on ever (small) degree of reordering.
    
    This patch implements tcp_advanced_rack() which tracks the
    most recent transmission time among the packets that have been
    delivered (ACKed or SACKed) in tp->rack.mstamp. This timestamp
    is the key to determine which packet has been lost.
    
    Consider an example that the sender sends six packets:
    T1: P1 (lost)
    T2: P2
    T3: P3
    T4: P4
    T100: sack of P2. rack.mstamp = T2
    T101: retransmit P1
    T102: sack of P2,P3,P4. rack.mstamp = T4
    T205: ACK of P4 since the hole is repaired. rack.mstamp = T101
    
    We need to be careful about spurious retransmission because it may
    falsely advance tp->rack.mstamp by an RTT or an RTO, causing RACK
    to falsely mark all packets lost, just like a spurious timeout.
    
    We identify spurious retransmission by the ACK's TS echo value.
    If TS option is not applicable but the retransmission is acknowledged
    less than min-RTT ago, it is likely to be spurious. We refrain from
    using the transmission time of these spurious retransmissions.
    
    The second half is implemented in the next patch that marks packet
    lost using RACK timestamp.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4a43152229ea..3c3a9fe057d3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1750,6 +1750,11 @@ int tcpv4_offload_init(void);
 void tcp_v4_init(void);
 void tcp_init(void);
 
+/* tcp_recovery.c */
+
+extern void tcp_rack_advance(struct tcp_sock *tp,
+			     const struct skb_mstamp *xmit_time, u8 sacked);
+
 /*
  * Save and compile IPv4 options, return a pointer to it
  */

commit f672258391b42a5c7cc2732c9c063e56a85c8dbe
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Oct 16 21:57:42 2015 -0700

    tcp: track min RTT using windowed min-filter
    
    Kathleen Nichols' algorithm for tracking the minimum RTT of a
    data stream over some measurement window. It uses constant space
    and constant time per update. Yet it almost always delivers
    the same minimum as an implementation that has to keep all
    the data in the window. The measurement window is tunable via
    sysctl.net.ipv4.tcp_min_rtt_wlen with a default value of 5 minutes.
    
    The algorithm keeps track of the best, 2nd best & 3rd best min
    values, maintaining an invariant that the measurement time of
    the n'th best >= n-1'th best. It also makes sure that the three
    values are widely separated in the time window since that bounds
    the worse case error when that data is monotonically increasing
    over the window.
    
    Upon getting a new min, we can forget everything earlier because
    it has no value - the new min is less than everything else in the
    window by definition and it's the most recent. So we restart fresh
    on every new min and overwrites the 2nd & 3rd choices. The same
    property holds for the 2nd & 3rd best.
    
    Therefore we have to maintain two invariants to maximize the
    information in the samples, one on values (1st.v <= 2nd.v <=
    3rd.v) and the other on times (now-win <=1st.t <= 2nd.t <= 3rd.t <=
    now). These invariants determine the structure of the code
    
    The RTT input to the windowed filter is the minimum RTT measured
    from ACK or SACK, or as the last resort from TCP timestamps.
    
    The accessor tcp_min_rtt() returns the minimum RTT seen in the
    window. ~0U indicates it is not available. The minimum is 1usec
    even if the true RTT is below that.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index eed94fc355c1..4a43152229ea 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -279,6 +279,7 @@ extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
 extern unsigned int sysctl_tcp_notsent_lowat;
 extern int sysctl_tcp_min_tso_segs;
+extern int sysctl_tcp_min_rtt_wlen;
 extern int sysctl_tcp_autocorking;
 extern int sysctl_tcp_invalid_ratelimit;
 extern int sysctl_tcp_pacing_ss_ratio;
@@ -671,6 +672,12 @@ static inline bool tcp_ca_dst_locked(const struct dst_entry *dst)
 	return dst_metric_locked(dst, RTAX_CC_ALGO);
 }
 
+/* Minimum RTT in usec. ~0 means not available. */
+static inline u32 tcp_min_rtt(const struct tcp_sock *tp)
+{
+	return tp->rtt_min[0].rtt;
+}
+
 /* Compute the actual receive window we are currently advertising.
  * Rcv_nxt can be after the window if our peer push more data
  * than the offered window.

commit dc6ef6be52154490c5c03f742e28bc781cc751b2
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 16 13:00:01 2015 -0700

    tcp: do not set queue_mapping on SYNACK
    
    At the time of commit fff326990789 ("tcp: reflect SYN queue_mapping into
    SYNACK packets") we had little ways to cope with SYN floods.
    
    We no longer need to reflect incoming skb queue mappings, and instead
    can pick a TX queue based on cpu cooking the SYNACK, with normal XPS
    affinities.
    
    Note that all SYNACK retransmits were picking TX queue 0, this no longer
    is a win given that SYNACK rtx are now distributed on all cpus.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a6be56d5f0e3..eed94fc355c1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1716,7 +1716,7 @@ struct tcp_request_sock_ops {
 	__u32 (*init_seq)(const struct sk_buff *skb);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
-			   u16 queue_mapping, struct tcp_fastopen_cookie *foc,
+			   struct tcp_fastopen_cookie *foc,
 			   bool attach_req);
 };
 

commit ca6fb06518836ef9b65dc0aac02ff97704d52a05
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:35 2015 -0700

    tcp: attach SYNACK messages to request sockets instead of listener
    
    If a listen backlog is very big (to avoid syncookies), then
    the listener sk->sk_wmem_alloc is the main source of false
    sharing, as we need to touch it twice per SYNACK re-transmit
    and TX completion.
    
    (One SYN packet takes listener lock once, but up to 6 SYNACK
    are generated)
    
    By attaching the skb to the request socket, we remove this
    source of contention.
    
    Tested:
    
     listen(fd, 10485760); // single listener (no SO_REUSEPORT)
     16 RX/TX queue NIC
     Sustain a SYNFLOOD attack of ~320,000 SYN per second,
     Sending ~1,400,000 SYNACK per second.
     Perf profiles now show listener spinlock being next bottleneck.
    
        20.29%  [kernel]  [k] queued_spin_lock_slowpath
        10.06%  [kernel]  [k] __inet_lookup_established
         5.12%  [kernel]  [k] reqsk_timer_handler
         3.22%  [kernel]  [k] get_next_timer_interrupt
         3.00%  [kernel]  [k] tcp_make_synack
         2.77%  [kernel]  [k] ipt_do_table
         2.70%  [kernel]  [k] run_timer_softirq
         2.50%  [kernel]  [k] ip_finish_output
         2.04%  [kernel]  [k] cascade
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 225e9561af35..a6be56d5f0e3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -462,7 +462,8 @@ int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
 int tcp_connect(struct sock *sk);
 struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,
 				struct request_sock *req,
-				struct tcp_fastopen_cookie *foc);
+				struct tcp_fastopen_cookie *foc,
+				bool attach_req);
 int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
@@ -1715,7 +1716,8 @@ struct tcp_request_sock_ops {
 	__u32 (*init_seq)(const struct sk_buff *skb);
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
-			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
+			   u16 queue_mapping, struct tcp_fastopen_cookie *foc,
+			   bool attach_req);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit 079096f103faca2dd87342cca6f23d4b34da8871
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:32 2015 -0700

    tcp/dccp: install syn_recv requests into ehash table
    
    In this patch, we insert request sockets into TCP/DCCP
    regular ehash table (where ESTABLISHED and TIMEWAIT sockets
    are) instead of using the per listener hash table.
    
    ACK packets find SYN_RECV pseudo sockets without having
    to find and lock the listener.
    
    In nominal conditions, this halves pressure on listener lock.
    
    Note that this will allow for SO_REUSEPORT refinements,
    so that we can select a listener using cpu/numa affinities instead
    of the prior 'consistent hash', since only SYN packets will
    apply this selection logic.
    
    We will shrink listen_sock in the following patch to ease
    code review.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ying Cai <ycai@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a26341d2ad67..225e9561af35 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1618,7 +1618,6 @@ static inline bool tcp_stream_is_thin(struct tcp_sock *tp)
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,
-	TCP_SEQ_STATE_OPENREQ,
 	TCP_SEQ_STATE_ESTABLISHED,
 };
 
@@ -1717,8 +1716,6 @@ struct tcp_request_sock_ops {
 	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
-	void (*queue_hash_add)(struct sock *sk, struct request_sock *req,
-			       const unsigned long timeout);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit aa3a0c8ce651b5e16124866b0a10d1b90b9ef022
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:30 2015 -0700

    tcp: get_openreq[46]() changes
    
    When request sockets are no longer in a per listener hash table
    but on regular TCP ehash, we need to access listener uid
    through req->rsk_listener
    
    get_openreq6() also gets a const for its request socket argument.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2c7dfe52f473..a26341d2ad67 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1637,7 +1637,6 @@ struct tcp_iter_state {
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;
 	int			bucket, offset, sbucket, num;
-	kuid_t			uid;
 	loff_t			last_pos;
 };
 

commit f964629e3338d9e5a78c9b354380d5a1e2fa4617
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:50 2015 -0700

    tcp: constify tcp_v{4|6}_route_req() sock argument
    
    These functions do not change the listener socket.
    Goal is to make sure tcp_conn_request() is not messing with
    listener in a racy way.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5aa6672c6f5b..2c7dfe52f473 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1711,7 +1711,7 @@ struct tcp_request_sock_ops {
 	__u32 (*cookie_init_seq)(const struct sk_buff *skb,
 				 __u16 *mss);
 #endif
-	struct dst_entry *(*route_req)(struct sock *sk, struct flowi *fl,
+	struct dst_entry *(*route_req)(const struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req,
 				       bool *strict);
 	__u32 (*init_seq)(const struct sk_buff *skb);

commit 3f684b4b1f1c86e3a6ac63389d1032e239fddd79
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:49 2015 -0700

    tcp: cookie_init_sequence() cleanups
    
    Some common IPv4/IPv6 code can be factorized.
    Also constify cookie_init_sequence() socket argument.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a1d2f5d6a430..5aa6672c6f5b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -491,8 +491,9 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
 
 /* syncookies: remember time of last synqueue overflow
  * But do not dirty this field too often (once per second is enough)
+ * It is racy as we do not hold a lock, but race is very minor.
  */
-static inline void tcp_synq_overflow(struct sock *sk)
+static inline void tcp_synq_overflow(const struct sock *sk)
 {
 	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
 	unsigned long now = jiffies;
@@ -519,8 +520,7 @@ static inline u32 tcp_cookie_time(void)
 
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
-__u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
-			      __u16 *mss);
+__u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mss);
 __u32 cookie_init_timestamp(struct request_sock *req);
 bool cookie_timestamp_decode(struct tcp_options_received *opt);
 bool cookie_ecn_ok(const struct tcp_options_received *opt,
@@ -533,8 +533,7 @@ struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 
 u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 			      const struct tcphdr *th, u16 *mssp);
-__u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
-			      __u16 *mss);
+__u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mss);
 #endif
 /* tcp_output.c */
 
@@ -1709,7 +1708,7 @@ struct tcp_request_sock_ops {
 			 const struct sock *sk_listener,
 			 struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
-	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,
+	__u32 (*cookie_init_seq)(const struct sk_buff *skb,
 				 __u16 *mss);
 #endif
 	struct dst_entry *(*route_req)(struct sock *sk, struct flowi *fl,
@@ -1725,14 +1724,16 @@ struct tcp_request_sock_ops {
 
 #ifdef CONFIG_SYN_COOKIES
 static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
-					 struct sock *sk, struct sk_buff *skb,
+					 const struct sock *sk, struct sk_buff *skb,
 					 __u16 *mss)
 {
-	return ops->cookie_init_seq(sk, skb, mss);
+	tcp_synq_overflow(sk);
+	NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);
+	return ops->cookie_init_seq(skb, mss);
 }
 #else
 static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
-					 struct sock *sk, struct sk_buff *skb,
+					 const struct sock *sk, struct sk_buff *skb,
 					 __u16 *mss)
 {
 	return 0;

commit 0c27171e66d94f9121fc00e87407ca7103bb6649
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:48 2015 -0700

    tcp/dccp: constify syn_recv_sock() method sock argument
    
    We'll soon no longer hold listener socket lock, these
    functions do not modify the socket in any way.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 85995c1291d0..a1d2f5d6a430 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -454,7 +454,7 @@ struct sock *tcp_create_openreq_child(const struct sock *sk,
 				      struct request_sock *req,
 				      struct sk_buff *skb);
 void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);
-struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
 				  struct request_sock *req,
 				  struct dst_entry *dst);
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);

commit c28c6f045945f53e842467bf0e86c5fac051643d
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:47 2015 -0700

    tcp: constify tcp_create_openreq_child() socket argument
    
    This method does not touch the listener socket.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1fe0bd458cb4..85995c1291d0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -450,7 +450,7 @@ void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 void tcp_v4_mtu_reduced(struct sock *sk);
 void tcp_req_err(struct sock *sk, u32 seq);
 int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
-struct sock *tcp_create_openreq_child(struct sock *sk,
+struct sock *tcp_create_openreq_child(const struct sock *sk,
 				      struct request_sock *req,
 				      struct sk_buff *skb);
 void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);

commit 72ab4a86f7a260d4c2a320b49662da107ce77a81
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:41 2015 -0700

    tcp: remove tcp_rcv_state_process() tcp_hdr argument
    
    Factorize code to get tcp header from skb. It makes no sense
    to duplicate code in callers.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1cfdedbe47e1..1fe0bd458cb4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -365,8 +365,7 @@ void tcp_wfree(struct sk_buff *skb);
 void tcp_write_timer_handler(struct sock *sk);
 void tcp_delack_timer_handler(struct sock *sk);
 int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
-int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
-			  const struct tcphdr *th);
+int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb);
 void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			 const struct tcphdr *th, unsigned int len);
 void tcp_rcv_space_adjust(struct sock *sk);

commit bda07a64c09c44ced789dbb815c71854f0c59839
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:40 2015 -0700

    tcp: remove unused len argument from tcp_rcv_state_process()
    
    Once we realize tcp_rcv_synsent_state_process() does not use
    its 'len' argument and we get rid of it, then it becomes clear
    this argument is no longer used in tcp_rcv_state_process()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cdbf63d3c5cf..1cfdedbe47e1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -366,7 +366,7 @@ void tcp_write_timer_handler(struct sock *sk);
 void tcp_delack_timer_handler(struct sock *sk);
 int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
-			  const struct tcphdr *th, unsigned int len);
+			  const struct tcphdr *th);
 void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			 const struct tcphdr *th, unsigned int len);
 void tcp_rcv_space_adjust(struct sock *sk);

commit 7c85af8810448d8ef59331be51e482413b5f503d
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 24 17:16:05 2015 -0700

    tcp: avoid reorders for TFO passive connections
    
    We found that a TCP Fast Open passive connection was vulnerable
    to reorders, as the exchange might look like
    
    [1] C -> S S <FO ...> <request>
    [2] S -> C S. ack request <options>
    [3] S -> C . <answer>
    
    packets [2] and [3] can be generated at almost the same time.
    
    If C receives the 3rd packet before the 2nd, it will drop it as
    the socket is in SYN_SENT state and expects a SYNACK.
    
    S will have to retransmit the answer.
    
    Current OOO avoidance in linux is defeated because SYNACK
    packets are attached to the LISTEN socket, while DATA packets
    are attached to the children. They might be sent by different cpus,
    and different TX queues might be selected.
    
    It turns out that for TFO, we created a child, which is a
    full blown socket in TCP_SYN_RECV state, and we simply can attach
    the SYNACK packet to this socket.
    
    This means that at the time tcp_sendmsg() pushes DATA packet,
    skb->ooo_okay will be set iff the SYNACK packet had been sent
    and TX completed.
    
    This removes the reorder source at the host level.
    
    We also removed the export of tcp_try_fastopen(), as it is no
    longer called from IPv6.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6630ab180f5c..cdbf63d3c5cf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1422,10 +1422,10 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
-bool tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
-		      struct request_sock *req,
-		      struct tcp_fastopen_cookie *foc,
-		      struct dst_entry *dst);
+struct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
+			      struct request_sock *req,
+			      struct tcp_fastopen_cookie *foc,
+			      struct dst_entry *dst);
 void tcp_fastopen_init_key_once(bool publish);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 

commit ea3bea3a1d38aab1542176b2ff11a99ce3db9656
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:23 2015 -0700

    tcp/dccp: constify rtx_synack() and friends
    
    This is done to make sure we do not change listener socket
    while sending SYNACK packets while socket lock is not held.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 868c53532169..6630ab180f5c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1676,7 +1676,7 @@ int tcp4_proc_init(void);
 void tcp4_proc_exit(void);
 #endif
 
-int tcp_rtx_synack(struct sock *sk, struct request_sock *req);
+int tcp_rtx_synack(const struct sock *sk, struct request_sock *req);
 int tcp_conn_request(struct request_sock_ops *rsk_ops,
 		     const struct tcp_request_sock_ops *af_ops,
 		     struct sock *sk, struct sk_buff *skb);

commit 0f935dbedc49a5044ebff08b47eef35a2f2bbe92
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:21 2015 -0700

    tcp: constify tcp_v{4|6}_send_synack() socket argument
    
    This documents fact that listener lock might not be held
    at the time SYNACK are sent.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 19f23590baa0..868c53532169 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1717,7 +1717,7 @@ struct tcp_request_sock_ops {
 				       const struct request_sock *req,
 				       bool *strict);
 	__u32 (*init_seq)(const struct sk_buff *skb);
-	int (*send_synack)(struct sock *sk, struct dst_entry *dst,
+	int (*send_synack)(const struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
 	void (*queue_hash_add)(struct sock *sk, struct request_sock *req,

commit 5d062de7f8ea1ca7c635957ff1144fba815ba34c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:19 2015 -0700

    tcp: constify tcp_make_synack() socket argument
    
    listener socket is not locked when tcp_make_synack() is called.
    
    We better make sure no field is written.
    
    There is one exception : Since SYNACK packets are attached to the listener
    at this moment (or SYN_RECV child in case of Fast Open),
    sock_wmalloc() needs to update sk->sk_wmem_alloc, but this is done using
    atomic operations so this is safe.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 45bc3c63c3fd..19f23590baa0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -461,7 +461,7 @@ struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
 int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
 int tcp_connect(struct sock *sk);
-struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
+struct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,
 				struct request_sock *req,
 				struct tcp_fastopen_cookie *foc);
 int tcp_disconnect(struct sock *sk, int flags);

commit b83e3deb974ca2c11e21256fe602e517afb83247
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:15 2015 -0700

    tcp: md5: constify tcp_md5_do_lookup() socket argument
    
    When TCP new listener is done, these functions will be called
    without socket lock being held. Make sure they don't change
    anything.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d37ad0c3ea9c..45bc3c63c3fd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1372,16 +1372,16 @@ int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
 		   int family, const u8 *newkey, u8 newkeylen, gfp_t gfp);
 int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
 		   int family);
-struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
+struct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,
 					 const struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
-struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
+struct tcp_md5sig_key *tcp_md5_do_lookup(const struct sock *sk,
 					 const union tcp_md5_addr *addr,
 					 int family);
 #define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_key)
 #else
-static inline struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
+static inline struct tcp_md5sig_key *tcp_md5_do_lookup(const struct sock *sk,
 					 const union tcp_md5_addr *addr,
 					 int family)
 {
@@ -1684,7 +1684,7 @@ int tcp_conn_request(struct request_sock_ops *rsk_ops,
 /* TCP af-specific functions */
 struct tcp_sock_af_ops {
 #ifdef CONFIG_TCP_MD5SIG
-	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
+	struct tcp_md5sig_key	*(*md5_lookup) (const struct sock *sk,
 						const struct sock *addr_sk);
 	int		(*calc_md5_hash)(char *location,
 					 const struct tcp_md5sig_key *md5,
@@ -1699,7 +1699,7 @@ struct tcp_sock_af_ops {
 struct tcp_request_sock_ops {
 	u16 mss_clamp;
 #ifdef CONFIG_TCP_MD5SIG
-	struct tcp_md5sig_key *(*req_md5_lookup)(struct sock *sk,
+	struct tcp_md5sig_key *(*req_md5_lookup)(const struct sock *sk,
 						 const struct sock *addr_sk);
 	int		(*calc_md5_hash) (char *location,
 					  const struct tcp_md5sig_key *md5,

commit b1964b5fce389a5660139ca39c25ff294da07b4f
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:09 2015 -0700

    tcp: constify tcp_openreq_init_rwin()
    
    Soon, listener socket wont be locked when tcp_openreq_init_rwin()
    is called. We need to read socket fields once, as their value
    could change under us.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c006255a0df1..d37ad0c3ea9c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1207,7 +1207,8 @@ static inline int tcp_full_space(const struct sock *sk)
 }
 
 extern void tcp_openreq_init_rwin(struct request_sock *req,
-				  struct sock *sk, struct dst_entry *dst);
+				  const struct sock *sk_listener,
+				  const struct dst_entry *dst);
 
 void tcp_enter_memory_pressure(struct sock *sk);
 

commit b40cf18ef7961b6d67732e234780586590510ce1
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:08 2015 -0700

    tcp: constify listener socket in tcp_v[46]_init_req()
    
    Soon, listener socket spinlock will no longer be held,
    add const arguments to tcp_v[46]_init_req() to make clear these
    functions can not mess socket fields.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5cf9672c13e2..c006255a0df1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1705,7 +1705,8 @@ struct tcp_request_sock_ops {
 					  const struct sock *sk,
 					  const struct sk_buff *skb);
 #endif
-	void (*init_req)(struct request_sock *req, struct sock *sk,
+	void (*init_req)(struct request_sock *req,
+			 const struct sock *sk_listener,
 			 struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
 	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,

commit 0f1c28ae74bb1a34d36fca2db5161611d58b3148
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri Sep 18 11:36:14 2015 -0700

    tcp: usec resolution SYN/ACK RTT
    
    Currently SYN/ACK RTT is measured in jiffies. For LAN the SYN/ACK
    RTT is often measured as 0ms or sometimes 1ms, which would affect
    RTT estimation and min RTT samping used by some congestion control.
    
    This patch improves SYN/ACK RTT to be usec resolution if platform
    supports it. While the timestamping of SYN/ACK is done in request
    sock, the RTT measurement is carefully arranged to avoid storing
    another u64 timestamp in tcp_sock.
    
    For regular handshake w/o SYNACK retransmission, the RTT is sampled
    right after the child socket is created and right before the request
    sock is released (tcp_check_req() in tcp_minisocks.c)
    
    For Fast Open the child socket is already created when SYN/ACK was
    sent, the RTT is sampled in tcp_rcv_state_process() after processing
    the final ACK an right before the request socket is released.
    
    If the SYN/ACK was retransmistted or SYN-cookie was used, we rely
    on TCP timestamps to measure the RTT. The sample is taken at the
    same place in tcp_rcv_state_process() after the timestamp values
    are validated in tcp_validate_incoming(). Note that we do not store
    TS echo value in request_sock for SYN-cookies, because the value
    is already stored in tp->rx_opt used by tcp_ack_update_rtt().
    
    One side benefit is that the RTT measurement now happens before
    initializing congestion control (of the passive side). Therefore
    the congestion control can use the SYN/ACK RTT.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0cab28cd43a9..5cf9672c13e2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -565,6 +565,7 @@ bool tcp_schedule_loss_probe(struct sock *sk);
 /* tcp_input.c */
 void tcp_resume_early_retransmit(struct sock *sk);
 void tcp_rearm_rto(struct sock *sk);
+void tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req);
 void tcp_reset(struct sock *sk);
 
 /* tcp_timer.c */

commit c3a8d9474684d391b0afc3970d9b249add15ec07
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Aug 31 15:58:47 2015 +0200

    tcp: use dctcp if enabled on the route to the initiator
    
    Currently, the following case doesn't use DCTCP, even if it should:
    A responder has f.e. Cubic as system wide default, but for a specific
    route to the initiating host, DCTCP is being set in RTAX_CC_ALGO. The
    initiating host then uses DCTCP as congestion control, but since the
    initiator sets ECT(0), tcp_ecn_create_request() doesn't set ecn_ok,
    and we have to fall back to Reno after 3WHS completes.
    
    We were thinking on how to solve this in a minimal, non-intrusive
    way without bloating tcp_ecn_create_request() needlessly: lets cache
    the CA ecn option flag in RTAX_FEATURES. In other words, when ECT(0)
    is set on the SYN packet, set ecn_ok=1 iff route RTAX_FEATURES
    contains the unexposed (internal-only) DST_FEATURE_ECN_CA. This allows
    to only do a single metric feature lookup inside tcp_ecn_create_request().
    
    Joint work with Florian Westphal.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4a7b03947a38..0cab28cd43a9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -888,7 +888,7 @@ void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
 struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
-u32 tcp_ca_get_key_by_name(const char *name);
+u32 tcp_ca_get_key_by_name(const char *name, bool *ecn_ca);
 #ifdef CONFIG_INET
 char *tcp_ca_get_name_by_key(u32 key, char *buffer);
 #else

commit 43e122b014c955a33220fabbd09c4b5e4f422c3c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Aug 21 17:38:02 2015 -0700

    tcp: refine pacing rate determination
    
    When TCP pacing was added back in linux-3.12, we chose
    to apply a fixed ratio of 200 % against current rate,
    to allow probing for optimal throughput even during
    slow start phase, where cwnd can be doubled every other gRTT.
    
    At Google, we found it was better applying a different ratio
    while in Congestion Avoidance phase.
    This ratio was set to 120 %.
    
    We've used the normal tcp_in_slow_start() helper for a while,
    then tuned the condition to select the conservative ratio
    as soon as cwnd >= ssthresh/2 :
    
    - After cwnd reduction, it is safer to ramp up more slowly,
      as we approach optimal cwnd.
    - Initial ramp up (ssthresh == INFINITY) still allows doubling
      cwnd every other RTT.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 309801f7eb82..4a7b03947a38 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -281,6 +281,8 @@ extern unsigned int sysctl_tcp_notsent_lowat;
 extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_autocorking;
 extern int sysctl_tcp_invalid_ratelimit;
+extern int sysctl_tcp_pacing_ss_ratio;
+extern int sysctl_tcp_pacing_ca_ratio;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 6f021c62d64f38092bc2a0c5fe7b81d5e5b21a00
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Aug 21 12:30:00 2015 -0700

    tcp: fix slow start after idle vs TSO/GSO
    
    slow start after idle might reduce cwnd, but we perform this
    after first packet was cooked and sent.
    
    With TSO/GSO, it means that we might send a full TSO packet
    even if cwnd should have been reduced to IW10.
    
    Moving the SSAI check in skb_entail() makes sense, because
    we slightly reduce number of times this check is done,
    especially for large send() and TCP Small queue callbacks from
    softirq context.
    
    As Neal pointed out, we also need to perform the check
    if/when receive window opens.
    
    Tested:
    
    Following packetdrill test demonstrates the problem
    // Test of slow start after idle
    
    `sysctl -q net.ipv4.tcp_slow_start_after_idle=1`
    
    0.000 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
    +0    setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
    +0    bind(3, ..., ...) = 0
    +0    listen(3, 1) = 0
    
    +0    < S 0:0(0) win 65535 <mss 1000,sackOK,nop,nop,nop,wscale 7>
    +0    > S. 0:0(0) ack 1 <mss 1460,nop,nop,sackOK,nop,wscale 6>
    +.100 < . 1:1(0) ack 1 win 511
    +0    accept(3, ..., ...) = 4
    +0    setsockopt(4, SOL_SOCKET, SO_SNDBUF, [200000], 4) = 0
    
    +0    write(4, ..., 26000) = 26000
    +0    > . 1:5001(5000) ack 1
    +0    > . 5001:10001(5000) ack 1
    +0    %{ assert tcpi_snd_cwnd == 10 }%
    
    +.100 < . 1:1(0) ack 10001 win 511
    +0    %{ assert tcpi_snd_cwnd == 20, tcpi_snd_cwnd }%
    +0    > . 10001:20001(10000) ack 1
    +0    > P. 20001:26001(6000) ack 1
    
    +.100 < . 1:1(0) ack 26001 win 511
    +0    %{ assert tcpi_snd_cwnd == 36, tcpi_snd_cwnd }%
    
    +4 write(4, ..., 20000) = 20000
    // If slow start after idle works properly, we should send 5 MSS here (cwnd/2)
    +0    > . 26001:31001(5000) ack 1
    +0    %{ assert tcpi_snd_cwnd == 10, tcpi_snd_cwnd }%
    +0    > . 31001:36001(5000) ack 1
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 364426a2be5a..309801f7eb82 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1165,6 +1165,19 @@ static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 }
 
 u32 tcp_default_init_rwnd(u32 mss);
+void tcp_cwnd_restart(struct sock *sk, s32 delta);
+
+static inline void tcp_slow_start_after_idle_check(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	s32 delta;
+
+	if (!sysctl_tcp_slow_start_after_idle || tp->packets_out)
+		return;
+	delta = tcp_time_stamp - tp->lsndtime;
+	if (delta > inet_csk(sk)->icsk_rto)
+		tcp_cwnd_restart(sk, delta);
+}
 
 /* Determine a window scaling and initial window to offer. */
 void tcp_select_initial_window(int __space, __u32 mss, __u32 *rcv_wnd,

commit 76174004a0f19785a328f40388e87e982bbf69b9
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 9 13:16:30 2015 -0700

    tcp: do not slow start when cwnd equals ssthresh
    
    In the original design slow start is only used to raise cwnd
    when cwnd is stricly below ssthresh. It makes little sense
    to slow start when cwnd == ssthresh: especially
    when hystart has set ssthresh in the initial ramp, or after
    recovery when cwnd resets to ssthresh. Not doing so will
    also help reduce the buffer bloat slightly.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dba22fc1b065..364426a2be5a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -991,7 +991,7 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 
 static inline bool tcp_in_slow_start(const struct tcp_sock *tp)
 {
-	return tp->snd_cwnd <= tp->snd_ssthresh;
+	return tp->snd_cwnd < tp->snd_ssthresh;
 }
 
 static inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)

commit 071d5080e33d6f24139e4213c2d9f97a2c21b602
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 9 13:16:29 2015 -0700

    tcp: add tcp_in_slow_start helper
    
    Add a helper to test the slow start condition in various congestion
    control modules and other places. This is to prepare a slight improvement
    in policy as to exactly when to slow start.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 950cfecaad3c..dba22fc1b065 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -989,6 +989,11 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 
 #define TCP_INFINITE_SSTHRESH	0x7fffffff
 
+static inline bool tcp_in_slow_start(const struct tcp_sock *tp)
+{
+	return tp->snd_cwnd <= tp->snd_ssthresh;
+}
+
 static inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)
 {
 	return tp->snd_ssthresh >= TCP_INFINITE_SSTHRESH;
@@ -1065,7 +1070,7 @@ static inline bool tcp_is_cwnd_limited(const struct sock *sk)
 	const struct tcp_sock *tp = tcp_sk(sk);
 
 	/* If in slow start, ensure cwnd grows to twice what was ACKed. */
-	if (tp->snd_cwnd <= tp->snd_ssthresh)
+	if (tcp_in_slow_start(tp))
 		return tp->snd_cwnd < 2 * tp->max_packets_out;
 
 	return tp->is_cwnd_limited;

commit f69ad292cfd13aa7ee00847320c6bb9ba2154e87
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jun 11 09:15:18 2015 -0700

    tcp: fill shinfo->gso_size at last moment
    
    In commit cd7d8498c9a5 ("tcp: change tcp_skb_pcount() location") we stored
    gso_segs in a temporary cache hot location.
    
    This patch does the same for gso_size.
    
    This allows to save 2 cache line misses in tcp xmit path for
    the last packet that is considered but not sent because of
    various conditions (cwnd, tso defer, receiver window, TSQ...)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 978cebedd3fc..950cfecaad3c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -730,11 +730,14 @@ struct tcp_skb_cb {
 		/* Note : tcp_tw_isn is used in input path only
 		 *	  (isn chosen by tcp_timewait_state_process())
 		 *
-		 * 	  tcp_gso_segs is used in write queue only,
-		 *	  cf tcp_skb_pcount()
+		 * 	  tcp_gso_segs/size are used in write queue only,
+		 *	  cf tcp_skb_pcount()/tcp_skb_mss()
 		 */
 		__u32		tcp_tw_isn;
-		__u32		tcp_gso_segs;
+		struct {
+			u16	tcp_gso_segs;
+			u16	tcp_gso_size;
+		};
 	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
@@ -790,10 +793,10 @@ static inline void tcp_skb_pcount_add(struct sk_buff *skb, int segs)
 	TCP_SKB_CB(skb)->tcp_gso_segs += segs;
 }
 
-/* This is valid iff tcp_skb_pcount() > 1. */
+/* This is valid iff skb is in write queue and tcp_skb_pcount() > 1. */
 static inline int tcp_skb_mss(const struct sk_buff *skb)
 {
-	return skb_shinfo(skb)->gso_size;
+	return TCP_SKB_CB(skb)->tcp_gso_size;
 }
 
 /* Events passed to congestion control interface */

commit b80c0e78582d4a3a004dc1ade4eb06babc6a4eea
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jun 4 18:30:43 2015 -0700

    tcp: get_cookie_sock() consolidation
    
    IPv4 and IPv6 share same implementation of get_cookie_sock(),
    and there is no point inlining it.
    
    We add tcp_ prefix to the common helper name and export it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2bb2bad21d5c..978cebedd3fc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -469,6 +469,9 @@ int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
 void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 
 /* From syncookies.c */
+struct sock *tcp_get_cookie_sock(struct sock *sk, struct sk_buff *skb,
+				 struct request_sock *req,
+				 struct dst_entry *dst);
 int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 		      u32 cookie);
 struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);

commit 492135557dc090a1abb2cfbe1a412757e3ed68ab
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue May 19 21:04:22 2015 +0200

    tcp: add rfc3168, section 6.1.1.1. fallback
    
    This work as a follow-up of commit f7b3bec6f516 ("net: allow setting ecn
    via routing table") and adds RFC3168 section 6.1.1.1. fallback for outgoing
    ECN connections. In other words, this work adds a retry with a non-ECN
    setup SYN packet, as suggested from the RFC on the first timeout:
    
      [...] A host that receives no reply to an ECN-setup SYN within the
      normal SYN retransmission timeout interval MAY resend the SYN and
      any subsequent SYN retransmissions with CWR and ECE cleared. [...]
    
    Schematic client-side view when assuming the server is in tcp_ecn=2 mode,
    that is, Linux default since 2009 via commit 255cac91c3c9 ("tcp: extend
    ECN sysctl to allow server-side only ECN"):
    
     1) Normal ECN-capable path:
    
        SYN ECE CWR ----->
                    <----- SYN ACK ECE
                ACK ----->
    
     2) Path with broken middlebox, when client has fallback:
    
        SYN ECE CWR ----X crappy middlebox drops packet
                          (timeout, rtx)
                SYN ----->
                    <----- SYN ACK
                ACK ----->
    
    In case we would not have the fallback implemented, the middlebox drop
    point would basically end up as:
    
        SYN ECE CWR ----X crappy middlebox drops packet
                          (timeout, rtx)
        SYN ECE CWR ----X crappy middlebox drops packet
                          (timeout, rtx)
        SYN ECE CWR ----X crappy middlebox drops packet
                          (timeout, rtx)
    
    In any case, it's rather a smaller percentage of sites where there would
    occur such additional setup latency: it was found in end of 2014 that ~56%
    of IPv4 and 65% of IPv6 servers of Alexa 1 million list would negotiate
    ECN (aka tcp_ecn=2 default), 0.42% of these webservers will fail to connect
    when trying to negotiate with ECN (tcp_ecn=1) due to timeouts, which the
    fallback would mitigate with a slight latency trade-off. Recent related
    paper on this topic:
    
      Brian Trammell, Mirja Kühlewind, Damiano Boppart, Iain Learmonth,
      Gorry Fairhurst, and Richard Scheffenegger:
        "Enabling Internet-Wide Deployment of Explicit Congestion Notification."
        Proc. PAM 2015, New York.
      http://ecn.ethz.ch/ecn-pam15.pdf
    
    Thus, when net.ipv4.tcp_ecn=1 is being set, the patch will perform RFC3168,
    section 6.1.1.1. fallback on timeout. For users explicitly not wanting this
    which can be in DC use case, we add a net.ipv4.tcp_ecn_fallback knob that
    allows for disabling the fallback.
    
    tp->ecn_flags are not being cleared in tcp_ecn_clear_syn() on output, but
    rather we let tcp_ecn_rcv_synack() take that over on input path in case a
    SYN ACK ECE was delayed. Thus a spurious SYN retransmission will not prevent
    ECN being negotiated eventually in that case.
    
    Reference: https://www.ietf.org/proceedings/92/slides/slides-92-iccrg-1.pdf
    Reference: https://www.ietf.org/proceedings/89/slides/slides-89-tsvarea-1.pdf
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Mirja Kühlewind <mirja.kuehlewind@tik.ee.ethz.ch>
    Signed-off-by: Brian Trammell <trammell@tik.ee.ethz.ch>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Dave That <dave.taht@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0d85223efa4c..2bb2bad21d5c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -712,6 +712,8 @@ static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 #define TCPHDR_ECE 0x40
 #define TCPHDR_CWR 0x80
 
+#define TCPHDR_SYN_ECN	(TCPHDR_SYN | TCPHDR_ECE | TCPHDR_CWR)
+
 /* This is what the send packet queuing engine uses to pass
  * TCP per-packet control information to the transmission code.
  * We also store the host-order sequence numbers in here too.

commit b8da51ebb1aa93908350f95efae73aecbc2e266c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 15 12:39:27 2015 -0700

    tcp: introduce tcp_under_memory_pressure()
    
    Introduce an optimized version of sk_under_memory_pressure()
    for TCP. Our intent is to use it in fast paths.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 841691a296dc..0d85223efa4c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -286,6 +286,14 @@ extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
+/* optimized version of sk_under_memory_pressure() for TCP sockets */
+static inline bool tcp_under_memory_pressure(const struct sock *sk)
+{
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return !!sk->sk_cgrp->memory_pressure;
+
+	return tcp_memory_pressure;
+}
 /*
  * The next routines deal with comparing 32 bit unsigned ints
  * and worry about wraparound (automatic with unsigned arithmetic).

commit a6c5ea4ccf0033591e6e476d7a273c0074c07aa7
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 15 12:39:26 2015 -0700

    tcp: rename sk_forced_wmem_schedule() to sk_forced_mem_schedule()
    
    We plan to use sk_forced_wmem_schedule() in input path as well,
    so make it non static and rename it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7ace6acbf5fd..841691a296dc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -311,6 +311,8 @@ static inline bool tcp_out_of_memory(struct sock *sk)
 	return false;
 }
 
+void sk_forced_mem_schedule(struct sock *sk, int size);
+
 static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 {
 	struct percpu_counter *ocp = sk->sk_prot->orphan_count;

commit 264ea103a7473f51aced838e68ed384ea2c759f5
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 14 14:26:56 2015 -0700

    tcp: syncookies: extend validity range
    
    Now we allow storing more request socks per listener, we might
    hit syncookie mode less often and hit following bug in our stack :
    
    When we send a burst of syncookies, then exit this mode,
    tcp_synq_no_recent_overflow() can return false if the ACK packets coming
    from clients are coming three seconds after the end of syncookie
    episode.
    
    This is a way too strong requirement and conflicts with rest of
    syncookie code which allows ACK to be aged up to 2 minutes.
    
    Perfectly valid ACK packets are dropped just because clients might be
    in a crowded wifi environment or on another planet.
    
    So let's fix this, and also change tcp_synq_overflow() to not
    dirty a cache line for every syncookie we send, as we are under attack.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Florian Westphal <fw@strlen.de>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b8ea12880fd9..7ace6acbf5fd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -326,18 +326,6 @@ static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 
 bool tcp_check_oom(struct sock *sk, int shift);
 
-/* syncookies: remember time of last synqueue overflow */
-static inline void tcp_synq_overflow(struct sock *sk)
-{
-	tcp_sk(sk)->rx_opt.ts_recent_stamp = jiffies;
-}
-
-/* syncookies: no recent synqueue overflow on this listening socket? */
-static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
-{
-	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
-	return time_after(jiffies, last_overflow + TCP_TIMEOUT_FALLBACK);
-}
 
 extern struct proto tcp_prot;
 
@@ -483,13 +471,35 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
  * i.e. a sent cookie is valid only at most for 2*60 seconds (or less if
  * the counter advances immediately after a cookie is generated).
  */
-#define MAX_SYNCOOKIE_AGE 2
+#define MAX_SYNCOOKIE_AGE	2
+#define TCP_SYNCOOKIE_PERIOD	(60 * HZ)
+#define TCP_SYNCOOKIE_VALID	(MAX_SYNCOOKIE_AGE * TCP_SYNCOOKIE_PERIOD)
+
+/* syncookies: remember time of last synqueue overflow
+ * But do not dirty this field too often (once per second is enough)
+ */
+static inline void tcp_synq_overflow(struct sock *sk)
+{
+	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	unsigned long now = jiffies;
+
+	if (time_after(now, last_overflow + HZ))
+		tcp_sk(sk)->rx_opt.ts_recent_stamp = now;
+}
+
+/* syncookies: no recent synqueue overflow on this listening socket? */
+static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
+{
+	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+
+	return time_after(jiffies, last_overflow + TCP_SYNCOOKIE_VALID);
+}
 
 static inline u32 tcp_cookie_time(void)
 {
 	u64 val = get_jiffies_64();
 
-	do_div(val, 60 * HZ);
+	do_div(val, TCP_SYNCOOKIE_PERIOD);
 	return val;
 }
 

commit e520af48c7e5acae5f17f82a79ba7ab7cf156f3b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 6 14:26:25 2015 -0700

    tcp: add TCPWinProbe and TCPKeepAlive SNMP counters
    
    Diagnosing problems related to Window Probes has been hard because
    we lack a counter.
    
    TCPWinProbe counts the number of ACK packets a sender has to send
    at regular intervals to make sure a reverse ACK packet opening back
    a window had not been lost.
    
    TCPKeepAlive counts the number of ACK packets sent to keep TCP
    flows alive (SO_KEEPALIVE)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7a2248a35b13..b8ea12880fd9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -527,7 +527,7 @@ int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int, gfp_t);
 
 void tcp_send_probe0(struct sock *);
 void tcp_send_partial(struct sock *);
-int tcp_write_wakeup(struct sock *);
+int tcp_write_wakeup(struct sock *, int mib);
 void tcp_send_fin(struct sock *sk);
 void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 int tcp_send_synack(struct sock *);

commit 21c8fe9915276d923f8c1e43434fd6d37a3b9aef
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 6 14:26:24 2015 -0700

    tcp: adjust window probe timers to safer values
    
    With the advent of small rto timers in datacenter TCP,
    (ip route ... rto_min x), the following can happen :
    
    1) Qdisc is full, transmit fails.
    
       TCP sets a timer based on icsk_rto to retry the transmit, without
       exponential backoff.
       With low icsk_rto, and lot of sockets, all cpus are servicing timer
       interrupts like crazy.
       Intent of the code was to retry with a timer between 200 (TCP_RTO_MIN)
       and 500ms (TCP_RESOURCE_PROBE_INTERVAL)
    
    2) Receivers can send zero windows if they don't drain their receive queue.
    
       TCP sends zero window probes, based on icsk_rto current value, with
       exponential backoff.
       With /proc/sys/net/ipv4/tcp_retries2 being 15 (or even smaller in
       some cases), sender can abort in less than one or two minutes !
       If receiver stops the sender, it obviously doesn't care of very tight
       rto. Probability of dropping the ACK reopening the window is not
       worth the risk.
    
    Lets change the base timer to be at least 200ms (TCP_RTO_MIN) for these
    events (but not normal RTO based retransmits)
    
    A followup patch adds a new SNMP counter, as it would have helped a lot
    diagnosing this issue.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6d204f3f9df8..7a2248a35b13 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1043,14 +1043,31 @@ static inline bool tcp_is_cwnd_limited(const struct sock *sk)
 	return tp->is_cwnd_limited;
 }
 
-static inline void tcp_check_probe_timer(struct sock *sk)
+/* Something is really bad, we could not queue an additional packet,
+ * because qdisc is full or receiver sent a 0 window.
+ * We do not want to add fuel to the fire, or abort too early,
+ * so make sure the timer we arm now is at least 200ms in the future,
+ * regardless of current icsk_rto value (as it could be ~2ms)
+ */
+static inline unsigned long tcp_probe0_base(const struct sock *sk)
 {
-	const struct tcp_sock *tp = tcp_sk(sk);
-	const struct inet_connection_sock *icsk = inet_csk(sk);
+	return max_t(unsigned long, inet_csk(sk)->icsk_rto, TCP_RTO_MIN);
+}
 
-	if (!tp->packets_out && !icsk->icsk_pending)
+/* Variant of inet_csk_rto_backoff() used for zero window probes */
+static inline unsigned long tcp_probe0_when(const struct sock *sk,
+					    unsigned long max_when)
+{
+	u64 when = (u64)tcp_probe0_base(sk) << inet_csk(sk)->icsk_backoff;
+
+	return (unsigned long)min_t(u64, when, max_when);
+}
+
+static inline void tcp_check_probe_timer(struct sock *sk)
+{
+	if (!tcp_sk(sk)->packets_out && !inet_csk(sk)->icsk_pending)
 		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
-					  icsk->icsk_rto, TCP_RTO_MAX);
+					  tcp_probe0_base(sk), TCP_RTO_MAX);
 }
 
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)

commit 64f40ff5bbdb1b679fb3c4dbc8230d6517d2b8dc
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 28 16:23:48 2015 -0700

    tcp: prepare CC get_info() access from getsockopt()
    
    We would like that optional info provided by Congestion Control
    modules using netlink can also be read using getsockopt()
    
    This patch changes get_info() to put this information in a buffer,
    instead of skb, like tcp_get_info(), so that following patch
    can reuse this common infrastructure.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dd7b4ea6a10c..6d204f3f9df8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -804,6 +804,8 @@ enum tcp_ca_ack_event_flags {
 /* Requires ECN/ECT set on all packets */
 #define TCP_CONG_NEEDS_ECN	0x2
 
+union tcp_cc_info;
+
 struct tcp_congestion_ops {
 	struct list_head	list;
 	u32 key;
@@ -829,7 +831,8 @@ struct tcp_congestion_ops {
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, u32 num_acked, s32 rtt_us);
 	/* get info for inet_diag (optional) */
-	int (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
+	size_t (*get_info)(struct sock *sk, u32 ext, int *attr,
+			   union tcp_cc_info *info);
 
 	char 		name[TCP_CA_NAME_MAX];
 	struct module 	*owner;

commit 0df48c26d8418c5c9fba63fac15b660d70ca2f1c
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 28 15:28:17 2015 -0700

    tcp: add tcpi_bytes_acked to tcp_info
    
    This patch tracks total number of bytes acked for a TCP socket.
    This is the sum of all changes done to tp->snd_una, and allows
    for precise tracking of delivered data.
    
    RFC4898 named this : tcpEStatsAppHCThruOctetsAcked
    
    This is a 64bit field, and can be fetched both from TCP_INFO
    getsockopt() if one has a handle on a TCP socket, or from inet_diag
    netlink facility (iproute2/ss patch will follow)
    
    Note that tp->bytes_acked was placed near tp->snd_una for
    best data locality and minimal performance impact.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Eric Salo <salo@google.com>
    Cc: Martin Lau <kafai@fb.com>
    Cc: Chris Rapier <rapier@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 051dc5c2802d..dd7b4ea6a10c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -576,7 +576,7 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 }
 
 /* tcp.c */
-void tcp_get_info(const struct sock *, struct tcp_info *);
+void tcp_get_info(struct sock *, struct tcp_info *);
 
 /* Read 'sendfile()'-style from a TCP socket */
 typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,

commit 521f1cf1dbb9d5ad858dca5dc75d1b45f64b6589
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Apr 16 18:10:35 2015 -0700

    inet_diag: fix access to tcp cc information
    
    Two different problems are fixed here :
    
    1) inet_sk_diag_fill() might be called without socket lock held.
       icsk->icsk_ca_ops can change under us and module be unloaded.
       -> Access to freed memory.
       Fix this using rcu_read_lock() to prevent module unload.
    
    2) Some TCP Congestion Control modules provide information
       but again this is not safe against icsk->icsk_ca_ops
       change and nla_put() errors were ignored. Some sockets
       could not get the additional info if skb was almost full.
    
    Fix this by returning a status from get_info() handlers and
    using rcu protection as well.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9598871485ce..051dc5c2802d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -829,7 +829,7 @@ struct tcp_congestion_ops {
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, u32 num_acked, s32 rtt_us);
 	/* get info for inet_diag (optional) */
-	void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
+	int (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
 
 	char 		name[TCP_CA_NAME_MAX];
 	struct module 	*owner;

commit 2646c831c00c5d22aa72b79d24069c1b412cda7c
Author: Daniel Lee <Longinus00@gmail.com>
Date:   Mon Apr 6 14:37:27 2015 -0700

    tcp: RFC7413 option support for Fast Open client
    
    Fast Open has been using an experimental option with a magic number
    (RFC6994). This patch makes the client by default use the RFC7413
    option (34) to get and send Fast Open cookies.  This patch makes
    the client solicit cookies from a given server first with the
    RFC7413 option. If that fails to elicit a cookie, then it tries
    the RFC6994 experimental option. If that also fails, it uses the
    RFC7413 option on all subsequent connect attempts.  If the server
    returns a Fast Open cookie then the client caches the form of the
    option that successfully elicited a cookie, and uses that form on
    later connects when it presents that cookie.
    
    The idea is to gradually obsolete the use of experimental options as
    the servers and clients upgrade, while keeping the interoperability
    meanwhile.
    
    Signed-off-by: Daniel Lee <Longinus00@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7292c3c575bc..9598871485ce 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1339,7 +1339,8 @@ void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
 			    struct tcp_fastopen_cookie *cookie, int *syn_loss,
 			    unsigned long *last_syn_loss);
 void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
-			    struct tcp_fastopen_cookie *cookie, bool syn_lost);
+			    struct tcp_fastopen_cookie *cookie, bool syn_lost,
+			    u16 try_exp);
 struct tcp_fastopen_request {
 	/* Fast Open cookie. Size 0 means a cookie request */
 	struct tcp_fastopen_cookie	cookie;

commit 7f9b838b71eb78a27de27a12ca5de8542fac3115
Author: Daniel Lee <Longinus00@gmail.com>
Date:   Mon Apr 6 14:37:26 2015 -0700

    tcp: RFC7413 option support for Fast Open server
    
    Fast Open has been using the experimental option with a magic number
    (RFC6994) to request and grant Fast Open cookies. This patch enables
    the server to support the official IANA option 34 in RFC7413 in
    addition.
    
    The change has passed all existing Fast Open tests with both
    old and new options at Google.
    
    Signed-off-by: Daniel Lee <Longinus00@gmail.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 963303fb96ae..7292c3c575bc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -179,6 +179,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_SACK             5       /* SACK Block */
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
+#define TCPOPT_FASTOPEN		34	/* Fast open (RFC7413) */
 #define TCPOPT_EXP		254	/* Experimental */
 /* Magic number to be after the option value for sharing TCP
  * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
@@ -194,6 +195,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERM      2
 #define TCPOLEN_TIMESTAMP      10
 #define TCPOLEN_MD5SIG         18
+#define TCPOLEN_FASTOPEN_BASE  2
 #define TCPOLEN_EXP_FASTOPEN_BASE  4
 
 /* But this is what stacks really send out. */

commit 41d25fe0927aabb1d4b671871a99a55bcd203257
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 25 15:08:47 2015 -0700

    tcp: tcp_syn_flood_action() can be static
    
    After commit 1fb6f159fd21 ("tcp: add tcp_conn_request"),
    tcp_syn_flood_action() is no longer used from IPv6.
    
    We can make it static, by moving it above tcp_conn_request()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 42690daa924e..963303fb96ae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -529,8 +529,6 @@ int tcp_write_wakeup(struct sock *);
 void tcp_send_fin(struct sock *sk);
 void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 int tcp_send_synack(struct sock *);
-bool tcp_syn_flood_action(struct sock *sk, const struct sk_buff *skb,
-			  const char *proto);
 void tcp_push_one(struct sock *, unsigned int mss_now);
 void tcp_send_ack(struct sock *sk);
 void tcp_send_delayed_ack(struct sock *sk);

commit fd3a154a00fb991872680f19021f5edbb40b4dbe
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 24 15:58:56 2015 -0700

    tcp: md5: get rid of tcp_v[46]_reqsk_md5_lookup()
    
    With request socks convergence, we no longer need
    different lookup methods. A request socket can
    use generic lookup function.
    
    Add const qualifier to 2nd tcp_v[46]_md5_lookup() parameter.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 992be858c370..42690daa924e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1303,7 +1303,7 @@ int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
 int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
 		   int family);
 struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
-					 struct sock *addr_sk);
+					 const struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
 struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
@@ -1614,7 +1614,7 @@ int tcp_conn_request(struct request_sock_ops *rsk_ops,
 struct tcp_sock_af_ops {
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
-						struct sock *addr_sk);
+						const struct sock *addr_sk);
 	int		(*calc_md5_hash)(char *location,
 					 const struct tcp_md5sig_key *md5,
 					 const struct sock *sk,
@@ -1628,8 +1628,8 @@ struct tcp_sock_af_ops {
 struct tcp_request_sock_ops {
 	u16 mss_clamp;
 #ifdef CONFIG_TCP_MD5SIG
-	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
-						struct request_sock *req);
+	struct tcp_md5sig_key *(*req_md5_lookup)(struct sock *sk,
+						 const struct sock *addr_sk);
 	int		(*calc_md5_hash) (char *location,
 					  const struct tcp_md5sig_key *md5,
 					  const struct sock *sk,

commit 39f8e58e53be32ab758d30536e0bd2e6ce766462
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 24 15:58:55 2015 -0700

    tcp: md5: remove request sock argument of calc_md5_hash()
    
    Since request and established sockets now have same base,
    there is no need to pass two pointers to tcp_v4_md5_hash_skb()
    or tcp_v6_md5_hash_skb()
    
    Also add a const qualifier to their struct tcp_md5sig_key argument.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index fe60e00e1919..992be858c370 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1296,9 +1296,8 @@ struct tcp_md5sig_pool {
 };
 
 /* - functions */
-int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
-			const struct sock *sk, const struct request_sock *req,
-			const struct sk_buff *skb);
+int tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,
+			const struct sock *sk, const struct sk_buff *skb);
 int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
 		   int family, const u8 *newkey, u8 newkeylen, gfp_t gfp);
 int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
@@ -1616,14 +1615,13 @@ struct tcp_sock_af_ops {
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
 						struct sock *addr_sk);
-	int			(*calc_md5_hash) (char *location,
-						  struct tcp_md5sig_key *md5,
-						  const struct sock *sk,
-						  const struct request_sock *req,
-						  const struct sk_buff *skb);
-	int			(*md5_parse) (struct sock *sk,
-					      char __user *optval,
-					      int optlen);
+	int		(*calc_md5_hash)(char *location,
+					 const struct tcp_md5sig_key *md5,
+					 const struct sock *sk,
+					 const struct sk_buff *skb);
+	int		(*md5_parse)(struct sock *sk,
+				     char __user *optval,
+				     int optlen);
 #endif
 };
 
@@ -1632,11 +1630,10 @@ struct tcp_request_sock_ops {
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
 						struct request_sock *req);
-	int			(*calc_md5_hash) (char *location,
-						  struct tcp_md5sig_key *md5,
-						  const struct sock *sk,
-						  const struct request_sock *req,
-						  const struct sk_buff *skb);
+	int		(*calc_md5_hash) (char *location,
+					  const struct tcp_md5sig_key *md5,
+					  const struct sock *sk,
+					  const struct sk_buff *skb);
 #endif
 	void (*init_req)(struct request_sock *req, struct sock *sk,
 			 struct sk_buff *skb);

commit 26e3736090e1037ac929787df21c05497479b77f
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 22 10:22:22 2015 -0700

    ipv4: tcp: handle ICMP messages on TCP_NEW_SYN_RECV request sockets
    
    tcp_v4_err() can restrict lookups to ehash table, and not to listeners.
    
    Note this patch creates the infrastructure, but this means that ICMP
    messages for request sockets are ignored until complete conversion.
    
    New tcp_req_err() helper is exported so that we can use it in IPv6
    in following patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1876262afd59..fe60e00e1919 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -447,6 +447,7 @@ const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
 void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 void tcp_v4_mtu_reduced(struct sock *sk);
+void tcp_req_err(struct sock *sk, u32 seq);
 int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_create_openreq_child(struct sock *sk,
 				      struct request_sock *req,

commit 42cb80a2353f42913ae78074ffa1f1b4a49e5436
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 22 10:22:19 2015 -0700

    inet: remove sk_listener parameter from syn_ack_timeout()
    
    It is not needed, and req->sk_listener points to the listener anyway.
    request_sock argument can be const.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 082fd79132b7..1876262afd59 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -433,7 +433,7 @@ int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
 int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
 			  char __user *optval, unsigned int optlen);
 void tcp_set_keepalive(struct sock *sk, int val);
-void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
+void tcp_syn_ack_timeout(const struct request_sock *req);
 int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 		int flags, int *addr_len);
 void tcp_parse_options(const struct sk_buff *skb,

commit 52452c542559ac980b48dbf22a30ee7fa0af507c
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 19 19:04:19 2015 -0700

    inet: drop prev pointer handling in request sock
    
    When request sock are put in ehash table, the whole notion
    of having a previous request to update dl_next is pointless.
    
    Also, following patch will get rid of big purge timer,
    so we want to delete a request sock without holding listener lock.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5b29835b81d8..082fd79132b7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -406,8 +406,7 @@ enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
 					      struct sk_buff *skb,
 					      const struct tcphdr *th);
 struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
-			   struct request_sock *req, struct request_sock **prev,
-			   bool fastopen);
+			   struct request_sock *req, bool fastopen);
 int tcp_child_process(struct sock *parent, struct sock *child,
 		      struct sk_buff *skb);
 void tcp_enter_loss(struct sock *sk);

commit 7970ddc8f9ffe149b392975da60739ccd1796dea
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Mar 16 21:06:20 2015 -0700

    tcp: uninline tcp_oow_rate_limited()
    
    tcp_oow_rate_limited() is hardly used in fast path, there is
    no point inlining it.
    
    Signed-of-by: Eric Dumazet <edumazet@google.com>
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9fe1d535cd6c..5b29835b81d8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1216,36 +1216,8 @@ static inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,
 	return true;
 }
 
-/* Return true if we're currently rate-limiting out-of-window ACKs and
- * thus shouldn't send a dupack right now. We rate-limit dupacks in
- * response to out-of-window SYNs or ACKs to mitigate ACK loops or DoS
- * attacks that send repeated SYNs or ACKs for the same connection. To
- * do this, we do not send a duplicate SYNACK or ACK if the remote
- * endpoint is sending out-of-window SYNs or pure ACKs at a high rate.
- */
-static inline bool tcp_oow_rate_limited(struct net *net,
-					const struct sk_buff *skb,
-					int mib_idx, u32 *last_oow_ack_time)
-{
-	/* Data packets without SYNs are not likely part of an ACK loop. */
-	if ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&
-	    !tcp_hdr(skb)->syn)
-		goto not_rate_limited;
-
-	if (*last_oow_ack_time) {
-		s32 elapsed = (s32)(tcp_time_stamp - *last_oow_ack_time);
-
-		if (0 <= elapsed && elapsed < sysctl_tcp_invalid_ratelimit) {
-			NET_INC_STATS_BH(net, mib_idx);
-			return true;	/* rate-limited: don't send yet! */
-		}
-	}
-
-	*last_oow_ack_time = tcp_time_stamp;
-
-not_rate_limited:
-	return false;	/* not rate-limited: go ahead, send dupack now! */
-}
+bool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,
+			  int mib_idx, u32 *last_oow_ack_time);
 
 static inline void tcp_mib_init(struct net *net)
 {

commit 1bfc4438a7ef99e8a6cba0bd3a86249430256621
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Mar 16 21:06:19 2015 -0700

    tcp: move tcp_openreq_init() to tcp_input.c
    
    This big helper is called once from tcp_conn_request(), there is no
    point having it in an include. Compiler will inline it anyway.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2e11e38205c2..9fe1d535cd6c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1137,31 +1137,6 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk->sk_rcvbuf);
 }
 
-static inline void tcp_openreq_init(struct request_sock *req,
-				    struct tcp_options_received *rx_opt,
-				    struct sk_buff *skb, struct sock *sk)
-{
-	struct inet_request_sock *ireq = inet_rsk(req);
-
-	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
-	req->cookie_ts = 0;
-	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
-	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
-	tcp_rsk(req)->snt_synack = tcp_time_stamp;
-	tcp_rsk(req)->last_oow_ack_time = 0;
-	req->mss = rx_opt->mss_clamp;
-	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
-	ireq->tstamp_ok = rx_opt->tstamp_ok;
-	ireq->sack_ok = rx_opt->sack_ok;
-	ireq->snd_wscale = rx_opt->snd_wscale;
-	ireq->wscale_ok = rx_opt->wscale_ok;
-	ireq->acked = 0;
-	ireq->ecn_ok = 0;
-	ireq->ir_rmt_port = tcp_hdr(skb)->source;
-	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
-	ireq->ir_mark = inet_request_mark(sk, skb);
-}
-
 extern void tcp_openreq_init_rwin(struct request_sock *req,
 				  struct sock *sk, struct dst_entry *dst);
 

commit 05cbc0db03e82128f2e7e353d4194dd24a1627fe
Author: Fan Du <fan.du@intel.com>
Date:   Fri Mar 6 11:18:24 2015 +0800

    ipv4: Create probe timer for tcp PMTU as per RFC4821
    
    As per RFC4821 7.3.  Selecting Probe Size, a probe timer should
    be armed once probing has converged. Once this timer expired,
    probing again to take advantage of any path PMTU change. The
    recommended probing interval is 10 minutes per RFC1981. Probing
    interval could be sysctled by sysctl_tcp_probe_interval.
    
    Eric Dumazet suggested to implement pseudo timer based on 32bits
    jiffies tcp_time_stamp instead of using classic timer for such
    rare event.
    
    Signed-off-by: Fan Du <fan.du@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1ad82e334e27..2e11e38205c2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -67,6 +67,9 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* The least MTU to use for probing */
 #define TCP_BASE_MSS		1024
 
+/* probing interval, default to 10 minutes as per RFC4821 */
+#define TCP_PROBE_INTERVAL	600
+
 /* Specify interval when tcp mtu probing will stop */
 #define TCP_PROBE_THRESHOLD	8
 

commit 6b58e0a5f32dedb609438bb9c9c82aa6e23381f2
Author: Fan Du <fan.du@intel.com>
Date:   Fri Mar 6 11:18:23 2015 +0800

    ipv4: Use binary search to choose tcp PMTU probe_size
    
    Current probe_size is chosen by doubling mss_cache,
    the probing process will end shortly with a sub-optimal
    mss size, and the link mtu will not be taken full
    advantage of, in return, this will make user to tweak
    tcp_base_mss with care.
    
    Use binary search to choose probe_size in a fine
    granularity manner, an optimal mss will be found
    to boost performance as its maxmium.
    
    In addition, introduce a sysctl_tcp_probe_threshold
    to control when probing will stop in respect to
    the width of search range.
    
    Test env:
    Docker instance with vxlan encapuslation(82599EB)
    iperf -c 10.0.0.24  -t 60
    
    before this patch:
    1.26 Gbits/sec
    
    After this patch: increase 26%
    1.59 Gbits/sec
    
    Signed-off-by: Fan Du <fan.du@intel.com>
    Acked-by: John Heffner <johnwheffner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 834089b0cffc..1ad82e334e27 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -67,6 +67,9 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* The least MTU to use for probing */
 #define TCP_BASE_MSS		1024
 
+/* Specify interval when tcp mtu probing will stop */
+#define TCP_PROBE_THRESHOLD	8
+
 /* After receiving this amount of duplicate ACKs fast retransmit starts. */
 #define TCP_FASTRETRANS_THRESH 3
 

commit dcd8fb8533ceb493146ce030d15f7965b82d0c27
Author: Fan Du <fan.du@intel.com>
Date:   Fri Mar 6 11:18:22 2015 +0800

    ipv4: Raise tcp PMTU probe mss base size
    
    Quotes from RFC4821 7.2.  Selecting Initial Values
    
       It is RECOMMENDED that search_low be initially set to an MTU size
       that is likely to work over a very wide range of environments.  Given
       today's technologies, a value of 1024 bytes is probably safe enough.
       The initial value for search_low SHOULD be configurable.
    
    Moreover, set a small value will introduce extra time for the search
    to converge. So set the initial probe base mss size to 1024 Bytes.
    
    Signed-off-by: Fan Du <fan.du@intel.com>
    Acked-by: John Heffner <johnwheffner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f87599d5af82..834089b0cffc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -65,7 +65,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCP_MIN_MSS		88U
 
 /* The least MTU to use for probing */
-#define TCP_BASE_MSS		512
+#define TCP_BASE_MSS		1024
 
 /* After receiving this amount of duplicate ACKs fast retransmit starts. */
 #define TCP_FASTRETRANS_THRESH 3

commit 1b784140474e4fc94281a49e96c67d29df0efbde
Author: Ying Xue <ying.xue@windriver.com>
Date:   Mon Mar 2 15:37:48 2015 +0800

    net: Remove iocb argument from sendmsg and recvmsg
    
    After TIPC doesn't depend on iocb argument in its internal
    implementations of sendmsg() and recvmsg() hooks defined in proto
    structure, no any user is using iocb argument in them at all now.
    Then we can drop the redundant iocb argument completely from kinds of
    implementations of both sendmsg() and recvmsg() in the entire
    networking stack.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Suggested-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Ying Xue <ying.xue@windriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8d6b983d5099..f87599d5af82 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -349,8 +349,7 @@ void tcp_v4_early_demux(struct sk_buff *skb);
 int tcp_v4_rcv(struct sk_buff *skb);
 
 int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
-int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
-		size_t size);
+int tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tcp_sendpage(struct sock *sk, struct page *page, int offset, size_t size,
 		 int flags);
 void tcp_release_cb(struct sock *sk);
@@ -430,8 +429,8 @@ int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
 			  char __user *optval, unsigned int optlen);
 void tcp_set_keepalive(struct sock *sk, int val);
 void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
-int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
-		size_t len, int nonblock, int flags, int *addr_len);
+int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
+		int flags, int *addr_len);
 void tcp_parse_options(const struct sk_buff *skb,
 		       struct tcp_options_received *opt_rx,
 		       int estab, struct tcp_fastopen_cookie *foc);

commit b0f9ca53cbb103e9240a29a974e0b6085e58f9f7
Author: Fan Du <fan.du@intel.com>
Date:   Tue Feb 10 09:53:16 2015 +0800

    ipv4: Namespecify TCP PMTU mechanism
    
    Packetization Layer Path MTU Discovery works separately beside
    Path MTU Discovery at IP level, different net namespace has
    various requirements on which one to chose, e.g., a virutalized
    container instance would require TCP PMTU to probe an usable
    effective mtu for underlying tunnel, while the host would
    employ classical ICMP based PMTU to function.
    
    Hence making TCP PMTU mechanism per net namespace to decouple
    two functionality. Furthermore the probe base MSS should also
    be configured separately for each namespace.
    
    Signed-off-by: Fan Du <fan.du@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index da4196fb78db..8d6b983d5099 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -262,8 +262,6 @@ extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
-extern int sysctl_tcp_mtu_probing;
-extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_thin_linear_timeouts;

commit a9b2c06dbef48ed31cff1764c5ce824829106f4f
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Feb 6 16:04:39 2015 -0500

    tcp: mitigate ACK loops for connections as tcp_request_sock
    
    In the SYN_RECV state, where the TCP connection is represented by
    tcp_request_sock, we now rate-limit SYNACKs in response to a client's
    retransmitted SYNs: we do not send a SYNACK in response to client SYN
    if it has been less than sysctl_tcp_invalid_ratelimit (default 500ms)
    since we last sent a SYNACK in response to a client's retransmitted
    SYN.
    
    This allows the vast majority of legitimate client connections to
    proceed unimpeded, even for the most aggressive platforms, iOS and
    MacOS, which actually retransmit SYNs 1-second intervals for several
    times in a row. They use SYN RTO timeouts following the progression:
    1,1,1,1,1,2,4,8,16,32.
    
    Reported-by: Avery Fay <avery@mixpanel.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b81f45c67b2e..da4196fb78db 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1145,6 +1145,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
 	tcp_rsk(req)->snt_synack = tcp_time_stamp;
+	tcp_rsk(req)->last_oow_ack_time = 0;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
 	ireq->tstamp_ok = rx_opt->tstamp_ok;

commit 032ee4236954eb214651cb9bfc1b38ffa8fd7a01
Author: Neal Cardwell <ncardwell@google.com>
Date:   Fri Feb 6 16:04:38 2015 -0500

    tcp: helpers to mitigate ACK loops by rate-limiting out-of-window dupacks
    
    Helpers for mitigating ACK loops by rate-limiting dupacks sent in
    response to incoming out-of-window packets.
    
    This patch includes:
    
    - rate-limiting logic
    - sysctl to control how often we allow dupacks to out-of-window packets
    - SNMP counter for cases where we rate-limited our dupack sending
    
    The rate-limiting logic in this patch decides to not send dupacks in
    response to out-of-window segments if (a) they are SYNs or pure ACKs
    and (b) the remote endpoint is sending them faster than the configured
    rate limit.
    
    We rate-limit our responses rather than blocking them entirely or
    resetting the connection, because legitimate connections can rely on
    dupacks in response to some out-of-window segments. For example, zero
    window probes are typically sent with a sequence number that is below
    the current window, and ZWPs thus expect to thus elicit a dupack in
    response.
    
    We allow dupacks in response to TCP segments with data, because these
    may be spurious retransmissions for which the remote endpoint wants to
    receive DSACKs. This is safe because segments with data can't
    realistically be part of ACK loops, which by their nature consist of
    each side sending pure/data-less ACKs to each other.
    
    The dupack interval is controlled by a new sysctl knob,
    tcp_invalid_ratelimit, given in milliseconds, in case an administrator
    needs to dial this upward in the face of a high-rate DoS attack. The
    name and units are chosen to be analogous to the existing analogous
    knob for ICMP, icmp_ratelimit.
    
    The default value for tcp_invalid_ratelimit is 500ms, which allows at
    most one such dupack per 500ms. This is chosen to be 2x faster than
    the 1-second minimum RTO interval allowed by RFC 6298 (section 2, rule
    2.4). We allow the extra 2x factor because network delay variations
    can cause packets sent at 1 second intervals to be compressed and
    arrive much closer.
    
    Reported-by: Avery Fay <avery@mixpanel.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 28e9bd3abceb..b81f45c67b2e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -274,6 +274,7 @@ extern int sysctl_tcp_challenge_ack_limit;
 extern unsigned int sysctl_tcp_notsent_lowat;
 extern int sysctl_tcp_min_tso_segs;
 extern int sysctl_tcp_autocorking;
+extern int sysctl_tcp_invalid_ratelimit;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -1236,6 +1237,37 @@ static inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,
 	return true;
 }
 
+/* Return true if we're currently rate-limiting out-of-window ACKs and
+ * thus shouldn't send a dupack right now. We rate-limit dupacks in
+ * response to out-of-window SYNs or ACKs to mitigate ACK loops or DoS
+ * attacks that send repeated SYNs or ACKs for the same connection. To
+ * do this, we do not send a duplicate SYNACK or ACK if the remote
+ * endpoint is sending out-of-window SYNs or pure ACKs at a high rate.
+ */
+static inline bool tcp_oow_rate_limited(struct net *net,
+					const struct sk_buff *skb,
+					int mib_idx, u32 *last_oow_ack_time)
+{
+	/* Data packets without SYNs are not likely part of an ACK loop. */
+	if ((TCP_SKB_CB(skb)->seq != TCP_SKB_CB(skb)->end_seq) &&
+	    !tcp_hdr(skb)->syn)
+		goto not_rate_limited;
+
+	if (*last_oow_ack_time) {
+		s32 elapsed = (s32)(tcp_time_stamp - *last_oow_ack_time);
+
+		if (0 <= elapsed && elapsed < sysctl_tcp_invalid_ratelimit) {
+			NET_INC_STATS_BH(net, mib_idx);
+			return true;	/* rate-limited: don't send yet! */
+		}
+	}
+
+	*last_oow_ack_time = tcp_time_stamp;
+
+not_rate_limited:
+	return false;	/* not rate-limited: go ahead, send dupack now! */
+}
+
 static inline void tcp_mib_init(struct net *net)
 {
 	/* See RFC 2012 */

commit 6e03f896b52cd2ca88942170c5c9c407ec0ede69
Merge: db79a621835e 9d82f5eb3376
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Feb 5 14:33:28 2015 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/vxlan.c
            drivers/vhost/net.c
            include/linux/if_vlan.h
            net/core/dev.c
    
    The net/core/dev.c conflict was the overlap of one commit marking an
    existing function static whilst another was adding a new function.
    
    In the include/linux/if_vlan.h case, the type used for a local
    variable was changed in 'net', whereas the function got rewritten
    to fix a stacked vlan bug in 'net-next'.
    
    In drivers/vhost/net.c, Al Viro's iov_iter conversions in 'net-next'
    overlapped with an endainness fix for VHOST 1.0 in 'net'.
    
    In drivers/net/vxlan.c, vxlan_find_vni() added a 'flags' parameter
    in 'net-next' whereas in 'net' there was a bug fix to pass in the
    correct network namespace pointer in calls to this function.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9878196578286c5ed494778ada01da094377a686
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 3 18:31:53 2015 -0800

    tcp: do not pace pure ack packets
    
    When we added pacing to TCP, we decided to let sch_fq take care
    of actual pacing.
    
    All TCP had to do was to compute sk->pacing_rate using simple formula:
    
    sk->pacing_rate = 2 * cwnd * mss / rtt
    
    It works well for senders (bulk flows), but not very well for receivers
    or even RPC :
    
    cwnd on the receiver can be less than 10, rtt can be around 100ms, so we
    can end up pacing ACK packets, slowing down the sender.
    
    Really, only the sender should pace, according to its own logic.
    
    Instead of adding a new bit in skb, or call yet another flow
    dissection, we tweak skb->truesize to a small value (2), and
    we instruct sch_fq to use new helper and not pace pure ack.
    
    Note this also helps TCP small queue, as ack packets present
    in qdisc/NIC do not prevent sending a data packet (RPC workload)
    
    This helps to reduce tx completion overhead, ack packets can use regular
    sock_wfree() instead of tcp_wfree() which is a bit more expensive.
    
    This has no impact in the case packets are sent to loopback interface,
    as we do not coalesce ack packets (were we would detect skb->truesize
    lie)
    
    In case netem (with a delay) is used, skb_orphan_partial() also sets
    skb->truesize to 1.
    
    This patch is a combination of two patches we used for about one year at
    Google.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b8fdc6bab3f3..637ee490ec81 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1713,4 +1713,19 @@ static inline struct ip_options_rcu *tcp_v4_save_options(struct sk_buff *skb)
 	return dopt;
 }
 
+/* locally generated TCP pure ACKs have skb->truesize == 2
+ * (check tcp_send_ack() in net/ipv4/tcp_output.c )
+ * This is much faster than dissecting the packet to find out.
+ * (Think of GRE encapsulations, IPv4, IPv6, ...)
+ */
+static inline bool skb_is_tcp_pure_ack(const struct sk_buff *skb)
+{
+	return skb->truesize == 2;
+}
+
+static inline void skb_set_tcp_pure_ack(struct sk_buff *skb)
+{
+	skb->truesize = 2;
+}
+
 #endif	/* _TCP_H */

commit e73ebb0881ea5534ce606c1d71b4ac44db5c6930
Author: Neal Cardwell <ncardwell@google.com>
Date:   Wed Jan 28 20:01:35 2015 -0500

    tcp: stretch ACK fixes prep
    
    LRO, GRO, delayed ACKs, and middleboxes can cause "stretch ACKs" that
    cover more than the RFC-specified maximum of 2 packets. These stretch
    ACKs can cause serious performance shortfalls in common congestion
    control algorithms that were designed and tuned years ago with
    receiver hosts that were not using LRO or GRO, and were instead
    politely ACKing every other packet.
    
    This patch series fixes Reno and CUBIC to handle stretch ACKs.
    
    This patch prepares for the upcoming stretch ACK bug fix patches. It
    adds an "acked" parameter to tcp_cong_avoid_ai() to allow for future
    fixes to tcp_cong_avoid_ai() to correctly handle stretch ACKs, and
    changes all congestion control algorithms to pass in 1 for the ACKed
    count. It also changes tcp_slow_start() to return the number of packet
    ACK "credits" that were not processed in slow start mode, and can be
    processed by the congestion control module in additive increase mode.
    
    In future patches we will fix tcp_cong_avoid_ai() to handle stretch
    ACKs, and fix Reno and CUBIC handling of stretch ACKs in slow start
    and additive increase mode.
    
    Reported-by: Eyal Perry <eyalpe@mellanox.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f50f29faf76f..9d9111ef43ae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -834,8 +834,8 @@ void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
 int tcp_set_congestion_control(struct sock *sk, const char *name);
-void tcp_slow_start(struct tcp_sock *tp, u32 acked);
-void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
+u32 tcp_slow_start(struct tcp_sock *tp, u32 acked);
+void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);
 
 u32 tcp_reno_ssthresh(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);

commit 81164413ad096bafe8ad1068f3f095a7dd081d8b
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Jan 5 23:57:48 2015 +0100

    net: tcp: add per route congestion control
    
    This work adds the possibility to define a per route/destination
    congestion control algorithm. Generally, this opens up the possibility
    for a machine with different links to enforce specific congestion
    control algorithms with optimal strategies for each of them based
    on their network characteristics, even transparently for a single
    application listening on all links.
    
    For our specific use case, this additionally facilitates deployment
    of DCTCP, for example, applications can easily serve internal
    traffic/dsts in DCTCP and external one with CUBIC. Other scenarios
    would also allow for utilizing e.g. long living, low priority
    background flows for certain destinations/routes while still being
    able for normal traffic to utilize the default congestion control
    algorithm. We also thought about a per netns setting (where different
    defaults are possible), but given its actually a link specific
    property, we argue that a per route/destination setting is the most
    natural and flexible.
    
    The administrator can utilize this through ip-route(8) by appending
    "congctl [lock] <name>", where <name> denotes the name of a
    congestion control algorithm and the optional lock parameter allows
    to enforce the given algorithm so that applications in user space
    would not be allowed to overwrite that algorithm for that destination.
    
    The dst metric lookups are being done when a dst entry is already
    available in order to avoid a costly lookup and still before the
    algorithms are being initialized, thus overhead is very low when the
    feature is not being used. While the client side would need to drop
    the current reference on the module, on server side this can actually
    even be avoided as we just got a flat-copied socket clone.
    
    Joint work with Florian Westphal.
    
    Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 95bb237152e0..b8fdc6bab3f3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -448,6 +448,7 @@ int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_create_openreq_child(struct sock *sk,
 				      struct request_sock *req,
 				      struct sk_buff *skb);
+void tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);
 struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
 				  struct request_sock *req,
 				  struct dst_entry *dst);
@@ -636,6 +637,11 @@ static inline u32 tcp_rto_min_us(struct sock *sk)
 	return jiffies_to_usecs(tcp_rto_min(sk));
 }
 
+static inline bool tcp_ca_dst_locked(const struct dst_entry *dst)
+{
+	return dst_metric_locked(dst, RTAX_CC_ALGO);
+}
+
 /* Compute the actual receive window we are currently advertising.
  * Rcv_nxt can be after the window if our peer push more data
  * than the offered window.

commit ea697639992d96da98016b8934e68a73876a2264
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Jan 5 23:57:47 2015 +0100

    net: tcp: add RTAX_CC_ALGO fib handling
    
    This patch adds the minimum necessary for the RTAX_CC_ALGO congestion
    control metric to be set up and dumped back to user space.
    
    While the internal representation of RTAX_CC_ALGO is handled as a u32
    key, we avoided to expose this implementation detail to user space, thus
    instead, we chose the netlink attribute that is being exchanged between
    user space to be the actual congestion control algorithm name, similarly
    as in the setsockopt(2) API in order to allow for maximum flexibility,
    even for 3rd party modules.
    
    It is a bit unfortunate that RTAX_QUICKACK used up a whole RTAX slot as
    it should have been stored in RTAX_FEATURES instead, we first thought
    about reusing it for the congestion control key, but it brings more
    complications and/or confusion than worth it.
    
    Joint work with Florian Westphal.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 135b70c9a734..95bb237152e0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -846,7 +846,14 @@ extern struct tcp_congestion_ops tcp_reno;
 
 struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
 u32 tcp_ca_get_key_by_name(const char *name);
+#ifdef CONFIG_INET
 char *tcp_ca_get_name_by_key(u32 key, char *buffer);
+#else
+static inline char *tcp_ca_get_name_by_key(u32 key, char *buffer)
+{
+	return NULL;
+}
+#endif
 
 static inline bool tcp_ca_needs_ecn(const struct sock *sk)
 {

commit c5c6a8ab45ec0f18733afb4aaade0d4a139d80b3
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Mon Jan 5 23:57:46 2015 +0100

    net: tcp: add key management to congestion control
    
    This patch adds necessary infrastructure to the congestion control
    framework for later per route congestion control support.
    
    For a per route congestion control possibility, our aim is to store
    a unique u32 key identifier into dst metrics, which can then be
    mapped into a tcp_congestion_ops struct. We argue that having a
    RTAX key entry is the most simple, generic and easy way to manage,
    and also keeps the memory footprint of dst entries lower on 64 bit
    than with storing a pointer directly, for example. Having a unique
    key id also allows for decoupling actual TCP congestion control
    module management from the FIB layer, i.e. we don't have to care
    about expensive module refcounting inside the FIB at this point.
    
    We first thought of using an IDR store for the realization, which
    takes over dynamic assignment of unused key space and also performs
    the key to pointer mapping in RCU. While doing so, we stumbled upon
    the issue that due to the nature of dynamic key distribution, it
    just so happens, arguably in very rare occasions, that excessive
    module loads and unloads can lead to a possible reuse of previously
    used key space. Thus, previously stale keys in the dst metric are
    now being reassigned to a different congestion control algorithm,
    which might lead to unexpected behaviour. One way to resolve this
    would have been to walk FIBs on the actually rare occasion of a
    module unload and reset the metric keys for each FIB in each netns,
    but that's just very costly.
    
    Therefore, we argue a better solution is to reuse the unique
    congestion control algorithm name member and map that into u32 key
    space through jhash. For that, we split the flags attribute (as it
    currently uses 2 bits only anyway) into two u32 attributes, flags
    and key, so that we can keep the cacheline boundary of 2 cachelines
    on x86_64 and cache the precalculated key at registration time for
    the fast path. On average we might expect 2 - 4 modules being loaded
    worst case perhaps 15, so a key collision possibility is extremely
    low, and guaranteed collision-free on LE/BE for all in-tree modules.
    Overall this results in much simpler code, and all without the
    overhead of an IDR. Due to the deterministic nature, modules can
    now be unloaded, the congestion control algorithm for a specific
    but unloaded key will fall back to the default one, and on module
    reload time it will switch back to the expected algorithm
    transparently.
    
    Joint work with Florian Westphal.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f50f29faf76f..135b70c9a734 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -787,6 +787,8 @@ enum tcp_ca_ack_event_flags {
 #define TCP_CA_MAX	128
 #define TCP_CA_BUF_MAX	(TCP_CA_NAME_MAX*TCP_CA_MAX)
 
+#define TCP_CA_UNSPEC	0
+
 /* Algorithm can be set on socket without CAP_NET_ADMIN privileges */
 #define TCP_CONG_NON_RESTRICTED 0x1
 /* Requires ECN/ECT set on all packets */
@@ -794,7 +796,8 @@ enum tcp_ca_ack_event_flags {
 
 struct tcp_congestion_ops {
 	struct list_head	list;
-	unsigned long flags;
+	u32 key;
+	u32 flags;
 
 	/* initialize private data (optional) */
 	void (*init)(struct sock *sk);
@@ -841,6 +844,10 @@ u32 tcp_reno_ssthresh(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
+struct tcp_congestion_ops *tcp_ca_find_key(u32 key);
+u32 tcp_ca_get_key_by_name(const char *name);
+char *tcp_ca_get_name_by_key(u32 key, char *buffer);
+
 static inline bool tcp_ca_needs_ecn(const struct sock *sk)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);

commit f7b3bec6f5167efaf56b756abfafb924cb1d3050
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Nov 3 17:35:03 2014 +0100

    net: allow setting ecn via routing table
    
    This patch allows to set ECN on a per-route basis in case the sysctl
    tcp_ecn is not set to 1. In other words, when ECN is set for specific
    routes, it provides a tcp_ecn=1 behaviour for that route while the rest
    of the stack acts according to the global settings.
    
    One can use 'ip route change dev $dev $net features ecn' to toggle this.
    
    Having a more fine-grained per-route setting can be beneficial for various
    reasons, for example, 1) within data centers, or 2) local ISPs may deploy
    ECN support for their own video/streaming services [1], etc.
    
    There was a recent measurement study/paper [2] which scanned the Alexa's
    publicly available top million websites list from a vantage point in US,
    Europe and Asia:
    
    Half of the Alexa list will now happily use ECN (tcp_ecn=2, most likely
    blamed to commit 255cac91c3 ("tcp: extend ECN sysctl to allow server-side
    only ECN") ;)); the break in connectivity on-path was found is about
    1 in 10,000 cases. Timeouts rather than receiving back RSTs were much
    more common in the negotiation phase (and mostly seen in the Alexa
    middle band, ranks around 50k-150k): from 12-thousand hosts on which
    there _may_ be ECN-linked connection failures, only 79 failed with RST
    when _not_ failing with RST when ECN is not requested.
    
    It's unclear though, how much equipment in the wild actually marks CE
    when buffers start to fill up.
    
    We thought about a fallback to non-ECN for retransmitted SYNs as another
    global option (which could perhaps one day be made default), but as Eric
    points out, there's much more work needed to detect broken middleboxes.
    
    Two examples Eric mentioned are buggy firewalls that accept only a single
    SYN per flow, and middleboxes that successfully let an ECN flow establish,
    but later mark CE for all packets (so cwnd converges to 1).
    
     [1] http://www.ietf.org/proceedings/89/slides/slides-89-tsvarea-1.pdf, p.15
     [2] http://ecn.ethz.ch/
    
    Joint work with Daniel Borkmann.
    
    Reference: http://thread.gmane.org/gmane.linux.network/335797
    Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 36c5084964cd..f50f29faf76f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -493,7 +493,7 @@ __u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
 __u32 cookie_init_timestamp(struct request_sock *req);
 bool cookie_timestamp_decode(struct tcp_options_received *opt);
 bool cookie_ecn_ok(const struct tcp_options_received *opt,
-		   const struct net *net);
+		   const struct net *net, const struct dst_entry *dst);
 
 /* From net/ipv6/syncookies.c */
 int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,

commit f1673381b1481a409238d4552a0700d490c5b36c
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Nov 3 17:35:02 2014 +0100

    syncookies: split cookie_check_timestamp() into two functions
    
    The function cookie_check_timestamp(), both called from IPv4/6 context,
    is being used to decode the echoed timestamp from the SYN/ACK into TCP
    options used for follow-up communication with the peer.
    
    We can remove ECN handling from that function, split it into a separate
    one, and simply rename the original function into cookie_decode_options().
    cookie_decode_options() just fills in tcp_option struct based on the
    echoed timestamp received from the peer. Anything that fails in this
    function will actually discard the request socket.
    
    While this is the natural place for decoding options such as ECN which
    commit 172d69e63c7f ("syncookies: add support for ECN") added, we argue
    that in particular for ECN handling, it can be checked at a later point
    in time as the request sock would actually not need to be dropped from
    this, but just ECN support turned off.
    
    Therefore, we split this functionality into cookie_ecn_ok(), which tells
    us if the timestamp indicates ECN support AND the tcp_ecn sysctl is enabled.
    
    This prepares for per-route ECN support: just looking at the tcp_ecn sysctl
    won't be enough anymore at that point; if the timestamp indicates ECN
    and sysctl tcp_ecn == 0, we will also need to check the ECN dst metric.
    
    This would mean adding a route lookup to cookie_check_timestamp(), which
    we definitely want to avoid. As we already do a route lookup at a later
    point in cookie_{v4,v6}_check(), we can simply make use of that as well
    for the new cookie_ecn_ok() function w/o any additional cost.
    
    Joint work with Daniel Borkmann.
    
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3a35b1500359..36c5084964cd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -490,17 +490,16 @@ u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
 			      __u16 *mss);
-#endif
-
 __u32 cookie_init_timestamp(struct request_sock *req);
-bool cookie_check_timestamp(struct tcp_options_received *opt, struct net *net,
-			    bool *ecn_ok);
+bool cookie_timestamp_decode(struct tcp_options_received *opt);
+bool cookie_ecn_ok(const struct tcp_options_received *opt,
+		   const struct net *net);
 
 /* From net/ipv6/syncookies.c */
 int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
 		      u32 cookie);
 struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
-#ifdef CONFIG_SYN_COOKIES
+
 u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 			      const struct tcphdr *th, u16 *mssp);
 __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,

commit dca145ffaa8d39ea1904491ac81b92b7049372c0
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 27 21:45:24 2014 -0700

    tcp: allow for bigger reordering level
    
    While testing upcoming Yaogong patch (converting out of order queue
    into an RB tree), I hit the max reordering level of linux TCP stack.
    
    Reordering level was limited to 127 for no good reason, and some
    network setups [1] can easily reach this limit and get limited
    throughput.
    
    Allow a new max limit of 300, and add a sysctl to allow admins to even
    allow bigger (or lower) values if needed.
    
    [1] Aggregation of links, per packet load balancing, fabrics not doing
     deep packet inspections, alternative TCP congestion modules...
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yaogong Wang <wygivan@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c73fc145ee45..3a35b1500359 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -70,9 +70,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* After receiving this amount of duplicate ACKs fast retransmit starts. */
 #define TCP_FASTRETRANS_THRESH 3
 
-/* Maximal reordering. */
-#define TCP_MAX_REORDERING	127
-
 /* Maximal number of ACKs sent quickly to accelerate slow-start. */
 #define TCP_MAX_QUICKACKS	16U
 
@@ -252,6 +249,7 @@ extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;
 extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
+extern int sysctl_tcp_max_reordering;
 extern int sysctl_tcp_dsack;
 extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];

commit 105970f6087ae240b00deaff85773ed9bf381145
Author: Kenjiro Nakayama <nakayamakenjiro@gmail.com>
Date:   Mon Oct 20 18:15:50 2014 +0900

    net: Remove trailing whitespace in tcp.h icmp.c syncookies.c
    
    Remove trailing whitespace in tcp.h icmp.c syncookies.c
    
    Signed-off-by: Kenjiro Nakayama <nakayamakenjiro@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4062b4f0d121..c73fc145ee45 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -55,9 +55,9 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 #define MAX_TCP_OPTION_SPACE 40
 
-/* 
+/*
  * Never offer a window over 32767 without using window scaling. Some
- * poor stacks do signed 16bit maths! 
+ * poor stacks do signed 16bit maths!
  */
 #define MAX_TCP_WINDOW		32767U
 
@@ -167,7 +167,7 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 /*
  *	TCP option
  */
- 
+
 #define TCPOPT_NOP		1	/* Padding */
 #define TCPOPT_EOL		0	/* End of options */
 #define TCPOPT_MSS		2	/* Segment size negotiating */
@@ -1104,16 +1104,16 @@ static inline int tcp_win_from_space(int space)
 		space - (space>>sysctl_tcp_adv_win_scale);
 }
 
-/* Note: caller must be prepared to deal with negative returns */ 
+/* Note: caller must be prepared to deal with negative returns */
 static inline int tcp_space(const struct sock *sk)
 {
 	return tcp_win_from_space(sk->sk_rcvbuf -
 				  atomic_read(&sk->sk_rmem_alloc));
-} 
+}
 
 static inline int tcp_full_space(const struct sock *sk)
 {
-	return tcp_win_from_space(sk->sk_rcvbuf); 
+	return tcp_win_from_space(sk->sk_rcvbuf);
 }
 
 static inline void tcp_openreq_init(struct request_sock *req,

commit 815afe1785da727ee48cd8e63ac4a3ec6c7459b3
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Oct 18 08:34:37 2014 -0700

    tcp: fix build error if IPv6 is not enabled
    
    $ make M=net/ipv4
      CC      net/ipv4/route.o
    In file included from net/ipv4/route.c:102:0:
    include/net/tcp.h: In function ‘tcp_v6_iif’:
    include/net/tcp.h:738:32: error: ‘union <anonymous>’ has no member named ‘h6’
      return TCP_SKB_CB(skb)->header.h6.iif;
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Fixes: 870c3151382c ("ipv6: introduce tcp_v6_iif()")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c9766f89deba..4062b4f0d121 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -730,6 +730,7 @@ struct tcp_skb_cb {
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
 
+#if IS_ENABLED(CONFIG_IPV6)
 /* This is the variant of inet6_iif() that must be used by TCP,
  * as TCP moves IP6CB into a different location in skb->cb[]
  */
@@ -737,6 +738,7 @@ static inline int tcp_v6_iif(const struct sk_buff *skb)
 {
 	return TCP_SKB_CB(skb)->header.h6.iif;
 }
+#endif
 
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.

commit 870c3151382c980590d4d609babf3b0243e7db93
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 17 09:17:20 2014 -0700

    ipv6: introduce tcp_v6_iif()
    
    Commit 971f10eca186 ("tcp: better TCP_SKB_CB layout to reduce cache line
    misses") added a regression for SO_BINDTODEVICE on IPv6.
    
    This is because we still use inet6_iif() which expects that IP6 control
    block is still at the beginning of skb->cb[]
    
    This patch adds tcp_v6_iif() helper and uses it where necessary.
    
    Because __inet6_lookup_skb() is used by TCP and DCCP, we add an iif
    parameter to it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Fixes: 971f10eca186 ("tcp: better TCP_SKB_CB layout to reduce cache line misses")
    Acked-by: Cong Wang <cwang@twopensource.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3a4bbbfdcf8d..c9766f89deba 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -729,6 +729,15 @@ struct tcp_skb_cb {
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
+
+/* This is the variant of inet6_iif() that must be used by TCP,
+ * as TCP moves IP6CB into a different location in skb->cb[]
+ */
+static inline int tcp_v6_iif(const struct sk_buff *skb)
+{
+	return TCP_SKB_CB(skb)->header.h6.iif;
+}
+
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */

commit 461b74c391c4ec9c766794e158508c357d8952e6
Author: Cong Wang <cwang@twopensource.com>
Date:   Wed Oct 15 14:33:22 2014 -0700

    ipv4: clean up cookie_v4_check()
    
    We can retrieve opt from skb, no need to pass it as a parameter.
    And opt should always be non-NULL, no need to check.
    
    Cc: Krzysztof Kolasa <kkolasa@winsoft.pl>
    Cc: Eric Dumazet <edumazet@google.com>
    Tested-by: Krzysztof Kolasa <kkolasa@winsoft.pl>
    Signed-off-by: Cong Wang <cwang@twopensource.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 869637a7caf7..3a4bbbfdcf8d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -468,8 +468,7 @@ void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 /* From syncookies.c */
 int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 		      u32 cookie);
-struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
-			     struct ip_options *opt);
+struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
 
 /* Syncookies use a monotonic timer which increments every 60 seconds.
@@ -1674,7 +1673,7 @@ static inline struct ip_options_rcu *tcp_v4_save_options(struct sk_buff *skb)
 	const struct ip_options *opt = &TCP_SKB_CB(skb)->header.h4.opt;
 	struct ip_options_rcu *dopt = NULL;
 
-	if (opt && opt->optlen) {
+	if (opt->optlen) {
 		int opt_size = sizeof(*dopt) + opt->optlen;
 
 		dopt = kmalloc(opt_size, GFP_ATOMIC);

commit e25f866fbc8a4bf387b5dbe8e25aa5b07e55c74f
Author: Cong Wang <cwang@twopensource.com>
Date:   Wed Oct 15 14:33:21 2014 -0700

    ipv4: share tcp_v4_save_options() with cookie_v4_check()
    
    cookie_v4_check() allocates ip_options_rcu in the same way
    with tcp_v4_save_options(), we can just make it a helper function.
    
    Cc: Krzysztof Kolasa <kkolasa@winsoft.pl>
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Cong Wang <cwang@twopensource.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 74efeda994b3..869637a7caf7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1666,4 +1666,24 @@ int tcpv4_offload_init(void);
 void tcp_v4_init(void);
 void tcp_init(void);
 
+/*
+ * Save and compile IPv4 options, return a pointer to it
+ */
+static inline struct ip_options_rcu *tcp_v4_save_options(struct sk_buff *skb)
+{
+	const struct ip_options *opt = &TCP_SKB_CB(skb)->header.h4.opt;
+	struct ip_options_rcu *dopt = NULL;
+
+	if (opt && opt->optlen) {
+		int opt_size = sizeof(*dopt) + opt->optlen;
+
+		dopt = kmalloc(opt_size, GFP_ATOMIC);
+		if (dopt && __ip_options_echo(&dopt->opt, skb, opt)) {
+			kfree(dopt);
+			dopt = NULL;
+		}
+	}
+	return dopt;
+}
+
 #endif	/* _TCP_H */

commit 35a9ad8af0bb0fa3525e6d0d20e32551d226f38e
Merge: d5935b07da53 64b1f00a0830
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 21:40:54 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Most notable changes in here:
    
       1) By far the biggest accomplishment, thanks to a large range of
          contributors, is the addition of multi-send for transmit.  This is
          the result of discussions back in Chicago, and the hard work of
          several individuals.
    
          Now, when the ->ndo_start_xmit() method of a driver sees
          skb->xmit_more as true, it can choose to defer the doorbell
          telling the driver to start processing the new TX queue entires.
    
          skb->xmit_more means that the generic networking is guaranteed to
          call the driver immediately with another SKB to send.
    
          There is logic added to the qdisc layer to dequeue multiple
          packets at a time, and the handling mis-predicted offloads in
          software is now done with no locks held.
    
          Finally, pktgen is extended to have a "burst" parameter that can
          be used to test a multi-send implementation.
    
          Several drivers have xmit_more support: i40e, igb, ixgbe, mlx4,
          virtio_net
    
          Adding support is almost trivial, so export more drivers to
          support this optimization soon.
    
          I want to thank, in no particular or implied order, Jesper
          Dangaard Brouer, Eric Dumazet, Alexander Duyck, Tom Herbert, Jamal
          Hadi Salim, John Fastabend, Florian Westphal, Daniel Borkmann,
          David Tat, Hannes Frederic Sowa, and Rusty Russell.
    
       2) PTP and timestamping support in bnx2x, from Michal Kalderon.
    
       3) Allow adjusting the rx_copybreak threshold for a driver via
          ethtool, and add rx_copybreak support to enic driver.  From
          Govindarajulu Varadarajan.
    
       4) Significant enhancements to the generic PHY layer and the bcm7xxx
          driver in particular (EEE support, auto power down, etc.) from
          Florian Fainelli.
    
       5) Allow raw buffers to be used for flow dissection, allowing drivers
          to determine the optimal "linear pull" size for devices that DMA
          into pools of pages.  The objective is to get exactly the
          necessary amount of headers into the linear SKB area pre-pulled,
          but no more.  The new interface drivers use is eth_get_headlen().
          From WANG Cong, with driver conversions (several had their own
          by-hand duplicated implementations) by Alexander Duyck and Eric
          Dumazet.
    
       6) Support checksumming more smoothly and efficiently for
          encapsulations, and add "foo over UDP" facility.  From Tom
          Herbert.
    
       7) Add Broadcom SF2 switch driver to DSA layer, from Florian
          Fainelli.
    
       8) eBPF now can load programs via a system call and has an extensive
          testsuite.  Alexei Starovoitov and Daniel Borkmann.
    
       9) Major overhaul of the packet scheduler to use RCU in several major
          areas such as the classifiers and rate estimators.  From John
          Fastabend.
    
      10) Add driver for Intel FM10000 Ethernet Switch, from Alexander
          Duyck.
    
      11) Rearrange TCP_SKB_CB() to reduce cache line misses, from Eric
          Dumazet.
    
      12) Add Datacenter TCP congestion control algorithm support, From
          Florian Westphal.
    
      13) Reorganize sk_buff so that __copy_skb_header() is significantly
          faster.  From Eric Dumazet"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1558 commits)
      netlabel: directly return netlbl_unlabel_genl_init()
      net: add netdev_txq_bql_{enqueue, complete}_prefetchw() helpers
      net: description of dma_cookie cause make xmldocs warning
      cxgb4: clean up a type issue
      cxgb4: potential shift wrapping bug
      i40e: skb->xmit_more support
      net: fs_enet: Add NAPI TX
      net: fs_enet: Remove non NAPI RX
      r8169:add support for RTL8168EP
      net_sched: copy exts->type in tcf_exts_change()
      wimax: convert printk to pr_foo()
      af_unix: remove 0 assignment on static
      ipv6: Do not warn for informational ICMP messages, regardless of type.
      Update Intel Ethernet Driver maintainers list
      bridge: Save frag_max_size between PRE_ROUTING and POST_ROUTING
      tipc: fix bug in multicast congestion handling
      net: better IFF_XMIT_DST_RELEASE support
      net/mlx4_en: remove NETDEV_TX_BUSY
      3c59x: fix bad split of cpu_to_le32(pci_map_single())
      net: bcmgenet: fix Tx ring priority programming
      ...

commit d0cd84817c745655428dbfdb1e3f754230b46bef
Merge: bdf428feb225 3f3340785672
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 7 20:39:25 2014 -0400

    Merge tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine updates from Dan Williams:
     "Even though this has fixes marked for -stable, given the size and the
      needed conflict resolutions this is 3.18-rc1/merge-window material.
    
      These patches have been languishing in my tree for a long while.  The
      fact that I do not have the time to do proper/prompt maintenance of
      this tree is a primary factor in the decision to step down as
      dmaengine maintainer.  That and the fact that the bulk of drivers/dma/
      activity is going through Vinod these days.
    
      The net_dma removal has not been in -next.  It has developed simple
      conflicts against mainline and net-next (for-3.18).
    
      Continuing thanks to Vinod for staying on top of drivers/dma/.
    
      Summary:
    
       1/ Step down as dmaengine maintainer see commit 08223d80df38
          "dmaengine maintainer update"
    
       2/ Removal of net_dma, as it has been marked 'broken' since 3.13
          (commit 77873803363c "net_dma: mark broken"), without reports of
          performance regression.
    
       3/ Miscellaneous fixes"
    
    * tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      net: make tcp_cleanup_rbuf private
      net_dma: revert 'copied_early'
      net_dma: simple removal
      dmaengine maintainer update
      dmatest: prevent memory leakage on error path in thread
      ioat: Use time_before_jiffies()
      dmaengine: fix xor sources continuation
      dma: mv_xor: Rename __mv_xor_slot_cleanup() to mv_xor_slot_cleanup()
      dma: mv_xor: Remove all callers of mv_xor_slot_cleanup()
      dma: mv_xor: Remove unneeded mv_xor_clean_completed_slots() call
      ioat: Use pci_enable_msix_exact() instead of pci_enable_msix()
      drivers: dma: Include appropriate header file in dca.c
      drivers: dma: Mark functions as static in dma_v3.c
      dma: mv_xor: Add DMA API error checks
      ioat/dca: Use dev_is_pci() to check whether it is pci device

commit a12a601ed163578084a48708ae376805f79a1ccf
Author: Li RongQing <roy.qing.li@gmail.com>
Date:   Tue Sep 30 09:49:55 2014 +0800

    tcp: Change tcp_slow_start function to return void
    
    No caller uses the return value, so make this function return void.
    
    Signed-off-by: Li RongQing <roy.qing.li@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 545a79aa4212..ba2f9d03076b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -830,7 +830,7 @@ void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
 int tcp_set_congestion_control(struct sock *sk, const char *name);
-int tcp_slow_start(struct tcp_sock *tp, u32 acked);
+void tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
 u32 tcp_reno_ssthresh(struct sock *sk);

commit d82bd1229885d550d03926cfa937703f6caa3cc0
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Sep 29 13:08:29 2014 +0200

    tcp: move TCP_ECN_create_request out of header
    
    After Octavian Purdilas tcp ipv4/ipv6 unification work this helper only
    has a single callsite.
    
    While at it, convert name to lowercase, suggested by Stephen.
    
    Suggested-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1f57c5363492..545a79aa4212 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -861,40 +861,6 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
-/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
- *
- * If we receive a SYN packet with these bits set, it means a
- * network is playing bad games with TOS bits. In order to
- * avoid possible false congestion notifications, we disable
- * TCP ECN negociation.
- *
- * Exception: tcp_ca wants ECN. This is required for DCTCP
- * congestion control; it requires setting ECT on all packets,
- * including SYN. We inverse the test in this case: If our
- * local socket wants ECN, but peer only set ece/cwr (but not
- * ECT in IP header) its probably a non-DCTCP aware sender.
- */
-static inline void
-TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb,
-		       const struct sock *listen_sk)
-{
-	const struct tcphdr *th = tcp_hdr(skb);
-	const struct net *net = sock_net(listen_sk);
-	bool th_ecn = th->ece && th->cwr;
-	bool ect, need_ecn;
-
-	if (!th_ecn)
-		return;
-
-	ect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);
-	need_ecn = tcp_ca_needs_ecn(listen_sk);
-
-	if (!ect && !need_ecn && net->ipv4.sysctl_tcp_ecn)
-		inet_rsk(req)->ecn_ok = 1;
-	else if (ect && need_ecn)
-		inet_rsk(req)->ecn_ok = 1;
-}
-
 /* These functions determine how the current flow behaves in respect of SACK
  * handling. SACK is negotiated with the peer, and therefore it can vary
  * between different flows.

commit 9890092e46b2996bb85f7f973e69424cb5c07bc0
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Sep 26 22:37:35 2014 +0200

    net: tcp: more detailed ACK events and events for CE marked packets
    
    DataCenter TCP (DCTCP) determines cwnd growth based on ECN information
    and ACK properties, e.g. ACK that updates window is treated differently
    than DUPACK.
    
    Also DCTCP needs information whether ACK was delayed ACK. Furthermore,
    DCTCP also implements a CE state machine that keeps track of CE markings
    of incoming packets.
    
    Therefore, extend the congestion control framework to provide these
    event types, so that DCTCP can be properly implemented as a normal
    congestion algorithm module outside of the core stack.
    
    Joint work with Daniel Borkmann and Glenn Judd.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Glenn Judd <glenn.judd@morganstanley.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7ec6a28305c0..1f57c5363492 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -763,10 +763,17 @@ enum tcp_ca_event {
 	CA_EVENT_CWND_RESTART,	/* congestion window restart */
 	CA_EVENT_COMPLETE_CWR,	/* end of congestion recovery */
 	CA_EVENT_LOSS,		/* loss timeout */
+	CA_EVENT_ECN_NO_CE,	/* ECT set, but not CE marked */
+	CA_EVENT_ECN_IS_CE,	/* received CE marked IP packet */
+	CA_EVENT_DELAYED_ACK,	/* Delayed ack is sent */
+	CA_EVENT_NON_DELAYED_ACK,
 };
 
+/* Information about inbound ACK, passed to cong_ops->in_ack_event() */
 enum tcp_ca_ack_event_flags {
-	CA_ACK_SLOWPATH = (1 << 0),
+	CA_ACK_SLOWPATH		= (1 << 0),	/* In slow path processing */
+	CA_ACK_WIN_UPDATE	= (1 << 1),	/* ACK updated window */
+	CA_ACK_ECE		= (1 << 2),	/* ECE bit is set on ack */
 };
 
 /*

commit 7354c8c389d18719dd71cc810da70b0921d66694
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Sep 26 22:37:34 2014 +0200

    net: tcp: split ack slow/fast events from cwnd_event
    
    The congestion control ops "cwnd_event" currently supports
    CA_EVENT_FAST_ACK and CA_EVENT_SLOW_ACK events (among others).
    Both FAST and SLOW_ACK are only used by Westwood congestion
    control algorithm.
    
    This removes both flags from cwnd_event and adds a new
    in_ack_event callback for this. The goal is to be able to
    provide more detailed information about ACKs, such as whether
    ECE flag was set, or whether the ACK resulted in a window
    update.
    
    It is required for DataCenter TCP (DCTCP) congestion control
    algorithm as it makes a different choice depending on ECE being
    set or not.
    
    Joint work with Daniel Borkmann and Glenn Judd.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Glenn Judd <glenn.judd@morganstanley.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a12f145cfbc3..7ec6a28305c0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -763,8 +763,10 @@ enum tcp_ca_event {
 	CA_EVENT_CWND_RESTART,	/* congestion window restart */
 	CA_EVENT_COMPLETE_CWR,	/* end of congestion recovery */
 	CA_EVENT_LOSS,		/* loss timeout */
-	CA_EVENT_FAST_ACK,	/* in sequence ack */
-	CA_EVENT_SLOW_ACK,	/* other ack */
+};
+
+enum tcp_ca_ack_event_flags {
+	CA_ACK_SLOWPATH = (1 << 0),
 };
 
 /*
@@ -796,6 +798,8 @@ struct tcp_congestion_ops {
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
 	void (*cwnd_event)(struct sock *sk, enum tcp_ca_event ev);
+	/* call when ack arrives (optional) */
+	void (*in_ack_event)(struct sock *sk, u32 flags);
 	/* new value of cwnd after loss (optional) */
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */

commit 30e502a34b8b21fae2c789da102bd9f6e99fef83
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Sep 26 22:37:33 2014 +0200

    net: tcp: add flag for ca to indicate that ECN is required
    
    This patch adds a flag to TCP congestion algorithms that allows
    for requesting to mark IPv4/IPv6 sockets with transport as ECN
    capable, that is, ECT(0), when required by a congestion algorithm.
    
    It is currently used and needed in DataCenter TCP (DCTCP), as it
    requires both peers to assert ECT on all IP packets sent - it
    uses ECN feedback (i.e. CE, Congestion Encountered information)
    from switches inside the data center to derive feedback to the
    end hosts.
    
    Therefore, simply add a new flag to icsk_ca_ops. Note that DCTCP's
    algorithm/behaviour slightly diverges from RFC3168, therefore this
    is only (!) enabled iff the assigned congestion control ops module
    has requested this. By that, we can tightly couple this logic really
    only to the provided congestion control ops.
    
    Joint work with Florian Westphal and Glenn Judd.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Glenn Judd <glenn.judd@morganstanley.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f99b0c072ee5..a12f145cfbc3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -733,23 +733,6 @@ struct tcp_skb_cb {
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
-/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
- *
- * If we receive a SYN packet with these bits set, it means a network is
- * playing bad games with TOS bits. In order to avoid possible false congestion
- * notifications, we disable TCP ECN negociation.
- */
-static inline void
-TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb,
-		struct net *net)
-{
-	const struct tcphdr *th = tcp_hdr(skb);
-
-	if (net->ipv4.sysctl_tcp_ecn && th->ece && th->cwr &&
-	    INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield))
-		inet_rsk(req)->ecn_ok = 1;
-}
-
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */
@@ -791,7 +774,10 @@ enum tcp_ca_event {
 #define TCP_CA_MAX	128
 #define TCP_CA_BUF_MAX	(TCP_CA_NAME_MAX*TCP_CA_MAX)
 
+/* Algorithm can be set on socket without CAP_NET_ADMIN privileges */
 #define TCP_CONG_NON_RESTRICTED 0x1
+/* Requires ECN/ECT set on all packets */
+#define TCP_CONG_NEEDS_ECN	0x2
 
 struct tcp_congestion_ops {
 	struct list_head	list;
@@ -840,6 +826,13 @@ u32 tcp_reno_ssthresh(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
+static inline bool tcp_ca_needs_ecn(const struct sock *sk)
+{
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+
+	return icsk->icsk_ca_ops->flags & TCP_CONG_NEEDS_ECN;
+}
+
 static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
@@ -857,6 +850,40 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
+/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
+ *
+ * If we receive a SYN packet with these bits set, it means a
+ * network is playing bad games with TOS bits. In order to
+ * avoid possible false congestion notifications, we disable
+ * TCP ECN negociation.
+ *
+ * Exception: tcp_ca wants ECN. This is required for DCTCP
+ * congestion control; it requires setting ECT on all packets,
+ * including SYN. We inverse the test in this case: If our
+ * local socket wants ECN, but peer only set ece/cwr (but not
+ * ECT in IP header) its probably a non-DCTCP aware sender.
+ */
+static inline void
+TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb,
+		       const struct sock *listen_sk)
+{
+	const struct tcphdr *th = tcp_hdr(skb);
+	const struct net *net = sock_net(listen_sk);
+	bool th_ecn = th->ece && th->cwr;
+	bool ect, need_ecn;
+
+	if (!th_ecn)
+		return;
+
+	ect = !INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield);
+	need_ecn = tcp_ca_needs_ecn(listen_sk);
+
+	if (!ect && !need_ecn && net->ipv4.sysctl_tcp_ecn)
+		inet_rsk(req)->ecn_ok = 1;
+	else if (ect && need_ecn)
+		inet_rsk(req)->ecn_ok = 1;
+}
+
 /* These functions determine how the current flow behaves in respect of SACK
  * handling. SACK is negotiated with the peer, and therefore it can vary
  * between different flows.

commit 55d8694fa82c9b5858ae5a78a210353961f908f9
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Sep 26 22:37:32 2014 +0200

    net: tcp: assign tcp cong_ops when tcp sk is created
    
    Split assignment and initialization from one into two functions.
    
    This is required by followup patches that add Datacenter TCP
    (DCTCP) congestion control algorithm - we need to be able to
    determine if the connection is moderated by DCTCP before the
    3WHS has finished.
    
    As we walk the available congestion control list during the
    assignment, we are always guaranteed to have Reno present as
    it's fixed compiled-in. Therefore, since we're doing the
    early assignment, we don't have a real use for the Reno alias
    tcp_init_congestion_ops anymore and can thus remove it.
    
    Actual usage of the congestion control operations are being
    made after the 3WHS has finished, in some cases however we
    can access get_info() via diag if implemented, therefore we
    need to zero out the private area for those modules.
    
    Joint work with Daniel Borkmann and Glenn Judd.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Glenn Judd <glenn.judd@morganstanley.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 02a9a2c366bf..f99b0c072ee5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -824,6 +824,7 @@ struct tcp_congestion_ops {
 int tcp_register_congestion_control(struct tcp_congestion_ops *type);
 void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 
+void tcp_assign_congestion_control(struct sock *sk);
 void tcp_init_congestion_control(struct sock *sk);
 void tcp_cleanup_congestion_control(struct sock *sk);
 int tcp_set_default_congestion_control(const char *name);
@@ -835,7 +836,6 @@ int tcp_set_congestion_control(struct sock *sk, const char *name);
 int tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
-extern struct tcp_congestion_ops tcp_init_congestion_ops;
 u32 tcp_reno_ssthresh(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;

commit cd7d8498c9a5d510c64db38d9f4f4fbc41790f09
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Sep 24 04:11:22 2014 -0700

    tcp: change tcp_skb_pcount() location
    
    Our goal is to access no more than one cache line access per skb in
    a write or receive queue when doing the various walks.
    
    After recent TCP_SKB_CB() reorganizations, it is almost done.
    
    Last part is tcp_skb_pcount() which currently uses
    skb_shinfo(skb)->gso_segs, which is a terrible choice, because it needs
    3 cache lines in current kernel (skb->head, skb->end, and
    shinfo->gso_segs are all in 3 different cache lines, far from skb->cb)
    
    This very simple patch reuses space currently taken by tcp_tw_isn
    only in input path, as tcp_skb_pcount is only needed for skb stored in
    write queue.
    
    This considerably speeds up tcp_ack(), granted we avoid shinfo->tx_flags
    to get SKBTX_ACK_TSTAMP, which seems possible.
    
    This also speeds up all sack processing in general.
    
    This speeds up tcp_sendmsg() because it no longer has to access/dirty
    shinfo.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4dc6641ee990..02a9a2c366bf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -698,7 +698,16 @@ static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
 struct tcp_skb_cb {
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
-	__u32		tcp_tw_isn;	/* isn chosen by tcp_timewait_state_process() */
+	union {
+		/* Note : tcp_tw_isn is used in input path only
+		 *	  (isn chosen by tcp_timewait_state_process())
+		 *
+		 * 	  tcp_gso_segs is used in write queue only,
+		 *	  cf tcp_skb_pcount()
+		 */
+		__u32		tcp_tw_isn;
+		__u32		tcp_gso_segs;
+	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
@@ -746,7 +755,17 @@ TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb,
  */
 static inline int tcp_skb_pcount(const struct sk_buff *skb)
 {
-	return skb_shinfo(skb)->gso_segs;
+	return TCP_SKB_CB(skb)->tcp_gso_segs;
+}
+
+static inline void tcp_skb_pcount_set(struct sk_buff *skb, int segs)
+{
+	TCP_SKB_CB(skb)->tcp_gso_segs = segs;
+}
+
+static inline void tcp_skb_pcount_add(struct sk_buff *skb, int segs)
+{
+	TCP_SKB_CB(skb)->tcp_gso_segs += segs;
 }
 
 /* This is valid iff tcp_skb_pcount() > 1. */

commit 971f10eca186cab238c49daa91f703c5a001b0b1
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Sep 27 09:50:57 2014 -0700

    tcp: better TCP_SKB_CB layout to reduce cache line misses
    
    TCP maintains lists of skb in write queue, and in receive queues
    (in order and out of order queues)
    
    Scanning these lists both in input and output path usually requires
    access to skb->next, TCP_SKB_CB(skb)->seq, and TCP_SKB_CB(skb)->end_seq
    
    These fields are currently in two different cache lines, meaning we
    waste lot of memory bandwidth when these queues are big and flows
    have either packet drops or packet reorders.
    
    We can move TCP_SKB_CB(skb)->header at the end of TCP_SKB_CB, because
    this header is not used in fast path. This allows TCP to search much faster
    in the skb lists.
    
    Even with regular flows, we save one cache line miss in fast path.
    
    Thanks to Christoph Paasch for noticing we need to cleanup
    skb->cb[] (IPCB/IP6CB) before entering IP stack in tx path,
    and that I forgot IPCB use in tcp_v4_hnd_req() and tcp_v4_save_options().
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a4201ef216e8..4dc6641ee990 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -696,12 +696,6 @@ static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
  * If this grows please adjust skbuff.h:skbuff->cb[xxx] size appropriately.
  */
 struct tcp_skb_cb {
-	union {
-		struct inet_skb_parm	h4;
-#if IS_ENABLED(CONFIG_IPV6)
-		struct inet6_skb_parm	h6;
-#endif
-	} header;	/* For incoming frames		*/
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
 	__u32		tcp_tw_isn;	/* isn chosen by tcp_timewait_state_process() */
@@ -720,6 +714,12 @@ struct tcp_skb_cb {
 	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
 	/* 1 byte hole */
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
+	union {
+		struct inet_skb_parm	h4;
+#if IS_ENABLED(CONFIG_IPV6)
+		struct inet6_skb_parm	h6;
+#endif
+	} header;	/* For incoming frames		*/
 };
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))

commit 3f334078567245429540e6461c81c749fce87f70
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 17:12:07 2013 -0800

    net: make tcp_cleanup_rbuf private
    
    net_dma was the only external user so this can become local to tcp.c
    again.
    
    Cc: James Morris <jmorris@namei.org>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Cc: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2c2f24ffa383..7ffbcbf90a64 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -371,7 +371,6 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			 const struct tcphdr *th, unsigned int len);
 void tcp_rcv_space_adjust(struct sock *sk);
-void tcp_cleanup_rbuf(struct sock *sk, int copied);
 int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
 void tcp_twsk_destructor(struct sock *sk);
 ssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,

commit 7bced397510ab569d31de4c70b39e13355046387
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 12:37:29 2013 -0800

    net_dma: simple removal
    
    Per commit "77873803363c net_dma: mark broken" net_dma is no longer used
    and there is no plan to fix it.
    
    This is the mechanical removal of bits in CONFIG_NET_DMA ifdef guards.
    Reverting the remainder of the net_dma induced changes is deferred to
    subsequent patches.
    
    Marked for stable due to Roman's report of a memory leak in
    dma_pin_iovec_pages():
    
        https://lkml.org/lkml/2014/9/3/177
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: David Whipple <whipple@securedatainnovations.ch>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8c4dd63134d4..2c2f24ffa383 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -27,7 +27,6 @@
 #include <linux/cache.h>
 #include <linux/percpu.h>
 #include <linux/skbuff.h>
-#include <linux/dmaengine.h>
 #include <linux/crypto.h>
 #include <linux/cryptohash.h>
 #include <linux/kref.h>
@@ -267,7 +266,6 @@ extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_low_latency;
-extern int sysctl_tcp_dma_copybreak;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
@@ -1023,12 +1021,6 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 	tp->ucopy.len = 0;
 	tp->ucopy.memory = 0;
 	skb_queue_head_init(&tp->ucopy.prequeue);
-#ifdef CONFIG_NET_DMA
-	tp->ucopy.dma_chan = NULL;
-	tp->ucopy.wakeup = 0;
-	tp->ucopy.pinned_list = NULL;
-	tp->ucopy.dma_cookie = 0;
-#endif
 }
 
 bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);

commit 7faee5c0d514162853a343d93e4a0b6bb8bfec21
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 5 15:33:33 2014 -0700

    tcp: remove TCP_SKB_CB(skb)->when
    
    After commit 740b0f1841f6 ("tcp: switch rtt estimations to usec resolution"),
    we no longer need to maintain timestamps in two different fields.
    
    TCP_SKB_CB(skb)->when can be removed, as same information sits in skb_mstamp.stamp_jiffies
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0cd7d2c65dc0..a4201ef216e8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -672,6 +672,12 @@ void tcp_send_window_probe(struct sock *sk);
  */
 #define tcp_time_stamp		((__u32)(jiffies))
 
+static inline u32 tcp_skb_timestamp(const struct sk_buff *skb)
+{
+	return skb->skb_mstamp.stamp_jiffies;
+}
+
+
 #define tcp_flag_byte(th) (((u_int8_t *)th)[13])
 
 #define TCPHDR_FIN 0x01
@@ -698,12 +704,7 @@ struct tcp_skb_cb {
 	} header;	/* For incoming frames		*/
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
-	union {
-		/* used in output path */
-		__u32		when;	/* used to compute rtt's	*/
-		/* used in input path */
-		__u32		tcp_tw_isn; /* isn chosen by tcp_timewait_state_process() */
-	};
+	__u32		tcp_tw_isn;	/* isn chosen by tcp_timewait_state_process() */
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
 	__u8		sacked;		/* State flags for SACK/FACK.	*/

commit 04317dafd11dd7b0ec19b85f098414abae6ed5f7
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 5 15:33:32 2014 -0700

    tcp: introduce TCP_SKB_CB(skb)->tcp_tw_isn
    
    TCP_SKB_CB(skb)->when has different meaning in output and input paths.
    
    In output path, it contains a timestamp.
    In input path, it contains an ISN, chosen by tcp_timewait_state_process()
    
    Lets add a different name to ease code comprehension.
    
    Note that 'when' field will disappear in following patch,
    as skb_mstamp already contains timestamp, the anonymous
    union will promptly disappear as well.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 590e01a476ac..0cd7d2c65dc0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -698,7 +698,12 @@ struct tcp_skb_cb {
 	} header;	/* For incoming frames		*/
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
-	__u32		when;		/* used to compute rtt's	*/
+	union {
+		/* used in output path */
+		__u32		when;	/* used to compute rtt's	*/
+		/* used in input path */
+		__u32		tcp_tw_isn; /* isn chosen by tcp_timewait_state_process() */
+	};
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 
 	__u8		sacked;		/* State flags for SACK/FACK.	*/

commit a26552afe89438eefbe097512b3187f2c7e929fd
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Thu Aug 14 22:06:12 2014 +0200

    tcp: don't allow syn packets without timestamps to pass tcp_tw_recycle logic
    
    tcp_tw_recycle heavily relies on tcp timestamps to build a per-host
    ordering of incoming connections and teardowns without the need to
    hold state on a specific quadruple for TCP_TIMEWAIT_LEN, but only for
    the last measured RTO. To do so, we keep the last seen timestamp in a
    per-host indexed data structure and verify if the incoming timestamp
    in a connection request is strictly greater than the saved one during
    last connection teardown. Thus we can verify later on that no old data
    packets will be accepted by the new connection.
    
    During moving a socket to time-wait state we already verify if timestamps
    where seen on a connection. Only if that was the case we let the
    time-wait socket expire after the RTO, otherwise normal TCP_TIMEWAIT_LEN
    will be used. But we don't verify this on incoming SYN packets. If a
    connection teardown was less than TCP_PAWS_MSL seconds in the past we
    cannot guarantee to not accept data packets from an old connection if
    no timestamps are present. We should drop this SYN packet. This patch
    closes this loophole.
    
    Please note, this patch does not make tcp_tw_recycle in any way more
    usable but only adds another safety check:
    Sporadic drops of SYN packets because of reordering in the network or
    in the socket backlog queues can happen. Users behing NAT trying to
    connect to a tcp_tw_recycle enabled server can get caught in blackholes
    and their connection requests may regullary get dropped because hosts
    behind an address translator don't have synchronized tcp timestamp clocks.
    tcp_tw_recycle cannot work if peers don't have tcp timestamps enabled.
    
    In general, use of tcp_tw_recycle is disadvised.
    
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Florian Westphal <fw@strlen.de>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e337e05035be..590e01a476ac 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -417,7 +417,7 @@ void tcp_update_metrics(struct sock *sk);
 void tcp_init_metrics(struct sock *sk);
 void tcp_metrics_init(void);
 bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
-			bool paws_check);
+			bool paws_check, bool timestamps);
 bool tcp_remember_stamp(struct sock *sk);
 bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
 void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);

commit 4fab9071950c2021d846e18351e0f46a1cffd67b
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu Aug 14 12:40:05 2014 -0400

    tcp: fix tcp_release_cb() to dispatch via address family for mtu_reduced()
    
    Make sure we use the correct address-family-specific function for
    handling MTU reductions from within tcp_release_cb().
    
    Previously AF_INET6 sockets were incorrectly always using the IPv6
    code path when sometimes they were handling IPv4 traffic and thus had
    an IPv4 dst.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Diagnosed-by: Willem de Bruijn <willemb@google.com>
    Fixes: 563d34d057862 ("tcp: dont drop MTU reduction indications")
    Reviewed-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 36f55254573f..e337e05035be 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -448,6 +448,7 @@ const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
  */
 
 void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
+void tcp_v4_mtu_reduced(struct sock *sk);
 int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
 struct sock *tcp_create_openreq_child(struct sock *sk,
 				      struct request_sock *req,

commit 9d186cac7ffb1831e9f34cb4a3a8b22abb9dd9d4
Author: Andrey Vagin <avagin@openvz.org>
Date:   Wed Aug 13 16:03:10 2014 +0400

    tcp: don't use timestamp from repaired skb-s to calculate RTT (v2)
    
    We don't know right timestamp for repaired skb-s. Wrong RTT estimations
    isn't good, because some congestion modules heavily depends on it.
    
    This patch adds the TCPCB_REPAIRED flag, which is included in
    TCPCB_RETRANS.
    
    Thanks to Eric for the advice how to fix this issue.
    
    This patch fixes the warning:
    [  879.562947] WARNING: CPU: 0 PID: 2825 at net/ipv4/tcp_input.c:3078 tcp_ack+0x11f5/0x1380()
    [  879.567253] CPU: 0 PID: 2825 Comm: socket-tcpbuf-l Not tainted 3.16.0-next-20140811 #1
    [  879.567829] Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
    [  879.568177]  0000000000000000 00000000c532680c ffff880039643d00 ffffffff817aa2d2
    [  879.568776]  0000000000000000 ffff880039643d38 ffffffff8109afbd ffff880039d6ba80
    [  879.569386]  ffff88003a449800 000000002983d6bd 0000000000000000 000000002983d6bc
    [  879.569982] Call Trace:
    [  879.570264]  [<ffffffff817aa2d2>] dump_stack+0x4d/0x66
    [  879.570599]  [<ffffffff8109afbd>] warn_slowpath_common+0x7d/0xa0
    [  879.570935]  [<ffffffff8109b0ea>] warn_slowpath_null+0x1a/0x20
    [  879.571292]  [<ffffffff816d0a05>] tcp_ack+0x11f5/0x1380
    [  879.571614]  [<ffffffff816d10bd>] tcp_rcv_established+0x1ed/0x710
    [  879.571958]  [<ffffffff816dc9da>] tcp_v4_do_rcv+0x10a/0x370
    [  879.572315]  [<ffffffff81657459>] release_sock+0x89/0x1d0
    [  879.572642]  [<ffffffff816c81a0>] do_tcp_setsockopt.isra.36+0x120/0x860
    [  879.573000]  [<ffffffff8110a52e>] ? rcu_read_lock_held+0x6e/0x80
    [  879.573352]  [<ffffffff816c8912>] tcp_setsockopt+0x32/0x40
    [  879.573678]  [<ffffffff81654ac4>] sock_common_setsockopt+0x14/0x20
    [  879.574031]  [<ffffffff816537b0>] SyS_setsockopt+0x80/0xf0
    [  879.574393]  [<ffffffff817b40a9>] system_call_fastpath+0x16/0x1b
    [  879.574730] ---[ end trace a17cbc38eb8c5c00 ]---
    
    v2: moving setting of skb->when for repaired skb-s in tcp_write_xmit,
        where it's set for other skb-s.
    
    Fixes: 431a91242d8d ("tcp: timestamp SYN+DATA messages")
    Fixes: 740b0f1841f6 ("tcp: switch rtt estimations to usec resolution")
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrey Vagin <avagin@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dafa1cbc149b..36f55254573f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -705,8 +705,10 @@ struct tcp_skb_cb {
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
 #define TCPCB_LOST		0x04	/* SKB is lost			*/
 #define TCPCB_TAGBITS		0x07	/* All tag bits			*/
+#define TCPCB_REPAIRED		0x10	/* SKB repaired (no skb_mstamp)	*/
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
-#define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
+#define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS| \
+				TCPCB_REPAIRED)
 
 	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
 	/* 1 byte hole */

commit 5ae344c949e79b8545a11db149f0a85a6e59e1f3
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Aug 4 19:12:29 2014 -0400

    tcp: reduce spurious retransmits due to transient SACK reneging
    
    This commit reduces spurious retransmits due to apparent SACK reneging
    by only reacting to SACK reneging that persists for a short delay.
    
    When a sequence space hole at snd_una is filled, some TCP receivers
    send a series of ACKs as they apparently scan their out-of-order queue
    and cumulatively ACK all the packets that have now been consecutiveyly
    received. This is essentially misbehavior B in "Misbehaviors in TCP
    SACK generation" ACM SIGCOMM Computer Communication Review, April
    2011, so we suspect that this is from several common OSes (Windows
    2000, Windows Server 2003, Windows XP). However, this issue has also
    been seen in other cases, e.g. the netdev thread "TCP being hoodwinked
    into spurious retransmissions by lack of timestamps?" from March 2014,
    where the receiver was thought to be a BSD box.
    
    Since snd_una would temporarily be adjacent to a previously SACKed
    range in these scenarios, this receiver behavior triggered the Linux
    SACK reneging code path in the sender. This led the sender to clear
    the SACK scoreboard, enter CA_Loss, and spuriously retransmit
    (potentially) every packet from the entire write queue at line rate
    just a few milliseconds before the ACK for each packet arrives at the
    sender.
    
    To avoid such situations, now when a sender sees apparent reneging it
    does not yet retransmit, but rather adjusts the RTO timer to give the
    receiver a little time (max(RTT/2, 10ms)) to send us some more ACKs
    that will restore sanity to the SACK scoreboard. If the reneging
    persists until this RTO then, as before, we clear the SACK scoreboard
    and enter CA_Loss.
    
    A 10ms delay tolerates a receiver sending such a stream of ACKs at
    56Kbit/sec. And to allow for receivers with slower or more congested
    paths, we wait for at least RTT/2.
    
    We validated the resulting max(RTT/2, 10ms) delay formula with a mix
    of North American and South American Google web server traffic, and
    found that for ACKs displaying transient reneging:
    
     (1) 90% of inter-ACK delays were less than 10ms
     (2) 99% of inter-ACK delays were less than RTT/2
    
    In tests on Google web servers this commit reduced reneging events by
    75%-90% (as measured by the TcpExtTCPSACKReneging counter), without
    any measurable impact on latency for user HTTP and SPDY requests.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0aeb2eb749dc..dafa1cbc149b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -411,7 +411,7 @@ struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
 			   bool fastopen);
 int tcp_child_process(struct sock *parent, struct sock *child,
 		      struct sk_buff *skb);
-void tcp_enter_loss(struct sock *sk, int how);
+void tcp_enter_loss(struct sock *sk);
 void tcp_clear_retrans(struct tcp_sock *tp);
 void tcp_update_metrics(struct sock *sk);
 void tcp_init_metrics(struct sock *sk);

commit 5ee2c941b5969eb1b5592f9731b3ee76a784641f
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Mon Jul 14 16:58:32 2014 +0200

    tcp: Remove unnecessary arg from tcp_enter_cwr and tcp_init_cwnd_reduction
    
    Since Yuchung's 9b44190dc11 (tcp: refactor F-RTO), tcp_enter_cwr is always
    called with set_ssthresh = 1. Thus, we can remove this argument from
    tcp_enter_cwr. Further, as we remove this one, tcp_init_cwnd_reduction
    is then always called with set_ssthresh = true, and so we can get rid of
    this argument as well.
    
    Cc: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c9a75dbba0c7..0aeb2eb749dc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -928,7 +928,7 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
-void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
+void tcp_enter_cwr(struct sock *sk);
 __u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);
 
 /* The maximum number of MSS of available cwnd for which TSO defers

commit 86c6a2c75ab97fe31844985169e26aea335432f9
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Jun 30 15:09:49 2014 -0400

    tcp: switch snt_synack back to measuring transmit time of first SYNACK
    
    Always store in snt_synack the time at which the server received the
    first client SYN and attempted to send the first SYNACK.
    
    Recent commit aa27fc501 ("tcp: tcp_v[46]_conn_request: fix snt_synack
    initialization") resolved an inconsistency between IPv4 and IPv6 in
    the initialization of snt_synack. This commit brings back the idea
    from 843f4a55e (tcp: use tcp_v4_send_synack on first SYN-ACK), which
    was going for the original behavior of snt_synack from the commit
    where it was added in 9ad7c049f0f79 ("tcp: RFC2988bis + taking RTT
    sample from 3WHS for the passive open side") in v3.1.
    
    In addition to being simpler (and probably a tiny bit faster),
    unconditionally storing the time of the first SYNACK attempt has been
    useful because it allows calculating a performance metric quantifying
    how long it took to establish a passive TCP connection.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Cc: Octavian Purdila <octavian.purdila@intel.com>
    Cc: Jerry Chu <hkchu@google.com>
    Acked-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0d5389aecf18..c9a75dbba0c7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1093,7 +1093,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	req->cookie_ts = 0;
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
-	tcp_rsk(req)->snt_synack = 0;
+	tcp_rsk(req)->snt_synack = tcp_time_stamp;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
 	ireq->tstamp_ok = rx_opt->tstamp_ok;

commit 1fb6f159fd21c640a28eb65fbd62ce8c9f6a777e
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:10:02 2014 +0300

    tcp: add tcp_conn_request
    
    Create tcp_conn_request and remove most of the code from
    tcp_v4_conn_request and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cec6e2cf0610..0d5389aecf18 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1574,6 +1574,9 @@ void tcp4_proc_exit(void);
 #endif
 
 int tcp_rtx_synack(struct sock *sk, struct request_sock *req);
+int tcp_conn_request(struct request_sock_ops *rsk_ops,
+		     const struct tcp_request_sock_ops *af_ops,
+		     struct sock *sk, struct sk_buff *skb);
 
 /* TCP af-specific functions */
 struct tcp_sock_af_ops {

commit 695da14eb0af21129187ed3810e329b21262e45f
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:10:01 2014 +0300

    tcp: add queue_add_hash to tcp_request_sock_ops
    
    Add queue_add_hash member to tcp_request_sock_ops so that we can later
    unify tcp_v4_conn_request and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 30fe98bc8957..cec6e2cf0610 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1615,6 +1615,8 @@ struct tcp_request_sock_ops {
 	int (*send_synack)(struct sock *sk, struct dst_entry *dst,
 			   struct flowi *fl, struct request_sock *req,
 			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
+	void (*queue_hash_add)(struct sock *sk, struct request_sock *req,
+			       const unsigned long timeout);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit 2aec4a297b21f3690486bbf8f7d5d29281ba6a48
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:10:00 2014 +0300

    tcp: add mss_clamp to tcp_request_sock_ops
    
    Add mss_clamp member to tcp_request_sock_ops so that we can later
    unify tcp_v4_conn_request and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8e9c28dccb80..30fe98bc8957 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1592,6 +1592,7 @@ struct tcp_sock_af_ops {
 };
 
 struct tcp_request_sock_ops {
+	u16 mss_clamp;
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
 						struct request_sock *req);

commit 5db92c994982ed826cf38f38d58bd09bc326aef6
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:59 2014 +0300

    tcp: unify tcp_v4_rtx_synack and tcp_v6_rtx_synack
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8c05c25018d5..8e9c28dccb80 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1573,6 +1573,8 @@ int tcp4_proc_init(void);
 void tcp4_proc_exit(void);
 #endif
 
+int tcp_rtx_synack(struct sock *sk, struct request_sock *req);
+
 /* TCP af-specific functions */
 struct tcp_sock_af_ops {
 #ifdef CONFIG_TCP_MD5SIG

commit d6274bd8d6ea84b7b54cc1c3fde6bcb6143b104f
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:58 2014 +0300

    tcp: add send_synack method to tcp_request_sock_ops
    
    Create a new tcp_request_sock_ops method to unify the IPv4/IPv6
    signature for tcp_v[46]_send_synack. This allows us to later unify
    tcp_v4_rtx_synack with tcp_v6_rtx_synack and tcp_v4_conn_request with
    tcp_v4_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8cacf0d6ed4d..8c05c25018d5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1609,6 +1609,9 @@ struct tcp_request_sock_ops {
 				       const struct request_sock *req,
 				       bool *strict);
 	__u32 (*init_seq)(const struct sk_buff *skb);
+	int (*send_synack)(struct sock *sk, struct dst_entry *dst,
+			   struct flowi *fl, struct request_sock *req,
+			   u16 queue_mapping, struct tcp_fastopen_cookie *foc);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit 936b8bdb53f90840e658904530f9db8d02ac804b
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:57 2014 +0300

    tcp: add init_seq method to tcp_request_sock_ops
    
    More work in preparation of unifying tcp_v4_conn_request and
    tcp_v6_conn_request: indirect the init sequence calls via the
    tcp_request_sock_ops.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 59fcc5934c79..8cacf0d6ed4d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1608,6 +1608,7 @@ struct tcp_request_sock_ops {
 	struct dst_entry *(*route_req)(struct sock *sk, struct flowi *fl,
 				       const struct request_sock *req,
 				       bool *strict);
+	__u32 (*init_seq)(const struct sk_buff *skb);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit d94e0417ad8d96d7d96b69335338ad942eaeecf1
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:55 2014 +0300

    tcp: add route_req method to tcp_request_sock_ops
    
    Create wrappers with same signature for the IPv4/IPv6 request routing
    calls and use these wrappers (via route_req method from
    tcp_request_sock_ops) in tcp_v4_conn_request and tcp_v6_conn_request
    with the purpose of unifying the two functions in a later patch.
    
    We can later drop the wrapper functions and modify inet_csk_route_req
    and inet6_cks_route_req to use the same signature.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 086d00ec6d8b..59fcc5934c79 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1605,6 +1605,9 @@ struct tcp_request_sock_ops {
 	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,
 				 __u16 *mss);
 #endif
+	struct dst_entry *(*route_req)(struct sock *sk, struct flowi *fl,
+				       const struct request_sock *req,
+				       bool *strict);
 };
 
 #ifdef CONFIG_SYN_COOKIES

commit fb7b37a7f3d6f7b7ba05ee526fee96810d5b92a8
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:54 2014 +0300

    tcp: add init_cookie_seq method to tcp_request_sock_ops
    
    Move the specific IPv4/IPv6 cookie sequence initialization to a new
    method in tcp_request_sock_ops in preparation for unifying
    tcp_v4_conn_request and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7ad8ce296c3b..086d00ec6d8b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -495,13 +495,6 @@ u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
 			      __u16 *mss);
-#else
-static inline __u32 cookie_v4_init_sequence(struct sock *sk,
-					    const struct sk_buff *skb,
-					    __u16 *mss)
-{
-	return 0;
-}
 #endif
 
 __u32 cookie_init_timestamp(struct request_sock *req);
@@ -517,13 +510,6 @@ u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
 			      const struct tcphdr *th, u16 *mssp);
 __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 			      __u16 *mss);
-#else
-static inline __u32 cookie_v6_init_sequence(struct sock *sk,
-					    struct sk_buff *skb,
-					    __u16 *mss)
-{
-	return 0;
-}
 #endif
 /* tcp_output.c */
 
@@ -1615,8 +1601,28 @@ struct tcp_request_sock_ops {
 #endif
 	void (*init_req)(struct request_sock *req, struct sock *sk,
 			 struct sk_buff *skb);
+#ifdef CONFIG_SYN_COOKIES
+	__u32 (*cookie_init_seq)(struct sock *sk, const struct sk_buff *skb,
+				 __u16 *mss);
+#endif
 };
 
+#ifdef CONFIG_SYN_COOKIES
+static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
+					 struct sock *sk, struct sk_buff *skb,
+					 __u16 *mss)
+{
+	return ops->cookie_init_seq(sk, skb, mss);
+}
+#else
+static inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,
+					 struct sock *sk, struct sk_buff *skb,
+					 __u16 *mss)
+{
+	return 0;
+}
+#endif
+
 int tcpv4_offload_init(void);
 
 void tcp_v4_init(void);

commit 16bea70aa7302b6f3bf3502d5a0efb4ea2ce4712
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:53 2014 +0300

    tcp: add init_req method to tcp_request_sock_ops
    
    Move the specific IPv4/IPv6 intializations to a new method in
    tcp_request_sock_ops in preparation for unifying tcp_v4_conn_request
    and tcp_v6_conn_request.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 39e47c4e4f19..7ad8ce296c3b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1613,6 +1613,8 @@ struct tcp_request_sock_ops {
 						  const struct request_sock *req,
 						  const struct sk_buff *skb);
 #endif
+	void (*init_req)(struct request_sock *req, struct sock *sk,
+			 struct sk_buff *skb);
 };
 
 int tcpv4_offload_init(void);

commit 57b47553f65e12e2e4f1608168374b0e651de843
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Wed Jun 25 17:09:50 2014 +0300

    tcp: cookie_v4_init_sequence: skb should be const
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 06a3023080c0..39e47c4e4f19 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -493,10 +493,11 @@ static inline u32 tcp_cookie_time(void)
 
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
-__u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, __u16 *mss);
+__u32 cookie_v4_init_sequence(struct sock *sk, const struct sk_buff *skb,
+			      __u16 *mss);
 #else
 static inline __u32 cookie_v4_init_sequence(struct sock *sk,
-					    struct sk_buff *skb,
+					    const struct sk_buff *skb,
 					    __u16 *mss)
 {
 	return 0;

commit e0f802fbcaa3bffe4728e37a8fa1279b5d554173
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Tue Jun 17 11:25:37 2014 +0300

    tcp: move ir_mark initialization to tcp_openreq_init
    
    ir_mark initialization is done for both TCP v4 and v6, move it in the
    common tcp_openreq_init function.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7286db80e8b8..06a3023080c0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1098,7 +1098,7 @@ static inline int tcp_full_space(const struct sock *sk)
 
 static inline void tcp_openreq_init(struct request_sock *req,
 				    struct tcp_options_received *rx_opt,
-				    struct sk_buff *skb)
+				    struct sk_buff *skb, struct sock *sk)
 {
 	struct inet_request_sock *ireq = inet_rsk(req);
 
@@ -1117,6 +1117,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->ecn_ok = 0;
 	ireq->ir_rmt_port = tcp_hdr(skb)->source;
 	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
+	ireq->ir_mark = inet_request_mark(sk, skb);
 }
 
 extern void tcp_openreq_init_rwin(struct request_sock *req,

commit 6cc55e096f1f2a8585cf8dc9049862f2376f66d4
Author: Octavian Purdila <octavian.purdila@intel.com>
Date:   Fri Jun 6 17:32:37 2014 +0300

    tcp: add gfp parameter to tcp_fragment
    
    tcp_fragment can be called from process context (from tso_fragment).
    Add a new gfp parameter to allow it to preserve atomic memory if
    possible.
    
    Signed-off-by: Octavian Purdila <octavian.purdila@intel.com>
    Reviewed-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e80abe4486cb..7286db80e8b8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -535,7 +535,7 @@ void tcp_retransmit_timer(struct sock *sk);
 void tcp_xmit_retransmit_queue(struct sock *);
 void tcp_simple_retransmit(struct sock *);
 int tcp_trim_head(struct sock *, struct sk_buff *, u32);
-int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int);
+int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int, gfp_t);
 
 void tcp_send_probe0(struct sock *);
 void tcp_send_partial(struct sock *);

commit ca8a22634381537c92b5a10308652e1c38fd9edf
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu May 22 10:41:08 2014 -0400

    tcp: make cwnd-limited checks measurement-based, and gentler
    
    Experience with the recent e114a710aa50 ("tcp: fix cwnd limited
    checking to improve congestion control") has shown that there are
    common cases where that commit can cause cwnd to be much larger than
    necessary. This leads to TSO autosizing cooking skbs that are too
    large, among other things.
    
    The main problems seemed to be:
    
    (1) That commit attempted to predict the future behavior of the
    connection by looking at the write queue (if TSO or TSQ limit
    sending). That prediction sometimes overestimated future outstanding
    packets.
    
    (2) That commit always allowed cwnd to grow to twice the number of
    outstanding packets (even in congestion avoidance, where this is not
    needed).
    
    This commit improves both of these, by:
    
    (1) Switching to a measurement-based approach where we explicitly
    track the largest number of packets in flight during the past window
    ("max_packets_out"), and remember whether we were cwnd-limited at the
    moment we finished sending that flight.
    
    (2) Only allowing cwnd to grow to twice the number of outstanding
    packets ("max_packets_out") in slow start. In congestion avoidance
    mode we now only allow cwnd to grow if it was fully utilized.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f5d6ca4a9d28..e80abe4486cb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -971,8 +971,9 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 
 /* We follow the spirit of RFC2861 to validate cwnd but implement a more
  * flexible approach. The RFC suggests cwnd should not be raised unless
- * it was fully used previously. But we allow cwnd to grow as long as the
- * application has used half the cwnd.
+ * it was fully used previously. And that's exactly what we do in
+ * congestion avoidance mode. But in slow start we allow cwnd to grow
+ * as long as the application has used half the cwnd.
  * Example :
  *    cwnd is 10 (IW10), but application sends 9 frames.
  *    We allow cwnd to reach 18 when all frames are ACKed.
@@ -985,7 +986,11 @@ static inline bool tcp_is_cwnd_limited(const struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 
-	return tp->snd_cwnd < 2 * tp->lsnd_pending;
+	/* If in slow start, ensure cwnd grows to twice what was ACKed. */
+	if (tp->snd_cwnd <= tp->snd_ssthresh)
+		return tp->snd_cwnd < 2 * tp->max_packets_out;
+
+	return tp->is_cwnd_limited;
 }
 
 static inline void tcp_check_probe_timer(struct sock *sk)

commit 843f4a55e336e6d0c7bb92e7f9621535bc8d5fcd
Author: Yuchung Cheng <ycheng@google.com>
Date:   Sun May 11 20:22:11 2014 -0700

    tcp: use tcp_v4_send_synack on first SYN-ACK
    
    To avoid large code duplication in IPv6, we need to first simplify
    the complicate SYN-ACK sending code in tcp_v4_conn_request().
    
    To use tcp_v4(6)_send_synack() to send all SYN-ACKs, we need to
    initialize the mini socket's receive window before trying to
    create the child socket and/or building the SYN-ACK packet. So we move
    that initialization from tcp_make_synack() to tcp_v4_conn_request()
    as a new function tcp_openreq_init_req_rwin().
    
    After this refactoring the SYN-ACK sending code is simpler and easier
    to implement Fast Open for IPv6.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Daniel Lee <longinus00@gmail.com>
    Signed-off-by: Jerry Chu <hkchu@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 17d7c6a3d037..f5d6ca4a9d28 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1114,6 +1114,9 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
 }
 
+extern void tcp_openreq_init_rwin(struct request_sock *req,
+				  struct sock *sk, struct dst_entry *dst);
+
 void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
@@ -1323,13 +1326,10 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
-int tcp_fastopen_create_child(struct sock *sk,
-			      struct sk_buff *skb,
-			      struct sk_buff *skb_synack,
-			      struct request_sock *req);
-bool tcp_fastopen_check(struct sock *sk, struct sk_buff *skb,
-			struct request_sock *req,
-			struct tcp_fastopen_cookie *foc);
+bool tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,
+		      struct request_sock *req,
+		      struct tcp_fastopen_cookie *foc,
+		      struct dst_entry *dst);
 void tcp_fastopen_init_key_once(bool publish);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 

commit 89278c9dc922272df921042aafa18311f3398c6c
Author: Yuchung Cheng <ycheng@google.com>
Date:   Sun May 11 20:22:10 2014 -0700

    tcp: simplify fast open cookie processing
    
    Consolidate various cookie checking and generation code to simplify
    the fast open processing. The main goal is to reduce code duplication
    in tcp_v4_conn_request() for IPv6 support.
    
    Removes two experimental sysctl flags TFO_SERVER_ALWAYS and
    TFO_SERVER_COOKIE_NOT_CHKD used primarily for developmental debugging
    purposes.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Daniel Lee <longinus00@gmail.com>
    Signed-off-by: Jerry Chu <hkchu@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 012236838583..17d7c6a3d037 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -220,8 +220,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define	TFO_SERVER_ENABLE	2
 #define	TFO_CLIENT_NO_COOKIE	4	/* Data in SYN w/o cookie option */
 
-/* Process SYN data but skip cookie validation */
-#define	TFO_SERVER_COOKIE_NOT_CHKED	0x100
 /* Accept SYN data w/o any cookie option */
 #define	TFO_SERVER_COOKIE_NOT_REQD	0x200
 
@@ -230,10 +228,6 @@ void tcp_time_wait(struct sock *sk, int state, int timeo);
  */
 #define	TFO_SERVER_WO_SOCKOPT1	0x400
 #define	TFO_SERVER_WO_SOCKOPT2	0x800
-/* Always create TFO child sockets on a TFO listener even when
- * cookie/data not present. (For testing purpose!)
- */
-#define	TFO_SERVER_ALWAYS	0x1000
 
 extern struct inet_timewait_death_row tcp_death_row;
 
@@ -1335,8 +1329,7 @@ int tcp_fastopen_create_child(struct sock *sk,
 			      struct request_sock *req);
 bool tcp_fastopen_check(struct sock *sk, struct sk_buff *skb,
 			struct request_sock *req,
-			struct tcp_fastopen_cookie *foc,
-			struct tcp_fastopen_cookie *valid_foc);
+			struct tcp_fastopen_cookie *foc);
 void tcp_fastopen_init_key_once(bool publish);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 

commit 5b7ed0892f2af4e60b9a8d2c71c77774512a6cb9
Author: Yuchung Cheng <ycheng@google.com>
Date:   Sun May 11 20:22:09 2014 -0700

    tcp: move fastopen functions to tcp_fastopen.c
    
    Move common TFO functions that will be used by both v4 and v6
    to tcp_fastopen.c. Create a helper tcp_fastopen_queue_check().
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Daniel Lee <longinus00@gmail.com>
    Signed-off-by: Jerry Chu <hkchu@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c9418456640..012236838583 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1329,8 +1329,14 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
-void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
-			     struct tcp_fastopen_cookie *foc);
+int tcp_fastopen_create_child(struct sock *sk,
+			      struct sk_buff *skb,
+			      struct sk_buff *skb_synack,
+			      struct request_sock *req);
+bool tcp_fastopen_check(struct sock *sk, struct sk_buff *skb,
+			struct request_sock *req,
+			struct tcp_fastopen_cookie *foc,
+			struct tcp_fastopen_cookie *valid_foc);
 void tcp_fastopen_init_key_once(bool publish);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 

commit 249015515fe3fc9818d86cb5c83bbc92505ad7dc
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 2 21:18:05 2014 -0700

    tcp: remove in_flight parameter from cong_avoid() methods
    
    Commit e114a710aa505 ("tcp: fix cwnd limited checking to improve
    congestion control") obsoleted in_flight parameter from
    tcp_is_cwnd_limited() and its callers.
    
    This patch does the removal as promised.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a9fe7bc4f4bb..3c9418456640 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -796,7 +796,7 @@ struct tcp_congestion_ops {
 	/* return slow start threshold (required) */
 	u32 (*ssthresh)(struct sock *sk);
 	/* do new cwnd calculation (required) */
-	void (*cong_avoid)(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
+	void (*cong_avoid)(struct sock *sk, u32 ack, u32 acked);
 	/* call before changing ca_state (optional) */
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
@@ -828,7 +828,7 @@ void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 u32 tcp_reno_ssthresh(struct sock *sk);
-void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
+void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);
 extern struct tcp_congestion_ops tcp_reno;
 
 static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
@@ -986,10 +986,8 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
  * risks 100% overshoot. The advantage is that we discourage application to
  * either send more filler packets or data to artificially blow up the cwnd
  * usage, and allow application-limited process to probe bw more aggressively.
- *
- * TODO: remove in_flight once we can fix all callers, and their callers...
  */
-static inline bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
+static inline bool tcp_is_cwnd_limited(const struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 

commit e114a710aa5058c0ba4aa1dfb105132aefeb5e04
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 30 11:58:13 2014 -0700

    tcp: fix cwnd limited checking to improve congestion control
    
    Yuchung discovered tcp_is_cwnd_limited() was returning false in
    slow start phase even if the application filled the socket write queue.
    
    All congestion modules take into account tcp_is_cwnd_limited()
    before increasing cwnd, so this behavior limits slow start from
    probing the bandwidth at full speed.
    
    The problem is that even if write queue is full (aka we are _not_
    application limited), cwnd can be under utilized if TSO should auto
    defer or TCP Small queues decided to hold packets.
    
    So the in_flight can be kept to smaller value, and we can get to the
    point tcp_is_cwnd_limited() returns false.
    
    With TCP Small Queues and FQ/pacing, this issue is more visible.
    
    We fix this by having tcp_cwnd_validate(), which is supposed to track
    such things, take into account unsent_segs, the number of segs that we
    are not sending at the moment due to TSO or TSQ, but intend to send
    real soon. Then when we are cwnd-limited, remember this fact while we
    are processing the window of ACKs that comes back.
    
    For example, suppose we have a brand new connection with cwnd=10; we
    are in slow start, and we send a flight of 9 packets. By the time we
    have received ACKs for all 9 packets we want our cwnd to be 18.
    We implement this by setting tp->lsnd_pending to 9, and
    considering ourselves to be cwnd-limited while cwnd is less than
    twice tp->lsnd_pending (2*9 -> 18).
    
    This makes tcp_is_cwnd_limited() more understandable, by removing
    the GSO/TSO kludge, that tried to work around the issue.
    
    Note the in_flight parameter can be removed in a followup cleanup
    patch.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 163d2b467d78..a9fe7bc4f4bb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -974,7 +974,27 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 {
 	return tp->snd_una + tp->snd_wnd;
 }
-bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
+
+/* We follow the spirit of RFC2861 to validate cwnd but implement a more
+ * flexible approach. The RFC suggests cwnd should not be raised unless
+ * it was fully used previously. But we allow cwnd to grow as long as the
+ * application has used half the cwnd.
+ * Example :
+ *    cwnd is 10 (IW10), but application sends 9 frames.
+ *    We allow cwnd to reach 18 when all frames are ACKed.
+ * This check is safe because it's as aggressive as slow start which already
+ * risks 100% overshoot. The advantage is that we discourage application to
+ * either send more filler packets or data to artificially blow up the cwnd
+ * usage, and allow application-limited process to probe bw more aggressively.
+ *
+ * TODO: remove in_flight once we can fix all callers, and their callers...
+ */
+static inline bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+
+	return tp->snd_cwnd < 2 * tp->lsnd_pending;
+}
 
 static inline void tcp_check_probe_timer(struct sock *sk)
 {

commit 86fd14ad1e8c4b8f5e9a7a27b26bdade91dd4bd0
Author: Weiping Pan <wpan@redhat.com>
Date:   Fri Apr 18 12:27:46 2014 +0800

    tcp: make tcp_cwnd_application_limited() static
    
    Make tcp_cwnd_application_limited() static and move it from tcp_input.c to
    tcp_output.c
    
    Signed-off-by: Weiping Pan <wpan@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 87d877408188..163d2b467d78 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -558,7 +558,6 @@ void tcp_send_loss_probe(struct sock *sk);
 bool tcp_schedule_loss_probe(struct sock *sk);
 
 /* tcp_input.c */
-void tcp_cwnd_application_limited(struct sock *sk);
 void tcp_resume_early_retransmit(struct sock *sk);
 void tcp_rearm_rto(struct sock *sk);
 void tcp_reset(struct sock *sk);

commit 04f58c88542b6b351efb4eea01134eb672e22e6e
Merge: 0fc31966035d 632b06aa2842
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Mar 25 20:29:20 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            Documentation/devicetree/bindings/net/micrel-ks8851.txt
            net/core/netpoll.c
    
    The net/core/netpoll.c conflict is a bug fix in 'net' happening
    to code which is completely removed in 'net-next'.
    
    In micrel-ks8851.txt we simply have overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 632623153196bf183a69686ed9c07eee98ff1bf8
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 19 21:02:21 2014 -0700

    tcp: syncookies: do not use getnstimeofday()
    
    While it is true that getnstimeofday() uses about 40 cycles if TSC
    is available, it can use 1600 cycles if hpet is the clocksource.
    
    Switch to get_jiffies_64(), as this is more than enough, and
    go back to 60 seconds periods.
    
    Fixes: 8c27bd75f04f ("tcp: syncookies: reduce cookie lifetime to 128 seconds")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Florian Westphal <fw@strlen.de>
    Acked-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8c4dd63134d4..743accec6c76 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -480,20 +480,21 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 #ifdef CONFIG_SYN_COOKIES
 #include <linux/ktime.h>
 
-/* Syncookies use a monotonic timer which increments every 64 seconds.
+/* Syncookies use a monotonic timer which increments every 60 seconds.
  * This counter is used both as a hash input and partially encoded into
  * the cookie value.  A cookie is only validated further if the delta
  * between the current counter value and the encoded one is less than this,
- * i.e. a sent cookie is valid only at most for 128 seconds (or less if
+ * i.e. a sent cookie is valid only at most for 2*60 seconds (or less if
  * the counter advances immediately after a cookie is generated).
  */
 #define MAX_SYNCOOKIE_AGE 2
 
 static inline u32 tcp_cookie_time(void)
 {
-	struct timespec now;
-	getnstimeofday(&now);
-	return now.tv_sec >> 6; /* 64 seconds granularity */
+	u64 val = get_jiffies_64();
+
+	do_div(val, 60 * HZ);
+	return val;
 }
 
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,

commit 67ddc87f162e2d0e29db2b6b21c5a3fbcb8be206
Merge: 6092c79fd00c c3bebc71c4bc
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Mar 5 20:32:02 2014 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/wireless/ath/ath9k/recv.c
            drivers/net/wireless/mwifiex/pcie.c
            net/ipv6/sit.c
    
    The SIT driver conflict consists of a bug fix being done by hand
    in 'net' (missing u64_stats_init()) whilst in 'net-next' a helper
    was created (netdev_alloc_pcpu_stats()) which takes care of this.
    
    The two wireless conflicts were overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 740b0f1841f6e39085b711d41db9ffb07198682b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Feb 26 14:02:48 2014 -0800

    tcp: switch rtt estimations to usec resolution
    
    Upcoming congestion controls for TCP require usec resolution for RTT
    estimations. Millisecond resolution is simply not enough these days.
    
    FQ/pacing in DC environments also require this change for finer control
    and removal of bimodal behavior due to the current hack in
    tcp_update_pacing_rate() for 'small rtt'
    
    TCP_CONG_RTT_STAMP is no longer needed.
    
    As Julian Anastasov pointed out, we need to keep user compatibility :
    tcp_metrics used to export RTT and RTTVAR in msec resolution,
    so we added RTT_US and RTTVAR_US. An iproute2 patch is needed
    to use the new attributes if provided by the kernel.
    
    In this example ss command displays a srtt of 32 usecs (10Gbit link)
    
    lpk51:~# ./ss -i dst lpk52
    Netid  State      Recv-Q Send-Q   Local Address:Port       Peer
    Address:Port
    tcp    ESTAB      0      1         10.246.11.51:42959
    10.246.11.52:64614
             cubic wscale:6,6 rto:201 rtt:0.032/0.001 ato:40 mss:1448
    cwnd:10 send
    3620.0Mbps pacing_rate 7240.0Mbps unacked:1 rcv_rtt:993 rcv_space:29559
    
    Updated iproute2 ip command displays :
    
    lpk51:~# ./ip tcp_metrics | grep 10.246.11.52
    10.246.11.52 age 561.914sec cwnd 10 rtt 274us rttvar 213us source
    10.246.11.51
    
    Old binary displays :
    
    lpk51:~# ip tcp_metrics | grep 10.246.11.52
    10.246.11.52 age 561.914sec cwnd 10 rtt 250us rttvar 125us source
    10.246.11.51
    
    With help from Julian Anastasov, Stephen Hemminger and Yuchung Cheng
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Cc: Stephen Hemminger <stephen@networkplumber.org>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Larry Brakmo <brakmo@google.com>
    Cc: Julian Anastasov <ja@ssi.bg>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1f820537741a..93eab0b9da60 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -31,6 +31,7 @@
 #include <linux/crypto.h>
 #include <linux/cryptohash.h>
 #include <linux/kref.h>
+#include <linux/ktime.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -478,7 +479,6 @@ int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 			     struct ip_options *opt);
 #ifdef CONFIG_SYN_COOKIES
-#include <linux/ktime.h>
 
 /* Syncookies use a monotonic timer which increments every 64 seconds.
  * This counter is used both as a hash input and partially encoded into
@@ -619,7 +619,7 @@ static inline void tcp_bound_rto(const struct sock *sk)
 
 static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 {
-	return (tp->srtt >> 3) + tp->rttvar;
+	return usecs_to_jiffies((tp->srtt_us >> 3) + tp->rttvar_us);
 }
 
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
@@ -656,6 +656,11 @@ static inline u32 tcp_rto_min(struct sock *sk)
 	return rto_min;
 }
 
+static inline u32 tcp_rto_min_us(struct sock *sk)
+{
+	return jiffies_to_usecs(tcp_rto_min(sk));
+}
+
 /* Compute the actual receive window we are currently advertising.
  * Rcv_nxt can be after the window if our peer push more data
  * than the offered window.
@@ -778,7 +783,6 @@ enum tcp_ca_event {
 #define TCP_CA_BUF_MAX	(TCP_CA_NAME_MAX*TCP_CA_MAX)
 
 #define TCP_CONG_NON_RESTRICTED 0x1
-#define TCP_CONG_RTT_STAMP	0x2
 
 struct tcp_congestion_ops {
 	struct list_head	list;

commit f5ddcbbb40aa0ba7fbfe22355d287603dbeeaaac
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Feb 20 10:09:18 2014 -0800

    net-tcp: fastopen: fix high order allocations
    
    This patch fixes two bugs in fastopen :
    
    1) The tcp_sendmsg(...,  @size) argument was ignored.
    
       Code was relying on user not fooling the kernel with iovec mismatches
    
    2) When MTU is about 64KB, tcp_send_syn_data() attempts order-5
    allocations, which are likely to fail when memory gets fragmented.
    
    Fixes: 783237e8daf13 ("net-tcp: Fast Open client - sending SYN-data")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Tested-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 56fc366da6d5..8c4dd63134d4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1303,7 +1303,8 @@ struct tcp_fastopen_request {
 	/* Fast Open cookie. Size 0 means a cookie request */
 	struct tcp_fastopen_cookie	cookie;
 	struct msghdr			*data;  /* data in MSG_FASTOPEN */
-	u16				copied;	/* queued in tcp_connect() */
+	size_t				size;
+	int				copied;	/* queued in tcp_connect() */
 };
 void tcp_free_fastopen_req(struct tcp_sock *tp);
 

commit 45f7435968363816f8fc4c6abef692808534140d
Author: Stanislav Fomichev <stfomichev@yandex-team.ru>
Date:   Wed Feb 12 17:35:21 2014 +0400

    tcp: remove unused min_cwnd member of tcp_congestion_ops
    
    Commit 684bad110757 "tcp: use PRR to reduce cwin in CWR state" removed all
    calls to min_cwnd, so we can safely remove it.
    Also, remove tcp_reno_min_cwnd because it was only used for min_cwnd.
    
    Signed-off-by: Stanislav Fomichev <stfomichev@yandex-team.ru>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 56fc366da6d5..1f820537741a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -791,8 +791,6 @@ struct tcp_congestion_ops {
 
 	/* return slow start threshold (required) */
 	u32 (*ssthresh)(struct sock *sk);
-	/* lower bound for congestion window (optional) */
-	u32 (*min_cwnd)(const struct sock *sk);
 	/* do new cwnd calculation (required) */
 	void (*cong_avoid)(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
 	/* call before changing ca_state (optional) */
@@ -827,7 +825,6 @@ void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 u32 tcp_reno_ssthresh(struct sock *sk);
 void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
-u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 
 static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)

commit f7e56a76acf642d21a8e7bd587e270ae7addc4db
Author: stephen hemminger <stephen@networkplumber.org>
Date:   Sun Dec 29 11:39:51 2013 -0800

    tcp: make local functions static
    
    The following are only used in one file:
      tcp_connect_init
      tcp_set_rto
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9cd62bc09055..56fc366da6d5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -468,7 +468,6 @@ struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 				struct tcp_fastopen_cookie *foc);
 int tcp_disconnect(struct sock *sk, int flags);
 
-void tcp_connect_init(struct sock *sk);
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
 int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
 void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
@@ -623,8 +622,6 @@ static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 	return (tp->srtt >> 3) + tp->rttvar;
 }
 
-void tcp_set_rto(struct sock *sk);
-
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
 	tp->pred_flags = htonl((tp->tcp_header_len << 26) |

commit d4589926d7a9a4b88e650bb9154bab71e8d2a7dd
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Dec 13 13:51:23 2013 -0800

    tcp: refine TSO splits
    
    While investigating performance problems on small RPC workloads,
    I noticed linux TCP stack was always splitting the last TSO skb
    into two parts (skbs). One being a multiple of MSS, and a small one
    with the Push flag. This split is done even if TCP_NODELAY is set,
    or if no small packet is in flight.
    
    Example with request/response of 4K/4K
    
    IP A > B: . ack 68432 win 2783 <nop,nop,timestamp 6524593 6525001>
    IP A > B: . 65537:68433(2896) ack 69632 win 2783 <nop,nop,timestamp 6524593 6525001>
    IP A > B: P 68433:69633(1200) ack 69632 win 2783 <nop,nop,timestamp 6524593 6525001>
    IP B > A: . ack 68433 win 2768 <nop,nop,timestamp 6525001 6524593>
    IP B > A: . 69632:72528(2896) ack 69633 win 2768 <nop,nop,timestamp 6525001 6524593>
    IP B > A: P 72528:73728(1200) ack 69633 win 2768 <nop,nop,timestamp 6525001 6524593>
    IP A > B: . ack 72528 win 2783 <nop,nop,timestamp 6524593 6525001>
    IP A > B: . 69633:72529(2896) ack 73728 win 2783 <nop,nop,timestamp 6524593 6525001>
    IP A > B: P 72529:73729(1200) ack 73728 win 2783 <nop,nop,timestamp 6524593 6525001>
    
    We can avoid this split by including the Nagle tests at the right place.
    
    Note : If some NIC had trouble sending TSO packets with a partial
    last segment, we would have hit the problem in GRO/forwarding workload already.
    
    tcp_minshall_update() is moved to tcp_output.c and is updated as we might
    feed a TSO packet with a partial last segment.
    
    This patch tremendously improves performance, as the traffic now looks
    like :
    
    IP A > B: . ack 98304 win 2783 <nop,nop,timestamp 6834277 6834685>
    IP A > B: P 94209:98305(4096) ack 98304 win 2783 <nop,nop,timestamp 6834277 6834685>
    IP B > A: . ack 98305 win 2768 <nop,nop,timestamp 6834686 6834277>
    IP B > A: P 98304:102400(4096) ack 98305 win 2768 <nop,nop,timestamp 6834686 6834277>
    IP A > B: . ack 102400 win 2783 <nop,nop,timestamp 6834279 6834686>
    IP A > B: P 98305:102401(4096) ack 102400 win 2783 <nop,nop,timestamp 6834279 6834686>
    IP B > A: . ack 102401 win 2768 <nop,nop,timestamp 6834687 6834279>
    IP B > A: P 102400:106496(4096) ack 102401 win 2768 <nop,nop,timestamp 6834687 6834279>
    IP A > B: . ack 106496 win 2783 <nop,nop,timestamp 6834280 6834687>
    IP A > B: P 102401:106497(4096) ack 106496 win 2783 <nop,nop,timestamp 6834280 6834687>
    IP B > A: . ack 106497 win 2768 <nop,nop,timestamp 6834688 6834280>
    IP B > A: P 106496:110592(4096) ack 106497 win 2768 <nop,nop,timestamp 6834688 6834280>
    
    Before :
    
    lpq83:~# nstat >/dev/null;perf stat ./super_netperf 200 -t TCP_RR -H lpq84 -l 20 -- -r 4K,4K
    280774
    
     Performance counter stats for './super_netperf 200 -t TCP_RR -H lpq84 -l 20 -- -r 4K,4K':
    
         205719.049006 task-clock                #    9.278 CPUs utilized
             8,449,968 context-switches          #    0.041 M/sec
             1,935,997 CPU-migrations            #    0.009 M/sec
               160,541 page-faults               #    0.780 K/sec
       548,478,722,290 cycles                    #    2.666 GHz                     [83.20%]
       455,240,670,857 stalled-cycles-frontend   #   83.00% frontend cycles idle    [83.48%]
       272,881,454,275 stalled-cycles-backend    #   49.75% backend  cycles idle    [66.73%]
       166,091,460,030 instructions              #    0.30  insns per cycle
                                                 #    2.74  stalled cycles per insn [83.39%]
        29,150,229,399 branches                  #  141.699 M/sec                   [83.30%]
         1,943,814,026 branch-misses             #    6.67% of all branches         [83.32%]
    
          22.173517844 seconds time elapsed
    
    lpq83:~# nstat | egrep "IpOutRequests|IpExtOutOctets"
    IpOutRequests                   16851063           0.0
    IpExtOutOctets                  23878580777        0.0
    
    After patch :
    
    lpq83:~# nstat >/dev/null;perf stat ./super_netperf 200 -t TCP_RR -H lpq84 -l 20 -- -r 4K,4K
    280877
    
     Performance counter stats for './super_netperf 200 -t TCP_RR -H lpq84 -l 20 -- -r 4K,4K':
    
         107496.071918 task-clock                #    4.847 CPUs utilized
             5,635,458 context-switches          #    0.052 M/sec
             1,374,707 CPU-migrations            #    0.013 M/sec
               160,920 page-faults               #    0.001 M/sec
       281,500,010,924 cycles                    #    2.619 GHz                     [83.28%]
       228,865,069,307 stalled-cycles-frontend   #   81.30% frontend cycles idle    [83.38%]
       142,462,742,658 stalled-cycles-backend    #   50.61% backend  cycles idle    [66.81%]
        95,227,712,566 instructions              #    0.34  insns per cycle
                                                 #    2.40  stalled cycles per insn [83.43%]
        16,209,868,171 branches                  #  150.795 M/sec                   [83.20%]
           874,252,952 branch-misses             #    5.39% of all branches         [83.37%]
    
          22.175821286 seconds time elapsed
    
    lpq83:~# nstat | egrep "IpOutRequests|IpExtOutOctets"
    IpOutRequests                   11239428           0.0
    IpExtOutOctets                  23595191035        0.0
    
    Indeed, the occupancy of tx skbs (IpExtOutOctets/IpOutRequests) is higher :
    2099 instead of 1417, thus helping GRO to be more efficient when using FQ packet
    scheduler.
    
    Many thanks to Neal for review and ideas.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Tested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f7e1ab2139ef..9cd62bc09055 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -978,13 +978,6 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 }
 bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
 
-static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
-				       const struct sk_buff *skb)
-{
-	if (skb->len < mss)
-		tp->snd_sml = TCP_SKB_CB(skb)->end_seq;
-}
-
 static inline void tcp_check_probe_timer(struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);

commit f54b311142a92ea2e42598e347b84e1655caf8e3
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 5 22:36:05 2013 -0800

    tcp: auto corking
    
    With the introduction of TCP Small Queues, TSO auto sizing, and TCP
    pacing, we can implement Automatic Corking in the kernel, to help
    applications doing small write()/sendmsg() to TCP sockets.
    
    Idea is to change tcp_push() to check if the current skb payload is
    under skb optimal size (a multiple of MSS bytes)
    
    If under 'size_goal', and at least one packet is still in Qdisc or
    NIC TX queues, set the TCP Small Queue Throttled bit, so that the push
    will be delayed up to TX completion time.
    
    This delay might allow the application to coalesce more bytes
    in the skb in following write()/sendmsg()/sendfile() system calls.
    
    The exact duration of the delay is depending on the dynamics
    of the system, and might be zero if no packet for this flow
    is actually held in Qdisc or NIC TX ring.
    
    Using FQ/pacing is a way to increase the probability of
    autocorking being triggered.
    
    Add a new sysctl (/proc/sys/net/ipv4/tcp_autocorking) to control
    this feature and default it to 1 (enabled)
    
    Add a new SNMP counter : nstat -a | grep TcpExtTCPAutoCorking
    This counter is incremented every time we detected skb was under used
    and its flush was deferred.
    
    Tested:
    
    Interesting effects when using line buffered commands under ssh.
    
    Excellent performance results in term of cpu usage and total throughput.
    
    lpq83:~# echo 1 >/proc/sys/net/ipv4/tcp_autocorking
    lpq83:~# perf stat ./super_netperf 4 -t TCP_STREAM -H lpq84 -- -m 128
    9410.39
    
     Performance counter stats for './super_netperf 4 -t TCP_STREAM -H lpq84 -- -m 128':
    
          35209.439626 task-clock                #    2.901 CPUs utilized
                 2,294 context-switches          #    0.065 K/sec
                   101 CPU-migrations            #    0.003 K/sec
                 4,079 page-faults               #    0.116 K/sec
        97,923,241,298 cycles                    #    2.781 GHz                     [83.31%]
        51,832,908,236 stalled-cycles-frontend   #   52.93% frontend cycles idle    [83.30%]
        25,697,986,603 stalled-cycles-backend    #   26.24% backend  cycles idle    [66.70%]
       102,225,978,536 instructions              #    1.04  insns per cycle
                                                 #    0.51  stalled cycles per insn [83.38%]
        18,657,696,819 branches                  #  529.906 M/sec                   [83.29%]
            91,679,646 branch-misses             #    0.49% of all branches         [83.40%]
    
          12.136204899 seconds time elapsed
    
    lpq83:~# echo 0 >/proc/sys/net/ipv4/tcp_autocorking
    lpq83:~# perf stat ./super_netperf 4 -t TCP_STREAM -H lpq84 -- -m 128
    6624.89
    
     Performance counter stats for './super_netperf 4 -t TCP_STREAM -H lpq84 -- -m 128':
          40045.864494 task-clock                #    3.301 CPUs utilized
                   171 context-switches          #    0.004 K/sec
                    53 CPU-migrations            #    0.001 K/sec
                 4,080 page-faults               #    0.102 K/sec
       111,340,458,645 cycles                    #    2.780 GHz                     [83.34%]
        61,778,039,277 stalled-cycles-frontend   #   55.49% frontend cycles idle    [83.31%]
        29,295,522,759 stalled-cycles-backend    #   26.31% backend  cycles idle    [66.67%]
       108,654,349,355 instructions              #    0.98  insns per cycle
                                                 #    0.57  stalled cycles per insn [83.34%]
        19,552,170,748 branches                  #  488.244 M/sec                   [83.34%]
           157,875,417 branch-misses             #    0.81% of all branches         [83.34%]
    
          12.130267788 seconds time elapsed
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 70e55d200610..f7e1ab2139ef 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -282,6 +282,7 @@ extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
 extern unsigned int sysctl_tcp_notsent_lowat;
 extern int sysctl_tcp_min_tso_segs;
+extern int sysctl_tcp_autocorking;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 9f9843a751d0a2057f9f3d313886e7e5e6ebaac9
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Oct 31 11:07:31 2013 -0700

    tcp: properly handle stretch acks in slow start
    
    Slow start now increases cwnd by 1 if an ACK acknowledges some packets,
    regardless the number of packets. Consequently slow start performance
    is highly dependent on the degree of the stretch ACKs caused by
    receiver or network ACK compression mechanisms (e.g., delayed-ACK,
    GRO, etc).  But slow start algorithm is to send twice the amount of
    packets of packets left so it should process a stretch ACK of degree
    N as if N ACKs of degree 1, then exits when cwnd exceeds ssthresh. A
    follow up patch will use the remainder of the N (if greater than 1)
    to adjust cwnd in the congestion avoidance phase.
    
    In addition this patch retires the experimental limited slow start
    (LSS) feature. LSS has multiple drawbacks but questionable benefit. The
    fractional cwnd increase in LSS requires a loop in slow start even
    though it's rarely used. Configuring such an increase step via a global
    sysctl on different BDPS seems hard. Finally and most importantly the
    slow start overshoot concern is now better covered by the Hybrid slow
    start (hystart) enabled by default.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2d7b4bdc972f..70e55d200610 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -275,7 +275,6 @@ extern int sysctl_tcp_mtu_probing;
 extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
-extern int sysctl_tcp_max_ssthresh;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
@@ -797,7 +796,7 @@ struct tcp_congestion_ops {
 	/* lower bound for congestion window (optional) */
 	u32 (*min_cwnd)(const struct sock *sk);
 	/* do new cwnd calculation (required) */
-	void (*cong_avoid)(struct sock *sk, u32 ack, u32 in_flight);
+	void (*cong_avoid)(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
 	/* call before changing ca_state (optional) */
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
@@ -824,12 +823,12 @@ void tcp_get_available_congestion_control(char *buf, size_t len);
 void tcp_get_allowed_congestion_control(char *buf, size_t len);
 int tcp_set_allowed_congestion_control(char *allowed);
 int tcp_set_congestion_control(struct sock *sk, const char *name);
-void tcp_slow_start(struct tcp_sock *tp);
+int tcp_slow_start(struct tcp_sock *tp, u32 acked);
 void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 u32 tcp_reno_ssthresh(struct sock *sk);
-void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight);
+void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked, u32 in_flight);
 u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 

commit a4fe34bf902b8f709c635ab37f1f39de0b86cff2
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Oct 19 16:25:36 2013 -0700

    tcp_memcontrol: Remove the per netns control.
    
    The code that is implemented is per memory cgroup not per netns, and
    having per netns bits is just confusing.  Remove the per netns bits to
    make it easier to see what is really going on.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b12e29a76590..2d7b4bdc972f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -259,6 +259,7 @@ extern int sysctl_tcp_max_orphans;
 extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_dsack;
+extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
 extern int sysctl_tcp_app_win;
@@ -348,8 +349,6 @@ extern struct proto tcp_prot;
 #define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
-void tcp_init_mem(struct net *net);
-
 void tcp_tasklet_init(void);
 
 void tcp_v4_err(struct sk_buff *skb, u32);

commit 222e83d2e0aecb6a5e8d42b1a8d51332a1eba960
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Sat Oct 19 21:48:58 2013 +0200

    tcp: switch tcp_fastopen key generation to net_get_random_once
    
    Changed key initialization of tcp_fastopen cookies to net_get_random_once.
    
    If the user sets a custom key net_get_random_once must be called at
    least once to ensure we don't overwrite the user provided key when the
    first cookie is generated later on.
    
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f30326f1c92b..b12e29a76590 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1322,7 +1322,7 @@ extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
 void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
 			     struct tcp_fastopen_cookie *foc);
-
+void tcp_fastopen_init_key_once(bool publish);
 #define TCP_FASTOPEN_KEY_LENGTH 16
 
 /* Fastopen key context */

commit b23a002fc6f0c19846ee0382f019429af54a27e9
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Sat Oct 19 21:48:56 2013 +0200

    inet: split syncookie keys for ipv4 and ipv6 and initialize with net_get_random_once
    
    This patch splits the secret key for syncookies for ipv4 and ipv6 and
    initializes them with net_get_random_once. This change was the reason I
    did this series. I think the initialization of the syncookie_secret is
    way to early.
    
    Cc: Florian Westphal <fw@strlen.de>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 372dcccfeed0..f30326f1c92b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -475,7 +475,6 @@ int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
 void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 
 /* From syncookies.c */
-extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
 int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 		      u32 cookie);
 struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,

commit 28be6e07e8bccee76b51bca8fdba52c1b28fc77c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 18 10:36:17 2013 -0700

    tcp: rename tcp_tso_segment()
    
    Rename tcp_tso_segment() to tcp_gso_segment(), to better reflect
    what is going on, and ease grep games.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1db3a016bff6..372dcccfeed0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1548,7 +1548,7 @@ extern struct request_sock_ops tcp6_request_sock_ops;
 
 void tcp_v4_destroy_sock(struct sock *sk);
 
-struct sk_buff *tcp_tso_segment(struct sk_buff *skb,
+struct sk_buff *tcp_gso_segment(struct sk_buff *skb,
 				netdev_features_t features);
 struct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb);
 int tcp_gro_complete(struct sk_buff *skb);

commit b44084c2c822f99dd3f2334b288b7e463d222662
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 00:04:37 2013 -0700

    inet: rename ir_loc_port to ir_num
    
    In commit 634fb979e8f ("inet: includes a sock_common in request_sock")
    I forgot that the two ports in sock_common do not have same byte order :
    
    skc_dport is __be16 (network order), but skc_num is __u16 (host order)
    
    So sparse complains because ir_loc_port (mapped into skc_num) is
    considered as __u16 while it should be __be16
    
    Let rename ir_loc_port to ireq->ir_num (analogy with inet->inet_num),
    and perform appropriate htons/ntohs conversions.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 24a06161d174..1db3a016bff6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1110,7 +1110,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->acked = 0;
 	ireq->ecn_ok = 0;
 	ireq->ir_rmt_port = tcp_hdr(skb)->source;
-	ireq->ir_loc_port = tcp_hdr(skb)->dest;
+	ireq->ir_num = ntohs(tcp_hdr(skb)->dest);
 }
 
 void tcp_enter_memory_pressure(struct sock *sk);

commit 634fb979e8f3a70f04c1f2f519d0cd1142eb5c1a
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 15:21:29 2013 -0700

    inet: includes a sock_common in request_sock
    
    TCP listener refactoring, part 5 :
    
    We want to be able to insert request sockets (SYN_RECV) into main
    ehash table instead of the per listener hash table to allow RCU
    lookups and remove listener lock contention.
    
    This patch includes the needed struct sock_common in front
    of struct request_sock
    
    This means there is no more inet6_request_sock IPv6 specific
    structure.
    
    Following inet_request_sock fields were renamed as they became
    macros to reference fields from struct sock_common.
    Prefix ir_ was chosen to avoid name collisions.
    
    loc_port   -> ir_loc_port
    loc_addr   -> ir_loc_addr
    rmt_addr   -> ir_rmt_addr
    rmt_port   -> ir_rmt_port
    iif        -> ir_iif
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 39bbfa1602b2..24a06161d174 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1109,8 +1109,8 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->wscale_ok = rx_opt->wscale_ok;
 	ireq->acked = 0;
 	ireq->ecn_ok = 0;
-	ireq->rmt_port = tcp_hdr(skb)->source;
-	ireq->loc_port = tcp_hdr(skb)->dest;
+	ireq->ir_rmt_port = tcp_hdr(skb)->source;
+	ireq->ir_loc_port = tcp_hdr(skb)->dest;
 }
 
 void tcp_enter_memory_pressure(struct sock *sk);

commit 05dbc7b59481ca891bbcfe6799a562d48159fbf7
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 3 00:22:02 2013 -0700

    tcp/dccp: remove twchain
    
    TCP listener refactoring, part 3 :
    
    Our goal is to hash SYN_RECV sockets into main ehash for fast lookup,
    and parallel SYN processing.
    
    Current inet_ehash_bucket contains two chains, one for ESTABLISH (and
    friend states) sockets, another for TIME_WAIT sockets only.
    
    As the hash table is sized to get at most one socket per bucket, it
    makes little sense to have separate twchain, as it makes the lookup
    slightly more complicated, and doubles hash table memory usage.
    
    If we make sure all socket types have the lookup keys at the same
    offsets, we can use a generic and faster lookup. It turns out TIME_WAIT
    and ESTABLISHED sockets already have common lookup fields for IPv4.
    
    [ INET_TW_MATCH() is no longer needed ]
    
    I'll provide a follow-up to factorize IPv6 lookup as well, to remove
    INET6_TW_MATCH()
    
    This way, SYN_RECV pseudo sockets will be supported the same.
    
    A new sock_gen_put() helper is added, doing either a sock_put() or
    inet_twsk_put() [ and will support SYN_RECV later ].
    
    Note this helper should only be called in real slow path, when rcu
    lookup found a socket that was moved to another identity (freed/reused
    immediately), but could eventually be used in other contexts, like
    sock_edemux()
    
    Before patch :
    
    dmesg | grep "TCP established"
    
    TCP established hash table entries: 524288 (order: 11, 8388608 bytes)
    
    After patch :
    
    TCP established hash table entries: 524288 (order: 10, 4194304 bytes)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index de870ee5582d..39bbfa1602b2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1519,7 +1519,6 @@ enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,
 	TCP_SEQ_STATE_OPENREQ,
 	TCP_SEQ_STATE_ESTABLISHED,
-	TCP_SEQ_STATE_TIME_WAIT,
 };
 
 int tcp_seq_open(struct inode *inode, struct file *file);

commit 8c27bd75f04fb9cb70c69c3cfe24f4e6d8e15906
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Sep 20 22:32:55 2013 +0200

    tcp: syncookies: reduce cookie lifetime to 128 seconds
    
    We currently accept cookies that were created less than 4 minutes ago
    (ie, cookies with counter delta 0-3).  Combined with the 8 mss table
    values, this yields 32 possible values (out of 2**32) that will be valid.
    
    Reducing the lifetime to < 2 minutes halves the guessing chance while
    still providing a large enough period.
    
    While at it, get rid of jiffies value -- they overflow too quickly on
    32 bit platforms.
    
    getnstimeofday is used to create a counter that increments every 64s.
    perf shows getnstimeofday cost is negible compared to sha_transform;
    normal tcp initial sequence number generation uses getnstimeofday, too.
    
    Reported-by: Jakob Lell <jakob@jakoblell.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0e47551e9bdb..de870ee5582d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -481,6 +481,24 @@ int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
 struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 			     struct ip_options *opt);
 #ifdef CONFIG_SYN_COOKIES
+#include <linux/ktime.h>
+
+/* Syncookies use a monotonic timer which increments every 64 seconds.
+ * This counter is used both as a hash input and partially encoded into
+ * the cookie value.  A cookie is only validated further if the delta
+ * between the current counter value and the encoded one is less than this,
+ * i.e. a sent cookie is valid only at most for 128 seconds (or less if
+ * the counter advances immediately after a cookie is generated).
+ */
+#define MAX_SYNCOOKIE_AGE 2
+
+static inline u32 tcp_cookie_time(void)
+{
+	struct timespec now;
+	getnstimeofday(&now);
+	return now.tv_sec >> 6; /* 64 seconds granularity */
+}
+
 u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
 			      u16 *mssp);
 __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, __u16 *mss);

commit 5c9f30236f53f3fbee170e2452dfa0e5dce4727b
Author: Joe Perches <joe@perches.com>
Date:   Mon Sep 23 11:33:32 2013 -0700

    tcp.h: Remove extern from function prototypes
    
    There are a mix of function prototypes with and without extern
    in the kernel sources.  Standardize on not using extern for
    function prototypes.
    
    Function prototypes don't need to be written with extern.
    extern is assumed by the compiler.  Its use is as unnecessary as
    using auto to declare automatic/local variables in a block.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b1aa324c5e65..0e47551e9bdb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -50,7 +50,7 @@
 extern struct inet_hashinfo tcp_hashinfo;
 
 extern struct percpu_counter tcp_orphan_count;
-extern void tcp_time_wait(struct sock *sk, int state, int timeo);
+void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 #define MAX_TCP_OPTION_SPACE 40
@@ -325,7 +325,7 @@ static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 	return false;
 }
 
-extern bool tcp_check_oom(struct sock *sk, int shift);
+bool tcp_check_oom(struct sock *sk, int shift);
 
 /* syncookies: remember time of last synqueue overflow */
 static inline void tcp_synq_overflow(struct sock *sk)
@@ -348,38 +348,38 @@ extern struct proto tcp_prot;
 #define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
-extern void tcp_init_mem(struct net *net);
-
-extern void tcp_tasklet_init(void);
-
-extern void tcp_v4_err(struct sk_buff *skb, u32);
-
-extern void tcp_shutdown (struct sock *sk, int how);
-
-extern void tcp_v4_early_demux(struct sk_buff *skb);
-extern int tcp_v4_rcv(struct sk_buff *skb);
-
-extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
-extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
-		       size_t size);
-extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
-			size_t size, int flags);
-extern void tcp_release_cb(struct sock *sk);
-extern void tcp_wfree(struct sk_buff *skb);
-extern void tcp_write_timer_handler(struct sock *sk);
-extern void tcp_delack_timer_handler(struct sock *sk);
-extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
-extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
-				 const struct tcphdr *th, unsigned int len);
-extern void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-				const struct tcphdr *th, unsigned int len);
-extern void tcp_rcv_space_adjust(struct sock *sk);
-extern void tcp_cleanup_rbuf(struct sock *sk, int copied);
-extern int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
-extern void tcp_twsk_destructor(struct sock *sk);
-extern ssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,
-			       struct pipe_inode_info *pipe, size_t len,
-			       unsigned int flags);
+void tcp_init_mem(struct net *net);
+
+void tcp_tasklet_init(void);
+
+void tcp_v4_err(struct sk_buff *skb, u32);
+
+void tcp_shutdown(struct sock *sk, int how);
+
+void tcp_v4_early_demux(struct sk_buff *skb);
+int tcp_v4_rcv(struct sk_buff *skb);
+
+int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
+int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
+		size_t size);
+int tcp_sendpage(struct sock *sk, struct page *page, int offset, size_t size,
+		 int flags);
+void tcp_release_cb(struct sock *sk);
+void tcp_wfree(struct sk_buff *skb);
+void tcp_write_timer_handler(struct sock *sk);
+void tcp_delack_timer_handler(struct sock *sk);
+int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
+int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
+			  const struct tcphdr *th, unsigned int len);
+void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
+			 const struct tcphdr *th, unsigned int len);
+void tcp_rcv_space_adjust(struct sock *sk);
+void tcp_cleanup_rbuf(struct sock *sk, int copied);
+int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
+void tcp_twsk_destructor(struct sock *sk);
+ssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,
+			struct pipe_inode_info *pipe, size_t len,
+			unsigned int flags);
 
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
@@ -409,66 +409,65 @@ enum tcp_tw_status {
 };
 
 
-extern enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
-						     struct sk_buff *skb,
-						     const struct tcphdr *th);
-extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
-				   struct request_sock *req,
-				   struct request_sock **prev,
-				   bool fastopen);
-extern int tcp_child_process(struct sock *parent, struct sock *child,
-			     struct sk_buff *skb);
-extern void tcp_enter_loss(struct sock *sk, int how);
-extern void tcp_clear_retrans(struct tcp_sock *tp);
-extern void tcp_update_metrics(struct sock *sk);
-extern void tcp_init_metrics(struct sock *sk);
-extern void tcp_metrics_init(void);
-extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check);
-extern bool tcp_remember_stamp(struct sock *sk);
-extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
-extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
-extern void tcp_disable_fack(struct tcp_sock *tp);
-extern void tcp_close(struct sock *sk, long timeout);
-extern void tcp_init_sock(struct sock *sk);
-extern unsigned int tcp_poll(struct file * file, struct socket *sock,
-			     struct poll_table_struct *wait);
-extern int tcp_getsockopt(struct sock *sk, int level, int optname,
+enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
+					      struct sk_buff *skb,
+					      const struct tcphdr *th);
+struct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,
+			   struct request_sock *req, struct request_sock **prev,
+			   bool fastopen);
+int tcp_child_process(struct sock *parent, struct sock *child,
+		      struct sk_buff *skb);
+void tcp_enter_loss(struct sock *sk, int how);
+void tcp_clear_retrans(struct tcp_sock *tp);
+void tcp_update_metrics(struct sock *sk);
+void tcp_init_metrics(struct sock *sk);
+void tcp_metrics_init(void);
+bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
+			bool paws_check);
+bool tcp_remember_stamp(struct sock *sk);
+bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
+void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
+void tcp_disable_fack(struct tcp_sock *tp);
+void tcp_close(struct sock *sk, long timeout);
+void tcp_init_sock(struct sock *sk);
+unsigned int tcp_poll(struct file *file, struct socket *sock,
+		      struct poll_table_struct *wait);
+int tcp_getsockopt(struct sock *sk, int level, int optname,
+		   char __user *optval, int __user *optlen);
+int tcp_setsockopt(struct sock *sk, int level, int optname,
+		   char __user *optval, unsigned int optlen);
+int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
 			  char __user *optval, int __user *optlen);
-extern int tcp_setsockopt(struct sock *sk, int level, int optname,
+int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
 			  char __user *optval, unsigned int optlen);
-extern int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
-				 char __user *optval, int __user *optlen);
-extern int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
-				 char __user *optval, unsigned int optlen);
-extern void tcp_set_keepalive(struct sock *sk, int val);
-extern void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
-extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
-		       size_t len, int nonblock, int flags, int *addr_len);
-extern void tcp_parse_options(const struct sk_buff *skb,
-			      struct tcp_options_received *opt_rx,
-			      int estab, struct tcp_fastopen_cookie *foc);
-extern const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
+void tcp_set_keepalive(struct sock *sk, int val);
+void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
+int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
+		size_t len, int nonblock, int flags, int *addr_len);
+void tcp_parse_options(const struct sk_buff *skb,
+		       struct tcp_options_received *opt_rx,
+		       int estab, struct tcp_fastopen_cookie *foc);
+const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
 /*
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
-extern int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
-extern struct sock * tcp_create_openreq_child(struct sock *sk,
-					      struct request_sock *req,
-					      struct sk_buff *skb);
-extern struct sock * tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
-					  struct request_sock *req,
-					  struct dst_entry *dst);
-extern int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
-extern int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr,
-			  int addr_len);
-extern int tcp_connect(struct sock *sk);
-extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
-					struct request_sock *req,
-					struct tcp_fastopen_cookie *foc);
-extern int tcp_disconnect(struct sock *sk, int flags);
+void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
+int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
+struct sock *tcp_create_openreq_child(struct sock *sk,
+				      struct request_sock *req,
+				      struct sk_buff *skb);
+struct sock *tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+				  struct request_sock *req,
+				  struct dst_entry *dst);
+int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
+int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);
+int tcp_connect(struct sock *sk);
+struct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,
+				struct request_sock *req,
+				struct tcp_fastopen_cookie *foc);
+int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_connect_init(struct sock *sk);
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
@@ -477,15 +476,14 @@ void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
-extern int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
-			     u32 cookie);
-extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
-				    struct ip_options *opt);
+int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
+		      u32 cookie);
+struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
+			     struct ip_options *opt);
 #ifdef CONFIG_SYN_COOKIES
-extern u32 __cookie_v4_init_sequence(const struct iphdr *iph,
-				     const struct tcphdr *th, u16 *mssp);
-extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
-				     __u16 *mss);
+u32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,
+			      u16 *mssp);
+__u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, __u16 *mss);
 #else
 static inline __u32 cookie_v4_init_sequence(struct sock *sk,
 					    struct sk_buff *skb,
@@ -495,19 +493,19 @@ static inline __u32 cookie_v4_init_sequence(struct sock *sk,
 }
 #endif
 
-extern __u32 cookie_init_timestamp(struct request_sock *req);
-extern bool cookie_check_timestamp(struct tcp_options_received *opt,
-				struct net *net, bool *ecn_ok);
+__u32 cookie_init_timestamp(struct request_sock *req);
+bool cookie_check_timestamp(struct tcp_options_received *opt, struct net *net,
+			    bool *ecn_ok);
 
 /* From net/ipv6/syncookies.c */
-extern int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
-			     u32 cookie);
-extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
+int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
+		      u32 cookie);
+struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
-extern u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
-				     const struct tcphdr *th, u16 *mssp);
-extern __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
-				     __u16 *mss);
+u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
+			      const struct tcphdr *th, u16 *mssp);
+__u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
+			      __u16 *mss);
 #else
 static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 					    struct sk_buff *skb,
@@ -518,47 +516,46 @@ static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 #endif
 /* tcp_output.c */
 
-extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
-				      int nonagle);
-extern bool tcp_may_send_now(struct sock *sk);
-extern int __tcp_retransmit_skb(struct sock *, struct sk_buff *);
-extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
-extern void tcp_retransmit_timer(struct sock *sk);
-extern void tcp_xmit_retransmit_queue(struct sock *);
-extern void tcp_simple_retransmit(struct sock *);
-extern int tcp_trim_head(struct sock *, struct sk_buff *, u32);
-extern int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int);
-
-extern void tcp_send_probe0(struct sock *);
-extern void tcp_send_partial(struct sock *);
-extern int tcp_write_wakeup(struct sock *);
-extern void tcp_send_fin(struct sock *sk);
-extern void tcp_send_active_reset(struct sock *sk, gfp_t priority);
-extern int tcp_send_synack(struct sock *);
-extern bool tcp_syn_flood_action(struct sock *sk,
-				 const struct sk_buff *skb,
-				 const char *proto);
-extern void tcp_push_one(struct sock *, unsigned int mss_now);
-extern void tcp_send_ack(struct sock *sk);
-extern void tcp_send_delayed_ack(struct sock *sk);
-extern void tcp_send_loss_probe(struct sock *sk);
-extern bool tcp_schedule_loss_probe(struct sock *sk);
+void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
+			       int nonagle);
+bool tcp_may_send_now(struct sock *sk);
+int __tcp_retransmit_skb(struct sock *, struct sk_buff *);
+int tcp_retransmit_skb(struct sock *, struct sk_buff *);
+void tcp_retransmit_timer(struct sock *sk);
+void tcp_xmit_retransmit_queue(struct sock *);
+void tcp_simple_retransmit(struct sock *);
+int tcp_trim_head(struct sock *, struct sk_buff *, u32);
+int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int);
+
+void tcp_send_probe0(struct sock *);
+void tcp_send_partial(struct sock *);
+int tcp_write_wakeup(struct sock *);
+void tcp_send_fin(struct sock *sk);
+void tcp_send_active_reset(struct sock *sk, gfp_t priority);
+int tcp_send_synack(struct sock *);
+bool tcp_syn_flood_action(struct sock *sk, const struct sk_buff *skb,
+			  const char *proto);
+void tcp_push_one(struct sock *, unsigned int mss_now);
+void tcp_send_ack(struct sock *sk);
+void tcp_send_delayed_ack(struct sock *sk);
+void tcp_send_loss_probe(struct sock *sk);
+bool tcp_schedule_loss_probe(struct sock *sk);
 
 /* tcp_input.c */
-extern void tcp_cwnd_application_limited(struct sock *sk);
-extern void tcp_resume_early_retransmit(struct sock *sk);
-extern void tcp_rearm_rto(struct sock *sk);
-extern void tcp_reset(struct sock *sk);
+void tcp_cwnd_application_limited(struct sock *sk);
+void tcp_resume_early_retransmit(struct sock *sk);
+void tcp_rearm_rto(struct sock *sk);
+void tcp_reset(struct sock *sk);
 
 /* tcp_timer.c */
-extern void tcp_init_xmit_timers(struct sock *);
+void tcp_init_xmit_timers(struct sock *);
 static inline void tcp_clear_xmit_timers(struct sock *sk)
 {
 	inet_csk_clear_xmit_timers(sk);
 }
 
-extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
-extern unsigned int tcp_current_mss(struct sock *sk);
+unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
+unsigned int tcp_current_mss(struct sock *sk);
 
 /* Bound MSS / TSO packet size with the half of the window */
 static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
@@ -584,20 +581,20 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 }
 
 /* tcp.c */
-extern void tcp_get_info(const struct sock *, struct tcp_info *);
+void tcp_get_info(const struct sock *, struct tcp_info *);
 
 /* Read 'sendfile()'-style from a TCP socket */
 typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
 				unsigned int, size_t);
-extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
-			 sk_read_actor_t recv_actor);
+int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
+		  sk_read_actor_t recv_actor);
 
-extern void tcp_initialize_rcv_mss(struct sock *sk);
+void tcp_initialize_rcv_mss(struct sock *sk);
 
-extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
-extern int tcp_mss_to_mtu(struct sock *sk, int mss);
-extern void tcp_mtup_init(struct sock *sk);
-extern void tcp_init_buffer_space(struct sock *sk);
+int tcp_mtu_to_mss(struct sock *sk, int pmtu);
+int tcp_mss_to_mtu(struct sock *sk, int mss);
+void tcp_mtup_init(struct sock *sk);
+void tcp_init_buffer_space(struct sock *sk);
 
 static inline void tcp_bound_rto(const struct sock *sk)
 {
@@ -610,7 +607,7 @@ static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 	return (tp->srtt >> 3) + tp->rttvar;
 }
 
-extern void tcp_set_rto(struct sock *sk);
+void tcp_set_rto(struct sock *sk);
 
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
@@ -663,7 +660,7 @@ static inline u32 tcp_receive_window(const struct tcp_sock *tp)
  * scaling applied to the result.  The caller does these things
  * if necessary.  This is a "raw" window selection.
  */
-extern u32 __tcp_select_window(struct sock *sk);
+u32 __tcp_select_window(struct sock *sk);
 
 void tcp_send_window_probe(struct sock *sk);
 
@@ -800,24 +797,24 @@ struct tcp_congestion_ops {
 	struct module 	*owner;
 };
 
-extern int tcp_register_congestion_control(struct tcp_congestion_ops *type);
-extern void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
+int tcp_register_congestion_control(struct tcp_congestion_ops *type);
+void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 
-extern void tcp_init_congestion_control(struct sock *sk);
-extern void tcp_cleanup_congestion_control(struct sock *sk);
-extern int tcp_set_default_congestion_control(const char *name);
-extern void tcp_get_default_congestion_control(char *name);
-extern void tcp_get_available_congestion_control(char *buf, size_t len);
-extern void tcp_get_allowed_congestion_control(char *buf, size_t len);
-extern int tcp_set_allowed_congestion_control(char *allowed);
-extern int tcp_set_congestion_control(struct sock *sk, const char *name);
-extern void tcp_slow_start(struct tcp_sock *tp);
-extern void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
+void tcp_init_congestion_control(struct sock *sk);
+void tcp_cleanup_congestion_control(struct sock *sk);
+int tcp_set_default_congestion_control(const char *name);
+void tcp_get_default_congestion_control(char *name);
+void tcp_get_available_congestion_control(char *buf, size_t len);
+void tcp_get_allowed_congestion_control(char *buf, size_t len);
+int tcp_set_allowed_congestion_control(char *allowed);
+int tcp_set_congestion_control(struct sock *sk, const char *name);
+void tcp_slow_start(struct tcp_sock *tp);
+void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
-extern u32 tcp_reno_ssthresh(struct sock *sk);
-extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight);
-extern u32 tcp_reno_min_cwnd(const struct sock *sk);
+u32 tcp_reno_ssthresh(struct sock *sk);
+void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight);
+u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 
 static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
@@ -936,8 +933,8 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
-extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
-extern __u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);
+void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
+__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);
 
 /* The maximum number of MSS of available cwnd for which TSO defers
  * sending if not using sysctl_tcp_tso_win_divisor.
@@ -963,7 +960,7 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 {
 	return tp->snd_una + tp->snd_wnd;
 }
-extern bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
+bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
 
 static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
 				       const struct sk_buff *skb)
@@ -1028,7 +1025,7 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 #endif
 }
 
-extern bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
+bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
 
 #undef STATE_TRACE
 
@@ -1039,9 +1036,9 @@ static const char *statename[]={
 	"Close Wait","Last ACK","Listen","Closing"
 };
 #endif
-extern void tcp_set_state(struct sock *sk, int state);
+void tcp_set_state(struct sock *sk, int state);
 
-extern void tcp_done(struct sock *sk);
+void tcp_done(struct sock *sk);
 
 static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 {
@@ -1049,13 +1046,12 @@ static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 	rx_opt->num_sacks = 0;
 }
 
-extern u32 tcp_default_init_rwnd(u32 mss);
+u32 tcp_default_init_rwnd(u32 mss);
 
 /* Determine a window scaling and initial window to offer. */
-extern void tcp_select_initial_window(int __space, __u32 mss,
-				      __u32 *rcv_wnd, __u32 *window_clamp,
-				      int wscale_ok, __u8 *rcv_wscale,
-				      __u32 init_rcv_wnd);
+void tcp_select_initial_window(int __space, __u32 mss, __u32 *rcv_wnd,
+			       __u32 *window_clamp, int wscale_ok,
+			       __u8 *rcv_wscale, __u32 init_rcv_wnd);
 
 static inline int tcp_win_from_space(int space)
 {
@@ -1099,7 +1095,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->loc_port = tcp_hdr(skb)->dest;
 }
 
-extern void tcp_enter_memory_pressure(struct sock *sk);
+void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 {
@@ -1252,21 +1248,20 @@ struct tcp_md5sig_pool {
 };
 
 /* - functions */
-extern int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
-			       const struct sock *sk,
-			       const struct request_sock *req,
-			       const struct sk_buff *skb);
-extern int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
-			  int family, const u8 *newkey,
-			  u8 newkeylen, gfp_t gfp);
-extern int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
-			  int family);
-extern struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
+int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
+			const struct sock *sk, const struct request_sock *req,
+			const struct sk_buff *skb);
+int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
+		   int family, const u8 *newkey, u8 newkeylen, gfp_t gfp);
+int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
+		   int family);
+struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
 					 struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
-extern struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
-			const union tcp_md5_addr *addr, int family);
+struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
+					 const union tcp_md5_addr *addr,
+					 int family);
 #define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_key)
 #else
 static inline struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
@@ -1278,27 +1273,26 @@ static inline struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 
-extern bool tcp_alloc_md5sig_pool(void);
+bool tcp_alloc_md5sig_pool(void);
 
-extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
+struct tcp_md5sig_pool *tcp_get_md5sig_pool(void);
 static inline void tcp_put_md5sig_pool(void)
 {
 	local_bh_enable();
 }
 
-extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
-extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
-				 unsigned int header_len);
-extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
-			    const struct tcp_md5sig_key *key);
+int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
+int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
+			  unsigned int header_len);
+int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
+		     const struct tcp_md5sig_key *key);
 
 /* From tcp_fastopen.c */
-extern void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
-				   struct tcp_fastopen_cookie *cookie,
-				   int *syn_loss, unsigned long *last_syn_loss);
-extern void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
-				   struct tcp_fastopen_cookie *cookie,
-				   bool syn_lost);
+void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
+			    struct tcp_fastopen_cookie *cookie, int *syn_loss,
+			    unsigned long *last_syn_loss);
+void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
+			    struct tcp_fastopen_cookie *cookie, bool syn_lost);
 struct tcp_fastopen_request {
 	/* Fast Open cookie. Size 0 means a cookie request */
 	struct tcp_fastopen_cookie	cookie;
@@ -1309,8 +1303,8 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
-extern void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
-				    struct tcp_fastopen_cookie *foc);
+void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
+			     struct tcp_fastopen_cookie *foc);
 
 #define TCP_FASTOPEN_KEY_LENGTH 16
 
@@ -1529,22 +1523,20 @@ struct tcp_iter_state {
 	loff_t			last_pos;
 };
 
-extern int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);
-extern void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo);
+int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);
+void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo);
 
 extern struct request_sock_ops tcp_request_sock_ops;
 extern struct request_sock_ops tcp6_request_sock_ops;
 
-extern void tcp_v4_destroy_sock(struct sock *sk);
+void tcp_v4_destroy_sock(struct sock *sk);
 
-extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb,
-				       netdev_features_t features);
-extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
-					struct sk_buff *skb);
-extern int tcp_gro_complete(struct sk_buff *skb);
+struct sk_buff *tcp_tso_segment(struct sk_buff *skb,
+				netdev_features_t features);
+struct sk_buff **tcp_gro_receive(struct sk_buff **head, struct sk_buff *skb);
+int tcp_gro_complete(struct sk_buff *skb);
 
-extern void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr,
-				__be32 daddr);
+void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr);
 
 static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
 {
@@ -1560,8 +1552,8 @@ static inline bool tcp_stream_memory_free(const struct sock *sk)
 }
 
 #ifdef CONFIG_PROC_FS
-extern int tcp4_proc_init(void);
-extern void tcp4_proc_exit(void);
+int tcp4_proc_init(void);
+void tcp4_proc_exit(void);
 #endif
 
 /* TCP af-specific functions */
@@ -1592,9 +1584,9 @@ struct tcp_request_sock_ops {
 #endif
 };
 
-extern int tcpv4_offload_init(void);
+int tcpv4_offload_init(void);
 
-extern void tcp_v4_init(void);
-extern void tcp_init(void);
+void tcp_v4_init(void);
+void tcp_init(void);
 
 #endif	/* _TCP_H */

commit c995ae2259ee36caf48bbfacf40111998dacd4af
Author: Vijay Subramanian <subramanian.vijay@gmail.com>
Date:   Tue Sep 3 12:23:22 2013 -0700

    tcp: Change return value of tcp_rcv_established()
    
    tcp_rcv_established() returns only one value namely 0. We change the return
    value to void (as suggested by David Miller).
    
    After commit 0c24604b (tcp: implement RFC 5961 4.2), we no longer send RSTs in
    response to SYNs. We can remove the check and processing on the return value of
    tcp_rcv_established().
    
    We also fix jtcp_rcv_established() in tcp_probe.c to match that of
    tcp_rcv_established().
    
    Signed-off-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6a6a88db462d..b1aa324c5e65 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -371,8 +371,8 @@ extern void tcp_delack_timer_handler(struct sock *sk);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 				 const struct tcphdr *th, unsigned int len);
-extern int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-			       const struct tcphdr *th, unsigned int len);
+extern void tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
+				const struct tcphdr *th, unsigned int len);
 extern void tcp_rcv_space_adjust(struct sock *sk);
 extern void tcp_cleanup_rbuf(struct sock *sk, int copied);
 extern int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);

commit 95bd09eb27507691520d39ee1044d6ad831c1168
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Aug 27 05:46:32 2013 -0700

    tcp: TSO packets automatic sizing
    
    After hearing many people over past years complaining against TSO being
    bursty or even buggy, we are proud to present automatic sizing of TSO
    packets.
    
    One part of the problem is that tcp_tso_should_defer() uses an heuristic
    relying on upcoming ACKS instead of a timer, but more generally, having
    big TSO packets makes little sense for low rates, as it tends to create
    micro bursts on the network, and general consensus is to reduce the
    buffering amount.
    
    This patch introduces a per socket sk_pacing_rate, that approximates
    the current sending rate, and allows us to size the TSO packets so
    that we try to send one packet every ms.
    
    This field could be set by other transports.
    
    Patch has no impact for high speed flows, where having large TSO packets
    makes sense to reach line rate.
    
    For other flows, this helps better packet scheduling and ACK clocking.
    
    This patch increases performance of TCP flows in lossy environments.
    
    A new sysctl (tcp_min_tso_segs) is added, to specify the
    minimal size of a TSO packet (default being 2).
    
    A follow-up patch will provide a new packet scheduler (FQ), using
    sk_pacing_rate as an input to perform optional per flow pacing.
    
    This explains why we chose to set sk_pacing_rate to twice the current
    rate, allowing 'slow start' ramp up.
    
    sk_pacing_rate = 2 * cwnd * mss / srtt
    
    v2: Neal Cardwell reported a suspect deferring of last two segments on
    initial write of 10 MSS, I had to change tcp_tso_should_defer() to take
    into account tp->xmit_size_goal_segs
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dd5e16f66f84..6a6a88db462d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -281,6 +281,7 @@ extern int sysctl_tcp_early_retrans;
 extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
 extern unsigned int sysctl_tcp_notsent_lowat;
+extern int sysctl_tcp_min_tso_segs;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 81eb6a1487718a89621a0e0be7fafd0cd7c429a4
Author: Patrick McHardy <kaber@trash.net>
Date:   Tue Aug 27 08:50:15 2013 +0200

    net: syncookies: export cookie_v6_init_sequence/cookie_v6_check
    
    Extract the local TCP stack independant parts of tcp_v6_init_sequence()
    and cookie_v6_check() and export them for use by the upcoming IPv6 SYNPROXY
    target.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Acked-by: David S. Miller <davem@davemloft.net>
    Tested-by: Martin Topholm <mph@one.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8e2b63678a83..dd5e16f66f84 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -499,8 +499,12 @@ extern bool cookie_check_timestamp(struct tcp_options_received *opt,
 				struct net *net, bool *ecn_ok);
 
 /* From net/ipv6/syncookies.c */
+extern int __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,
+			     u32 cookie);
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
+extern u32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,
+				     const struct tcphdr *th, u16 *mssp);
 extern __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 				     __u16 *mss);
 #else

commit 0198230b7705eb2386e53778d944e307eef0cc71
Author: Patrick McHardy <kaber@trash.net>
Date:   Tue Aug 27 08:50:13 2013 +0200

    net: syncookies: export cookie_v4_init_sequence/cookie_v4_check
    
    Extract the local TCP stack independant parts of tcp_v4_init_sequence()
    and cookie_v4_check() and export them for use by the upcoming SYNPROXY
    target.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Acked-by: David S. Miller <davem@davemloft.net>
    Tested-by: Martin Topholm <mph@one.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 09cb5c11ceea..8e2b63678a83 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -476,9 +476,13 @@ void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
+extern int __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,
+			     u32 cookie);
 extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
 				    struct ip_options *opt);
 #ifdef CONFIG_SYN_COOKIES
+extern u32 __cookie_v4_init_sequence(const struct iphdr *iph,
+				     const struct tcphdr *th, u16 *mssp);
 extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 				     __u16 *mss);
 #else

commit 149479d019e06df5a7f4096f95c00cfb1380309c
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Aug 8 14:06:22 2013 -0700

    tcp: add server ip to encrypt cookie in fast open
    
    Encrypt the cookie with both server and client IPv4 addresses,
    such that multi-homed server will grant different cookies
    based on both the source and destination IPs. No client change
    is needed since cookie is opaque to the client.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 27b652f379ef..09cb5c11ceea 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1300,7 +1300,8 @@ void tcp_free_fastopen_req(struct tcp_sock *tp);
 
 extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
 int tcp_fastopen_reset_cipher(void *key, unsigned int len);
-void tcp_fastopen_cookie_gen(__be32 addr, struct tcp_fastopen_cookie *foc);
+extern void tcp_fastopen_cookie_gen(__be32 src, __be32 dst,
+				    struct tcp_fastopen_cookie *foc);
 
 #define TCP_FASTOPEN_KEY_LENGTH 16
 

commit c0155b2da4cd6583b1b729451249ca346e1c05a2
Author: Dmitry Popov <dp@highloadlab.com>
Date:   Wed Jul 31 13:39:45 2013 +0400

    tcp: Remove unused tcpct declarations and comments
    
    Remove declaration, 4 defines and confusing comment that are no longer used
    since 1a2c6181c4 ("tcp: Remove TCPCT").
    
    Signed-off-by: Dmitry Popov <dp@highloadlab.com>
    Acked-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 18fc999dae3c..27b652f379ef 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -192,10 +192,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_TIMESTAMP      10
 #define TCPOLEN_MD5SIG         18
 #define TCPOLEN_EXP_FASTOPEN_BASE  4
-#define TCPOLEN_COOKIE_BASE    2	/* Cookie-less header extension */
-#define TCPOLEN_COOKIE_PAIR    3	/* Cookie pair header extension */
-#define TCPOLEN_COOKIE_MIN     (TCPOLEN_COOKIE_BASE+TCP_COOKIE_MIN)
-#define TCPOLEN_COOKIE_MAX     (TCPOLEN_COOKIE_BASE+TCP_COOKIE_MAX)
 
 /* But this is what stacks really send out. */
 #define TCPOLEN_TSTAMP_ALIGNED		12

commit c9bee3b7fdecb0c1d070c7b54113b3bdfb9a3d36
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 22 20:27:07 2013 -0700

    tcp: TCP_NOTSENT_LOWAT socket option
    
    Idea of this patch is to add optional limitation of number of
    unsent bytes in TCP sockets, to reduce usage of kernel memory.
    
    TCP receiver might announce a big window, and TCP sender autotuning
    might allow a large amount of bytes in write queue, but this has little
    performance impact if a large part of this buffering is wasted :
    
    Write queue needs to be large only to deal with large BDP, not
    necessarily to cope with scheduling delays (incoming ACKS make room
    for the application to queue more bytes)
    
    For most workloads, using a value of 128 KB or less is OK to give
    applications enough time to react to POLLOUT events in time
    (or being awaken in a blocking sendmsg())
    
    This patch adds two ways to set the limit :
    
    1) Per socket option TCP_NOTSENT_LOWAT
    
    2) A sysctl (/proc/sys/net/ipv4/tcp_notsent_lowat) for sockets
    not using TCP_NOTSENT_LOWAT socket option (or setting a zero value)
    Default value being UINT_MAX (0xFFFFFFFF), meaning this has no effect.
    
    This changes poll()/select()/epoll() to report POLLOUT
    only if number of unsent bytes is below tp->nosent_lowat
    
    Note this might increase number of sendmsg()/sendfile() calls
    when using non blocking sockets,
    and increase number of context switches for blocking sockets.
    
    Note this is not related to SO_SNDLOWAT (as SO_SNDLOWAT is
    defined as :
     Specify the minimum number of bytes in the buffer until
     the socket layer will pass the data to the protocol)
    
    Tested:
    
    netperf sessions, and watching /proc/net/protocols "memory" column for TCP
    
    With 200 concurrent netperf -t TCP_STREAM sessions, amount of kernel memory
    used by TCP buffers shrinks by ~55 % (20567 pages instead of 45458)
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   45458   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   45458   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   20567   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   20567   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    Using 128KB has no bad effect on the throughput or cpu usage
    of a single flow, although there is an increase of context switches.
    
    A bonus is that we hold socket lock for a shorter amount
    of time and should improve latencies of ACK processing.
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1651584     6291456     16384  20.00   17447.90   10^6bits/s  3.13  S      -1.00  U      0.353   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
               412,514 context-switches
    
         200.034645535 seconds time elapsed
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1593240     6291456     16384  20.00   17321.16   10^6bits/s  3.35  S      -1.00  U      0.381   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
             2,675,818 context-switches
    
         200.029651391 seconds time elapsed
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-By: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c5868471abae..18fc999dae3c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -284,6 +284,7 @@ extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
 extern int sysctl_tcp_limit_output_bytes;
 extern int sysctl_tcp_challenge_ack_limit;
+extern unsigned int sysctl_tcp_notsent_lowat;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -1539,6 +1540,19 @@ extern int tcp_gro_complete(struct sk_buff *skb);
 extern void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr,
 				__be32 daddr);
 
+static inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)
+{
+	return tp->notsent_lowat ?: sysctl_tcp_notsent_lowat;
+}
+
+static inline bool tcp_stream_memory_free(const struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	u32 notsent_bytes = tp->write_seq - tp->snd_nxt;
+
+	return notsent_bytes < tcp_notsent_lowat(tp);
+}
+
 #ifdef CONFIG_PROC_FS
 extern int tcp4_proc_init(void);
 extern void tcp4_proc_exit(void);

commit 5b08e47caf1f2034a3a5b566bbccc8b0be3961ca
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Jul 22 16:20:46 2013 -0700

    tcp: prefer packet timing to TS-ECR for RTT
    
    Prefer packet timings to TS-ecr for RTT measurements when both
    sources are available. That's because broken middle-boxes and remote
    peer can return packets with corrupted TS ECR fields. Similarly most
    congestion controls that require RTT signals favor timing-based
    sources as well. Also check for bad TS ECR values to avoid RTT
    blow-ups. It has happened on production Web servers.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f9777dbede75..c5868471abae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -591,7 +591,6 @@ extern void tcp_initialize_rcv_mss(struct sock *sk);
 extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
 extern int tcp_mss_to_mtu(struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
-extern void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt);
 extern void tcp_init_buffer_space(struct sock *sk);
 
 static inline void tcp_bound_rto(const struct sock *sk)

commit 375fe02c91792917aa26d68a87ab110d1937f44e
Author: Yuchung Cheng <ycheng@google.com>
Date:   Mon Jul 22 16:20:45 2013 -0700

    tcp: consolidate SYNACK RTT sampling
    
    The first patch consolidates SYNACK and other RTT measurement to use a
    central function tcp_ack_update_rtt(). A (small) bonus is now SYNACK
    RTT measurement happens after PAWS check, potentially reducing the
    impact of RTO seeding on bad TCP timestamps values.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d1980054ec75..f9777dbede75 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1094,15 +1094,6 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->loc_port = tcp_hdr(skb)->dest;
 }
 
-/* Compute time elapsed between SYNACK and the ACK completing 3WHS */
-static inline void tcp_synack_rtt_meas(struct sock *sk,
-				       struct request_sock *req)
-{
-	if (tcp_rsk(req)->snt_synack)
-		tcp_valid_rtt_meas(sk,
-		    tcp_time_stamp - tcp_rsk(req)->snt_synack);
-}
-
 extern void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)

commit 7ae8639c9d6d5aba7990b4fad3506ff7b4667a45
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 25 01:21:06 2013 -0700

    tcp: remove invalid __rcu annotation
    
    struct tcp_fastopen_context has a field named tfm, which is a pointer
    to a crypto_cipher structure.
    
    It currently has a __rcu annotation, which is not needed at all.
    
    tcp_fastopen_ctx is the pointer fetched by rcu_dereference(), but once
    we have a pointer to current tcp_fastopen_context, we do not use/need
    rcu_dereference() to access tfm.
    
    This fixes a lot of sparse errors like the following :
    
    net/ipv4/tcp_fastopen.c:21:31: warning: incorrect type in argument 1 (different address spaces)
    net/ipv4/tcp_fastopen.c:21:31:    expected struct crypto_cipher *tfm
    net/ipv4/tcp_fastopen.c:21:31:    got struct crypto_cipher [noderef] <asn:4>*tfm
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6fa80831dc40..d1980054ec75 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1319,9 +1319,9 @@ void tcp_fastopen_cookie_gen(__be32 addr, struct tcp_fastopen_cookie *foc);
 
 /* Fastopen key context */
 struct tcp_fastopen_context {
-	struct crypto_cipher __rcu	*tfm;
-	__u8				key[TCP_FASTOPEN_KEY_LENGTH];
-	struct rcu_head			rcu;
+	struct crypto_cipher	*tfm;
+	__u8			key[TCP_FASTOPEN_KEY_LENGTH];
+	struct rcu_head		rcu;
 };
 
 /* write queue abstraction */

commit 85f16525a2eb66e6092cbd8dcf42371df8334ed0
Author: Yuchung Cheng <ycheng@google.com>
Date:   Tue Jun 11 15:35:32 2013 -0700

    tcp: properly send new data in fast recovery in first RTT
    
    Linux sends new unset data during disorder and recovery state if all
    (suspected) lost packets have been retransmitted ( RFC5681, section
    3.2 step 1 & 2, RFC3517 section 4, NexSeg() Rule 2).  One requirement
    is to keep the receive window about twice the estimated sender's
    congestion window (tcp_rcv_space_adjust()), assuming the fast
    retransmits repair the losses in the next round trip.
    
    But currently it's not the case on the first round trip in either
    normal or Fast Open connection, beucase the initial receive window
    is identical to (expected) sender's initial congestion window. The
    fix is to double it.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0d637e9403a5..6fa80831dc40 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -61,9 +61,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
  */
 #define MAX_TCP_WINDOW		32767U
 
-/* Offer an initial receive window of 10 mss. */
-#define TCP_DEFAULT_INIT_RCVWND	10
-
 /* Minimal accepted MSS. It is (60+60+8) - (20+20). */
 #define TCP_MIN_MSS		88U
 
@@ -1047,6 +1044,8 @@ static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 	rx_opt->num_sacks = 0;
 }
 
+extern u32 tcp_default_init_rwnd(u32 mss);
+
 /* Determine a window scaling and initial window to offer. */
 extern void tcp_select_initial_window(int __space, __u32 mss,
 				      __u32 *rcv_wnd, __u32 *window_clamp,

commit 28850dc7c71da9d0c0e39246e9ff6913f41f8d0a
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Jun 7 05:11:46 2013 +0000

    net: tcp: move GRO/GSO functions to tcp_offload
    
    Would be good to make things explicit and move those functions to
    a new file called tcp_offload.c, thus make this similar to tcpv6_offload.c.
    While moving all related functions into tcp_offload.c, we can also
    make some of them static, since they are only used there. Also, add
    an explicit registration function.
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bf1cc3dced5e..0d637e9403a5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1541,15 +1541,14 @@ extern struct request_sock_ops tcp6_request_sock_ops;
 
 extern void tcp_v4_destroy_sock(struct sock *sk);
 
-extern int tcp_v4_gso_send_check(struct sk_buff *skb);
 extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb,
 				       netdev_features_t features);
 extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
 					struct sk_buff *skb);
-extern struct sk_buff **tcp4_gro_receive(struct sk_buff **head,
-					 struct sk_buff *skb);
 extern int tcp_gro_complete(struct sk_buff *skb);
-extern int tcp4_gro_complete(struct sk_buff *skb);
+
+extern void __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr,
+				__be32 daddr);
 
 #ifdef CONFIG_PROC_FS
 extern int tcp4_proc_init(void);
@@ -1584,6 +1583,8 @@ struct tcp_request_sock_ops {
 #endif
 };
 
+extern int tcpv4_offload_init(void);
+
 extern void tcp_v4_init(void);
 extern void tcp_init(void);
 

commit 71cea17ed39fdf1c0634f530ddc6a2c2fc601c2b
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon May 20 06:52:26 2013 +0000

    tcp: md5: remove spinlock usage in fast path
    
    TCP md5 code uses per cpu variables but protects access to them with
    a shared spinlock, which is a contention point.
    
    [ tcp_md5sig_pool_lock is locked twice per incoming packet ]
    
    Makes things much simpler, by allocating crypto structures once, first
    time a socket needs md5 keys, and not deallocating them as they are
    really small.
    
    Next step would be to allow crypto allocations being done in a NUMA
    aware way.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e1c3723f161f..bf1cc3dced5e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1283,11 +1283,13 @@ static inline struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 
-extern struct tcp_md5sig_pool __percpu *tcp_alloc_md5sig_pool(struct sock *);
-extern void tcp_free_md5sig_pool(void);
+extern bool tcp_alloc_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
-extern void tcp_put_md5sig_pool(void);
+static inline void tcp_put_md5sig_pool(void)
+{
+	local_bh_enable();
+}
 
 extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
 extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,

commit 3e59cb0ddfd2c59991f38e89352ad8a3c71b2374
Author: Yuchung Cheng <ycheng@google.com>
Date:   Fri May 17 13:45:05 2013 +0000

    tcp: remove bad timeout logic in fast recovery
    
    tcp_timeout_skb() was intended to trigger fast recovery on timeout,
    unfortunately in reality it often causes spurious retransmission
    storms during fast recovery. The particular sign is a fast retransmit
    over the highest sacked sequence (SND.FACK).
    
    Currently the RTO timer re-arming (as in RFC6298) offers a nice cushion
    to avoid spurious timeout: when SND.UNA advances the sender re-arms
    RTO and extends the timeout by icsk_rto. The sender does not offset
    the time elapsed since the packet at SND.UNA was sent.
    
    But if the next (DUP)ACK arrives later than ~RTTVAR and triggers
    tcp_fastretrans_alert(), then tcp_timeout_skb() will mark any packet
    sent before the icsk_rto interval lost, including one that's above the
    highest sacked sequence. Most likely a large part of scorebard will be
    marked.
    
    If most packets are not lost then the subsequent DUPACKs with new SACK
    blocks will cause the sender to continue to retransmit packets beyond
    SND.FACK spuriously. Even if only one packet is lost the sender may
    falsely retransmit almost the entire window.
    
    The situation becomes common in the world of bufferbloat: the RTT
    continues to grow as the queue builds up but RTTVAR remains small and
    close to the minimum 200ms. If a data packet is lost and the DUPACK
    triggered by the next data packet is slightly delayed, then a spurious
    retransmission storm forms.
    
    As the original comment on tcp_timeout_skb() suggests: the usefulness
    of this feature is questionable. It also wastes cycles walking the
    sack scoreboard and is actually harmful because of false recovery.
    
    It's time to remove this.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5bba80fbd1d9..e1c3723f161f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1193,7 +1193,6 @@ static inline void tcp_mib_init(struct net *net)
 static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
 {
 	tp->lost_skb_hint = NULL;
-	tp->scoreboard_skb_hint = NULL;
 }
 
 static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)

commit d6a4a10411764cf1c3a5dad4f06c5ebe5194488b
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 12 11:31:52 2013 +0000

    tcp: GSO should be TSQ friendly
    
    I noticed that TSQ (TCP Small queues) was less effective when TSO is
    turned off, and GSO is on. If BQL is not enabled, TSQ has then no
    effect.
    
    It turns out the GSO engine frees the original gso_skb at the time the
    fragments are generated and queued to the NIC.
    
    We should instead call the tcp_wfree() destructor for the last fragment,
    to keep the flow control as intended in TSQ. This effectively limits
    the number of queued packets on qdisc + NIC layers.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4475aaf0af57..5bba80fbd1d9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -370,6 +370,7 @@ extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
 			size_t size, int flags);
 extern void tcp_release_cb(struct sock *sk);
+extern void tcp_wfree(struct sk_buff *skb);
 extern void tcp_write_timer_handler(struct sock *sk);
 extern void tcp_delack_timer_handler(struct sock *sk);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);

commit 8466563e16d5198b6efeb3b51791b95b6aaacb6b
Author: Neal Cardwell <ncardwell@google.com>
Date:   Tue Apr 2 13:13:07 2013 -0400

    tcp: Remove dead sysctl_tcp_cookie_size declaration
    
    Remove a declaration left over from the TCPCT-ectomy. This sysctl is
    no longer referenced anywhere since 1a2c6181c4 ("tcp: Remove TCPCT").
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d1dcb596230e..4475aaf0af57 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -282,7 +282,6 @@ extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_max_ssthresh;
-extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;

commit 9b44190dc114c1720b34975b5bfc65aece112ced
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Mar 20 13:32:58 2013 +0000

    tcp: refactor F-RTO
    
    The patch series refactor the F-RTO feature (RFC4138/5682).
    
    This is to simplify the loss recovery processing. Existing F-RTO
    was developed during the experimental stage (RFC4138) and has
    many experimental features.  It takes a separate code path from
    the traditional timeout processing by overloading CA_Disorder
    instead of using CA_Loss state. This complicates CA_Disorder state
    handling because it's also used for handling dubious ACKs and undos.
    While the algorithm in the RFC does not change the congestion control,
    the implementation intercepts congestion control in various places
    (e.g., frto_cwnd in tcp_ack()).
    
    The new code implements newer F-RTO RFC5682 using CA_Loss processing
    path.  F-RTO becomes a small extension in the timeout processing
    and interfaces with congestion control and Eifel undo modules.
    It lets congestion control (module) determines how many to send
    independently.  F-RTO only chooses what to send in order to detect
    spurious retranmission. If timeout is found spurious it invokes
    existing Eifel undo algorithms like DSACK or TCP timestamp based
    detection.
    
    The first patch removes all F-RTO code except the sysctl_tcp_frto is
    left for the new implementation.  Since CA_EVENT_FRTO is removed, TCP
    westwood now computes ssthresh on regular timeout CA_EVENT_LOSS event.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7f2f17198d75..d1dcb596230e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -272,7 +272,6 @@ extern int sysctl_tcp_app_win;
 extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
-extern int sysctl_tcp_frto_response;
 extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_dma_copybreak;
 extern int sysctl_tcp_nometrics_save;
@@ -424,8 +423,6 @@ extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
 				   bool fastopen);
 extern int tcp_child_process(struct sock *parent, struct sock *child,
 			     struct sk_buff *skb);
-extern bool tcp_use_frto(struct sock *sk);
-extern void tcp_enter_frto(struct sock *sk);
 extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
@@ -756,7 +753,6 @@ enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */
 	CA_EVENT_CWND_RESTART,	/* congestion window restart */
 	CA_EVENT_COMPLETE_CWR,	/* end of congestion recovery */
-	CA_EVENT_FRTO,		/* fast recovery timeout */
 	CA_EVENT_LOSS,		/* loss timeout */
 	CA_EVENT_FAST_ACK,	/* in sequence ack */
 	CA_EVENT_SLOW_ACK,	/* other ack */

commit 1a2c6181c4a1922021b4d7df373bba612c3e5f04
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Sun Mar 17 08:23:34 2013 +0000

    tcp: Remove TCPCT
    
    TCPCT uses option-number 253, reserved for experimental use and should
    not be used in production environments.
    Further, TCPCT does not fully implement RFC 6013.
    
    As a nice side-effect, removing TCPCT increases TCP's performance for
    very short flows:
    
    Doing an apache-benchmark with -c 100 -n 100000, sending HTTP-requests
    for files of 1KB size.
    
    before this patch:
            average (among 7 runs) of 20845.5 Requests/Second
    after:
            average (among 7 runs) of 21403.6 Requests/Second
    
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ab9f947b118b..7f2f17198d75 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -179,7 +179,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_SACK             5       /* SACK Block */
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
-#define TCPOPT_COOKIE		253	/* Cookie extension (experimental) */
 #define TCPOPT_EXP		254	/* Experimental */
 /* Magic number to be after the option value for sharing TCP
  * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
@@ -454,7 +453,7 @@ extern void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
 extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t len, int nonblock, int flags, int *addr_len);
 extern void tcp_parse_options(const struct sk_buff *skb,
-			      struct tcp_options_received *opt_rx, const u8 **hvpp,
+			      struct tcp_options_received *opt_rx,
 			      int estab, struct tcp_fastopen_cookie *foc);
 extern const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
@@ -476,7 +475,6 @@ extern int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr,
 extern int tcp_connect(struct sock *sk);
 extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 					struct request_sock *req,
-					struct request_values *rvp,
 					struct tcp_fastopen_cookie *foc);
 extern int tcp_disconnect(struct sock *sk, int flags);
 
@@ -1589,91 +1587,6 @@ struct tcp_request_sock_ops {
 #endif
 };
 
-/* Using SHA1 for now, define some constants.
- */
-#define COOKIE_DIGEST_WORDS (SHA_DIGEST_WORDS)
-#define COOKIE_MESSAGE_WORDS (SHA_MESSAGE_BYTES / 4)
-#define COOKIE_WORKSPACE_WORDS (COOKIE_DIGEST_WORDS + COOKIE_MESSAGE_WORDS)
-
-extern int tcp_cookie_generator(u32 *bakery);
-
-/**
- *	struct tcp_cookie_values - each socket needs extra space for the
- *	cookies, together with (optional) space for any SYN data.
- *
- *	A tcp_sock contains a pointer to the current value, and this is
- *	cloned to the tcp_timewait_sock.
- *
- * @cookie_pair:	variable data from the option exchange.
- *
- * @cookie_desired:	user specified tcpct_cookie_desired.  Zero
- *			indicates default (sysctl_tcp_cookie_size).
- *			After cookie sent, remembers size of cookie.
- *			Range 0, TCP_COOKIE_MIN to TCP_COOKIE_MAX.
- *
- * @s_data_desired:	user specified tcpct_s_data_desired.  When the
- *			constant payload is specified (@s_data_constant),
- *			holds its length instead.
- *			Range 0 to TCP_MSS_DESIRED.
- *
- * @s_data_payload:	constant data that is to be included in the
- *			payload of SYN or SYNACK segments when the
- *			cookie option is present.
- */
-struct tcp_cookie_values {
-	struct kref	kref;
-	u8		cookie_pair[TCP_COOKIE_PAIR_SIZE];
-	u8		cookie_pair_size;
-	u8		cookie_desired;
-	u16		s_data_desired:11,
-			s_data_constant:1,
-			s_data_in:1,
-			s_data_out:1,
-			s_data_unused:2;
-	u8		s_data_payload[0];
-};
-
-static inline void tcp_cookie_values_release(struct kref *kref)
-{
-	kfree(container_of(kref, struct tcp_cookie_values, kref));
-}
-
-/* The length of constant payload data.  Note that s_data_desired is
- * overloaded, depending on s_data_constant: either the length of constant
- * data (returned here) or the limit on variable data.
- */
-static inline int tcp_s_data_size(const struct tcp_sock *tp)
-{
-	return (tp->cookie_values != NULL && tp->cookie_values->s_data_constant)
-		? tp->cookie_values->s_data_desired
-		: 0;
-}
-
-/**
- *	struct tcp_extend_values - tcp_ipv?.c to tcp_output.c workspace.
- *
- *	As tcp_request_sock has already been extended in other places, the
- *	only remaining method is to pass stack values along as function
- *	parameters.  These parameters are not needed after sending SYNACK.
- *
- * @cookie_bakery:	cryptographic secret and message workspace.
- *
- * @cookie_plus:	bytes in authenticator/cookie option, copied from
- *			struct tcp_options_received (above).
- */
-struct tcp_extend_values {
-	struct request_values		rv;
-	u32				cookie_bakery[COOKIE_WORKSPACE_WORDS];
-	u8				cookie_plus:6,
-					cookie_out_never:1,
-					cookie_in_always:1;
-};
-
-static inline struct tcp_extend_values *tcp_xv(struct request_values *rvp)
-{
-	return (struct tcp_extend_values *)rvp;
-}
-
 extern void tcp_v4_init(void);
 extern void tcp_init(void);
 

commit 6ba8a3b19e764b6a65e4030ab0999be50c291e6c
Author: Nandita Dukkipati <nanditad@google.com>
Date:   Mon Mar 11 10:00:43 2013 +0000

    tcp: Tail loss probe (TLP)
    
    This patch series implement the Tail loss probe (TLP) algorithm described
    in http://tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01. The
    first patch implements the basic algorithm.
    
    TLP's goal is to reduce tail latency of short transactions. It achieves
    this by converting retransmission timeouts (RTOs) occuring due
    to tail losses (losses at end of transactions) into fast recovery.
    TLP transmits one packet in two round-trips when a connection is in
    Open state and isn't receiving any ACKs. The transmitted packet, aka
    loss probe, can be either new or a retransmission. When there is tail
    loss, the ACK from a loss probe triggers FACK/early-retransmit based
    fast recovery, thus avoiding a costly RTO. In the absence of loss,
    there is no change in the connection state.
    
    PTO stands for probe timeout. It is a timer event indicating
    that an ACK is overdue and triggers a loss probe packet. The PTO value
    is set to max(2*SRTT, 10ms) and is adjusted to account for delayed
    ACK timer when there is only one oustanding packet.
    
    TLP Algorithm
    
    On transmission of new data in Open state:
      -> packets_out > 1: schedule PTO in max(2*SRTT, 10ms).
      -> packets_out == 1: schedule PTO in max(2*RTT, 1.5*RTT + 200ms)
      -> PTO = min(PTO, RTO)
    
    Conditions for scheduling PTO:
      -> Connection is in Open state.
      -> Connection is either cwnd limited or no new data to send.
      -> Number of probes per tail loss episode is limited to one.
      -> Connection is SACK enabled.
    
    When PTO fires:
      new_segment_exists:
        -> transmit new segment.
        -> packets_out++. cwnd remains same.
    
      no_new_packet:
        -> retransmit the last segment.
           Its ACK triggers FACK or early retransmit based recovery.
    
    ACK path:
      -> rearm RTO at start of ACK processing.
      -> reschedule PTO if need be.
    
    In addition, the patch includes a small variation to the Early Retransmit
    (ER) algorithm, such that ER and TLP together can in principle recover any
    N-degree of tail loss through fast recovery. TLP is controlled by the same
    sysctl as ER, tcp_early_retrans sysctl.
    tcp_early_retrans==0; disables TLP and ER.
                     ==1; enables RFC5827 ER.
                     ==2; delayed ER.
                     ==3; TLP and delayed ER. [DEFAULT]
                     ==4; TLP only.
    
    The TLP patch series have been extensively tested on Google Web servers.
    It is most effective for short Web trasactions, where it reduced RTOs by 15%
    and improved HTTP response time (average by 6%, 99th percentile by 10%).
    The transmitted probes account for <0.5% of the overall transmissions.
    
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a2baa5e4ba31..ab9f947b118b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -543,6 +543,8 @@ extern bool tcp_syn_flood_action(struct sock *sk,
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);
+extern void tcp_send_loss_probe(struct sock *sk);
+extern bool tcp_schedule_loss_probe(struct sock *sk);
 
 /* tcp_input.c */
 extern void tcp_cwnd_application_limited(struct sock *sk);
@@ -873,8 +875,8 @@ static inline void tcp_enable_fack(struct tcp_sock *tp)
 static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
 {
 	tp->do_early_retrans = sysctl_tcp_early_retrans &&
-		!sysctl_tcp_thin_dupack && sysctl_tcp_reordering == 3;
-	tp->early_retrans_delayed = 0;
+		sysctl_tcp_early_retrans < 4 && !sysctl_tcp_thin_dupack &&
+		sysctl_tcp_reordering == 3;
 }
 
 static inline void tcp_disable_early_retrans(struct tcp_sock *tp)

commit b2fb4f54ecd47c42413d54b4666b06cf93c05abf
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 6 12:58:01 2013 +0000

    tcp: uninline tcp_prequeue()
    
    tcp_prequeue() became too big to be inlined.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cf0694d4ad60..a2baa5e4ba31 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1030,50 +1030,7 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 #endif
 }
 
-/* Packet is added to VJ-style prequeue for processing in process
- * context, if a reader task is waiting. Apparently, this exciting
- * idea (VJ's mail "Re: query about TCP header on tcp-ip" of 07 Sep 93)
- * failed somewhere. Latency? Burstiness? Well, at least now we will
- * see, why it failed. 8)8)				  --ANK
- *
- * NOTE: is this not too big to inline?
- */
-static inline bool tcp_prequeue(struct sock *sk, struct sk_buff *skb)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	if (sysctl_tcp_low_latency || !tp->ucopy.task)
-		return false;
-
-	if (skb->len <= tcp_hdrlen(skb) &&
-	    skb_queue_len(&tp->ucopy.prequeue) == 0)
-		return false;
-
-	__skb_queue_tail(&tp->ucopy.prequeue, skb);
-	tp->ucopy.memory += skb->truesize;
-	if (tp->ucopy.memory > sk->sk_rcvbuf) {
-		struct sk_buff *skb1;
-
-		BUG_ON(sock_owned_by_user(sk));
-
-		while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
-			sk_backlog_rcv(sk, skb1);
-			NET_INC_STATS_BH(sock_net(sk),
-					 LINUX_MIB_TCPPREQUEUEDROPPED);
-		}
-
-		tp->ucopy.memory = 0;
-	} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
-		wake_up_interruptible_sync_poll(sk_sleep(sk),
-					   POLLIN | POLLRDNORM | POLLRDBAND);
-		if (!inet_csk_ack_scheduled(sk))
-			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
-						  (3 * tcp_rto_min(sk)) / 4,
-						  TCP_RTO_MAX);
-	}
-	return true;
-}
-
+extern bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
 
 #undef STATE_TRACE
 

commit 79ffef1fe213851f44bfccf037170a140e929f85
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Feb 27 07:05:03 2013 +0000

    tcp: avoid wakeups for pure ACK
    
    TCP prequeue mechanism purpose is to let incoming packets
    being processed by the thread currently blocked in tcp_recvmsg(),
    instead of behalf of the softirq handler, to better adapt flow
    control on receiver host capacity to schedule the consumer.
    
    But in typical request/answer workloads, we send request, then
    block to receive the answer. And before the actual answer, TCP
    stack receives the ACK packets acknowledging the request.
    
    Processing pure ACK on behalf of the thread blocked in tcp_recvmsg()
    is a waste of resources, as thread has to immediately sleep again
    because it got no payload.
    
    This patch avoids the extra context switches and scheduler overhead.
    
    Before patch :
    
    a:~# echo 0 >/proc/sys/net/ipv4/tcp_low_latency
    a:~# perf stat ./super_netperf 300 -t TCP_RR -l 10 -H 7.7.7.84 -- -r 8k,8k
    231676
    
     Performance counter stats for './super_netperf 300 -t TCP_RR -l 10 -H 7.7.7.84 -- -r 8k,8k':
    
         116251.501765 task-clock                #   11.369 CPUs utilized
             5,025,463 context-switches          #    0.043 M/sec
             1,074,511 CPU-migrations            #    0.009 M/sec
               216,923 page-faults               #    0.002 M/sec
       311,636,972,396 cycles                    #    2.681 GHz
       260,507,138,069 stalled-cycles-frontend   #   83.59% frontend cycles idle
       155,590,092,840 stalled-cycles-backend    #   49.93% backend  cycles idle
       100,101,255,411 instructions              #    0.32  insns per cycle
                                                 #    2.60  stalled cycles per insn
        16,535,930,999 branches                  #  142.243 M/sec
           646,483,591 branch-misses             #    3.91% of all branches
    
          10.225482774 seconds time elapsed
    
    After patch :
    
    a:~# echo 0 >/proc/sys/net/ipv4/tcp_low_latency
    a:~# perf stat ./super_netperf 300 -t TCP_RR -l 10 -H 7.7.7.84 -- -r 8k,8k
    233297
    
     Performance counter stats for './super_netperf 300 -t TCP_RR -l 10 -H 7.7.7.84 -- -r 8k,8k':
    
          91084.870855 task-clock                #    8.887 CPUs utilized
             2,485,916 context-switches          #    0.027 M/sec
               815,520 CPU-migrations            #    0.009 M/sec
               216,932 page-faults               #    0.002 M/sec
       245,195,022,629 cycles                    #    2.692 GHz
       202,635,777,041 stalled-cycles-frontend   #   82.64% frontend cycles idle
       124,280,372,407 stalled-cycles-backend    #   50.69% backend  cycles idle
        83,457,289,618 instructions              #    0.34  insns per cycle
                                                 #    2.43  stalled cycles per insn
        13,431,472,361 branches                  #  147.461 M/sec
           504,470,665 branch-misses             #    3.76% of all branches
    
          10.249594448 seconds time elapsed
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 23f2e98d4b65..cf0694d4ad60 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1045,6 +1045,10 @@ static inline bool tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 	if (sysctl_tcp_low_latency || !tp->ucopy.task)
 		return false;
 
+	if (skb->len <= tcp_hdrlen(skb) &&
+	    skb_queue_len(&tp->ucopy.prequeue) == 0)
+		return false;
+
 	__skb_queue_tail(&tp->ucopy.prequeue, skb);
 	tp->ucopy.memory += skb->truesize;
 	if (tp->ucopy.memory > sk->sk_rcvbuf) {

commit ca2eb5679f8ddffff60156af42595df44a315ef0
Author: Stephen Hemminger <stephen@networkplumber.org>
Date:   Tue Feb 5 07:25:17 2013 +0000

    tcp: remove Appropriate Byte Count support
    
    TCP Appropriate Byte Count was added by me, but later disabled.
    There is no point in maintaining it since it is a potential source
    of bugs and Linux already implements other better window protection
    heuristics.
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 614af8b7758e..23f2e98d4b65 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -279,7 +279,6 @@ extern int sysctl_tcp_dma_copybreak;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
-extern int sysctl_tcp_abc;
 extern int sysctl_tcp_mtu_probing;
 extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;

commit 5d134f1c1f36166e8a738de92c4d2f4c262ff91b
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Sat Jan 5 16:10:48 2013 +0000

    tcp: make sysctl_tcp_ecn namespace aware
    
    As per suggestion from Eric Dumazet this patch makes tcp_ecn sysctl
    namespace aware.  The reason behind this patch is to ease the testing
    of ecn problems on the internet and allows applications to tune their
    own use of ecn.
    
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index aed42c785153..614af8b7758e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -266,7 +266,6 @@ extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;
 extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
-extern int sysctl_tcp_ecn;
 extern int sysctl_tcp_dsack;
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
@@ -504,7 +503,8 @@ static inline __u32 cookie_v4_init_sequence(struct sock *sk,
 #endif
 
 extern __u32 cookie_init_timestamp(struct request_sock *req);
-extern bool cookie_check_timestamp(struct tcp_options_received *opt, bool *);
+extern bool cookie_check_timestamp(struct tcp_options_received *opt,
+				struct net *net, bool *ecn_ok);
 
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
@@ -728,11 +728,12 @@ struct tcp_skb_cb {
  * notifications, we disable TCP ECN negociation.
  */
 static inline void
-TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb)
+TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb,
+		struct net *net)
 {
 	const struct tcphdr *th = tcp_hdr(skb);
 
-	if (sysctl_tcp_ecn && th->ece && th->cwr &&
+	if (net->ipv4.sysctl_tcp_ecn && th->ece && th->cwr &&
 	    INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield))
 		inet_rsk(req)->ecn_ok = 1;
 }

commit 6be35c700f742e911ecedd07fcc43d4439922334
Merge: e37aa63e87bd 520dfe3a3645
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 18:07:07 2012 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking changes from David Miller:
    
    1) Allow to dump, monitor, and change the bridge multicast database
       using netlink.  From Cong Wang.
    
    2) RFC 5961 TCP blind data injection attack mitigation, from Eric
       Dumazet.
    
    3) Networking user namespace support from Eric W. Biederman.
    
    4) tuntap/virtio-net multiqueue support by Jason Wang.
    
    5) Support for checksum offload of encapsulated packets (basically,
       tunneled traffic can still be checksummed by HW).  From Joseph
       Gasparakis.
    
    6) Allow BPF filter access to VLAN tags, from Eric Dumazet and
       Daniel Borkmann.
    
    7) Bridge port parameters over netlink and BPDU blocking support
       from Stephen Hemminger.
    
    8) Improve data access patterns during inet socket demux by rearranging
       socket layout, from Eric Dumazet.
    
    9) TIPC protocol updates and cleanups from Ying Xue, Paul Gortmaker, and
       Jon Maloy.
    
    10) Update TCP socket hash sizing to be more in line with current day
        realities.  The existing heurstics were choosen a decade ago.
        From Eric Dumazet.
    
    11) Fix races, queue bloat, and excessive wakeups in ATM and
        associated drivers, from Krzysztof Mazur and David Woodhouse.
    
    12) Support DOVE (Distributed Overlay Virtual Ethernet) extensions
        in VXLAN driver, from David Stevens.
    
    13) Add "oops_only" mode to netconsole, from Amerigo Wang.
    
    14) Support set and query of VEB/VEPA bridge mode via PF_BRIDGE, also
        allow DCB netlink to work on namespaces other than the initial
        namespace.  From John Fastabend.
    
    15) Support PTP in the Tigon3 driver, from Matt Carlson.
    
    16) tun/vhost zero copy fixes and improvements, plus turn it on
        by default, from Michael S. Tsirkin.
    
    17) Support per-association statistics in SCTP, from Michele
        Baldessari.
    
    And many, many, driver updates, cleanups, and improvements.  Too
    numerous to mention individually.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1722 commits)
      net/mlx4_en: Add support for destination MAC in steering rules
      net/mlx4_en: Use generic etherdevice.h functions.
      net: ethtool: Add destination MAC address to flow steering API
      bridge: add support of adding and deleting mdb entries
      bridge: notify mdb changes via netlink
      ndisc: Unexport ndisc_{build,send}_skb().
      uapi: add missing netconf.h to export list
      pkt_sched: avoid requeues if possible
      solos-pci: fix double-free of TX skb in DMA mode
      bnx2: Fix accidental reversions.
      bna: Driver Version Updated to 3.1.2.1
      bna: Firmware update
      bna: Add RX State
      bna: Rx Page Based Allocation
      bna: TX Intr Coalescing Fix
      bna: Tx and Rx Optimizations
      bna: Code Cleanup and Enhancements
      ath9k: check pdata variable before dereferencing it
      ath5k: RX timestamp is reported at end of frame
      ath9k_htc: RX timestamp is reported at end of frame
      ...

commit 93b174ad71b08e504c2cf6e8a58ecce778b77a40
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Dec 6 08:45:32 2012 +0000

    tcp: bug fix Fast Open client retransmission
    
    If SYN-ACK partially acks SYN-data, the client retransmits the
    remaining data by tcp_retransmit_skb(). This increments lost recovery
    state variables like tp->retrans_out in Open state. If loss recovery
    happens before the retransmission is acked, it triggers the WARN_ON
    check in tcp_fastretrans_alert(). For example: the client sends
    SYN-data, gets SYN-ACK acking only ISN, retransmits data, sends
    another 4 data packets and get 3 dupacks.
    
    Since the retransmission is not caused by network drop it should not
    update the recovery state variables. Further the server may return a
    smaller MSS than the cached MSS used for SYN-data, so the retranmission
    needs a loop. Otherwise some data will not be retransmitted until timeout
    or other loss recovery events.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6feeccd83dd7..4af45e33105d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -525,6 +525,7 @@ static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 				      int nonagle);
 extern bool tcp_may_send_now(struct sock *sk);
+extern int __tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_retransmit_timer(struct sock *sk);
 extern void tcp_xmit_retransmit_queue(struct sock *);

commit 8d8be8389f1818b07505bbf2ec3fb2d556d84496
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu Nov 22 17:48:59 2012 +0000

    tcp: remove dead prototype for tcp_v4_get_peer()
    
    This function no longer exists.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6feeccd83dd7..3202bde2a741 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -369,7 +369,6 @@ extern void tcp_shutdown (struct sock *sk, int how);
 extern void tcp_v4_early_demux(struct sk_buff *skb);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
-extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t size);

commit 016818d076871c4ee34db1e8d74dc17ac1de626a
Author: Neal Cardwell <ncardwell@google.com>
Date:   Sat Sep 22 04:18:55 2012 +0000

    tcp: TCP Fast Open Server - take SYNACK RTT after completing 3WHS
    
    When taking SYNACK RTT samples for servers using TCP Fast Open, fix
    the code to ensure that we only call tcp_valid_rtt_meas() after we
    receive the ACK that completes the 3-way handshake.
    
    Previously we were always taking an RTT sample in
    tcp_v4_syn_recv_sock(). However, for TCP Fast Open connections
    tcp_v4_conn_req_fastopen() calls tcp_v4_syn_recv_sock() at the time we
    receive the SYN. So for TFO we must wait until tcp_rcv_state_process()
    to take the RTT sample.
    
    To fix this, we wait until after TFO calls tcp_v4_syn_recv_sock()
    before we set the snt_synack timestamp, since tcp_synack_rtt_meas()
    already ensures that we only take a SYNACK RTT sample if snt_synack is
    non-zero. To be careful, we only take a snt_synack timestamp when
    a SYNACK transmit or retransmit succeeds.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a718d0e3d8e7..6feeccd83dd7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1125,6 +1125,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	req->cookie_ts = 0;
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
+	tcp_rsk(req)->snt_synack = 0;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
 	ireq->tstamp_ok = rx_opt->tstamp_ok;

commit 623df484a777f3c00c1ea3d6a7565b8d8ac688a1
Author: Neal Cardwell <ncardwell@google.com>
Date:   Sat Sep 22 04:18:54 2012 +0000

    tcp: extract code to compute SYNACK RTT
    
    In preparation for adding another spot where we compute the SYNACK
    RTT, extract this code so that it can be shared.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a8cb00c0c6d9..a718d0e3d8e7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1137,6 +1137,15 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->loc_port = tcp_hdr(skb)->dest;
 }
 
+/* Compute time elapsed between SYNACK and the ACK completing 3WHS */
+static inline void tcp_synack_rtt_meas(struct sock *sk,
+				       struct request_sock *req)
+{
+	if (tcp_rsk(req)->snt_synack)
+		tcp_valid_rtt_meas(sk,
+		    tcp_time_stamp - tcp_rsk(req)->snt_synack);
+}
+
 extern void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)

commit 684bad1107571d35610a674c61b3544efb5a5b13
Author: Yuchung Cheng <ycheng@google.com>
Date:   Sun Sep 2 17:38:04 2012 +0000

    tcp: use PRR to reduce cwin in CWR state
    
    Use proportional rate reduction (PRR) algorithm to reduce cwnd in CWR state,
    in addition to Recovery state. Retire the current rate-halving in CWR.
    When losses are detected via ACKs in CWR state, the sender enters Recovery
    state but the cwnd reduction continues and does not restart.
    
    Rename and refactor cwnd reduction functions since both CWR and Recovery
    use the same algorithm:
    tcp_init_cwnd_reduction() is new and initiates reduction state variables.
    tcp_cwnd_reduction() is previously tcp_update_cwnd_in_recovery().
    tcp_ends_cwnd_reduction() is previously  tcp_complete_cwr().
    
    The rate halving functions and logic such as tcp_cwnd_down(), tcp_min_cwnd(),
    and the cwnd moderation inside tcp_enter_cwr() are removed. The unused
    parameter, flag, in tcp_cwnd_reduction() is also removed.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1421b02a7905..a8cb00c0c6d9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -913,15 +913,21 @@ static inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)
 	return tp->snd_ssthresh >= TCP_INFINITE_SSTHRESH;
 }
 
+static inline bool tcp_in_cwnd_reduction(const struct sock *sk)
+{
+	return (TCPF_CA_CWR | TCPF_CA_Recovery) &
+	       (1 << inet_csk(sk)->icsk_ca_state);
+}
+
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
- * The exception is rate halving phase, when cwnd is decreasing towards
+ * The exception is cwnd reduction phase, when cwnd is decreasing towards
  * ssthresh.
  */
 static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
 
-	if ((1 << inet_csk(sk)->icsk_ca_state) & (TCPF_CA_CWR | TCPF_CA_Recovery))
+	if (tcp_in_cwnd_reduction(sk))
 		return tp->snd_ssthresh;
 	else
 		return max(tp->snd_ssthresh,

commit 8336886f786fdacbc19b719c1f7ea91eb70706d4
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 31 12:29:12 2012 +0000

    tcp: TCP Fast Open Server - support TFO listeners
    
    This patch builds on top of the previous patch to add the support
    for TFO listeners. This includes -
    
    1. allocating, properly initializing, and managing the per listener
    fastopen_queue structure when TFO is enabled
    
    2. changes to the inet_csk_accept code to support TFO. E.g., the
    request_sock can no longer be freed upon accept(), not until 3WHS
    finishes
    
    3. allowing a TCP_SYN_RECV socket to properly poll() and sendmsg()
    if it's a TFO socket
    
    4. properly closing a TFO listener, and a TFO socket before 3WHS
    finishes
    
    5. supporting TCP_FASTOPEN socket option
    
    6. modifying tcp_check_req() to use to check a TFO socket as well
    as request_sock
    
    7. supporting TCP's TFO cookie option
    
    8. adding a new SYN-ACK retransmit handler to use the timer directly
    off the TFO socket rather than the listener socket. Note that TFO
    server side will not retransmit anything other than SYN-ACK until
    the 3WHS is completed.
    
    The patch also contains an important function
    "reqsk_fastopen_remove()" to manage the somewhat complex relation
    between a listener, its request_sock, and the corresponding child
    socket. See the comment above the function for the detail.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9f8821e3293a..1421b02a7905 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -424,7 +424,8 @@ extern enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *
 						     const struct tcphdr *th);
 extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
 				   struct request_sock *req,
-				   struct request_sock **prev);
+				   struct request_sock **prev,
+				   bool fastopen);
 extern int tcp_child_process(struct sock *parent, struct sock *child,
 			     struct sk_buff *skb);
 extern bool tcp_use_frto(struct sock *sk);
@@ -478,7 +479,8 @@ extern int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr,
 extern int tcp_connect(struct sock *sk);
 extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 					struct request_sock *req,
-					struct request_values *rvp);
+					struct request_values *rvp,
+					struct tcp_fastopen_cookie *foc);
 extern int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_connect_init(struct sock *sk);

commit 1046716368979dee857a2b8a91c4a8833f21b9cb
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 31 12:29:11 2012 +0000

    tcp: TCP Fast Open Server - header & support functions
    
    This patch adds all the necessary data structure and support
    functions to implement TFO server side. It also documents a number
    of flags for the sysctl_tcp_fastopen knob, and adds a few Linux
    extension MIBs.
    
    In addition, it includes the following:
    
    1. a new TCP_FASTOPEN socket option an application must call to
    supply a max backlog allowed in order to enable TFO on its listener.
    
    2. A number of key data structures:
    "fastopen_rsk" in tcp_sock - for a big socket to access its
    request_sock for retransmission and ack processing purpose. It is
    non-NULL iff 3WHS not completed.
    
    "fastopenq" in request_sock_queue - points to a per Fast Open
    listener data structure "fastopen_queue" to keep track of qlen (# of
    outstanding Fast Open requests) and max_qlen, among other things.
    
    "listener" in tcp_request_sock - to point to the original listener
    for book-keeping purpose, i.e., to maintain qlen against max_qlen
    as part of defense against IP spoofing attack.
    
    3. various data structure and functions, many in tcp_fastopen.c, to
    support server side Fast Open cookie operations, including
    /proc/sys/net/ipv4/tcp_fastopen_key to allow manual rekeying.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0fca06f16463..9f8821e3293a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -224,8 +224,24 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* Bit Flags for sysctl_tcp_fastopen */
 #define	TFO_CLIENT_ENABLE	1
+#define	TFO_SERVER_ENABLE	2
 #define	TFO_CLIENT_NO_COOKIE	4	/* Data in SYN w/o cookie option */
 
+/* Process SYN data but skip cookie validation */
+#define	TFO_SERVER_COOKIE_NOT_CHKED	0x100
+/* Accept SYN data w/o any cookie option */
+#define	TFO_SERVER_COOKIE_NOT_REQD	0x200
+
+/* Force enable TFO on all listeners, i.e., not requiring the
+ * TCP_FASTOPEN socket option. SOCKOPT1/2 determine how to set max_qlen.
+ */
+#define	TFO_SERVER_WO_SOCKOPT1	0x400
+#define	TFO_SERVER_WO_SOCKOPT2	0x800
+/* Always create TFO child sockets on a TFO listener even when
+ * cookie/data not present. (For testing purpose!)
+ */
+#define	TFO_SERVER_ALWAYS	0x1000
+
 extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */
@@ -421,12 +437,6 @@ extern void tcp_metrics_init(void);
 extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check);
 extern bool tcp_remember_stamp(struct sock *sk);
 extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
-extern void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
-				   struct tcp_fastopen_cookie *cookie,
-				   int *syn_loss, unsigned long *last_syn_loss);
-extern void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
-				   struct tcp_fastopen_cookie *cookie,
-				   bool syn_lost);
 extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);
@@ -537,6 +547,7 @@ extern void tcp_send_delayed_ack(struct sock *sk);
 extern void tcp_cwnd_application_limited(struct sock *sk);
 extern void tcp_resume_early_retransmit(struct sock *sk);
 extern void tcp_rearm_rto(struct sock *sk);
+extern void tcp_reset(struct sock *sk);
 
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);
@@ -586,6 +597,7 @@ extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
 extern int tcp_mss_to_mtu(struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
 extern void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt);
+extern void tcp_init_buffer_space(struct sock *sk);
 
 static inline void tcp_bound_rto(const struct sock *sk)
 {
@@ -1104,6 +1116,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
 	req->cookie_ts = 0;
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
+	tcp_rsk(req)->rcv_nxt = TCP_SKB_CB(skb)->seq + 1;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
 	ireq->tstamp_ok = rx_opt->tstamp_ok;
@@ -1308,15 +1321,34 @@ extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
 			    const struct tcp_md5sig_key *key);
 
+/* From tcp_fastopen.c */
+extern void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
+				   struct tcp_fastopen_cookie *cookie,
+				   int *syn_loss, unsigned long *last_syn_loss);
+extern void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
+				   struct tcp_fastopen_cookie *cookie,
+				   bool syn_lost);
 struct tcp_fastopen_request {
 	/* Fast Open cookie. Size 0 means a cookie request */
 	struct tcp_fastopen_cookie	cookie;
 	struct msghdr			*data;  /* data in MSG_FASTOPEN */
 	u16				copied;	/* queued in tcp_connect() */
 };
-
 void tcp_free_fastopen_req(struct tcp_sock *tp);
 
+extern struct tcp_fastopen_context __rcu *tcp_fastopen_ctx;
+int tcp_fastopen_reset_cipher(void *key, unsigned int len);
+void tcp_fastopen_cookie_gen(__be32 addr, struct tcp_fastopen_cookie *foc);
+
+#define TCP_FASTOPEN_KEY_LENGTH 16
+
+/* Fastopen key context */
+struct tcp_fastopen_context {
+	struct crypto_cipher __rcu	*tfm;
+	__u8				key[TCP_FASTOPEN_KEY_LENGTH];
+	struct rcu_head			rcu;
+};
+
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
 {

commit 6c9ff979d1921e9fd05d89e1383121c2503759b9
Author: Alex Bergmann <alex@linlab.net>
Date:   Fri Aug 31 02:48:31 2012 +0000

    tcp: Increase timeout for SYN segments
    
    Commit 9ad7c049 ("tcp: RFC2988bis + taking RTT sample from 3WHS for
    the passive open side") changed the initRTO from 3secs to 1sec in
    accordance to RFC6298 (former RFC2988bis). This reduced the time till
    the last SYN retransmission packet gets sent from 93secs to 31secs.
    
    RFC1122 is stating that the retransmission should be done for at least 3
    minutes, but this seems to be quite high.
    
      "However, the values of R1 and R2 may be different for SYN
      and data segments.  In particular, R2 for a SYN segment MUST
      be set large enough to provide retransmission of the segment
      for at least 3 minutes.  The application can close the
      connection (i.e., give up on the open attempt) sooner, of
      course."
    
    This patch increases the value of TCP_SYN_RETRIES to the value of 6,
    providing a retransmission window of 63secs.
    
    The comments for SYN and SYNACK retries have also been updated to
    describe the current settings. The same goes for the documentation file
    "Documentation/networking/ip-sysctl.txt".
    
    Signed-off-by: Alexander Bergmann <alex@linlab.net>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9a0021d16d91..0fca06f16463 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -98,11 +98,21 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 				 * 15 is ~13-30min depending on RTO.
 				 */
 
-#define TCP_SYN_RETRIES	 5	/* number of times to retry active opening a
-				 * connection: ~180sec is RFC minimum	*/
+#define TCP_SYN_RETRIES	 6	/* This is how many retries are done
+				 * when active opening a connection.
+				 * RFC1122 says the minimum retry MUST
+				 * be at least 180secs.  Nevertheless
+				 * this value is corresponding to
+				 * 63secs of retransmission with the
+				 * current initial RTO.
+				 */
 
-#define TCP_SYNACK_RETRIES 5	/* number of times to retry passive opening a
-				 * connection: ~180sec is RFC minimum	*/
+#define TCP_SYNACK_RETRIES 5	/* This is how may retries are done
+				 * when passive opening a connection.
+				 * This is corresponding to 31secs of
+				 * retransmission with the current
+				 * initial RTO.
+				 */
 
 #define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT
 				  * state, about 60 seconds	*/

commit e6acb384807406c1a6ad3ddc91191f7658e63b7a
Merge: 255e87657a84 898132ae76d1
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Aug 24 18:54:37 2012 -0400

    Merge branch 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    This is an initial merge in of Eric Biederman's work to start adding
    user namespace support to the networking.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a7cb5a49bf64ba64864ae16a6be028f8b0d3cc06
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 24 01:10:10 2012 -0600

    userns: Print out socket uids in a user namespace aware fashion.
    
    Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Cc: James Morris <jmorris@namei.org>
    Cc: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Sridhar Samudrala <sri@us.ibm.com>
    Acked-by: Vlad Yasevich <vyasevich@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e19124b84cd2..91e746736a8f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1509,7 +1509,8 @@ struct tcp_iter_state {
 	sa_family_t		family;
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;
-	int			bucket, offset, sbucket, num, uid;
+	int			bucket, offset, sbucket, num;
+	kuid_t			uid;
 	loff_t			last_pos;
 };
 

commit 63d02d157ec4124990258d66517b6c11fd6df0cf
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Aug 9 14:11:00 2012 +0000

    net: tcp: ipv6_mapped needs sk_rx_dst_set method
    
    commit 5d299f3d3c8a2fb (net: ipv6: fix TCP early demux) added a
    regression for ipv6_mapped case.
    
    [   67.422369] SELinux: initialized (dev autofs, type autofs), uses
    genfs_contexts
    [   67.449678] SELinux: initialized (dev autofs, type autofs), uses
    genfs_contexts
    [   92.631060] BUG: unable to handle kernel NULL pointer dereference at
    (null)
    [   92.631435] IP: [<          (null)>]           (null)
    [   92.631645] PGD 0
    [   92.631846] Oops: 0010 [#1] SMP
    [   92.632095] Modules linked in: autofs4 sunrpc ipv6 dm_mirror
    dm_region_hash dm_log dm_multipath dm_mod video sbs sbshc battery ac lp
    parport sg snd_hda_intel snd_hda_codec snd_seq_oss snd_seq_midi_event
    snd_seq snd_seq_device pcspkr snd_pcm_oss snd_mixer_oss snd_pcm
    snd_timer serio_raw button floppy snd i2c_i801 i2c_core soundcore
    snd_page_alloc shpchp ide_cd_mod cdrom microcode ehci_hcd ohci_hcd
    uhci_hcd
    [   92.634294] CPU 0
    [   92.634294] Pid: 4469, comm: sendmail Not tainted 3.6.0-rc1 #3
    [   92.634294] RIP: 0010:[<0000000000000000>]  [<          (null)>]
    (null)
    [   92.634294] RSP: 0018:ffff880245fc7cb0  EFLAGS: 00010282
    [   92.634294] RAX: ffffffffa01985f0 RBX: ffff88024827ad00 RCX:
    0000000000000000
    [   92.634294] RDX: 0000000000000218 RSI: ffff880254735380 RDI:
    ffff88024827ad00
    [   92.634294] RBP: ffff880245fc7cc8 R08: 0000000000000001 R09:
    0000000000000000
    [   92.634294] R10: 0000000000000000 R11: ffff880245fc7bf8 R12:
    ffff880254735380
    [   92.634294] R13: ffff880254735380 R14: 0000000000000000 R15:
    7fffffffffff0218
    [   92.634294] FS:  00007f4516ccd6f0(0000) GS:ffff880256600000(0000)
    knlGS:0000000000000000
    [   92.634294] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    [   92.634294] CR2: 0000000000000000 CR3: 0000000245ed1000 CR4:
    00000000000007f0
    [   92.634294] DR0: 0000000000000000 DR1: 0000000000000000 DR2:
    0000000000000000
    [   92.634294] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7:
    0000000000000400
    [   92.634294] Process sendmail (pid: 4469, threadinfo ffff880245fc6000,
    task ffff880254b8cac0)
    [   92.634294] Stack:
    [   92.634294]  ffffffff813837a7 ffff88024827ad00 ffff880254b6b0e8
    ffff880245fc7d68
    [   92.634294]  ffffffff81385083 00000000001d2680 ffff8802547353a8
    ffff880245fc7d18
    [   92.634294]  ffffffff8105903a ffff88024827ad60 0000000000000002
    00000000000000ff
    [   92.634294] Call Trace:
    [   92.634294]  [<ffffffff813837a7>] ? tcp_finish_connect+0x2c/0xfa
    [   92.634294]  [<ffffffff81385083>] tcp_rcv_state_process+0x2b6/0x9c6
    [   92.634294]  [<ffffffff8105903a>] ? sched_clock_cpu+0xc3/0xd1
    [   92.634294]  [<ffffffff81059073>] ? local_clock+0x2b/0x3c
    [   92.634294]  [<ffffffff8138caf3>] tcp_v4_do_rcv+0x63a/0x670
    [   92.634294]  [<ffffffff8133278e>] release_sock+0x128/0x1bd
    [   92.634294]  [<ffffffff8139f060>] __inet_stream_connect+0x1b1/0x352
    [   92.634294]  [<ffffffff813325f5>] ? lock_sock_nested+0x74/0x7f
    [   92.634294]  [<ffffffff8104b333>] ? wake_up_bit+0x25/0x25
    [   92.634294]  [<ffffffff813325f5>] ? lock_sock_nested+0x74/0x7f
    [   92.634294]  [<ffffffff8139f223>] ? inet_stream_connect+0x22/0x4b
    [   92.634294]  [<ffffffff8139f234>] inet_stream_connect+0x33/0x4b
    [   92.634294]  [<ffffffff8132e8cf>] sys_connect+0x78/0x9e
    [   92.634294]  [<ffffffff813fd407>] ? sysret_check+0x1b/0x56
    [   92.634294]  [<ffffffff81088503>] ? __audit_syscall_entry+0x195/0x1c8
    [   92.634294]  [<ffffffff811cc26e>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [   92.634294]  [<ffffffff813fd3e2>] system_call_fastpath+0x16/0x1b
    [   92.634294] Code:  Bad RIP value.
    [   92.634294] RIP  [<          (null)>]           (null)
    [   92.634294]  RSP <ffff880245fc7cb0>
    [   92.634294] CR2: 0000000000000000
    [   92.648982] ---[ end trace 24e2bed94314c8d9 ]---
    [   92.649146] Kernel panic - not syncing: Fatal exception in interrupt
    
    Fix this using inet_sk_rx_dst_set(), and export this function in case
    IPv6 is modular.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e19124b84cd2..1f000ffe7075 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -464,6 +464,7 @@ extern int tcp_disconnect(struct sock *sk, int flags);
 void tcp_connect_init(struct sock *sk);
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
 int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
+void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];

commit 6f458dfb409272082c9bfa412f77ff2fc21c626f
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jul 20 05:45:50 2012 +0000

    tcp: improve latencies of timer triggered events
    
    Modern TCP stack highly depends on tcp_write_timer() having a small
    latency, but current implementation doesn't exactly meet the
    expectations.
    
    When a timer fires but finds the socket is owned by the user, it rearms
    itself for an additional delay hoping next run will be more
    successful.
    
    tcp_write_timer() for example uses a 50ms delay for next try, and it
    defeats many attempts to get predictable TCP behavior in term of
    latencies.
    
    Use the recently introduced tcp_release_cb(), so that the user owning
    the socket will call various handlers right before socket release.
    
    This will permit us to post a followup patch to address the
    tcp_tso_should_defer() syndrome (some deferred packets have to wait
    RTO timer to be transmitted, while cwnd should allow us to send them
    sooner)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: H.K. Jerry Chu <hkchu@google.com>
    Cc: John Heffner <johnwheffner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bc7c134ec054..e19124b84cd2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -350,6 +350,8 @@ extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
 			size_t size, int flags);
 extern void tcp_release_cb(struct sock *sk);
+extern void tcp_write_timer_handler(struct sock *sk);
+extern void tcp_delack_timer_handler(struct sock *sk);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 				 const struct tcphdr *th, unsigned int len);

commit 67da22d23fa6f3324e03bcd0580b914b2e4afbf3
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:11 2012 +0000

    net-tcp: Fast Open client - cookie-less mode
    
    In trusted networks, e.g., intranet, data-center, the client does not
    need to use Fast Open cookie to mitigate DoS attacks. In cookie-less
    mode, sendmsg() with MSG_FASTOPEN flag will send SYN-data regardless
    of cookie availability.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e07878d246aa..bc7c134ec054 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -214,6 +214,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 /* Bit Flags for sysctl_tcp_fastopen */
 #define	TFO_CLIENT_ENABLE	1
+#define	TFO_CLIENT_NO_COOKIE	4	/* Data in SYN w/o cookie option */
 
 extern struct inet_timewait_death_row tcp_death_row;
 

commit aab4874355679c70f93993cf3b3fd74643b9ac33
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:10 2012 +0000

    net-tcp: Fast Open client - detecting SYN-data drops
    
    On paths with firewalls dropping SYN with data or experimental TCP options,
    Fast Open connections will have experience SYN timeout and bad performance.
    The solution is to track such incidents in the cookie cache and disables
    Fast Open temporarily.
    
    Since only the original SYN includes data and/or Fast Open option, the
    SYN-ACK has some tell-tale sign (tcp_rcv_fastopen_synack()) to detect
    such drops. If a path has recurring Fast Open SYN drops, Fast Open is
    disabled for 2^(recurring_losses) minutes starting from four minutes up to
    roughly one and half day. sendmsg with MSG_FASTOPEN flag will succeed but
    it behaves as connect() then write().
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c0258100d70c..e07878d246aa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -409,9 +409,11 @@ extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst,
 extern bool tcp_remember_stamp(struct sock *sk);
 extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
-				   struct tcp_fastopen_cookie *cookie);
+				   struct tcp_fastopen_cookie *cookie,
+				   int *syn_loss, unsigned long *last_syn_loss);
 extern void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
-				   struct tcp_fastopen_cookie *cookie);
+				   struct tcp_fastopen_cookie *cookie,
+				   bool syn_lost);
 extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);

commit cf60af03ca4e71134206809ea892e49b92a88896
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:09 2012 +0000

    net-tcp: Fast Open client - sendmsg(MSG_FASTOPEN)
    
    sendmsg() (or sendto()) with MSG_FASTOPEN is a combo of connect(2)
    and write(2). The application should replace connect() with it to
    send data in the opening SYN packet.
    
    For blocking socket, sendmsg() blocks until all the data are buffered
    locally and the handshake is completed like connect() call. It
    returns similar errno like connect() if the TCP handshake fails.
    
    For non-blocking socket, it returns the number of bytes queued (and
    transmitted in the SYN-data packet) if cookie is available. If cookie
    is not available, it transmits a data-less SYN packet with Fast Open
    cookie request option and returns -EINPROGRESS like connect().
    
    Using MSG_FASTOPEN on connecting or connected socket will result in
    simlar errno like repeating connect() calls. Therefore the application
    should only use this flag on new sockets.
    
    The buffer size of sendmsg() is independent of the MSS of the connection.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 867557b4244a..c0258100d70c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -212,6 +212,9 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* TCP initial congestion window as per draft-hkchu-tcpm-initcwnd-01 */
 #define TCP_INIT_CWND		10
 
+/* Bit Flags for sysctl_tcp_fastopen */
+#define	TFO_CLIENT_ENABLE	1
+
 extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */

commit 783237e8daf13481ee234997cbbbb823872ac388
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:07 2012 +0000

    net-tcp: Fast Open client - sending SYN-data
    
    This patch implements sending SYN-data in tcp_connect(). The data is
    from tcp_sendmsg() with flag MSG_FASTOPEN (implemented in a later patch).
    
    The length of the cookie in tcp_fastopen_req, init'd to 0, controls the
    type of the SYN. If the cookie is not cached (len==0), the host sends
    data-less SYN with Fast Open cookie request option to solicit a cookie
    from the remote. If cookie is not available (len > 0), the host sends
    a SYN-data with Fast Open cookie option. If cookie length is negative,
      the SYN will not include any Fast Open option (for fall back operations).
    
    To deal with middleboxes that may drop SYN with data or experimental TCP
    option, the SYN-data is only sent once. SYN retransmits do not include
    data or Fast Open options. The connection will fall back to regular TCP
    handshake.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e601da197361..867557b4244a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1289,6 +1289,15 @@ extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
 			    const struct tcp_md5sig_key *key);
 
+struct tcp_fastopen_request {
+	/* Fast Open cookie. Size 0 means a cookie request */
+	struct tcp_fastopen_cookie	cookie;
+	struct msghdr			*data;  /* data in MSG_FASTOPEN */
+	u16				copied;	/* queued in tcp_connect() */
+};
+
+void tcp_free_fastopen_req(struct tcp_sock *tp);
+
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
 {

commit 1fe4c481ba637660793217769695c146a037bd54
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:06 2012 +0000

    net-tcp: Fast Open client - cookie cache
    
    With help from Eric Dumazet, add Fast Open metrics in tcp metrics cache.
    The basic ones are MSS and the cookies. Later patch will cache more to
    handle unfriendly middleboxes.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5aed3718fde8..e601da197361 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -405,6 +405,10 @@ extern void tcp_metrics_init(void);
 extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check);
 extern bool tcp_remember_stamp(struct sock *sk);
 extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
+extern void tcp_fastopen_cache_get(struct sock *sk, u16 *mss,
+				   struct tcp_fastopen_cookie *cookie);
+extern void tcp_fastopen_cache_set(struct sock *sk, u16 mss,
+				   struct tcp_fastopen_cookie *cookie);
 extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);

commit 2100c8d2d9db23c0a09901a782bb4e3b21bee298
Author: Yuchung Cheng <ycheng@google.com>
Date:   Thu Jul 19 06:43:05 2012 +0000

    net-tcp: Fast Open base
    
    This patch impelements the common code for both the client and server.
    
    1. TCP Fast Open option processing. Since Fast Open does not have an
       option number assigned by IANA yet, it shares the experiment option
       code 254 by implementing draft-ietf-tcpm-experimental-options
       with a 16 bits magic number 0xF989. This enables global experiments
       without clashing the scarce(2) experimental options available for TCP.
    
       When the draft status becomes standard (maybe), the client should
       switch to the new option number assigned while the server supports
       both numbers for transistion.
    
    2. The new sysctl tcp_fastopen
    
    3. A place holder init function
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 85c5090bfe25..5aed3718fde8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -170,6 +170,11 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
 #define TCPOPT_COOKIE		253	/* Cookie extension (experimental) */
+#define TCPOPT_EXP		254	/* Experimental */
+/* Magic number to be after the option value for sharing TCP
+ * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
+ */
+#define TCPOPT_FASTOPEN_MAGIC	0xF989
 
 /*
  *     TCP option lengths
@@ -180,6 +185,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERM      2
 #define TCPOLEN_TIMESTAMP      10
 #define TCPOLEN_MD5SIG         18
+#define TCPOLEN_EXP_FASTOPEN_BASE  4
 #define TCPOLEN_COOKIE_BASE    2	/* Cookie-less header extension */
 #define TCPOLEN_COOKIE_PAIR    3	/* Cookie pair header extension */
 #define TCPOLEN_COOKIE_MIN     (TCPOLEN_COOKIE_BASE+TCP_COOKIE_MIN)
@@ -222,6 +228,7 @@ extern int sysctl_tcp_retries1;
 extern int sysctl_tcp_retries2;
 extern int sysctl_tcp_orphan_retries;
 extern int sysctl_tcp_syncookies;
+extern int sysctl_tcp_fastopen;
 extern int sysctl_tcp_retrans_collapse;
 extern int sysctl_tcp_stdurg;
 extern int sysctl_tcp_rfc1337;
@@ -418,7 +425,7 @@ extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t len, int nonblock, int flags, int *addr_len);
 extern void tcp_parse_options(const struct sk_buff *skb,
 			      struct tcp_options_received *opt_rx, const u8 **hvpp,
-			      int estab);
+			      int estab, struct tcp_fastopen_cookie *foc);
 extern const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
 /*

commit 282f23c6ee343126156dd41218b22ece96d747e3
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jul 17 10:13:05 2012 +0200

    tcp: implement RFC 5961 3.2
    
    Implement the RFC 5691 mitigation against Blind
    Reset attack using RST bit.
    
    Idea is to validate incoming RST sequence,
    to match RCV.NXT value, instead of previouly accepted
    window : (RCV.NXT <= SEG.SEQ < RCV.NXT+RCV.WND)
    
    If sequence is in window but not an exact match, send
    a "challenge ACK", so that the other part can resend an
    RST with the appropriate sequence.
    
    Add a new sysctl, tcp_challenge_ack_limit, to limit
    number of challenge ACK sent per second.
    
    Add a new SNMP counter to count number of challenge acks sent.
    (netstat -s | grep TCPChallengeACK)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Kiran Kumar Kella <kkiran@broadcom.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 439984b9af49..85c5090bfe25 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -254,6 +254,7 @@ extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
 extern int sysctl_tcp_limit_output_bytes;
+extern int sysctl_tcp_challenge_ack_limit;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 46d3ceabd8d98ed0ad10f20c595ca784e34786c5
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Jul 11 05:50:31 2012 +0000

    tcp: TCP Small Queues
    
    This introduce TSQ (TCP Small Queues)
    
    TSQ goal is to reduce number of TCP packets in xmit queues (qdisc &
    device queues), to reduce RTT and cwnd bias, part of the bufferbloat
    problem.
    
    sk->sk_wmem_alloc not allowed to grow above a given limit,
    allowing no more than ~128KB [1] per tcp socket in qdisc/dev layers at a
    given time.
    
    TSO packets are sized/capped to half the limit, so that we have two
    TSO packets in flight, allowing better bandwidth use.
    
    As a side effect, setting the limit to 40000 automatically reduces the
    standard gso max limit (65536) to 40000/2 : It can help to reduce
    latencies of high prio packets, having smaller TSO packets.
    
    This means we divert sock_wfree() to a tcp_wfree() handler, to
    queue/send following frames when skb_orphan() [2] is called for the
    already queued skbs.
    
    Results on my dev machines (tg3/ixgbe nics) are really impressive,
    using standard pfifo_fast, and with or without TSO/GSO.
    
    Without reduction of nominal bandwidth, we have reduction of buffering
    per bulk sender :
    < 1ms on Gbit (instead of 50ms with TSO)
    < 8ms on 100Mbit (instead of 132 ms)
    
    I no longer have 4 MBytes backlogged in qdisc by a single netperf
    session, and both side socket autotuning no longer use 4 Mbytes.
    
    As skb destructor cannot restart xmit itself ( as qdisc lock might be
    taken at this point ), we delegate the work to a tasklet. We use one
    tasklest per cpu for performance reasons.
    
    If tasklet finds a socket owned by the user, it sets TSQ_OWNED flag.
    This flag is tested in a new protocol method called from release_sock(),
    to eventually send new segments.
    
    [1] New /proc/sys/net/ipv4/tcp_limit_output_bytes tunable
    [2] skb_orphan() is usually called at TX completion time,
      but some drivers call it in their start_xmit() handler.
      These drivers should at least use BQL, or else a single TCP
      session can still fill the whole NIC TX ring, since TSQ will
      have no effect.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Dave Taht <dave.taht@bufferbloat.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3618fefae049..439984b9af49 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -253,6 +253,7 @@ extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 extern int sysctl_tcp_early_retrans;
+extern int sysctl_tcp_limit_output_bytes;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -321,6 +322,8 @@ extern struct proto tcp_prot;
 
 extern void tcp_init_mem(struct net *net);
 
+extern void tcp_tasklet_init(void);
+
 extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);
@@ -334,6 +337,7 @@ extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t size);
 extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
 			size_t size, int flags);
+extern void tcp_release_cb(struct sock *sk);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 				 const struct tcphdr *th, unsigned int len);

commit 81166dd6fa8eb780b2132d32fbc77eb6ac04e44e
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 10 03:14:24 2012 -0700

    tcp: Move timestamps from inetpeer to metrics cache.
    
    With help from Lin Ming.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0900d63d1627..3618fefae049 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -390,7 +390,10 @@ extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
 extern void tcp_init_metrics(struct sock *sk);
 extern void tcp_metrics_init(void);
-extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
+extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst, bool paws_check);
+extern bool tcp_remember_stamp(struct sock *sk);
+extern bool tcp_tw_remember_stamp(struct inet_timewait_sock *tw);
+extern void tcp_fetch_timewait_stamp(struct sock *sk, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);
 extern void tcp_init_sock(struct sock *sk);

commit 51c5d0c4b169bf762f09e0d5b283a7f0b2a45739
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 10 00:49:14 2012 -0700

    tcp: Maintain dynamic metrics in local cache.
    
    Maintain a local hash table of TCP dynamic metrics blobs.
    
    Computed TCP metrics are no longer maintained in the route metrics.
    
    The table uses RCU and an extremely simple hash so that it has low
    latency and low overhead.  A simple hash is legitimate because we only
    make metrics blobs for fully established connections.
    
    Some tweaking of the default hash table sizes, metric timeouts, and
    the hash chain length limit certainly could use some tweaking.  But
    the basic design seems sound.
    
    With help from Eric Dumazet and Joe Perches.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5478356ea8c5..0900d63d1627 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -389,6 +389,7 @@ extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
 extern void tcp_init_metrics(struct sock *sk);
+extern void tcp_metrics_init(void);
 extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);

commit ab92bb2f679d66c7e12a6b1c0cdd76fe308f6546
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jul 9 16:19:30 2012 -0700

    tcp: Abstract back handling peer aliveness test into helper function.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 98ca797001a2..5478356ea8c5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -389,6 +389,7 @@ extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
 extern void tcp_init_metrics(struct sock *sk);
+extern bool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);
 extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);
 extern void tcp_init_sock(struct sock *sk);

commit 4aabd8ef8c43677cfee3e1e36c5a79edddb41942
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jul 9 16:07:30 2012 -0700

    tcp: Move dynamnic metrics handling into seperate file.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 53fb7d814170..98ca797001a2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -388,6 +388,8 @@ extern void tcp_enter_frto(struct sock *sk);
 extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
+extern void tcp_init_metrics(struct sock *sk);
+extern void tcp_disable_fack(struct tcp_sock *tp);
 extern void tcp_close(struct sock *sk, long timeout);
 extern void tcp_init_sock(struct sock *sk);
 extern unsigned int tcp_poll(struct file * file, struct socket *sock,
@@ -556,6 +558,8 @@ static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
 	return (tp->srtt >> 3) + tp->rttvar;
 }
 
+extern void tcp_set_rto(struct sock *sk);
+
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
 	tp->pred_flags = htonl((tp->tcp_header_len << 26) |

commit 160eb5a6b14ca2eab5c598bdbbb24c24624bad34
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jun 27 22:01:22 2012 -0700

    ipv4: Kill early demux method return value.
    
    It's completely unnecessary.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6660ffc4963d..53fb7d814170 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -325,7 +325,7 @@ extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);
 
-extern int tcp_v4_early_demux(struct sk_buff *skb);
+extern void tcp_v4_early_demux(struct sk_buff *skb);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);

commit c10237e077cef50e925f052e49f3b4fead9d71f9
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jun 27 17:05:06 2012 -0700

    Revert "ipv4: tcp: dont cache unconfirmed intput dst"
    
    This reverts commit c074da2810c118b3812f32d6754bd9ead2f169e7.
    
    This change has several unwanted side effects:
    
    1) Sockets will cache the DST_NOCACHE route in sk->sk_rx_dst and we'll
       thus never create a real cached route.
    
    2) All TCP traffic will use DST_NOCACHE and never use the routing
       cache at all.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 917ed2e55e8c..6660ffc4963d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -325,7 +325,7 @@ extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);
 
-extern int tcp_v4_early_demux(struct sk_buff *skb, bool *nocache);
+extern int tcp_v4_early_demux(struct sk_buff *skb);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);

commit c074da2810c118b3812f32d6754bd9ead2f169e7
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 26 23:14:15 2012 +0000

    ipv4: tcp: dont cache unconfirmed intput dst
    
    DDOS synflood attacks hit badly IP route cache.
    
    On typical machines, this cache is allowed to hold up to 8 Millions dst
    entries, 256 bytes for each, for a total of 2GB of memory.
    
    rt_garbage_collect() triggers and tries to cleanup things.
    
    Eventually route cache is disabled but machine is under fire and might
    OOM and crash.
    
    This patch exploits the new TCP early demux, to set a nocache
    boolean in case incoming TCP frame is for a not yet ESTABLISHED or
    TIMEWAIT socket.
    
    This 'nocache' boolean is then used in case dst entry is not found in
    route cache, to create an unhashed dst entry (DST_NOCACHE)
    
    SYN-cookie-ACK sent use a similar mechanism (ipv4: tcp: dont cache
    output dst for syncookies), so after this patch, a machine is able to
    absorb a DDOS synflood attack without polluting its IP route cache.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Hans Schillstrom <hans.schillstrom@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6660ffc4963d..917ed2e55e8c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -325,7 +325,7 @@ extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);
 
-extern int tcp_v4_early_demux(struct sk_buff *skb);
+extern int tcp_v4_early_demux(struct sk_buff *skb, bool *nocache);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);

commit 41063e9dd11956f2d285e12e4342e1d232ba0ea2
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 19 21:22:05 2012 -0700

    ipv4: Early TCP socket demux.
    
    Input packet processing for local sockets involves two major demuxes.
    One for the route and one for the socket.
    
    But we can optimize this down to one demux for certain kinds of local
    sockets.
    
    Currently we only do this for established TCP sockets, but it could
    at least in theory be expanded to other kinds of connections.
    
    If a TCP socket is established then it's identity is fully specified.
    
    This means that whatever input route was used during the three-way
    handshake must work equally well for the rest of the connection since
    the keys will not change.
    
    Once we move to established state, we cache the receive packet's input
    route to use later.
    
    Like the existing cached route in sk->sk_dst_cache used for output
    packets, we have to check for route invalidations using dst->obsolete
    and dst->ops->check().
    
    Early demux occurs outside of a socket locked section, so when a route
    invalidation occurs we defer the fixup of sk->sk_rx_dst until we are
    actually inside of established state packet processing and thus have
    the socket locked.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9332f342259a..6660ffc4963d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -325,6 +325,7 @@ extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);
 
+extern int tcp_v4_early_demux(struct sk_buff *skb);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);

commit 2397849baa7c44c242e5d5142d5d16d1e7ed53d0
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jun 9 14:56:12 2012 -0700

    [PATCH] tcp: Cache inetpeer in timewait socket, and only when necessary.
    
    Since it's guarenteed that we will access the inetpeer if we're trying
    to do timewait recycling and TCP options were enabled on the
    connection, just cache the peer in the timewait socket.
    
    In the future, inetpeer lookups will be context dependent (per routing
    realm), and this helps facilitate that as well.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 424591866037..9332f342259a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -328,7 +328,6 @@ extern void tcp_shutdown (struct sock *sk, int how);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);
-extern void *tcp_v4_tw_get_peer(struct sock *sk);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t size);

commit 4670fd819e7f47392c7c6fc6168ea2857c66d163
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jun 9 01:25:47 2012 -0700

    tcp: Get rid of inetpeer special cases.
    
    The get_peer method TCP uses is full of special cases that make no
    sense accommodating, and it also gets in the way of doing more
    reasonable things here.
    
    First of all, if the socket doesn't have a usable cached route, there
    is no sense in trying to optimize timewait recycling.
    
    Likewise for the case where we have IP options, such as SRR enabled,
    that make the IP header destination address (and thus the destination
    address of the route key) differ from that of the connection's
    destination address.
    
    Just return a NULL peer in these cases, and thus we're also able to
    get rid of the clumsy inetpeer release logic.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e79aa48d9fc1..424591866037 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -327,7 +327,7 @@ extern void tcp_shutdown (struct sock *sk, int how);
 
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
-extern struct inet_peer *tcp_v4_get_peer(struct sock *sk, bool *release_it);
+extern struct inet_peer *tcp_v4_get_peer(struct sock *sk);
 extern void *tcp_v4_tw_get_peer(struct sock *sk);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,

commit a2a385d627e1549da4b43a8b3dfe370589766e1c
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 16 23:15:34 2012 +0000

    tcp: bool conversions
    
    bool conversions where possible.
    
    __inline__ -> inline
    
    space cleanups
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index aaf5de9448c9..e79aa48d9fc1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -263,14 +263,14 @@ extern int tcp_memory_pressure;
  * and worry about wraparound (automatic with unsigned arithmetic).
  */
 
-static inline int before(__u32 seq1, __u32 seq2)
+static inline bool before(__u32 seq1, __u32 seq2)
 {
         return (__s32)(seq1-seq2) < 0;
 }
 #define after(seq2, seq1) 	before(seq1, seq2)
 
 /* is s2<=s1<=s3 ? */
-static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)
+static inline bool between(__u32 seq1, __u32 seq2, __u32 seq3)
 {
 	return seq3 - seq2 >= seq1 - seq2;
 }
@@ -305,7 +305,7 @@ static inline void tcp_synq_overflow(struct sock *sk)
 }
 
 /* syncookies: no recent synqueue overflow on this listening socket? */
-static inline int tcp_synq_no_recent_overflow(const struct sock *sk)
+static inline bool tcp_synq_no_recent_overflow(const struct sock *sk)
 {
 	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
 	return time_after(jiffies, last_overflow + TCP_TIMEOUT_FALLBACK);
@@ -383,7 +383,7 @@ extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
 				   struct request_sock **prev);
 extern int tcp_child_process(struct sock *parent, struct sock *child,
 			     struct sk_buff *skb);
-extern int tcp_use_frto(struct sock *sk);
+extern bool tcp_use_frto(struct sock *sk);
 extern void tcp_enter_frto(struct sock *sk);
 extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
@@ -470,7 +470,7 @@ static inline __u32 cookie_v6_init_sequence(struct sock *sk,
 
 extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 				      int nonagle);
-extern int tcp_may_send_now(struct sock *sk);
+extern bool tcp_may_send_now(struct sock *sk);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_retransmit_timer(struct sock *sk);
 extern void tcp_xmit_retransmit_queue(struct sock *);
@@ -484,9 +484,9 @@ extern int tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
 extern void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 extern int tcp_send_synack(struct sock *);
-extern int tcp_syn_flood_action(struct sock *sk,
-				const struct sk_buff *skb,
-				const char *proto);
+extern bool tcp_syn_flood_action(struct sock *sk,
+				 const struct sk_buff *skb,
+				 const char *proto);
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);
@@ -794,12 +794,12 @@ static inline int tcp_is_sack(const struct tcp_sock *tp)
 	return tp->rx_opt.sack_ok;
 }
 
-static inline int tcp_is_reno(const struct tcp_sock *tp)
+static inline bool tcp_is_reno(const struct tcp_sock *tp)
 {
 	return !tcp_is_sack(tp);
 }
 
-static inline int tcp_is_fack(const struct tcp_sock *tp)
+static inline bool tcp_is_fack(const struct tcp_sock *tp)
 {
 	return tp->rx_opt.sack_ok & TCP_FACK_ENABLED;
 }
@@ -901,7 +901,7 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 {
 	return tp->snd_una + tp->snd_wnd;
 }
-extern int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
+extern bool tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
 
 static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
 				       const struct sk_buff *skb)
@@ -944,7 +944,7 @@ static inline __sum16 __tcp_checksum_complete(struct sk_buff *skb)
 	return __skb_checksum_complete(skb);
 }
 
-static inline int tcp_checksum_complete(struct sk_buff *skb)
+static inline bool tcp_checksum_complete(struct sk_buff *skb)
 {
 	return !skb_csum_unnecessary(skb) &&
 		__tcp_checksum_complete(skb);
@@ -974,12 +974,12 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
  *
  * NOTE: is this not too big to inline?
  */
-static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
+static inline bool tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	if (sysctl_tcp_low_latency || !tp->ucopy.task)
-		return 0;
+		return false;
 
 	__skb_queue_tail(&tp->ucopy.prequeue, skb);
 	tp->ucopy.memory += skb->truesize;
@@ -1003,7 +1003,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 						  (3 * tcp_rto_min(sk)) / 4,
 						  TCP_RTO_MAX);
 	}
-	return 1;
+	return true;
 }
 
 
@@ -1108,28 +1108,28 @@ static inline int tcp_fin_time(const struct sock *sk)
 	return fin_timeout;
 }
 
-static inline int tcp_paws_check(const struct tcp_options_received *rx_opt,
-				 int paws_win)
+static inline bool tcp_paws_check(const struct tcp_options_received *rx_opt,
+				  int paws_win)
 {
 	if ((s32)(rx_opt->ts_recent - rx_opt->rcv_tsval) <= paws_win)
-		return 1;
+		return true;
 	if (unlikely(get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS))
-		return 1;
+		return true;
 	/*
 	 * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
 	 * then following tcp messages have valid values. Ignore 0 value,
 	 * or else 'negative' tsval might forbid us to accept their packets.
 	 */
 	if (!rx_opt->ts_recent)
-		return 1;
-	return 0;
+		return true;
+	return false;
 }
 
-static inline int tcp_paws_reject(const struct tcp_options_received *rx_opt,
-				  int rst)
+static inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,
+				   int rst)
 {
 	if (tcp_paws_check(rx_opt, 0))
-		return 0;
+		return false;
 
 	/* RST segments are not recommended to carry timestamp,
 	   and, if they do, it is recommended to ignore PAWS because
@@ -1144,8 +1144,8 @@ static inline int tcp_paws_reject(const struct tcp_options_received *rx_opt,
 	   However, we can relax time bounds for RST segments to MSL.
 	 */
 	if (rst && get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_MSL)
-		return 0;
-	return 1;
+		return false;
+	return true;
 }
 
 static inline void tcp_mib_init(struct net *net)
@@ -1383,7 +1383,7 @@ static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 
-static inline int tcp_write_queue_empty(struct sock *sk)
+static inline bool tcp_write_queue_empty(struct sock *sk)
 {
 	return skb_queue_empty(&sk->sk_write_queue);
 }
@@ -1440,7 +1440,7 @@ static inline void tcp_highest_sack_combine(struct sock *sk,
 /* Determines whether this is a thin stream (which may suffer from
  * increased latency). Used to trigger latency-reducing mechanisms.
  */
-static inline unsigned int tcp_stream_is_thin(struct tcp_sock *tp)
+static inline bool tcp_stream_is_thin(struct tcp_sock *tp)
 {
 	return tp->packets_out < 4 && !tcp_in_initial_slowstart(tp);
 }

commit 292e8d8c853889140ed77b7b37c66979b13080ae
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu May 10 01:49:41 2012 +0000

    tcp: Move rcvq sending to tcp_input.c
    
    It actually works on the input queue and will use its read mem
    routines, thus it's better to have in in the tcp_input.c file.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 92faa6a7ea97..aaf5de9448c9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -432,8 +432,7 @@ extern int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_connect_init(struct sock *sk);
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
-int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,
-			       int hdrlen, bool *fragstolen);
+int tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];

commit bd14b1b2e29bd6812597f896dde06eaf7c6d2f24
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 4 05:14:02 2012 +0000

    tcp: be more strict before accepting ECN negociation
    
    It appears some networks play bad games with the two bits reserved for
    ECN. This can trigger false congestion notifications and very slow
    transferts.
    
    Since RFC 3168 (6.1.1) forbids SYN packets to carry CT bits, we can
    disable TCP ECN negociation if it happens we receive mangled CT bits in
    the SYN packet.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Perry Lorier <perryl@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Wilmer van der Gaast <wilmer@google.com>
    Cc: Ankur Jain <jankur@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Dave Täht <dave.taht@bufferbloat.net>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c826ed7b007b..92faa6a7ea97 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -367,13 +367,6 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 #define	TCP_ECN_DEMAND_CWR	4
 #define	TCP_ECN_SEEN		8
 
-static __inline__ void
-TCP_ECN_create_request(struct request_sock *req, struct tcphdr *th)
-{
-	if (sysctl_tcp_ecn && th->ece && th->cwr)
-		inet_rsk(req)->ecn_ok = 1;
-}
-
 enum tcp_tw_status {
 	TCP_TW_SUCCESS = 0,
 	TCP_TW_RST = 1,
@@ -671,6 +664,22 @@ struct tcp_skb_cb {
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
+/* RFC3168 : 6.1.1 SYN packets must not have ECT/ECN bits set
+ *
+ * If we receive a SYN packet with these bits set, it means a network is
+ * playing bad games with TOS bits. In order to avoid possible false congestion
+ * notifications, we disable TCP ECN negociation.
+ */
+static inline void
+TCP_ECN_create_request(struct request_sock *req, const struct sk_buff *skb)
+{
+	const struct tcphdr *th = tcp_hdr(skb);
+
+	if (sysctl_tcp_ecn && th->ece && th->cwr &&
+	    INET_ECN_is_not_ect(TCP_SKB_CB(skb)->ip_dsfield))
+		inet_rsk(req)->ecn_ok = 1;
+}
+
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */

commit b081f85c2977b1cbb6e635d53d9512f1ef985972
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 2 09:58:29 2012 +0000

    net: implement tcp coalescing in tcp_queue_rcv()
    
    Extend tcp coalescing implementing it from tcp_queue_rcv(), the main
    receiver function when application is not blocked in recvmsg().
    
    Function tcp_queue_rcv() is moved a bit to allow its call from
    tcp_data_queue()
    
    This gives good results especially if GRO could not kick, and if skb
    head is a fragment.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5283aa4bfa23..c826ed7b007b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -439,7 +439,8 @@ extern int tcp_disconnect(struct sock *sk, int flags);
 
 void tcp_connect_init(struct sock *sk);
 void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
-void tcp_queue_rcv(struct sock *sk, struct sk_buff *skb, int hdrlen);
+int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,
+			       int hdrlen, bool *fragstolen);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];

commit 750ea2bafa55aaed208b2583470ecd7122225634
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 2 13:30:04 2012 +0000

    tcp: early retransmit: delayed fast retransmit
    
    Implementing the advanced early retransmit (sysctl_tcp_early_retrans==2).
    Delays the fast retransmit by an interval of RTT/4. We borrow the
    RTO timer to implement the delay. If we receive another ACK or send
    a new packet, the timer is cancelled and restored to original RTO
    value offset by time elapsed.  When the delayed-ER timer fires,
    we enter fast recovery and perform fast retransmit.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 685437a16c97..5283aa4bfa23 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -500,6 +500,8 @@ extern void tcp_send_delayed_ack(struct sock *sk);
 
 /* tcp_input.c */
 extern void tcp_cwnd_application_limited(struct sock *sk);
+extern void tcp_resume_early_retransmit(struct sock *sk);
+extern void tcp_rearm_rto(struct sock *sk);
 
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);
@@ -805,6 +807,7 @@ static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
 {
 	tp->do_early_retrans = sysctl_tcp_early_retrans &&
 		!sysctl_tcp_thin_dupack && sysctl_tcp_reordering == 3;
+	tp->early_retrans_delayed = 0;
 }
 
 static inline void tcp_disable_early_retrans(struct tcp_sock *tp)

commit eed530b6c67624db3f2cf477bac7c4d005d8f7ba
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed May 2 13:30:03 2012 +0000

    tcp: early retransmit
    
    This patch implements RFC 5827 early retransmit (ER) for TCP.
    It reduces DUPACK threshold (dupthresh) if outstanding packets are
    less than 4 to recover losses by fast recovery instead of timeout.
    
    While the algorithm is simple, small but frequent network reordering
    makes this feature dangerous: the connection repeatedly enter
    false recovery and degrade performance. Therefore we implement
    a mitigation suggested in the appendix of the RFC that delays
    entering fast recovery by a small interval, i.e., RTT/4. Currently
    ER is conservative and is disabled for the rest of the connection
    after the first reordering event. A large scale web server
    experiment on the performance impact of ER is summarized in
    section 6 of the paper "Proportional Rate Reduction for TCP”,
    IMC 2011. http://conferences.sigcomm.org/imc/2011/docs/p155.pdf
    
    Note that Linux has a similar feature called THIN_DUPACK. The
    differences are THIN_DUPACK do not mitigate reorderings and is only
    used after slow start. Currently ER is disabled if THIN_DUPACK is
    enabled. I would be happy to merge THIN_DUPACK feature with ER if
    people think it's a good idea.
    
    ER is enabled by sysctl_tcp_early_retrans:
      0: Disables ER
    
      1: Reduce dupthresh to packets_out - 1 when outstanding packets < 4.
    
      2: (Default) reduce dupthresh like mode 1. In addition, delay
         entering fast recovery by RTT/4.
    
    Note: mode 2 is implemented in the third part of this patch series.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0fb84de6da36..685437a16c97 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -252,6 +252,7 @@ extern int sysctl_tcp_max_ssthresh;
 extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
+extern int sysctl_tcp_early_retrans;
 
 extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -797,6 +798,20 @@ static inline void tcp_enable_fack(struct tcp_sock *tp)
 	tp->rx_opt.sack_ok |= TCP_FACK_ENABLED;
 }
 
+/* TCP early-retransmit (ER) is similar to but more conservative than
+ * the thin-dupack feature.  Enable ER only if thin-dupack is disabled.
+ */
+static inline void tcp_enable_early_retrans(struct tcp_sock *tp)
+{
+	tp->do_early_retrans = sysctl_tcp_early_retrans &&
+		!sysctl_tcp_thin_dupack && sysctl_tcp_reordering == 3;
+}
+
+static inline void tcp_disable_early_retrans(struct tcp_sock *tp)
+{
+	tp->do_early_retrans = 0;
+}
+
 static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
 {
 	return tp->sacked_out + tp->lost_out;

commit 67469601406c12ced3db9956aeb0ef0854e2952f
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Apr 24 07:37:38 2012 +0000

    ipv6: RTAX_FEATURE_ALLFRAG causes inefficient TCP segment sizing
    
    Quoting Tore Anderson from :
    https://bugzilla.kernel.org/show_bug.cgi?id=42572
    
    When RTAX_FEATURE_ALLFRAG is set on a route, the effective TCP segment
    size does not take into account the size of the IPv6 Fragmentation
    header that needs to be included in outbound packets, causing every
    transmitted TCP segment to be fragmented across two IPv6 packets, the
    latter of which will only contain 8 bytes of actual payload.
    
    RTAX_FEATURE_ALLFRAG is typically set on a route in response to
    receving a ICMPv6 Packet Too Big message indicating a Path MTU of less
    than 1280 bytes. 1280 bytes is the minimum IPv6 MTU, however ICMPv6
    PTBs with MTU < 1280 are still valid, in particular when an IPv6
    packet is sent to an IPv4 destination through a stateless translator.
    Any ICMPv4 Need To Fragment packets originated from the IPv4 part of
    the path will be translated to ICMPv6 PTB which may then indicate an
    MTU of less than 1280.
    
    The Linux kernel refuses to reduce the effective MTU to anything below
    1280 bytes, instead it sets it to exactly 1280 bytes, and
    RTAX_FEATURE_ALLFRAG is also set. However, the TCP segment size appears
    to be set to 1240 bytes (1280 Path MTU - 40 bytes of IPv6 header),
    instead of 1232 (additionally taking into account the 8 bytes required
    by the IPv6 Fragmentation extension header).
    
    This in turn results in rather inefficient transmission, as every
    transmitted TCP segment now is split in two fragments containing
    1232+8 bytes of payload.
    
    After this patch, all the outgoing packets that includes a
    Fragmentation header all are "atomic" or "non-fragmented" fragments,
    i.e., they both have Offset=0 and More Fragments=0.
    
    With help from David S. Miller
    
    Reported-by: Tore Anderson <tore@fud.no>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Maciej Żenczykowski <maze@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Tested-by: Tore Anderson <tore@fud.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index fc880e92164a..0fb84de6da36 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -544,8 +544,8 @@ extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 
 extern void tcp_initialize_rcv_mss(struct sock *sk);
 
-extern int tcp_mtu_to_mss(const struct sock *sk, int pmtu);
-extern int tcp_mss_to_mtu(const struct sock *sk, int mss);
+extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
+extern int tcp_mss_to_mtu(struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
 extern void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt);
 

commit 900f65d361d333c949ef76a828343075f4fdf523
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu Apr 19 09:55:21 2012 +0000

    tcp: move duplicate code from tcp_v4_init_sock()/tcp_v6_init_sock()
    
    This commit moves the (substantial) common code shared between
    tcp_v4_init_sock() and tcp_v6_init_sock() to a new address-family
    independent function, tcp_init_sock().
    
    Centralizing this functionality should help avoid drift issues,
    e.g. where the IPv4 side is updated without a corresponding update to
    IPv6. There was already some drift: IPv4 initialized snd_cwnd to
    TCP_INIT_CWND, while the IPv6 side was still initializing snd_cwnd to
    2 (in this case it should not matter, since snd_cwnd is also
    initialized in tcp_init_metrics(), but the general risks and
    maintenance overhead remain).
    
    When diffing the old and new code, note that new tcp_init_sock()
    function uses the order of steps from the tcp_v4_init_sock()
    implementation (the order is slightly different in
    tcp_v6_init_sock()).
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b4ccb8a6c9c1..fc880e92164a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -395,6 +395,7 @@ extern void tcp_enter_loss(struct sock *sk, int how);
 extern void tcp_clear_retrans(struct tcp_sock *tp);
 extern void tcp_update_metrics(struct sock *sk);
 extern void tcp_close(struct sock *sk, long timeout);
+extern void tcp_init_sock(struct sock *sk);
 extern unsigned int tcp_poll(struct file * file, struct socket *sock,
 			     struct poll_table_struct *wait);
 extern int tcp_getsockopt(struct sock *sk, int level, int optname,

commit ee9952831cfd0bbe834f4a26489d7dce74582e37
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Apr 19 03:40:39 2012 +0000

    tcp: Initial repair mode
    
    This includes (according the the previous description):
    
    * TCP_REPAIR sockoption
    
    This one just puts the socket in/out of the repair mode.
    Allowed for CAP_NET_ADMIN and for closed/establised sockets only.
    When repair mode is turned off and the socket happens to be in
    the established state the window probe is sent to the peer to
    'unlock' the connection.
    
    * TCP_REPAIR_QUEUE sockoption
    
    This one sets the queue which we're about to repair. The
    'no-queue' is set by default.
    
    * TCP_QUEUE_SEQ socoption
    
    Sets the write_seq/rcv_nxt of a selected repaired queue.
    Allowed for TCP_CLOSE-d sockets only. When the socket changes
    its state the other seq-s are changed by the kernel according
    to the protocol rules (most of the existing code is actually
    reused).
    
    * Ability to forcibly bind a socket to a port
    
    The sk->sk_reuse is set to SK_FORCE_REUSE.
    
    * Immediate connect modification
    
    The connect syscall initializes the connection, then directly jumps
    to the code which finalizes it.
    
    * Silent close modification
    
    The close just aborts the connection (similar to SO_LINGER with 0
    time) but without sending any FIN/RST-s to peer.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 633fde27f24b..b4ccb8a6c9c1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -612,6 +612,8 @@ static inline u32 tcp_receive_window(const struct tcp_sock *tp)
  */
 extern u32 __tcp_select_window(struct sock *sk);
 
+void tcp_send_window_probe(struct sock *sk);
+
 /* TCP timestamps are only 32-bits, this causes a slight
  * complication on 64-bit systems since we store a snapshot
  * of jiffies in the buffer control blocks below.  We decided

commit 370816aef0c5436c2adbec3966038f36ca326933
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Apr 19 03:40:01 2012 +0000

    tcp: Move code around
    
    This is just the preparation patch, which makes the needed for
    TCP repair code ready for use.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d5984e353826..633fde27f24b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -435,6 +435,9 @@ extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
 					struct request_values *rvp);
 extern int tcp_disconnect(struct sock *sk, int flags);
 
+void tcp_connect_init(struct sock *sk);
+void tcp_finish_connect(struct sock *sk, struct sk_buff *skb);
+void tcp_queue_rcv(struct sock *sk, struct sk_buff *skb, int hdrlen);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];

commit f4f9f6e75d67ddfcfea79a2108217bc654aef3af
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Apr 16 07:08:06 2012 +0000

    tcp: restore formatting of macros for tcp_skb_cb sacked field
    
    Commit b82d1bb4 inadvertendly placed unrelated new code between
    TCPCB_EVER_RETRANS and TCPCB_RETRANS and the other macros that refer
    to the sacked field in the struct tcp_skb_cb (probably because there
    was a misleading empty line there). This commit fixes up the
    formatting so that all macros related to the sacked field are adjacent
    again.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 025c31aa65df..d5984e353826 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -645,16 +645,17 @@ struct tcp_skb_cb {
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
 	__u32		when;		/* used to compute rtt's	*/
 	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
+
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
 #define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
 #define TCPCB_LOST		0x04	/* SKB is lost			*/
 #define TCPCB_TAGBITS		0x07	/* All tag bits			*/
-	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
-	/* 1 byte hole */
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
 
+	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
+	/* 1 byte hole */
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 };
 

commit 95c961747284a6b83a5e2d81240e214b0fa3464d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun Apr 15 05:58:06 2012 +0000

    net: cleanup unsigned to unsigned int
    
    Use of "unsigned int" is preferred to bare "unsigned" in net tree.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 057f0168462e..025c31aa65df 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1226,7 +1226,7 @@ extern void tcp_put_md5sig_pool(void);
 
 extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
 extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
-				 unsigned header_len);
+				 unsigned int header_len);
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
 			    const struct tcp_md5sig_key *key);
 

commit fd4f2cead6983735a4e6283126b9276873d7ff09
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Apr 12 19:48:40 2012 +0000

    tcp: RFC6298 supersedes RFC2988bis
    
    Updates some comments to track RFC6298
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: H.K. Jerry Chu <hkchu@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f75a04d752cb..057f0168462e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -123,7 +123,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #endif
 #define TCP_RTO_MAX	((unsigned)(120*HZ))
 #define TCP_RTO_MIN	((unsigned)(HZ/5))
-#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))	/* RFC2988bis initial RTO value	*/
+#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))	/* RFC6298 2.1 initial RTO value	*/
 #define TCP_TIMEOUT_FALLBACK ((unsigned)(3*HZ))	/* RFC 1122 initial RTO value, now
 						 * used as a fallback RTO for the
 						 * initial data transmission if no

commit ed2d265d1266736bd294332d7f649003943ae36e
Merge: f1d38e423a69 6c03438edeb5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 24 10:08:39 2012 -0700

    Merge tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull <linux/bug.h> cleanup from Paul Gortmaker:
     "The changes shown here are to unify linux's BUG support under the one
      <linux/bug.h> file.  Due to historical reasons, we have some BUG code
      in bug.h and some in kernel.h -- i.e.  the support for BUILD_BUG in
      linux/kernel.h predates the addition of linux/bug.h, but old code in
      kernel.h wasn't moved to bug.h at that time.  As a band-aid, kernel.h
      was including <asm/bug.h> to pseudo link them.
    
      This has caused confusion[1] and general yuck/WTF[2] reactions.  Here
      is an example that violates the principle of least surprise:
    
          CC      lib/string.o
          lib/string.c: In function 'strlcat':
          lib/string.c:225:2: error: implicit declaration of function 'BUILD_BUG_ON'
          make[2]: *** [lib/string.o] Error 1
          $
          $ grep linux/bug.h lib/string.c
          #include <linux/bug.h>
          $
    
      We've included <linux/bug.h> for the BUG infrastructure and yet we
      still get a compile fail! [We've not kernel.h for BUILD_BUG_ON.] Ugh -
      very confusing for someone who is new to kernel development.
    
      With the above in mind, the goals of this changeset are:
    
      1) find and fix any include/*.h files that were relying on the
         implicit presence of BUG code.
      2) find and fix any C files that were consuming kernel.h and hence
         relying on implicitly getting some/all BUG code.
      3) Move the BUG related code living in kernel.h to <linux/bug.h>
      4) remove the asm/bug.h from kernel.h to finally break the chain.
    
      During development, the order was more like 3-4, build-test, 1-2.  But
      to ensure that git history for bisect doesn't get needless build
      failures introduced, the commits have been reorderd to fix the problem
      areas in advance.
    
            [1]  https://lkml.org/lkml/2012/1/3/90
            [2]  https://lkml.org/lkml/2012/1/17/414"
    
    Fix up conflicts (new radeon file, reiserfs header cleanups) as per Paul
    and linux-next.
    
    * tag 'bug-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      kernel.h: doesn't explicitly use bug.h, so don't include it.
      bug: consolidate BUILD_BUG_ON with other bug code
      BUG: headers with BUG/BUG_ON etc. need linux/bug.h
      bug.h: add include of it to various implicit C users
      lib: fix implicit users of kernel.h for TAINT_WARN
      spinlock: macroize assert_spin_locked to avoid bug.h dependency
      x86: relocate get/set debugreg fcns to include/asm/debugreg.

commit 187f1882b5b0748b3c4c22274663fdb372ac0452
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 23 20:12:59 2011 -0500

    BUG: headers with BUG/BUG_ON etc. need linux/bug.h
    
    If a header file is making use of BUG, BUG_ON, BUILD_BUG_ON, or any
    other BUG variant in a static inline (i.e. not in a #define) then
    that header really should be including <linux/bug.h> and not just
    expecting it to be implicitly present.
    
    We can make this change risk-free, since if the files using these
    headers didn't have exposure to linux/bug.h already, they would have
    been causing compile failures/warnings.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 42c29bfbcee3..ad8d0a865551 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -22,6 +22,7 @@
 
 #include <linux/list.h>
 #include <linux/tcp.h>
+#include <linux/bug.h>
 #include <linux/slab.h>
 #include <linux/cache.h>
 #include <linux/percpu.h>

commit b4017c5368f992fb8fb3a2545a0977082c6664e4
Merge: 97767a87f3be 413708bbaf5c
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Mar 1 17:57:40 2012 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/broadcom/tg3.c
    
    Conflicts in the statistics regression bug fix from 'net',
    but happily Matt Carlson originally posted the fix against
    'net-next' so I used that to resolve this.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ecb971923614775a118bc05ad16b2bde450cac7d
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Feb 27 17:52:52 2012 -0500

    tcp: fix comment for tp->highest_sack
    
    There was an off-by-one error in the comments describing the
    highest_sack field in struct tcp_sock. The comments previously claimed
    that it was the "start sequence of the highest skb with SACKed
    bit". This commit fixes the comments to note that it is the "start
    sequence of the skb just *after* the highest skb with SACKed bit".
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 42c29bfbcee3..2d80c291fffb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1364,8 +1364,9 @@ static inline void tcp_push_pending_frames(struct sock *sk)
 	}
 }
 
-/* Start sequence of the highest skb with SACKed bit, valid only if
- * sacked > 0 or when the caller has ensured validity by itself.
+/* Start sequence of the skb just after the highest skb with SACKed
+ * bit, valid only if sacked_out > 0 or when the caller has ensured
+ * validity by itself.
  */
 static inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)
 {

commit dd48dc34fe7639a8b2e22d8b609672f5f81aa7cb
Merge: 8d9eb069eafc 5962b35c1de3
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Feb 4 16:39:32 2012 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit efcdbf24fd5daa88060869e51ed49f68b7ac8708
Author: Arun Sharma <asharma@fb.com>
Date:   Mon Jan 30 14:16:06 2012 -0800

    net: Disambiguate kernel message
    
    Some of our machines were reporting:
    
    TCP: too many of orphaned sockets
    
    even when the number of orphaned sockets was well below the
    limit.
    
    We print a different message depending on whether we're out
    of TCP memory or there are too many orphaned sockets.
    
    Also move the check out of line and cleanup the messages
    that were printed.
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Suggested-by: Mohan Srinivasan <mohan@fb.com>
    Cc: netdev@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: David Miller <davem@davemloft.net>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d49db0113a06..42c29bfbcee3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -273,6 +273,14 @@ static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)
 	return seq3 - seq2 >= seq1 - seq2;
 }
 
+static inline bool tcp_out_of_memory(struct sock *sk)
+{
+	if (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
+	    sk_memory_allocated(sk) > sk_prot_mem_limits(sk, 2))
+		return true;
+	return false;
+}
+
 static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 {
 	struct percpu_counter *ocp = sk->sk_prot->orphan_count;
@@ -283,13 +291,11 @@ static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 		if (orphans << shift > sysctl_tcp_max_orphans)
 			return true;
 	}
-
-	if (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
-	    sk_memory_allocated(sk) > sk_prot_mem_limits(sk, 2))
-		return true;
 	return false;
 }
 
+extern bool tcp_check_oom(struct sock *sk, int shift);
+
 /* syncookies: remember time of last synqueue overflow */
 static inline void tcp_synq_overflow(struct sock *sk)
 {

commit a8afca032998850ec63e83d555cdcf0eb5680cd6
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 31 18:45:40 2012 +0000

    tcp: md5: protects md5sig_info with RCU
    
    This patch makes sure we use appropriate memory barriers before
    publishing tp->md5sig_info, allowing tcp_md5_do_lookup() being used from
    tcp_v4_send_reset() without holding socket lock (upcoming patch from
    Shawn Lu)
    
    Note we also need to respect rcu grace period before its freeing, since
    we can free socket without this grace period thanks to
    SLAB_DESTROY_BY_RCU
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Shawn Lu <shawn.lu@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 10ae4c7b6b4f..78880ba0f560 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1150,6 +1150,7 @@ struct tcp_md5sig_key {
 /* - sock block */
 struct tcp_md5sig_info {
 	struct hlist_head	head;
+	struct rcu_head		rcu;
 };
 
 /* - pseudo header */

commit a915da9b69273815527ccb3789421cb7027b545b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 31 05:18:33 2012 +0000

    tcp: md5: rcu conversion
    
    In order to be able to support proper RST messages for TCP MD5 flows, we
    need to allow access to MD5 keys without locking listener socket.
    
    This conversion is a nice cleanup, and shrinks size of timewait sockets
    by 80 bytes.
    
    IPv6 code reuses generic code found in IPv4 instead of duplicating it.
    
    Control path uses GFP_KERNEL allocations instead of GFP_ATOMIC.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Shawn Lu <shawn.lu@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c903462adf5..10ae4c7b6b4f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1130,35 +1130,26 @@ static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
 /* MD5 Signature */
 struct crypto_hash;
 
+union tcp_md5_addr {
+	struct in_addr  a4;
+#if IS_ENABLED(CONFIG_IPV6)
+	struct in6_addr	a6;
+#endif
+};
+
 /* - key database */
 struct tcp_md5sig_key {
-	u8			*key;
+	struct hlist_node	node;
 	u8			keylen;
-};
-
-struct tcp4_md5sig_key {
-	struct tcp_md5sig_key	base;
-	__be32			addr;
-};
-
-struct tcp6_md5sig_key {
-	struct tcp_md5sig_key	base;
-#if 0
-	u32			scope_id;	/* XXX */
-#endif
-	struct in6_addr		addr;
+	u8			family; /* AF_INET or AF_INET6 */
+	union tcp_md5_addr	addr;
+	u8			key[TCP_MD5SIG_MAXKEYLEN];
+	struct rcu_head		rcu;
 };
 
 /* - sock block */
 struct tcp_md5sig_info {
-	struct tcp4_md5sig_key	*keys4;
-#if IS_ENABLED(CONFIG_IPV6)
-	struct tcp6_md5sig_key	*keys6;
-	u32			entries6;
-	u32			alloced6;
-#endif
-	u32			entries4;
-	u32			alloced4;
+	struct hlist_head	head;
 };
 
 /* - pseudo header */
@@ -1195,19 +1186,25 @@ extern int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
 			       const struct sock *sk,
 			       const struct request_sock *req,
 			       const struct sk_buff *skb);
-extern struct tcp_md5sig_key * tcp_v4_md5_lookup(struct sock *sk,
-						 struct sock *addr_sk);
-extern int tcp_v4_md5_do_add(struct sock *sk, __be32 addr, u8 *newkey,
-			     u8 newkeylen);
-extern int tcp_v4_md5_do_del(struct sock *sk, __be32 addr);
+extern int tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,
+			  int family, const u8 *newkey,
+			  u8 newkeylen, gfp_t gfp);
+extern int tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,
+			  int family);
+extern struct tcp_md5sig_key *tcp_v4_md5_lookup(struct sock *sk,
+					 struct sock *addr_sk);
 
 #ifdef CONFIG_TCP_MD5SIG
-#define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_keylen ? 		 \
-				 &(struct tcp_md5sig_key) {		 \
-					.key = (twsk)->tw_md5_key,	 \
-					.keylen = (twsk)->tw_md5_keylen, \
-				} : NULL)
+extern struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
+			const union tcp_md5_addr *addr, int family);
+#define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_key)
 #else
+static inline struct tcp_md5sig_key *tcp_md5_do_lookup(struct sock *sk,
+					 const union tcp_md5_addr *addr,
+					 int family)
+{
+	return NULL;
+}
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 

commit a2d91241a80ec9bbc5ab24b9a2c4d730b3fa5730
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 31 01:04:42 2012 +0000

    tcp: md5: remove obsolete md5_add() method
    
    We no longer use md5_add() method from struct tcp_sock_af_ops
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0118ea999f67..3c903462adf5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1462,10 +1462,6 @@ struct tcp_sock_af_ops {
 						  const struct sock *sk,
 						  const struct request_sock *req,
 						  const struct sk_buff *skb);
-	int			(*md5_add) (struct sock *sk,
-					    struct sock *addr_sk,
-					    u8 *newkey,
-					    u8 len);
 	int			(*md5_parse) (struct sock *sk,
 					      char __user *optval,
 					      int optlen);

commit 4acb41903b2f99f3dffd4c3df9acc84ca5942cb2
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Jan 30 01:20:17 2012 +0000

    net/tcp: Fix tcp memory limits initialization when !CONFIG_SYSCTL
    
    sysctl_tcp_mem() initialization was moved to sysctl_tcp_ipv4.c
    in commit 3dc43e3e4d0b52197d3205214fe8f162f9e0c334, since it
    became a per-ns value.
    
    That code, however, will never run when CONFIG_SYSCTL is
    disabled, leading to bogus values on those fields - causing hung
    TCP sockets.
    
    This patch fixes it by keeping an initialization code in
    tcp_init(). It will be overwritten by the first net namespace
    init if CONFIG_SYSCTL is compiled in, and do the right thing if
    it is compiled out.
    
    It is also named properly as tcp_init_mem(), to properly signal
    its non-sysctl side effect on TCP limits.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: David S. Miller <davem@davemloft.net>
    Link: http://lkml.kernel.org/r/4F22D05A.8030604@parallels.com
    [ renamed the function, tidied up the changelog a bit ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0118ea999f67..d49db0113a06 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -311,6 +311,8 @@ extern struct proto tcp_prot;
 #define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
+extern void tcp_init_mem(struct net *net);
+
 extern void tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void tcp_shutdown (struct sock *sk, int how);

commit ab56222a32b9dbaae19c1d37f07b0ac4fc3c27ec
Author: Vijay Subramanian <subramanian.vijay@gmail.com>
Date:   Tue Dec 20 13:23:24 2011 +0000

    tcp: Replace constants with #define macros
    
    to record the state of SACK/FACK and DSACK for better readability and maintenance.
    
    Signed-off-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a4f52e154843..0118ea999f67 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -773,12 +773,12 @@ static inline int tcp_is_reno(const struct tcp_sock *tp)
 
 static inline int tcp_is_fack(const struct tcp_sock *tp)
 {
-	return tp->rx_opt.sack_ok & 2;
+	return tp->rx_opt.sack_ok & TCP_FACK_ENABLED;
 }
 
 static inline void tcp_enable_fack(struct tcp_sock *tp)
 {
-	tp->rx_opt.sack_ok |= 2;
+	tp->rx_opt.sack_ok |= TCP_FACK_ENABLED;
 }
 
 static inline unsigned int tcp_left_out(const struct tcp_sock *tp)

commit 3dc43e3e4d0b52197d3205214fe8f162f9e0c334
Author: Glauber Costa <glommer@parallels.com>
Date:   Sun Dec 11 21:47:05 2011 +0000

    per-netns ipv4 sysctl_tcp_mem
    
    This patch allows each namespace to independently set up
    its levels for tcp memory pressure thresholds. This patch
    alone does not buy much: we need to make this values
    per group of process somehow. This is achieved in the
    patches that follows in this patchset.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 913473b4eda7..a4f52e154843 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -230,7 +230,6 @@ extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_ecn;
 extern int sysctl_tcp_dsack;
-extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
 extern int sysctl_tcp_app_win;

commit 180d8cd942ce336b2c869d324855c40c5db478ad
Author: Glauber Costa <glommer@parallels.com>
Date:   Sun Dec 11 21:47:02 2011 +0000

    foundations of per-cgroup memory pressure controlling.
    
    This patch replaces all uses of struct sock fields' memory_pressure,
    memory_allocated, sockets_allocated, and sysctl_mem to acessor
    macros. Those macros can either receive a socket argument, or a mem_cgroup
    argument, depending on the context they live in.
    
    Since we're only doing a macro wrapping here, no performance impact at all is
    expected in the case where we don't have cgroups disabled.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Hiroyouki Kamezawa <kamezawa.hiroyu@jp.fujitsu.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: Eric W. Biederman <ebiederm@xmission.com>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 02f070d339ba..913473b4eda7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -44,6 +44,7 @@
 #include <net/dst.h>
 
 #include <linux/seq_file.h>
+#include <linux/memcontrol.h>
 
 extern struct inet_hashinfo tcp_hashinfo;
 
@@ -285,7 +286,7 @@ static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 	}
 
 	if (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
-	    atomic_long_read(&tcp_memory_allocated) > sysctl_tcp_mem[2])
+	    sk_memory_allocated(sk) > sk_prot_mem_limits(sk, 2))
 		return true;
 	return false;
 }

commit dfd56b8b38fff3586f36232db58e1e9f7885a605
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sat Dec 10 09:48:31 2011 +0000

    net: use IS_ENABLED(CONFIG_IPV6)
    
    Instead of testing defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 87e3c80bfa00..02f070d339ba 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -628,7 +628,7 @@ extern u32 __tcp_select_window(struct sock *sk);
 struct tcp_skb_cb {
 	union {
 		struct inet_skb_parm	h4;
-#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
+#if IS_ENABLED(CONFIG_IPV6)
 		struct inet6_skb_parm	h6;
 #endif
 	} header;	/* For incoming frames		*/
@@ -1152,7 +1152,7 @@ struct tcp6_md5sig_key {
 /* - sock block */
 struct tcp_md5sig_info {
 	struct tcp4_md5sig_key	*keys4;
-#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#if IS_ENABLED(CONFIG_IPV6)
 	struct tcp6_md5sig_key	*keys6;
 	u32			entries6;
 	u32			alloced6;
@@ -1179,7 +1179,7 @@ struct tcp6_pseudohdr {
 
 union tcp_md5sum_block {
 	struct tcp4_pseudohdr ip4;
-#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#if IS_ENABLED(CONFIG_IPV6)
 	struct tcp6_pseudohdr ip6;
 #endif
 };

commit 6b5a5c0dbb11dcff4e1b0f1ef87a723197948ed4
Author: Neal Cardwell <ncardwell@google.com>
Date:   Mon Nov 21 17:15:14 2011 +0000

    tcp: do not scale TSO segment size with reordering degree
    
    Since 2005 (c1b4a7e69576d65efc31a8cea0714173c2841244)
    tcp_tso_should_defer has been using tcp_max_burst() as a target limit
    for deciding how large to make outgoing TSO packets when not using
    sysctl_tcp_tso_win_divisor. But since 2008
    (dd9e0dda66ba38a2ddd1405ac279894260dc5c36) tcp_max_burst() returns the
    reordering degree. We should not have tcp_tso_should_defer attempt to
    build larger segments just because there is more reordering. This
    commit splits the notion of deferral size used in TSO from the notion
    of burst size used in cwnd moderation, and returns the TSO deferral
    limit to its original value.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 113160b84588..87e3c80bfa00 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -834,6 +834,14 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);
 
+/* The maximum number of MSS of available cwnd for which TSO defers
+ * sending if not using sysctl_tcp_tso_win_divisor.
+ */
+static inline __u32 tcp_max_tso_deferred_mss(const struct tcp_sock *tp)
+{
+	return 3;
+}
+
 /* Slow start with delack produces 3 packets of burst, so that
  * it is safe "de facto".  This will be the default - same as
  * the default reordering threshold - but if reordering increases,

commit c8f44affb7244f2ac3e703cab13d55ede27621bb
Author: Michał Mirosław <mirq-linux@rere.qmqm.pl>
Date:   Tue Nov 15 15:29:55 2011 +0000

    net: introduce and use netdev_features_t for device features sets
    
    v2:     add couple missing conversions in drivers
            split unexporting netdev_fix_features()
            implemented %pNF
            convert sock::sk_route_(no?)caps
    
    Signed-off-by: Michał Mirosław <mirq-linux@rere.qmqm.pl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bb18c4d69aba..113160b84588 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1430,7 +1430,8 @@ extern struct request_sock_ops tcp6_request_sock_ops;
 extern void tcp_v4_destroy_sock(struct sock *sk);
 
 extern int tcp_v4_gso_send_check(struct sk_buff *skb);
-extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, u32 features);
+extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb,
+				       netdev_features_t features);
 extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
 					struct sk_buff *skb);
 extern struct sk_buff **tcp4_gro_receive(struct sk_buff **head,

commit 73cb88ecb950ee67906d02354f781ea293bcf895
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Sun Oct 30 06:46:30 2011 +0000

    net: make the tcp and udp file_operations for the /proc stuff const
    
    the tcp and udp code creates a set of struct file_operations at runtime
    while it can also be done at compile time, with the added benefit of then
    having these file operations be const.
    
    the trickiest part was to get the "THIS_MODULE" reference right; the naive
    method of declaring a struct in the place of registration would not work
    for this reason.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e147f42d643d..bb18c4d69aba 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1403,11 +1403,13 @@ enum tcp_seq_states {
 	TCP_SEQ_STATE_TIME_WAIT,
 };
 
+int tcp_seq_open(struct inode *inode, struct file *file);
+
 struct tcp_seq_afinfo {
-	char			*name;
-	sa_family_t		family;
-	struct file_operations	seq_fops;
-	struct seq_operations	seq_ops;
+	char				*name;
+	sa_family_t			family;
+	const struct file_operations	*seq_fops;
+	struct seq_operations		seq_ops;
 };
 
 struct tcp_iter_state {

commit 78d81d15b74246c7cedf84894434890b33da3907
Author: Flavio Leitner <fbl@redhat.com>
Date:   Mon Oct 24 08:15:10 2011 +0000

    TCP: remove TCP_DEBUG
    
    It was enabled by default and the messages guarded
    by the define are useful.
    
    Signed-off-by: Flavio Leitner <fbl@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ed0e81452827..e147f42d643d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -18,7 +18,6 @@
 #ifndef _TCP_H
 #define _TCP_H
 
-#define TCP_DEBUG 1
 #define FASTRETRANS_DEBUG 1
 
 #include <linux/list.h>

commit 318cf7aaa0a6d20ecf6be33eb771291e5ff2e3b9
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Oct 24 02:46:04 2011 -0400

    tcp: md5: add more const attributes
    
    Now tcp_md5_hash_header() has a const tcphdr argument, we can add more
    const attributes to callers.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 910cc29f9e97..ed0e81452827 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1185,8 +1185,9 @@ struct tcp_md5sig_pool {
 
 /* - functions */
 extern int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
-			       struct sock *sk, struct request_sock *req,
-			       struct sk_buff *skb);
+			       const struct sock *sk,
+			       const struct request_sock *req,
+			       const struct sk_buff *skb);
 extern struct tcp_md5sig_key * tcp_v4_md5_lookup(struct sock *sk,
 						 struct sock *addr_sk);
 extern int tcp_v4_md5_do_add(struct sock *sk, __be32 addr, u8 *newkey,
@@ -1448,9 +1449,9 @@ struct tcp_sock_af_ops {
 						struct sock *addr_sk);
 	int			(*calc_md5_hash) (char *location,
 						  struct tcp_md5sig_key *md5,
-						  struct sock *sk,
-						  struct request_sock *req,
-						  struct sk_buff *skb);
+						  const struct sock *sk,
+						  const struct request_sock *req,
+						  const struct sk_buff *skb);
 	int			(*md5_add) (struct sock *sk,
 					    struct sock *addr_sk,
 					    u8 *newkey,
@@ -1467,9 +1468,9 @@ struct tcp_request_sock_ops {
 						struct request_sock *req);
 	int			(*calc_md5_hash) (char *location,
 						  struct tcp_md5sig_key *md5,
-						  struct sock *sk,
-						  struct request_sock *req,
-						  struct sk_buff *skb);
+						  const struct sock *sk,
+						  const struct request_sock *req,
+						  const struct sk_buff *skb);
 #endif
 };
 

commit ca35a0ef85e8ed6df6d5ab01fb6c3530cca0c469
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Oct 24 01:52:35 2011 -0400

    tcp: md5: dont write skb head in tcp_md5_hash_header()
    
    tcp_md5_hash_header() writes into skb header a temporary zero value,
    this might confuse other users of this area.
    
    Since tcphdr is small (20 bytes), copy it in a temporary variable and
    make the change in the copy.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3edef0bebdd1..910cc29f9e97 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1209,7 +1209,7 @@ extern void tcp_free_md5sig_pool(void);
 extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
 extern void tcp_put_md5sig_pool(void);
 
-extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, struct tcphdr *);
+extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, const struct tcphdr *);
 extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
 				 unsigned header_len);
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,

commit cf533ea53ebfae41be15b103d78e7ebec30b9969
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Oct 21 05:22:42 2011 -0400

    tcp: add const qualifiers where possible
    
    Adding const qualifiers to pointers can ease code review, and spot some
    bugs. It might allow compiler to optimize code further.
    
    For example, is it legal to temporary write a null cksum into tcphdr
    in tcp_md5_hash_header() ? I am afraid a sniffer could catch the
    temporary null value...
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0113d306fcb0..3edef0bebdd1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -327,9 +327,9 @@ extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
 			size_t size, int flags);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
-				 struct tcphdr *th, unsigned len);
+				 const struct tcphdr *th, unsigned int len);
 extern int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
-			       struct tcphdr *th, unsigned len);
+			       const struct tcphdr *th, unsigned int len);
 extern void tcp_rcv_space_adjust(struct sock *sk);
 extern void tcp_cleanup_rbuf(struct sock *sk, int copied);
 extern int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
@@ -401,10 +401,10 @@ extern void tcp_set_keepalive(struct sock *sk, int val);
 extern void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
 extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t len, int nonblock, int flags, int *addr_len);
-extern void tcp_parse_options(struct sk_buff *skb,
-			      struct tcp_options_received *opt_rx, u8 **hvpp,
+extern void tcp_parse_options(const struct sk_buff *skb,
+			      struct tcp_options_received *opt_rx, const u8 **hvpp,
 			      int estab);
-extern u8 *tcp_parse_md5sig_option(struct tcphdr *th);
+extern const u8 *tcp_parse_md5sig_option(const struct tcphdr *th);
 
 /*
  *	TCP v4 functions exported for the inet6 API
@@ -450,7 +450,7 @@ extern bool cookie_check_timestamp(struct tcp_options_received *opt, bool *);
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 #ifdef CONFIG_SYN_COOKIES
-extern __u32 cookie_v6_init_sequence(struct sock *sk, struct sk_buff *skb,
+extern __u32 cookie_v6_init_sequence(struct sock *sk, const struct sk_buff *skb,
 				     __u16 *mss);
 #else
 static inline __u32 cookie_v6_init_sequence(struct sock *sk,
@@ -522,7 +522,7 @@ static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 }
 
 /* tcp.c */
-extern void tcp_get_info(struct sock *, struct tcp_info *);
+extern void tcp_get_info(const struct sock *, struct tcp_info *);
 
 /* Read 'sendfile()'-style from a TCP socket */
 typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
@@ -532,8 +532,8 @@ extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 
 extern void tcp_initialize_rcv_mss(struct sock *sk);
 
-extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
-extern int tcp_mss_to_mtu(struct sock *sk, int mss);
+extern int tcp_mtu_to_mss(const struct sock *sk, int pmtu);
+extern int tcp_mss_to_mtu(const struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
 extern void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt);
 
@@ -574,7 +574,7 @@ static inline void tcp_fast_path_check(struct sock *sk)
 /* Compute the actual rto_min value */
 static inline u32 tcp_rto_min(struct sock *sk)
 {
-	struct dst_entry *dst = __sk_dst_get(sk);
+	const struct dst_entry *dst = __sk_dst_get(sk);
 	u32 rto_min = TCP_RTO_MIN;
 
 	if (dst && dst_metric_locked(dst, RTAX_RTO_MIN))
@@ -820,6 +820,7 @@ static inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)
 static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 {
 	const struct tcp_sock *tp = tcp_sk(sk);
+
 	if ((1 << inet_csk(sk)->icsk_ca_state) & (TCPF_CA_CWR | TCPF_CA_Recovery))
 		return tp->snd_ssthresh;
 	else
@@ -832,7 +833,7 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
-extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
+extern __u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);
 
 /* Slow start with delack produces 3 packets of burst, so that
  * it is safe "de facto".  This will be the default - same as
@@ -861,7 +862,7 @@ static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
 
 static inline void tcp_check_probe_timer(struct sock *sk)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
+	const struct tcp_sock *tp = tcp_sk(sk);
 	const struct inet_connection_sock *icsk = inet_csk(sk);
 
 	if (!tp->packets_out && !icsk->icsk_pending)
@@ -1209,10 +1210,10 @@ extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
 extern void tcp_put_md5sig_pool(void);
 
 extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, struct tcphdr *);
-extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, struct sk_buff *,
+extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,
 				 unsigned header_len);
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
-			    struct tcp_md5sig_key *key);
+			    const struct tcp_md5sig_key *key);
 
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
@@ -1225,22 +1226,24 @@ static inline void tcp_write_queue_purge(struct sock *sk)
 	tcp_clear_all_retrans_hints(tcp_sk(sk));
 }
 
-static inline struct sk_buff *tcp_write_queue_head(struct sock *sk)
+static inline struct sk_buff *tcp_write_queue_head(const struct sock *sk)
 {
 	return skb_peek(&sk->sk_write_queue);
 }
 
-static inline struct sk_buff *tcp_write_queue_tail(struct sock *sk)
+static inline struct sk_buff *tcp_write_queue_tail(const struct sock *sk)
 {
 	return skb_peek_tail(&sk->sk_write_queue);
 }
 
-static inline struct sk_buff *tcp_write_queue_next(struct sock *sk, struct sk_buff *skb)
+static inline struct sk_buff *tcp_write_queue_next(const struct sock *sk,
+						   const struct sk_buff *skb)
 {
 	return skb_queue_next(&sk->sk_write_queue, skb);
 }
 
-static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_buff *skb)
+static inline struct sk_buff *tcp_write_queue_prev(const struct sock *sk,
+						   const struct sk_buff *skb)
 {
 	return skb_queue_prev(&sk->sk_write_queue, skb);
 }
@@ -1254,7 +1257,7 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
-static inline struct sk_buff *tcp_send_head(struct sock *sk)
+static inline struct sk_buff *tcp_send_head(const struct sock *sk)
 {
 	return sk->sk_send_head;
 }
@@ -1265,7 +1268,7 @@ static inline bool tcp_skb_is_last(const struct sock *sk,
 	return skb_queue_is_last(&sk->sk_write_queue, skb);
 }
 
-static inline void tcp_advance_send_head(struct sock *sk, struct sk_buff *skb)
+static inline void tcp_advance_send_head(struct sock *sk, const struct sk_buff *skb)
 {
 	if (tcp_skb_is_last(sk, skb))
 		sk->sk_send_head = NULL;

commit 4de075e0438ba54b8f42cbbc1263d404229dc997
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Sep 27 13:25:05 2011 -0400

    tcp: rename tcp_skb_cb flags
    
    Rename struct tcp_skb_cb "flags" to "tcp_flags" to ease code review and
    maintenance.
    
    Its content is a combination of FIN/SYN/RST/PSH/ACK/URG/ECE/CWR flags
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 28a9997d783a..0113d306fcb0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -636,7 +636,7 @@ struct tcp_skb_cb {
 	__u32		seq;		/* Starting sequence number	*/
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
 	__u32		when;		/* used to compute rtt's	*/
-	__u8		flags;		/* TCP header flags.		*/
+	__u8		tcp_flags;	/* TCP header flags. (tcp[13])	*/
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
 #define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/

commit b82d1bb4fd206ed305f9e955eeffc4a678149442
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Sep 27 02:20:08 2011 -0400

    tcp: unalias tcp_skb_cb flags and ip_dsfield
    
    struct tcp_skb_cb contains a "flags" field containing either tcp flags
    or IP dsfield depending on context (input or output path)
    
    Introduce ip_dsfield to make the difference clear and ease maintenance.
    If later we want to save space, we can union flags/ip_dsfield
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 702aefc8d43d..28a9997d783a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -642,7 +642,8 @@ struct tcp_skb_cb {
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
 #define TCPCB_LOST		0x04	/* SKB is lost			*/
 #define TCPCB_TAGBITS		0x07	/* All tag bits			*/
-
+	__u8		ip_dsfield;	/* IPv4 tos or IPv6 dsfield	*/
+	/* 1 byte hole */
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
 

commit 7a269ffad72f3604b8982fa09c387670e0d2ee14
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Sep 22 20:02:19 2011 +0000

    tcp: ECN blackhole should not force quickack mode
    
    While playing with a new ADSL box at home, I discovered that ECN
    blackhole can trigger suboptimal quickack mode on linux : We send one
    ACK for each incoming data frame, without any delay and eventual
    piggyback.
    
    This is because TCP_ECN_check_ce() considers that if no ECT is seen on a
    segment, this is because this segment was a retransmit.
    
    Refine this heuristic and apply it only if we seen ECT in a previous
    segment, to detect ECN blackhole at IP level.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Jamal Hadi Salim <jhs@mojatatu.com>
    CC: Jerry Chu <hkchu@google.com>
    CC: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    CC: Jim Gettys <jg@freedesktop.org>
    CC: Dave Taht <dave.taht@gmail.com>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f357befaaa01..702aefc8d43d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -356,6 +356,7 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 #define	TCP_ECN_OK		1
 #define	TCP_ECN_QUEUE_CWR	2
 #define	TCP_ECN_DEMAND_CWR	4
+#define	TCP_ECN_SEEN		8
 
 static __inline__ void
 TCP_ECN_create_request(struct request_sock *req, struct tcphdr *th)

commit 8decf868790b48a727d7e7ca164f2bcd3c1389c0
Merge: 3fc72370186b d93dc5c4478c
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Sep 22 03:23:13 2011 -0400

    Merge branch 'master' of github.com:davem330/net
    
    Conflicts:
            MAINTAINERS
            drivers/net/Kconfig
            drivers/net/ethernet/broadcom/bnx2x/bnx2x_link.c
            drivers/net/ethernet/broadcom/tg3.c
            drivers/net/wireless/iwlwifi/iwl-pci.c
            drivers/net/wireless/iwlwifi/iwl-trans-tx-pcie.c
            drivers/net/wireless/rt2x00/rt2800usb.c
            drivers/net/wireless/wl12xx/main.c

commit e05c82d3666119075615fdbf6abca0266344f27b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun Sep 18 21:02:55 2011 -0400

    tcp: fix build error if !CONFIG_SYN_COOKIES
    
    commit 946cedccbd7387 (tcp: Change possible SYN flooding messages)
    added a build error if CONFIG_SYN_COOKIES=n
    
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e9b48b094683..acc620a4a45f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -431,17 +431,34 @@ extern int tcp_disconnect(struct sock *sk, int flags);
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
 extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
 				    struct ip_options *opt);
+#ifdef CONFIG_SYN_COOKIES
 extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 				     __u16 *mss);
+#else
+static inline __u32 cookie_v4_init_sequence(struct sock *sk,
+					    struct sk_buff *skb,
+					    __u16 *mss)
+{
+	return 0;
+}
+#endif
 
 extern __u32 cookie_init_timestamp(struct request_sock *req);
 extern bool cookie_check_timestamp(struct tcp_options_received *opt, bool *);
 
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
+#ifdef CONFIG_SYN_COOKIES
 extern __u32 cookie_v6_init_sequence(struct sock *sk, struct sk_buff *skb,
 				     __u16 *mss);
-
+#else
+static inline __u32 cookie_v6_init_sequence(struct sock *sk,
+					    struct sk_buff *skb,
+					    __u16 *mss)
+{
+	return 0;
+}
+#endif
 /* tcp_output.c */
 
 extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,

commit 765cf9976e937f1cfe9159bf4534967c8bf8eb6d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Sep 12 20:28:37 2011 +0000

    tcp: md5: remove one indirection level in tcp_md5sig_pool
    
    tcp_md5sig_pool is currently an 'array' (a percpu object) of pointers to
    struct tcp_md5sig_pool. Only the pointers are NUMA aware, but objects
    themselves are all allocated on a single node.
    
    Remove this extra indirection to get proper percpu memory (NUMA aware)
    and make code simpler.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 149a415d1e0a..d6ca00072cdf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1180,7 +1180,7 @@ extern int tcp_v4_md5_do_del(struct sock *sk, __be32 addr);
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 
-extern struct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *);
+extern struct tcp_md5sig_pool __percpu *tcp_alloc_md5sig_pool(struct sock *);
 extern void tcp_free_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);

commit 946cedccbd7387488d2cee5da92cdfeb28d2e670
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Aug 30 03:21:44 2011 +0000

    tcp: Change possible SYN flooding messages
    
    "Possible SYN flooding on port xxxx " messages can fill logs on servers.
    
    Change logic to log the message only once per listener, and add two new
    SNMP counters to track :
    
    TCPReqQFullDoCookies : number of times a SYNCOOKIE was replied to client
    
    TCPReqQFullDrop : number of times a SYN request was dropped because
    syncookies were not enabled.
    
    Based on a prior patch from Tom Herbert, and suggestions from David.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 149a415d1e0a..e9b48b094683 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -460,6 +460,9 @@ extern int tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
 extern void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 extern int tcp_send_synack(struct sock *);
+extern int tcp_syn_flood_action(struct sock *sk,
+				const struct sk_buff *skb,
+				const char *proto);
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);

commit 9ad7c049f0f79c418e293b1b68cf10d68f54fcdb
Author: Jerry Chu <hkchu@google.com>
Date:   Wed Jun 8 11:08:38 2011 +0000

    tcp: RFC2988bis + taking RTT sample from 3WHS for the passive open side
    
    This patch lowers the default initRTO from 3secs to 1sec per
    RFC2988bis. It falls back to 3secs if the SYN or SYN-ACK packet
    has been retransmitted, AND the TCP timestamp option is not on.
    
    It also adds support to take RTT sample during 3WHS on the passive
    open side, just like its active open counterpart, and uses it, if
    valid, to seed the initRTO for the data transmission phase.
    
    The patch also resets ssthresh to its initial default at the
    beginning of the data transmission phase, and reduces cwnd to 1 if
    there has been MORE THAN ONE retransmission during 3WHS per RFC5681.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cda30ea354a2..149a415d1e0a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -122,7 +122,13 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #endif
 #define TCP_RTO_MAX	((unsigned)(120*HZ))
 #define TCP_RTO_MIN	((unsigned)(HZ/5))
-#define TCP_TIMEOUT_INIT ((unsigned)(3*HZ))	/* RFC 1122 initial RTO value	*/
+#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))	/* RFC2988bis initial RTO value	*/
+#define TCP_TIMEOUT_FALLBACK ((unsigned)(3*HZ))	/* RFC 1122 initial RTO value, now
+						 * used as a fallback RTO for the
+						 * initial data transmission if no
+						 * valid RTT sample has been acquired,
+						 * most likely due to retrans in 3WHS.
+						 */
 
 #define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U)) /* Maximal interval between probes
 					                 * for local resources.
@@ -295,7 +301,7 @@ static inline void tcp_synq_overflow(struct sock *sk)
 static inline int tcp_synq_no_recent_overflow(const struct sock *sk)
 {
 	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
-	return time_after(jiffies, last_overflow + TCP_TIMEOUT_INIT);
+	return time_after(jiffies, last_overflow + TCP_TIMEOUT_FALLBACK);
 }
 
 extern struct proto tcp_prot;
@@ -508,6 +514,7 @@ extern void tcp_initialize_rcv_mss(struct sock *sk);
 extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
 extern int tcp_mss_to_mtu(struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
+extern void tcp_valid_rtt_meas(struct sock *sk, u32 seq_rtt);
 
 static inline void tcp_bound_rto(const struct sock *sk)
 {

commit 089c34827e52346f0303d1e6a7b744c1f4da3095
Author: Shan Wei <shanwei@cn.fujitsu.com>
Date:   Sat Feb 19 21:55:45 2011 +0000

    tcp: Remove debug macro of TCP_CHECK_TIMER
    
    Now, TCP_CHECK_TIMER is not used for debuging, it does nothing.
    And, it has been there for several years, maybe 6 years.
    
    Remove it to keep code clearer.
    
    Signed-off-by: Shan Wei <shanwei@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index adfe6dbe9053..cda30ea354a2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1068,8 +1068,6 @@ static inline int tcp_paws_reject(const struct tcp_options_received *rx_opt,
 	return 1;
 }
 
-#define TCP_CHECK_TIMER(sk) do { } while (0)
-
 static inline void tcp_mib_init(struct net *net)
 {
 	/* See RFC 2012 */

commit 7eb38527c4e485923fa3f87d11ce11b4e6ebf807
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Feb 5 18:13:45 2011 -0800

    tcp: Add reference to initial CWND ietf draft.
    
    Suggested by Alexander Zimmermann
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7118668ad534..adfe6dbe9053 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -196,7 +196,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* TCP thin-stream limits */
 #define TCP_THIN_LINEAR_RETRIES 6       /* After 6 linear retries, do exp. backoff */
 
-/* TCP initial congestion window */
+/* TCP initial congestion window as per draft-hkchu-tcpm-initcwnd-01 */
 #define TCP_INIT_CWND		10
 
 extern struct inet_timewait_death_row tcp_death_row;

commit 442b9635c569fef038d5367a7acd906db4677ae1
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Feb 2 17:05:11 2011 -0800

    tcp: Increase the initial congestion window to 10.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Nandita Dukkipati <nanditad@google.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 917911165e3b..7118668ad534 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -196,6 +196,9 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* TCP thin-stream limits */
 #define TCP_THIN_LINEAR_RETRIES 6       /* After 6 linear retries, do exp. backoff */
 
+/* TCP initial congestion window */
+#define TCP_INIT_CWND		10
+
 extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */
@@ -799,15 +802,6 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
-/*
- * Convert RFC 3390 larger initial window into an equivalent number of packets.
- * This is based on the numbers specified in RFC 5681, 3.1.
- */
-static inline u32 rfc3390_bytes_to_packets(const u32 smss)
-{
-	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
-}
-
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 

commit 04ed3e741d0f133e02bed7fa5c98edba128f90e7
Author: Michał Mirosław <mirq-linux@rere.qmqm.pl>
Date:   Mon Jan 24 15:32:47 2011 -0800

    net: change netdev->features to u32
    
    Quoting Ben Hutchings: we presumably won't be defining features that
    can only be enabled on 64-bit architectures.
    
    Occurences found by `grep -r` on net/, drivers/net, include/
    
    [ Move features and vlan_features next to each other in
      struct netdev, as per Eric Dumazet's suggestion -DaveM ]
    
    Signed-off-by: Michał Mirosław <mirq-linux@rere.qmqm.pl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 38509f047382..917911165e3b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1404,7 +1404,7 @@ extern struct request_sock_ops tcp6_request_sock_ops;
 extern void tcp_v4_destroy_sock(struct sock *sk);
 
 extern int tcp_v4_gso_send_check(struct sk_buff *skb);
-extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features);
+extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, u32 features);
 extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
 					struct sk_buff *skb);
 extern struct sk_buff **tcp4_gro_receive(struct sk_buff **head,

commit 356f039822b8d802138f7121c80d2a9286976dbd
Author: Nandita Dukkipati <nanditad@google.com>
Date:   Mon Dec 20 14:15:56 2010 +0000

    TCP: increase default initial receive window.
    
    This patch changes the default initial receive window to 10 mss
    (defined constant). The default window is limited to the maximum
    of 10*1460 and 2*mss (when mss > 1460).
    
    draft-ietf-tcpm-initcwnd-00 is a proposal to the IETF that recommends
    increasing TCP's initial congestion window to 10 mss or about 15KB.
    Leading up to this proposal were several large-scale live Internet
    experiments with an initial congestion window of 10 mss (IW10), where
    we showed that the average latency of HTTP responses improved by
    approximately 10%. This was accompanied by a slight increase in
    retransmission rate (0.5%), most of which is coming from applications
    opening multiple simultaneous connections. To understand the extreme
    worst case scenarios, and fairness issues (IW10 versus IW3), we further
    conducted controlled testbed experiments. We came away finding minimal
    negative impact even under low link bandwidths (dial-ups) and small
    buffers.  These results are extremely encouraging to adopting IW10.
    
    However, an initial congestion window of 10 mss is useless unless a TCP
    receiver advertises an initial receive window of at least 10 mss.
    Fortunately, in the large-scale Internet experiments we found that most
    widely used operating systems advertised large initial receive windows
    of 64KB, allowing us to experiment with a wide range of initial
    congestion windows. Linux systems were among the few exceptions that
    advertised a small receive window of 6KB. The purpose of this patch is
    to fix this shortcoming.
    
    References:
    1. A comprehensive list of all IW10 references to date.
    http://code.google.com/speed/protocols/tcpm-IW10.html
    
    2. Paper describing results from large-scale Internet experiments with IW10.
    http://ccr.sigcomm.org/drupal/?q=node/621
    
    3. Controlled testbed experiments under worst case scenarios and a
    fairness study.
    http://www.ietf.org/proceedings/79/slides/tcpm-0.pdf
    
    4. Raw test data from testbed experiments (Linux senders/receivers)
    with initial congestion and receive windows of both 10 mss.
    http://research.csc.ncsu.edu/netsrv/?q=content/iw10
    
    5. Internet-Draft. Increasing TCP's Initial Window.
    https://datatracker.ietf.org/doc/draft-ietf-tcpm-initcwnd/
    
    Signed-off-by: Nandita Dukkipati <nanditad@google.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b4480300cadf..38509f047382 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -60,6 +60,9 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
  */
 #define MAX_TCP_WINDOW		32767U
 
+/* Offer an initial receive window of 10 mss. */
+#define TCP_DEFAULT_INIT_RCVWND	10
+
 /* Minimal accepted MSS. It is (60+60+8) - (20+20). */
 #define TCP_MIN_MSS		88U
 

commit 4c306a9291a077879fc3e933326caac3bc319caa
Author: Shan Wei <shanwei@cn.fujitsu.com>
Date:   Sun Dec 19 21:59:35 2010 -0800

    net: kill unused macros
    
    These macros never be used, so remove them.
    
    Signed-off-by: Shan Wei <shanwei@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2ab6c9c1c53a..b4480300cadf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1158,8 +1158,6 @@ struct tcp_md5sig_pool {
 	union tcp_md5sum_block	md5_blk;
 };
 
-#define TCP_MD5SIG_MAXKEYS	(~(u32)0)	/* really?! */
-
 /* - functions */
 extern int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
 			       struct sock *sk, struct request_sock *req,

commit bc2ce894e113ed95b92541134b002fdc641e8080
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Dec 16 14:08:34 2010 -0800

    tcp: relax tcp_paws_check()
    
    Some windows versions have wrong RFC1323 implementations, with SYN and
    SYNACKS messages containing zero tcp timestamps.
    
    We relaxed in commit fc1ad92dfc4e363 the passive connection case
    (Windows connects to a linux machine), but the reverse case (linux
    connects to a Windows machine) has an analogue problem when tsvals from
    windows machine are 'negative' (high order bit set) : PAWS triggers and
    we drops incoming messages.
    
    Fix this by making zero ts_recent value special, allowing frame to be
    processed.
    
    Based on a report and initial patch from Dmitiy Balakin
    
    Bugzilla reference : https://bugzilla.kernel.org/show_bug.cgi?id=24842
    
    Reported-by: dmitriy.balakin@nicneiron.ru
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3f227baee4be..2ab6c9c1c53a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1038,7 +1038,13 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt,
 		return 1;
 	if (unlikely(get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS))
 		return 1;
-
+	/*
+	 * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
+	 * then following tcp messages have valid values. Ignore 0 value,
+	 * or else 'negative' tsval might forbid us to accept their packets.
+	 */
+	if (!rx_opt->ts_recent)
+		return 1;
 	return 0;
 }
 

commit dca9b2404a6d6579828da2425c051462701efd3f
Author: Shan Wei <shanwei@cn.fujitsu.com>
Date:   Wed Dec 1 18:05:17 2010 +0000

    net: kill unused macros from head file
    
    These macros have been defined for several years since v2.6.12-rc2（tracing by git）,
    but never be used. So remove them.
    
    Signed-off-by: Shan Wei <shanwei@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4097320caa25..3f227baee4be 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -100,12 +100,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCP_SYNACK_RETRIES 5	/* number of times to retry passive opening a
 				 * connection: ~180sec is RFC minimum	*/
 
-
-#define TCP_ORPHAN_RETRIES 7	/* number of times to retry on an orphaned
-				 * socket. 7 is ~50sec-16min.
-				 */
-
-
 #define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT
 				  * state, about 60 seconds	*/
 #define TCP_FIN_TIMEOUT	TCP_TIMEWAIT_LEN

commit ccb7c410ddc054b8c1ae780319bc98ae092d3854
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Dec 1 18:09:13 2010 -0800

    timewait_sock: Create and use getpeer op.
    
    The only thing AF-specific about remembering the timestamp
    for a time-wait TCP socket is getting the peer.
    
    Abstract that behind a new timewait_sock_ops vector.
    
    Support for real IPV6 sockets is not filled in yet, but
    curiously this makes timewait recycling start to work
    for v4-mapped ipv6 sockets.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3e239641d4ee..4097320caa25 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -313,6 +313,7 @@ extern void tcp_shutdown (struct sock *sk, int how);
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern struct inet_peer *tcp_v4_get_peer(struct sock *sk, bool *release_it);
+extern void *tcp_v4_tw_get_peer(struct sock *sk);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t size);

commit 3f419d2d487821093ee46e898b5f8747f9edc9cd
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Nov 29 13:37:14 2010 -0800

    inet: Turn ->remember_stamp into ->get_peer in connection AF ops.
    
    Then we can make a completely generic tcp_remember_stamp()
    that uses ->get_peer() as a helper, minimizing the AF specific
    code and minimizing the eventual code duplication when we implement
    the ipv6 side of TW recycling.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e36c874c7fb1..3e239641d4ee 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -312,7 +312,7 @@ extern void tcp_shutdown (struct sock *sk, int how);
 
 extern int tcp_v4_rcv(struct sk_buff *skb);
 
-extern int tcp_v4_remember_stamp(struct sock *sk);
+extern struct inet_peer *tcp_v4_get_peer(struct sock *sk, bool *release_it);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		       size_t size);

commit 8d987e5c75107ca7515fa19e857cfa24aab6ec8f
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 9 23:24:26 2010 +0000

    net: avoid limits overflow
    
    Robin Holt tried to boot a 16TB machine and found some limits were
    reached : sysctl_tcp_mem[2], sysctl_udp_mem[2]
    
    We can switch infrastructure to use long "instead" of "int", now
    atomic_long_t primitives are available for free.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Reported-by: Robin Holt <holt@sgi.com>
    Reviewed-by: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4fee0424af7e..e36c874c7fb1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -224,7 +224,7 @@ extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_ecn;
 extern int sysctl_tcp_dsack;
-extern int sysctl_tcp_mem[3];
+extern long sysctl_tcp_mem[3];
 extern int sysctl_tcp_wmem[3];
 extern int sysctl_tcp_rmem[3];
 extern int sysctl_tcp_app_win;
@@ -247,7 +247,7 @@ extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
 extern int sysctl_tcp_thin_dupack;
 
-extern atomic_t tcp_memory_allocated;
+extern atomic_long_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
@@ -280,7 +280,7 @@ static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 	}
 
 	if (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
-	    atomic_read(&tcp_memory_allocated) > sysctl_tcp_mem[2])
+	    atomic_long_read(&tcp_memory_allocated) > sysctl_tcp_mem[2])
 		return true;
 	return false;
 }

commit 1b9f409293529da4630bfc5d6d8e7d7451a6ccb5
Author: stephen hemminger <shemminger@vyatta.com>
Date:   Tue Sep 28 19:30:14 2010 +0000

    tcp: tcp_enter_quickack_mode can be static
    
    Function only used in tcp_input.c
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 914a60c7ad62..4fee0424af7e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -346,8 +346,6 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 	}
 }
 
-extern void tcp_enter_quickack_mode(struct sock *sk);
-
 #define	TCP_ECN_OK		1
 #define	TCP_ECN_QUEUE_CWR	2
 #define	TCP_ECN_DEMAND_CWR	4

commit e40051d134f7ee95c8c1f7a3471e84eafc9ab326
Merge: 42099d7a3941 2cc6d2bf3d61
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Sep 27 01:03:03 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/net/qlcnic/qlcnic_init.c
            net/ipv4/ip_output.c

commit 01f83d69844d307be2aa6fea88b0e8fe5cbdb2f4
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Wed Sep 15 10:27:52 2010 -0700

    tcp: Prevent overzealous packetization by SWS logic.
    
    If peer uses tiny MSS (say, 75 bytes) and similarly tiny advertised
    window, the SWS logic will packetize to half the MSS unnecessarily.
    
    This causes problems with some embedded devices.
    
    However for large MSS devices we do want to half-MSS packetize
    otherwise we never get enough packets into the pipe for things
    like fast retransmit and recovery to work.
    
    Be careful also to handle the case where MSS > window, otherwise
    we'll never send until the probe timer.
    
    Reported-by: ツ Leandro Melo de Sales <leandroal@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index eaa9582779d0..3e4b33e36602 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -475,8 +475,22 @@ extern unsigned int tcp_current_mss(struct sock *sk);
 /* Bound MSS / TSO packet size with the half of the window */
 static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
 {
-	if (tp->max_window && pktsize > (tp->max_window >> 1))
-		return max(tp->max_window >> 1, 68U - tp->tcp_header_len);
+	int cutoff;
+
+	/* When peer uses tiny windows, there is no use in packetizing
+	 * to sub-MSS pieces for the sake of SWS or making sure there
+	 * are enough packets in the pipe for fast recovery.
+	 *
+	 * On the other hand, for extremely large MSS devices, handling
+	 * smaller than MSS windows in this way does make sense.
+	 */
+	if (tp->max_window >= 512)
+		cutoff = (tp->max_window >> 1);
+	else
+		cutoff = tp->max_window;
+
+	if (cutoff && pktsize > cutoff)
+		return max_t(int, cutoff, 68U - tp->tcp_header_len);
 	else
 		return pktsize;
 }

commit e548833df83c3554229eff0672900bfe958b45fd
Merge: cbd9da7be869 053d8f662270
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Sep 9 22:27:33 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            net/mac80211/main.c

commit 3d5b99ae82f8742e3bb1f8634fd11ac36ea19ee1
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Sun Aug 29 19:27:34 2010 +0000

    TCP: update initial windows according to RFC 5681
    
    This updates the use of larger initial windows, as originally specified in
    RFC 3390, to use the newer IW values specified in RFC 5681, section 3.1.
    
    The changes made in RFC 5681 are:
     a) the setting now is more clearly specified in units of segments (as the
        comments  by John Heffner emphasized, this was not very clear in RFC 3390);
     b) for connections with 1095 < SMSS <= 2190 there is now a change:
        - RFC 3390 says that IW <= 4380,
        - RFC 5681 says that IW = 3 * SMSS <= 6570.
    
    Since RFC 3390 is older and "only" proposed standard, whereas the newer RFC 5681
    is already draft standard, it seems preferable to use the newer IW variant.
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a64022199b62..11dbacc886c0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -781,17 +781,11 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 
 /*
  * Convert RFC 3390 larger initial window into an equivalent number of packets.
- *
- * John Heffner states:
- *
- *	The RFC specifies a window of no more than 4380 bytes
- *	unless 2*MSS > 4380.  Reading the pseudocode in the RFC
- *	is a bit misleading because they use a clamp at 4380 bytes
- *	rather than a multiplier in the relevant range.
+ * This is based on the numbers specified in RFC 5681, 3.1.
  */
 static inline u32 rfc3390_bytes_to_packets(const u32 smss)
 {
-	return smss <= 1095 ? 4 : (smss > 1460 ? 2 : 3);
+	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
 }
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);

commit 22b71c8f4f3db8df92f5e7b081c265bc56c0bd2f
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Sun Aug 29 19:23:12 2010 +0000

    tcp/dccp: Consolidate common code for RFC 3390 conversion
    
    This patch consolidates initial-window code common to TCP and CCID-2:
     * TCP uses RFC 3390 in a packet-oriented manner (tcp_input.c) and
     * CCID-2 uses RFC 3390 in packet-oriented manner (RFC 4341).
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index df6a2eb20193..a64022199b62 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -779,6 +779,21 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
+/*
+ * Convert RFC 3390 larger initial window into an equivalent number of packets.
+ *
+ * John Heffner states:
+ *
+ *	The RFC specifies a window of no more than 4380 bytes
+ *	unless 2*MSS > 4380.  Reading the pseudocode in the RFC
+ *	is a bit misleading because they use a clamp at 4380 bytes
+ *	rather than a multiplier in the relevant range.
+ */
+static inline u32 rfc3390_bytes_to_packets(const u32 smss)
+{
+	return smss <= 1095 ? 4 : (smss > 1460 ? 2 : 3);
+}
+
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 

commit ad1af0fedba14f82b240a03fe20eb9b2fdbd0357
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Aug 25 02:27:49 2010 -0700

    tcp: Combat per-cpu skew in orphan tests.
    
    As reported by Anton Blanchard when we use
    percpu_counter_read_positive() to make our orphan socket limit checks,
    the check can be off by up to num_cpus_online() * batch (which is 32
    by default) which on a 128 cpu machine can be as large as the default
    orphan limit itself.
    
    Fix this by doing the full expensive sum check if the optimized check
    triggers.
    
    Reported-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index df6a2eb20193..eaa9582779d0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -268,11 +268,21 @@ static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)
 	return seq3 - seq2 >= seq1 - seq2;
 }
 
-static inline int tcp_too_many_orphans(struct sock *sk, int num)
+static inline bool tcp_too_many_orphans(struct sock *sk, int shift)
 {
-	return (num > sysctl_tcp_max_orphans) ||
-		(sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
-		 atomic_read(&tcp_memory_allocated) > sysctl_tcp_mem[2]);
+	struct percpu_counter *ocp = sk->sk_prot->orphan_count;
+	int orphans = percpu_counter_read_positive(ocp);
+
+	if (orphans << shift > sysctl_tcp_max_orphans) {
+		orphans = percpu_counter_sum_positive(ocp);
+		if (orphans << shift > sysctl_tcp_max_orphans)
+			return true;
+	}
+
+	if (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
+	    atomic_read(&tcp_memory_allocated) > sysctl_tcp_mem[2])
+		return true;
+	return false;
 }
 
 /* syncookies: remember time of last synqueue overflow */

commit f86586fa48a94c7a9c7f36650a2e0e4a947881c9
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Jul 15 21:41:00 2010 -0700

    tcp: sizeof struct tcp_skb_cb is 44
    
    Correct comment stating sizeof(struct tcp_skb_cb) is 36 or 40, since its
    44 bytes, since commit 951dbc8ac714b04 ([IPV6]: Move nextheader offset
    to the IP6CB).
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 468b01f01c13..df6a2eb20193 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -570,11 +570,10 @@ extern u32 __tcp_select_window(struct sock *sk);
 #define TCPHDR_CWR 0x80
 
 /* This is what the send packet queuing engine uses to pass
- * TCP per-packet control information to the transmission
- * code.  We also store the host-order sequence numbers in
- * here too.  This is 36 bytes on 32-bit architectures,
- * 40 bytes on 64-bit machines, if this grows please adjust
- * skbuff.h:skbuff->cb[xxx] size appropriately.
+ * TCP per-packet control information to the transmission code.
+ * We also store the host-order sequence numbers in here too.
+ * This is 44 bytes if IPV6 is enabled.
+ * If this grows please adjust skbuff.h:skbuff->cb[xxx] size appropriately.
  */
 struct tcp_skb_cb {
 	union {

commit 7ba42910073f8432934d61a6c08b1023c408fb62
Author: Changli Gao <xiaosuo@gmail.com>
Date:   Sat Jul 10 20:41:55 2010 +0000

    inet, inet6: make tcp_sendmsg() and tcp_sendpage() through inet_sendmsg() and inet_sendpage()
    
    a new boolean flag no_autobind is added to structure proto to avoid the autobind
    calls when the protocol is TCP. Then sock_rps_record_flow() is called int the
    TCP's sendmsg() and sendpage() pathes.
    
    Signed-off-by: Changli Gao <xiaosuo@gmail.com>
    ----
     include/net/inet_common.h |    4 ++++
     include/net/sock.h        |    1 +
     include/net/tcp.h         |    8 ++++----
     net/ipv4/af_inet.c        |   15 +++++++++------
     net/ipv4/tcp.c            |   11 +++++------
     net/ipv4/tcp_ipv4.c       |    3 +++
     net/ipv6/af_inet6.c       |    8 ++++----
     net/ipv6/tcp_ipv6.c       |    3 +++
     8 files changed, 33 insertions(+), 20 deletions(-)
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 33ce5242a17a..468b01f01c13 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -304,10 +304,10 @@ extern int tcp_v4_rcv(struct sk_buff *skb);
 
 extern int tcp_v4_remember_stamp(struct sock *sk);
 extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
-extern int tcp_sendmsg(struct kiocb *iocb, struct socket *sock,
-		       struct msghdr *msg, size_t size);
-extern ssize_t tcp_sendpage(struct socket *sock, struct page *page, int offset,
-			    size_t size, int flags);
+extern int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
+		       size_t size);
+extern int tcp_sendpage(struct sock *sk, struct page *page, int offset,
+			size_t size, int flags);
 extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
 extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 				 struct tcphdr *th, unsigned len);

commit 53d3176b282cc105493babb0fef36c8b873f6201
Author: Changli Gao <xiaosuo@gmail.com>
Date:   Sat Jul 10 20:41:06 2010 +0000

    net: cleanups
    
    remove useless blanks.
    
    Signed-off-by: Changli Gao <xiaosuo@gmail.com>
    ----
     include/net/inet_common.h |   55 ++++-------
     include/net/tcp.h         |  222 +++++++++++++++++-----------------------------
     include/net/udp.h         |   38 +++----
     3 files changed, 123 insertions(+), 192 deletions(-)
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c2f96c2cc89c..33ce5242a17a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -296,45 +296,30 @@ extern struct proto tcp_prot;
 #define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 #define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
-extern void			tcp_v4_err(struct sk_buff *skb, u32);
-
-extern void			tcp_shutdown (struct sock *sk, int how);
-
-extern int			tcp_v4_rcv(struct sk_buff *skb);
-
-extern int			tcp_v4_remember_stamp(struct sock *sk);
-
-extern int		    	tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
-
-extern int			tcp_sendmsg(struct kiocb *iocb, struct socket *sock,
-					    struct msghdr *msg, size_t size);
-extern ssize_t			tcp_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags);
-
-extern int			tcp_ioctl(struct sock *sk, 
-					  int cmd, 
-					  unsigned long arg);
-
-extern int			tcp_rcv_state_process(struct sock *sk, 
-						      struct sk_buff *skb,
-						      struct tcphdr *th,
-						      unsigned len);
-
-extern int			tcp_rcv_established(struct sock *sk, 
-						    struct sk_buff *skb,
-						    struct tcphdr *th, 
-						    unsigned len);
-
-extern void			tcp_rcv_space_adjust(struct sock *sk);
-
-extern void			tcp_cleanup_rbuf(struct sock *sk, int copied);
-
-extern int			tcp_twsk_unique(struct sock *sk,
-						struct sock *sktw, void *twp);
-
-extern void			tcp_twsk_destructor(struct sock *sk);
-
-extern ssize_t			tcp_splice_read(struct socket *sk, loff_t *ppos,
-					        struct pipe_inode_info *pipe, size_t len, unsigned int flags);
+extern void tcp_v4_err(struct sk_buff *skb, u32);
+
+extern void tcp_shutdown (struct sock *sk, int how);
+
+extern int tcp_v4_rcv(struct sk_buff *skb);
+
+extern int tcp_v4_remember_stamp(struct sock *sk);
+extern int tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
+extern int tcp_sendmsg(struct kiocb *iocb, struct socket *sock,
+		       struct msghdr *msg, size_t size);
+extern ssize_t tcp_sendpage(struct socket *sock, struct page *page, int offset,
+			    size_t size, int flags);
+extern int tcp_ioctl(struct sock *sk, int cmd, unsigned long arg);
+extern int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
+				 struct tcphdr *th, unsigned len);
+extern int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
+			       struct tcphdr *th, unsigned len);
+extern void tcp_rcv_space_adjust(struct sock *sk);
+extern void tcp_cleanup_rbuf(struct sock *sk, int copied);
+extern int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);
+extern void tcp_twsk_destructor(struct sock *sk);
+extern ssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,
+			       struct pipe_inode_info *pipe, size_t len,
+			       unsigned int flags);
 
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
@@ -372,88 +357,59 @@ enum tcp_tw_status {
 };
 
 
-extern enum tcp_tw_status	tcp_timewait_state_process(struct inet_timewait_sock *tw,
-							   struct sk_buff *skb,
-							   const struct tcphdr *th);
-
-extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
-					      struct request_sock *req,
-					      struct request_sock **prev);
-extern int			tcp_child_process(struct sock *parent,
-						  struct sock *child,
-						  struct sk_buff *skb);
-extern int			tcp_use_frto(struct sock *sk);
-extern void			tcp_enter_frto(struct sock *sk);
-extern void			tcp_enter_loss(struct sock *sk, int how);
-extern void			tcp_clear_retrans(struct tcp_sock *tp);
-extern void			tcp_update_metrics(struct sock *sk);
-
-extern void			tcp_close(struct sock *sk, 
-					  long timeout);
-extern unsigned int		tcp_poll(struct file * file, struct socket *sock, struct poll_table_struct *wait);
-
-extern int			tcp_getsockopt(struct sock *sk, int level, 
-					       int optname,
-					       char __user *optval, 
-					       int __user *optlen);
-extern int			tcp_setsockopt(struct sock *sk, int level, 
-					       int optname, char __user *optval, 
-					       unsigned int optlen);
-extern int			compat_tcp_getsockopt(struct sock *sk,
-					int level, int optname,
-					char __user *optval, int __user *optlen);
-extern int			compat_tcp_setsockopt(struct sock *sk,
-					int level, int optname,
-					char __user *optval, unsigned int optlen);
-extern void			tcp_set_keepalive(struct sock *sk, int val);
-extern void			tcp_syn_ack_timeout(struct sock *sk,
-						    struct request_sock *req);
-extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
-					    struct msghdr *msg,
-					    size_t len, int nonblock, 
-					    int flags, int *addr_len);
-
-extern void			tcp_parse_options(struct sk_buff *skb,
-						  struct tcp_options_received *opt_rx,
-						  u8 **hvpp,
-						  int estab);
-
-extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
+extern enum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,
+						     struct sk_buff *skb,
+						     const struct tcphdr *th);
+extern struct sock * tcp_check_req(struct sock *sk,struct sk_buff *skb,
+				   struct request_sock *req,
+				   struct request_sock **prev);
+extern int tcp_child_process(struct sock *parent, struct sock *child,
+			     struct sk_buff *skb);
+extern int tcp_use_frto(struct sock *sk);
+extern void tcp_enter_frto(struct sock *sk);
+extern void tcp_enter_loss(struct sock *sk, int how);
+extern void tcp_clear_retrans(struct tcp_sock *tp);
+extern void tcp_update_metrics(struct sock *sk);
+extern void tcp_close(struct sock *sk, long timeout);
+extern unsigned int tcp_poll(struct file * file, struct socket *sock,
+			     struct poll_table_struct *wait);
+extern int tcp_getsockopt(struct sock *sk, int level, int optname,
+			  char __user *optval, int __user *optlen);
+extern int tcp_setsockopt(struct sock *sk, int level, int optname,
+			  char __user *optval, unsigned int optlen);
+extern int compat_tcp_getsockopt(struct sock *sk, int level, int optname,
+				 char __user *optval, int __user *optlen);
+extern int compat_tcp_setsockopt(struct sock *sk, int level, int optname,
+				 char __user *optval, unsigned int optlen);
+extern void tcp_set_keepalive(struct sock *sk, int val);
+extern void tcp_syn_ack_timeout(struct sock *sk, struct request_sock *req);
+extern int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
+		       size_t len, int nonblock, int flags, int *addr_len);
+extern void tcp_parse_options(struct sk_buff *skb,
+			      struct tcp_options_received *opt_rx, u8 **hvpp,
+			      int estab);
+extern u8 *tcp_parse_md5sig_option(struct tcphdr *th);
 
 /*
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern void		       	tcp_v4_send_check(struct sock *sk,
-						  struct sk_buff *skb);
-
-extern int			tcp_v4_conn_request(struct sock *sk,
-						    struct sk_buff *skb);
-
-extern struct sock *		tcp_create_openreq_child(struct sock *sk,
-							 struct request_sock *req,
-							 struct sk_buff *skb);
-
-extern struct sock *		tcp_v4_syn_recv_sock(struct sock *sk,
-						     struct sk_buff *skb,
-						     struct request_sock *req,
-							struct dst_entry *dst);
-
-extern int			tcp_v4_do_rcv(struct sock *sk,
+extern void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);
+extern int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);
+extern struct sock * tcp_create_openreq_child(struct sock *sk,
+					      struct request_sock *req,
 					      struct sk_buff *skb);
-
-extern int			tcp_v4_connect(struct sock *sk,
-					       struct sockaddr *uaddr,
-					       int addr_len);
-
-extern int			tcp_connect(struct sock *sk);
-
-extern struct sk_buff *		tcp_make_synack(struct sock *sk,
-						struct dst_entry *dst,
-						struct request_sock *req,
-						struct request_values *rvp);
-
-extern int			tcp_disconnect(struct sock *sk, int flags);
+extern struct sock * tcp_v4_syn_recv_sock(struct sock *sk, struct sk_buff *skb,
+					  struct request_sock *req,
+					  struct dst_entry *dst);
+extern int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);
+extern int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr,
+			  int addr_len);
+extern int tcp_connect(struct sock *sk);
+extern struct sk_buff * tcp_make_synack(struct sock *sk, struct dst_entry *dst,
+					struct request_sock *req,
+					struct request_values *rvp);
+extern int tcp_disconnect(struct sock *sk, int flags);
 
 
 /* From syncookies.c */
@@ -485,10 +441,10 @@ extern int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int);
 
 extern void tcp_send_probe0(struct sock *);
 extern void tcp_send_partial(struct sock *);
-extern int  tcp_write_wakeup(struct sock *);
+extern int tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
 extern void tcp_send_active_reset(struct sock *sk, gfp_t priority);
-extern int  tcp_send_synack(struct sock *);
+extern int tcp_send_synack(struct sock *);
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);
@@ -592,7 +548,7 @@ static inline u32 tcp_receive_window(const struct tcp_sock *tp)
  * scaling applied to the result.  The caller does these things
  * if necessary.  This is a "raw" window selection.
  */
-extern u32	__tcp_select_window(struct sock *sk);
+extern u32 __tcp_select_window(struct sock *sk);
 
 /* TCP timestamps are only 32-bits, this causes a slight
  * complication on 64-bit systems since we store a snapshot
@@ -1174,22 +1130,14 @@ struct tcp_md5sig_pool {
 #define TCP_MD5SIG_MAXKEYS	(~(u32)0)	/* really?! */
 
 /* - functions */
-extern int			tcp_v4_md5_hash_skb(char *md5_hash,
-						    struct tcp_md5sig_key *key,
-						    struct sock *sk,
-						    struct request_sock *req,
-						    struct sk_buff *skb);
-
-extern struct tcp_md5sig_key	*tcp_v4_md5_lookup(struct sock *sk,
-						   struct sock *addr_sk);
-
-extern int			tcp_v4_md5_do_add(struct sock *sk,
-						  __be32 addr,
-						  u8 *newkey,
-						  u8 newkeylen);
-
-extern int			tcp_v4_md5_do_del(struct sock *sk,
-						  __be32 addr);
+extern int tcp_v4_md5_hash_skb(char *md5_hash, struct tcp_md5sig_key *key,
+			       struct sock *sk, struct request_sock *req,
+			       struct sk_buff *skb);
+extern struct tcp_md5sig_key * tcp_v4_md5_lookup(struct sock *sk,
+						 struct sock *addr_sk);
+extern int tcp_v4_md5_do_add(struct sock *sk, __be32 addr, u8 *newkey,
+			     u8 newkeylen);
+extern int tcp_v4_md5_do_del(struct sock *sk, __be32 addr);
 
 #ifdef CONFIG_TCP_MD5SIG
 #define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_keylen ? 		 \
@@ -1202,10 +1150,10 @@ extern int			tcp_v4_md5_do_del(struct sock *sk,
 #endif
 
 extern struct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *);
-extern void			tcp_free_md5sig_pool(void);
+extern void tcp_free_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
-extern void			tcp_put_md5sig_pool(void);
+extern void tcp_put_md5sig_pool(void);
 
 extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, struct tcphdr *);
 extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, struct sk_buff *,
@@ -1433,7 +1381,7 @@ extern int tcp_gro_complete(struct sk_buff *skb);
 extern int tcp4_gro_complete(struct sk_buff *skb);
 
 #ifdef CONFIG_PROC_FS
-extern int  tcp4_proc_init(void);
+extern int tcp4_proc_init(void);
 extern void tcp4_proc_exit(void);
 #endif
 

commit 172d69e63c7f1e8300d0e1c1bbd8eb0f630faa15
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Jun 21 11:48:45 2010 +0000

    syncookies: add support for ECN
    
    Allows use of ECN when syncookies are in effect by encoding ecn_ok
    into the syn-ack tcp timestamp.
    
    While at it, remove a uneeded #ifdef CONFIG_SYN_COOKIES.
    With CONFIG_SYN_COOKIES=nm want_cookie is ifdef'd to 0 and gcc
    removes the "if (0)".
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 18c246c9b009..c2f96c2cc89c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -464,7 +464,7 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 				     __u16 *mss);
 
 extern __u32 cookie_init_timestamp(struct request_sock *req);
-extern bool cookie_check_timestamp(struct tcp_options_received *tcp_opt);
+extern bool cookie_check_timestamp(struct tcp_options_received *opt, bool *);
 
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);

commit 8c76368174ed2359739f1b7b8a9c042b1ef839c4
Author: Florian Westphal <fw@strlen.de>
Date:   Wed Jun 16 14:42:15 2010 -0700

    syncookies: check decoded options against sysctl settings
    
    Discard the ACK if we find options that do not match current sysctl
    settings.
    
    Previously it was possible to create a connection with sack, wscale,
    etc. enabled even if the feature was disabled via sysctl.
    
    Also remove an unneeded call to tcp_sack_reset() in
    cookie_check_timestamp: Both call sites (cookie_v4_check,
    cookie_v6_check) zero "struct tcp_options_received", hand it to
    tcp_parse_options() (which does not change tcp_opt->num_sacks/dsack)
    and then call cookie_check_timestamp().
    
    Even if num_sacks/dsacks were changed, the structure is allocated on
    the stack and after cookie_check_timestamp returns only a few selected
    members are copied to the inet_request_sock.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9e68e25c8b82..18c246c9b009 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -464,7 +464,7 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 				     __u16 *mss);
 
 extern __u32 cookie_init_timestamp(struct request_sock *req);
-extern void cookie_check_timestamp(struct tcp_options_received *tcp_opt);
+extern bool cookie_check_timestamp(struct tcp_options_received *tcp_opt);
 
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);

commit a3433f35a55f7604742cae620c6dc6edfc70db6a
Author: Changli Gao <xiaosuo@gmail.com>
Date:   Sat Jun 12 14:01:43 2010 +0000

    tcp: unify tcp flag macros
    
    unify tcp flag macros: TCPHDR_FIN, TCPHDR_SYN, TCPHDR_RST, TCPHDR_PSH,
    TCPHDR_ACK, TCPHDR_URG, TCPHDR_ECE and TCPHDR_CWR. TCBCB_FLAG_* are replaced
    with the corresponding TCPHDR_*.
    
    Signed-off-by: Changli Gao <xiaosuo@gmail.com>
    ----
     include/net/tcp.h                      |   24 ++++++-------
     net/ipv4/tcp.c                         |    8 ++--
     net/ipv4/tcp_input.c                   |    2 -
     net/ipv4/tcp_output.c                  |   59 ++++++++++++++++-----------------
     net/netfilter/nf_conntrack_proto_tcp.c |   32 ++++++-----------
     net/netfilter/xt_TCPMSS.c              |    4 --
     6 files changed, 58 insertions(+), 71 deletions(-)
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 573166484413..9e68e25c8b82 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -602,6 +602,17 @@ extern u32	__tcp_select_window(struct sock *sk);
  */
 #define tcp_time_stamp		((__u32)(jiffies))
 
+#define tcp_flag_byte(th) (((u_int8_t *)th)[13])
+
+#define TCPHDR_FIN 0x01
+#define TCPHDR_SYN 0x02
+#define TCPHDR_RST 0x04
+#define TCPHDR_PSH 0x08
+#define TCPHDR_ACK 0x10
+#define TCPHDR_URG 0x20
+#define TCPHDR_ECE 0x40
+#define TCPHDR_CWR 0x80
+
 /* This is what the send packet queuing engine uses to pass
  * TCP per-packet control information to the transmission
  * code.  We also store the host-order sequence numbers in
@@ -620,19 +631,6 @@ struct tcp_skb_cb {
 	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
 	__u32		when;		/* used to compute rtt's	*/
 	__u8		flags;		/* TCP header flags.		*/
-
-	/* NOTE: These must match up to the flags byte in a
-	 *       real TCP header.
-	 */
-#define TCPCB_FLAG_FIN		0x01
-#define TCPCB_FLAG_SYN		0x02
-#define TCPCB_FLAG_RST		0x04
-#define TCPCB_FLAG_PSH		0x08
-#define TCPCB_FLAG_ACK		0x10
-#define TCPCB_FLAG_URG		0x20
-#define TCPCB_FLAG_ECE		0x40
-#define TCPCB_FLAG_CWR		0x80
-
 	__u8		sacked;		/* State flags for SACK/FACK.	*/
 #define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
 #define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/

commit a8b690f98baf9fb1902b8eeab801351ea603fa3a
Author: Tom Herbert <therbert@google.com>
Date:   Mon Jun 7 00:43:42 2010 -0700

    tcp: Fix slowness in read /proc/net/tcp
    
    This patch address a serious performance issue in reading the
    TCP sockets table (/proc/net/tcp).
    
    Reading the full table is done by a number of sequential read
    operations.  At each read operation, a seek is done to find the
    last socket that was previously read.  This seek operation requires
    that the sockets in the table need to be counted up to the current
    file position, and to count each of these requires taking a lock for
    each non-empty bucket.  The whole algorithm is O(n^2).
    
    The fix is to cache the last bucket value, offset within the bucket,
    and the file position returned by the last read operation.   On the
    next sequential read, the bucket and offset are used to find the
    last read socket immediately without needing ot scan the previous
    buckets  the table.  This algorithm t read the whole table is O(n).
    
    The improvement offered by this patch is easily show by performing
    cat'ing /proc/net/tcp on a machine with a lot of connections.  With
    about 182K connections in the table, I see the following:
    
    - Without patch
    time cat /proc/net/tcp > /dev/null
    
    real    1m56.729s
    user    0m0.214s
    sys     1m56.344s
    
    - With patch
    time cat /proc/net/tcp > /dev/null
    
    real    0m0.894s
    user    0m0.290s
    sys     0m0.594s
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a1449144848a..573166484413 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1413,7 +1413,8 @@ struct tcp_iter_state {
 	sa_family_t		family;
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;
-	int			bucket, sbucket, num, uid;
+	int			bucket, offset, sbucket, num, uid;
+	loff_t			last_pos;
 };
 
 extern int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);

commit 6811d58fc148c393f80a9f5a9db49d7e75cdc546
Merge: c4949f074332 c02db8c6290b
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 16 22:26:58 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            include/linux/if_link.h

commit 35790c0421121364883a167bab8a2e37e1f67f78
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun May 16 00:34:04 2010 -0700

    tcp: fix MD5 (RFC2385) support
    
    TCP MD5 support uses percpu data for temporary storage. It currently
    disables preemption so that same storage cannot be reclaimed by another
    thread on same cpu.
    
    We also have to make sure a softirq handler wont try to use also same
    context. Various bug reports demonstrated corruptions.
    
    Fix is to disable preemption and BH.
    
    Reported-by: Bhaskar Dutta <bhaskie@gmail.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 75be5a28815d..aa04b9a5093b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1197,30 +1197,15 @@ extern int			tcp_v4_md5_do_del(struct sock *sk,
 extern struct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *);
 extern void			tcp_free_md5sig_pool(void);
 
-extern struct tcp_md5sig_pool	*__tcp_get_md5sig_pool(int cpu);
-extern void			__tcp_put_md5sig_pool(void);
+extern struct tcp_md5sig_pool	*tcp_get_md5sig_pool(void);
+extern void			tcp_put_md5sig_pool(void);
+
 extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, struct tcphdr *);
 extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, struct sk_buff *,
 				 unsigned header_len);
 extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
 			    struct tcp_md5sig_key *key);
 
-static inline
-struct tcp_md5sig_pool		*tcp_get_md5sig_pool(void)
-{
-	int cpu = get_cpu();
-	struct tcp_md5sig_pool *ret = __tcp_get_md5sig_pool(cpu);
-	if (!ret)
-		put_cpu();
-	return ret;
-}
-
-static inline void		tcp_put_md5sig_pool(void)
-{
-	__tcp_put_md5sig_pool();
-	put_cpu();
-}
-
 /* write queue abstraction */
 static inline void tcp_write_queue_purge(struct sock *sk)
 {

commit 6c37e5de456987f5bc80879afde05aa120784095
Author: Flavio Leitner <fleitner@redhat.com>
Date:   Mon Apr 26 18:33:27 2010 +0000

    TCP: avoid to send keepalive probes if receiving data
    
    RFC 1122 says the following:
    ...
      Keep-alive packets MUST only be sent when no data or
      acknowledgement packets have been received for the
      connection within an interval.
    ...
    
    The acknowledgement packet is reseting the keepalive
    timer but the data packet isn't. This patch fixes it by
    checking the timestamp of the last received data packet
    too when the keepalive timer expires.
    
    Signed-off-by: Flavio Leitner <fleitner@redhat.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3f87fd87bc9c..fb5c66b2ab81 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1033,6 +1033,14 @@ static inline int keepalive_probes(const struct tcp_sock *tp)
 	return tp->keepalive_probes ? : sysctl_tcp_keepalive_probes;
 }
 
+static inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)
+{
+	const struct inet_connection_sock *icsk = &tp->inet_conn;
+
+	return min_t(u32, tcp_time_stamp - icsk->icsk_ack.lrcvtime,
+			  tcp_time_stamp - tp->rcv_tstamp);
+}
+
 static inline int tcp_fin_time(const struct sock *sk)
 {
 	int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;

commit aa2ea0586d9dbe56a334d835a43b45e8c2104e77
Author: Tom Herbert <therbert@google.com>
Date:   Thu Apr 22 07:00:24 2010 +0000

    tcp: fix outsegs stat for TSO segments
    
    Account for TSO segments of an skb in TCP_MIB_OUTSEGS counter.  Without
    doing this, the counter can be off by orders of magnitude from the
    actual number of segments sent.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b7d83d204a93..3f87fd87bc9c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -294,6 +294,7 @@ extern struct proto tcp_prot;
 #define TCP_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
 #define TCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mib.tcp_statistics, field)
 #define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
+#define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)
 
 extern void			tcp_v4_err(struct sk_buff *skb, u32);
 

commit aa395145165cb06a0d0885221bbe0ce4a564391d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Apr 20 13:03:51 2010 +0000

    net: sk_sleep() helper
    
    Define a new function to return the waitqueue of a "struct sock".
    
    static inline wait_queue_head_t *sk_sleep(struct sock *sk)
    {
            return sk->sk_sleep;
    }
    
    Change all read occurrences of sk_sleep by a call to this function.
    
    Needed for a future RCU conversion. sk_sleep wont be a field directly
    available.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 70c5159f4b36..b7d83d204a93 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -939,7 +939,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 
 		tp->ucopy.memory = 0;
 	} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
-		wake_up_interruptible_sync_poll(sk->sk_sleep,
+		wake_up_interruptible_sync_poll(sk_sleep(sk),
 					   POLLIN | POLLRDNORM | POLLRDBAND);
 		if (!inet_csk_ack_scheduled(sk))
 			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,

commit bb29624614c2afe2873ee8ee97cf09df42701694
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Apr 11 02:15:55 2010 +0000

    inet: Remove unused send_check length argument
    
    inet: Remove unused send_check length argument
    
    This patch removes the unused length argument from the send_check
    function in struct inet_connection_sock_af_ops.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Tested-by: Yinghai <yinghai.lu@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 75be5a28815d..70c5159f4b36 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -423,7 +423,7 @@ extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern void		       	tcp_v4_send_check(struct sock *sk, int len,
+extern void		       	tcp_v4_send_check(struct sock *sk,
 						  struct sk_buff *skb);
 
 extern int			tcp_v4_conn_request(struct sock *sk,

commit c839d30a41dd92eb32d7fcfa2b4e99042fc64bf2
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Mar 3 04:46:50 2010 +0000

    net: add scheduler sync hint to tcp_prequeue().
    
    Decreases the odds wakee will suffer from frequent cache misses.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 56f0aec40ed6..75be5a28815d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -939,7 +939,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 
 		tp->ucopy.memory = 0;
 	} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
-		wake_up_interruptible_poll(sk->sk_sleep,
+		wake_up_interruptible_sync_poll(sk->sk_sleep,
 					   POLLIN | POLLRDNORM | POLLRDBAND);
 		if (!inet_csk_ack_scheduled(sk))
 			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,

commit 7e38017557bc0b87434d184f8804cadb102bb903
Author: Andreas Petlund <apetlund@simula.no>
Date:   Thu Feb 18 04:48:19 2010 +0000

    net: TCP thin dupack
    
    This patch enables fast retransmissions after one dupACK for
    TCP if the stream is identified as thin. This will reduce
    latencies for thin streams that are not able to trigger fast
    retransmissions due to high packet interarrival time. This
    mechanism is only active if enabled by iocontrol or syscontrol
    and the stream is identified as thin.
    
    Signed-off-by: Andreas Petlund <apetlund@simula.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6278fc734abd..56f0aec40ed6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -245,6 +245,7 @@ extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_max_ssthresh;
 extern int sysctl_tcp_cookie_size;
 extern int sysctl_tcp_thin_linear_timeouts;
+extern int sysctl_tcp_thin_dupack;
 
 extern atomic_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 36e31b0af58728071e8023cf8e20c5166b700717
Author: Andreas Petlund <apetlund@simula.no>
Date:   Thu Feb 18 02:47:01 2010 +0000

    net: TCP thin linear timeouts
    
    This patch will make TCP use only linear timeouts if the
    stream is thin. This will help to avoid the very high latencies
    that thin stream suffer because of exponential backoff. This
    mechanism is only active if enabled by iocontrol or syscontrol
    and the stream is identified as thin. A maximum of 6 linear
    timeouts is tried before exponential backoff is resumed.
    
    Signed-off-by: Andreas Petlund <apetlund@simula.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0bdc3f640247..6278fc734abd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -196,6 +196,9 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCP_NAGLE_CORK		2	/* Socket is corked	    */
 #define TCP_NAGLE_PUSH		4	/* Cork is overridden for already queued data */
 
+/* TCP thin-stream limits */
+#define TCP_THIN_LINEAR_RETRIES 6       /* After 6 linear retries, do exp. backoff */
+
 extern struct inet_timewait_death_row tcp_death_row;
 
 /* sysctl variables for tcp */
@@ -241,6 +244,7 @@ extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_max_ssthresh;
 extern int sysctl_tcp_cookie_size;
+extern int sysctl_tcp_thin_linear_timeouts;
 
 extern atomic_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;

commit 5aa4b32fc86408705337e941ed716880c63d1590
Author: Andreas Petlund <apetlund@simula.no>
Date:   Thu Feb 18 02:45:45 2010 +0000

    net: TCP thin-stream detection
    
    Inline function to dynamically detect thin streams based on
    the number of packets in flight. Used to dynamically trigger
    thin-stream mechanisms if enabled by ioctl or sysctl.
    
    Signed-off-by: Andreas Petlund <apetlund@simula.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 75a00c80bdda..0bdc3f640247 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1386,6 +1386,14 @@ static inline void tcp_highest_sack_combine(struct sock *sk,
 		tcp_sk(sk)->highest_sack = new;
 }
 
+/* Determines whether this is a thin stream (which may suffer from
+ * increased latency). Used to trigger latency-reducing mechanisms.
+ */
+static inline unsigned int tcp_stream_is_thin(struct tcp_sock *tp)
+{
+	return tp->packets_out < 4 && !tcp_in_initial_slowstart(tp);
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,

commit 7d720c3e4f0c4fc152a6bf17e24244a3c85412d2
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 16 15:20:26 2010 +0000

    percpu: add __percpu sparse annotations to net
    
    Add __percpu sparse annotations to net.
    
    These annotations are to make sparse consider percpu variables to be
    in a different address space and warn if accessed without going
    through percpu accessors.  This patch doesn't affect normal builds.
    
    The macro and type tricks around snmp stats make things a bit
    interesting.  DEFINE/DECLARE_SNMP_STAT() macros mark the target field
    as __percpu and SNMP_UPD_PO_STATS() macro is updated accordingly.  All
    snmp_mib_*() users which used to cast the argument to (void **) are
    updated to cast it to (void __percpu **).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Vlad Yasevich <vladislav.yasevich@hp.com>
    Cc: netdev@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 87d164b9bd8f..75a00c80bdda 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1189,7 +1189,7 @@ extern int			tcp_v4_md5_do_del(struct sock *sk,
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 
-extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(struct sock *);
+extern struct tcp_md5sig_pool * __percpu *tcp_alloc_md5sig_pool(struct sock *);
 extern void			tcp_free_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*__tcp_get_md5sig_pool(int cpu);

commit 72659ecce68588b74f6c46862c2b4cec137d7a5a
Author: Octavian Purdila <opurdila@ixiacom.com>
Date:   Sun Jan 17 19:09:39 2010 -0800

    tcp: account SYN-ACK timeouts & retransmissions
    
    Currently we don't increment SYN-ACK timeouts & retransmissions
    although we do increment the same stats for SYN. We seem to have lost
    the SYN-ACK accounting with the introduction of tcp_syn_recv_timer
    (commit 2248761e in the netdev-vger-cvs tree).
    
    This patch fixes this issue. In the process we also rename the v4/v6
    syn/ack retransmit functions for clarity. We also add a new
    request_socket operations (syn_ack_timeout) so we can keep code in
    inet_connection_sock.c protocol agnostic.
    
    Signed-off-by: Octavian Purdila <opurdila@ixiacom.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 788c99f98597..87d164b9bd8f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -400,6 +400,8 @@ extern int			compat_tcp_setsockopt(struct sock *sk,
 					int level, int optname,
 					char __user *optval, unsigned int optlen);
 extern void			tcp_set_keepalive(struct sock *sk, int val);
+extern void			tcp_syn_ack_timeout(struct sock *sk,
+						    struct request_sock *req);
 extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 					    struct msghdr *msg,
 					    size_t len, int nonblock, 

commit 31d12926e37291970dd4f6e9940df3897766a81d
Author: laurent chavey <chavey@google.com>
Date:   Tue Dec 15 11:15:28 2009 +0000

    net: Add rtnetlink init_rcvwnd to set the TCP initial receive window
    
    Add rtnetlink init_rcvwnd to set the TCP initial receive window size
    advertised by passive and active TCP connections.
    The current Linux TCP implementation limits the advertised TCP initial
    receive window to the one prescribed by slow start. For short lived
    TCP connections used for transaction type of traffic (i.e. http
    requests), bounding the advertised TCP initial receive window results
    in increased latency to complete the transaction.
    Support for setting initial congestion window is already supported
    using rtnetlink init_cwnd, but the feature is useless without the
    ability to set a larger TCP initial receive window.
    The rtnetlink init_rcvwnd allows increasing the TCP initial receive
    window, allowing TCP connection to advertise larger TCP receive window
    than the ones bounded by slow start.
    
    Signed-off-by: Laurent Chavey <chavey@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 185e22baecb1..788c99f98597 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -965,7 +965,8 @@ static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 /* Determine a window scaling and initial window to offer. */
 extern void tcp_select_initial_window(int __space, __u32 mss,
 				      __u32 *rcv_wnd, __u32 *window_clamp,
-				      int wscale_ok, __u8 *rcv_wscale);
+				      int wscale_ok, __u8 *rcv_wscale,
+				      __u32 init_rcv_wnd);
 
 static inline int tcp_win_from_space(int space)
 {

commit 12d50c46dc0f7fd2e625c4befaa5fa5740a7a594
Author: Krishna Kumar <krkumar2@in.ibm.com>
Date:   Tue Dec 8 22:26:13 2009 +0000

    tcp: Remove check in __tcp_push_pending_frames
    
    tcp_push checks tcp_send_head and calls __tcp_push_pending_frames,
    which again checks tcp_send_head, and this unnecessary check is
    done for every other caller of __tcp_push_pending_frames.
    
    Remove tcp_send_head check in __tcp_push_pending_frames and add
    the check to tcp_push_pending_frames. Other functions call
    __tcp_push_pending_frames only when tcp_send_head would evaluate
    to true.
    
    Signed-off-by: Krishna Kumar <krkumar2@in.ibm.com>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 34f5cc24d903..185e22baecb1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -856,13 +856,6 @@ static inline void tcp_check_probe_timer(struct sock *sk)
 					  icsk->icsk_rto, TCP_RTO_MAX);
 }
 
-static inline void tcp_push_pending_frames(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	__tcp_push_pending_frames(sk, tcp_current_mss(sk), tp->nonagle);
-}
-
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)
 {
 	tp->snd_wl1 = seq;
@@ -1342,6 +1335,15 @@ static inline int tcp_write_queue_empty(struct sock *sk)
 	return skb_queue_empty(&sk->sk_write_queue);
 }
 
+static inline void tcp_push_pending_frames(struct sock *sk)
+{
+	if (tcp_send_head(sk)) {
+		struct tcp_sock *tp = tcp_sk(sk);
+
+		__tcp_push_pending_frames(sk, tcp_current_mss(sk), tp->nonagle);
+	}
+}
+
 /* Start sequence of the highest skb with SACKed bit, valid only if
  * sacked > 0 or when the caller has ensured validity by itself.
  */

commit bb5b7c11263dbbe78253cd05945a6bf8f55add8e
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Dec 15 20:56:42 2009 -0800

    tcp: Revert per-route SACK/DSACK/TIMESTAMP changes.
    
    It creates a regression, triggering badness for SYN_RECV
    sockets, for example:
    
    [19148.022102] Badness at net/ipv4/inet_connection_sock.c:293
    [19148.022570] NIP: c02a0914 LR: c02a0904 CTR: 00000000
    [19148.023035] REGS: eeecbd30 TRAP: 0700   Not tainted  (2.6.32)
    [19148.023496] MSR: 00029032 <EE,ME,CE,IR,DR>  CR: 24002442  XER: 00000000
    [19148.024012] TASK = eee9a820[1756] 'privoxy' THREAD: eeeca000
    
    This is likely caused by the change in the 'estab' parameter
    passed to tcp_parse_options() when invoked by the functions
    in net/ipv4/tcp_minisocks.c
    
    But even if that is fixed, the ->conn_request() changes made in
    this patch series is fundamentally wrong.  They try to use the
    listening socket's 'dst' to probe the route settings.  The
    listening socket doesn't even have a route, and you can't
    get the right route (the child request one) until much later
    after we setup all of the state, and it must be done by hand.
    
    This stuff really isn't ready, so the best thing to do is a
    full revert.  This reverts the following commits:
    
    f55017a93f1a74d50244b1254b9a2bd7ac9bbf7d
    022c3f7d82f0f1c68018696f2f027b87b9bb45c2
    1aba721eba1d84a2defce45b950272cee1e6c72a
    cda42ebd67ee5fdf09d7057b5a4584d36fe8a335
    345cda2fd695534be5a4494f1b59da9daed33663
    dc343475ed062e13fc260acccaab91d7d80fd5b2
    05eaade2782fb0c90d3034fd7a7d5a16266182bb
    6a2a2d6bf8581216e08be15fcb563cfd6c430e1e
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1b6f7d348cee..34f5cc24d903 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -408,8 +408,7 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
 						  u8 **hvpp,
-						  int estab,
-						  struct dst_entry *dst);
+						  int estab);
 
 extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
 

commit 501706565b2d4d2d40d0d301d5411ede099b8a6f
Merge: e93737b0f015 2fe77b81c77e
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 11 17:12:17 2009 -0800

    Merge branch 'master' of /home/davem/src/GIT/linux-2.6/
    
    Conflicts:
            include/net/tcp.h

commit 4ef58d4e2ad1fa2a3e5bbf41af2284671fca8cf8
Merge: f6c4c8195b5e d014d043869c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 9 19:43:33 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (42 commits)
      tree-wide: fix misspelling of "definition" in comments
      reiserfs: fix misspelling of "journaled"
      doc: Fix a typo in slub.txt.
      inotify: remove superfluous return code check
      hdlc: spelling fix in find_pvc() comment
      doc: fix regulator docs cut-and-pasteism
      mtd: Fix comment in Kconfig
      doc: Fix IRQ chip docs
      tree-wide: fix assorted typos all over the place
      drivers/ata/libata-sff.c: comment spelling fixes
      fix typos/grammos in Documentation/edac.txt
      sysctl: add missing comments
      fs/debugfs/inode.c: fix comment typos
      sgivwfb: Make use of ARRAY_SIZE.
      sky2: fix sky2_link_down copy/paste comment error
      tree-wide: fix typos "couter" -> "counter"
      tree-wide: fix typos "offest" -> "offset"
      fix kerneldoc for set_irq_msi()
      spidev: fix double "of of" in comment
      comment typo fix: sybsystem -> subsystem
      ...

commit 2f7de5710a4d394920405febc2a9937c69e16dda
Author: Damian Lukowski <damian@tvk.rwth-aachen.de>
Date:   Mon Dec 7 06:06:16 2009 +0000

    tcp: Stalling connections: Move timeout calculation routine
    
    This patch moves retransmits_timed_out() from include/net/tcp.h
    to tcp_timer.c, where it is used.
    
    Reported-by: Frederic Leroy <fredo@starox.org>
    Signed-off-by: Damian Lukowski <damian@tvk.rwth-aachen.de>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e54bd85d9d40..0248c181a92c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1261,34 +1261,6 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
-/* This function calculates a "timeout" which is equivalent to the timeout of a
- * TCP connection after "boundary" unsucessful, exponentially backed-off
- * retransmissions with an initial RTO of TCP_RTO_MIN.
- */
-static inline bool retransmits_timed_out(struct sock *sk,
-					 unsigned int boundary)
-{
-	unsigned int timeout, linear_backoff_thresh;
-	unsigned int start_ts;
-
-	if (!inet_csk(sk)->icsk_retransmits)
-		return false;
-
-	if (unlikely(!tcp_sk(sk)->retrans_stamp))
-		start_ts = TCP_SKB_CB(tcp_write_queue_head(sk))->when;
-	else
-		start_ts = tcp_sk(sk)->retrans_stamp;
-
-	linear_backoff_thresh = ilog2(TCP_RTO_MAX/TCP_RTO_MIN);
-
-	if (boundary <= linear_backoff_thresh)
-		timeout = ((2 << boundary) - 1) * TCP_RTO_MIN;
-	else
-		timeout = ((2 << linear_backoff_thresh) - 1) * TCP_RTO_MIN +
-			  (boundary - linear_backoff_thresh) * TCP_RTO_MAX;
-
-	return (tcp_time_stamp - start_ts) >= timeout;
-}
 
 static inline struct sk_buff *tcp_send_head(struct sock *sk)
 {

commit 07f29bc5bbae4e53e982ab956fed7207990a7786
Author: Damian Lukowski <damian@tvk.rwth-aachen.de>
Date:   Mon Dec 7 06:06:15 2009 +0000

    tcp: Stalling connections: Fix timeout calculation routine
    
    This patch fixes a problem in the TCP connection timeout calculation.
    Currently, timeout decisions are made on the basis of the current
    tcp_time_stamp and retrans_stamp, which is usually set at the first
    retransmission.
    However, if the retransmission fails in tcp_retransmit_skb(),
    retrans_stamp is not updated and remains zero. This leads to wrong
    decisions in retransmits_timed_out() if tcp_time_stamp is larger than
    the specified timeout, which is very likely.
    In this case, the TCP connection dies after the first attempted
    (and unsuccessful) retransmission.
    
    With this patch, tcp_skb_cb->when is used instead, when retrans_stamp
    is not available.
    
    This bug has been introduced together with retransmits_timed_out() in
    2.6.32, as the number of retransmissions has been used for timeout
    decisions before. The corresponding commit was
    6fa12c85031485dff38ce550c24f10da23b0adaa (Revert Backoff [v3]:
    Calculate TCP's connection close threshold as a time value.).
    
    Thanks to Ilpo Järvinen for code suggestions and Frederic Leroy for
    testing.
    
    Reported-by: Frederic Leroy <fredo@starox.org>
    Signed-off-by: Damian Lukowski <damian@tvk.rwth-aachen.de>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e2d2ca2509be..e54bd85d9d40 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1265,14 +1265,20 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
  * TCP connection after "boundary" unsucessful, exponentially backed-off
  * retransmissions with an initial RTO of TCP_RTO_MIN.
  */
-static inline bool retransmits_timed_out(const struct sock *sk,
+static inline bool retransmits_timed_out(struct sock *sk,
 					 unsigned int boundary)
 {
 	unsigned int timeout, linear_backoff_thresh;
+	unsigned int start_ts;
 
 	if (!inet_csk(sk)->icsk_retransmits)
 		return false;
 
+	if (unlikely(!tcp_sk(sk)->retrans_stamp))
+		start_ts = TCP_SKB_CB(tcp_write_queue_head(sk))->when;
+	else
+		start_ts = tcp_sk(sk)->retrans_stamp;
+
 	linear_backoff_thresh = ilog2(TCP_RTO_MAX/TCP_RTO_MIN);
 
 	if (boundary <= linear_backoff_thresh)
@@ -1281,7 +1287,7 @@ static inline bool retransmits_timed_out(const struct sock *sk,
 		timeout = ((2 << linear_backoff_thresh) - 1) * TCP_RTO_MIN +
 			  (boundary - linear_backoff_thresh) * TCP_RTO_MAX;
 
-	return (tcp_time_stamp - tcp_sk(sk)->retrans_stamp) >= timeout;
+	return (tcp_time_stamp - start_ts) >= timeout;
 }
 
 static inline struct sk_buff *tcp_send_head(struct sock *sk)

commit af901ca181d92aac3a7dc265144a9081a86d8f39
Author: André Goddard Rosa <andre.goddard@gmail.com>
Date:   Sat Nov 14 13:09:05 2009 -0200

    tree-wide: fix assorted typos all over the place
    
    That is "success", "unknown", "through", "performance", "[re|un]mapping"
    , "access", "default", "reasonable", "[con]currently", "temperature"
    , "channel", "[un]used", "application", "example","hierarchy", "therefore"
    , "[over|under]flow", "contiguous", "threshold", "enough" and others.
    
    Signed-off-by: André Goddard Rosa <andre.goddard@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 03a49c703377..1827e7f217d1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1260,7 +1260,7 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
 /* This function calculates a "timeout" which is equivalent to the timeout of a
- * TCP connection after "boundary" unsucessful, exponentially backed-off
+ * TCP connection after "boundary" unsuccessful, exponentially backed-off
  * retransmissions with an initial RTO of TCP_RTO_MIN.
  */
 static inline bool retransmits_timed_out(const struct sock *sk,

commit 8818a9d884e3a589899be3303958fff182e98e55
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Wed Dec 2 22:24:02 2009 -0800

    tcp: clear hints to avoid a stale one (nfs only affected?)
    
    Eric Dumazet mentioned in a context of another problem:
    
    "Well, it seems NFS reuses its socket, so maybe we miss some
    cleaning as spotted in this old patch"
    
    I've not check under which conditions that actually happens but
    if true, we need to make sure we don't accidently leave stale
    hints behind when the write queue had to be purged (whether reusing
    with NFS can actually happen if purging took place is something I'm
    not sure of).
    
    ...At least it compiles.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 28b04ff8c967..e2d2ca2509be 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1229,6 +1229,7 @@ static inline void tcp_write_queue_purge(struct sock *sk)
 	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
 		sk_wmem_free_skb(sk, skb);
 	sk_mem_reclaim(sk);
+	tcp_clear_all_retrans_hints(tcp_sk(sk));
 }
 
 static inline struct sk_buff *tcp_write_queue_head(struct sock *sk)

commit 4957faade11b3a278c3b3cade3411ddc20afa791
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:25:27 2009 +0000

    TCPCT part 1g: Responder Cookie => Initiator
    
    Parse incoming TCP_COOKIE option(s).
    
    Calculate <SYN,ACK> TCP_COOKIE option.
    
    Send optional <SYN,ACK> data.
    
    This is a significantly revised implementation of an earlier (year-old)
    patch that no longer applies cleanly, with permission of the original
    author (Adam Langley):
    
        http://thread.gmane.org/gmane.linux.network/102586
    
    Requires:
       TCPCT part 1a: add request_values parameter for sending SYNACK
       TCPCT part 1b: generate Responder Cookie secret
       TCPCT part 1c: sysctl_tcp_cookie_size, socket option TCP_COOKIE_TRANSACTIONS
       TCPCT part 1d: define TCP cookie option, extend existing struct's
       TCPCT part 1e: implement socket option TCP_COOKIE_TRANSACTIONS
       TCPCT part 1f: Initiator Cookie => Responder
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f9abd9becabd..28b04ff8c967 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -407,6 +407,7 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
+						  u8 **hvpp,
 						  int estab,
 						  struct dst_entry *dst);
 

commit 435cf559f02ea3a3159eb316f97dc88bdebe9432
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:17:05 2009 +0000

    TCPCT part 1d: define TCP cookie option, extend existing struct's
    
    Data structures are carefully composed to require minimal additions.
    For example, the struct tcp_options_received cookie_plus variable fits
    between existing 16-bit and 8-bit variables, requiring no additional
    space (taking alignment into consideration).  There are no additions to
    tcp_request_sock, and only 1 pointer in tcp_sock.
    
    This is a significantly revised implementation of an earlier (year-old)
    patch that no longer applies cleanly, with permission of the original
    author (Adam Langley):
    
        http://thread.gmane.org/gmane.linux.network/102586
    
    The principle difference is using a TCP option to carry the cookie nonce,
    instead of a user configured offset in the data.  This is more flexible and
    less subject to user configuration error.  Such a cookie option has been
    suggested for many years, and is also useful without SYN data, allowing
    several related concepts to use the same extension option.
    
        "Re: SYN floods (was: does history repeat itself?)", September 9, 1996.
        http://www.merit.net/mail.archives/nanog/1996-09/msg00235.html
    
        "Re: what a new TCP header might look like", May 12, 1998.
        ftp://ftp.isi.edu/end2end/end2end-interest-1998.mail
    
    These functions will also be used in subsequent patches that implement
    additional features.
    
    Requires:
       TCPCT part 1a: add request_values parameter for sending SYNACK
       TCPCT part 1b: generate Responder Cookie secret
       TCPCT part 1c: sysctl_tcp_cookie_size, socket option TCP_COOKIE_TRANSACTIONS
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 738b65f01e26..f9abd9becabd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -30,6 +30,7 @@
 #include <linux/dmaengine.h>
 #include <linux/crypto.h>
 #include <linux/cryptohash.h>
+#include <linux/kref.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -164,6 +165,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_SACK             5       /* SACK Block */
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
 #define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
+#define TCPOPT_COOKIE		253	/* Cookie extension (experimental) */
 
 /*
  *     TCP option lengths
@@ -174,6 +176,10 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_PERM      2
 #define TCPOLEN_TIMESTAMP      10
 #define TCPOLEN_MD5SIG         18
+#define TCPOLEN_COOKIE_BASE    2	/* Cookie-less header extension */
+#define TCPOLEN_COOKIE_PAIR    3	/* Cookie pair header extension */
+#define TCPOLEN_COOKIE_MIN     (TCPOLEN_COOKIE_BASE+TCP_COOKIE_MIN)
+#define TCPOLEN_COOKIE_MAX     (TCPOLEN_COOKIE_BASE+TCP_COOKIE_MAX)
 
 /* But this is what stacks really send out. */
 #define TCPOLEN_TSTAMP_ALIGNED		12
@@ -1482,6 +1488,83 @@ struct tcp_request_sock_ops {
 
 extern int tcp_cookie_generator(u32 *bakery);
 
+/**
+ *	struct tcp_cookie_values - each socket needs extra space for the
+ *	cookies, together with (optional) space for any SYN data.
+ *
+ *	A tcp_sock contains a pointer to the current value, and this is
+ *	cloned to the tcp_timewait_sock.
+ *
+ * @cookie_pair:	variable data from the option exchange.
+ *
+ * @cookie_desired:	user specified tcpct_cookie_desired.  Zero
+ *			indicates default (sysctl_tcp_cookie_size).
+ *			After cookie sent, remembers size of cookie.
+ *			Range 0, TCP_COOKIE_MIN to TCP_COOKIE_MAX.
+ *
+ * @s_data_desired:	user specified tcpct_s_data_desired.  When the
+ *			constant payload is specified (@s_data_constant),
+ *			holds its length instead.
+ *			Range 0 to TCP_MSS_DESIRED.
+ *
+ * @s_data_payload:	constant data that is to be included in the
+ *			payload of SYN or SYNACK segments when the
+ *			cookie option is present.
+ */
+struct tcp_cookie_values {
+	struct kref	kref;
+	u8		cookie_pair[TCP_COOKIE_PAIR_SIZE];
+	u8		cookie_pair_size;
+	u8		cookie_desired;
+	u16		s_data_desired:11,
+			s_data_constant:1,
+			s_data_in:1,
+			s_data_out:1,
+			s_data_unused:2;
+	u8		s_data_payload[0];
+};
+
+static inline void tcp_cookie_values_release(struct kref *kref)
+{
+	kfree(container_of(kref, struct tcp_cookie_values, kref));
+}
+
+/* The length of constant payload data.  Note that s_data_desired is
+ * overloaded, depending on s_data_constant: either the length of constant
+ * data (returned here) or the limit on variable data.
+ */
+static inline int tcp_s_data_size(const struct tcp_sock *tp)
+{
+	return (tp->cookie_values != NULL && tp->cookie_values->s_data_constant)
+		? tp->cookie_values->s_data_desired
+		: 0;
+}
+
+/**
+ *	struct tcp_extend_values - tcp_ipv?.c to tcp_output.c workspace.
+ *
+ *	As tcp_request_sock has already been extended in other places, the
+ *	only remaining method is to pass stack values along as function
+ *	parameters.  These parameters are not needed after sending SYNACK.
+ *
+ * @cookie_bakery:	cryptographic secret and message workspace.
+ *
+ * @cookie_plus:	bytes in authenticator/cookie option, copied from
+ *			struct tcp_options_received (above).
+ */
+struct tcp_extend_values {
+	struct request_values		rv;
+	u32				cookie_bakery[COOKIE_WORKSPACE_WORDS];
+	u8				cookie_plus:6,
+					cookie_out_never:1,
+					cookie_in_always:1;
+};
+
+static inline struct tcp_extend_values *tcp_xv(struct request_values *rvp)
+{
+	return (struct tcp_extend_values *)rvp;
+}
+
 extern void tcp_v4_init(void);
 extern void tcp_init(void);
 

commit 519855c508b9a17878c0977a3cdefc09b59b30df
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:14:19 2009 +0000

    TCPCT part 1c: sysctl_tcp_cookie_size, socket option TCP_COOKIE_TRANSACTIONS
    
    Define sysctl (tcp_cookie_size) to turn on and off the cookie option
    default globally, instead of a compiled configuration option.
    
    Define per socket option (TCP_COOKIE_TRANSACTIONS) for setting constant
    data values, retrieving variable cookie values, and other facilities.
    
    Move inline tcp_clear_options() unchanged from net/tcp.h to linux/tcp.h,
    near its corresponding struct tcp_options_received (prior to changes).
    
    This is a straightforward re-implementation of an earlier (year-old)
    patch that no longer applies cleanly, with permission of the original
    author (Adam Langley):
    
        http://thread.gmane.org/gmane.linux.network/102586
    
    These functions will also be used in subsequent patches that implement
    additional features.
    
    Requires:
       net: TCP_MSS_DEFAULT, TCP_MSS_DESIRED
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4a99a8e39121..738b65f01e26 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -234,6 +234,7 @@ extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_max_ssthresh;
+extern int sysctl_tcp_cookie_size;
 
 extern atomic_t tcp_memory_allocated;
 extern struct percpu_counter tcp_sockets_allocated;
@@ -340,11 +341,6 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 
 extern void tcp_enter_quickack_mode(struct sock *sk);
 
-static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
-{
- 	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
-}
-
 #define	TCP_ECN_OK		1
 #define	TCP_ECN_QUEUE_CWR	2
 #define	TCP_ECN_DEMAND_CWR	4

commit da5c78c82629a167794436e4306b4cf1faddea90
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:12:09 2009 +0000

    TCPCT part 1b: generate Responder Cookie secret
    
    Define (missing) hash message size for SHA1.
    
    Define hashing size constants specific to TCP cookies.
    
    Add new function: tcp_cookie_generator().
    
    Maintain global secret values for tcp_cookie_generator().
    
    This is a significantly revised implementation of earlier (15-year-old)
    Photuris [RFC-2522] code for the KA9Q cooperative multitasking platform.
    
    Linux RCU technique appears to be well-suited to this application, though
    neither of the circular queue items are freed.
    
    These functions will also be used in subsequent patches that implement
    additional features.
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ec183fda05d0..4a99a8e39121 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1478,6 +1478,14 @@ struct tcp_request_sock_ops {
 #endif
 };
 
+/* Using SHA1 for now, define some constants.
+ */
+#define COOKIE_DIGEST_WORDS (SHA_DIGEST_WORDS)
+#define COOKIE_MESSAGE_WORDS (SHA_MESSAGE_BYTES / 4)
+#define COOKIE_WORKSPACE_WORDS (COOKIE_DIGEST_WORDS + COOKIE_MESSAGE_WORDS)
+
+extern int tcp_cookie_generator(u32 *bakery);
+
 extern void tcp_v4_init(void);
 extern void tcp_init(void);
 

commit e6b4d11367519bc71729c09d05a126b133c755be
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:07:39 2009 +0000

    TCPCT part 1a: add request_values parameter for sending SYNACK
    
    Add optional function parameters associated with sending SYNACK.
    These parameters are not needed after sending SYNACK, and are not
    used for retransmission.  Avoids extending struct tcp_request_sock,
    and avoids allocating kernel memory.
    
    Also affects DCCP as it uses common struct request_sock_ops,
    but this parameter is currently reserved for future use.
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 325bfcf5c934..ec183fda05d0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -440,7 +440,8 @@ extern int			tcp_connect(struct sock *sk);
 
 extern struct sk_buff *		tcp_make_synack(struct sock *sk,
 						struct dst_entry *dst,
-						struct request_sock *req);
+						struct request_sock *req,
+						struct request_values *rvp);
 
 extern int			tcp_disconnect(struct sock *sk, int flags);
 

commit bee7ca9ec03a26676ea2b1c28dc4039348eff3e1
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Tue Nov 10 09:51:18 2009 +0000

    net: TCP_MSS_DEFAULT, TCP_MSS_DESIRED
    
    Define two symbols needed in both kernel and user space.
    
    Remove old (somewhat incorrect) kernel variant that wasn't used in
    most cases.  Default should apply to both RMSS and SMSS (RFC2581).
    
    Replace numeric constants with defined symbols.
    
    Stand-alone patch, originally developed for TCPCT.
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bf20f88fd033..325bfcf5c934 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -62,9 +62,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* Minimal accepted MSS. It is (60+60+8) - (20+20). */
 #define TCP_MIN_MSS		88U
 
-/* Minimal RCV_MSS. */
-#define TCP_MIN_RCVMSS		536U
-
 /* The least MTU to use for probing */
 #define TCP_BASE_MSS		512
 

commit fd2c3ef761fbc5e6c27fa7d40b30cda06bfcd7d8
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 3 03:26:03 2009 +0000

    net: cleanup include/net
    
    This cleanup patch puts struct/union/enum opening braces,
    in first line to ease grep games.
    
    struct something
    {
    
    becomes :
    
    struct something {
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 740d09be8e2d..bf20f88fd033 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -359,8 +359,7 @@ TCP_ECN_create_request(struct request_sock *req, struct tcphdr *th)
 		inet_rsk(req)->ecn_ok = 1;
 }
 
-enum tcp_tw_status
-{
+enum tcp_tw_status {
 	TCP_TW_SUCCESS = 0,
 	TCP_TW_RST = 1,
 	TCP_TW_ACK = 2,

commit 022c3f7d82f0f1c68018696f2f027b87b9bb45c2
Author: Gilad Ben-Yossef <gilad@codefidence.com>
Date:   Wed Oct 28 04:15:22 2009 +0000

    Allow tcp_parse_options to consult dst entry
    
    We need tcp_parse_options to be aware of dst_entry to
    take into account per dst_entry TCP options settings
    
    Signed-off-by: Gilad Ben-Yossef <gilad@codefidence.com>
    Sigend-off-by: Ori Finkelman <ori@comsleep.com>
    Sigend-off-by: Yony Amit <yony@comsleep.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 03a49c703377..740d09be8e2d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -409,7 +409,8 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
-						  int estab);
+						  int estab,
+						  struct dst_entry *dst);
 
 extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
 

commit b7058842c940ad2c08dd829b21e5c92ebe3b8758
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Sep 30 16:12:20 2009 -0700

    net: Make setsockopt() optlen be unsigned.
    
    This provides safety against negative optlen at the type
    level instead of depending upon (sometimes non-trivial)
    checks against this sprinkled all over the the place, in
    each and every implementation.
    
    Based upon work done by Arjan van de Ven and feedback
    from Linus Torvalds.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 56b76027b85e..03a49c703377 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -394,13 +394,13 @@ extern int			tcp_getsockopt(struct sock *sk, int level,
 					       int __user *optlen);
 extern int			tcp_setsockopt(struct sock *sk, int level, 
 					       int optname, char __user *optval, 
-					       int optlen);
+					       unsigned int optlen);
 extern int			compat_tcp_getsockopt(struct sock *sk,
 					int level, int optname,
 					char __user *optval, int __user *optlen);
 extern int			compat_tcp_setsockopt(struct sock *sk,
 					int level, int optname,
-					char __user *optval, int optlen);
+					char __user *optval, unsigned int optlen);
 extern void			tcp_set_keepalive(struct sock *sk, int val);
 extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 					    struct msghdr *msg,

commit 0b6a05c1dbebe8c616e2e5b0f52b7a01fd792911
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Tue Sep 15 01:30:10 2009 -0700

    tcp: fix ssthresh u16 leftover
    
    It was once upon time so that snd_sthresh was a 16-bit quantity.
    ...That has not been true for long period of time. I run across
    some ancient compares which still seem to trust such legacy.
    Put all that magic into a single place, I hopefully found all
    of them.
    
    Compile tested, though linking of allyesconfig is ridiculous
    nowadays it seems.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b71a446d58f6..56b76027b85e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -793,6 +793,13 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 	return tp->packets_out - tcp_left_out(tp) + tp->retrans_out;
 }
 
+#define TCP_INFINITE_SSTHRESH	0x7fffffff
+
+static inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)
+{
+	return tp->snd_ssthresh >= TCP_INFINITE_SSTHRESH;
+}
+
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
  * The exception is rate halving phase, when cwnd is decreasing towards
  * ssthresh.

commit aa1330766c49199bdab4d4a9096d98b072df9044
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Sep 2 23:45:45 2009 -0700

    tcp: replace hard coded GFP_KERNEL with sk_allocation
    
    This fixed a lockdep warning which appeared when doing stress
    memory tests over NFS:
    
            inconsistent {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} usage.
    
            page reclaim => nfs_writepage => tcp_sendmsg => lock sk_lock
    
            mount_root => nfs_root_data => tcp_close => lock sk_lock =>
                            tcp_send_fin => alloc_skb_fclone => page reclaim
    
    David raised a concern that if the allocation fails in tcp_send_fin(), and it's
    GFP_ATOMIC, we are going to yield() (which sleeps) and loop endlessly waiting
    for the allocation to succeed.
    
    But fact is, the original GFP_KERNEL also sleeps. GFP_ATOMIC+yield() looks
    weird, but it is no worse the implicit sleep inside GFP_KERNEL. Both could
    loop endlessly under memory pressure.
    
    CC: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    CC: David S. Miller <davem@davemloft.net>
    CC: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index df50bc40b5fd..b71a446d58f6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1186,7 +1186,7 @@ extern int			tcp_v4_md5_do_del(struct sock *sk,
 #define tcp_twsk_md5_key(twsk)	NULL
 #endif
 
-extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(void);
+extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(struct sock *);
 extern void			tcp_free_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*__tcp_get_md5sig_pool(int cpu);

commit 5152fc7de3ae31b46692022ea63ce0501280f5b1
Author: Damian Lukowski <damian@tvk.rwth-aachen.de>
Date:   Tue Sep 1 10:24:00 2009 +0000

    RTO connection timeout: coding style fixes and comments
    
    This patch affects the retransmits_timed_out() function.
    
    Changes:
    1) Variables have more meaningful names
    2) retransmits_timed_out() has an introductionary comment.
    3) Small coding style changes.
    
    Signed-off-by: Damian Lukowski <damian@tvk.rwth-aachen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e5319495f15e..df50bc40b5fd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1252,22 +1252,27 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
+/* This function calculates a "timeout" which is equivalent to the timeout of a
+ * TCP connection after "boundary" unsucessful, exponentially backed-off
+ * retransmissions with an initial RTO of TCP_RTO_MIN.
+ */
 static inline bool retransmits_timed_out(const struct sock *sk,
 					 unsigned int boundary)
 {
-	int limit, K;
+	unsigned int timeout, linear_backoff_thresh;
+
 	if (!inet_csk(sk)->icsk_retransmits)
 		return false;
 
-	K = ilog2(TCP_RTO_MAX/TCP_RTO_MIN);
+	linear_backoff_thresh = ilog2(TCP_RTO_MAX/TCP_RTO_MIN);
 
-	if (boundary <= K)
-		limit = ((2 << boundary) - 1) * TCP_RTO_MIN;
+	if (boundary <= linear_backoff_thresh)
+		timeout = ((2 << boundary) - 1) * TCP_RTO_MIN;
 	else
-		limit = ((2 << K) - 1) * TCP_RTO_MIN +
-			(boundary - K) * TCP_RTO_MAX;
+		timeout = ((2 << linear_backoff_thresh) - 1) * TCP_RTO_MIN +
+			  (boundary - linear_backoff_thresh) * TCP_RTO_MAX;
 
-	return (tcp_time_stamp - tcp_sk(sk)->retrans_stamp) >= limit;
+	return (tcp_time_stamp - tcp_sk(sk)->retrans_stamp) >= timeout;
 }
 
 static inline struct sk_buff *tcp_send_head(struct sock *sk)

commit 6fa12c85031485dff38ce550c24f10da23b0adaa
Author: Damian Lukowski <damian@tvk.rwth-aachen.de>
Date:   Wed Aug 26 00:16:34 2009 +0000

    Revert Backoff [v3]: Calculate TCP's connection close threshold as a time value.
    
    RFC 1122 specifies two threshold values R1 and R2 for connection timeouts,
    which may represent a number of allowed retransmissions or a timeout value.
    Currently linux uses sysctl_tcp_retries{1,2} to specify the thresholds
    in number of allowed retransmissions.
    
    For any desired threshold R2 (by means of time) one can specify tcp_retries2
    (by means of number of retransmissions) such that TCP will not time out
    earlier than R2. This is the case, because the RTO schedule follows a fixed
    pattern, namely exponential backoff.
    
    However, the RTO behaviour is not predictable any more if RTO backoffs can be
    reverted, as it is the case in the draft
    "Make TCP more Robust to Long Connectivity Disruptions"
    (http://tools.ietf.org/html/draft-zimmermann-tcp-lcd).
    
    In the worst case TCP would time out a connection after 3.2 seconds, if the
    initial RTO equaled MIN_RTO and each backoff has been reverted.
    
    This patch introduces a function retransmits_timed_out(N),
    which calculates the timeout of a TCP connection, assuming an initial
    RTO of MIN_RTO and N unsuccessful, exponentially backed-off retransmissions.
    
    Whenever timeout decisions are made by comparing the retransmission counter
    to some value N, this function can be used, instead.
    
    The meaning of tcp_retries2 will be changed, as many more RTO retransmissions
    can occur than the value indicates. However, it yields a timeout which is
    similar to the one of an unpatched, exponentially backing off TCP in the same
    scenario. As no application could rely on an RTO greater than MIN_RTO, there
    should be no risk of a regression.
    
    Signed-off-by: Damian Lukowski <damian@tvk.rwth-aachen.de>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 54f212ce8aaf..e5319495f15e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1252,6 +1252,24 @@ static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_bu
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
 	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
+static inline bool retransmits_timed_out(const struct sock *sk,
+					 unsigned int boundary)
+{
+	int limit, K;
+	if (!inet_csk(sk)->icsk_retransmits)
+		return false;
+
+	K = ilog2(TCP_RTO_MAX/TCP_RTO_MIN);
+
+	if (boundary <= K)
+		limit = ((2 << boundary) - 1) * TCP_RTO_MIN;
+	else
+		limit = ((2 << K) - 1) * TCP_RTO_MIN +
+			(boundary - K) * TCP_RTO_MAX;
+
+	return (tcp_time_stamp - tcp_sk(sk)->retrans_stamp) >= limit;
+}
+
 static inline struct sk_buff *tcp_send_head(struct sock *sk)
 {
 	return sk->sk_send_head;

commit f1ecd5d9e7366609d640ff4040304ea197fbc618
Author: Damian Lukowski <damian@tvk.rwth-aachen.de>
Date:   Wed Aug 26 00:16:31 2009 +0000

    Revert Backoff [v3]: Revert RTO on ICMP destination unreachable
    
    Here, an ICMP host/network unreachable message, whose payload fits to
    TCP's SND.UNA, is taken as an indication that the RTO retransmission has
    not been lost due to congestion, but because of a route failure
    somewhere along the path.
    With true congestion, a router won't trigger such a message and the
    patched TCP will operate as standard TCP.
    
    This patch reverts one RTO backoff, if an ICMP host/network unreachable
    message, whose payload fits to TCP's SND.UNA, arrives.
    Based on the new RTO, the retransmission timer is reset to reflect the
    remaining time, or - if the revert clocked out the timer - a retransmission
    is sent out immediately.
    Backoffs are only reverted, if TCP is in RTO loss recovery, i.e. if
    there have been retransmissions and reversible backoffs, already.
    
    Changes from v2:
    1) Renaming of skb in tcp_v4_err() moved to another patch.
    2) Reintroduced tcp_bound_rto() and __tcp_set_rto().
    3) Fixed code comments.
    
    Signed-off-by: Damian Lukowski <damian@tvk.rwth-aachen.de>
    Acked-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cbb2a4889fc9..54f212ce8aaf 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -469,6 +469,7 @@ extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
 				      int nonagle);
 extern int tcp_may_send_now(struct sock *sk);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
+extern void tcp_retransmit_timer(struct sock *sk);
 extern void tcp_xmit_retransmit_queue(struct sock *);
 extern void tcp_simple_retransmit(struct sock *);
 extern int tcp_trim_head(struct sock *, struct sk_buff *, u32);
@@ -521,6 +522,17 @@ extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
 extern int tcp_mss_to_mtu(struct sock *sk, int mss);
 extern void tcp_mtup_init(struct sock *sk);
 
+static inline void tcp_bound_rto(const struct sock *sk)
+{
+	if (inet_csk(sk)->icsk_rto > TCP_RTO_MAX)
+		inet_csk(sk)->icsk_rto = TCP_RTO_MAX;
+}
+
+static inline u32 __tcp_set_rto(const struct tcp_sock *tp)
+{
+	return (tp->srtt >> 3) + tp->rttvar;
+}
+
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
 	tp->pred_flags = htonl((tp->tcp_header_len << 26) |

commit df19a6267705456f463871ae2aabc44299909d2a
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Aug 28 23:48:54 2009 -0700

    tcp: keepalive cleanups
    
    Introduce keepalive_probes(tp) helper, and use it, like
    keepalive_time_when(tp) and keepalive_intvl_when(tp)
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 88af84306471..cbb2a4889fc9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1007,6 +1007,11 @@ static inline int keepalive_time_when(const struct tcp_sock *tp)
 	return tp->keepalive_time ? : sysctl_tcp_keepalive_time;
 }
 
+static inline int keepalive_probes(const struct tcp_sock *tp)
+{
+	return tp->keepalive_probes ? : sysctl_tcp_keepalive_probes;
+}
+
 static inline int tcp_fin_time(const struct sock *sk)
 {
 	int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;

commit e3afe7b75ed8f809c1473ea9b39267487c187ccb
Author: John Dykstra <john.dykstra1@gmail.com>
Date:   Thu Jul 16 05:04:51 2009 +0000

    tcp: Fix MD5 signature checking on IPv4 mapped sockets
    
    Fix MD5 signature checking so that an IPv4 active open
    to an IPv6 socket can succeed.  In particular, use the
    correct address family's signature generation function
    for the SYN/ACK.
    
    Reported-by:   Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: John Dykstra <john.dykstra1@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 19f4150f4d4d..88af84306471 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1425,6 +1425,11 @@ struct tcp_request_sock_ops {
 #ifdef CONFIG_TCP_MD5SIG
 	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
 						struct request_sock *req);
+	int			(*calc_md5_hash) (char *location,
+						  struct tcp_md5sig_key *md5,
+						  struct sock *sk,
+						  struct request_sock *req,
+						  struct sk_buff *skb);
 #endif
 };
 

commit 22f6dacdfcfdc792d068e9c41234808860498d04
Merge: 928f308f556f 9b05126baa14
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 8 02:48:30 2009 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            include/net/tcp.h

commit 7aedec2ad5314b173e78ca3f4edb4ceaa02248bb
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu May 7 07:20:39 2009 +0000

    tcp: tcp_prequeue() can use keyed wakeups
    
    We can avoid waking up tasks not interested in receive notifications,
    using wake_up_interruptible_poll() instead of wake_up_interruptible()
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ac37228b7001..87d210bb12a4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -908,7 +908,8 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 
 		tp->ucopy.memory = 0;
 	} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
-		wake_up_interruptible(sk->sk_sleep);
+		wake_up_interruptible_poll(sk->sk_sleep,
+					   POLLIN | POLLRDNORM | POLLRDBAND);
 		if (!inet_csk_ack_scheduled(sk))
 			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
 						  (3 * TCP_RTO_MIN) / 4,

commit f5f8d86b231e0489c33542c42afbb14d32411ee8
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu May 7 07:08:38 2009 +0000

    tcp: tcp_prequeue() cleanup
    
    Small cleanup patch to reduce line lengths, before a change in
    tcp_prequeue().
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b55b4891029e..ac37228b7001 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -890,30 +890,31 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	if (!sysctl_tcp_low_latency && tp->ucopy.task) {
-		__skb_queue_tail(&tp->ucopy.prequeue, skb);
-		tp->ucopy.memory += skb->truesize;
-		if (tp->ucopy.memory > sk->sk_rcvbuf) {
-			struct sk_buff *skb1;
-
-			BUG_ON(sock_owned_by_user(sk));
-
-			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
-				sk_backlog_rcv(sk, skb1);
-				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPREQUEUEDROPPED);
-			}
-
-			tp->ucopy.memory = 0;
-		} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
-			wake_up_interruptible(sk->sk_sleep);
-			if (!inet_csk_ack_scheduled(sk))
-				inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
-						          (3 * TCP_RTO_MIN) / 4,
-							  TCP_RTO_MAX);
+	if (sysctl_tcp_low_latency || !tp->ucopy.task)
+		return 0;
+
+	__skb_queue_tail(&tp->ucopy.prequeue, skb);
+	tp->ucopy.memory += skb->truesize;
+	if (tp->ucopy.memory > sk->sk_rcvbuf) {
+		struct sk_buff *skb1;
+
+		BUG_ON(sock_owned_by_user(sk));
+
+		while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
+			sk_backlog_rcv(sk, skb1);
+			NET_INC_STATS_BH(sock_net(sk),
+					 LINUX_MIB_TCPPREQUEUEDROPPED);
 		}
-		return 1;
+
+		tp->ucopy.memory = 0;
+	} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
+		wake_up_interruptible(sk->sk_sleep);
+		if (!inet_csk_ack_scheduled(sk))
+			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
+						  (3 * TCP_RTO_MIN) / 4,
+						  TCP_RTO_MAX);
 	}
-	return 0;
+	return 1;
 }
 
 

commit 0c266898b42fe4e4e2f9edfc9d3474c10f93aa6a
Author: Satoru SATOH <satoru.satoh@gmail.com>
Date:   Mon May 4 11:11:01 2009 -0700

    tcp: Fix tcp_prequeue() to get correct rto_min value
    
    tcp_prequeue() refers to the constant value (TCP_RTO_MIN) regardless of
    the actual value might be tuned. The following patches fix this and make
    tcp_prequeue get the actual value returns from tcp_rto_min().
    
    Signed-off-by: Satoru SATOH <satoru.satoh@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1b94b9bfe2dc..646dbe3962ea 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -41,6 +41,7 @@
 #include <net/ip.h>
 #include <net/tcp_states.h>
 #include <net/inet_ecn.h>
+#include <net/dst.h>
 
 #include <linux/seq_file.h>
 
@@ -530,6 +531,17 @@ static inline void tcp_fast_path_check(struct sock *sk)
 		tcp_fast_path_on(tp);
 }
 
+/* Compute the actual rto_min value */
+static inline u32 tcp_rto_min(struct sock *sk)
+{
+	struct dst_entry *dst = __sk_dst_get(sk);
+	u32 rto_min = TCP_RTO_MIN;
+
+	if (dst && dst_metric_locked(dst, RTAX_RTO_MIN))
+		rto_min = dst_metric_rtt(dst, RTAX_RTO_MIN);
+	return rto_min;
+}
+
 /* Compute the actual receive window we are currently advertising.
  * Rcv_nxt can be after the window if our peer push more data
  * than the offered window.
@@ -895,7 +907,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 			wake_up_interruptible(sk->sk_sleep);
 			if (!inet_csk_ack_scheduled(sk))
 				inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
-						          (3 * TCP_RTO_MIN) / 4,
+						          (3 * tcp_rto_min(sk)) / 4,
 							  TCP_RTO_MAX);
 		}
 		return 1;

commit a0f82f64e26929776c58a5c93c2ecb38e3d82815
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Apr 19 09:43:48 2009 +0000

    syncookies: remove last_synq_overflow from struct tcp_sock
    
    last_synq_overflow eats 4 or 8 bytes in struct tcp_sock, even
    though it is only used when a listening sockets syn queue
    is full.
    
    We can (ab)use rx_opt.ts_recent_stamp to store the same information;
    it is not used otherwise as long as a socket is in listen state.
    
    Move linger2 around to avoid splitting struct mtu_probe
    across cacheline boundary on 32 bit arches.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 1b94b9bfe2dc..b55b4891029e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -265,6 +265,19 @@ static inline int tcp_too_many_orphans(struct sock *sk, int num)
 		 atomic_read(&tcp_memory_allocated) > sysctl_tcp_mem[2]);
 }
 
+/* syncookies: remember time of last synqueue overflow */
+static inline void tcp_synq_overflow(struct sock *sk)
+{
+	tcp_sk(sk)->rx_opt.ts_recent_stamp = jiffies;
+}
+
+/* syncookies: no recent synqueue overflow on this listening socket? */
+static inline int tcp_synq_no_recent_overflow(const struct sock *sk)
+{
+	unsigned long last_overflow = tcp_sk(sk)->rx_opt.ts_recent_stamp;
+	return time_after(jiffies, last_overflow + TCP_TIMEOUT_INIT);
+}
+
 extern struct proto tcp_prot;
 
 #define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)

commit 797108d134a91afca9fa59c572336b279bc66afb
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Wed Apr 1 23:15:17 2009 +0000

    tcp: add helper for counter tweaking due mid-wq change
    
    We need full-scale adjustment to fix a TCP miscount in the next
    patch, so just move it into a helper and call for that from the
    other places.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e54c76d75495..1b94b9bfe2dc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -616,21 +616,6 @@ static inline int tcp_skb_mss(const struct sk_buff *skb)
 	return skb_shinfo(skb)->gso_size;
 }
 
-static inline void tcp_dec_pcount_approx_int(__u32 *count, const int decr)
-{
-	if (*count) {
-		*count -= decr;
-		if ((int)*count < 0)
-			*count = 0;
-	}
-}
-
-static inline void tcp_dec_pcount_approx(__u32 *count,
-					 const struct sk_buff *skb)
-{
-	tcp_dec_pcount_approx_int(count, tcp_skb_pcount(skb));
-}
-
 /* Events passed to congestion control interface */
 enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */

commit 0c54b85f2828128274f319a1eb3ce7f604fe2a53
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Mar 14 14:23:05 2009 +0000

    tcp: simplify tcp_current_mss
    
    There's very little need for most of the callsites to get
    tp->xmit_goal_size updated. That will cost us divide as is,
    so slice the function in two. Also, the only users of the
    tp->xmit_goal_size are directly behind tcp_current_mss(),
    so there's no need to store that variable into tcp_sock
    at all! The drop of xmit_goal_size currently leaves 16-bit
    hole and some reorganization would again be necessary to
    change that (but I'm aiming to fill that hole with u16
    xmit_goal_size_segs to cache the results of the remaining
    divide to get that tso on regression).
    
    Bring xmit_goal_size parts into tcp.c
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Cc: Evgeniy Polyakov <zbr@ioremap.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 255ca35bea05..e54c76d75495 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -481,7 +481,16 @@ static inline void tcp_clear_xmit_timers(struct sock *sk)
 }
 
 extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
-extern unsigned int tcp_current_mss(struct sock *sk, int large);
+extern unsigned int tcp_current_mss(struct sock *sk);
+
+/* Bound MSS / TSO packet size with the half of the window */
+static inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)
+{
+	if (tp->max_window && pktsize > (tp->max_window >> 1))
+		return max(tp->max_window >> 1, 68U - tp->tcp_header_len);
+	else
+		return pktsize;
+}
 
 /* tcp.c */
 extern void tcp_get_info(struct sock *, struct tcp_info *);
@@ -822,7 +831,7 @@ static inline void tcp_push_pending_frames(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
-	__tcp_push_pending_frames(sk, tcp_current_mss(sk, 1), tp->nonagle);
+	__tcp_push_pending_frames(sk, tcp_current_mss(sk), tp->nonagle);
 }
 
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)

commit c887e6d2d9aee56ee7c9f2af4cec3a5efdcc4c72
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Mar 14 14:23:03 2009 +0000

    tcp: consolidate paws check
    
    Wow, it was quite tricky to merge that stream of negations
    but I think I finally got it right:
    
    check & replace_ts_recent:
    (s32)(rcv_tsval - ts_recent) >= 0                  => 0
    (s32)(ts_recent - rcv_tsval) <= 0                  => 0
    
    discard:
    (s32)(ts_recent - rcv_tsval)  > TCP_PAWS_WINDOW    => 1
    (s32)(ts_recent - rcv_tsval) <= TCP_PAWS_WINDOW    => 0
    
    I toggled the return values of tcp_paws_check around since
    the old encoding added yet-another negation making tracking
    of truth-values really complicated.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d74ac301e6bc..255ca35bea05 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -997,11 +997,21 @@ static inline int tcp_fin_time(const struct sock *sk)
 	return fin_timeout;
 }
 
-static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int rst)
+static inline int tcp_paws_check(const struct tcp_options_received *rx_opt,
+				 int paws_win)
 {
-	if ((s32)(rx_opt->rcv_tsval - rx_opt->ts_recent) >= 0)
-		return 0;
-	if (get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)
+	if ((s32)(rx_opt->ts_recent - rx_opt->rcv_tsval) <= paws_win)
+		return 1;
+	if (unlikely(get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS))
+		return 1;
+
+	return 0;
+}
+
+static inline int tcp_paws_reject(const struct tcp_options_received *rx_opt,
+				  int rst)
+{
+	if (tcp_paws_check(rx_opt, 0))
 		return 0;
 
 	/* RST segments are not recommended to carry timestamp,

commit ee7537b63a28b42b22e48842dfeedc66d96b71f1
Author: Hantzis Fotis <xantzis@ceid.upatras.gr>
Date:   Mon Mar 2 22:42:02 2009 -0800

    tcp: tcp_init_wl / tcp_update_wl argument cleanup
    
    The above functions from include/net/tcp.h have been defined with an
    argument that they never use. The argument is 'u32 ack' which is never
    used inside the function body, and thus it can be removed. The rest of
    the patch involves the necessary changes to the function callers of the
    above two functions.
    
    Signed-off-by: Hantzis Fotis <xantzis@ceid.upatras.gr>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 055e4946d4c8..d74ac301e6bc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -825,12 +825,12 @@ static inline void tcp_push_pending_frames(struct sock *sk)
 	__tcp_push_pending_frames(sk, tcp_current_mss(sk, 1), tp->nonagle);
 }
 
-static inline void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)
 {
 	tp->snd_wl1 = seq;
 }
 
-static inline void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_update_wl(struct tcp_sock *tp, u32 seq)
 {
 	tp->snd_wl1 = seq;
 }

commit cabeccbd172cc305f4383f5a4808ae254745275f
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Feb 28 04:44:38 2009 +0000

    tcp: kill eff_sacks "cache", the sole user can calculate itself
    
    Also fixes insignificant bug that would cause sending of stale
    SACK block (would occur in some corner cases).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0366a559afec..055e4946d4c8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -926,7 +926,6 @@ extern void tcp_done(struct sock *sk);
 static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 {
 	rx_opt->dsack = 0;
-	rx_opt->eff_sacks = 0;
 	rx_opt->num_sacks = 0;
 }
 

commit 758ce5c8d11d6fc57fe5f1dbc237aa8ff6386eac
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Feb 28 04:44:37 2009 +0000

    tcp: add helper for AI algorithm
    
    It seems that implementation in yeah was inconsistent to what
    other did as it would increase cwnd one ack earlier than the
    others do.
    
    Size benefits:
    
      bictcp_cong_avoid |  -36
      tcp_cong_avoid_ai |  +52
      bictcp_cong_avoid |  -34
      tcp_scalable_cong_avoid |  -36
      tcp_veno_cong_avoid |  -12
      tcp_yeah_cong_avoid |  -38
    
    = -104 bytes total
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 218235de8963..0366a559afec 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -685,6 +685,7 @@ extern void tcp_get_allowed_congestion_control(char *buf, size_t len);
 extern int tcp_set_allowed_congestion_control(char *allowed);
 extern int tcp_set_congestion_control(struct sock *sk, const char *name);
 extern void tcp_slow_start(struct tcp_sock *tp);
+extern void tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct sock *sk);

commit bf296b125b21b8d558ceb6ec30bb4eba2730cd6b
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Dec 15 23:43:36 2008 -0800

    tcp: Add GRO support
    
    This patch adds the TCP-specific portion of GRO.  The criterion for
    merging is extremely strict (the TCP header must match exactly apart
    from the checksum) so as to allow refragmentation.  Otherwise this
    is pretty much identical to LRO, except that we support the merging
    of ECN packets.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index de1e91d959b8..218235de8963 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1358,6 +1358,12 @@ extern void tcp_v4_destroy_sock(struct sock *sk);
 
 extern int tcp_v4_gso_send_check(struct sk_buff *skb);
 extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features);
+extern struct sk_buff **tcp_gro_receive(struct sk_buff **head,
+					struct sk_buff *skb);
+extern struct sk_buff **tcp4_gro_receive(struct sk_buff **head,
+					 struct sk_buff *skb);
+extern int tcp_gro_complete(struct sk_buff *skb);
+extern int tcp4_gro_complete(struct sk_buff *skb);
 
 #ifdef CONFIG_PROC_FS
 extern int  tcp4_proc_init(void);

commit dd24c00191d5e4a1ae896aafe33c6b8095ab4bd1
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Nov 25 21:17:14 2008 -0800

    net: Use a percpu_counter for orphan_count
    
    Instead of using one atomic_t per protocol, use a percpu_counter
    for "orphan_count", to reduce cache line contention on
    heavy duty network servers.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cbca3b8a133d..de1e91d959b8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -46,7 +46,7 @@
 
 extern struct inet_hashinfo tcp_hashinfo;
 
-extern atomic_t tcp_orphan_count;
+extern struct percpu_counter tcp_orphan_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)

commit 1748376b6626acf59c24e9592ac67b3fe2a0e026
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Nov 25 21:16:35 2008 -0800

    net: Use a percpu_counter for sockets_allocated
    
    Instead of using one atomic_t per protocol, use a percpu_counter
    for "sockets_allocated", to reduce cache line contention on
    heavy duty network servers.
    
    Note : We revert commit (248969ae31e1b3276fc4399d67ce29a5d81e6fd9
    net: af_unix can make unix_nr_socks visbile in /proc),
    since it is not anymore used after sock_prot_inuse_add() addition
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e8ae90a8c35e..cbca3b8a133d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -238,7 +238,7 @@ extern int sysctl_tcp_slow_start_after_idle;
 extern int sysctl_tcp_max_ssthresh;
 
 extern atomic_t tcp_memory_allocated;
-extern atomic_t tcp_sockets_allocated;
+extern struct percpu_counter tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
 /*

commit 8eecaba900e89643029fd2c253ad8ebb60761165
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Tue Nov 25 13:45:29 2008 -0800

    tcp: tcp_limit_reno_sacked can become static
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 265392470b26..e8ae90a8c35e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -761,8 +761,6 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 	return tp->packets_out - tcp_left_out(tp) + tp->retrans_out;
 }
 
-extern int tcp_limit_reno_sacked(struct tcp_sock *tp);
-
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
  * The exception is rate halving phase, when cwnd is decreasing towards
  * ssthresh.

commit 832d11c5cd076abc0aa1eaf7be96c81d1a59ce41
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Nov 24 21:20:15 2008 -0800

    tcp: Try to restore large SKBs while SACK processing
    
    During SACK processing, most of the benefits of TSO are eaten by
    the SACK blocks that one-by-one fragment SKBs to MSS sized chunks.
    Then we're in problems when cleanup work for them has to be done
    when a large cumulative ACK comes. Try to return back to pre-split
    state already while more and more SACK info gets discovered by
    combining newly discovered SACK areas with the previous skb if
    that's SACKed as well.
    
    This approach has a number of benefits:
    
    1) The processing overhead is spread more equally over the RTT
    2) Write queue has less skbs to process (affect everything
       which has to walk in the queue past the sacked areas)
    3) Write queue is consistent whole the time, so no other parts
       of TCP has to be aware of this (this was not the case with
       some other approach that was, well, quite intrusive all
       around).
    4) Clean_rtx_queue can release most of the pages using single
       put_page instead of previous PAGE_SIZE/mss+1 calls
    
    In case a hole is fully filled by the new SACK block, we attempt
    to combine the next skb too which allows construction of skbs
    that are even larger than what tso split them to and it handles
    hole per on every nth patterns that often occur during slow start
    overshoot pretty nicely. Though this to be really useful also
    a retransmission would have to get lost since cumulative ACKs
    advance one hole at a time in the most typical case.
    
    TODO: handle upwards only merging. That should be rather easy
    when segment is fully sacked but I'm leaving that as future
    work item (it won't make very large difference anyway since
    this current approach already covers quite a lot of normal
    cases).
    
    I was earlier thinking of some sophisticated way of tracking
    timestamps of the first and the last segment but later on
    realized that it won't be that necessary at all to store the
    timestamp of the last segment. The cases that can occur are
    basically either:
      1) ambiguous => no sensible measurement can be taken anyway
      2) non-ambiguous is due to reordering => having the timestamp
         of the last segment there is just skewing things more off
         than does some good since the ack got triggered by one of
         the holes (besides some substle issues that would make
         determining right hole/skb even harder problem). Anyway,
         it has nothing to do with this change then.
    
    I choose to route some abnormal looking cases with goto noop,
    some could be handled differently (eg., by stopping the
    walking at that skb but again). In general, they either
    shouldn't happen at all or are rare enough to make no difference
    in practice.
    
    In theory this change (as whole) could cause some macroscale
    regression (global) because of cache misses that are taken over
    the round-trip time but it gets very likely better because of much
    less (local) cache misses per other write queue walkers and the
    big recovery clearing cumulative ack.
    
    Worth to note that these benefits would be very easy to get also
    without TSO/GSO being on as long as the data is in pages so that
    we can merge them. Currently I won't let that happen because
    DSACK splitting at fragment that would mess up pcounts due to
    sk_can_gso in tcp_set_skb_tso_segs. Once DSACKs fragments gets
    avoided, we have some conditions that can be made less strict.
    
    TODO: I will probably have to convert the excessive pointer
    passing to struct sacktag_state... :-)
    
    My testing revealed that considerable amount of skbs couldn't
    be shifted because they were cloned (most likely still awaiting
    tx reclaim)...
    
    [The rest is considering future work instead since I got
    repeatably EFAULT to tcpdump's recvfrom when I added
    pskb_expand_head to deal with clones, so I separated that
    into another, later patch]
    
    ...To counter that, I gave up on the fifth advantage:
    
    5) When growing previous SACK block, less allocs for new skbs
       are done, basically a new alloc is needed only when new hole
       is detected and when the previous skb runs out of frags space
    
    ...which now only happens of if reclaim is fast enough to dispose
    the clone before the SACK block comes in (the window is RTT long),
    otherwise we'll have to alloc some.
    
    With clones being handled I got these numbers (will be somewhat
    worse without that), taken with fine-grained mibs:
    
                      TCPSackShifted 398
                       TCPSackMerged 877
                TCPSackShiftFallback 320
          TCPSACKCOLLAPSEFALLBACKGSO 0
      TCPSACKCOLLAPSEFALLBACKSKBBITS 0
      TCPSACKCOLLAPSEFALLBACKSKBDATA 0
        TCPSACKCOLLAPSEFALLBACKBELOW 0
        TCPSACKCOLLAPSEFALLBACKFIRST 1
     TCPSACKCOLLAPSEFALLBACKPREVBITS 318
          TCPSACKCOLLAPSEFALLBACKMSS 1
       TCPSACKCOLLAPSEFALLBACKNOHEAD 0
        TCPSACKCOLLAPSEFALLBACKSHIFT 0
              TCPSACKCOLLAPSENOOPSEQ 0
      TCPSACKCOLLAPSENOOPSMALLPCOUNT 0
         TCPSACKCOLLAPSENOOPSMALLLEN 0
                 TCPSACKCOLLAPSEHOLE 12
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 90b4c3b4c336..265392470b26 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1192,6 +1192,11 @@ static inline struct sk_buff *tcp_write_queue_next(struct sock *sk, struct sk_bu
 	return skb_queue_next(&sk->sk_write_queue, skb);
 }
 
+static inline struct sk_buff *tcp_write_queue_prev(struct sock *sk, struct sk_buff *skb)
+{
+	return skb_queue_prev(&sk->sk_write_queue, skb);
+}
+
 #define tcp_for_write_queue(skb, sk)					\
 	skb_queue_walk(&(sk)->sk_write_queue, skb)
 

commit e1aa680fa40e7492260a09cb57d94002245cc8fe
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Nov 24 21:11:55 2008 -0800

    tcp: move tcp_simple_retransmit to tcp_input
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8f26b28fb407..90b4c3b4c336 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -472,8 +472,6 @@ extern void tcp_send_delayed_ack(struct sock *sk);
 
 /* tcp_input.c */
 extern void tcp_cwnd_application_limited(struct sock *sk);
-extern void tcp_skb_mark_lost_uncond_verify(struct tcp_sock *tp,
-					    struct sk_buff *skb);
 
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);

commit 38a7ddffa4b79d7b1fbc9bf2fa82b21b72622858
Author: Petr Tesarik <ptesarik@suse.cz>
Date:   Thu Nov 13 22:44:11 2008 -0800

    tcp: remove an unnecessary field in struct tcp_skb_cb
    
    The urg_ptr field is not used anywhere and is merely confusing.
    
    Signed-off-by: Petr Tesarik <ptesarik@suse.cz>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 438014d57610..8f26b28fb407 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -590,7 +590,6 @@ struct tcp_skb_cb {
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
 
-	__u16		urg_ptr;	/* Valid w/URG flags is set.	*/
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 };
 

commit c57943a1c96214ee68f3890bb6772841ffbfd606
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 7 14:18:42 2008 -0700

    net: wrap sk->sk_backlog_rcv()
    
    Wrap calling sk->sk_backlog_rcv() in a function. This will allow extending the
    generic sk_backlog_rcv behaviour.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f6cc34143154..438014d57610 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -896,7 +896,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 			BUG_ON(sock_owned_by_user(sk));
 
 			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
-				sk->sk_backlog_rcv(sk, skb1);
+				sk_backlog_rcv(sk, skb1);
 				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPREQUEUEDROPPED);
 			}
 

commit a3116ac5c216fc3c145906a46df9ce542ff7dcf2
Author: KOVACS Krisztian <hidden@sch.bme.hu>
Date:   Wed Oct 1 07:46:49 2008 -0700

    tcp: Port redirection support for TCP
    
    Current TCP code relies on the local port of the listening socket
    being the same as the destination address of the incoming
    connection. Port redirection used by many transparent proxying
    techniques obviously breaks this, so we have to store the original
    destination port address.
    
    This patch extends struct inet_request_sock and stores the incoming
    destination port value there. It also modifies the handshake code to
    use that value as the source port when sending reply packets.
    
    Signed-off-by: KOVACS Krisztian <hidden@sch.bme.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 12c9b4fec040..f6cc34143154 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -976,6 +976,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->acked = 0;
 	ireq->ecn_ok = 0;
 	ireq->rmt_port = tcp_hdr(skb)->source;
+	ireq->loc_port = tcp_hdr(skb)->dest;
 }
 
 extern void tcp_enter_memory_pressure(struct sock *sk);

commit cd07a8ea0dd4b204919b4c9ced8d9efdd9924495
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 23 00:50:13 2008 -0700

    tcp: Use SKB queue handling interfaces instead of by-hand versions.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5c5327e9a557..12c9b4fec040 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1181,49 +1181,45 @@ static inline void tcp_write_queue_purge(struct sock *sk)
 
 static inline struct sk_buff *tcp_write_queue_head(struct sock *sk)
 {
-	struct sk_buff *skb = sk->sk_write_queue.next;
-	if (skb == (struct sk_buff *) &sk->sk_write_queue)
-		return NULL;
-	return skb;
+	return skb_peek(&sk->sk_write_queue);
 }
 
 static inline struct sk_buff *tcp_write_queue_tail(struct sock *sk)
 {
-	struct sk_buff *skb = sk->sk_write_queue.prev;
-	if (skb == (struct sk_buff *) &sk->sk_write_queue)
-		return NULL;
-	return skb;
+	return skb_peek_tail(&sk->sk_write_queue);
 }
 
 static inline struct sk_buff *tcp_write_queue_next(struct sock *sk, struct sk_buff *skb)
 {
-	return skb->next;
+	return skb_queue_next(&sk->sk_write_queue, skb);
 }
 
 #define tcp_for_write_queue(skb, sk)					\
-		for (skb = (sk)->sk_write_queue.next;			\
-		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
-		     skb = skb->next)
+	skb_queue_walk(&(sk)->sk_write_queue, skb)
 
 #define tcp_for_write_queue_from(skb, sk)				\
-		for (; (skb != (struct sk_buff *)&(sk)->sk_write_queue);\
-		     skb = skb->next)
+	skb_queue_walk_from(&(sk)->sk_write_queue, skb)
 
 #define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
-		for (tmp = skb->next;					\
-		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
-		     skb = tmp, tmp = skb->next)
+	skb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)
 
 static inline struct sk_buff *tcp_send_head(struct sock *sk)
 {
 	return sk->sk_send_head;
 }
 
+static inline bool tcp_skb_is_last(const struct sock *sk,
+				   const struct sk_buff *skb)
+{
+	return skb_queue_is_last(&sk->sk_write_queue, skb);
+}
+
 static inline void tcp_advance_send_head(struct sock *sk, struct sk_buff *skb)
 {
-	sk->sk_send_head = skb->next;
-	if (sk->sk_send_head == (struct sk_buff *)&sk->sk_write_queue)
+	if (tcp_skb_is_last(sk, skb))
 		sk->sk_send_head = NULL;
+	else
+		sk->sk_send_head = tcp_write_queue_next(sk, skb);
 }
 
 static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)
@@ -1284,12 +1280,6 @@ static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 
-static inline bool tcp_skb_is_last(const struct sock *sk,
-				   const struct sk_buff *skb)
-{
-	return skb_queue_is_last(&sk->sk_write_queue, skb);
-}
-
 static inline int tcp_write_queue_empty(struct sock *sk)
 {
 	return skb_queue_empty(&sk->sk_write_queue);

commit d258b4914bcda9177bcc7bbd8e1a97b281b460af
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 23 00:34:37 2008 -0700

    tcp: Use skb_queue_is_last() instead of by-hand version.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f857c3eff710..5c5327e9a557 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1284,10 +1284,10 @@ static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
 	__skb_unlink(skb, &sk->sk_write_queue);
 }
 
-static inline int tcp_skb_is_last(const struct sock *sk,
-				  const struct sk_buff *skb)
+static inline bool tcp_skb_is_last(const struct sock *sk,
+				   const struct sk_buff *skb)
 {
-	return skb->next == (struct sk_buff *)&sk->sk_write_queue;
+	return skb_queue_is_last(&sk->sk_write_queue, skb);
 }
 
 static inline int tcp_write_queue_empty(struct sock *sk)

commit 43f59c89399fd76883a06c551f24794e98409432
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Sep 21 21:28:51 2008 -0700

    net: Remove __skb_insert() calls outside of skbuff internals.
    
    This minor cleanup simplifies later changes which will convert
    struct sk_buff and friends over to using struct list_head.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ea815723d414..f857c3eff710 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1268,12 +1268,12 @@ static inline void tcp_insert_write_queue_after(struct sk_buff *skb,
 	__skb_queue_after(&sk->sk_write_queue, skb, buff);
 }
 
-/* Insert skb between prev and next on the write queue of sk.  */
+/* Insert new before skb on the write queue of sk.  */
 static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 						  struct sk_buff *skb,
 						  struct sock *sk)
 {
-	__skb_insert(new, skb->prev, skb, &sk->sk_write_queue);
+	__skb_queue_before(&sk->sk_write_queue, skb, new);
 
 	if (sk->sk_send_head == skb)
 		sk->sk_send_head = new;

commit ef9da47c7cc64d69526331f315e76b5680d4048f
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:25:15 2008 -0700

    tcp: don't clear retransmit_skb_hint when not necessary
    
    Most importantly avoid doing it with cumulative ACK. Not clearing
    means that we no longer need n^2 processing in resolution of each
    fast recovery.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 220f54cf42ec..ea815723d414 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1037,10 +1037,15 @@ static inline void tcp_mib_init(struct net *net)
 }
 
 /* from STCP */
-static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
+static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
 {
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
+}
+
+static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
+{
+	tcp_clear_retrans_hints_partial(tp);
 	tp->retransmit_skb_hint = NULL;
 }
 

commit 0e1c54c2a405494281e0639aacc90db03b50ae77
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:24:21 2008 -0700

    tcp: reorganize retransmit code loops
    
    Both loops are quite similar, so they can be combined
    with little effort. As a result, forward_skb_hint becomes
    obsolete as well.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d0e90c50722b..220f54cf42ec 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1042,7 +1042,6 @@ static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
 	tp->retransmit_skb_hint = NULL;
-	tp->forward_skb_hint = NULL;
 }
 
 /* MD5 Signature */

commit 006f582c73f4eda35e06fd323193c3df43fb3459
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:20:20 2008 -0700

    tcp: convert retransmit_cnt_hint to seqno
    
    Main benefit in this is that we can then freely point
    the retransmit_skb_hint to anywhere we want to because
    there's no longer need to know what would be the count
    changes involve, and since this is really used only as a
    terminator, unnecessary work is one time walk at most,
    and if some retransmissions are necessary after that
    point later on, the walk is not full waste of time
    anyway.
    
    Since retransmit_high must be kept valid, all lost
    markers must ensure that.
    
    Now I also have learned how those "holes" in the
    rexmittable skbs can appear, mtu probe does them. So
    I removed the misleading comment as well.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b71676326950..d0e90c50722b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -472,6 +472,8 @@ extern void tcp_send_delayed_ack(struct sock *sk);
 
 /* tcp_input.c */
 extern void tcp_cwnd_application_limited(struct sock *sk);
+extern void tcp_skb_mark_lost_uncond_verify(struct tcp_sock *tp,
+					    struct sk_buff *skb);
 
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);

commit 64edc2736e23994e0334b70c5ff08dc33e2ebbd9
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Sep 20 21:18:32 2008 -0700

    tcp: Partial hint clearing has again become meaningless
    
    Ie., the difference between partial and all clearing doesn't
    exists anymore since the SACK optimizations got dropped by
    an sacktag rewrite.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8983386356a5..b71676326950 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1035,7 +1035,7 @@ static inline void tcp_mib_init(struct net *net)
 }
 
 /* from STCP */
-static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
+static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
 {
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
@@ -1043,11 +1043,6 @@ static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
 	tp->forward_skb_hint = NULL;
 }
 
-static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
-{
-	tcp_clear_retrans_hints_partial(tp);
-}
-
 /* MD5 Signature */
 struct crypto_hash;
 

commit 410e27a49bb98bc7fa3ff5fc05cc313817b9f253
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Tue Sep 9 13:27:22 2008 +0200

    This reverts "Merge branch 'dccp' of git://eden-feed.erg.abdn.ac.uk/dccp_exp"
    as it accentally contained the wrong set of patches. These will be
    submitted separately.
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6bc4b8148ca0..8983386356a5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -782,21 +782,6 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
-/*
- * Convert RFC3390 larger initial windows into an equivalent number of packets.
- *
- * John Heffner states:
- *
- *	The RFC specifies a window of no more than 4380 bytes
- *	unless 2*MSS > 4380.  Reading the pseudocode in the RFC
- *	is a bit misleading because they use a clamp at 4380 bytes
- *	rather than a multiplier in the relevant range.
- */
-static inline u32 rfc3390_bytes_to_packets(const u32 bytes)
-{
-	return bytes <= 1095 ? 4 : (bytes > 1460 ? 2 : 3);
-}
-
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 

commit 6224877b2ca4be5de96270a8ae490fe2ba11b0e0
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Thu Sep 4 07:30:19 2008 +0200

    tcp/dccp: Consolidate common code for RFC 3390 conversion
    
    This patch consolidates the code common to TCP and CCID-2:
     * TCP uses RFC 3390 in a packet-oriented manner (tcp_input.c) and
     * CCID-2 uses RFC 3390 in packet-oriented manner (RFC 4341).
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8983386356a5..6bc4b8148ca0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -782,6 +782,21 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 /* Use define here intentionally to get WARN_ON location shown at the caller */
 #define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
+/*
+ * Convert RFC3390 larger initial windows into an equivalent number of packets.
+ *
+ * John Heffner states:
+ *
+ *	The RFC specifies a window of no more than 4380 bytes
+ *	unless 2*MSS > 4380.  Reading the pseudocode in the RFC
+ *	is a bit misleading because they use a clamp at 4380 bytes
+ *	rather than a multiplier in the relevant range.
+ */
+static inline u32 rfc3390_bytes_to_packets(const u32 bytes)
+{
+	return bytes <= 1095 ? 4 : (bytes > 1460 ? 2 : 3);
+}
+
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 

commit 33ad798c924b4a1afad3593f2796d465040aadd5
Author: Adam Langley <agl@imperialviolet.org>
Date:   Sat Jul 19 00:04:31 2008 -0700

    tcp: options clean up
    
    This should fix the following bugs:
      * Connections with MD5 signatures produce invalid packets whenever SACK
        options are included
      * MD5 signatures are counted twice in the MSS calculations
    
    Behaviour changes:
      * A SYN with MD5 + SACK + TS elicits a SYNACK with MD5 + SACK
    
        This is because we can't fit any SACK blocks in a packet with MD5 + TS
        options. There was discussion about disabling SACK rather than TS in
        order to fit in better with old, buggy kernels, but that was deemed to
        be unnecessary.
    
      * SYNs with MD5 don't include a TS option
    
        See above.
    
    Additionally, it removes a bunch of duplicated logic for calculating options,
    which should help avoid these sort of issues in the future.
    
    Signed-off-by: Adam Langley <agl@imperialviolet.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 31f5bbfc59bc..8983386356a5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -50,6 +50,7 @@ extern atomic_t tcp_orphan_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
+#define MAX_TCP_OPTION_SPACE 40
 
 /* 
  * Never offer a window over 32767 without using window scaling. Some
@@ -184,6 +185,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_BASE_ALIGNED	4
 #define TCPOLEN_SACK_PERBLOCK		8
 #define TCPOLEN_MD5SIG_ALIGNED		20
+#define TCPOLEN_MSS_ALIGNED		4
 
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */

commit 49a72dfb8814c2d65bd9f8c9c6daf6395a1ec58d
Author: Adam Langley <agl@imperialviolet.org>
Date:   Sat Jul 19 00:01:42 2008 -0700

    tcp: Fix MD5 signatures for non-linear skbs
    
    Currently, the MD5 code assumes that the SKBs are linear and, in the case
    that they aren't, happily goes off and hashes off the end of the SKB and
    into random memory.
    
    Reported by Stephen Hemminger in [1]. Advice thanks to Stephen and Evgeniy
    Polyakov. Also includes a couple of missed route_caps from Stephen's patch
    in [2].
    
    [1] http://marc.info/?l=linux-netdev&m=121445989106145&w=2
    [2] http://marc.info/?l=linux-netdev&m=121459157816964&w=2
    
    Signed-off-by: Adam Langley <agl@imperialviolet.org>
    Acked-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 92d7b551dc55..31f5bbfc59bc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1112,20 +1112,12 @@ struct tcp_md5sig_pool {
 #define TCP_MD5SIG_MAXKEYS	(~(u32)0)	/* really?! */
 
 /* - functions */
-extern int			tcp_calc_md5_hash(char *md5_hash,
-						  struct tcp_md5sig_key *key,
-						  int bplen,
-						  struct tcphdr *th,
-						  unsigned int tcplen,
-						  struct tcp_md5sig_pool *hp);
-
-extern int			tcp_v4_calc_md5_hash(char *md5_hash,
-						     struct tcp_md5sig_key *key,
-						     struct sock *sk,
-						     struct dst_entry *dst,
-						     struct request_sock *req,
-						     struct tcphdr *th,
-						     unsigned int tcplen);
+extern int			tcp_v4_md5_hash_skb(char *md5_hash,
+						    struct tcp_md5sig_key *key,
+						    struct sock *sk,
+						    struct request_sock *req,
+						    struct sk_buff *skb);
+
 extern struct tcp_md5sig_key	*tcp_v4_md5_lookup(struct sock *sk,
 						   struct sock *addr_sk);
 
@@ -1152,6 +1144,11 @@ extern void			tcp_free_md5sig_pool(void);
 
 extern struct tcp_md5sig_pool	*__tcp_get_md5sig_pool(int cpu);
 extern void			__tcp_put_md5sig_pool(void);
+extern int tcp_md5_hash_header(struct tcp_md5sig_pool *, struct tcphdr *);
+extern int tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, struct sk_buff *,
+				 unsigned header_len);
+extern int tcp_md5_hash_key(struct tcp_md5sig_pool *hp,
+			    struct tcp_md5sig_key *key);
 
 static inline
 struct tcp_md5sig_pool		*tcp_get_md5sig_pool(void)
@@ -1381,10 +1378,8 @@ struct tcp_sock_af_ops {
 	int			(*calc_md5_hash) (char *location,
 						  struct tcp_md5sig_key *md5,
 						  struct sock *sk,
-						  struct dst_entry *dst,
 						  struct request_sock *req,
-						  struct tcphdr *th,
-						  unsigned int len);
+						  struct sk_buff *skb);
 	int			(*md5_add) (struct sock *sk,
 					    struct sock *addr_sk,
 					    u8 *newkey,

commit 57ef42d59d1c1d79be59fc3c6380ae14234e38c3
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Jul 18 04:02:08 2008 -0700

    mib: put tcp statistics on struct net
    
    Proc temporary uses stats from init_net.
    
    BTW, TCP_XXX_STATS are beautiful (w/o do { } while (0) facing) again :)
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 60e5be8b925b..92d7b551dc55 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -265,11 +265,10 @@ static inline int tcp_too_many_orphans(struct sock *sk, int num)
 
 extern struct proto tcp_prot;
 
-DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
-#define TCP_INC_STATS(net, field)	do { (void)net; SNMP_INC_STATS(tcp_statistics, field); } while (0)
-#define TCP_INC_STATS_BH(net, field)	do { (void)net; SNMP_INC_STATS_BH(tcp_statistics, field); } while (0)
-#define TCP_DEC_STATS(net, field)	do { (void)net; SNMP_DEC_STATS(tcp_statistics, field); } while (0)
-#define TCP_ADD_STATS_USER(net, field, val) do { (void)net; SNMP_ADD_STATS_USER(tcp_statistics, field, val); } while (0)
+#define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
+#define TCP_INC_STATS_BH(net, field)	SNMP_INC_STATS_BH((net)->mib.tcp_statistics, field)
+#define TCP_DEC_STATS(net, field)	SNMP_DEC_STATS((net)->mib.tcp_statistics, field)
+#define TCP_ADD_STATS_USER(net, field, val) SNMP_ADD_STATS_USER((net)->mib.tcp_statistics, field, val)
 
 extern void			tcp_v4_err(struct sk_buff *skb, u32);
 

commit de0744af1fe2d0a3d428f6af0f2fe1f6179b1a9c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:31:16 2008 -0700

    mib: add net to NET_INC_STATS_BH
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c25cb5278b95..60e5be8b925b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -894,7 +894,7 @@ static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 
 			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
 				sk->sk_backlog_rcv(sk, skb1);
-				NET_INC_STATS_BH(LINUX_MIB_TCPPREQUEUEDROPPED);
+				NET_INC_STATS_BH(sock_net(sk), LINUX_MIB_TCPPREQUEUEDROPPED);
 			}
 
 			tp->ucopy.memory = 0;

commit 5c52ba170f8167511bdb65b981f4582100c40675
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:28:10 2008 -0700

    sock: add net to prot->enter_memory_pressure callback
    
    The tcp_enter_memory_pressure calls NET_INC_STATS, but doesn't
    have where to get the net from.
    
    I decided to add a sk argument, not the net itself, only to factor
    all the required sock_net(sk) calls inside the enter_memory_pressure
    callback itself.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4d788181654e..c25cb5278b95 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -975,7 +975,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->rmt_port = tcp_hdr(skb)->source;
 }
 
-extern void tcp_enter_memory_pressure(void);
+extern void tcp_enter_memory_pressure(struct sock *sk);
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 {

commit cf1100a7a4f2573f50f9a923b53373977328e3c8
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:27:38 2008 -0700

    mib: add net to TCP_ADD_STATS_USER
    
    Now we're done with the TCP_XXX_STATS macros.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 92d87691aa61..4d788181654e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -269,7 +269,7 @@ DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
 #define TCP_INC_STATS(net, field)	do { (void)net; SNMP_INC_STATS(tcp_statistics, field); } while (0)
 #define TCP_INC_STATS_BH(net, field)	do { (void)net; SNMP_INC_STATS_BH(tcp_statistics, field); } while (0)
 #define TCP_DEC_STATS(net, field)	do { (void)net; SNMP_DEC_STATS(tcp_statistics, field); } while (0)
-#define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
+#define TCP_ADD_STATS_USER(net, field, val) do { (void)net; SNMP_ADD_STATS_USER(tcp_statistics, field, val); } while (0)
 
 extern void			tcp_v4_err(struct sk_buff *skb, u32);
 
@@ -1027,10 +1027,10 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 static inline void tcp_mib_init(struct net *net)
 {
 	/* See RFC 2012 */
-	TCP_ADD_STATS_USER(TCP_MIB_RTOALGORITHM, 1);
-	TCP_ADD_STATS_USER(TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);
-	TCP_ADD_STATS_USER(TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);
-	TCP_ADD_STATS_USER(TCP_MIB_MAXCONN, -1);
+	TCP_ADD_STATS_USER(net, TCP_MIB_RTOALGORITHM, 1);
+	TCP_ADD_STATS_USER(net, TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);
+	TCP_ADD_STATS_USER(net, TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);
+	TCP_ADD_STATS_USER(net, TCP_MIB_MAXCONN, -1);
 }
 
 /* from STCP */

commit 74688e487a407a33d42879957b478601aca616b8
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:22:46 2008 -0700

    mib: add net to TCP_DEC_STATS
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8a2cd076be82..92d87691aa61 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -268,7 +268,7 @@ extern struct proto tcp_prot;
 DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
 #define TCP_INC_STATS(net, field)	do { (void)net; SNMP_INC_STATS(tcp_statistics, field); } while (0)
 #define TCP_INC_STATS_BH(net, field)	do { (void)net; SNMP_INC_STATS_BH(tcp_statistics, field); } while (0)
-#define TCP_DEC_STATS(field)		SNMP_DEC_STATS(tcp_statistics, field)
+#define TCP_DEC_STATS(net, field)	do { (void)net; SNMP_DEC_STATS(tcp_statistics, field); } while (0)
 #define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
 
 extern void			tcp_v4_err(struct sk_buff *skb, u32);

commit 63231bddf6514778792d3784f63822473d250fc0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:22:25 2008 -0700

    mib: add net to TCP_INC_STATS_BH
    
    Same as before - the sock is always there to get the net from,
    but there are also some places with the net already saved on
    the stack.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 15065157afc9..8a2cd076be82 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -267,7 +267,7 @@ extern struct proto tcp_prot;
 
 DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
 #define TCP_INC_STATS(net, field)	do { (void)net; SNMP_INC_STATS(tcp_statistics, field); } while (0)
-#define TCP_INC_STATS_BH(field)		SNMP_INC_STATS_BH(tcp_statistics, field)
+#define TCP_INC_STATS_BH(net, field)	do { (void)net; SNMP_INC_STATS_BH(tcp_statistics, field); } while (0)
 #define TCP_DEC_STATS(field)		SNMP_DEC_STATS(tcp_statistics, field)
 #define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
 

commit 81cc8a75d944fa39fc333c2c329c8e8b3c62cada
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:22:04 2008 -0700

    mib: add net to TCP_INC_STATS
    
    Fortunately (almost) all the TCP code has a sock to get the net from :)
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b9d0ba618b6b..15065157afc9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -266,7 +266,7 @@ static inline int tcp_too_many_orphans(struct sock *sk, int num)
 extern struct proto tcp_prot;
 
 DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
-#define TCP_INC_STATS(field)		SNMP_INC_STATS(tcp_statistics, field)
+#define TCP_INC_STATS(net, field)	do { (void)net; SNMP_INC_STATS(tcp_statistics, field); } while (0)
 #define TCP_INC_STATS_BH(field)		SNMP_INC_STATS_BH(tcp_statistics, field)
 #define TCP_DEC_STATS(field)		SNMP_DEC_STATS(tcp_statistics, field)
 #define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)

commit a9c19329eccdb145a08a4a2e969d7b40c54c9bcc
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:21:42 2008 -0700

    tcp: add net to tcp_mib_init
    
    This one sets TCP MIBs after zeroing them, and thus requires
    the net.
    
    The existing single caller can use init_net (temporarily).
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 048f8de724c2..b9d0ba618b6b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1024,7 +1024,7 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 
 #define TCP_CHECK_TIMER(sk) do { } while (0)
 
-static inline void tcp_mib_init(void)
+static inline void tcp_mib_init(struct net *net)
 {
 	/* See RFC 2012 */
 	TCP_ADD_STATS_USER(TCP_MIB_RTOALGORITHM, 1);

commit f10f84314d5adc0ba6ca2a0877442de63e226130
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:21:20 2008 -0700

    mib: drop unused TCP_XXX_STATS macros
    
    TCP_INC_STATS_USER and TCP_ADD_STATS_BH are currently unused.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6b827cc6c52f..048f8de724c2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -268,9 +268,7 @@ extern struct proto tcp_prot;
 DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
 #define TCP_INC_STATS(field)		SNMP_INC_STATS(tcp_statistics, field)
 #define TCP_INC_STATS_BH(field)		SNMP_INC_STATS_BH(tcp_statistics, field)
-#define TCP_INC_STATS_USER(field) 	SNMP_INC_STATS_USER(tcp_statistics, field)
 #define TCP_DEC_STATS(field)		SNMP_DEC_STATS(tcp_statistics, field)
-#define TCP_ADD_STATS_BH(field, val)	SNMP_ADD_STATS_BH(tcp_statistics, field, val)
 #define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
 
 extern void			tcp_v4_err(struct sk_buff *skb, u32);

commit 7d06b2e053d2d536348e3a0f6bb02982a41bea37
Author: Brian Haley <brian.haley@hp.com>
Date:   Sat Jun 14 17:04:49 2008 -0700

    net: change proto destroy method to return void
    
    Change struct proto destroy function pointer to return void.  Noticed
    by Al Viro.
    
    Signed-off-by: Brian Haley <brian.haley@hp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b5a1b9eb12e8..6b827cc6c52f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1366,7 +1366,7 @@ extern void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo);
 extern struct request_sock_ops tcp_request_sock_ops;
 extern struct request_sock_ops tcp6_request_sock_ops;
 
-extern int tcp_v4_destroy_sock(struct sock *sk);
+extern void tcp_v4_destroy_sock(struct sock *sk);
 
 extern int tcp_v4_gso_send_check(struct sk_buff *skb);
 extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features);

commit 4ae127d1b6c71f9240dd4245f240e6dd8fc98014
Merge: 875ec4333b99 7775c9753b94
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 13 20:52:39 2008 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
    
            drivers/net/smc911x.c

commit ec0a196626bd12e0ba108d7daa6d95a4fb25c2c5
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 12 16:31:35 2008 -0700

    tcp: Revert 'process defer accept as established' changes.
    
    This reverts two changesets, ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
    ("[TCP]: TCP_DEFER_ACCEPT updates - process as established") and
    the follow-on bug fix 9ae27e0adbf471c7a6b80102e38e1d5a346b3b38
    ("tcp: Fix slab corruption with ipv6 and tcp6fuzz").
    
    This change causes several problems, first reported by Ingo Molnar
    as a distcc-over-loopback regression where connections were getting
    stuck.
    
    Ilpo Järvinen first spotted the locking problems.  The new function
    added by this code, tcp_defer_accept_check(), only has the
    child socket locked, yet it is modifying state of the parent
    listening socket.
    
    Fixing that is non-trivial at best, because we can't simply just grab
    the parent listening socket lock at this point, because it would
    create an ABBA deadlock.  The normal ordering is parent listening
    socket --> child socket, but this code path would require the
    reverse lock ordering.
    
    Next is a problem noticed by Vitaliy Gusev, he noted:
    
    ----------------------------------------
    >--- a/net/ipv4/tcp_timer.c
    >+++ b/net/ipv4/tcp_timer.c
    >@@ -481,6 +481,11 @@ static void tcp_keepalive_timer (unsigned long data)
    >               goto death;
    >       }
    >
    >+      if (tp->defer_tcp_accept.request && sk->sk_state == TCP_ESTABLISHED) {
    >+              tcp_send_active_reset(sk, GFP_ATOMIC);
    >+              goto death;
    
    Here socket sk is not attached to listening socket's request queue. tcp_done()
    will not call inet_csk_destroy_sock() (and tcp_v4_destroy_sock() which should
    release this sk) as socket is not DEAD. Therefore socket sk will be lost for
    freeing.
    ----------------------------------------
    
    Finally, Alexey Kuznetsov argues that there might not even be any
    real value or advantage to these new semantics even if we fix all
    of the bugs:
    
    ----------------------------------------
    Hiding from accept() sockets with only out-of-order data only
    is the only thing which is impossible with old approach. Is this really
    so valuable? My opinion: no, this is nothing but a new loophole
    to consume memory without control.
    ----------------------------------------
    
    So revert this thing for now.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d448310c82c1..cf54034019d9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -139,7 +139,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define MAX_TCP_KEEPINTVL	32767
 #define MAX_TCP_KEEPCNT		127
 #define MAX_TCP_SYNCNT		127
-#define MAX_TCP_ACCEPT_DEFERRED 65535
 
 #define TCP_SYNQ_INTERVAL	(HZ/5)	/* Period of SYNACK timer */
 

commit 9501f9722922f2e80e1f9dc6682311d65c2b5690
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Fri Apr 18 12:45:16 2008 +0900

    tcp md5sig: Let the caller pass appropriate key for tcp_v{4,6}_do_calc_md5_hash().
    
    As we do for other socket/timewait-socket specific parameters,
    let the callers pass appropriate arguments to
    tcp_v{4,6}_do_calc_md5_hash().
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index eac26b73bcc3..07005ebb47a7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1142,6 +1142,16 @@ extern int			tcp_v4_md5_do_add(struct sock *sk,
 extern int			tcp_v4_md5_do_del(struct sock *sk,
 						  __be32 addr);
 
+#ifdef CONFIG_TCP_MD5SIG
+#define tcp_twsk_md5_key(twsk)	((twsk)->tw_md5_keylen ? 		 \
+				 &(struct tcp_md5sig_key) {		 \
+					.key = (twsk)->tw_md5_key,	 \
+					.keylen = (twsk)->tw_md5_keylen, \
+				} : NULL)
+#else
+#define tcp_twsk_md5_key(twsk)	NULL
+#endif
+
 extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(void);
 extern void			tcp_free_md5sig_pool(void);
 

commit 8d26d76dd4a4c87ef037a44a42a0608ffc730199
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Thu Apr 17 13:19:16 2008 +0900

    tcp md5sig: Share most of hash calcucaltion bits between IPv4 and IPv6.
    
    We can share most part of the hash calculation code because
    the only difference between IPv4 and IPv6 is their pseudo headers.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ae2549c45fc7..eac26b73bcc3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1117,6 +1117,13 @@ struct tcp_md5sig_pool {
 #define TCP_MD5SIG_MAXKEYS	(~(u32)0)	/* really?! */
 
 /* - functions */
+extern int			tcp_calc_md5_hash(char *md5_hash,
+						  struct tcp_md5sig_key *key,
+						  int bplen,
+						  struct tcphdr *th,
+						  unsigned int tcplen,
+						  struct tcp_md5sig_pool *hp);
+
 extern int			tcp_v4_calc_md5_hash(char *md5_hash,
 						     struct tcp_md5sig_key *key,
 						     struct sock *sk,

commit 076fb7223357769c39f3ddf900bba6752369c76a
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Thu Apr 17 12:48:12 2008 +0900

    tcp md5sig: Remove redundant protocol argument.
    
    Protocol is always TCP, so remove useless protocol argument.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b1c4b78f1cb8..ae2549c45fc7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1123,7 +1123,6 @@ extern int			tcp_v4_calc_md5_hash(char *md5_hash,
 						     struct dst_entry *dst,
 						     struct request_sock *req,
 						     struct tcphdr *th,
-						     int protocol,
 						     unsigned int tcplen);
 extern struct tcp_md5sig_key	*tcp_v4_md5_lookup(struct sock *sk,
 						   struct sock *addr_sk);
@@ -1373,7 +1372,6 @@ struct tcp_sock_af_ops {
 						  struct dst_entry *dst,
 						  struct request_sock *req,
 						  struct tcphdr *th,
-						  int protocol,
 						  unsigned int len);
 	int			(*md5_add) (struct sock *sk,
 					    struct sock *addr_sk,

commit 7d5d5525bd88313e6fd90c0659665aee5114bc2d
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Thu Apr 17 12:29:53 2008 +0900

    tcp md5sig: Share MD5 Signature option parser between IPv4 and IPv6.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 633147cb6bbc..b1c4b78f1cb8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -399,6 +399,8 @@ extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
 						  int estab);
 
+extern u8			*tcp_parse_md5sig_option(struct tcphdr *th);
+
 /*
  *	TCP v4 functions exported for the inet6 API
  */

commit 45d465bc237ab1e1ebb4c65b9b318830dafb7509
Author: Rami Rosen <ramirose@gmail.com>
Date:   Tue Jun 10 12:37:42 2008 -0700

    ipv4: Remove unused declaration from include/net/tcp.h.
    
    - The tcp_unhash() method in /include/net/tcp.h is no more needed, as the
    unhash method in tcp_prot structure is now inet_unhash (instead of
    tcp_unhash in the
    past); see tcp_prot structure in net/ipv4/tcp_ipv4.c.
    
    - So, this patch removes tcp_unhash() declaration from include/net/tcp.h
    
    Signed-off-by: Rami Rosen <ramirose@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 633147cb6bbc..d448310c82c1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -433,7 +433,6 @@ extern struct sk_buff *		tcp_make_synack(struct sock *sk,
 
 extern int			tcp_disconnect(struct sock *sk, int flags);
 
-extern void			tcp_unhash(struct sock *sk);
 
 /* From syncookies.c */
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];

commit dd9e0dda66ba38a2ddd1405ac279894260dc5c36
Author: John Heffner <johnwheffner@gmail.com>
Date:   Tue Apr 15 15:26:39 2008 -0700

    [TCP]: Increase the max_burst threshold from 3 to tp->reordering.
    
    This change is necessary to allow cwnd to grow during persistent
    reordering.  Cwnd moderation is applied when in the disorder state
    and an ack that fills the hole comes in.  If the hole was greater
    than 3 packets, but less than tp->reordering, cwnd will shrink when
    it should not have.
    
    Signed-off-by: John Heffner <jheffner@napa.(none)>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2c14edf7b076..633147cb6bbc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -787,11 +787,14 @@ extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 
 /* Slow start with delack produces 3 packets of burst, so that
- * it is safe "de facto".
+ * it is safe "de facto".  This will be the default - same as
+ * the default reordering threshold - but if reordering increases,
+ * we must be able to allow cwnd to burst at least this much in order
+ * to not pull it back when holes are filled.
  */
 static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
 {
-	return 3;
+	return tp->reordering;
 }
 
 /* Returns end sequence number of the receiver's advertised window */

commit df39e8ba56a788733d369068c7319e04b1da3cd5
Merge: f5572855ec49 159d83363b62
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Apr 14 02:30:23 2008 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
    
            drivers/net/ehea/ehea_main.c
            drivers/net/wireless/iwlwifi/Kconfig
            drivers/net/wireless/rt2x00/rt61pci.c
            net/ipv4/inet_timewait_sock.c
            net/ipv6/raw.c
            net/mac80211/ieee80211_sta.c

commit 7de6c033367ab86f39c7723392caf73325cbf286
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Mon Apr 14 00:05:09 2008 -0700

    [SKB]: __skb_append = __skb_queue_after
    
    This expresses __skb_append in terms of __skb_queue_after, exploiting that
    
      __skb_append(old, new, list) = __skb_queue_after(list, old, new).
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 58d82822414d..2ab350eca02e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1247,7 +1247,7 @@ static inline void tcp_insert_write_queue_after(struct sk_buff *skb,
 						struct sk_buff *buff,
 						struct sock *sk)
 {
-	__skb_append(skb, buff, &sk->sk_write_queue);
+	__skb_queue_after(&sk->sk_write_queue, skb, buff);
 }
 
 /* Insert skb between prev and next on the write queue of sk.  */

commit 5f4472c5a640c9671ca5becaebdfd6e651482176
Author: Denis V. Lunev <den@openvz.org>
Date:   Sun Apr 13 22:13:53 2008 -0700

    [TCP]: Remove owner from tcp_seq_afinfo.
    
    Move it to tcp_seq_afinfo->seq_fops as should be.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 93479ebf3e33..58d82822414d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1325,7 +1325,6 @@ enum tcp_seq_states {
 };
 
 struct tcp_seq_afinfo {
-	struct module		*owner;
 	char			*name;
 	sa_family_t		family;
 	struct file_operations	seq_fops;

commit 68fcadd16c371d5e0698ba366f33a4f990ce83ce
Author: Denis V. Lunev <den@openvz.org>
Date:   Sun Apr 13 22:13:30 2008 -0700

    [TCP]: Place file operations directly into tcp_seq_afinfo.
    
    No need to have separate never-used variable.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2c9a650412c0..93479ebf3e33 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1328,7 +1328,7 @@ struct tcp_seq_afinfo {
 	struct module		*owner;
 	char			*name;
 	sa_family_t		family;
-	struct file_operations	*seq_fops;
+	struct file_operations	seq_fops;
 	struct seq_operations	seq_ops;
 };
 

commit 9427c4b36b8fe652df1d7c89eae678948e1f4b32
Author: Denis V. Lunev <den@openvz.org>
Date:   Sun Apr 13 22:12:13 2008 -0700

    [TCP]: Move seq_ops from tcp_iter_state to tcp_seq_afinfo.
    
    No need to create seq_operations for each instance of 'netstat'.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f5b61e0f01f2..2c9a650412c0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1328,8 +1328,8 @@ struct tcp_seq_afinfo {
 	struct module		*owner;
 	char			*name;
 	sa_family_t		family;
-	int			(*seq_show) (struct seq_file *m, void *v);
 	struct file_operations	*seq_fops;
+	struct seq_operations	seq_ops;
 };
 
 struct tcp_iter_state {
@@ -1338,7 +1338,6 @@ struct tcp_iter_state {
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;
 	int			bucket, sbucket, num, uid;
-	struct seq_operations	seq_ops;
 };
 
 extern int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);

commit a4146b1b2c6ba995db08b1a2aef5af1b17b151e6
Author: Denis V. Lunev <den@openvz.org>
Date:   Sun Apr 13 22:11:14 2008 -0700

    [TCP]: Replace struct net on tcp_iter_state with seq_net_private.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7b41bb962b9c..f5b61e0f01f2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1333,7 +1333,7 @@ struct tcp_seq_afinfo {
 };
 
 struct tcp_iter_state {
-	struct net              *net;
+	struct seq_net_private	p;
 	sa_family_t		family;
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;

commit 4dfc2817025965a2fc78a18c50f540736a6b5c24
Author: Florian Westphal <fw@strlen.de>
Date:   Thu Apr 10 03:12:40 2008 -0700

    [Syncookies]: Add support for TCP options via timestamps.
    
    Allow the use of SACK and window scaling when syncookies are used
    and the client supports tcp timestamps. Options are encoded into
    the timestamp sent in the syn-ack and restored from the timestamp
    echo when the ack is received.
    
    Based on earlier work by Glenn Griffin.
    This patch avoids increasing the size of structs by encoding TCP
    options into the least significant bits of the timestamp and
    by not using any 'timestamp offset'.
    
    The downside is that the timestamp sent in the packet after the synack
    will increase by several seconds.
    
    changes since v1:
     don't duplicate timestamp echo decoding function, put it into ipv4/syncookie.c
     and have ipv6/syncookies.c use it.
     Feedback from Glenn Griffin: fix line indented with spaces, kill redundant if ()
    
    Reviewed-by: Hagen Paul Pfeifer <hagen@jauu.net>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 723b36851dde..7b41bb962b9c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -442,6 +442,9 @@ extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 				     __u16 *mss);
 
+extern __u32 cookie_init_timestamp(struct request_sock *req);
+extern void cookie_check_timestamp(struct tcp_options_received *tcp_opt);
+
 /* From net/ipv6/syncookies.c */
 extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
 extern __u32 cookie_v6_init_sequence(struct sock *sk, struct sk_buff *skb,
@@ -956,6 +959,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	struct inet_request_sock *ireq = inet_rsk(req);
 
 	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
+	req->cookie_ts = 0;
 	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;

commit 882bebaaca4bb1484078d44ef011f918c0e1e14e
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Apr 7 22:33:07 2008 -0700

    [TCP]: tcp_simple_retransmit can cause S+L
    
    This fixes Bugzilla #10384
    
    tcp_simple_retransmit does L increment without any checking
    whatsoever for overflowing S+L when Reno is in use.
    
    The simplest scenario I can currently think of is rather
    complex in practice (there might be some more straightforward
    cases though). Ie., if mss is reduced during mtu probing, it
    may end up marking everything lost and if some duplicate ACKs
    arrived prior to that sacked_out will be non-zero as well,
    leading to S+L > packets_out, tcp_clean_rtx_queue on the next
    cumulative ACK or tcp_fastretrans_alert on the next duplicate
    ACK will fix the S counter.
    
    More straightforward (but questionable) solution would be to
    just call tcp_reset_reno_sack() in tcp_simple_retransmit but
    it would negatively impact the probe's retransmission, ie.,
    the retransmissions would not occur if some duplicate ACKs
    had arrived.
    
    So I had to add reno sacked_out reseting to CA_Loss state
    when the first cumulative ACK arrives (this stale sacked_out
    might actually be the explanation for the reports of left_out
    overflows in kernel prior to 2.6.23 and S+L overflow reports
    of 2.6.24). However, this alone won't be enough to fix kernel
    before 2.6.24 because it is building on top of the commit
    1b6d427bb7e ([TCP]: Reduce sacked_out with reno when purging
    write_queue) to keep the sacked_out from overflowing.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Reported-by: Alessandro Suardi <alessandro.suardi@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7de4ea3a04d9..4fd3eb2f8ec2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -752,6 +752,8 @@ static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 	return tp->packets_out - tcp_left_out(tp) + tp->retrans_out;
 }
 
+extern int tcp_limit_reno_sacked(struct tcp_sock *tp);
+
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
  * The exception is rate halving phase, when cwnd is decreasing towards
  * ssthresh.

commit 2051f11fb86b0056fec440fe7e9fa8370d60a5c6
Author: Florian Westphal <fw@strlen.de>
Date:   Sun Mar 23 22:21:28 2008 -0700

    [TCP]: Shrink syncookie_secret by 8 byte.
    
    the first u32 copied from syncookie_secret is overwritten by the
    minute-counter four lines below.  After adjusting the destination
    address, the size of syncookie_secret can be reduced accordingly.
    
    AFAICS, the only other user of syncookie_secret[] is the ipv6
    syncookie support.  Because ipv6 syncookies only grab 44 bytes from
    syncookie_secret[], this shouldn't affect them in any way.
    
    With fixes from Glenn Griffin.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Glenn Griffin <ggriffin.kernel@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 67cc3956d29c..723b36851dde 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -436,7 +436,7 @@ extern int			tcp_disconnect(struct sock *sk, int flags);
 extern void			tcp_unhash(struct sock *sk);
 
 /* From syncookies.c */
-extern __u32 syncookie_secret[2][16-3+SHA_DIGEST_WORDS];
+extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
 extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
 				    struct ip_options *opt);
 extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 

commit ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
Author: Patrick McManus <mcmanus@ducksong.com>
Date:   Fri Mar 21 16:33:01 2008 -0700

    [TCP]: TCP_DEFER_ACCEPT updates - process as established
    
    Change TCP_DEFER_ACCEPT implementation so that it transitions a
    connection to ESTABLISHED after handshake is complete instead of
    leaving it in SYN-RECV until some data arrvies. Place connection in
    accept queue when first data packet arrives from slow path.
    
    Benefits:
      - established connection is now reset if it never makes it
       to the accept queue
    
     - diagnostic state of established matches with the packet traces
       showing completed handshake
    
     - TCP_DEFER_ACCEPT timeouts are expressed in seconds and can now be
       enforced with reasonable accuracy instead of rounding up to next
       exponential back-off of syn-ack retry.
    
    Signed-off-by: Patrick McManus <mcmanus@ducksong.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 847e1634e1f4..67cc3956d29c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -139,6 +139,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define MAX_TCP_KEEPINTVL	32767
 #define MAX_TCP_KEEPCNT		127
 #define MAX_TCP_SYNCNT		127
+#define MAX_TCP_ACCEPT_DEFERRED 65535
 
 #define TCP_SYNQ_INTERVAL	(HZ/5)	/* Period of SYNACK timer */
 

commit 6f8b13bcb3369a5df2e63acc422bed6098f5b8c4
Author: Daniel Lezcano <dlezcano@fr.ibm.com>
Date:   Fri Mar 21 04:14:45 2008 -0700

    [NETNS][IPV6] tcp6 - make proc per namespace
    
    Make the proc for tcp6 to be per namespace.
    
    Signed-off-by: Daniel Lezcano <dlezcano@fr.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6b08dab1b1fa..847e1634e1f4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1336,8 +1336,8 @@ struct tcp_iter_state {
 	struct seq_operations	seq_ops;
 };
 
-extern int tcp_proc_register(struct tcp_seq_afinfo *afinfo);
-extern void tcp_proc_unregister(struct tcp_seq_afinfo *afinfo);
+extern int tcp_proc_register(struct net *net, struct tcp_seq_afinfo *afinfo);
+extern void tcp_proc_unregister(struct net *net, struct tcp_seq_afinfo *afinfo);
 
 extern struct request_sock_ops tcp_request_sock_ops;
 extern struct request_sock_ops tcp6_request_sock_ops;

commit f40c8174d3c21bf178283f3ef3aa8c7bf238fdec
Author: Daniel Lezcano <dlezcano@fr.ibm.com>
Date:   Fri Mar 21 04:13:54 2008 -0700

    [NETNS][IPV4] tcp - make proc handle the network namespaces
    
    This patch, like udp proc, makes the proc functions to take care of
    which namespace the socket belongs.
    
    Signed-off-by: Daniel Lezcano <dlezcano@fr.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 11119e33acfe..6b08dab1b1fa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1328,6 +1328,7 @@ struct tcp_seq_afinfo {
 };
 
 struct tcp_iter_state {
+	struct net              *net;
 	sa_family_t		family;
 	enum tcp_seq_states	state;
 	struct sock		*syn_wait_sk;

commit c6aefafb7ec620911d46174eed514f9df639e5a4
Author: Glenn Griffin <ggriffin.kernel@gmail.com>
Date:   Thu Feb 7 21:49:26 2008 -0800

    [TCP]: Add IPv6 support to TCP SYN cookies
    
    Updated to incorporate Eric's suggestion of using a per cpu buffer
    rather than allocating on the stack.  Just a two line change, but will
    resend in it's entirety.
    
    Signed-off-by: Glenn Griffin <ggriffin.kernel@gmail.com>
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ae9774b478f5..11119e33acfe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -29,6 +29,7 @@
 #include <linux/skbuff.h>
 #include <linux/dmaengine.h>
 #include <linux/crypto.h>
+#include <linux/cryptohash.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -434,11 +435,17 @@ extern int			tcp_disconnect(struct sock *sk, int flags);
 extern void			tcp_unhash(struct sock *sk);
 
 /* From syncookies.c */
+extern __u32 syncookie_secret[2][16-3+SHA_DIGEST_WORDS];
 extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
 				    struct ip_options *opt);
 extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
 				     __u16 *mss);
 
+/* From net/ipv6/syncookies.c */
+extern struct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);
+extern __u32 cookie_v6_init_sequence(struct sock *sk, struct sk_buff *skb,
+				     __u16 *mss);
+
 /* tcp_output.c */
 
 extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
@@ -1332,6 +1339,7 @@ extern int tcp_proc_register(struct tcp_seq_afinfo *afinfo);
 extern void tcp_proc_unregister(struct tcp_seq_afinfo *afinfo);
 
 extern struct request_sock_ops tcp_request_sock_ops;
+extern struct request_sock_ops tcp6_request_sock_ops;
 
 extern int tcp_v4_destroy_sock(struct sock *sk);
 

commit 9b0f976f27f00a81cf47643d90854659626795b4
Author: Denis V. Lunev <den@openvz.org>
Date:   Fri Feb 29 11:13:15 2008 -0800

    [INET]: Remove struct net_proto_family* from _init calls.
    
    struct net_proto_family* is not used in icmp[v6]_init, ndisc_init,
    igmp_init and tcp_v4_init. Remove it.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Acked-by: Daniel Lezcano <dlezcano@fr.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7de4ea3a04d9..ae9774b478f5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1373,7 +1373,7 @@ struct tcp_request_sock_ops {
 #endif
 };
 
-extern void tcp_v4_init(struct net_proto_family *ops);
+extern void tcp_v4_init(void);
 extern void tcp_init(void);
 
 #endif	/* _TCP_H */

commit cea14e0ed6082bd3dff19203d01340de6e7ca067
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Jan 12 03:19:12 2008 -0800

    [TCP]: Uninline tcp_is_cwnd_limited
    
    net/ipv4/tcp_cong.c:
      tcp_reno_cong_avoid |  -65
     1 function changed, 65 bytes removed, diff: -65
    
    net/ipv4/arp.c:
      arp_ignore |   -5
     1 function changed, 5 bytes removed, diff: -5
    
    net/ipv4/tcp_bic.c:
      bictcp_cong_avoid |  -57
     1 function changed, 57 bytes removed, diff: -57
    
    net/ipv4/tcp_cubic.c:
      bictcp_cong_avoid |  -61
     1 function changed, 61 bytes removed, diff: -61
    
    net/ipv4/tcp_highspeed.c:
      hstcp_cong_avoid |  -63
     1 function changed, 63 bytes removed, diff: -63
    
    net/ipv4/tcp_hybla.c:
      hybla_cong_avoid |  -85
     1 function changed, 85 bytes removed, diff: -85
    
    net/ipv4/tcp_htcp.c:
      htcp_cong_avoid |  -57
     1 function changed, 57 bytes removed, diff: -57
    
    net/ipv4/tcp_veno.c:
      tcp_veno_cong_avoid |  -52
     1 function changed, 52 bytes removed, diff: -52
    
    net/ipv4/tcp_scalable.c:
      tcp_scalable_cong_avoid |  -61
     1 function changed, 61 bytes removed, diff: -61
    
    net/ipv4/tcp_yeah.c:
      tcp_yeah_cong_avoid |  -75
     1 function changed, 75 bytes removed, diff: -75
    
    net/ipv4/tcp_illinois.c:
      tcp_illinois_cong_avoid |  -54
     1 function changed, 54 bytes removed, diff: -54
    
    net/dccp/ccids/ccid3.c:
      ccid3_update_send_interval |   -7
      ccid3_hc_tx_packet_recv    |   +7
     2 functions changed, 7 bytes added, 7 bytes removed, diff: +0
    
    net/ipv4/tcp_cong.c:
      tcp_is_cwnd_limited |  +88
     1 function changed, 88 bytes added, diff: +88
    
    built-in.o:
     14 functions changed, 95 bytes added, 642 bytes removed, diff: -547
    
    ...Again some gcc artifacts visible as well.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 306580ce1e53..7de4ea3a04d9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -786,27 +786,7 @@ static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
 {
 	return tp->snd_una + tp->snd_wnd;
 }
-
-/* RFC2861 Check whether we are limited by application or congestion window
- * This is the inverse of cwnd check in tcp_tso_should_defer
- */
-static inline int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
-{
-	const struct tcp_sock *tp = tcp_sk(sk);
-	u32 left;
-
-	if (in_flight >= tp->snd_cwnd)
-		return 1;
-
-	if (!sk_can_gso(sk))
-		return 0;
-
-	left = tp->snd_cwnd - in_flight;
-	if (sysctl_tcp_tso_win_divisor)
-		return left * sysctl_tcp_tso_win_divisor < tp->snd_cwnd;
-	else
-		return left <= tcp_max_burst(tp);
-}
+extern int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight);
 
 static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
 				       const struct sk_buff *skb)

commit 490d5046930276aae50dd16942649bfc626056f7
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sat Jan 12 03:17:20 2008 -0800

    [TCP]: Uninline tcp_set_state
    
    net/ipv4/tcp.c:
      tcp_close_state | -226
      tcp_done        | -145
      tcp_close       | -564
      tcp_disconnect  | -141
     4 functions changed, 1076 bytes removed, diff: -1076
    
    net/ipv4/tcp_input.c:
      tcp_fin               |  -86
      tcp_rcv_state_process | -164
     2 functions changed, 250 bytes removed, diff: -250
    
    net/ipv4/tcp_ipv4.c:
      tcp_v4_connect | -209
     1 function changed, 209 bytes removed, diff: -209
    
    net/ipv4/arp.c:
      arp_ignore |   +5
     1 function changed, 5 bytes added, diff: +5
    
    net/ipv6/tcp_ipv6.c:
      tcp_v6_connect | -158
     1 function changed, 158 bytes removed, diff: -158
    
    net/sunrpc/xprtsock.c:
      xs_sendpages |   -2
     1 function changed, 2 bytes removed, diff: -2
    
    net/dccp/ccids/ccid3.c:
      ccid3_update_send_interval |   +7
     1 function changed, 7 bytes added, diff: +7
    
    net/ipv4/tcp.c:
      tcp_set_state | +238
     1 function changed, 238 bytes added, diff: +238
    
    built-in.o:
     12 functions changed, 250 bytes added, 1695 bytes removed, diff: -1445
    
    I've no explanation why some unrelated changes seem to occur
    consistently as well (arp_ignore, ccid3_update_send_interval;
    I checked the arp_ignore asm and it seems to be due to some
    reordered of operation order causing some extra opcodes to be
    generated). Still, the benefits are pretty obvious from the
    codiff's results.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 48081ada92a7..306580ce1e53 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -926,40 +926,7 @@ static const char *statename[]={
 	"Close Wait","Last ACK","Listen","Closing"
 };
 #endif
-
-static inline void tcp_set_state(struct sock *sk, int state)
-{
-	int oldstate = sk->sk_state;
-
-	switch (state) {
-	case TCP_ESTABLISHED:
-		if (oldstate != TCP_ESTABLISHED)
-			TCP_INC_STATS(TCP_MIB_CURRESTAB);
-		break;
-
-	case TCP_CLOSE:
-		if (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)
-			TCP_INC_STATS(TCP_MIB_ESTABRESETS);
-
-		sk->sk_prot->unhash(sk);
-		if (inet_csk(sk)->icsk_bind_hash &&
-		    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
-			inet_put_port(&tcp_hashinfo, sk);
-		/* fall through */
-	default:
-		if (oldstate==TCP_ESTABLISHED)
-			TCP_DEC_STATS(TCP_MIB_CURRESTAB);
-	}
-
-	/* Change state AFTER socket is unhashed to avoid closed
-	 * socket sitting in hash tables.
-	 */
-	sk->sk_state = state;
-
-#ifdef STATE_TRACE
-	SOCK_DEBUG(sk, "TCP sk=%p, State %s -> %s\n",sk, statename[oldstate],statename[state]);
-#endif	
-}
+extern void tcp_set_state(struct sock *sk, int state);
 
 extern void tcp_done(struct sock *sk);
 

commit 4828e7f49a402930e8b3e72de695c8d37e0f98ee
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Dec 31 04:50:19 2007 -0800

    [TCP]: Remove TCPCB_URG & TCPCB_AT_TAIL as unnecessary
    
    The snd_up check should be enough. I suspect this has been
    there to provide a minor optimization in clean_rtx_queue which
    used to have a small if (!->sacked) block which could skip
    snd_up check among the other work.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6a732d4919fe..48081ada92a7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -578,10 +578,6 @@ struct tcp_skb_cb {
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
 
-#define TCPCB_URG		0x20	/* Urgent pointer advanced here	*/
-
-#define TCPCB_AT_TAIL		(TCPCB_URG)
-
 	__u16		urg_ptr;	/* Valid w/URG flags is set.	*/
 	__u32		ack_seq;	/* Sequence number ACK'd	*/
 };

commit 90840defabbd819180c7528e12d550776848f833
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Dec 31 04:48:41 2007 -0800

    [TCP]: Introduce tcp_wnd_end() to reduce line lengths
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 76286e80205a..6a732d4919fe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -785,6 +785,12 @@ static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
 	return 3;
 }
 
+/* Returns end sequence number of the receiver's advertised window */
+static inline u32 tcp_wnd_end(const struct tcp_sock *tp)
+{
+	return tp->snd_una + tp->snd_wnd;
+}
+
 /* RFC2861 Check whether we are limited by application or congestion window
  * This is the inverse of cwnd check in tcp_tso_should_defer
  */

commit 3ab224be6d69de912ee21302745ea45a99274dbc
Author: Hideo Aoki <haoki@redhat.com>
Date:   Mon Dec 31 00:11:19 2007 -0800

    [NET] CORE: Introducing new memory accounting interface.
    
    This patch introduces new memory accounting functions for each network
    protocol. Most of them are renamed from memory accounting functions
    for stream protocols. At the same time, some stream memory accounting
    functions are removed since other functions do same thing.
    
    Renaming:
            sk_stream_free_skb()            ->      sk_wmem_free_skb()
            __sk_stream_mem_reclaim()       ->      __sk_mem_reclaim()
            sk_stream_mem_reclaim()         ->      sk_mem_reclaim()
            sk_stream_mem_schedule          ->      __sk_mem_schedule()
            sk_stream_pages()               ->      sk_mem_pages()
            sk_stream_rmem_schedule()       ->      sk_rmem_schedule()
            sk_stream_wmem_schedule()       ->      sk_wmem_schedule()
            sk_charge_skb()                 ->      sk_mem_charge()
    
    Removeing
            sk_stream_rfree():      consolidates into sock_rfree()
            sk_stream_set_owner_r(): consolidates into skb_set_owner_r()
            sk_stream_mem_schedule()
    
    The following functions are added.
            sk_has_account(): check if the protocol supports accounting
            sk_mem_uncharge(): do the opposite of sk_mem_charge()
    
    In addition, to achieve consolidation, updating sk_wmem_queued is
    removed from sk_mem_charge().
    
    Next, to consolidate memory accounting functions, this patch adds
    memory accounting calls to network core functions. Moreover, present
    memory accounting call is renamed to new accounting call.
    
    Finally we replace present memory accounting calls with new interface
    in TCP and SCTP.
    
    Signed-off-by: Takahiro Yasui <tyasui@redhat.com>
    Signed-off-by: Hideo Aoki <haoki@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 13ebe11a0af7..76286e80205a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1196,8 +1196,8 @@ static inline void tcp_write_queue_purge(struct sock *sk)
 	struct sk_buff *skb;
 
 	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
-		sk_stream_free_skb(sk, skb);
-	sk_stream_mem_reclaim(sk);
+		sk_wmem_free_skb(sk, skb);
+	sk_mem_reclaim(sk);
 }
 
 static inline struct sk_buff *tcp_write_queue_head(struct sock *sk)

commit 9cb5734e5b9b26097c7fa28a9c6426a204cc15e3
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Sat Jan 12 02:16:03 2008 -0800

    [TCP]: Convert several length variable to unsigned.
    
    Several length variables cannot be negative, so convert int to
    unsigned int.  This also allows us to do sane shift operations
    on those variables.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5ec1cacca8a1..13ebe11a0af7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1155,7 +1155,8 @@ extern int			tcp_v4_calc_md5_hash(char *md5_hash,
 						     struct dst_entry *dst,
 						     struct request_sock *req,
 						     struct tcphdr *th,
-						     int protocol, int tcplen);
+						     int protocol,
+						     unsigned int tcplen);
 extern struct tcp_md5sig_key	*tcp_v4_md5_lookup(struct sock *sk,
 						   struct sock *addr_sk);
 
@@ -1404,7 +1405,8 @@ struct tcp_sock_af_ops {
 						  struct dst_entry *dst,
 						  struct request_sock *req,
 						  struct tcphdr *th,
-						  int protocol, int len);
+						  int protocol,
+						  unsigned int len);
 	int			(*md5_add) (struct sock *sk,
 					    struct sock *addr_sk,
 					    u8 *newkey,

commit 6859d49475d4f32abe640372117e4b687906e6b6
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun Dec 2 00:48:06 2007 +0200

    [TCP]: Abstract tp->highest_sack accessing & point to next skb
    
    Pointing to the next skb is necessary to avoid referencing
    already SACKed skbs which will soon be on a separate list.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6e392babda4a..5ec1cacca8a1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1267,8 +1267,12 @@ static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb
 	__tcp_add_write_queue_tail(sk, skb);
 
 	/* Queue it, remembering where we must start sending. */
-	if (sk->sk_send_head == NULL)
+	if (sk->sk_send_head == NULL) {
 		sk->sk_send_head = skb;
+
+		if (tcp_sk(sk)->highest_sack == NULL)
+			tcp_sk(sk)->highest_sack = skb;
+	}
 }
 
 static inline void __tcp_add_write_queue_head(struct sock *sk, struct sk_buff *skb)
@@ -1318,9 +1322,38 @@ static inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)
 {
 	if (!tp->sacked_out)
 		return tp->snd_una;
+
+	if (tp->highest_sack == NULL)
+		return tp->snd_nxt;
+
 	return TCP_SKB_CB(tp->highest_sack)->seq;
 }
 
+static inline void tcp_advance_highest_sack(struct sock *sk, struct sk_buff *skb)
+{
+	tcp_sk(sk)->highest_sack = tcp_skb_is_last(sk, skb) ? NULL :
+						tcp_write_queue_next(sk, skb);
+}
+
+static inline struct sk_buff *tcp_highest_sack(struct sock *sk)
+{
+	return tcp_sk(sk)->highest_sack;
+}
+
+static inline void tcp_highest_sack_reset(struct sock *sk)
+{
+	tcp_sk(sk)->highest_sack = tcp_write_queue_head(sk);
+}
+
+/* Called when old skb is about to be deleted (to be combined with new skb) */
+static inline void tcp_highest_sack_combine(struct sock *sk,
+					    struct sk_buff *old,
+					    struct sk_buff *new)
+{
+	if (tcp_sk(sk)->sacked_out && (old == tcp_sk(sk)->highest_sack))
+		tcp_sk(sk)->highest_sack = new;
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,

commit 234b68607006f3721679e900809ccb99e8bfb10c
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun Dec 2 00:48:02 2007 +0200

    [TCP]: Add tcp_for_write_queue_from_safe and use it in mtu_probe
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cdd0050b97a4..6e392babda4a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1229,6 +1229,11 @@ static inline struct sk_buff *tcp_write_queue_next(struct sock *sk, struct sk_bu
 		for (; (skb != (struct sk_buff *)&(sk)->sk_write_queue);\
 		     skb = skb->next)
 
+#define tcp_for_write_queue_from_safe(skb, tmp, sk)			\
+		for (tmp = skb->next;					\
+		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
+		     skb = tmp, tmp = skb->next)
+
 static inline struct sk_buff *tcp_send_head(struct sock *sk)
 {
 	return sk->sk_send_head;

commit c3a05c6050a339c92e49fae0ba77dbba0d41fd99
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun Dec 2 00:47:59 2007 +0200

    [TCP]: Cong.ctrl modules: remove unused good_ack from cong_avoid
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7e583261f3a0..cdd0050b97a4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -652,7 +652,7 @@ struct tcp_congestion_ops {
 	/* lower bound for congestion window (optional) */
 	u32 (*min_cwnd)(const struct sock *sk);
 	/* do new cwnd calculation (required) */
-	void (*cong_avoid)(struct sock *sk, u32 ack, u32 in_flight, int good_ack);
+	void (*cong_avoid)(struct sock *sk, u32 ack, u32 in_flight);
 	/* call before changing ca_state (optional) */
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
@@ -683,7 +683,7 @@ extern void tcp_slow_start(struct tcp_sock *tp);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct sock *sk);
-extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight, int flag);
+extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight);
 extern u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 

commit 8512430e554a84275669f78f86dce18566d5cf7a
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Nov 26 20:17:38 2007 +0800

    [TCP]: Move FRTO checks out from write queue abstraction funcs
    
    Better place exists in update_send_head (other non-queue related
    adjustments are done there as well) which is the only caller of
    tcp_advance_send_head (now that the bogus call from mtu_probe is
    gone).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f0c5e7a29400..7e583261f3a0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1236,14 +1236,9 @@ static inline struct sk_buff *tcp_send_head(struct sock *sk)
 
 static inline void tcp_advance_send_head(struct sock *sk, struct sk_buff *skb)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
-
 	sk->sk_send_head = skb->next;
 	if (sk->sk_send_head == (struct sk_buff *)&sk->sk_write_queue)
 		sk->sk_send_head = NULL;
-	/* Don't override Nagle indefinately with F-RTO */
-	if (tp->frto_counter == 2)
-		tp->frto_counter = 3;
 }
 
 static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)

commit 68f8353b480e5f2e136c38a511abdbb88eaa8ce2
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Nov 15 19:50:37 2007 -0800

    [TCP]: Rewrite SACK block processing & sack_recv_cache use
    
    Key points of this patch are:
    
      - In case new SACK information is advance only type, no skb
        processing below previously discovered highest point is done
      - Optimize cases below highest point too since there's no need
        to always go up to highest point (which is very likely still
        present in that SACK), this is not entirely true though
        because I'm dropping the fastpath_skb_hint which could
        previously optimize those cases even better. Whether that's
        significant, I'm not too sure.
    
    Currently it will provide skipping by walking. Combined with
    RB-tree, all skipping would become fast too regardless of window
    size (can be done incrementally later).
    
    Previously a number of cases in TCP SACK processing fails to
    take advantage of costly stored information in sack_recv_cache,
    most importantly, expected events such as cumulative ACK and new
    hole ACKs. Processing on such ACKs result in rather long walks
    building up latencies (which easily gets nasty when window is
    huge). Those latencies are often completely unnecessary
    compared with the amount of _new_ information received, usually
    for cumulative ACK there's no new information at all, yet TCP
    walks whole queue unnecessary potentially taking a number of
    costly cache misses on the way, etc.!
    
    Since the inclusion of highest_sack, there's a lot information
    that is very likely redundant (SACK fastpath hint stuff,
    fackets_out, highest_sack), though there's no ultimate guarantee
    that they'll remain the same whole the time (in all unearthly
    scenarios). Take advantage of this knowledge here and drop
    fastpath hint and use direct access to highest SACKed skb as
    a replacement.
    
    Effectively "special cased" fastpath is dropped. This change
    adds some complexity to introduce better coveraged "fastpath",
    though the added complexity should make TCP behave more cache
    friendly.
    
    The current ACK's SACK blocks are compared against each cached
    block individially and only ranges that are new are then scanned
    by the high constant walk. For other parts of write queue, even
    when in previously known part of the SACK blocks, a faster skip
    function is used (if necessary at all). In addition, whenever
    possible, TCP fast-forwards to highest_sack skb that was made
    available by an earlier patch. In typical case, no other things
    but this fast-forward and mandatory markings after that occur
    making the access pattern quite similar to the former fastpath
    "special case".
    
    DSACKs are special case that must always be walked.
    
    The local to recv_sack_cache copying could be more intelligent
    w.r.t DSACKs which are likely to be there only once but that
    is left to a separate patch.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0ede804b16d4..f0c5e7a29400 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1081,7 +1081,6 @@ static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
 static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
 {
 	tcp_clear_retrans_hints_partial(tp);
-	tp->fastpath_skb_hint = NULL;
 }
 
 /* MD5 Signature */

commit a47e5a988a575e64c8c9bae65a1dfe3dca7cba32
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Nov 15 19:41:46 2007 -0800

    [TCP]: Convert highest_sack to sk_buff to allow direct access
    
    It is going to replace the sack fastpath hint quite soon... :-)
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d893b4480769..0ede804b16d4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1312,6 +1312,16 @@ static inline int tcp_write_queue_empty(struct sock *sk)
 	return skb_queue_empty(&sk->sk_write_queue);
 }
 
+/* Start sequence of the highest skb with SACKed bit, valid only if
+ * sacked > 0 or when the caller has ensured validity by itself.
+ */
+static inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)
+{
+	if (!tp->sacked_out)
+		return tp->snd_una;
+	return TCP_SKB_CB(tp->highest_sack)->seq;
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,

commit 9c55e01c0cc835818475a6ce8c4d684df9949ac8
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Nov 6 23:30:13 2007 -0800

    [TCP]: Splice receive support.
    
    Support for network splice receive.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cb5b033e0e59..d893b4480769 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -309,6 +309,9 @@ extern int			tcp_twsk_unique(struct sock *sk,
 
 extern void			tcp_twsk_destructor(struct sock *sk);
 
+extern ssize_t			tcp_splice_read(struct socket *sk, loff_t *ppos,
+					        struct pipe_inode_info *pipe, size_t len, unsigned int flags);
+
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
 {

commit 6e42141009ff18297fe19d19296738b742f861db
Author: Ilpo Jrvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Nov 19 23:24:09 2007 -0800

    [TCP] MTUprobe: fix potential sk_send_head corruption
    
    When the abstraction functions got added, conversion here was
    made incorrectly. As a result, the skb may end up pointing
    to skb which got included to the probe skb and then was freed.
    For it to trigger, however, skb_transmit must fail sending as
    well.
    
    Signed-off-by: Ilpo Jrvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d695cea7730d..cb5b033e0e59 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1288,6 +1288,9 @@ static inline void tcp_insert_write_queue_before(struct sk_buff *new,
 						  struct sock *sk)
 {
 	__skb_insert(new, skb->prev, skb, &sk->sk_write_queue);
+
+	if (sk->sk_send_head == skb)
+		sk->sk_send_head = new;
 }
 
 static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)

commit c1bd24b76879f61b2d10609b0dabde400792a6ec
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Oct 23 21:08:54 2007 -0700

    [TCP]: Remove unneeded implicit type cast when calling tcp_minshall_update()
    
    The tcp_minshall_update() function is called in exactly one place, and is
    passed an unsigned integer for the mss_len argument.  Make the sign of the
    argument match the sign of the passed variable in order to eliminate an
    unneeded implicit type cast and a mixed sign comparison in
    tcp_minshall_update().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 92049e681258..d695cea7730d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -803,7 +803,7 @@ static inline int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
 		return left <= tcp_max_burst(tp);
 }
 
-static inline void tcp_minshall_update(struct tcp_sock *tp, int mss,
+static inline void tcp_minshall_update(struct tcp_sock *tp, unsigned int mss,
 				       const struct sk_buff *skb)
 {
 	if (skb->len < mss)

commit 0800f170263d19b882e519441156c5f6ed190fc1
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Thu Sep 20 11:40:37 2007 -0700

    [TCP]: Minor coding style fixup.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 456983d1fcec..92049e681258 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1067,14 +1067,16 @@ static inline void tcp_mib_init(void)
 }
 
 /* from STCP */
-static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp) {
+static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)
+{
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
 	tp->retransmit_skb_hint = NULL;
 	tp->forward_skb_hint = NULL;
 }
 
-static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp) {
+static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)
+{
 	tcp_clear_retrans_hints_partial(tp);
 	tp->fastpath_skb_hint = NULL;
 }

commit b76892051cf1c04d95872838e70146f65e3b9d75
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Sep 20 11:37:19 2007 -0700

    [TCP]: Avoid clearing sacktag hint in trivial situations
    
    There's no reason to clear the sacktag skb hint when small part
    of the rexmit queue changes. Account changes (if any) instead when
    fragmenting/collapsing. RTO/FRTO do not touch SACKED_ACKED bits so
    no need to discard SACK tag hint at all.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d78ad9bfcfa7..456983d1fcec 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1067,11 +1067,15 @@ static inline void tcp_mib_init(void)
 }
 
 /* from STCP */
-static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp) {
+static inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp) {
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
 	tp->retransmit_skb_hint = NULL;
 	tp->forward_skb_hint = NULL;
+}
+
+static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp) {
+	tcp_clear_retrans_hints_partial(tp);
 	tp->fastpath_skb_hint = NULL;
 }
 

commit 5af4ec236f7c98f3671fb26731457a172d85e0e6
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Sep 20 11:30:48 2007 -0700

    [TCP]: clear_all_retrans_hints prefixed by tcp_
    
    In addition, fix its function comment spacing.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4ba256a3f5e0..d78ad9bfcfa7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1066,8 +1066,8 @@ static inline void tcp_mib_init(void)
 	TCP_ADD_STATS_USER(TCP_MIB_MAXCONN, -1);
 }
 
-/*from STCP */
-static inline void clear_all_retrans_hints(struct tcp_sock *tp){
+/* from STCP */
+static inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp) {
 	tp->lost_skb_hint = NULL;
 	tp->scoreboard_skb_hint = NULL;
 	tp->retransmit_skb_hint = NULL;

commit 6ff03ac355cc6c10f7b1f44dd466d41213acebca
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Aug 24 22:44:06 2007 -0700

    [TCP]: tcp_packets_out_inc to tcp_output.c (no callers elsewhere)
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 66827177ae8b..4ba256a3f5e0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -614,18 +614,6 @@ static inline void tcp_dec_pcount_approx(__u32 *count,
 	tcp_dec_pcount_approx_int(count, tcp_skb_pcount(skb));
 }
 
-static inline void tcp_packets_out_inc(struct sock *sk,
-				       const struct sk_buff *skb)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	int orig = tp->packets_out;
-
-	tp->packets_out += tcp_skb_pcount(skb);
-	if (!orig)
-		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
-					  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
-}
-
 /* Events passed to congestion control interface */
 enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */

commit e9144bd8da80f3136b23c615609798e371e885ac
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Aug 24 22:43:14 2007 -0700

    [TCP]: Remove unnecessary wrapper tcp_packets_out_dec
    
    Makes caller side more obvious, there's no need to have
    a wrapper for this oneliner!
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0a4ed6e85c6f..66827177ae8b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -626,12 +626,6 @@ static inline void tcp_packets_out_inc(struct sock *sk,
 					  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
 }
 
-static inline void tcp_packets_out_dec(struct tcp_sock *tp, 
-				       const struct sk_buff *skb)
-{
-	tp->packets_out -= tcp_skb_pcount(skb);
-}
-
 /* Events passed to congestion control interface */
 enum tcp_ca_event {
 	CA_EVENT_TX_START,	/* first transmit when no packets in flight */

commit e60402d0a909ca2e6e2fbdf9ed004ef0fae36d33
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 15:14:46 2007 +0300

    [TCP]: Move sack_ok access to obviously named funcs & cleanup
    
    Previously code had IsReno/IsFack defined as macros that were
    local to tcp_input.c though sack_ok field has user elsewhere too
    for the same purpose. This changes them to static inlines as
    preferred according the current coding style and unifies the
    access to sack_ok across multiple files. Magic bitops of sack_ok
    for FACK and DSACK are also abstracted to functions with
    appropriate names.
    
    Note:
    - One sack_ok = 1 remains but that's self explanary, i.e., it
      enables sack
    - Couple of !IsReno cases are changed to tcp_is_sack
    - There were no users for IsDSack => I dropped it
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b92bdc7c92a9..0a4ed6e85c6f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -719,6 +719,34 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
+/* These functions determine how the current flow behaves in respect of SACK
+ * handling. SACK is negotiated with the peer, and therefore it can vary
+ * between different flows.
+ *
+ * tcp_is_sack - SACK enabled
+ * tcp_is_reno - No SACK
+ * tcp_is_fack - FACK enabled, implies SACK enabled
+ */
+static inline int tcp_is_sack(const struct tcp_sock *tp)
+{
+	return tp->rx_opt.sack_ok;
+}
+
+static inline int tcp_is_reno(const struct tcp_sock *tp)
+{
+	return !tcp_is_sack(tp);
+}
+
+static inline int tcp_is_fack(const struct tcp_sock *tp)
+{
+	return tp->rx_opt.sack_ok & 2;
+}
+
+static inline void tcp_enable_fack(struct tcp_sock *tp)
+{
+	tp->rx_opt.sack_ok |= 2;
+}
+
 static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
 {
 	return tp->sacked_out + tp->lost_out;

commit b9c4595bc4947faa236a849324fe3492e388d949
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Jul 27 16:36:17 2007 +0300

    [TCP]: Don't panic if S+L skb is detected
    
    BUG_ON is an overkill. In fact, I was mislead by BUG_TRAP
    severity (equals to WARN_ON) which is much lower than BUG_ON's
    (that panics).
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 064c92fe00d2..b92bdc7c92a9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -758,8 +758,8 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 			    (tp->snd_cwnd >> 2)));
 }
 
-/* Use define here intentionally to get BUG_ON location shown at the caller */
-#define tcp_verify_left_out(tp)	BUG_ON(tcp_left_out(tp) > tp->packets_out)
+/* Use define here intentionally to get WARN_ON location shown at the caller */
+#define tcp_verify_left_out(tp)	WARN_ON(tcp_left_out(tp) > tp->packets_out)
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);

commit 1b6d427bb7eb69e6dc4f194a5b0f4a382a16ff82
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 14:53:36 2007 +0300

    [TCP]: Reduce sacked_out with reno when purging write_queue
    
    Previously TCP had a transitional state during which reno
    counted segments that are already below the current window into
    sacked_out, which is now prevented. In addition, re-try now
    the unconditional S+L skb catching.
    
    This approach conservatively calls just remove_sack and leaves
    reset_sack() calls alone. The best solution to the whole problem
    would be to first calculate the new sacked_out fully (this patch
    does not move reno_sack_reset calls from original sites and thus
    does not implement this). However, that would require very
    invasive change to fastretrans_alert (perhaps even slicing it to
    two halves). Alternatively, all callers of tcp_packets_in_flight
    (i.e., users that depend on sacked_out) should be postponed
    until the new sacked_out has been calculated but it isn't any
    simpler alternative.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7042c32085f5..064c92fe00d2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -759,8 +759,7 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 }
 
 /* Use define here intentionally to get BUG_ON location shown at the caller */
-#define tcp_verify_left_out(tp) \
-	BUG_ON(tp->rx_opt.sack_ok && (tcp_left_out(tp) > tp->packets_out))
+#define tcp_verify_left_out(tp)	BUG_ON(tcp_left_out(tp) > tp->packets_out)
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);

commit 005903bc3a0e8473fef809e8775db52dcd3cde63
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 14:44:16 2007 +0300

    [TCP]: Left out sync->verify (the new meaning of it) & definify
    
    Left_out was dropped a while ago, thus leaving verifying
    consistency of the "left out" as only task for the function in
    question. Thus make it's name more appropriate.
    
    In addition, it is intentionally converted to #define instead
    of static inline because the location of the invariant failure
    is the most important thing to have if this ever triggers. I
    think it would have been helpful e.g. in this case where the
    location of the failure point had to be based on some quesswork:
        http://lkml.org/lkml/2007/5/2/464
    ...Luckily the guesswork seems to have proved to be correct.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 39d0df6afe47..7042c32085f5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -758,10 +758,9 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 			    (tp->snd_cwnd >> 2)));
 }
 
-static inline void tcp_sync_left_out(struct tcp_sock *tp)
-{
-	BUG_ON(tp->rx_opt.sack_ok && (tcp_left_out(tp) > tp->packets_out));
-}
+/* Use define here intentionally to get BUG_ON location shown at the caller */
+#define tcp_verify_left_out(tp) \
+	BUG_ON(tp->rx_opt.sack_ok && (tcp_left_out(tp) > tp->packets_out))
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);

commit 83ae40885f33e406c87c86b0bd4b6fd31a741f12
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 14:37:30 2007 +0300

    [TCP]: Add tcp_left_out(tp) "back" to get cleaner looking lines
    
    tp->left_out got removed but nothing came to replace it back
    then (users just did addition by themselves), so add function
    for users now.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 299872d461c7..39d0df6afe47 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -719,6 +719,11 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
+static inline unsigned int tcp_left_out(const struct tcp_sock *tp)
+{
+	return tp->sacked_out + tp->lost_out;
+}
+
 /* This determines how many packets are "in the network" to the best
  * of our knowledge.  In many cases it is conservative, but where
  * detailed information is available from the receiver (via SACK
@@ -735,8 +740,7 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
  */
 static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 {
-	return tp->packets_out - (tp->sacked_out + tp->lost_out) +
-		tp->retrans_out;
+	return tp->packets_out - tcp_left_out(tp) + tp->retrans_out;
 }
 
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
@@ -756,8 +760,7 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 
 static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
-	BUG_ON(tp->rx_opt.sack_ok &&
-	       (tp->sacked_out + tp->lost_out > tp->packets_out));
+	BUG_ON(tp->rx_opt.sack_ok && (tcp_left_out(tp) > tp->packets_out));
 }
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);

commit b5860bbac7be1381626f3dc8a0cb970a60fcefb4
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu Aug 9 14:33:18 2007 +0300

    [TCP]: Tighten tcp_sock's belt, drop left_out
    
    It is easily calculable when needed and user are not that many
    after all.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9d3438f6b52f..299872d461c7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -735,7 +735,8 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
  */
 static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 {
-	return (tp->packets_out - tp->left_out + tp->retrans_out);
+	return tp->packets_out - (tp->sacked_out + tp->lost_out) +
+		tp->retrans_out;
 }
 
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
@@ -757,7 +758,6 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
 	BUG_ON(tp->rx_opt.sack_ok &&
 	       (tp->sacked_out + tp->lost_out > tp->packets_out));
-	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);

commit af610b4ca19f513a50d47ea93ed57241383c8081
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Jun 15 12:58:38 2007 +0300

    [TCP]: Add tcp_dec_pcount_approx int variant
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dde04afbe0a4..9d3438f6b52f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -599,16 +599,21 @@ static inline int tcp_skb_mss(const struct sk_buff *skb)
 	return skb_shinfo(skb)->gso_size;
 }
 
-static inline void tcp_dec_pcount_approx(__u32 *count,
-					 const struct sk_buff *skb)
+static inline void tcp_dec_pcount_approx_int(__u32 *count, const int decr)
 {
 	if (*count) {
-		*count -= tcp_skb_pcount(skb);
+		*count -= decr;
 		if ((int)*count < 0)
 			*count = 0;
 	}
 }
 
+static inline void tcp_dec_pcount_approx(__u32 *count,
+					 const struct sk_buff *skb)
+{
+	tcp_dec_pcount_approx_int(count, tcp_skb_pcount(skb));
+}
+
 static inline void tcp_packets_out_inc(struct sock *sk,
 				       const struct sk_buff *skb)
 {

commit bdf1ee5d3bd38d0c44bd7baa74e07adcbe4ceab1
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Sun May 27 02:04:16 2007 -0700

    [TCP]: Move code from tcp_ecn.h to tcp*.c and tcp.h & remove it
    
    No other users exist for tcp_ecn.h. Very few things remain in
    tcp.h, for most TCP ECN functions callers reside within a
    single .c file and can be placed there.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 54053de0bdd7..dde04afbe0a4 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -39,6 +39,7 @@
 #include <net/snmp.h>
 #include <net/ip.h>
 #include <net/tcp_states.h>
+#include <net/inet_ecn.h>
 
 #include <linux/seq_file.h>
 
@@ -330,6 +331,17 @@ static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
  	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
 }
 
+#define	TCP_ECN_OK		1
+#define	TCP_ECN_QUEUE_CWR	2
+#define	TCP_ECN_DEMAND_CWR	4
+
+static __inline__ void
+TCP_ECN_create_request(struct request_sock *req, struct tcphdr *th)
+{
+	if (sysctl_tcp_ecn && th->ece && th->cwr)
+		inet_rsk(req)->ecn_ok = 1;
+}
+
 enum tcp_tw_status
 {
 	TCP_TW_SUCCESS = 0,
@@ -573,8 +585,6 @@ struct tcp_skb_cb {
 
 #define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
 
-#include <net/tcp_ecn.h>
-
 /* Due to TSO, an SKB can be composed of multiple actual
  * packets.  To keep these tracked properly, we use this.
  */

commit f8ab18d2d987a59ccbf0495032b2aef05b730037
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Fri Sep 28 15:18:35 2007 -0700

    [TCP]: Fix MD5 signature handling on big-endian.
    
    Based upon a report and initial patch by Peter Lieven.
    
    tcp4_md5sig_key and tcp6_md5sig_key need to start with
    the exact same members as tcp_md5sig_key.  Because they
    are both cast to that type by tcp_v{4,6}_md5_do_lookup().
    
    Unfortunately tcp{4,6}_md5sig_key use a u16 for the key
    length instead of a u8, which is what tcp_md5sig_key
    uses.  This just so happens to work by accident on
    little-endian, but on big-endian it doesn't.
    
    Instead of casting, just place tcp_md5sig_key as the first member of
    the address-family specific structures, adjust the access sites, and
    kill off the ugly casts.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 185c7ecce4cc..54053de0bdd7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1059,14 +1059,12 @@ struct tcp_md5sig_key {
 };
 
 struct tcp4_md5sig_key {
-	u8			*key;
-	u16			keylen;
+	struct tcp_md5sig_key	base;
 	__be32			addr;
 };
 
 struct tcp6_md5sig_key {
-	u8			*key;
-	u16			keylen;
+	struct tcp_md5sig_key	base;
 #if 0
 	u32			scope_id;	/* XXX */
 #endif

commit 3516ffb0fef710749daf288c0fe146503e0cf9d4
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Thu Aug 2 19:23:56 2007 -0700

    [TCP]: Invoke tcp_sendmsg() directly, do not use inet_sendmsg().
    
    As discovered by Evegniy Polyakov, if we try to sendmsg after
    a connection reset, we can do incredibly stupid things.
    
    The core issue is that inet_sendmsg() tries to autobind the
    socket, but we should never do that for TCP.  Instead we should
    just go straight into TCP's sendmsg() code which will do all
    of the necessary state and pending socket error checks.
    
    TCP's sendpage already directly vectors to tcp_sendpage(), so this
    merely brings sendmsg() in line with that.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c209361ab74a..185c7ecce4cc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -281,7 +281,7 @@ extern int			tcp_v4_remember_stamp(struct sock *sk);
 
 extern int		    	tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 
-extern int			tcp_sendmsg(struct kiocb *iocb, struct sock *sk,
+extern int			tcp_sendmsg(struct kiocb *iocb, struct socket *sock,
 					    struct msghdr *msg, size_t size);
 extern ssize_t			tcp_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags);
 

commit 30cfd0baf0a0c4329fff1ef4b622919297969ec8
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Wed Jul 25 23:49:34 2007 -0700

    [TCP]: congestion control API pass RTT in microseconds
    
    This patch changes the API for the callback that is done after an ACK is
    received. It solves a couple of issues:
    
      * Some congestion controls want higher resolution value of RTT
        (controlled by TCP_CONG_RTT_SAMPLE flag). These don't really want a ktime, but
        all compute a RTT in microseconds.
    
      * Other congestion control could use RTT at jiffies resolution.
    
    To keep API consistent the units should be the same for both cases, just the
    resolution should change.
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 8b404b1ef7c8..c209361ab74a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -660,7 +660,7 @@ struct tcp_congestion_ops {
 	/* new value of cwnd after loss (optional) */
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
-	void (*pkts_acked)(struct sock *sk, u32 num_acked, ktime_t last);
+	void (*pkts_acked)(struct sock *sk, u32 num_acked, s32 rtt_us);
 	/* get info for inet_diag (optional) */
 	void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
 

commit 16751347a060a10c09b11593bb179fd5b0240c04
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Mon Jul 16 18:35:52 2007 -0700

    [TCP]: remove unused argument to cong_avoid op
    
    None of the existing TCP congestion controls use the rtt value pased
    in the ca_ops->cong_avoid interface.  Which is lucky because seq_rtt
    could have been -1 when handling a duplicate ack.
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a8af9ae00177..8b404b1ef7c8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -652,8 +652,7 @@ struct tcp_congestion_ops {
 	/* lower bound for congestion window (optional) */
 	u32 (*min_cwnd)(const struct sock *sk);
 	/* do new cwnd calculation (required) */
-	void (*cong_avoid)(struct sock *sk, u32 ack,
-			   u32 rtt, u32 in_flight, int good_ack);
+	void (*cong_avoid)(struct sock *sk, u32 ack, u32 in_flight, int good_ack);
 	/* call before changing ca_state (optional) */
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
@@ -684,8 +683,7 @@ extern void tcp_slow_start(struct tcp_sock *tp);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct sock *sk);
-extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack,
-				u32 rtt, u32 in_flight, int flag);
+extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 in_flight, int flag);
 extern u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 

commit e4fd5da39f99d5921dda1fe3d93652fbd925fbfd
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Tue May 29 13:19:18 2007 -0700

    [TCP]: Consolidate checking for tcp orphan count being too big.
    
    tcp_out_of_resources() and tcp_close() perform the
    same checking of number of orphan sockets. Move this
    code into common place.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e22b4f0305a3..a8af9ae00177 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -254,6 +254,12 @@ static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)
 	return seq3 - seq2 >= seq1 - seq2;
 }
 
+static inline int tcp_too_many_orphans(struct sock *sk, int num)
+{
+	return (num > sysctl_tcp_max_orphans) ||
+		(sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&
+		 atomic_read(&tcp_memory_allocated) > sysctl_tcp_mem[2]);
+}
 
 extern struct proto tcp_prot;
 

commit 0ec96822d5c0df77107c03b8d9a81a436ab707fc
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Thu May 3 03:30:34 2007 -0700

    [TCP]: Use S+L catcher only with SACK for now
    
    TCP has a transitional state when SACK is not in use during
    which this invariant is temporarily broken. Without SACK,
    tcp_clean_rtx_queue does not decrement sacked_out. Therefore
    calls to tcp_sync_left_out before sacked_out is again
    corrected by tcp_fastretrans_alert can trigger this trap as
    sacked_out still has couple of segments that are already out
    of window.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ef8f9d4dae85..e22b4f0305a3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -736,7 +736,8 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 
 static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
-	BUG_ON(tp->sacked_out + tp->lost_out > tp->packets_out);
+	BUG_ON(tp->rx_opt.sack_ok &&
+	       (tp->sacked_out + tp->lost_out > tp->packets_out));
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 

commit d551e4541dd60ae53459f77a971f2d6043431f5f
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Apr 30 00:42:20 2007 -0700

    [TCP] FRTO: RFC4138 allows Nagle override when new data must be sent
    
    This is a corner case where less than MSS sized new data thingie
    is awaiting in the send queue. For F-RTO to work correctly, a
    new data segment must be sent at certain point or F-RTO cannot
    be used at all. RFC4138 allows overriding of Nagle at that
    point.
    
    Implementation uses frto_counter states 2 and 3 to distinguish
    when Nagle override is needed.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c6ecd455edab..ef8f9d4dae85 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1199,9 +1199,14 @@ static inline struct sk_buff *tcp_send_head(struct sock *sk)
 
 static inline void tcp_advance_send_head(struct sock *sk, struct sk_buff *skb)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
+
 	sk->sk_send_head = skb->next;
 	if (sk->sk_send_head == (struct sk_buff *)&sk->sk_write_queue)
 		sk->sk_send_head = NULL;
+	/* Don't override Nagle indefinately with F-RTO */
+	if (tp->frto_counter == 2)
+		tp->frto_counter = 3;
 }
 
 static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)

commit 34588b4c046c34773e5a1a962da7b78b05c4d1bd
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Mon Apr 30 00:57:33 2007 -0700

    [TCP]: Catch skb with S+L bugs earlier
    
    SACKED_ACKED and LOST are mutually exclusive with SACK, thus
    having their sum larger than packets_out is bug with SACK.
    Eventually these bugs trigger traps in the tcp_clean_rtx_queue
    with SACK but it's much more informative to do this here.
    
    Non-SACK TCP, however, could get more than packets_out duplicate
    ACKs which each increment sacked_out, so it makes sense to do
    this kind of limitting for non-SACK TCP but not for SACK enabled
    one. Perhaps the author had the opposite in mind but did the
    logic accidently wrong way around? Anyway, the sacked_out
    incrementer code for non-SACK already deals this issue before
    calling sync_left_out so this trapping can be done
    unconditionally.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a385797f160a..c6ecd455edab 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -736,9 +736,7 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 
 static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
-	if (tp->rx_opt.sack_ok &&
-	    (tp->sacked_out >= tp->packets_out - tp->lost_out))
-		tp->sacked_out = tp->packets_out - tp->lost_out;
+	BUG_ON(tp->sacked_out + tp->lost_out > tp->packets_out);
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 

commit 164891aadf1721fca4dce473bb0e0998181537c6
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Mon Apr 23 22:26:16 2007 -0700

    [TCP]: Congestion control API update.
    
    Do some simple changes to make congestion control API faster/cleaner.
    * use ktime_t rather than timeval
    * merge rtt sampling into existing ack callback
      this means one indirect call versus two per ack.
    * use flags bits to store options/settings
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 43910fe3c448..a385797f160a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -629,9 +629,12 @@ enum tcp_ca_event {
 #define TCP_CA_MAX	128
 #define TCP_CA_BUF_MAX	(TCP_CA_NAME_MAX*TCP_CA_MAX)
 
+#define TCP_CONG_NON_RESTRICTED 0x1
+#define TCP_CONG_RTT_STAMP	0x2
+
 struct tcp_congestion_ops {
 	struct list_head	list;
-	int	non_restricted;
+	unsigned long flags;
 
 	/* initialize private data (optional) */
 	void (*init)(struct sock *sk);
@@ -645,8 +648,6 @@ struct tcp_congestion_ops {
 	/* do new cwnd calculation (required) */
 	void (*cong_avoid)(struct sock *sk, u32 ack,
 			   u32 rtt, u32 in_flight, int good_ack);
-	/* round trip time sample per acked packet (optional) */
-	void (*rtt_sample)(struct sock *sk, u32 usrtt);
 	/* call before changing ca_state (optional) */
 	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
@@ -654,7 +655,7 @@ struct tcp_congestion_ops {
 	/* new value of cwnd after loss (optional) */
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
-	void (*pkts_acked)(struct sock *sk, u32 num_acked);
+	void (*pkts_acked)(struct sock *sk, u32 num_acked, ktime_t last);
 	/* get info for inet_diag (optional) */
 	void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
 

commit 9e412ba7632f71259a53085665d4983b78257b7c
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Apr 20 22:18:02 2007 -0700

    [TCP]: Sed magic converts func(sk, tp, ...) -> func(sk, ...)
    
    This is (mostly) automated change using magic:
    
    sed -e '/struct sock \*sk/ N' -e '/struct sock \*sk/ N'
        -e '/struct sock \*sk/ N' -e '/struct sock \*sk/ N'
        -e 's|struct sock \*sk,[\n\t ]*struct tcp_sock \*tp\([^{]*\n{\n\)|
              struct sock \*sk\1\tstruct tcp_sock *tp = tcp_sk(sk);\n|g'
        -e 's|struct sock \*sk, struct tcp_sock \*tp|
              struct sock \*sk|g' -e 's|sk, tp\([^-]\)|sk\1|g'
    
    Fixed four unused variable (tp) warnings that were introduced.
    
    In addition, manually added newlines after local variables and
    tweaked function arguments positioning.
    
    $ gcc --version
    gcc (GCC) 4.1.1 20060525 (Red Hat 4.1.1-1)
    ...
    $ codiff -fV built-in.o.old built-in.o.new
    net/ipv4/route.c:
      rt_cache_flush |  +14
     1 function changed, 14 bytes added
    
    net/ipv4/tcp.c:
      tcp_setsockopt |   -5
      tcp_sendpage   |  -25
      tcp_sendmsg    |  -16
     3 functions changed, 46 bytes removed
    
    net/ipv4/tcp_input.c:
      tcp_try_undo_recovery |   +3
      tcp_try_undo_dsack    |   +2
      tcp_mark_head_lost    |  -12
      tcp_ack               |  -15
      tcp_event_data_recv   |  -32
      tcp_rcv_state_process |  -10
      tcp_rcv_established   |   +1
     7 functions changed, 6 bytes added, 69 bytes removed, diff: -63
    
    net/ipv4/tcp_output.c:
      update_send_head          |   -9
      tcp_transmit_skb          |  +19
      tcp_cwnd_validate         |   +1
      tcp_write_wakeup          |  -17
      __tcp_push_pending_frames |  -25
      tcp_push_one              |   -8
      tcp_send_fin              |   -4
     7 functions changed, 20 bytes added, 63 bytes removed, diff: -43
    
    built-in.o.new:
     18 functions changed, 40 bytes added, 178 bytes removed, diff: -138
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e79803353c83..43910fe3c448 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -420,9 +420,9 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 
 /* tcp_output.c */
 
-extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
-				      unsigned int cur_mss, int nonagle);
-extern int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp);
+extern void __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,
+				      int nonagle);
+extern int tcp_may_send_now(struct sock *sk);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_xmit_retransmit_queue(struct sock *);
 extern void tcp_simple_retransmit(struct sock *);
@@ -479,8 +479,10 @@ static inline void tcp_fast_path_on(struct tcp_sock *tp)
 	__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);
 }
 
-static inline void tcp_fast_path_check(struct sock *sk, struct tcp_sock *tp)
+static inline void tcp_fast_path_check(struct sock *sk)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
+
 	if (skb_queue_empty(&tp->out_of_order_queue) &&
 	    tp->rcv_wnd &&
 	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
@@ -591,10 +593,10 @@ static inline void tcp_dec_pcount_approx(__u32 *count,
 	}
 }
 
-static inline void tcp_packets_out_inc(struct sock *sk, 
-				       struct tcp_sock *tp,
+static inline void tcp_packets_out_inc(struct sock *sk,
 				       const struct sk_buff *skb)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
 	int orig = tp->packets_out;
 
 	tp->packets_out += tcp_skb_pcount(skb);
@@ -778,18 +780,21 @@ static inline void tcp_minshall_update(struct tcp_sock *tp, int mss,
 		tp->snd_sml = TCP_SKB_CB(skb)->end_seq;
 }
 
-static inline void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
+static inline void tcp_check_probe_timer(struct sock *sk)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
 	const struct inet_connection_sock *icsk = inet_csk(sk);
+
 	if (!tp->packets_out && !icsk->icsk_pending)
 		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
 					  icsk->icsk_rto, TCP_RTO_MAX);
 }
 
-static inline void tcp_push_pending_frames(struct sock *sk,
-					   struct tcp_sock *tp)
+static inline void tcp_push_pending_frames(struct sock *sk)
 {
-	__tcp_push_pending_frames(sk, tp, tcp_current_mss(sk, 1), tp->nonagle);
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	__tcp_push_pending_frames(sk, tcp_current_mss(sk, 1), tp->nonagle);
 }
 
 static inline void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)

commit 4ac02bab77438b484a5cf855a002fb6a1d592894
Author: Andi Kleen <ak@suse.de>
Date:   Fri Apr 20 17:11:46 2007 -0700

    [TCP]: Uninline tcp_done().
    
    The function is quite big and has several call sites and nothing
    to collapse by compiler optimization on inlining.
    
    Besides it's nicer to read in a in .c file.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 07f724e02f84..e79803353c83 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -921,21 +921,7 @@ static inline void tcp_set_state(struct sock *sk, int state)
 #endif	
 }
 
-static inline void tcp_done(struct sock *sk)
-{
-	if(sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
-		TCP_INC_STATS_BH(TCP_MIB_ATTEMPTFAILS);
-
-	tcp_set_state(sk, TCP_CLOSE);
-	tcp_clear_xmit_timers(sk);
-
-	sk->sk_shutdown = SHUTDOWN_MASK;
-
-	if (!sock_flag(sk, SOCK_DEAD))
-		sk->sk_state_change(sk);
-	else
-		inet_csk_destroy_sock(sk);
-}
+extern void tcp_done(struct sock *sk);
 
 static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 {

commit 604763722c655c7e3f31ecf6f7b4dafcd26a7a15
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Apr 9 11:59:39 2007 -0700

    [NET]: Treat CHECKSUM_PARTIAL as CHECKSUM_UNNECESSARY
    
    When a transmitted packet is looped back directly, CHECKSUM_PARTIAL
    maps to the semantics of CHECKSUM_UNNECESSARY.  Therefore we should
    treat it as such in the stack.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index af9273204cfd..07f724e02f84 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -818,7 +818,7 @@ static inline __sum16 __tcp_checksum_complete(struct sk_buff *skb)
 
 static inline int tcp_checksum_complete(struct sk_buff *skb)
 {
-	return skb->ip_summed != CHECKSUM_UNNECESSARY &&
+	return !skb_csum_unnecessary(skb) &&
 		__tcp_checksum_complete(skb);
 }
 

commit aa8223c7bb0b05183e1737881ed21827aa5b9e73
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Apr 10 21:04:22 2007 -0700

    [SK_BUFF]: Introduce tcp_hdr(), remove skb->h.th
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6dacc352dcf1..af9273204cfd 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -984,7 +984,7 @@ static inline void tcp_openreq_init(struct request_sock *req,
 	ireq->wscale_ok = rx_opt->wscale_ok;
 	ireq->acked = 0;
 	ireq->ecn_ok = 0;
-	ireq->rmt_port = skb->h.th->source;
+	ireq->rmt_port = tcp_hdr(skb)->source;
 }
 
 extern void tcp_enter_memory_pressure(void);

commit fe067e8ab5e0dc5ca3c54634924c628da92090b4
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Wed Mar 7 12:12:44 2007 -0800

    [TCP]: Abstract out all write queue operations.
    
    This allows the write queue implementation to be changed,
    for example, to one which allows fast interval searching.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 181c0600af1c..6dacc352dcf1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1162,6 +1162,120 @@ static inline void		tcp_put_md5sig_pool(void)
 	put_cpu();
 }
 
+/* write queue abstraction */
+static inline void tcp_write_queue_purge(struct sock *sk)
+{
+	struct sk_buff *skb;
+
+	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
+		sk_stream_free_skb(sk, skb);
+	sk_stream_mem_reclaim(sk);
+}
+
+static inline struct sk_buff *tcp_write_queue_head(struct sock *sk)
+{
+	struct sk_buff *skb = sk->sk_write_queue.next;
+	if (skb == (struct sk_buff *) &sk->sk_write_queue)
+		return NULL;
+	return skb;
+}
+
+static inline struct sk_buff *tcp_write_queue_tail(struct sock *sk)
+{
+	struct sk_buff *skb = sk->sk_write_queue.prev;
+	if (skb == (struct sk_buff *) &sk->sk_write_queue)
+		return NULL;
+	return skb;
+}
+
+static inline struct sk_buff *tcp_write_queue_next(struct sock *sk, struct sk_buff *skb)
+{
+	return skb->next;
+}
+
+#define tcp_for_write_queue(skb, sk)					\
+		for (skb = (sk)->sk_write_queue.next;			\
+		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
+		     skb = skb->next)
+
+#define tcp_for_write_queue_from(skb, sk)				\
+		for (; (skb != (struct sk_buff *)&(sk)->sk_write_queue);\
+		     skb = skb->next)
+
+static inline struct sk_buff *tcp_send_head(struct sock *sk)
+{
+	return sk->sk_send_head;
+}
+
+static inline void tcp_advance_send_head(struct sock *sk, struct sk_buff *skb)
+{
+	sk->sk_send_head = skb->next;
+	if (sk->sk_send_head == (struct sk_buff *)&sk->sk_write_queue)
+		sk->sk_send_head = NULL;
+}
+
+static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)
+{
+	if (sk->sk_send_head == skb_unlinked)
+		sk->sk_send_head = NULL;
+}
+
+static inline void tcp_init_send_head(struct sock *sk)
+{
+	sk->sk_send_head = NULL;
+}
+
+static inline void __tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
+{
+	__skb_queue_tail(&sk->sk_write_queue, skb);
+}
+
+static inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)
+{
+	__tcp_add_write_queue_tail(sk, skb);
+
+	/* Queue it, remembering where we must start sending. */
+	if (sk->sk_send_head == NULL)
+		sk->sk_send_head = skb;
+}
+
+static inline void __tcp_add_write_queue_head(struct sock *sk, struct sk_buff *skb)
+{
+	__skb_queue_head(&sk->sk_write_queue, skb);
+}
+
+/* Insert buff after skb on the write queue of sk.  */
+static inline void tcp_insert_write_queue_after(struct sk_buff *skb,
+						struct sk_buff *buff,
+						struct sock *sk)
+{
+	__skb_append(skb, buff, &sk->sk_write_queue);
+}
+
+/* Insert skb between prev and next on the write queue of sk.  */
+static inline void tcp_insert_write_queue_before(struct sk_buff *new,
+						  struct sk_buff *skb,
+						  struct sock *sk)
+{
+	__skb_insert(new, skb->prev, skb, &sk->sk_write_queue);
+}
+
+static inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)
+{
+	__skb_unlink(skb, &sk->sk_write_queue);
+}
+
+static inline int tcp_skb_is_last(const struct sock *sk,
+				  const struct sk_buff *skb)
+{
+	return skb->next == (struct sk_buff *)&sk->sk_write_queue;
+}
+
+static inline int tcp_write_queue_empty(struct sock *sk)
+{
+	return skb_queue_empty(&sk->sk_write_queue);
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,

commit 9d729f72dca9406025bcfa9c1f660d71d9ef0ff5
Author: James Morris <jmorris@namei.org>
Date:   Sun Mar 4 16:12:44 2007 -0800

    [NET]: Convert xtime.tv_sec to get_seconds()
    
    Where appropriate, convert references to xtime.tv_sec to the
    get_seconds() helper function.
    
    Signed-off-by: James Morris <jmorris@namei.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f0c9e3400a09..181c0600af1c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1014,7 +1014,7 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 {
 	if ((s32)(rx_opt->rcv_tsval - rx_opt->ts_recent) >= 0)
 		return 0;
-	if (xtime.tv_sec >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)
+	if (get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)
 		return 0;
 
 	/* RST segments are not recommended to carry timestamp,
@@ -1029,7 +1029,7 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 
 	   However, we can relax time bounds for RST segments to MSL.
 	 */
-	if (rst && xtime.tv_sec >= rx_opt->ts_recent_stamp + TCP_PAWS_MSL)
+	if (rst && get_seconds() >= rx_opt->ts_recent_stamp + TCP_PAWS_MSL)
 		return 0;
 	return 1;
 }

commit 3cfe3baaf07c9e40a75f9a70662de56df1c246a8
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Tue Feb 27 10:09:49 2007 -0800

    [TCP]: Add two new spurious RTO responses to FRTO
    
    New sysctl tcp_frto_response is added to select amongst these
    responses:
            - Rate halving based; reuses CA_CWR state (default)
            - Very conservative; used to be the only one available (=1)
            - Undo cwr; undoes ssthresh and cwnd reductions (=2)
    
    The response with rate halving requires a new parameter to
    tcp_enter_cwr because FRTO has already reduced ssthresh and
    doing a second reduction there has to be prevented. In addition,
    to keep things nice on 80 cols screen, a local variable was
    added.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6d09f5085f6a..f0c9e3400a09 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -220,6 +220,7 @@ extern int sysctl_tcp_app_win;
 extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
+extern int sysctl_tcp_frto_response;
 extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_dma_copybreak;
 extern int sysctl_tcp_nometrics_save;
@@ -738,7 +739,7 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 
-extern void tcp_enter_cwr(struct sock *sk);
+extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 
 /* Slow start with delack produces 3 packets of burst, so that

commit 886236c1247ab5e2ad9c73f6e9a652e3ae3c8b07
Author: John Heffner <jheffner@psc.edu>
Date:   Sun Mar 25 19:21:45 2007 -0700

    [TCP]: Add RFC3742 Limited Slow-Start, controlled by variable sysctl_tcp_max_ssthresh.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7fd6b77519c3..6d09f5085f6a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -230,6 +230,7 @@ extern int sysctl_tcp_mtu_probing;
 extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
 extern int sysctl_tcp_slow_start_after_idle;
+extern int sysctl_tcp_max_ssthresh;
 
 extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;

commit 46d0de4ed92650b95f27acae09914996bbe624e7
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Wed Feb 21 23:10:39 2007 -0800

    [TCP] FRTO: Entry is allowed only during (New)Reno like recovery
    
    This interpretation comes from RFC4138:
        "If the sender implements some loss recovery algorithm other
         than Reno or NewReno [FHG04], the F-RTO algorithm SHOULD
         NOT be entered when earlier fast recovery is underway."
    
    I think the RFC means to say (especially in the light of
    Appendix B) that ...recovery is underway (not just fast recovery)
    or was underway when it was interrupted by an earlier (F-)RTO
    that hasn't yet been resolved (snd_una has not advanced enough).
    Thus, my interpretation is that whenever TCP has ever
    retransmitted other than head, basic version cannot be used
    because then the order assumptions which are used as FRTO basis
    do not hold.
    
    NewReno has only the head segment retransmitted at a time.
    Therefore, walk up to the segment that has not been SACKed, if
    that segment is not retransmitted nor anything before it, we know
    for sure, that nothing after the non-SACKed segment should be
    either. This assumption is valid because TCPCB_EVER_RETRANS does
    not leave holes but each non-SACKed segment is rexmitted
    in-order.
    
    Check for retrans_out > 1 avoids more expensive walk through the
    skb list, as we can know the result beforehand: F-RTO will not be
    allowed.
    
    SACKed skb can turn into non-SACked only in the extremely rare
    case of SACK reneging, in this case we might fail to detect
    retransmissions if there were them for any other than head. To
    get rid of that feature, whole rexmit queue would have to be
    walked (always) or FRTO should be prevented when SACK reneging
    happens. Of course RTO should still trigger after reneging which
    makes this issue even less likely to show up. And as long as the
    response is as conservative as it's now, nothing bad happens even
    then.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 572a77bb6907..7fd6b77519c3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -341,7 +341,7 @@ extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
 extern int			tcp_child_process(struct sock *parent,
 						  struct sock *child,
 						  struct sk_buff *skb);
-extern int			tcp_use_frto(const struct sock *sk);
+extern int			tcp_use_frto(struct sock *sk);
 extern void			tcp_enter_frto(struct sock *sk);
 extern void			tcp_enter_loss(struct sock *sk, int how);
 extern void			tcp_clear_retrans(struct tcp_sock *tp);

commit bdaae17da81db79b9aa4dfbf43305cfeef64f6a8
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Wed Feb 21 22:59:58 2007 -0800

    [TCP] FRTO: Moved tcp_use_frto from tcp.h to tcp_input.c
    
    In addition, removed inline.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5c472f255b77..572a77bb6907 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -341,6 +341,7 @@ extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
 extern int			tcp_child_process(struct sock *parent,
 						  struct sock *child,
 						  struct sk_buff *skb);
+extern int			tcp_use_frto(const struct sock *sk);
 extern void			tcp_enter_frto(struct sock *sk);
 extern void			tcp_enter_loss(struct sock *sk, int how);
 extern void			tcp_clear_retrans(struct tcp_sock *tp);
@@ -1033,19 +1034,6 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 
 #define TCP_CHECK_TIMER(sk) do { } while (0)
 
-static inline int tcp_use_frto(const struct sock *sk)
-{
-	const struct tcp_sock *tp = tcp_sk(sk);
-	
-	/* F-RTO must be activated in sysctl and there must be some
-	 * unsent new data, and the advertised window should allow
-	 * sending it.
-	 */
-	return (sysctl_tcp_frto && sk->sk_send_head &&
-		!after(TCP_SKB_CB(sk->sk_send_head)->end_seq,
-		       tp->snd_una + tp->snd_wnd));
-}
-
 static inline void tcp_mib_init(void)
 {
 	/* See RFC 2012 */

commit ba7808eac17360dda459f82222859b0e3879854b
Author: Frederik Deweerdt <frederik.deweerdt@gmail.com>
Date:   Sun Feb 4 20:15:27 2007 -0800

    [TCP]: remove tcp header from tcp_v4_check (take #2)
    
    The tcphdr struct passed to tcp_v4_check is not used, the following
    patch removes it from the parameter list.
    
    This adds the netfilter modifications missing in the patch I sent
    for rc3-mm1.
    
    Signed-off-by: Frederik Deweerdt <frederik.deweerdt@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cd8fa0c858ae..5c472f255b77 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -802,9 +802,8 @@ static inline void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 /*
  * Calculate(/check) TCP checksum
  */
-static inline __sum16 tcp_v4_check(struct tcphdr *th, int len,
-			       __be32 saddr, __be32 daddr,
-			       __wsum base)
+static inline __sum16 tcp_v4_check(int len, __be32 saddr,
+				   __be32 daddr, __wsum base)
 {
 	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
 }

commit 0d630cc0a6cb8ad544b5913a26ff1349ed7c8587
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Thu Jan 4 12:25:16 2007 -0800

    [TCP]: Use old definition of before
    
    This reverts the new (unambiguous) definition of the TCP `before'
    relation. As pointed out in an example by Herbert Xu, there is
    existing code which implicitly requires the old definition in order
    to work correctly.
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b7d8317f22ac..cd8fa0c858ae 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -242,7 +242,7 @@ extern int tcp_memory_pressure;
 
 static inline int before(__u32 seq1, __u32 seq2)
 {
-        return (__s32)(seq2-seq1) > 0;
+        return (__s32)(seq1-seq2) < 0;
 }
 #define after(seq2, seq1) 	before(seq1, seq2)
 

commit 9a036b9c33f74c989c4c8ac0abe05e0ed88f1f25
Author: Gerrit Renker <gerrit@erg.abdn.ac.uk>
Date:   Wed Dec 20 10:25:55 2006 -0800

    [TCP]: Fix ambiguity in the `before' relation.
    
    While looking at DCCP sequence numbers, I stumbled over a problem with
    the following definition of before in tcp.h:
    
    static inline int before(__u32 seq1, __u32 seq2)
    {
            return (__s32)(seq1-seq2) < 0;
    }
    
    Problem: This definition suffers from an an ambiguity, i.e. always
    
               before(a, (a + 2^31) % 2^32)) = 1
               before((a + 2^31) % 2^32), a) = 1
    
             In text: when the difference between a and b amounts to 2^31,
             a is always considered `before' b, the function can not decide.
             The reason is that implicitly 0 is `before' 1 ... 2^31-1 ... 2^31
    
    Solution: There is a simple fix, by defining before in such a way that
              0 is no longer `before' 2^31, i.e. 0 `before' 1 ... 2^31-1
              By not using the middle between 0 and 2^32, before can be made
              unambiguous.
              This is achieved by testing whether seq2-seq1 > 0 (using signed
              32-bit arithmetic).
    
    I attach a patch to codify this. Also the `after' relation is basically
    a redefinition of `before', it is now defined as a macro after before.
    
    Signed-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c99774f15eba..b7d8317f22ac 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -242,14 +242,9 @@ extern int tcp_memory_pressure;
 
 static inline int before(__u32 seq1, __u32 seq2)
 {
-        return (__s32)(seq1-seq2) < 0;
+        return (__s32)(seq2-seq1) > 0;
 }
-
-static inline int after(__u32 seq1, __u32 seq2)
-{
-	return (__s32)(seq2-seq1) < 0;
-}
-
+#define after(seq2, seq1) 	before(seq1, seq2)
 
 /* is s2<=s1<=s3 ? */
 static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)

commit 8e5200f54062b8af0ed1d186ea0f113854786d89
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Nov 20 18:06:37 2006 -0800

    [NET]: Fix assorted misannotations (from md5 and udplite merges).
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index aa7989c53791..c99774f15eba 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1154,7 +1154,7 @@ extern int			tcp_v4_md5_do_add(struct sock *sk,
 						  u8 newkeylen);
 
 extern int			tcp_v4_md5_do_del(struct sock *sk,
-						  u32 addr);
+						  __be32 addr);
 
 extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(void);
 extern void			tcp_free_md5sig_pool(void);

commit b51655b958dfb1176bfcf99466231fdbef8751ff
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 14 21:40:42 2006 -0800

    [NET]: Annotate __skb_checksum_complete() and friends.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 826aaecdb994..aa7989c53791 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -814,7 +814,7 @@ static inline __sum16 tcp_v4_check(struct tcphdr *th, int len,
 	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
 }
 
-static inline int __tcp_checksum_complete(struct sk_buff *skb)
+static inline __sum16 __tcp_checksum_complete(struct sk_buff *skb)
 {
 	return __skb_checksum_complete(skb);
 }

commit 6b11687ef003ed595033da89643c8995676f929d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 14 21:28:24 2006 -0800

    [NET]: Annotate csum_tcpudp_magic() callers in net/*
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 363960872de0..826aaecdb994 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -807,9 +807,9 @@ static inline void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 /*
  * Calculate(/check) TCP checksum
  */
-static inline u16 tcp_v4_check(struct tcphdr *th, int len,
-			       unsigned long saddr, unsigned long daddr, 
-			       unsigned long base)
+static inline __sum16 tcp_v4_check(struct tcphdr *th, int len,
+			       __be32 saddr, __be32 daddr,
+			       __wsum base)
 {
 	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
 }

commit cfb6eeb4c860592edd123fdea908d23c6ad1c7dc
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Tue Nov 14 19:07:45 2006 -0800

    [TCP]: MD5 Signature Option (RFC2385) support.
    
    Based on implementation by Rick Payne.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e1a5d29d0a1f..363960872de0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -28,6 +28,7 @@
 #include <linux/percpu.h>
 #include <linux/skbuff.h>
 #include <linux/dmaengine.h>
+#include <linux/crypto.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -161,6 +162,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOPT_SACK_PERM        4       /* SACK Permitted */
 #define TCPOPT_SACK             5       /* SACK Block */
 #define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
+#define TCPOPT_MD5SIG		19	/* MD5 Signature (RFC2385) */
 
 /*
  *     TCP option lengths
@@ -170,6 +172,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_WINDOW         3
 #define TCPOLEN_SACK_PERM      2
 #define TCPOLEN_TIMESTAMP      10
+#define TCPOLEN_MD5SIG         18
 
 /* But this is what stacks really send out. */
 #define TCPOLEN_TSTAMP_ALIGNED		12
@@ -178,6 +181,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define TCPOLEN_SACK_BASE		2
 #define TCPOLEN_SACK_BASE_ALIGNED	4
 #define TCPOLEN_SACK_PERBLOCK		8
+#define TCPOLEN_MD5SIG_ALIGNED		20
 
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
@@ -299,6 +303,8 @@ extern void			tcp_cleanup_rbuf(struct sock *sk, int copied);
 extern int			tcp_twsk_unique(struct sock *sk,
 						struct sock *sktw, void *twp);
 
+extern void			tcp_twsk_destructor(struct sock *sk);
+
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
 {
@@ -1064,6 +1070,114 @@ static inline void clear_all_retrans_hints(struct tcp_sock *tp){
 	tp->fastpath_skb_hint = NULL;
 }
 
+/* MD5 Signature */
+struct crypto_hash;
+
+/* - key database */
+struct tcp_md5sig_key {
+	u8			*key;
+	u8			keylen;
+};
+
+struct tcp4_md5sig_key {
+	u8			*key;
+	u16			keylen;
+	__be32			addr;
+};
+
+struct tcp6_md5sig_key {
+	u8			*key;
+	u16			keylen;
+#if 0
+	u32			scope_id;	/* XXX */
+#endif
+	struct in6_addr		addr;
+};
+
+/* - sock block */
+struct tcp_md5sig_info {
+	struct tcp4_md5sig_key	*keys4;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	struct tcp6_md5sig_key	*keys6;
+	u32			entries6;
+	u32			alloced6;
+#endif
+	u32			entries4;
+	u32			alloced4;
+};
+
+/* - pseudo header */
+struct tcp4_pseudohdr {
+	__be32		saddr;
+	__be32		daddr;
+	__u8		pad;
+	__u8		protocol;
+	__be16		len;
+};
+
+struct tcp6_pseudohdr {
+	struct in6_addr	saddr;
+	struct in6_addr daddr;
+	__be32		len;
+	__be32		protocol;	/* including padding */
+};
+
+union tcp_md5sum_block {
+	struct tcp4_pseudohdr ip4;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	struct tcp6_pseudohdr ip6;
+#endif
+};
+
+/* - pool: digest algorithm, hash description and scratch buffer */
+struct tcp_md5sig_pool {
+	struct hash_desc	md5_desc;
+	union tcp_md5sum_block	md5_blk;
+};
+
+#define TCP_MD5SIG_MAXKEYS	(~(u32)0)	/* really?! */
+
+/* - functions */
+extern int			tcp_v4_calc_md5_hash(char *md5_hash,
+						     struct tcp_md5sig_key *key,
+						     struct sock *sk,
+						     struct dst_entry *dst,
+						     struct request_sock *req,
+						     struct tcphdr *th,
+						     int protocol, int tcplen);
+extern struct tcp_md5sig_key	*tcp_v4_md5_lookup(struct sock *sk,
+						   struct sock *addr_sk);
+
+extern int			tcp_v4_md5_do_add(struct sock *sk,
+						  __be32 addr,
+						  u8 *newkey,
+						  u8 newkeylen);
+
+extern int			tcp_v4_md5_do_del(struct sock *sk,
+						  u32 addr);
+
+extern struct tcp_md5sig_pool	**tcp_alloc_md5sig_pool(void);
+extern void			tcp_free_md5sig_pool(void);
+
+extern struct tcp_md5sig_pool	*__tcp_get_md5sig_pool(int cpu);
+extern void			__tcp_put_md5sig_pool(void);
+
+static inline
+struct tcp_md5sig_pool		*tcp_get_md5sig_pool(void)
+{
+	int cpu = get_cpu();
+	struct tcp_md5sig_pool *ret = __tcp_get_md5sig_pool(cpu);
+	if (!ret)
+		put_cpu();
+	return ret;
+}
+
+static inline void		tcp_put_md5sig_pool(void)
+{
+	__tcp_put_md5sig_pool();
+	put_cpu();
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,
@@ -1103,6 +1217,35 @@ extern int  tcp4_proc_init(void);
 extern void tcp4_proc_exit(void);
 #endif
 
+/* TCP af-specific functions */
+struct tcp_sock_af_ops {
+#ifdef CONFIG_TCP_MD5SIG
+	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
+						struct sock *addr_sk);
+	int			(*calc_md5_hash) (char *location,
+						  struct tcp_md5sig_key *md5,
+						  struct sock *sk,
+						  struct dst_entry *dst,
+						  struct request_sock *req,
+						  struct tcphdr *th,
+						  int protocol, int len);
+	int			(*md5_add) (struct sock *sk,
+					    struct sock *addr_sk,
+					    u8 *newkey,
+					    u8 len);
+	int			(*md5_parse) (struct sock *sk,
+					      char __user *optval,
+					      int optlen);
+#endif
+};
+
+struct tcp_request_sock_ops {
+#ifdef CONFIG_TCP_MD5SIG
+	struct tcp_md5sig_key	*(*md5_lookup) (struct sock *sk,
+						struct request_sock *req);
+#endif
+};
+
 extern void tcp_v4_init(struct net_proto_family *ops);
 extern void tcp_init(void);
 

commit ce7bc3bf15cbf5dc5a5587ccb6b04c5b4dde4336
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 9 16:35:15 2006 -0800

    [TCP]: Restrict congestion control choices.
    
    Allow normal users to only choose among a restricted set of congestion
    control choices.  The default is reno and what ever has been configured
    as default. But the policy can be changed by administrator at any time.
    
    For example, to allow any choice:
        cp /proc/sys/net/ipv4/tcp_available_congestion_control \
           /proc/sys/net/ipv4/tcp_allowed_congestion_control
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6af4baf5b769..e1a5d29d0a1f 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -625,6 +625,7 @@ enum tcp_ca_event {
 
 struct tcp_congestion_ops {
 	struct list_head	list;
+	int	non_restricted;
 
 	/* initialize private data (optional) */
 	void (*init)(struct sock *sk);
@@ -663,6 +664,8 @@ extern void tcp_cleanup_congestion_control(struct sock *sk);
 extern int tcp_set_default_congestion_control(const char *name);
 extern void tcp_get_default_congestion_control(char *name);
 extern void tcp_get_available_congestion_control(char *buf, size_t len);
+extern void tcp_get_allowed_congestion_control(char *buf, size_t len);
+extern int tcp_set_allowed_congestion_control(char *allowed);
 extern int tcp_set_congestion_control(struct sock *sk, const char *name);
 extern void tcp_slow_start(struct tcp_sock *tp);
 

commit 3ff825b28d3345ef381eceae22bf9d92231f23dc
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 9 16:32:06 2006 -0800

    [TCP]: Add tcp_available_congestion_control sysctl.
    
    Create /proc/sys/net/ipv4/tcp_available_congestion_control
    that reflects currently available TCP choices.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 246916c2321e..6af4baf5b769 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -620,6 +620,9 @@ enum tcp_ca_event {
  * Interface for adding new TCP congestion control handlers
  */
 #define TCP_CA_NAME_MAX	16
+#define TCP_CA_MAX	128
+#define TCP_CA_BUF_MAX	(TCP_CA_NAME_MAX*TCP_CA_MAX)
+
 struct tcp_congestion_ops {
 	struct list_head	list;
 
@@ -659,6 +662,7 @@ extern void tcp_init_congestion_control(struct sock *sk);
 extern void tcp_cleanup_congestion_control(struct sock *sk);
 extern int tcp_set_default_congestion_control(const char *name);
 extern void tcp_get_default_congestion_control(char *name);
+extern void tcp_get_available_congestion_control(char *buf, size_t len);
 extern int tcp_set_congestion_control(struct sock *sk, const char *name);
 extern void tcp_slow_start(struct tcp_sock *tp);
 

commit 72a3effaf633bcae9034b7e176bdbd78d64a71db
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Nov 16 02:30:37 2006 -0800

    [NET]: Size listen hash tables using backlog hint
    
    We currently allocate a fixed size (TCP_SYNQ_HSIZE=512) slots hash table for
    each LISTEN socket, regardless of various parameters (listen backlog for
    example)
    
    On x86_64, this means order-1 allocations (might fail), even for 'small'
    sockets, expecting few connections. On the contrary, a huge server wanting a
    backlog of 50000 is slowed down a bit because of this fixed limit.
    
    This patch makes the sizing of listen hash table a dynamic parameter,
    depending of :
    - net.core.somaxconn tunable (default is 128)
    - net.ipv4.tcp_max_syn_backlog tunable (default : 256, 1024 or 128)
    - backlog value given by user application  (2nd parameter of listen())
    
    For large allocations (bigger than PAGE_SIZE), we use vmalloc() instead of
    kmalloc().
    
    We still limit memory allocation with the two existing tunables (somaxconn &
    tcp_max_syn_backlog). So for standard setups, this patch actually reduce RAM
    usage.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 7a093d0aa0fe..246916c2321e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -138,7 +138,6 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 #define MAX_TCP_SYNCNT		127
 
 #define TCP_SYNQ_INTERVAL	(HZ/5)	/* Period of SYNACK timer */
-#define TCP_SYNQ_HSIZE		512	/* Size of SYNACK hash table */
 
 #define TCP_PAWS_24DAYS	(60 * 60 * 24 * 24)
 #define TCP_PAWS_MSL	60		/* Per-host timestamps are invalidated

commit 3687b1dc6fe83a500ba4d3235704594f6a111a2d
Author: Wei Yongjun <yjwei@nanjing-fnst.com>
Date:   Sun Jul 30 20:35:54 2006 -0700

    [TCP]: SNMPv2 tcpAttemptFails counter error
    
    Refer to RFC2012, tcpAttemptFails is defined as following:
      tcpAttemptFails OBJECT-TYPE
          SYNTAX      Counter32
          MAX-ACCESS  read-only
          STATUS      current
          DESCRIPTION
                  "The number of times TCP connections have made a direct
                  transition to the CLOSED state from either the SYN-SENT
                  state or the SYN-RCVD state, plus the number of times TCP
                  connections have made a direct transition to the LISTEN
                  state from the SYN-RCVD state."
          ::= { tcp 7 }
    
    When I lookup into RFC793, I found that the state change should occured
    under following condition:
      1. SYN-SENT -> CLOSED
         a) Received ACK,RST segment when SYN-SENT state.
    
      2. SYN-RCVD -> CLOSED
         b) Received SYN segment when SYN-RCVD state(came from LISTEN).
         c) Received RST segment when SYN-RCVD state(came from SYN-SENT).
         d) Received SYN segment when SYN-RCVD state(came from SYN-SENT).
    
      3. SYN-RCVD -> LISTEN
         e) Received RST segment when SYN-RCVD state(came from LISTEN).
    
    In my test, those direct state transition can not be counted to
    tcpAttemptFails.
    
    Signed-off-by: Wei Yongjun <yjwei@nanjing-fnst.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0720bddff1e9..7a093d0aa0fe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -914,6 +914,9 @@ static inline void tcp_set_state(struct sock *sk, int state)
 
 static inline void tcp_done(struct sock *sk)
 {
+	if(sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)
+		TCP_INC_STATS_BH(TCP_MIB_ATTEMPTFAILS);
+
 	tcp_set_state(sk, TCP_CLOSE);
 	tcp_clear_xmit_timers(sk);
 

commit a430a43d087545c96542ee64573237919109d370
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sat Jul 8 13:34:56 2006 -0700

    [NET] gso: Fix up GSO packets with broken checksums
    
    Certain subsystems in the stack (e.g., netfilter) can break the partial
    checksum on GSO packets.  Until they're fixed, this patch allows this to
    work by recomputing the partial checksums through the GSO mechanism.
    
    Once they've all been converted to update the partial checksum instead of
    clearing it, this workaround can be removed.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3cd803b0d7a5..0720bddff1e9 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1086,6 +1086,7 @@ extern struct request_sock_ops tcp_request_sock_ops;
 
 extern int tcp_v4_destroy_sock(struct sock *sk);
 
+extern int tcp_v4_gso_send_check(struct sk_buff *skb);
 extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features);
 
 #ifdef CONFIG_PROC_FS

commit bcd76111178ebccedd46a9b3eaff65c78e5a70af
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Jun 30 13:36:35 2006 -0700

    [NET]: Generalise TSO-specific bits from skb_setup_caps
    
    This patch generalises the TSO-specific bits from sk_setup_caps by adding
    the sk_gso_type member to struct sock.  This makes sk_setup_caps generic
    so that it can be used by TCPv6 or UFO.
    
    The only catch is that whoever uses this must provide a GSO implementation
    for their protocol which I think is a fair deal :) For now UFO continues to
    live without a GSO implementation which is OK since it doesn't use the sock
    caps field at the moment.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 624921e76332..3cd803b0d7a5 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -751,7 +751,7 @@ static inline int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
 	if (in_flight >= tp->snd_cwnd)
 		return 1;
 
-	if (!(sk->sk_route_caps & NETIF_F_TSO))
+	if (!sk_can_gso(sk))
 		return 0;
 
 	left = tp->snd_cwnd - in_flight;

commit 576a30eb6453439b3c37ba24455ac7090c247b5a
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Jun 27 13:22:38 2006 -0700

    [NET]: Added GSO header verification
    
    When GSO packets come from an untrusted source (e.g., a Xen guest domain),
    we need to verify the header integrity before passing it to the hardware.
    
    Since the first step in GSO is to verify the header, we can reuse that
    code by adding a new bit to gso_type: SKB_GSO_DODGY.  Packets with this
    bit set can only be fed directly to devices with the corresponding bit
    NETIF_F_GSO_ROBUST.  If the device doesn't have that bit, then the skb
    is fed to the GSO engine which will allow the packet to be sent to the
    hardware if it passes the header check.
    
    This patch changes the sg flag to a full features flag.  The same method
    can be used to implement TSO ECN support.  We simply have to mark packets
    with CWR set with SKB_GSO_ECN so that only hardware with a corresponding
    NETIF_F_TSO_ECN can accept them.  The GSO engine can either fully segment
    the packet, or segment the first MTU and pass the rest to the hardware for
    further segmentation.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ca3d38dfc00b..624921e76332 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1086,7 +1086,7 @@ extern struct request_sock_ops tcp_request_sock_ops;
 
 extern int tcp_v4_destroy_sock(struct sock *sk);
 
-extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int sg);
+extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int features);
 
 #ifdef CONFIG_PROC_FS
 extern int  tcp4_proc_init(void);

commit f4c50d990dcf11a296679dc05de3873783236711
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Jun 22 03:02:40 2006 -0700

    [NET]: Add software TSOv4
    
    This patch adds the GSO implementation for IPv4 TCP.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b197a9e615c1..ca3d38dfc00b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1086,6 +1086,8 @@ extern struct request_sock_ops tcp_request_sock_ops;
 
 extern int tcp_v4_destroy_sock(struct sock *sk);
 
+extern struct sk_buff *tcp_tso_segment(struct sk_buff *skb, int sg);
+
 #ifdef CONFIG_PROC_FS
 extern int  tcp4_proc_init(void);
 extern void tcp4_proc_exit(void);

commit 7967168cefdbc63bf332d6b1548eca7cd65ebbcc
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Jun 22 02:40:14 2006 -0700

    [NET]: Merge TSO/UFO fields in sk_buff
    
    Having separate fields in sk_buff for TSO/UFO (tso_size/ufo_size) is not
    going to scale if we add any more segmentation methods (e.g., DCCP).  So
    let's merge them.
    
    They were used to tell the protocol of a packet.  This function has been
    subsumed by the new gso_type field.  This is essentially a set of netdev
    feature bits (shifted by 16 bits) that are required to process a specific
    skb.  As such it's easy to tell whether a given device can process a GSO
    skb: you just have to and the gso_type field and the netdev's features
    field.
    
    I've made gso_type a conjunction.  The idea is that you have a base type
    (e.g., SKB_GSO_TCPV4) that can be modified further to support new features.
    For example, if we add a hardware TSO type that supports ECN, they would
    declare NETIF_F_TSO | NETIF_F_TSO_ECN.  All TSO packets with CWR set would
    have a gso_type of SKB_GSO_TCPV4 | SKB_GSO_TCPV4_ECN while all other TSO
    packets would be SKB_GSO_TCPV4.  This means that only the CWR packets need
    to be emulated in software.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5f4eb5c79689..b197a9e615c1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -569,13 +569,13 @@ struct tcp_skb_cb {
  */
 static inline int tcp_skb_pcount(const struct sk_buff *skb)
 {
-	return skb_shinfo(skb)->tso_segs;
+	return skb_shinfo(skb)->gso_segs;
 }
 
 /* This is valid iff tcp_skb_pcount() > 1. */
 static inline int tcp_skb_mss(const struct sk_buff *skb)
 {
-	return skb_shinfo(skb)->tso_size;
+	return skb_shinfo(skb)->gso_size;
 }
 
 static inline void tcp_dec_pcount_approx(__u32 *count,

commit cee4cca740d209bcb4b9857baa2253d5ba4e3fbe
Merge: 2edc322d420a 9348f0de2d2b
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:10:08 2006 -0700

    Merge git://git.infradead.org/hdrcleanup-2.6
    
    * git://git.infradead.org/hdrcleanup-2.6: (63 commits)
      [S390] __FD_foo definitions.
      Switch to __s32 types in joystick.h instead of C99 types for consistency.
      Add <sys/types.h> to headers included for userspace in <linux/input.h>
      Move inclusion of <linux/compat.h> out of user scope in asm-x86_64/mtrr.h
      Remove struct fddi_statistics from user view in <linux/if_fddi.h>
      Move user-visible parts of drivers/s390/crypto/z90crypt.h to include/asm-s390
      Revert include/media changes: Mauro says those ioctls are only used in-kernel(!)
      Include <linux/types.h> and use __uXX types in <linux/cramfs_fs.h>
      Use __uXX types in <linux/i2o_dev.h>, include <linux/ioctl.h> too
      Remove private struct dx_hash_info from public view in <linux/ext3_fs.h>
      Include <linux/types.h> and use __uXX types in <linux/affs_hardblocks.h>
      Use __uXX types in <linux/divert.h> for struct divert_blk et al.
      Use __u32 for elf_addr_t in <asm-powerpc/elf.h>, not u32. It's user-visible.
      Remove PPP_FCS from user view in <linux/ppp_defs.h>, remove __P mess entirely
      Use __uXX types in user-visible structures in <linux/nbd.h>
      Don't use 'u32' in user-visible struct ip_conntrack_old_tuple.
      Use __uXX types for S390 DASD volume label definitions which are user-visible
      S390 BIODASDREADCMB ioctl should use __u64 not u64 type.
      Remove unneeded inclusion of <linux/time.h> from <linux/ufs_fs.h>
      Fix private integer types used in V4L2 ioctls.
      ...
    
    Manually resolve conflict in include/linux/mtd/physmap.h

commit 35089bb203f44e33b6bbb6c4de0b0708f9a48921
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Tue Jun 13 22:33:04 2006 -0700

    [TCP]: Add tcp_slow_start_after_idle sysctl.
    
    A lot of people have asked for a way to disable tcp_cwnd_restart(),
    and it seems reasonable to add a sysctl to do that.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index de88c5472bfc..bfc71f954bbe 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -227,6 +227,7 @@ extern int sysctl_tcp_abc;
 extern int sysctl_tcp_mtu_probing;
 extern int sysctl_tcp_base_mss;
 extern int sysctl_tcp_workaround_signed_windows;
+extern int sysctl_tcp_slow_start_after_idle;
 
 extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;

commit 72dc5b9225c53310c010b68a70ea97c8c8e24bdf
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Mon Jun 5 17:30:08 2006 -0700

    [TCP]: Minimum congestion window consolidation.
    
    Many of the TCP congestion methods all just use ssthresh
    as the minimum congestion window on decrease.  Rather than
    duplicating the code, just have that be the default if that
    handle in the ops structure is not set.
    
    Minor behaviour change to TCP compound.  It probably wants
    to use this (ssthresh) as lower bound, rather than ssthresh/2
    because the latter causes undershoot on loss.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f1f472746e6c..de88c5472bfc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -632,7 +632,7 @@ struct tcp_congestion_ops {
 	/* return slow start threshold (required) */
 	u32 (*ssthresh)(struct sock *sk);
 	/* lower bound for congestion window (optional) */
-	u32 (*min_cwnd)(struct sock *sk);
+	u32 (*min_cwnd)(const struct sock *sk);
 	/* do new cwnd calculation (required) */
 	void (*cong_avoid)(struct sock *sk, u32 ack,
 			   u32 rtt, u32 in_flight, int good_ack);
@@ -667,7 +667,7 @@ extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct sock *sk);
 extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack,
 				u32 rtt, u32 in_flight, int flag);
-extern u32 tcp_reno_min_cwnd(struct sock *sk);
+extern u32 tcp_reno_min_cwnd(const struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 
 static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)

commit 9593782585e0cf70babe787a8463d492a68b1744
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 18:02:55 2006 -0700

    [I/OAT]: Add a sysctl for tuning the I/OAT offloaded I/O threshold
    
    Any socket recv of less than this ammount will not be offloaded
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 578cccf275d3..f1f472746e6c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -219,6 +219,7 @@ extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_low_latency;
+extern int sysctl_tcp_dma_copybreak;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;

commit 0e4b4992b8007c6b62ec143cbbb292f98813ca11
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 18:00:16 2006 -0700

    [I/OAT]: Rename cleanup_rbuf to tcp_cleanup_rbuf and make non-static
    
    Needed to be able to call tcp_cleanup_rbuf in tcp_input.c for I/OAT
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d0c2c2fa7587..578cccf275d3 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -294,6 +294,8 @@ extern int			tcp_rcv_established(struct sock *sk,
 
 extern void			tcp_rcv_space_adjust(struct sock *sk);
 
+extern void			tcp_cleanup_rbuf(struct sock *sk, int copied);
+
 extern int			tcp_twsk_unique(struct sock *sk,
 						struct sock *sktw, void *twp);
 

commit 97fc2f0848c928c63c2ae619deee61a0b1107b69
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 17:55:33 2006 -0700

    [I/OAT]: Structure changes for TCP recv offload to I/OAT
    
    Adds an async_wait_queue and some additional fields to tcp_sock, and a
    dma_cookie_t to sk_buff.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c989db8a7aa..d0c2c2fa7587 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -28,6 +28,7 @@
 #include <linux/cache.h>
 #include <linux/percpu.h>
 #include <linux/skbuff.h>
+#include <linux/dmaengine.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -817,6 +818,12 @@ static inline void tcp_prequeue_init(struct tcp_sock *tp)
 	tp->ucopy.len = 0;
 	tp->ucopy.memory = 0;
 	skb_queue_head_init(&tp->ucopy.prequeue);
+#ifdef CONFIG_NET_DMA
+	tp->ucopy.dma_chan = NULL;
+	tp->ucopy.wakeup = 0;
+	tp->ucopy.pinned_list = NULL;
+	tp->ucopy.dma_cookie = 0;
+#endif
 }
 
 /* Packet is added to VJ-style prequeue for processing in process

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 3c989db8a7aa..9e88dcd5f134 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -21,7 +21,6 @@
 #define TCP_DEBUG 1
 #define FASTRETRANS_DEBUG 1
 
-#include <linux/config.h>
 #include <linux/list.h>
 #include <linux/tcp.h>
 #include <linux/slab.h>

commit 0803dbed7a23721d091639c9e173c0389dcd524a
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Fri Mar 31 02:25:46 2006 -0800

    [TCP]: Kill unused extern decl for tcp_v4_hash_connecting()
    
    Noticed by Alan Menegotto.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9418f4d1afbb..3c989db8a7aa 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -405,9 +405,6 @@ extern int			tcp_disconnect(struct sock *sk, int flags);
 
 extern void			tcp_unhash(struct sock *sk);
 
-extern int			tcp_v4_hash_connecting(struct sock *sk);
-
-
 /* From syncookies.c */
 extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
 				    struct ip_options *opt);

commit 3fdadf7d27e3fbcf72930941884387d1f4936f04
Author: Dmitry Mishin <dim@openvz.org>
Date:   Mon Mar 20 22:45:21 2006 -0800

    [NET]: {get|set}sockopt compatibility layer
    
    This patch extends {get|set}sockopt compatibility layer in order to
    move protocol specific parts to their place and avoid huge universal
    net/compat.c file in the future.
    
    Signed-off-by: Dmitry Mishin <dim@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 457e224de468..9418f4d1afbb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -353,6 +353,12 @@ extern int			tcp_getsockopt(struct sock *sk, int level,
 extern int			tcp_setsockopt(struct sock *sk, int level, 
 					       int optname, char __user *optval, 
 					       int optlen);
+extern int			compat_tcp_getsockopt(struct sock *sk,
+					int level, int optname,
+					char __user *optval, int __user *optlen);
+extern int			compat_tcp_setsockopt(struct sock *sk,
+					int level, int optname,
+					char __user *optval, int optlen);
 extern void			tcp_set_keepalive(struct sock *sk, int val);
 extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 					    struct msghdr *msg,

commit 15d99e02babae8bc20b836917ace07d93e318149
Author: Rick Jones <rick.jones2@hp.com>
Date:   Mon Mar 20 22:40:29 2006 -0800

    [TCP]: sysctl to allow TCP window > 32767 sans wscale
    
    Back in the dark ages, we had to be conservative and only allow 15-bit
    window fields if the window scale option was not negotiated.  Some
    ancient stacks used a signed 16-bit quantity for the window field of
    the TCP header and would get confused.
    
    Those days are long gone, so we can use the full 16-bits by default
    now.
    
    There is a sysctl added so that we can still interact with such old
    stacks
    
    Signed-off-by: Rick Jones <rick.jones2@hp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 16879fa560de..457e224de468 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -224,6 +224,7 @@ extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_abc;
 extern int sysctl_tcp_mtu_probing;
 extern int sysctl_tcp_base_mss;
+extern int sysctl_tcp_workaround_signed_windows;
 
 extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;

commit 5d424d5a674f782d0659a3b66d951f412901faee
Author: John Heffner <jheffner@psc.edu>
Date:   Mon Mar 20 17:53:41 2006 -0800

    [TCP]: MTU probing
    
    Implementation of packetization layer path mtu discovery for TCP, based on
    the internet-draft currently found at
    <http://www.ietf.org/internet-drafts/draft-ietf-pmtud-method-05.txt>.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 77f21c65bbca..16879fa560de 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -60,6 +60,9 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* Minimal RCV_MSS. */
 #define TCP_MIN_RCVMSS		536U
 
+/* The least MTU to use for probing */
+#define TCP_BASE_MSS		512
+
 /* After receiving this amount of duplicate ACKs fast retransmit starts. */
 #define TCP_FASTRETRANS_THRESH 3
 
@@ -219,6 +222,8 @@ extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 extern int sysctl_tcp_abc;
+extern int sysctl_tcp_mtu_probing;
+extern int sysctl_tcp_base_mss;
 
 extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;
@@ -447,6 +452,10 @@ extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 
 extern void tcp_initialize_rcv_mss(struct sock *sk);
 
+extern int tcp_mtu_to_mss(struct sock *sk, int pmtu);
+extern int tcp_mss_to_mtu(struct sock *sk, int mss);
+extern void tcp_mtup_init(struct sock *sk);
+
 static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
 	tp->pred_flags = htonl((tp->tcp_header_len << 26) |

commit 40efc6fa179f440a008333ea98f701bc35a1f97f
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Tue Jan 3 16:03:49 2006 -0800

    [TCP]: less inline's
    
    TCP inline usage cleanup:
     * get rid of inline in several places
     * replace __inline__ with inline where possible
     * move functions used in one file out of tcp.h
     * let compiler decide on used once cases
    
    On x86_64:
       text    data     bss     dec     hex filename
    3594701  648348  567400 4810449  4966d1 vmlinux.orig
    3593133  648580  567400 4809113  496199 vmlinux
    
    On sparc64:
       text    data     bss     dec     hex filename
    2538278  406152  530392 3474822  350586 vmlinux.ORIG
    2536382  406384  530392 3473158  34ff06 vmlinux
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 369930497401..77f21c65bbca 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -445,34 +445,16 @@ typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
 extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 			 sk_read_actor_t recv_actor);
 
-/* Initialize RCV_MSS value.
- * RCV_MSS is an our guess about MSS used by the peer.
- * We haven't any direct information about the MSS.
- * It's better to underestimate the RCV_MSS rather than overestimate.
- * Overestimations make us ACKing less frequently than needed.
- * Underestimations are more easy to detect and fix by tcp_measure_rcv_mss().
- */
+extern void tcp_initialize_rcv_mss(struct sock *sk);
 
-static inline void tcp_initialize_rcv_mss(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	unsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);
-
-	hint = min(hint, tp->rcv_wnd/2);
-	hint = min(hint, TCP_MIN_RCVMSS);
-	hint = max(hint, TCP_MIN_MSS);
-
-	inet_csk(sk)->icsk_ack.rcv_mss = hint;
-}
-
-static __inline__ void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
+static inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
 {
 	tp->pred_flags = htonl((tp->tcp_header_len << 26) |
 			       ntohl(TCP_FLAG_ACK) |
 			       snd_wnd);
 }
 
-static __inline__ void tcp_fast_path_on(struct tcp_sock *tp)
+static inline void tcp_fast_path_on(struct tcp_sock *tp)
 {
 	__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);
 }
@@ -490,7 +472,7 @@ static inline void tcp_fast_path_check(struct sock *sk, struct tcp_sock *tp)
  * Rcv_nxt can be after the window if our peer push more data
  * than the offered window.
  */
-static __inline__ u32 tcp_receive_window(const struct tcp_sock *tp)
+static inline u32 tcp_receive_window(const struct tcp_sock *tp)
 {
 	s32 win = tp->rcv_wup + tp->rcv_wnd - tp->rcv_nxt;
 
@@ -662,6 +644,7 @@ extern void tcp_cleanup_congestion_control(struct sock *sk);
 extern int tcp_set_default_congestion_control(const char *name);
 extern void tcp_get_default_congestion_control(char *name);
 extern int tcp_set_congestion_control(struct sock *sk, const char *name);
+extern void tcp_slow_start(struct tcp_sock *tp);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct sock *sk);
@@ -701,7 +684,7 @@ static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
  *	"Packets left network, but not honestly ACKed yet" PLUS
  *	"Packets fast retransmitted"
  */
-static __inline__ unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
+static inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 {
 	return (tp->packets_out - tp->left_out + tp->retrans_out);
 }
@@ -721,33 +704,6 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 			    (tp->snd_cwnd >> 2)));
 }
 
-/*
- * Linear increase during slow start
- */
-static inline void tcp_slow_start(struct tcp_sock *tp)
-{
-	if (sysctl_tcp_abc) {
-		/* RFC3465: Slow Start
-		 * TCP sender SHOULD increase cwnd by the number of
-		 * previously unacknowledged bytes ACKed by each incoming
-		 * acknowledgment, provided the increase is not more than L
-		 */
-		if (tp->bytes_acked < tp->mss_cache)
-			return;
-
-		/* We MAY increase by 2 if discovered delayed ack */
-		if (sysctl_tcp_abc > 1 && tp->bytes_acked > 2*tp->mss_cache) {
-			if (tp->snd_cwnd < tp->snd_cwnd_clamp)
-				tp->snd_cwnd++;
-		}
-	}
-	tp->bytes_acked = 0;
-
-	if (tp->snd_cwnd < tp->snd_cwnd_clamp)
-		tp->snd_cwnd++;
-}
-
-
 static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
 	if (tp->rx_opt.sack_ok &&
@@ -756,34 +712,7 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 
-/* Set slow start threshold and cwnd not falling to slow start */
-static inline void __tcp_enter_cwr(struct sock *sk)
-{
-	const struct inet_connection_sock *icsk = inet_csk(sk);
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	tp->undo_marker = 0;
-	tp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);
-	tp->snd_cwnd = min(tp->snd_cwnd,
-			   tcp_packets_in_flight(tp) + 1U);
-	tp->snd_cwnd_cnt = 0;
-	tp->high_seq = tp->snd_nxt;
-	tp->snd_cwnd_stamp = tcp_time_stamp;
-	TCP_ECN_queue_cwr(tp);
-}
-
-static inline void tcp_enter_cwr(struct sock *sk)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	tp->prior_ssthresh = 0;
-	tp->bytes_acked = 0;
-	if (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {
-		__tcp_enter_cwr(sk);
-		tcp_set_ca_state(sk, TCP_CA_CWR);
-	}
-}
-
+extern void tcp_enter_cwr(struct sock *sk);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 
 /* Slow start with delack produces 3 packets of burst, so that
@@ -815,14 +744,14 @@ static inline int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
 		return left <= tcp_max_burst(tp);
 }
 
-static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss, 
-					   const struct sk_buff *skb)
+static inline void tcp_minshall_update(struct tcp_sock *tp, int mss,
+				       const struct sk_buff *skb)
 {
 	if (skb->len < mss)
 		tp->snd_sml = TCP_SKB_CB(skb)->end_seq;
 }
 
-static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
+static inline void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
 	if (!tp->packets_out && !icsk->icsk_pending)
@@ -830,18 +759,18 @@ static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *t
 					  icsk->icsk_rto, TCP_RTO_MAX);
 }
 
-static __inline__ void tcp_push_pending_frames(struct sock *sk,
-					       struct tcp_sock *tp)
+static inline void tcp_push_pending_frames(struct sock *sk,
+					   struct tcp_sock *tp)
 {
 	__tcp_push_pending_frames(sk, tp, tcp_current_mss(sk, 1), tp->nonagle);
 }
 
-static __inline__ void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 {
 	tp->snd_wl1 = seq;
 }
 
-static __inline__ void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+static inline void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 {
 	tp->snd_wl1 = seq;
 }
@@ -849,19 +778,19 @@ static __inline__ void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 /*
  * Calculate(/check) TCP checksum
  */
-static __inline__ u16 tcp_v4_check(struct tcphdr *th, int len,
-				   unsigned long saddr, unsigned long daddr, 
-				   unsigned long base)
+static inline u16 tcp_v4_check(struct tcphdr *th, int len,
+			       unsigned long saddr, unsigned long daddr, 
+			       unsigned long base)
 {
 	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
 }
 
-static __inline__ int __tcp_checksum_complete(struct sk_buff *skb)
+static inline int __tcp_checksum_complete(struct sk_buff *skb)
 {
 	return __skb_checksum_complete(skb);
 }
 
-static __inline__ int tcp_checksum_complete(struct sk_buff *skb)
+static inline int tcp_checksum_complete(struct sk_buff *skb)
 {
 	return skb->ip_summed != CHECKSUM_UNNECESSARY &&
 		__tcp_checksum_complete(skb);
@@ -869,7 +798,7 @@ static __inline__ int tcp_checksum_complete(struct sk_buff *skb)
 
 /* Prequeue for VJ style copy to user, combined with checksumming. */
 
-static __inline__ void tcp_prequeue_init(struct tcp_sock *tp)
+static inline void tcp_prequeue_init(struct tcp_sock *tp)
 {
 	tp->ucopy.task = NULL;
 	tp->ucopy.len = 0;
@@ -885,7 +814,7 @@ static __inline__ void tcp_prequeue_init(struct tcp_sock *tp)
  *
  * NOTE: is this not too big to inline?
  */
-static __inline__ int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
+static inline int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 
@@ -926,7 +855,7 @@ static const char *statename[]={
 };
 #endif
 
-static __inline__ void tcp_set_state(struct sock *sk, int state)
+static inline void tcp_set_state(struct sock *sk, int state)
 {
 	int oldstate = sk->sk_state;
 
@@ -960,7 +889,7 @@ static __inline__ void tcp_set_state(struct sock *sk, int state)
 #endif	
 }
 
-static __inline__ void tcp_done(struct sock *sk)
+static inline void tcp_done(struct sock *sk)
 {
 	tcp_set_state(sk, TCP_CLOSE);
 	tcp_clear_xmit_timers(sk);
@@ -973,81 +902,13 @@ static __inline__ void tcp_done(struct sock *sk)
 		inet_csk_destroy_sock(sk);
 }
 
-static __inline__ void tcp_sack_reset(struct tcp_options_received *rx_opt)
+static inline void tcp_sack_reset(struct tcp_options_received *rx_opt)
 {
 	rx_opt->dsack = 0;
 	rx_opt->eff_sacks = 0;
 	rx_opt->num_sacks = 0;
 }
 
-static __inline__ void tcp_build_and_update_options(__u32 *ptr, struct tcp_sock *tp, __u32 tstamp)
-{
-	if (tp->rx_opt.tstamp_ok) {
-		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) |
-					  (TCPOPT_NOP << 16) |
-					  (TCPOPT_TIMESTAMP << 8) |
-					  TCPOLEN_TIMESTAMP);
-		*ptr++ = htonl(tstamp);
-		*ptr++ = htonl(tp->rx_opt.ts_recent);
-	}
-	if (tp->rx_opt.eff_sacks) {
-		struct tcp_sack_block *sp = tp->rx_opt.dsack ? tp->duplicate_sack : tp->selective_acks;
-		int this_sack;
-
-		*ptr++ = htonl((TCPOPT_NOP  << 24) |
-			       (TCPOPT_NOP  << 16) |
-			       (TCPOPT_SACK <<  8) |
-			       (TCPOLEN_SACK_BASE + (tp->rx_opt.eff_sacks *
-						     TCPOLEN_SACK_PERBLOCK)));
-		for(this_sack = 0; this_sack < tp->rx_opt.eff_sacks; this_sack++) {
-			*ptr++ = htonl(sp[this_sack].start_seq);
-			*ptr++ = htonl(sp[this_sack].end_seq);
-		}
-		if (tp->rx_opt.dsack) {
-			tp->rx_opt.dsack = 0;
-			tp->rx_opt.eff_sacks--;
-		}
-	}
-}
-
-/* Construct a tcp options header for a SYN or SYN_ACK packet.
- * If this is every changed make sure to change the definition of
- * MAX_SYN_SIZE to match the new maximum number of options that you
- * can generate.
- */
-static inline void tcp_syn_build_options(__u32 *ptr, int mss, int ts, int sack,
-					     int offer_wscale, int wscale, __u32 tstamp, __u32 ts_recent)
-{
-	/* We always get an MSS option.
-	 * The option bytes which will be seen in normal data
-	 * packets should timestamps be used, must be in the MSS
-	 * advertised.  But we subtract them from tp->mss_cache so
-	 * that calculations in tcp_sendmsg are simpler etc.
-	 * So account for this fact here if necessary.  If we
-	 * don't do this correctly, as a receiver we won't
-	 * recognize data packets as being full sized when we
-	 * should, and thus we won't abide by the delayed ACK
-	 * rules correctly.
-	 * SACKs don't matter, we never delay an ACK when we
-	 * have any of those going out.
-	 */
-	*ptr++ = htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | mss);
-	if (ts) {
-		if(sack)
-			*ptr++ = __constant_htonl((TCPOPT_SACK_PERM << 24) | (TCPOLEN_SACK_PERM << 16) |
-						  (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);
-		else
-			*ptr++ = __constant_htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
-						  (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);
-		*ptr++ = htonl(tstamp);		/* TSVAL */
-		*ptr++ = htonl(ts_recent);	/* TSECR */
-	} else if(sack)
-		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
-					  (TCPOPT_SACK_PERM << 8) | TCPOLEN_SACK_PERM);
-	if (offer_wscale)
-		*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_WINDOW << 16) | (TCPOLEN_WINDOW << 8) | (wscale));
-}
-
 /* Determine a window scaling and initial window to offer. */
 extern void tcp_select_initial_window(int __space, __u32 mss,
 				      __u32 *rcv_wnd, __u32 *window_clamp,
@@ -1072,9 +933,9 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk->sk_rcvbuf); 
 }
 
-static __inline__ void tcp_openreq_init(struct request_sock *req,
-					struct tcp_options_received *rx_opt,
-					struct sk_buff *skb)
+static inline void tcp_openreq_init(struct request_sock *req,
+				    struct tcp_options_received *rx_opt,
+				    struct sk_buff *skb)
 {
 	struct inet_request_sock *ireq = inet_rsk(req);
 

commit 8639a11e23d9eb0a6ceac2feed27acdfbb158f95
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 27 15:17:57 2005 -0200

    [TCP]: Don't use __constant_htonl for a non const arg
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 176221cd0cce..369930497401 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -994,11 +994,11 @@ static __inline__ void tcp_build_and_update_options(__u32 *ptr, struct tcp_sock
 		struct tcp_sack_block *sp = tp->rx_opt.dsack ? tp->duplicate_sack : tp->selective_acks;
 		int this_sack;
 
-		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) |
-					  (TCPOPT_NOP << 16) |
-					  (TCPOPT_SACK << 8) |
-					  (TCPOLEN_SACK_BASE +
-					   (tp->rx_opt.eff_sacks * TCPOLEN_SACK_PERBLOCK)));
+		*ptr++ = htonl((TCPOPT_NOP  << 24) |
+			       (TCPOPT_NOP  << 16) |
+			       (TCPOPT_SACK <<  8) |
+			       (TCPOLEN_SACK_BASE + (tp->rx_opt.eff_sacks *
+						     TCPOLEN_SACK_PERBLOCK)));
 		for(this_sack = 0; this_sack < tp->rx_opt.eff_sacks; this_sack++) {
 			*ptr++ = htonl(sp[this_sack].start_seq);
 			*ptr++ = htonl(sp[this_sack].end_seq);

commit 6d6ee43e0b8b8d4847627fd43739b98ec2b9404f
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:25:19 2005 -0800

    [TWSK]: Introduce struct timewait_sock_ops
    
    So that we can share several timewait sockets related functions and
    make the timewait mini sockets infrastructure closer to the request
    mini sockets one.
    
    Next changesets will take advantage of this, moving more code out of
    TCP and DCCP v4 and v6 to common infrastructure.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 83b117a25c2a..176221cd0cce 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -287,6 +287,9 @@ extern int			tcp_rcv_established(struct sock *sk,
 
 extern void			tcp_rcv_space_adjust(struct sock *sk);
 
+extern int			tcp_twsk_unique(struct sock *sk,
+						struct sock *sktw, void *twp);
+
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
 {

commit 8292a17a399ffb7c5c8b083db4ad994e090055f7
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:15:52 2005 -0800

    [ICSK]: Rename struct tcp_func to struct inet_connection_sock_af_ops
    
    And move it to struct inet_connection_sock. DCCP will use it in the
    upcoming changesets.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d78025f9fbea..83b117a25c2a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -224,53 +224,6 @@ extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
-/*
- *	Pointers to address related TCP functions
- *	(i.e. things that depend on the address family)
- */
-
-struct tcp_func {
-	int			(*queue_xmit)		(struct sk_buff *skb,
-							 int ipfragok);
-
-	void			(*send_check)		(struct sock *sk,
-							 struct tcphdr *th,
-							 int len,
-							 struct sk_buff *skb);
-
-	int			(*rebuild_header)	(struct sock *sk);
-
-	int			(*conn_request)		(struct sock *sk,
-							 struct sk_buff *skb);
-
-	struct sock *		(*syn_recv_sock)	(struct sock *sk,
-							 struct sk_buff *skb,
-							 struct request_sock *req,
-							 struct dst_entry *dst);
-    
-	int			(*remember_stamp)	(struct sock *sk);
-
-	__u16			net_header_len;
-
-	int			(*setsockopt)		(struct sock *sk, 
-							 int level, 
-							 int optname, 
-							 char __user *optval, 
-							 int optlen);
-
-	int			(*getsockopt)		(struct sock *sk, 
-							 int level, 
-							 int optname, 
-							 char __user *optval, 
-							 int __user *optlen);
-
-
-	void			(*addr2sockaddr)	(struct sock *sk,
-							 struct sockaddr *);
-
-	int sockaddr_len;
-};
-
 /*
  * The next routines deal with comparing 32 bit unsigned ints
  * and worry about wraparound (automatic with unsigned arithmetic).
@@ -405,8 +358,7 @@ extern void			tcp_parse_options(struct sk_buff *skb,
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern void		       	tcp_v4_send_check(struct sock *sk, 
-						  struct tcphdr *th, int len, 
+extern void		       	tcp_v4_send_check(struct sock *sk, int len,
 						  struct sk_buff *skb);
 
 extern int			tcp_v4_conn_request(struct sock *sk,

commit 31f3426904e066f17e3f88c468a2f7c869ad4aac
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Tue Nov 15 15:17:10 2005 -0800

    [TCP]: More spelling fixes.
    
    From Joe Perches
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0f9848011972..d78025f9fbea 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -552,8 +552,8 @@ extern u32	__tcp_select_window(struct sock *sk);
 
 /* TCP timestamps are only 32-bits, this causes a slight
  * complication on 64-bit systems since we store a snapshot
- * of jiffies in the buffer control blocks below.  We decidedly
- * only use of the low 32-bits of jiffies and hide the ugly
+ * of jiffies in the buffer control blocks below.  We decided
+ * to use only the low 32-bits of jiffies and hide the ugly
  * casts with the following macro.
  */
 #define tcp_time_stamp		((__u32)(jiffies))

commit 6a438bbe68c7013a42d9c5aee5a40d7dafdbe6ec
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:14:59 2005 -0800

    [TCP]: speed up SACK processing
    
    Use "hints" to speed up the SACK processing. Various forms
    of this have been used by TCP developers (Web100, STCP, BIC)
    to avoid the 2x linear search of outstanding segments.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6e6f0f3f1dd8..0f9848011972 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1207,6 +1207,15 @@ static inline void tcp_mib_init(void)
 	TCP_ADD_STATS_USER(TCP_MIB_MAXCONN, -1);
 }
 
+/*from STCP */
+static inline void clear_all_retrans_hints(struct tcp_sock *tp){
+	tp->lost_skb_hint = NULL;
+	tp->scoreboard_skb_hint = NULL;
+	tp->retransmit_skb_hint = NULL;
+	tp->forward_skb_hint = NULL;
+	tp->fastpath_skb_hint = NULL;
+}
+
 /* /proc */
 enum tcp_seq_states {
 	TCP_SEQ_STATE_LISTENING,

commit caa20d9abe810be2ede9612b6c9db6ce7d6edf80
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:13:47 2005 -0800

    [TCP]: spelling fixes
    
    Minor spelling fixes for TCP code.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 44ba4a21cbdc..6e6f0f3f1dd8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -89,10 +89,10 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 				 */
 
 #define TCP_SYN_RETRIES	 5	/* number of times to retry active opening a
-				 * connection: ~180sec is RFC minumum	*/
+				 * connection: ~180sec is RFC minimum	*/
 
 #define TCP_SYNACK_RETRIES 5	/* number of times to retry passive opening a
-				 * connection: ~180sec is RFC minumum	*/
+				 * connection: ~180sec is RFC minimum	*/
 
 
 #define TCP_ORPHAN_RETRIES 7	/* number of times to retry on an orphaned
@@ -180,7 +180,7 @@ extern void tcp_time_wait(struct sock *sk, int state, int timeo);
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
 #define TCP_NAGLE_CORK		2	/* Socket is corked	    */
-#define TCP_NAGLE_PUSH		4	/* Cork is overriden for already queued data */
+#define TCP_NAGLE_PUSH		4	/* Cork is overridden for already queued data */
 
 extern struct inet_timewait_death_row tcp_death_row;
 
@@ -552,13 +552,13 @@ extern u32	__tcp_select_window(struct sock *sk);
 
 /* TCP timestamps are only 32-bits, this causes a slight
  * complication on 64-bit systems since we store a snapshot
- * of jiffies in the buffer control blocks below.  We decidely
+ * of jiffies in the buffer control blocks below.  We decidedly
  * only use of the low 32-bits of jiffies and hide the ugly
  * casts with the following macro.
  */
 #define tcp_time_stamp		((__u32)(jiffies))
 
-/* This is what the send packet queueing engine uses to pass
+/* This is what the send packet queuing engine uses to pass
  * TCP per-packet control information to the transmission
  * code.  We also store the host-order sequence numbers in
  * here too.  This is 36 bytes on 32-bit architectures,
@@ -598,7 +598,7 @@ struct tcp_skb_cb {
 #define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
 #define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
 
-#define TCPCB_URG		0x20	/* Urgent pointer advenced here	*/
+#define TCPCB_URG		0x20	/* Urgent pointer advanced here	*/
 
 #define TCPCB_AT_TAIL		(TCPCB_URG)
 

commit 9772efb970780aeed488c19d8b4afd46c3b484af
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:09:53 2005 -0800

    [TCP]: Appropriate Byte Count support
    
    This is an updated version of the RFC3465 ABC patch originally
    for Linux 2.6.11-rc4 by Yee-Ting Li. ABC is a way of counting
    bytes ack'd rather than packets when updating congestion control.
    
    The orignal ABC described in the RFC applied to a Reno style
    algorithm. For advanced congestion control there is little
    change after leaving slow start.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 54c399886275..44ba4a21cbdc 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -218,6 +218,7 @@ extern int sysctl_tcp_low_latency;
 extern int sysctl_tcp_nometrics_save;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
+extern int sysctl_tcp_abc;
 
 extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;
@@ -770,6 +771,23 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
  */
 static inline void tcp_slow_start(struct tcp_sock *tp)
 {
+	if (sysctl_tcp_abc) {
+		/* RFC3465: Slow Start
+		 * TCP sender SHOULD increase cwnd by the number of
+		 * previously unacknowledged bytes ACKed by each incoming
+		 * acknowledgment, provided the increase is not more than L
+		 */
+		if (tp->bytes_acked < tp->mss_cache)
+			return;
+
+		/* We MAY increase by 2 if discovered delayed ack */
+		if (sysctl_tcp_abc > 1 && tp->bytes_acked > 2*tp->mss_cache) {
+			if (tp->snd_cwnd < tp->snd_cwnd_clamp)
+				tp->snd_cwnd++;
+		}
+	}
+	tp->bytes_acked = 0;
+
 	if (tp->snd_cwnd < tp->snd_cwnd_clamp)
 		tp->snd_cwnd++;
 }
@@ -804,6 +822,7 @@ static inline void tcp_enter_cwr(struct sock *sk)
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	tp->prior_ssthresh = 0;
+	tp->bytes_acked = 0;
 	if (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {
 		__tcp_enter_cwr(sk);
 		tcp_set_ca_state(sk, TCP_CA_CWR);

commit 7faffa1c7fb9b8e8917e3225d4e2638270c0a48b
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:07:24 2005 -0800

    [TCP]: add tcp_slow_start helper
    
    Move all the code that does linear TCP slowstart to one
    inline function to ease later patch to add ABC support.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 15bdbc6bd571..54c399886275 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -765,6 +765,16 @@ static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 			    (tp->snd_cwnd >> 2)));
 }
 
+/*
+ * Linear increase during slow start
+ */
+static inline void tcp_slow_start(struct tcp_sock *tp)
+{
+	if (tp->snd_cwnd < tp->snd_cwnd_clamp)
+		tp->snd_cwnd++;
+}
+
+
 static inline void tcp_sync_left_out(struct tcp_sock *tp)
 {
 	if (tp->rx_opt.sack_ok &&

commit f4805eded7d38c4e42bf473dc5eb2f34853beb06
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 16:53:30 2005 -0800

    [TCP]: fix congestion window update when using TSO deferal
    
    TCP peformance with TSO over networks with delay is awful.
    On a 100Mbit link with 150ms delay, we get 4Mbits/sec with TSO and
    50Mbits/sec without TSO.
    
    The problem is with TSO, we intentionally do not keep the maximum
    number of packets in flight to fill the window, we hold out to until
    we can send a MSS chunk. But, we also don't update the congestion window
    unless we have filled, as per RFC2861.
    
    This patch replaces the check for the congestion window being full
    with something smarter that accounts for TSO.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 96cc3b434e40..15bdbc6bd571 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -810,6 +810,27 @@ static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
 	return 3;
 }
 
+/* RFC2861 Check whether we are limited by application or congestion window
+ * This is the inverse of cwnd check in tcp_tso_should_defer
+ */
+static inline int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	u32 left;
+
+	if (in_flight >= tp->snd_cwnd)
+		return 1;
+
+	if (!(sk->sk_route_caps & NETIF_F_TSO))
+		return 0;
+
+	left = tp->snd_cwnd - in_flight;
+	if (sysctl_tcp_tso_win_divisor)
+		return left * sysctl_tcp_tso_win_divisor < tp->snd_cwnd;
+	else
+		return left <= tcp_max_burst(tp);
+}
+
 static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss, 
 					   const struct sk_buff *skb)
 {

commit fb286bb2990a107009dbf25f6ffebeb7df77f9be
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Nov 10 13:01:24 2005 -0800

    [NET]: Detect hardware rx checksum faults correctly
    
    Here is the patch that introduces the generic skb_checksum_complete
    which also checks for hardware RX checksum faults.  If that happens,
    it'll call netdev_rx_csum_fault which currently prints out a stack
    trace with the device name.  In future it can turn off RX checksum.
    
    I've converted every spot under net/ that does RX checksum checks to
    use skb_checksum_complete or __skb_checksum_complete with the
    exceptions of:
    
    * Those places where checksums are done bit by bit.  These will call
    netdev_rx_csum_fault directly.
    
    * The following have not been completely checked/converted:
    
    ipmr
    ip_vs
    netfilter
    dccp
    
    This patch is based on patches and suggestions from Stephen Hemminger
    and David S. Miller.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index c24339c4e310..96cc3b434e40 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -27,6 +27,7 @@
 #include <linux/slab.h>
 #include <linux/cache.h>
 #include <linux/percpu.h>
+#include <linux/skbuff.h>
 
 #include <net/inet_connection_sock.h>
 #include <net/inet_timewait_sock.h>
@@ -852,7 +853,7 @@ static __inline__ u16 tcp_v4_check(struct tcphdr *th, int len,
 
 static __inline__ int __tcp_checksum_complete(struct sk_buff *skb)
 {
-	return (unsigned short)csum_fold(skb_checksum(skb, 0, skb->len, skb->csum));
+	return __skb_checksum_complete(skb);
 }
 
 static __inline__ int tcp_checksum_complete(struct sk_buff *skb)

commit dd0fc66fb33cd610bc1a5db8a5e232d34879b4d7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Oct 7 07:46:04 2005 +0100

    [PATCH] gfp flags annotations - part 1
    
     - added typedef unsigned int __nocast gfp_t;
    
     - replaced __nocast uses for gfp flags with gfp_t - it gives exactly
       the same warnings as far as sparse is concerned, doesn't change
       generated code (from gcc point of view we replaced unsigned int with
       typedef) and documents what's going on far better.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 97af77c4d096..c24339c4e310 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -460,8 +460,7 @@ extern void tcp_send_probe0(struct sock *);
 extern void tcp_send_partial(struct sock *);
 extern int  tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
-extern void tcp_send_active_reset(struct sock *sk,
-                                  unsigned int __nocast priority);
+extern void tcp_send_active_reset(struct sock *sk, gfp_t priority);
 extern int  tcp_send_synack(struct sock *);
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);

commit 6475be16fd9b3c6746ca4d18959246b13c669ea8
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Sep 1 22:47:01 2005 -0700

    [TCP]: Keep TSO enabled even during loss events.
    
    All we need to do is resegment the queue so that
    we record SACK information accurately.  The edges
    of the SACK blocks guide our resegmenting decisions.
    
    With help from Herbert Xu.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d6bcf1317a6a..97af77c4d096 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -454,6 +454,7 @@ extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_xmit_retransmit_queue(struct sock *);
 extern void tcp_simple_retransmit(struct sock *);
 extern int tcp_trim_head(struct sock *, struct sk_buff *, u32);
+extern int tcp_fragment(struct sock *, struct sk_buff *, u32, unsigned int);
 
 extern void tcp_send_probe0(struct sock *);
 extern void tcp_send_partial(struct sock *);

commit 20380731bc2897f2952ae055420972ded4cd786e
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Aug 16 02:18:02 2005 -0300

    [NET]: Fix sparse warnings
    
    Of this type, mostly:
    
    CHECK   net/ipv6/netfilter.c
    net/ipv6/netfilter.c:96:12: warning: symbol 'ipv6_netfilter_init' was not declared. Should it be static?
    net/ipv6/netfilter.c:101:6: warning: symbol 'ipv6_netfilter_fini' was not declared. Should it be static?
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d958260af23c..d6bcf1317a6a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1183,4 +1183,16 @@ struct tcp_iter_state {
 extern int tcp_proc_register(struct tcp_seq_afinfo *afinfo);
 extern void tcp_proc_unregister(struct tcp_seq_afinfo *afinfo);
 
+extern struct request_sock_ops tcp_request_sock_ops;
+
+extern int tcp_v4_destroy_sock(struct sock *sk);
+
+#ifdef CONFIG_PROC_FS
+extern int  tcp4_proc_init(void);
+extern void tcp4_proc_exit(void);
+#endif
+
+extern void tcp_v4_init(struct net_proto_family *ops);
+extern void tcp_init(void);
+
 #endif	/* _TCP_H */

commit 17b085eacef81a6286bd478f2ec75e04abb091cb
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Fri Aug 12 12:59:17 2005 -0300

    [INET_DIAG]: Move the tcp_diag interface to the proper place
    
    With this the previous setup is back, i.e. tcp_diag can be built as a module,
    as dccp_diag and both share the infrastructure available in inet_diag.
    
    If one selects CONFIG_INET_DIAG as module CONFIG_INET_TCP_DIAG will also be
    built as a module, as will CONFIG_INET_DCCP_DIAG, if CONFIG_IP_DCCP was
    selected static or as a module, if CONFIG_INET_DIAG is y, being statically
    linked CONFIG_INET_TCP_DIAG will follow suit and CONFIG_INET_DCCP_DIAG will be
    built in the same manner as CONFIG_IP_DCCP.
    
    Now to aim at UDP, converting it to use inet_hashinfo, so that we can use
    iproute2 for UDP sockets as well.
    
    Ah, just to show an example of this new infrastructure working for DCCP :-)
    
    [root@qemu ~]# ./ss -dane
    State      Recv-Q Send-Q Local Address:Port  Peer Address:Port
    LISTEN     0      0                  *:5001             *:*     ino:942 sk:cfd503a0
    ESTAB      0      0          127.0.0.1:5001     127.0.0.1:32770 ino:943 sk:cfd50a60
    ESTAB      0      0          127.0.0.1:32770    127.0.0.1:5001  ino:947 sk:cfd50700
    TIME-WAIT  0      0          127.0.0.1:32769    127.0.0.1:5001  timer:(timewait,3.430ms,0) ino:0 sk:cf209620
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index fef122782b44..d958260af23c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -479,7 +479,7 @@ static inline void tcp_clear_xmit_timers(struct sock *sk)
 extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
 extern unsigned int tcp_current_mss(struct sock *sk, int large);
 
-/* tcp_diag.c */
+/* tcp.c */
 extern void tcp_get_info(struct sock *, struct tcp_info *);
 
 /* Read 'sendfile()'-style from a TCP socket */

commit 73c1f4a033675f168df7e98bbeeafca3c644b8a6
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Fri Aug 12 12:51:49 2005 -0300

    [TCPDIAG]: Just rename everything to inet_diag
    
    Next changeset will rename tcp_diag.[ch] to inet_diag.[ch].
    
    I'm taking this longer route so as to easy review, making clear the changes
    made all along the way.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0b3f7294c5c7..fef122782b44 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -690,7 +690,7 @@ struct tcp_congestion_ops {
 	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
 	void (*pkts_acked)(struct sock *sk, u32 num_acked);
-	/* get info for tcp_diag (optional) */
+	/* get info for inet_diag (optional) */
 	void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
 
 	char 		name[TCP_CA_NAME_MAX];

commit 6687e988d9aeaccad6774e6a8304f681f3ec0a03
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Wed Aug 10 04:03:31 2005 -0300

    [ICSK]: Move TCP congestion avoidance members to icsk
    
    This changeset basically moves tcp_sk()->{ca_ops,ca_state,etc} to inet_csk(),
    minimal renaming/moving done in this changeset to ease review.
    
    Most of it is just changes of struct tcp_sock * to struct sock * parameters.
    
    With this we move to a state closer to two interesting goals:
    
    1. Generalisation of net/ipv4/tcp_diag.c, becoming inet_diag.c, being used
       for any INET transport protocol that has struct inet_hashinfo and are
       derived from struct inet_connection_sock. Keeps the userspace API, that will
       just not display DCCP sockets, while newer versions of tools can support
       DCCP.
    
    2. INET generic transport pluggable Congestion Avoidance infrastructure, using
       the current TCP CA infrastructure with DCCP.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d489ac548e4b..0b3f7294c5c7 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -669,29 +669,29 @@ struct tcp_congestion_ops {
 	struct list_head	list;
 
 	/* initialize private data (optional) */
-	void (*init)(struct tcp_sock *tp);
+	void (*init)(struct sock *sk);
 	/* cleanup private data  (optional) */
-	void (*release)(struct tcp_sock *tp);
+	void (*release)(struct sock *sk);
 
 	/* return slow start threshold (required) */
-	u32 (*ssthresh)(struct tcp_sock *tp);
+	u32 (*ssthresh)(struct sock *sk);
 	/* lower bound for congestion window (optional) */
-	u32 (*min_cwnd)(struct tcp_sock *tp);
+	u32 (*min_cwnd)(struct sock *sk);
 	/* do new cwnd calculation (required) */
-	void (*cong_avoid)(struct tcp_sock *tp, u32 ack,
+	void (*cong_avoid)(struct sock *sk, u32 ack,
 			   u32 rtt, u32 in_flight, int good_ack);
 	/* round trip time sample per acked packet (optional) */
-	void (*rtt_sample)(struct tcp_sock *tp, u32 usrtt);
+	void (*rtt_sample)(struct sock *sk, u32 usrtt);
 	/* call before changing ca_state (optional) */
-	void (*set_state)(struct tcp_sock *tp, u8 new_state);
+	void (*set_state)(struct sock *sk, u8 new_state);
 	/* call when cwnd event occurs (optional) */
-	void (*cwnd_event)(struct tcp_sock *tp, enum tcp_ca_event ev);
+	void (*cwnd_event)(struct sock *sk, enum tcp_ca_event ev);
 	/* new value of cwnd after loss (optional) */
-	u32  (*undo_cwnd)(struct tcp_sock *tp);
+	u32  (*undo_cwnd)(struct sock *sk);
 	/* hook for packet ack accounting (optional) */
-	void (*pkts_acked)(struct tcp_sock *tp, u32 num_acked);
+	void (*pkts_acked)(struct sock *sk, u32 num_acked);
 	/* get info for tcp_diag (optional) */
-	void (*get_info)(struct tcp_sock *tp, u32 ext, struct sk_buff *skb);
+	void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
 
 	char 		name[TCP_CA_NAME_MAX];
 	struct module 	*owner;
@@ -700,30 +700,34 @@ struct tcp_congestion_ops {
 extern int tcp_register_congestion_control(struct tcp_congestion_ops *type);
 extern void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
 
-extern void tcp_init_congestion_control(struct tcp_sock *tp);
-extern void tcp_cleanup_congestion_control(struct tcp_sock *tp);
+extern void tcp_init_congestion_control(struct sock *sk);
+extern void tcp_cleanup_congestion_control(struct sock *sk);
 extern int tcp_set_default_congestion_control(const char *name);
 extern void tcp_get_default_congestion_control(char *name);
-extern int tcp_set_congestion_control(struct tcp_sock *tp, const char *name);
+extern int tcp_set_congestion_control(struct sock *sk, const char *name);
 
 extern struct tcp_congestion_ops tcp_init_congestion_ops;
-extern u32 tcp_reno_ssthresh(struct tcp_sock *tp);
-extern void tcp_reno_cong_avoid(struct tcp_sock *tp, u32 ack,
+extern u32 tcp_reno_ssthresh(struct sock *sk);
+extern void tcp_reno_cong_avoid(struct sock *sk, u32 ack,
 				u32 rtt, u32 in_flight, int flag);
-extern u32 tcp_reno_min_cwnd(struct tcp_sock *tp);
+extern u32 tcp_reno_min_cwnd(struct sock *sk);
 extern struct tcp_congestion_ops tcp_reno;
 
-static inline void tcp_set_ca_state(struct tcp_sock *tp, u8 ca_state)
+static inline void tcp_set_ca_state(struct sock *sk, const u8 ca_state)
 {
-	if (tp->ca_ops->set_state)
-		tp->ca_ops->set_state(tp, ca_state);
-	tp->ca_state = ca_state;
+	struct inet_connection_sock *icsk = inet_csk(sk);
+
+	if (icsk->icsk_ca_ops->set_state)
+		icsk->icsk_ca_ops->set_state(sk, ca_state);
+	icsk->icsk_ca_state = ca_state;
 }
 
-static inline void tcp_ca_event(struct tcp_sock *tp, enum tcp_ca_event event)
+static inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)
 {
-	if (tp->ca_ops->cwnd_event)
-		tp->ca_ops->cwnd_event(tp, event);
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+
+	if (icsk->icsk_ca_ops->cwnd_event)
+		icsk->icsk_ca_ops->cwnd_event(sk, event);
 }
 
 /* This determines how many packets are "in the network" to the best
@@ -749,9 +753,10 @@ static __inline__ unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
  * The exception is rate halving phase, when cwnd is decreasing towards
  * ssthresh.
  */
-static inline __u32 tcp_current_ssthresh(struct tcp_sock *tp)
+static inline __u32 tcp_current_ssthresh(const struct sock *sk)
 {
-	if ((1<<tp->ca_state)&(TCPF_CA_CWR|TCPF_CA_Recovery))
+	const struct tcp_sock *tp = tcp_sk(sk);
+	if ((1 << inet_csk(sk)->icsk_ca_state) & (TCPF_CA_CWR | TCPF_CA_Recovery))
 		return tp->snd_ssthresh;
 	else
 		return max(tp->snd_ssthresh,
@@ -768,10 +773,13 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 }
 
 /* Set slow start threshold and cwnd not falling to slow start */
-static inline void __tcp_enter_cwr(struct tcp_sock *tp)
+static inline void __tcp_enter_cwr(struct sock *sk)
 {
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+	struct tcp_sock *tp = tcp_sk(sk);
+
 	tp->undo_marker = 0;
-	tp->snd_ssthresh = tp->ca_ops->ssthresh(tp);
+	tp->snd_ssthresh = icsk->icsk_ca_ops->ssthresh(sk);
 	tp->snd_cwnd = min(tp->snd_cwnd,
 			   tcp_packets_in_flight(tp) + 1U);
 	tp->snd_cwnd_cnt = 0;
@@ -780,12 +788,14 @@ static inline void __tcp_enter_cwr(struct tcp_sock *tp)
 	TCP_ECN_queue_cwr(tp);
 }
 
-static inline void tcp_enter_cwr(struct tcp_sock *tp)
+static inline void tcp_enter_cwr(struct sock *sk)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
+
 	tp->prior_ssthresh = 0;
-	if (tp->ca_state < TCP_CA_CWR) {
-		__tcp_enter_cwr(tp);
-		tcp_set_ca_state(tp, TCP_CA_CWR);
+	if (inet_csk(sk)->icsk_ca_state < TCP_CA_CWR) {
+		__tcp_enter_cwr(sk);
+		tcp_set_ca_state(sk, TCP_CA_CWR);
 	}
 }
 

commit 696ab2d3bffc746fb8cf3712f066d42b9886aeed
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Aug 9 20:45:03 2005 -0700

    [TIMEWAIT]: Move inet_timewait_death_row routines to net/ipv4/inet_timewait_sock.c
    
    Also export the ones that will be used in the next changeset, when
    DCCP uses this infrastructure.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4c4cd4fb1ed8..d489ac548e4b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -44,8 +44,6 @@ extern struct inet_hashinfo tcp_hashinfo;
 
 extern atomic_t tcp_orphan_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
-extern void inet_twsk_deschedule(struct inet_timewait_sock *tw,
-				 struct inet_timewait_death_row *twdr);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 

commit 295ff7edb8f72b77d524759266f7524deae379b3
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Aug 9 20:44:40 2005 -0700

    [TIMEWAIT]: Introduce inet_timewait_death_row
    
    That groups all of the tables and variables associated to the TCP timewait
    schedulling/recycling/killing code, that now can be isolated from the TCP
    specific code and used by other transport protocols, such as DCCP.
    
    Next changeset will move this code to net/ipv4/inet_timewait_sock.c
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 077db859ae0d..4c4cd4fb1ed8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -29,6 +29,7 @@
 #include <linux/percpu.h>
 
 #include <net/inet_connection_sock.h>
+#include <net/inet_timewait_sock.h>
 #include <net/inet_hashtables.h>
 #include <net/checksum.h>
 #include <net/request_sock.h>
@@ -42,9 +43,9 @@
 extern struct inet_hashinfo tcp_hashinfo;
 
 extern atomic_t tcp_orphan_count;
-extern int tcp_tw_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
-extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
+extern void inet_twsk_deschedule(struct inet_timewait_sock *tw,
+				 struct inet_timewait_death_row *twdr);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 
@@ -148,33 +149,6 @@ extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
 					 * timestamps. It must be less than
 					 * minimal timewait lifetime.
 					 */
-
-#define TCP_TW_RECYCLE_SLOTS_LOG	5
-#define TCP_TW_RECYCLE_SLOTS		(1<<TCP_TW_RECYCLE_SLOTS_LOG)
-
-/* If time > 4sec, it is "slow" path, no recycling is required,
-   so that we select tick to get range about 4 seconds.
- */
-
-#if HZ <= 16 || HZ > 4096
-# error Unsupported: HZ <= 16 or HZ > 4096
-#elif HZ <= 32
-# define TCP_TW_RECYCLE_TICK (5+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 64
-# define TCP_TW_RECYCLE_TICK (6+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 128
-# define TCP_TW_RECYCLE_TICK (7+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 256
-# define TCP_TW_RECYCLE_TICK (8+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 512
-# define TCP_TW_RECYCLE_TICK (9+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 1024
-# define TCP_TW_RECYCLE_TICK (10+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#elif HZ <= 2048
-# define TCP_TW_RECYCLE_TICK (11+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#else
-# define TCP_TW_RECYCLE_TICK (12+2-TCP_TW_RECYCLE_SLOTS_LOG)
-#endif
 /*
  *	TCP option
  */
@@ -209,12 +183,13 @@ extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
 #define TCP_NAGLE_CORK		2	/* Socket is corked	    */
 #define TCP_NAGLE_PUSH		4	/* Cork is overriden for already queued data */
 
+extern struct inet_timewait_death_row tcp_death_row;
+
 /* sysctl variables for tcp */
 extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;
 extern int sysctl_tcp_fin_timeout;
-extern int sysctl_tcp_tw_recycle;
 extern int sysctl_tcp_keepalive_time;
 extern int sysctl_tcp_keepalive_probes;
 extern int sysctl_tcp_keepalive_intvl;
@@ -229,7 +204,6 @@ extern int sysctl_tcp_stdurg;
 extern int sysctl_tcp_rfc1337;
 extern int sysctl_tcp_abort_on_overflow;
 extern int sysctl_tcp_max_orphans;
-extern int sysctl_tcp_max_tw_buckets;
 extern int sysctl_tcp_fack;
 extern int sysctl_tcp_reordering;
 extern int sysctl_tcp_ecn;

commit a019d6fe2b9da68ea4ba6cf3c4e86fc1dbf554c3
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:15:09 2005 -0700

    [ICSK]: Move generalised functions from tcp to inet_connection_sock
    
    This also improves reqsk_queue_prune and renames it to
    inet_csk_reqsk_queue_prune, as it deals with both inet_connection_sock
    and inet_request_sock objects, not just with request_sock ones thus
    belonging to inet_request_sock.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 2423f059b62b..077db859ae0d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -423,9 +423,6 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 					    size_t len, int nonblock, 
 					    int flags, int *addr_len);
 
-extern int			inet_csk_listen_start(struct sock *sk,
-						      const int nr_table_entries);
-
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,
 						  int estab);
@@ -861,9 +858,6 @@ static __inline__ void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 	tp->snd_wl1 = seq;
 }
 
-extern void inet_csk_destroy_sock(struct sock *sk);
-
-
 /*
  * Calculate(/check) TCP checksum
  */

commit 295f7324ff8d9ea58b4d3ec93b1aaa1d80e048a9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:56 2005 -0700

    [ICSK]: Introduce reqsk_queue_prune from code in tcp_synack_timer
    
    With this we're very close to getting all of the current TCP
    refactorings in my dccp-2.6 tree merged, next changeset will export
    some functions needed by the current DCCP code and then dccp-2.6.git
    will be born!
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 68f1ec1c583a..2423f059b62b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -423,7 +423,8 @@ extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
 					    size_t len, int nonblock, 
 					    int flags, int *addr_len);
 
-extern int			tcp_listen_start(struct sock *sk);
+extern int			inet_csk_listen_start(struct sock *sk,
+						      const int nr_table_entries);
 
 extern void			tcp_parse_options(struct sk_buff *skb,
 						  struct tcp_options_received *opt_rx,

commit 0a5578cf8e5e045aaa68643c17ce885426697c6b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:41 2005 -0700

    [ICSK]: Generalise tcp_listen_{start,stop}
    
    This also moved inet_iif from tcp to inet_hashtables.h, as it is
    needed by the inet_lookup callers, perhaps this needs a bit of
    polishing, but for now seems fine.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index dd9a5a288f88..68f1ec1c583a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -860,7 +860,7 @@ static __inline__ void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 	tp->snd_wl1 = seq;
 }
 
-extern void tcp_destroy_sock(struct sock *sk);
+extern void inet_csk_destroy_sock(struct sock *sk);
 
 
 /*
@@ -987,7 +987,7 @@ static __inline__ void tcp_done(struct sock *sk)
 	if (!sock_flag(sk, SOCK_DEAD))
 		sk->sk_state_change(sk);
 	else
-		tcp_destroy_sock(sk);
+		inet_csk_destroy_sock(sk);
 }
 
 static __inline__ void tcp_sack_reset(struct tcp_options_received *rx_opt)

commit 3f421baa4720b708022f8bcc52a61e5cd6f10bf8
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:08 2005 -0700

    [NET]: Just move the inet_connection_sock function from tcp sources
    
    Completing the previous changeset, this also generalises tcp_v4_synq_add,
    renaming it to inet_csk_reqsk_queue_hash_add, already geing used in the
    DCCP tree, which I plan to merge RSN.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a943c79c88b0..dd9a5a288f88 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -19,18 +19,16 @@
 #define _TCP_H
 
 #define TCP_DEBUG 1
-#define INET_CSK_DEBUG 1
 #define FASTRETRANS_DEBUG 1
 
-/* Cancel timers, when they are not required. */
-#undef INET_CSK_CLEAR_TIMERS
-
 #include <linux/config.h>
 #include <linux/list.h>
 #include <linux/tcp.h>
 #include <linux/slab.h>
 #include <linux/cache.h>
 #include <linux/percpu.h>
+
+#include <net/inet_connection_sock.h>
 #include <net/inet_hashtables.h>
 #include <net/checksum.h>
 #include <net/request_sock.h>
@@ -206,11 +204,6 @@ extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
 #define TCPOLEN_SACK_BASE_ALIGNED	4
 #define TCPOLEN_SACK_PERBLOCK		8
 
-#define ICSK_TIME_RETRANS	1	/* Retransmit timer */
-#define ICSK_TIME_DACK		2	/* Delayed ack timer */
-#define ICSK_TIME_PROBE0	3	/* Zero window probe timer */
-#define ICSK_TIME_KEEPOPEN	4	/* Keepalive timer */
-
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
 #define TCP_NAGLE_CORK		2	/* Socket is corked	    */
@@ -257,12 +250,6 @@ extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
-#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
-#define AF_INET_FAMILY(fam) ((fam) == AF_INET)
-#else
-#define AF_INET_FAMILY(fam) 1
-#endif
-
 /*
  *	Pointers to address related TCP functions
  *	(i.e. things that depend on the address family)
@@ -373,22 +360,6 @@ extern int			tcp_rcv_established(struct sock *sk,
 
 extern void			tcp_rcv_space_adjust(struct sock *sk);
 
-enum inet_csk_ack_state_t {
-	ICSK_ACK_SCHED	= 1,
-	ICSK_ACK_TIMER  = 2,
-	ICSK_ACK_PUSHED = 4
-};
-
-static inline void inet_csk_schedule_ack(struct sock *sk)
-{
-	inet_csk(sk)->icsk_ack.pending |= ICSK_ACK_SCHED;
-}
-
-static inline int inet_csk_ack_scheduled(const struct sock *sk)
-{
-	return inet_csk(sk)->icsk_ack.pending & ICSK_ACK_SCHED;
-}
-
 static inline void tcp_dec_quickack_mode(struct sock *sk,
 					 const unsigned int pkts)
 {
@@ -406,11 +377,6 @@ static inline void tcp_dec_quickack_mode(struct sock *sk,
 
 extern void tcp_enter_quickack_mode(struct sock *sk);
 
-static inline void inet_csk_delack_init(struct sock *sk)
-{
-	memset(&inet_csk(sk)->icsk_ack, 0, sizeof(inet_csk(sk)->icsk_ack));
-}
-
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
 {
  	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
@@ -442,7 +408,6 @@ extern void			tcp_update_metrics(struct sock *sk);
 
 extern void			tcp_close(struct sock *sk, 
 					  long timeout);
-extern struct sock *		inet_csk_accept(struct sock *sk, int flags, int *err);
 extern unsigned int		tcp_poll(struct file * file, struct socket *sock, struct poll_table_struct *wait);
 
 extern int			tcp_getsockopt(struct sock *sk, int level, 
@@ -541,15 +506,9 @@ static inline void tcp_clear_xmit_timers(struct sock *sk)
 	inet_csk_clear_xmit_timers(sk);
 }
 
-extern void inet_csk_delete_keepalive_timer(struct sock *sk);
-extern void inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long timeout);
 extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
 extern unsigned int tcp_current_mss(struct sock *sk, int large);
 
-#ifdef INET_CSK_DEBUG
-extern const char inet_csk_timer_bug_msg[];
-#endif
-
 /* tcp_diag.c */
 extern void tcp_get_info(struct sock *, struct tcp_info *);
 
@@ -559,60 +518,6 @@ typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
 extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 			 sk_read_actor_t recv_actor);
 
-static inline void inet_csk_clear_xmit_timer(struct sock *sk, const int what)
-{
-	struct inet_connection_sock *icsk = inet_csk(sk);
-	
-	if (what == ICSK_TIME_RETRANS || what == ICSK_TIME_PROBE0) {
-		icsk->icsk_pending = 0;
-#ifdef INET_CSK_CLEAR_TIMERS
-		sk_stop_timer(sk, &icsk->icsk_retransmit_timer);
-#endif
-	} else if (what == ICSK_TIME_DACK) {
-		icsk->icsk_ack.blocked = icsk->icsk_ack.pending = 0;
-#ifdef INET_CSK_CLEAR_TIMERS
-		sk_stop_timer(sk, &icsk->icsk_delack_timer);
-#endif
-	}
-#ifdef INET_CSK_DEBUG
-	else {
-		pr_debug(inet_csk_timer_bug_msg);
-	}
-#endif
-}
-
-/*
- *	Reset the retransmission timer
- */
-static inline void inet_csk_reset_xmit_timer(struct sock *sk, const int what,
-					     unsigned long when)
-{
-	struct inet_connection_sock *icsk = inet_csk(sk);
-
-	if (when > TCP_RTO_MAX) {
-#ifdef INET_CSK_DEBUG
-		pr_debug("reset_xmit_timer: sk=%p %d when=0x%lx, caller=%p\n",
-			 sk, what, when, current_text_addr());
-#endif
-		when = TCP_RTO_MAX;
-	}
-
-	if (what == ICSK_TIME_RETRANS || what == ICSK_TIME_PROBE0) {
-		icsk->icsk_pending = what;
-		icsk->icsk_timeout = jiffies + when;
-		sk_reset_timer(sk, &icsk->icsk_retransmit_timer, icsk->icsk_timeout);
-	} else if (what == ICSK_TIME_DACK) {
-		icsk->icsk_ack.pending |= ICSK_ACK_TIMER;
-		icsk->icsk_ack.timeout = jiffies + when;
-		sk_reset_timer(sk, &icsk->icsk_delack_timer, icsk->icsk_ack.timeout);
-	}
-#ifdef INET_CSK_DEBUG
-	else {
-		pr_debug(inet_csk_timer_bug_msg);
-	}
-#endif
-}
-
 /* Initialize RCV_MSS value.
  * RCV_MSS is an our guess about MSS used by the peer.
  * We haven't any direct information about the MSS.
@@ -765,7 +670,8 @@ static inline void tcp_packets_out_inc(struct sock *sk,
 
 	tp->packets_out += tcp_skb_pcount(skb);
 	if (!orig)
-		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS, inet_csk(sk)->icsk_rto);
+		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
+					  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
 }
 
 static inline void tcp_packets_out_dec(struct tcp_sock *tp, 
@@ -934,7 +840,8 @@ static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *t
 {
 	const struct inet_connection_sock *icsk = inet_csk(sk);
 	if (!tp->packets_out && !icsk->icsk_pending)
-		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0, icsk->icsk_rto);
+		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,
+					  icsk->icsk_rto, TCP_RTO_MAX);
 }
 
 static __inline__ void tcp_push_pending_frames(struct sock *sk,
@@ -1017,7 +924,8 @@ static __inline__ int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 			wake_up_interruptible(sk->sk_sleep);
 			if (!inet_csk_ack_scheduled(sk))
 				inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
-						          (3 * TCP_RTO_MIN) / 4);
+						          (3 * TCP_RTO_MIN) / 4,
+							  TCP_RTO_MAX);
 		}
 		return 1;
 	}
@@ -1181,58 +1089,6 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk->sk_rcvbuf); 
 }
 
-static inline void inet_csk_reqsk_queue_add(struct sock *sk,
-					    struct request_sock *req,
-					    struct sock *child)
-{
-	reqsk_queue_add(&inet_csk(sk)->icsk_accept_queue, req, sk, child);
-}
-
-static inline void inet_csk_reqsk_queue_removed(struct sock *sk,
-						struct request_sock *req)
-{
-	if (reqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req) == 0)
-		inet_csk_delete_keepalive_timer(sk);
-}
-
-static inline void inet_csk_reqsk_queue_added(struct sock *sk,
-					      const unsigned long timeout)
-{
-	if (reqsk_queue_added(&inet_csk(sk)->icsk_accept_queue) == 0)
-		inet_csk_reset_keepalive_timer(sk, timeout);
-}
-
-static inline int inet_csk_reqsk_queue_len(const struct sock *sk)
-{
-	return reqsk_queue_len(&inet_csk(sk)->icsk_accept_queue);
-}
-
-static inline int inet_csk_reqsk_queue_young(const struct sock *sk)
-{
-	return reqsk_queue_len_young(&inet_csk(sk)->icsk_accept_queue);
-}
-
-static inline int inet_csk_reqsk_queue_is_full(const struct sock *sk)
-{
-	return reqsk_queue_is_full(&inet_csk(sk)->icsk_accept_queue);
-}
-
-static inline void inet_csk_reqsk_queue_unlink(struct sock *sk,
-					       struct request_sock *req,
-					       struct request_sock **prev)
-{
-	reqsk_queue_unlink(&inet_csk(sk)->icsk_accept_queue, req, prev);
-}
-
-static inline void inet_csk_reqsk_queue_drop(struct sock *sk,
-					     struct request_sock *req,
-					     struct request_sock **prev)
-{
-	inet_csk_reqsk_queue_unlink(sk, req, prev);
-	inet_csk_reqsk_queue_removed(sk, req);
-	reqsk_free(req);
-}
-
 static __inline__ void tcp_openreq_init(struct request_sock *req,
 					struct tcp_options_received *rx_opt,
 					struct sk_buff *skb)

commit 463c84b97f24010a67cd871746d6a7e4c925a5f9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:10:42 2005 -0700

    [NET]: Introduce inet_connection_sock
    
    This creates struct inet_connection_sock, moving members out of struct
    tcp_sock that are shareable with other INET connection oriented
    protocols, such as DCCP, that in my private tree already uses most of
    these members.
    
    The functions that operate on these members were renamed, using a
    inet_csk_ prefix while not being moved yet to a new file, so as to
    ease the review of these changes.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index cf8e664176ad..a943c79c88b0 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -19,10 +19,11 @@
 #define _TCP_H
 
 #define TCP_DEBUG 1
+#define INET_CSK_DEBUG 1
 #define FASTRETRANS_DEBUG 1
 
 /* Cancel timers, when they are not required. */
-#undef TCP_CLEAR_TIMERS
+#undef INET_CSK_CLEAR_TIMERS
 
 #include <linux/config.h>
 #include <linux/list.h>
@@ -205,10 +206,10 @@ extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
 #define TCPOLEN_SACK_BASE_ALIGNED	4
 #define TCPOLEN_SACK_PERBLOCK		8
 
-#define TCP_TIME_RETRANS	1	/* Retransmit timer */
-#define TCP_TIME_DACK		2	/* Delayed ack timer */
-#define TCP_TIME_PROBE0		3	/* Zero window probe timer */
-#define TCP_TIME_KEEPOPEN	4	/* Keepalive timer */
+#define ICSK_TIME_RETRANS	1	/* Retransmit timer */
+#define ICSK_TIME_DACK		2	/* Delayed ack timer */
+#define ICSK_TIME_PROBE0	3	/* Zero window probe timer */
+#define ICSK_TIME_KEEPOPEN	4	/* Keepalive timer */
 
 /* Flags in tp->nonagle */
 #define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
@@ -257,9 +258,9 @@ extern atomic_t tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
 #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
-#define TCP_INET_FAMILY(fam) ((fam) == AF_INET)
+#define AF_INET_FAMILY(fam) ((fam) == AF_INET)
 #else
-#define TCP_INET_FAMILY(fam) 1
+#define AF_INET_FAMILY(fam) 1
 #endif
 
 /*
@@ -372,41 +373,42 @@ extern int			tcp_rcv_established(struct sock *sk,
 
 extern void			tcp_rcv_space_adjust(struct sock *sk);
 
-enum tcp_ack_state_t
-{
-	TCP_ACK_SCHED = 1,
-	TCP_ACK_TIMER = 2,
-	TCP_ACK_PUSHED= 4
+enum inet_csk_ack_state_t {
+	ICSK_ACK_SCHED	= 1,
+	ICSK_ACK_TIMER  = 2,
+	ICSK_ACK_PUSHED = 4
 };
 
-static inline void tcp_schedule_ack(struct tcp_sock *tp)
+static inline void inet_csk_schedule_ack(struct sock *sk)
 {
-	tp->ack.pending |= TCP_ACK_SCHED;
+	inet_csk(sk)->icsk_ack.pending |= ICSK_ACK_SCHED;
 }
 
-static inline int tcp_ack_scheduled(struct tcp_sock *tp)
+static inline int inet_csk_ack_scheduled(const struct sock *sk)
 {
-	return tp->ack.pending&TCP_ACK_SCHED;
+	return inet_csk(sk)->icsk_ack.pending & ICSK_ACK_SCHED;
 }
 
-static __inline__ void tcp_dec_quickack_mode(struct tcp_sock *tp, unsigned int pkts)
+static inline void tcp_dec_quickack_mode(struct sock *sk,
+					 const unsigned int pkts)
 {
-	if (tp->ack.quick) {
-		if (pkts >= tp->ack.quick) {
-			tp->ack.quick = 0;
+	struct inet_connection_sock *icsk = inet_csk(sk);
 
+	if (icsk->icsk_ack.quick) {
+		if (pkts >= icsk->icsk_ack.quick) {
+			icsk->icsk_ack.quick = 0;
 			/* Leaving quickack mode we deflate ATO. */
-			tp->ack.ato = TCP_ATO_MIN;
+			icsk->icsk_ack.ato   = TCP_ATO_MIN;
 		} else
-			tp->ack.quick -= pkts;
+			icsk->icsk_ack.quick -= pkts;
 	}
 }
 
-extern void tcp_enter_quickack_mode(struct tcp_sock *tp);
+extern void tcp_enter_quickack_mode(struct sock *sk);
 
-static __inline__ void tcp_delack_init(struct tcp_sock *tp)
+static inline void inet_csk_delack_init(struct sock *sk)
 {
-	memset(&tp->ack, 0, sizeof(tp->ack));
+	memset(&inet_csk(sk)->icsk_ack, 0, sizeof(inet_csk(sk)->icsk_ack));
 }
 
 static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
@@ -440,7 +442,7 @@ extern void			tcp_update_metrics(struct sock *sk);
 
 extern void			tcp_close(struct sock *sk, 
 					  long timeout);
-extern struct sock *		tcp_accept(struct sock *sk, int flags, int *err);
+extern struct sock *		inet_csk_accept(struct sock *sk, int flags, int *err);
 extern unsigned int		tcp_poll(struct file * file, struct socket *sock, struct poll_table_struct *wait);
 
 extern int			tcp_getsockopt(struct sock *sk, int level, 
@@ -534,15 +536,18 @@ extern void tcp_cwnd_application_limited(struct sock *sk);
 
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);
-extern void tcp_clear_xmit_timers(struct sock *);
+static inline void tcp_clear_xmit_timers(struct sock *sk)
+{
+	inet_csk_clear_xmit_timers(sk);
+}
 
-extern void tcp_delete_keepalive_timer(struct sock *);
-extern void tcp_reset_keepalive_timer(struct sock *, unsigned long);
+extern void inet_csk_delete_keepalive_timer(struct sock *sk);
+extern void inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long timeout);
 extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
 extern unsigned int tcp_current_mss(struct sock *sk, int large);
 
-#ifdef TCP_DEBUG
-extern const char tcp_timer_bug_msg[];
+#ifdef INET_CSK_DEBUG
+extern const char inet_csk_timer_bug_msg[];
 #endif
 
 /* tcp_diag.c */
@@ -554,70 +559,58 @@ typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
 extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
 			 sk_read_actor_t recv_actor);
 
-static inline void tcp_clear_xmit_timer(struct sock *sk, int what)
+static inline void inet_csk_clear_xmit_timer(struct sock *sk, const int what)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
+	struct inet_connection_sock *icsk = inet_csk(sk);
 	
-	switch (what) {
-	case TCP_TIME_RETRANS:
-	case TCP_TIME_PROBE0:
-		tp->pending = 0;
-
-#ifdef TCP_CLEAR_TIMERS
-		sk_stop_timer(sk, &tp->retransmit_timer);
+	if (what == ICSK_TIME_RETRANS || what == ICSK_TIME_PROBE0) {
+		icsk->icsk_pending = 0;
+#ifdef INET_CSK_CLEAR_TIMERS
+		sk_stop_timer(sk, &icsk->icsk_retransmit_timer);
 #endif
-		break;
-	case TCP_TIME_DACK:
-		tp->ack.blocked = 0;
-		tp->ack.pending = 0;
-
-#ifdef TCP_CLEAR_TIMERS
-		sk_stop_timer(sk, &tp->delack_timer);
+	} else if (what == ICSK_TIME_DACK) {
+		icsk->icsk_ack.blocked = icsk->icsk_ack.pending = 0;
+#ifdef INET_CSK_CLEAR_TIMERS
+		sk_stop_timer(sk, &icsk->icsk_delack_timer);
 #endif
-		break;
-	default:
-#ifdef TCP_DEBUG
-		printk(tcp_timer_bug_msg);
+	}
+#ifdef INET_CSK_DEBUG
+	else {
+		pr_debug(inet_csk_timer_bug_msg);
+	}
 #endif
-		return;
-	};
-
 }
 
 /*
  *	Reset the retransmission timer
  */
-static inline void tcp_reset_xmit_timer(struct sock *sk, int what, unsigned long when)
+static inline void inet_csk_reset_xmit_timer(struct sock *sk, const int what,
+					     unsigned long when)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
+	struct inet_connection_sock *icsk = inet_csk(sk);
 
 	if (when > TCP_RTO_MAX) {
-#ifdef TCP_DEBUG
-		printk(KERN_DEBUG "reset_xmit_timer sk=%p %d when=0x%lx, caller=%p\n", sk, what, when, current_text_addr());
+#ifdef INET_CSK_DEBUG
+		pr_debug("reset_xmit_timer: sk=%p %d when=0x%lx, caller=%p\n",
+			 sk, what, when, current_text_addr());
 #endif
 		when = TCP_RTO_MAX;
 	}
 
-	switch (what) {
-	case TCP_TIME_RETRANS:
-	case TCP_TIME_PROBE0:
-		tp->pending = what;
-		tp->timeout = jiffies+when;
-		sk_reset_timer(sk, &tp->retransmit_timer, tp->timeout);
-		break;
-
-	case TCP_TIME_DACK:
-		tp->ack.pending |= TCP_ACK_TIMER;
-		tp->ack.timeout = jiffies+when;
-		sk_reset_timer(sk, &tp->delack_timer, tp->ack.timeout);
-		break;
-
-	default:
-#ifdef TCP_DEBUG
-		printk(tcp_timer_bug_msg);
+	if (what == ICSK_TIME_RETRANS || what == ICSK_TIME_PROBE0) {
+		icsk->icsk_pending = what;
+		icsk->icsk_timeout = jiffies + when;
+		sk_reset_timer(sk, &icsk->icsk_retransmit_timer, icsk->icsk_timeout);
+	} else if (what == ICSK_TIME_DACK) {
+		icsk->icsk_ack.pending |= ICSK_ACK_TIMER;
+		icsk->icsk_ack.timeout = jiffies + when;
+		sk_reset_timer(sk, &icsk->icsk_delack_timer, icsk->icsk_ack.timeout);
+	}
+#ifdef INET_CSK_DEBUG
+	else {
+		pr_debug(inet_csk_timer_bug_msg);
+	}
 #endif
-		return;
-	};
 }
 
 /* Initialize RCV_MSS value.
@@ -637,7 +630,7 @@ static inline void tcp_initialize_rcv_mss(struct sock *sk)
 	hint = min(hint, TCP_MIN_RCVMSS);
 	hint = max(hint, TCP_MIN_MSS);
 
-	tp->ack.rcv_mss = hint;
+	inet_csk(sk)->icsk_ack.rcv_mss = hint;
 }
 
 static __inline__ void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
@@ -772,7 +765,7 @@ static inline void tcp_packets_out_inc(struct sock *sk,
 
 	tp->packets_out += tcp_skb_pcount(skb);
 	if (!orig)
-		tcp_reset_xmit_timer(sk, TCP_TIME_RETRANS, tp->rto);
+		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS, inet_csk(sk)->icsk_rto);
 }
 
 static inline void tcp_packets_out_dec(struct tcp_sock *tp, 
@@ -939,8 +932,9 @@ static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss,
 
 static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
 {
-	if (!tp->packets_out && !tp->pending)
-		tcp_reset_xmit_timer(sk, TCP_TIME_PROBE0, tp->rto);
+	const struct inet_connection_sock *icsk = inet_csk(sk);
+	if (!tp->packets_out && !icsk->icsk_pending)
+		inet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0, icsk->icsk_rto);
 }
 
 static __inline__ void tcp_push_pending_frames(struct sock *sk,
@@ -1021,8 +1015,9 @@ static __inline__ int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
 			tp->ucopy.memory = 0;
 		} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
 			wake_up_interruptible(sk->sk_sleep);
-			if (!tcp_ack_scheduled(tp))
-				tcp_reset_xmit_timer(sk, TCP_TIME_DACK, (3*TCP_RTO_MIN)/4);
+			if (!inet_csk_ack_scheduled(sk))
+				inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
+						          (3 * TCP_RTO_MIN) / 4);
 		}
 		return 1;
 	}
@@ -1055,7 +1050,7 @@ static __inline__ void tcp_set_state(struct sock *sk, int state)
 			TCP_INC_STATS(TCP_MIB_ESTABRESETS);
 
 		sk->sk_prot->unhash(sk);
-		if (inet_sk(sk)->bind_hash &&
+		if (inet_csk(sk)->icsk_bind_hash &&
 		    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
 			inet_put_port(&tcp_hashinfo, sk);
 		/* fall through */
@@ -1186,51 +1181,55 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk->sk_rcvbuf); 
 }
 
-static inline void tcp_acceptq_queue(struct sock *sk, struct request_sock *req,
-					 struct sock *child)
+static inline void inet_csk_reqsk_queue_add(struct sock *sk,
+					    struct request_sock *req,
+					    struct sock *child)
 {
-	reqsk_queue_add(&tcp_sk(sk)->accept_queue, req, sk, child);
+	reqsk_queue_add(&inet_csk(sk)->icsk_accept_queue, req, sk, child);
 }
 
-static inline void
-tcp_synq_removed(struct sock *sk, struct request_sock *req)
+static inline void inet_csk_reqsk_queue_removed(struct sock *sk,
+						struct request_sock *req)
 {
-	if (reqsk_queue_removed(&tcp_sk(sk)->accept_queue, req) == 0)
-		tcp_delete_keepalive_timer(sk);
+	if (reqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req) == 0)
+		inet_csk_delete_keepalive_timer(sk);
 }
 
-static inline void tcp_synq_added(struct sock *sk)
+static inline void inet_csk_reqsk_queue_added(struct sock *sk,
+					      const unsigned long timeout)
 {
-	if (reqsk_queue_added(&tcp_sk(sk)->accept_queue) == 0)
-		tcp_reset_keepalive_timer(sk, TCP_TIMEOUT_INIT);
+	if (reqsk_queue_added(&inet_csk(sk)->icsk_accept_queue) == 0)
+		inet_csk_reset_keepalive_timer(sk, timeout);
 }
 
-static inline int tcp_synq_len(struct sock *sk)
+static inline int inet_csk_reqsk_queue_len(const struct sock *sk)
 {
-	return reqsk_queue_len(&tcp_sk(sk)->accept_queue);
+	return reqsk_queue_len(&inet_csk(sk)->icsk_accept_queue);
 }
 
-static inline int tcp_synq_young(struct sock *sk)
+static inline int inet_csk_reqsk_queue_young(const struct sock *sk)
 {
-	return reqsk_queue_len_young(&tcp_sk(sk)->accept_queue);
+	return reqsk_queue_len_young(&inet_csk(sk)->icsk_accept_queue);
 }
 
-static inline int tcp_synq_is_full(struct sock *sk)
+static inline int inet_csk_reqsk_queue_is_full(const struct sock *sk)
 {
-	return reqsk_queue_is_full(&tcp_sk(sk)->accept_queue);
+	return reqsk_queue_is_full(&inet_csk(sk)->icsk_accept_queue);
 }
 
-static inline void tcp_synq_unlink(struct tcp_sock *tp, struct request_sock *req,
-				   struct request_sock **prev)
+static inline void inet_csk_reqsk_queue_unlink(struct sock *sk,
+					       struct request_sock *req,
+					       struct request_sock **prev)
 {
-	reqsk_queue_unlink(&tp->accept_queue, req, prev);
+	reqsk_queue_unlink(&inet_csk(sk)->icsk_accept_queue, req, prev);
 }
 
-static inline void tcp_synq_drop(struct sock *sk, struct request_sock *req,
-				     struct request_sock **prev)
+static inline void inet_csk_reqsk_queue_drop(struct sock *sk,
+					     struct request_sock *req,
+					     struct request_sock **prev)
 {
-	tcp_synq_unlink(tcp_sk(sk), req, prev);
-	tcp_synq_removed(sk, req);
+	inet_csk_reqsk_queue_unlink(sk, req, prev);
+	inet_csk_reqsk_queue_removed(sk, req);
 	reqsk_free(req);
 }
 
@@ -1265,12 +1264,13 @@ static inline int keepalive_time_when(const struct tcp_sock *tp)
 	return tp->keepalive_time ? : sysctl_tcp_keepalive_time;
 }
 
-static inline int tcp_fin_time(const struct tcp_sock *tp)
+static inline int tcp_fin_time(const struct sock *sk)
 {
-	int fin_timeout = tp->linger2 ? : sysctl_tcp_fin_timeout;
+	int fin_timeout = tcp_sk(sk)->linger2 ? : sysctl_tcp_fin_timeout;
+	const int rto = inet_csk(sk)->icsk_rto;
 
-	if (fin_timeout < (tp->rto<<2) - (tp->rto>>1))
-		fin_timeout = (tp->rto<<2) - (tp->rto>>1);
+	if (fin_timeout < (rto << 2) - (rto >> 1))
+		fin_timeout = (rto << 2) - (rto >> 1);
 
 	return fin_timeout;
 }

commit 8feaf0c0a5488b3d898a9c207eb6678f44ba3f26
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:09:30 2005 -0700

    [INET]: Generalise tcp_tw_bucket, aka TIME_WAIT sockets
    
    This paves the way to generalise the rest of the sock ID lookup
    routines and saves some bytes in TCPv4 TIME_WAIT sockets on distro
    kernels (where IPv6 is always built as a module):
    
    [root@qemu ~]# grep tw_sock /proc/slabinfo
    tw_sock_TCPv6  0  0  128  31  1
    tw_sock_TCP    0  0   96  41  1
    [root@qemu ~]#
    
    Now if a protocol wants to use the TIME_WAIT generic infrastructure it
    only has to set the sk_prot->twsk_obj_size field with the size of its
    inet_timewait_sock derived sock and proto_register will create
    sk_prot->twsk_slab, for now its only for INET sockets, but we can
    introduce timewait_sock later if some non INET transport protocolo
    wants to use this stuff.
    
    Next changesets will take advantage of this new infrastructure to
    generalise even more TCP code.
    
    [acme@toy net-2.6.14]$ grep built-in /tmp/before.size /tmp/after.size
    /tmp/before.size: 188646   11764    5068  205478   322a6 net/ipv4/built-in.o
    /tmp/after.size:  188144   11764    5068  204976   320b0 net/ipv4/built-in.o
    [acme@toy net-2.6.14]$
    
    Tested with both IPv4 & IPv6 (::1 (localhost) & ::ffff:172.20.0.1
    (qemu host)).
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9d026d81d8c8..cf8e664176ad 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -38,207 +38,14 @@
 #include <net/ip.h>
 #include <net/tcp_states.h>
 
-#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
-#include <linux/ipv6.h>
-#endif
 #include <linux/seq_file.h>
 
 extern struct inet_hashinfo tcp_hashinfo;
 
-#if (BITS_PER_LONG == 64)
-#define TCP_ADDRCMP_ALIGN_BYTES 8
-#else
-#define TCP_ADDRCMP_ALIGN_BYTES 4
-#endif
-
-/* This is a TIME_WAIT bucket.  It works around the memory consumption
- * problems of sockets in such a state on heavily loaded servers, but
- * without violating the protocol specification.
- */
-struct tcp_tw_bucket {
-	/*
-	 * Now struct sock also uses sock_common, so please just
-	 * don't add nothing before this first member (__tw_common) --acme
-	 */
-	struct sock_common	__tw_common;
-#define tw_family		__tw_common.skc_family
-#define tw_state		__tw_common.skc_state
-#define tw_reuse		__tw_common.skc_reuse
-#define tw_bound_dev_if		__tw_common.skc_bound_dev_if
-#define tw_node			__tw_common.skc_node
-#define tw_bind_node		__tw_common.skc_bind_node
-#define tw_refcnt		__tw_common.skc_refcnt
-	volatile unsigned char	tw_substate;
-	unsigned char		tw_rcv_wscale;
-	__u16			tw_sport;
-	/* Socket demultiplex comparisons on incoming packets. */
-	/* these five are in inet_sock */
-	__u32			tw_daddr
-		__attribute__((aligned(TCP_ADDRCMP_ALIGN_BYTES)));
-	__u32			tw_rcv_saddr;
-	__u16			tw_dport;
-	__u16			tw_num;
-	/* And these are ours. */
-	int			tw_hashent;
-	int			tw_timeout;
-	__u32			tw_rcv_nxt;
-	__u32			tw_snd_nxt;
-	__u32			tw_rcv_wnd;
-	__u32			tw_ts_recent;
-	long			tw_ts_recent_stamp;
-	unsigned long		tw_ttd;
-	struct inet_bind_bucket	*tw_tb;
-	struct hlist_node	tw_death_node;
-#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
-	struct in6_addr		tw_v6_daddr;
-	struct in6_addr		tw_v6_rcv_saddr;
-	int			tw_v6_ipv6only;
-#endif
-};
-
-static __inline__ void tw_add_node(struct tcp_tw_bucket *tw,
-				   struct hlist_head *list)
-{
-	hlist_add_head(&tw->tw_node, list);
-}
-
-static __inline__ void tw_add_bind_node(struct tcp_tw_bucket *tw,
-					struct hlist_head *list)
-{
-	hlist_add_head(&tw->tw_bind_node, list);
-}
-
-static inline int tw_dead_hashed(struct tcp_tw_bucket *tw)
-{
-	return tw->tw_death_node.pprev != NULL;
-}
-
-static __inline__ void tw_dead_node_init(struct tcp_tw_bucket *tw)
-{
-	tw->tw_death_node.pprev = NULL;
-}
-
-static __inline__ void __tw_del_dead_node(struct tcp_tw_bucket *tw)
-{
-	__hlist_del(&tw->tw_death_node);
-	tw_dead_node_init(tw);
-}
-
-static __inline__ int tw_del_dead_node(struct tcp_tw_bucket *tw)
-{
-	if (tw_dead_hashed(tw)) {
-		__tw_del_dead_node(tw);
-		return 1;
-	}
-	return 0;
-}
-
-#define tw_for_each(tw, node, head) \
-	hlist_for_each_entry(tw, node, head, tw_node)
-
-#define tw_for_each_inmate(tw, node, jail) \
-	hlist_for_each_entry(tw, node, jail, tw_death_node)
-
-#define tw_for_each_inmate_safe(tw, node, safe, jail) \
-	hlist_for_each_entry_safe(tw, node, safe, jail, tw_death_node)
-
-#define tcptw_sk(__sk)	((struct tcp_tw_bucket *)(__sk))
-
-static inline u32 tcp_v4_rcv_saddr(const struct sock *sk)
-{
-	return likely(sk->sk_state != TCP_TIME_WAIT) ?
-		inet_sk(sk)->rcv_saddr : tcptw_sk(sk)->tw_rcv_saddr;
-}
-
-#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
-static inline struct in6_addr *__tcp_v6_rcv_saddr(const struct sock *sk)
-{
-	return likely(sk->sk_state != TCP_TIME_WAIT) ?
-		&inet6_sk(sk)->rcv_saddr : &tcptw_sk(sk)->tw_v6_rcv_saddr;
-}
-
-static inline struct in6_addr *tcp_v6_rcv_saddr(const struct sock *sk)
-{
-	return sk->sk_family == AF_INET6 ? __tcp_v6_rcv_saddr(sk) : NULL;
-}
-
-#define tcptw_sk_ipv6only(__sk)	(tcptw_sk(__sk)->tw_v6_ipv6only)
-
-static inline int tcp_v6_ipv6only(const struct sock *sk)
-{
-	return likely(sk->sk_state != TCP_TIME_WAIT) ?
-		ipv6_only_sock(sk) : tcptw_sk_ipv6only(sk);
-}
-#else
-# define __tcp_v6_rcv_saddr(__sk)	NULL
-# define tcp_v6_rcv_saddr(__sk)		NULL
-# define tcptw_sk_ipv6only(__sk)	0
-# define tcp_v6_ipv6only(__sk)		0
-#endif
-
-extern kmem_cache_t *tcp_timewait_cachep;
-
-static inline void tcp_tw_put(struct tcp_tw_bucket *tw)
-{
-	if (atomic_dec_and_test(&tw->tw_refcnt)) {
-#ifdef SOCK_REFCNT_DEBUG
-		printk(KERN_DEBUG "tw_bucket %p released\n", tw);
-#endif
-		kmem_cache_free(tcp_timewait_cachep, tw);
-	}
-}
-
 extern atomic_t tcp_orphan_count;
 extern int tcp_tw_count;
 extern void tcp_time_wait(struct sock *sk, int state, int timeo);
-extern void tcp_tw_deschedule(struct tcp_tw_bucket *tw);
-
-
-/* Socket demux engine toys. */
-#ifdef __BIG_ENDIAN
-#define TCP_COMBINED_PORTS(__sport, __dport) \
-	(((__u32)(__sport)<<16) | (__u32)(__dport))
-#else /* __LITTLE_ENDIAN */
-#define TCP_COMBINED_PORTS(__sport, __dport) \
-	(((__u32)(__dport)<<16) | (__u32)(__sport))
-#endif
-
-#if (BITS_PER_LONG == 64)
-#ifdef __BIG_ENDIAN
-#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr) \
-	__u64 __name = (((__u64)(__saddr))<<32)|((__u64)(__daddr));
-#else /* __LITTLE_ENDIAN */
-#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr) \
-	__u64 __name = (((__u64)(__daddr))<<32)|((__u64)(__saddr));
-#endif /* __BIG_ENDIAN */
-#define TCP_IPV4_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
-	(((*((__u64 *)&(inet_sk(__sk)->daddr)))== (__cookie))	&&	\
-	 ((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))	&&	\
-	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
-#define TCP_IPV4_TW_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
-	(((*((__u64 *)&(tcptw_sk(__sk)->tw_daddr))) == (__cookie)) &&	\
-	 ((*((__u32 *)&(tcptw_sk(__sk)->tw_dport))) == (__ports)) &&	\
-	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
-#else /* 32-bit arch */
-#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr)
-#define TCP_IPV4_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
-	((inet_sk(__sk)->daddr			== (__saddr))	&&	\
-	 (inet_sk(__sk)->rcv_saddr		== (__daddr))	&&	\
-	 ((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))	&&	\
-	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
-#define TCP_IPV4_TW_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
-	((tcptw_sk(__sk)->tw_daddr		== (__saddr))	&&	\
-	 (tcptw_sk(__sk)->tw_rcv_saddr		== (__daddr))	&&	\
-	 ((*((__u32 *)&(tcptw_sk(__sk)->tw_dport))) == (__ports)) &&	\
-	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
-#endif /* 64-bit arch */
-
-#define TCP_IPV6_MATCH(__sk, __saddr, __daddr, __ports, __dif)	   \
-	(((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))   	&& \
-	 ((__sk)->sk_family		== AF_INET6)		&& \
-	 ipv6_addr_equal(&inet6_sk(__sk)->daddr, (__saddr))	&& \
-	 ipv6_addr_equal(&inet6_sk(__sk)->rcv_saddr, (__daddr))	&& \
-	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+extern void tcp_tw_deschedule(struct inet_timewait_sock *tw);
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 
@@ -543,7 +350,7 @@ extern int			tcp_v4_rcv(struct sk_buff *skb);
 
 extern int			tcp_v4_remember_stamp(struct sock *sk);
 
-extern int		    	tcp_v4_tw_remember_stamp(struct tcp_tw_bucket *tw);
+extern int		    	tcp_v4_tw_remember_stamp(struct inet_timewait_sock *tw);
 
 extern int			tcp_sendmsg(struct kiocb *iocb, struct sock *sk,
 					    struct msghdr *msg, size_t size);
@@ -616,10 +423,9 @@ enum tcp_tw_status
 };
 
 
-extern enum tcp_tw_status	tcp_timewait_state_process(struct tcp_tw_bucket *tw,
+extern enum tcp_tw_status	tcp_timewait_state_process(struct inet_timewait_sock *tw,
 							   struct sk_buff *skb,
-							   struct tcphdr *th,
-							   unsigned len);
+							   const struct tcphdr *th);
 
 extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
 					      struct request_sock *req,

commit c752f0739f09b803aed191c4765a3b6650a08653
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:08:28 2005 -0700

    [TCP]: Move the tcp sock states to net/tcp_states.h
    
    Lots of places just needs the states, not even linux/tcp.h, where this
    enum was, needs it.
    
    This speeds up development of the refactorings as less sources are
    rebuilt when things get moved from net/tcp.h.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index bc110cc7022b..9d026d81d8c8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -36,6 +36,8 @@
 #include <net/sock.h>
 #include <net/snmp.h>
 #include <net/ip.h>
+#include <net/tcp_states.h>
+
 #if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
 #include <linux/ipv6.h>
 #endif

commit f3f05f7046e7c85b04af390d95a82a27160dd5d0
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:08:09 2005 -0700

    [INET]: Generalise the tcp_listen_ lock routines
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 99e47695d4b6..bc110cc7022b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1447,27 +1447,6 @@ static __inline__ void tcp_openreq_init(struct request_sock *req,
 
 extern void tcp_enter_memory_pressure(void);
 
-extern void tcp_listen_wlock(void);
-
-/* - We may sleep inside this lock.
- * - If sleeping is not required (or called from BH),
- *   use plain read_(un)lock(&inet_hashinfo.lhash_lock).
- */
-
-static inline void tcp_listen_lock(void)
-{
-	/* read_lock synchronizes to candidates to writers */
-	read_lock(&tcp_hashinfo.lhash_lock);
-	atomic_inc(&tcp_hashinfo.lhash_users);
-	read_unlock(&tcp_hashinfo.lhash_lock);
-}
-
-static inline void tcp_listen_unlock(void)
-{
-	if (atomic_dec_and_test(&tcp_hashinfo.lhash_users))
-		wake_up(&tcp_hashinfo.lhash_wait);
-}
-
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)
 {
 	return tp->keepalive_intvl ? : sysctl_tcp_keepalive_intvl;

commit 6e04e02165a7209a71db553b7bc48d68421e5ebf
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:07:35 2005 -0700

    [INET]: Move tcp_port_rover to inet_hashinfo
    
    Also expose all of the tcp_hashinfo members, i.e. killing those
    tcp_ehash, etc macros, this will more clearly expose already generic
    functions and some that need just a bit of work to become generic, as
    we'll see in the upcoming changesets.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9eb8ff7c911e..99e47695d4b6 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -41,19 +41,7 @@
 #endif
 #include <linux/seq_file.h>
 
-extern struct inet_hashinfo	tcp_hashinfo;
-#define tcp_ehash		(tcp_hashinfo.ehash)
-#define tcp_bhash		(tcp_hashinfo.bhash)
-#define tcp_ehash_size		(tcp_hashinfo.ehash_size)
-#define tcp_bhash_size		(tcp_hashinfo.bhash_size)
-#define tcp_listening_hash	(tcp_hashinfo.listening_hash)
-#define tcp_lhash_lock		(tcp_hashinfo.lhash_lock)
-#define tcp_lhash_users		(tcp_hashinfo.lhash_users)
-#define tcp_lhash_wait		(tcp_hashinfo.lhash_wait)
-#define tcp_portalloc_lock	(tcp_hashinfo.portalloc_lock)
-#define tcp_bucket_cachep	(tcp_hashinfo.bind_bucket_cachep)
-
-extern int tcp_port_rover;
+extern struct inet_hashinfo tcp_hashinfo;
 
 #if (BITS_PER_LONG == 64)
 #define TCP_ADDRCMP_ALIGN_BYTES 8
@@ -1463,21 +1451,21 @@ extern void tcp_listen_wlock(void);
 
 /* - We may sleep inside this lock.
  * - If sleeping is not required (or called from BH),
- *   use plain read_(un)lock(&tcp_lhash_lock).
+ *   use plain read_(un)lock(&inet_hashinfo.lhash_lock).
  */
 
 static inline void tcp_listen_lock(void)
 {
 	/* read_lock synchronizes to candidates to writers */
-	read_lock(&tcp_lhash_lock);
-	atomic_inc(&tcp_lhash_users);
-	read_unlock(&tcp_lhash_lock);
+	read_lock(&tcp_hashinfo.lhash_lock);
+	atomic_inc(&tcp_hashinfo.lhash_users);
+	read_unlock(&tcp_hashinfo.lhash_lock);
 }
 
 static inline void tcp_listen_unlock(void)
 {
-	if (atomic_dec_and_test(&tcp_lhash_users))
-		wake_up(&tcp_lhash_wait);
+	if (atomic_dec_and_test(&tcp_hashinfo.lhash_users))
+		wake_up(&tcp_hashinfo.lhash_wait);
 }
 
 static inline int keepalive_intvl_when(const struct tcp_sock *tp)

commit 2d8c4ce51903636ce0f60addc8134aa50ab8fa76
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:07:13 2005 -0700

    [INET]: Generalise tcp_bind_hash & tcp_inherit_port
    
    This required moving tcp_bucket_cachep to inet_hashinfo.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6c6c879e7e87..9eb8ff7c911e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -51,14 +51,10 @@ extern struct inet_hashinfo	tcp_hashinfo;
 #define tcp_lhash_users		(tcp_hashinfo.lhash_users)
 #define tcp_lhash_wait		(tcp_hashinfo.lhash_wait)
 #define tcp_portalloc_lock	(tcp_hashinfo.portalloc_lock)
-
-extern kmem_cache_t *tcp_bucket_cachep;
+#define tcp_bucket_cachep	(tcp_hashinfo.bind_bucket_cachep)
 
 extern int tcp_port_rover;
 
-extern void tcp_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,
-			  unsigned short snum);
-
 #if (BITS_PER_LONG == 64)
 #define TCP_ADDRCMP_ALIGN_BYTES 8
 #else
@@ -549,9 +545,6 @@ DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
 #define TCP_ADD_STATS_BH(field, val)	SNMP_ADD_STATS_BH(tcp_statistics, field, val)
 #define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
 
-extern void			tcp_put_port(struct sock *sk);
-extern void			tcp_inherit_port(struct sock *sk, struct sock *child);
-
 extern void			tcp_v4_err(struct sk_buff *skb, u32);
 
 extern void			tcp_shutdown (struct sock *sk, int how);
@@ -1268,7 +1261,7 @@ static __inline__ void tcp_set_state(struct sock *sk, int state)
 		sk->sk_prot->unhash(sk);
 		if (inet_sk(sk)->bind_hash &&
 		    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
-			tcp_put_port(sk);
+			inet_put_port(&tcp_hashinfo, sk);
 		/* fall through */
 	default:
 		if (oldstate==TCP_ESTABLISHED)

commit a55ebcc4c4532107ad9eee1c9bb698ab5f12c00f
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:01:14 2005 -0700

    [INET]: Move bind_hash from tcp_sk to inet_sk
    
    This should really be in a inet_connection_sock, but I'm leaving it
    for a later optimization, when some more fields common to INET
    transport protocols now in tcp_sk or inet_sk will be chunked out into
    inet_connection_sock, for now its better to concentrate on getting the
    changes in the core merged to leave the DCCP tree with only DCCP
    specific code.
    
    Next changesets will take advantage of this move to generalise things
    like tcp_bind_hash, tcp_put_port, tcp_inherit_port, making the later
    receive a inet_hashinfo parameter, and even __tcp_tw_hashdance, etc in
    the future, when tcp_tw_bucket gets transformed into the struct
    timewait_sock hierarchy.
    
    tcp_destroy_sock also is eligible as soon as tcp_orphan_count gets
    moved to sk_prot.
    
    A cascade of incremental changes will ultimately make the tcp_lookup
    functions be fully generic.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ff5d30ac2b06..6c6c879e7e87 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1266,7 +1266,7 @@ static __inline__ void tcp_set_state(struct sock *sk, int state)
 			TCP_INC_STATS(TCP_MIB_ESTABRESETS);
 
 		sk->sk_prot->unhash(sk);
-		if (tcp_sk(sk)->bind_hash &&
+		if (inet_sk(sk)->bind_hash &&
 		    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
 			tcp_put_port(sk);
 		/* fall through */

commit 77d8bf9c6208eb535f05718168ffcc476be0ca8c
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:00:51 2005 -0700

    [INET]: Move the TCP hashtable functions/structs to inet_hashtables.[ch]
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6c9f6f7cab5c..ff5d30ac2b06 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -30,6 +30,7 @@
 #include <linux/slab.h>
 #include <linux/cache.h>
 #include <linux/percpu.h>
+#include <net/inet_hashtables.h>
 #include <net/checksum.h>
 #include <net/request_sock.h>
 #include <net/sock.h>
@@ -40,101 +41,6 @@
 #endif
 #include <linux/seq_file.h>
 
-/* This is for all connections with a full identity, no wildcards.
- * New scheme, half the table is for TIME_WAIT, the other half is
- * for the rest.  I'll experiment with dynamic table growth later.
- */
-struct inet_ehash_bucket {
-	rwlock_t	  lock;
-	struct hlist_head chain;
-} __attribute__((__aligned__(8)));
-
-/* This is for listening sockets, thus all sockets which possess wildcards. */
-#define INET_LHTABLE_SIZE	32	/* Yes, really, this is all you need. */
-
-/* There are a few simple rules, which allow for local port reuse by
- * an application.  In essence:
- *
- *	1) Sockets bound to different interfaces may share a local port.
- *	   Failing that, goto test 2.
- *	2) If all sockets have sk->sk_reuse set, and none of them are in
- *	   TCP_LISTEN state, the port may be shared.
- *	   Failing that, goto test 3.
- *	3) If all sockets are bound to a specific inet_sk(sk)->rcv_saddr local
- *	   address, and none of them are the same, the port may be
- *	   shared.
- *	   Failing this, the port cannot be shared.
- *
- * The interesting point, is test #2.  This is what an FTP server does
- * all day.  To optimize this case we use a specific flag bit defined
- * below.  As we add sockets to a bind bucket list, we perform a
- * check of: (newsk->sk_reuse && (newsk->sk_state != TCP_LISTEN))
- * As long as all sockets added to a bind bucket pass this test,
- * the flag bit will be set.
- * The resulting situation is that tcp_v[46]_verify_bind() can just check
- * for this flag bit, if it is set and the socket trying to bind has
- * sk->sk_reuse set, we don't even have to walk the owners list at all,
- * we return that it is ok to bind this socket to the requested local port.
- *
- * Sounds like a lot of work, but it is worth it.  In a more naive
- * implementation (ie. current FreeBSD etc.) the entire list of ports
- * must be walked for each data port opened by an ftp server.  Needless
- * to say, this does not scale at all.  With a couple thousand FTP
- * users logged onto your box, isn't it nice to know that new data
- * ports are created in O(1) time?  I thought so. ;-)	-DaveM
- */
-struct inet_bind_bucket {
-	unsigned short		port;
-	signed short		fastreuse;
-	struct hlist_node	node;
-	struct hlist_head	owners;
-};
-
-#define inet_bind_bucket_for_each(tb, node, head) \
-	hlist_for_each_entry(tb, node, head, node)
-
-struct inet_bind_hashbucket {
-	spinlock_t		lock;
-	struct hlist_head	chain;
-};
-
-struct inet_hashinfo {
-	/* This is for sockets with full identity only.  Sockets here will
-	 * always be without wildcards and will have the following invariant:
-	 *
-	 *          TCP_ESTABLISHED <= sk->sk_state < TCP_CLOSE
-	 *
-	 * First half of the table is for sockets not in TIME_WAIT, second half
-	 * is for TIME_WAIT sockets only.
-	 */
-	struct inet_ehash_bucket	*ehash;
-
-	/* Ok, let's try this, I give up, we do need a local binding
-	 * TCP hash as well as the others for fast bind/connect.
-	 */
-	struct inet_bind_hashbucket	*bhash;
-
-	int				bhash_size;
-	int				ehash_size;
-
-	/* All sockets in TCP_LISTEN state will be in here.  This is the only
-	 * table where wildcard'd TCP sockets can exist.  Hash function here
-	 * is just local port number.
-	 */
-	struct hlist_head		listening_hash[INET_LHTABLE_SIZE];
-
-	/* All the above members are written once at bootup and
-	 * never written again _or_ are predominantly read-access.
-	 *
-	 * Now align to a new cache line as all the following members
-	 * are often dirty.
-	 */
-	rwlock_t			lhash_lock ____cacheline_aligned;
-	atomic_t			lhash_users;
-	wait_queue_head_t		lhash_wait;
-	spinlock_t			portalloc_lock;
-};
-
 extern struct inet_hashinfo	tcp_hashinfo;
 #define tcp_ehash		(tcp_hashinfo.ehash)
 #define tcp_bhash		(tcp_hashinfo.bhash)
@@ -147,19 +53,8 @@ extern struct inet_hashinfo	tcp_hashinfo;
 #define tcp_portalloc_lock	(tcp_hashinfo.portalloc_lock)
 
 extern kmem_cache_t *tcp_bucket_cachep;
-extern struct inet_bind_bucket *
-	    inet_bind_bucket_create(kmem_cache_t *cachep,
-				    struct inet_bind_hashbucket *head,
-				    const unsigned short snum);
-extern void inet_bind_bucket_destroy(kmem_cache_t *cachep,
-				     struct inet_bind_bucket *tb);
-extern int tcp_port_rover;
 
-/* These are AF independent. */
-static inline int inet_bhashfn(const __u16 lport, const int bhash_size)
-{
-	return lport & (bhash_size - 1);
-}
+extern int tcp_port_rover;
 
 extern void tcp_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,
 			  unsigned short snum);
@@ -359,17 +254,6 @@ extern void tcp_tw_deschedule(struct tcp_tw_bucket *tw);
 	 ipv6_addr_equal(&inet6_sk(__sk)->rcv_saddr, (__daddr))	&& \
 	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
 
-/* These can have wildcards, don't try too hard. */
-static inline int inet_lhashfn(const unsigned short num)
-{
-	return num & (INET_LHTABLE_SIZE - 1);
-}
-
-static inline int inet_sk_listen_hashfn(const struct sock *sk)
-{
-	return inet_lhashfn(inet_sk(sk)->num);
-}
-
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
 
 /* 

commit 0f7ff9274e72fd254fbd1ab117bbc1db6e7cdb34
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:59:44 2005 -0700

    [INET]: Just rename the TCP hashtable functions/structs to inet_
    
    This is to break down the complexity of the series of patches,
    making it very clear that this one just does:
    
    1. renames tcp_ prefixed hashtable functions and data structures that
       were already mostly generic to inet_ to share it with DCCP and
       other INET transport protocols.
    
    2. Removes not used functions (__tb_head & tb_head)
    
    3. Removes some leftover prototypes in the headers (tcp_bucket_unlock &
       tcp_v4_build_header)
    
    Next changesets will move tcp_sk(sk)->bind_hash to inet_sock so that we can
    make functions such as tcp_inherit_port, __tcp_inherit_port, tcp_v4_get_port,
    __tcp_put_port,  generic and get others like tcp_destroy_sock closer to generic
    (tcp_orphan_count will go to sk->sk_prot to allow this).
    
    Eventually most of these functions will be used passing the transport protocol
    inet_hashinfo structure.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 0c769adb0463..6c9f6f7cab5c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -44,13 +44,13 @@
  * New scheme, half the table is for TIME_WAIT, the other half is
  * for the rest.  I'll experiment with dynamic table growth later.
  */
-struct tcp_ehash_bucket {
+struct inet_ehash_bucket {
 	rwlock_t	  lock;
 	struct hlist_head chain;
 } __attribute__((__aligned__(8)));
 
 /* This is for listening sockets, thus all sockets which possess wildcards. */
-#define TCP_LHTABLE_SIZE	32	/* Yes, really, this is all you need. */
+#define INET_LHTABLE_SIZE	32	/* Yes, really, this is all you need. */
 
 /* There are a few simple rules, which allow for local port reuse by
  * an application.  In essence:
@@ -83,31 +83,22 @@ struct tcp_ehash_bucket {
  * users logged onto your box, isn't it nice to know that new data
  * ports are created in O(1) time?  I thought so. ;-)	-DaveM
  */
-struct tcp_bind_bucket {
+struct inet_bind_bucket {
 	unsigned short		port;
 	signed short		fastreuse;
 	struct hlist_node	node;
 	struct hlist_head	owners;
 };
 
-#define tb_for_each(tb, node, head) hlist_for_each_entry(tb, node, head, node)
+#define inet_bind_bucket_for_each(tb, node, head) \
+	hlist_for_each_entry(tb, node, head, node)
 
-struct tcp_bind_hashbucket {
+struct inet_bind_hashbucket {
 	spinlock_t		lock;
 	struct hlist_head	chain;
 };
 
-static inline struct tcp_bind_bucket *__tb_head(struct tcp_bind_hashbucket *head)
-{
-	return hlist_entry(head->chain.first, struct tcp_bind_bucket, node);
-}
-
-static inline struct tcp_bind_bucket *tb_head(struct tcp_bind_hashbucket *head)
-{
-	return hlist_empty(&head->chain) ? NULL : __tb_head(head);
-}
-
-extern struct tcp_hashinfo {
+struct inet_hashinfo {
 	/* This is for sockets with full identity only.  Sockets here will
 	 * always be without wildcards and will have the following invariant:
 	 *
@@ -116,21 +107,21 @@ extern struct tcp_hashinfo {
 	 * First half of the table is for sockets not in TIME_WAIT, second half
 	 * is for TIME_WAIT sockets only.
 	 */
-	struct tcp_ehash_bucket *__tcp_ehash;
+	struct inet_ehash_bucket	*ehash;
 
 	/* Ok, let's try this, I give up, we do need a local binding
 	 * TCP hash as well as the others for fast bind/connect.
 	 */
-	struct tcp_bind_hashbucket *__tcp_bhash;
+	struct inet_bind_hashbucket	*bhash;
 
-	int __tcp_bhash_size;
-	int __tcp_ehash_size;
+	int				bhash_size;
+	int				ehash_size;
 
 	/* All sockets in TCP_LISTEN state will be in here.  This is the only
 	 * table where wildcard'd TCP sockets can exist.  Hash function here
 	 * is just local port number.
 	 */
-	struct hlist_head __tcp_listening_hash[TCP_LHTABLE_SIZE];
+	struct hlist_head		listening_hash[INET_LHTABLE_SIZE];
 
 	/* All the above members are written once at bootup and
 	 * never written again _or_ are predominantly read-access.
@@ -138,36 +129,39 @@ extern struct tcp_hashinfo {
 	 * Now align to a new cache line as all the following members
 	 * are often dirty.
 	 */
-	rwlock_t __tcp_lhash_lock ____cacheline_aligned;
-	atomic_t __tcp_lhash_users;
-	wait_queue_head_t __tcp_lhash_wait;
-	spinlock_t __tcp_portalloc_lock;
-} tcp_hashinfo;
-
-#define tcp_ehash	(tcp_hashinfo.__tcp_ehash)
-#define tcp_bhash	(tcp_hashinfo.__tcp_bhash)
-#define tcp_ehash_size	(tcp_hashinfo.__tcp_ehash_size)
-#define tcp_bhash_size	(tcp_hashinfo.__tcp_bhash_size)
-#define tcp_listening_hash (tcp_hashinfo.__tcp_listening_hash)
-#define tcp_lhash_lock	(tcp_hashinfo.__tcp_lhash_lock)
-#define tcp_lhash_users	(tcp_hashinfo.__tcp_lhash_users)
-#define tcp_lhash_wait	(tcp_hashinfo.__tcp_lhash_wait)
-#define tcp_portalloc_lock (tcp_hashinfo.__tcp_portalloc_lock)
+	rwlock_t			lhash_lock ____cacheline_aligned;
+	atomic_t			lhash_users;
+	wait_queue_head_t		lhash_wait;
+	spinlock_t			portalloc_lock;
+};
+
+extern struct inet_hashinfo	tcp_hashinfo;
+#define tcp_ehash		(tcp_hashinfo.ehash)
+#define tcp_bhash		(tcp_hashinfo.bhash)
+#define tcp_ehash_size		(tcp_hashinfo.ehash_size)
+#define tcp_bhash_size		(tcp_hashinfo.bhash_size)
+#define tcp_listening_hash	(tcp_hashinfo.listening_hash)
+#define tcp_lhash_lock		(tcp_hashinfo.lhash_lock)
+#define tcp_lhash_users		(tcp_hashinfo.lhash_users)
+#define tcp_lhash_wait		(tcp_hashinfo.lhash_wait)
+#define tcp_portalloc_lock	(tcp_hashinfo.portalloc_lock)
 
 extern kmem_cache_t *tcp_bucket_cachep;
-extern struct tcp_bind_bucket *tcp_bucket_create(struct tcp_bind_hashbucket *head,
-						 unsigned short snum);
-extern void tcp_bucket_destroy(struct tcp_bind_bucket *tb);
-extern void tcp_bucket_unlock(struct sock *sk);
+extern struct inet_bind_bucket *
+	    inet_bind_bucket_create(kmem_cache_t *cachep,
+				    struct inet_bind_hashbucket *head,
+				    const unsigned short snum);
+extern void inet_bind_bucket_destroy(kmem_cache_t *cachep,
+				     struct inet_bind_bucket *tb);
 extern int tcp_port_rover;
 
 /* These are AF independent. */
-static __inline__ int tcp_bhashfn(__u16 lport)
+static inline int inet_bhashfn(const __u16 lport, const int bhash_size)
 {
-	return (lport & (tcp_bhash_size - 1));
+	return lport & (bhash_size - 1);
 }
 
-extern void tcp_bind_hash(struct sock *sk, struct tcp_bind_bucket *tb,
+extern void tcp_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,
 			  unsigned short snum);
 
 #if (BITS_PER_LONG == 64)
@@ -212,7 +206,7 @@ struct tcp_tw_bucket {
 	__u32			tw_ts_recent;
 	long			tw_ts_recent_stamp;
 	unsigned long		tw_ttd;
-	struct tcp_bind_bucket	*tw_tb;
+	struct inet_bind_bucket	*tw_tb;
 	struct hlist_node	tw_death_node;
 #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
 	struct in6_addr		tw_v6_daddr;
@@ -366,14 +360,14 @@ extern void tcp_tw_deschedule(struct tcp_tw_bucket *tw);
 	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
 
 /* These can have wildcards, don't try too hard. */
-static __inline__ int tcp_lhashfn(unsigned short num)
+static inline int inet_lhashfn(const unsigned short num)
 {
-	return num & (TCP_LHTABLE_SIZE - 1);
+	return num & (INET_LHTABLE_SIZE - 1);
 }
 
-static __inline__ int tcp_sk_listen_hashfn(struct sock *sk)
+static inline int inet_sk_listen_hashfn(const struct sock *sk)
 {
-	return tcp_lhashfn(inet_sk(sk)->num);
+	return inet_lhashfn(inet_sk(sk)->num);
 }
 
 #define MAX_TCP_HEADER	(128 + MAX_HEADER)
@@ -799,9 +793,6 @@ extern void			tcp_parse_options(struct sk_buff *skb,
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern int		       	tcp_v4_build_header(struct sock *sk, 
-						    struct sk_buff *skb);
-
 extern void		       	tcp_v4_send_check(struct sock *sk, 
 						  struct tcphdr *th, int len, 
 						  struct sk_buff *skb);

commit 32519f11d38ea8f4f60896763bacec7db1760f9c
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:50:02 2005 -0700

    [INET]: Introduce inet_sk_rebuild_header
    
    From tcp_v4_rebuild_header, that already was pretty generic, I only
    needed to use sk->sk_protocol instead of the hardcoded IPPROTO_TCP and
    establish the requirement that INET transport layer protocols that
    want to use this function map TCP_SYN_SENT to its equivalent state.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d95661a3aeeb..0c769adb0463 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -799,8 +799,6 @@ extern void			tcp_parse_options(struct sk_buff *skb,
  *	TCP v4 functions exported for the inet6 API
  */
 
-extern int		       	tcp_v4_rebuild_header(struct sock *sk);
-
 extern int		       	tcp_v4_build_header(struct sock *sk, 
 						    struct sk_buff *skb);
 

commit 6cbb0df788b90777a7ed0f9d8261260353f48076
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:49:02 2005 -0700

    [SOCK]: Introduce sk_setup_caps
    
    From tcp_v4_setup_caps, that always is preceded by a call to
    __sk_dst_set, so coalesce this sequence into sk_setup_caps, removing
    one call to a TCP function in the IP layer.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 31984733777b..d95661a3aeeb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1658,15 +1658,6 @@ static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int
 	return 1;
 }
 
-static inline void tcp_v4_setup_caps(struct sock *sk, struct dst_entry *dst)
-{
-	sk->sk_route_caps = dst->dev->features;
-	if (sk->sk_route_caps & NETIF_F_TSO) {
-		if (sock_flag(sk, SOCK_NO_LARGESEND) || dst->header_len)
-			sk->sk_route_caps &= ~NETIF_F_TSO;
-	}
-}
-
 #define TCP_CHECK_TIMER(sk) do { } while (0)
 
 static inline int tcp_use_frto(const struct sock *sk)

commit e6848976b721eeb5551cd94673faafeef78d9f35
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:45:38 2005 -0700

    [NET]: Cleanup INET_REFCNT_DEBUG code
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 5010f0c5a56e..31984733777b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -306,7 +306,7 @@ extern kmem_cache_t *tcp_timewait_cachep;
 static inline void tcp_tw_put(struct tcp_tw_bucket *tw)
 {
 	if (atomic_dec_and_test(&tw->tw_refcnt)) {
-#ifdef INET_REFCNT_DEBUG
+#ifdef SOCK_REFCNT_DEBUG
 		printk(KERN_DEBUG "tw_bucket %p released\n", tw);
 #endif
 		kmem_cache_free(tcp_timewait_cachep, tw);

commit d1b04c081e3fb0a08ac108737e4efa9f4830c916
Author: Baruch Even <baruch@ev-en.org>
Date:   Sat Jul 30 17:41:59 2005 -0700

    [NET]: Spelling mistakes threshoulds -> thresholds
    
    Just simple spelling mistake fixes.
    
    Signed-Off-By: Baruch Even <baruch@ev-en.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f4f9aba07ac2..5010f0c5a56e 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1236,7 +1236,7 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 
-/* Set slow start threshould and cwnd not falling to slow start */
+/* Set slow start threshold and cwnd not falling to slow start */
 static inline void __tcp_enter_cwr(struct tcp_sock *tp)
 {
 	tp->undo_marker = 0;

commit 86a76caf8705e3524e15f343f3c4806939a06dc8
Author: Victor Fusco <victor@cetuc.puc-rio.br>
Date:   Fri Jul 8 14:57:47 2005 -0700

    [NET]: Fix sparse warnings
    
    From: Victor Fusco <victor@cetuc.puc-rio.br>
    
    Fix the sparse warning "implicit cast to nocast type"
    
    Signed-off-by: Victor Fusco <victor@cetuc.puc-rio.br>
    Signed-off-by: Domen Puncer <domen@coderock.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4d5b12e4dc11..f4f9aba07ac2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -860,7 +860,8 @@ extern void tcp_send_probe0(struct sock *);
 extern void tcp_send_partial(struct sock *);
 extern int  tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
-extern void tcp_send_active_reset(struct sock *sk, int priority);
+extern void tcp_send_active_reset(struct sock *sk,
+                                  unsigned int __nocast priority);
 extern int  tcp_send_synack(struct sock *);
 extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);

commit b03efcfb2180289718991bb984044ce6c5b7d1b0
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jul 8 14:57:23 2005 -0700

    [NET]: Transform skb_queue_len() binary tests into skb_queue_empty()
    
    This is part of the grand scheme to eliminate the qlen
    member of skb_queue_head, and subsequently remove the
    'list' member of sk_buff.
    
    Most users of skb_queue_len() want to know if the queue is
    empty or not, and that's trivially done with skb_queue_empty()
    which doesn't use the skb_queue_head->qlen member and instead
    uses the queue list emptyness as the test.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a166918ca56d..4d5b12e4dc11 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -991,7 +991,7 @@ static __inline__ void tcp_fast_path_on(struct tcp_sock *tp)
 
 static inline void tcp_fast_path_check(struct sock *sk, struct tcp_sock *tp)
 {
-	if (skb_queue_len(&tp->out_of_order_queue) == 0 &&
+	if (skb_queue_empty(&tp->out_of_order_queue) &&
 	    tp->rcv_wnd &&
 	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
 	    !tp->urg_data)

commit c1b4a7e69576d65efc31a8cea0714173c2841244
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:24:38 2005 -0700

    [TCP]: Move to new TSO segmenting scheme.
    
    Make TSO segment transmit size decisions at send time not earlier.
    
    The basic scheme is that we try to build as large a TSO frame as
    possible when pulling in the user data, but the size of the TSO frame
    output to the card is determined at transmit time.
    
    This is guided by tp->xmit_size_goal.  It is always set to a multiple
    of MSS and tells sendmsg/sendpage how large an SKB to try and build.
    
    Later, tcp_write_xmit() and tcp_push_one() chop up the packet if
    necessary and conditions warrant.  These routines can also decide to
    "defer" in order to wait for more ACKs to arrive and thus allow larger
    TSO frames to be emitted.
    
    A general observation is that TSO elongates the pipe, thus requiring a
    larger congestion window and larger buffering especially at the sender
    side.  Therefore, it is important that applications 1) get a large
    enough socket send buffer (this is accomplished by our dynamic send
    buffer expansion code) 2) do large enough writes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index b19238027da8..a166918ca56d 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -862,7 +862,7 @@ extern int  tcp_write_wakeup(struct sock *);
 extern void tcp_send_fin(struct sock *sk);
 extern void tcp_send_active_reset(struct sock *sk, int priority);
 extern int  tcp_send_synack(struct sock *);
-extern void tcp_push_one(struct sock *, unsigned mss_now);
+extern void tcp_push_one(struct sock *, unsigned int mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);
 
@@ -968,7 +968,7 @@ static inline void tcp_reset_xmit_timer(struct sock *sk, int what, unsigned long
 static inline void tcp_initialize_rcv_mss(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
-	unsigned int hint = min(tp->advmss, tp->mss_cache_std);
+	unsigned int hint = min_t(unsigned int, tp->advmss, tp->mss_cache);
 
 	hint = min(hint, tp->rcv_wnd/2);
 	hint = min(hint, TCP_MIN_RCVMSS);

commit 55c97f3e990c1ff63957c64f6cb10711a09fd70e
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:19:38 2005 -0700

    [TCP]: Fix __tcp_push_pending_frames() 'nonagle' handling.
    
    'nonagle' should be passed to the tcp_snd_test() function
    as 'TCP_NAGLE_PUSH' if we are checking an SKB not at the
    tail of the write_queue.  This is because Nagle does not
    apply to such frames since we cannot possibly tack more
    data onto them.
    
    However, while doing this __tcp_push_pending_frames() makes
    all of the packets in the write_queue use this modified
    'nonagle' value.
    
    Fix the bug and simplify this function by just calling
    tcp_write_xmit() directly if sk_send_head is non-NULL.
    
    As a result, we can now make tcp_data_snd_check() just call
    tcp_push_pending_frames() instead of the specialized
    __tcp_data_snd_check().
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9416236cc395..b19238027da8 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -848,7 +848,6 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 
 /* tcp_output.c */
 
-extern void __tcp_data_snd_check(struct sock *sk, struct sk_buff *skb);
 extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
 				      unsigned int cur_mss, int nonagle);
 extern int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp);

commit a2e2a59c93cc8ba39caa9011c2573f429e40ccd9
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:19:23 2005 -0700

    [TCP]: Fix redundant calculations of tcp_current_mss()
    
    tcp_write_xmit() uses tcp_current_mss(), but some of it's callers,
    namely __tcp_push_pending_frames(), already has this value available
    already.
    
    While we're here, fix the "cur_mss" argument to be "unsigned int"
    instead of plain "unsigned".
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f32e7aed2c75..9416236cc395 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -850,7 +850,7 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 
 extern void __tcp_data_snd_check(struct sock *sk, struct sk_buff *skb);
 extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
-				      unsigned cur_mss, int nonagle);
+				      unsigned int cur_mss, int nonagle);
 extern int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_xmit_retransmit_queue(struct sock *);

commit a762a9800752f05fa8768bb0ac35d0e7f1bcfe7f
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:18:51 2005 -0700

    [TCP]: Kill extra cwnd validate in __tcp_push_pending_frames().
    
    The tcp_cwnd_validate() function should only be invoked
    if we actually send some frames, yet __tcp_push_pending_frames()
    will always invoke it.  tcp_write_xmit() does the call for us,
    so the call here can simply be removed.
    
    Also, tcp_write_xmit() can be marked static.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 4888f9d3f56b..f32e7aed2c75 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -848,7 +848,6 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 
 /* tcp_output.c */
 
-extern int tcp_write_xmit(struct sock *, int nonagle);
 extern void __tcp_data_snd_check(struct sock *sk, struct sk_buff *skb);
 extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
 				      unsigned cur_mss, int nonagle);
@@ -868,6 +867,9 @@ extern void tcp_push_one(struct sock *, unsigned mss_now);
 extern void tcp_send_ack(struct sock *sk);
 extern void tcp_send_delayed_ack(struct sock *sk);
 
+/* tcp_input.c */
+extern void tcp_cwnd_application_limited(struct sock *sk);
+
 /* tcp_timer.c */
 extern void tcp_init_xmit_timers(struct sock *);
 extern void tcp_clear_xmit_timers(struct sock *);
@@ -1234,28 +1236,6 @@ static inline void tcp_sync_left_out(struct tcp_sock *tp)
 	tp->left_out = tp->sacked_out + tp->lost_out;
 }
 
-extern void tcp_cwnd_application_limited(struct sock *sk);
-
-/* Congestion window validation. (RFC2861) */
-
-static inline void tcp_cwnd_validate(struct sock *sk, struct tcp_sock *tp)
-{
-	__u32 packets_out = tp->packets_out;
-
-	if (packets_out >= tp->snd_cwnd) {
-		/* Network is feed fully. */
-		tp->snd_cwnd_used = 0;
-		tp->snd_cwnd_stamp = tcp_time_stamp;
-	} else {
-		/* Network starves. */
-		if (tp->packets_out > tp->snd_cwnd_used)
-			tp->snd_cwnd_used = tp->packets_out;
-
-		if ((s32)(tcp_time_stamp - tp->snd_cwnd_stamp) >= tp->rto)
-			tcp_cwnd_application_limited(sk);
-	}
-}
-
 /* Set slow start threshould and cwnd not falling to slow start */
 static inline void __tcp_enter_cwr(struct tcp_sock *tp)
 {

commit 84d3e7b9573291a1ea845bdd51b74bb484597661
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:18:18 2005 -0700

    [TCP]: Move __tcp_data_snd_check into tcp_output.c
    
    It reimplements portions of tcp_snd_check(), so it
    we move it to tcp_output.c we can consolidate it's
    logic much easier in a later change.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f2b104532de1..4888f9d3f56b 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -849,6 +849,7 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 /* tcp_output.c */
 
 extern int tcp_write_xmit(struct sock *, int nonagle);
+extern void __tcp_data_snd_check(struct sock *sk, struct sk_buff *skb);
 extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
 				      unsigned cur_mss, int nonagle);
 extern int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp);

commit f6302d1d78f77c2d4c8bd32b0afc2df7fdf5f281
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:18:03 2005 -0700

    [TCP]: Move send test logic out of net/tcp.h
    
    This just moves the code into tcp_output.c, no code logic changes are
    made by this patch.
    
    Using this as a baseline, we can begin to untangle the mess of
    comparisons for the Nagle test et al.  We will also be able to reduce
    all of the redundant computation that occurs when outputting data
    packets.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index afe41c5de2f2..f2b104532de1 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -849,6 +849,9 @@ extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb,
 /* tcp_output.c */
 
 extern int tcp_write_xmit(struct sock *, int nonagle);
+extern void __tcp_push_pending_frames(struct sock *sk, struct tcp_sock *tp,
+				      unsigned cur_mss, int nonagle);
+extern int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp);
 extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
 extern void tcp_xmit_retransmit_queue(struct sock *);
 extern void tcp_simple_retransmit(struct sock *);
@@ -1284,12 +1287,6 @@ static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
 	return 3;
 }
 
-static __inline__ int tcp_minshall_check(const struct tcp_sock *tp)
-{
-	return after(tp->snd_sml,tp->snd_una) &&
-		!after(tp->snd_sml, tp->snd_nxt);
-}
-
 static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss, 
 					   const struct sk_buff *skb)
 {
@@ -1297,122 +1294,18 @@ static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss,
 		tp->snd_sml = TCP_SKB_CB(skb)->end_seq;
 }
 
-/* Return 0, if packet can be sent now without violation Nagle's rules:
-   1. It is full sized.
-   2. Or it contains FIN.
-   3. Or TCP_NODELAY was set.
-   4. Or TCP_CORK is not set, and all sent packets are ACKed.
-      With Minshall's modification: all sent small packets are ACKed.
- */
-
-static __inline__ int
-tcp_nagle_check(const struct tcp_sock *tp, const struct sk_buff *skb, 
-		unsigned mss_now, int nonagle)
-{
-	return (skb->len < mss_now &&
-		!(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN) &&
-		((nonagle&TCP_NAGLE_CORK) ||
-		 (!nonagle &&
-		  tp->packets_out &&
-		  tcp_minshall_check(tp))));
-}
-
-extern void tcp_set_skb_tso_segs(struct sock *, struct sk_buff *);
-
-/* This checks if the data bearing packet SKB (usually sk->sk_send_head)
- * should be put on the wire right now.
- */
-static __inline__ int tcp_snd_test(struct sock *sk,
-				   struct sk_buff *skb,
-				   unsigned cur_mss, int nonagle)
-{
-	struct tcp_sock *tp = tcp_sk(sk);
-	int pkts = tcp_skb_pcount(skb);
-
-	if (!pkts) {
-		tcp_set_skb_tso_segs(sk, skb);
-		pkts = tcp_skb_pcount(skb);
-	}
-
-	/*	RFC 1122 - section 4.2.3.4
-	 *
-	 *	We must queue if
-	 *
-	 *	a) The right edge of this frame exceeds the window
-	 *	b) There are packets in flight and we have a small segment
-	 *	   [SWS avoidance and Nagle algorithm]
-	 *	   (part of SWS is done on packetization)
-	 *	   Minshall version sounds: there are no _small_
-	 *	   segments in flight. (tcp_nagle_check)
-	 *	c) We have too many packets 'in flight'
-	 *
-	 * 	Don't use the nagle rule for urgent data (or
-	 *	for the final FIN -DaveM).
-	 *
-	 *	Also, Nagle rule does not apply to frames, which
-	 *	sit in the middle of queue (they have no chances
-	 *	to get new data) and if room at tail of skb is
-	 *	not enough to save something seriously (<32 for now).
-	 */
-
-	/* Don't be strict about the congestion window for the
-	 * final FIN frame.  -DaveM
-	 */
-	return (((nonagle&TCP_NAGLE_PUSH) || tp->urg_mode
-		 || !tcp_nagle_check(tp, skb, cur_mss, nonagle)) &&
-		(((tcp_packets_in_flight(tp) + (pkts-1)) < tp->snd_cwnd) ||
-		 (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)) &&
-		!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una + tp->snd_wnd));
-}
-
 static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
 {
 	if (!tp->packets_out && !tp->pending)
 		tcp_reset_xmit_timer(sk, TCP_TIME_PROBE0, tp->rto);
 }
 
-static __inline__ int tcp_skb_is_last(const struct sock *sk, 
-				      const struct sk_buff *skb)
-{
-	return skb->next == (struct sk_buff *)&sk->sk_write_queue;
-}
-
-/* Push out any pending frames which were held back due to
- * TCP_CORK or attempt at coalescing tiny packets.
- * The socket must be locked by the caller.
- */
-static __inline__ void __tcp_push_pending_frames(struct sock *sk,
-						 struct tcp_sock *tp,
-						 unsigned cur_mss,
-						 int nonagle)
-{
-	struct sk_buff *skb = sk->sk_send_head;
-
-	if (skb) {
-		if (!tcp_skb_is_last(sk, skb))
-			nonagle = TCP_NAGLE_PUSH;
-		if (!tcp_snd_test(sk, skb, cur_mss, nonagle) ||
-		    tcp_write_xmit(sk, nonagle))
-			tcp_check_probe_timer(sk, tp);
-	}
-	tcp_cwnd_validate(sk, tp);
-}
-
 static __inline__ void tcp_push_pending_frames(struct sock *sk,
 					       struct tcp_sock *tp)
 {
 	__tcp_push_pending_frames(sk, tp, tcp_current_mss(sk, 1), tp->nonagle);
 }
 
-static __inline__ int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp)
-{
-	struct sk_buff *skb = sk->sk_send_head;
-
-	return (skb &&
-		tcp_snd_test(sk, skb, tcp_current_mss(sk, 1),
-			     tcp_skb_is_last(sk, skb) ? TCP_NAGLE_PUSH : tp->nonagle));
-}
-
 static __inline__ void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
 {
 	tp->snd_wl1 = seq;

commit fc6415bcb0f58f03adb910e56d7e1df6368794e0
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:17:45 2005 -0700

    [TCP]: Fix quick-ack decrementing with TSO.
    
    On each packet output, we call tcp_dec_quickack_mode()
    if the ACK flag is set.  It drops tp->ack.quick until
    it hits zero, at which time we deflate the ATO value.
    
    When doing TSO, we are emitting multiple packets with
    ACK set, so we should decrement tp->ack.quick that many
    segments.
    
    Note that, unlike this case, tcp_enter_cwr() should not
    take the tcp_skb_pcount(skb) into consideration.  That
    function, one time, readjusts tp->snd_cwnd and moves
    into TCP_CA_CWR state.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index ec9e20c27179..afe41c5de2f2 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -721,11 +721,16 @@ static inline int tcp_ack_scheduled(struct tcp_sock *tp)
 	return tp->ack.pending&TCP_ACK_SCHED;
 }
 
-static __inline__ void tcp_dec_quickack_mode(struct tcp_sock *tp)
+static __inline__ void tcp_dec_quickack_mode(struct tcp_sock *tp, unsigned int pkts)
 {
-	if (tp->ack.quick && --tp->ack.quick == 0) {
-		/* Leaving quickack mode we deflate ATO. */
-		tp->ack.ato = TCP_ATO_MIN;
+	if (tp->ack.quick) {
+		if (pkts >= tp->ack.quick) {
+			tp->ack.quick = 0;
+
+			/* Leaving quickack mode we deflate ATO. */
+			tp->ack.ato = TCP_ATO_MIN;
+		} else
+			tp->ack.quick -= pkts;
 	}
 }
 

commit a8acfbac75c2ffdd66fb5dfcdb7ab5aaced94fd8
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 23 23:45:02 2005 -0700

    [TCP]: Need to declare 'tcp_reno' in net/tcp.h
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d04b21188ccb..ec9e20c27179 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1169,6 +1169,7 @@ extern u32 tcp_reno_ssthresh(struct tcp_sock *tp);
 extern void tcp_reno_cong_avoid(struct tcp_sock *tp, u32 ack,
 				u32 rtt, u32 in_flight, int flag);
 extern u32 tcp_reno_min_cwnd(struct tcp_sock *tp);
+extern struct tcp_congestion_ops tcp_reno;
 
 static inline void tcp_set_ca_state(struct tcp_sock *tp, u8 ca_state)
 {

commit 5f8ef48d240963093451bcf83df89f1a1364f51d
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Jun 23 20:37:36 2005 -0700

    [TCP]: Allow choosing TCP congestion control via sockopt.
    
    Allow using setsockopt to set TCP congestion control to use on a per
    socket basis.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e427cf35915c..d04b21188ccb 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1162,8 +1162,9 @@ extern void tcp_init_congestion_control(struct tcp_sock *tp);
 extern void tcp_cleanup_congestion_control(struct tcp_sock *tp);
 extern int tcp_set_default_congestion_control(const char *name);
 extern void tcp_get_default_congestion_control(char *name);
+extern int tcp_set_congestion_control(struct tcp_sock *tp, const char *name);
 
-extern struct tcp_congestion_ops tcp_reno;
+extern struct tcp_congestion_ops tcp_init_congestion_ops;
 extern u32 tcp_reno_ssthresh(struct tcp_sock *tp);
 extern void tcp_reno_cong_avoid(struct tcp_sock *tp, u32 ack,
 				u32 rtt, u32 in_flight, int flag);

commit 317a76f9a44b437d6301718f4e5d08bd93f98da7
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Jun 23 12:19:55 2005 -0700

    [TCP]: Add pluggable congestion control algorithm infrastructure.
    
    Allow TCP to have multiple pluggable congestion control algorithms.
    Algorithms are defined by a set of operations and can be built in
    or modules.  The legacy "new RENO" algorithm is used as a starting
    point and fallback.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index f730935b824a..e427cf35915c 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -505,25 +505,6 @@ static __inline__ int tcp_sk_listen_hashfn(struct sock *sk)
 #else
 # define TCP_TW_RECYCLE_TICK (12+2-TCP_TW_RECYCLE_SLOTS_LOG)
 #endif
-
-#define BICTCP_BETA_SCALE    1024	/* Scale factor beta calculation
-					 * max_cwnd = snd_cwnd * beta
-					 */
-#define BICTCP_MAX_INCREMENT 32		/*
-					 * Limit on the amount of
-					 * increment allowed during
-					 * binary search.
-					 */
-#define BICTCP_FUNC_OF_MIN_INCR 11	/*
-					 * log(B/Smin)/log(B/(B-1))+1,
-					 * Smin:min increment
-					 * B:log factor
-					 */
-#define BICTCP_B		4	 /*
-					  * In binary search,
-					  * go to point (max+min)/N
-					  */
-
 /*
  *	TCP option
  */
@@ -596,16 +577,7 @@ extern int sysctl_tcp_adv_win_scale;
 extern int sysctl_tcp_tw_reuse;
 extern int sysctl_tcp_frto;
 extern int sysctl_tcp_low_latency;
-extern int sysctl_tcp_westwood;
-extern int sysctl_tcp_vegas_cong_avoid;
-extern int sysctl_tcp_vegas_alpha;
-extern int sysctl_tcp_vegas_beta;
-extern int sysctl_tcp_vegas_gamma;
 extern int sysctl_tcp_nometrics_save;
-extern int sysctl_tcp_bic;
-extern int sysctl_tcp_bic_fast_convergence;
-extern int sysctl_tcp_bic_low_window;
-extern int sysctl_tcp_bic_beta;
 extern int sysctl_tcp_moderate_rcvbuf;
 extern int sysctl_tcp_tso_win_divisor;
 
@@ -1136,6 +1108,80 @@ static inline void tcp_packets_out_dec(struct tcp_sock *tp,
 	tp->packets_out -= tcp_skb_pcount(skb);
 }
 
+/* Events passed to congestion control interface */
+enum tcp_ca_event {
+	CA_EVENT_TX_START,	/* first transmit when no packets in flight */
+	CA_EVENT_CWND_RESTART,	/* congestion window restart */
+	CA_EVENT_COMPLETE_CWR,	/* end of congestion recovery */
+	CA_EVENT_FRTO,		/* fast recovery timeout */
+	CA_EVENT_LOSS,		/* loss timeout */
+	CA_EVENT_FAST_ACK,	/* in sequence ack */
+	CA_EVENT_SLOW_ACK,	/* other ack */
+};
+
+/*
+ * Interface for adding new TCP congestion control handlers
+ */
+#define TCP_CA_NAME_MAX	16
+struct tcp_congestion_ops {
+	struct list_head	list;
+
+	/* initialize private data (optional) */
+	void (*init)(struct tcp_sock *tp);
+	/* cleanup private data  (optional) */
+	void (*release)(struct tcp_sock *tp);
+
+	/* return slow start threshold (required) */
+	u32 (*ssthresh)(struct tcp_sock *tp);
+	/* lower bound for congestion window (optional) */
+	u32 (*min_cwnd)(struct tcp_sock *tp);
+	/* do new cwnd calculation (required) */
+	void (*cong_avoid)(struct tcp_sock *tp, u32 ack,
+			   u32 rtt, u32 in_flight, int good_ack);
+	/* round trip time sample per acked packet (optional) */
+	void (*rtt_sample)(struct tcp_sock *tp, u32 usrtt);
+	/* call before changing ca_state (optional) */
+	void (*set_state)(struct tcp_sock *tp, u8 new_state);
+	/* call when cwnd event occurs (optional) */
+	void (*cwnd_event)(struct tcp_sock *tp, enum tcp_ca_event ev);
+	/* new value of cwnd after loss (optional) */
+	u32  (*undo_cwnd)(struct tcp_sock *tp);
+	/* hook for packet ack accounting (optional) */
+	void (*pkts_acked)(struct tcp_sock *tp, u32 num_acked);
+	/* get info for tcp_diag (optional) */
+	void (*get_info)(struct tcp_sock *tp, u32 ext, struct sk_buff *skb);
+
+	char 		name[TCP_CA_NAME_MAX];
+	struct module 	*owner;
+};
+
+extern int tcp_register_congestion_control(struct tcp_congestion_ops *type);
+extern void tcp_unregister_congestion_control(struct tcp_congestion_ops *type);
+
+extern void tcp_init_congestion_control(struct tcp_sock *tp);
+extern void tcp_cleanup_congestion_control(struct tcp_sock *tp);
+extern int tcp_set_default_congestion_control(const char *name);
+extern void tcp_get_default_congestion_control(char *name);
+
+extern struct tcp_congestion_ops tcp_reno;
+extern u32 tcp_reno_ssthresh(struct tcp_sock *tp);
+extern void tcp_reno_cong_avoid(struct tcp_sock *tp, u32 ack,
+				u32 rtt, u32 in_flight, int flag);
+extern u32 tcp_reno_min_cwnd(struct tcp_sock *tp);
+
+static inline void tcp_set_ca_state(struct tcp_sock *tp, u8 ca_state)
+{
+	if (tp->ca_ops->set_state)
+		tp->ca_ops->set_state(tp, ca_state);
+	tp->ca_state = ca_state;
+}
+
+static inline void tcp_ca_event(struct tcp_sock *tp, enum tcp_ca_event event)
+{
+	if (tp->ca_ops->cwnd_event)
+		tp->ca_ops->cwnd_event(tp, event);
+}
+
 /* This determines how many packets are "in the network" to the best
  * of our knowledge.  In many cases it is conservative, but where
  * detailed information is available from the receiver (via SACK
@@ -1155,91 +1201,6 @@ static __inline__ unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
 	return (tp->packets_out - tp->left_out + tp->retrans_out);
 }
 
-/*
- * Which congestion algorithim is in use on the connection.
- */
-#define tcp_is_vegas(__tp)	((__tp)->adv_cong == TCP_VEGAS)
-#define tcp_is_westwood(__tp)	((__tp)->adv_cong == TCP_WESTWOOD)
-#define tcp_is_bic(__tp)	((__tp)->adv_cong == TCP_BIC)
-
-/* Recalculate snd_ssthresh, we want to set it to:
- *
- * Reno:
- * 	one half the current congestion window, but no
- *	less than two segments
- *
- * BIC:
- *	behave like Reno until low_window is reached,
- *	then increase congestion window slowly
- */
-static inline __u32 tcp_recalc_ssthresh(struct tcp_sock *tp)
-{
-	if (tcp_is_bic(tp)) {
-		if (sysctl_tcp_bic_fast_convergence &&
-		    tp->snd_cwnd < tp->bictcp.last_max_cwnd)
-			tp->bictcp.last_max_cwnd = (tp->snd_cwnd * 
-						    (BICTCP_BETA_SCALE
-						     + sysctl_tcp_bic_beta))
-				/ (2 * BICTCP_BETA_SCALE);
-		else
-			tp->bictcp.last_max_cwnd = tp->snd_cwnd;
-
-		if (tp->snd_cwnd > sysctl_tcp_bic_low_window)
-			return max((tp->snd_cwnd * sysctl_tcp_bic_beta)
-				   / BICTCP_BETA_SCALE, 2U);
-	}
-
-	return max(tp->snd_cwnd >> 1U, 2U);
-}
-
-/* Stop taking Vegas samples for now. */
-#define tcp_vegas_disable(__tp)	((__tp)->vegas.doing_vegas_now = 0)
-    
-static inline void tcp_vegas_enable(struct tcp_sock *tp)
-{
-	/* There are several situations when we must "re-start" Vegas:
-	 *
-	 *  o when a connection is established
-	 *  o after an RTO
-	 *  o after fast recovery
-	 *  o when we send a packet and there is no outstanding
-	 *    unacknowledged data (restarting an idle connection)
-	 *
-	 * In these circumstances we cannot do a Vegas calculation at the
-	 * end of the first RTT, because any calculation we do is using
-	 * stale info -- both the saved cwnd and congestion feedback are
-	 * stale.
-	 *
-	 * Instead we must wait until the completion of an RTT during
-	 * which we actually receive ACKs.
-	 */
-    
-	/* Begin taking Vegas samples next time we send something. */
-	tp->vegas.doing_vegas_now = 1;
-     
-	/* Set the beginning of the next send window. */
-	tp->vegas.beg_snd_nxt = tp->snd_nxt;
-
-	tp->vegas.cntRTT = 0;
-	tp->vegas.minRTT = 0x7fffffff;
-}
-
-/* Should we be taking Vegas samples right now? */
-#define tcp_vegas_enabled(__tp)	((__tp)->vegas.doing_vegas_now)
-
-extern void tcp_ca_init(struct tcp_sock *tp);
-
-static inline void tcp_set_ca_state(struct tcp_sock *tp, u8 ca_state)
-{
-	if (tcp_is_vegas(tp)) {
-		if (ca_state == TCP_CA_Open) 
-			tcp_vegas_enable(tp);
-		else
-			tcp_vegas_disable(tp);
-	}
-	tp->ca_state = ca_state;
-}
-
 /* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
  * The exception is rate halving phase, when cwnd is decreasing towards
  * ssthresh.
@@ -1288,7 +1249,7 @@ static inline void tcp_cwnd_validate(struct sock *sk, struct tcp_sock *tp)
 static inline void __tcp_enter_cwr(struct tcp_sock *tp)
 {
 	tp->undo_marker = 0;
-	tp->snd_ssthresh = tcp_recalc_ssthresh(tp);
+	tp->snd_ssthresh = tp->ca_ops->ssthresh(tp);
 	tp->snd_cwnd = min(tp->snd_cwnd,
 			   tcp_packets_in_flight(tp) + 1U);
 	tp->snd_cwnd_cnt = 0;
@@ -1876,52 +1837,4 @@ struct tcp_iter_state {
 extern int tcp_proc_register(struct tcp_seq_afinfo *afinfo);
 extern void tcp_proc_unregister(struct tcp_seq_afinfo *afinfo);
 
-/* TCP Westwood functions and constants */
-
-#define TCP_WESTWOOD_INIT_RTT  (20*HZ)           /* maybe too conservative?! */
-#define TCP_WESTWOOD_RTT_MIN   (HZ/20)           /* 50ms */
-
-static inline void tcp_westwood_update_rtt(struct tcp_sock *tp, __u32 rtt_seq)
-{
-        if (tcp_is_westwood(tp))
-                tp->westwood.rtt = rtt_seq;
-}
-
-static inline __u32 __tcp_westwood_bw_rttmin(const struct tcp_sock *tp)
-{
-        return max((tp->westwood.bw_est) * (tp->westwood.rtt_min) /
-		   (__u32) (tp->mss_cache_std),
-		   2U);
-}
-
-static inline __u32 tcp_westwood_bw_rttmin(const struct tcp_sock *tp)
-{
-	return tcp_is_westwood(tp) ? __tcp_westwood_bw_rttmin(tp) : 0;
-}
-
-static inline int tcp_westwood_ssthresh(struct tcp_sock *tp)
-{
-	__u32 ssthresh = 0;
-
-	if (tcp_is_westwood(tp)) {
-		ssthresh = __tcp_westwood_bw_rttmin(tp);
-		if (ssthresh)
-			tp->snd_ssthresh = ssthresh;  
-	}
-
-	return (ssthresh != 0);
-}
-
-static inline int tcp_westwood_cwnd(struct tcp_sock *tp)
-{
-	__u32 cwnd = 0;
-
-	if (tcp_is_westwood(tp)) {
-		cwnd = __tcp_westwood_bw_rttmin(tp);
-		if (cwnd)
-			tp->snd_cwnd = cwnd;
-	}
-
-	return (cwnd != 0);
-}
 #endif	/* _TCP_H */

commit e52c1f17e4ea8e61bd26eb25f1a184202693c2b9
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jun 18 22:49:40 2005 -0700

    [NET]: Move sysctl_max_syn_backlog into request_sock.c
    
    This fixes the CONFIG_INET=n build failure noticed
    by Andrew Morton.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index a2e323c54457..f730935b824a 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -564,7 +564,6 @@ static __inline__ int tcp_sk_listen_hashfn(struct sock *sk)
 #define TCP_NAGLE_PUSH		4	/* Cork is overriden for already queued data */
 
 /* sysctl variables for tcp */
-extern int sysctl_max_syn_backlog;
 extern int sysctl_tcp_timestamps;
 extern int sysctl_tcp_window_scaling;
 extern int sysctl_tcp_sack;

commit 0e87506fcc734647c7b2497eee4eb81e785c857a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:59 2005 -0700

    [NET] Generalise tcp_listen_opt
    
    This chunks out the accept_queue and tcp_listen_opt code and moves
    them to net/core/request_sock.c and include/net/request_sock.h, to
    make it useful for other transport protocols, DCCP being the first one
    to use it.
    
    Next patches will rename tcp_listen_opt to accept_sock and remove the
    inline tcp functions that just call a reqsk_queue_ function.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 6663086a5e35..a2e323c54457 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1686,71 +1686,41 @@ static inline int tcp_full_space(const struct sock *sk)
 static inline void tcp_acceptq_queue(struct sock *sk, struct request_sock *req,
 					 struct sock *child)
 {
-	struct tcp_sock *tp = tcp_sk(sk);
-
-	req->sk = child;
-	sk_acceptq_added(sk);
-
-	if (!tp->accept_queue_tail) {
-		tp->accept_queue = req;
-	} else {
-		tp->accept_queue_tail->dl_next = req;
-	}
-	tp->accept_queue_tail = req;
-	req->dl_next = NULL;
+	reqsk_queue_add(&tcp_sk(sk)->accept_queue, req, sk, child);
 }
 
-struct tcp_listen_opt
-{
-	u8			max_qlen_log;	/* log_2 of maximal queued SYNs */
-	int			qlen;
-	int			qlen_young;
-	int			clock_hand;
-	u32			hash_rnd;
-	struct request_sock	*syn_table[TCP_SYNQ_HSIZE];
-};
-
 static inline void
 tcp_synq_removed(struct sock *sk, struct request_sock *req)
 {
-	struct tcp_listen_opt *lopt = tcp_sk(sk)->listen_opt;
-
-	if (--lopt->qlen == 0)
+	if (reqsk_queue_removed(&tcp_sk(sk)->accept_queue, req) == 0)
 		tcp_delete_keepalive_timer(sk);
-	if (req->retrans == 0)
-		lopt->qlen_young--;
 }
 
 static inline void tcp_synq_added(struct sock *sk)
 {
-	struct tcp_listen_opt *lopt = tcp_sk(sk)->listen_opt;
-
-	if (lopt->qlen++ == 0)
+	if (reqsk_queue_added(&tcp_sk(sk)->accept_queue) == 0)
 		tcp_reset_keepalive_timer(sk, TCP_TIMEOUT_INIT);
-	lopt->qlen_young++;
 }
 
 static inline int tcp_synq_len(struct sock *sk)
 {
-	return tcp_sk(sk)->listen_opt->qlen;
+	return reqsk_queue_len(&tcp_sk(sk)->accept_queue);
 }
 
 static inline int tcp_synq_young(struct sock *sk)
 {
-	return tcp_sk(sk)->listen_opt->qlen_young;
+	return reqsk_queue_len_young(&tcp_sk(sk)->accept_queue);
 }
 
 static inline int tcp_synq_is_full(struct sock *sk)
 {
-	return tcp_synq_len(sk) >> tcp_sk(sk)->listen_opt->max_qlen_log;
+	return reqsk_queue_is_full(&tcp_sk(sk)->accept_queue);
 }
 
 static inline void tcp_synq_unlink(struct tcp_sock *tp, struct request_sock *req,
-				       struct request_sock **prev)
+				   struct request_sock **prev)
 {
-	write_lock(&tp->syn_wait_lock);
-	*prev = req->dl_next;
-	write_unlock(&tp->syn_wait_lock);
+	reqsk_queue_unlink(&tp->accept_queue, req, prev);
 }
 
 static inline void tcp_synq_drop(struct sock *sk, struct request_sock *req,

commit 60236fdd08b2169045a3bbfc5ffe1576e6c3c17b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:21 2005 -0700

    [NET] Rename open_request to request_sock
    
    Ok, this one just renames some stuff to have a better namespace and to
    dissassociate it from TCP:
    
    struct open_request  -> struct request_sock
    tcp_openreq_alloc    -> reqsk_alloc
    tcp_openreq_free     -> reqsk_free
    tcp_openreq_fastfree -> __reqsk_free
    
    With this most of the infrastructure closely resembles a struct
    sock methods subset.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index d438ba566b89..6663086a5e35 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -641,7 +641,7 @@ struct tcp_func {
 
 	struct sock *		(*syn_recv_sock)	(struct sock *sk,
 							 struct sk_buff *skb,
-							 struct open_request *req,
+							 struct request_sock *req,
 							 struct dst_entry *dst);
     
 	int			(*remember_stamp)	(struct sock *sk);
@@ -785,8 +785,8 @@ extern enum tcp_tw_status	tcp_timewait_state_process(struct tcp_tw_bucket *tw,
 							   unsigned len);
 
 extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
-					      struct open_request *req,
-					      struct open_request **prev);
+					      struct request_sock *req,
+					      struct request_sock **prev);
 extern int			tcp_child_process(struct sock *parent,
 						  struct sock *child,
 						  struct sk_buff *skb);
@@ -836,12 +836,12 @@ extern int			tcp_v4_conn_request(struct sock *sk,
 						    struct sk_buff *skb);
 
 extern struct sock *		tcp_create_openreq_child(struct sock *sk,
-							 struct open_request *req,
+							 struct request_sock *req,
 							 struct sk_buff *skb);
 
 extern struct sock *		tcp_v4_syn_recv_sock(struct sock *sk,
 						     struct sk_buff *skb,
-						     struct open_request *req,
+						     struct request_sock *req,
 							struct dst_entry *dst);
 
 extern int			tcp_v4_do_rcv(struct sock *sk,
@@ -855,7 +855,7 @@ extern int			tcp_connect(struct sock *sk);
 
 extern struct sk_buff *		tcp_make_synack(struct sock *sk,
 						struct dst_entry *dst,
-						struct open_request *req);
+						struct request_sock *req);
 
 extern int			tcp_disconnect(struct sock *sk, int flags);
 
@@ -1683,7 +1683,7 @@ static inline int tcp_full_space(const struct sock *sk)
 	return tcp_win_from_space(sk->sk_rcvbuf); 
 }
 
-static inline void tcp_acceptq_queue(struct sock *sk, struct open_request *req,
+static inline void tcp_acceptq_queue(struct sock *sk, struct request_sock *req,
 					 struct sock *child)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
@@ -1707,11 +1707,11 @@ struct tcp_listen_opt
 	int			qlen_young;
 	int			clock_hand;
 	u32			hash_rnd;
-	struct open_request	*syn_table[TCP_SYNQ_HSIZE];
+	struct request_sock	*syn_table[TCP_SYNQ_HSIZE];
 };
 
 static inline void
-tcp_synq_removed(struct sock *sk, struct open_request *req)
+tcp_synq_removed(struct sock *sk, struct request_sock *req)
 {
 	struct tcp_listen_opt *lopt = tcp_sk(sk)->listen_opt;
 
@@ -1745,23 +1745,23 @@ static inline int tcp_synq_is_full(struct sock *sk)
 	return tcp_synq_len(sk) >> tcp_sk(sk)->listen_opt->max_qlen_log;
 }
 
-static inline void tcp_synq_unlink(struct tcp_sock *tp, struct open_request *req,
-				       struct open_request **prev)
+static inline void tcp_synq_unlink(struct tcp_sock *tp, struct request_sock *req,
+				       struct request_sock **prev)
 {
 	write_lock(&tp->syn_wait_lock);
 	*prev = req->dl_next;
 	write_unlock(&tp->syn_wait_lock);
 }
 
-static inline void tcp_synq_drop(struct sock *sk, struct open_request *req,
-				     struct open_request **prev)
+static inline void tcp_synq_drop(struct sock *sk, struct request_sock *req,
+				     struct request_sock **prev)
 {
 	tcp_synq_unlink(tcp_sk(sk), req, prev);
 	tcp_synq_removed(sk, req);
-	tcp_openreq_free(req);
+	reqsk_free(req);
 }
 
-static __inline__ void tcp_openreq_init(struct open_request *req,
+static __inline__ void tcp_openreq_init(struct request_sock *req,
 					struct tcp_options_received *rx_opt,
 					struct sk_buff *skb)
 {

commit 2e6599cb899ba4b133f42cbf9d2b1883d2dc583a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:46:52 2005 -0700

    [NET] Generalise TCP's struct open_request minisock infrastructure
    
    Kept this first changeset minimal, without changing existing names to
    ease peer review.
    
    Basicaly tcp_openreq_alloc now receives the or_calltable, that in turn
    has two new members:
    
    ->slab, that replaces tcp_openreq_cachep
    ->obj_size, to inform the size of the openreq descendant for
      a specific protocol
    
    The protocol specific fields in struct open_request were moved to a
    class hierarchy, with the things that are common to all connection
    oriented PF_INET protocols in struct inet_request_sock, the TCP ones
    in tcp_request_sock, that is an inet_request_sock, that is an
    open_request.
    
    I.e. this uses the same approach used for the struct sock class
    hierarchy, with sk_prot indicating if the protocol wants to use the
    open_request infrastructure by filling in sk_prot->rsk_prot with an
    or_calltable.
    
    Results? Performance is improved and TCP v4 now uses only 64 bytes per
    open request minisock, down from 96 without this patch :-)
    
    Next changeset will rename some of the structs, fields and functions
    mentioned above, struct or_calltable is way unclear, better name it
    struct request_sock_ops, s/struct open_request/struct request_sock/g,
    etc.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index e71f8ba3e101..d438ba566b89 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -31,6 +31,7 @@
 #include <linux/cache.h>
 #include <linux/percpu.h>
 #include <net/checksum.h>
+#include <net/request_sock.h>
 #include <net/sock.h>
 #include <net/snmp.h>
 #include <net/ip.h>
@@ -613,74 +614,6 @@ extern atomic_t tcp_memory_allocated;
 extern atomic_t tcp_sockets_allocated;
 extern int tcp_memory_pressure;
 
-struct open_request;
-
-struct or_calltable {
-	int  family;
-	int  (*rtx_syn_ack)	(struct sock *sk, struct open_request *req, struct dst_entry*);
-	void (*send_ack)	(struct sk_buff *skb, struct open_request *req);
-	void (*destructor)	(struct open_request *req);
-	void (*send_reset)	(struct sk_buff *skb);
-};
-
-struct tcp_v4_open_req {
-	__u32			loc_addr;
-	__u32			rmt_addr;
-	struct ip_options	*opt;
-};
-
-#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
-struct tcp_v6_open_req {
-	struct in6_addr		loc_addr;
-	struct in6_addr		rmt_addr;
-	struct sk_buff		*pktopts;
-	int			iif;
-};
-#endif
-
-/* this structure is too big */
-struct open_request {
-	struct open_request	*dl_next; /* Must be first member! */
-	__u32			rcv_isn;
-	__u32			snt_isn;
-	__u16			rmt_port;
-	__u16			mss;
-	__u8			retrans;
-	__u8			__pad;
-	__u16	snd_wscale : 4, 
-		rcv_wscale : 4, 
-		tstamp_ok : 1,
-		sack_ok : 1,
-		wscale_ok : 1,
-		ecn_ok : 1,
-		acked : 1;
-	/* The following two fields can be easily recomputed I think -AK */
-	__u32			window_clamp;	/* window clamp at creation time */
-	__u32			rcv_wnd;	/* rcv_wnd offered first time */
-	__u32			ts_recent;
-	unsigned long		expires;
-	struct or_calltable	*class;
-	struct sock		*sk;
-	union {
-		struct tcp_v4_open_req v4_req;
-#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
-		struct tcp_v6_open_req v6_req;
-#endif
-	} af;
-};
-
-/* SLAB cache for open requests. */
-extern kmem_cache_t *tcp_openreq_cachep;
-
-#define tcp_openreq_alloc()		kmem_cache_alloc(tcp_openreq_cachep, SLAB_ATOMIC)
-#define tcp_openreq_fastfree(req)	kmem_cache_free(tcp_openreq_cachep, req)
-
-static inline void tcp_openreq_free(struct open_request *req)
-{
-	req->class->destructor(req);
-	tcp_openreq_fastfree(req);
-}
-
 #if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
 #define TCP_INET_FAMILY(fam) ((fam) == AF_INET)
 #else
@@ -1832,17 +1765,19 @@ static __inline__ void tcp_openreq_init(struct open_request *req,
 					struct tcp_options_received *rx_opt,
 					struct sk_buff *skb)
 {
+	struct inet_request_sock *ireq = inet_rsk(req);
+
 	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
-	req->rcv_isn = TCP_SKB_CB(skb)->seq;
+	tcp_rsk(req)->rcv_isn = TCP_SKB_CB(skb)->seq;
 	req->mss = rx_opt->mss_clamp;
 	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
-	req->tstamp_ok = rx_opt->tstamp_ok;
-	req->sack_ok = rx_opt->sack_ok;
-	req->snd_wscale = rx_opt->snd_wscale;
-	req->wscale_ok = rx_opt->wscale_ok;
-	req->acked = 0;
-	req->ecn_ok = 0;
-	req->rmt_port = skb->h.th->source;
+	ireq->tstamp_ok = rx_opt->tstamp_ok;
+	ireq->sack_ok = rx_opt->sack_ok;
+	ireq->snd_wscale = rx_opt->snd_wscale;
+	ireq->wscale_ok = rx_opt->wscale_ok;
+	ireq->acked = 0;
+	ireq->ecn_ok = 0;
+	ireq->rmt_port = skb->h.th->source;
 }
 
 extern void tcp_enter_memory_pressure(void);

commit 02c30a84e6298b6b20a56f0896ac80b47839e134
Author: Jesper Juhl <juhl-lkml@dif.dk>
Date:   Thu May 5 16:16:16 2005 -0700

    [PATCH] update Ross Biro bouncing email address
    
    Ross moved.  Remove the bad email address so people will find the correct
    one in ./CREDITS.
    
    Signed-off-by: Jesper Juhl <juhl-lkml@dif.dk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 9355ae5b1d75..e71f8ba3e101 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -7,7 +7,7 @@
  *
  * Version:	@(#)tcp.h	1.0.5	05/23/93
  *
- * Authors:	Ross Biro, <bir7@leland.Stanford.Edu>
+ * Authors:	Ross Biro
  *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
  *
  *		This program is free software; you can redistribute it and/or

commit d5ac99a648b8c61d0c7f1c32a8ab7f1dca0123d2
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Sun Apr 24 19:12:33 2005 -0700

    [TCP]: skb pcount with MTU discovery
    
    The problem is that when doing MTU discovery, the too-large segments in
    the write queue will be calculated as having a pcount of >1.  When
    tcp_write_xmit() is trying to send, tcp_snd_test() fails the cwnd test
    when pcount > cwnd.
    
    The segments are eventually transmitted one at a time by keepalive, but
    this can take a long time.
    
    This patch checks if TSO is enabled when setting pcount.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 503810a70e21..9355ae5b1d75 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -1417,19 +1417,20 @@ tcp_nagle_check(const struct tcp_sock *tp, const struct sk_buff *skb,
 		  tcp_minshall_check(tp))));
 }
 
-extern void tcp_set_skb_tso_segs(struct sk_buff *, unsigned int);
+extern void tcp_set_skb_tso_segs(struct sock *, struct sk_buff *);
 
 /* This checks if the data bearing packet SKB (usually sk->sk_send_head)
  * should be put on the wire right now.
  */
-static __inline__ int tcp_snd_test(const struct tcp_sock *tp, 
+static __inline__ int tcp_snd_test(struct sock *sk,
 				   struct sk_buff *skb,
 				   unsigned cur_mss, int nonagle)
 {
+	struct tcp_sock *tp = tcp_sk(sk);
 	int pkts = tcp_skb_pcount(skb);
 
 	if (!pkts) {
-		tcp_set_skb_tso_segs(skb, tp->mss_cache_std);
+		tcp_set_skb_tso_segs(sk, skb);
 		pkts = tcp_skb_pcount(skb);
 	}
 
@@ -1490,7 +1491,7 @@ static __inline__ void __tcp_push_pending_frames(struct sock *sk,
 	if (skb) {
 		if (!tcp_skb_is_last(sk, skb))
 			nonagle = TCP_NAGLE_PUSH;
-		if (!tcp_snd_test(tp, skb, cur_mss, nonagle) ||
+		if (!tcp_snd_test(sk, skb, cur_mss, nonagle) ||
 		    tcp_write_xmit(sk, nonagle))
 			tcp_check_probe_timer(sk, tp);
 	}
@@ -1508,7 +1509,7 @@ static __inline__ int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp)
 	struct sk_buff *skb = sk->sk_send_head;
 
 	return (skb &&
-		tcp_snd_test(tp, skb, tcp_current_mss(sk, 1),
+		tcp_snd_test(sk, skb, tcp_current_mss(sk, 1),
 			     tcp_skb_is_last(sk, skb) ? TCP_NAGLE_PUSH : tp->nonagle));
 }
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/net/tcp.h b/include/net/tcp.h
new file mode 100644
index 000000000000..503810a70e21
--- /dev/null
+++ b/include/net/tcp.h
@@ -0,0 +1,2022 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the TCP module.
+ *
+ * Version:	@(#)tcp.h	1.0.5	05/23/93
+ *
+ * Authors:	Ross Biro, <bir7@leland.Stanford.Edu>
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _TCP_H
+#define _TCP_H
+
+#define TCP_DEBUG 1
+#define FASTRETRANS_DEBUG 1
+
+/* Cancel timers, when they are not required. */
+#undef TCP_CLEAR_TIMERS
+
+#include <linux/config.h>
+#include <linux/list.h>
+#include <linux/tcp.h>
+#include <linux/slab.h>
+#include <linux/cache.h>
+#include <linux/percpu.h>
+#include <net/checksum.h>
+#include <net/sock.h>
+#include <net/snmp.h>
+#include <net/ip.h>
+#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
+#include <linux/ipv6.h>
+#endif
+#include <linux/seq_file.h>
+
+/* This is for all connections with a full identity, no wildcards.
+ * New scheme, half the table is for TIME_WAIT, the other half is
+ * for the rest.  I'll experiment with dynamic table growth later.
+ */
+struct tcp_ehash_bucket {
+	rwlock_t	  lock;
+	struct hlist_head chain;
+} __attribute__((__aligned__(8)));
+
+/* This is for listening sockets, thus all sockets which possess wildcards. */
+#define TCP_LHTABLE_SIZE	32	/* Yes, really, this is all you need. */
+
+/* There are a few simple rules, which allow for local port reuse by
+ * an application.  In essence:
+ *
+ *	1) Sockets bound to different interfaces may share a local port.
+ *	   Failing that, goto test 2.
+ *	2) If all sockets have sk->sk_reuse set, and none of them are in
+ *	   TCP_LISTEN state, the port may be shared.
+ *	   Failing that, goto test 3.
+ *	3) If all sockets are bound to a specific inet_sk(sk)->rcv_saddr local
+ *	   address, and none of them are the same, the port may be
+ *	   shared.
+ *	   Failing this, the port cannot be shared.
+ *
+ * The interesting point, is test #2.  This is what an FTP server does
+ * all day.  To optimize this case we use a specific flag bit defined
+ * below.  As we add sockets to a bind bucket list, we perform a
+ * check of: (newsk->sk_reuse && (newsk->sk_state != TCP_LISTEN))
+ * As long as all sockets added to a bind bucket pass this test,
+ * the flag bit will be set.
+ * The resulting situation is that tcp_v[46]_verify_bind() can just check
+ * for this flag bit, if it is set and the socket trying to bind has
+ * sk->sk_reuse set, we don't even have to walk the owners list at all,
+ * we return that it is ok to bind this socket to the requested local port.
+ *
+ * Sounds like a lot of work, but it is worth it.  In a more naive
+ * implementation (ie. current FreeBSD etc.) the entire list of ports
+ * must be walked for each data port opened by an ftp server.  Needless
+ * to say, this does not scale at all.  With a couple thousand FTP
+ * users logged onto your box, isn't it nice to know that new data
+ * ports are created in O(1) time?  I thought so. ;-)	-DaveM
+ */
+struct tcp_bind_bucket {
+	unsigned short		port;
+	signed short		fastreuse;
+	struct hlist_node	node;
+	struct hlist_head	owners;
+};
+
+#define tb_for_each(tb, node, head) hlist_for_each_entry(tb, node, head, node)
+
+struct tcp_bind_hashbucket {
+	spinlock_t		lock;
+	struct hlist_head	chain;
+};
+
+static inline struct tcp_bind_bucket *__tb_head(struct tcp_bind_hashbucket *head)
+{
+	return hlist_entry(head->chain.first, struct tcp_bind_bucket, node);
+}
+
+static inline struct tcp_bind_bucket *tb_head(struct tcp_bind_hashbucket *head)
+{
+	return hlist_empty(&head->chain) ? NULL : __tb_head(head);
+}
+
+extern struct tcp_hashinfo {
+	/* This is for sockets with full identity only.  Sockets here will
+	 * always be without wildcards and will have the following invariant:
+	 *
+	 *          TCP_ESTABLISHED <= sk->sk_state < TCP_CLOSE
+	 *
+	 * First half of the table is for sockets not in TIME_WAIT, second half
+	 * is for TIME_WAIT sockets only.
+	 */
+	struct tcp_ehash_bucket *__tcp_ehash;
+
+	/* Ok, let's try this, I give up, we do need a local binding
+	 * TCP hash as well as the others for fast bind/connect.
+	 */
+	struct tcp_bind_hashbucket *__tcp_bhash;
+
+	int __tcp_bhash_size;
+	int __tcp_ehash_size;
+
+	/* All sockets in TCP_LISTEN state will be in here.  This is the only
+	 * table where wildcard'd TCP sockets can exist.  Hash function here
+	 * is just local port number.
+	 */
+	struct hlist_head __tcp_listening_hash[TCP_LHTABLE_SIZE];
+
+	/* All the above members are written once at bootup and
+	 * never written again _or_ are predominantly read-access.
+	 *
+	 * Now align to a new cache line as all the following members
+	 * are often dirty.
+	 */
+	rwlock_t __tcp_lhash_lock ____cacheline_aligned;
+	atomic_t __tcp_lhash_users;
+	wait_queue_head_t __tcp_lhash_wait;
+	spinlock_t __tcp_portalloc_lock;
+} tcp_hashinfo;
+
+#define tcp_ehash	(tcp_hashinfo.__tcp_ehash)
+#define tcp_bhash	(tcp_hashinfo.__tcp_bhash)
+#define tcp_ehash_size	(tcp_hashinfo.__tcp_ehash_size)
+#define tcp_bhash_size	(tcp_hashinfo.__tcp_bhash_size)
+#define tcp_listening_hash (tcp_hashinfo.__tcp_listening_hash)
+#define tcp_lhash_lock	(tcp_hashinfo.__tcp_lhash_lock)
+#define tcp_lhash_users	(tcp_hashinfo.__tcp_lhash_users)
+#define tcp_lhash_wait	(tcp_hashinfo.__tcp_lhash_wait)
+#define tcp_portalloc_lock (tcp_hashinfo.__tcp_portalloc_lock)
+
+extern kmem_cache_t *tcp_bucket_cachep;
+extern struct tcp_bind_bucket *tcp_bucket_create(struct tcp_bind_hashbucket *head,
+						 unsigned short snum);
+extern void tcp_bucket_destroy(struct tcp_bind_bucket *tb);
+extern void tcp_bucket_unlock(struct sock *sk);
+extern int tcp_port_rover;
+
+/* These are AF independent. */
+static __inline__ int tcp_bhashfn(__u16 lport)
+{
+	return (lport & (tcp_bhash_size - 1));
+}
+
+extern void tcp_bind_hash(struct sock *sk, struct tcp_bind_bucket *tb,
+			  unsigned short snum);
+
+#if (BITS_PER_LONG == 64)
+#define TCP_ADDRCMP_ALIGN_BYTES 8
+#else
+#define TCP_ADDRCMP_ALIGN_BYTES 4
+#endif
+
+/* This is a TIME_WAIT bucket.  It works around the memory consumption
+ * problems of sockets in such a state on heavily loaded servers, but
+ * without violating the protocol specification.
+ */
+struct tcp_tw_bucket {
+	/*
+	 * Now struct sock also uses sock_common, so please just
+	 * don't add nothing before this first member (__tw_common) --acme
+	 */
+	struct sock_common	__tw_common;
+#define tw_family		__tw_common.skc_family
+#define tw_state		__tw_common.skc_state
+#define tw_reuse		__tw_common.skc_reuse
+#define tw_bound_dev_if		__tw_common.skc_bound_dev_if
+#define tw_node			__tw_common.skc_node
+#define tw_bind_node		__tw_common.skc_bind_node
+#define tw_refcnt		__tw_common.skc_refcnt
+	volatile unsigned char	tw_substate;
+	unsigned char		tw_rcv_wscale;
+	__u16			tw_sport;
+	/* Socket demultiplex comparisons on incoming packets. */
+	/* these five are in inet_sock */
+	__u32			tw_daddr
+		__attribute__((aligned(TCP_ADDRCMP_ALIGN_BYTES)));
+	__u32			tw_rcv_saddr;
+	__u16			tw_dport;
+	__u16			tw_num;
+	/* And these are ours. */
+	int			tw_hashent;
+	int			tw_timeout;
+	__u32			tw_rcv_nxt;
+	__u32			tw_snd_nxt;
+	__u32			tw_rcv_wnd;
+	__u32			tw_ts_recent;
+	long			tw_ts_recent_stamp;
+	unsigned long		tw_ttd;
+	struct tcp_bind_bucket	*tw_tb;
+	struct hlist_node	tw_death_node;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	struct in6_addr		tw_v6_daddr;
+	struct in6_addr		tw_v6_rcv_saddr;
+	int			tw_v6_ipv6only;
+#endif
+};
+
+static __inline__ void tw_add_node(struct tcp_tw_bucket *tw,
+				   struct hlist_head *list)
+{
+	hlist_add_head(&tw->tw_node, list);
+}
+
+static __inline__ void tw_add_bind_node(struct tcp_tw_bucket *tw,
+					struct hlist_head *list)
+{
+	hlist_add_head(&tw->tw_bind_node, list);
+}
+
+static inline int tw_dead_hashed(struct tcp_tw_bucket *tw)
+{
+	return tw->tw_death_node.pprev != NULL;
+}
+
+static __inline__ void tw_dead_node_init(struct tcp_tw_bucket *tw)
+{
+	tw->tw_death_node.pprev = NULL;
+}
+
+static __inline__ void __tw_del_dead_node(struct tcp_tw_bucket *tw)
+{
+	__hlist_del(&tw->tw_death_node);
+	tw_dead_node_init(tw);
+}
+
+static __inline__ int tw_del_dead_node(struct tcp_tw_bucket *tw)
+{
+	if (tw_dead_hashed(tw)) {
+		__tw_del_dead_node(tw);
+		return 1;
+	}
+	return 0;
+}
+
+#define tw_for_each(tw, node, head) \
+	hlist_for_each_entry(tw, node, head, tw_node)
+
+#define tw_for_each_inmate(tw, node, jail) \
+	hlist_for_each_entry(tw, node, jail, tw_death_node)
+
+#define tw_for_each_inmate_safe(tw, node, safe, jail) \
+	hlist_for_each_entry_safe(tw, node, safe, jail, tw_death_node)
+
+#define tcptw_sk(__sk)	((struct tcp_tw_bucket *)(__sk))
+
+static inline u32 tcp_v4_rcv_saddr(const struct sock *sk)
+{
+	return likely(sk->sk_state != TCP_TIME_WAIT) ?
+		inet_sk(sk)->rcv_saddr : tcptw_sk(sk)->tw_rcv_saddr;
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static inline struct in6_addr *__tcp_v6_rcv_saddr(const struct sock *sk)
+{
+	return likely(sk->sk_state != TCP_TIME_WAIT) ?
+		&inet6_sk(sk)->rcv_saddr : &tcptw_sk(sk)->tw_v6_rcv_saddr;
+}
+
+static inline struct in6_addr *tcp_v6_rcv_saddr(const struct sock *sk)
+{
+	return sk->sk_family == AF_INET6 ? __tcp_v6_rcv_saddr(sk) : NULL;
+}
+
+#define tcptw_sk_ipv6only(__sk)	(tcptw_sk(__sk)->tw_v6_ipv6only)
+
+static inline int tcp_v6_ipv6only(const struct sock *sk)
+{
+	return likely(sk->sk_state != TCP_TIME_WAIT) ?
+		ipv6_only_sock(sk) : tcptw_sk_ipv6only(sk);
+}
+#else
+# define __tcp_v6_rcv_saddr(__sk)	NULL
+# define tcp_v6_rcv_saddr(__sk)		NULL
+# define tcptw_sk_ipv6only(__sk)	0
+# define tcp_v6_ipv6only(__sk)		0
+#endif
+
+extern kmem_cache_t *tcp_timewait_cachep;
+
+static inline void tcp_tw_put(struct tcp_tw_bucket *tw)
+{
+	if (atomic_dec_and_test(&tw->tw_refcnt)) {
+#ifdef INET_REFCNT_DEBUG
+		printk(KERN_DEBUG "tw_bucket %p released\n", tw);
+#endif
+		kmem_cache_free(tcp_timewait_cachep, tw);
+	}
+}
+
+extern atomic_t tcp_orphan_count;
+extern int tcp_tw_count;
+extern void tcp_time_wait(struct sock *sk, int state, int timeo);
+extern void tcp_tw_deschedule(struct tcp_tw_bucket *tw);
+
+
+/* Socket demux engine toys. */
+#ifdef __BIG_ENDIAN
+#define TCP_COMBINED_PORTS(__sport, __dport) \
+	(((__u32)(__sport)<<16) | (__u32)(__dport))
+#else /* __LITTLE_ENDIAN */
+#define TCP_COMBINED_PORTS(__sport, __dport) \
+	(((__u32)(__dport)<<16) | (__u32)(__sport))
+#endif
+
+#if (BITS_PER_LONG == 64)
+#ifdef __BIG_ENDIAN
+#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr) \
+	__u64 __name = (((__u64)(__saddr))<<32)|((__u64)(__daddr));
+#else /* __LITTLE_ENDIAN */
+#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr) \
+	__u64 __name = (((__u64)(__daddr))<<32)|((__u64)(__saddr));
+#endif /* __BIG_ENDIAN */
+#define TCP_IPV4_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
+	(((*((__u64 *)&(inet_sk(__sk)->daddr)))== (__cookie))	&&	\
+	 ((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))	&&	\
+	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+#define TCP_IPV4_TW_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
+	(((*((__u64 *)&(tcptw_sk(__sk)->tw_daddr))) == (__cookie)) &&	\
+	 ((*((__u32 *)&(tcptw_sk(__sk)->tw_dport))) == (__ports)) &&	\
+	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+#else /* 32-bit arch */
+#define TCP_V4_ADDR_COOKIE(__name, __saddr, __daddr)
+#define TCP_IPV4_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
+	((inet_sk(__sk)->daddr			== (__saddr))	&&	\
+	 (inet_sk(__sk)->rcv_saddr		== (__daddr))	&&	\
+	 ((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))	&&	\
+	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+#define TCP_IPV4_TW_MATCH(__sk, __cookie, __saddr, __daddr, __ports, __dif)\
+	((tcptw_sk(__sk)->tw_daddr		== (__saddr))	&&	\
+	 (tcptw_sk(__sk)->tw_rcv_saddr		== (__daddr))	&&	\
+	 ((*((__u32 *)&(tcptw_sk(__sk)->tw_dport))) == (__ports)) &&	\
+	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+#endif /* 64-bit arch */
+
+#define TCP_IPV6_MATCH(__sk, __saddr, __daddr, __ports, __dif)	   \
+	(((*((__u32 *)&(inet_sk(__sk)->dport)))== (__ports))   	&& \
+	 ((__sk)->sk_family		== AF_INET6)		&& \
+	 ipv6_addr_equal(&inet6_sk(__sk)->daddr, (__saddr))	&& \
+	 ipv6_addr_equal(&inet6_sk(__sk)->rcv_saddr, (__daddr))	&& \
+	 (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
+
+/* These can have wildcards, don't try too hard. */
+static __inline__ int tcp_lhashfn(unsigned short num)
+{
+	return num & (TCP_LHTABLE_SIZE - 1);
+}
+
+static __inline__ int tcp_sk_listen_hashfn(struct sock *sk)
+{
+	return tcp_lhashfn(inet_sk(sk)->num);
+}
+
+#define MAX_TCP_HEADER	(128 + MAX_HEADER)
+
+/* 
+ * Never offer a window over 32767 without using window scaling. Some
+ * poor stacks do signed 16bit maths! 
+ */
+#define MAX_TCP_WINDOW		32767U
+
+/* Minimal accepted MSS. It is (60+60+8) - (20+20). */
+#define TCP_MIN_MSS		88U
+
+/* Minimal RCV_MSS. */
+#define TCP_MIN_RCVMSS		536U
+
+/* After receiving this amount of duplicate ACKs fast retransmit starts. */
+#define TCP_FASTRETRANS_THRESH 3
+
+/* Maximal reordering. */
+#define TCP_MAX_REORDERING	127
+
+/* Maximal number of ACKs sent quickly to accelerate slow-start. */
+#define TCP_MAX_QUICKACKS	16U
+
+/* urg_data states */
+#define TCP_URG_VALID	0x0100
+#define TCP_URG_NOTYET	0x0200
+#define TCP_URG_READ	0x0400
+
+#define TCP_RETR1	3	/*
+				 * This is how many retries it does before it
+				 * tries to figure out if the gateway is
+				 * down. Minimal RFC value is 3; it corresponds
+				 * to ~3sec-8min depending on RTO.
+				 */
+
+#define TCP_RETR2	15	/*
+				 * This should take at least
+				 * 90 minutes to time out.
+				 * RFC1122 says that the limit is 100 sec.
+				 * 15 is ~13-30min depending on RTO.
+				 */
+
+#define TCP_SYN_RETRIES	 5	/* number of times to retry active opening a
+				 * connection: ~180sec is RFC minumum	*/
+
+#define TCP_SYNACK_RETRIES 5	/* number of times to retry passive opening a
+				 * connection: ~180sec is RFC minumum	*/
+
+
+#define TCP_ORPHAN_RETRIES 7	/* number of times to retry on an orphaned
+				 * socket. 7 is ~50sec-16min.
+				 */
+
+
+#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT
+				  * state, about 60 seconds	*/
+#define TCP_FIN_TIMEOUT	TCP_TIMEWAIT_LEN
+                                 /* BSD style FIN_WAIT2 deadlock breaker.
+				  * It used to be 3min, new value is 60sec,
+				  * to combine FIN-WAIT-2 timeout with
+				  * TIME-WAIT timer.
+				  */
+
+#define TCP_DELACK_MAX	((unsigned)(HZ/5))	/* maximal time to delay before sending an ACK */
+#if HZ >= 100
+#define TCP_DELACK_MIN	((unsigned)(HZ/25))	/* minimal time to delay before sending an ACK */
+#define TCP_ATO_MIN	((unsigned)(HZ/25))
+#else
+#define TCP_DELACK_MIN	4U
+#define TCP_ATO_MIN	4U
+#endif
+#define TCP_RTO_MAX	((unsigned)(120*HZ))
+#define TCP_RTO_MIN	((unsigned)(HZ/5))
+#define TCP_TIMEOUT_INIT ((unsigned)(3*HZ))	/* RFC 1122 initial RTO value	*/
+
+#define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U)) /* Maximal interval between probes
+					                 * for local resources.
+					                 */
+
+#define TCP_KEEPALIVE_TIME	(120*60*HZ)	/* two hours */
+#define TCP_KEEPALIVE_PROBES	9		/* Max of 9 keepalive probes	*/
+#define TCP_KEEPALIVE_INTVL	(75*HZ)
+
+#define MAX_TCP_KEEPIDLE	32767
+#define MAX_TCP_KEEPINTVL	32767
+#define MAX_TCP_KEEPCNT		127
+#define MAX_TCP_SYNCNT		127
+
+#define TCP_SYNQ_INTERVAL	(HZ/5)	/* Period of SYNACK timer */
+#define TCP_SYNQ_HSIZE		512	/* Size of SYNACK hash table */
+
+#define TCP_PAWS_24DAYS	(60 * 60 * 24 * 24)
+#define TCP_PAWS_MSL	60		/* Per-host timestamps are invalidated
+					 * after this time. It should be equal
+					 * (or greater than) TCP_TIMEWAIT_LEN
+					 * to provide reliability equal to one
+					 * provided by timewait state.
+					 */
+#define TCP_PAWS_WINDOW	1		/* Replay window for per-host
+					 * timestamps. It must be less than
+					 * minimal timewait lifetime.
+					 */
+
+#define TCP_TW_RECYCLE_SLOTS_LOG	5
+#define TCP_TW_RECYCLE_SLOTS		(1<<TCP_TW_RECYCLE_SLOTS_LOG)
+
+/* If time > 4sec, it is "slow" path, no recycling is required,
+   so that we select tick to get range about 4 seconds.
+ */
+
+#if HZ <= 16 || HZ > 4096
+# error Unsupported: HZ <= 16 or HZ > 4096
+#elif HZ <= 32
+# define TCP_TW_RECYCLE_TICK (5+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 64
+# define TCP_TW_RECYCLE_TICK (6+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 128
+# define TCP_TW_RECYCLE_TICK (7+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 256
+# define TCP_TW_RECYCLE_TICK (8+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 512
+# define TCP_TW_RECYCLE_TICK (9+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 1024
+# define TCP_TW_RECYCLE_TICK (10+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#elif HZ <= 2048
+# define TCP_TW_RECYCLE_TICK (11+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#else
+# define TCP_TW_RECYCLE_TICK (12+2-TCP_TW_RECYCLE_SLOTS_LOG)
+#endif
+
+#define BICTCP_BETA_SCALE    1024	/* Scale factor beta calculation
+					 * max_cwnd = snd_cwnd * beta
+					 */
+#define BICTCP_MAX_INCREMENT 32		/*
+					 * Limit on the amount of
+					 * increment allowed during
+					 * binary search.
+					 */
+#define BICTCP_FUNC_OF_MIN_INCR 11	/*
+					 * log(B/Smin)/log(B/(B-1))+1,
+					 * Smin:min increment
+					 * B:log factor
+					 */
+#define BICTCP_B		4	 /*
+					  * In binary search,
+					  * go to point (max+min)/N
+					  */
+
+/*
+ *	TCP option
+ */
+ 
+#define TCPOPT_NOP		1	/* Padding */
+#define TCPOPT_EOL		0	/* End of options */
+#define TCPOPT_MSS		2	/* Segment size negotiating */
+#define TCPOPT_WINDOW		3	/* Window scaling */
+#define TCPOPT_SACK_PERM        4       /* SACK Permitted */
+#define TCPOPT_SACK             5       /* SACK Block */
+#define TCPOPT_TIMESTAMP	8	/* Better RTT estimations/PAWS */
+
+/*
+ *     TCP option lengths
+ */
+
+#define TCPOLEN_MSS            4
+#define TCPOLEN_WINDOW         3
+#define TCPOLEN_SACK_PERM      2
+#define TCPOLEN_TIMESTAMP      10
+
+/* But this is what stacks really send out. */
+#define TCPOLEN_TSTAMP_ALIGNED		12
+#define TCPOLEN_WSCALE_ALIGNED		4
+#define TCPOLEN_SACKPERM_ALIGNED	4
+#define TCPOLEN_SACK_BASE		2
+#define TCPOLEN_SACK_BASE_ALIGNED	4
+#define TCPOLEN_SACK_PERBLOCK		8
+
+#define TCP_TIME_RETRANS	1	/* Retransmit timer */
+#define TCP_TIME_DACK		2	/* Delayed ack timer */
+#define TCP_TIME_PROBE0		3	/* Zero window probe timer */
+#define TCP_TIME_KEEPOPEN	4	/* Keepalive timer */
+
+/* Flags in tp->nonagle */
+#define TCP_NAGLE_OFF		1	/* Nagle's algo is disabled */
+#define TCP_NAGLE_CORK		2	/* Socket is corked	    */
+#define TCP_NAGLE_PUSH		4	/* Cork is overriden for already queued data */
+
+/* sysctl variables for tcp */
+extern int sysctl_max_syn_backlog;
+extern int sysctl_tcp_timestamps;
+extern int sysctl_tcp_window_scaling;
+extern int sysctl_tcp_sack;
+extern int sysctl_tcp_fin_timeout;
+extern int sysctl_tcp_tw_recycle;
+extern int sysctl_tcp_keepalive_time;
+extern int sysctl_tcp_keepalive_probes;
+extern int sysctl_tcp_keepalive_intvl;
+extern int sysctl_tcp_syn_retries;
+extern int sysctl_tcp_synack_retries;
+extern int sysctl_tcp_retries1;
+extern int sysctl_tcp_retries2;
+extern int sysctl_tcp_orphan_retries;
+extern int sysctl_tcp_syncookies;
+extern int sysctl_tcp_retrans_collapse;
+extern int sysctl_tcp_stdurg;
+extern int sysctl_tcp_rfc1337;
+extern int sysctl_tcp_abort_on_overflow;
+extern int sysctl_tcp_max_orphans;
+extern int sysctl_tcp_max_tw_buckets;
+extern int sysctl_tcp_fack;
+extern int sysctl_tcp_reordering;
+extern int sysctl_tcp_ecn;
+extern int sysctl_tcp_dsack;
+extern int sysctl_tcp_mem[3];
+extern int sysctl_tcp_wmem[3];
+extern int sysctl_tcp_rmem[3];
+extern int sysctl_tcp_app_win;
+extern int sysctl_tcp_adv_win_scale;
+extern int sysctl_tcp_tw_reuse;
+extern int sysctl_tcp_frto;
+extern int sysctl_tcp_low_latency;
+extern int sysctl_tcp_westwood;
+extern int sysctl_tcp_vegas_cong_avoid;
+extern int sysctl_tcp_vegas_alpha;
+extern int sysctl_tcp_vegas_beta;
+extern int sysctl_tcp_vegas_gamma;
+extern int sysctl_tcp_nometrics_save;
+extern int sysctl_tcp_bic;
+extern int sysctl_tcp_bic_fast_convergence;
+extern int sysctl_tcp_bic_low_window;
+extern int sysctl_tcp_bic_beta;
+extern int sysctl_tcp_moderate_rcvbuf;
+extern int sysctl_tcp_tso_win_divisor;
+
+extern atomic_t tcp_memory_allocated;
+extern atomic_t tcp_sockets_allocated;
+extern int tcp_memory_pressure;
+
+struct open_request;
+
+struct or_calltable {
+	int  family;
+	int  (*rtx_syn_ack)	(struct sock *sk, struct open_request *req, struct dst_entry*);
+	void (*send_ack)	(struct sk_buff *skb, struct open_request *req);
+	void (*destructor)	(struct open_request *req);
+	void (*send_reset)	(struct sk_buff *skb);
+};
+
+struct tcp_v4_open_req {
+	__u32			loc_addr;
+	__u32			rmt_addr;
+	struct ip_options	*opt;
+};
+
+#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
+struct tcp_v6_open_req {
+	struct in6_addr		loc_addr;
+	struct in6_addr		rmt_addr;
+	struct sk_buff		*pktopts;
+	int			iif;
+};
+#endif
+
+/* this structure is too big */
+struct open_request {
+	struct open_request	*dl_next; /* Must be first member! */
+	__u32			rcv_isn;
+	__u32			snt_isn;
+	__u16			rmt_port;
+	__u16			mss;
+	__u8			retrans;
+	__u8			__pad;
+	__u16	snd_wscale : 4, 
+		rcv_wscale : 4, 
+		tstamp_ok : 1,
+		sack_ok : 1,
+		wscale_ok : 1,
+		ecn_ok : 1,
+		acked : 1;
+	/* The following two fields can be easily recomputed I think -AK */
+	__u32			window_clamp;	/* window clamp at creation time */
+	__u32			rcv_wnd;	/* rcv_wnd offered first time */
+	__u32			ts_recent;
+	unsigned long		expires;
+	struct or_calltable	*class;
+	struct sock		*sk;
+	union {
+		struct tcp_v4_open_req v4_req;
+#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
+		struct tcp_v6_open_req v6_req;
+#endif
+	} af;
+};
+
+/* SLAB cache for open requests. */
+extern kmem_cache_t *tcp_openreq_cachep;
+
+#define tcp_openreq_alloc()		kmem_cache_alloc(tcp_openreq_cachep, SLAB_ATOMIC)
+#define tcp_openreq_fastfree(req)	kmem_cache_free(tcp_openreq_cachep, req)
+
+static inline void tcp_openreq_free(struct open_request *req)
+{
+	req->class->destructor(req);
+	tcp_openreq_fastfree(req);
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#define TCP_INET_FAMILY(fam) ((fam) == AF_INET)
+#else
+#define TCP_INET_FAMILY(fam) 1
+#endif
+
+/*
+ *	Pointers to address related TCP functions
+ *	(i.e. things that depend on the address family)
+ */
+
+struct tcp_func {
+	int			(*queue_xmit)		(struct sk_buff *skb,
+							 int ipfragok);
+
+	void			(*send_check)		(struct sock *sk,
+							 struct tcphdr *th,
+							 int len,
+							 struct sk_buff *skb);
+
+	int			(*rebuild_header)	(struct sock *sk);
+
+	int			(*conn_request)		(struct sock *sk,
+							 struct sk_buff *skb);
+
+	struct sock *		(*syn_recv_sock)	(struct sock *sk,
+							 struct sk_buff *skb,
+							 struct open_request *req,
+							 struct dst_entry *dst);
+    
+	int			(*remember_stamp)	(struct sock *sk);
+
+	__u16			net_header_len;
+
+	int			(*setsockopt)		(struct sock *sk, 
+							 int level, 
+							 int optname, 
+							 char __user *optval, 
+							 int optlen);
+
+	int			(*getsockopt)		(struct sock *sk, 
+							 int level, 
+							 int optname, 
+							 char __user *optval, 
+							 int __user *optlen);
+
+
+	void			(*addr2sockaddr)	(struct sock *sk,
+							 struct sockaddr *);
+
+	int sockaddr_len;
+};
+
+/*
+ * The next routines deal with comparing 32 bit unsigned ints
+ * and worry about wraparound (automatic with unsigned arithmetic).
+ */
+
+static inline int before(__u32 seq1, __u32 seq2)
+{
+        return (__s32)(seq1-seq2) < 0;
+}
+
+static inline int after(__u32 seq1, __u32 seq2)
+{
+	return (__s32)(seq2-seq1) < 0;
+}
+
+
+/* is s2<=s1<=s3 ? */
+static inline int between(__u32 seq1, __u32 seq2, __u32 seq3)
+{
+	return seq3 - seq2 >= seq1 - seq2;
+}
+
+
+extern struct proto tcp_prot;
+
+DECLARE_SNMP_STAT(struct tcp_mib, tcp_statistics);
+#define TCP_INC_STATS(field)		SNMP_INC_STATS(tcp_statistics, field)
+#define TCP_INC_STATS_BH(field)		SNMP_INC_STATS_BH(tcp_statistics, field)
+#define TCP_INC_STATS_USER(field) 	SNMP_INC_STATS_USER(tcp_statistics, field)
+#define TCP_DEC_STATS(field)		SNMP_DEC_STATS(tcp_statistics, field)
+#define TCP_ADD_STATS_BH(field, val)	SNMP_ADD_STATS_BH(tcp_statistics, field, val)
+#define TCP_ADD_STATS_USER(field, val)	SNMP_ADD_STATS_USER(tcp_statistics, field, val)
+
+extern void			tcp_put_port(struct sock *sk);
+extern void			tcp_inherit_port(struct sock *sk, struct sock *child);
+
+extern void			tcp_v4_err(struct sk_buff *skb, u32);
+
+extern void			tcp_shutdown (struct sock *sk, int how);
+
+extern int			tcp_v4_rcv(struct sk_buff *skb);
+
+extern int			tcp_v4_remember_stamp(struct sock *sk);
+
+extern int		    	tcp_v4_tw_remember_stamp(struct tcp_tw_bucket *tw);
+
+extern int			tcp_sendmsg(struct kiocb *iocb, struct sock *sk,
+					    struct msghdr *msg, size_t size);
+extern ssize_t			tcp_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags);
+
+extern int			tcp_ioctl(struct sock *sk, 
+					  int cmd, 
+					  unsigned long arg);
+
+extern int			tcp_rcv_state_process(struct sock *sk, 
+						      struct sk_buff *skb,
+						      struct tcphdr *th,
+						      unsigned len);
+
+extern int			tcp_rcv_established(struct sock *sk, 
+						    struct sk_buff *skb,
+						    struct tcphdr *th, 
+						    unsigned len);
+
+extern void			tcp_rcv_space_adjust(struct sock *sk);
+
+enum tcp_ack_state_t
+{
+	TCP_ACK_SCHED = 1,
+	TCP_ACK_TIMER = 2,
+	TCP_ACK_PUSHED= 4
+};
+
+static inline void tcp_schedule_ack(struct tcp_sock *tp)
+{
+	tp->ack.pending |= TCP_ACK_SCHED;
+}
+
+static inline int tcp_ack_scheduled(struct tcp_sock *tp)
+{
+	return tp->ack.pending&TCP_ACK_SCHED;
+}
+
+static __inline__ void tcp_dec_quickack_mode(struct tcp_sock *tp)
+{
+	if (tp->ack.quick && --tp->ack.quick == 0) {
+		/* Leaving quickack mode we deflate ATO. */
+		tp->ack.ato = TCP_ATO_MIN;
+	}
+}
+
+extern void tcp_enter_quickack_mode(struct tcp_sock *tp);
+
+static __inline__ void tcp_delack_init(struct tcp_sock *tp)
+{
+	memset(&tp->ack, 0, sizeof(tp->ack));
+}
+
+static inline void tcp_clear_options(struct tcp_options_received *rx_opt)
+{
+ 	rx_opt->tstamp_ok = rx_opt->sack_ok = rx_opt->wscale_ok = rx_opt->snd_wscale = 0;
+}
+
+enum tcp_tw_status
+{
+	TCP_TW_SUCCESS = 0,
+	TCP_TW_RST = 1,
+	TCP_TW_ACK = 2,
+	TCP_TW_SYN = 3
+};
+
+
+extern enum tcp_tw_status	tcp_timewait_state_process(struct tcp_tw_bucket *tw,
+							   struct sk_buff *skb,
+							   struct tcphdr *th,
+							   unsigned len);
+
+extern struct sock *		tcp_check_req(struct sock *sk,struct sk_buff *skb,
+					      struct open_request *req,
+					      struct open_request **prev);
+extern int			tcp_child_process(struct sock *parent,
+						  struct sock *child,
+						  struct sk_buff *skb);
+extern void			tcp_enter_frto(struct sock *sk);
+extern void			tcp_enter_loss(struct sock *sk, int how);
+extern void			tcp_clear_retrans(struct tcp_sock *tp);
+extern void			tcp_update_metrics(struct sock *sk);
+
+extern void			tcp_close(struct sock *sk, 
+					  long timeout);
+extern struct sock *		tcp_accept(struct sock *sk, int flags, int *err);
+extern unsigned int		tcp_poll(struct file * file, struct socket *sock, struct poll_table_struct *wait);
+
+extern int			tcp_getsockopt(struct sock *sk, int level, 
+					       int optname,
+					       char __user *optval, 
+					       int __user *optlen);
+extern int			tcp_setsockopt(struct sock *sk, int level, 
+					       int optname, char __user *optval, 
+					       int optlen);
+extern void			tcp_set_keepalive(struct sock *sk, int val);
+extern int			tcp_recvmsg(struct kiocb *iocb, struct sock *sk,
+					    struct msghdr *msg,
+					    size_t len, int nonblock, 
+					    int flags, int *addr_len);
+
+extern int			tcp_listen_start(struct sock *sk);
+
+extern void			tcp_parse_options(struct sk_buff *skb,
+						  struct tcp_options_received *opt_rx,
+						  int estab);
+
+/*
+ *	TCP v4 functions exported for the inet6 API
+ */
+
+extern int		       	tcp_v4_rebuild_header(struct sock *sk);
+
+extern int		       	tcp_v4_build_header(struct sock *sk, 
+						    struct sk_buff *skb);
+
+extern void		       	tcp_v4_send_check(struct sock *sk, 
+						  struct tcphdr *th, int len, 
+						  struct sk_buff *skb);
+
+extern int			tcp_v4_conn_request(struct sock *sk,
+						    struct sk_buff *skb);
+
+extern struct sock *		tcp_create_openreq_child(struct sock *sk,
+							 struct open_request *req,
+							 struct sk_buff *skb);
+
+extern struct sock *		tcp_v4_syn_recv_sock(struct sock *sk,
+						     struct sk_buff *skb,
+						     struct open_request *req,
+							struct dst_entry *dst);
+
+extern int			tcp_v4_do_rcv(struct sock *sk,
+					      struct sk_buff *skb);
+
+extern int			tcp_v4_connect(struct sock *sk,
+					       struct sockaddr *uaddr,
+					       int addr_len);
+
+extern int			tcp_connect(struct sock *sk);
+
+extern struct sk_buff *		tcp_make_synack(struct sock *sk,
+						struct dst_entry *dst,
+						struct open_request *req);
+
+extern int			tcp_disconnect(struct sock *sk, int flags);
+
+extern void			tcp_unhash(struct sock *sk);
+
+extern int			tcp_v4_hash_connecting(struct sock *sk);
+
+
+/* From syncookies.c */
+extern struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb, 
+				    struct ip_options *opt);
+extern __u32 cookie_v4_init_sequence(struct sock *sk, struct sk_buff *skb, 
+				     __u16 *mss);
+
+/* tcp_output.c */
+
+extern int tcp_write_xmit(struct sock *, int nonagle);
+extern int tcp_retransmit_skb(struct sock *, struct sk_buff *);
+extern void tcp_xmit_retransmit_queue(struct sock *);
+extern void tcp_simple_retransmit(struct sock *);
+extern int tcp_trim_head(struct sock *, struct sk_buff *, u32);
+
+extern void tcp_send_probe0(struct sock *);
+extern void tcp_send_partial(struct sock *);
+extern int  tcp_write_wakeup(struct sock *);
+extern void tcp_send_fin(struct sock *sk);
+extern void tcp_send_active_reset(struct sock *sk, int priority);
+extern int  tcp_send_synack(struct sock *);
+extern void tcp_push_one(struct sock *, unsigned mss_now);
+extern void tcp_send_ack(struct sock *sk);
+extern void tcp_send_delayed_ack(struct sock *sk);
+
+/* tcp_timer.c */
+extern void tcp_init_xmit_timers(struct sock *);
+extern void tcp_clear_xmit_timers(struct sock *);
+
+extern void tcp_delete_keepalive_timer(struct sock *);
+extern void tcp_reset_keepalive_timer(struct sock *, unsigned long);
+extern unsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);
+extern unsigned int tcp_current_mss(struct sock *sk, int large);
+
+#ifdef TCP_DEBUG
+extern const char tcp_timer_bug_msg[];
+#endif
+
+/* tcp_diag.c */
+extern void tcp_get_info(struct sock *, struct tcp_info *);
+
+/* Read 'sendfile()'-style from a TCP socket */
+typedef int (*sk_read_actor_t)(read_descriptor_t *, struct sk_buff *,
+				unsigned int, size_t);
+extern int tcp_read_sock(struct sock *sk, read_descriptor_t *desc,
+			 sk_read_actor_t recv_actor);
+
+static inline void tcp_clear_xmit_timer(struct sock *sk, int what)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	
+	switch (what) {
+	case TCP_TIME_RETRANS:
+	case TCP_TIME_PROBE0:
+		tp->pending = 0;
+
+#ifdef TCP_CLEAR_TIMERS
+		sk_stop_timer(sk, &tp->retransmit_timer);
+#endif
+		break;
+	case TCP_TIME_DACK:
+		tp->ack.blocked = 0;
+		tp->ack.pending = 0;
+
+#ifdef TCP_CLEAR_TIMERS
+		sk_stop_timer(sk, &tp->delack_timer);
+#endif
+		break;
+	default:
+#ifdef TCP_DEBUG
+		printk(tcp_timer_bug_msg);
+#endif
+		return;
+	};
+
+}
+
+/*
+ *	Reset the retransmission timer
+ */
+static inline void tcp_reset_xmit_timer(struct sock *sk, int what, unsigned long when)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (when > TCP_RTO_MAX) {
+#ifdef TCP_DEBUG
+		printk(KERN_DEBUG "reset_xmit_timer sk=%p %d when=0x%lx, caller=%p\n", sk, what, when, current_text_addr());
+#endif
+		when = TCP_RTO_MAX;
+	}
+
+	switch (what) {
+	case TCP_TIME_RETRANS:
+	case TCP_TIME_PROBE0:
+		tp->pending = what;
+		tp->timeout = jiffies+when;
+		sk_reset_timer(sk, &tp->retransmit_timer, tp->timeout);
+		break;
+
+	case TCP_TIME_DACK:
+		tp->ack.pending |= TCP_ACK_TIMER;
+		tp->ack.timeout = jiffies+when;
+		sk_reset_timer(sk, &tp->delack_timer, tp->ack.timeout);
+		break;
+
+	default:
+#ifdef TCP_DEBUG
+		printk(tcp_timer_bug_msg);
+#endif
+		return;
+	};
+}
+
+/* Initialize RCV_MSS value.
+ * RCV_MSS is an our guess about MSS used by the peer.
+ * We haven't any direct information about the MSS.
+ * It's better to underestimate the RCV_MSS rather than overestimate.
+ * Overestimations make us ACKing less frequently than needed.
+ * Underestimations are more easy to detect and fix by tcp_measure_rcv_mss().
+ */
+
+static inline void tcp_initialize_rcv_mss(struct sock *sk)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	unsigned int hint = min(tp->advmss, tp->mss_cache_std);
+
+	hint = min(hint, tp->rcv_wnd/2);
+	hint = min(hint, TCP_MIN_RCVMSS);
+	hint = max(hint, TCP_MIN_MSS);
+
+	tp->ack.rcv_mss = hint;
+}
+
+static __inline__ void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)
+{
+	tp->pred_flags = htonl((tp->tcp_header_len << 26) |
+			       ntohl(TCP_FLAG_ACK) |
+			       snd_wnd);
+}
+
+static __inline__ void tcp_fast_path_on(struct tcp_sock *tp)
+{
+	__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);
+}
+
+static inline void tcp_fast_path_check(struct sock *sk, struct tcp_sock *tp)
+{
+	if (skb_queue_len(&tp->out_of_order_queue) == 0 &&
+	    tp->rcv_wnd &&
+	    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&
+	    !tp->urg_data)
+		tcp_fast_path_on(tp);
+}
+
+/* Compute the actual receive window we are currently advertising.
+ * Rcv_nxt can be after the window if our peer push more data
+ * than the offered window.
+ */
+static __inline__ u32 tcp_receive_window(const struct tcp_sock *tp)
+{
+	s32 win = tp->rcv_wup + tp->rcv_wnd - tp->rcv_nxt;
+
+	if (win < 0)
+		win = 0;
+	return (u32) win;
+}
+
+/* Choose a new window, without checks for shrinking, and without
+ * scaling applied to the result.  The caller does these things
+ * if necessary.  This is a "raw" window selection.
+ */
+extern u32	__tcp_select_window(struct sock *sk);
+
+/* TCP timestamps are only 32-bits, this causes a slight
+ * complication on 64-bit systems since we store a snapshot
+ * of jiffies in the buffer control blocks below.  We decidely
+ * only use of the low 32-bits of jiffies and hide the ugly
+ * casts with the following macro.
+ */
+#define tcp_time_stamp		((__u32)(jiffies))
+
+/* This is what the send packet queueing engine uses to pass
+ * TCP per-packet control information to the transmission
+ * code.  We also store the host-order sequence numbers in
+ * here too.  This is 36 bytes on 32-bit architectures,
+ * 40 bytes on 64-bit machines, if this grows please adjust
+ * skbuff.h:skbuff->cb[xxx] size appropriately.
+ */
+struct tcp_skb_cb {
+	union {
+		struct inet_skb_parm	h4;
+#if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
+		struct inet6_skb_parm	h6;
+#endif
+	} header;	/* For incoming frames		*/
+	__u32		seq;		/* Starting sequence number	*/
+	__u32		end_seq;	/* SEQ + FIN + SYN + datalen	*/
+	__u32		when;		/* used to compute rtt's	*/
+	__u8		flags;		/* TCP header flags.		*/
+
+	/* NOTE: These must match up to the flags byte in a
+	 *       real TCP header.
+	 */
+#define TCPCB_FLAG_FIN		0x01
+#define TCPCB_FLAG_SYN		0x02
+#define TCPCB_FLAG_RST		0x04
+#define TCPCB_FLAG_PSH		0x08
+#define TCPCB_FLAG_ACK		0x10
+#define TCPCB_FLAG_URG		0x20
+#define TCPCB_FLAG_ECE		0x40
+#define TCPCB_FLAG_CWR		0x80
+
+	__u8		sacked;		/* State flags for SACK/FACK.	*/
+#define TCPCB_SACKED_ACKED	0x01	/* SKB ACK'd by a SACK block	*/
+#define TCPCB_SACKED_RETRANS	0x02	/* SKB retransmitted		*/
+#define TCPCB_LOST		0x04	/* SKB is lost			*/
+#define TCPCB_TAGBITS		0x07	/* All tag bits			*/
+
+#define TCPCB_EVER_RETRANS	0x80	/* Ever retransmitted frame	*/
+#define TCPCB_RETRANS		(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS)
+
+#define TCPCB_URG		0x20	/* Urgent pointer advenced here	*/
+
+#define TCPCB_AT_TAIL		(TCPCB_URG)
+
+	__u16		urg_ptr;	/* Valid w/URG flags is set.	*/
+	__u32		ack_seq;	/* Sequence number ACK'd	*/
+};
+
+#define TCP_SKB_CB(__skb)	((struct tcp_skb_cb *)&((__skb)->cb[0]))
+
+#include <net/tcp_ecn.h>
+
+/* Due to TSO, an SKB can be composed of multiple actual
+ * packets.  To keep these tracked properly, we use this.
+ */
+static inline int tcp_skb_pcount(const struct sk_buff *skb)
+{
+	return skb_shinfo(skb)->tso_segs;
+}
+
+/* This is valid iff tcp_skb_pcount() > 1. */
+static inline int tcp_skb_mss(const struct sk_buff *skb)
+{
+	return skb_shinfo(skb)->tso_size;
+}
+
+static inline void tcp_dec_pcount_approx(__u32 *count,
+					 const struct sk_buff *skb)
+{
+	if (*count) {
+		*count -= tcp_skb_pcount(skb);
+		if ((int)*count < 0)
+			*count = 0;
+	}
+}
+
+static inline void tcp_packets_out_inc(struct sock *sk, 
+				       struct tcp_sock *tp,
+				       const struct sk_buff *skb)
+{
+	int orig = tp->packets_out;
+
+	tp->packets_out += tcp_skb_pcount(skb);
+	if (!orig)
+		tcp_reset_xmit_timer(sk, TCP_TIME_RETRANS, tp->rto);
+}
+
+static inline void tcp_packets_out_dec(struct tcp_sock *tp, 
+				       const struct sk_buff *skb)
+{
+	tp->packets_out -= tcp_skb_pcount(skb);
+}
+
+/* This determines how many packets are "in the network" to the best
+ * of our knowledge.  In many cases it is conservative, but where
+ * detailed information is available from the receiver (via SACK
+ * blocks etc.) we can make more aggressive calculations.
+ *
+ * Use this for decisions involving congestion control, use just
+ * tp->packets_out to determine if the send queue is empty or not.
+ *
+ * Read this equation as:
+ *
+ *	"Packets sent once on transmission queue" MINUS
+ *	"Packets left network, but not honestly ACKed yet" PLUS
+ *	"Packets fast retransmitted"
+ */
+static __inline__ unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)
+{
+	return (tp->packets_out - tp->left_out + tp->retrans_out);
+}
+
+/*
+ * Which congestion algorithim is in use on the connection.
+ */
+#define tcp_is_vegas(__tp)	((__tp)->adv_cong == TCP_VEGAS)
+#define tcp_is_westwood(__tp)	((__tp)->adv_cong == TCP_WESTWOOD)
+#define tcp_is_bic(__tp)	((__tp)->adv_cong == TCP_BIC)
+
+/* Recalculate snd_ssthresh, we want to set it to:
+ *
+ * Reno:
+ * 	one half the current congestion window, but no
+ *	less than two segments
+ *
+ * BIC:
+ *	behave like Reno until low_window is reached,
+ *	then increase congestion window slowly
+ */
+static inline __u32 tcp_recalc_ssthresh(struct tcp_sock *tp)
+{
+	if (tcp_is_bic(tp)) {
+		if (sysctl_tcp_bic_fast_convergence &&
+		    tp->snd_cwnd < tp->bictcp.last_max_cwnd)
+			tp->bictcp.last_max_cwnd = (tp->snd_cwnd * 
+						    (BICTCP_BETA_SCALE
+						     + sysctl_tcp_bic_beta))
+				/ (2 * BICTCP_BETA_SCALE);
+		else
+			tp->bictcp.last_max_cwnd = tp->snd_cwnd;
+
+		if (tp->snd_cwnd > sysctl_tcp_bic_low_window)
+			return max((tp->snd_cwnd * sysctl_tcp_bic_beta)
+				   / BICTCP_BETA_SCALE, 2U);
+	}
+
+	return max(tp->snd_cwnd >> 1U, 2U);
+}
+
+/* Stop taking Vegas samples for now. */
+#define tcp_vegas_disable(__tp)	((__tp)->vegas.doing_vegas_now = 0)
+    
+static inline void tcp_vegas_enable(struct tcp_sock *tp)
+{
+	/* There are several situations when we must "re-start" Vegas:
+	 *
+	 *  o when a connection is established
+	 *  o after an RTO
+	 *  o after fast recovery
+	 *  o when we send a packet and there is no outstanding
+	 *    unacknowledged data (restarting an idle connection)
+	 *
+	 * In these circumstances we cannot do a Vegas calculation at the
+	 * end of the first RTT, because any calculation we do is using
+	 * stale info -- both the saved cwnd and congestion feedback are
+	 * stale.
+	 *
+	 * Instead we must wait until the completion of an RTT during
+	 * which we actually receive ACKs.
+	 */
+    
+	/* Begin taking Vegas samples next time we send something. */
+	tp->vegas.doing_vegas_now = 1;
+     
+	/* Set the beginning of the next send window. */
+	tp->vegas.beg_snd_nxt = tp->snd_nxt;
+
+	tp->vegas.cntRTT = 0;
+	tp->vegas.minRTT = 0x7fffffff;
+}
+
+/* Should we be taking Vegas samples right now? */
+#define tcp_vegas_enabled(__tp)	((__tp)->vegas.doing_vegas_now)
+
+extern void tcp_ca_init(struct tcp_sock *tp);
+
+static inline void tcp_set_ca_state(struct tcp_sock *tp, u8 ca_state)
+{
+	if (tcp_is_vegas(tp)) {
+		if (ca_state == TCP_CA_Open) 
+			tcp_vegas_enable(tp);
+		else
+			tcp_vegas_disable(tp);
+	}
+	tp->ca_state = ca_state;
+}
+
+/* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
+ * The exception is rate halving phase, when cwnd is decreasing towards
+ * ssthresh.
+ */
+static inline __u32 tcp_current_ssthresh(struct tcp_sock *tp)
+{
+	if ((1<<tp->ca_state)&(TCPF_CA_CWR|TCPF_CA_Recovery))
+		return tp->snd_ssthresh;
+	else
+		return max(tp->snd_ssthresh,
+			   ((tp->snd_cwnd >> 1) +
+			    (tp->snd_cwnd >> 2)));
+}
+
+static inline void tcp_sync_left_out(struct tcp_sock *tp)
+{
+	if (tp->rx_opt.sack_ok &&
+	    (tp->sacked_out >= tp->packets_out - tp->lost_out))
+		tp->sacked_out = tp->packets_out - tp->lost_out;
+	tp->left_out = tp->sacked_out + tp->lost_out;
+}
+
+extern void tcp_cwnd_application_limited(struct sock *sk);
+
+/* Congestion window validation. (RFC2861) */
+
+static inline void tcp_cwnd_validate(struct sock *sk, struct tcp_sock *tp)
+{
+	__u32 packets_out = tp->packets_out;
+
+	if (packets_out >= tp->snd_cwnd) {
+		/* Network is feed fully. */
+		tp->snd_cwnd_used = 0;
+		tp->snd_cwnd_stamp = tcp_time_stamp;
+	} else {
+		/* Network starves. */
+		if (tp->packets_out > tp->snd_cwnd_used)
+			tp->snd_cwnd_used = tp->packets_out;
+
+		if ((s32)(tcp_time_stamp - tp->snd_cwnd_stamp) >= tp->rto)
+			tcp_cwnd_application_limited(sk);
+	}
+}
+
+/* Set slow start threshould and cwnd not falling to slow start */
+static inline void __tcp_enter_cwr(struct tcp_sock *tp)
+{
+	tp->undo_marker = 0;
+	tp->snd_ssthresh = tcp_recalc_ssthresh(tp);
+	tp->snd_cwnd = min(tp->snd_cwnd,
+			   tcp_packets_in_flight(tp) + 1U);
+	tp->snd_cwnd_cnt = 0;
+	tp->high_seq = tp->snd_nxt;
+	tp->snd_cwnd_stamp = tcp_time_stamp;
+	TCP_ECN_queue_cwr(tp);
+}
+
+static inline void tcp_enter_cwr(struct tcp_sock *tp)
+{
+	tp->prior_ssthresh = 0;
+	if (tp->ca_state < TCP_CA_CWR) {
+		__tcp_enter_cwr(tp);
+		tcp_set_ca_state(tp, TCP_CA_CWR);
+	}
+}
+
+extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
+
+/* Slow start with delack produces 3 packets of burst, so that
+ * it is safe "de facto".
+ */
+static __inline__ __u32 tcp_max_burst(const struct tcp_sock *tp)
+{
+	return 3;
+}
+
+static __inline__ int tcp_minshall_check(const struct tcp_sock *tp)
+{
+	return after(tp->snd_sml,tp->snd_una) &&
+		!after(tp->snd_sml, tp->snd_nxt);
+}
+
+static __inline__ void tcp_minshall_update(struct tcp_sock *tp, int mss, 
+					   const struct sk_buff *skb)
+{
+	if (skb->len < mss)
+		tp->snd_sml = TCP_SKB_CB(skb)->end_seq;
+}
+
+/* Return 0, if packet can be sent now without violation Nagle's rules:
+   1. It is full sized.
+   2. Or it contains FIN.
+   3. Or TCP_NODELAY was set.
+   4. Or TCP_CORK is not set, and all sent packets are ACKed.
+      With Minshall's modification: all sent small packets are ACKed.
+ */
+
+static __inline__ int
+tcp_nagle_check(const struct tcp_sock *tp, const struct sk_buff *skb, 
+		unsigned mss_now, int nonagle)
+{
+	return (skb->len < mss_now &&
+		!(TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN) &&
+		((nonagle&TCP_NAGLE_CORK) ||
+		 (!nonagle &&
+		  tp->packets_out &&
+		  tcp_minshall_check(tp))));
+}
+
+extern void tcp_set_skb_tso_segs(struct sk_buff *, unsigned int);
+
+/* This checks if the data bearing packet SKB (usually sk->sk_send_head)
+ * should be put on the wire right now.
+ */
+static __inline__ int tcp_snd_test(const struct tcp_sock *tp, 
+				   struct sk_buff *skb,
+				   unsigned cur_mss, int nonagle)
+{
+	int pkts = tcp_skb_pcount(skb);
+
+	if (!pkts) {
+		tcp_set_skb_tso_segs(skb, tp->mss_cache_std);
+		pkts = tcp_skb_pcount(skb);
+	}
+
+	/*	RFC 1122 - section 4.2.3.4
+	 *
+	 *	We must queue if
+	 *
+	 *	a) The right edge of this frame exceeds the window
+	 *	b) There are packets in flight and we have a small segment
+	 *	   [SWS avoidance and Nagle algorithm]
+	 *	   (part of SWS is done on packetization)
+	 *	   Minshall version sounds: there are no _small_
+	 *	   segments in flight. (tcp_nagle_check)
+	 *	c) We have too many packets 'in flight'
+	 *
+	 * 	Don't use the nagle rule for urgent data (or
+	 *	for the final FIN -DaveM).
+	 *
+	 *	Also, Nagle rule does not apply to frames, which
+	 *	sit in the middle of queue (they have no chances
+	 *	to get new data) and if room at tail of skb is
+	 *	not enough to save something seriously (<32 for now).
+	 */
+
+	/* Don't be strict about the congestion window for the
+	 * final FIN frame.  -DaveM
+	 */
+	return (((nonagle&TCP_NAGLE_PUSH) || tp->urg_mode
+		 || !tcp_nagle_check(tp, skb, cur_mss, nonagle)) &&
+		(((tcp_packets_in_flight(tp) + (pkts-1)) < tp->snd_cwnd) ||
+		 (TCP_SKB_CB(skb)->flags & TCPCB_FLAG_FIN)) &&
+		!after(TCP_SKB_CB(skb)->end_seq, tp->snd_una + tp->snd_wnd));
+}
+
+static __inline__ void tcp_check_probe_timer(struct sock *sk, struct tcp_sock *tp)
+{
+	if (!tp->packets_out && !tp->pending)
+		tcp_reset_xmit_timer(sk, TCP_TIME_PROBE0, tp->rto);
+}
+
+static __inline__ int tcp_skb_is_last(const struct sock *sk, 
+				      const struct sk_buff *skb)
+{
+	return skb->next == (struct sk_buff *)&sk->sk_write_queue;
+}
+
+/* Push out any pending frames which were held back due to
+ * TCP_CORK or attempt at coalescing tiny packets.
+ * The socket must be locked by the caller.
+ */
+static __inline__ void __tcp_push_pending_frames(struct sock *sk,
+						 struct tcp_sock *tp,
+						 unsigned cur_mss,
+						 int nonagle)
+{
+	struct sk_buff *skb = sk->sk_send_head;
+
+	if (skb) {
+		if (!tcp_skb_is_last(sk, skb))
+			nonagle = TCP_NAGLE_PUSH;
+		if (!tcp_snd_test(tp, skb, cur_mss, nonagle) ||
+		    tcp_write_xmit(sk, nonagle))
+			tcp_check_probe_timer(sk, tp);
+	}
+	tcp_cwnd_validate(sk, tp);
+}
+
+static __inline__ void tcp_push_pending_frames(struct sock *sk,
+					       struct tcp_sock *tp)
+{
+	__tcp_push_pending_frames(sk, tp, tcp_current_mss(sk, 1), tp->nonagle);
+}
+
+static __inline__ int tcp_may_send_now(struct sock *sk, struct tcp_sock *tp)
+{
+	struct sk_buff *skb = sk->sk_send_head;
+
+	return (skb &&
+		tcp_snd_test(tp, skb, tcp_current_mss(sk, 1),
+			     tcp_skb_is_last(sk, skb) ? TCP_NAGLE_PUSH : tp->nonagle));
+}
+
+static __inline__ void tcp_init_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+{
+	tp->snd_wl1 = seq;
+}
+
+static __inline__ void tcp_update_wl(struct tcp_sock *tp, u32 ack, u32 seq)
+{
+	tp->snd_wl1 = seq;
+}
+
+extern void tcp_destroy_sock(struct sock *sk);
+
+
+/*
+ * Calculate(/check) TCP checksum
+ */
+static __inline__ u16 tcp_v4_check(struct tcphdr *th, int len,
+				   unsigned long saddr, unsigned long daddr, 
+				   unsigned long base)
+{
+	return csum_tcpudp_magic(saddr,daddr,len,IPPROTO_TCP,base);
+}
+
+static __inline__ int __tcp_checksum_complete(struct sk_buff *skb)
+{
+	return (unsigned short)csum_fold(skb_checksum(skb, 0, skb->len, skb->csum));
+}
+
+static __inline__ int tcp_checksum_complete(struct sk_buff *skb)
+{
+	return skb->ip_summed != CHECKSUM_UNNECESSARY &&
+		__tcp_checksum_complete(skb);
+}
+
+/* Prequeue for VJ style copy to user, combined with checksumming. */
+
+static __inline__ void tcp_prequeue_init(struct tcp_sock *tp)
+{
+	tp->ucopy.task = NULL;
+	tp->ucopy.len = 0;
+	tp->ucopy.memory = 0;
+	skb_queue_head_init(&tp->ucopy.prequeue);
+}
+
+/* Packet is added to VJ-style prequeue for processing in process
+ * context, if a reader task is waiting. Apparently, this exciting
+ * idea (VJ's mail "Re: query about TCP header on tcp-ip" of 07 Sep 93)
+ * failed somewhere. Latency? Burstiness? Well, at least now we will
+ * see, why it failed. 8)8)				  --ANK
+ *
+ * NOTE: is this not too big to inline?
+ */
+static __inline__ int tcp_prequeue(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	if (!sysctl_tcp_low_latency && tp->ucopy.task) {
+		__skb_queue_tail(&tp->ucopy.prequeue, skb);
+		tp->ucopy.memory += skb->truesize;
+		if (tp->ucopy.memory > sk->sk_rcvbuf) {
+			struct sk_buff *skb1;
+
+			BUG_ON(sock_owned_by_user(sk));
+
+			while ((skb1 = __skb_dequeue(&tp->ucopy.prequeue)) != NULL) {
+				sk->sk_backlog_rcv(sk, skb1);
+				NET_INC_STATS_BH(LINUX_MIB_TCPPREQUEUEDROPPED);
+			}
+
+			tp->ucopy.memory = 0;
+		} else if (skb_queue_len(&tp->ucopy.prequeue) == 1) {
+			wake_up_interruptible(sk->sk_sleep);
+			if (!tcp_ack_scheduled(tp))
+				tcp_reset_xmit_timer(sk, TCP_TIME_DACK, (3*TCP_RTO_MIN)/4);
+		}
+		return 1;
+	}
+	return 0;
+}
+
+
+#undef STATE_TRACE
+
+#ifdef STATE_TRACE
+static const char *statename[]={
+	"Unused","Established","Syn Sent","Syn Recv",
+	"Fin Wait 1","Fin Wait 2","Time Wait", "Close",
+	"Close Wait","Last ACK","Listen","Closing"
+};
+#endif
+
+static __inline__ void tcp_set_state(struct sock *sk, int state)
+{
+	int oldstate = sk->sk_state;
+
+	switch (state) {
+	case TCP_ESTABLISHED:
+		if (oldstate != TCP_ESTABLISHED)
+			TCP_INC_STATS(TCP_MIB_CURRESTAB);
+		break;
+
+	case TCP_CLOSE:
+		if (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)
+			TCP_INC_STATS(TCP_MIB_ESTABRESETS);
+
+		sk->sk_prot->unhash(sk);
+		if (tcp_sk(sk)->bind_hash &&
+		    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
+			tcp_put_port(sk);
+		/* fall through */
+	default:
+		if (oldstate==TCP_ESTABLISHED)
+			TCP_DEC_STATS(TCP_MIB_CURRESTAB);
+	}
+
+	/* Change state AFTER socket is unhashed to avoid closed
+	 * socket sitting in hash tables.
+	 */
+	sk->sk_state = state;
+
+#ifdef STATE_TRACE
+	SOCK_DEBUG(sk, "TCP sk=%p, State %s -> %s\n",sk, statename[oldstate],statename[state]);
+#endif	
+}
+
+static __inline__ void tcp_done(struct sock *sk)
+{
+	tcp_set_state(sk, TCP_CLOSE);
+	tcp_clear_xmit_timers(sk);
+
+	sk->sk_shutdown = SHUTDOWN_MASK;
+
+	if (!sock_flag(sk, SOCK_DEAD))
+		sk->sk_state_change(sk);
+	else
+		tcp_destroy_sock(sk);
+}
+
+static __inline__ void tcp_sack_reset(struct tcp_options_received *rx_opt)
+{
+	rx_opt->dsack = 0;
+	rx_opt->eff_sacks = 0;
+	rx_opt->num_sacks = 0;
+}
+
+static __inline__ void tcp_build_and_update_options(__u32 *ptr, struct tcp_sock *tp, __u32 tstamp)
+{
+	if (tp->rx_opt.tstamp_ok) {
+		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) |
+					  (TCPOPT_NOP << 16) |
+					  (TCPOPT_TIMESTAMP << 8) |
+					  TCPOLEN_TIMESTAMP);
+		*ptr++ = htonl(tstamp);
+		*ptr++ = htonl(tp->rx_opt.ts_recent);
+	}
+	if (tp->rx_opt.eff_sacks) {
+		struct tcp_sack_block *sp = tp->rx_opt.dsack ? tp->duplicate_sack : tp->selective_acks;
+		int this_sack;
+
+		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) |
+					  (TCPOPT_NOP << 16) |
+					  (TCPOPT_SACK << 8) |
+					  (TCPOLEN_SACK_BASE +
+					   (tp->rx_opt.eff_sacks * TCPOLEN_SACK_PERBLOCK)));
+		for(this_sack = 0; this_sack < tp->rx_opt.eff_sacks; this_sack++) {
+			*ptr++ = htonl(sp[this_sack].start_seq);
+			*ptr++ = htonl(sp[this_sack].end_seq);
+		}
+		if (tp->rx_opt.dsack) {
+			tp->rx_opt.dsack = 0;
+			tp->rx_opt.eff_sacks--;
+		}
+	}
+}
+
+/* Construct a tcp options header for a SYN or SYN_ACK packet.
+ * If this is every changed make sure to change the definition of
+ * MAX_SYN_SIZE to match the new maximum number of options that you
+ * can generate.
+ */
+static inline void tcp_syn_build_options(__u32 *ptr, int mss, int ts, int sack,
+					     int offer_wscale, int wscale, __u32 tstamp, __u32 ts_recent)
+{
+	/* We always get an MSS option.
+	 * The option bytes which will be seen in normal data
+	 * packets should timestamps be used, must be in the MSS
+	 * advertised.  But we subtract them from tp->mss_cache so
+	 * that calculations in tcp_sendmsg are simpler etc.
+	 * So account for this fact here if necessary.  If we
+	 * don't do this correctly, as a receiver we won't
+	 * recognize data packets as being full sized when we
+	 * should, and thus we won't abide by the delayed ACK
+	 * rules correctly.
+	 * SACKs don't matter, we never delay an ACK when we
+	 * have any of those going out.
+	 */
+	*ptr++ = htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | mss);
+	if (ts) {
+		if(sack)
+			*ptr++ = __constant_htonl((TCPOPT_SACK_PERM << 24) | (TCPOLEN_SACK_PERM << 16) |
+						  (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);
+		else
+			*ptr++ = __constant_htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
+						  (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP);
+		*ptr++ = htonl(tstamp);		/* TSVAL */
+		*ptr++ = htonl(ts_recent);	/* TSECR */
+	} else if(sack)
+		*ptr++ = __constant_htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |
+					  (TCPOPT_SACK_PERM << 8) | TCPOLEN_SACK_PERM);
+	if (offer_wscale)
+		*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_WINDOW << 16) | (TCPOLEN_WINDOW << 8) | (wscale));
+}
+
+/* Determine a window scaling and initial window to offer. */
+extern void tcp_select_initial_window(int __space, __u32 mss,
+				      __u32 *rcv_wnd, __u32 *window_clamp,
+				      int wscale_ok, __u8 *rcv_wscale);
+
+static inline int tcp_win_from_space(int space)
+{
+	return sysctl_tcp_adv_win_scale<=0 ?
+		(space>>(-sysctl_tcp_adv_win_scale)) :
+		space - (space>>sysctl_tcp_adv_win_scale);
+}
+
+/* Note: caller must be prepared to deal with negative returns */ 
+static inline int tcp_space(const struct sock *sk)
+{
+	return tcp_win_from_space(sk->sk_rcvbuf -
+				  atomic_read(&sk->sk_rmem_alloc));
+} 
+
+static inline int tcp_full_space(const struct sock *sk)
+{
+	return tcp_win_from_space(sk->sk_rcvbuf); 
+}
+
+static inline void tcp_acceptq_queue(struct sock *sk, struct open_request *req,
+					 struct sock *child)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+
+	req->sk = child;
+	sk_acceptq_added(sk);
+
+	if (!tp->accept_queue_tail) {
+		tp->accept_queue = req;
+	} else {
+		tp->accept_queue_tail->dl_next = req;
+	}
+	tp->accept_queue_tail = req;
+	req->dl_next = NULL;
+}
+
+struct tcp_listen_opt
+{
+	u8			max_qlen_log;	/* log_2 of maximal queued SYNs */
+	int			qlen;
+	int			qlen_young;
+	int			clock_hand;
+	u32			hash_rnd;
+	struct open_request	*syn_table[TCP_SYNQ_HSIZE];
+};
+
+static inline void
+tcp_synq_removed(struct sock *sk, struct open_request *req)
+{
+	struct tcp_listen_opt *lopt = tcp_sk(sk)->listen_opt;
+
+	if (--lopt->qlen == 0)
+		tcp_delete_keepalive_timer(sk);
+	if (req->retrans == 0)
+		lopt->qlen_young--;
+}
+
+static inline void tcp_synq_added(struct sock *sk)
+{
+	struct tcp_listen_opt *lopt = tcp_sk(sk)->listen_opt;
+
+	if (lopt->qlen++ == 0)
+		tcp_reset_keepalive_timer(sk, TCP_TIMEOUT_INIT);
+	lopt->qlen_young++;
+}
+
+static inline int tcp_synq_len(struct sock *sk)
+{
+	return tcp_sk(sk)->listen_opt->qlen;
+}
+
+static inline int tcp_synq_young(struct sock *sk)
+{
+	return tcp_sk(sk)->listen_opt->qlen_young;
+}
+
+static inline int tcp_synq_is_full(struct sock *sk)
+{
+	return tcp_synq_len(sk) >> tcp_sk(sk)->listen_opt->max_qlen_log;
+}
+
+static inline void tcp_synq_unlink(struct tcp_sock *tp, struct open_request *req,
+				       struct open_request **prev)
+{
+	write_lock(&tp->syn_wait_lock);
+	*prev = req->dl_next;
+	write_unlock(&tp->syn_wait_lock);
+}
+
+static inline void tcp_synq_drop(struct sock *sk, struct open_request *req,
+				     struct open_request **prev)
+{
+	tcp_synq_unlink(tcp_sk(sk), req, prev);
+	tcp_synq_removed(sk, req);
+	tcp_openreq_free(req);
+}
+
+static __inline__ void tcp_openreq_init(struct open_request *req,
+					struct tcp_options_received *rx_opt,
+					struct sk_buff *skb)
+{
+	req->rcv_wnd = 0;		/* So that tcp_send_synack() knows! */
+	req->rcv_isn = TCP_SKB_CB(skb)->seq;
+	req->mss = rx_opt->mss_clamp;
+	req->ts_recent = rx_opt->saw_tstamp ? rx_opt->rcv_tsval : 0;
+	req->tstamp_ok = rx_opt->tstamp_ok;
+	req->sack_ok = rx_opt->sack_ok;
+	req->snd_wscale = rx_opt->snd_wscale;
+	req->wscale_ok = rx_opt->wscale_ok;
+	req->acked = 0;
+	req->ecn_ok = 0;
+	req->rmt_port = skb->h.th->source;
+}
+
+extern void tcp_enter_memory_pressure(void);
+
+extern void tcp_listen_wlock(void);
+
+/* - We may sleep inside this lock.
+ * - If sleeping is not required (or called from BH),
+ *   use plain read_(un)lock(&tcp_lhash_lock).
+ */
+
+static inline void tcp_listen_lock(void)
+{
+	/* read_lock synchronizes to candidates to writers */
+	read_lock(&tcp_lhash_lock);
+	atomic_inc(&tcp_lhash_users);
+	read_unlock(&tcp_lhash_lock);
+}
+
+static inline void tcp_listen_unlock(void)
+{
+	if (atomic_dec_and_test(&tcp_lhash_users))
+		wake_up(&tcp_lhash_wait);
+}
+
+static inline int keepalive_intvl_when(const struct tcp_sock *tp)
+{
+	return tp->keepalive_intvl ? : sysctl_tcp_keepalive_intvl;
+}
+
+static inline int keepalive_time_when(const struct tcp_sock *tp)
+{
+	return tp->keepalive_time ? : sysctl_tcp_keepalive_time;
+}
+
+static inline int tcp_fin_time(const struct tcp_sock *tp)
+{
+	int fin_timeout = tp->linger2 ? : sysctl_tcp_fin_timeout;
+
+	if (fin_timeout < (tp->rto<<2) - (tp->rto>>1))
+		fin_timeout = (tp->rto<<2) - (tp->rto>>1);
+
+	return fin_timeout;
+}
+
+static inline int tcp_paws_check(const struct tcp_options_received *rx_opt, int rst)
+{
+	if ((s32)(rx_opt->rcv_tsval - rx_opt->ts_recent) >= 0)
+		return 0;
+	if (xtime.tv_sec >= rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)
+		return 0;
+
+	/* RST segments are not recommended to carry timestamp,
+	   and, if they do, it is recommended to ignore PAWS because
+	   "their cleanup function should take precedence over timestamps."
+	   Certainly, it is mistake. It is necessary to understand the reasons
+	   of this constraint to relax it: if peer reboots, clock may go
+	   out-of-sync and half-open connections will not be reset.
+	   Actually, the problem would be not existing if all
+	   the implementations followed draft about maintaining clock
+	   via reboots. Linux-2.2 DOES NOT!
+
+	   However, we can relax time bounds for RST segments to MSL.
+	 */
+	if (rst && xtime.tv_sec >= rx_opt->ts_recent_stamp + TCP_PAWS_MSL)
+		return 0;
+	return 1;
+}
+
+static inline void tcp_v4_setup_caps(struct sock *sk, struct dst_entry *dst)
+{
+	sk->sk_route_caps = dst->dev->features;
+	if (sk->sk_route_caps & NETIF_F_TSO) {
+		if (sock_flag(sk, SOCK_NO_LARGESEND) || dst->header_len)
+			sk->sk_route_caps &= ~NETIF_F_TSO;
+	}
+}
+
+#define TCP_CHECK_TIMER(sk) do { } while (0)
+
+static inline int tcp_use_frto(const struct sock *sk)
+{
+	const struct tcp_sock *tp = tcp_sk(sk);
+	
+	/* F-RTO must be activated in sysctl and there must be some
+	 * unsent new data, and the advertised window should allow
+	 * sending it.
+	 */
+	return (sysctl_tcp_frto && sk->sk_send_head &&
+		!after(TCP_SKB_CB(sk->sk_send_head)->end_seq,
+		       tp->snd_una + tp->snd_wnd));
+}
+
+static inline void tcp_mib_init(void)
+{
+	/* See RFC 2012 */
+	TCP_ADD_STATS_USER(TCP_MIB_RTOALGORITHM, 1);
+	TCP_ADD_STATS_USER(TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);
+	TCP_ADD_STATS_USER(TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);
+	TCP_ADD_STATS_USER(TCP_MIB_MAXCONN, -1);
+}
+
+/* /proc */
+enum tcp_seq_states {
+	TCP_SEQ_STATE_LISTENING,
+	TCP_SEQ_STATE_OPENREQ,
+	TCP_SEQ_STATE_ESTABLISHED,
+	TCP_SEQ_STATE_TIME_WAIT,
+};
+
+struct tcp_seq_afinfo {
+	struct module		*owner;
+	char			*name;
+	sa_family_t		family;
+	int			(*seq_show) (struct seq_file *m, void *v);
+	struct file_operations	*seq_fops;
+};
+
+struct tcp_iter_state {
+	sa_family_t		family;
+	enum tcp_seq_states	state;
+	struct sock		*syn_wait_sk;
+	int			bucket, sbucket, num, uid;
+	struct seq_operations	seq_ops;
+};
+
+extern int tcp_proc_register(struct tcp_seq_afinfo *afinfo);
+extern void tcp_proc_unregister(struct tcp_seq_afinfo *afinfo);
+
+/* TCP Westwood functions and constants */
+
+#define TCP_WESTWOOD_INIT_RTT  (20*HZ)           /* maybe too conservative?! */
+#define TCP_WESTWOOD_RTT_MIN   (HZ/20)           /* 50ms */
+
+static inline void tcp_westwood_update_rtt(struct tcp_sock *tp, __u32 rtt_seq)
+{
+        if (tcp_is_westwood(tp))
+                tp->westwood.rtt = rtt_seq;
+}
+
+static inline __u32 __tcp_westwood_bw_rttmin(const struct tcp_sock *tp)
+{
+        return max((tp->westwood.bw_est) * (tp->westwood.rtt_min) /
+		   (__u32) (tp->mss_cache_std),
+		   2U);
+}
+
+static inline __u32 tcp_westwood_bw_rttmin(const struct tcp_sock *tp)
+{
+	return tcp_is_westwood(tp) ? __tcp_westwood_bw_rttmin(tp) : 0;
+}
+
+static inline int tcp_westwood_ssthresh(struct tcp_sock *tp)
+{
+	__u32 ssthresh = 0;
+
+	if (tcp_is_westwood(tp)) {
+		ssthresh = __tcp_westwood_bw_rttmin(tp);
+		if (ssthresh)
+			tp->snd_ssthresh = ssthresh;  
+	}
+
+	return (ssthresh != 0);
+}
+
+static inline int tcp_westwood_cwnd(struct tcp_sock *tp)
+{
+	__u32 cwnd = 0;
+
+	if (tcp_is_westwood(tp)) {
+		cwnd = __tcp_westwood_bw_rttmin(tp);
+		if (cwnd)
+			tp->snd_cwnd = cwnd;
+	}
+
+	return (cwnd != 0);
+}
+#endif	/* _TCP_H */
