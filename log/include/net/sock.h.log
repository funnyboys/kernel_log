commit c9a368f1c0fbe2e3a21ebf231caeae58b18b2681
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jul 8 23:11:10 2020 -0700

    bpf: net: Avoid incorrect bpf_sk_reuseport_detach call
    
    bpf_sk_reuseport_detach is currently called when sk->sk_user_data
    is not NULL.  It is incorrect because sk->sk_user_data may not be
    managed by the bpf's reuseport_array.  It has been reported in [1] that,
    the bpf_sk_reuseport_detach() which is called from udp_lib_unhash() has
    corrupted the sk_user_data managed by l2tp.
    
    This patch solves it by using another bit (defined as SK_USER_DATA_BPF)
    of the sk_user_data pointer value.  It marks that a sk_user_data is
    managed/owned by BPF.
    
    The patch depends on a PTRMASK introduced in
    commit f1ff5ce2cd5e ("net, sk_msg: Clear sk_user_data pointer on clone if tagged").
    
    [ Note: sk->sk_user_data is used by bpf's reuseport_array only when a sk is
      added to the bpf's reuseport_array.
      i.e. doing setsockopt(SO_REUSEPORT) and having "sk->sk_reuseport == 1"
      alone will not stop sk->sk_user_data being used by other means. ]
    
    [1]: https://lore.kernel.org/netdev/20200706121259.GA20199@katalix.com/
    
    Fixes: 5dc4c4b7d4e8 ("bpf: Introduce BPF_MAP_TYPE_REUSEPORT_SOCKARRAY")
    Reported-by: James Chapman <jchapman@katalix.com>
    Reported-by: syzbot+9f092552ba9a5efca5df@syzkaller.appspotmail.com
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Tested-by: James Chapman <jchapman@katalix.com>
    Acked-by: James Chapman <jchapman@katalix.com>
    Link: https://lore.kernel.org/bpf/20200709061110.4019316-1-kafai@fb.com

diff --git a/include/net/sock.h b/include/net/sock.h
index 3428619faae4..1183507df95b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -533,7 +533,8 @@ enum sk_pacing {
  * be copied.
  */
 #define SK_USER_DATA_NOCOPY	1UL
-#define SK_USER_DATA_PTRMASK	~(SK_USER_DATA_NOCOPY)
+#define SK_USER_DATA_BPF	2UL	/* Managed by BPF */
+#define SK_USER_DATA_PTRMASK	~(SK_USER_DATA_NOCOPY | SK_USER_DATA_BPF)
 
 /**
  * sk_user_data_is_nocopy - Test if sk_user_data pointer must not be copied

commit 41b14fb8724d5a4b382a63cb4a1a61880347ccb8
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jun 22 23:26:04 2020 +0300

    net: Do not clear the sock TX queue in sk_set_socket()
    
    Clearing the sock TX queue in sk_set_socket() might cause unexpected
    out-of-order transmit when called from sock_orphan(), as outstanding
    packets can pick a different TX queue and bypass the ones already queued.
    
    This is undesired in general. More specifically, it breaks the in-order
    scheduling property guarantee for device-offloaded TLS sockets.
    
    Remove the call to sk_tx_queue_clear() in sk_set_socket(), and add it
    explicitly only where needed.
    
    Fixes: e022f0b4a03f ("net: Introduce sk_tx_queue_mapping")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c53cc42b5ab9..3428619faae4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1848,7 +1848,6 @@ static inline int sk_rx_queue_get(const struct sock *sk)
 
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 {
-	sk_tx_queue_clear(sk);
 	sk->sk_socket = sock;
 }
 

commit 8ea204c2b658eaef55b4716fde469fb66c589a3d
Author: Ferenc Fejes <fejes@inf.elte.hu>
Date:   Sat May 30 23:09:00 2020 +0200

    net: Make locking in sock_bindtoindex optional
    
    The sock_bindtoindex intended for kernel wide usage however
    it will lock the socket regardless of the context. This modification
    relax this behavior optionally: locking the socket will be optional
    by calling the sock_bindtoindex with lock_sk = true.
    
    The modification applied to all users of the sock_bindtoindex.
    
    Signed-off-by: Ferenc Fejes <fejes@inf.elte.hu>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/bee6355da40d9e991b2f2d12b67d55ebb5f5b207.1590871065.git.fejes@inf.elte.hu

diff --git a/include/net/sock.h b/include/net/sock.h
index 6e9f713a7860..c53cc42b5ab9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2690,7 +2690,7 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 
 void sock_def_readable(struct sock *sk);
 
-int sock_bindtoindex(struct sock *sk, int ifindex);
+int sock_bindtoindex(struct sock *sk, int ifindex, bool lock_sk);
 void sock_enable_timestamps(struct sock *sk);
 void sock_no_linger(struct sock *sk);
 void sock_set_keepalive(struct sock *sk);

commit c0425a4249e9d313eec5f81c0bde8a286ebf9a63
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri May 29 14:09:42 2020 +0200

    net: add a new bind_add method
    
    The SCTP protocol allows to bind multiple address to a socket.  That
    feature is currently only exposed as a socket option.  Add a bind_add
    method struct proto that allows to bind additional addresses, and
    switch the dlm code to use the method instead of going through the
    socket option from kernel space.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d994daa418ec..6e9f713a7860 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1156,7 +1156,9 @@ struct proto {
 	int			(*sendpage)(struct sock *sk, struct page *page,
 					int offset, size_t size, int flags);
 	int			(*bind)(struct sock *sk,
-					struct sockaddr *uaddr, int addr_len);
+					struct sockaddr *addr, int addr_len);
+	int			(*bind_add)(struct sock *sk,
+					struct sockaddr *addr, int addr_len);
 
 	int			(*backlog_rcv) (struct sock *sk,
 						struct sk_buff *skb);
@@ -2698,4 +2700,6 @@ void sock_set_reuseaddr(struct sock *sk);
 void sock_set_reuseport(struct sock *sk);
 void sock_set_sndtimeo(struct sock *sk, s64 secs);
 
+int sock_bind_add(struct sock *sk, struct sockaddr *addr, int addr_len);
+
 #endif	/* _SOCK_H */

commit fe31a326a4aadb4a3ba2b21deacc380d06802737
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:17 2020 +0200

    net: add sock_set_reuseport
    
    Add a helper to directly set the SO_REUSEPORT sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c997289aabbf..d994daa418ec 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2695,6 +2695,7 @@ void sock_set_keepalive(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_rcvbuf(struct sock *sk, int val);
 void sock_set_reuseaddr(struct sock *sk);
+void sock_set_reuseport(struct sock *sk);
 void sock_set_sndtimeo(struct sock *sk, s64 secs);
 
 #endif	/* _SOCK_H */

commit 26cfabf9cdd273650126d84a48a7f8dedbcded48
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:16 2020 +0200

    net: add sock_set_rcvbuf
    
    Add a helper to directly set the SO_RCVBUFFORCE sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index dc08c176238f..c997289aabbf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2693,6 +2693,7 @@ void sock_enable_timestamps(struct sock *sk);
 void sock_no_linger(struct sock *sk);
 void sock_set_keepalive(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
+void sock_set_rcvbuf(struct sock *sk, int val);
 void sock_set_reuseaddr(struct sock *sk);
 void sock_set_sndtimeo(struct sock *sk, s64 secs);
 

commit ce3d9544cecacd40389c399d2b7ca31acc533b70
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:15 2020 +0200

    net: add sock_set_keepalive
    
    Add a helper to directly set the SO_KEEPALIVE sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 99ef43508d2b..dc08c176238f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2691,6 +2691,7 @@ void sock_def_readable(struct sock *sk);
 int sock_bindtoindex(struct sock *sk, int ifindex);
 void sock_enable_timestamps(struct sock *sk);
 void sock_no_linger(struct sock *sk);
+void sock_set_keepalive(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_reuseaddr(struct sock *sk);
 void sock_set_sndtimeo(struct sock *sk, s64 secs);

commit 783da70e83967efeacf3c02c9dcfdc2b17bd62eb
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:14 2020 +0200

    net: add sock_enable_timestamps
    
    Add a helper to directly enable timestamps instead of setting the
    SO_TIMESTAMP* sockopts from kernel space and going through a fake
    uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cdec7bc055d5..99ef43508d2b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2689,6 +2689,7 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 void sock_def_readable(struct sock *sk);
 
 int sock_bindtoindex(struct sock *sk, int ifindex);
+void sock_enable_timestamps(struct sock *sk);
 void sock_no_linger(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_reuseaddr(struct sock *sk);

commit 7594888c782e735f8a7b110094307a4dbe7b3f03
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:13 2020 +0200

    net: add sock_bindtoindex
    
    Add a helper to directly set the SO_BINDTOIFINDEX sockopt from kernel
    space without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9a7b9e98685a..cdec7bc055d5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2688,6 +2688,7 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 
 void sock_def_readable(struct sock *sk);
 
+int sock_bindtoindex(struct sock *sk, int ifindex);
 void sock_no_linger(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_reuseaddr(struct sock *sk);

commit 76ee0785f42afbc0418072b7179d95f450d3c9a8
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:12 2020 +0200

    net: add sock_set_sndtimeo
    
    Add a helper to directly set the SO_SNDTIMEO_NEW sockopt from kernel
    space without going through a fake uaccess.  The interface is
    simplified to only pass the seconds value, as that is the only
    thing needed at the moment.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a3a43141a4be..9a7b9e98685a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2691,5 +2691,6 @@ void sock_def_readable(struct sock *sk);
 void sock_no_linger(struct sock *sk);
 void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_reuseaddr(struct sock *sk);
+void sock_set_sndtimeo(struct sock *sk, s64 secs);
 
 #endif	/* _SOCK_H */

commit 6e43496745e75ac49d644df984d2f4ee5b5b6b4e
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:11 2020 +0200

    net: add sock_set_priority
    
    Add a helper to directly set the SO_PRIORITY sockopt from kernel space
    without going through a fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6ed00bf009bb..a3a43141a4be 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2689,6 +2689,7 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 void sock_def_readable(struct sock *sk);
 
 void sock_no_linger(struct sock *sk);
+void sock_set_priority(struct sock *sk, u32 priority);
 void sock_set_reuseaddr(struct sock *sk);
 
 #endif	/* _SOCK_H */

commit c433594c07457d2b2e41a87014bfad9bec279abf
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:10 2020 +0200

    net: add sock_no_linger
    
    Add a helper to directly set the SO_LINGER sockopt from kernel space
    with onoff set to true and a linger time of 0 without going through a
    fake uaccess.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2ec085044790..6ed00bf009bb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2688,6 +2688,7 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 
 void sock_def_readable(struct sock *sk);
 
+void sock_no_linger(struct sock *sk);
 void sock_set_reuseaddr(struct sock *sk);
 
 #endif	/* _SOCK_H */

commit b58f0e8f38c0a44afa59601a115bd231f23471e1
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu May 28 07:12:09 2020 +0200

    net: add sock_set_reuseaddr
    
    Add a helper to directly set the SO_REUSEADDR sockopt from kernel space
    without going through a fake uaccess.
    
    For this the iscsi target now has to formally depend on inet to avoid
    a mostly theoretical compile failure.  For actual operation it already
    did depend on having ipv4 or ipv6 support.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3e8c6d4b4b59..2ec085044790 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2688,4 +2688,6 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 
 void sock_def_readable(struct sock *sk);
 
+void sock_set_reuseaddr(struct sock *sk);
+
 #endif	/* _SOCK_H */

commit 045065f06f938d3171b3ffacb34453421a32c1e3
Author: Lothar Rubusch <l.rubusch@gmail.com>
Date:   Tue Apr 7 22:55:25 2020 +0000

    net: sock.h: fix skb_steal_sock() kernel-doc
    
    Fix warnings related to kernel-doc notation, and wording in
    function description.
    
    Signed-off-by: Lothar Rubusch <l.rubusch@gmail.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Tested-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6d84784d33fa..3e8c6d4b4b59 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2553,9 +2553,9 @@ sk_is_refcounted(struct sock *sk)
 }
 
 /**
- * skb_steal_sock
- * @skb to steal the socket from
- * @refcounted is set to true if the socket is reference-counted
+ * skb_steal_sock - steal a socket from an sk_buff
+ * @skb: sk_buff to steal the socket from
+ * @refcounted: is set to true if the socket is reference-counted
  */
 static inline struct sock *
 skb_steal_sock(struct sk_buff *skb, bool *refcounted)

commit 7ae215d23c12a939005f35d1848ca55b6109b9c0
Author: Joe Stringer <joe@wand.net.nz>
Date:   Sun Mar 29 15:53:40 2020 -0700

    bpf: Don't refcount LISTEN sockets in sk_assign()
    
    Avoid taking a reference on listen sockets by checking the socket type
    in the sk_assign and in the corresponding skb_steal_sock() code in the
    the transport layer, and by ensuring that the prefetch free (sock_pfree)
    function uses the same logic to check whether the socket is refcounted.
    
    Suggested-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200329225342.16317-4-joe@wand.net.nz

diff --git a/include/net/sock.h b/include/net/sock.h
index f81d528845f6..6d84784d33fa 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2537,6 +2537,21 @@ skb_sk_is_prefetched(struct sk_buff *skb)
 #endif /* CONFIG_INET */
 }
 
+/* This helper checks if a socket is a full socket,
+ * ie _not_ a timewait or request socket.
+ */
+static inline bool sk_fullsock(const struct sock *sk)
+{
+	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);
+}
+
+static inline bool
+sk_is_refcounted(struct sock *sk)
+{
+	/* Only full sockets have sk->sk_flags. */
+	return !sk_fullsock(sk) || !sock_flag(sk, SOCK_RCU_FREE);
+}
+
 /**
  * skb_steal_sock
  * @skb to steal the socket from
@@ -2549,6 +2564,8 @@ skb_steal_sock(struct sk_buff *skb, bool *refcounted)
 		struct sock *sk = skb->sk;
 
 		*refcounted = true;
+		if (skb_sk_is_prefetched(skb))
+			*refcounted = sk_is_refcounted(sk);
 		skb->destructor = NULL;
 		skb->sk = NULL;
 		return sk;
@@ -2557,14 +2574,6 @@ skb_steal_sock(struct sk_buff *skb, bool *refcounted)
 	return NULL;
 }
 
-/* This helper checks if a socket is a full socket,
- * ie _not_ a timewait or request socket.
- */
-static inline bool sk_fullsock(const struct sock *sk)
-{
-	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);
-}
-
 /* Checks if this SKB belongs to an HW offloaded socket
  * and whether any SW fallbacks are required based on dev.
  * Check decrypted mark in case skb_orphan() cleared socket.

commit 71489e21d720a09388b565d60ef87ae993c10528
Author: Joe Stringer <joe@wand.net.nz>
Date:   Sun Mar 29 15:53:39 2020 -0700

    net: Track socket refcounts in skb_steal_sock()
    
    Refactor the UDP/TCP handlers slightly to allow skb_steal_sock() to make
    the determination of whether the socket is reference counted in the case
    where it is prefetched by earlier logic such as early_demux.
    
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200329225342.16317-3-joe@wand.net.nz

diff --git a/include/net/sock.h b/include/net/sock.h
index dc398cee7873..f81d528845f6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2537,15 +2537,23 @@ skb_sk_is_prefetched(struct sk_buff *skb)
 #endif /* CONFIG_INET */
 }
 
-static inline struct sock *skb_steal_sock(struct sk_buff *skb)
+/**
+ * skb_steal_sock
+ * @skb to steal the socket from
+ * @refcounted is set to true if the socket is reference-counted
+ */
+static inline struct sock *
+skb_steal_sock(struct sk_buff *skb, bool *refcounted)
 {
 	if (skb->sk) {
 		struct sock *sk = skb->sk;
 
+		*refcounted = true;
 		skb->destructor = NULL;
 		skb->sk = NULL;
 		return sk;
 	}
+	*refcounted = false;
 	return NULL;
 }
 

commit cf7fbe660f2dbd738ab58aea8e9b0ca6ad232449
Author: Joe Stringer <joe@wand.net.nz>
Date:   Sun Mar 29 15:53:38 2020 -0700

    bpf: Add socket assign support
    
    Add support for TPROXY via a new bpf helper, bpf_sk_assign().
    
    This helper requires the BPF program to discover the socket via a call
    to bpf_sk*_lookup_*(), then pass this socket to the new helper. The
    helper takes its own reference to the socket in addition to any existing
    reference that may or may not currently be obtained for the duration of
    BPF processing. For the destination socket to receive the traffic, the
    traffic must be routed towards that socket via local route. The
    simplest example route is below, but in practice you may want to route
    traffic more narrowly (eg by CIDR):
    
      $ ip route add local default dev lo
    
    This patch avoids trying to introduce an extra bit into the skb->sk, as
    that would require more invasive changes to all code interacting with
    the socket to ensure that the bit is handled correctly, such as all
    error-handling cases along the path from the helper in BPF through to
    the orphan path in the input. Instead, we opt to use the destructor
    variable to switch on the prefetch of the socket.
    
    Signed-off-by: Joe Stringer <joe@wand.net.nz>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200329225342.16317-2-joe@wand.net.nz

diff --git a/include/net/sock.h b/include/net/sock.h
index b5cca7bae69b..dc398cee7873 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1659,6 +1659,7 @@ void sock_rfree(struct sk_buff *skb);
 void sock_efree(struct sk_buff *skb);
 #ifdef CONFIG_INET
 void sock_edemux(struct sk_buff *skb);
+void sock_pfree(struct sk_buff *skb);
 #else
 #define sock_edemux sock_efree
 #endif
@@ -2526,6 +2527,16 @@ void sock_net_set(struct sock *sk, struct net *net)
 	write_pnet(&sk->sk_net, net);
 }
 
+static inline bool
+skb_sk_is_prefetched(struct sk_buff *skb)
+{
+#ifdef CONFIG_INET
+	return skb->destructor == sock_pfree;
+#else
+	return false;
+#endif /* CONFIG_INET */
+}
+
 static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 {
 	if (skb->sk) {

commit b105e8e281ac2dbea4229982ad57fbefab05963d
Merge: e65ee2fb54d4 eb1e1478b6f4
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 21 15:22:45 2020 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2020-02-21
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 25 non-merge commits during the last 4 day(s) which contain
    a total of 33 files changed, 2433 insertions(+), 161 deletions(-).
    
    The main changes are:
    
    1) Allow for adding TCP listen sockets into sock_map/hash so they can be used
       with reuseport BPF programs, from Jakub Sitnicki.
    
    2) Add a new bpf_program__set_attach_target() helper for adding libbpf support
       to specify the tracepoint/function dynamically, from Eelco Chaudron.
    
    3) Add bpf_read_branch_records() BPF helper which helps use cases like profile
       guided optimizations, from Daniel Xu.
    
    4) Enable bpf_perf_event_read_value() in all tracing programs, from Song Liu.
    
    5) Relax BTF mandatory check if only used for libbpf itself e.g. to process
       BTF defined maps, from Andrii Nakryiko.
    
    6) Move BPF selftests -mcpu compilation attribute from 'probe' to 'v3' as it has
       been observed that former fails in envs with low memlock, from Yonghong Song.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f1ff5ce2cd5ef3335f19c0f6576582c87045b04f
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Tue Feb 18 17:10:14 2020 +0000

    net, sk_msg: Clear sk_user_data pointer on clone if tagged
    
    sk_user_data can hold a pointer to an object that is not intended to be
    shared between the parent socket and the child that gets a pointer copy on
    clone. This is the case when sk_user_data points at reference-counted
    object, like struct sk_psock.
    
    One way to resolve it is to tag the pointer with a no-copy flag by
    repurposing its lowest bit. Based on the bit-flag value we clear the child
    sk_user_data pointer after cloning the parent socket.
    
    The no-copy flag is stored in the pointer itself as opposed to externally,
    say in socket flags, to guarantee that the pointer and the flag are copied
    from parent to child socket in an atomic fashion. Parent socket state is
    subject to change while copying, we don't hold any locks at that time.
    
    This approach relies on an assumption that sk_user_data holds a pointer to
    an object aligned at least 2 bytes. A manual audit of existing users of
    rcu_dereference_sk_user_data helper confirms our assumption.
    
    Also, an RCU-protected sk_user_data is not likely to hold a pointer to a
    char value or a pathological case of "struct { char c; }". To be safe, warn
    when the flag-bit is set when setting sk_user_data to catch any future
    misuses.
    
    It is worth considering why clearing sk_user_data unconditionally is not an
    option. There exist users, DRBD, NVMe, and Xen drivers being among them,
    that rely on the pointer being copied when cloning the listening socket.
    
    Potentially we could distinguish these users by checking if the listening
    socket has been created in kernel-space via sock_create_kern, and hence has
    sk_kern_sock flag set. However, this is not the case for NVMe and Xen
    drivers, which create sockets without marking them as belonging to the
    kernel.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200218171023.844439-3-jakub@cloudflare.com

diff --git a/include/net/sock.h b/include/net/sock.h
index 02162b0378f7..9f37fdfd15d4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -502,10 +502,43 @@ enum sk_pacing {
 	SK_PACING_FQ		= 2,
 };
 
+/* Pointer stored in sk_user_data might not be suitable for copying
+ * when cloning the socket. For instance, it can point to a reference
+ * counted object. sk_user_data bottom bit is set if pointer must not
+ * be copied.
+ */
+#define SK_USER_DATA_NOCOPY	1UL
+#define SK_USER_DATA_PTRMASK	~(SK_USER_DATA_NOCOPY)
+
+/**
+ * sk_user_data_is_nocopy - Test if sk_user_data pointer must not be copied
+ * @sk: socket
+ */
+static inline bool sk_user_data_is_nocopy(const struct sock *sk)
+{
+	return ((uintptr_t)sk->sk_user_data & SK_USER_DATA_NOCOPY);
+}
+
 #define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))
 
-#define rcu_dereference_sk_user_data(sk)	rcu_dereference(__sk_user_data((sk)))
-#define rcu_assign_sk_user_data(sk, ptr)	rcu_assign_pointer(__sk_user_data((sk)), ptr)
+#define rcu_dereference_sk_user_data(sk)				\
+({									\
+	void *__tmp = rcu_dereference(__sk_user_data((sk)));		\
+	(void *)((uintptr_t)__tmp & SK_USER_DATA_PTRMASK);		\
+})
+#define rcu_assign_sk_user_data(sk, ptr)				\
+({									\
+	uintptr_t __tmp = (uintptr_t)(ptr);				\
+	WARN_ON_ONCE(__tmp & ~SK_USER_DATA_PTRMASK);			\
+	rcu_assign_pointer(__sk_user_data((sk)), __tmp);		\
+})
+#define rcu_assign_sk_user_data_nocopy(sk, ptr)				\
+({									\
+	uintptr_t __tmp = (uintptr_t)(ptr);				\
+	WARN_ON_ONCE(__tmp & ~SK_USER_DATA_PTRMASK);			\
+	rcu_assign_pointer(__sk_user_data((sk)),			\
+			   __tmp | SK_USER_DATA_NOCOPY);		\
+})
 
 /*
  * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK

commit 66256e0b15bd72e1e1c24c4cef4281a95636781c
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Sat Feb 15 11:42:37 2020 -0800

    net/sock.h: fix all kernel-doc warnings
    
    Fix all kernel-doc warnings for <net/sock.h>.
    Fixes these warnings:
    
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_addrpair' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_portpair' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_ipv6only' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_net_refcnt' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_v6_daddr' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_v6_rcv_saddr' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_cookie' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_listener' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_tw_dr' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_rcv_wnd' not described in 'sock_common'
    ../include/net/sock.h:232: warning: Function parameter or member 'skc_tw_rcv_nxt' not described in 'sock_common'
    
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_rx_skb_cache' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_wq_raw' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'tcp_rtx_queue' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_tx_skb_cache' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_route_forced_caps' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_txtime_report_errors' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_validate_xmit_skb' not described in 'sock'
    ../include/net/sock.h:498: warning: Function parameter or member 'sk_bpf_storage' not described in 'sock'
    
    ../include/net/sock.h:2024: warning: No description found for return value of 'sk_wmem_alloc_get'
    ../include/net/sock.h:2035: warning: No description found for return value of 'sk_rmem_alloc_get'
    ../include/net/sock.h:2046: warning: No description found for return value of 'sk_has_allocations'
    ../include/net/sock.h:2082: warning: No description found for return value of 'skwq_has_sleeper'
    ../include/net/sock.h:2244: warning: No description found for return value of 'sk_page_frag'
    ../include/net/sock.h:2444: warning: Function parameter or member 'tcp_rx_skb_cache_key' not described in 'DECLARE_STATIC_KEY_FALSE'
    ../include/net/sock.h:2444: warning: Excess function parameter 'sk' description in 'DECLARE_STATIC_KEY_FALSE'
    ../include/net/sock.h:2444: warning: Excess function parameter 'skb' description in 'DECLARE_STATIC_KEY_FALSE'
    
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 02162b0378f7..328564525526 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -117,19 +117,26 @@ typedef __u64 __bitwise __addrpair;
  *	struct sock_common - minimal network layer representation of sockets
  *	@skc_daddr: Foreign IPv4 addr
  *	@skc_rcv_saddr: Bound local IPv4 addr
+ *	@skc_addrpair: 8-byte-aligned __u64 union of @skc_daddr & @skc_rcv_saddr
  *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
  *	@skc_dport: placeholder for inet_dport/tw_dport
  *	@skc_num: placeholder for inet_num/tw_num
+ *	@skc_portpair: __u32 union of @skc_dport & @skc_num
  *	@skc_family: network address family
  *	@skc_state: Connection state
  *	@skc_reuse: %SO_REUSEADDR setting
  *	@skc_reuseport: %SO_REUSEPORT setting
+ *	@skc_ipv6only: socket is IPV6 only
+ *	@skc_net_refcnt: socket is using net ref counting
  *	@skc_bound_dev_if: bound device index if != 0
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
  *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
  *	@skc_prot: protocol handlers inside a network family
  *	@skc_net: reference to the network namespace of this socket
+ *	@skc_v6_daddr: IPV6 destination address
+ *	@skc_v6_rcv_saddr: IPV6 source address
+ *	@skc_cookie: socket's cookie value
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
  *	@skc_tx_queue_mapping: tx queue number for this connection
@@ -137,7 +144,15 @@ typedef __u64 __bitwise __addrpair;
  *	@skc_flags: place holder for sk_flags
  *		%SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
  *		%SO_OOBINLINE settings, %SO_TIMESTAMPING settings
+ *	@skc_listener: connection request listener socket (aka rsk_listener)
+ *		[union with @skc_flags]
+ *	@skc_tw_dr: (aka tw_dr) ptr to &struct inet_timewait_death_row
+ *		[union with @skc_flags]
  *	@skc_incoming_cpu: record/match cpu processing incoming packets
+ *	@skc_rcv_wnd: (aka rsk_rcv_wnd) TCP receive window size (possibly scaled)
+ *		[union with @skc_incoming_cpu]
+ *	@skc_tw_rcv_nxt: (aka tw_rcv_nxt) TCP window next expected seq number
+ *		[union with @skc_incoming_cpu]
  *	@skc_refcnt: reference count
  *
  *	This is the minimal network layer representation of sockets, the header
@@ -245,6 +260,7 @@ struct bpf_sk_storage;
   *	@sk_dst_cache: destination cache
   *	@sk_dst_pending_confirm: need to confirm neighbour
   *	@sk_policy: flow policy
+  *	@sk_rx_skb_cache: cache copy of recently accessed RX skb
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
   *	@sk_tsq_flags: TCP Small Queues flags
@@ -265,6 +281,8 @@ struct bpf_sk_storage;
   *	@sk_no_check_rx: allow zero checksum in RX packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
+  *	@sk_route_forced_caps: static, forced route capabilities
+  *		(set in tcp_init_sock())
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
   *	@sk_gso_max_size: Maximum GSO segment size to build
   *	@sk_gso_max_segs: Maximum number of GSO segments
@@ -303,6 +321,8 @@ struct bpf_sk_storage;
   *	@sk_frag: cached page frag
   *	@sk_peek_off: current peek_offset value
   *	@sk_send_head: front of stuff to transmit
+  *	@tcp_rtx_queue: TCP re-transmit queue [union with @sk_send_head]
+  *	@sk_tx_skb_cache: cache copy of recently accessed TX skb
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
   *	@sk_cgrp_data: cgroup data for this cgroup
@@ -313,11 +333,14 @@ struct bpf_sk_storage;
   *	@sk_write_space: callback to indicate there is bf sending space available
   *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
   *	@sk_backlog_rcv: callback to process the backlog
+  *	@sk_validate_xmit_skb: ptr to an optional validate function
   *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
   *	@sk_reuseport_cb: reuseport group container
+  *	@sk_bpf_storage: ptr to cache and control for bpf_sk_storage
   *	@sk_rcu: used during RCU grace period
   *	@sk_clockid: clockid used by time-based scheduling (SO_TXTIME)
   *	@sk_txtime_deadline_mode: set deadline mode for SO_TXTIME
+  *	@sk_txtime_report_errors: set report errors mode for SO_TXTIME
   *	@sk_txtime_unused: unused txtime flags
   */
 struct sock {
@@ -393,7 +416,9 @@ struct sock {
 	struct sk_filter __rcu	*sk_filter;
 	union {
 		struct socket_wq __rcu	*sk_wq;
+		/* private: */
 		struct socket_wq	*sk_wq_raw;
+		/* public: */
 	};
 #ifdef CONFIG_XFRM
 	struct xfrm_policy __rcu *sk_policy[2];
@@ -2017,7 +2042,7 @@ static inline int skb_copy_to_page_nocache(struct sock *sk, struct iov_iter *fro
  * sk_wmem_alloc_get - returns write allocations
  * @sk: socket
  *
- * Returns sk_wmem_alloc minus initial offset of one
+ * Return: sk_wmem_alloc minus initial offset of one
  */
 static inline int sk_wmem_alloc_get(const struct sock *sk)
 {
@@ -2028,7 +2053,7 @@ static inline int sk_wmem_alloc_get(const struct sock *sk)
  * sk_rmem_alloc_get - returns read allocations
  * @sk: socket
  *
- * Returns sk_rmem_alloc
+ * Return: sk_rmem_alloc
  */
 static inline int sk_rmem_alloc_get(const struct sock *sk)
 {
@@ -2039,7 +2064,7 @@ static inline int sk_rmem_alloc_get(const struct sock *sk)
  * sk_has_allocations - check if allocations are outstanding
  * @sk: socket
  *
- * Returns true if socket has write or read allocations
+ * Return: true if socket has write or read allocations
  */
 static inline bool sk_has_allocations(const struct sock *sk)
 {
@@ -2050,7 +2075,7 @@ static inline bool sk_has_allocations(const struct sock *sk)
  * skwq_has_sleeper - check if there are any waiting processes
  * @wq: struct socket_wq
  *
- * Returns true if socket_wq has waiting processes
+ * Return: true if socket_wq has waiting processes
  *
  * The purpose of the skwq_has_sleeper and sock_poll_wait is to wrap the memory
  * barrier call. They were added due to the race found within the tcp code.
@@ -2238,6 +2263,9 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,
  * gfpflags_allow_blocking() isn't enough here as direct reclaim may nest
  * inside other socket operations and end up recursing into sk_page_frag()
  * while it's already in use.
+ *
+ * Return: a per task page_frag if context allows that,
+ * otherwise a per socket one.
  */
 static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
@@ -2432,6 +2460,7 @@ static inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)
 			   &skb_shinfo(skb)->tskey);
 }
 
+DECLARE_STATIC_KEY_FALSE(tcp_rx_skb_cache_key);
 /**
  * sk_eat_skb - Release a skb if it is no longer needed
  * @sk: socket to eat this skb from
@@ -2440,7 +2469,6 @@ static inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.
 */
-DECLARE_STATIC_KEY_FALSE(tcp_rx_skb_cache_key);
 static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);

commit 954b3c4397792c8614aa4aaf25030ae87ece8307
Merge: c5d19a6ecfce 85cc12f85138
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jan 23 08:10:16 2020 +0100

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf-next 2020-01-22
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 92 non-merge commits during the last 16 day(s) which contain
    a total of 320 files changed, 7532 insertions(+), 1448 deletions(-).
    
    The main changes are:
    
    1) function by function verification and program extensions from Alexei.
    
    2) massive cleanup of selftests/bpf from Toke and Andrii.
    
    3) batched bpf map operations from Brian and Yonghong.
    
    4) tcp congestion control in bpf from Martin.
    
    5) bulking for non-map xdp_redirect form Toke.
    
    6) bpf_send_signal_thread helper from Yonghong.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 43a825afc91e2b06af1e8e7422198e759c2c5e20
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jan 20 10:29:17 2020 +0100

    xsk, net: Make sock_def_readable() have external linkage
    
    XDP sockets use the default implementation of struct sock's
    sk_data_ready callback, which is sock_def_readable(). This function
    is called in the XDP socket fast-path, and involves a retpoline. By
    letting sock_def_readable() have external linkage, and being called
    directly, the retpoline can be avoided.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200120092917.13949-1-bjorn.topel@gmail.com

diff --git a/include/net/sock.h b/include/net/sock.h
index 8dff68b4c316..0891c55f1e82 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2612,4 +2612,6 @@ static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
 	return false;
 }
 
+void sock_def_readable(struct sock *sk);
+
 #endif	/* _SOCK_H */

commit e66b2f31a068dd67172008459678821a79e4ea24
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Thu Jan 9 07:59:23 2020 -0800

    tcp: clean ext on tx recycle
    
    Otherwise we will find stray/unexpected/old extensions value on next
    iteration.
    
    On tcp_write_xmit() we can end-up splitting an already queued skb in two
    parts, via tso_fragment(). The newly created skb can be allocated via
    the tx cache and an upper layer will not be aware of it, so that upper
    layer cannot set the ext properly.
    
    Resetting the ext on recycle ensures that stale data is not propagated
    in to packet headers or elsewhere.
    
    An alternative would be add an additional hook in tso_fragment() or in
    sk_stream_alloc_skb() to init the ext for upper layers that need it.
    
    Co-developed-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8766f9bc3e70..432ff73d20f3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1464,6 +1464,7 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 	sk_mem_uncharge(sk, skb->truesize);
 	if (static_branch_unlikely(&tcp_tx_skb_cache_key) &&
 	    !sk->sk_tx_skb_cache && !skb_cloned(skb)) {
+		skb_ext_reset(skb);
 		skb_zcopy_clear(skb, true);
 		sk->sk_tx_skb_cache = skb;
 		return;

commit bf9765145b856fa2e238a5b8a54453795ba30ad6
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:15 2020 -0800

    sock: Make sk_protocol a 16-bit value
    
    Match the 16-bit width of skbuff->protocol. Fills an 8-bit hole so
    sizeof(struct sock) does not change.
    
    Also take care of BPF field access for sk_type/sk_protocol. Both of them
    are now outside the bitfield, so we can use load instructions without
    further shifting/masking.
    
    v5 -> v6:
     - update eBPF accessors, too (Intel's kbuild test robot)
    v2 -> v3:
     - keep 'sk_type' 2 bytes aligned (Eric)
    v1 -> v2:
     - preserve sk_pacing_shift as bit field (Eric)
    
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: bpf@vger.kernel.org
    Co-developed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Co-developed-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 091e55428415..8766f9bc3e70 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -436,30 +436,15 @@ struct sock {
 	 * Because of non atomicity rules, all
 	 * changes are protected by socket lock.
 	 */
-	unsigned int		__sk_flags_offset[0];
-#ifdef __BIG_ENDIAN_BITFIELD
-#define SK_FL_PROTO_SHIFT  16
-#define SK_FL_PROTO_MASK   0x00ff0000
-
-#define SK_FL_TYPE_SHIFT   0
-#define SK_FL_TYPE_MASK    0x0000ffff
-#else
-#define SK_FL_PROTO_SHIFT  8
-#define SK_FL_PROTO_MASK   0x0000ff00
-
-#define SK_FL_TYPE_SHIFT   16
-#define SK_FL_TYPE_MASK    0xffff0000
-#endif
-
-	unsigned int		sk_padding : 1,
+	u8			sk_padding : 1,
 				sk_kern_sock : 1,
 				sk_no_check_tx : 1,
 				sk_no_check_rx : 1,
-				sk_userlocks : 4,
-				sk_protocol  : 8,
-				sk_type      : 16;
-	u16			sk_gso_max_segs;
+				sk_userlocks : 4;
 	u8			sk_pacing_shift;
+	u16			sk_type;
+	u16			sk_protocol;
+	u16			sk_gso_max_segs;
 	unsigned long	        sk_lingertime;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;

commit e9cdced78dc20c1592c1fb98ed064943007a46c5
Author: Mat Martineau <mathew.j.martineau@linux.intel.com>
Date:   Thu Jan 9 07:59:14 2020 -0800

    net: Make sock protocol value checks more specific
    
    SK_PROTOCOL_MAX is only used in two places, for DECNet and AX.25. The
    limits have more to do with the those protocol definitions than they do
    with the data type of sk_protocol, so remove SK_PROTOCOL_MAX and use
    U8_MAX directly.
    
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8dff68b4c316..091e55428415 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -458,7 +458,6 @@ struct sock {
 				sk_userlocks : 4,
 				sk_protocol  : 8,
 				sk_type      : 16;
-#define SK_PROTOCOL_MAX U8_MAX
 	u16			sk_gso_max_segs;
 	u8			sk_pacing_shift;
 	unsigned long	        sk_lingertime;

commit 78bac77b521b032f96077c21241cc5d5668482c5
Merge: 0dd1e3773ae8 4bfeadfc0712
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 22 09:54:33 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Pull networking fixes from David Miller:
    
     1) Several nf_flow_table_offload fixes from Pablo Neira Ayuso,
        including adding a missing ipv6 match description.
    
     2) Several heap overflow fixes in mwifiex from qize wang and Ganapathi
        Bhat.
    
     3) Fix uninit value in bond_neigh_init(), from Eric Dumazet.
    
     4) Fix non-ACPI probing of nxp-nci, from Stephan Gerhold.
    
     5) Fix use after free in tipc_disc_rcv(), from Tuong Lien.
    
     6) Enforce limit of 33 tail calls in mips and riscv JIT, from Paul
        Chaignon.
    
     7) Multicast MAC limit test is off by one in qede, from Manish Chopra.
    
     8) Fix established socket lookup race when socket goes from
        TCP_ESTABLISHED to TCP_LISTEN, because there lacks an intervening
        RCU grace period. From Eric Dumazet.
    
     9) Don't send empty SKBs from tcp_write_xmit(), also from Eric Dumazet.
    
    10) Fix active backup transition after link failure in bonding, from
        Mahesh Bandewar.
    
    11) Avoid zero sized hash table in gtp driver, from Taehee Yoo.
    
    12) Fix wrong interface passed to ->mac_link_up(), from Russell King.
    
    13) Fix DSA egress flooding settings in b53, from Florian Fainelli.
    
    14) Memory leak in gmac_setup_txqs(), from Navid Emamdoost.
    
    15) Fix double free in dpaa2-ptp code, from Ioana Ciornei.
    
    16) Reject invalid MTU values in stmmac, from Jose Abreu.
    
    17) Fix refcount leak in error path of u32 classifier, from Davide
        Caratti.
    
    18) Fix regression causing iwlwifi firmware crashes on boot, from Anders
        Kaseorg.
    
    19) Fix inverted return value logic in llc2 code, from Chan Shu Tak.
    
    20) Disable hardware GRO when XDP is attached to qede, frm Manish
        Chopra.
    
    21) Since we encode state in the low pointer bits, dst metrics must be
        at least 4 byte aligned, which is not necessarily true on m68k. Add
        annotations to fix this, from Geert Uytterhoeven.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (160 commits)
      sfc: Include XDP packet headroom in buffer step size.
      sfc: fix channel allocation with brute force
      net: dst: Force 4-byte alignment of dst_metrics
      selftests: pmtu: fix init mtu value in description
      hv_netvsc: Fix unwanted rx_table reset
      net: phy: ensure that phy IDs are correctly typed
      mod_devicetable: fix PHY module format
      qede: Disable hardware gro when xdp prog is installed
      net: ena: fix issues in setting interrupt moderation params in ethtool
      net: ena: fix default tx interrupt moderation interval
      net/smc: unregister ib devices in reboot_event
      net: stmmac: platform: Fix MDIO init for platforms without PHY
      llc2: Fix return statement of llc_stat_ev_rx_null_dsap_xid_c (and _test_c)
      net: hisilicon: Fix a BUG trigered by wrong bytes_compl
      net: dsa: ksz: use common define for tag len
      s390/qeth: don't return -ENOTSUPP to userspace
      s390/qeth: fix promiscuous mode after reset
      s390/qeth: handle error due to unsupported transport mode
      cxgb4: fix refcount init for TC-MQPRIO offload
      tc-testing: initial tdc selftests for cls_u32
      ...

commit 7c68fa2bddda6d942bd387c9ba5b4300737fd991
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Dec 16 18:51:03 2019 -0800

    net: annotate lockless accesses to sk->sk_pacing_shift
    
    sk->sk_pacing_shift can be read and written without lock
    synchronization. This patch adds annotations to
    document this fact and avoid future syzbot complains.
    
    This might also avoid unexpected false sharing
    in sk_pacing_shift_update(), as the compiler
    could remove the conditional check and always
    write over sk->sk_pacing_shift :
    
    if (sk->sk_pacing_shift != val)
            sk->sk_pacing_shift = val;
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 04c274a20620..22be668457bf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2588,9 +2588,9 @@ static inline int sk_get_rmem0(const struct sock *sk, const struct proto *proto)
  */
 static inline void sk_pacing_shift_update(struct sock *sk, int val)
 {
-	if (!sk || !sk_fullsock(sk) || sk->sk_pacing_shift == val)
+	if (!sk || !sk_fullsock(sk) || READ_ONCE(sk->sk_pacing_shift) == val)
 		return;
-	sk->sk_pacing_shift = val;
+	WRITE_ONCE(sk->sk_pacing_shift, val);
 }
 
 /* if a socket is bound to a device, check that the given device

commit 8dbd76e79a16b45b2ccb01d2f2e08dbf64e71e40
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Dec 13 18:20:41 2019 -0800

    tcp/dccp: fix possible race __inet_lookup_established()
    
    Michal Kubecek and Firo Yang did a very nice analysis of crashes
    happening in __inet_lookup_established().
    
    Since a TCP socket can go from TCP_ESTABLISH to TCP_LISTEN
    (via a close()/socket()/listen() cycle) without a RCU grace period,
    I should not have changed listeners linkage in their hash table.
    
    They must use the nulls protocol (Documentation/RCU/rculist_nulls.txt),
    so that a lookup can detect a socket in a hash list was moved in
    another one.
    
    Since we added code in commit d296ba60d8e2 ("soreuseport: Resolve
    merge conflict for v4/v6 ordering fix"), we have to add
    hlist_nulls_add_tail_rcu() helper.
    
    Fixes: 3b24d854cb35 ("tcp/dccp: do not touch listener sk_refcnt under synflood")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Michal Kubecek <mkubecek@suse.cz>
    Reported-by: Firo Yang <firo.yang@suse.com>
    Reviewed-by: Michal Kubecek <mkubecek@suse.cz>
    Link: https://lore.kernel.org/netdev/20191120083919.GH27852@unicorn.suse.cz/
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 87d54ef57f00..04c274a20620 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -722,6 +722,11 @@ static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_h
 	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
 }
 
+static inline void __sk_nulls_add_node_tail_rcu(struct sock *sk, struct hlist_nulls_head *list)
+{
+	hlist_nulls_add_tail_rcu(&sk->sk_nulls_node, list);
+}
+
 static inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	sock_hold(sk);

commit c593642c8be046915ca3a4a300243a68077cd207
Author: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
Date:   Mon Dec 9 10:31:43 2019 -0800

    treewide: Use sizeof_field() macro
    
    Replace all the occurrences of FIELD_SIZEOF() with sizeof_field() except
    at places where these are defined. Later patches will remove the unused
    definition of FIELD_SIZEOF().
    
    This patch is generated using following script:
    
    EXCLUDE_FILES="include/linux/stddef.h|include/linux/kernel.h"
    
    git grep -l -e "\bFIELD_SIZEOF\b" | while read file;
    do
    
            if [[ "$file" =~ $EXCLUDE_FILES ]]; then
                    continue
            fi
            sed -i  -e 's/\bFIELD_SIZEOF\b/sizeof_field/g' $file;
    done
    
    Signed-off-by: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
    Link: https://lore.kernel.org/r/20190924105839.110713-3-pankaj.laxminarayan.bharadiya@intel.com
    Co-developed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: David Miller <davem@davemloft.net> # for net

diff --git a/include/net/sock.h b/include/net/sock.h
index 87d54ef57f00..80f996406bba 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2305,7 +2305,7 @@ struct sock_skb_cb {
  * using skb->cb[] would keep using it directly and utilize its
  * alignement guarantee.
  */
-#define SOCK_SKB_CB_OFFSET ((FIELD_SIZEOF(struct sk_buff, cb) - \
+#define SOCK_SKB_CB_OFFSET ((sizeof_field(struct sk_buff, cb) - \
 			    sizeof(struct sock_skb_cb)))
 
 #define SOCK_SKB_CB(__skb) ((struct sock_skb_cb *)((__skb)->cb + \

commit 168829ad09ca9cdfdc664b2110d0e3569932c12d
Merge: 1ae78780eda5 500543c53a54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 16:02:40 2019 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - A comprehensive rewrite of the robust/PI futex code's exit handling
         to fix various exit races. (Thomas Gleixner et al)
    
       - Rework the generic REFCOUNT_FULL implementation using
         atomic_fetch_* operations so that the performance impact of the
         cmpxchg() loops is mitigated for common refcount operations.
    
         With these performance improvements the generic implementation of
         refcount_t should be good enough for everybody - and this got
         confirmed by performance testing, so remove ARCH_HAS_REFCOUNT and
         REFCOUNT_FULL entirely, leaving the generic implementation enabled
         unconditionally. (Will Deacon)
    
       - Other misc changes, fixes, cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      lkdtm: Remove references to CONFIG_REFCOUNT_FULL
      locking/refcount: Remove unused 'refcount_error_report()' function
      locking/refcount: Consolidate implementations of refcount_t
      locking/refcount: Consolidate REFCOUNT_{MAX,SATURATED} definitions
      locking/refcount: Move saturation warnings out of line
      locking/refcount: Improve performance of generic REFCOUNT_FULL code
      locking/refcount: Move the bulk of the REFCOUNT_FULL implementation into the <linux/refcount.h> header
      locking/refcount: Remove unused refcount_*_checked() variants
      locking/refcount: Ensure integer operands are treated as signed
      locking/refcount: Define constants for saturation and max refcount values
      futex: Prevent exit livelock
      futex: Provide distinct return value when owner is exiting
      futex: Add mutex around futex exit
      futex: Provide state handling for exec() as well
      futex: Sanitize exit state handling
      futex: Mark the begin of futex exit explicitly
      futex: Set task::futex_state to DEAD right after handling futex exit
      futex: Split futex_mm_release() for exit/exec
      exit/exec: Seperate mm_release()
      futex: Replace PF_EXITPIDONE with a state
      ...

commit 14684b93019a2d2ece0df5acaf921924541b928d
Merge: 92da362c07d4 0058b0a506e4
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 9 11:04:37 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    One conflict in the BPF samples Makefile, some fixes in 'net' whilst
    we were converting over to Makefile.target rules in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9ed498c6280a2f2b51d02df96df53037272ede49
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Nov 6 10:04:11 2019 -0800

    net: silence data-races on sk_backlog.tail
    
    sk->sk_backlog.tail might be read without holding the socket spinlock,
    we need to add proper READ_ONCE()/WRITE_ONCE() to silence the warnings.
    
    KCSAN reported :
    
    BUG: KCSAN: data-race in tcp_add_backlog / tcp_recvmsg
    
    write to 0xffff8881265109f8 of 8 bytes by interrupt on cpu 1:
     __sk_add_backlog include/net/sock.h:907 [inline]
     sk_add_backlog include/net/sock.h:938 [inline]
     tcp_add_backlog+0x476/0xce0 net/ipv4/tcp_ipv4.c:1759
     tcp_v4_rcv+0x1a70/0x1bd0 net/ipv4/tcp_ipv4.c:1947
     ip_protocol_deliver_rcu+0x4d/0x420 net/ipv4/ip_input.c:204
     ip_local_deliver_finish+0x110/0x140 net/ipv4/ip_input.c:231
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_local_deliver+0x133/0x210 net/ipv4/ip_input.c:252
     dst_input include/net/dst.h:442 [inline]
     ip_rcv_finish+0x121/0x160 net/ipv4/ip_input.c:413
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_rcv+0x18f/0x1a0 net/ipv4/ip_input.c:523
     __netif_receive_skb_one_core+0xa7/0xe0 net/core/dev.c:4929
     __netif_receive_skb+0x37/0xf0 net/core/dev.c:5043
     netif_receive_skb_internal+0x59/0x190 net/core/dev.c:5133
     napi_skb_finish net/core/dev.c:5596 [inline]
     napi_gro_receive+0x28f/0x330 net/core/dev.c:5629
     receive_buf+0x284/0x30b0 drivers/net/virtio_net.c:1061
     virtnet_receive drivers/net/virtio_net.c:1323 [inline]
     virtnet_poll+0x436/0x7d0 drivers/net/virtio_net.c:1428
     napi_poll net/core/dev.c:6311 [inline]
     net_rx_action+0x3ae/0xa90 net/core/dev.c:6379
     __do_softirq+0x115/0x33f kernel/softirq.c:292
     invoke_softirq kernel/softirq.c:373 [inline]
     irq_exit+0xbb/0xe0 kernel/softirq.c:413
     exiting_irq arch/x86/include/asm/apic.h:536 [inline]
     do_IRQ+0xa6/0x180 arch/x86/kernel/irq.c:263
     ret_from_intr+0x0/0x19
     native_safe_halt+0xe/0x10 arch/x86/kernel/paravirt.c:71
     arch_cpu_idle+0x1f/0x30 arch/x86/kernel/process.c:571
     default_idle_call+0x1e/0x40 kernel/sched/idle.c:94
     cpuidle_idle_call kernel/sched/idle.c:154 [inline]
     do_idle+0x1af/0x280 kernel/sched/idle.c:263
     cpu_startup_entry+0x1b/0x20 kernel/sched/idle.c:355
     start_secondary+0x208/0x260 arch/x86/kernel/smpboot.c:264
     secondary_startup_64+0xa4/0xb0 arch/x86/kernel/head_64.S:241
    
    read to 0xffff8881265109f8 of 8 bytes by task 8057 on cpu 0:
     tcp_recvmsg+0x46e/0x1b40 net/ipv4/tcp.c:2050
     inet_recvmsg+0xbb/0x250 net/ipv4/af_inet.c:838
     sock_recvmsg_nosec net/socket.c:871 [inline]
     sock_recvmsg net/socket.c:889 [inline]
     sock_recvmsg+0x92/0xb0 net/socket.c:885
     sock_read_iter+0x15f/0x1e0 net/socket.c:967
     call_read_iter include/linux/fs.h:1889 [inline]
     new_sync_read+0x389/0x4f0 fs/read_write.c:414
     __vfs_read+0xb1/0xc0 fs/read_write.c:427
     vfs_read fs/read_write.c:461 [inline]
     vfs_read+0x143/0x2c0 fs/read_write.c:446
     ksys_read+0xd5/0x1b0 fs/read_write.c:587
     __do_sys_read fs/read_write.c:597 [inline]
     __se_sys_read fs/read_write.c:595 [inline]
     __x64_sys_read+0x4c/0x60 fs/read_write.c:595
     do_syscall_64+0xcc/0x370 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Reported by Kernel Concurrency Sanitizer on:
    CPU: 0 PID: 8057 Comm: syz-fuzzer Not tainted 5.4.0-rc6+ #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d4d3ef5ba049..bd210c78dc9d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -899,11 +899,11 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb_dst_force(skb);
 
 	if (!sk->sk_backlog.tail)
-		sk->sk_backlog.head = skb;
+		WRITE_ONCE(sk->sk_backlog.head, skb);
 	else
 		sk->sk_backlog.tail->next = skb;
 
-	sk->sk_backlog.tail = skb;
+	WRITE_ONCE(sk->sk_backlog.tail, skb);
 	skb->next = NULL;
 }
 

commit 099ecf59f05b5f30f42ebac0ab8cb94f9b18c90c
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 5 14:11:54 2019 -0800

    net: annotate lockless accesses to sk->sk_max_ack_backlog
    
    sk->sk_max_ack_backlog can be read without any lock being held
    at least in TCP/DCCP cases.
    
    We need to use READ_ONCE()/WRITE_ONCE() to avoid load/store tearing
    and/or potential KCSAN warnings.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a126784aa7d9..d4d3ef5ba049 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -869,7 +869,7 @@ static inline void sk_acceptq_added(struct sock *sk)
 
 static inline bool sk_acceptq_is_full(const struct sock *sk)
 {
-	return READ_ONCE(sk->sk_ack_backlog) > sk->sk_max_ack_backlog;
+	return READ_ONCE(sk->sk_ack_backlog) > READ_ONCE(sk->sk_max_ack_backlog);
 }
 
 /*

commit 288efe8606b62d0753ba6722b36ef241877251fd
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 5 14:11:53 2019 -0800

    net: annotate lockless accesses to sk->sk_ack_backlog
    
    sk->sk_ack_backlog can be read without any lock being held.
    We need to use READ_ONCE()/WRITE_ONCE() to avoid load/store tearing
    and/or potential KCSAN warnings.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f2f853439b65..a126784aa7d9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -859,17 +859,17 @@ static inline gfp_t sk_gfp_mask(const struct sock *sk, gfp_t gfp_mask)
 
 static inline void sk_acceptq_removed(struct sock *sk)
 {
-	sk->sk_ack_backlog--;
+	WRITE_ONCE(sk->sk_ack_backlog, sk->sk_ack_backlog - 1);
 }
 
 static inline void sk_acceptq_added(struct sock *sk)
 {
-	sk->sk_ack_backlog++;
+	WRITE_ONCE(sk->sk_ack_backlog, sk->sk_ack_backlog + 1);
 }
 
 static inline bool sk_acceptq_is_full(const struct sock *sk)
 {
-	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
+	return READ_ONCE(sk->sk_ack_backlog) > sk->sk_max_ack_backlog;
 }
 
 /*

commit 25c7a6d1f90e208ec27ca854b1381ed39842ec57
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 5 14:11:51 2019 -0800

    net: avoid potential false sharing in neighbor related code
    
    There are common instances of the following construct :
    
            if (n->confirmed != now)
                    n->confirmed = now;
    
    A C compiler could legally remove the conditional.
    
    Use READ_ONCE()/WRITE_ONCE() to avoid this problem.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ac6042d0af32..f2f853439b65 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1939,8 +1939,8 @@ struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
 static inline void sk_dst_confirm(struct sock *sk)
 {
-	if (!sk->sk_dst_pending_confirm)
-		sk->sk_dst_pending_confirm = 1;
+	if (!READ_ONCE(sk->sk_dst_pending_confirm))
+		WRITE_ONCE(sk->sk_dst_pending_confirm, 1);
 }
 
 static inline void sock_confirm_neigh(struct sk_buff *skb, struct neighbour *n)
@@ -1950,10 +1950,10 @@ static inline void sock_confirm_neigh(struct sk_buff *skb, struct neighbour *n)
 		unsigned long now = jiffies;
 
 		/* avoid dirtying neighbour */
-		if (n->confirmed != now)
-			n->confirmed = now;
-		if (sk && sk->sk_dst_pending_confirm)
-			sk->sk_dst_pending_confirm = 0;
+		if (READ_ONCE(n->confirmed) != now)
+			WRITE_ONCE(n->confirmed, now);
+		if (sk && READ_ONCE(sk->sk_dst_pending_confirm))
+			WRITE_ONCE(sk->sk_dst_pending_confirm, 0);
 	}
 }
 

commit f75359f3ac855940c5718af10ba089b8977bf339
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Nov 4 21:38:43 2019 -0800

    net: prevent load/store tearing on sk->sk_stamp
    
    Add a couple of READ_ONCE() and WRITE_ONCE() to prevent
    load-tearing and store-tearing in sock_read_timestamp()
    and sock_write_timestamp()
    
    This might prevent another KCSAN report.
    
    Fixes: 3a0ed3e96197 ("sock: Make sock->sk_stamp thread-safe")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Deepa Dinamani <deepa.kernel@gmail.com>
    Acked-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8f9adcfac41b..718e62fbe869 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2342,7 +2342,7 @@ static inline ktime_t sock_read_timestamp(struct sock *sk)
 
 	return kt;
 #else
-	return sk->sk_stamp;
+	return READ_ONCE(sk->sk_stamp);
 #endif
 }
 
@@ -2353,7 +2353,7 @@ static inline void sock_write_timestamp(struct sock *sk, ktime_t kt)
 	sk->sk_stamp = kt;
 	write_sequnlock(&sk->sk_stamp_seq);
 #else
-	sk->sk_stamp = kt;
+	WRITE_ONCE(sk->sk_stamp, kt);
 #endif
 }
 

commit d31e95585ca697fb31440c6fe30113adc85ecfbd
Merge: c23fcbbc6aa4 1204c70d9dcb
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 2 13:12:51 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    The only slightly tricky merge conflict was the netdevsim because the
    mutex locking fix overlapped a lot of driver reload reorganization.
    
    The rest were (relatively) trivial in nature.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7170a977743b72cf3eb46ef6ef89885dc7ad3621
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 30 13:00:04 2019 -0700

    net: annotate accesses to sk->sk_incoming_cpu
    
    This socket field can be read and written by concurrent cpus.
    
    Use READ_ONCE() and WRITE_ONCE() annotations to document this,
    and avoid some compiler 'optimizations'.
    
    KCSAN reported :
    
    BUG: KCSAN: data-race in tcp_v4_rcv / tcp_v4_rcv
    
    write to 0xffff88812220763c of 4 bytes by interrupt on cpu 0:
     sk_incoming_cpu_update include/net/sock.h:953 [inline]
     tcp_v4_rcv+0x1b3c/0x1bb0 net/ipv4/tcp_ipv4.c:1934
     ip_protocol_deliver_rcu+0x4d/0x420 net/ipv4/ip_input.c:204
     ip_local_deliver_finish+0x110/0x140 net/ipv4/ip_input.c:231
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_local_deliver+0x133/0x210 net/ipv4/ip_input.c:252
     dst_input include/net/dst.h:442 [inline]
     ip_rcv_finish+0x121/0x160 net/ipv4/ip_input.c:413
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_rcv+0x18f/0x1a0 net/ipv4/ip_input.c:523
     __netif_receive_skb_one_core+0xa7/0xe0 net/core/dev.c:5010
     __netif_receive_skb+0x37/0xf0 net/core/dev.c:5124
     process_backlog+0x1d3/0x420 net/core/dev.c:5955
     napi_poll net/core/dev.c:6392 [inline]
     net_rx_action+0x3ae/0xa90 net/core/dev.c:6460
     __do_softirq+0x115/0x33f kernel/softirq.c:292
     do_softirq_own_stack+0x2a/0x40 arch/x86/entry/entry_64.S:1082
     do_softirq.part.0+0x6b/0x80 kernel/softirq.c:337
     do_softirq kernel/softirq.c:329 [inline]
     __local_bh_enable_ip+0x76/0x80 kernel/softirq.c:189
    
    read to 0xffff88812220763c of 4 bytes by interrupt on cpu 1:
     sk_incoming_cpu_update include/net/sock.h:952 [inline]
     tcp_v4_rcv+0x181a/0x1bb0 net/ipv4/tcp_ipv4.c:1934
     ip_protocol_deliver_rcu+0x4d/0x420 net/ipv4/ip_input.c:204
     ip_local_deliver_finish+0x110/0x140 net/ipv4/ip_input.c:231
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_local_deliver+0x133/0x210 net/ipv4/ip_input.c:252
     dst_input include/net/dst.h:442 [inline]
     ip_rcv_finish+0x121/0x160 net/ipv4/ip_input.c:413
     NF_HOOK include/linux/netfilter.h:305 [inline]
     NF_HOOK include/linux/netfilter.h:299 [inline]
     ip_rcv+0x18f/0x1a0 net/ipv4/ip_input.c:523
     __netif_receive_skb_one_core+0xa7/0xe0 net/core/dev.c:5010
     __netif_receive_skb+0x37/0xf0 net/core/dev.c:5124
     process_backlog+0x1d3/0x420 net/core/dev.c:5955
     napi_poll net/core/dev.c:6392 [inline]
     net_rx_action+0x3ae/0xa90 net/core/dev.c:6460
     __do_softirq+0x115/0x33f kernel/softirq.c:292
     run_ksoftirqd+0x46/0x60 kernel/softirq.c:603
     smpboot_thread_fn+0x37d/0x4a0 kernel/smpboot.c:165
    
    Reported by Kernel Concurrency Sanitizer on:
    CPU: 1 PID: 16 Comm: ksoftirqd/1 Not tainted 5.4.0-rc3+ #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c31a9ed86d5a..8f9adcfac41b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -954,8 +954,8 @@ static inline void sk_incoming_cpu_update(struct sock *sk)
 {
 	int cpu = raw_smp_processor_id();
 
-	if (unlikely(sk->sk_incoming_cpu != cpu))
-		sk->sk_incoming_cpu = cpu;
+	if (unlikely(READ_ONCE(sk->sk_incoming_cpu) != cpu))
+		WRITE_ONCE(sk->sk_incoming_cpu, cpu);
 }
 
 static inline void sock_rps_record_flow_hash(__u32 hash)

commit 8466a57dfbb0c9bf6db4685ed9c4144b8deec688
Author: Ursula Braun <ubraun@linux.ibm.com>
Date:   Tue Oct 29 12:43:46 2019 +0100

    net/smc: remove unneeded include for smc.h
    
    The only smc-related reference in net/sock.h is struct smc_hashinfo.
    But just its address is refered to. Thus there is no need for the
    include of net/smc.h. Remove it.
    
    Suggested-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed by: Karsten Graul <kgraul@linux.ibm.com>
    Signed-off-by: Ursula Braun <ubraun@linux.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 380312cc67a9..09c26a5ecbff 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -66,7 +66,6 @@
 #include <net/checksum.h>
 #include <net/tcp_states.h>
 #include <linux/net_tstamp.h>
-#include <net/smc.h>
 #include <net/l3mdev.h>
 
 /*

commit 20eb4f29b60286e0d6dc01d9c260b4bd383c58fb
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 24 13:50:27 2019 -0700

    net: fix sk_page_frag() recursion from memory reclaim
    
    sk_page_frag() optimizes skb_frag allocations by using per-task
    skb_frag cache when it knows it's the only user.  The condition is
    determined by seeing whether the socket allocation mask allows
    blocking - if the allocation may block, it obviously owns the task's
    context and ergo exclusively owns current->task_frag.
    
    Unfortunately, this misses recursion through memory reclaim path.
    Please take a look at the following backtrace.
    
     [2] RIP: 0010:tcp_sendmsg_locked+0xccf/0xe10
         ...
         tcp_sendmsg+0x27/0x40
         sock_sendmsg+0x30/0x40
         sock_xmit.isra.24+0xa1/0x170 [nbd]
         nbd_send_cmd+0x1d2/0x690 [nbd]
         nbd_queue_rq+0x1b5/0x3b0 [nbd]
         __blk_mq_try_issue_directly+0x108/0x1b0
         blk_mq_request_issue_directly+0xbd/0xe0
         blk_mq_try_issue_list_directly+0x41/0xb0
         blk_mq_sched_insert_requests+0xa2/0xe0
         blk_mq_flush_plug_list+0x205/0x2a0
         blk_flush_plug_list+0xc3/0xf0
     [1] blk_finish_plug+0x21/0x2e
         _xfs_buf_ioapply+0x313/0x460
         __xfs_buf_submit+0x67/0x220
         xfs_buf_read_map+0x113/0x1a0
         xfs_trans_read_buf_map+0xbf/0x330
         xfs_btree_read_buf_block.constprop.42+0x95/0xd0
         xfs_btree_lookup_get_block+0x95/0x170
         xfs_btree_lookup+0xcc/0x470
         xfs_bmap_del_extent_real+0x254/0x9a0
         __xfs_bunmapi+0x45c/0xab0
         xfs_bunmapi+0x15/0x30
         xfs_itruncate_extents_flags+0xca/0x250
         xfs_free_eofblocks+0x181/0x1e0
         xfs_fs_destroy_inode+0xa8/0x1b0
         destroy_inode+0x38/0x70
         dispose_list+0x35/0x50
         prune_icache_sb+0x52/0x70
         super_cache_scan+0x120/0x1a0
         do_shrink_slab+0x120/0x290
         shrink_slab+0x216/0x2b0
         shrink_node+0x1b6/0x4a0
         do_try_to_free_pages+0xc6/0x370
         try_to_free_mem_cgroup_pages+0xe3/0x1e0
         try_charge+0x29e/0x790
         mem_cgroup_charge_skmem+0x6a/0x100
         __sk_mem_raise_allocated+0x18e/0x390
         __sk_mem_schedule+0x2a/0x40
     [0] tcp_sendmsg_locked+0x8eb/0xe10
         tcp_sendmsg+0x27/0x40
         sock_sendmsg+0x30/0x40
         ___sys_sendmsg+0x26d/0x2b0
         __sys_sendmsg+0x57/0xa0
         do_syscall_64+0x42/0x100
         entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    In [0], tcp_send_msg_locked() was using current->page_frag when it
    called sk_wmem_schedule().  It already calculated how many bytes can
    be fit into current->page_frag.  Due to memory pressure,
    sk_wmem_schedule() called into memory reclaim path which called into
    xfs and then IO issue path.  Because the filesystem in question is
    backed by nbd, the control goes back into the tcp layer - back into
    tcp_sendmsg_locked().
    
    nbd sets sk_allocation to (GFP_NOIO | __GFP_MEMALLOC) which makes
    sense - it's in the process of freeing memory and wants to be able to,
    e.g., drop clean pages to make forward progress.  However, this
    confused sk_page_frag() called from [2].  Because it only tests
    whether the allocation allows blocking which it does, it now thinks
    current->page_frag can be used again although it already was being
    used in [0].
    
    After [2] used current->page_frag, the offset would be increased by
    the used amount.  When the control returns to [0],
    current->page_frag's offset is increased and the previously calculated
    number of bytes now may overrun the end of allocated memory leading to
    silent memory corruptions.
    
    Fix it by adding gfpflags_normal_context() which tests sleepable &&
    !reclaim and use it to determine whether to use current->task_frag.
    
    v2: Eric didn't like gfp flags being tested twice.  Introduce a new
        helper gfpflags_normal_context() and combine the two tests.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f69b58bff7e5..c31a9ed86d5a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2242,12 +2242,17 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,
  * sk_page_frag - return an appropriate page_frag
  * @sk: socket
  *
- * If socket allocation mode allows current thread to sleep, it means its
- * safe to use the per task page_frag instead of the per socket one.
+ * Use the per task page_frag instead of the per socket one for
+ * optimization when we know that we're in the normal context and owns
+ * everything that's associated with %current.
+ *
+ * gfpflags_allow_blocking() isn't enough here as direct reclaim may nest
+ * inside other socket operations and end up recursing into sk_page_frag()
+ * while it's already in use.
  */
 static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
-	if (gfpflags_allow_blocking(sk->sk_allocation))
+	if (gfpflags_normal_context(sk->sk_allocation))
 		return &current->task_frag;
 
 	return &sk->sk_frag;

commit 2f184393e0c2d409c62262f57f2a57efdf9370b8
Merge: ebcd670d05d5 531e93d11470
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Oct 19 22:51:25 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Several cases of overlapping changes which were for the most
    part trivially resolvable.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ab4e846a82d0ae00176de19f2db3c5c64f8eb5f2
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:46 2019 -0700

    tcp: annotate sk->sk_wmem_queued lockless reads
    
    For the sake of tcp_poll(), there are few places where we fetch
    sk->sk_wmem_queued while this field can change from IRQ or other cpu.
    
    We need to add READ_ONCE() annotations, and also make sure write
    sides use corresponding WRITE_ONCE() to avoid store-tearing.
    
    sk_wmem_queued_add() helper is added so that we can in
    the future convert to ADD_ONCE() or equivalent if/when
    available.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3d1e7502333e..f69b58bff7e5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -878,12 +878,17 @@ static inline bool sk_acceptq_is_full(const struct sock *sk)
  */
 static inline int sk_stream_min_wspace(const struct sock *sk)
 {
-	return sk->sk_wmem_queued >> 1;
+	return READ_ONCE(sk->sk_wmem_queued) >> 1;
 }
 
 static inline int sk_stream_wspace(const struct sock *sk)
 {
-	return READ_ONCE(sk->sk_sndbuf) - sk->sk_wmem_queued;
+	return READ_ONCE(sk->sk_sndbuf) - READ_ONCE(sk->sk_wmem_queued);
+}
+
+static inline void sk_wmem_queued_add(struct sock *sk, int val)
+{
+	WRITE_ONCE(sk->sk_wmem_queued, sk->sk_wmem_queued + val);
 }
 
 void sk_stream_write_space(struct sock *sk);
@@ -1207,7 +1212,7 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 
 static inline bool __sk_stream_memory_free(const struct sock *sk, int wake)
 {
-	if (sk->sk_wmem_queued >= READ_ONCE(sk->sk_sndbuf))
+	if (READ_ONCE(sk->sk_wmem_queued) >= READ_ONCE(sk->sk_sndbuf))
 		return false;
 
 	return sk->sk_prot->stream_memory_free ?
@@ -1467,7 +1472,7 @@ DECLARE_STATIC_KEY_FALSE(tcp_tx_skb_cache_key);
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
-	sk->sk_wmem_queued -= skb->truesize;
+	sk_wmem_queued_add(sk, -skb->truesize);
 	sk_mem_uncharge(sk, skb->truesize);
 	if (static_branch_unlikely(&tcp_tx_skb_cache_key) &&
 	    !sk->sk_tx_skb_cache && !skb_cloned(skb)) {
@@ -2014,7 +2019,7 @@ static inline int skb_copy_to_page_nocache(struct sock *sk, struct iov_iter *fro
 	skb->len	     += copy;
 	skb->data_len	     += copy;
 	skb->truesize	     += copy;
-	sk->sk_wmem_queued   += copy;
+	sk_wmem_queued_add(sk, copy);
 	sk_mem_charge(sk, copy);
 	return 0;
 }

commit e292f05e0df73f9fcc93329663936e1ded97a988
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 10 20:17:45 2019 -0700

    tcp: annotate sk->sk_sndbuf lockless reads
    
    For the sake of tcp_poll(), there are few places where we fetch
    sk->sk_sndbuf while this field can change from IRQ or other cpu.
    
    We need to add READ_ONCE() annotations, and also make sure write
    sides use corresponding WRITE_ONCE() to avoid store-tearing.
    
    Note that other transports probably need similar fixes.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 79f54e1f8827..3d1e7502333e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -883,7 +883,7 @@ static inline int sk_stream_min_wspace(const struct sock *sk)
 
 static inline int sk_stream_wspace(const struct sock *sk)
 {
-	return sk->sk_sndbuf - sk->sk_wmem_queued;
+	return READ_ONCE(sk->sk_sndbuf) - sk->sk_wmem_queued;
 }
 
 void sk_stream_write_space(struct sock *sk);
@@ -1207,7 +1207,7 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 
 static inline bool __sk_stream_memory_free(const struct sock *sk, int wake)
 {
-	if (sk->sk_wmem_queued >= sk->sk_sndbuf)
+	if (sk->sk_wmem_queued >= READ_ONCE(sk->sk_sndbuf))
 		return false;
 
 	return sk->sk_prot->stream_memory_free ?
@@ -2220,10 +2220,14 @@ static inline void sk_wake_async(const struct sock *sk, int how, int band)
 
 static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 {
-	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
-		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
-		sk->sk_sndbuf = max_t(u32, sk->sk_sndbuf, SOCK_MIN_SNDBUF);
-	}
+	u32 val;
+
+	if (sk->sk_userlocks & SOCK_SNDBUF_LOCK)
+		return;
+
+	val = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
+
+	WRITE_ONCE(sk->sk_sndbuf, max_t(u32, val, SOCK_MIN_SNDBUF));
 }
 
 struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,
@@ -2251,7 +2255,7 @@ bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
  */
 static inline bool sock_writeable(const struct sock *sk)
 {
-	return refcount_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
+	return refcount_read(&sk->sk_wmem_alloc) < (READ_ONCE(sk->sk_sndbuf) >> 1);
 }
 
 static inline gfp_t gfp_any(void)

commit eac66402d1c342f07ff38f8d631ff95eb7ad3220
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 15:32:35 2019 -0700

    net: annotate sk->sk_rcvlowat lockless reads
    
    sock_rcvlowat() or int_sk_rcvlowat() might be called without the socket
    lock for example from tcp_poll().
    
    Use READ_ONCE() to document the fact that other cpus might change
    sk->sk_rcvlowat under us and avoid KCSAN splats.
    
    Use WRITE_ONCE() on write sides too.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c53f1a1d905..79f54e1f8827 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2271,7 +2271,9 @@ static inline long sock_sndtimeo(const struct sock *sk, bool noblock)
 
 static inline int sock_rcvlowat(const struct sock *sk, int waitall, int len)
 {
-	return (waitall ? len : min_t(int, sk->sk_rcvlowat, len)) ? : 1;
+	int v = waitall ? len : min_t(int, READ_ONCE(sk->sk_rcvlowat), len);
+
+	return v ?: 1;
 }
 
 /* Alas, with timeout socket operations are not restartable.

commit 5facae4f3549b5cf7c0e10ec312a65ffd43b5726
Author: Qian Cai <cai@lca.pw>
Date:   Thu Sep 19 12:09:40 2019 -0400

    locking/lockdep: Remove unused @nested argument from lock_release()
    
    Since the following commit:
    
      b4adfe8e05f1 ("locking/lockdep: Remove unused argument in __lock_release")
    
    @nested is no longer used in lock_release(), so remove it from all
    lock_release() calls and friends.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: alexander.levin@microsoft.com
    Cc: daniel@iogearbox.net
    Cc: davem@davemloft.net
    Cc: dri-devel@lists.freedesktop.org
    Cc: duyuyang@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: hannes@cmpxchg.org
    Cc: intel-gfx@lists.freedesktop.org
    Cc: jack@suse.com
    Cc: jlbec@evilplan.or
    Cc: joonas.lahtinen@linux.intel.com
    Cc: joseph.qi@linux.alibaba.com
    Cc: jslaby@suse.com
    Cc: juri.lelli@redhat.com
    Cc: maarten.lankhorst@linux.intel.com
    Cc: mark@fasheh.com
    Cc: mhocko@kernel.org
    Cc: mripard@kernel.org
    Cc: ocfs2-devel@oss.oracle.com
    Cc: rodrigo.vivi@intel.com
    Cc: sean@poorly.run
    Cc: st@kernel.org
    Cc: tj@kernel.org
    Cc: tytso@mit.edu
    Cc: vdavydov.dev@gmail.com
    Cc: vincent.guittot@linaro.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1568909380-32199-1-git-send-email-cai@lca.pw
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c53f1a1d905..e46db0c846d2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1484,7 +1484,7 @@ static inline void sock_release_ownership(struct sock *sk)
 		sk->sk_lock.owned = 0;
 
 		/* The sk_lock has mutex_unlock() semantics: */
-		mutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);
+		mutex_release(&sk->sk_lock.dep_map, _RET_IP_);
 	}
 }
 

commit 193d357d087309f2d5ab8e8caab1af5e3bc29fa0
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Oct 3 23:56:37 2019 +0300

    net: spread "enum sock_flags"
    
    Some ints are "enum sock_flags" in fact.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c53f1a1d905..ab905c4b1f0e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2512,7 +2512,7 @@ static inline bool sk_listener(const struct sock *sk)
 	return (1 << sk->sk_state) & (TCPF_LISTEN | TCPF_NEW_SYN_RECV);
 }
 
-void sock_enable_timestamp(struct sock *sk, int flag);
+void sock_enable_timestamp(struct sock *sk, enum sock_flags flag);
 int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len, int level,
 		       int type);
 

commit 414776621d1006e57e80e6db7fdc3837897aaa64
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Aug 7 17:03:59 2019 -0700

    net/tls: prevent skb_orphan() from leaking TLS plain text with offload
    
    sk_validate_xmit_skb() and drivers depend on the sk member of
    struct sk_buff to identify segments requiring encryption.
    Any operation which removes or does not preserve the original TLS
    socket such as skb_orphan() or skb_clone() will cause clear text
    leaks.
    
    Make the TCP socket underlying an offloaded TLS connection
    mark all skbs as decrypted, if TLS TX is in offload mode.
    Then in sk_validate_xmit_skb() catch skbs which have no socket
    (or a socket with no validation) and decrypted flag set.
    
    Note that CONFIG_SOCK_VALIDATE_XMIT, CONFIG_TLS_DEVICE and
    sk->sk_validate_xmit_skb are slightly interchangeable right now,
    they all imply TLS offload. The new checks are guarded by
    CONFIG_TLS_DEVICE because that's the option guarding the
    sk_buff->decrypted member.
    
    Second, smaller issue with orphaning is that it breaks
    the guarantee that packets will be delivered to device
    queues in-order. All TLS offload drivers depend on that
    scheduling property. This means skb_orphan_partial()'s
    trick of preserving partial socket references will cause
    issues in the drivers. We need a full orphan, and as a
    result netem delay/throttling will cause all TLS offload
    skbs to be dropped.
    
    Reusing the sk_buff->decrypted flag also protects from
    leaking clear text when incoming, decrypted skb is redirected
    (e.g. by TC).
    
    See commit 0608c69c9a80 ("bpf: sk_msg, sock{map|hash} redirect
    through ULP") for justification why the internal flag is safe.
    The only location which could leak the flag in is tcp_bpf_sendmsg(),
    which is taken care of by clearing the previously unused bit.
    
    v2:
     - remove superfluous decrypted mark copy (Willem);
     - remove the stale doc entry (Boris);
     - rely entirely on EOR marking to prevent coalescing (Boris);
     - use an internal sendpages flag instead of marking the socket
       (Boris).
    v3 (Willem):
     - reorganize the can_skb_orphan_partial() condition;
     - fix the flag leak-in through tcp_bpf_sendmsg.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Reviewed-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 228db3998e46..2c53f1a1d905 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2482,6 +2482,7 @@ static inline bool sk_fullsock(const struct sock *sk)
 
 /* Checks if this SKB belongs to an HW offloaded socket
  * and whether any SW fallbacks are required based on dev.
+ * Check decrypted mark in case skb_orphan() cleared socket.
  */
 static inline struct sk_buff *sk_validate_xmit_skb(struct sk_buff *skb,
 						   struct net_device *dev)
@@ -2489,8 +2490,15 @@ static inline struct sk_buff *sk_validate_xmit_skb(struct sk_buff *skb,
 #ifdef CONFIG_SOCK_VALIDATE_XMIT
 	struct sock *sk = skb->sk;
 
-	if (sk && sk_fullsock(sk) && sk->sk_validate_xmit_skb)
+	if (sk && sk_fullsock(sk) && sk->sk_validate_xmit_skb) {
 		skb = sk->sk_validate_xmit_skb(sk, dev, skb);
+#ifdef CONFIG_TLS_DEVICE
+	} else if (unlikely(skb->decrypted)) {
+		pr_warn_ratelimited("unencrypted skb with no associated socket - dropping\n");
+		kfree_skb(skb);
+		skb = NULL;
+#endif
+	}
 #endif
 
 	return skb;

commit 333f7909a8573145811c4ab7d8c9092301707721
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Jul 5 20:14:16 2019 +0100

    coallocate socket_wq with socket itself
    
    socket->wq is assign-once, set when we are initializing both
    struct socket it's in and struct socket_wq it points to.  As the
    matter of fact, the only reason for separate allocation was the
    ability to RCU-delay freeing of socket_wq.  RCU-delaying the
    freeing of socket itself gets rid of that need, so we can just
    fold struct socket_wq into the end of struct socket and simplify
    the life both for sock_alloc_inode() (one allocation instead of
    two) and for tun/tap oddballs, where we used to embed struct socket
    and struct socket_wq into the same structure (now - embedding just
    the struct socket).
    
    Note that reference to struct socket_wq in struct sock does remain
    a reference - that's unchanged.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6cbc16136357..228db3998e46 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1822,7 +1822,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
 	WARN_ON(parent->sk);
 	write_lock_bh(&sk->sk_callback_lock);
-	rcu_assign_pointer(sk->sk_wq, parent->wq);
+	rcu_assign_pointer(sk->sk_wq, &parent->wq);
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
 	sk->sk_uid = SOCK_INODE(parent)->i_uid;
@@ -2100,7 +2100,7 @@ static inline void sock_poll_wait(struct file *filp, struct socket *sock,
 				  poll_table *p)
 {
 	if (!poll_does_not_wait(p)) {
-		poll_wait(filp, &sock->wq->wait, p);
+		poll_wait(filp, &sock->wq.wait, p);
 		/* We need to be sure we are in sync with the
 		 * socket flags modification.
 		 *

commit ce27ec60648d8e066227cb2f58b1d3d4f7253d08
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jun 14 16:22:21 2019 -0700

    net: add high_order_alloc_disable sysctl/static key
    
    >From linux-3.7, (commit 5640f7685831 "net: use a per task frag
    allocator") TCP sendmsg() has preferred using order-3 allocations.
    
    While it gives good results for most cases, we had reports
    that heavy uses of TCP over loopback were hitting a spinlock
    contention in page allocations/freeing.
    
    This commits adds a sysctl so that admins can opt-in
    for order-0 allocations. Hopefully mm layer might optimize
    order-3 allocations in the future since it could give us
    a nice boost  (see 8 lines of following benchmark)
    
    The following benchmark shows a win when more than 8 TCP_STREAM
    threads are running (56 x86 cores server in my tests)
    
    for thr in {1..30}
    do
     sysctl -wq net.core.high_order_alloc_disable=0
     T0=`./super_netperf $thr -H 127.0.0.1 -l 15`
     sysctl -wq net.core.high_order_alloc_disable=1
     T1=`./super_netperf $thr -H 127.0.0.1 -l 15`
     echo $thr:$T0:$T1
    done
    
    1: 49979: 37267
    2: 98745: 76286
    3: 141088: 110051
    4: 177414: 144772
    5: 197587: 173563
    6: 215377: 208448
    7: 241061: 234087
    8: 267155: 263373
    9: 295069: 297402
    10: 312393: 335213
    11: 340462: 368778
    12: 371366: 403954
    13: 412344: 443713
    14: 426617: 473580
    15: 474418: 507861
    16: 503261: 538539
    17: 522331: 563096
    18: 532409: 567084
    19: 550824: 605240
    20: 525493: 641988
    21: 564574: 665843
    22: 567349: 690868
    23: 583846: 710917
    24: 588715: 736306
    25: 603212: 763494
    26: 604083: 792654
    27: 602241: 796450
    28: 604291: 797993
    29: 611610: 833249
    30: 577356: 841062
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7d7f4ce63bb2..6cbc16136357 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2534,6 +2534,8 @@ extern int sysctl_optmem_max;
 extern __u32 sysctl_wmem_default;
 extern __u32 sysctl_rmem_default;
 
+DECLARE_STATIC_KEY_FALSE(net_high_order_alloc_disable_key);
+
 static inline int sk_get_wmem0(const struct sock *sk, const struct proto *proto)
 {
 	/* Does this proto have per netns sysctl_wmem ? */

commit 0b7d7f6b22084a3156f267c85303908a8f4c9a08
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jun 14 16:22:20 2019 -0700

    tcp: add tcp_tx_skb_cache sysctl
    
    Feng Tang reported a performance regression after introduction
    of per TCP socket tx/rx caches, for TCP over loopback (netperf)
    
    There is high chance the regression is caused by a change on
    how well the 32 KB per-thread page (current->task_frag) can
    be recycled, and lack of pcp caches for order-3 pages.
    
    I could not reproduce the regression myself, cpus all being
    spinning on the mm spinlocks for page allocs/freeing, regardless
    of enabling or disabling the per tcp socket caches.
    
    It seems best to disable the feature by default, and let
    admins enabling it.
    
    MM layer either needs to provide scalable order-3 pages
    allocations, or could attempt a trylock on zone->lock if
    the caller only attempts to get a high-order page and is
    able to fallback to order-0 ones in case of pressure.
    
    Tests run on a 56 cores host (112 hyper threads)
    
    -       35.49%  netperf                  [kernel.vmlinux]         [k] queued_spin_lock_slowpath
       - 35.49% queued_spin_lock_slowpath
              - 18.18% get_page_from_freelist
                     - __alloc_pages_nodemask
                            - 18.18% alloc_pages_current
                                     skb_page_frag_refill
                                     sk_page_frag_refill
                                     tcp_sendmsg_locked
                                     tcp_sendmsg
                                     inet_sendmsg
                                     sock_sendmsg
                                     __sys_sendto
                                     __x64_sys_sendto
                                     do_syscall_64
                                     entry_SYSCALL_64_after_hwframe
                                     __libc_send
              + 17.31% __free_pages_ok
    +       31.43%  swapper                  [kernel.vmlinux]         [k] intel_idle
    +        9.12%  netperf                  [kernel.vmlinux]         [k] copy_user_enhanced_fast_string
    +        6.53%  netserver                [kernel.vmlinux]         [k] copy_user_enhanced_fast_string
    +        0.69%  netserver                [kernel.vmlinux]         [k] queued_spin_lock_slowpath
    +        0.68%  netperf                  [kernel.vmlinux]         [k] skb_release_data
    +        0.52%  netperf                  [kernel.vmlinux]         [k] tcp_sendmsg_locked
             0.46%  netperf                  [kernel.vmlinux]         [k] _raw_spin_lock_irqsave
    
    Fixes: 472c2e07eef0 ("tcp: add one skb cache for tx")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b02645e2dfad..7d7f4ce63bb2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1463,12 +1463,14 @@ static inline void sk_mem_uncharge(struct sock *sk, int size)
 		__sk_mem_reclaim(sk, 1 << 20);
 }
 
+DECLARE_STATIC_KEY_FALSE(tcp_tx_skb_cache_key);
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
 	sk->sk_wmem_queued -= skb->truesize;
 	sk_mem_uncharge(sk, skb->truesize);
-	if (!sk->sk_tx_skb_cache && !skb_cloned(skb)) {
+	if (static_branch_unlikely(&tcp_tx_skb_cache_key) &&
+	    !sk->sk_tx_skb_cache && !skb_cloned(skb)) {
 		skb_zcopy_clear(skb, true);
 		sk->sk_tx_skb_cache = skb;
 		return;

commit ede61ca474a0348b975d9824565b66c7595461de
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jun 14 16:22:19 2019 -0700

    tcp: add tcp_rx_skb_cache sysctl
    
    Instead of relying on rps_needed, it is safer to use a separate
    static key, since we do not want to enable TCP rx_skb_cache
    by default. This feature can cause huge increase of memory
    usage on hosts with millions of sockets.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e9d769c04637..b02645e2dfad 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2433,13 +2433,11 @@ static inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.
 */
+DECLARE_STATIC_KEY_FALSE(tcp_rx_skb_cache_key);
 static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
-	if (
-#ifdef CONFIG_RPS
-	    !static_branch_unlikely(&rps_needed) &&
-#endif
+	if (static_branch_unlikely(&tcp_rx_skb_cache_key) &&
 	    !sk->sk_rx_skb_cache) {
 		sk->sk_rx_skb_cache = skb;
 		skb_orphan(skb);

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0680fa988497..e9d769c04637 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * INET		An implementation of the TCP/IP protocol suite for the LINUX
  *		operating system.  INET is implemented using the  BSD Socket
@@ -30,12 +31,6 @@
  *              			respective headers and ipv4/v6, etc now
  *              			use private slabcaches for its socks
  *              Pedro Hortas	:	New flags field for socket options
- *
- *
- *		This program is free software; you can redistribute it and/or
- *		modify it under the terms of the GNU General Public License
- *		as published by the Free Software Foundation; either version
- *		2 of the License, or (at your option) any later version.
  */
 #ifndef _SOCK_H
 #define _SOCK_H

commit 858f5017446764e8bca0b29589a3b164186ae471
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 15 09:10:15 2019 -0700

    tcp: do not recycle cloned skbs
    
    It is illegal to change arbitrary fields in skb_shared_info if the
    skb is cloned.
    
    Before calling skb_zcopy_clear() we need to ensure this rule,
    therefore we need to move the test from sk_stream_alloc_skb()
    to sk_wmem_free_skb()
    
    Fixes: 4f661542a402 ("tcp: fix zerocopy and notsent_lowat issues")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Diagnosed-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4d208c0f9c14..0680fa988497 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1473,7 +1473,7 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
 	sk->sk_wmem_queued -= skb->truesize;
 	sk_mem_uncharge(sk, skb->truesize);
-	if (!sk->sk_tx_skb_cache) {
+	if (!sk->sk_tx_skb_cache && !skb_cloned(skb)) {
 		skb_zcopy_clear(skb, true);
 		sk->sk_tx_skb_cache = skb;
 		return;

commit 6ac99e8f23d4b10258406ca0dd7bffca5f31da9d
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Apr 26 16:39:39 2019 -0700

    bpf: Introduce bpf sk local storage
    
    After allowing a bpf prog to
    - directly read the skb->sk ptr
    - get the fullsock bpf_sock by "bpf_sk_fullsock()"
    - get the bpf_tcp_sock by "bpf_tcp_sock()"
    - get the listener sock by "bpf_get_listener_sock()"
    - avoid duplicating the fields of "(bpf_)sock" and "(bpf_)tcp_sock"
      into different bpf running context.
    
    this patch is another effort to make bpf's network programming
    more intuitive to do (together with memory and performance benefit).
    
    When bpf prog needs to store data for a sk, the current practice is to
    define a map with the usual 4-tuples (src/dst ip/port) as the key.
    If multiple bpf progs require to store different sk data, multiple maps
    have to be defined.  Hence, wasting memory to store the duplicated
    keys (i.e. 4 tuples here) in each of the bpf map.
    [ The smallest key could be the sk pointer itself which requires
      some enhancement in the verifier and it is a separate topic. ]
    
    Also, the bpf prog needs to clean up the elem when sk is freed.
    Otherwise, the bpf map will become full and un-usable quickly.
    The sk-free tracking currently could be done during sk state
    transition (e.g. BPF_SOCK_OPS_STATE_CB).
    
    The size of the map needs to be predefined which then usually ended-up
    with an over-provisioned map in production.  Even the map was re-sizable,
    while the sk naturally come and go away already, this potential re-size
    operation is arguably redundant if the data can be directly connected
    to the sk itself instead of proxy-ing through a bpf map.
    
    This patch introduces sk->sk_bpf_storage to provide local storage space
    at sk for bpf prog to use.  The space will be allocated when the first bpf
    prog has created data for this particular sk.
    
    The design optimizes the bpf prog's lookup (and then optionally followed by
    an inline update).  bpf_spin_lock should be used if the inline update needs
    to be protected.
    
    BPF_MAP_TYPE_SK_STORAGE:
    -----------------------
    To define a bpf "sk-local-storage", a BPF_MAP_TYPE_SK_STORAGE map (new in
    this patch) needs to be created.  Multiple BPF_MAP_TYPE_SK_STORAGE maps can
    be created to fit different bpf progs' needs.  The map enforces
    BTF to allow printing the sk-local-storage during a system-wise
    sk dump (e.g. "ss -ta") in the future.
    
    The purpose of a BPF_MAP_TYPE_SK_STORAGE map is not for lookup/update/delete
    a "sk-local-storage" data from a particular sk.
    Think of the map as a meta-data (or "type") of a "sk-local-storage".  This
    particular "type" of "sk-local-storage" data can then be stored in any sk.
    
    The main purposes of this map are mostly:
    1. Define the size of a "sk-local-storage" type.
    2. Provide a similar syscall userspace API as the map (e.g. lookup/update,
       map-id, map-btf...etc.)
    3. Keep track of all sk's storages of this "type" and clean them up
       when the map is freed.
    
    sk->sk_bpf_storage:
    ------------------
    The main lookup/update/delete is done on sk->sk_bpf_storage (which
    is a "struct bpf_sk_storage").  When doing a lookup,
    the "map" pointer is now used as the "key" to search on the
    sk_storage->list.  The "map" pointer is actually serving
    as the "type" of the "sk-local-storage" that is being
    requested.
    
    To allow very fast lookup, it should be as fast as looking up an
    array at a stable-offset.  At the same time, it is not ideal to
    set a hard limit on the number of sk-local-storage "type" that the
    system can have.  Hence, this patch takes a cache approach.
    The last search result from sk_storage->list is cached in
    sk_storage->cache[] which is a stable sized array.  Each
    "sk-local-storage" type has a stable offset to the cache[] array.
    In the future, a map's flag could be introduced to do cache
    opt-out/enforcement if it became necessary.
    
    The cache size is 16 (i.e. 16 types of "sk-local-storage").
    Programs can share map.  On the program side, having a few bpf_progs
    running in the networking hotpath is already a lot.  The bpf_prog
    should have already consolidated the existing sock-key-ed map usage
    to minimize the map lookup penalty.  16 has enough runway to grow.
    
    All sk-local-storage data will be removed from sk->sk_bpf_storage
    during sk destruction.
    
    bpf_sk_storage_get() and bpf_sk_storage_delete():
    ------------------------------------------------
    Instead of using bpf_map_(lookup|update|delete)_elem(),
    the bpf prog needs to use the new helper bpf_sk_storage_get() and
    bpf_sk_storage_delete().  The verifier can then enforce the
    ARG_PTR_TO_SOCKET argument.  The bpf_sk_storage_get() also allows to
    "create" new elem if one does not exist in the sk.  It is done by
    the new BPF_SK_STORAGE_GET_F_CREATE flag.  An optional value can also be
    provided as the initial value during BPF_SK_STORAGE_GET_F_CREATE.
    The BPF_MAP_TYPE_SK_STORAGE also supports bpf_spin_lock.  Together,
    it has eliminated the potential use cases for an equivalent
    bpf_map_update_elem() API (for bpf_prog) in this patch.
    
    Misc notes:
    ----------
    1. map_get_next_key is not supported.  From the userspace syscall
       perspective,  the map has the socket fd as the key while the map
       can be shared by pinned-file or map-id.
    
       Since btf is enforced, the existing "ss" could be enhanced to pretty
       print the local-storage.
    
       Supporting a kernel defined btf with 4 tuples as the return key could
       be explored later also.
    
    2. The sk->sk_lock cannot be acquired.  Atomic operations is used instead.
       e.g. cmpxchg is done on the sk->sk_bpf_storage ptr.
       Please refer to the source code comments for the details in
       synchronization cases and considerations.
    
    3. The mem is charged to the sk->sk_omem_alloc as the sk filter does.
    
    Benchmark:
    ---------
    Here is the benchmark data collected by turning on
    the "kernel.bpf_stats_enabled" sysctl.
    Two bpf progs are tested:
    
    One bpf prog with the usual bpf hashmap (max_entries = 8192) with the
    sk ptr as the key. (verifier is modified to support sk ptr as the key
    That should have shortened the key lookup time.)
    
    Another bpf prog is with the new BPF_MAP_TYPE_SK_STORAGE.
    
    Both are storing a "u32 cnt", do a lookup on "egress_skb/cgroup" for
    each egress skb and then bump the cnt.  netperf is used to drive
    data with 4096 connected UDP sockets.
    
    BPF_MAP_TYPE_HASH with a modifier verifier (152ns per bpf run)
    27: cgroup_skb  name egress_sk_map  tag 74f56e832918070b run_time_ns 58280107540 run_cnt 381347633
        loaded_at 2019-04-15T13:46:39-0700  uid 0
        xlated 344B  jited 258B  memlock 4096B  map_ids 16
        btf_id 5
    
    BPF_MAP_TYPE_SK_STORAGE in this patch (66ns per bpf run)
    30: cgroup_skb  name egress_sk_stora  tag d4aa70984cc7bbf6 run_time_ns 25617093319 run_cnt 390989739
        loaded_at 2019-04-15T13:47:54-0700  uid 0
        xlated 168B  jited 156B  memlock 4096B  map_ids 17
        btf_id 6
    
    Here is a high-level picture on how are the objects organized:
    
           sk
        ┌──────┐
        │      │
        │      │
        │      │
        │*sk_bpf_storage─────▶ bpf_sk_storage
        └──────┘                 ┌───────┐
                     ┌───────────┤ list  │
                     │           │       │
                     │           │       │
                     │           │       │
                     │           └───────┘
                     │
                     │     elem
                     │  ┌────────┐
                     ├─▶│ snode  │
                     │  ├────────┤
                     │  │  data  │          bpf_map
                     │  ├────────┤        ┌─────────┐
                     │  │map_node│◀─┬─────┤  list   │
                     │  └────────┘  │     │         │
                     │              │     │         │
                     │     elem     │     │         │
                     │  ┌────────┐  │     └─────────┘
                     └─▶│ snode  │  │
                        ├────────┤  │
       bpf_map          │  data  │  │
     ┌─────────┐        ├────────┤  │
     │  list   ├───────▶│map_node│  │
     │         │        └────────┘  │
     │         │                    │
     │         │           elem     │
     └─────────┘        ┌────────┐  │
                     ┌─▶│ snode  │  │
                     │  ├────────┤  │
                     │  │  data  │  │
                     │  ├────────┤  │
                     │  │map_node│◀─┘
                     │  └────────┘
                     │
                     │
                     │          ┌───────┐
         sk          └──────────│ list  │
      ┌──────┐                  │       │
      │      │                  │       │
      │      │                  │       │
      │      │                  └───────┘
      │*sk_bpf_storage───────▶bpf_sk_storage
      └──────┘
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 784cd19d5ff7..4d208c0f9c14 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -236,6 +236,8 @@ struct sock_common {
 	/* public: */
 };
 
+struct bpf_sk_storage;
+
 /**
   *	struct sock - network layer representation of sockets
   *	@__sk_common: shared layout with inet_timewait_sock
@@ -510,6 +512,9 @@ struct sock {
 #endif
 	void                    (*sk_destruct)(struct sock *sk);
 	struct sock_reuseport __rcu	*sk_reuseport_cb;
+#ifdef CONFIG_BPF_SYSCALL
+	struct bpf_sk_storage __rcu	*sk_bpf_storage;
+#endif
 	struct rcu_head		sk_rcu;
 };
 

commit c7cbdbf29f488a19982cd9f4a109887f18028bbb
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Apr 17 22:51:48 2019 +0200

    net: rework SIOCGSTAMP ioctl handling
    
    The SIOCGSTAMP/SIOCGSTAMPNS ioctl commands are implemented by many
    socket protocol handlers, and all of those end up calling the same
    sock_get_timestamp()/sock_get_timestampns() helper functions, which
    results in a lot of duplicate code.
    
    With the introduction of 64-bit time_t on 32-bit architectures, this
    gets worse, as we then need four different ioctl commands in each
    socket protocol implementation.
    
    To simplify that, let's add a new .gettstamp() operation in
    struct proto_ops, and move ioctl implementation into the common
    sock_ioctl()/compat_sock_ioctl_trans() functions that these all go
    through.
    
    We can reuse the sock_get_timestamp() implementation, but generalize
    it so it can deal with both native and compat mode, as well as
    timeval and timespec structures.
    
    Acked-by: Stefan Schmidt <stefan@datenfreihafen.org>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Acked-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Link: https://lore.kernel.org/lkml/CAK8P3a038aDQQotzua_QtKGhq8O9n+rdiz2=WDCp82ys8eUT+A@mail.gmail.com/
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bdd77bbce7d8..784cd19d5ff7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1614,6 +1614,8 @@ int sock_setsockopt(struct socket *sock, int level, int op,
 
 int sock_getsockopt(struct socket *sock, int level, int op,
 		    char __user *optval, int __user *optlen);
+int sock_gettstamp(struct socket *sock, void __user *userstamp,
+		   bool timeval, bool time32);
 struct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,
 				    int noblock, int *errcode);
 struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
@@ -2503,8 +2505,6 @@ static inline bool sk_listener(const struct sock *sk)
 }
 
 void sock_enable_timestamp(struct sock *sk, int flag);
-int sock_get_timestamp(struct sock *, struct timeval __user *);
-int sock_get_timestampns(struct sock *, struct timespec __user *);
 int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len, int level,
 		       int type);
 

commit 6b0a7f84ea1fe248df96ccc4dd86e817e32ef65b
Merge: cea0aa9cbd5a fe5cdef29e41
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 17 11:26:25 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflict resolution of af_smc.c from Stephen Rothwell.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 07603b230895a74ebb1e2a1231ac45c29c2a8cd3
Author: Ursula Braun <ubraun@linux.ibm.com>
Date:   Thu Apr 11 11:17:32 2019 +0200

    net/smc: propagate file from SMC to TCP socket
    
    fcntl(fd, F_SETOWN, getpid()) selects the recipient of SIGURG signals
    that are delivered when out-of-band data arrives on socket fd.
    If an SMC socket program makes use of such an fcntl() call, it fails
    in case of fallback to TCP-mode. In case of fallback the traffic is
    processed with the internal TCP socket. Propagating field "file" from the
    SMC socket to the internal TCP socket fixes the issue.
    
    Reviewed-by: Karsten Graul <kgraul@linux.ibm.com>
    Signed-off-by: Ursula Braun <ubraun@linux.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8de5ee258b93..341f8bafa0cf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2084,12 +2084,6 @@ static inline bool skwq_has_sleeper(struct socket_wq *wq)
  * @p:              poll_table
  *
  * See the comments in the wq_has_sleeper function.
- *
- * Do not derive sock from filp->private_data here. An SMC socket establishes
- * an internal TCP socket that is used in the fallback case. All socket
- * operations on the SMC socket are then forwarded to the TCP socket. In case of
- * poll, the filp->private_data pointer references the SMC socket because the
- * TCP socket has no file assigned.
  */
 static inline void sock_poll_wait(struct file *filp, struct socket *sock,
 				  poll_table *p)

commit 4f661542a40217713f2cee0bb6678fbb30d9d367
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 26 08:34:55 2019 -0700

    tcp: fix zerocopy and notsent_lowat issues
    
    My recent patch had at least three problems :
    
    1) TX zerocopy wants notification when skb is acknowledged,
       thus we need to call skb_zcopy_clear() if the skb is
       cached into sk->sk_tx_skb_cache
    
    2) Some applications might expect precise EPOLLOUT
       notifications, so we need to update sk->sk_wmem_queued
       and call sk_mem_uncharge() from sk_wmem_free_skb()
       in all cases. The SOCK_QUEUE_SHRUNK flag must also be set.
    
    3) Reuse of saved skb should have used skb_cloned() instead
      of simply checking if the fast clone has been freed.
    
    Fixes: 472c2e07eef0 ("tcp: add one skb cache for tx")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 577d91fb5626..7fa223278522 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1465,13 +1465,14 @@ static inline void sk_mem_uncharge(struct sock *sk, int size)
 
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued -= skb->truesize;
+	sk_mem_uncharge(sk, skb->truesize);
 	if (!sk->sk_tx_skb_cache) {
+		skb_zcopy_clear(skb, true);
 		sk->sk_tx_skb_cache = skb;
 		return;
 	}
-	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
-	sk->sk_wmem_queued -= skb->truesize;
-	sk_mem_uncharge(sk, skb->truesize);
 	__kfree_skb(skb);
 }
 

commit 8b27dae5a2e89a61c46c6dbc76c040c0e6d0ed4c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 22 08:56:40 2019 -0700

    tcp: add one skb cache for rx
    
    Often times, recvmsg() system calls and BH handling for a particular
    TCP socket are done on different cpus.
    
    This means the incoming skb had to be allocated on a cpu,
    but freed on another.
    
    This incurs a high spinlock contention in slab layer for small rpc,
    but also a high number of cache line ping pongs for larger packets.
    
    A full size GRO packet might use 45 page fragments, meaning
    that up to 45 put_page() can be involved.
    
    More over performing the __kfree_skb() in the recvmsg() context
    adds a latency for user applications, and increase probability
    of trapping them in backlog processing, since the BH handler
    might found the socket owned by the user.
    
    This patch, combined with the prior one increases the rpc
    performance by about 10 % on servers with large number of cores.
    
    (tcp_rr workload with 10,000 flows and 112 threads reach 9 Mpps
     instead of 8 Mpps)
    
    This also increases single bulk flow performance on 40Gbit+ links,
    since in this case there are often two cpus working in tandem :
    
     - CPU handling the NIC rx interrupts, feeding the receive queue,
      and (after this patch) freeing the skbs that were consumed.
    
     - CPU in recvmsg() system call, essentially 100 % busy copying out
      data to user space.
    
    Having at most one skb in a per-socket cache has very little risk
    of memory exhaustion, and since it is protected by socket lock,
    its management is essentially free.
    
    Note that if rps/rfs is used, we do not enable this feature, because
    there is high chance that the same cpu is handling both the recvmsg()
    system call and the TCP rx path, but that another cpu did the skb
    allocations in the device driver right before the RPS/RFS logic.
    
    To properly handle this case, it seems we would need to record
    on which cpu skb was allocated, and use a different channel
    to give skbs back to this cpu.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 314c47a8f5d1..577d91fb5626 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -368,6 +368,7 @@ struct sock {
 	atomic_t		sk_drops;
 	int			sk_rcvlowat;
 	struct sk_buff_head	sk_error_queue;
+	struct sk_buff		*sk_rx_skb_cache;
 	struct sk_buff_head	sk_receive_queue;
 	/*
 	 * The backlog queue is special, it is always used with
@@ -2438,6 +2439,15 @@ static inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)
 static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
+	if (
+#ifdef CONFIG_RPS
+	    !static_branch_unlikely(&rps_needed) &&
+#endif
+	    !sk->sk_rx_skb_cache) {
+		sk->sk_rx_skb_cache = skb;
+		skb_orphan(skb);
+		return;
+	}
 	__kfree_skb(skb);
 }
 

commit 472c2e07eef045145bc1493cc94a01c87140780a
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 22 08:56:39 2019 -0700

    tcp: add one skb cache for tx
    
    On hosts with a lot of cores, RPC workloads suffer from heavy contention on slab spinlocks.
    
        20.69%  [kernel]       [k] queued_spin_lock_slowpath
         5.64%  [kernel]       [k] _raw_spin_lock
         3.83%  [kernel]       [k] syscall_return_via_sysret
         3.48%  [kernel]       [k] __entry_text_start
         1.76%  [kernel]       [k] __netif_receive_skb_core
         1.64%  [kernel]       [k] __fget
    
    For each sendmsg(), we allocate one skb, and free it at the time ACK packet comes.
    
    In many cases, ACK packets are handled by another cpus, and this unfortunately
    incurs heavy costs for slab layer.
    
    This patch uses an extra pointer in socket structure, so that we try to reuse
    the same skb and avoid these expensive costs.
    
    We cache at most one skb per socket so this should be safe as far as
    memory pressure is concerned.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index fecdf639225c..314c47a8f5d1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -414,6 +414,7 @@ struct sock {
 		struct sk_buff	*sk_send_head;
 		struct rb_root	tcp_rtx_queue;
 	};
+	struct sk_buff		*sk_tx_skb_cache;
 	struct sk_buff_head	sk_write_queue;
 	__s32			sk_peek_off;
 	int			sk_write_pending;
@@ -1463,6 +1464,10 @@ static inline void sk_mem_uncharge(struct sock *sk, int size)
 
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
+	if (!sk->sk_tx_skb_cache) {
+		sk->sk_tx_skb_cache = skb;
+		return;
+	}
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
 	sk->sk_wmem_queued -= skb->truesize;
 	sk_mem_uncharge(sk, skb->truesize);

commit dc05360fee660a9dbe59824b3f7896534210432b
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 22 08:56:38 2019 -0700

    net: convert rps_needed and rfs_needed to new static branch api
    
    We prefer static_branch_unlikely() over static_key_false() these days.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8de5ee258b93..fecdf639225c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -966,7 +966,7 @@ static inline void sock_rps_record_flow_hash(__u32 hash)
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
 #ifdef CONFIG_RPS
-	if (static_key_false(&rfs_needed)) {
+	if (static_branch_unlikely(&rfs_needed)) {
 		/* Reading sk->sk_rxhash might incur an expensive cache line
 		 * miss.
 		 *

commit a4dc6a49156b1f8d6e17251ffda17c9e6a5db78a
Author: Maxime Chevallier <maxime.chevallier@bootlin.com>
Date:   Sat Mar 16 14:41:30 2019 +0100

    packets: Always register packet sk in the same order
    
    When using fanouts with AF_PACKET, the demux functions such as
    fanout_demux_cpu will return an index in the fanout socket array, which
    corresponds to the selected socket.
    
    The ordering of this array depends on the order the sockets were added
    to a given fanout group, so for FANOUT_CPU this means sockets are bound
    to cpus in the order they are configured, which is OK.
    
    However, when stopping then restarting the interface these sockets are
    bound to, the sockets are reassigned to the fanout group in the reverse
    order, due to the fact that they were inserted at the head of the
    interface's AF_PACKET socket list.
    
    This means that traffic that was directed to the first socket in the
    fanout group is now directed to the last one after an interface restart.
    
    In the case of FANOUT_CPU, traffic from CPU0 will be directed to the
    socket that used to receive traffic from the last CPU after an interface
    restart.
    
    This commit introduces a helper to add a socket at the tail of a list,
    then uses it to register AF_PACKET sockets.
    
    Note that this changes the order in which sockets are listed in /proc and
    with sock_diag.
    
    Fixes: dc99f600698d ("packet: Add fanout support")
    Signed-off-by: Maxime Chevallier <maxime.chevallier@bootlin.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 328cb7cb7b0b..8de5ee258b93 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -710,6 +710,12 @@ static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
 		hlist_add_head_rcu(&sk->sk_node, list);
 }
 
+static inline void sk_add_node_tail_rcu(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	hlist_add_tail_rcu(&sk->sk_node, list);
+}
+
 static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);

commit 3313da8188cc346a205783c22c37e821b4b7016d
Merge: 50f444aa50a4 24f0a48743a2
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 15 12:38:38 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The netfilter conflicts were rather simple overlapping
    changes.
    
    However, the cls_tcindex.c stuff was a bit more complex.
    
    On the 'net' side, Cong is fixing several races and memory
    leaks.  Whilst on the 'net-next' side we have Vlad adding
    the rtnl-ness support.
    
    What I've decided to do, in order to resolve this, is revert the
    conversion over to using a workqueue that Cong did, bringing us back
    to pure RCU.  I did it this way because I believe that either Cong's
    races don't apply with have Vlad did things, or Cong will have to
    implement the race fix slightly differently.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5bf325a53202b8728cf7013b72688c46071e212e
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 12 12:26:27 2019 -0800

    net: fix possible overflow in __sk_mem_raise_allocated()
    
    With many active TCP sockets, fat TCP sockets could fool
    __sk_mem_raise_allocated() thanks to an overflow.
    
    They would increase their share of the memory, instead
    of decreasing it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2b229f7be8eb..f43f935cb113 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1277,7 +1277,7 @@ static inline void sk_sockets_allocated_inc(struct sock *sk)
 	percpu_counter_inc(sk->sk_prot->sockets_allocated);
 }
 
-static inline int
+static inline u64
 sk_sockets_allocated_read_positive(struct sock *sk)
 {
 	return percpu_counter_read_positive(sk->sk_prot->sockets_allocated);

commit 887feae36aee6c08e0dafcdaa5ba921abbb2c56b
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Sat Feb 2 07:34:50 2019 -0800

    socket: Add SO_TIMESTAMP[NS]_NEW
    
    Add SO_TIMESTAMP_NEW and SO_TIMESTAMPNS_NEW variants of
    socket timestamp options.
    These are the y2038 safe versions of the SO_TIMESTAMP_OLD
    and SO_TIMESTAMPNS_OLD for all architectures.
    
    Note that the format of scm_timestamping.ts[0] is not changed
    in this patch.
    
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Cc: jejb@parisc-linux.org
    Cc: ralf@linux-mips.org
    Cc: rth@twiddle.net
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linux-rdma@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Cc: sparclinux@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2b229f7be8eb..6679f3c120b0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -805,6 +805,7 @@ enum sock_flags {
 	SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
 	SOCK_TXTIME,
 	SOCK_XDP, /* XDP is attached */
+	SOCK_TSTAMP_NEW, /* Indicates 64 bit timestamps always */
 };
 
 #define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))

commit 3a0ed3e9619738067214871e9cb826fa23b2ddb9
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Thu Dec 27 18:55:09 2018 -0800

    sock: Make sock->sk_stamp thread-safe
    
    Al Viro mentioned (Message-ID
    <20170626041334.GZ10672@ZenIV.linux.org.uk>)
    that there is probably a race condition
    lurking in accesses of sk_stamp on 32-bit machines.
    
    sock->sk_stamp is of type ktime_t which is always an s64.
    On a 32 bit architecture, we might run into situations of
    unsafe access as the access to the field becomes non atomic.
    
    Use seqlocks for synchronization.
    This allows us to avoid using spinlocks for readers as
    readers do not need mutual exclusion.
    
    Another approach to solve this is to require sk_lock for all
    modifications of the timestamps. The current approach allows
    for timestamps to have their own lock: sk_stamp_lock.
    This allows for the patch to not compete with already
    existing critical sections, and side effects are limited
    to the paths in the patch.
    
    The addition of the new field maintains the data locality
    optimizations from
    commit 9115e8cd2a0c ("net: reorganize struct sock for better data
    locality")
    
    Note that all the instances of the sk_stamp accesses
    are either through the ioctl or the syscall recvmsg.
    
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6235c286ef9..2b229f7be8eb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -298,6 +298,7 @@ struct sock_common {
   *	@sk_filter: socket filtering instructions
   *	@sk_timer: sock cleanup timer
   *	@sk_stamp: time stamp of last packet received
+  *	@sk_stamp_seq: lock for accessing sk_stamp on 32 bit architectures only
   *	@sk_tsflags: SO_TIMESTAMPING socket options
   *	@sk_tskey: counter to disambiguate concurrent tstamp requests
   *	@sk_zckey: counter to order MSG_ZEROCOPY notifications
@@ -474,6 +475,9 @@ struct sock {
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
 	ktime_t			sk_stamp;
+#if BITS_PER_LONG==32
+	seqlock_t		sk_stamp_seq;
+#endif
 	u16			sk_tsflags;
 	u8			sk_shutdown;
 	u32			sk_tskey;
@@ -2297,6 +2301,34 @@ static inline void sk_drops_add(struct sock *sk, const struct sk_buff *skb)
 	atomic_add(segs, &sk->sk_drops);
 }
 
+static inline ktime_t sock_read_timestamp(struct sock *sk)
+{
+#if BITS_PER_LONG==32
+	unsigned int seq;
+	ktime_t kt;
+
+	do {
+		seq = read_seqbegin(&sk->sk_stamp_seq);
+		kt = sk->sk_stamp;
+	} while (read_seqretry(&sk->sk_stamp_seq, seq));
+
+	return kt;
+#else
+	return sk->sk_stamp;
+#endif
+}
+
+static inline void sock_write_timestamp(struct sock *sk, ktime_t kt)
+{
+#if BITS_PER_LONG==32
+	write_seqlock(&sk->sk_stamp_seq);
+	sk->sk_stamp = kt;
+	write_sequnlock(&sk->sk_stamp_seq);
+#else
+	sk->sk_stamp = kt;
+#endif
+}
+
 void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 			   struct sk_buff *skb);
 void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
@@ -2321,7 +2353,7 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 	     (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);
 	else
-		sk->sk_stamp = kt;
+		sock_write_timestamp(sk, kt);
 
 	if (sock_flag(sk, SOCK_WIFI_STATUS) && skb->wifi_acked_valid)
 		__sock_recv_wifi_status(msg, sk, skb);
@@ -2342,9 +2374,9 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 	if (sk->sk_flags & FLAGS_TS_OR_DROPS || sk->sk_tsflags & TSFLAGS_ANY)
 		__sock_recv_ts_and_drops(msg, sk, skb);
 	else if (unlikely(sock_flag(sk, SOCK_TIMESTAMP)))
-		sk->sk_stamp = skb->tstamp;
+		sock_write_timestamp(sk, skb->tstamp);
 	else if (unlikely(sk->sk_stamp == SK_DEFAULT_STAMP))
-		sk->sk_stamp = 0;
+		sock_write_timestamp(sk, 0);
 }
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);

commit 2be09de7d6a06f58e768de1255a687c9aaa66606
Merge: 44a7b3b6e3a4 1d51b4b1d3f2
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 20 10:53:28 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Lots of conflicts, by happily all cases of overlapping
    changes, parallel adds, things of that nature.
    
    Thanks to Stephen Rothwell, Saeed Mahameed, and others
    for their guidance in these resolutions.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8f932f762e7928d250e21006b00ff9b7718b0a64
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Dec 17 12:24:00 2018 -0500

    net: add missing SOF_TIMESTAMPING_OPT_ID support
    
    SOF_TIMESTAMPING_OPT_ID is supported on TCP, UDP and RAW sockets.
    But it was missing on RAW with IPPROTO_IP, PF_PACKET and CAN.
    
    Add skb_setup_tx_timestamp that configures both tx_flags and tskey
    for these paths that do not need corking or use bytestream keys.
    
    Fixes: 09c2d251b707 ("net-timestamp: add key to disambiguate concurrent datagrams")
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f665d74ae509..0e3a09380655 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2340,22 +2340,39 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);
 
 /**
- * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
+ * _sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @sk:		socket sending this packet
  * @tsflags:	timestamping flags to use
  * @tx_flags:	completed with instructions for time stamping
+ * @tskey:      filled in with next sk_tskey (not for TCP, which uses seqno)
  *
  * Note: callers should take care of initial ``*tx_flags`` value (usually 0)
  */
-static inline void sock_tx_timestamp(const struct sock *sk, __u16 tsflags,
-				     __u8 *tx_flags)
+static inline void _sock_tx_timestamp(struct sock *sk, __u16 tsflags,
+				      __u8 *tx_flags, __u32 *tskey)
 {
-	if (unlikely(tsflags))
+	if (unlikely(tsflags)) {
 		__sock_tx_timestamp(tsflags, tx_flags);
+		if (tsflags & SOF_TIMESTAMPING_OPT_ID && tskey &&
+		    tsflags & SOF_TIMESTAMPING_TX_RECORD_MASK)
+			*tskey = sk->sk_tskey++;
+	}
 	if (unlikely(sock_flag(sk, SOCK_WIFI_STATUS)))
 		*tx_flags |= SKBTX_WIFI_STATUS;
 }
 
+static inline void sock_tx_timestamp(struct sock *sk, __u16 tsflags,
+				     __u8 *tx_flags)
+{
+	_sock_tx_timestamp(sk, tsflags, tx_flags, NULL);
+}
+
+static inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)
+{
+	_sock_tx_timestamp(skb->sk, tsflags, &skb_shinfo(skb)->tx_flags,
+			   &skb_shinfo(skb)->tskey);
+}
+
 /**
  * sk_eat_skb - Release a skb if it is no longer needed
  * @sk: socket to eat this skb from

commit a74f0fa082b76c6a76cba5672f36218518bfdc09
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 4 07:58:17 2018 -0800

    tcp: reduce POLLOUT events caused by TCP_NOTSENT_LOWAT
    
    TCP_NOTSENT_LOWAT socket option or sysctl was added in linux-3.12
    as a step to enable bigger tcp sndbuf limits.
    
    It works reasonably well, but the following happens :
    
    Once the limit is reached, TCP stack generates
    an [E]POLLOUT event for every incoming ACK packet.
    
    This causes a high number of context switches.
    
    This patch implements the strategy David Miller added
    in sock_def_write_space() :
    
     - If TCP socket has a notsent_lowat constraint of X bytes,
       allow sendmsg() to fill up to X bytes, but send [E]POLLOUT
       only if number of notsent bytes is below X/2
    
    This considerably reduces TCP_NOTSENT_LOWAT overhead,
    while allowing to keep the pipe full.
    
    Tested:
     100 ms RTT netem testbed between A and B, 100 concurrent TCP_STREAM
    
    A:/# cat /proc/sys/net/ipv4/tcp_wmem
    4096    262144  64000000
    A:/# super_netperf 100 -H B -l 1000 -- -K bbr &
    
    A:/# grep TCP /proc/net/sockstat
    TCP: inuse 203 orphan 0 tw 19 alloc 414 mem 1364904 # This is about 54 MB of memory per flow :/
    
    A:/# vmstat 5 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     0  0      0 256220672  13532 694976    0    0    10     0   28   14  0  1 99  0  0
     2  0      0 256320016  13532 698480    0    0   512     0 715901 5927  0 10 90  0  0
     0  0      0 256197232  13532 700992    0    0   735    13 771161 5849  0 11 89  0  0
     1  0      0 256233824  13532 703320    0    0   512    23 719650 6635  0 11 89  0  0
     2  0      0 256226880  13532 705780    0    0   642     4 775650 6009  0 12 88  0  0
    
    A:/# echo 2097152 >/proc/sys/net/ipv4/tcp_notsent_lowat
    
    A:/# grep TCP /proc/net/sockstat
    TCP: inuse 203 orphan 0 tw 19 alloc 414 mem 86411 # 3.5 MB per flow
    
    A:/# vmstat 5 5  # check that context switches have not inflated too much.
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     2  0      0 260386512  13592 662148    0    0    10     0   17   14  0  1 99  0  0
     0  0      0 260519680  13592 604184    0    0   512    13 726843 12424  0 10 90  0  0
     1  1      0 260435424  13592 598360    0    0   512    25 764645 12925  0 10 90  0  0
     1  0      0 260855392  13592 578380    0    0   512     7 722943 13624  0 11 88  0  0
     1  0      0 260445008  13592 601176    0    0   614    34 772288 14317  0 10 90  0  0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f665d74ae509..df390a3e23fe 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1110,7 +1110,7 @@ struct proto {
 	unsigned int		inuse_idx;
 #endif
 
-	bool			(*stream_memory_free)(const struct sock *sk);
+	bool			(*stream_memory_free)(const struct sock *sk, int wake);
 	bool			(*stream_memory_read)(const struct sock *sk);
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
@@ -1192,19 +1192,29 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
-static inline bool sk_stream_memory_free(const struct sock *sk)
+static inline bool __sk_stream_memory_free(const struct sock *sk, int wake)
 {
 	if (sk->sk_wmem_queued >= sk->sk_sndbuf)
 		return false;
 
 	return sk->sk_prot->stream_memory_free ?
-		sk->sk_prot->stream_memory_free(sk) : true;
+		sk->sk_prot->stream_memory_free(sk, wake) : true;
 }
 
-static inline bool sk_stream_is_writeable(const struct sock *sk)
+static inline bool sk_stream_memory_free(const struct sock *sk)
+{
+	return __sk_stream_memory_free(sk, 0);
+}
+
+static inline bool __sk_stream_is_writeable(const struct sock *sk, int wake)
 {
 	return sk_stream_wspace(sk) >= sk_stream_min_wspace(sk) &&
-	       sk_stream_memory_free(sk);
+	       __sk_stream_memory_free(sk, wake);
+}
+
+static inline bool sk_stream_is_writeable(const struct sock *sk)
+{
+	return __sk_stream_is_writeable(sk, 0);
 }
 
 static inline int sk_under_cgroup_hierarchy(struct sock *sk,

commit 89ab066d4229acd32e323f1569833302544a4186
Author: Karsten Graul <kgraul@linux.ibm.com>
Date:   Tue Oct 23 13:40:39 2018 +0200

    Revert "net: simplify sock_poll_wait"
    
    This reverts commit dd979b4df817e9976f18fb6f9d134d6bc4a3c317.
    
    This broke tcp_poll for SMC fallback: An AF_SMC socket establishes an
    internal TCP socket for the initial handshake with the remote peer.
    Whenever the SMC connection can not be established this TCP socket is
    used as a fallback. All socket operations on the SMC socket are then
    forwarded to the TCP socket. In case of poll, the file->private_data
    pointer references the SMC socket because the TCP socket has no file
    assigned. This causes tcp_poll to wait on the wrong socket.
    
    Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2440f8b407eb..f665d74ae509 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2059,14 +2059,20 @@ static inline bool skwq_has_sleeper(struct socket_wq *wq)
 /**
  * sock_poll_wait - place memory barrier behind the poll_wait call.
  * @filp:           file
+ * @sock:           socket to wait on
  * @p:              poll_table
  *
  * See the comments in the wq_has_sleeper function.
+ *
+ * Do not derive sock from filp->private_data here. An SMC socket establishes
+ * an internal TCP socket that is used in the fallback case. All socket
+ * operations on the SMC socket are then forwarded to the TCP socket. In case of
+ * poll, the filp->private_data pointer references the SMC socket because the
+ * TCP socket has no file assigned.
  */
-static inline void sock_poll_wait(struct file *filp, poll_table *p)
+static inline void sock_poll_wait(struct file *filp, struct socket *sock,
+				  poll_table *p)
 {
-	struct socket *sock = filp->private_data;
-
 	if (!poll_does_not_wait(p)) {
 		poll_wait(filp, &sock->wq->wait, p);
 		/* We need to be sure we are in sync with the

commit e85679511e48168b0f066b6ae585556b5e0d8f5b
Merge: c45d7150656f 0b592b5a01be
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Oct 15 23:21:07 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-10-16
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Convert BPF sockmap and kTLS to both use a new sk_msg API and enable
       sk_msg BPF integration for the latter, from Daniel and John.
    
    2) Enable BPF syscall side to indicate for maps that they do not support
       a map lookup operation as opposed to just missing key, from Prashant.
    
    3) Add bpftool map create command which after map creation pins the
       map into bpf fs for further processing, from Jakub.
    
    4) Add bpftool support for attaching programs to maps allowing sock_map
       and sock_hash to be used from bpftool, from John.
    
    5) Improve syscall BPF map update/delete path for map-in-map types to
       wait a RCU grace period for pending references to complete, from Daniel.
    
    6) Couple of follow-up fixes for the BPF socket lookup to get it
       enabled also when IPv6 is compiled as a module, from Joe.
    
    7) Fix a generic-XDP bug to handle the case when the Ethernet header
       was mangled and thus update skb's protocol and data, from Jesper.
    
    8) Add a missing BTF header length check between header copies from
       user space, from Wenwen.
    
    9) Minor fixups in libbpf to use __u32 instead u32 types and include
       proper perf_event.h uapi header instead of perf internal one, from Yonghong.
    
    10) Allow to pass user-defined flags through EXTRA_CFLAGS and EXTRA_LDFLAGS
        to bpftool's build, from Jiri.
    
    11) BPF kselftest tweaks to add LWTUNNEL to config fragment and to install
        with_addr.sh script from flow dissector selftest, from Anders.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 76a9ebe811fb3d0605cb084f1ae6be5610541865
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 15 09:37:53 2018 -0700

    net: extend sk_pacing_rate to unsigned long
    
    sk_pacing_rate has beed introduced as a u32 field in 2013,
    effectively limiting per flow pacing to 34Gbit.
    
    We believe it is time to allow TCP to pace high speed flows
    on 64bit hosts, as we now can reach 100Gbit on one TCP flow.
    
    This patch adds no cost for 32bit kernels.
    
    The tcpi_pacing_rate and tcpi_max_pacing_rate were already
    exported as 64bit, so iproute2/ss command require no changes.
    
    Unfortunately the SO_MAX_PACING_RATE socket option will stay
    32bit and we will need to add a new option to let applications
    control high pacing rates.
    
    State      Recv-Q Send-Q Local Address:Port             Peer Address:Port
    ESTAB      0      1787144  10.246.9.76:49992             10.246.9.77:36741
                     timer:(on,003ms,0) ino:91863 sk:2 <->
     skmem:(r0,rb540000,t66440,tb2363904,f605944,w1822984,o0,bl0,d0)
     ts sack bbr wscale:8,8 rto:201 rtt:0.057/0.006 mss:1448
     rcvmss:536 advmss:1448
     cwnd:138 ssthresh:178 bytes_acked:256699822585 segs_out:177279177
     segs_in:3916318 data_segs_out:177279175
     bbr:(bw:31276.8Mbps,mrtt:0,pacing_gain:1.25,cwnd_gain:2)
     send 28045.5Mbps lastrcv:73333
     pacing_rate 38705.0Mbps delivery_rate 22997.6Mbps
     busy:73333ms unacked:135 retrans:0/157 rcv_space:14480
     notsent:2085120 minrtt:0.013
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 751549ac0a84..cfaf261936c8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -422,8 +422,8 @@ struct sock {
 	struct timer_list	sk_timer;
 	__u32			sk_priority;
 	__u32			sk_mark;
-	u32			sk_pacing_rate; /* bytes per second */
-	u32			sk_max_pacing_rate;
+	unsigned long		sk_pacing_rate; /* bytes per second */
+	unsigned long		sk_max_pacing_rate;
 	struct page_frag	sk_frag;
 	netdev_features_t	sk_route_caps;
 	netdev_features_t	sk_route_nocaps;

commit d829e9c4112b52f4f00195900fd4c685f61365ab
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:59 2018 +0200

    tls: convert to generic sk_msg interface
    
    Convert kTLS over to make use of sk_msg interface for plaintext and
    encrypted scattergather data, so it reuses all the sk_msg helpers
    and data structure which later on in a second step enables to glue
    this to BPF.
    
    This also allows to remove quite a bit of open coded helpers which
    are covered by the sk_msg API. Recent changes in kTLs 80ece6a03aaf
    ("tls: Remove redundant vars from tls record structure") and
    4e6d47206c32 ("tls: Add support for inplace records encryption")
    changed the data path handling a bit; while we've kept the latter
    optimization intact, we had to undo the former change to better
    fit the sk_msg model, hence the sg_aead_in and sg_aead_out have
    been brought back and are linked into the sk_msg sgs. Now the kTLS
    record contains a msg_plaintext and msg_encrypted sk_msg each.
    
    In the original code, the zerocopy_from_iter() has been used out
    of TX but also RX path. For the strparser skb-based RX path,
    we've left the zerocopy_from_iter() in decrypt_internal() mostly
    untouched, meaning it has been moved into tls_setup_from_iter()
    with charging logic removed (as not used from RX). Given RX path
    is not based on sk_msg objects, we haven't pursued setting up a
    dummy sk_msg to call into sk_msg_zerocopy_from_iter(), but it
    could be an option to prusue in a later step.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 751549ac0a84..7470c45d182d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2214,10 +2214,6 @@ static inline struct page_frag *sk_page_frag(struct sock *sk)
 
 bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
 
-int sk_alloc_sg(struct sock *sk, int len, struct scatterlist *sg,
-		int sg_start, int *sg_curr, unsigned int *sg_size,
-		int first_coalesce);
-
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */

commit 8873c064d1de579ea23412a6d3eee972593f142b
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 1 23:24:26 2018 -0700

    tcp: do not release socket ownership in tcp_close()
    
    syzkaller was able to hit the WARN_ON(sock_owned_by_user(sk));
    in tcp_close()
    
    While a socket is being closed, it is very possible other
    threads find it in rtnetlink dump.
    
    tcp_get_info() will acquire the socket lock for a short amount
    of time (slow = lock_sock_fast(sk)/unlock_sock_fast(sk, slow);),
    enough to trigger the warning.
    
    Fixes: 67db3e4bfbc9 ("tcp: no longer hold ehash lock while calling tcp_get_info()")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 38cae35f6e16..751549ac0a84 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1492,6 +1492,7 @@ static inline void lock_sock(struct sock *sk)
 	lock_sock_nested(sk, 0);
 }
 
+void __release_sock(struct sock *sk);
 void release_sock(struct sock *sk);
 
 /* BH context may only use the following locking interface. */

commit e4a2a3048ed93f0c354ad837f1d45fc8d389d538
Author: Jason Wang <jasowang@redhat.com>
Date:   Wed Sep 12 11:16:59 2018 +0800

    net: sock: introduce SOCK_XDP
    
    This patch introduces a new sock flag - SOCK_XDP. This will be used
    for notifying the upper layer that XDP program is attached on the
    lower socket, and requires for extra headroom.
    
    TUN will be the first user.
    
    Signed-off-by: Jason Wang <jasowang@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 433f45fc2d68..38cae35f6e16 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -800,6 +800,7 @@ enum sock_flags {
 	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 	SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
 	SOCK_TXTIME,
+	SOCK_XDP, /* XDP is attached */
 };
 
 #define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))

commit e6476c21447c4b17c47e476aade6facf050f31e8
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 30 09:45:07 2018 +0200

    net: remove bogus RCU annotations on socket.wq
    
    We never use RCU protection for it, just a lot of cargo-cult
    rcu_deference_protects calls.
    
    Note that we do keep the kfree_rcu call for it, as the references through
    struct sock are RCU protected and thus might require a grace period before
    freeing.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2afea5d1bdfe..433f45fc2d68 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1788,7 +1788,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
 	WARN_ON(parent->sk);
 	write_lock_bh(&sk->sk_callback_lock);
-	sk->sk_wq = parent->wq;
+	rcu_assign_pointer(sk->sk_wq, parent->wq);
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
 	sk->sk_uid = SOCK_INODE(parent)->i_uid;

commit d8bbd13beeaacd6494954bf5b945b54ccb2af309
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 30 09:42:11 2018 +0200

    net: don not detour through struct sock to find the poll waitqueue
    
    For any open socket file descriptor sock->sk->sk_wq->wait will always
    point to sock->wq->wait.  That means we can do the shorter dereference
    and removal a NULL check and don't have to not worry about any RCU
    protection.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0518f61926ec..2afea5d1bdfe 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2064,10 +2064,9 @@ static inline bool skwq_has_sleeper(struct socket_wq *wq)
 static inline void sock_poll_wait(struct file *filp, poll_table *p)
 {
 	struct socket *sock = filp->private_data;
-	wait_queue_head_t *wq = sk_sleep(sock->sk);
 
-	if (!poll_does_not_wait(p) && wq) {
-		poll_wait(filp, wq, p);
+	if (!poll_does_not_wait(p)) {
+		poll_wait(filp, &sock->wq->wait, p);
 		/* We need to be sure we are in sync with the
 		 * socket flags modification.
 		 *

commit dd979b4df817e9976f18fb6f9d134d6bc4a3c317
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 30 09:42:10 2018 +0200

    net: simplify sock_poll_wait
    
    The wait_address argument is always directly derived from the filp
    argument, so remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 83b747538bd0..0518f61926ec 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2057,16 +2057,17 @@ static inline bool skwq_has_sleeper(struct socket_wq *wq)
 /**
  * sock_poll_wait - place memory barrier behind the poll_wait call.
  * @filp:           file
- * @wait_address:   socket wait queue
  * @p:              poll_table
  *
  * See the comments in the wq_has_sleeper function.
  */
-static inline void sock_poll_wait(struct file *filp,
-		wait_queue_head_t *wait_address, poll_table *p)
+static inline void sock_poll_wait(struct file *filp, poll_table *p)
 {
-	if (!poll_does_not_wait(p) && wait_address) {
-		poll_wait(filp, wait_address, p);
+	struct socket *sock = filp->private_data;
+	wait_queue_head_t *wq = sk_sleep(sock->sk);
+
+	if (!poll_does_not_wait(p) && wq) {
+		poll_wait(filp, wq, p);
 		/* We need to be sure we are in sync with the
 		 * socket flags modification.
 		 *

commit 657a0667025e77cc17f8a38b93e60a2bc24d830c
Author: Willem de Bruijn <willemb@google.com>
Date:   Fri Jul 6 10:12:56 2018 -0400

    sock: sockc cookie initializer
    
    Initialize the cookie in one location to reduce code duplication and
    avoid bugs from inconsistent initialization, such as that fixed in
    commit 9887cba19978 ("ip: limit use of gso_size to udp").
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e0eac9ef44b5..83b747538bd0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1600,6 +1600,12 @@ struct sockcm_cookie {
 	u16 tsflags;
 };
 
+static inline void sockcm_init(struct sockcm_cookie *sockc,
+			       const struct sock *sk)
+{
+	*sockc = (struct sockcm_cookie) { .tsflags = sk->sk_tsflags };
+}
+
 int __sock_cmsg_send(struct sock *sk, struct msghdr *msg, struct cmsghdr *cmsg,
 		     struct sockcm_cookie *sockc);
 int sock_cmsg_send(struct sock *sk, struct msghdr *msg,

commit 4b15c7075352668d4467ced7594b676707d11cae
Author: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
Date:   Tue Jul 3 15:43:00 2018 -0700

    net/sched: Make etf report drops on error_queue
    
    Use the socket error queue for reporting dropped packets if the
    socket has enabled that feature through the SO_TXTIME API.
    
    Packets are dropped either on enqueue() if they aren't accepted by the
    qdisc or on dequeue() if the system misses their deadline. Those are
    reported as different errors so applications can react accordingly.
    
    Userspace can retrieve the errors through the socket error queue and the
    corresponding cmsg interfaces. A struct sock_extended_err* is used for
    returning the error data, and the packet's timestamp can be retrieved by
    adding both ee_data and ee_info fields as e.g.:
    
        ((__u64) serr->ee_data << 32) + serr->ee_info
    
    This feature is disabled by default and must be explicitly enabled by
    applications. Enabling it can bring some overhead for the Tx cycles
    of the application.
    
    Signed-off-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 68347b9821c6..e0eac9ef44b5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -481,7 +481,8 @@ struct sock {
 
 	u8			sk_clockid;
 	u8			sk_txtime_deadline_mode : 1,
-				sk_txtime_unused : 7;
+				sk_txtime_report_errors : 1,
+				sk_txtime_unused : 6;
 
 	struct socket		*sk_socket;
 	void			*sk_user_data;

commit 80b14dee2bea128928537d61c333f24cb8cbb62f
Author: Richard Cochran <rcochran@linutronix.de>
Date:   Tue Jul 3 15:42:48 2018 -0700

    net: Add a new socket option for a future transmit time.
    
    This patch introduces SO_TXTIME. User space enables this option in
    order to pass a desired future transmit time in a CMSG when calling
    sendmsg(2). The argument to this socket option is a 8-bytes long struct
    provided by the uapi header net_tstamp.h defined as:
    
    struct sock_txtime {
            clockid_t       clockid;
            u32             flags;
    };
    
    Note that new fields were added to struct sock by filling a 2-bytes
    hole found in the struct. For that reason, neither the struct size or
    number of cachelines were altered.
    
    Signed-off-by: Richard Cochran <rcochran@linutronix.de>
    Signed-off-by: Jesus Sanchez-Palencia <jesus.sanchez-palencia@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2ed99bfa4595..68347b9821c6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -319,6 +319,9 @@ struct sock_common {
   *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
   *	@sk_reuseport_cb: reuseport group container
   *	@sk_rcu: used during RCU grace period
+  *	@sk_clockid: clockid used by time-based scheduling (SO_TXTIME)
+  *	@sk_txtime_deadline_mode: set deadline mode for SO_TXTIME
+  *	@sk_txtime_unused: unused txtime flags
   */
 struct sock {
 	/*
@@ -475,6 +478,11 @@ struct sock {
 	u8			sk_shutdown;
 	u32			sk_tskey;
 	atomic_t		sk_zckey;
+
+	u8			sk_clockid;
+	u8			sk_txtime_deadline_mode : 1,
+				sk_txtime_unused : 7;
+
 	struct socket		*sk_socket;
 	void			*sk_user_data;
 #ifdef CONFIG_SECURITY
@@ -790,6 +798,7 @@ enum sock_flags {
 	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 	SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
+	SOCK_TXTIME,
 };
 
 #define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))
@@ -1585,6 +1594,7 @@ void sock_kzfree_s(struct sock *sk, void *mem, int size);
 void sk_send_sigurg(struct sock *sk);
 
 struct sockcm_cookie {
+	u64 transmit_time;
 	u32 mark;
 	u16 tsflags;
 };

commit fc9bab24e9c654f62f3d411fc0b041be9e487e9d
Author: Amritha Nambiar <amritha.nambiar@intel.com>
Date:   Fri Jun 29 21:27:02 2018 -0700

    net: Enable Tx queue selection based on Rx queues
    
    This patch adds support to pick Tx queue based on the Rx queue(s) map
    configuration set by the admin through the sysfs attribute
    for each Tx queue. If the user configuration for receive queue(s) map
    does not apply, then the Tx queue selection falls back to CPU(s) map
    based selection and finally to hashing.
    
    Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2b097cc89727..2ed99bfa4595 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1730,6 +1730,16 @@ static inline void sk_rx_queue_clear(struct sock *sk)
 #endif
 }
 
+#ifdef CONFIG_XPS
+static inline int sk_rx_queue_get(const struct sock *sk)
+{
+	if (sk && sk->sk_rx_queue_mapping != NO_QUEUE_MAPPING)
+		return sk->sk_rx_queue_mapping;
+
+	return -1;
+}
+#endif
+
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 {
 	sk_tx_queue_clear(sk);

commit c6345ce7d361dce1b5d02a2181ccb598c27fd7ae
Author: Amritha Nambiar <amritha.nambiar@intel.com>
Date:   Fri Jun 29 21:26:57 2018 -0700

    net: Record receive queue number for a connection
    
    This patch adds a new field to sock_common 'skc_rx_queue_mapping'
    which holds the receive queue number for the connection. The Rx queue
    is marked in tcp_finish_connect() to allow a client app to do
    SO_INCOMING_NAPI_ID after a connect() call to get the right queue
    association for a socket. Rx queue is also marked in tcp_conn_request()
    to allow syn-ack to go on the right tx-queue associated with
    the queue on which syn is received.
    
    Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
    Signed-off-by: Sridhar Samudrala <sridhar.samudrala@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 37b09c84504b..2b097cc89727 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -139,6 +139,7 @@ typedef __u64 __bitwise __addrpair;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
  *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_rx_queue_mapping: rx queue number for this connection
  *	@skc_flags: place holder for sk_flags
  *		%SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
  *		%SO_OOBINLINE settings, %SO_TIMESTAMPING settings
@@ -215,6 +216,9 @@ struct sock_common {
 		struct hlist_nulls_node skc_nulls_node;
 	};
 	unsigned short		skc_tx_queue_mapping;
+#ifdef CONFIG_XPS
+	unsigned short		skc_rx_queue_mapping;
+#endif
 	union {
 		int		skc_incoming_cpu;
 		u32		skc_rcv_wnd;
@@ -326,6 +330,9 @@ struct sock {
 #define sk_nulls_node		__sk_common.skc_nulls_node
 #define sk_refcnt		__sk_common.skc_refcnt
 #define sk_tx_queue_mapping	__sk_common.skc_tx_queue_mapping
+#ifdef CONFIG_XPS
+#define sk_rx_queue_mapping	__sk_common.skc_rx_queue_mapping
+#endif
 
 #define sk_dontcopy_begin	__sk_common.skc_dontcopy_begin
 #define sk_dontcopy_end		__sk_common.skc_dontcopy_end
@@ -1702,6 +1709,27 @@ static inline int sk_tx_queue_get(const struct sock *sk)
 	return -1;
 }
 
+static inline void sk_rx_queue_set(struct sock *sk, const struct sk_buff *skb)
+{
+#ifdef CONFIG_XPS
+	if (skb_rx_queue_recorded(skb)) {
+		u16 rx_queue = skb_get_rx_queue(skb);
+
+		if (WARN_ON_ONCE(rx_queue == NO_QUEUE_MAPPING))
+			return;
+
+		sk->sk_rx_queue_mapping = rx_queue;
+	}
+#endif
+}
+
+static inline void sk_rx_queue_clear(struct sock *sk)
+{
+#ifdef CONFIG_XPS
+	sk->sk_rx_queue_mapping = NO_QUEUE_MAPPING;
+#endif
+}
+
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 {
 	sk_tx_queue_clear(sk);

commit 755c31cd85aea35cf7a5e7253851b52c08eff6e9
Author: Amritha Nambiar <amritha.nambiar@intel.com>
Date:   Fri Jun 29 21:26:51 2018 -0700

    net: sock: Change tx_queue_mapping in sock_common to unsigned short
    
    Change 'skc_tx_queue_mapping' field in sock_common structure from
    'int' to 'unsigned short' type with ~0 indicating unset and
    other positive queue values being set. This will accommodate adding
    a new 'unsigned short' field in sock_common in the next patch for
    rx_queue_mapping.
    
    Signed-off-by: Amritha Nambiar <amritha.nambiar@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b3b75419eafe..37b09c84504b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -214,7 +214,7 @@ struct sock_common {
 		struct hlist_node	skc_node;
 		struct hlist_nulls_node skc_nulls_node;
 	};
-	int			skc_tx_queue_mapping;
+	unsigned short		skc_tx_queue_mapping;
 	union {
 		int		skc_incoming_cpu;
 		u32		skc_rcv_wnd;
@@ -1681,17 +1681,25 @@ static inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
 
 static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
 {
+	/* sk_tx_queue_mapping accept only upto a 16-bit value */
+	if (WARN_ON_ONCE((unsigned short)tx_queue >= USHRT_MAX))
+		return;
 	sk->sk_tx_queue_mapping = tx_queue;
 }
 
+#define NO_QUEUE_MAPPING	USHRT_MAX
+
 static inline void sk_tx_queue_clear(struct sock *sk)
 {
-	sk->sk_tx_queue_mapping = -1;
+	sk->sk_tx_queue_mapping = NO_QUEUE_MAPPING;
 }
 
 static inline int sk_tx_queue_get(const struct sock *sk)
 {
-	return sk ? sk->sk_tx_queue_mapping : -1;
+	if (sk && sk->sk_tx_queue_mapping != NO_QUEUE_MAPPING)
+		return sk->sk_tx_queue_mapping;
+
+	return -1;
 }
 
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)

commit 1c8c5a9d38f607c0b6fd12c91cbe1a4418762a21
Merge: 285767604576 7170e6045a6a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 6 18:39:49 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add Maglev hashing scheduler to IPVS, from Inju Song.
    
     2) Lots of new TC subsystem tests from Roman Mashak.
    
     3) Add TCP zero copy receive and fix delayed acks and autotuning with
        SO_RCVLOWAT, from Eric Dumazet.
    
     4) Add XDP_REDIRECT support to mlx5 driver, from Jesper Dangaard
        Brouer.
    
     5) Add ttl inherit support to vxlan, from Hangbin Liu.
    
     6) Properly separate ipv6 routes into their logically independant
        components. fib6_info for the routing table, and fib6_nh for sets of
        nexthops, which thus can be shared. From David Ahern.
    
     7) Add bpf_xdp_adjust_tail helper, which can be used to generate ICMP
        messages from XDP programs. From Nikita V. Shirokov.
    
     8) Lots of long overdue cleanups to the r8169 driver, from Heiner
        Kallweit.
    
     9) Add BTF ("BPF Type Format"), from Martin KaFai Lau.
    
    10) Add traffic condition monitoring to iwlwifi, from Luca Coelho.
    
    11) Plumb extack down into fib_rules, from Roopa Prabhu.
    
    12) Add Flower classifier offload support to igb, from Vinicius Costa
        Gomes.
    
    13) Add UDP GSO support, from Willem de Bruijn.
    
    14) Add documentation for eBPF helpers, from Quentin Monnet.
    
    15) Add TLS tx offload to mlx5, from Ilya Lesokhin.
    
    16) Allow applications to be given the number of bytes available to read
        on a socket via a control message returned from recvmsg(), from
        Soheil Hassas Yeganeh.
    
    17) Add x86_32 eBPF JIT compiler, from Wang YanQing.
    
    18) Add AF_XDP sockets, with zerocopy support infrastructure as well.
        From Björn Töpel.
    
    19) Remove indirect load support from all of the BPF JITs and handle
        these operations in the verifier by translating them into native BPF
        instead. From Daniel Borkmann.
    
    20) Add GRO support to ipv6 gre tunnels, from Eran Ben Elisha.
    
    21) Allow XDP programs to do lookups in the main kernel routing tables
        for forwarding. From David Ahern.
    
    22) Allow drivers to store hardware state into an ELF section of kernel
        dump vmcore files, and use it in cxgb4. From Rahul Lakkireddy.
    
    23) Various RACK and loss detection improvements in TCP, from Yuchung
        Cheng.
    
    24) Add TCP SACK compression, from Eric Dumazet.
    
    25) Add User Mode Helper support and basic bpfilter infrastructure, from
        Alexei Starovoitov.
    
    26) Support ports and protocol values in RTM_GETROUTE, from Roopa
        Prabhu.
    
    27) Support bulking in ->ndo_xdp_xmit() API, from Jesper Dangaard
        Brouer.
    
    28) Add lots of forwarding selftests, from Petr Machata.
    
    29) Add generic network device failover driver, from Sridhar Samudrala.
    
    * ra.kernel.org:/pub/scm/linux/kernel/git/davem/net-next: (1959 commits)
      strparser: Add __strp_unpause and use it in ktls.
      rxrpc: Fix terminal retransmission connection ID to include the channel
      net: hns3: Optimize PF CMDQ interrupt switching process
      net: hns3: Fix for VF mailbox receiving unknown message
      net: hns3: Fix for VF mailbox cannot receiving PF response
      bnx2x: use the right constant
      Revert "net: sched: cls: Fix offloading when ingress dev is vxlan"
      net: dsa: b53: Fix for brcm tag issue in Cygnus SoC
      enic: fix UDP rss bits
      netdev-FAQ: clarify DaveM's position for stable backports
      rtnetlink: validate attributes in do_setlink()
      mlxsw: Add extack messages for port_{un, }split failures
      netdevsim: Add extack error message for devlink reload
      devlink: Add extack to reload and port_{un, }split operations
      net: metrics: add proper netlink validation
      ipmr: fix error path when ipmr_new_table fails
      ip6mr: only set ip6mr_table from setsockopt when ip6mr_new_table succeeds
      net: hns3: remove unused hclgevf_cfg_func_mta_filter
      netfilter: provide udp*_lib_lookup for nf_tproxy
      qed*: Utilize FW 8.37.2.0
      ...

commit 984652dd8b1f0998b9a181944ad5a00d06f9586f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Apr 9 15:26:26 2018 +0200

    net: remove sock_no_poll
    
    Now that sock_poll handles a NULL ->poll or ->poll_mask there is no need
    for a stub.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/net/sock.h b/include/net/sock.h
index 74d725fdbe0f..4d2e8ad98985 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1591,8 +1591,6 @@ int sock_no_connect(struct socket *, struct sockaddr *, int, int);
 int sock_no_socketpair(struct socket *, struct socket *);
 int sock_no_accept(struct socket *, struct socket *, int, bool);
 int sock_no_getname(struct socket *, struct sockaddr *, int);
-__poll_t sock_no_poll(struct file *, struct socket *,
-			  struct poll_table_struct *);
 int sock_no_ioctl(struct socket *, unsigned int, unsigned long);
 int sock_no_listen(struct socket *, int);
 int sock_no_shutdown(struct socket *, int);

commit a7950ae8213cf38343fd27ad1fb58f3f04e3130f
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue May 8 09:06:59 2018 -0700

    net/sock: Update memalloc_socks static key to modern api
    
    No changes in refcount semantics -- key init is false; replace
    
    static_key_slow_inc|dec   with   static_branch_inc|dec
    static_key_false          with   static_branch_unlikely
    
    Added a '_key' suffix to memalloc_socks, for better self
    documentation.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3c568b36ee36..4f7c584e9765 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -808,10 +808,10 @@ static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
 }
 
 #ifdef CONFIG_NET
-extern struct static_key memalloc_socks;
+DECLARE_STATIC_KEY_FALSE(memalloc_socks_key);
 static inline int sk_memalloc_socks(void)
 {
-	return static_key_false(&memalloc_socks);
+	return static_branch_unlikely(&memalloc_socks_key);
 }
 #else
 

commit ebf4e808fa0b22e551baf862e17c26c325c068f4
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:12 2018 +0300

    net: Add Software fallback infrastructure for socket dependent offloads
    
    With socket dependent offloads we rely on the netdev to transform
    the transmitted packets before sending them to the wire.
    When a packet from an offloaded socket is rerouted to a different
    device we need to detect it and do the transformation in software.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 74d725fdbe0f..3c568b36ee36 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -481,6 +481,11 @@ struct sock {
 	void			(*sk_error_report)(struct sock *sk);
 	int			(*sk_backlog_rcv)(struct sock *sk,
 						  struct sk_buff *skb);
+#ifdef CONFIG_SOCK_VALIDATE_XMIT
+	struct sk_buff*		(*sk_validate_xmit_skb)(struct sock *sk,
+							struct net_device *dev,
+							struct sk_buff *skb);
+#endif
 	void                    (*sk_destruct)(struct sock *sk);
 	struct sock_reuseport __rcu	*sk_reuseport_cb;
 	struct rcu_head		sk_rcu;
@@ -2332,6 +2337,22 @@ static inline bool sk_fullsock(const struct sock *sk)
 	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);
 }
 
+/* Checks if this SKB belongs to an HW offloaded socket
+ * and whether any SW fallbacks are required based on dev.
+ */
+static inline struct sk_buff *sk_validate_xmit_skb(struct sk_buff *skb,
+						   struct net_device *dev)
+{
+#ifdef CONFIG_SOCK_VALIDATE_XMIT
+	struct sock *sk = skb->sk;
+
+	if (sk && sk_fullsock(sk) && sk->sk_validate_xmit_skb)
+		skb = sk->sk_validate_xmit_skb(sk, dev, skb);
+#endif
+
+	return skb;
+}
+
 /* This helper checks if a socket is a LISTEN or NEW_SYN_RECV
  * SYNACK messages can be attached to either ones (depending on SYNCOOKIE)
  */

commit 7bbdb81ee3de73f2381ceec1bbee831f4c913b5c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 5 16:21:31 2018 -0700

    slab: make usercopy region 32-bit
    
    If kmem case sizes are 32-bit, then usecopy region should be too.
    
    Link: http://lkml.kernel.org/r/20180305200730.15812-21-adobriyan@gmail.com
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 49bd2c1796b0..74d725fdbe0f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1114,8 +1114,8 @@ struct proto {
 	struct kmem_cache	*slab;
 	unsigned int		obj_size;
 	slab_flags_t		slab_flags;
-	size_t			useroffset;	/* Usercopy region offset */
-	size_t			usersize;	/* Usercopy region size */
+	unsigned int		useroffset;	/* Usercopy region offset */
+	unsigned int		usersize;	/* Usercopy region size */
 
 	struct percpu_counter	*orphan_count;
 

commit d74bad4e74ee373787a9ae24197c17b7cdc428d5
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:05 2018 -0700

    bpf: Hooks for sys_connect
    
    == The problem ==
    
    See description of the problem in the initial patch of this patch set.
    
    == The solution ==
    
    The patch provides much more reliable in-kernel solution for the 2nd
    part of the problem: making outgoing connecttion from desired IP.
    
    It adds new attach types `BPF_CGROUP_INET4_CONNECT` and
    `BPF_CGROUP_INET6_CONNECT` for program type
    `BPF_PROG_TYPE_CGROUP_SOCK_ADDR` that can be used to override both
    source and destination of a connection at connect(2) time.
    
    Local end of connection can be bound to desired IP using newly
    introduced BPF-helper `bpf_bind()`. It allows to bind to only IP though,
    and doesn't support binding to port, i.e. leverages
    `IP_BIND_ADDRESS_NO_PORT` socket option. There are two reasons for this:
    * looking for a free port is expensive and can affect performance
      significantly;
    * there is no use-case for port.
    
    As for remote end (`struct sockaddr *` passed by user), both parts of it
    can be overridden, remote IP and remote port. It's useful if an
    application inside cgroup wants to connect to another application inside
    same cgroup or to itself, but knows nothing about IP assigned to the
    cgroup.
    
    Support is added for IPv4 and IPv6, for TCP and UDP.
    
    IPv4 and IPv6 have separate attach types for same reason as sys_bind
    hooks, i.e. to prevent reading from / writing to e.g. user_ip6 fields
    when user passes sockaddr_in since it'd be out-of-bound.
    
    == Implementation notes ==
    
    The patch introduces new field in `struct proto`: `pre_connect` that is
    a pointer to a function with same signature as `connect` but is called
    before it. The reason is in some cases BPF hooks should be called way
    before control is passed to `sk->sk_prot->connect`. Specifically
    `inet_dgram_connect` autobinds socket before calling
    `sk->sk_prot->connect` and there is no way to call `bpf_bind()` from
    hooks from e.g. `ip4_datagram_connect` or `ip6_datagram_connect` since
    it'd cause double-bind. On the other hand `proto.pre_connect` provides a
    flexible way to add BPF hooks for connect only for necessary `proto` and
    call them at desired time before `connect`. Since `bpf_bind()` is
    allowed to bind only to IP and autobind in `inet_dgram_connect` binds
    only port there is no chance of double-bind.
    
    bpf_bind() sets `force_bind_address_no_port` to bind to only IP despite
    of value of `bind_address_no_port` socket field.
    
    bpf_bind() sets `with_lock` to `false` when calling to __inet_bind()
    and __inet6_bind() since all call-sites, where bpf_bind() is called,
    already hold socket lock.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b8ff435fa96e..49bd2c1796b0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1026,6 +1026,9 @@ static inline void sk_prot_clear_nulls(struct sock *sk, int size)
 struct proto {
 	void			(*close)(struct sock *sk,
 					long timeout);
+	int			(*pre_connect)(struct sock *sk,
+					struct sockaddr *uaddr,
+					int addr_len);
 	int			(*connect)(struct sock *sk,
 					struct sockaddr *uaddr,
 					int addr_len);

commit 8934ce2fd08171e8605f7fada91ee7619fe17ab8
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Wed Mar 28 12:49:15 2018 -0700

    bpf: sockmap redirect ingress support
    
    Add support for the BPF_F_INGRESS flag in sk_msg redirect helper.
    To do this add a scatterlist ring for receiving socks to check
    before calling into regular recvmsg call path. Additionally, because
    the poll wakeup logic only checked the skb recv queue we need to
    add a hook in TCP stack (similar to write side) so that we have
    a way to wake up polling socks when a scatterlist is redirected
    to that sock.
    
    After this all that is needed is for the redirect helper to
    push the scatterlist into the psock receive queue.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 709311132d4c..b8ff435fa96e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1085,6 +1085,7 @@ struct proto {
 #endif
 
 	bool			(*stream_memory_free)(const struct sock *sk);
+	bool			(*stream_memory_read)(const struct sock *sk);
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
 	void			(*leave_memory_pressure)(struct sock *sk);

commit 03fe2debbb2771fb90881e4ce8109b09cf772a5c
Merge: 6686c459e144 f36b7534b833
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Mar 23 11:24:57 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Fun set of conflict resolutions here...
    
    For the mac80211 stuff, these were fortunately just parallel
    adds.  Trivially resolved.
    
    In drivers/net/phy/phy.c we had a bug fix in 'net' that moved the
    function phy_disable_interrupts() earlier in the file, whilst in
    'net-next' the phy_error() call from this function was removed.
    
    In net/ipv4/xfrm4_policy.c, David Ahern's changes to remove the
    'rt_table_id' member of rtable collided with a bug fix in 'net' that
    added a new struct member "rt_mtu_locked" which needs to be copied
    over here.
    
    The mlxsw driver conflict consisted of net-next separating
    the span code and definitions into separate files, whilst
    a 'net' bug fix made some changes to that moved code.
    
    The mlx5 infiniband conflict resolution was quite non-trivial,
    the RDMA tree's merge commit was used as a guide here, and
    here are their notes:
    
    ====================
    
        Due to bug fixes found by the syzkaller bot and taken into the for-rc
        branch after development for the 4.17 merge window had already started
        being taken into the for-next branch, there were fairly non-trivial
        merge issues that would need to be resolved between the for-rc branch
        and the for-next branch.  This merge resolves those conflicts and
        provides a unified base upon which ongoing development for 4.17 can
        be based.
    
        Conflicts:
                drivers/infiniband/hw/mlx5/main.c - Commit 42cea83f9524
                (IB/mlx5: Fix cleanup order on unload) added to for-rc and
                commit b5ca15ad7e61 (IB/mlx5: Add proper representors support)
                add as part of the devel cycle both needed to modify the
                init/de-init functions used by mlx5.  To support the new
                representors, the new functions added by the cleanup patch
                needed to be made non-static, and the init/de-init list
                added by the representors patch needed to be modified to
                match the init/de-init list changes made by the cleanup
                patch.
        Updates:
                drivers/infiniband/hw/mlx5/mlx5_ib.h - Update function
                prototypes added by representors patch to reflect new function
                names as changed by cleanup patch
                drivers/infiniband/hw/mlx5/ib_rep.c - Update init/de-init
                stage list to match new order from cleanup patch
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8c05dbf04b2882c3c0bc43fe7668c720210877f3
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Mar 18 12:57:05 2018 -0700

    net: generalize sk_alloc_sg to work with scatterlist rings
    
    The current implementation of sk_alloc_sg expects scatterlist to always
    start at entry 0 and complete at entry MAX_SKB_FRAGS.
    
    Future patches will want to support starting at arbitrary offset into
    scatterlist so add an additional sg_start parameters and then default
    to the current values in TLS code paths.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 447150c51feb..b7c75e024e37 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2142,7 +2142,7 @@ static inline struct page_frag *sk_page_frag(struct sock *sk)
 bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
 
 int sk_alloc_sg(struct sock *sk, int len, struct scatterlist *sg,
-		int *sg_num_elem, unsigned int *sg_size,
+		int sg_start, int *sg_curr, unsigned int *sg_size,
 		int first_coalesce);
 
 /*

commit 2c3682f0be97a5f57c6c8b40fa154dfc77efb461
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Mar 18 12:56:49 2018 -0700

    sock: make static tls function alloc_sg generic sock helper
    
    The TLS ULP module builds scatterlists from a sock using
    page_frag_refill(). This is going to be useful for other ULPs
    so move it into sock file for more general use.
    
    In the process remove useless goto at end of while loop.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b9624581d639..447150c51feb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2141,6 +2141,10 @@ static inline struct page_frag *sk_page_frag(struct sock *sk)
 
 bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
 
+int sk_alloc_sg(struct sock *sk, int len, struct scatterlist *sg,
+		int *sg_num_elem, unsigned int *sg_size,
+		int first_coalesce);
+
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */

commit bf2ae2e4bf9360e07c0cdfa166bcdc0afd92f4ce
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Mar 10 18:57:50 2018 +0800

    sock_diag: request _diag module only when the family or proto has been registered
    
    Now when using 'ss' in iproute, kernel would try to load all _diag
    modules, which also causes corresponding family and proto modules
    to be loaded as well due to module dependencies.
    
    Like after running 'ss', sctp, dccp, af_packet (if it works as a module)
    would be loaded.
    
    For example:
    
      $ lsmod|grep sctp
      $ ss
      $ lsmod|grep sctp
      sctp_diag              16384  0
      sctp                  323584  5 sctp_diag
      inet_diag              24576  4 raw_diag,tcp_diag,sctp_diag,udp_diag
      libcrc32c              16384  3 nf_conntrack,nf_nat,sctp
    
    As these family and proto modules are loaded unintentionally, it
    could cause some problems, like:
    
    - Some debug tools use 'ss' to collect the socket info, which loads all
      those diag and family and protocol modules. It's noisy for identifying
      issues.
    
    - Users usually expect to drop sctp init packet silently when they
      have no sense of sctp protocol instead of sending abort back.
    
    - It wastes resources (especially with multiple netns), and SCTP module
      can't be unloaded once it's loaded.
    
    ...
    
    In short, it's really inappropriate to have these family and proto
    modules loaded unexpectedly when just doing debugging with inet_diag.
    
    This patch is to introduce sock_load_diag_module() where it loads
    the _diag module only when it's corresponding family or proto has
    been already registered.
    
    Note that we can't just load _diag module without the family or
    proto loaded, as some symbols used in _diag module are from the
    family or proto module.
    
    v1->v2:
      - move inet proto check to inet_diag to avoid a compiling err.
    v2->v3:
      - define sock_load_diag_module in sock.c and export one symbol
        only.
      - improve the changelog.
    
    Reported-by: Sabrina Dubroca <sd@queasysnail.net>
    Acked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Acked-by: Phil Sutter <phil@nwl.cc>
    Acked-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 169c92afcafa..ae23f3b389ca 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1137,6 +1137,7 @@ struct proto {
 
 int proto_register(struct proto *prot, int alloc_slab);
 void proto_unregister(struct proto *prot);
+int sock_load_diag_module(int family, int protocol);
 
 #ifdef SOCK_REFCNT_DEBUG
 static inline void sk_refcnt_debug_inc(struct sock *sk)

commit dead7cdb0daec58490891e59f4fae0c5c76fa5f3
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Feb 19 11:56:49 2018 -0800

    tcp: remove sk_check_csum_caps()
    
    Since TCP relies on GSO, we do not need this helper anymore.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f0f576ff5603..b9624581d639 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1863,15 +1863,6 @@ static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
 	sk->sk_route_caps &= ~flags;
 }
 
-static inline bool sk_check_csum_caps(struct sock *sk)
-{
-	return (sk->sk_route_caps & NETIF_F_HW_CSUM) ||
-	       (sk->sk_family == PF_INET &&
-		(sk->sk_route_caps & NETIF_F_IP_CSUM)) ||
-	       (sk->sk_family == PF_INET6 &&
-		(sk->sk_route_caps & NETIF_F_IPV6_CSUM));
-}
-
 static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
 					   struct iov_iter *from, char *to,
 					   int copy, int offset)

commit 0a6b2a1dc2a2105f178255fe495eb914b09cb37a
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Feb 19 11:56:47 2018 -0800

    tcp: switch to GSO being always on
    
    Oleksandr Natalenko reported performance issues with BBR without FQ
    packet scheduler that were root caused to lack of SG and GSO/TSO on
    his configuration.
    
    In this mode, TCP internal pacing has to setup a high resolution timer
    for each MSS sent.
    
    We could implement in TCP a strategy similar to the one adopted
    in commit fefa569a9d4b ("net_sched: sch_fq: account for schedule/timers drifts")
    or decide to finally switch TCP stack to a GSO only mode.
    
    This has many benefits :
    
    1) Most TCP developments are done with TSO in mind.
    2) Less high-resolution timers needs to be armed for TCP-pacing
    3) GSO can benefit of xmit_more hint
    4) Receiver GRO is more effective (as if TSO was used for real on sender)
       -> Lower ACK traffic
    5) Write queues have less overhead (one skb holds about 64KB of payload)
    6) SACK coalescing just works.
    7) rtx rb-tree contains less packets, SACK is cheaper.
    
    This patch implements the minimum patch, but we can remove some legacy
    code as follow ups.
    
    Tested:
    
    On 40Gbit link, one netperf -t TCP_STREAM
    
    BBR+fq:
    sg on:  26 Gbits/sec
    sg off: 15.7 Gbits/sec   (was 2.3 Gbit before patch)
    
    BBR+pfifo_fast:
    sg on:  24.2 Gbits/sec
    sg off: 14.9 Gbits/sec  (was 0.66 Gbit before patch !!! )
    
    BBR+fq_codel:
    sg on:  24.4 Gbits/sec
    sg off: 15 Gbits/sec  (was 0.66 Gbit before patch !!! )
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3aa7b7d6e6c7..f0f576ff5603 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -417,6 +417,7 @@ struct sock {
 	struct page_frag	sk_frag;
 	netdev_features_t	sk_route_caps;
 	netdev_features_t	sk_route_nocaps;
+	netdev_features_t	sk_route_forced_caps;
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
 	gfp_t			sk_allocation;

commit 9b2c45d479d0fb8647c9e83359df69162b5fbe5f
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Mon Feb 12 20:00:20 2018 +0100

    net: make getname() functions return length rather than use int* parameter
    
    Changes since v1:
    Added changes in these files:
        drivers/infiniband/hw/usnic/usnic_transport.c
        drivers/staging/lustre/lnet/lnet/lib-socket.c
        drivers/target/iscsi/iscsi_target_login.c
        drivers/vhost/net.c
        fs/dlm/lowcomms.c
        fs/ocfs2/cluster/tcp.c
        security/tomoyo/network.c
    
    Before:
    All these functions either return a negative error indicator,
    or store length of sockaddr into "int *socklen" parameter
    and return zero on success.
    
    "int *socklen" parameter is awkward. For example, if caller does not
    care, it still needs to provide on-stack storage for the value
    it does not need.
    
    None of the many FOO_getname() functions of various protocols
    ever used old value of *socklen. They always just overwrite it.
    
    This change drops this parameter, and makes all these functions, on success,
    return length of sockaddr. It's always >= 0 and can be differentiated
    from an error.
    
    Tests in callers are changed from "if (err)" to "if (err < 0)", where needed.
    
    rpc_sockname() lost "int buflen" parameter, since its only use was
    to be passed to kernel_getsockname() as &buflen and subsequently
    not used in any way.
    
    Userspace API is not changed.
    
        text    data     bss      dec     hex filename
    30108430 2633624  873672 33615726 200ef6e vmlinux.before.o
    30108109 2633612  873672 33615393 200ee21 vmlinux.o
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: linux-kernel@vger.kernel.org
    CC: netdev@vger.kernel.org
    CC: linux-bluetooth@vger.kernel.org
    CC: linux-decnet-user@lists.sourceforge.net
    CC: linux-wireless@vger.kernel.org
    CC: linux-rdma@vger.kernel.org
    CC: linux-sctp@vger.kernel.org
    CC: linux-nfs@vger.kernel.org
    CC: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 169c92afcafa..3aa7b7d6e6c7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1584,7 +1584,7 @@ int sock_no_bind(struct socket *, struct sockaddr *, int);
 int sock_no_connect(struct socket *, struct sockaddr *, int, int);
 int sock_no_socketpair(struct socket *, struct socket *);
 int sock_no_accept(struct socket *, struct socket *, int, bool);
-int sock_no_getname(struct socket *, struct sockaddr *, int *, int);
+int sock_no_getname(struct socket *, struct sockaddr *, int);
 __poll_t sock_no_poll(struct file *, struct socket *,
 			  struct poll_table_struct *);
 int sock_no_ioctl(struct socket *, unsigned int, unsigned long);

commit 617aebe6a97efa539cc4b8a52adccd89596e6be0
Merge: 0771ad44a20b e47e311843de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 3 16:25:42 2018 -0800

    Merge tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull hardened usercopy whitelisting from Kees Cook:
     "Currently, hardened usercopy performs dynamic bounds checking on slab
      cache objects. This is good, but still leaves a lot of kernel memory
      available to be copied to/from userspace in the face of bugs.
    
      To further restrict what memory is available for copying, this creates
      a way to whitelist specific areas of a given slab cache object for
      copying to/from userspace, allowing much finer granularity of access
      control.
    
      Slab caches that are never exposed to userspace can declare no
      whitelist for their objects, thereby keeping them unavailable to
      userspace via dynamic copy operations. (Note, an implicit form of
      whitelisting is the use of constant sizes in usercopy operations and
      get_user()/put_user(); these bypass all hardened usercopy checks since
      these sizes cannot change at runtime.)
    
      This new check is WARN-by-default, so any mistakes can be found over
      the next several releases without breaking anyone's system.
    
      The series has roughly the following sections:
       - remove %p and improve reporting with offset
       - prepare infrastructure and whitelist kmalloc
       - update VFS subsystem with whitelists
       - update SCSI subsystem with whitelists
       - update network subsystem with whitelists
       - update process memory with whitelists
       - update per-architecture thread_struct with whitelists
       - update KVM with whitelists and fix ioctl bug
       - mark all other allocations as not whitelisted
       - update lkdtm for more sensible test overage"
    
    * tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux: (38 commits)
      lkdtm: Update usercopy tests for whitelisting
      usercopy: Restrict non-usercopy caches to size 0
      kvm: x86: fix KVM_XEN_HVM_CONFIG ioctl
      kvm: whitelist struct kvm_vcpu_arch
      arm: Implement thread_struct whitelist for hardened usercopy
      arm64: Implement thread_struct whitelist for hardened usercopy
      x86: Implement thread_struct whitelist for hardened usercopy
      fork: Provide usercopy whitelisting for task_struct
      fork: Define usercopy region in thread_stack slab caches
      fork: Define usercopy region in mm_struct slab caches
      net: Restrict unwhitelisted proto caches to size 0
      sctp: Copy struct sctp_sock.autoclose to userspace using put_user()
      sctp: Define usercopy region in SCTP proto slab cache
      caif: Define usercopy region in caif proto slab cache
      ip: Define usercopy region in IP proto slab cache
      net: Define usercopy region in struct proto slab cache
      scsi: Define usercopy region in scsi_sense_cache slab cache
      cifs: Define usercopy region in cifs_request slab cache
      vxfs: Define usercopy region in vxfs_inode slab cache
      ufs: Define usercopy region in ufs_inode_cache slab cache
      ...

commit b2fe5fa68642860e7de76167c3111623aa0d5de1
Merge: a103950e0dd2 a54667f6728c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 31 14:31:10 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Significantly shrink the core networking routing structures. Result
        of http://vger.kernel.org/~davem/seoul2017_netdev_keynote.pdf
    
     2) Add netdevsim driver for testing various offloads, from Jakub
        Kicinski.
    
     3) Support cross-chip FDB operations in DSA, from Vivien Didelot.
    
     4) Add a 2nd listener hash table for TCP, similar to what was done for
        UDP. From Martin KaFai Lau.
    
     5) Add eBPF based queue selection to tun, from Jason Wang.
    
     6) Lockless qdisc support, from John Fastabend.
    
     7) SCTP stream interleave support, from Xin Long.
    
     8) Smoother TCP receive autotuning, from Eric Dumazet.
    
     9) Lots of erspan tunneling enhancements, from William Tu.
    
    10) Add true function call support to BPF, from Alexei Starovoitov.
    
    11) Add explicit support for GRO HW offloading, from Michael Chan.
    
    12) Support extack generation in more netlink subsystems. From Alexander
        Aring, Quentin Monnet, and Jakub Kicinski.
    
    13) Add 1000BaseX, flow control, and EEE support to mvneta driver. From
        Russell King.
    
    14) Add flow table abstraction to netfilter, from Pablo Neira Ayuso.
    
    15) Many improvements and simplifications to the NFP driver bpf JIT,
        from Jakub Kicinski.
    
    16) Support for ipv6 non-equal cost multipath routing, from Ido
        Schimmel.
    
    17) Add resource abstration to devlink, from Arkadi Sharshevsky.
    
    18) Packet scheduler classifier shared filter block support, from Jiri
        Pirko.
    
    19) Avoid locking in act_csum, from Davide Caratti.
    
    20) devinet_ioctl() simplifications from Al viro.
    
    21) More TCP bpf improvements from Lawrence Brakmo.
    
    22) Add support for onlink ipv6 route flag, similar to ipv4, from David
        Ahern.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1925 commits)
      tls: Add support for encryption using async offload accelerator
      ip6mr: fix stale iterator
      net/sched: kconfig: Remove blank help texts
      openvswitch: meter: Use 64-bit arithmetic instead of 32-bit
      tcp_nv: fix potential integer overflow in tcpnv_acked
      r8169: fix RTL8168EP take too long to complete driver initialization.
      qmi_wwan: Add support for Quectel EP06
      rtnetlink: enable IFLA_IF_NETNSID for RTM_NEWLINK
      ipmr: Fix ptrdiff_t print formatting
      ibmvnic: Wait for device response when changing MAC
      qlcnic: fix deadlock bug
      tcp: release sk_frag.page in tcp_disconnect
      ipv4: Get the address of interface correctly.
      net_sched: gen_estimator: fix lockdep splat
      net: macb: Handle HRESP error
      net/mlx5e: IPoIB, Fix copy-paste bug in flow steering refactoring
      ipv6: addrconf: break critical section in addrconf_verify_rtnl()
      ipv6: change route cache aging logic
      i40e/i40evf: Update DESC_NEEDED value to reflect larger value
      bnxt_en: cleanup DIM work on device shutdown
      ...

commit 168fe32a072a4b8dc81a3aebf0e5e588d38e2955
Merge: 13ddd1667e7f c71d227fc413
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 17:58:07 2018 -0800

    Merge branch 'misc.poll' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull poll annotations from Al Viro:
     "This introduces a __bitwise type for POLL### bitmap, and propagates
      the annotations through the tree. Most of that stuff is as simple as
      'make ->poll() instances return __poll_t and do the same to local
      variables used to hold the future return value'.
    
      Some of the obvious brainos found in process are fixed (e.g. POLLIN
      misspelled as POLL_IN). At that point the amount of sparse warnings is
      low and most of them are for genuine bugs - e.g. ->poll() instance
      deciding to return -EINVAL instead of a bitmap. I hadn't touched those
      in this series - it's large enough as it is.
    
      Another problem it has caught was eventpoll() ABI mess; select.c and
      eventpoll.c assumed that corresponding POLL### and EPOLL### were
      equal. That's true for some, but not all of them - EPOLL### are
      arch-independent, but POLL### are not.
    
      The last commit in this series separates userland POLL### values from
      the (now arch-independent) kernel-side ones, converting between them
      in the few places where they are copied to/from userland. AFAICS, this
      is the least disruptive fix preserving poll(2) ABI and making epoll()
      work on all architectures.
    
      As it is, it's simply broken on sparc - try to give it EPOLLWRNORM and
      it will trigger only on what would've triggered EPOLLWRBAND on other
      architectures. EPOLLWRBAND and EPOLLRDHUP, OTOH, are never triggered
      at all on sparc. With this patch they should work consistently on all
      architectures"
    
    * 'misc.poll' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (37 commits)
      make kernel-side POLL... arch-independent
      eventpoll: no need to mask the result of epi_item_poll() again
      eventpoll: constify struct epoll_event pointers
      debugging printk in sg_poll() uses %x to print POLL... bitmap
      annotate poll(2) guts
      9p: untangle ->poll() mess
      ->si_band gets POLL... bitmap stored into a user-visible long field
      ring_buffer_poll_wait() return value used as return value of ->poll()
      the rest of drivers/*: annotate ->poll() instances
      media: annotate ->poll() instances
      fs: annotate ->poll() instances
      ipc, kernel, mm: annotate ->poll() instances
      net: annotate ->poll() instances
      apparmor: annotate ->poll() instances
      tomoyo: annotate ->poll() instances
      sound: annotate ->poll() instances
      acpi: annotate ->poll() instances
      crypto: annotate ->poll() instances
      block: annotate ->poll() instances
      x86: annotate ->poll() instances
      ...

commit 05b93801a23c21a6f355f4c492c51715d6ccc96d
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Wed Jan 17 07:14:14 2018 -0800

    lockdep: Convert some users to const
    
    These users of lockdep_is_held() either wanted lockdep_is_held to
    take a const pointer, or would benefit from providing a const pointer.
    
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Link: https://lkml.kernel.org/r/20180117151414.23686-4-willy@infradead.org

diff --git a/include/net/sock.h b/include/net/sock.h
index 7a7b14e9628a..c4a424fe6fdd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1445,10 +1445,8 @@ do {									\
 } while (0)
 
 #ifdef CONFIG_LOCKDEP
-static inline bool lockdep_sock_is_held(const struct sock *csk)
+static inline bool lockdep_sock_is_held(const struct sock *sk)
 {
-	struct sock *sk = (struct sock *)csk;
-
 	return lockdep_is_held(&sk->sk_lock) ||
 	       lockdep_is_held(&sk->sk_lock.slock);
 }

commit 30c2c9f158f6c9cef41e916d1c7c11097df4befb
Author: David Windsor <dave@nullcore.net>
Date:   Sat Jun 10 22:50:42 2017 -0400

    net: Define usercopy region in struct proto slab cache
    
    In support of usercopy hardening, this patch defines a region in the
    struct proto slab cache in which userspace copy operations are allowed.
    Some protocols need to copy objects to/from userspace, and they can
    declare the region via their proto structure with the new usersize and
    useroffset fields. Initially, if no region is specified (usersize ==
    0), the entire field is marked as whitelisted. This allows protocols
    to be whitelisted in subsequent patches. Once all protocols have been
    annotated, the full-whitelist default can be removed.
    
    This region is known as the slab cache's usercopy region. Slab caches
    can now check that each dynamically sized copy operation involving
    cache-managed memory falls entirely within the slab's usercopy region.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on my
    understanding of the code. Changes or omissions from the original code are
    mine and don't reflect the original grsecurity/PaX code.
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split off per-proto patches]
    [kees: add logic for by-default full-whitelist]
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Paolo Abeni <pabeni@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: netdev@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 79e1a2c7912c..b77a710ee831 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1112,6 +1112,8 @@ struct proto {
 	struct kmem_cache	*slab;
 	unsigned int		obj_size;
 	slab_flags_t		slab_flags;
+	size_t			useroffset;	/* Usercopy region offset */
+	size_t			usersize;	/* Usercopy region size */
 
 	struct percpu_counter	*orphan_count;
 

commit 54dc3e3324829d346c959ff774626d9c6c9a65b5
Author: David Ahern <dsahern@gmail.com>
Date:   Thu Jan 4 14:03:54 2018 -0800

    net: ipv6: Allow connect to linklocal address from socket bound to vrf
    
    Allow a process bound to a VRF to connect to a linklocal address.
    Currently, this fails because of a mismatch between the scope of the
    linklocal address and the sk_bound_dev_if inherited by the VRF binding:
        $ ssh -6 fe80::70b8:cff:fedd:ead8%eth1
        ssh: connect to host fe80::70b8:cff:fedd:ead8%eth1 port 22: Invalid argument
    
    Relax the scope check to allow the socket to be bound to the same L3
    device as the scope id.
    
    This makes ipv6 linklocal consistent with other relaxed checks enabled
    by commits 1ff23beebdd3 ("net: l3mdev: Allow send on enslaved interface")
    and 7bb387c5ab12a ("net: Allow IP_MULTICAST_IF to set index to L3 slave").
    
    Signed-off-by: David Ahern <dsahern@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 66fd3951e6f3..73b7830b0bb8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -72,6 +72,7 @@
 #include <net/tcp_states.h>
 #include <linux/net_tstamp.h>
 #include <net/smc.h>
+#include <net/l3mdev.h>
 
 /*
  * This structure really needs to be cleaned up.
@@ -2399,4 +2400,23 @@ static inline void sk_pacing_shift_update(struct sock *sk, int val)
 	sk->sk_pacing_shift = val;
 }
 
+/* if a socket is bound to a device, check that the given device
+ * index is either the same or that the socket is bound to an L3
+ * master device and the given device index is also enslaved to
+ * that L3 master
+ */
+static inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)
+{
+	int mdif;
+
+	if (!sk->sk_bound_dev_if || sk->sk_bound_dev_if == dif)
+		return true;
+
+	mdif = l3mdev_master_ifindex_by_index(sock_net(sk), dif);
+	if (mdif && mdif == sk->sk_bound_dev_if)
+		return true;
+
+	return false;
+}
+
 #endif	/* _SOCK_H */

commit 6bb8824732f69de0f233ae6b1a8158e149627b38
Merge: d367341b25bd 2758b3e3e630
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 29 15:14:27 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    net/ipv6/ip6_gre.c is a case of parallel adds.
    
    include/trace/events/tcp.h is a little bit more tricky.  The removal
    of in-trace-macro ifdefs in 'net' paralleled with moving
    show_tcp_state_name and friends over to include/trace/events/sock.h
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 602f7a2714a3b3aa4bec82ab0a86a9f5a2c4aa61
Author: Tom Herbert <tom@quantonium.net>
Date:   Thu Dec 28 11:00:43 2017 -0800

    sock: Add sock_owned_by_user_nocheck
    
    This allows checking socket lock ownership with producing lockdep
    warnings.
    
    Signed-off-by: Tom Herbert <tom@quantonium.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9155da422692..7a7b14e9628a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1514,6 +1514,11 @@ static inline bool sock_owned_by_user(const struct sock *sk)
 	return sk->sk_lock.owned;
 }
 
+static inline bool sock_owned_by_user_nocheck(const struct sock *sk)
+{
+	return sk->sk_lock.owned;
+}
+
 /* no reclassification while locks are held */
 static inline bool sock_allow_reclassification(const struct sock *csk)
 {

commit 986ffdfd08dbaae721e82720e6bfc2c307e732dd
Author: Yafang Shao <laoar.shao@gmail.com>
Date:   Wed Dec 20 11:12:52 2017 +0800

    net: sock: replace sk_state_load with inet_sk_state_load and remove sk_state_store
    
    sk_state_load is only used by AF_INET/AF_INET6, so rename it to
    inet_sk_state_load and move it into inet_sock.h.
    
    sk_state_store is removed as it is not used any more.
    
    Signed-off-by: Yafang Shao <laoar.shao@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0a32f3ce381c..6c1db823f8b9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2333,31 +2333,6 @@ static inline bool sk_listener(const struct sock *sk)
 	return (1 << sk->sk_state) & (TCPF_LISTEN | TCPF_NEW_SYN_RECV);
 }
 
-/**
- * sk_state_load - read sk->sk_state for lockless contexts
- * @sk: socket pointer
- *
- * Paired with sk_state_store(). Used in places we do not hold socket lock :
- * tcp_diag_get_info(), tcp_get_info(), tcp_poll(), get_tcp4_sock() ...
- */
-static inline int sk_state_load(const struct sock *sk)
-{
-	return smp_load_acquire(&sk->sk_state);
-}
-
-/**
- * sk_state_store - update sk->sk_state
- * @sk: socket pointer
- * @newstate: new state
- *
- * Paired with sk_state_load(). Should be used in contexts where
- * state change might impact lockless readers.
- */
-static inline void sk_state_store(struct sock *sk, int newstate)
-{
-	smp_store_release(&sk->sk_state, newstate);
-}
-
 void sock_enable_timestamp(struct sock *sk, int flag);
 int sock_get_timestamp(struct sock *, struct timeval __user *);
 int sock_get_timestampns(struct sock *, struct timespec __user *);

commit 648845ab7e200993dccd3948c719c858368c91e7
Author: Tonghao Zhang <xiangxia.m.yue@gmail.com>
Date:   Thu Dec 14 05:51:58 2017 -0800

    sock: Move the socket inuse to namespace.
    
    In some case, we want to know how many sockets are in use in
    different _net_ namespaces. It's a key resource metric.
    
    This patch add a member in struct netns_core. This is a counter
    for socket-inuse in the _net_ namespace. The patch will add/sub
    counter in the sk_alloc, sk_clone_lock and __sk_free.
    
    This patch will not counter the socket created in kernel.
    It's not very useful for userspace to know how many kernel
    sockets we created.
    
    The main reasons for doing this are that:
    
    1. When linux calls the 'do_exit' for process to exit, the functions
    'exit_task_namespaces' and 'exit_task_work' will be called sequentially.
    'exit_task_namespaces' may have destroyed the _net_ namespace, but
    'sock_release' called in 'exit_task_work' may use the _net_ namespace
    if we counter the socket-inuse in sock_release.
    
    2. socket and sock are in pair. More important, sock holds the _net_
    namespace. We counter the socket-inuse in sock, for avoiding holding
    _net_ namespace again in socket. It's a easy way to maintain the code.
    
    Signed-off-by: Martin Zhang <zhangjunweimartin@didichuxing.com>
    Signed-off-by: Tonghao Zhang <zhangtonghao@didichuxing.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9a9047268d37..0a32f3ce381c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1262,6 +1262,7 @@ proto_memory_pressure(struct proto *prot)
 /* Called with local bh disabled */
 void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
 int sock_prot_inuse_get(struct net *net, struct proto *proto);
+int sock_inuse_get(struct net *net);
 #else
 static inline void sock_prot_inuse_add(struct net *net, struct proto *prot,
 		int inc)

commit c9f1f58dc2eba550f208809d272bf0b14f41edba
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 12 06:34:19 2017 -0800

    net: sk_pacing_shift_update() helper
    
    In commit 3a9b76fd0db9 ("tcp: allow drivers to tweak TSQ logic")
    I gave a code sample to set sk->sk_pacing_shift that was not complete.
    
    Better add a helper that can be used by drivers without worries,
    and maybe amended in the future.
    
    A wifi driver might use it from its ndo_start_xmit()
    
    Following call would setup TCP to allow up to ~8ms of queued data per
    flow.
    
    sk_pacing_shift_update(skb->sk, 7);
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9155da422692..9a9047268d37 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2407,4 +2407,15 @@ static inline int sk_get_rmem0(const struct sock *sk, const struct proto *proto)
 	return *proto->sysctl_rmem;
 }
 
+/* Default TCP Small queue budget is ~1 ms of data (1sec >> 10)
+ * Some wifi drivers need to tweak it to get more chunks.
+ * They can use this helper from their ndo_start_xmit()
+ */
+static inline void sk_pacing_shift_update(struct sock *sk, int val)
+{
+	if (!sk || !sk_fullsock(sk) || sk->sk_pacing_shift == val)
+		return;
+	sk->sk_pacing_shift = val;
+}
+
 #endif	/* _SOCK_H */

commit d7efc6c11b277d9d80b99b1334a78bfe7d7edf10
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 5 12:45:56 2017 -0800

    net: remove hlist_nulls_add_tail_rcu()
    
    Alexander Potapenko reported use of uninitialized memory [1]
    
    This happens when inserting a request socket into TCP ehash,
    in __sk_nulls_add_node_rcu(), since sk_reuseport is not initialized.
    
    Bug was added by commit d894ba18d4e4 ("soreuseport: fix ordering for
    mixed v4/v6 sockets")
    
    Note that d296ba60d8e2 ("soreuseport: Resolve merge conflict for v4/v6
    ordering fix") missed the opportunity to get rid of
    hlist_nulls_add_tail_rcu() :
    
    Both UDP sockets and TCP/DCCP listeners no longer use
    __sk_nulls_add_node_rcu() for their hash insertion.
    
    Since all other sockets have unique 4-tuple, the reuseport status
    has no special meaning, so we can always use hlist_nulls_add_head_rcu()
    for them and save few cycles/instructions.
    
    [1]
    
    ==================================================================
    BUG: KMSAN: use of uninitialized memory in inet_ehash_insert+0xd40/0x1050
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.13.0+ #3288
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:16
     dump_stack+0x185/0x1d0 lib/dump_stack.c:52
     kmsan_report+0x13f/0x1c0 mm/kmsan/kmsan.c:1016
     __msan_warning_32+0x69/0xb0 mm/kmsan/kmsan_instr.c:766
     __sk_nulls_add_node_rcu ./include/net/sock.h:684
     inet_ehash_insert+0xd40/0x1050 net/ipv4/inet_hashtables.c:413
     reqsk_queue_hash_req net/ipv4/inet_connection_sock.c:754
     inet_csk_reqsk_queue_hash_add+0x1cc/0x300 net/ipv4/inet_connection_sock.c:765
     tcp_conn_request+0x31e7/0x36f0 net/ipv4/tcp_input.c:6414
     tcp_v4_conn_request+0x16d/0x220 net/ipv4/tcp_ipv4.c:1314
     tcp_rcv_state_process+0x42a/0x7210 net/ipv4/tcp_input.c:5917
     tcp_v4_do_rcv+0xa6a/0xcd0 net/ipv4/tcp_ipv4.c:1483
     tcp_v4_rcv+0x3de0/0x4ab0 net/ipv4/tcp_ipv4.c:1763
     ip_local_deliver_finish+0x6bb/0xcb0 net/ipv4/ip_input.c:216
     NF_HOOK ./include/linux/netfilter.h:248
     ip_local_deliver+0x3fa/0x480 net/ipv4/ip_input.c:257
     dst_input ./include/net/dst.h:477
     ip_rcv_finish+0x6fb/0x1540 net/ipv4/ip_input.c:397
     NF_HOOK ./include/linux/netfilter.h:248
     ip_rcv+0x10f6/0x15c0 net/ipv4/ip_input.c:488
     __netif_receive_skb_core+0x36f6/0x3f60 net/core/dev.c:4298
     __netif_receive_skb net/core/dev.c:4336
     netif_receive_skb_internal+0x63c/0x19c0 net/core/dev.c:4497
     napi_skb_finish net/core/dev.c:4858
     napi_gro_receive+0x629/0xa50 net/core/dev.c:4889
     e1000_receive_skb drivers/net/ethernet/intel/e1000/e1000_main.c:4018
     e1000_clean_rx_irq+0x1492/0x1d30
    drivers/net/ethernet/intel/e1000/e1000_main.c:4474
     e1000_clean+0x43aa/0x5970 drivers/net/ethernet/intel/e1000/e1000_main.c:3819
     napi_poll net/core/dev.c:5500
     net_rx_action+0x73c/0x1820 net/core/dev.c:5566
     __do_softirq+0x4b4/0x8dd kernel/softirq.c:284
     invoke_softirq kernel/softirq.c:364
     irq_exit+0x203/0x240 kernel/softirq.c:405
     exiting_irq+0xe/0x10 ./arch/x86/include/asm/apic.h:638
     do_IRQ+0x15e/0x1a0 arch/x86/kernel/irq.c:263
     common_interrupt+0x86/0x86
    
    Fixes: d894ba18d4e4 ("soreuseport: fix ordering for mixed v4/v6 sockets")
    Fixes: d296ba60d8e2 ("soreuseport: Resolve merge conflict for v4/v6 ordering fix")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Alexander Potapenko <glider@google.com>
    Acked-by: Craig Gallek <kraig@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 79e1a2c7912c..9155da422692 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -685,11 +685,7 @@ static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
 
 static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
-	if (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&
-	    sk->sk_family == AF_INET6)
-		hlist_nulls_add_tail_rcu(&sk->sk_nulls_node, list);
-	else
-		hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
+	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
 }
 
 static inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)

commit ade994f4f6c8c3ef4c3bfc2d02166262fb9d089c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Jul 3 00:01:49 2017 -0400

    net: annotate ->poll() instances
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index 79e1a2c7912c..b33078333518 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1582,7 +1582,7 @@ int sock_no_connect(struct socket *, struct sockaddr *, int, int);
 int sock_no_socketpair(struct socket *, struct socket *);
 int sock_no_accept(struct socket *, struct socket *, int, bool);
 int sock_no_getname(struct socket *, struct sockaddr *, int *, int);
-unsigned int sock_no_poll(struct file *, struct socket *,
+__poll_t sock_no_poll(struct file *, struct socket *,
 			  struct poll_table_struct *);
 int sock_no_ioctl(struct socket *, unsigned int, unsigned long);
 int sock_no_listen(struct socket *, int);

commit 7c225c69f86c934e3be9be63ecde754e286838d7
Merge: 6363b3f3ac5b 1b7176aea0a9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 19:42:40 2017 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - a few misc bits
    
     - ocfs2 updates
    
     - almost all of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (131 commits)
      memory hotplug: fix comments when adding section
      mm: make alloc_node_mem_map a void call if we don't have CONFIG_FLAT_NODE_MEM_MAP
      mm: simplify nodemask printing
      mm,oom_reaper: remove pointless kthread_run() error check
      mm/page_ext.c: check if page_ext is not prepared
      writeback: remove unused function parameter
      mm: do not rely on preempt_count in print_vma_addr
      mm, sparse: do not swamp log with huge vmemmap allocation failures
      mm/hmm: remove redundant variable align_end
      mm/list_lru.c: mark expected switch fall-through
      mm/shmem.c: mark expected switch fall-through
      mm/page_alloc.c: broken deferred calculation
      mm: don't warn about allocations which stall for too long
      fs: fuse: account fuse_inode slab memory as reclaimable
      mm, page_alloc: fix potential false positive in __zone_watermark_ok
      mm: mlock: remove lru_add_drain_all()
      mm, sysctl: make NUMA stats configurable
      shmem: convert shmem_init_inodecache() to void
      Unify migrate_pages and move_pages access checks
      mm, pagevec: rename pagevec drained field
      ...

commit 4950276672fce5c241857540f8561c440663673d
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:51 2017 -0800

    kmemcheck: remove annotations
    
    Patch series "kmemcheck: kill kmemcheck", v2.
    
    As discussed at LSF/MM, kill kmemcheck.
    
    KASan is a replacement that is able to work without the limitation of
    kmemcheck (single CPU, slow).  KASan is already upstream.
    
    We are also not aware of any users of kmemcheck (or users who don't
    consider KASan as a suitable replacement).
    
    The only objection was that since KASAN wasn't supported by all GCC
    versions provided by distros at that time we should hold off for 2
    years, and try again.
    
    Now that 2 years have passed, and all distros provide gcc that supports
    KASAN, kill kmemcheck again for the very same reasons.
    
    This patch (of 4):
    
    Remove kmemcheck annotations, and calls to kmemcheck from the kernel.
    
    [alexander.levin@verizon.com: correctly remove kmemcheck call from dma_map_sg_attrs]
      Link: http://lkml.kernel.org/r/20171012192151.26531-1-alexander.levin@verizon.com
    Link: http://lkml.kernel.org/r/20171007030159.22241-2-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index c577286dbffb..a63e6a8bb7e0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -436,7 +436,6 @@ struct sock {
 #define SK_FL_TYPE_MASK    0xffff0000
 #endif
 
-	kmemcheck_bitfield_begin(flags);
 	unsigned int		sk_padding : 1,
 				sk_kern_sock : 1,
 				sk_no_check_tx : 1,
@@ -445,8 +444,6 @@ struct sock {
 				sk_protocol  : 8,
 				sk_type      : 16;
 #define SK_PROTOCOL_MAX U8_MAX
-	kmemcheck_bitfield_end(flags);
-
 	u16			sk_gso_max_segs;
 	unsigned long	        sk_lingertime;
 	struct proto		*sk_prot_creator;

commit d50112edde1d0c621520e53747044009f11c656b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:18 2017 -0800

    slab, slub, slob: add slab_flags_t
    
    Add sparse-checked slab_flags_t for struct kmem_cache::flags (SLAB_POISON,
    etc).
    
    SLAB is bloated temporarily by switching to "unsigned long", but only
    temporarily.
    
    Link: http://lkml.kernel.org/r/20171021100225.GA22428@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6b9a8d1a6df..c577286dbffb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1105,7 +1105,7 @@ struct proto {
 
 	struct kmem_cache	*slab;
 	unsigned int		obj_size;
-	int			slab_flags;
+	slab_flags_t		slab_flags;
 
 	struct percpu_counter	*orphan_count;
 

commit 3a9b76fd0db9f0d426533f96a68a62a58753a51e
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Nov 11 15:54:12 2017 -0800

    tcp: allow drivers to tweak TSQ logic
    
    I had many reports that TSQ logic breaks wifi aggregation.
    
    Current logic is to allow up to 1 ms of bytes to be queued into qdisc
    and drivers queues.
    
    But Wifi aggregation needs a bigger budget to allow bigger rates to
    be discovered by various TCP Congestion Controls algorithms.
    
    This patch adds an extra socket field, allowing wifi drivers to select
    another log scale to derive TCP Small Queue credit from current pacing
    rate.
    
    Initial value is 10, meaning that this patch does not change current
    behavior.
    
    We expect wifi drivers to set this field to smaller values (tests have
    been done with values from 6 to 9)
    
    They would have to use following template :
    
    if (skb->sk && skb->sk->sk_pacing_shift != MY_PACING_SHIFT)
         skb->sk->sk_pacing_shift = MY_PACING_SHIFT;
    
    Ref: https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1670041
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Johannes Berg <johannes.berg@intel.com>
    Cc: Toke Høiland-Jørgensen <toke@toke.dk>
    Cc: Kir Kolyshkin <kir@openvz.org>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 688a823dccc3..f8715c5af37d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -267,6 +267,7 @@ struct sock_common {
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
   *	@sk_gso_max_size: Maximum GSO segment size to build
   *	@sk_gso_max_segs: Maximum number of GSO segments
+  *	@sk_pacing_shift: scaling factor for TCP Small Queues
   *	@sk_lingertime: %SO_LINGER l_linger setting
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
@@ -451,6 +452,7 @@ struct sock {
 	kmemcheck_bitfield_end(flags);
 
 	u16			sk_gso_max_segs;
+	u8			sk_pacing_shift;
 	unsigned long	        sk_lingertime;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;

commit a3dcaf17ee54f1d01d22cc2b22cab0b4f60d78cf
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 7 00:29:27 2017 -0800

    net: allow per netns sysctl_rmem and sysctl_wmem for protos
    
    As we want to gradually implement per netns sysctl_rmem and sysctl_wmem
    on per protocol basis, add two new fields in struct proto,
    and two new helpers : sk_get_wmem0() and sk_get_rmem0()
    
    First user will be TCP. Then UDP and SCTP can be easily converted,
    while DECNET probably wont get this support.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6f1be9726e02..688a823dccc3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1101,8 +1101,12 @@ struct proto {
 	 */
 	unsigned long		*memory_pressure;
 	long			*sysctl_mem;
+
 	int			*sysctl_wmem;
 	int			*sysctl_rmem;
+	u32			sysctl_wmem_offset;
+	u32			sysctl_rmem_offset;
+
 	int			max_header;
 	bool			no_autobind;
 
@@ -2390,4 +2394,22 @@ extern int sysctl_optmem_max;
 extern __u32 sysctl_wmem_default;
 extern __u32 sysctl_rmem_default;
 
+static inline int sk_get_wmem0(const struct sock *sk, const struct proto *proto)
+{
+	/* Does this proto have per netns sysctl_wmem ? */
+	if (proto->sysctl_wmem_offset)
+		return *(int *)((void *)sock_net(sk) + proto->sysctl_wmem_offset);
+
+	return *proto->sysctl_wmem;
+}
+
+static inline int sk_get_rmem0(const struct sock *sk, const struct proto *proto)
+{
+	/* Does this proto have per netns sysctl_rmem ? */
+	if (proto->sysctl_rmem_offset)
+		return *(int *)((void *)sock_net(sk) + proto->sysctl_rmem_offset);
+
+	return *proto->sysctl_rmem;
+}
+
 #endif	/* _SOCK_H */

commit b6f4f8484d88b69f700907200a9a9ec73806355f
Author: Tim Hansen <devtimhansen@gmail.com>
Date:   Mon Oct 23 15:35:58 2017 -0400

    net/sock: Update sk rcu iterator macro.
    
    Mark hlist node in sk rcu iterator as protected by the rcu.
    hlist_next_rcu accomplishes this and silences the warnings
    sparse throws.
    
    Found with make C=1 net/ipv4/udp.o on linux-next tag
    next-20171009.
    
    Signed-off-by: Tim Hansen <devtimhansen@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4827094f1db4..6f1be9726e02 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -737,10 +737,10 @@ static inline void sk_add_bind_node(struct sock *sk,
  *
  */
 #define sk_for_each_entry_offset_rcu(tpos, pos, head, offset)		       \
-	for (pos = rcu_dereference((head)->first);			       \
+	for (pos = rcu_dereference(hlist_first_rcu(head));		       \
 	     pos != NULL &&						       \
 		({ tpos = (typeof(*tpos) *)((void *)pos - offset); 1;});       \
-	     pos = rcu_dereference(pos->next))
+	     pos = rcu_dereference(hlist_next_rcu(pos)))
 
 static inline struct user_namespace *sk_user_ns(struct sock *sk)
 {

commit 75c119afe14f74b4dd967d75ed9f57ab6c0ef045
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 5 22:21:27 2017 -0700

    tcp: implement rb-tree based retransmit queue
    
    Using a linear list to store all skbs in write queue has been okay
    for quite a while : O(N) is not too bad when N < 500.
    
    Things get messy when N is the order of 100,000 : Modern TCP stacks
    want 10Gbit+ of throughput even with 200 ms RTT flows.
    
    40 ns per cache line miss means a full scan can use 4 ms,
    blowing away CPU caches.
    
    SACK processing often can use various hints to avoid parsing
    whole retransmit queue. But with high packet losses and/or high
    reordering, hints no longer work.
    
    Sender has to process thousands of unfriendly SACK, accumulating
    a huge socket backlog, burning a cpu and massively dropping packets.
    
    Using an rb-tree for retransmit queue has been avoided for years
    because it added complexity and overhead, but now is the time
    to be more resistant and say no to quadratic behavior.
    
    1) RTX queue is no longer part of the write queue : already sent skbs
    are stored in one rb-tree.
    
    2) Since reaching the head of write queue no longer needs
    sk->sk_send_head, we added an union of sk_send_head and tcp_rtx_queue
    
    Tested:
    
     On receiver :
     netem on ingress : delay 150ms 200us loss 1
     GRO disabled to force stress and SACK storms.
    
    for f in `seq 1 10`
    do
     ./netperf -H lpaa6 -l30 -- -K bbr -o THROUGHPUT|tail -1
    done | awk '{print $0} {sum += $0} END {printf "%7u\n",sum}'
    
    Before patch :
    
    323.87
    351.48
    339.59
    338.62
    306.72
    204.07
    304.93
    291.88
    202.47
    176.88
       2840
    
    After patch:
    
    1700.83
    2207.98
    2070.17
    1544.26
    2114.76
    2124.89
    1693.14
    1080.91
    2216.82
    1299.94
      18053
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6b9a8d1a6df..4827094f1db4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -60,7 +60,7 @@
 #include <linux/sched.h>
 #include <linux/wait.h>
 #include <linux/cgroup-defs.h>
-
+#include <linux/rbtree.h>
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
 #include <linux/poll.h>
@@ -397,7 +397,10 @@ struct sock {
 	int			sk_wmem_queued;
 	refcount_t		sk_wmem_alloc;
 	unsigned long		sk_tsq_flags;
-	struct sk_buff		*sk_send_head;
+	union {
+		struct sk_buff	*sk_send_head;
+		struct rb_root	tcp_rtx_queue;
+	};
 	struct sk_buff_head	sk_write_queue;
 	__s32			sk_peek_off;
 	int			sk_write_pending;

commit 222d7dbd258dad4cd5241c43ef818141fad5a87a
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 21 09:15:46 2017 -0700

    net: prevent dst uses after free
    
    In linux-4.13, Wei worked hard to convert dst to a traditional
    refcounted model, removing GC.
    
    We now want to make sure a dst refcount can not transition from 0 back
    to 1.
    
    The problem here is that input path attached a not refcounted dst to an
    skb. Then later, because packet is forwarded and hits skb_dst_force()
    before exiting RCU section, we might try to take a refcount on one dst
    that is about to be freed, if another cpu saw 1 -> 0 transition in
    dst_release() and queued the dst for freeing after one RCU grace period.
    
    Lets unify skb_dst_force() and skb_dst_force_safe(), since we should
    always perform the complete check against dst refcount, and not assume
    it is not zero.
    
    Bugzilla : https://bugzilla.kernel.org/show_bug.cgi?id=197005
    
    [  989.919496]  skb_dst_force+0x32/0x34
    [  989.919498]  __dev_queue_xmit+0x1ad/0x482
    [  989.919501]  ? eth_header+0x28/0xc6
    [  989.919502]  dev_queue_xmit+0xb/0xd
    [  989.919504]  neigh_connected_output+0x9b/0xb4
    [  989.919507]  ip_finish_output2+0x234/0x294
    [  989.919509]  ? ipt_do_table+0x369/0x388
    [  989.919510]  ip_finish_output+0x12c/0x13f
    [  989.919512]  ip_output+0x53/0x87
    [  989.919513]  ip_forward_finish+0x53/0x5a
    [  989.919515]  ip_forward+0x2cb/0x3e6
    [  989.919516]  ? pskb_trim_rcsum.part.9+0x4b/0x4b
    [  989.919518]  ip_rcv_finish+0x2e2/0x321
    [  989.919519]  ip_rcv+0x26f/0x2eb
    [  989.919522]  ? vlan_do_receive+0x4f/0x289
    [  989.919523]  __netif_receive_skb_core+0x467/0x50b
    [  989.919526]  ? tcp_gro_receive+0x239/0x239
    [  989.919529]  ? inet_gro_receive+0x226/0x238
    [  989.919530]  __netif_receive_skb+0x4d/0x5f
    [  989.919532]  netif_receive_skb_internal+0x5c/0xaf
    [  989.919533]  napi_gro_receive+0x45/0x81
    [  989.919536]  ixgbe_poll+0xc8a/0xf09
    [  989.919539]  ? kmem_cache_free_bulk+0x1b6/0x1f7
    [  989.919540]  net_rx_action+0xf4/0x266
    [  989.919543]  __do_softirq+0xa8/0x19d
    [  989.919545]  irq_exit+0x5d/0x6b
    [  989.919546]  do_IRQ+0x9c/0xb5
    [  989.919548]  common_interrupt+0x93/0x93
    [  989.919548]  </IRQ>
    
    Similarly dst_clone() can use dst_hold() helper to have additional
    debugging, as a follow up to commit 44ebe79149ff ("net: add debug
    atomic_inc_not_zero() in dst_hold()")
    
    In net-next we will convert dst atomic_t to refcount_t for peace of
    mind.
    
    Fixes: a4c2fd7f7891 ("net: remove DST_NOCACHE flag")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Wei Wang <weiwan@google.com>
    Reported-by: Paweł Staszewski <pstaszewski@itcare.pl>
    Bisected-by: Paweł Staszewski <pstaszewski@itcare.pl>
    Acked-by: Wei Wang <weiwan@google.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 03a362568357..a6b9a8d1a6df 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -856,7 +856,7 @@ void sk_stream_write_space(struct sock *sk);
 static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	/* dont let skb dst not refcounted, we are going to leave rcu lock */
-	skb_dst_force_safe(skb);
+	skb_dst_force(skb);
 
 	if (!sk->sk_backlog.tail)
 		sk->sk_backlog.head = skb;

commit eaa72dc47488d599439cd0fd0f8c4f1bcb3906bb
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Aug 29 15:16:01 2017 -0700

    neigh: increase queue_len_bytes to match wmem_default
    
    Florian reported UDP xmit drops that could be root caused to the
    too small neigh limit.
    
    Current limit is 64 KB, meaning that even a single UDP socket would hit
    it, since its default sk_sndbuf comes from net.core.wmem_default
    (~212992 bytes on 64bit arches).
    
    Once ARP/ND resolution is in progress, we should allow a little more
    packets to be queued, at least for one producer.
    
    Once neigh arp_queue is filled, a rogue socket should hit its sk_sndbuf
    limit and either block in sendmsg() or return -EAGAIN.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1c2912d433e8..03a362568357 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2368,6 +2368,16 @@ bool sk_net_capable(const struct sock *sk, int cap);
 
 void sk_get_meminfo(const struct sock *sk, u32 *meminfo);
 
+/* Take into consideration the size of the struct sk_buff overhead in the
+ * determination of these values, since that is non-constant across
+ * platforms.  This makes socket queueing behavior and performance
+ * not depend upon such differences.
+ */
+#define _SK_MEM_PACKETS		256
+#define _SK_MEM_OVERHEAD	SKB_TRUESIZE(256)
+#define SK_WMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
+#define SK_RMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
+
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 

commit e2a7c34fb2856fd5306e307e170e3dde358d0dce
Merge: 7d3f0cd43fee 6470812e2226
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Aug 21 17:06:42 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit a0917e0bc6efc05834c0c1eafebd579a9c75e6e9
Author: Matthew Dawson <matthew@mjdsystems.ca>
Date:   Fri Aug 18 15:04:54 2017 -0400

    datagram: When peeking datagrams with offset < 0 don't skip empty skbs
    
    Due to commit e6afc8ace6dd5cef5e812f26c72579da8806f5ac ("udp: remove
    headers from UDP packets before queueing"), when udp packets are being
    peeked the requested extra offset is always 0 as there is no need to skip
    the udp header.  However, when the offset is 0 and the next skb is
    of length 0, it is only returned once.  The behaviour can be seen with
    the following python script:
    
    from socket import *;
    f=socket(AF_INET6, SOCK_DGRAM | SOCK_NONBLOCK, 0);
    g=socket(AF_INET6, SOCK_DGRAM | SOCK_NONBLOCK, 0);
    f.bind(('::', 0));
    addr=('::1', f.getsockname()[1]);
    g.sendto(b'', addr)
    g.sendto(b'b', addr)
    print(f.recvfrom(10, MSG_PEEK));
    print(f.recvfrom(10, MSG_PEEK));
    
    Where the expected output should be the empty string twice.
    
    Instead, make sk_peek_offset return negative values, and pass those values
    to __skb_try_recv_datagram/__skb_try_recv_from_queue.  If the passed offset
    to __skb_try_recv_from_queue is negative, the checked skb is never skipped.
    __skb_try_recv_from_queue will then ensure the offset is reset back to 0
    if a peek is requested without an offset, unless no packets are found.
    
    Also simplify the if condition in __skb_try_recv_from_queue.  If _off is
    greater then 0, and off is greater then or equal to skb->len, then
    (_off || skb->len) must always be true assuming skb->len >= 0 is always
    true.
    
    Also remove a redundant check around a call to sk_peek_offset in af_unix.c,
    as it double checked if MSG_PEEK was set in the flags.
    
    V2:
     - Moved the negative fixup into __skb_try_recv_from_queue, and remove now
    redundant checks
     - Fix peeking in udp{,v6}_recvmsg to report the right value when the
    offset is 0
    
    V3:
     - Marked new branch in __skb_try_recv_from_queue as unlikely.
    
    Signed-off-by: Matthew Dawson <matthew@mjdsystems.ca>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7c0632c7e870..aeeec62992ca 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -507,9 +507,7 @@ int sk_set_peek_off(struct sock *sk, int val);
 static inline int sk_peek_offset(struct sock *sk, int flags)
 {
 	if (unlikely(flags & MSG_PEEK)) {
-		s32 off = READ_ONCE(sk->sk_peek_off);
-		if (off >= 0)
-			return off;
+		return READ_ONCE(sk->sk_peek_off);
 	}
 
 	return 0;

commit 52267790ef52d7513879238ca9fac22c1733e0e3
Author: Willem de Bruijn <willemb@google.com>
Date:   Thu Aug 3 16:29:39 2017 -0400

    sock: add MSG_ZEROCOPY
    
    The kernel supports zerocopy sendmsg in virtio and tap. Expand the
    infrastructure to support other socket types. Introduce a completion
    notification channel over the socket error queue. Notifications are
    returned with ee_origin SO_EE_ORIGIN_ZEROCOPY. ee_errno is 0 to avoid
    blocking the send/recv path on receiving notifications.
    
    Add reference counting, to support the skb split, merge, resize and
    clone operations possible with SOCK_STREAM and other socket types.
    
    The patch does not yet modify any datapaths.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0f778d3c4300..fe1a0bc25cd3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -294,6 +294,7 @@ struct sock_common {
   *	@sk_stamp: time stamp of last packet received
   *	@sk_tsflags: SO_TIMESTAMPING socket options
   *	@sk_tskey: counter to disambiguate concurrent tstamp requests
+  *	@sk_zckey: counter to order MSG_ZEROCOPY notifications
   *	@sk_socket: Identd and reporting IO signals
   *	@sk_user_data: RPC layer private data
   *	@sk_frag: cached page frag
@@ -462,6 +463,7 @@ struct sock {
 	u16			sk_tsflags;
 	u8			sk_shutdown;
 	u32			sk_tskey;
+	atomic_t		sk_zckey;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
 #ifdef CONFIG_SECURITY

commit 98ba0bd5505dcbb90322a4be07bcfe6b8a18c73f
Author: Willem de Bruijn <willemb@google.com>
Date:   Thu Aug 3 16:29:37 2017 -0400

    sock: allocate skbs from optmem
    
    Add sock_omalloc and sock_ofree to be able to allocate control skbs,
    for instance for looping errors onto sk_error_queue.
    
    The transmit budget (sk_wmem_alloc) is involved in transmit skb
    shaping, most notably in TCP Small Queues. Using this budget for
    control packets would impact transmission.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 393c38e9f6aa..0f778d3c4300 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1531,6 +1531,8 @@ struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
 			     gfp_t priority);
 void __sock_wfree(struct sk_buff *skb);
 void sock_wfree(struct sk_buff *skb);
+struct sk_buff *sock_omalloc(struct sock *sk, unsigned long size,
+			     gfp_t priority);
 void skb_orphan_partial(struct sk_buff *skb);
 void sock_rfree(struct sk_buff *skb);
 void sock_efree(struct sk_buff *skb);

commit 306b13eb3cf9515a8214bbf5d69d811371d05792
Author: Tom Herbert <tom@quantonium.net>
Date:   Fri Jul 28 16:22:41 2017 -0700

    proto_ops: Add locked held versions of sendmsg and sendpage
    
    Add new proto_ops sendmsg_locked and sendpage_locked that can be
    called when the socket lock is already held. Correspondingly, add
    kernel_sendmsg_locked and kernel_sendpage_locked as front end
    functions.
    
    These functions will be used in zero proxy so that we can take
    the socket lock in a ULP sendmsg/sendpage and then directly call the
    backend transport proto_ops functions.
    
    Signed-off-by: Tom Herbert <tom@quantonium.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7c0632c7e870..393c38e9f6aa 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1582,11 +1582,14 @@ int sock_no_shutdown(struct socket *, int);
 int sock_no_getsockopt(struct socket *, int , int, char __user *, int __user *);
 int sock_no_setsockopt(struct socket *, int, int, char __user *, unsigned int);
 int sock_no_sendmsg(struct socket *, struct msghdr *, size_t);
+int sock_no_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t len);
 int sock_no_recvmsg(struct socket *, struct msghdr *, size_t, int);
 int sock_no_mmap(struct file *file, struct socket *sock,
 		 struct vm_area_struct *vma);
 ssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset,
 			 size_t size, int flags);
+ssize_t sock_no_sendpage_locked(struct sock *sk, struct page *page,
+				int offset, size_t size, int flags);
 
 /*
  * Functions to fill in entries in struct proto_ops when a protocol

commit e06fdaf40a5c021dd4a2ec797e8b724f07360070
Merge: a90c6ac2b565 8acdf5055974
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 19 08:55:18 2017 -0700

    Merge tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull structure randomization updates from Kees Cook:
     "Now that IPC and other changes have landed, enable manual markings for
      randstruct plugin, including the task_struct.
    
      This is the rest of what was staged in -next for the gcc-plugins, and
      comes in three patches, largest first:
    
       - mark "easy" structs with __randomize_layout
    
       - mark task_struct with an optional anonymous struct to isolate the
         __randomize_layout section
    
       - mark structs to opt _out_ of automated marking (which will come
         later)
    
      And, FWIW, this continues to pass allmodconfig (normal and patched to
      enable gcc-plugins) builds of x86_64, i386, arm64, arm, powerpc, and
      s390 for me"
    
    * tag 'gcc-plugins-v4.13-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      randstruct: opt-out externally exposed function pointer structs
      task_struct: Allow randomized layout
      randstruct: Mark various structs for randomization

commit 771edcafa753aad304babd3bab8e413574d5db3b
Author: stephen hemminger <stephen@networkplumber.org>
Date:   Wed Jul 12 09:29:06 2017 -0700

    socket: add documentation for missing elements
    
    Fill in missing kernel-doc for missing elements in struct sock.
    
    Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8c85791fc196..f69c8c2782df 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -246,6 +246,7 @@ struct sock_common {
   *	@sk_policy: flow policy
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
+  *	@sk_tsq_flags: TCP Small Queues flags
   *	@sk_write_queue: Packet sending queue
   *	@sk_omem_alloc: "o" is "option" or "other"
   *	@sk_wmem_queued: persistent queue size
@@ -257,6 +258,7 @@ struct sock_common {
   *	@sk_pacing_status: Pacing status (requested, handled by sch_fq)
   *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
   *	@sk_sndbuf: size of send buffer in bytes
+  *	@__sk_flags_offset: empty field used to determine location of bitfield
   *	@sk_padding: unused element for alignment
   *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
   *	@sk_no_check_rx: allow zero checksum in RX packets
@@ -277,6 +279,7 @@ struct sock_common {
   *	@sk_drops: raw/udp drops counter
   *	@sk_ack_backlog: current listen backlog
   *	@sk_max_ack_backlog: listen backlog set in listen()
+  *	@sk_uid: user id of owner
   *	@sk_priority: %SO_PRIORITY setting
   *	@sk_type: socket type (%SOCK_STREAM, etc)
   *	@sk_protocol: which protocol this socket belongs in this network family

commit 0ffdaf5b41cf4435ece14d1d3e977ce69012a20d
Author: Sowmini Varadhan <sowmini.varadhan@oracle.com>
Date:   Thu Jul 6 08:15:07 2017 -0700

    net/sock: add WARN_ON(parent->sk) in sock_graft()
    
    sock_graft() unilaterally sets up parent->sk based on the
    assumption that the existing parent->sk is null. If this
    condition is not true, then the existing parent->sk would
    be leaked, so add a WARN_ON() to alert callers who may fall
    in this category.
    
    Signed-off-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 48e4d5c38f85..8c85791fc196 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1697,6 +1697,7 @@ static inline void sock_orphan(struct sock *sk)
 
 static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
+	WARN_ON(parent->sk);
 	write_lock_bh(&sk->sk_callback_lock);
 	sk->sk_wq = parent->wq;
 	parent->sk = sk;

commit 5518b69b76680a4f2df96b1deca260059db0c2de
Merge: 8ad06e56dcbc 0e72582270c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 12:31:59 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Reasonably busy this cycle, but perhaps not as busy as in the 4.12
      merge window:
    
       1) Several optimizations for UDP processing under high load from
          Paolo Abeni.
    
       2) Support pacing internally in TCP when using the sch_fq packet
          scheduler for this is not practical. From Eric Dumazet.
    
       3) Support mutliple filter chains per qdisc, from Jiri Pirko.
    
       4) Move to 1ms TCP timestamp clock, from Eric Dumazet.
    
       5) Add batch dequeueing to vhost_net, from Jason Wang.
    
       6) Flesh out more completely SCTP checksum offload support, from
          Davide Caratti.
    
       7) More plumbing of extended netlink ACKs, from David Ahern, Pablo
          Neira Ayuso, and Matthias Schiffer.
    
       8) Add devlink support to nfp driver, from Simon Horman.
    
       9) Add RTM_F_FIB_MATCH flag to RTM_GETROUTE queries, from Roopa
          Prabhu.
    
      10) Add stack depth tracking to BPF verifier and use this information
          in the various eBPF JITs. From Alexei Starovoitov.
    
      11) Support XDP on qed device VFs, from Yuval Mintz.
    
      12) Introduce BPF PROG ID for better introspection of installed BPF
          programs. From Martin KaFai Lau.
    
      13) Add bpf_set_hash helper for TC bpf programs, from Daniel Borkmann.
    
      14) For loads, allow narrower accesses in bpf verifier checking, from
          Yonghong Song.
    
      15) Support MIPS in the BPF selftests and samples infrastructure, the
          MIPS eBPF JIT will be merged in via the MIPS GIT tree. From David
          Daney.
    
      16) Support kernel based TLS, from Dave Watson and others.
    
      17) Remove completely DST garbage collection, from Wei Wang.
    
      18) Allow installing TCP MD5 rules using prefixes, from Ivan
          Delalande.
    
      19) Add XDP support to Intel i40e driver, from Björn Töpel
    
      20) Add support for TC flower offload in nfp driver, from Simon
          Horman, Pieter Jansen van Vuuren, Benjamin LaHaise, Jakub
          Kicinski, and Bert van Leeuwen.
    
      21) IPSEC offloading support in mlx5, from Ilan Tayari.
    
      22) Add HW PTP support to macb driver, from Rafal Ozieblo.
    
      23) Networking refcount_t conversions, From Elena Reshetova.
    
      24) Add sock_ops support to BPF, from Lawrence Brako. This is useful
          for tuning the TCP sockopt settings of a group of applications,
          currently via CGROUPs"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1899 commits)
      net: phy: dp83867: add workaround for incorrect RX_CTRL pin strap
      dt-bindings: phy: dp83867: provide a workaround for incorrect RX_CTRL pin strap
      cxgb4: Support for get_ts_info ethtool method
      cxgb4: Add PTP Hardware Clock (PHC) support
      cxgb4: time stamping interface for PTP
      nfp: default to chained metadata prepend format
      nfp: remove legacy MAC address lookup
      nfp: improve order of interfaces in breakout mode
      net: macb: remove extraneous return when MACB_EXT_DESC is defined
      bpf: add missing break in for the TCP_BPF_SNDCWND_CLAMP case
      bpf: fix return in load_bpf_file
      mpls: fix rtm policy in mpls_getroute
      net, ax25: convert ax25_cb.refcount from atomic_t to refcount_t
      net, ax25: convert ax25_route.refcount from atomic_t to refcount_t
      net, ax25: convert ax25_uid_assoc.refcount from atomic_t to refcount_t
      net, sctp: convert sctp_ep_common.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_transport.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_chunk.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_datamsg.refcnt from atomic_t to refcount_t
      net, sctp: convert sctp_auth_bytes.refcnt from atomic_t to refcount_t
      ...

commit 41c6d650f6537e55a1b53438c646fbc3f49176bf
Author: Reshetova, Elena <elena.reshetova@intel.com>
Date:   Fri Jun 30 13:08:01 2017 +0300

    net: convert sock.sk_refcnt from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    This patch uses refcount_inc_not_zero() instead of
    atomic_inc_not_zero_hint() due to absense of a _hint()
    version of refcount API. If the hint() version must
    be used, we might need to revisit API.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5284e50fc81a..60200f4f4028 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -66,6 +66,7 @@
 #include <linux/poll.h>
 
 #include <linux/atomic.h>
+#include <linux/refcount.h>
 #include <net/dst.h>
 #include <net/checksum.h>
 #include <net/tcp_states.h>
@@ -219,7 +220,7 @@ struct sock_common {
 		u32		skc_tw_rcv_nxt; /* struct tcp_timewait_sock  */
 	};
 
-	atomic_t		skc_refcnt;
+	refcount_t		skc_refcnt;
 	/* private: */
 	int                     skc_dontcopy_end[0];
 	union {
@@ -611,7 +612,7 @@ static inline bool __sk_del_node_init(struct sock *sk)
 
 static __always_inline void sock_hold(struct sock *sk)
 {
-	atomic_inc(&sk->sk_refcnt);
+	refcount_inc(&sk->sk_refcnt);
 }
 
 /* Ungrab socket in the context, which assumes that socket refcnt
@@ -619,7 +620,7 @@ static __always_inline void sock_hold(struct sock *sk)
  */
 static __always_inline void __sock_put(struct sock *sk)
 {
-	atomic_dec(&sk->sk_refcnt);
+	refcount_dec(&sk->sk_refcnt);
 }
 
 static inline bool sk_del_node_init(struct sock *sk)
@@ -628,7 +629,7 @@ static inline bool sk_del_node_init(struct sock *sk)
 
 	if (rc) {
 		/* paranoid for a while -acme */
-		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		WARN_ON(refcount_read(&sk->sk_refcnt) == 1);
 		__sock_put(sk);
 	}
 	return rc;
@@ -650,7 +651,7 @@ static inline bool sk_nulls_del_node_init_rcu(struct sock *sk)
 
 	if (rc) {
 		/* paranoid for a while -acme */
-		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		WARN_ON(refcount_read(&sk->sk_refcnt) == 1);
 		__sock_put(sk);
 	}
 	return rc;
@@ -1144,9 +1145,9 @@ static inline void sk_refcnt_debug_dec(struct sock *sk)
 
 static inline void sk_refcnt_debug_release(const struct sock *sk)
 {
-	if (atomic_read(&sk->sk_refcnt) != 1)
+	if (refcount_read(&sk->sk_refcnt) != 1)
 		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",
-		       sk->sk_prot->name, sk, atomic_read(&sk->sk_refcnt));
+		       sk->sk_prot->name, sk, refcount_read(&sk->sk_refcnt));
 }
 #else /* SOCK_REFCNT_DEBUG */
 #define sk_refcnt_debug_inc(sk) do { } while (0)
@@ -1636,7 +1637,7 @@ void sock_init_data(struct socket *sock, struct sock *sk);
 /* Ungrab socket and destroy it, if it was the last reference. */
 static inline void sock_put(struct sock *sk)
 {
-	if (atomic_dec_and_test(&sk->sk_refcnt))
+	if (refcount_dec_and_test(&sk->sk_refcnt))
 		sk_free(sk);
 }
 /* Generic version of sock_put(), dealing with all sockets

commit 14afee4b6092fde451ee17604e5f5c89da33e71e
Author: Reshetova, Elena <elena.reshetova@intel.com>
Date:   Fri Jun 30 13:08:00 2017 +0300

    net: convert sock.sk_wmem_alloc from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 00d09140e354..5284e50fc81a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -390,7 +390,7 @@ struct sock {
 
 	/* ===== cache line for TX ===== */
 	int			sk_wmem_queued;
-	atomic_t		sk_wmem_alloc;
+	refcount_t		sk_wmem_alloc;
 	unsigned long		sk_tsq_flags;
 	struct sk_buff		*sk_send_head;
 	struct sk_buff_head	sk_write_queue;
@@ -1911,7 +1911,7 @@ static inline int skb_copy_to_page_nocache(struct sock *sk, struct iov_iter *fro
  */
 static inline int sk_wmem_alloc_get(const struct sock *sk)
 {
-	return atomic_read(&sk->sk_wmem_alloc) - 1;
+	return refcount_read(&sk->sk_wmem_alloc) - 1;
 }
 
 /**
@@ -2055,7 +2055,7 @@ static inline unsigned long sock_wspace(struct sock *sk)
 	int amt = 0;
 
 	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
-		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
+		amt = sk->sk_sndbuf - refcount_read(&sk->sk_wmem_alloc);
 		if (amt < 0)
 			amt = 0;
 	}
@@ -2136,7 +2136,7 @@ bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
  */
 static inline bool sock_writeable(const struct sock *sk)
 {
-	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
+	return refcount_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
 }
 
 static inline gfp_t gfp_any(void)

commit 3859a271a003aba01e45b85c9d8b355eb7bf25f9
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Oct 28 01:22:25 2016 -0700

    randstruct: Mark various structs for randomization
    
    This marks many critical kernel structures for randomization. These are
    structures that have been targeted in the past in security exploits, or
    contain functions pointers, pointers to function pointer tables, lists,
    workqueues, ref-counters, credentials, permissions, or are otherwise
    sensitive. This initial list was extracted from Brad Spengler/PaX Team's
    code in the last public patch of grsecurity/PaX based on my understanding
    of the code. Changes or omissions from the original code are mine and
    don't reflect the original grsecurity/PaX code.
    
    Left out of this list is task_struct, which requires special handling
    and will be covered in a subsequent patch.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index f33e3d134e0b..d349297db9e9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1113,7 +1113,7 @@ struct proto {
 	atomic_t		socks;
 #endif
 	int			(*diag_destroy)(struct sock *sk, int err);
-};
+} __randomize_layout;
 
 int proto_register(struct proto *prot, int alloc_slab);
 void proto_unregister(struct proto *prot);

commit 34cfb542b5b1762c73479a4a61e0a3b77253f876
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Wed Jun 21 11:45:31 2017 +0200

    sock: avoid dirtying incoming_cpu if not needed
    
    for connected socket, the incoming_cpu field in the sock struct
    is not going to change frequently, but we are setting it
    unconditionally for each packet.
    
    Since sk_incoming_cpu and sk_flags share the same cacheline,
    and the latter is access by udp_recvmsg(), this cause a cache
    miss for each packet for UDP connected socket.
    
    With this patch, we set the incoming cpu field only when the
    ingress cpu really changes.
    
    This gives a small but measurable performance improvement for
    connected UDP socket.
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 858891c36f94..00d09140e354 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -907,7 +907,10 @@ static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 
 static inline void sk_incoming_cpu_update(struct sock *sk)
 {
-	sk->sk_incoming_cpu = raw_smp_processor_id();
+	int cpu = raw_smp_processor_id();
+
+	if (unlikely(sk->sk_incoming_cpu != cpu))
+		sk->sk_incoming_cpu = cpu;
 }
 
 static inline void sock_rps_record_flow_hash(__u32 hash)

commit 0604475119de5f80dc051a5db055c6a2a75bd542
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 7 13:29:12 2017 -0700

    tcp: add TCPMemoryPressuresChrono counter
    
    DRAM supply shortage and poor memory pressure tracking in TCP
    stack makes any change in SO_SNDBUF/SO_RCVBUF (or equivalent autotuning
    limits) and tcp_mem[] quite hazardous.
    
    TCPMemoryPressures SNMP counter is an indication of tcp_mem sysctl
    limits being hit, but only tracking number of transitions.
    
    If TCP stack behavior under stress was perfect :
    1) It would maintain memory usage close to the limit.
    2) Memory pressure state would be entered for short times.
    
    We certainly prefer 100 events lasting 10ms compared to one event
    lasting 200 seconds.
    
    This patch adds a new SNMP counter tracking cumulative duration of
    memory pressure events, given in ms units.
    
    $ cat /proc/sys/net/ipv4/tcp_mem
    3088    4117    6176
    $ grep TCP /proc/net/sockstat
    TCP: inuse 180 orphan 0 tw 2 alloc 234 mem 4140
    $ nstat -n ; sleep 10 ; nstat |grep Pressure
    TcpExtTCPMemoryPressures        1700
    TcpExtTCPMemoryPressuresChrono  5209
    
    v2: Used EXPORT_SYMBOL_GPL() instead of EXPORT_SYMBOL() as David
    instructed.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3467d9e89e7d..858891c36f94 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1080,6 +1080,7 @@ struct proto {
 	bool			(*stream_memory_free)(const struct sock *sk);
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
+	void			(*leave_memory_pressure)(struct sock *sk);
 	atomic_long_t		*memory_allocated;	/* Current allocated memory. */
 	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
 	/*
@@ -1088,7 +1089,7 @@ struct proto {
 	 * All the __sk_mem_schedule() is of this nature: accounting
 	 * is strict, actions are advisory and have some latency.
 	 */
-	int			*memory_pressure;
+	unsigned long		*memory_pressure;
 	long			*sysctl_mem;
 	int			*sysctl_wmem;
 	int			*sysctl_rmem;
@@ -1193,25 +1194,6 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 	return !!*sk->sk_prot->memory_pressure;
 }
 
-static inline void sk_leave_memory_pressure(struct sock *sk)
-{
-	int *memory_pressure = sk->sk_prot->memory_pressure;
-
-	if (!memory_pressure)
-		return;
-
-	if (*memory_pressure)
-		*memory_pressure = 0;
-}
-
-static inline void sk_enter_memory_pressure(struct sock *sk)
-{
-	if (!sk->sk_prot->enter_memory_pressure)
-		return;
-
-	sk->sk_prot->enter_memory_pressure(sk);
-}
-
 static inline long
 sk_memory_allocated(const struct sock *sk)
 {

commit 6312811be26f4a97fb36f53ffffafa5086833a28
Merge: 468f8763fea1 5d47c31b59f6
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Thu May 18 11:03:08 2017 -0600

    Merge remote-tracking branch 'mauro-exp/docbook3' into death-to-docbook
    
    Mauro says:
    
    This patch series convert the remaining DocBooks to ReST.
    
    The first version was originally
    send as 3 patch series:
    
       [PATCH 00/36] Convert DocBook documents to ReST
       [PATCH 0/5] Convert more books to ReST
       [PATCH 00/13] Get rid of DocBook
    
    The lsm book was added as if it were a text file under
    Documentation. The plan is to merge it with another file
    under Documentation/security, after both this series and
    a security Documentation patch series gets merged.
    
    It also adjusts some Sphinx-pedantic errors/warnings on
    some kernel-doc markups.
    
    I also added some patches here to add PDF output for all
    existing ReST books.

commit 218af599fa635b107cfe10acf3249c4dfe5e4123
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 16 04:24:36 2017 -0700

    tcp: internal implementation for pacing
    
    BBR congestion control depends on pacing, and pacing is
    currently handled by sch_fq packet scheduler for performance reasons,
    and also because implemening pacing with FQ was convenient to truly
    avoid bursts.
    
    However there are many cases where this packet scheduler constraint
    is not practical.
    - Many linux hosts are not focusing on handling thousands of TCP
      flows in the most efficient way.
    - Some routers use fq_codel or other AQM, but still would like
      to use BBR for the few TCP flows they initiate/terminate.
    
    This patch implements an automatic fallback to internal pacing.
    
    Pacing is requested either by BBR or use of SO_MAX_PACING_RATE option.
    
    If sch_fq happens to be in the egress path, pacing is delegated to
    the qdisc, otherwise pacing is done by TCP itself.
    
    One advantage of pacing from TCP stack is to get more precise rtt
    estimations, and less work done from TX completion, since TCP Small
    queue limits are not generally hit. Setups with single TX queue but
    many cpus might even benefit from this.
    
    Note that unlike sch_fq, we do not take into account header sizes.
    Taking care of these headers would add additional complexity for
    no practical differences in behavior.
    
    Some performance numbers using 800 TCP_STREAM flows rate limited to
    ~48 Mbit per second on 40Gbit NIC.
    
    If MQ+pfifo_fast is used on the NIC :
    
    $ sar -n DEV 1 5 | grep eth
    14:48:44         eth0 725743.00 2932134.00  46776.76 4335184.68      0.00      0.00      1.00
    14:48:45         eth0 725349.00 2932112.00  46751.86 4335158.90      0.00      0.00      0.00
    14:48:46         eth0 725101.00 2931153.00  46735.07 4333748.63      0.00      0.00      0.00
    14:48:47         eth0 725099.00 2931161.00  46735.11 4333760.44      0.00      0.00      1.00
    14:48:48         eth0 725160.00 2931731.00  46738.88 4334606.07      0.00      0.00      0.00
    Average:         eth0 725290.40 2931658.20  46747.54 4334491.74      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     4  0      0 259825920  45644 2708324    0    0    21     2  247   98  0  0 100  0  0
     4  0      0 259823744  45644 2708356    0    0     0     0 2400825 159843  0 19 81  0  0
     0  0      0 259824208  45644 2708072    0    0     0     0 2407351 159929  0 19 81  0  0
     1  0      0 259824592  45644 2708128    0    0     0     0 2405183 160386  0 19 80  0  0
     1  0      0 259824272  45644 2707868    0    0     0    32 2396361 158037  0 19 81  0  0
    
    Now use MQ+FQ :
    
    lpaa23:~# echo fq >/proc/sys/net/core/default_qdisc
    lpaa23:~# tc qdisc replace dev eth0 root mq
    
    $ sar -n DEV 1 5 | grep eth
    14:49:57         eth0 678614.00 2727930.00  43739.13 4033279.14      0.00      0.00      0.00
    14:49:58         eth0 677620.00 2723971.00  43674.69 4027429.62      0.00      0.00      1.00
    14:49:59         eth0 676396.00 2719050.00  43596.83 4020125.02      0.00      0.00      0.00
    14:50:00         eth0 675197.00 2714173.00  43518.62 4012938.90      0.00      0.00      1.00
    14:50:01         eth0 676388.00 2719063.00  43595.47 4020171.64      0.00      0.00      0.00
    Average:         eth0 676843.00 2720837.40  43624.95 4022788.86      0.00      0.00      0.40
    $ vmstat 1 5
    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
     r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
     2  0      0 259832240  46008 2710912    0    0    21     2  223  192  0  1 99  0  0
     1  0      0 259832896  46008 2710744    0    0     0     0 1702206 198078  0 17 82  0  0
     0  0      0 259830272  46008 2710596    0    0     0     0 1696340 197756  1 17 83  0  0
     4  0      0 259829168  46024 2710584    0    0    16     0 1688472 197158  1 17 82  0  0
     3  0      0 259830224  46024 2710408    0    0     0     0 1692450 197212  0 18 82  0  0
    
    As expected, number of interrupts per second is very different.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Cc: Jerry Chu <hkchu@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 42264035dec0..3467d9e89e7d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -253,6 +253,7 @@ struct sock_common {
   *	@sk_ll_usec: usecs to busypoll when there is no data
   *	@sk_allocation: allocation mode
   *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
+  *	@sk_pacing_status: Pacing status (requested, handled by sch_fq)
   *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_padding: unused element for alignment
@@ -396,7 +397,7 @@ struct sock {
 	__s32			sk_peek_off;
 	int			sk_write_pending;
 	__u32			sk_dst_pending_confirm;
-	/* Note: 32bit hole on 64bit arches */
+	u32			sk_pacing_status; /* see enum sk_pacing */
 	long			sk_sndtimeo;
 	struct timer_list	sk_timer;
 	__u32			sk_priority;
@@ -475,6 +476,12 @@ struct sock {
 	struct rcu_head		sk_rcu;
 };
 
+enum sk_pacing {
+	SK_PACING_NONE		= 0,
+	SK_PACING_NEEDED	= 1,
+	SK_PACING_FQ		= 2,
+};
+
 #define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))
 
 #define rcu_dereference_sk_user_data(sk)	rcu_dereference(__sk_user_data((sk)))

commit 65101aeca52241a05e66f23c96eb896c9412718d
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Tue May 16 11:20:13 2017 +0200

    net/sock: factor out dequeue/peek with offset code
    
    And update __sk_queue_drop_skb() to work on the specified queue.
    This will help the udp protocol to use an additional private
    rx queue in a later patch.
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f33e3d134e0b..42264035dec0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2035,8 +2035,8 @@ void sk_reset_timer(struct sock *sk, struct timer_list *timer,
 
 void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
-int __sk_queue_drop_skb(struct sock *sk, struct sk_buff *skb,
-			unsigned int flags,
+int __sk_queue_drop_skb(struct sock *sk, struct sk_buff_head *sk_queue,
+			struct sk_buff *skb, unsigned int flags,
 			void (*destructor)(struct sock *sk,
 					   struct sk_buff *skb));
 int __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);

commit d651983dde41a854e25664d98cbfc999d55785a8
Author: Mauro Carvalho Chehab <mchehab@s-opensource.com>
Date:   Fri May 12 09:35:46 2017 -0300

    net: fix some identation issues at kernel-doc markups
    
    Sphinx is very pedantic with regards to identation and
    escape sequences:
    
      ./include/net/sock.h:1967: ERROR: Unexpected indentation.
      ./include/net/sock.h:1969: ERROR: Unexpected indentation.
      ./include/net/sock.h:1970: WARNING: Block quote ends without a blank line; unexpected unindent.
      ./include/net/sock.h:1971: WARNING: Block quote ends without a blank line; unexpected unindent.
      ./include/net/sock.h:2268: WARNING: Inline emphasis start-string without end-string.
      ./net/core/sock.c:2686: ERROR: Unexpected indentation.
      ./net/core/sock.c:2687: WARNING: Block quote ends without a blank line; unexpected unindent.
      ./net/core/datagram.c:182: WARNING: Inline emphasis start-string without end-string.
      ./include/linux/netdevice.h:1444: ERROR: Unexpected indentation.
      ./drivers/net/phy/phy.c:381: ERROR: Unexpected indentation.
      ./drivers/net/phy/phy.c:382: WARNING: Block quote ends without a blank line; unexpected unindent.
    
    - Fix spacing where needed;
    - Properly escape constants;
    - Use a literal block for a race description.
    
    No functional changes.
    
    Signed-off-by: Mauro Carvalho Chehab <mchehab@s-opensource.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 66349e49d468..9ca99b5c1328 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1953,11 +1953,10 @@ static inline bool sk_has_allocations(const struct sock *sk)
  * The purpose of the skwq_has_sleeper and sock_poll_wait is to wrap the memory
  * barrier call. They were added due to the race found within the tcp code.
  *
- * Consider following tcp code paths:
+ * Consider following tcp code paths::
  *
- * CPU1                  CPU2
- *
- * sys_select            receive packet
+ *   CPU1                CPU2
+ *   sys_select          receive packet
  *   ...                 ...
  *   __add_wait_queue    update tp->rcv_nxt
  *   ...                 ...
@@ -2264,7 +2263,7 @@ void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);
  * @tsflags:	timestamping flags to use
  * @tx_flags:	completed with instructions for time stamping
  *
- * Note : callers should take care of initial *tx_flags value (usually 0)
+ * Note: callers should take care of initial ``*tx_flags`` value (usually 0)
  */
 static inline void sock_tx_timestamp(const struct sock *sk, __u16 tsflags,
 				     __u8 *tx_flags)

commit de4d195308ad589626571dbe5789cebf9695a204
Merge: dc9edaab90de 20652ed6e44f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 10 09:50:55 2017 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes are:
    
       - Debloat RCU headers
    
       - Parallelize SRCU callback handling (plus overlapping patches)
    
       - Improve the performance of Tree SRCU on a CPU-hotplug stress test
    
       - Documentation updates
    
       - Miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (74 commits)
      rcu: Open-code the rcu_cblist_n_lazy_cbs() function
      rcu: Open-code the rcu_cblist_n_cbs() function
      rcu: Open-code the rcu_cblist_empty() function
      rcu: Separately compile large rcu_segcblist functions
      srcu: Debloat the <linux/rcu_segcblist.h> header
      srcu: Adjust default auto-expediting holdoff
      srcu: Specify auto-expedite holdoff time
      srcu: Expedite first synchronize_srcu() when idle
      srcu: Expedited grace periods with reduced memory contention
      srcu: Make rcutorture writer stalls print SRCU GP state
      srcu: Exact tracking of srcu_data structures containing callbacks
      srcu: Make SRCU be built by default
      srcu: Fix Kconfig botch when SRCU not selected
      rcu: Make non-preemptive schedule be Tasks RCU quiescent state
      srcu: Expedite srcu_schedule_cbs_snp() callback invocation
      srcu: Parallelize callback handling
      kvm: Move srcu_struct fields to end of struct kvm
      rcu: Fix typo in PER_RCU_NODE_PERIOD header comment
      rcu: Use true/false in assignment to bool
      rcu: Use bool value directly
      ...

commit 58d30c36d472b75e8e9962d6a640be19d9389128
Merge: 94836ecf1e73 f2094107ac82
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Apr 23 11:12:44 2017 +0200

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
     - Documentation updates.
    
     - Miscellaneous fixes.
    
     - Parallelize SRCU callback handling (plus overlapping patches).
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5f0d5a3ae7cff0d7fa943c199c3a2e44f23e1fac
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 18 02:53:44 2017 -0800

    mm: Rename SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU
    
    A group of Linux kernel hackers reported chasing a bug that resulted
    from their assumption that SLAB_DESTROY_BY_RCU provided an existence
    guarantee, that is, that no block from such a slab would be reallocated
    during an RCU read-side critical section.  Of course, that is not the
    case.  Instead, SLAB_DESTROY_BY_RCU only prevents freeing of an entire
    slab of blocks.
    
    However, there is a phrase for this, namely "type safety".  This commit
    therefore renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU in order
    to avoid future instances of this sort of confusion.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: <linux-mm@kvack.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    [ paulmck: Add comments mentioning the old name, as requested by Eric
      Dumazet, in order to help people familiar with the old name find
      the new one. ]
    Acked-by: David Rientjes <rientjes@google.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5e5997654db6..59cdccaa30e7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -993,7 +993,7 @@ struct smc_hashinfo;
 struct module;
 
 /*
- * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes
+ * caches using SLAB_TYPESAFE_BY_RCU should let .next pointer from nulls nodes
  * un-modified. Special care is taken when initializing object to zero.
  */
 static inline void sk_prot_clear_nulls(struct sock *sk, int size)

commit d3fbff306c215946cdbcf9ace4d0b78e9f72b5c4
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 31 14:59:25 2017 -0700

    sock: correctly test SOCK_TIMESTAMP in sock_recv_ts_and_drops()
    
    It seems the code does not match the intent.
    
    This broke packetdrill, and probably other programs.
    
    Fixes: 6c7c98bad488 ("sock: avoid dirtying sk_stamp, if possible")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8e53158a7d95..66349e49d468 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2250,7 +2250,7 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 
 	if (sk->sk_flags & FLAGS_TS_OR_DROPS || sk->sk_tsflags & TSFLAGS_ANY)
 		__sock_recv_ts_and_drops(msg, sk, skb);
-	else if (unlikely(sk->sk_flags & SOCK_TIMESTAMP))
+	else if (unlikely(sock_flag(sk, SOCK_TIMESTAMP)))
 		sk->sk_stamp = skb->tstamp;
 	else if (unlikely(sk->sk_stamp == SK_DEFAULT_STAMP))
 		sk->sk_stamp = 0;

commit 6c7c98bad4883a4a8710c96b2b44de482865eb6e
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Thu Mar 30 14:03:06 2017 +0200

    sock: avoid dirtying sk_stamp, if possible
    
    sock_recv_ts_and_drops() unconditionally set sk->sk_stamp for
    every packet, even if the SOCK_TIMESTAMP flag is not set in the
    related socket.
    If selinux is enabled, this cause a cache miss for every packet
    since sk->sk_stamp and sk->sk_security share the same cacheline.
    With this change sk_stamp is set only if the SOCK_TIMESTAMP
    flag is set, and is cleared for the first packet, so that the user
    perceived behavior is unchanged.
    
    This gives up to 5% speed-up under udp-flood with small packets.
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cb241a0e8434..8e53158a7d95 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2239,6 +2239,7 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 			      struct sk_buff *skb);
 
+#define SK_DEFAULT_STAMP (-1L * NSEC_PER_SEC)
 static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 					  struct sk_buff *skb)
 {
@@ -2249,8 +2250,10 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 
 	if (sk->sk_flags & FLAGS_TS_OR_DROPS || sk->sk_tsflags & TSFLAGS_ANY)
 		__sock_recv_ts_and_drops(msg, sk, skb);
-	else
+	else if (unlikely(sk->sk_flags & SOCK_TIMESTAMP))
 		sk->sk_stamp = skb->tstamp;
+	else if (unlikely(sk->sk_stamp == SK_DEFAULT_STAMP))
+		sk->sk_stamp = 0;
 }
 
 void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);

commit a2d133b1d465016d0d97560b11f54ba0ace56d3e
Author: Josh Hunt <johunt@akamai.com>
Date:   Mon Mar 20 15:22:03 2017 -0400

    sock: introduce SO_MEMINFO getsockopt
    
    Allows reading of SK_MEMINFO_VARS via socket option. This way an
    application can get all meminfo related information in single socket
    option call instead of multiple calls.
    
    Adds helper function, sk_get_meminfo(), and uses that for both
    getsockopt and sock_diag_put_meminfo().
    
    Suggested by Eric Dumazet.
    
    Signed-off-by: Josh Hunt <johunt@akamai.com>
    Reviewed-by: Jason Baron <jbaron@akamai.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 08142be8938e..cb241a0e8434 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2362,6 +2362,8 @@ bool sk_ns_capable(const struct sock *sk,
 bool sk_capable(const struct sock *sk, int cap);
 bool sk_net_capable(const struct sock *sk, int cap);
 
+void sk_get_meminfo(const struct sock *sk, u32 *meminfo);
+
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 

commit 101c431492d297dd0d111b461d8d324895676bee
Merge: 9c79ddaa0f96 95422dec6bd4
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Mar 15 11:59:10 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/broadcom/genet/bcmgenet.c
            net/core/sock.c
    
    Conflicts were overlapping changes in bcmgenet and the
    lockdep handling of sockets.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cdfbabfb2f0ce983fdaa42f20e5f7842178fc01e
Author: David Howells <dhowells@redhat.com>
Date:   Thu Mar 9 08:09:05 2017 +0000

    net: Work around lockdep limitation in sockets that use sockets
    
    Lockdep issues a circular dependency warning when AFS issues an operation
    through AF_RXRPC from a context in which the VFS/VM holds the mmap_sem.
    
    The theory lockdep comes up with is as follows:
    
     (1) If the pagefault handler decides it needs to read pages from AFS, it
         calls AFS with mmap_sem held and AFS begins an AF_RXRPC call, but
         creating a call requires the socket lock:
    
            mmap_sem must be taken before sk_lock-AF_RXRPC
    
     (2) afs_open_socket() opens an AF_RXRPC socket and binds it.  rxrpc_bind()
         binds the underlying UDP socket whilst holding its socket lock.
         inet_bind() takes its own socket lock:
    
            sk_lock-AF_RXRPC must be taken before sk_lock-AF_INET
    
     (3) Reading from a TCP socket into a userspace buffer might cause a fault
         and thus cause the kernel to take the mmap_sem, but the TCP socket is
         locked whilst doing this:
    
            sk_lock-AF_INET must be taken before mmap_sem
    
    However, lockdep's theory is wrong in this instance because it deals only
    with lock classes and not individual locks.  The AF_INET lock in (2) isn't
    really equivalent to the AF_INET lock in (3) as the former deals with a
    socket entirely internal to the kernel that never sees userspace.  This is
    a limitation in the design of lockdep.
    
    Fix the general case by:
    
     (1) Double up all the locking keys used in sockets so that one set are
         used if the socket is created by userspace and the other set is used
         if the socket is created by the kernel.
    
     (2) Store the kern parameter passed to sk_alloc() in a variable in the
         sock struct (sk_kern_sock).  This informs sock_lock_init(),
         sock_init_data() and sk_clone_lock() as to the lock keys to be used.
    
         Note that the child created by sk_clone_lock() inherits the parent's
         kern setting.
    
     (3) Add a 'kern' parameter to ->accept() that is analogous to the one
         passed in to ->create() that distinguishes whether kernel_accept() or
         sys_accept4() was the caller and can be passed to sk_alloc().
    
         Note that a lot of accept functions merely dequeue an already
         allocated socket.  I haven't touched these as the new socket already
         exists before we get the parameter.
    
         Note also that there are a couple of places where I've made the accepted
         socket unconditionally kernel-based:
    
            irda_accept()
            rds_rcp_accept_one()
            tcp_accept_from_sock()
    
         because they follow a sock_create_kern() and accept off of that.
    
    Whilst creating this, I noticed that lustre and ocfs don't create sockets
    through sock_create_kern() and thus they aren't marked as for-kernel,
    though they appear to be internal.  I wonder if these should do that so
    that they use the new set of lock keys.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5e5997654db6..03252d53975d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -236,6 +236,7 @@ struct sock_common {
   *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
   *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
   *	@sk_lock:	synchronizer
+  *	@sk_kern_sock: True if sock is using kernel lock classes
   *	@sk_rcvbuf: size of receive buffer in bytes
   *	@sk_wq: sock wait queue and async head
   *	@sk_rx_dst: receive input route used by early demux
@@ -430,7 +431,8 @@ struct sock {
 #endif
 
 	kmemcheck_bitfield_begin(flags);
-	unsigned int		sk_padding : 2,
+	unsigned int		sk_padding : 1,
+				sk_kern_sock : 1,
 				sk_no_check_tx : 1,
 				sk_no_check_rx : 1,
 				sk_userlocks : 4,
@@ -1015,7 +1017,8 @@ struct proto {
 					int addr_len);
 	int			(*disconnect)(struct sock *sk, int flags);
 
-	struct sock *		(*accept)(struct sock *sk, int flags, int *err);
+	struct sock *		(*accept)(struct sock *sk, int flags, int *err,
+					  bool kern);
 
 	int			(*ioctl)(struct sock *sk, int cmd,
 					 unsigned long arg);
@@ -1573,7 +1576,7 @@ int sock_cmsg_send(struct sock *sk, struct msghdr *msg,
 int sock_no_bind(struct socket *, struct sockaddr *, int);
 int sock_no_connect(struct socket *, struct sockaddr *, int, int);
 int sock_no_socketpair(struct socket *, struct socket *);
-int sock_no_accept(struct socket *, struct socket *, int);
+int sock_no_accept(struct socket *, struct socket *, int, bool);
 int sock_no_getname(struct socket *, struct sockaddr *, int *, int);
 unsigned int sock_no_poll(struct file *, struct socket *,
 			  struct poll_table_struct *);

commit 95964c6de787eb21468a29b94e9d25e1a24d6a37
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Mar 6 11:23:55 2017 -0800

    net: use proper lockdep annotation in __sk_dst_set()
    
    __sk_dst_set() must be called while we own the socket.
    
    We can get proper lockdep coverage using lockdep_sock_is_held()
    and rcu_dereference_protected()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5e5997654db6..6db7693b9e61 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1780,11 +1780,8 @@ __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 
 	sk_tx_queue_clear(sk);
 	sk->sk_dst_pending_confirm = 0;
-	/*
-	 * This can be called while sk is owned by the caller only,
-	 * with no state that can be checked in a rcu_dereference_check() cond
-	 */
-	old_dst = rcu_dereference_raw(sk->sk_dst_cache);
+	old_dst = rcu_dereference_protected(sk->sk_dst_cache,
+					    lockdep_sock_is_held(sk));
 	rcu_assign_pointer(sk->sk_dst_cache, dst);
 	dst_release(old_dst);
 }

commit 94352d45092c23874532221b4d1e4721df9d63df
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Mar 1 16:35:08 2017 -0300

    net: Introduce sk_clone_lock() error path routine
    
    When handling problems in cloning a socket with the sk_clone_locked()
    function we need to perform several steps that were open coded in it and
    its callers, so introduce a routine to avoid this duplication:
    sk_free_unlock_clone().
    
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Gerrit Renker <gerrit@erg.abdn.ac.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/n/net-ui6laqkotycunhtmqryl9bfx@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9ccefa5c5487..5e5997654db6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1526,6 +1526,7 @@ struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
 void sk_free(struct sock *sk);
 void sk_destruct(struct sock *sk);
 struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
+void sk_free_unlock_clone(struct sock *sk);
 
 struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
 			     gfp_t priority);

commit 3efa70d78f218e4c9276b0bac0545e5184c1c47b
Merge: 76e0e70e6452 926af6273fc6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Feb 7 16:29:30 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The conflict was an interaction between a bug fix in the
    netvsc driver in 'net' and an optimization of the RX path
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4ff0620354f2b39b9fe2a91c22c4de9d1fba0c8e
Author: Julian Anastasov <ja@ssi.bg>
Date:   Mon Feb 6 23:14:12 2017 +0200

    net: add dst_pending_confirm flag to skbuff
    
    Add new skbuff flag to allow protocols to confirm neighbour.
    When same struct dst_entry can be used for many different
    neighbours we can not use it for pending confirmations.
    
    Add sock_confirm_neigh() helper to confirm the neighbour and
    use it for IPv4, IPv6 and VRF before dst_neigh_output.
    
    Signed-off-by: Julian Anastasov <ja@ssi.bg>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 85d856b94b4b..6f83e78eaa5a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1821,6 +1821,20 @@ static inline void sk_dst_confirm(struct sock *sk)
 		sk->sk_dst_pending_confirm = 1;
 }
 
+static inline void sock_confirm_neigh(struct sk_buff *skb, struct neighbour *n)
+{
+	if (skb_get_dst_pending_confirm(skb)) {
+		struct sock *sk = skb->sk;
+		unsigned long now = jiffies;
+
+		/* avoid dirtying neighbour */
+		if (n->confirmed != now)
+			n->confirmed = now;
+		if (sk && sk->sk_dst_pending_confirm)
+			sk->sk_dst_pending_confirm = 0;
+	}
+}
+
 bool sk_mc_loop(struct sock *sk);
 
 static inline bool sk_can_gso(const struct sock *sk)

commit 9b8805a325591cf5b6b9df71200de25a2bd721fd
Author: Julian Anastasov <ja@ssi.bg>
Date:   Mon Feb 6 23:14:11 2017 +0200

    sock: add sk_dst_pending_confirm flag
    
    Add new sock flag to allow sockets to confirm neighbour.
    When same struct dst_entry can be used for many different
    neighbours we can not use it for pending confirmations.
    As not all call paths lock the socket use full word for
    the flag.
    
    Add sk_dst_confirm as replacement for dst_confirm when
    called for received packets.
    
    Signed-off-by: Julian Anastasov <ja@ssi.bg>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 94e65fd70354..85d856b94b4b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -240,6 +240,7 @@ struct sock_common {
   *	@sk_wq: sock wait queue and async head
   *	@sk_rx_dst: receive input route used by early demux
   *	@sk_dst_cache: destination cache
+  *	@sk_dst_pending_confirm: need to confirm neighbour
   *	@sk_policy: flow policy
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
@@ -393,6 +394,8 @@ struct sock {
 	struct sk_buff_head	sk_write_queue;
 	__s32			sk_peek_off;
 	int			sk_write_pending;
+	__u32			sk_dst_pending_confirm;
+	/* Note: 32bit hole on 64bit arches */
 	long			sk_sndtimeo;
 	struct timer_list	sk_timer;
 	__u32			sk_priority;
@@ -1764,6 +1767,7 @@ static inline void dst_negative_advice(struct sock *sk)
 		if (ndst != dst) {
 			rcu_assign_pointer(sk->sk_dst_cache, ndst);
 			sk_tx_queue_clear(sk);
+			sk->sk_dst_pending_confirm = 0;
 		}
 	}
 }
@@ -1774,6 +1778,7 @@ __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 	struct dst_entry *old_dst;
 
 	sk_tx_queue_clear(sk);
+	sk->sk_dst_pending_confirm = 0;
 	/*
 	 * This can be called while sk is owned by the caller only,
 	 * with no state that can be checked in a rcu_dereference_check() cond
@@ -1789,6 +1794,7 @@ sk_dst_set(struct sock *sk, struct dst_entry *dst)
 	struct dst_entry *old_dst;
 
 	sk_tx_queue_clear(sk);
+	sk->sk_dst_pending_confirm = 0;
 	old_dst = xchg((__force struct dst_entry **)&sk->sk_dst_cache, dst);
 	dst_release(old_dst);
 }
@@ -1809,6 +1815,12 @@ struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
 struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
+static inline void sk_dst_confirm(struct sock *sk)
+{
+	if (!sk->sk_dst_pending_confirm)
+		sk->sk_dst_pending_confirm = 1;
+}
+
 bool sk_mc_loop(struct sock *sk);
 
 static inline bool sk_can_gso(const struct sock *sk)

commit 69629464e0b587f3711739b3aa2bcdaf2e075276
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Feb 5 09:25:24 2017 -0800

    udp: properly cope with csum errors
    
    Dmitry reported that UDP sockets being destroyed would trigger the
    WARN_ON(atomic_read(&sk->sk_rmem_alloc)); in inet_sock_destruct()
    
    It turns out we do not properly destroy skb(s) that have wrong UDP
    checksum.
    
    Thanks again to syzkaller team.
    
    Fixes : 7c13f97ffde6 ("udp: do fwd memory scheduling on dequeue")
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Paolo Abeni <pabeni@redhat.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f0e867f58722..c4f5e6fca17c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2006,7 +2006,9 @@ void sk_reset_timer(struct sock *sk, struct timer_list *timer,
 void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
 int __sk_queue_drop_skb(struct sock *sk, struct sk_buff *skb,
-			unsigned int flags);
+			unsigned int flags,
+			void (*destructor)(struct sock *sk,
+					   struct sk_buff *skb));
 int __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 

commit 158f323b9868b59967ad96957c4ca388161be321
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jan 27 07:11:27 2017 -0800

    net: adjust skb->truesize in pskb_expand_head()
    
    Slava Shwartsman reported a warning in skb_try_coalesce(), when we
    detect skb->truesize is completely wrong.
    
    In his case, issue came from IPv6 reassembly coping with malicious
    datagrams, that forced various pskb_may_pull() to reallocate a bigger
    skb->head than the one allocated by NIC driver before entering GRO
    layer.
    
    Current code does not change skb->truesize, leaving this burden to
    callers if they care enough.
    
    Blindly changing skb->truesize in pskb_expand_head() is not
    easy, as some producers might track skb->truesize, for example
    in xmit path for back pressure feedback (sk->sk_wmem_alloc)
    
    We can detect the cases where it should be safe to change
    skb->truesize :
    
    1) skb is not attached to a socket.
    2) If it is attached to a socket, destructor is sock_edemux()
    
    My audit gave only two callers doing their own skb->truesize
    manipulation.
    
    I had to remove skb parameter in sock_edemux macro when
    CONFIG_INET is not set to avoid a compile error.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Slava Shwartsman <slavash@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7144750d14e5..94e65fd70354 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1534,7 +1534,7 @@ void sock_efree(struct sk_buff *skb);
 #ifdef CONFIG_INET
 void sock_edemux(struct sk_buff *skb);
 #else
-#define sock_edemux(skb) sock_efree(skb)
+#define sock_edemux sock_efree
 #endif
 
 int sock_setsockopt(struct socket *sock, int level, int op,

commit 6c59ebd356ff2ca64cdf1f61c5fe17f6fa8fc045
Author: Geliang Tang <geliangtang@gmail.com>
Date:   Fri Jan 20 22:27:04 2017 +0800

    sock: use hlist_entry_safe
    
    Use hlist_entry_safe() instead of open-coding it.
    
    Signed-off-by: Geliang Tang <geliangtang@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 389a0a619b45..7144750d14e5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -544,8 +544,7 @@ static inline struct sock *sk_nulls_head(const struct hlist_nulls_head *head)
 
 static inline struct sock *sk_next(const struct sock *sk)
 {
-	return sk->sk_node.next ?
-		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
+	return hlist_entry_safe(sk->sk_node.next, struct sock, sk_node);
 }
 
 static inline struct sock *sk_nulls_next(const struct sock *sk)

commit f16a7dd5cf27eeda187425c9c7d96802a549f9c4
Author: Ursula Braun <ubraun@linux.vnet.ibm.com>
Date:   Mon Jan 9 16:55:26 2017 +0100

    smc: netlink interface for SMC sockets
    
    Support for SMC socket monitoring via netlink sockets of protocol
    NETLINK_SOCK_DIAG.
    
    Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 99deda67eba0..389a0a619b45 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -70,6 +70,7 @@
 #include <net/checksum.h>
 #include <net/tcp_states.h>
 #include <linux/net_tstamp.h>
+#include <net/smc.h>
 
 /*
  * This structure really needs to be cleaned up.
@@ -986,6 +987,7 @@ struct request_sock_ops;
 struct timewait_sock_ops;
 struct inet_hashinfo;
 struct raw_hashinfo;
+struct smc_hashinfo;
 struct module;
 
 /*
@@ -1094,6 +1096,7 @@ struct proto {
 		struct inet_hashinfo	*hashinfo;
 		struct udp_table	*udp_table;
 		struct raw_hashinfo	*raw_hash;
+		struct smc_hashinfo	*smc_hash;
 	} h;
 
 	struct module		*owner;

commit 4b9d07a44015a0e940448fa3885b894349e8b162
Author: Ursula Braun <ubraun@linux.vnet.ibm.com>
Date:   Mon Jan 9 16:55:12 2017 +0100

    net: introduce keepalive function in struct proto
    
    Direct call of tcp_set_keepalive() function from protocol-agnostic
    sock_setsockopt() function in net/core/sock.c violates network
    layering. And newly introduced protocol (SMC-R) will need its own
    keepalive function. Therefore, add "keepalive" function pointer
    to "struct proto", and call it from sock_setsockopt() via this pointer.
    
    Signed-off-by: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Reviewed-by: Utz Bacher <utz.bacher@de.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f0e867f58722..99deda67eba0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1024,6 +1024,7 @@ struct proto {
 	int			(*getsockopt)(struct sock *sk, int level,
 					int optname, char __user *optval,
 					int __user *option);
+	void			(*keepalive)(struct sock *sk, int valbool);
 #ifdef CONFIG_COMPAT
 	int			(*compat_setsockopt)(struct sock *sk,
 					int level,

commit 2456e855354415bfaeb7badaa14e11b3e02c8466
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 25 11:38:40 2016 +0100

    ktime: Get rid of the union
    
    ktime is a union because the initial implementation stored the time in
    scalar nanoseconds on 64 bit machine and in a endianess optimized timespec
    variant for 32bit machines. The Y2038 cleanup removed the timespec variant
    and switched everything to scalar nanoseconds. The union remained, but
    become completely pointless.
    
    Get rid of the union and just keep ktime_t as simple typedef of type s64.
    
    The conversion was done with coccinelle and some manual mopping up.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 282d065e286b..f0e867f58722 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2193,8 +2193,8 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 	 */
 	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
 	    (sk->sk_tsflags & SOF_TIMESTAMPING_RX_SOFTWARE) ||
-	    (kt.tv64 && sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) ||
-	    (hwtstamps->hwtstamp.tv64 &&
+	    (kt && sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) ||
+	    (hwtstamps->hwtstamp &&
 	     (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);
 	else

commit 9a19a6db37ee0b7a6db796b3dcd6bb6e7237d6ea
Merge: bd9999cd6a5e c4364f837caf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 10:24:44 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
    
     - more ->d_init() stuff (work.dcache)
    
     - pathname resolution cleanups (work.namei)
    
     - a few missing iov_iter primitives - copy_from_iter_full() and
       friends. Either copy the full requested amount, advance the iterator
       and return true, or fail, return false and do _not_ advance the
       iterator. Quite a few open-coded callers converted (and became more
       readable and harder to fuck up that way) (work.iov_iter)
    
     - several assorted patches, the big one being logfs removal
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      logfs: remove from tree
      vfs: fix put_compat_statfs64() does not handle errors
      namei: fold should_follow_link() with the step into not-followed link
      namei: pass both WALK_GET and WALK_MORE to should_follow_link()
      namei: invert WALK_PUT logics
      namei: shift interpretation of LOOKUP_FOLLOW inside should_follow_link()
      namei: saner calling conventions for mountpoint_last()
      namei.c: get rid of user_path_parent()
      switch getfrag callbacks to ..._full() primitives
      make skb_add_data,{_nocache}() and skb_copy_to_page_nocache() advance only on success
      [iov_iter] new primitives - copy_from_iter_full() and friends
      don't open-code file_inode()
      ceph: switch to use of ->d_init()
      ceph: unify dentry_operations instances
      lustre: switch to use of ->d_init()

commit 3665f3817cd354ab7a811b3a4f282c4f5cb1a0d0
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Dec 7 10:05:36 2016 -0800

    net: do not read sk_drops if application does not care
    
    sk_drops can be an often written field, do not read it unless
    application showed interest.
    
    Note that sk_drops can be read via inet_diag, so applications
    can avoid getting this info from every received packet.
    
    In the future, 'reading' sk_drops might require folding per node or per
    cpu fields, and thus become even more expensive than today.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2729e77950b7..e17aa3de2b4d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2163,7 +2163,8 @@ struct sock_skb_cb {
 static inline void
 sock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)
 {
-	SOCK_SKB_CB(skb)->dropcount = atomic_read(&sk->sk_drops);
+	SOCK_SKB_CB(skb)->dropcount = sock_flag(sk, SOCK_RXQ_OVFL) ?
+						atomic_read(&sk->sk_drops) : 0;
 }
 
 static inline void sk_drops_add(struct sock *sk, const struct sk_buff *skb)

commit 13bfff25c081f4e060af761c4082b5a96f756810
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Dec 7 08:29:10 2016 -0800

    net: rfs: add a jump label
    
    RFS is not commonly used, so add a jump label to avoid some conditionals
    in fast path.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1749e38d0301..2729e77950b7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -913,17 +913,20 @@ static inline void sock_rps_record_flow_hash(__u32 hash)
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
 #ifdef CONFIG_RPS
-	/* Reading sk->sk_rxhash might incur an expensive cache line miss.
-	 *
-	 * TCP_ESTABLISHED does cover almost all states where RFS
-	 * might be useful, and is cheaper [1] than testing :
-	 *	IPv4: inet_sk(sk)->inet_daddr
-	 * 	IPv6: ipv6_addr_any(&sk->sk_v6_daddr)
-	 * OR	an additional socket flag
-	 * [1] : sk_state and sk_prot are in the same cache line.
-	 */
-	if (sk->sk_state == TCP_ESTABLISHED)
-		sock_rps_record_flow_hash(sk->sk_rxhash);
+	if (static_key_false(&rfs_needed)) {
+		/* Reading sk->sk_rxhash might incur an expensive cache line
+		 * miss.
+		 *
+		 * TCP_ESTABLISHED does cover almost all states where RFS
+		 * might be useful, and is cheaper [1] than testing :
+		 *	IPv4: inet_sk(sk)->inet_daddr
+		 * 	IPv6: ipv6_addr_any(&sk->sk_v6_daddr)
+		 * OR	an additional socket flag
+		 * [1] : sk_state and sk_prot are in the same cache line.
+		 */
+		if (sk->sk_state == TCP_ESTABLISHED)
+			sock_rps_record_flow_hash(sk->sk_rxhash);
+	}
 #endif
 }
 

commit 5b8e2f61b9df529ca4af057daf7bfb1de348bdf1
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 6 19:32:50 2016 -0800

    net: sock_rps_record_flow() is for connected sockets
    
    Paolo noticed a cache line miss in UDP recvmsg() to access
    sk_rxhash, sharing a cache line with sk_drops.
    
    sk_drops might be heavily incremented by cpus handling a flood targeting
    this socket.
    
    We might place sk_drops on a separate cache line, but lets try
    to avoid wasting 64 bytes per socket just for this, since we have
    other bottlenecks to take care of.
    
    sock_rps_record_flow() should only access sk_rxhash for connected
    flows.
    
    Testing sk_state for TCP_ESTABLISHED covers most of the cases for
    connected sockets, for a zero cost, since system calls using
    sock_rps_record_flow() also access sk->sk_prot which is on the
    same cache line.
    
    A follow up patch will provide a static_key (Jump Label) since most
    hosts do not even use RFS.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6dfe3aa22b97..1749e38d0301 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -913,7 +913,17 @@ static inline void sock_rps_record_flow_hash(__u32 hash)
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
 #ifdef CONFIG_RPS
-	sock_rps_record_flow_hash(sk->sk_rxhash);
+	/* Reading sk->sk_rxhash might incur an expensive cache line miss.
+	 *
+	 * TCP_ESTABLISHED does cover almost all states where RFS
+	 * might be useful, and is cheaper [1] than testing :
+	 *	IPv4: inet_sk(sk)->inet_daddr
+	 * 	IPv6: ipv6_addr_any(&sk->sk_v6_daddr)
+	 * OR	an additional socket flag
+	 * [1] : sk_state and sk_prot are in the same cache line.
+	 */
+	if (sk->sk_state == TCP_ESTABLISHED)
+		sock_rps_record_flow_hash(sk->sk_rxhash);
 #endif
 }
 

commit 15e6cb46c9b09711d1224ae5418b53140e1ba444
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 1 22:42:45 2016 -0400

    make skb_add_data,{_nocache}() and skb_copy_to_page_nocache() advance only on success
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index 92b269709b9a..5dd0fed82a06 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1783,13 +1783,13 @@ static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
 {
 	if (skb->ip_summed == CHECKSUM_NONE) {
 		__wsum csum = 0;
-		if (csum_and_copy_from_iter(to, copy, &csum, from) != copy)
+		if (!csum_and_copy_from_iter_full(to, copy, &csum, from))
 			return -EFAULT;
 		skb->csum = csum_block_add(skb->csum, csum, offset);
 	} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {
-		if (copy_from_iter_nocache(to, copy, from) != copy)
+		if (!copy_from_iter_full_nocache(to, copy, from))
 			return -EFAULT;
-	} else if (copy_from_iter(to, copy, from) != copy)
+	} else if (!copy_from_iter_full(to, copy, from))
 		return -EFAULT;
 
 	return 0;

commit 9115e8cd2a0c6eaaa900c462721f12e1d45f326c
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Dec 3 11:14:56 2016 -0800

    net: reorganize struct sock for better data locality
    
    Group fields used in TX path, and keep some cache lines mostly read
    to permit sharing among cpus.
    
    Gained two 4 bytes holes on 64bit arches.
    
    Added a place holder for tcp tsq_flags, next to sk_wmem_alloc
    to speed up tcp_wfree() in the following patch.
    
    I have not added ____cacheline_aligned_in_smp, this might be done later.
    I prefer doing this once inet and tcp/udp sockets reorg is also done.
    
    Tested with both TCP and UDP.
    
    UDP receiver performance under flood increased by ~20 % :
    Accessing sk_filter/sk_wq/sk_napi_id no longer stalls because sk_drops
    was moved away from a critical cache line, now mostly read and shared.
    
            /* --- cacheline 4 boundary (256 bytes) --- */
            unsigned int               sk_napi_id;           /* 0x100   0x4 */
            int                        sk_rcvbuf;            /* 0x104   0x4 */
            struct sk_filter *         sk_filter;            /* 0x108   0x8 */
            union {
                    struct socket_wq * sk_wq;                /*         0x8 */
                    struct socket_wq * sk_wq_raw;            /*         0x8 */
            };                                               /* 0x110   0x8 */
            struct xfrm_policy *       sk_policy[2];         /* 0x118  0x10 */
            struct dst_entry *         sk_rx_dst;            /* 0x128   0x8 */
            struct dst_entry *         sk_dst_cache;         /* 0x130   0x8 */
            atomic_t                   sk_omem_alloc;        /* 0x138   0x4 */
            int                        sk_sndbuf;            /* 0x13c   0x4 */
            /* --- cacheline 5 boundary (320 bytes) --- */
            int                        sk_wmem_queued;       /* 0x140   0x4 */
            atomic_t                   sk_wmem_alloc;        /* 0x144   0x4 */
            long unsigned int          sk_tsq_flags;         /* 0x148   0x8 */
            struct sk_buff *           sk_send_head;         /* 0x150   0x8 */
            struct sk_buff_head        sk_write_queue;       /* 0x158  0x18 */
            __s32                      sk_peek_off;          /* 0x170   0x4 */
            int                        sk_write_pending;     /* 0x174   0x4 */
            long int                   sk_sndtimeo;          /* 0x178   0x8 */
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 69afda6bea15..6dfe3aa22b97 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -343,6 +343,9 @@ struct sock {
 #define sk_rxhash		__sk_common.skc_rxhash
 
 	socket_lock_t		sk_lock;
+	atomic_t		sk_drops;
+	int			sk_rcvlowat;
+	struct sk_buff_head	sk_error_queue;
 	struct sk_buff_head	sk_receive_queue;
 	/*
 	 * The backlog queue is special, it is always used with
@@ -359,14 +362,13 @@ struct sock {
 		struct sk_buff	*tail;
 	} sk_backlog;
 #define sk_rmem_alloc sk_backlog.rmem_alloc
-	int			sk_forward_alloc;
 
-	__u32			sk_txhash;
+	int			sk_forward_alloc;
 #ifdef CONFIG_NET_RX_BUSY_POLL
-	unsigned int		sk_napi_id;
 	unsigned int		sk_ll_usec;
+	/* ===== mostly read cache line ===== */
+	unsigned int		sk_napi_id;
 #endif
-	atomic_t		sk_drops;
 	int			sk_rcvbuf;
 
 	struct sk_filter __rcu	*sk_filter;
@@ -379,11 +381,30 @@ struct sock {
 #endif
 	struct dst_entry	*sk_rx_dst;
 	struct dst_entry __rcu	*sk_dst_cache;
-	/* Note: 32bit hole on 64bit arches */
-	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;
+
+	/* ===== cache line for TX ===== */
+	int			sk_wmem_queued;
+	atomic_t		sk_wmem_alloc;
+	unsigned long		sk_tsq_flags;
+	struct sk_buff		*sk_send_head;
 	struct sk_buff_head	sk_write_queue;
+	__s32			sk_peek_off;
+	int			sk_write_pending;
+	long			sk_sndtimeo;
+	struct timer_list	sk_timer;
+	__u32			sk_priority;
+	__u32			sk_mark;
+	u32			sk_pacing_rate; /* bytes per second */
+	u32			sk_max_pacing_rate;
+	struct page_frag	sk_frag;
+	netdev_features_t	sk_route_caps;
+	netdev_features_t	sk_route_nocaps;
+	int			sk_gso_type;
+	unsigned int		sk_gso_max_size;
+	gfp_t			sk_allocation;
+	__u32			sk_txhash;
 
 	/*
 	 * Because of non atomicity rules, all
@@ -414,42 +435,24 @@ struct sock {
 #define SK_PROTOCOL_MAX U8_MAX
 	kmemcheck_bitfield_end(flags);
 
-	int			sk_wmem_queued;
-	gfp_t			sk_allocation;
-	u32			sk_pacing_rate; /* bytes per second */
-	u32			sk_max_pacing_rate;
-	netdev_features_t	sk_route_caps;
-	netdev_features_t	sk_route_nocaps;
-	int			sk_gso_type;
-	unsigned int		sk_gso_max_size;
 	u16			sk_gso_max_segs;
-	int			sk_rcvlowat;
 	unsigned long	        sk_lingertime;
-	struct sk_buff_head	sk_error_queue;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
 				sk_err_soft;
 	u32			sk_ack_backlog;
 	u32			sk_max_ack_backlog;
-	__u32			sk_priority;
-	__u32			sk_mark;
 	kuid_t			sk_uid;
 	struct pid		*sk_peer_pid;
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
-	long			sk_sndtimeo;
-	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;
 	u16			sk_tsflags;
 	u8			sk_shutdown;
 	u32			sk_tskey;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
-	struct page_frag	sk_frag;
-	struct sk_buff		*sk_send_head;
-	__s32			sk_peek_off;
-	int			sk_write_pending;
 #ifdef CONFIG_SECURITY
 	void			*sk_security;
 #endif

commit aa4c1037a30f4e88f444e83d42c2befbe0d5caf5
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Thu Dec 1 08:48:06 2016 -0800

    bpf: Add support for reading socket family, type, protocol
    
    Add socket family, type and protocol to bpf_sock allowing bpf programs
    read-only access.
    
    Add __sk_flags_offset[0] to struct sock before the bitfield to
    programmtically determine the offset of the unsigned int containing
    protocol and type.
    
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 442cbb118a07..69afda6bea15 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -389,6 +389,21 @@ struct sock {
 	 * Because of non atomicity rules, all
 	 * changes are protected by socket lock.
 	 */
+	unsigned int		__sk_flags_offset[0];
+#ifdef __BIG_ENDIAN_BITFIELD
+#define SK_FL_PROTO_SHIFT  16
+#define SK_FL_PROTO_MASK   0x00ff0000
+
+#define SK_FL_TYPE_SHIFT   0
+#define SK_FL_TYPE_MASK    0x0000ffff
+#else
+#define SK_FL_PROTO_SHIFT  8
+#define SK_FL_PROTO_MASK   0x0000ff00
+
+#define SK_FL_TYPE_SHIFT   16
+#define SK_FL_TYPE_MASK    0xffff0000
+#endif
+
 	kmemcheck_bitfield_begin(flags);
 	unsigned int		sk_padding : 2,
 				sk_no_check_tx : 1,

commit bb598c1b8c9bf56981927dcb8c0dc34b8ff95342
Merge: eb2ca35f1814 e76d21c40bd6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Nov 15 10:54:36 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several cases of bug fixes in 'net' overlapping other changes in
    'net-next-.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d9dc8b0f8b4ec8cdc48ad5a20a3105387138be82
Author: WANG Cong <xiyou.wangcong@gmail.com>
Date:   Fri Nov 11 10:20:50 2016 -0800

    net: fix sleeping for sk_wait_event()
    
    Similar to commit 14135f30e33c ("inet: fix sleeping inside inet_wait_for_connect()"),
    sk_wait_event() needs to fix too, because release_sock() is blocking,
    it changes the process state back to running after sleep, which breaks
    the previous prepare_to_wait().
    
    Switch to the new wait API.
    
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cf617ee16723..9d905ed0cd25 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -915,14 +915,16 @@ static inline void sock_rps_reset_rxhash(struct sock *sk)
 #endif
 }
 
-#define sk_wait_event(__sk, __timeo, __condition)			\
+#define sk_wait_event(__sk, __timeo, __condition, __wait)		\
 	({	int __rc;						\
 		release_sock(__sk);					\
 		__rc = __condition;					\
 		if (!__rc) {						\
-			*(__timeo) = schedule_timeout(*(__timeo));	\
+			*(__timeo) = wait_woken(__wait,			\
+						TASK_INTERRUPTIBLE,	\
+						*(__timeo));		\
 		}							\
-		sched_annotate_sleep();						\
+		sched_annotate_sleep();					\
 		lock_sock(__sk);					\
 		__rc = __condition;					\
 		__rc;							\

commit 86741ec25462e4c8cdce6df2f41ead05568c7d5e
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Fri Nov 4 02:23:41 2016 +0900

    net: core: Add a UID field to struct sock.
    
    Protocol sockets (struct sock) don't have UIDs, but most of the
    time, they map 1:1 to userspace sockets (struct socket) which do.
    
    Various operations such as the iptables xt_owner match need
    access to the "UID of a socket", and do so by following the
    backpointer to the struct socket. This involves taking
    sk_callback_lock and doesn't work when there is no socket
    because userspace has already called close().
    
    Simplify this by adding a sk_uid field to struct sock whose value
    matches the UID of the corresponding struct socket. The semantics
    are as follows:
    
    1. Whenever sk_socket is non-null: sk_uid is the same as the UID
       in sk_socket, i.e., matches the return value of sock_i_uid.
       Specifically, the UID is set when userspace calls socket(),
       fchown(), or accept().
    2. When sk_socket is NULL, sk_uid is defined as follows:
       - For a socket that no longer has a sk_socket because
         userspace has called close(): the previous UID.
       - For a cloned socket (e.g., an incoming connection that is
         established but on which userspace has not yet called
         accept): the UID of the socket it was cloned from.
       - For a socket that has never had an sk_socket: UID 0 inside
         the user namespace corresponding to the network namespace
         the socket belongs to.
    
    Kernel sockets created by sock_create_kern are a special case
    of #1 and sk_uid is the user that created them. For kernel
    sockets created at network namespace creation time, such as the
    per-processor ICMP and TCP sockets, this is the user that created
    the network namespace.
    
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 93331a1492db..cf617ee16723 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -419,6 +419,7 @@ struct sock {
 	u32			sk_max_ack_backlog;
 	__u32			sk_priority;
 	__u32			sk_mark;
+	kuid_t			sk_uid;
 	struct pid		*sk_peer_pid;
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
@@ -1664,6 +1665,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	sk->sk_wq = parent->wq;
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
+	sk->sk_uid = SOCK_INODE(parent)->i_uid;
 	security_sock_graft(sk, parent);
 	write_unlock_bh(&sk->sk_callback_lock);
 }
@@ -1671,6 +1673,11 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 kuid_t sock_i_uid(struct sock *sk);
 unsigned long sock_i_ino(struct sock *sk);
 
+static inline kuid_t sock_net_uid(const struct net *net, const struct sock *sk)
+{
+	return sk ? sk->sk_uid : make_kuid(net->user_ns, 0);
+}
+
 static inline u32 net_tx_rndhash(void)
 {
 	u32 v = prandom_u32();

commit c3f24cfb3e508c70c26ee8569d537c8ca67a36c6
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Nov 2 17:14:41 2016 -0700

    dccp: do not release listeners too soon
    
    Andrey Konovalov reported following error while fuzzing with syzkaller :
    
    IPv4: Attempt to release alive inet socket ffff880068e98940
    kasan: CONFIG_KASAN_INLINE enabled
    kasan: GPF could be caused by NULL-ptr deref or user memory access
    general protection fault: 0000 [#1] SMP KASAN
    Modules linked in:
    CPU: 1 PID: 3905 Comm: a.out Not tainted 4.9.0-rc3+ #333
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    task: ffff88006b9e0000 task.stack: ffff880068770000
    RIP: 0010:[<ffffffff819ead5f>]  [<ffffffff819ead5f>]
    selinux_socket_sock_rcv_skb+0xff/0x6a0 security/selinux/hooks.c:4639
    RSP: 0018:ffff8800687771c8  EFLAGS: 00010202
    RAX: ffff88006b9e0000 RBX: 1ffff1000d0eee3f RCX: 1ffff1000d1d312a
    RDX: 1ffff1000d1d31a6 RSI: dffffc0000000000 RDI: 0000000000000010
    RBP: ffff880068777360 R08: 0000000000000000 R09: 0000000000000002
    R10: dffffc0000000000 R11: 0000000000000006 R12: ffff880068e98940
    R13: 0000000000000002 R14: ffff880068777338 R15: 0000000000000000
    FS:  00007f00ff760700(0000) GS:ffff88006cd00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000020008000 CR3: 000000006a308000 CR4: 00000000000006e0
    Stack:
     ffff8800687771e0 ffffffff812508a5 ffff8800686f3168 0000000000000007
     ffff88006ac8cdfc ffff8800665ea500 0000000041b58ab3 ffffffff847b5480
     ffffffff819eac60 ffff88006b9e0860 ffff88006b9e0868 ffff88006b9e07f0
    Call Trace:
     [<ffffffff819c8dd5>] security_sock_rcv_skb+0x75/0xb0 security/security.c:1317
     [<ffffffff82c2a9e7>] sk_filter_trim_cap+0x67/0x10e0 net/core/filter.c:81
     [<ffffffff82b81e60>] __sk_receive_skb+0x30/0xa00 net/core/sock.c:460
     [<ffffffff838bbf12>] dccp_v4_rcv+0xdb2/0x1910 net/dccp/ipv4.c:873
     [<ffffffff83069d22>] ip_local_deliver_finish+0x332/0xad0
    net/ipv4/ip_input.c:216
     [<     inline     >] NF_HOOK_THRESH ./include/linux/netfilter.h:232
     [<     inline     >] NF_HOOK ./include/linux/netfilter.h:255
     [<ffffffff8306abd2>] ip_local_deliver+0x1c2/0x4b0 net/ipv4/ip_input.c:257
     [<     inline     >] dst_input ./include/net/dst.h:507
     [<ffffffff83068500>] ip_rcv_finish+0x750/0x1c40 net/ipv4/ip_input.c:396
     [<     inline     >] NF_HOOK_THRESH ./include/linux/netfilter.h:232
     [<     inline     >] NF_HOOK ./include/linux/netfilter.h:255
     [<ffffffff8306b82f>] ip_rcv+0x96f/0x12f0 net/ipv4/ip_input.c:487
     [<ffffffff82bd9fb7>] __netif_receive_skb_core+0x1897/0x2a50 net/core/dev.c:4213
     [<ffffffff82bdb19a>] __netif_receive_skb+0x2a/0x170 net/core/dev.c:4251
     [<ffffffff82bdb493>] netif_receive_skb_internal+0x1b3/0x390 net/core/dev.c:4279
     [<ffffffff82bdb6b8>] netif_receive_skb+0x48/0x250 net/core/dev.c:4303
     [<ffffffff8241fc75>] tun_get_user+0xbd5/0x28a0 drivers/net/tun.c:1308
     [<ffffffff82421b5a>] tun_chr_write_iter+0xda/0x190 drivers/net/tun.c:1332
     [<     inline     >] new_sync_write fs/read_write.c:499
     [<ffffffff8151bd44>] __vfs_write+0x334/0x570 fs/read_write.c:512
     [<ffffffff8151f85b>] vfs_write+0x17b/0x500 fs/read_write.c:560
     [<     inline     >] SYSC_write fs/read_write.c:607
     [<ffffffff81523184>] SyS_write+0xd4/0x1a0 fs/read_write.c:599
     [<ffffffff83fc02c1>] entry_SYSCALL_64_fastpath+0x1f/0xc2
    
    It turns out DCCP calls __sk_receive_skb(), and this broke when
    lookups no longer took a reference on listeners.
    
    Fix this issue by adding a @refcounted parameter to __sk_receive_skb(),
    so that sock_put() is used only when needed.
    
    Fixes: 3b24d854cb35 ("tcp/dccp: do not touch listener sk_refcnt under synflood")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Andrey Konovalov <andreyknvl@google.com>
    Tested-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 73c6b008f1b7..92b269709b9a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1596,11 +1596,11 @@ static inline void sock_put(struct sock *sk)
 void sock_gen_put(struct sock *sk);
 
 int __sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested,
-		     unsigned int trim_cap);
+		     unsigned int trim_cap, bool refcounted);
 static inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
 				 const int nested)
 {
-	return __sk_receive_skb(sk, skb, nested, 1);
+	return __sk_receive_skb(sk, skb, nested, 1, true);
 }
 
 static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)

commit bd68a2a854ad5a85f0c8d0a9c8048ca3f6391efb
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Oct 31 13:32:55 2016 -0700

    net: set SK_MEM_QUANTUM to 4096
    
    Systems with large pages (64KB pages for example) do not always have
    huge quantity of memory.
    
    A big SK_MEM_QUANTUM value leads to fewer interactions with the
    global counters (like tcp_memory_allocated) but might trigger
    memory pressure much faster, giving suboptimal TCP performance
    since windows are lowered to ridiculous values.
    
    Note that sysctl_mem units being in pages and in ABI, we also need
    to change sk_prot_mem_limits() accordingly.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f13ac87a8015..93331a1492db 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1162,11 +1162,6 @@ static inline void sk_enter_memory_pressure(struct sock *sk)
 	sk->sk_prot->enter_memory_pressure(sk);
 }
 
-static inline long sk_prot_mem_limits(const struct sock *sk, int index)
-{
-	return sk->sk_prot->sysctl_mem[index];
-}
-
 static inline long
 sk_memory_allocated(const struct sock *sk)
 {
@@ -1281,11 +1276,27 @@ int __sk_mem_schedule(struct sock *sk, int size, int kind);
 void __sk_mem_reduce_allocated(struct sock *sk, int amount);
 void __sk_mem_reclaim(struct sock *sk, int amount);
 
-#define SK_MEM_QUANTUM ((int)PAGE_SIZE)
+/* We used to have PAGE_SIZE here, but systems with 64KB pages
+ * do not necessarily have 16x time more memory than 4KB ones.
+ */
+#define SK_MEM_QUANTUM 4096
 #define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
 #define SK_MEM_SEND	0
 #define SK_MEM_RECV	1
 
+/* sysctl_mem values are in pages, we convert them in SK_MEM_QUANTUM units */
+static inline long sk_prot_mem_limits(const struct sock *sk, int index)
+{
+	long val = sk->sk_prot->sysctl_mem[index];
+
+#if PAGE_SIZE > SK_MEM_QUANTUM
+	val <<= PAGE_SHIFT - SK_MEM_QUANTUM_SHIFT;
+#elif PAGE_SIZE < SK_MEM_QUANTUM
+	val >>= SK_MEM_QUANTUM_SHIFT - PAGE_SHIFT;
+#endif
+	return val;
+}
+
 static inline int sk_mem_pages(int amt)
 {
 	return (amt + SK_MEM_QUANTUM - 1) >> SK_MEM_QUANTUM_SHIFT;

commit 27058af401e49d88a905df000dd26f443fcfa8ce
Merge: 357f4aae859b 2a26d99b251b
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Oct 30 12:42:58 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Mostly simple overlapping changes.
    
    For example, David Ahern's adjacency list revamp in 'net-next'
    conflicted with an adjacency list traversal bug fix in 'net'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 293de7dee4f9602676846dbeb84b1580123306b4
Author: Stephen Hemminger <sthemmin@microsoft.com>
Date:   Sun Oct 23 09:28:29 2016 -0700

    doc: update docbook annotations for socket and skb
    
    The skbuff and sock structure both had missing parameter annotation
    values.
    
    Signed-off-by: Stephen Hemminger <sthemmin@microsoft.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ebf75db08e06..73c6b008f1b7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -252,6 +252,7 @@ struct sock_common {
   *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
   *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
   *	@sk_sndbuf: size of send buffer in bytes
+  *	@sk_padding: unused element for alignment
   *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
   *	@sk_no_check_rx: allow zero checksum in RX packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
@@ -302,7 +303,8 @@ struct sock_common {
   *	@sk_backlog_rcv: callback to process the backlog
   *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
   *	@sk_reuseport_cb: reuseport group container
- */
+  *	@sk_rcu: used during RCU grace period
+  */
 struct sock {
 	/*
 	 * Now struct inet_timewait_sock also uses sock_common, so please just

commit f8c3bf00d440df2bc2c3f669d460868d9ba67226
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Fri Oct 21 13:55:45 2016 +0200

    net/socket: factor out helpers for memory and queue manipulation
    
    Basic sock operations that udp code can use with its own
    memory accounting schema. No functional change is introduced
    in the existing APIs.
    
    v4 -> v5:
      - avoid whitespace changes
    
    v2 -> v4:
      - avoid exporting __sock_enqueue_skb
    
    v1 -> v2:
      - avoid export sock_rmem_free
    
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ebf75db08e06..276489553338 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1274,7 +1274,9 @@ static inline struct inode *SOCK_INODE(struct socket *socket)
 /*
  * Functions for memory accounting
  */
+int __sk_mem_raise_allocated(struct sock *sk, int size, int amt, int kind);
 int __sk_mem_schedule(struct sock *sk, int size, int kind);
+void __sk_mem_reduce_allocated(struct sock *sk, int amount);
 void __sk_mem_reclaim(struct sock *sk, int amount);
 
 #define SK_MEM_QUANTUM ((int)PAGE_SIZE)
@@ -1950,6 +1952,8 @@ void sk_reset_timer(struct sock *sk, struct timer_list *timer,
 
 void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
+int __sk_queue_drop_skb(struct sock *sk, struct sk_buff *skb,
+			unsigned int flags);
 int __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 

commit d6989d4bbe6c4d1c2a76696833a07f044e85694d
Merge: 0364a8824c02 b1f2beb87bb0
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 23 06:46:57 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 20c64d5cd5a2bdcdc8982a06cb05e5e1bd851a3d
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Sep 15 08:48:46 2016 -0700

    net: avoid sk_forward_alloc overflows
    
    A malicious TCP receiver, sending SACK, can force the sender to split
    skbs in write queue and increase its memory usage.
    
    Then, when socket is closed and its write queue purged, we might
    overflow sk_forward_alloc (It becomes negative)
    
    sk_mem_reclaim() does nothing in this case, and more than 2GB
    are leaked from TCP perspective (tcp_memory_allocated is not changed)
    
    Then warnings trigger from inet_sock_destruct() and
    sk_stream_kill_queues() seeing a not zero sk_forward_alloc
    
    All TCP stack can be stuck because TCP is under memory pressure.
    
    A simple fix is to preemptively reclaim from sk_mem_uncharge().
    
    This makes sure a socket wont have more than 2 MB forward allocated,
    after burst and idle period.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ff5be7e8ddea..8741988e6880 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1332,6 +1332,16 @@ static inline void sk_mem_uncharge(struct sock *sk, int size)
 	if (!sk_has_account(sk))
 		return;
 	sk->sk_forward_alloc += size;
+
+	/* Avoid a possible overflow.
+	 * TCP send queues can make this happen, if sk_mem_reclaim()
+	 * is not called and more than 2 GBytes are released at once.
+	 *
+	 * If we reach 2 MBytes, reclaim 1 MBytes right now, there is
+	 * no need to hold that much forward allocation anyway.
+	 */
+	if (unlikely(sk->sk_forward_alloc >= 1 << 21))
+		__sk_mem_reclaim(sk, 1 << 20);
 }
 
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)

commit ba2489b0e0113f68a25fe7a563842c2b591829d7
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Aug 23 11:39:29 2016 -0700

    net: remove clear_sk() method
    
    We no longer use this handler, we can delete it.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1bc57609f8e1..c797c57f4d9f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1020,7 +1020,6 @@ struct proto {
 	void			(*unhash)(struct sock *sk);
 	void			(*rehash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
-	void			(*clear_sk)(struct sock *sk, int size);
 
 	/* Keeping track of sockets in use */
 #ifdef CONFIG_PROC_FS

commit 4cac8204661a6d1a842e47911933f1e90b392c84
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Aug 23 11:39:27 2016 -0700

    udp: get rid of sk_prot_clear_portaddr_nulls()
    
    Since we no longer use SLAB_DESTROY_BY_RCU for UDP,
    we do not need sk_prot_clear_portaddr_nulls() helper.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2aab9b63bf16..1bc57609f8e1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1242,8 +1242,6 @@ static inline int __sk_prot_rehash(struct sock *sk)
 	return sk->sk_prot->hash(sk);
 }
 
-void sk_prot_clear_portaddr_nulls(struct sock *sk, int size);
-
 /* About 10 seconds */
 #define SOCK_DESTROY_TIME (10*HZ)
 

commit 54fd9c2dff144ed287ab3b8189dcdcd4d298d0cc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Aug 18 01:00:41 2016 +0200

    bpf: get rid of cgroup helper related ifdefs
    
    As recently discussed during the task_under_cgroup_hierarchy() addition,
    we should get rid of the ifdefs surrounding the bpf_skb_under_cgroup()
    helper. If related functionality is not built-in, the helper cannot be
    used anyway, which is also in line with what we do for all other helpers.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ff5be7e8ddea..2aab9b63bf16 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1114,6 +1114,16 @@ static inline bool sk_stream_is_writeable(const struct sock *sk)
 	       sk_stream_memory_free(sk);
 }
 
+static inline int sk_under_cgroup_hierarchy(struct sock *sk,
+					    struct cgroup *ancestor)
+{
+#ifdef CONFIG_SOCK_CGROUP_DATA
+	return cgroup_is_descendant(sock_cgroup_ptr(&sk->sk_cgrp_data),
+				    ancestor);
+#else
+	return -ENOTSUPP;
+#endif
+}
 
 static inline bool sk_has_memory_pressure(const struct sock *sk)
 {

commit 4f0c40d94461cfd23893a17335b2ab78ecb333c8
Author: Willem de Bruijn <willemb@google.com>
Date:   Tue Jul 12 18:18:57 2016 -0400

    dccp: limit sk_filter trim to payload
    
    Dccp verifies packet integrity, including length, at initial rcv in
    dccp_invalid_packet, later pulls headers in dccp_enqueue_skb.
    
    A call to sk_filter in-between can cause __skb_pull to wrap skb->len.
    skb_copy_datagram_msg interprets this as a negative value, so
    (correctly) fails with EFAULT. The negative length is reported in
    ioctl SIOCINQ or possibly in a DCCP_WARN in dccp_close.
    
    Introduce an sk_receive_skb variant that caps how small a filter
    program can trim packets, and call this in dccp with the header
    length. Excessively trimmed packets are now processed normally and
    queued for reception as 0B payloads.
    
    Fixes: 7c657876b63c ("[DCCP]: Initial implementation")
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 649d2a8c17fc..ff5be7e8ddea 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1576,7 +1576,13 @@ static inline void sock_put(struct sock *sk)
  */
 void sock_gen_put(struct sock *sk);
 
-int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested);
+int __sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested,
+		     unsigned int trim_cap);
+static inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
+				 const int nested)
+{
+	return __sk_receive_skb(sk, skb, nested, 1);
+}
 
 static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
 {

commit fc64869c48494a401b1fb627c9ecc4e6c1d74b0d
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Wed May 18 19:19:27 2016 +0300

    net: sock: move ->sk_shutdown out of bitfields.
    
    ->sk_shutdown bits share one bitfield with some other bits in sock struct,
    such as ->sk_no_check_[r,t]x, ->sk_userlocks ...
    sock_setsockopt() may write to these bits, while holding the socket lock.
    
    In case of AF_UNIX sockets, we change ->sk_shutdown bits while holding only
    unix_state_lock(). So concurrent setsockopt() and shutdown() may lead
    to corrupting these bits.
    
    Fix this by moving ->sk_shutdown bits out of bitfield into a separate byte.
    This will not change the 'struct sock' size since ->sk_shutdown moved into
    previously unused 16-bit hole.
    
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Suggested-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c9c8b19df27c..649d2a8c17fc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -382,8 +382,13 @@ struct sock {
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;
 	struct sk_buff_head	sk_write_queue;
+
+	/*
+	 * Because of non atomicity rules, all
+	 * changes are protected by socket lock.
+	 */
 	kmemcheck_bitfield_begin(flags);
-	unsigned int		sk_shutdown  : 2,
+	unsigned int		sk_padding : 2,
 				sk_no_check_tx : 1,
 				sk_no_check_rx : 1,
 				sk_userlocks : 4,
@@ -391,6 +396,7 @@ struct sock {
 				sk_type      : 16;
 #define SK_PROTOCOL_MAX U8_MAX
 	kmemcheck_bitfield_end(flags);
+
 	int			sk_wmem_queued;
 	gfp_t			sk_allocation;
 	u32			sk_pacing_rate; /* bytes per second */
@@ -418,6 +424,7 @@ struct sock {
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;
 	u16			sk_tsflags;
+	u8			sk_shutdown;
 	u32			sk_tskey;
 	struct socket		*sk_socket;
 	void			*sk_user_data;

commit 46cc6e4976e3d9058490f20d93bc7805f7f2d81e
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 3 16:56:03 2016 -0700

    tcp: fix lockdep splat in tcp_snd_una_update()
    
    tcp_snd_una_update() and tcp_rcv_nxt_update() call
    u64_stats_update_begin() either from process context or BH handler.
    
    This triggers a lockdep splat on 32bit & SMP builds.
    
    We could add u64_stats_update_begin_bh() variant but this would
    slow down 32bit builds with useless local_disable_bh() and
    local_enable_bh() pairs, since we own the socket lock at this point.
    
    I add sock_owned_by_me() helper to have proper lockdep support
    even on 64bit builds, and new u64_stats_update_begin_raw()
    and u64_stats_update_end_raw methods.
    
    Fixes: c10d9310edf5 ("tcp: do not assume TCP code is non preemptible")
    Reported-by: Fabio Estevam <festevam@gmail.com>
    Diagnosed-by: Francois Romieu <romieu@fr.zoreil.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Fabio Estevam <fabio.estevam@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 45f5b492c658..c9c8b19df27c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1421,11 +1421,16 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
  * accesses from user process context.
  */
 
-static inline bool sock_owned_by_user(const struct sock *sk)
+static inline void sock_owned_by_me(const struct sock *sk)
 {
 #ifdef CONFIG_LOCKDEP
 	WARN_ON_ONCE(!lockdep_sock_is_held(sk) && debug_locks);
 #endif
+}
+
+static inline bool sock_owned_by_user(const struct sock *sk)
+{
+	sock_owned_by_me(sk);
 	return sk->sk_lock.owned;
 }
 

commit 1d2077ac0165c0d173a2255e37cf4dc5033d92c7
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon May 2 10:56:27 2016 -0700

    net: add __sock_wfree() helper
    
    Hosts sending lot of ACK packets exhibit high sock_wfree() cost
    because of cache line miss to test SOCK_USE_WRITE_QUEUE
    
    We could move this flag close to sk_wmem_alloc but it is better
    to perform the atomic_sub_and_test() on a clean cache line,
    as it avoid one extra bus transaction.
    
    skb_orphan_partial() can also have a fast track for packets that either
    are TCP acks, or already went through another skb_orphan_partial()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1dbb1f9f7c1b..45f5b492c658 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1445,6 +1445,7 @@ struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
 
 struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
 			     gfp_t priority);
+void __sock_wfree(struct sk_buff *skb);
 void sock_wfree(struct sk_buff *skb);
 void skb_orphan_partial(struct sk_buff *skb);
 void sock_rfree(struct sk_buff *skb);

commit d41a69f1d390fa3f2546498103cdcd78b30676ff
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 29 14:16:53 2016 -0700

    tcp: make tcp_sendmsg() aware of socket backlog
    
    Large sendmsg()/write() hold socket lock for the duration of the call,
    unless sk->sk_sndbuf limit is hit. This is bad because incoming packets
    are parked into socket backlog for a long time.
    Critical decisions like fast retransmit might be delayed.
    Receivers have to maintain a big out of order queue with additional cpu
    overhead, and also possible stalls in TX once windows are full.
    
    Bidirectional flows are particularly hurt since the backlog can become
    quite big if the copy from user space triggers IO (page faults)
    
    Some applications learnt to use sendmsg() (or sendmmsg()) with small
    chunks to avoid this issue.
    
    Kernel should know better, right ?
    
    Add a generic sk_flush_backlog() helper and use it right
    before a new skb is allocated. Typically we put 64KB of payload
    per skb (unless MSG_EOR is requested) and checking socket backlog
    every 64KB gives good results.
    
    As a matter of fact, tests with TSO/GSO disabled give very nice
    results, as we manage to keep a small write queue and smaller
    perceived rtt.
    
    Note that sk_flush_backlog() maintains socket ownership,
    so is not equivalent to a {release_sock(sk); lock_sock(sk);},
    to ensure implicit atomicity rules that sendmsg() was
    giving to (possibly buggy) applications.
    
    In this simple implementation, I chose to not call tcp_release_cb(),
    but we might consider this later.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3df778ccaa82..1dbb1f9f7c1b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -926,6 +926,17 @@ void sk_stream_kill_queues(struct sock *sk);
 void sk_set_memalloc(struct sock *sk);
 void sk_clear_memalloc(struct sock *sk);
 
+void __sk_flush_backlog(struct sock *sk);
+
+static inline bool sk_flush_backlog(struct sock *sk)
+{
+	if (unlikely(READ_ONCE(sk->sk_backlog.tail))) {
+		__sk_flush_backlog(sk);
+		return true;
+	}
+	return false;
+}
+
 int sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb);
 
 struct request_sock_ops;

commit 4be735225f7cd040ca81c18740e7b672021bafeb
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 25 10:39:34 2016 -0700

    net: SOCKWQ_ASYNC_WAITDATA optimizations
    
    SOCKWQ_ASYNC_WAITDATA is set/cleared in sk_wait_data()
    and equivalent functions, so that sock_wake_async() can send
    a SIGIO only when necessary.
    
    Since these atomic operations are really not needed unless
    socket expressed interest in FASYNC, we can omit them in most
    cases.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0f48aad9f8e8..3df778ccaa82 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1940,7 +1940,8 @@ static inline unsigned long sock_wspace(struct sock *sk)
  */
 static inline void sk_set_bit(int nr, struct sock *sk)
 {
-	if (nr == SOCKWQ_ASYNC_NOSPACE && !sock_flag(sk, SOCK_FASYNC))
+	if ((nr == SOCKWQ_ASYNC_NOSPACE || nr == SOCKWQ_ASYNC_WAITDATA) &&
+	    !sock_flag(sk, SOCK_FASYNC))
 		return;
 
 	set_bit(nr, &sk->sk_wq_raw->flags);
@@ -1948,7 +1949,8 @@ static inline void sk_set_bit(int nr, struct sock *sk)
 
 static inline void sk_clear_bit(int nr, struct sock *sk)
 {
-	if (nr == SOCKWQ_ASYNC_NOSPACE && !sock_flag(sk, SOCK_FASYNC))
+	if ((nr == SOCKWQ_ASYNC_NOSPACE || nr == SOCKWQ_ASYNC_WAITDATA) &&
+	    !sock_flag(sk, SOCK_FASYNC))
 		return;
 
 	clear_bit(nr, &sk->sk_wq_raw->flags);

commit 9317bb69824ec8d078b0b786b6971aedb0af3d4f
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 25 10:39:32 2016 -0700

    net: SOCKWQ_ASYNC_NOSPACE optimizations
    
    SOCKWQ_ASYNC_NOSPACE is tested in sock_wake_async()
    so that a SIGIO signal is sent when needed.
    
    tcp_sendmsg() clears the bit.
    tcp_poll() sets the bit when stream is not writeable.
    
    We can avoid two atomic operations by first checking if socket
    is actually interested in the FASYNC business (most sockets in
    real applications do not use AIO, but select()/poll()/epoll())
    
    This also removes one cache line miss to access sk->sk_wq->flags
    in tcp_sendmsg()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d63b8494124e..0f48aad9f8e8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1940,11 +1940,17 @@ static inline unsigned long sock_wspace(struct sock *sk)
  */
 static inline void sk_set_bit(int nr, struct sock *sk)
 {
+	if (nr == SOCKWQ_ASYNC_NOSPACE && !sock_flag(sk, SOCK_FASYNC))
+		return;
+
 	set_bit(nr, &sk->sk_wq_raw->flags);
 }
 
 static inline void sk_clear_bit(int nr, struct sock *sk)
 {
+	if (nr == SOCKWQ_ASYNC_NOSPACE && !sock_flag(sk, SOCK_FASYNC))
+		return;
+
 	clear_bit(nr, &sk->sk_wq_raw->flags);
 }
 

commit d296ba60d8e2de23a350796a567a3aa90fe1cb6e
Author: Craig Gallek <kraig@google.com>
Date:   Mon Apr 25 10:42:12 2016 -0400

    soreuseport: Resolve merge conflict for v4/v6 ordering fix
    
    d894ba18d4e4 ("soreuseport: fix ordering for mixed v4/v6 sockets")
    was merged as a bug fix to the net tree.  Two conflicting changes
    were committed to net-next before the above fix was merged back to
    net-next:
    ca065d0cf80f ("udp: no longer use SLAB_DESTROY_BY_RCU")
    3b24d854cb35 ("tcp/dccp: do not touch listener sk_refcnt under synflood")
    
    These changes switched the datastructure used for TCP and UDP sockets
    from hlist_nulls to hlist.  This patch applies the necessary parts
    of the net tree fix to net-next which were not automatic as part of the
    merge.
    
    Fixes: 1602f49b58ab ("Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net")
    Signed-off-by: Craig Gallek <kraig@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2fdb87f176cf..d63b8494124e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -630,7 +630,11 @@ static inline void sk_add_node(struct sock *sk, struct hlist_head *list)
 static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
 {
 	sock_hold(sk);
-	hlist_add_head_rcu(&sk->sk_node, list);
+	if (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&
+	    sk->sk_family == AF_INET6)
+		hlist_add_tail_rcu(&sk->sk_node, list);
+	else
+		hlist_add_head_rcu(&sk->sk_node, list);
 }
 
 static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)

commit 5e91f6ce4c584d231763437a3ea3aded8e672363
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Apr 25 06:34:09 2016 -0700

    sock: relax WARN_ON() in sock_owned_by_user()
    
    Valdis reported tons of stack dumps caused by WARN_ON() in
    sock_owned_by_user()
    
    This test needs to be relaxed if/when lockdep disables itself.
    
    Note that other lockdep_sock_is_held() callers are all from
    rcu_dereference_protected() sections which already are disabled
    if/when lockdep has been disabled.
    
    Fixes: fafc4e1ea1a4 ("sock: tigthen lockdep checks for sock_owned_by_user")
    Reported-by: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 52448baf19d7..2fdb87f176cf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1409,7 +1409,7 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 static inline bool sock_owned_by_user(const struct sock *sk)
 {
 #ifdef CONFIG_LOCKDEP
-	WARN_ON(!lockdep_sock_is_held(sk));
+	WARN_ON_ONCE(!lockdep_sock_is_held(sk) && debug_locks);
 #endif
 	return sk->sk_lock.owned;
 }

commit 1602f49b58abcb0d34a5f0a29d68e7c1769547aa
Merge: 22d37b6b0058 5f44abd041c5
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Apr 23 18:26:24 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts were two cases of simple overlapping changes,
    nothing serious.
    
    In the UDP case, we need to add a hlist_add_tail_rcu()
    to linux/rculist.h, because we've moved UDP socket handling
    away from using nulls lists.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d894ba18d4e449b3a7f6eb491f16c9e02933736e
Author: Craig Gallek <kraig@google.com>
Date:   Tue Apr 12 13:11:25 2016 -0400

    soreuseport: fix ordering for mixed v4/v6 sockets
    
    With the SO_REUSEPORT socket option, it is possible to create sockets
    in the AF_INET and AF_INET6 domains which are bound to the same IPv4 address.
    This is only possible with SO_REUSEPORT and when not using IPV6_V6ONLY on
    the AF_INET6 sockets.
    
    Prior to the commits referenced below, an incoming IPv4 packet would
    always be routed to a socket of type AF_INET when this mixed-mode was used.
    After those changes, the same packet would be routed to the most recently
    bound socket (if this happened to be an AF_INET6 socket, it would
    have an IPv4 mapped IPv6 address).
    
    The change in behavior occurred because the recent SO_REUSEPORT optimizations
    short-circuit the socket scoring logic as soon as they find a match.  They
    did not take into account the scoring logic that favors AF_INET sockets
    over AF_INET6 sockets in the event of a tie.
    
    To fix this problem, this patch changes the insertion order of AF_INET
    and AF_INET6 addresses in the TCP and UDP socket lists when the sockets
    have SO_REUSEPORT set.  AF_INET sockets will be inserted at the head of the
    list and AF_INET6 sockets with SO_REUSEPORT set will always be inserted at
    the tail of the list.  This will force AF_INET sockets to always be
    considered first.
    
    Fixes: e32ea7e74727 ("soreuseport: fast reuseport UDP socket selection")
    Fixes: 125e80b88687 ("soreuseport: fast reuseport TCP socket selection")
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Signed-off-by: Craig Gallek <kraig@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 255d3e03727b..121ffc115c4f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -630,7 +630,11 @@ static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
 
 static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
-	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
+	if (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&
+	    sk->sk_family == AF_INET6)
+		hlist_nulls_add_tail_rcu(&sk->sk_nulls_node, list);
+	else
+		hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
 }
 
 static inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)

commit f9a7cbbf18f1640907d6ca345b8337e4b50ea56f
Author: Denys Vlasenko <dvlasenk@redhat.com>
Date:   Fri Apr 8 17:51:54 2016 +0200

    net: force inlining of netif_tx_start/stop_queue, sock_hold, __sock_put
    
    Sometimes gcc mysteriously doesn't inline
    very small functions we expect to be inlined. See
        https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66122
    Arguably, gcc should do better, but gcc people aren't willing
    to invest time into it, asking to use __always_inline instead.
    
    With this .config:
    http://busybox.net/~vda/kernel_config_OPTIMIZE_INLINING_and_Os,
    the following functions get deinlined many times.
    
    netif_tx_stop_queue: 207 copies, 590 calls:
            55                      push   %rbp
            48 89 e5                mov    %rsp,%rbp
            f0 80 8f e0 01 00 00 01 lock orb $0x1,0x1e0(%rdi)
            5d                      pop    %rbp
            c3                      retq
    
    netif_tx_start_queue: 47 copies, 111 calls
            55                      push   %rbp
            48 89 e5                mov    %rsp,%rbp
            f0 80 a7 e0 01 00 00 fe lock andb $0xfe,0x1e0(%rdi)
            5d                      pop    %rbp
            c3                      retq
    
    sock_hold: 39 copies, 124 calls
            55                      push   %rbp
            48 89 e5                mov    %rsp,%rbp
            f0 ff 87 80 00 00 00    lock incl 0x80(%rdi)
            5d                      pop    %rbp
            c3                      retq
    
    __sock_put: 6 copies, 13 calls
            55                      push   %rbp
            48 89 e5                mov    %rsp,%rbp
            f0 ff 8f 80 00 00 00    lock decl 0x80(%rdi)
            5d                      pop    %rbp
            c3                      retq
    
    This patch fixes this via s/inline/__always_inline/.
    
    Code size decrease after the patch is ~2.5k:
    
        text      data      bss       dec     hex filename
    56719876  56364551 36196352 149280779 8e5d80b vmlinux_before
    56717440  56364551 36196352 149278343 8e5ce87 vmlinux
    
    Signed-off-by: Denys Vlasenko <dvlasenk@redhat.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: linux-kernel@vger.kernel.org
    CC: netdev@vger.kernel.org
    CC: netfilter-devel@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index baba58770ac5..d997ec13a643 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -569,7 +569,7 @@ static inline bool __sk_del_node_init(struct sock *sk)
    modifications.
  */
 
-static inline void sock_hold(struct sock *sk)
+static __always_inline void sock_hold(struct sock *sk)
 {
 	atomic_inc(&sk->sk_refcnt);
 }
@@ -577,7 +577,7 @@ static inline void sock_hold(struct sock *sk)
 /* Ungrab socket in the context, which assumes that socket refcnt
    cannot hit zero, f.e. it is true in context of any socketcall.
  */
-static inline void __sock_put(struct sock *sk)
+static __always_inline void __sock_put(struct sock *sk)
 {
 	atomic_dec(&sk->sk_refcnt);
 }

commit fafc4e1ea1a4c1eb13a30c9426fb799f5efacbc3
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Fri Apr 8 15:11:27 2016 +0200

    sock: tigthen lockdep checks for sock_owned_by_user
    
    sock_owned_by_user should not be used without socket lock held. It seems
    to be a common practice to check .owned before lock reclassification, so
    provide a little help to abstract this check away.
    
    Cc: linux-cifs@vger.kernel.org
    Cc: linux-bluetooth@vger.kernel.org
    Cc: linux-nfs@vger.kernel.org
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 81d6fecec0a2..baba58770ac5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1316,21 +1316,6 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 	__kfree_skb(skb);
 }
 
-/* Used by processes to "lock" a socket state, so that
- * interrupts and bottom half handlers won't change it
- * from under us. It essentially blocks any incoming
- * packets, so that we won't get any new data or any
- * packets that change the state of the socket.
- *
- * While locked, BH processing will add new packets to
- * the backlog queue.  This queue is processed by the
- * owner of the socket lock right before it is released.
- *
- * Since ~2.3.5 it is also exclusive sleep lock serializing
- * accesses from user process context.
- */
-#define sock_owned_by_user(sk)	((sk)->sk_lock.owned)
-
 static inline void sock_release_ownership(struct sock *sk)
 {
 	if (sk->sk_lock.owned) {
@@ -1403,6 +1388,35 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 		spin_unlock_bh(&sk->sk_lock.slock);
 }
 
+/* Used by processes to "lock" a socket state, so that
+ * interrupts and bottom half handlers won't change it
+ * from under us. It essentially blocks any incoming
+ * packets, so that we won't get any new data or any
+ * packets that change the state of the socket.
+ *
+ * While locked, BH processing will add new packets to
+ * the backlog queue.  This queue is processed by the
+ * owner of the socket lock right before it is released.
+ *
+ * Since ~2.3.5 it is also exclusive sleep lock serializing
+ * accesses from user process context.
+ */
+
+static inline bool sock_owned_by_user(const struct sock *sk)
+{
+#ifdef CONFIG_LOCKDEP
+	WARN_ON(!lockdep_sock_is_held(sk));
+#endif
+	return sk->sk_lock.owned;
+}
+
+/* no reclassification while locks are held */
+static inline bool sock_allow_reclassification(const struct sock *csk)
+{
+	struct sock *sk = (struct sock *)csk;
+
+	return !sk->sk_lock.owned && !spin_is_locked(&sk->sk_lock.slock);
+}
 
 struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
 		      struct proto *prot, int kern);

commit b33b0a1bf69faff89693df49519fa7b459f5d807
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Apr 7 20:40:25 2016 -0400

    net: Fix build failure due to lockdep_sock_is_held().
    
    Needs to be protected with CONFIG_LOCKDEP.
    
    Based upon a patch by Hannes Frederic Sowa.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 46b29374df8e..81d6fecec0a2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1360,6 +1360,7 @@ do {									\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
+#ifdef CONFIG_LOCKDEP
 static inline bool lockdep_sock_is_held(const struct sock *csk)
 {
 	struct sock *sk = (struct sock *)csk;
@@ -1367,6 +1368,7 @@ static inline bool lockdep_sock_is_held(const struct sock *csk)
 	return lockdep_is_held(&sk->sk_lock) ||
 	       lockdep_is_held(&sk->sk_lock.slock);
 }
+#endif
 
 void lock_sock_nested(struct sock *sk, int subclass);
 

commit 03be98226c14d787939381b9f42d81764ea8eedc
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Thu Apr 7 23:53:35 2016 +0200

    sock: make lockdep_sock_is_held static inline
    
    I forgot to add inline to lockdep_sock_is_held, so it generated all
    kinds of build warnings if not build with lockdep support.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index eb2d7c3e120b..46b29374df8e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1360,7 +1360,7 @@ do {									\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
-static bool lockdep_sock_is_held(const struct sock *csk)
+static inline bool lockdep_sock_is_held(const struct sock *csk)
 {
 	struct sock *sk = (struct sock *)csk;
 

commit 1e1d04e678cf72442f57ce82803c7a407769135f
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Tue Apr 5 17:10:15 2016 +0200

    net: introduce lockdep_is_held and update various places to use it
    
    The socket is either locked if we hold the slock spin_lock for
    lock_sock_fast and unlock_sock_fast or we own the lock (sk_lock.owned
    != 0). Check for this and at the same time improve that the current
    thread/cpu is really holding the lock.
    
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91cee51086dc..eb2d7c3e120b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1360,6 +1360,14 @@ do {									\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
+static bool lockdep_sock_is_held(const struct sock *csk)
+{
+	struct sock *sk = (struct sock *)csk;
+
+	return lockdep_is_held(&sk->sk_lock) ||
+	       lockdep_is_held(&sk->sk_lock.slock);
+}
+
 void lock_sock_nested(struct sock *sk, int subclass);
 
 static inline void lock_sock(struct sock *sk)
@@ -1598,8 +1606,8 @@ static inline void sk_rethink_txhash(struct sock *sk)
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
-	return rcu_dereference_check(sk->sk_dst_cache, sock_owned_by_user(sk) ||
-						       lockdep_is_held(&sk->sk_lock.slock));
+	return rcu_dereference_check(sk->sk_dst_cache,
+				     lockdep_sock_is_held(sk));
 }
 
 static inline struct dst_entry *

commit 61881cfb5ad80c1d0a46ca6d08b7e271892b2ff6
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Tue Apr 5 17:10:14 2016 +0200

    sock: fix lockdep annotation in release_sock
    
    During release_sock we use callbacks to finish the processing
    of outstanding skbs on the socket. We actually are still locked,
    sk_locked.owned == 1, but we already told lockdep that the mutex
    is released. This could lead to false positives in lockdep for
    lockdep_sock_is_held (we don't hold the slock spinlock during processing
    the outstanding skbs).
    
    I took over this patch from Eric Dumazet and tested it.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1decb7a22261..91cee51086dc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1333,7 +1333,12 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 
 static inline void sock_release_ownership(struct sock *sk)
 {
-	sk->sk_lock.owned = 0;
+	if (sk->sk_lock.owned) {
+		sk->sk_lock.owned = 0;
+
+		/* The sk_lock has mutex_unlock() semantics: */
+		mutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);
+	}
 }
 
 /*

commit 627d2d6b550094d88f9e518e15967e7bf906ebbf
Author: samanthakumar <samanthakumar@google.com>
Date:   Tue Apr 5 12:41:16 2016 -0400

    udp: enable MSG_PEEK at non-zero offset
    
    Enable peeking at UDP datagrams at the offset specified with socket
    option SOL_SOCKET/SO_PEEK_OFF. Peek at any datagram in the queue, up
    to the end of the given datagram.
    
    Implement the SO_PEEK_OFF semantics introduced in commit ef64a54f6e55
    ("sock: Introduce the SO_PEEK_OFF sock option"). Increase the offset
    on peek, decrease it on regular reads.
    
    When peeking, always checksum the packet immediately, to avoid
    recomputation on subsequent peeks and final read.
    
    The socket lock is not held for the duration of udp_recvmsg, so
    peek and read operations can run concurrently. Only the last store
    to sk_peek_off is preserved.
    
    Signed-off-by: Sam Kumar <samanthakumar@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b75998952482..1decb7a22261 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -457,6 +457,8 @@ struct sock {
 #define SK_CAN_REUSE	1
 #define SK_FORCE_REUSE	2
 
+int sk_set_peek_off(struct sock *sk, int val);
+
 static inline int sk_peek_offset(struct sock *sk, int flags)
 {
 	if (unlikely(flags & MSG_PEEK)) {

commit e6afc8ace6dd5cef5e812f26c72579da8806f5ac
Author: samanthakumar <samanthakumar@google.com>
Date:   Tue Apr 5 12:41:15 2016 -0400

    udp: remove headers from UDP packets before queueing
    
    Remove UDP transport headers before queueing packets for reception.
    This change simplifies a follow-up patch to add MSG_PEEK support.
    
    Signed-off-by: Sam Kumar <samanthakumar@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 09aec75eb184..b75998952482 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1864,6 +1864,7 @@ void sk_reset_timer(struct sock *sk, struct timer_list *timer,
 
 void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
+int __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
 int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);

commit b9bb53f3836f4eb2bdeb3447be11042bd29c2408
Author: Willem de Bruijn <willemb@google.com>
Date:   Tue Apr 5 12:41:14 2016 -0400

    sock: convert sk_peek_offset functions to WRITE_ONCE
    
    Make the peek offset interface safe to use in lockless environments.
    Use READ_ONCE and WRITE_ONCE to avoid race conditions between testing
    and updating the peek offset.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 310c4367ea83..09aec75eb184 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -459,26 +459,28 @@ struct sock {
 
 static inline int sk_peek_offset(struct sock *sk, int flags)
 {
-	if ((flags & MSG_PEEK) && (sk->sk_peek_off >= 0))
-		return sk->sk_peek_off;
-	else
-		return 0;
+	if (unlikely(flags & MSG_PEEK)) {
+		s32 off = READ_ONCE(sk->sk_peek_off);
+		if (off >= 0)
+			return off;
+	}
+
+	return 0;
 }
 
 static inline void sk_peek_offset_bwd(struct sock *sk, int val)
 {
-	if (sk->sk_peek_off >= 0) {
-		if (sk->sk_peek_off >= val)
-			sk->sk_peek_off -= val;
-		else
-			sk->sk_peek_off = 0;
+	s32 off = READ_ONCE(sk->sk_peek_off);
+
+	if (unlikely(off >= 0)) {
+		off = max_t(s32, off - val, 0);
+		WRITE_ONCE(sk->sk_peek_off, off);
 	}
 }
 
 static inline void sk_peek_offset_fwd(struct sock *sk, int val)
 {
-	if (sk->sk_peek_off >= 0)
-		sk->sk_peek_off += val;
+	sk_peek_offset_bwd(sk, -val);
 }
 
 /*

commit 532182cd610782db8c18230c2747626562032205
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 1 08:52:19 2016 -0700

    tcp: increment sk_drops for dropped rx packets
    
    Now ss can report sk_drops, we can instruct TCP to increment
    this per socket counter when it drops an incoming frame, to refine
    monitoring and debugging.
    
    Following patch takes care of listeners drops.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7ad73db9dde2..310c4367ea83 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2012,6 +2012,13 @@ sock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)
 	SOCK_SKB_CB(skb)->dropcount = atomic_read(&sk->sk_drops);
 }
 
+static inline void sk_drops_add(struct sock *sk, const struct sk_buff *skb)
+{
+	int segs = max_t(u16, 1, skb_shinfo(skb)->gso_segs);
+
+	atomic_add(segs, &sk->sk_drops);
+}
+
 void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 			   struct sk_buff *skb);
 void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,

commit ca065d0cf80fa547724440a8bf37f1e674d917c0
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 1 08:52:13 2016 -0700

    udp: no longer use SLAB_DESTROY_BY_RCU
    
    Tom Herbert would like not touching UDP socket refcnt for encapsulated
    traffic. For this to happen, we need to use normal RCU rules, with a grace
    period before freeing a socket. UDP sockets are not short lived in the
    high usage case, so the added cost of call_rcu() should not be a concern.
    
    This actually removes a lot of complexity in UDP stack.
    
    Multicast receives no longer need to hold a bucket spinlock.
    
    Note that ip early demux still needs to take a reference on the socket.
    
    Same remark for functions used by xt_socket and xt_PROXY netfilter modules,
    but this might be changed later.
    
    Performance for a single UDP socket receiving flood traffic from
    many RX queues/cpus.
    
    Simple udp_rx using simple recvfrom() loop :
    438 kpps instead of 374 kpps : 17 % increase of the peak rate.
    
    v2: Addressed Willem de Bruijn feedback in multicast handling
     - keep early demux break in __udp4_lib_demux_lookup()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <tom@herbertland.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Tested-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9e77353a92ae..7ad73db9dde2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -178,7 +178,7 @@ struct sock_common {
 	int			skc_bound_dev_if;
 	union {
 		struct hlist_node	skc_bind_node;
-		struct hlist_nulls_node skc_portaddr_node;
+		struct hlist_node	skc_portaddr_node;
 	};
 	struct proto		*skc_prot;
 	possible_net_t		skc_net;
@@ -670,18 +670,18 @@ static inline void sk_add_bind_node(struct sock *sk,
 	hlist_for_each_entry(__sk, list, sk_bind_node)
 
 /**
- * sk_nulls_for_each_entry_offset - iterate over a list at a given struct offset
+ * sk_for_each_entry_offset_rcu - iterate over a list at a given struct offset
  * @tpos:	the type * to use as a loop cursor.
  * @pos:	the &struct hlist_node to use as a loop cursor.
  * @head:	the head for your list.
  * @offset:	offset of hlist_node within the struct.
  *
  */
-#define sk_nulls_for_each_entry_offset(tpos, pos, head, offset)		       \
-	for (pos = (head)->first;					       \
-	     (!is_a_nulls(pos)) &&					       \
+#define sk_for_each_entry_offset_rcu(tpos, pos, head, offset)		       \
+	for (pos = rcu_dereference((head)->first);			       \
+	     pos != NULL &&						       \
 		({ tpos = (typeof(*tpos) *)((void *)pos - offset); 1;});       \
-	     pos = pos->next)
+	     pos = rcu_dereference(pos->next))
 
 static inline struct user_namespace *sk_user_ns(struct sock *sk)
 {

commit a4298e4522d687a79af8f8fbb7eca68399ab2d81
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 1 08:52:12 2016 -0700

    net: add SOCK_RCU_FREE socket flag
    
    We want a generic way to insert an RCU grace period before socket
    freeing for cases where RCU_SLAB_DESTROY_BY_RCU is adding too
    much overhead.
    
    SLAB_DESTROY_BY_RCU strict rules force us to take a reference
    on the socket sk_refcnt, and it is a performance problem for UDP
    encapsulation, or TCP synflood behavior, as many CPUs might
    attempt the atomic operations on a shared sk_refcnt
    
    UDP sockets and TCP listeners can set SOCK_RCU_FREE so that their
    lookup can use traditional RCU rules, without refcount changes.
    They can set the flag only once hashed and visible by other cpus.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <tom@herbertland.com>
    Tested-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e91b87f54f99..9e77353a92ae 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -438,6 +438,7 @@ struct sock {
 						  struct sk_buff *skb);
 	void                    (*sk_destruct)(struct sock *sk);
 	struct sock_reuseport __rcu	*sk_reuseport_cb;
+	struct rcu_head		sk_rcu;
 };
 
 #define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))
@@ -720,6 +721,7 @@ enum sock_flags {
 		     */
 	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
+	SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
 };
 
 #define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))

commit c14ac9451c34832554db33386a4393be8bba3a7b
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Sat Apr 2 23:08:12 2016 -0400

    sock: enable timestamping using control messages
    
    Currently, SOL_TIMESTAMPING can only be enabled using setsockopt.
    This is very costly when users want to sample writes to gather
    tx timestamps.
    
    Add support for enabling SO_TIMESTAMPING via control messages by
    using tsflags added in `struct sockcm_cookie` (added in the previous
    patches in this series) to set the tx_flags of the last skb created in
    a sendmsg. With this patch, the timestamp recording bits in tx_flags
    of the skbuff is overridden if SO_TIMESTAMPING is passed in a cmsg.
    
    Please note that this is only effective for overriding the recording
    timestamps flags. Users should enable timestamp reporting (e.g.,
    SOF_TIMESTAMPING_SOFTWARE | SOF_TIMESTAMPING_OPT_ID) using
    socket options and then should ask for SOF_TIMESTAMPING_TX_*
    using control messages per sendmsg to sample timestamps for each
    write.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index af012da5e608..e91b87f54f99 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2057,19 +2057,21 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 		sk->sk_stamp = skb->tstamp;
 }
 
-void __sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags);
+void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);
 
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @sk:		socket sending this packet
+ * @tsflags:	timestamping flags to use
  * @tx_flags:	completed with instructions for time stamping
  *
  * Note : callers should take care of initial *tx_flags value (usually 0)
  */
-static inline void sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags)
+static inline void sock_tx_timestamp(const struct sock *sk, __u16 tsflags,
+				     __u8 *tx_flags)
 {
-	if (unlikely(sk->sk_tsflags))
-		__sock_tx_timestamp(sk, tx_flags);
+	if (unlikely(tsflags))
+		__sock_tx_timestamp(tsflags, tx_flags);
 	if (unlikely(sock_flag(sk, SOCK_WIFI_STATUS)))
 		*tx_flags |= SKBTX_WIFI_STATUS;
 }

commit 3dd17e63f5131bf2528f34aa5e3e57758175af92
Author: Soheil Hassas Yeganeh <soheil@google.com>
Date:   Sat Apr 2 23:08:09 2016 -0400

    sock: accept SO_TIMESTAMPING flags in socket cmsg
    
    Accept SO_TIMESTAMPING in control messages of the SOL_SOCKET level
    as a basis to accept timestamping requests per write.
    
    This implementation only accepts TX recording flags (i.e.,
    SOF_TIMESTAMPING_TX_HARDWARE, SOF_TIMESTAMPING_TX_SOFTWARE,
    SOF_TIMESTAMPING_TX_SCHED, and SOF_TIMESTAMPING_TX_ACK) in
    control messages. Users need to set reporting flags (e.g.,
    SOF_TIMESTAMPING_OPT_ID) per socket via socket options.
    
    This commit adds a tsflags field in sockcm_cookie which is
    set in __sock_cmsg_send. It only override the SOF_TIMESTAMPING_TX_*
    bits in sockcm_cookie.tsflags allowing the control message
    to override the recording behavior per write, yet maintaining
    the value of other flags.
    
    This patch implements validating the control message and setting
    tsflags in struct sockcm_cookie. Next commits in this series will
    actually implement timestamping per write for different protocols.
    
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 03772d4b06e6..af012da5e608 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1418,6 +1418,7 @@ void sk_send_sigurg(struct sock *sk);
 
 struct sockcm_cookie {
 	u32 mark;
+	u16 tsflags;
 };
 
 int __sock_cmsg_send(struct sock *sk, struct msghdr *msg, struct cmsghdr *cmsg,

commit 39771b127b412377d6354893c7d43ee8f2edecfd
Author: Willem de Bruijn <willemb@google.com>
Date:   Sat Apr 2 23:08:06 2016 -0400

    sock: break up sock_cmsg_snd into __sock_cmsg_snd and loop
    
    To process cmsg's of the SOL_SOCKET level in addition to
    cmsgs of another level, protocols can call sock_cmsg_send().
    This causes a double walk on the cmsghdr list, one for SOL_SOCKET
    and one for the other level.
    
    Extract the inner demultiplex logic from the loop that walks the list,
    to allow having this called directly from a walker in the protocol
    specific code.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Soheil Hassas Yeganeh <soheil@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 255d3e03727b..03772d4b06e6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1420,6 +1420,8 @@ struct sockcm_cookie {
 	u32 mark;
 };
 
+int __sock_cmsg_send(struct sock *sk, struct msghdr *msg, struct cmsghdr *cmsg,
+		     struct sockcm_cookie *sockc);
 int sock_cmsg_send(struct sock *sk, struct msghdr *msg,
 		   struct sockcm_cookie *sockc);
 

commit 086c653f5862591a9cfe2386f5650d03adacc33a
Author: Craig Gallek <kraig@google.com>
Date:   Wed Feb 10 11:50:35 2016 -0500

    sock: struct proto hash function may error
    
    In order to support fast reuseport lookups in TCP, the hash function
    defined in struct proto must be capable of returning an error code.
    This patch changes the function signature of all related hash functions
    to return an integer and handles or propagates this return value at
    all call sites.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f5ea148853e2..255d3e03727b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -984,7 +984,7 @@ struct proto {
 	void		(*release_cb)(struct sock *sk);
 
 	/* Keeping track of sk's, looking them up, and port selection methods. */
-	void			(*hash)(struct sock *sk);
+	int			(*hash)(struct sock *sk);
 	void			(*unhash)(struct sock *sk);
 	void			(*rehash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
@@ -1194,10 +1194,10 @@ static inline void sock_prot_inuse_add(struct net *net, struct proto *prot,
 /* With per-bucket locks this operation is not-atomic, so that
  * this version is not worse.
  */
-static inline void __sk_prot_rehash(struct sock *sk)
+static inline int __sk_prot_rehash(struct sock *sk)
 {
 	sk->sk_prot->unhash(sk);
-	sk->sk_prot->hash(sk);
+	return sk->sk_prot->hash(sk);
 }
 
 void sk_prot_clear_portaddr_nulls(struct sock *sk, int size);

commit 4877be9019baaf1432f9117bff4873e4ad518d91
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 21 15:56:28 2016 -0500

    net: sock: remove dead cgroup methods from struct proto
    
    The cgroup methods are no longer used after baac50bbc3cd ("net:
    tcp_memcontrol: simplify linkage between socket and page counter").
    The hunk to delete them was included in the original patch but must
    have gotten lost during conflict resolution on the way upstream.
    
    Fixes: baac50bbc3cd ("net: tcp_memcontrol: simplify linkage between socket and page counter")
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b9e7b3d863a0..f5ea148853e2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1035,18 +1035,6 @@ struct proto {
 	struct list_head	node;
 #ifdef SOCK_REFCNT_DEBUG
 	atomic_t		socks;
-#endif
-#ifdef CONFIG_MEMCG_KMEM
-	/*
-	 * cgroup specific init/deinit functions. Called once for all
-	 * protocols that implement it, from cgroups populate function.
-	 * This function has to setup any files the protocol want to
-	 * appear in the kmem cgroup filesystem.
-	 */
-	int			(*init_cgroup)(struct mem_cgroup *memcg,
-					       struct cgroup_subsys *ss);
-	void			(*destroy_cgroup)(struct mem_cgroup *memcg);
-	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
 #endif
 	int			(*diag_destroy)(struct sock *sk, int err);
 };

commit 80e95fe0fdcde2812c341ad4209d62dc1a7af53b
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:20 2016 -0800

    mm: memcontrol: generalize the socket accounting jump label
    
    The unified hierarchy memory controller is going to use this jump label
    as well to control the networking callbacks.  Move it to the memory
    controller code and give it a more generic name.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index be96a8dcbc74..b9e7b3d863a0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1079,13 +1079,6 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
-#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_NET)
-extern struct static_key memcg_socket_limit_enabled;
-#define mem_cgroup_sockets_enabled static_key_false(&memcg_socket_limit_enabled)
-#else
-#define mem_cgroup_sockets_enabled 0
-#endif
-
 static inline bool sk_stream_memory_free(const struct sock *sk)
 {
 	if (sk->sk_wmem_queued >= sk->sk_sndbuf)

commit baac50bbc3cdfd184ebf586b1704edbfcee866df
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:17 2016 -0800

    net: tcp_memcontrol: simplify linkage between socket and page counter
    
    There won't be any separate counters for socket memory consumed by
    protocols other than TCP in the future.  Remove the indirection and link
    sockets directly to their owning memory cgroup.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 94a6c1a740b9..be96a8dcbc74 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -71,22 +71,6 @@
 #include <net/tcp_states.h>
 #include <linux/net_tstamp.h>
 
-struct cgroup;
-struct cgroup_subsys;
-#ifdef CONFIG_NET
-int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss);
-void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg);
-#else
-static inline
-int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss)
-{
-	return 0;
-}
-static inline
-void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg)
-{
-}
-#endif
 /*
  * This structure really needs to be cleaned up.
  * Most of it is for TCP, and not used by any of
@@ -245,7 +229,6 @@ struct sock_common {
 	/* public: */
 };
 
-struct cg_proto;
 /**
   *	struct sock - network layer representation of sockets
   *	@__sk_common: shared layout with inet_timewait_sock
@@ -310,7 +293,7 @@ struct cg_proto;
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
   *	@sk_cgrp_data: cgroup data for this cgroup
-  *	@sk_cgrp: this socket's cgroup-specific proto data
+  *	@sk_memcg: this socket's memory cgroup association
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
   *	@sk_data_ready: callback to indicate there is data to be processed
@@ -446,7 +429,7 @@ struct sock {
 	void			*sk_security;
 #endif
 	struct sock_cgroup_data	sk_cgrp_data;
-	struct cg_proto		*sk_cgrp;
+	struct mem_cgroup	*sk_memcg;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk);
 	void			(*sk_write_space)(struct sock *sk);
@@ -1129,8 +1112,8 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 	if (!sk->sk_prot->memory_pressure)
 		return false;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
-	    mem_cgroup_under_socket_pressure(sk->sk_cgrp))
+	if (mem_cgroup_sockets_enabled && sk->sk_memcg &&
+	    mem_cgroup_under_socket_pressure(sk->sk_memcg))
 		return true;
 
 	return !!*sk->sk_prot->memory_pressure;

commit e805605c721021879a1469bdae45c6f80bc985f4
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:14 2016 -0800

    net: tcp_memcontrol: sanitize tcp memory accounting callbacks
    
    There won't be a tcp control soft limit, so integrating the memcg code
    into the global skmem limiting scheme complicates things unnecessarily.
    Replace this with simple and clear charge and uncharge calls--hidden
    behind a jump label--to account skb memory.
    
    Note that this is not purely aesthetic: as a result of shoehorning the
    per-memcg code into the same memory accounting functions that handle the
    global level, the old code would compare the per-memcg consumption
    against the smaller of the per-memcg limit and the global limit.  This
    allowed the total consumption of multiple sockets to exceed the global
    limit, as long as the individual sockets stayed within bounds.  After
    this change, the code will always compare the per-memcg consumption to
    the per-memcg limit, and the global consumption to the global limit, and
    thus close this loophole.
    
    Without a soft limit, the per-memcg memory pressure state in sockets is
    generally questionable.  However, we did it until now, so we continue to
    enter it when the hard limit is hit, and packets are dropped, to let
    other sockets in the cgroup know that they shouldn't grow their transmit
    windows, either.  However, keep it simple in the new callback model and
    leave memory pressure lazily when the next packet is accepted (as
    opposed to doing it synchroneously when packets are processed).  When
    packets are dropped, network performance will already be in the toilet,
    so that should be a reasonable trade-off.
    
    As described above, consumption is now checked on the per-memcg level
    and the global level separately.  Likewise, memory pressure states are
    maintained on both the per-memcg level and the global level, and a
    socket is considered under pressure when either level asserts as much.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8b1f8e5d3a48..94a6c1a740b9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1129,8 +1129,9 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 	if (!sk->sk_prot->memory_pressure)
 		return false;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return !!sk->sk_cgrp->memory_pressure;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
+	    mem_cgroup_under_socket_pressure(sk->sk_cgrp))
+		return true;
 
 	return !!*sk->sk_prot->memory_pressure;
 }
@@ -1144,9 +1145,6 @@ static inline void sk_leave_memory_pressure(struct sock *sk)
 
 	if (*memory_pressure)
 		*memory_pressure = 0;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		sk->sk_cgrp->memory_pressure = 0;
 }
 
 static inline void sk_enter_memory_pressure(struct sock *sk)
@@ -1154,76 +1152,30 @@ static inline void sk_enter_memory_pressure(struct sock *sk)
 	if (!sk->sk_prot->enter_memory_pressure)
 		return;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		sk->sk_cgrp->memory_pressure = 1;
-
 	sk->sk_prot->enter_memory_pressure(sk);
 }
 
 static inline long sk_prot_mem_limits(const struct sock *sk, int index)
 {
-	long limit = sk->sk_prot->sysctl_mem[index];
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		limit = min_t(long, limit, sk->sk_cgrp->memory_allocated.limit);
-
-	return limit;
-}
-
-static inline void memcg_memory_allocated_add(struct cg_proto *prot,
-					      unsigned long amt,
-					      int *parent_status)
-{
-	struct page_counter *counter;
-
-	if (page_counter_try_charge(&prot->memory_allocated, amt, &counter))
-		return;
-
-	page_counter_charge(&prot->memory_allocated, amt);
-	*parent_status = OVER_LIMIT;
-}
-
-static inline void memcg_memory_allocated_sub(struct cg_proto *prot,
-					      unsigned long amt)
-{
-	page_counter_uncharge(&prot->memory_allocated, amt);
+	return sk->sk_prot->sysctl_mem[index];
 }
 
 static inline long
 sk_memory_allocated(const struct sock *sk)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return page_counter_read(&sk->sk_cgrp->memory_allocated);
-
-	return atomic_long_read(prot->memory_allocated);
+	return atomic_long_read(sk->sk_prot->memory_allocated);
 }
 
 static inline long
-sk_memory_allocated_add(struct sock *sk, int amt, int *parent_status)
+sk_memory_allocated_add(struct sock *sk, int amt)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
-		memcg_memory_allocated_add(sk->sk_cgrp, amt, parent_status);
-		/* update the root cgroup regardless */
-		atomic_long_add_return(amt, prot->memory_allocated);
-		return page_counter_read(&sk->sk_cgrp->memory_allocated);
-	}
-
-	return atomic_long_add_return(amt, prot->memory_allocated);
+	return atomic_long_add_return(amt, sk->sk_prot->memory_allocated);
 }
 
 static inline void
 sk_memory_allocated_sub(struct sock *sk, int amt)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		memcg_memory_allocated_sub(sk->sk_cgrp, amt);
-
-	atomic_long_sub(amt, prot->memory_allocated);
+	atomic_long_sub(amt, sk->sk_prot->memory_allocated);
 }
 
 static inline void sk_sockets_allocated_dec(struct sock *sk)

commit 80f23124f57c77915a7b4201d8dcba38a38b23f0
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:11 2016 -0800

    net: tcp_memcontrol: simplify the per-memcg limit access
    
    tcp_memcontrol replicates the global sysctl_mem limit array per cgroup,
    but it only ever sets these entries to the value of the memory_allocated
    page_counter limit.  Use the latter directly.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1f15937ec208..8b1f8e5d3a48 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1162,10 +1162,12 @@ static inline void sk_enter_memory_pressure(struct sock *sk)
 
 static inline long sk_prot_mem_limits(const struct sock *sk, int index)
 {
-	long *prot = sk->sk_prot->sysctl_mem;
+	long limit = sk->sk_prot->sysctl_mem[index];
+
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		prot = sk->sk_cgrp->sysctl_mem;
-	return prot[index];
+		limit = min_t(long, limit, sk->sk_cgrp->memory_allocated.limit);
+
+	return limit;
 }
 
 static inline void memcg_memory_allocated_add(struct cg_proto *prot,

commit af95d7df4059cfeab7e7c244f3564214aada7dad
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:08 2016 -0800

    net: tcp_memcontrol: remove dead per-memcg count of allocated sockets
    
    The number of allocated sockets is used for calculations in the soft
    limit phase, where packets are accepted but the socket is under memory
    pressure.
     Since there is no soft limit phase in tcp_memcontrol, and memory
    pressure is only entered when packets are already dropped, this is
    actually dead code.  Remove it.
    
    As this is the last user of parent_cg_proto(), remove that too.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index d3b035c7362b..1f15937ec208 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1098,19 +1098,9 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 
 #if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_NET)
 extern struct static_key memcg_socket_limit_enabled;
-static inline struct cg_proto *parent_cg_proto(struct proto *proto,
-					       struct cg_proto *cg_proto)
-{
-	return proto->proto_cgroup(parent_mem_cgroup(cg_proto->memcg));
-}
 #define mem_cgroup_sockets_enabled static_key_false(&memcg_socket_limit_enabled)
 #else
 #define mem_cgroup_sockets_enabled 0
-static inline struct cg_proto *parent_cg_proto(struct proto *proto,
-					       struct cg_proto *cg_proto)
-{
-	return NULL;
-}
 #endif
 
 static inline bool sk_stream_memory_free(const struct sock *sk)
@@ -1236,41 +1226,18 @@ sk_memory_allocated_sub(struct sock *sk, int amt)
 
 static inline void sk_sockets_allocated_dec(struct sock *sk)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
-		struct cg_proto *cg_proto = sk->sk_cgrp;
-
-		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			percpu_counter_dec(&cg_proto->sockets_allocated);
-	}
-
-	percpu_counter_dec(prot->sockets_allocated);
+	percpu_counter_dec(sk->sk_prot->sockets_allocated);
 }
 
 static inline void sk_sockets_allocated_inc(struct sock *sk)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
-		struct cg_proto *cg_proto = sk->sk_cgrp;
-
-		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			percpu_counter_inc(&cg_proto->sockets_allocated);
-	}
-
-	percpu_counter_inc(prot->sockets_allocated);
+	percpu_counter_inc(sk->sk_prot->sockets_allocated);
 }
 
 static inline int
 sk_sockets_allocated_read_positive(struct sock *sk)
 {
-	struct proto *prot = sk->sk_prot;
-
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return percpu_counter_read_positive(&sk->sk_cgrp->sockets_allocated);
-
-	return percpu_counter_read_positive(prot->sockets_allocated);
+	return percpu_counter_read_positive(sk->sk_prot->sockets_allocated);
 }
 
 static inline int

commit 931f3f4beb031cd483c1c8ab159ef1f8bdbe8888
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:21:02 2016 -0800

    net: tcp_memcontrol: remove bogus hierarchy pressure propagation
    
    When a cgroup currently breaches its socket memory limit, it enters
    memory pressure mode for itself and its *ancestors*.  This throttles
    transmission in unrelated sibling and cousin subtrees that have nothing
    to do with the breached limit.
    
    On the contrary, breaching a limit should make that group and its
    *children* enter memory pressure mode.  But this happens already, albeit
    lazily: if an ancestor limit is breached, siblings will enter memory
    pressure on their own once the next packet arrives for them.
    
    So no additional hierarchy code is needed.  Remove the bogus stuff.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9ef3d7c984b4..d3b035c7362b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1155,14 +1155,8 @@ static inline void sk_leave_memory_pressure(struct sock *sk)
 	if (*memory_pressure)
 		*memory_pressure = 0;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
-		struct cg_proto *cg_proto = sk->sk_cgrp;
-		struct proto *prot = sk->sk_prot;
-
-		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			cg_proto->memory_pressure = 0;
-	}
-
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		sk->sk_cgrp->memory_pressure = 0;
 }
 
 static inline void sk_enter_memory_pressure(struct sock *sk)
@@ -1170,13 +1164,8 @@ static inline void sk_enter_memory_pressure(struct sock *sk)
 	if (!sk->sk_prot->enter_memory_pressure)
 		return;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
-		struct cg_proto *cg_proto = sk->sk_cgrp;
-		struct proto *prot = sk->sk_prot;
-
-		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			cg_proto->memory_pressure = 1;
-	}
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		sk->sk_cgrp->memory_pressure = 1;
 
 	sk->sk_prot->enter_memory_pressure(sk);
 }

commit 8c2c2358b236530bc2c79b4c2a447cbdbc3d96d7
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jan 14 15:20:59 2016 -0800

    net: tcp_memcontrol: properly detect ancestor socket pressure
    
    When charging socket memory, the code currently checks only the local
    page counter for excess to determine whether the memcg is under socket
    pressure.  But even if the local counter is fine, one of the ancestors
    could have breached its limit, which should also force this child to
    enter socket pressure.  This currently doesn't happen.
    
    Fix this by using page_counter_try_charge() first.  If that fails, it
    means that either the local counter or one of the ancestors are in
    excess of their limit, and the child should enter socket pressure.
    
    Fixes: 3e32cb2e0a12 ("mm: memcontrol: lockless page counters")
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index e830c1006935..9ef3d7c984b4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1193,11 +1193,13 @@ static inline void memcg_memory_allocated_add(struct cg_proto *prot,
 					      unsigned long amt,
 					      int *parent_status)
 {
-	page_counter_charge(&prot->memory_allocated, amt);
+	struct page_counter *counter;
+
+	if (page_counter_try_charge(&prot->memory_allocated, amt, &counter))
+		return;
 
-	if (page_counter_read(&prot->memory_allocated) >
-	    prot->memory_allocated.limit)
-		*parent_status = OVER_LIMIT;
+	page_counter_charge(&prot->memory_allocated, amt);
+	*parent_status = OVER_LIMIT;
 }
 
 static inline void memcg_memory_allocated_sub(struct cg_proto *prot,

commit ef456144da8ef507c8cf504284b6042e9201a05c
Author: Craig Gallek <kraig@google.com>
Date:   Mon Jan 4 17:41:45 2016 -0500

    soreuseport: define reuseport groups
    
    struct sock_reuseport is an optional shared structure referenced by each
    socket belonging to a reuseport group.  When a socket is bound to an
    address/port not yet in use and the reuseport flag has been set, the
    structure will be allocated and attached to the newly bound socket.
    When subsequent calls to bind are made for the same address/port, the
    shared structure will be updated to include the new socket and the
    newly bound socket will reference the group structure.
    
    Usually, when an incoming packet was destined for a reuseport group,
    all sockets in the same group needed to be considered before a
    dispatching decision was made.  With this structure, an appropriate
    socket can be found after looking up just one socket in the group.
    
    This shared structure will also allow for more complicated decisions to
    be made when selecting a socket (eg a BPF filter).
    
    This work is based off a similar implementation written by
    Ying Cai <ycai@google.com> for implementing policy-based reuseport
    selection.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3794cdde837a..e830c1006935 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -318,6 +318,7 @@ struct cg_proto;
   *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
   *	@sk_backlog_rcv: callback to process the backlog
   *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
+  *	@sk_reuseport_cb: reuseport group container
  */
 struct sock {
 	/*
@@ -453,6 +454,7 @@ struct sock {
 	int			(*sk_backlog_rcv)(struct sock *sk,
 						  struct sk_buff *skb);
 	void                    (*sk_destruct)(struct sock *sk);
+	struct sock_reuseport __rcu	*sk_reuseport_cb;
 };
 
 #define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))

commit b3e0d3d7bab14f2544a3314bec53a23dc7dd2206
Merge: 3268e5cb494d 73796d8bf273
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 17 22:08:28 2015 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/geneve.c
    
    Here we had an overlapping change, where in 'net' the extraneous stats
    bump was being removed whilst in 'net-next' the final argument to
    udp_tunnel6_xmit_skb() was being changed.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7bbadd2d1009575dad675afc16650ebb5aa10612
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Mon Dec 14 23:30:43 2015 +0100

    net: fix warnings in 'make htmldocs' by moving macro definition out of field declaration
    
    Docbook does not like the definition of macros inside a field declaration
    and adds a warning. Move the definition out.
    
    Fixes: 79462ad02e86180 ("net: add validation for the socket syscall protocol argument")
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 28790fe18206..14d3c0734007 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -403,8 +403,8 @@ struct sock {
 				sk_no_check_rx : 1,
 				sk_userlocks : 4,
 				sk_protocol  : 8,
-#define SK_PROTOCOL_MAX U8_MAX
 				sk_type      : 16;
+#define SK_PROTOCOL_MAX U8_MAX
 	kmemcheck_bitfield_end(flags);
 	int			sk_wmem_queued;
 	gfp_t			sk_allocation;

commit 64be0aed59ad519d6f2160868734f7e278290ac1
Author: Lorenzo Colitti <lorenzo@google.com>
Date:   Wed Dec 16 12:30:03 2015 +0900

    net: diag: Add the ability to destroy a socket.
    
    This patch adds a SOCK_DESTROY operation, a destroy function
    pointer to sock_diag_handler, and a diag_destroy function
    pointer.  It does not include any implementation code.
    
    Signed-off-by: Lorenzo Colitti <lorenzo@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ab0269f4b2cc..6e6e8a25d997 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1060,6 +1060,7 @@ struct proto {
 	void			(*destroy_cgroup)(struct mem_cgroup *memcg);
 	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
 #endif
+	int			(*diag_destroy)(struct sock *sk, int err);
 };
 
 int proto_register(struct proto *prot, int alloc_slab);

commit 9a49850d0af7b9fd14d091dfe61ef6cb369f86b9
Author: Tom Herbert <tom@herbertland.com>
Date:   Mon Dec 14 11:19:45 2015 -0800

    tcp: Fix conditions to determine checksum offload
    
    In tcp_send_sendpage and tcp_sendmsg we check the route capabilities to
    determine if checksum offload can be performed. This check currently
    does not take the IP protocol into account for devices that advertise
    only one of NETIF_F_IPV6_CSUM or NETIF_F_IP_CSUM. This patch adds a
    function to check capabilities for checksum offload with a socket
    called sk_check_csum_caps. This function checks for specific IPv4 or
    IPv6 offload support based on the family of the socket.
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0ca22b014de1..ab0269f4b2cc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1791,6 +1791,15 @@ static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
 	sk->sk_route_caps &= ~flags;
 }
 
+static inline bool sk_check_csum_caps(struct sock *sk)
+{
+	return (sk->sk_route_caps & NETIF_F_HW_CSUM) ||
+	       (sk->sk_family == PF_INET &&
+		(sk->sk_route_caps & NETIF_F_IP_CSUM)) ||
+	       (sk->sk_family == PF_INET6 &&
+		(sk->sk_route_caps & NETIF_F_IPV6_CSUM));
+}
+
 static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
 					   struct iov_iter *from, char *to,
 					   int copy, int offset)

commit 5037e9ef9454917b047f9f3a19b4dd179fbf7cd4
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Dec 14 14:08:53 2015 -0800

    net: fix IP early demux races
    
    David Wilder reported crashes caused by dst reuse.
    
    <quote David>
      I am seeing a crash on a distro V4.2.3 kernel caused by a double
      release of a dst_entry.  In ipv4_dst_destroy() the call to
      list_empty() finds a poisoned next pointer, indicating the dst_entry
      has already been removed from the list and freed. The crash occurs
      18 to 24 hours into a run of a network stress exerciser.
    </quote>
    
    Thanks to his detailed report and analysis, we were able to understand
    the core issue.
    
    IP early demux can associate a dst to skb, after a lookup in TCP/UDP
    sockets.
    
    When socket cache is not properly set, we want to store into
    sk->sk_dst_cache the dst for future IP early demux lookups,
    by acquiring a stable refcount on the dst.
    
    Problem is this acquisition is simply using an atomic_inc(),
    which works well, unless the dst was queued for destruction from
    dst_release() noticing dst refcount went to zero, if DST_NOCACHE
    was set on dst.
    
    We need to make sure current refcount is not zero before incrementing
    it, or risk double free as David reported.
    
    This patch, being a stable candidate, adds two new helpers, and use
    them only from IP early demux problematic paths.
    
    It might be possible to merge in net-next skb_dst_force() and
    skb_dst_force_safe(), but I prefer having the smallest patch for stable
    kernels : Maybe some skb_dst_force() callers do not expect skb->dst
    can suddenly be cleared.
    
    Can probably be backported back to linux-3.6 kernels
    
    Reported-by: David J. Wilder <dwilder@us.ibm.com>
    Tested-by: David J. Wilder <dwilder@us.ibm.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c4205e0a3a2d..28790fe18206 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -817,7 +817,7 @@ void sk_stream_write_space(struct sock *sk);
 static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	/* dont let skb dst not refcounted, we are going to leave rcu lock */
-	skb_dst_force(skb);
+	skb_dst_force_safe(skb);
 
 	if (!sk->sk_backlog.tail)
 		sk->sk_backlog.head = skb;

commit 79462ad02e861803b3840cc782248c7359451cd9
Author: Hannes Frederic Sowa <hannes@stressinduktion.org>
Date:   Mon Dec 14 22:03:39 2015 +0100

    net: add validation for the socket syscall protocol argument
    
    郭永刚 reported that one could simply crash the kernel as root by
    using a simple program:
    
            int socket_fd;
            struct sockaddr_in addr;
            addr.sin_port = 0;
            addr.sin_addr.s_addr = INADDR_ANY;
            addr.sin_family = 10;
    
            socket_fd = socket(10,3,0x40000000);
            connect(socket_fd , &addr,16);
    
    AF_INET, AF_INET6 sockets actually only support 8-bit protocol
    identifiers. inet_sock's skc_protocol field thus is sized accordingly,
    thus larger protocol identifiers simply cut off the higher bits and
    store a zero in the protocol fields.
    
    This could lead to e.g. NULL function pointer because as a result of
    the cut off inet_num is zero and we call down to inet_autobind, which
    is NULL for raw sockets.
    
    kernel: Call Trace:
    kernel:  [<ffffffff816db90e>] ? inet_autobind+0x2e/0x70
    kernel:  [<ffffffff816db9a4>] inet_dgram_connect+0x54/0x80
    kernel:  [<ffffffff81645069>] SYSC_connect+0xd9/0x110
    kernel:  [<ffffffff810ac51b>] ? ptrace_notify+0x5b/0x80
    kernel:  [<ffffffff810236d8>] ? syscall_trace_enter_phase2+0x108/0x200
    kernel:  [<ffffffff81645e0e>] SyS_connect+0xe/0x10
    kernel:  [<ffffffff81779515>] tracesys_phase2+0x84/0x89
    
    I found no particular commit which introduced this problem.
    
    CVE: CVE-2015-8543
    Cc: Cong Wang <cwang@twopensource.com>
    Reported-by: 郭永刚 <guoyonggang@360.cn>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index eaef41433d7a..c4205e0a3a2d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -403,6 +403,7 @@ struct sock {
 				sk_no_check_rx : 1,
 				sk_userlocks : 4,
 				sk_protocol  : 8,
+#define SK_PROTOCOL_MAX U8_MAX
 				sk_type      : 16;
 	kmemcheck_bitfield_end(flags);
 	int			sk_wmem_queued;

commit d188ba86dd07a72ebebfa22fe9cb0b0572e57740
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Dec 8 07:22:02 2015 -0800

    xfrm: add rcu protection to sk->sk_policy[]
    
    XFRM can deal with SYNACK messages, sent while listener socket
    is not locked. We add proper rcu protection to __xfrm_sk_clone_policy()
    and xfrm_sk_policy_lookup()
    
    This might serve as the first step to remove xfrm.xfrm_policy_lock
    use in fast path.
    
    Fixes: fa76ce7328b2 ("inet: get rid of central tcp/dccp listener timer")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b1d475b5db68..eaef41433d7a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -388,7 +388,7 @@ struct sock {
 		struct socket_wq	*sk_wq_raw;
 	};
 #ifdef CONFIG_XFRM
-	struct xfrm_policy	*sk_policy[2];
+	struct xfrm_policy __rcu *sk_policy[2];
 #endif
 	struct dst_entry	*sk_rx_dst;
 	struct dst_entry __rcu	*sk_dst_cache;

commit 2a56a1fec290bf0bc4676bbf4efdb3744953a3e7
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 7 17:38:52 2015 -0500

    net: wrap sock->sk_cgrp_prioidx and ->sk_classid inside a struct
    
    Introduce sock->sk_cgrp_data which is a struct sock_cgroup_data.
    ->sk_cgroup_prioidx and ->sk_classid are moved into it.  The struct
    and its accessors are defined in cgroup-defs.h.  This is to prepare
    for overloading the fields with a cgroup pointer.
    
    This patch mostly performs equivalent conversions but the followings
    are noteworthy.
    
    * Equality test before updating classid is removed from
      sock_update_classid().  This shouldn't make any noticeable
      difference and a similar test will be implemented on the helper side
      later.
    
    * sock_update_netprioidx() now takes struct sock_cgroup_data and can
      be moved to netprio_cgroup.h without causing include dependency
      loop.  Moved.
    
    * The dummy version of sock_update_netprioidx() converted to a static
      inline function while at it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a95bcf7d6efa..0ca22b014de1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -59,6 +59,7 @@
 #include <linux/static_key.h>
 #include <linux/sched.h>
 #include <linux/wait.h>
+#include <linux/cgroup-defs.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -308,8 +309,7 @@ struct cg_proto;
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
-  *	@sk_cgrp_prioidx: socket group's priority map index
-  *	@sk_classid: this socket's cgroup classid
+  *	@sk_cgrp_data: cgroup data for this cgroup
   *	@sk_cgrp: this socket's cgroup-specific proto data
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
@@ -443,12 +443,7 @@ struct sock {
 #ifdef CONFIG_SECURITY
 	void			*sk_security;
 #endif
-#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
-	u16			sk_cgrp_prioidx;
-#endif
-#ifdef CONFIG_CGROUP_NET_CLASSID
-	u32			sk_classid;
-#endif
+	struct sock_cgroup_data	sk_cgrp_data;
 	struct cg_proto		*sk_cgrp;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk);

commit 297dbde19cf6a0ccb6fd4396c6220a5912ed61e8
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 7 17:38:51 2015 -0500

    netprio_cgroup: limit the maximum css->id to USHRT_MAX
    
    netprio builds per-netdev contiguous priomap array which is indexed by
    css->id.  The array is allocated using kzalloc() effectively limiting
    the maximum ID supported to some thousand range.  This patch caps the
    maximum supported css->id to USHRT_MAX which should be way above what
    is actually useable.
    
    This allows reducing sock->sk_cgrp_prioidx to u16 from u32.  The freed
    up part will be used to overload the cgroup related fields.
    sock->sk_cgrp_prioidx's position is swapped with sk_mark so that the
    two cgroup related fields are adjacent.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    CC: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6f58b84fc742..a95bcf7d6efa 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -288,7 +288,6 @@ struct cg_proto;
   *	@sk_ack_backlog: current listen backlog
   *	@sk_max_ack_backlog: listen backlog set in listen()
   *	@sk_priority: %SO_PRIORITY setting
-  *	@sk_cgrp_prioidx: socket group's priority map index
   *	@sk_type: socket type (%SOCK_STREAM, etc)
   *	@sk_protocol: which protocol this socket belongs in this network family
   *	@sk_peer_pid: &struct pid for this socket's peer
@@ -309,6 +308,7 @@ struct cg_proto;
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
+  *	@sk_cgrp_prioidx: socket group's priority map index
   *	@sk_classid: this socket's cgroup classid
   *	@sk_cgrp: this socket's cgroup-specific proto data
   *	@sk_write_pending: a write to stream socket waits to start
@@ -425,9 +425,7 @@ struct sock {
 	u32			sk_ack_backlog;
 	u32			sk_max_ack_backlog;
 	__u32			sk_priority;
-#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
-	__u32			sk_cgrp_prioidx;
-#endif
+	__u32			sk_mark;
 	struct pid		*sk_peer_pid;
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
@@ -445,7 +443,9 @@ struct sock {
 #ifdef CONFIG_SECURITY
 	void			*sk_security;
 #endif
-	__u32			sk_mark;
+#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
+	u16			sk_cgrp_prioidx;
+#endif
 #ifdef CONFIG_CGROUP_NET_CLASSID
 	u32			sk_classid;
 #endif

commit 01ce63c90170283a9855d1db4fe81934dddce648
Author: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
Date:   Fri Dec 4 15:14:04 2015 -0200

    sctp: update the netstamp_needed counter when copying sockets
    
    Dmitry Vyukov reported that SCTP was triggering a WARN on socket destroy
    related to disabling sock timestamp.
    
    When SCTP accepts an association or peel one off, it copies sock flags
    but forgot to call net_enable_timestamp() if a packet timestamping flag
    was copied, leading to extra calls to net_disable_timestamp() whenever
    such clones were closed.
    
    The fix is to call net_enable_timestamp() whenever we copy a sock with
    that flag on, like tcp does.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Acked-by: Vlad Yasevich <vyasevich@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 52d27ee924f4..b1d475b5db68 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -740,6 +740,8 @@ enum sock_flags {
 	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 };
 
+#define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))
+
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
 {
 	nsk->sk_flags = osk->sk_flags;

commit f188b951f33a0464338f94f928338f84fc0e4392
Merge: 6b20da4d8f3f 071f5d105a0a
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 3 21:03:21 2015 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/renesas/ravb_main.c
            kernel/bpf/syscall.c
            net/ipv4/ipmr.c
    
    All three conflicts were cases of overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6bd4f355df2eae80b8a5c7b097371cd1e05f20d5
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Dec 2 21:53:57 2015 -0800

    ipv6: kill sk_dst_lock
    
    While testing the np->opt RCU conversion, I found that UDP/IPv6 was
    using a mixture of xchg() and sk_dst_lock to protect concurrent changes
    to sk->sk_dst_cache, leading to possible corruptions and crashes.
    
    ip6_sk_dst_lookup_flow() uses sk_dst_check() anyway, so the simplest
    way to fix the mess is to remove sk_dst_lock completely, as we did for
    IPv4.
    
    __ip6_dst_store() and ip6_dst_store() share same implementation.
    
    sk_setup_caps() being called with socket lock being held or not,
    we have to use sk_dst_set() instead of __sk_dst_set()
    
    Note that I had to move the "np->dst_cookie = rt6_get_cookie(rt);"
    in ip6_dst_store() before the sk_setup_caps(sk, dst) call.
    
    This is because ip6_dst_store() can be called from process context,
    without any lock held.
    
    As soon as the dst is installed in sk->sk_dst_cache, dst can be freed
    from another cpu doing a concurrent ip6_dst_store()
    
    Doing the dst dereference before doing the install is needed to make
    sure no use after free would trigger.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0434138c5f95..52d27ee924f4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -254,7 +254,6 @@ struct cg_proto;
   *	@sk_wq: sock wait queue and async head
   *	@sk_rx_dst: receive input route used by early demux
   *	@sk_dst_cache: destination cache
-  *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
@@ -393,7 +392,7 @@ struct sock {
 #endif
 	struct dst_entry	*sk_rx_dst;
 	struct dst_entry __rcu	*sk_dst_cache;
-	spinlock_t		sk_dst_lock;
+	/* Note: 32bit hole on 64bit arches */
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;

commit 7450aaf61f0ae2ee6cc6491138d11df2c25e7609
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Nov 30 08:57:28 2015 -0800

    tcp: suppress too verbose messages in tcp_send_ack()
    
    If tcp_send_ack() can not allocate skb, we properly handle this
    and setup a timer to try later.
    
    Use __GFP_NOWARN to avoid polluting syslog in the case host is
    under memory pressure, so that pertinent messages are not lost under
    a flood of useless information.
    
    sk_gfp_atomic() can use its gfp_mask argument (all callers currently
    were using GFP_ATOMIC before this patch)
    
    We rename sk_gfp_atomic() to sk_gfp_mask() to clearly express this
    function now takes into account its second argument (gfp_mask)
    
    Note that when tcp_transmit_skb() is called with clone_it set to false,
    we do not attempt memory allocations, so can pass a 0 gfp_mask, which
    most compilers can emit faster than a non zero or constant value.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 62d35afcb3ac..9065f8b7e646 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -775,9 +775,9 @@ static inline int sk_memalloc_socks(void)
 
 #endif
 
-static inline gfp_t sk_gfp_atomic(const struct sock *sk, gfp_t gfp_mask)
+static inline gfp_t sk_gfp_mask(const struct sock *sk, gfp_t gfp_mask)
 {
-	return GFP_ATOMIC | (sk->sk_allocation & __GFP_MEMALLOC);
+	return gfp_mask | (sk->sk_allocation & __GFP_MEMALLOC);
 }
 
 static inline void sk_acceptq_removed(struct sock *sk)

commit ceb5d58b217098a657f3850b7a2640f995032e62
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Nov 29 20:03:11 2015 -0800

    net: fix sock_wake_async() rcu protection
    
    Dmitry provided a syzkaller (http://github.com/google/syzkaller)
    triggering a fault in sock_wake_async() when async IO is requested.
    
    Said program stressed af_unix sockets, but the issue is generic
    and should be addressed in core networking stack.
    
    The problem is that by the time sock_wake_async() is called,
    we should not access the @flags field of 'struct socket',
    as the inode containing this socket might be freed without
    further notice, and without RCU grace period.
    
    We already maintain an RCU protected structure, "struct socket_wq"
    so moving SOCKWQ_ASYNC_NOSPACE & SOCKWQ_ASYNC_WAITDATA into it
    is the safe route.
    
    It also reduces number of cache lines needing dirtying, so might
    provide a performance improvement anyway.
    
    In followup patches, we might move remaining flags (SOCK_NOSPACE,
    SOCK_PASSCRED, SOCK_PASSSEC) to save 8 bytes and let 'struct socket'
    being mostly read and let it being shared between cpus.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c155d09d8af4..0434138c5f95 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -384,8 +384,10 @@ struct sock {
 	int			sk_rcvbuf;
 
 	struct sk_filter __rcu	*sk_filter;
-	struct socket_wq __rcu	*sk_wq;
-
+	union {
+		struct socket_wq __rcu	*sk_wq;
+		struct socket_wq	*sk_wq_raw;
+	};
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
@@ -2005,20 +2007,27 @@ static inline unsigned long sock_wspace(struct sock *sk)
 	return amt;
 }
 
+/* Note:
+ *  We use sk->sk_wq_raw, from contexts knowing this
+ *  pointer is not NULL and cannot disappear/change.
+ */
 static inline void sk_set_bit(int nr, struct sock *sk)
 {
-	set_bit(nr, &sk->sk_socket->flags);
+	set_bit(nr, &sk->sk_wq_raw->flags);
 }
 
 static inline void sk_clear_bit(int nr, struct sock *sk)
 {
-	clear_bit(nr, &sk->sk_socket->flags);
+	clear_bit(nr, &sk->sk_wq_raw->flags);
 }
 
-static inline void sk_wake_async(struct sock *sk, int how, int band)
+static inline void sk_wake_async(const struct sock *sk, int how, int band)
 {
-	if (sock_flag(sk, SOCK_FASYNC))
-		sock_wake_async(sk->sk_socket, how, band);
+	if (sock_flag(sk, SOCK_FASYNC)) {
+		rcu_read_lock();
+		sock_wake_async(rcu_dereference(sk->sk_wq), how, band);
+		rcu_read_unlock();
+	}
 }
 
 /* Since sk_{r,w}mem_alloc sums skb->truesize, even a small frame might

commit 9cd3e072b0be17446e37d7414eac8a3499e0601e
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Nov 29 20:03:10 2015 -0800

    net: rename SOCK_ASYNC_NOSPACE and SOCK_ASYNC_WAITDATA
    
    This patch is a cleanup to make following patch easier to
    review.
    
    Goal is to move SOCK_ASYNC_NOSPACE and SOCK_ASYNC_WAITDATA
    from (struct socket)->flags to a (struct socket_wq)->flags
    to benefit from RCU protection in sock_wake_async()
    
    To ease backports, we rename both constants.
    
    Two new helpers, sk_set_bit(int nr, struct sock *sk)
    and sk_clear_bit(int net, struct sock *sk) are added so that
    following patch can change their implementation.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7f89e4ba18d1..c155d09d8af4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2005,6 +2005,16 @@ static inline unsigned long sock_wspace(struct sock *sk)
 	return amt;
 }
 
+static inline void sk_set_bit(int nr, struct sock *sk)
+{
+	set_bit(nr, &sk->sk_socket->flags);
+}
+
+static inline void sk_clear_bit(int nr, struct sock *sk)
+{
+	clear_bit(nr, &sk->sk_socket->flags);
+}
+
 static inline void sk_wake_async(struct sock *sk, int how, int band)
 {
 	if (sock_flag(sk, SOCK_FASYNC))

commit 1ce0bf50ae2233c7115a18c0c623662d177b434c
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Nov 26 13:55:39 2015 +0800

    net: Generalise wq_has_sleeper helper
    
    The memory barrier in the helper wq_has_sleeper is needed by just
    about every user of waitqueue_active.  This patch generalises it
    by making it take a wait_queue_head_t directly.  The existing
    helper is renamed to skwq_has_sleeper.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7f89e4ba18d1..62d35afcb3ac 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -58,6 +58,7 @@
 #include <linux/memcontrol.h>
 #include <linux/static_key.h>
 #include <linux/sched.h>
+#include <linux/wait.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -1879,12 +1880,12 @@ static inline bool sk_has_allocations(const struct sock *sk)
 }
 
 /**
- * wq_has_sleeper - check if there are any waiting processes
+ * skwq_has_sleeper - check if there are any waiting processes
  * @wq: struct socket_wq
  *
  * Returns true if socket_wq has waiting processes
  *
- * The purpose of the wq_has_sleeper and sock_poll_wait is to wrap the memory
+ * The purpose of the skwq_has_sleeper and sock_poll_wait is to wrap the memory
  * barrier call. They were added due to the race found within the tcp code.
  *
  * Consider following tcp code paths:
@@ -1910,15 +1911,9 @@ static inline bool sk_has_allocations(const struct sock *sk)
  * data on the socket.
  *
  */
-static inline bool wq_has_sleeper(struct socket_wq *wq)
+static inline bool skwq_has_sleeper(struct socket_wq *wq)
 {
-	/* We need to be sure we are in sync with the
-	 * add_wait_queue modifications to the wait queue.
-	 *
-	 * This memory barrier is paired in the sock_poll_wait.
-	 */
-	smp_mb();
-	return wq && waitqueue_active(&wq->wait);
+	return wq && wq_has_sleeper(&wq->wait);
 }
 
 /**

commit 00fd38d938db3f1ab1c486549afc450cb7e751b1
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Nov 12 08:43:18 2015 -0800

    tcp: ensure proper barriers in lockless contexts
    
    Some functions access TCP sockets without holding a lock and
    might output non consistent data, depending on compiler and or
    architecture.
    
    tcp_diag_get_info(), tcp_get_info(), tcp_poll(), get_tcp4_sock() ...
    
    Introduce sk_state_load() and sk_state_store() to fix the issues,
    and more clearly document where this lack of locking is happening.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bbf7c2cf15b4..7f89e4ba18d1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2226,6 +2226,31 @@ static inline bool sk_listener(const struct sock *sk)
 	return (1 << sk->sk_state) & (TCPF_LISTEN | TCPF_NEW_SYN_RECV);
 }
 
+/**
+ * sk_state_load - read sk->sk_state for lockless contexts
+ * @sk: socket pointer
+ *
+ * Paired with sk_state_store(). Used in places we do not hold socket lock :
+ * tcp_diag_get_info(), tcp_get_info(), tcp_poll(), get_tcp4_sock() ...
+ */
+static inline int sk_state_load(const struct sock *sk)
+{
+	return smp_load_acquire(&sk->sk_state);
+}
+
+/**
+ * sk_state_store - update sk->sk_state
+ * @sk: socket pointer
+ * @newstate: new state
+ *
+ * Paired with sk_state_load(). Should be used in contexts where
+ * state change might impact lockless readers.
+ */
+static inline void sk_state_store(struct sock *sk, int newstate)
+{
+	smp_store_release(&sk->sk_state, newstate);
+}
+
 void sock_enable_timestamp(struct sock *sk, int flag);
 int sock_get_timestamp(struct sock *, struct timeval __user *);
 int sock_get_timestampns(struct sock *, struct timespec __user *);

commit d0164adc89f6bb374d304ffcc375c6d2652fe67d
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:21 2015 -0800

    mm, page_alloc: distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd
    
    __GFP_WAIT has been used to identify atomic context in callers that hold
    spinlocks or are in interrupts.  They are expected to be high priority and
    have access one of two watermarks lower than "min" which can be referred
    to as the "atomic reserve".  __GFP_HIGH users get access to the first
    lower watermark and can be called the "high priority reserve".
    
    Over time, callers had a requirement to not block when fallback options
    were available.  Some have abused __GFP_WAIT leading to a situation where
    an optimisitic allocation with a fallback option can access atomic
    reserves.
    
    This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
    cannot sleep and have no alternative.  High priority users continue to use
    __GFP_HIGH.  __GFP_DIRECT_RECLAIM identifies callers that can sleep and
    are willing to enter direct reclaim.  __GFP_KSWAPD_RECLAIM to identify
    callers that want to wake kswapd for background reclaim.  __GFP_WAIT is
    redefined as a caller that is willing to enter direct reclaim and wake
    kswapd for background reclaim.
    
    This patch then converts a number of sites
    
    o __GFP_ATOMIC is used by callers that are high priority and have memory
      pools for those requests. GFP_ATOMIC uses this flag.
    
    o Callers that have a limited mempool to guarantee forward progress clear
      __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM. bio allocations fall
      into this category where kswapd will still be woken but atomic reserves
      are not used as there is a one-entry mempool to guarantee progress.
    
    o Callers that are checking if they are non-blocking should use the
      helper gfpflags_allow_blocking() where possible. This is because
      checking for __GFP_WAIT as was done historically now can trigger false
      positives. Some exceptions like dm-crypt.c exist where the code intent
      is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
      flag manipulations.
    
    o Callers that built their own GFP flags instead of starting with GFP_KERNEL
      and friends now also need to specify __GFP_KSWAPD_RECLAIM.
    
    The first key hazard to watch out for is callers that removed __GFP_WAIT
    and was depending on access to atomic reserves for inconspicuous reasons.
    In some cases it may be appropriate for them to use __GFP_HIGH.
    
    The second key hazard is callers that assembled their own combination of
    GFP flags instead of starting with something like GFP_KERNEL.  They may
    now wish to specify __GFP_KSWAPD_RECLAIM.  It's almost certainly harmless
    if it's missed in most cases as other activity will wake kswapd.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index f570e75e3da9..bbf7c2cf15b4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2041,7 +2041,7 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,
  */
 static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
-	if (sk->sk_allocation & __GFP_WAIT)
+	if (gfpflags_allow_blocking(sk->sk_allocation))
 		return &current->task_frag;
 
 	return &sk->sk_frag;

commit 9e17f8a475fca81950fdddc08df428ed66cf441f
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Nov 1 15:36:55 2015 -0800

    net: make skb_set_owner_w() more robust
    
    skb_set_owner_w() is called from various places that assume
    skb->sk always point to a full blown socket (as it changes
    sk->sk_wmem_alloc)
    
    We'd like to attach skb to request sockets, and in the future
    to timewait sockets as well. For these kind of pseudo sockets,
    we need to take a traditional refcount and use sock_edemux()
    as the destructor.
    
    It is now time to un-inline skb_set_owner_w(), being too big.
    
    Fixes: ca6fb0651883 ("tcp: attach SYNACK messages to request sockets instead of listener")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Bisected-by: Haiyang Zhang <haiyangz@microsoft.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index aeed5c95f3ca..f570e75e3da9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1951,6 +1951,8 @@ static inline void skb_set_hash_from_sk(struct sk_buff *skb, struct sock *sk)
 	}
 }
 
+void skb_set_owner_w(struct sk_buff *skb, struct sock *sk);
+
 /*
  *	Queue a received datagram if it will fit. Stream and sequenced
  *	protocols can't normally use this as they need to fit buffers in
@@ -1959,21 +1961,6 @@ static inline void skb_set_hash_from_sk(struct sk_buff *skb, struct sock *sk)
  *	Inlined as it's very short and called for pretty much every
  *	packet ever received.
  */
-
-static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
-{
-	skb_orphan(skb);
-	skb->sk = sk;
-	skb->destructor = sock_wfree;
-	skb_set_hash_from_sk(skb, sk);
-	/*
-	 * We used to take a refcount on sk, but following operation
-	 * is enough to guarantee sk_free() wont free this sock until
-	 * all in-flight packets are completed
-	 */
-	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
-}
-
 static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 {
 	skb_orphan(skb);

commit 26440c835f8b1a491e2704118ac55bf87334366c
Merge: 371f1c7e0d85 1099f8604411
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Oct 20 06:08:27 2015 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/usb/asix_common.c
            net/ipv4/inet_connection_sock.c
            net/switchdev/switchdev.c
    
    In the inet_connection_sock.c case the request socket hashing scheme
    is completely different in net-next.
    
    The other two conflicts were overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c7c49b8fde26b74277188bdc6c9dca38db6fa35b
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 18:52:25 2015 -0700

    net: add pfmemalloc check in sk_add_backlog()
    
    Greg reported crashes hitting the following check in __sk_backlog_rcv()
    
            BUG_ON(!sock_flag(sk, SOCK_MEMALLOC));
    
    The pfmemalloc bit is currently checked in sk_filter().
    
    This works correctly for TCP, because sk_filter() is ran in
    tcp_v[46]_rcv() before hitting the prequeue or backlog checks.
    
    For UDP or other protocols, this does not work, because the sk_filter()
    is ran from sock_queue_rcv_skb(), which might be called _after_ backlog
    queuing if socket is owned by user by the time packet is processed by
    softirq handler.
    
    Fixes: b4b9e35585089 ("netvm: set PF_MEMALLOC as appropriate during SKB processing")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Greg Thelen <gthelen@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7aa78440559a..e23717013a4e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -828,6 +828,14 @@ static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *s
 	if (sk_rcvqueues_full(sk, limit))
 		return -ENOBUFS;
 
+	/*
+	 * If the skb was allocated from pfmemalloc reserves, only
+	 * allow SOCK_MEMALLOC sockets to use it as this socket is
+	 * helping free memory
+	 */
+	if (skb_pfmemalloc(skb) && !sock_flag(sk, SOCK_MEMALLOC))
+		return -ENOMEM;
+
 	__sk_add_backlog(sk, skb);
 	sk->sk_backlog.len += skb->truesize;
 	return 0;

commit d475f090bf1c0dc2999e98bbf2e7cb2243358849
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:24 2015 -0700

    tcp: shrink tcp_timewait_sock by 8 bytes
    
    Reducing tcp_timewait_sock from 280 bytes to 272 bytes
    allows SLAB to pack 15 objects per page instead of 14 (on x86)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 19cfe1fc911c..64a75458d22c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -229,6 +229,7 @@ struct sock_common {
 	union {
 		int		skc_incoming_cpu;
 		u32		skc_rcv_wnd;
+		u32		skc_tw_rcv_nxt; /* struct tcp_timewait_sock  */
 	};
 
 	atomic_t		skc_refcnt;
@@ -237,6 +238,7 @@ struct sock_common {
 	union {
 		u32		skc_rxhash;
 		u32		skc_window_clamp;
+		u32		skc_tw_snd_nxt; /* struct tcp_timewait_sock */
 	};
 	/* public: */
 };

commit ed53d0ab761f5c71d77c8dc05fd19c0a851200db
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:23 2015 -0700

    net: shrink struct sock and request_sock by 8 bytes
    
    One 32bit hole is following skc_refcnt, use it.
    skc_incoming_cpu can also be an union for request_sock rcv_wnd.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 65712409464b..19cfe1fc911c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -226,11 +226,18 @@ struct sock_common {
 		struct hlist_nulls_node skc_nulls_node;
 	};
 	int			skc_tx_queue_mapping;
-	int			skc_incoming_cpu;
+	union {
+		int		skc_incoming_cpu;
+		u32		skc_rcv_wnd;
+	};
 
 	atomic_t		skc_refcnt;
 	/* private: */
 	int                     skc_dontcopy_end[0];
+	union {
+		u32		skc_rxhash;
+		u32		skc_window_clamp;
+	};
 	/* public: */
 };
 
@@ -287,7 +294,6 @@ struct cg_proto;
   *	@sk_rcvlowat: %SO_RCVLOWAT setting
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
-  *	@sk_rxhash: flow hash received from netif layer
   *	@sk_txhash: computed flow hash for use on transmit
   *	@sk_filter: socket filtering instructions
   *	@sk_timer: sock cleanup timer
@@ -346,6 +352,7 @@ struct sock {
 #define sk_cookie		__sk_common.skc_cookie
 #define sk_incoming_cpu		__sk_common.skc_incoming_cpu
 #define sk_flags		__sk_common.skc_flags
+#define sk_rxhash		__sk_common.skc_rxhash
 
 	socket_lock_t		sk_lock;
 	struct sk_buff_head	sk_receive_queue;
@@ -365,9 +372,6 @@ struct sock {
 	} sk_backlog;
 #define sk_rmem_alloc sk_backlog.rmem_alloc
 	int			sk_forward_alloc;
-#ifdef CONFIG_RPS
-	__u32			sk_rxhash;
-#endif
 
 	__u32			sk_txhash;
 #ifdef CONFIG_NET_RX_BUSY_POLL

commit 8e5eb54d303b7cb1174977ca79030e135728c95e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:22 2015 -0700

    net: align sk_refcnt on 128 bytes boundary
    
    sk->sk_refcnt is dirtied for every TCP/UDP incoming packet.
    This is a performance issue if multiple cpus hit a common socket,
    or multiple sockets are chained due to SO_REUSEPORT.
    
    By moving sk_refcnt 8 bytes further, first 128 bytes of sockets
    are mostly read. As they contain the lookup keys, this has
    a considerable performance impact, as cpus can cache them.
    
    These 8 bytes are not wasted, we use them as a place holder
    for various fields, depending on the socket type.
    
    Tested:
     SYN flood hitting a 16 RX queues NIC.
     TCP listener using 16 sockets and SO_REUSEPORT
     and SO_INCOMING_CPU for proper siloing.
    
     Could process 6.0 Mpps SYN instead of 4.2 Mpps
    
     Kernel profile looked like :
        11.68%  [kernel]  [k] sha_transform
         6.51%  [kernel]  [k] __inet_lookup_listener
         5.07%  [kernel]  [k] __inet_lookup_established
         4.15%  [kernel]  [k] memcpy_erms
         3.46%  [kernel]  [k] ipt_do_table
         2.74%  [kernel]  [k] fib_table_lookup
         2.54%  [kernel]  [k] tcp_make_synack
         2.34%  [kernel]  [k] tcp_conn_request
         2.05%  [kernel]  [k] __netif_receive_skb_core
         2.03%  [kernel]  [k] kmem_cache_alloc
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cf54739f30d5..65712409464b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -150,6 +150,9 @@ typedef __u64 __bitwise __addrpair;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
  *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_flags: place holder for sk_flags
+ *		%SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
+ *		%SO_OOBINLINE settings, %SO_TIMESTAMPING settings
  *	@skc_incoming_cpu: record/match cpu processing incoming packets
  *	@skc_refcnt: reference count
  *
@@ -201,6 +204,16 @@ struct sock_common {
 
 	atomic64_t		skc_cookie;
 
+	/* following fields are padding to force
+	 * offset(struct sock, sk_refcnt) == 128 on 64bit arches
+	 * assuming IPV6 is enabled. We use this padding differently
+	 * for different kind of 'sockets'
+	 */
+	union {
+		unsigned long	skc_flags;
+		struct sock	*skc_listener; /* request_sock */
+		struct inet_timewait_death_row *skc_tw_dr; /* inet_timewait_sock */
+	};
 	/*
 	 * fields between dontcopy_begin/dontcopy_end
 	 * are not copied in sock_copy()
@@ -246,8 +259,6 @@ struct cg_proto;
   *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
   *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
   *	@sk_sndbuf: size of send buffer in bytes
-  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
-  *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
   *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
   *	@sk_no_check_rx: allow zero checksum in RX packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
@@ -334,6 +345,7 @@ struct sock {
 #define sk_v6_rcv_saddr	__sk_common.skc_v6_rcv_saddr
 #define sk_cookie		__sk_common.skc_cookie
 #define sk_incoming_cpu		__sk_common.skc_incoming_cpu
+#define sk_flags		__sk_common.skc_flags
 
 	socket_lock_t		sk_lock;
 	struct sk_buff_head	sk_receive_queue;
@@ -371,7 +383,6 @@ struct sock {
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
-	unsigned long 		sk_flags;
 	struct dst_entry	*sk_rx_dst;
 	struct dst_entry __rcu	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;

commit 70da268b569d32a9fddeea85dc18043de9d89f89
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:21 2015 -0700

    net: SO_INCOMING_CPU setsockopt() support
    
    SO_INCOMING_CPU as added in commit 2c8c56e15df3 was a getsockopt() command
    to fetch incoming cpu handling a particular TCP flow after accept()
    
    This commits adds setsockopt() support and extends SO_REUSEPORT selection
    logic : If a TCP listener or UDP socket has this option set, a packet is
    delivered to this socket only if CPU handling the packet matches the specified
    one.
    
    This allows to build very efficient TCP servers, using one listener per
    RX queue, as the associated TCP listener should only accept flows handled
    in softirq by the same cpu.
    This provides optimal NUMA behavior and keep cpu caches hot.
    
    Note that __inet_lookup_listener() still has to iterate over the list of
    all listeners. Following patch puts sk_refcnt in a different cache line
    to let this iteration hit only shared and read mostly cache lines.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9322cafd191b..cf54739f30d5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -150,6 +150,7 @@ typedef __u64 __bitwise __addrpair;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
  *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_incoming_cpu: record/match cpu processing incoming packets
  *	@skc_refcnt: reference count
  *
  *	This is the minimal network layer representation of sockets, the header
@@ -212,6 +213,8 @@ struct sock_common {
 		struct hlist_nulls_node skc_nulls_node;
 	};
 	int			skc_tx_queue_mapping;
+	int			skc_incoming_cpu;
+
 	atomic_t		skc_refcnt;
 	/* private: */
 	int                     skc_dontcopy_end[0];
@@ -274,7 +277,6 @@ struct cg_proto;
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
   *	@sk_rxhash: flow hash received from netif layer
-  *	@sk_incoming_cpu: record cpu processing incoming packets
   *	@sk_txhash: computed flow hash for use on transmit
   *	@sk_filter: socket filtering instructions
   *	@sk_timer: sock cleanup timer
@@ -331,6 +333,7 @@ struct sock {
 #define sk_v6_daddr		__sk_common.skc_v6_daddr
 #define sk_v6_rcv_saddr	__sk_common.skc_v6_rcv_saddr
 #define sk_cookie		__sk_common.skc_cookie
+#define sk_incoming_cpu		__sk_common.skc_incoming_cpu
 
 	socket_lock_t		sk_lock;
 	struct sk_buff_head	sk_receive_queue;
@@ -353,11 +356,6 @@ struct sock {
 #ifdef CONFIG_RPS
 	__u32			sk_rxhash;
 #endif
-	u16			sk_incoming_cpu;
-	/* 16bit hole
-	 * Warned : sk_incoming_cpu can be set from softirq,
-	 * Do not use this hole without fully understanding possible issues.
-	 */
 
 	__u32			sk_txhash;
 #ifdef CONFIG_NET_RX_BUSY_POLL

commit f28ea365cdefc3b4fd0373e70b0106a0cd9b4c23
Author: Edward Jee <edjee@google.com>
Date:   Thu Oct 8 14:56:48 2015 -0700

    sock: support per-packet fwmark
    
    It's useful to allow users to set fwmark for an individual packet,
    without changing the socket state. The function this patch adds in
    sock layer can be used by the protocols that need such a feature.
    
    Signed-off-by: Edward Hyunkoo Jee <edjee@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 771ca1996442..9322cafd191b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1514,6 +1514,13 @@ void sock_kfree_s(struct sock *sk, void *mem, int size);
 void sock_kzfree_s(struct sock *sk, void *mem, int size);
 void sk_send_sigurg(struct sock *sk);
 
+struct sockcm_cookie {
+	u32 mark;
+};
+
+int sock_cmsg_send(struct sock *sk, struct msghdr *msg,
+		   struct sockcm_cookie *sockc);
+
 /*
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.

commit e446f9dfe17bbaa76a1fe22912636f38be1e1af8
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 05:01:55 2015 -0700

    net: synack packets can be attached to request sockets
    
    selinux needs few changes to accommodate fact that SYNACK messages
    can be attached to a request socket, lacking sk_security pointer
    
    (Only syncookies are still attached to a TCP_LISTEN socket)
    
    Adds a new sk_listener() helper, and use it in selinux and sch_fq
    
    Fixes: ca6fb0651883 ("tcp: attach SYNACK messages to request sockets instead of listener")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported by: kernel test robot <ying.huang@linux.intel.com>
    Cc: Paul Moore <paul@paul-moore.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Eric Paris <eparis@parisplace.org>
    Acked-by: Paul Moore <paul@paul-moore.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index dfe2eb8e1132..771ca1996442 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2201,6 +2201,14 @@ static inline bool sk_fullsock(const struct sock *sk)
 	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);
 }
 
+/* This helper checks if a socket is a LISTEN or NEW_SYN_RECV
+ * SYNACK messages can be attached to either ones (depending on SYNCOOKIE)
+ */
+static inline bool sk_listener(const struct sock *sk)
+{
+	return (1 << sk->sk_state) & (TCPF_LISTEN | TCPF_NEW_SYN_RECV);
+}
+
 void sock_enable_timestamp(struct sock *sk, int flag);
 int sock_get_timestamp(struct sock *, struct timeval __user *);
 int sock_get_timestampns(struct sock *, struct timespec __user *);

commit 87e002b21aafccfe71faeec62f3543d30600a518
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:45 2015 -0700

    net: constify sk_gfp_atomic() sock argument
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 94dff7f566f5..dfe2eb8e1132 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -759,7 +759,7 @@ static inline int sk_memalloc_socks(void)
 
 #endif
 
-static inline gfp_t sk_gfp_atomic(struct sock *sk, gfp_t gfp_mask)
+static inline gfp_t sk_gfp_atomic(const struct sock *sk, gfp_t gfp_mask)
 {
 	return GFP_ATOMIC | (sk->sk_allocation & __GFP_MEMALLOC);
 }

commit 58d607d3e52f2b15902f58a1161da9fb3b0f6d47
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 15 15:24:20 2015 -0700

    tcp: provide skb->hash to synack packets
    
    In commit b73c3d0e4f0e ("net: Save TX flow hash in sock and set in skbuf
    on xmit"), Tom provided a l4 hash to most outgoing TCP packets.
    
    We'd like to provide one as well for SYNACK packets, so that all packets
    of a given flow share same txhash, to later enable bonding driver to
    also use skb->hash to perform slave selection.
    
    Note that a SYNACK retransmit shuffles the tx hash, as Tom did
    in commit 265f94ff54d62 ("net: Recompute sk_txhash on negative routing
    advice") for established sockets.
    
    This has nice effect making TCP flows resilient to some kind of black
    holes, even at connection establish phase.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <tom@herbertland.com>
    Cc: Mahesh Bandewar <maheshb@google.com>
    Acked-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7aa78440559a..94dff7f566f5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1654,12 +1654,16 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 kuid_t sock_i_uid(struct sock *sk);
 unsigned long sock_i_ino(struct sock *sk);
 
-static inline void sk_set_txhash(struct sock *sk)
+static inline u32 net_tx_rndhash(void)
 {
-	sk->sk_txhash = prandom_u32();
+	u32 v = prandom_u32();
+
+	return v ?: 1;
+}
 
-	if (unlikely(!sk->sk_txhash))
-		sk->sk_txhash = 1;
+static inline void sk_set_txhash(struct sock *sk)
+{
+	sk->sk_txhash = net_tx_rndhash();
 }
 
 static inline void sk_rethink_txhash(struct sock *sk)

commit e752eb68811aeece2220e183e23369a34122fb5e
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Sep 8 15:01:16 2015 -0700

    memcg: move memcg_proto_active from sock.h
    
    The only user is sock_update_memcg which is living in memcontrol.c so it
    doesn't make much sense to pollute sock.h by this inline helper.  Move it
    to memcontrol.c and open code it into its only caller.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index a98c71ea40c5..7aa78440559a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1045,11 +1045,6 @@ struct proto {
 int proto_register(struct proto *prot, int alloc_slab);
 void proto_unregister(struct proto *prot);
 
-static inline bool memcg_proto_active(struct cg_proto *cg_proto)
-{
-	return test_bit(MEMCG_SOCK_ACTIVE, &cg_proto->flags);
-}
-
 #ifdef SOCK_REFCNT_DEBUG
 static inline void sk_refcnt_debug_inc(struct sock *sk)
 {

commit 33398cf2f360c5ce24c8a22436d52a06ad4e5eb5
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Sep 8 15:01:02 2015 -0700

    memcg: export struct mem_cgroup
    
    mem_cgroup structure is defined in mm/memcontrol.c currently which means
    that the code outside of this file has to use external API even for
    trivial access stuff.
    
    This patch exports mm_struct with its dependencies and makes some of the
    exported functions inlines.  This even helps to reduce the code size a bit
    (make defconfig + CONFIG_MEMCG=y)
    
      text          data    bss     dec              hex    filename
      12355346        1823792 1089536 15268674         e8fb42 vmlinux.before
      12354970        1823792 1089536 15268298         e8f9ca vmlinux.after
    
    This is not much (370B) but better than nothing.
    
    We also save a function call in some hot paths like callers of
    mem_cgroup_count_vm_event which is used for accounting.
    
    The patch doesn't introduce any functional changes.
    
    [vdavykov@parallels.com: inline memcg_kmem_is_active]
    [vdavykov@parallels.com: do not expose type outside of CONFIG_MEMCG]
    [akpm@linux-foundation.org: memcontrol.h needs eventfd.h for eventfd_ctx]
    [akpm@linux-foundation.org: export mem_cgroup_from_task() to modules]
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: Vladimir Davydov <vdavydov@parallels.com>
    Suggested-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 43c6abcf06ab..a98c71ea40c5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1042,34 +1042,6 @@ struct proto {
 #endif
 };
 
-/*
- * Bits in struct cg_proto.flags
- */
-enum cg_proto_flags {
-	/* Currently active and new sockets should be assigned to cgroups */
-	MEMCG_SOCK_ACTIVE,
-	/* It was ever activated; we must disarm static keys on destruction */
-	MEMCG_SOCK_ACTIVATED,
-};
-
-struct cg_proto {
-	struct page_counter	memory_allocated;	/* Current allocated memory. */
-	struct percpu_counter	sockets_allocated;	/* Current number of sockets. */
-	int			memory_pressure;
-	long			sysctl_mem[3];
-	unsigned long		flags;
-	/*
-	 * memcg field is used to find which memcg we belong directly
-	 * Each memcg struct can hold more than one cg_proto, so container_of
-	 * won't really cut.
-	 *
-	 * The elegant solution would be having an inverse function to
-	 * proto_cgroup in struct proto, but that means polluting the structure
-	 * for everybody, instead of just for memcg users.
-	 */
-	struct mem_cgroup	*memcg;
-};
-
 int proto_register(struct proto *prot, int alloc_slab);
 void proto_unregister(struct proto *prot);
 

commit 5510b3c2a173921374ec847848fb20b98e1c698a
Merge: 17f901e8915c 7c764cec3703
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jul 31 23:52:20 2015 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            arch/s390/net/bpf_jit_comp.c
            drivers/net/ethernet/ti/netcp_ethss.c
            net/bridge/br_multicast.c
            net/ipv4/ip_fragment.c
    
    All four conflicts were cases of simple overlapping
    changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 265f94ff54d62503663d9c788ba1f082e448f8b8
Author: Tom Herbert <tom@herbertland.com>
Date:   Tue Jul 28 16:02:06 2015 -0700

    net: Recompute sk_txhash on negative routing advice
    
    When a connection is failing a transport protocol calls
    dst_negative_advice to try to get a better route. This patch includes
    changing the sk_txhash in that function. This provides a rudimentary
    method to try to find a different path in the network since sk_txhash
    affects ECMP on the local host and through the network (via flow labels
    or UDP source port in encapsulation).
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index fe735c4841f6..24aa75c5317a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1695,6 +1695,12 @@ static inline void sk_set_txhash(struct sock *sk)
 		sk->sk_txhash = 1;
 }
 
+static inline void sk_rethink_txhash(struct sock *sk)
+{
+	if (sk->sk_txhash)
+		sk_set_txhash(sk);
+}
+
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
@@ -1719,6 +1725,8 @@ static inline void dst_negative_advice(struct sock *sk)
 {
 	struct dst_entry *ndst, *dst = __sk_dst_get(sk);
 
+	sk_rethink_txhash(sk);
+
 	if (dst && dst->ops->negative_advice) {
 		ndst = dst->ops->negative_advice(dst);
 

commit 877d1f6291f8e391237e324be58479a3e3a7407c
Author: Tom Herbert <tom@herbertland.com>
Date:   Tue Jul 28 16:02:05 2015 -0700

    net: Set sk_txhash from a random number
    
    This patch creates sk_set_txhash and eliminates protocol specific
    inet_set_txhash and ip6_set_txhash. sk_set_txhash simply sets a
    random number instead of performing flow dissection. sk_set_txash
    is also allowed to be called multiple times for the same socket,
    we'll need this when redoing the hash for negative routing advice.
    
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4353ef70bf48..fe735c4841f6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1687,6 +1687,14 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 kuid_t sock_i_uid(struct sock *sk);
 unsigned long sock_i_ino(struct sock *sk);
 
+static inline void sk_set_txhash(struct sock *sk)
+{
+	sk->sk_txhash = prandom_u32();
+
+	if (unlikely(!sk->sk_txhash))
+		sk->sk_txhash = 1;
+}
+
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {

commit dfbafc995304ebb9a9b03f65083e6e9cea143b20
Author: Sabrina Dubroca <sd@queasysnail.net>
Date:   Fri Jul 24 18:19:25 2015 +0200

    tcp: fix recv with flags MSG_WAITALL | MSG_PEEK
    
    Currently, tcp_recvmsg enters a busy loop in sk_wait_data if called
    with flags = MSG_WAITALL | MSG_PEEK.
    
    sk_wait_data waits for sk_receive_queue not empty, but in this case,
    the receive queue is not empty, but does not contain any skb that we
    can use.
    
    Add a "last skb seen on receive queue" argument to sk_wait_data, so
    that it sleeps until the receive queue has new skbs.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=99461
    Link: https://sourceware.org/bugzilla/show_bug.cgi?id=18493
    Link: https://bugzilla.redhat.com/show_bug.cgi?id=1205258
    Reported-by: Enrico Scholz <rh-bugzilla@ensc.de>
    Reported-by: Dan Searle <dan@censornet.com>
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 05a8c1aea251..f21f0708ec59 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -902,7 +902,7 @@ void sk_stream_kill_queues(struct sock *sk);
 void sk_set_memalloc(struct sock *sk);
 void sk_clear_memalloc(struct sock *sk);
 
-int sk_wait_data(struct sock *sk, long *timeo);
+int sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb);
 
 struct request_sock_ops;
 struct timewait_sock_ops;

commit e181a5430491f038c198f0eacc3142d6e871c2da
Author: Mathias Krause <minipli@googlemail.com>
Date:   Sun Jul 19 22:21:13 2015 +0200

    net: #ifdefify sk_classid member of struct sock
    
    The sk_classid member is only required when CONFIG_CGROUP_NET_CLASSID is
    enabled. #ifdefify it to reduce the size of struct sock on 32 bit
    systems, at least.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 05a8c1aea251..4353ef70bf48 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -429,7 +429,9 @@ struct sock {
 	void			*sk_security;
 #endif
 	__u32			sk_mark;
+#ifdef CONFIG_CGROUP_NET_CLASSID
 	u32			sk_classid;
+#endif
 	struct cg_proto		*sk_cgrp;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk);

commit 1830fcea5bbed2719a9dc32aebe802f72ddf14ab
Author: David Miller <davem@davemloft.net>
Date:   Thu Jun 25 06:19:15 2015 -0700

    net: Kill sock->sk_protinfo
    
    No more users, so it can now be removed.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 14d539c040d7..05a8c1aea251 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -277,7 +277,6 @@ struct cg_proto;
   *	@sk_incoming_cpu: record cpu processing incoming packets
   *	@sk_txhash: computed flow hash for use on transmit
   *	@sk_filter: socket filtering instructions
-  *	@sk_protinfo: private area, net family specific, when not using slab
   *	@sk_timer: sock cleanup timer
   *	@sk_stamp: time stamp of last packet received
   *	@sk_tsflags: SO_TIMESTAMPING socket options
@@ -416,7 +415,6 @@ struct sock {
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
 	long			sk_sndtimeo;
-	void			*sk_protinfo;
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;
 	u16			sk_tsflags;

commit 638579f00a9b810ae37c7086b0d20634c1e0234e
Author: Zhaowei Yuan <zhaowei.yuan@samsung.com>
Date:   Wed Jun 17 17:56:27 2015 +0800

    net: Update out-of-date comment
    
    Struct inet_proto no longer exists, so update the
    comment which is out of date.
    
    Signed-off-by: Zhaowei Yuan <zhaowei.yuan@samsung.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3e8258699270..14d539c040d7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -926,7 +926,6 @@ static inline void sk_prot_clear_nulls(struct sock *sk, int size)
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
- * transport -> network interface is defined by struct inet_proto
  */
 struct proto {
 	void			(*close)(struct sock *sk,

commit eb4cb008529ca08e0d8c0fa54e8f739520197a65
Author: Craig Gallek <kraig@google.com>
Date:   Mon Jun 15 11:26:18 2015 -0400

    sock_diag: define destruction multicast groups
    
    These groups will contain socket-destruction events for
    AF_INET/AF_INET6, IPPROTO_TCP/IPPROTO_UDP.
    
    Near the end of socket destruction, a check for listeners is
    performed.  In the presence of a listener, rather than completely
    cleanup the socket, a unit of work will be added to a private
    work queue which will first broadcast information about the socket
    and then finish the cleanup operation.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 26c1c3171e00..3e8258699270 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1518,6 +1518,7 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
 		      struct proto *prot, int kern);
 void sk_free(struct sock *sk);
+void sk_destruct(struct sock *sk);
 struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
 
 struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,

commit eb9344781a2f8381ed60cd9e662d9ced2d168ecb
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue May 19 13:26:55 2015 -0700

    tcp: add a force_schedule argument to sk_stream_alloc_skb()
    
    In commit 8e4d980ac215 ("tcp: fix behavior for epoll edge trigger")
    we fixed a possible hang of TCP sockets under memory pressure,
    by allowing sk_stream_alloc_skb() to use sk_forced_mem_schedule()
    if no packet is in socket write queue.
    
    It turns out there are other cases where we want to force memory
    schedule :
    
    tcp_fragment() & tso_fragment() need to split a big TSO packet into
    two smaller ones. If we block here because of TCP memory pressure,
    we can effectively block TCP socket from sending new data.
    If no further ACK is coming, this hang would be definitive, and socket
    has no chance to effectively reduce its memory usage.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4581a60636f8..26c1c3171e00 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2025,7 +2025,8 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 	}
 }
 
-struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp);
+struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,
+				    bool force_schedule);
 
 /**
  * sk_page_frag - return an appropriate page_frag

commit 1a24e04e4b50939daa3041682b38b82c896ca438
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri May 15 12:39:25 2015 -0700

    net: fix sk_mem_reclaim_partial()
    
    sk_mem_reclaim_partial() goal is to ensure each socket has
    one SK_MEM_QUANTUM forward allocation. This is needed both for
    performance and better handling of memory pressure situations in
    follow up patches.
    
    SK_MEM_QUANTUM is currently a page, but might be reduced to 4096 bytes
    as some arches have 64KB pages.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d882f4c8e438..4581a60636f8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1368,7 +1368,7 @@ static inline struct inode *SOCK_INODE(struct socket *socket)
  * Functions for memory accounting
  */
 int __sk_mem_schedule(struct sock *sk, int size, int kind);
-void __sk_mem_reclaim(struct sock *sk);
+void __sk_mem_reclaim(struct sock *sk, int amount);
 
 #define SK_MEM_QUANTUM ((int)PAGE_SIZE)
 #define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
@@ -1409,7 +1409,7 @@ static inline void sk_mem_reclaim(struct sock *sk)
 	if (!sk_has_account(sk))
 		return;
 	if (sk->sk_forward_alloc >= SK_MEM_QUANTUM)
-		__sk_mem_reclaim(sk);
+		__sk_mem_reclaim(sk, sk->sk_forward_alloc);
 }
 
 static inline void sk_mem_reclaim_partial(struct sock *sk)
@@ -1417,7 +1417,7 @@ static inline void sk_mem_reclaim_partial(struct sock *sk)
 	if (!sk_has_account(sk))
 		return;
 	if (sk->sk_forward_alloc > SK_MEM_QUANTUM)
-		__sk_mem_reclaim(sk);
+		__sk_mem_reclaim(sk, sk->sk_forward_alloc - 1);
 }
 
 static inline void sk_mem_charge(struct sock *sk, int size)

commit affb9792f1d99e1e4d64411e147b648d65f2576e
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 8 21:12:13 2015 -0500

    net: kill sk_change_net and sk_release_kernel
    
    These functions are no longer needed and no longer used kill them.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9e6b2c0b4741..d882f4c8e438 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1518,7 +1518,6 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
 		      struct proto *prot, int kern);
 void sk_free(struct sock *sk);
-void sk_release_kernel(struct sock *sk);
 struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
 
 struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
@@ -2194,22 +2193,6 @@ void sock_net_set(struct sock *sk, struct net *net)
 	write_pnet(&sk->sk_net, net);
 }
 
-/*
- * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
- * They should not hold a reference to a namespace in order to allow
- * to stop it.
- * Sockets after sk_change_net should be released using sk_release_kernel
- */
-static inline void sk_change_net(struct sock *sk, struct net *net)
-{
-	struct net *current_net = sock_net(sk);
-
-	if (!net_eq(current_net, net)) {
-		put_net(current_net);
-		sock_net_set(sk, net);
-	}
-}
-
 static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 {
 	if (skb->sk) {

commit 26abe14379f8e2fa3fd1bcf97c9a7ad9364886fe
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 8 21:10:31 2015 -0500

    net: Modify sk_alloc to not reference count the netns of kernel sockets.
    
    Now that sk_alloc knows when a kernel socket is being allocated modify
    it to not reference count the network namespace of kernel sockets.
    
    Keep track of if a socket needs reference counting by adding a flag to
    struct sock called sk_net_refcnt.
    
    Update all of the callers of sock_create_kern to stop using
    sk_change_net and sk_release_kernel as those hacks are no longer
    needed, to avoid reference counting a kernel socket.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d8dcf91732b0..9e6b2c0b4741 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -184,6 +184,7 @@ struct sock_common {
 	unsigned char		skc_reuse:4;
 	unsigned char		skc_reuseport:1;
 	unsigned char		skc_ipv6only:1;
+	unsigned char		skc_net_refcnt:1;
 	int			skc_bound_dev_if;
 	union {
 		struct hlist_node	skc_bind_node;
@@ -323,6 +324,7 @@ struct sock {
 #define sk_reuse		__sk_common.skc_reuse
 #define sk_reuseport		__sk_common.skc_reuseport
 #define sk_ipv6only		__sk_common.skc_ipv6only
+#define sk_net_refcnt		__sk_common.skc_net_refcnt
 #define sk_bound_dev_if		__sk_common.skc_bound_dev_if
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_prot			__sk_common.skc_prot

commit 11aa9c28b4209242a9de0a661a7b3405adb568a0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 8 21:09:13 2015 -0500

    net: Pass kern from net_proto_family.create to sk_alloc
    
    In preparation for changing how struct net is refcounted
    on kernel sockets pass the knowledge that we are creating
    a kernel socket from sock_create_kern through to sk_alloc.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3a4898ec8c67..d8dcf91732b0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1514,7 +1514,7 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 
 
 struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
-		      struct proto *prot);
+		      struct proto *prot, int kern);
 void sk_free(struct sock *sk);
 void sk_release_kernel(struct sock *sk);
 struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);

commit 237dae889051ed4ebf438b08ca6c0e7c54b97774
Merge: 7abccdba25be e2e40f2c1ed4
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 9 00:00:30 2015 -0400

    Merge branch 'iocb' into for-davem
    
    trivial conflict in net/socket.c and non-trivial one in crypto -
    that one had evaded aio_complete() removal.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit c85d6975ef923cffdd56de3e0e6aba0977282cff
Merge: 60302ff631f0 f22e6e847115
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Apr 6 21:52:19 2015 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/mellanox/mlx4/cmd.c
            net/core/fib_rules.c
            net/ipv4/fib_frontend.c
    
    The fib_rules.c and fib_frontend.c conflicts were locking adjustments
    in 'net' overlapping addition and removal of code in 'net-next'.
    
    The mlx4 conflict was a bug fix in 'net' happening in the same
    place a constant was being replaced with a more suitable macro.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f60e5990d9c1424af9dbca60a23ba2a1c7c1ce90
Author: hannes@stressinduktion.org <hannes@stressinduktion.org>
Date:   Wed Apr 1 17:07:44 2015 +0200

    ipv6: protect skb->sk accesses from recursive dereference inside the stack
    
    We should not consult skb->sk for output decisions in xmit recursion
    levels > 0 in the stack. Otherwise local socket settings could influence
    the result of e.g. tunnel encapsulation process.
    
    ipv6 does not conform with this in three places:
    
    1) ip6_fragment: we do consult ipv6_npinfo for frag_size
    
    2) sk_mc_loop in ipv6 uses skb->sk and checks if we should
       loop the packet back to the local socket
    
    3) ip6_skb_dst_mtu could query the settings from the user socket and
       force a wrong MTU
    
    Furthermore:
    In sk_mc_loop we could potentially land in WARN_ON(1) if we use a
    PF_PACKET socket ontop of an IPv6-backed vxlan device.
    
    Reuse xmit_recursion as we are currently only interested in protecting
    tunnel devices.
    
    Cc: Jiri Pirko <jiri@resnulli.us>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ab186b1d31ff..e4079c28e6b8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1762,6 +1762,8 @@ struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
 struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
+bool sk_mc_loop(struct sock *sk);
+
 static inline bool sk_can_gso(const struct sock *sk)
 {
 	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);

commit e2e40f2c1ed433c5e224525c8c862fd32e5d3df2
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Feb 22 08:58:50 2015 -0800

    fs: move struct kiocb to fs.h
    
    struct kiocb now is a generic I/O container, so move it to fs.h.
    Also do a #include diet for aio.h while we're at it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index ab186b1d31ff..71c1300025e2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -57,7 +57,6 @@
 #include <linux/page_counter.h>
 #include <linux/memcontrol.h>
 #include <linux/static_key.h>
-#include <linux/aio.h>
 #include <linux/sched.h>
 
 #include <linux/filter.h>

commit becb74f0acca19b5abfcb24dc602530f3deea66a
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 19 19:04:21 2015 -0700

    net: increase sk_[max_]ack_backlog
    
    sk_ack_backlog & sk_max_ack_backlog were 16bit fields, meaning
    listen() backlog was limited to 65535.
    
    It is time to increase the width to allow much bigger backlog,
    if admins change /proc/sys/net/core/somaxconn &
    /proc/sys/net/ipv4/tcp_max_syn_backlog default values.
    
    Tested:
    
    echo 5000000 >/proc/sys/net/core/somaxconn
    echo 5000000 >/proc/sys/net/ipv4/tcp_max_syn_backlog
    
    Ran a SYNFLOOD test against a listener using listen(fd, 5000000)
    
    myhost~# grep request_sock_TCP /proc/slabinfo
    request_sock_TCP  4185642 4411940    304   13    1 : tunables   54   27    8 : slabdata 339380 339380      0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e0360f5a53e9..3f9b8ce56948 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -405,8 +405,8 @@ struct sock {
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
 				sk_err_soft;
-	unsigned short		sk_ack_backlog;
-	unsigned short		sk_max_ack_backlog;
+	u32			sk_ack_backlog;
+	u32			sk_max_ack_backlog;
 	__u32			sk_priority;
 #if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
 	__u32			sk_cgrp_prioidx;

commit 1d0ab253872cdd3d8e7913f59c266c7fd01771d0
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 15 21:12:12 2015 -0700

    net: add sk_fullsock() helper
    
    We have many places where we want to check if a socket is
    not a timewait or request socket. Use a helper to avoid
    hard coding this.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f10832ca2e90..e0360f5a53e9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -67,6 +67,7 @@
 #include <linux/atomic.h>
 #include <net/dst.h>
 #include <net/checksum.h>
+#include <net/tcp_states.h>
 #include <linux/net_tstamp.h>
 
 struct cgroup;
@@ -2218,6 +2219,14 @@ static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 	return NULL;
 }
 
+/* This helper checks if a socket is a full socket,
+ * ie _not_ a timewait or request socket.
+ */
+static inline bool sk_fullsock(const struct sock *sk)
+{
+	return (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);
+}
+
 void sock_enable_timestamp(struct sock *sk, int flag);
 int sock_get_timestamp(struct sock *, struct timeval __user *);
 int sock_get_timestampns(struct sock *, struct timespec __user *);

commit 41b822c59e21414d829bcfd00df0c8f7f13b1b95
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 12 16:44:08 2015 -0700

    inet: prepare sock_edemux() & sock_gen_put() for new SYN_RECV state
    
    sock_edemux() & sock_gen_put() should be ready to cope with request socks.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9411c3421dd3..f10832ca2e90 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1625,7 +1625,7 @@ static inline void sock_put(struct sock *sk)
 		sk_free(sk);
 }
 /* Generic version of sock_put(), dealing with all sockets
- * (TCP_TIMEWAIT, ESTABLISHED...)
+ * (TCP_TIMEWAIT, TCP_NEW_SYN_RECV, ESTABLISHED...)
  */
 void sock_gen_put(struct sock *sk);
 

commit 0c5c9fb55106333e773de8c9dd321fa8240caeb3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 11 23:06:44 2015 -0500

    net: Introduce possible_net_t
    
    Having to say
    > #ifdef CONFIG_NET_NS
    >       struct net *net;
    > #endif
    
    in structures is a little bit wordy and a little bit error prone.
    
    Instead it is possible to say:
    > typedef struct {
    > #ifdef CONFIG_NET_NS
    >       struct net *net;
    > #endif
    > } possible_net_t;
    
    And then in a header say:
    
    >       possible_net_t net;
    
    Which is cleaner and easier to use and easier to test, as the
    possible_net_t is always there no matter what the compile options.
    
    Further this allows read_pnet and write_pnet to be functions in all
    cases which is better at catching typos.
    
    This change adds possible_net_t, updates the definitions of read_pnet
    and write_pnet, updates optional struct net * variables that
    write_pnet uses on to have the type possible_net_t, and finally fixes
    up the b0rked users of read_pnet and write_pnet.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 95b2c1c220f9..9411c3421dd3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -190,9 +190,7 @@ struct sock_common {
 		struct hlist_nulls_node skc_portaddr_node;
 	};
 	struct proto		*skc_prot;
-#ifdef CONFIG_NET_NS
-	struct net	 	*skc_net;
-#endif
+	possible_net_t		skc_net;
 
 #if IS_ENABLED(CONFIG_IPV6)
 	struct in6_addr		skc_v6_daddr;

commit efd7ef1c1929d7a0329d4349252863c04d6f1729
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 11 23:04:08 2015 -0500

    net: Kill hold_net release_net
    
    hold_net and release_net were an idea that turned out to be useless.
    The code has been disabled since 2008.  Kill the code it is long past due.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d996c633bec2..95b2c1c220f9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2204,7 +2204,7 @@ static inline void sk_change_net(struct sock *sk, struct net *net)
 
 	if (!net_eq(current_net, net)) {
 		put_net(current_net);
-		sock_net_set(sk, hold_net(net));
+		sock_net_set(sk, net);
 	}
 }
 

commit 33cf7c90fe2f97afb1cadaa0cfb782cb9d1b9ee2
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 11 18:53:14 2015 -0700

    net: add real socket cookies
    
    A long standing problem in netlink socket dumps is the use
    of kernel socket addresses as cookies.
    
    1) It is a security concern.
    
    2) Sockets can be reused quite quickly, so there is
       no guarantee a cookie is used once and identify
       a flow.
    
    3) request sock, establish sock, and timewait socks
       for a given flow have different cookies.
    
    Part of our effort to bring better TCP statistics requires
    to switch to a different allocator.
    
    In this patch, I chose to use a per network namespace 64bit generator,
    and to use it only in the case a socket needs to be dumped to netlink.
    (This might be refined later if needed)
    
    Note that I tried to carry cookies from request sock, to establish sock,
    then timewait sockets.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Eric Salo <salo@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 250822cc1e02..d996c633bec2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -199,6 +199,8 @@ struct sock_common {
 	struct in6_addr		skc_v6_rcv_saddr;
 #endif
 
+	atomic64_t		skc_cookie;
+
 	/*
 	 * fields between dontcopy_begin/dontcopy_end
 	 * are not copied in sock_copy()
@@ -329,6 +331,7 @@ struct sock {
 #define sk_net			__sk_common.skc_net
 #define sk_v6_daddr		__sk_common.skc_v6_daddr
 #define sk_v6_rcv_saddr	__sk_common.skc_v6_rcv_saddr
+#define sk_cookie		__sk_common.skc_cookie
 
 	socket_lock_t		sk_lock;
 	struct sk_buff_head	sk_receive_queue;

commit 1b784140474e4fc94281a49e96c67d29df0efbde
Author: Ying Xue <ying.xue@windriver.com>
Date:   Mon Mar 2 15:37:48 2015 +0800

    net: Remove iocb argument from sendmsg and recvmsg
    
    After TIPC doesn't depend on iocb argument in its internal
    implementations of sendmsg() and recvmsg() hooks defined in proto
    structure, no any user is using iocb argument in them at all now.
    Then we can drop the redundant iocb argument completely from kinds of
    implementations of both sendmsg() and recvmsg() in the entire
    networking stack.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Suggested-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Ying Xue <ying.xue@windriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 38369d3580a1..250822cc1e02 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -958,10 +958,9 @@ struct proto {
 	int			(*compat_ioctl)(struct sock *sk,
 					unsigned int cmd, unsigned long arg);
 #endif
-	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
-					   struct msghdr *msg, size_t len);
-	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
-					   struct msghdr *msg,
+	int			(*sendmsg)(struct sock *sk, struct msghdr *msg,
+					   size_t len);
+	int			(*recvmsg)(struct sock *sk, struct msghdr *msg,
 					   size_t len, int noblock, int flags,
 					   int *addr_len);
 	int			(*sendpage)(struct sock *sk, struct page *page,
@@ -1562,9 +1561,8 @@ int sock_no_listen(struct socket *, int);
 int sock_no_shutdown(struct socket *, int);
 int sock_no_getsockopt(struct socket *, int , int, char __user *, int __user *);
 int sock_no_setsockopt(struct socket *, int, int, char __user *, unsigned int);
-int sock_no_sendmsg(struct kiocb *, struct socket *, struct msghdr *, size_t);
-int sock_no_recvmsg(struct kiocb *, struct socket *, struct msghdr *, size_t,
-		    int);
+int sock_no_sendmsg(struct socket *, struct msghdr *, size_t);
+int sock_no_recvmsg(struct socket *, struct msghdr *, size_t, int);
 int sock_no_mmap(struct file *file, struct socket *sock,
 		 struct vm_area_struct *vma);
 ssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset,
@@ -1576,8 +1574,8 @@ ssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset,
  */
 int sock_common_getsockopt(struct socket *sock, int level, int optname,
 				  char __user *optval, int __user *optlen);
-int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
-			       struct msghdr *msg, size_t size, int flags);
+int sock_common_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
+			int flags);
 int sock_common_setsockopt(struct socket *sock, int level, int optname,
 				  char __user *optval, unsigned int optlen);
 int compat_sock_common_getsockopt(struct socket *sock, int level,

commit 744d5a3e9fe2690dd85d9991dbb078301694658b
Author: Eyal Birger <eyal.birger@gmail.com>
Date:   Sun Mar 1 14:58:31 2015 +0200

    net: move skb->dropcount to skb->cb[]
    
    Commit 977750076d98 ("af_packet: add interframe drop cmsg (v6)")
    unionized skb->mark and skb->dropcount in order to allow recording
    of the socket drop count while maintaining struct sk_buff size.
    
    skb->dropcount was introduced since there was no available room
    in skb->cb[] in packet sockets. However, its introduction led to
    the inability to export skb->mark, or any other aliased field to
    userspace if so desired.
    
    Moving the dropcount metric to skb->cb[] eliminates this problem
    at the expense of 4 bytes less in skb->cb[] for protocol families
    using it.
    
    Signed-off-by: Eyal Birger <eyal.birger@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0996fe451e5f..38369d3580a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2078,13 +2078,27 @@ static inline int sock_intr_errno(long timeo)
 	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
 }
 
+struct sock_skb_cb {
+	u32 dropcount;
+};
+
+/* Store sock_skb_cb at the end of skb->cb[] so protocol families
+ * using skb->cb[] would keep using it directly and utilize its
+ * alignement guarantee.
+ */
+#define SOCK_SKB_CB_OFFSET ((FIELD_SIZEOF(struct sk_buff, cb) - \
+			    sizeof(struct sock_skb_cb)))
+
+#define SOCK_SKB_CB(__skb) ((struct sock_skb_cb *)((__skb)->cb + \
+			    SOCK_SKB_CB_OFFSET))
+
 #define sock_skb_cb_check_size(size) \
-	BUILD_BUG_ON((size) > FIELD_SIZEOF(struct sk_buff, cb))
+	BUILD_BUG_ON((size) > SOCK_SKB_CB_OFFSET)
 
 static inline void
 sock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)
 {
-	skb->dropcount = atomic_read(&sk->sk_drops);
+	SOCK_SKB_CB(skb)->dropcount = atomic_read(&sk->sk_drops);
 }
 
 void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,

commit 3bc3b96f3b455bd14a8ccd83ffffc85625aba641
Author: Eyal Birger <eyal.birger@gmail.com>
Date:   Sun Mar 1 14:58:30 2015 +0200

    net: add common accessor for setting dropcount on packets
    
    As part of an effort to move skb->dropcount to skb->cb[], use
    a common function in order to set dropcount in struct sk_buff.
    
    Signed-off-by: Eyal Birger <eyal.birger@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a2502d248641..0996fe451e5f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2081,6 +2081,12 @@ static inline int sock_intr_errno(long timeo)
 #define sock_skb_cb_check_size(size) \
 	BUILD_BUG_ON((size) > FIELD_SIZEOF(struct sk_buff, cb))
 
+static inline void
+sock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)
+{
+	skb->dropcount = atomic_read(&sk->sk_drops);
+}
+
 void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 			   struct sk_buff *skb);
 void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,

commit b4772ef879a8f7d8c56118c2ae5a296fcf6f81d2
Author: Eyal Birger <eyal.birger@gmail.com>
Date:   Sun Mar 1 14:58:29 2015 +0200

    net: use common macro for assering skb->cb[] available size in protocol families
    
    As part of an effort to move skb->dropcount to skb->cb[] use a common
    macro in protocol families using skb->cb[] for ancillary data to
    validate available room in skb->cb[].
    
    Signed-off-by: Eyal Birger <eyal.birger@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ab186b1d31ff..a2502d248641 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2078,6 +2078,9 @@ static inline int sock_intr_errno(long timeo)
 	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
 }
 
+#define sock_skb_cb_check_size(size) \
+	BUILD_BUG_ON((size) > FIELD_SIZEOF(struct sk_buff, cb))
+
 void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 			   struct sk_buff *skb);
 void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,

commit f48b80a5e22200347e91f96b8b237b24b93c7192
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Thu Feb 12 14:59:56 2015 -0800

    memcg: cleanup static keys decrement
    
    Move memcg_socket_limit_enabled decrement to tcp_destroy_cgroup (called
    from memcg_destroy_kmem -> mem_cgroup_sockets_destroy) and zap a bunch of
    wrapper functions.
    
    Although this patch moves static keys decrement from __mem_cgroup_free to
    mem_cgroup_css_free, it does not introduce any functional changes, because
    the keys are incremented on setting the limit (tcp or kmem), which can
    only happen after successful mem_cgroup_css_online.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtisu.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index e13824570b0f..ab186b1d31ff 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1077,11 +1077,6 @@ static inline bool memcg_proto_active(struct cg_proto *cg_proto)
 	return test_bit(MEMCG_SOCK_ACTIVE, &cg_proto->flags);
 }
 
-static inline bool memcg_proto_activated(struct cg_proto *cg_proto)
-{
-	return test_bit(MEMCG_SOCK_ACTIVATED, &cg_proto->flags);
-}
-
 #ifdef SOCK_REFCNT_DEBUG
 static inline void sk_refcnt_debug_inc(struct sock *sk)
 {

commit 567e4b79731c352a17d73c483959f795d3593e03
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 6 12:59:01 2015 -0800

    net: rfs: add hash collision detection
    
    Receive Flow Steering is a nice solution but suffers from
    hash collisions when a mix of connected and unconnected traffic
    is received on the host, when flow hash table is populated.
    
    Also, clearing flow in inet_release() makes RFS not very good
    for short lived flows, as many packets can follow close().
    (FIN , ACK packets, ...)
    
    This patch extends the information stored into global hash table
    to not only include cpu number, but upper part of the hash value.
    
    I use a 32bit value, and dynamically split it in two parts.
    
    For host with less than 64 possible cpus, this gives 6 bits for the
    cpu number, and 26 (32-6) bits for the upper part of the hash.
    
    Since hash bucket selection use low order bits of the hash, we have
    a full hash match, if /proc/sys/net/core/rps_sock_flow_entries is big
    enough.
    
    If the hash found in flow table does not match, we fallback to RPS (if
    it is enabled for the rxqueue).
    
    This means that a packet for an non connected flow can avoid the
    IPI through a unrelated/victim CPU.
    
    This also means we no longer have to clear the table at socket
    close time, and this helps short lived flows performance.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d28b8fededd6..e13824570b0f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -857,18 +857,6 @@ static inline void sock_rps_record_flow_hash(__u32 hash)
 #endif
 }
 
-static inline void sock_rps_reset_flow_hash(__u32 hash)
-{
-#ifdef CONFIG_RPS
-	struct rps_sock_flow_table *sock_flow_table;
-
-	rcu_read_lock();
-	sock_flow_table = rcu_dereference(rps_sock_flow_table);
-	rps_reset_sock_flow(sock_flow_table, hash);
-	rcu_read_unlock();
-#endif
-}
-
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
 #ifdef CONFIG_RPS
@@ -876,28 +864,18 @@ static inline void sock_rps_record_flow(const struct sock *sk)
 #endif
 }
 
-static inline void sock_rps_reset_flow(const struct sock *sk)
-{
-#ifdef CONFIG_RPS
-	sock_rps_reset_flow_hash(sk->sk_rxhash);
-#endif
-}
-
 static inline void sock_rps_save_rxhash(struct sock *sk,
 					const struct sk_buff *skb)
 {
 #ifdef CONFIG_RPS
-	if (unlikely(sk->sk_rxhash != skb->hash)) {
-		sock_rps_reset_flow(sk);
+	if (unlikely(sk->sk_rxhash != skb->hash))
 		sk->sk_rxhash = skb->hash;
-	}
 #endif
 }
 
 static inline void sock_rps_reset_rxhash(struct sock *sk)
 {
 #ifdef CONFIG_RPS
-	sock_rps_reset_flow(sk);
 	sk->sk_rxhash = 0;
 #endif
 }

commit f2683b743f2334ef49a5361bf596dd1fbd2c9be4
Merge: 987819657828 57dd8a0735aa
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Feb 4 20:46:55 2015 -0800

    Merge branch 'for-davem' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    More iov_iter work from Al Viro.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 57be5bdad759b9dde8b0d0cc630782a1a4ac4b9f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Nov 28 13:40:20 2014 -0500

    ip: convert tcp_sendmsg() to iov_iter primitives
    
    patch is actually smaller than it seems to be - most of it is unindenting
    the inner loop body in tcp_sendmsg() itself...
    
    the bit in tcp_input.c is going to get reverted very soon - that's what
    memcpy_from_msg() will become, but not in this commit; let's keep it
    reasonably contained...
    
    There's one potentially subtle change here: in case of short copy from
    userland, mainline tcp_send_syn_data() discards the skb it has allocated
    and falls back to normal path, where we'll send as much as possible after
    rereading the same data again.  This patch trims SYN+data skb instead -
    that way we don't need to copy from the same place twice.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index 15341499786c..1e45e599a3ab 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1803,27 +1803,25 @@ static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
 }
 
 static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
-					   char __user *from, char *to,
+					   struct iov_iter *from, char *to,
 					   int copy, int offset)
 {
 	if (skb->ip_summed == CHECKSUM_NONE) {
-		int err = 0;
-		__wsum csum = csum_and_copy_from_user(from, to, copy, 0, &err);
-		if (err)
-			return err;
+		__wsum csum = 0;
+		if (csum_and_copy_from_iter(to, copy, &csum, from) != copy)
+			return -EFAULT;
 		skb->csum = csum_block_add(skb->csum, csum, offset);
 	} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {
-		if (!access_ok(VERIFY_READ, from, copy) ||
-		    __copy_from_user_nocache(to, from, copy))
+		if (copy_from_iter_nocache(to, copy, from) != copy)
 			return -EFAULT;
-	} else if (copy_from_user(to, from, copy))
+	} else if (copy_from_iter(to, copy, from) != copy)
 		return -EFAULT;
 
 	return 0;
 }
 
 static inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,
-				       char __user *from, int copy)
+				       struct iov_iter *from, int copy)
 {
 	int err, offset = skb->len;
 
@@ -1835,7 +1833,7 @@ static inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,
 	return err;
 }
 
-static inline int skb_copy_to_page_nocache(struct sock *sk, char __user *from,
+static inline int skb_copy_to_page_nocache(struct sock *sk, struct iov_iter *from,
 					   struct sk_buff *skb,
 					   struct page *page,
 					   int off, int copy)

commit b245be1f4db1a0394e4b6eb66059814b46670ac3
Author: Willem de Bruijn <willemb@google.com>
Date:   Fri Jan 30 13:29:32 2015 -0500

    net-timestamp: no-payload only sysctl
    
    Tx timestamps are looped onto the error queue on top of an skb. This
    mechanism leaks packet headers to processes unless the no-payload
    options SOF_TIMESTAMPING_OPT_TSONLY is set.
    
    Add a sysctl that optionally drops looped timestamp with data. This
    only affects processes without CAP_NET_RAW.
    
    The policy is checked when timestamps are generated in the stack.
    It is possible for timestamps with data to be reported after the
    sysctl is set, if these were queued internally earlier.
    
    No vulnerability is immediately known that exploits knowledge
    gleaned from packet headers, but it may still be preferable to allow
    administrators to lock down this path at the cost of possible
    breakage of legacy applications.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    
    ----
    
    Changes
      (v1 -> v2)
      - test socket CAP_NET_RAW instead of capable(CAP_NET_RAW)
      (rfc -> v1)
      - document the sysctl in Documentation/sysctl/net.txt
      - fix access control race: read .._OPT_TSONLY only once,
            use same value for permission check and skb generation.
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 15341499786c..511ef7c8889b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2239,6 +2239,7 @@ bool sk_net_capable(const struct sock *sk, int cap);
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 
+extern int sysctl_tstamp_allow_data;
 extern int sysctl_optmem_max;
 
 extern __u32 sysctl_wmem_default;

commit 7cc05662682da4b0e0a4fdf3c3f190577803ae81
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 28 18:04:53 2015 +0100

    net: remove sock_iocb
    
    The sock_iocb structure is allocate on stack for each read/write-like
    operation on sockets, and contains various fields of which only the
    embedded msghdr and sometimes a pointer to the scm_cookie is ever used.
    Get rid of the sock_iocb and put a msghdr directly on the stack and pass
    the scm_cookie explicitly to netlink_mmap_sendmsg.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2210fec65669..15341499786c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1374,29 +1374,6 @@ void sk_prot_clear_portaddr_nulls(struct sock *sk, int size);
 #define SOCK_BINDADDR_LOCK	4
 #define SOCK_BINDPORT_LOCK	8
 
-/* sock_iocb: used to kick off async processing of socket ios */
-struct sock_iocb {
-	struct list_head	list;
-
-	int			flags;
-	int			size;
-	struct socket		*sock;
-	struct sock		*sk;
-	struct scm_cookie	*scm;
-	struct msghdr		*msg, async_msg;
-	struct kiocb		*kiocb;
-};
-
-static inline struct sock_iocb *kiocb_to_siocb(struct kiocb *iocb)
-{
-	return (struct sock_iocb *)iocb->private;
-}
-
-static inline struct kiocb *siocb_to_kiocb(struct sock_iocb *si)
-{
-	return si->kiocb;
-}
-
 struct socket_alloc {
 	struct socket socket;
 	struct inode vfs_inode;

commit e3aa91a7cb21a595169b20c64f63ca39a91a0c43
Merge: 78a45c6f0678 8606813a6c89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 13 13:33:26 2014 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto update from Herbert Xu:
     - The crypto API is now documented :)
     - Disallow arbitrary module loading through crypto API.
     - Allow get request with empty driver name through crypto_user.
     - Allow speed testing of arbitrary hash functions.
     - Add caam support for ctr(aes), gcm(aes) and their derivatives.
     - nx now supports concurrent hashing properly.
     - Add sahara support for SHA1/256.
     - Add ARM64 version of CRC32.
     - Misc fixes.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (77 commits)
      crypto: tcrypt - Allow speed testing of arbitrary hash functions
      crypto: af_alg - add user space interface for AEAD
      crypto: qat - fix problem with coalescing enable logic
      crypto: sahara - add support for SHA1/256
      crypto: sahara - replace tasklets with kthread
      crypto: sahara - add support for i.MX53
      crypto: sahara - fix spinlock initialization
      crypto: arm - replace memset by memzero_explicit
      crypto: powerpc - replace memset by memzero_explicit
      crypto: sha - replace memset by memzero_explicit
      crypto: sparc - replace memset by memzero_explicit
      crypto: algif_skcipher - initialize upon init request
      crypto: algif_skcipher - removed unneeded code
      crypto: algif_skcipher - Fixed blocking recvmsg
      crypto: drbg - use memzero_explicit() for clearing sensitive data
      crypto: drbg - use MODULE_ALIAS_CRYPTO
      crypto: include crypto- module prefix in template
      crypto: user - add MODULE_ALIAS
      crypto: sha-mb - remove a bogus NULL check
      crytpo: qat - Fix 64 bytes requests
      ...

commit 70e71ca0af244f48a5dcf56dc435243792e3a495
Merge: bae41e45b740 00c83b01d580
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 11 14:27:06 2014 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) New offloading infrastructure and example 'rocker' driver for
        offloading of switching and routing to hardware.
    
        This work was done by a large group of dedicated individuals, not
        limited to: Scott Feldman, Jiri Pirko, Thomas Graf, John Fastabend,
        Jamal Hadi Salim, Andy Gospodarek, Florian Fainelli, Roopa Prabhu
    
     2) Start making the networking operate on IOV iterators instead of
        modifying iov objects in-situ during transfers.  Thanks to Al Viro
        and Herbert Xu.
    
     3) A set of new netlink interfaces for the TIPC stack, from Richard
        Alpe.
    
     4) Remove unnecessary looping during ipv6 routing lookups, from Martin
        KaFai Lau.
    
     5) Add PAUSE frame generation support to gianfar driver, from Matei
        Pavaluca.
    
     6) Allow for larger reordering levels in TCP, which are easily
        achievable in the real world right now, from Eric Dumazet.
    
     7) Add a variable of napi_schedule that doesn't need to disable cpu
        interrupts, from Eric Dumazet.
    
     8) Use a doubly linked list to optimize neigh_parms_release(), from
        Nicolas Dichtel.
    
     9) Various enhancements to the kernel BPF verifier, and allow eBPF
        programs to actually be attached to sockets.  From Alexei
        Starovoitov.
    
    10) Support TSO/LSO in sunvnet driver, from David L Stevens.
    
    11) Allow controlling ECN usage via routing metrics, from Florian
        Westphal.
    
    12) Remote checksum offload, from Tom Herbert.
    
    13) Add split-header receive, BQL, and xmit_more support to amd-xgbe
        driver, from Thomas Lendacky.
    
    14) Add MPLS support to openvswitch, from Simon Horman.
    
    15) Support wildcard tunnel endpoints in ipv6 tunnels, from Steffen
        Klassert.
    
    16) Do gro flushes on a per-device basis using a timer, from Eric
        Dumazet.  This tries to resolve the conflicting goals between the
        desired handling of bulk vs.  RPC-like traffic.
    
    17) Allow userspace to ask for the CPU upon what a packet was
        received/steered, via SO_INCOMING_CPU.  From Eric Dumazet.
    
    18) Limit GSO packets to half the current congestion window, from Eric
        Dumazet.
    
    19) Add a generic helper so that all drivers set their RSS keys in a
        consistent way, from Eric Dumazet.
    
    20) Add xmit_more support to enic driver, from Govindarajulu
        Varadarajan.
    
    21) Add VLAN packet scheduler action, from Jiri Pirko.
    
    22) Support configurable RSS hash functions via ethtool, from Eyal
        Perry.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1820 commits)
      Fix race condition between vxlan_sock_add and vxlan_sock_release
      net/macb: fix compilation warning for print_hex_dump() called with skb->mac_header
      net/mlx4: Add support for A0 steering
      net/mlx4: Refactor QUERY_PORT
      net/mlx4_core: Add explicit error message when rule doesn't meet configuration
      net/mlx4: Add A0 hybrid steering
      net/mlx4: Add mlx4_bitmap zone allocator
      net/mlx4: Add a check if there are too many reserved QPs
      net/mlx4: Change QP allocation scheme
      net/mlx4_core: Use tasklet for user-space CQ completion events
      net/mlx4_core: Mask out host side virtualization features for guests
      net/mlx4_en: Set csum level for encapsulated packets
      be2net: Export tunnel offloads only when a VxLAN tunnel is created
      gianfar: Fix dma check map error when DMA_API_DEBUG is enabled
      cxgb4/csiostor: Don't use MASTER_MUST for fw_hello call
      net: fec: only enable mdio interrupt before phy device link up
      net: fec: clear all interrupt events to support i.MX6SX
      net: fec: reset fep link status in suspend function
      net: sock: fix access via invalid file descriptor
      net: introduce helper macro for_each_cmsghdr
      ...

commit 3e32cb2e0a12b6915056ff04601cf1bb9b44f967
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Dec 10 15:42:31 2014 -0800

    mm: memcontrol: lockless page counters
    
    Memory is internally accounted in bytes, using spinlock-protected 64-bit
    counters, even though the smallest accounting delta is a page.  The
    counter interface is also convoluted and does too many things.
    
    Introduce a new lockless word-sized page counter API, then change all
    memory accounting over to it.  The translation from and to bytes then only
    happens when interfacing with userspace.
    
    The removed locking overhead is noticable when scaling beyond the per-cpu
    charge caches - on a 4-socket machine with 144-threads, the following test
    shows the performance differences of 288 memcgs concurrently running a
    page fault benchmark:
    
    vanilla:
    
       18631648.500498      task-clock (msec)         #  140.643 CPUs utilized            ( +-  0.33% )
             1,380,638      context-switches          #    0.074 K/sec                    ( +-  0.75% )
                24,390      cpu-migrations            #    0.001 K/sec                    ( +-  8.44% )
         1,843,305,768      page-faults               #    0.099 M/sec                    ( +-  0.00% )
    50,134,994,088,218      cycles                    #    2.691 GHz                      ( +-  0.33% )
       <not supported>      stalled-cycles-frontend
       <not supported>      stalled-cycles-backend
     8,049,712,224,651      instructions              #    0.16  insns per cycle          ( +-  0.04% )
     1,586,970,584,979      branches                  #   85.176 M/sec                    ( +-  0.05% )
         1,724,989,949      branch-misses             #    0.11% of all branches          ( +-  0.48% )
    
         132.474343877 seconds time elapsed                                          ( +-  0.21% )
    
    lockless:
    
       12195979.037525      task-clock (msec)         #  133.480 CPUs utilized            ( +-  0.18% )
               832,850      context-switches          #    0.068 K/sec                    ( +-  0.54% )
                15,624      cpu-migrations            #    0.001 K/sec                    ( +- 10.17% )
         1,843,304,774      page-faults               #    0.151 M/sec                    ( +-  0.00% )
    32,811,216,801,141      cycles                    #    2.690 GHz                      ( +-  0.18% )
       <not supported>      stalled-cycles-frontend
       <not supported>      stalled-cycles-backend
     9,999,265,091,727      instructions              #    0.30  insns per cycle          ( +-  0.10% )
     2,076,759,325,203      branches                  #  170.282 M/sec                    ( +-  0.12% )
         1,656,917,214      branch-misses             #    0.08% of all branches          ( +-  0.55% )
    
          91.369330729 seconds time elapsed                                          ( +-  0.45% )
    
    On top of improved scalability, this also gets rid of the icky long long
    types in the very heart of memcg, which is great for 32 bit and also makes
    the code a lot more readable.
    
    Notable differences between the old and new API:
    
    - res_counter_charge() and res_counter_charge_nofail() become
      page_counter_try_charge() and page_counter_charge() resp. to match
      the more common kernel naming scheme of try_do()/do()
    
    - res_counter_uncharge_until() is only ever used to cancel a local
      counter and never to uncharge bigger segments of a hierarchy, so
      it's replaced by the simpler page_counter_cancel()
    
    - res_counter_set_limit() is replaced by page_counter_limit(), which
      expects its callers to serialize against themselves
    
    - res_counter_memparse_write_strategy() is replaced by
      page_counter_limit(), which rounds down to the nearest page size -
      rather than up.  This is more reasonable for explicitely requested
      hard upper limits.
    
    - to keep charging light-weight, page_counter_try_charge() charges
      speculatively, only to roll back if the result exceeds the limit.
      Because of this, a failing bigger charge can temporarily lock out
      smaller charges that would otherwise succeed.  The error is bounded
      to the difference between the smallest and the biggest possible
      charge size, so for memcg, this means that a failing THP charge can
      send base page charges into reclaim upto 2MB (4MB) before the limit
      would have been reached.  This should be acceptable.
    
    [akpm@linux-foundation.org: add includes for WARN_ON_ONCE and memparse]
    [akpm@linux-foundation.org: add includes for WARN_ON_ONCE, memparse, strncmp, and PAGE_SIZE]
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index e6f235ebf6c9..7ff44e062a38 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -54,8 +54,8 @@
 #include <linux/security.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
+#include <linux/page_counter.h>
 #include <linux/memcontrol.h>
-#include <linux/res_counter.h>
 #include <linux/static_key.h>
 #include <linux/aio.h>
 #include <linux/sched.h>
@@ -1062,7 +1062,7 @@ enum cg_proto_flags {
 };
 
 struct cg_proto {
-	struct res_counter	memory_allocated;	/* Current allocated memory. */
+	struct page_counter	memory_allocated;	/* Current allocated memory. */
 	struct percpu_counter	sockets_allocated;	/* Current number of sockets. */
 	int			memory_pressure;
 	long			sysctl_mem[3];
@@ -1214,34 +1214,26 @@ static inline void memcg_memory_allocated_add(struct cg_proto *prot,
 					      unsigned long amt,
 					      int *parent_status)
 {
-	struct res_counter *fail;
-	int ret;
+	page_counter_charge(&prot->memory_allocated, amt);
 
-	ret = res_counter_charge_nofail(&prot->memory_allocated,
-					amt << PAGE_SHIFT, &fail);
-	if (ret < 0)
+	if (page_counter_read(&prot->memory_allocated) >
+	    prot->memory_allocated.limit)
 		*parent_status = OVER_LIMIT;
 }
 
 static inline void memcg_memory_allocated_sub(struct cg_proto *prot,
 					      unsigned long amt)
 {
-	res_counter_uncharge(&prot->memory_allocated, amt << PAGE_SHIFT);
-}
-
-static inline u64 memcg_memory_allocated_read(struct cg_proto *prot)
-{
-	u64 ret;
-	ret = res_counter_read_u64(&prot->memory_allocated, RES_USAGE);
-	return ret >> PAGE_SHIFT;
+	page_counter_uncharge(&prot->memory_allocated, amt);
 }
 
 static inline long
 sk_memory_allocated(const struct sock *sk)
 {
 	struct proto *prot = sk->sk_prot;
+
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return memcg_memory_allocated_read(sk->sk_cgrp);
+		return page_counter_read(&sk->sk_cgrp->memory_allocated);
 
 	return atomic_long_read(prot->memory_allocated);
 }
@@ -1255,7 +1247,7 @@ sk_memory_allocated_add(struct sock *sk, int amt, int *parent_status)
 		memcg_memory_allocated_add(sk->sk_cgrp, amt, parent_status);
 		/* update the root cgroup regardless */
 		atomic_long_add_return(amt, prot->memory_allocated);
-		return memcg_memory_allocated_read(sk->sk_cgrp);
+		return page_counter_read(&sk->sk_cgrp->memory_allocated);
 	}
 
 	return atomic_long_add_return(amt, prot->memory_allocated);

commit 79e886599e6416d0de26e8562e4464577d081c3d
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Wed Nov 19 17:13:11 2014 +0100

    crypto: algif - add and use sock_kzfree_s() instead of memzero_explicit()
    
    Commit e1bd95bf7c25 ("crypto: algif - zeroize IV buffer") and
    2a6af25befd0 ("crypto: algif - zeroize message digest buffer")
    added memzero_explicit() calls on buffers that are later on
    passed back to sock_kfree_s().
    
    This is a discussed follow-up that, instead, extends the sock
    API and adds sock_kzfree_s(), which internally uses kzfree()
    instead of kfree() for passing the buffers back to slab.
    
    Having sock_kzfree_s() allows to keep the changes more minimal
    by just having a drop-in replacement instead of adding
    memzero_explicit() calls everywhere before sock_kfree_s().
    
    In kzfree(), the compiler is not allowed to optimize the memset()
    away and thus there's no need for memzero_explicit(). Both,
    sock_kfree_s() and sock_kzfree_s() are wrappers for
    __sock_kfree_s() and call into kfree() resp. kzfree(); here,
    __sock_kfree_s() needs to be explicitly inlined as we want the
    compiler to optimize the call and condition away and thus it
    produces e.g. on x86_64 the _same_ assembler output for
    sock_kfree_s() before and after, and thus also allows for
    avoiding code duplication.
    
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7db3db112baa..37d6cc5dcf33 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1588,6 +1588,7 @@ struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
 				     int *errcode, int max_page_order);
 void *sock_kmalloc(struct sock *sk, int size, gfp_t priority);
 void sock_kfree_s(struct sock *sk, void *mem, int size);
+void sock_kzfree_s(struct sock *sk, void *mem, int size);
 void sk_send_sigurg(struct sock *sk);
 
 /*

commit 232365f660b0016dcf618723707f91f4a95013db
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Nov 10 17:30:00 2014 -0500

    bury skb_copy_to_page()
    
    no callers since 3.0
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index 83a669f83bae..df9b89bce8ff 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1884,29 +1884,6 @@ static inline int skb_copy_to_page_nocache(struct sock *sk, char __user *from,
 	return 0;
 }
 
-static inline int skb_copy_to_page(struct sock *sk, char __user *from,
-				   struct sk_buff *skb, struct page *page,
-				   int off, int copy)
-{
-	if (skb->ip_summed == CHECKSUM_NONE) {
-		int err = 0;
-		__wsum csum = csum_and_copy_from_user(from,
-						     page_address(page) + off,
-							    copy, 0, &err);
-		if (err)
-			return err;
-		skb->csum = csum_block_add(skb->csum, csum, skb->len);
-	} else if (copy_from_user(page_address(page) + off, from, copy))
-		return -EFAULT;
-
-	skb->len	     += copy;
-	skb->data_len	     += copy;
-	skb->truesize	     += copy;
-	sk->sk_wmem_queued   += copy;
-	sk_mem_charge(sk, copy);
-	return 0;
-}
-
 /**
  * sk_wmem_alloc_get - returns write allocations
  * @sk: socket

commit ba7a46f16dd29f93303daeb1fee8af316c5a07f4
Author: Joe Perches <joe@perches.com>
Date:   Tue Nov 11 10:59:17 2014 -0800

    net: Convert LIMIT_NETDEBUG to net_dbg_ratelimited
    
    Use the more common dynamic_debug capable net_dbg_ratelimited
    and remove the LIMIT_NETDEBUG macro.
    
    All messages are still ratelimited.
    
    Some KERN_<LEVEL> uses are changed to KERN_DEBUG.
    
    This may have some negative impact on messages that were
    emitted at KERN_INFO that are not not enabled at all unless
    DEBUG is defined or dynamic_debug is enabled.  Even so,
    these messages are now _not_ emitted by default.
    
    This also eliminates the use of the net_msg_warn sysctl
    "/proc/sys/net/core/warnings".  For backward compatibility,
    the sysctl is not removed, but it has no function.  The extern
    declaration of net_msg_warn is removed from sock.h and made
    static in net/core/sysctl_net_core.c
    
    Miscellanea:
    
    o Update the sysctl documentation
    o Remove the embedded uses of pr_fmt
    o Coalesce format fragments
    o Realign arguments
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7789b59c0c40..83a669f83bae 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2288,13 +2288,6 @@ bool sk_ns_capable(const struct sock *sk,
 bool sk_capable(const struct sock *sk, int cap);
 bool sk_net_capable(const struct sock *sk, int cap);
 
-/*
- *	Enable debug/info messages
- */
-extern int net_msg_warn;
-#define LIMIT_NETDEBUG(fmt, args...) \
-	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
-
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 

commit 2c8c56e15df3d4c2af3d656e44feb18789f75837
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Nov 11 05:54:28 2014 -0800

    net: introduce SO_INCOMING_CPU
    
    Alternative to RPS/RFS is to use hardware support for multiple
    queues.
    
    Then split a set of million of sockets into worker threads, each
    one using epoll() to manage events on its own socket pool.
    
    Ideally, we want one thread per RX/TX queue/cpu, but we have no way to
    know after accept() or connect() on which queue/cpu a socket is managed.
    
    We normally use one cpu per RX queue (IRQ smp_affinity being properly
    set), so remembering on socket structure which cpu delivered last packet
    is enough to solve the problem.
    
    After accept(), connect(), or even file descriptor passing around
    processes, applications can use :
    
     int cpu;
     socklen_t len = sizeof(cpu);
    
     getsockopt(fd, SOL_SOCKET, SO_INCOMING_CPU, &cpu, &len);
    
    And use this information to put the socket into the right silo
    for optimal performance, as all networking stack should run
    on the appropriate cpu, without need to send IPI (RPS/RFS).
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6767d75ecb17..7789b59c0c40 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -273,6 +273,7 @@ struct cg_proto;
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
   *	@sk_rxhash: flow hash received from netif layer
+  *	@sk_incoming_cpu: record cpu processing incoming packets
   *	@sk_txhash: computed flow hash for use on transmit
   *	@sk_filter: socket filtering instructions
   *	@sk_protinfo: private area, net family specific, when not using slab
@@ -350,6 +351,12 @@ struct sock {
 #ifdef CONFIG_RPS
 	__u32			sk_rxhash;
 #endif
+	u16			sk_incoming_cpu;
+	/* 16bit hole
+	 * Warned : sk_incoming_cpu can be set from softirq,
+	 * Do not use this hole without fully understanding possible issues.
+	 */
+
 	__u32			sk_txhash;
 #ifdef CONFIG_NET_RX_BUSY_POLL
 	unsigned int		sk_napi_id;
@@ -833,6 +840,11 @@ static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 	return sk->sk_backlog_rcv(sk, skb);
 }
 
+static inline void sk_incoming_cpu_update(struct sock *sk)
+{
+	sk->sk_incoming_cpu = raw_smp_processor_id();
+}
+
 static inline void sock_rps_record_flow_hash(__u32 hash)
 {
 #ifdef CONFIG_RPS

commit 926c512685ddd8f26f1c789218391530ccd54a35
Author: Joe Perches <joe@perches.com>
Date:   Wed Nov 5 15:42:09 2014 -0800

    sock.h: Remove unused NETDEBUG macro
    
    It's unused now, just delete it.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7db3db112baa..6767d75ecb17 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2280,9 +2280,6 @@ bool sk_net_capable(const struct sock *sk, int cap);
  *	Enable debug/info messages
  */
 extern int net_msg_warn;
-#define NETDEBUG(fmt, args...) \
-	do { if (net_msg_warn) printk(fmt,##args); } while (0)
-
 #define LIMIT_NETDEBUG(fmt, args...) \
 	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
 

commit 26cabd31259ba43f68026ce3f62b78094124333f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 10:18:54 2014 +0200

    sched, net: Clean up sk_wait_event() vs. might_sleep()
    
    WARNING: CPU: 1 PID: 1744 at kernel/sched/core.c:7104 __might_sleep+0x58/0x90()
    do not call blocking ops when !TASK_RUNNING; state=1 set at [<ffffffff81070e10>] prepare_to_wait+0x50 /0xa0
    
     [<ffffffff8105bc38>] __might_sleep+0x58/0x90
     [<ffffffff8148c671>] lock_sock_nested+0x31/0xb0
     [<ffffffff81498aaa>] sk_stream_wait_memory+0x18a/0x2d0
    
    Which is a false positive because sk_wait_event() will already have
    TASK_RUNNING at that point if it would've gone through
    schedule_timeout().
    
    So annotate with sched_annotate_sleep(); which goes away on !DEBUG builds.
    
    Reported-by: Ilya Dryomov <ilya.dryomov@inktank.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140924082242.524407432@infradead.org
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: netdev@vger.kernel.org
    Cc: tglx@linutronix.de
    Cc: ilya.dryomov@inktank.com
    Cc: umgwanakikbuti@gmail.com
    Cc: oleg@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7db3db112baa..e6f235ebf6c9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -897,6 +897,7 @@ static inline void sock_rps_reset_rxhash(struct sock *sk)
 		if (!__rc) {						\
 			*(__timeo) = schedule_timeout(*(__timeo));	\
 		}							\
+		sched_annotate_sleep();						\
 		lock_sock(__sk);					\
 		__rc = __condition;					\
 		__rc;							\

commit 35a9ad8af0bb0fa3525e6d0d20e32551d226f38e
Merge: d5935b07da53 64b1f00a0830
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 21:40:54 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Most notable changes in here:
    
       1) By far the biggest accomplishment, thanks to a large range of
          contributors, is the addition of multi-send for transmit.  This is
          the result of discussions back in Chicago, and the hard work of
          several individuals.
    
          Now, when the ->ndo_start_xmit() method of a driver sees
          skb->xmit_more as true, it can choose to defer the doorbell
          telling the driver to start processing the new TX queue entires.
    
          skb->xmit_more means that the generic networking is guaranteed to
          call the driver immediately with another SKB to send.
    
          There is logic added to the qdisc layer to dequeue multiple
          packets at a time, and the handling mis-predicted offloads in
          software is now done with no locks held.
    
          Finally, pktgen is extended to have a "burst" parameter that can
          be used to test a multi-send implementation.
    
          Several drivers have xmit_more support: i40e, igb, ixgbe, mlx4,
          virtio_net
    
          Adding support is almost trivial, so export more drivers to
          support this optimization soon.
    
          I want to thank, in no particular or implied order, Jesper
          Dangaard Brouer, Eric Dumazet, Alexander Duyck, Tom Herbert, Jamal
          Hadi Salim, John Fastabend, Florian Westphal, Daniel Borkmann,
          David Tat, Hannes Frederic Sowa, and Rusty Russell.
    
       2) PTP and timestamping support in bnx2x, from Michal Kalderon.
    
       3) Allow adjusting the rx_copybreak threshold for a driver via
          ethtool, and add rx_copybreak support to enic driver.  From
          Govindarajulu Varadarajan.
    
       4) Significant enhancements to the generic PHY layer and the bcm7xxx
          driver in particular (EEE support, auto power down, etc.) from
          Florian Fainelli.
    
       5) Allow raw buffers to be used for flow dissection, allowing drivers
          to determine the optimal "linear pull" size for devices that DMA
          into pools of pages.  The objective is to get exactly the
          necessary amount of headers into the linear SKB area pre-pulled,
          but no more.  The new interface drivers use is eth_get_headlen().
          From WANG Cong, with driver conversions (several had their own
          by-hand duplicated implementations) by Alexander Duyck and Eric
          Dumazet.
    
       6) Support checksumming more smoothly and efficiently for
          encapsulations, and add "foo over UDP" facility.  From Tom
          Herbert.
    
       7) Add Broadcom SF2 switch driver to DSA layer, from Florian
          Fainelli.
    
       8) eBPF now can load programs via a system call and has an extensive
          testsuite.  Alexei Starovoitov and Daniel Borkmann.
    
       9) Major overhaul of the packet scheduler to use RCU in several major
          areas such as the classifiers and rate estimators.  From John
          Fastabend.
    
      10) Add driver for Intel FM10000 Ethernet Switch, from Alexander
          Duyck.
    
      11) Rearrange TCP_SKB_CB() to reduce cache line misses, from Eric
          Dumazet.
    
      12) Add Datacenter TCP congestion control algorithm support, From
          Florian Westphal.
    
      13) Reorganize sk_buff so that __copy_skb_header() is significantly
          faster.  From Eric Dumazet"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1558 commits)
      netlabel: directly return netlbl_unlabel_genl_init()
      net: add netdev_txq_bql_{enqueue, complete}_prefetchw() helpers
      net: description of dma_cookie cause make xmldocs warning
      cxgb4: clean up a type issue
      cxgb4: potential shift wrapping bug
      i40e: skb->xmit_more support
      net: fs_enet: Add NAPI TX
      net: fs_enet: Remove non NAPI RX
      r8169:add support for RTL8168EP
      net_sched: copy exts->type in tcf_exts_change()
      wimax: convert printk to pr_foo()
      af_unix: remove 0 assignment on static
      ipv6: Do not warn for informational ICMP messages, regardless of type.
      Update Intel Ethernet Driver maintainers list
      bridge: Save frag_max_size between PRE_ROUTING and POST_ROUTING
      tipc: fix bug in multicast congestion handling
      net: better IFF_XMIT_DST_RELEASE support
      net/mlx4_en: remove NETDEV_TX_BUSY
      3c59x: fix bad split of cpu_to_le32(pci_map_single())
      net: bcmgenet: fix Tx ring priority programming
      ...

commit d0cd84817c745655428dbfdb1e3f754230b46bef
Merge: bdf428feb225 3f3340785672
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 7 20:39:25 2014 -0400

    Merge tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine updates from Dan Williams:
     "Even though this has fixes marked for -stable, given the size and the
      needed conflict resolutions this is 3.18-rc1/merge-window material.
    
      These patches have been languishing in my tree for a long while.  The
      fact that I do not have the time to do proper/prompt maintenance of
      this tree is a primary factor in the decision to step down as
      dmaengine maintainer.  That and the fact that the bulk of drivers/dma/
      activity is going through Vinod these days.
    
      The net_dma removal has not been in -next.  It has developed simple
      conflicts against mainline and net-next (for-3.18).
    
      Continuing thanks to Vinod for staying on top of drivers/dma/.
    
      Summary:
    
       1/ Step down as dmaengine maintainer see commit 08223d80df38
          "dmaengine maintainer update"
    
       2/ Removal of net_dma, as it has been marked 'broken' since 3.13
          (commit 77873803363c "net_dma: mark broken"), without reports of
          performance regression.
    
       3/ Miscellaneous fixes"
    
    * tag 'dmaengine-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      net: make tcp_cleanup_rbuf private
      net_dma: revert 'copied_early'
      net_dma: simple removal
      dmaengine maintainer update
      dmatest: prevent memory leakage on error path in thread
      ioat: Use time_before_jiffies()
      dmaengine: fix xor sources continuation
      dma: mv_xor: Rename __mv_xor_slot_cleanup() to mv_xor_slot_cleanup()
      dma: mv_xor: Remove all callers of mv_xor_slot_cleanup()
      dma: mv_xor: Remove unneeded mv_xor_clean_completed_slots() call
      ioat: Use pci_enable_msix_exact() instead of pci_enable_msix()
      drivers: dma: Include appropriate header file in dca.c
      drivers: dma: Mark functions as static in dma_v3.c
      dma: mv_xor: Add DMA API error checks
      ioat/dca: Use dev_is_pci() to check whether it is pci device

commit 7bced397510ab569d31de4c70b39e13355046387
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 12:37:29 2013 -0800

    net_dma: simple removal
    
    Per commit "77873803363c net_dma: mark broken" net_dma is no longer used
    and there is no plan to fix it.
    
    This is the mechanical removal of bits in CONFIG_NET_DMA ifdef guards.
    Reverting the remainder of the net_dma induced changes is deferred to
    subsequent patches.
    
    Marked for stable due to Roman's report of a memory leak in
    dma_pin_iovec_pages():
    
        https://lkml.org/lkml/2014/9/3/177
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: David Whipple <whipple@securedatainnovations.ch>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index b9586a137cad..3353b47f3d40 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -231,7 +231,6 @@ struct cg_proto;
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
   *	@sk_write_queue: Packet sending queue
-  *	@sk_async_wait_queue: DMA copied packets
   *	@sk_omem_alloc: "o" is "option" or "other"
   *	@sk_wmem_queued: persistent queue size
   *	@sk_forward_alloc: space allocated forward
@@ -354,10 +353,6 @@ struct sock {
 	struct sk_filter __rcu	*sk_filter;
 	struct socket_wq __rcu	*sk_wq;
 
-#ifdef CONFIG_NET_DMA
-	struct sk_buff_head	sk_async_wait_queue;
-#endif
-
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
@@ -2214,27 +2209,15 @@ void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
  * sk_eat_skb - Release a skb if it is no longer needed
  * @sk: socket to eat this skb from
  * @skb: socket buffer to eat
- * @copied_early: flag indicating whether DMA operations copied this data early
  *
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.
 */
-#ifdef CONFIG_NET_DMA
-static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, bool copied_early)
-{
-	__skb_unlink(skb, &sk->sk_receive_queue);
-	if (!copied_early)
-		__kfree_skb(skb);
-	else
-		__skb_queue_tail(&sk->sk_async_wait_queue, skb);
-}
-#else
-static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, bool copied_early)
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
 	__kfree_skb(skb);
 }
-#endif
 
 static inline
 struct net *sock_net(const struct sock *sk)

commit 67cc0d4077951295f42bed63805e91b46c24477b
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Sep 8 19:58:58 2014 -0400

    net-timestamp: optimize sock_tx_timestamp default path
    
    Few packets have timestamping enabled. Exit sock_tx_timestamp quickly
    in this common case.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 049ab1b732a6..515a4d01e932 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2199,6 +2199,8 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 		sk->sk_stamp = skb->tstamp;
 }
 
+void __sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags);
+
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @sk:		socket sending this packet
@@ -2206,7 +2208,13 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
  *
  * Note : callers should take care of initial *tx_flags value (usually 0)
  */
-void sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags);
+static inline void sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags)
+{
+	if (unlikely(sk->sk_tsflags))
+		__sock_tx_timestamp(sk, tx_flags);
+	if (unlikely(sock_flag(sk, SOCK_WIFI_STATUS)))
+		*tx_flags |= SKBTX_WIFI_STATUS;
+}
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed

commit eb84d6b60491a3ca3d90d62ee5346b007770d40d
Merge: 97a13e5289ba d030671f3f26
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Sep 7 21:41:53 2014 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 82eabd9eb2ec1603282a2c3f74dfcb6fe0aaea0e
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Thu Sep 4 13:32:11 2014 -0400

    net: merge cases where sock_efree and sock_edemux are the same function
    
    Since sock_efree and sock_demux are essentially the same code for non-TCP
    sockets and the case where CONFIG_INET is not defined we can combine the
    code or replace the call to sock_edemux in several spots.  As a result we
    can avoid a bit of unnecessary code or code duplication.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e02be37a3d91..ad23e80cb8d3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1575,7 +1575,11 @@ void sock_wfree(struct sk_buff *skb);
 void skb_orphan_partial(struct sk_buff *skb);
 void sock_rfree(struct sk_buff *skb);
 void sock_efree(struct sk_buff *skb);
+#ifdef CONFIG_INET
 void sock_edemux(struct sk_buff *skb);
+#else
+#define sock_edemux(skb) sock_efree(skb)
+#endif
 
 int sock_setsockopt(struct socket *sock, int level, int op,
 		    char __user *optval, unsigned int optlen);

commit 62bccb8cdb69051b95a55ab0c489e3cab261c8ef
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Thu Sep 4 13:31:35 2014 -0400

    net-timestamp: Make the clone operation stand-alone from phy timestamping
    
    The phy timestamping takes a different path than the regular timestamping
    does in that it will create a clone first so that the packets needing to be
    timestamped can be placed in a queue, or the context block could be used.
    
    In order to support these use cases I am pulling the core of the code out
    so it can be used in other drivers beyond just phy devices.
    
    In addition I have added a destructor named sock_efree which is meant to
    provide a simple way for dropping the reference to skb exceptions that
    aren't part of either the receive or send windows for the socket, and I
    have removed some duplication in spots where this destructor could be used
    in place of sock_edemux.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3fde6130789d..e02be37a3d91 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1574,6 +1574,7 @@ struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
 void sock_wfree(struct sk_buff *skb);
 void skb_orphan_partial(struct sk_buff *skb);
 void sock_rfree(struct sk_buff *skb);
+void sock_efree(struct sk_buff *skb);
 void sock_edemux(struct sk_buff *skb);
 
 int sock_setsockopt(struct socket *sock, int level, int op,

commit c199105d154e029cd8c94cccd35bd073e64acc45
Author: Willem de Bruijn <willemb@google.com>
Date:   Wed Sep 3 12:01:18 2014 -0400

    net-timestamp: only report sw timestamp if reporting bit is set
    
    The timestamping API has separate bits for generating and reporting
    timestamps. A software timestamp should only be reported for a packet
    when the packet has the relevant generation flag (SKBTX_..) set
    and the socket has reporting bit SOF_TIMESTAMPING_SOFTWARE set.
    
    The second check was accidentally removed. Reinstitute the original
    behavior.
    
    Tested:
      Without this patch, Documentation/networking/txtimestamp reports
      timestamps regardless of whether SOF_TIMESTAMPING_SOFTWARE is set.
      After the patch, it only reports them when the flag is set.
    
    Fixes: f24b9be5957b ("net-timestamp: extend SCM_TIMESTAMPING ancillary data struct")
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7f2ab72f321a..b9a5bd0ed9f3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2165,9 +2165,7 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 	 */
 	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
 	    (sk->sk_tsflags & SOF_TIMESTAMPING_RX_SOFTWARE) ||
-	    (kt.tv64 &&
-	     (sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE ||
-	      skb_shinfo(skb)->tx_flags & SKBTX_ANY_SW_TSTAMP)) ||
+	    (kt.tv64 && sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) ||
 	    (hwtstamps->hwtstamp.tv64 &&
 	     (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);

commit 364a9e93243d1785f310c0964af0e24bf1adac03
Author: Willem de Bruijn <willemb@google.com>
Date:   Sun Aug 31 21:30:27 2014 -0400

    sock: deduplicate errqueue dequeue
    
    sk->sk_error_queue is dequeued in four locations. All share the
    exact same logic. Deduplicate.
    
    Also collapse the two critical sections for dequeue (at the top of
    the recv handler) and signal (at the bottom).
    
    This moves signal generation for the next packet forward, which should
    be harmless.
    
    It also changes the behavior if the recv handler exits early with an
    error. Previously, a signal for follow-up packets on the errqueue
    would then not be scheduled. The new behavior, to always signal, is
    arguably a bug fix.
    
    For rxrpc, the change causes the same function to be called repeatedly
    for each queued packet (because the recv handler == sk_error_report).
    It is likely that all packets will fail for the same reason (e.g.,
    memory exhaustion).
    
    This code runs without sk_lock held, so it is not safe to trust that
    sk->sk_err is immutable inbetween releasing q->lock and the subsequent
    test. Introduce int err just to avoid this potential race.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7f2ab72f321a..3fde6130789d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2041,6 +2041,7 @@ void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
 int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
+struct sk_buff *sock_dequeue_err_skb(struct sock *sk);
 
 /*
  *	Recover an error report and clear atomically

commit 4fab9071950c2021d846e18351e0f46a1cffd67b
Author: Neal Cardwell <ncardwell@google.com>
Date:   Thu Aug 14 12:40:05 2014 -0400

    tcp: fix tcp_release_cb() to dispatch via address family for mtu_reduced()
    
    Make sure we use the correct address-family-specific function for
    handling MTU reductions from within tcp_release_cb().
    
    Previously AF_INET6 sockets were incorrectly always using the IPv6
    code path when sometimes they were handling IPv4 traffic and thus had
    an IPv4 dst.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Diagnosed-by: Willem de Bruijn <willemb@google.com>
    Fixes: 563d34d057862 ("tcp: dont drop MTU reduction indications")
    Reviewed-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 38805fa02e48..7f2ab72f321a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -987,7 +987,6 @@ struct proto {
 						struct sk_buff *skb);
 
 	void		(*release_cb)(struct sock *sk);
-	void		(*mtu_reduced)(struct sock *sk);
 
 	/* Keeping track of sk's, looking them up, and port selection methods. */
 	void			(*hash)(struct sock *sk);

commit 140c55d4b59581680dc8963612bdc79d19f7bef6
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Aug 6 11:49:29 2014 +0200

    net-timestamp: sock_tx_timestamp() fix
    
    sock_tx_timestamp() should not ignore initial *tx_flags value, as TCP
    stack can store SKBTX_SHARED_FRAG in it.
    
    Also first argument (struct sock *) can be const.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Fixes: 4ed2d765dfac ("net-timestamp: TCP timestamping")
    Cc: Willem de Bruijn <willemb@google.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 52fe0bc5598a..38805fa02e48 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2199,9 +2199,11 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @sk:		socket sending this packet
- * @tx_flags:	filled with instructions for time stamping
+ * @tx_flags:	completed with instructions for time stamping
+ *
+ * Note : callers should take care of initial *tx_flags value (usually 0)
  */
-void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
+void sock_tx_timestamp(const struct sock *sk, __u8 *tx_flags);
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed

commit 09c2d251b70723650ba47e83571ff49281320f7c
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Aug 4 22:11:47 2014 -0400

    net-timestamp: add key to disambiguate concurrent datagrams
    
    Datagrams timestamped on transmission can coexist in the kernel stack
    and be reordered in packet scheduling. When reading looped datagrams
    from the socket error queue it is not always possible to unique
    correlate looped data with original send() call (for application
    level retransmits). Even if possible, it may be expensive and complex,
    requiring packet inspection.
    
    Introduce a data-independent ID mechanism to associate timestamps with
    send calls. Pass an ID alongside the timestamp in field ee_data of
    sock_extended_err.
    
    The ID is a simple 32 bit unsigned int that is associated with the
    socket and incremented on each send() call for which software tx
    timestamp generation is enabled.
    
    The feature is enabled only if SOF_TIMESTAMPING_OPT_ID is set, to
    avoid changing ee_data for existing applications that expect it 0.
    The counter is reset each time the flag is reenabled. Reenabling
    does not change the ID of already submitted data. It is possible
    to receive out of order IDs if the timestamp stream is not quiesced
    first.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a21129716aae..52fe0bc5598a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -280,6 +280,7 @@ struct cg_proto;
   *	@sk_timer: sock cleanup timer
   *	@sk_stamp: time stamp of last packet received
   *	@sk_tsflags: SO_TIMESTAMPING socket options
+  *	@sk_tskey: counter to disambiguate concurrent tstamp requests
   *	@sk_socket: Identd and reporting IO signals
   *	@sk_user_data: RPC layer private data
   *	@sk_frag: cached page frag
@@ -414,6 +415,7 @@ struct sock {
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;
 	u16			sk_tsflags;
+	u32			sk_tskey;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
 	struct page_frag	sk_frag;

commit b9f40e21ef4298650ab33e35740fa85bd57706d5
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Aug 4 22:11:46 2014 -0400

    net-timestamp: move timestamp flags out of sk_flags
    
    sk_flags is reaching its limit. New timestamping options will not fit.
    Move all of them into a new field sk->sk_tsflags.
    
    Added benefit is that this removes boilerplate code to convert between
    SOF_TIMESTAMPING_.. and SOCK_TIMESTAMPING_.. in getsockopt/setsockopt.
    
    SOCK_TIMESTAMPING_RX_SOFTWARE is also used to toggle the receive
    timestamp logic (netstamp_needed). That can be simplified and this
    last key removed, but will leave that for a separate patch.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    
    ----
    
    The u16 in sock can be moved into a 16-bit hole below sk_gso_max_segs,
    though that scatters tstamp fields throughout the struct.
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 02f5b35e65f1..a21129716aae 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -67,6 +67,7 @@
 #include <linux/atomic.h>
 #include <net/dst.h>
 #include <net/checksum.h>
+#include <linux/net_tstamp.h>
 
 struct cgroup;
 struct cgroup_subsys;
@@ -278,6 +279,7 @@ struct cg_proto;
   *	@sk_protinfo: private area, net family specific, when not using slab
   *	@sk_timer: sock cleanup timer
   *	@sk_stamp: time stamp of last packet received
+  *	@sk_tsflags: SO_TIMESTAMPING socket options
   *	@sk_socket: Identd and reporting IO signals
   *	@sk_user_data: RPC layer private data
   *	@sk_frag: cached page frag
@@ -411,6 +413,7 @@ struct sock {
 	void			*sk_protinfo;
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;
+	u16			sk_tsflags;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
 	struct page_frag	sk_frag;
@@ -701,12 +704,7 @@ enum sock_flags {
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 	SOCK_MEMALLOC, /* VM depends on this socket for swapping */
-	SOCK_TIMESTAMPING_TX_HARDWARE,  /* %SOF_TIMESTAMPING_TX_HARDWARE */
-	SOCK_TIMESTAMPING_TX_SOFTWARE,  /* %SOF_TIMESTAMPING_TX_SOFTWARE */
-	SOCK_TIMESTAMPING_RX_HARDWARE,  /* %SOF_TIMESTAMPING_RX_HARDWARE */
 	SOCK_TIMESTAMPING_RX_SOFTWARE,  /* %SOF_TIMESTAMPING_RX_SOFTWARE */
-	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
-	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
 	SOCK_FASYNC, /* fasync() active */
 	SOCK_RXQ_OVFL,
 	SOCK_ZEROCOPY, /* buffers from userspace */
@@ -2160,20 +2158,17 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 
 	/*
 	 * generate control messages if
-	 * - receive time stamping in software requested (SOCK_RCVTSTAMP
-	 *   or SOCK_TIMESTAMPING_RX_SOFTWARE)
+	 * - receive time stamping in software requested
 	 * - software time stamp available and wanted
-	 *   (SOCK_TIMESTAMPING_SOFTWARE)
 	 * - hardware time stamps available and wanted
-	 *   SOCK_TIMESTAMPING_RAW_HARDWARE
 	 */
 	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
-	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
+	    (sk->sk_tsflags & SOF_TIMESTAMPING_RX_SOFTWARE) ||
 	    (kt.tv64 &&
-	     (sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE) ||
+	     (sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE ||
 	      skb_shinfo(skb)->tx_flags & SKBTX_ANY_SW_TSTAMP)) ||
 	    (hwtstamps->hwtstamp.tv64 &&
-	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)))
+	     (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);
 	else
 		sk->sk_stamp = kt;
@@ -2189,11 +2184,11 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 					  struct sk_buff *skb)
 {
 #define FLAGS_TS_OR_DROPS ((1UL << SOCK_RXQ_OVFL)			| \
-			   (1UL << SOCK_RCVTSTAMP)			| \
-			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
-			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE))
+			   (1UL << SOCK_RCVTSTAMP))
+#define TSFLAGS_ANY	  (SOF_TIMESTAMPING_SOFTWARE			| \
+			   SOF_TIMESTAMPING_RAW_HARDWARE)
 
-	if (sk->sk_flags & FLAGS_TS_OR_DROPS)
+	if (sk->sk_flags & FLAGS_TS_OR_DROPS || sk->sk_tsflags & TSFLAGS_ANY)
 		__sock_recv_ts_and_drops(msg, sk, skb);
 	else
 		sk->sk_stamp = skb->tstamp;
@@ -2203,8 +2198,6 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @sk:		socket sending this packet
  * @tx_flags:	filled with instructions for time stamping
- *
- * Currently only depends on SOCK_TIMESTAMPING* flags.
  */
 void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
 

commit f24b9be5957b38bb420b838115040dc2031b7d0c
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Aug 4 22:11:45 2014 -0400

    net-timestamp: extend SCM_TIMESTAMPING ancillary data struct
    
    Applications that request kernel tx timestamps with SO_TIMESTAMPING
    read timestamps as recvmsg() ancillary data. The response is defined
    implicitly as timespec[3].
    
    1) define struct scm_timestamping explicitly and
    
    2) add support for new tstamp types. On tx, scm_timestamping always
       accompanies a sock_extended_err. Define previously unused field
       ee_info to signal the type of ts[0]. Introduce SCM_TSTAMP_SND to
       define the existing behavior.
    
    The reception path is not modified. On rx, no struct similar to
    sock_extended_err is passed along with SCM_TIMESTAMPING.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b91c8868ab8d..02f5b35e65f1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2169,7 +2169,9 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 	 */
 	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
 	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
-	    (kt.tv64 && sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE)) ||
+	    (kt.tv64 &&
+	     (sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE) ||
+	      skb_shinfo(skb)->tx_flags & SKBTX_ANY_SW_TSTAMP)) ||
 	    (hwtstamps->hwtstamp.tv64 &&
 	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);

commit 4d276eb6a478307a28ae843836c455bf04b37a3c
Author: Willem de Bruijn <willemb@google.com>
Date:   Fri Jul 25 18:01:32 2014 -0400

    net: remove deprecated syststamp timestamp
    
    The SO_TIMESTAMPING API defines three types of timestamps: software,
    hardware in raw format (hwtstamp) and hardware converted to system
    format (syststamp). The last has been deprecated in favor of combining
    hwtstamp with a PTP clock driver. There are no active users in the
    kernel.
    
    The option was device driver dependent. If set, but without hardware
    support, the correct behavior is to return zero in the relevant field
    in the SCM_TIMESTAMPING ancillary message. Without device drivers
    implementing the option, this field is effectively always zero.
    
    Remove the internal plumbing to dissuage new drivers from implementing
    the feature. Keep the SOF_TIMESTAMPING_SYS_HARDWARE flag, however, to
    avoid breaking existing applications that request the timestamp.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 720773304a85..b91c8868ab8d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -707,7 +707,6 @@ enum sock_flags {
 	SOCK_TIMESTAMPING_RX_SOFTWARE,  /* %SOF_TIMESTAMPING_RX_SOFTWARE */
 	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
 	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
-	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
 	SOCK_FASYNC, /* fasync() active */
 	SOCK_RXQ_OVFL,
 	SOCK_ZEROCOPY, /* buffers from userspace */
@@ -2166,16 +2165,13 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 	 * - software time stamp available and wanted
 	 *   (SOCK_TIMESTAMPING_SOFTWARE)
 	 * - hardware time stamps available and wanted
-	 *   (SOCK_TIMESTAMPING_SYS_HARDWARE or
-	 *   SOCK_TIMESTAMPING_RAW_HARDWARE)
+	 *   SOCK_TIMESTAMPING_RAW_HARDWARE
 	 */
 	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
 	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
 	    (kt.tv64 && sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE)) ||
 	    (hwtstamps->hwtstamp.tv64 &&
-	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)) ||
-	    (hwtstamps->syststamp.tv64 &&
-	     sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE)))
+	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);
 	else
 		sk->sk_stamp = kt;
@@ -2193,8 +2189,7 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 #define FLAGS_TS_OR_DROPS ((1UL << SOCK_RXQ_OVFL)			| \
 			   (1UL << SOCK_RCVTSTAMP)			| \
 			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
-			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE)	| \
-			   (1UL << SOCK_TIMESTAMPING_SYS_HARDWARE))
+			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE))
 
 	if (sk->sk_flags & FLAGS_TS_OR_DROPS)
 		__sock_recv_ts_and_drops(msg, sk, skb);

commit 274f482d33a309c87096f2983601ceda2761094e
Author: Sorin Dumitru <sorin@returnze.ro>
Date:   Tue Jul 22 21:16:51 2014 +0300

    sock: remove skb argument from sk_rcvqueues_full
    
    It hasn't been used since commit 0fd7bac(net: relax rcvbuf limits).
    
    Signed-off-by: Sorin Dumitru <sorin@returnze.ro>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 28f734601b50..720773304a85 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -810,8 +810,7 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
  * Do not take into account this skb truesize,
  * to allow even a single big packet to come.
  */
-static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
-				     unsigned int limit)
+static inline bool sk_rcvqueues_full(const struct sock *sk, unsigned int limit)
 {
 	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
 
@@ -822,7 +821,7 @@ static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff
 static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,
 					      unsigned int limit)
 {
-	if (sk_rcvqueues_full(sk, skb, limit))
+	if (sk_rcvqueues_full(sk, limit))
 		return -ENOBUFS;
 
 	__sk_add_backlog(sk, skb);

commit 2dc41cff7545d55c6294525c811594576f8e119c
Author: David Held <drheld@google.com>
Date:   Tue Jul 15 23:28:32 2014 -0400

    udp: Use hash2 for long hash1 chains in __udp*_lib_mcast_deliver.
    
    Many multicast sources can have the same port which can result in a very
    large list when hashing by port only. Hash by address and port instead
    if this is the case. This makes multicast more similar to unicast.
    
    On a 24-core machine receiving from 500 multicast sockets on the same
    port, before this patch 80% of system CPU was used up by spin locking
    and only ~25% of packets were successfully delivered.
    
    With this patch, all packets are delivered and kernel overhead is ~8%
    system CPU on spinlocks.
    
    Signed-off-by: David Held <drheld@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 29e48a6d1ded..28f734601b50 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -660,6 +660,20 @@ static inline void sk_add_bind_node(struct sock *sk,
 #define sk_for_each_bound(__sk, list) \
 	hlist_for_each_entry(__sk, list, sk_bind_node)
 
+/**
+ * sk_nulls_for_each_entry_offset - iterate over a list at a given struct offset
+ * @tpos:	the type * to use as a loop cursor.
+ * @pos:	the &struct hlist_node to use as a loop cursor.
+ * @head:	the head for your list.
+ * @offset:	offset of hlist_node within the struct.
+ *
+ */
+#define sk_nulls_for_each_entry_offset(tpos, pos, head, offset)		       \
+	for (pos = (head)->first;					       \
+	     (!is_a_nulls(pos)) &&					       \
+		({ tpos = (typeof(*tpos) *)((void *)pos - offset); 1;});       \
+	     pos = pos->next)
+
 static inline struct user_namespace *sk_user_ns(struct sock *sk)
 {
 	/* Careful only use this in a context where these parameters

commit 1a98c69af1ecd97bfd1f4e4539924a9192434e36
Merge: 7a575f6b907e b6603fe574af
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jul 16 14:09:34 2014 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b73c3d0e4f0e1961e15bec18720e48aabebe2109
Author: Tom Herbert <therbert@google.com>
Date:   Tue Jul 1 21:32:17 2014 -0700

    net: Save TX flow hash in sock and set in skbuf on xmit
    
    For a connected socket we can precompute the flow hash for setting
    in skb->hash on output. This is a performance advantage over
    calculating the skb->hash for every packet on the connection. The
    computation is done using the common hash algorithm to be consistent
    with computations done for packets of the connection in other states
    where thers is no socket (e.g. time-wait, syn-recv, syn-cookies).
    
    This patch adds sk_txhash to the sock structure. inet_set_txhash and
    ip6_set_txhash functions are added which are called from points in
    TCP and UDP where socket moves to established state.
    
    skb_set_hash_from_sk is a function which sets skb->hash from the
    sock txhash value. This is called in UDP and TCP transmit path when
    transmitting within the context of a socket.
    
    Tested: ran super_netperf with 200 TCP_RR streams over a vxlan
    interface (in this case skb_get_hash called on every TX packet to
    create a UDP source port).
    
    Before fix:
    
      95.02% CPU utilization
      154/256/505 90/95/99% latencies
      1.13042e+06 tps
    
      Time in functions:
        0.28% skb_flow_dissect
        0.21% __skb_get_hash
    
    After fix:
    
      94.95% CPU utilization
      156/254/485 90/95/99% latencies
      1.15447e+06
    
      Neither __skb_get_hash nor skb_flow_dissect appear in perf
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8d4c9473e7d7..cb84b2f1ad8f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -273,6 +273,7 @@ struct cg_proto;
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
   *	@sk_rxhash: flow hash received from netif layer
+  *	@sk_txhash: computed flow hash for use on transmit
   *	@sk_filter: socket filtering instructions
   *	@sk_protinfo: private area, net family specific, when not using slab
   *	@sk_timer: sock cleanup timer
@@ -347,6 +348,7 @@ struct sock {
 #ifdef CONFIG_RPS
 	__u32			sk_rxhash;
 #endif
+	__u32			sk_txhash;
 #ifdef CONFIG_NET_RX_BUSY_POLL
 	unsigned int		sk_napi_id;
 	unsigned int		sk_ll_usec;
@@ -1980,6 +1982,14 @@ static inline void sock_poll_wait(struct file *filp,
 	}
 }
 
+static inline void skb_set_hash_from_sk(struct sk_buff *skb, struct sock *sk)
+{
+	if (sk->sk_txhash) {
+		skb->l4_hash = 1;
+		skb->hash = sk->sk_txhash;
+	}
+}
+
 /*
  *	Queue a received datagram if it will fit. Stream and sequenced
  *	protocols can't normally use this as they need to fit buffers in
@@ -1994,6 +2004,7 @@ static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
 	skb_orphan(skb);
 	skb->sk = sk;
 	skb->destructor = sock_wfree;
+	skb_set_hash_from_sk(skb, sk);
 	/*
 	 * We used to take a refcount on sk, but following operation
 	 * is enough to guarantee sk_free() wont free this sock until

commit 5925a0555bdaf0b396a84318cbc21ba085f6c0d3
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jul 2 02:39:38 2014 -0700

    net: fix sparse warning in sk_dst_set()
    
    sk_dst_cache has __rcu annotation, so we need a cast to avoid
    following sparse error :
    
    include/net/sock.h:1774:19: warning: incorrect type in initializer (different address spaces)
    include/net/sock.h:1774:19:    expected struct dst_entry [noderef] <asn:4>*__ret
    include/net/sock.h:1774:19:    got struct dst_entry *dst
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Fixes: 7f502361531e ("ipv4: irq safe sk_dst_[re]set() and ipv4_sk_update_pmtu() fix")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c556fd9b05ac..156350745700 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1771,7 +1771,7 @@ sk_dst_set(struct sock *sk, struct dst_entry *dst)
 	struct dst_entry *old_dst;
 
 	sk_tx_queue_clear(sk);
-	old_dst = xchg(&sk->sk_dst_cache, dst);
+	old_dst = xchg((__force struct dst_entry **)&sk->sk_dst_cache, dst);
 	dst_release(old_dst);
 }
 

commit 9fe516ba3fb29b6f6a752ffd93342fdee500ec01
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jun 27 08:36:16 2014 -0700

    inet: move ipv6only in sock_common
    
    When an UDP application switches from AF_INET to AF_INET6 sockets, we
    have a small performance degradation for IPv4 communications because of
    extra cache line misses to access ipv6only information.
    
    This can also be noticed for TCP listeners, as ipv6_only_sock() is also
    used from __inet_lookup_listener()->compute_score()
    
    This is magnified when SO_REUSEPORT is used.
    
    Move ipv6only into struct sock_common so that it is available at
    no extra cost in lookups.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 173cae485de1..8d4c9473e7d7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -181,7 +181,8 @@ struct sock_common {
 	unsigned short		skc_family;
 	volatile unsigned char	skc_state;
 	unsigned char		skc_reuse:4;
-	unsigned char		skc_reuseport:4;
+	unsigned char		skc_reuseport:1;
+	unsigned char		skc_ipv6only:1;
 	int			skc_bound_dev_if;
 	union {
 		struct hlist_node	skc_bind_node;
@@ -317,6 +318,7 @@ struct sock {
 #define sk_state		__sk_common.skc_state
 #define sk_reuse		__sk_common.skc_reuse
 #define sk_reuseport		__sk_common.skc_reuseport
+#define sk_ipv6only		__sk_common.skc_ipv6only
 #define sk_bound_dev_if		__sk_common.skc_bound_dev_if
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_prot			__sk_common.skc_prot

commit 7f502361531e9eecb396cf99bdc9e9a59f7ebd7f
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jun 30 01:26:23 2014 -0700

    ipv4: irq safe sk_dst_[re]set() and ipv4_sk_update_pmtu() fix
    
    We have two different ways to handle changes to sk->sk_dst
    
    First way (used by TCP) assumes socket lock is owned by caller, and use
    no extra lock : __sk_dst_set() & __sk_dst_reset()
    
    Another way (used by UDP) uses sk_dst_lock because socket lock is not
    always taken. Note that sk_dst_lock is not softirq safe.
    
    These ways are not inter changeable for a given socket type.
    
    ipv4_sk_update_pmtu(), added in linux-3.8, added a race, as it used
    the socket lock as synchronization, but users might be UDP sockets.
    
    Instead of converting sk_dst_lock to a softirq safe version, use xchg()
    as we did for sk_rx_dst in commit e47eb5dfb296b ("udp: ipv4: do not use
    sk_dst_lock from softirq context")
    
    In a follow up patch, we probably can remove sk_dst_lock, as it is
    only used in IPv6.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Fixes: 9cb3a50c5f63e ("ipv4: Invalidate the socket cached route on pmtu events if possible")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 173cae485de1..c556fd9b05ac 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1768,9 +1768,11 @@ __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 static inline void
 sk_dst_set(struct sock *sk, struct dst_entry *dst)
 {
-	spin_lock(&sk->sk_dst_lock);
-	__sk_dst_set(sk, dst);
-	spin_unlock(&sk->sk_dst_lock);
+	struct dst_entry *old_dst;
+
+	sk_tx_queue_clear(sk);
+	old_dst = xchg(&sk->sk_dst_cache, dst);
+	dst_release(old_dst);
 }
 
 static inline void
@@ -1782,9 +1784,7 @@ __sk_dst_reset(struct sock *sk)
 static inline void
 sk_dst_reset(struct sock *sk)
 {
-	spin_lock(&sk->sk_dst_lock);
-	__sk_dst_reset(sk);
-	spin_unlock(&sk->sk_dst_lock);
+	sk_dst_set(sk, NULL);
 }
 
 struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);

commit f88649721268999bdff09777847080a52004f691
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 24 10:05:11 2014 -0700

    ipv4: fix dst race in sk_dst_get()
    
    When IP route cache had been removed in linux-3.6, we broke assumption
    that dst entries were all freed after rcu grace period. DST_NOCACHE
    dst were supposed to be freed from dst_release(). But it appears
    we want to keep such dst around, either in UDP sockets or tunnels.
    
    In sk_dst_get() we need to make sure dst refcount is not 0
    before incrementing it, or else we might end up freeing a dst
    twice.
    
    DST_NOCACHE set on a dst does not mean this dst can not be attached
    to a socket or a tunnel.
    
    Then, before actual freeing, we need to observe a rcu grace period
    to make sure all other cpus can catch the fact the dst is no longer
    usable.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Dormando <dormando@rydia.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 07b7fcd60d80..173cae485de1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1730,8 +1730,8 @@ sk_dst_get(struct sock *sk)
 
 	rcu_read_lock();
 	dst = rcu_dereference(sk->sk_dst_cache);
-	if (dst)
-		dst_hold(dst);
+	if (dst && !atomic_inc_not_zero(&dst->__refcnt))
+		dst = NULL;
 	rcu_read_unlock();
 	return dst;
 }

commit 28448b80456feafe07e2d05b6363b00f61f6171e
Author: Tom Herbert <therbert@google.com>
Date:   Fri May 23 08:47:19 2014 -0700

    net: Split sk_no_check into sk_no_check_{rx,tx}
    
    Define separate fields in the sock structure for configuring disabling
    checksums in both TX and RX-- sk_no_check_tx and sk_no_check_rx.
    The SO_NO_CHECK socket option only affects sk_no_check_tx. Also,
    removed UDP_CSUM_* defines since they are no longer necessary.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 21569cf456ed..07b7fcd60d80 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -243,7 +243,8 @@ struct cg_proto;
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
   *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
-  *	@sk_no_check: %SO_NO_CHECK setting, whether or not checkup packets
+  *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
+  *	@sk_no_check_rx: allow zero checksum in RX packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
@@ -371,7 +372,8 @@ struct sock {
 	struct sk_buff_head	sk_write_queue;
 	kmemcheck_bitfield_begin(flags);
 	unsigned int		sk_shutdown  : 2,
-				sk_no_check  : 2,
+				sk_no_check_tx : 1,
+				sk_no_check_rx : 1,
 				sk_userlocks : 4,
 				sk_protocol  : 8,
 				sk_type      : 16;

commit a3b299da869d6e78cf42ae0b1b41797bcb8c5e4b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Apr 23 14:26:56 2014 -0700

    net: Add variants of capable for use on on sockets
    
    sk_net_capable - The common case, operations that are safe in a network namespace.
    sk_capable - Operations that are not known to be safe in a network namespace
    sk_ns_capable - The general case for special cases.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8338a14e4805..21569cf456ed 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2255,6 +2255,11 @@ int sock_get_timestampns(struct sock *, struct timespec __user *);
 int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len, int level,
 		       int type);
 
+bool sk_ns_capable(const struct sock *sk,
+		   struct user_namespace *user_ns, int cap);
+bool sk_capable(const struct sock *sk, int cap);
+bool sk_net_capable(const struct sock *sk, int cap);
+
 /*
  *	Enable debug/info messages
  */

commit 676d23690fb62b5d51ba5d659935e9f7d9da9f8e
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Apr 11 16:15:36 2014 -0400

    net: Fix use after free by removing length arg from sk_data_ready callbacks.
    
    Several spots in the kernel perform a sequence like:
    
            skb_queue_tail(&sk->s_receive_queue, skb);
            sk->sk_data_ready(sk, skb->len);
    
    But at the moment we place the SKB onto the socket receive queue it
    can be consumed and freed up.  So this skb->len access is potentially
    to freed up memory.
    
    Furthermore, the skb->len can be modified by the consumer so it is
    possible that the value isn't accurate.
    
    And finally, no actual implementation of this callback actually uses
    the length argument.  And since nobody actually cared about it's
    value, lots of call sites pass arbitrary values in such as '0' and
    even '1'.
    
    So just remove the length argument from the callback, that way there
    is no confusion whatsoever and all of these use-after-free cases get
    fixed as a side effect.
    
    Based upon a patch by Eric Dumazet and his suggestion to audit this
    issue tree-wide.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 06a5668f05c9..8338a14e4805 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -418,7 +418,7 @@ struct sock {
 	u32			sk_classid;
 	struct cg_proto		*sk_cgrp;
 	void			(*sk_state_change)(struct sock *sk);
-	void			(*sk_data_ready)(struct sock *sk, int bytes);
+	void			(*sk_data_ready)(struct sock *sk);
 	void			(*sk_write_space)(struct sock *sk);
 	void			(*sk_error_report)(struct sock *sk);
 	int			(*sk_backlog_rcv)(struct sock *sk,

commit fbc907f0b1386c02e00516aa78a0fa6b0454fd0b
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri Mar 28 18:58:20 2014 +0100

    net: filter: move filter accounting to filter core
    
    This patch basically does two things, i) removes the extern keyword
    from the include/linux/filter.h file to be more consistent with the
    rest of Joe's changes, and ii) moves filter accounting into the filter
    core framework.
    
    Filter accounting mainly done through sk_filter_{un,}charge() take
    care of the case when sockets are being cloned through sk_clone_lock()
    so that removal of the filter on one socket won't result in eviction
    as it's still referenced by the other.
    
    These functions actually belong to net/core/filter.c and not
    include/net/sock.h as we want to keep all that in a central place.
    It's also not in fast-path so uninlining them is fine and even allows
    us to get rd of sk_filter_release_rcu()'s EXPORT_SYMBOL and a forward
    declaration.
    
    Joint work with Alexei Starovoitov.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8d7c431a0660..06a5668f05c9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1621,33 +1621,6 @@ void sk_common_release(struct sock *sk);
 /* Initialise core socket variables */
 void sock_init_data(struct socket *sock, struct sock *sk);
 
-void sk_filter_release_rcu(struct rcu_head *rcu);
-
-/**
- *	sk_filter_release - release a socket filter
- *	@fp: filter to remove
- *
- *	Remove a filter from a socket and release its resources.
- */
-
-static inline void sk_filter_release(struct sk_filter *fp)
-{
-	if (atomic_dec_and_test(&fp->refcnt))
-		call_rcu(&fp->rcu, sk_filter_release_rcu);
-}
-
-static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
-{
-	atomic_sub(sk_filter_size(fp->len), &sk->sk_omem_alloc);
-	sk_filter_release(fp);
-}
-
-static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)
-{
-	atomic_inc(&fp->refcnt);
-	atomic_add(sk_filter_size(fp->len), &sk->sk_omem_alloc);
-}
-
 /*
  * Socket reference counting postulates.
  *

commit 61b905da33ae25edb6b9d2a5de21e34c3a77efe3
Author: Tom Herbert <therbert@google.com>
Date:   Mon Mar 24 15:34:47 2014 -0700

    net: Rename skb->rxhash to skb->hash
    
    The packet hash can be considered a property of the packet, not just
    on RX path.
    
    This patch changes name of rxhash and l4_rxhash skbuff fields to be
    hash and l4_hash respectively. This includes changing uses of the
    field in the code which don't call the access functions.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Mahesh Bandewar <maheshb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 625e65b12366..8d7c431a0660 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -862,9 +862,9 @@ static inline void sock_rps_save_rxhash(struct sock *sk,
 					const struct sk_buff *skb)
 {
 #ifdef CONFIG_RPS
-	if (unlikely(sk->sk_rxhash != skb->rxhash)) {
+	if (unlikely(sk->sk_rxhash != skb->hash)) {
 		sock_rps_reset_flow(sk);
-		sk->sk_rxhash = skb->rxhash;
+		sk->sk_rxhash = skb->hash;
 	}
 #endif
 }

commit 85dcce7a73f1cc59f7a96fe52713b1630f4ca272
Merge: 4c4e4113db24 a4ecdf82f8ea
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Mar 14 22:31:55 2014 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/usb/r8152.c
            drivers/net/xen-netback/netback.c
    
    Both the r8152 and netback conflicts were simple overlapping
    changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c3f9b01849ef3bc69024990092b9f42e20df7797
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Mar 10 09:50:11 2014 -0700

    tcp: tcp_release_cb() should release socket ownership
    
    Lars Persson reported following deadlock :
    
    -000 |M:0x0:0x802B6AF8(asm) <-- arch_spin_lock
    -001 |tcp_v4_rcv(skb = 0x8BD527A0) <-- sk = 0x8BE6B2A0
    -002 |ip_local_deliver_finish(skb = 0x8BD527A0)
    -003 |__netif_receive_skb_core(skb = 0x8BD527A0, ?)
    -004 |netif_receive_skb(skb = 0x8BD527A0)
    -005 |elk_poll(napi = 0x8C770500, budget = 64)
    -006 |net_rx_action(?)
    -007 |__do_softirq()
    -008 |do_softirq()
    -009 |local_bh_enable()
    -010 |tcp_rcv_established(sk = 0x8BE6B2A0, skb = 0x87D3A9E0, th = 0x814EBE14, ?)
    -011 |tcp_v4_do_rcv(sk = 0x8BE6B2A0, skb = 0x87D3A9E0)
    -012 |tcp_delack_timer_handler(sk = 0x8BE6B2A0)
    -013 |tcp_release_cb(sk = 0x8BE6B2A0)
    -014 |release_sock(sk = 0x8BE6B2A0)
    -015 |tcp_sendmsg(?, sk = 0x8BE6B2A0, ?, ?)
    -016 |sock_sendmsg(sock = 0x8518C4C0, msg = 0x87D8DAA8, size = 4096)
    -017 |kernel_sendmsg(?, ?, ?, ?, size = 4096)
    -018 |smb_send_kvec()
    -019 |smb_send_rqst(server = 0x87C4D400, rqst = 0x87D8DBA0)
    -020 |cifs_call_async()
    -021 |cifs_async_writev(wdata = 0x87FD6580)
    -022 |cifs_writepages(mapping = 0x852096E4, wbc = 0x87D8DC88)
    -023 |__writeback_single_inode(inode = 0x852095D0, wbc = 0x87D8DC88)
    -024 |writeback_sb_inodes(sb = 0x87D6D800, wb = 0x87E4A9C0, work = 0x87D8DD88)
    -025 |__writeback_inodes_wb(wb = 0x87E4A9C0, work = 0x87D8DD88)
    -026 |wb_writeback(wb = 0x87E4A9C0, work = 0x87D8DD88)
    -027 |wb_do_writeback(wb = 0x87E4A9C0, force_wait = 0)
    -028 |bdi_writeback_workfn(work = 0x87E4A9CC)
    -029 |process_one_work(worker = 0x8B045880, work = 0x87E4A9CC)
    -030 |worker_thread(__worker = 0x8B045880)
    -031 |kthread(_create = 0x87CADD90)
    -032 |ret_from_kernel_thread(asm)
    
    Bug occurs because __tcp_checksum_complete_user() enables BH, assuming
    it is running from softirq context.
    
    Lars trace involved a NIC without RX checksum support but other points
    are problematic as well, like the prequeue stuff.
    
    Problem is triggered by a timer, that found socket being owned by user.
    
    tcp_release_cb() should call tcp_write_timer_handler() or
    tcp_delack_timer_handler() in the appropriate context :
    
    BH disabled and socket lock held, but 'owned' field cleared,
    as if they were running from timer handlers.
    
    Fixes: 6f458dfb4092 ("tcp: improve latencies of timer triggered events")
    Reported-by: Lars Persson <lars.persson@axis.com>
    Tested-by: Lars Persson <lars.persson@axis.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7c4167bc8266..b9586a137cad 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1488,6 +1488,11 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
  */
 #define sock_owned_by_user(sk)	((sk)->sk_lock.owned)
 
+static inline void sock_release_ownership(struct sock *sk)
+{
+	sk->sk_lock.owned = 0;
+}
+
 /*
  * Macro so as to not evaluate some arguments when
  * lockdep is not enabled.

commit 5812521be0f79583a26e203ac5f23de679cbdd94
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Mon Mar 10 09:57:34 2014 +0800

    net: add a pre-check of net_ns in sk_change_net()
    
    We do not need to switch the net_ns if the target net_ns the same
    as the current one, so here we add a pre-check of net_ns to avoid
    this as David suggested.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5c3f7c3624aa..967856970a51 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2252,8 +2252,12 @@ void sock_net_set(struct sock *sk, struct net *net)
  */
 static inline void sk_change_net(struct sock *sk, struct net *net)
 {
-	put_net(sock_net(sk));
-	sock_net_set(sk, hold_net(net));
+	struct net *current_net = sock_net(sk);
+
+	if (!net_eq(current_net, net)) {
+		put_net(current_net);
+		sock_net_set(sk, hold_net(net));
+	}
 }
 
 static inline struct sock *skb_steal_sock(struct sk_buff *skb)

commit adca4767821e54c72d4a2f467af77923f2c87e07
Author: Andrew Lutomirski <luto@amacapital.net>
Date:   Tue Mar 4 17:24:10 2014 -0800

    net: Improve SO_TIMESTAMPING documentation and fix a minor code bug
    
    The original documentation was very unclear.
    
    The code fix is presumably related to the formerly unclear
    documentation: SOCK_TIMESTAMPING_RX_SOFTWARE has no effect on
    __sock_recv_timestamp's behavior, so calling __sock_recv_ts_and_drops
    from sock_recv_ts_and_drops if only SOCK_TIMESTAMPING_RX_SOFTWARE is
    set is pointless.  This should have no user-observable effect.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5c3f7c3624aa..7c4167bc8266 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2186,7 +2186,6 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 {
 #define FLAGS_TS_OR_DROPS ((1UL << SOCK_RXQ_OVFL)			| \
 			   (1UL << SOCK_RCVTSTAMP)			| \
-			   (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE)	| \
 			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
 			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE)	| \
 			   (1UL << SOCK_TIMESTAMPING_SYS_HARDWARE))

commit 855404efae0d449cc491978d54ea5d117a3cb271
Merge: a1d4b03a076d 82a37132f300
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Jan 5 20:18:50 2014 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/pablo/nf-next
    
    Pablo Neira Ayuso says:
    
    ====================
    netfilter/IPVS updates for net-next
    
    The following patchset contains Netfilter updates for your net-next tree,
    they are:
    
    * Add full port randomization support. Some crazy researchers found a way
      to reconstruct the secure ephemeral ports that are allocated in random mode
      by sending off-path bursts of UDP packets to overrun the socket buffer of
      the DNS resolver to trigger retransmissions, then if the timing for the
      DNS resolution done by a client is larger than usual, then they conclude
      that the port that received the burst of UDP packets is the one that was
      opened. It seems a bit aggressive method to me but it seems to work for
      them. As a result, Daniel Borkmann and Hannes Frederic Sowa came up with a
      new NAT mode to fully randomize ports using prandom.
    
    * Add a new classifier to x_tables based on the socket net_cls set via
      cgroups. These includes two patches to prepare the field as requested by
      Zefan Li. Also from Daniel Borkmann.
    
    * Use prandom instead of get_random_bytes in several locations of the
      netfilter code, from Florian Westphal.
    
    * Allow to use the CTA_MARK_MASK in ctnetlink when mangling the conntrack
      mark, also from Florian Westphal.
    
    * Fix compilation warning due to unused variable in IPVS, from Geert
      Uytterhoeven.
    
    * Add support for UID/GID via nfnetlink_queue, from Valentina Giusti.
    
    * Add IPComp extension to x_tables, from Fan Du.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8f09898bf02fc24b7a525e9cfc78f38dcdf3a4eb
Author: stephen hemminger <stephen@networkplumber.org>
Date:   Fri Jan 3 09:17:14 2014 -0800

    socket: cleanups
    
    Namespace related cleaning
    
     * make cred_to_ucred static
     * remove unused sock_rmalloc function
    
    Signed-off-by: Stephen Hemminger <stephen@networkplumber.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bd716b6996ff..8d9af66ccf2c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1549,8 +1549,6 @@ struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
 
 struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
 			     gfp_t priority);
-struct sk_buff *sock_rmalloc(struct sock *sk, unsigned long size, int force,
-			     gfp_t priority);
 void sock_wfree(struct sk_buff *skb);
 void skb_orphan_partial(struct sk_buff *skb);
 void sock_rfree(struct sk_buff *skb);

commit 86f8515f9721fa171483f0fe0391968fbb949cc9
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Sun Dec 29 17:27:11 2013 +0100

    net: netprio: rename config to be more consistent with cgroup configs
    
    While we're at it and introduced CGROUP_NET_CLASSID, lets also make
    NETPRIO_CGROUP more consistent with the rest of cgroups and rename it
    into CONFIG_CGROUP_NET_PRIO so that for networking, we now have
    CONFIG_CGROUP_NET_{PRIO,CLASSID}. This not only makes the CONFIG
    option consistent among networking cgroups, but also among cgroups
    CONFIG conventions in general as the vast majority has a prefix of
    CONFIG_CGROUP_<SUBSYS>.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Cc: Zefan Li <lizefan@huawei.com>
    Cc: cgroups@vger.kernel.org
    Acked-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2ef3c3eca47a..ef5e2be6eaf3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -395,7 +395,7 @@ struct sock {
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
-#if IS_ENABLED(CONFIG_NETPRIO_CGROUP)
+#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
 	__u32			sk_cgrp_prioidx;
 #endif
 	struct pid		*sk_peer_pid;

commit c9d8ca0454a28d0f835138d7294ede4fc6d95572
Author: Zhi Yong Wu <wuzhy@linux.vnet.ibm.com>
Date:   Wed Jan 1 04:31:01 2014 +0800

    net, rps: fix build failure when CONFIG_RPS isn't set
    
    In file included from net/socket.c:99:0:
    include/net/sock.h: In function ‘sock_rps_record_flow’:
    include/net/sock.h:849:30: error: ‘const struct sock’ has no member named ‘sk_rxhash’
    include/net/sock.h: In function ‘sock_rps_reset_flow’:
    include/net/sock.h:854:29: error: ‘const struct sock’ has no member named ‘sk_rxhash’
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Zhi Yong Wu <wuzhy@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8ee90add69d2..bd716b6996ff 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -846,12 +846,16 @@ static inline void sock_rps_reset_flow_hash(__u32 hash)
 
 static inline void sock_rps_record_flow(const struct sock *sk)
 {
+#ifdef CONFIG_RPS
 	sock_rps_record_flow_hash(sk->sk_rxhash);
+#endif
 }
 
 static inline void sock_rps_reset_flow(const struct sock *sk)
 {
+#ifdef CONFIG_RPS
 	sock_rps_reset_flow_hash(sk->sk_rxhash);
+#endif
 }
 
 static inline void sock_rps_save_rxhash(struct sock *sk,

commit fe47755852d1f299b55a6e6594bb6e082ac103d4
Author: Tom Herbert <therbert@google.com>
Date:   Sun Dec 22 18:54:31 2013 +0800

    net: Allow setting sock flow hash without a sock
    
    This patch adds sock_rps_record_flow_hash and sock_rps_reset_flow_hash
    which take a hash value as an argument and sets the sock_flow_table
    accordingly.  This allows the table to be populated in cases where flow
    is being tracked outside of a sock structure.
    
    sock_rps_record_flow and sock_rps_reset_flow call this function
    where the hash is taken from sk_rxhash.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: Zhi Yong Wu <wuzhy@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2ef3c3eca47a..8ee90add69d2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -820,30 +820,40 @@ static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 	return sk->sk_backlog_rcv(sk, skb);
 }
 
-static inline void sock_rps_record_flow(const struct sock *sk)
+static inline void sock_rps_record_flow_hash(__u32 hash)
 {
 #ifdef CONFIG_RPS
 	struct rps_sock_flow_table *sock_flow_table;
 
 	rcu_read_lock();
 	sock_flow_table = rcu_dereference(rps_sock_flow_table);
-	rps_record_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rps_record_sock_flow(sock_flow_table, hash);
 	rcu_read_unlock();
 #endif
 }
 
-static inline void sock_rps_reset_flow(const struct sock *sk)
+static inline void sock_rps_reset_flow_hash(__u32 hash)
 {
 #ifdef CONFIG_RPS
 	struct rps_sock_flow_table *sock_flow_table;
 
 	rcu_read_lock();
 	sock_flow_table = rcu_dereference(rps_sock_flow_table);
-	rps_reset_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rps_reset_sock_flow(sock_flow_table, hash);
 	rcu_read_unlock();
 #endif
 }
 
+static inline void sock_rps_record_flow(const struct sock *sk)
+{
+	sock_rps_record_flow_hash(sk->sk_rxhash);
+}
+
+static inline void sock_rps_reset_flow(const struct sock *sk)
+{
+	sock_rps_reset_flow_hash(sk->sk_rxhash);
+}
+
 static inline void sock_rps_save_rxhash(struct sock *sk,
 					const struct sk_buff *skb)
 {

commit 7f2cbdc28c034ef2c3be729681f631d5744e3cd5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Dec 4 20:12:04 2013 -0800

    tcp_memcontrol: Cleanup/fix cg_proto->memory_pressure handling.
    
    kill memcg_tcp_enter_memory_pressure.  The only function of
    memcg_tcp_enter_memory_pressure was to reduce deal with the
    unnecessary abstraction that was tcp_memcontrol.  Now that struct
    tcp_memcontrol is gone remove this unnecessary function, the
    unnecessary function pointer, and modify sk_enter_memory_pressure to
    set this field directly, just as sk_leave_memory_pressure cleas this
    field directly.
    
    This fixes a small bug I intruduced when killing struct tcp_memcontrol
    that caused memcg_tcp_enter_memory_pressure to never be called and
    thus failed to ever set cg_proto->memory_pressure.
    
    Remove the cg_proto enter_memory_pressure function as it now serves
    no useful purpose.
    
    Don't test cg_proto->memory_presser in sk_leave_memory_pressure before
    clearing it.  The test was originally there to ensure that the pointer
    was non-NULL.  Now that cg_proto is not a pointer the pointer does not
    matter.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e3a18ff0c38b..2ef3c3eca47a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1035,7 +1035,6 @@ enum cg_proto_flags {
 };
 
 struct cg_proto {
-	void			(*enter_memory_pressure)(struct sock *sk);
 	struct res_counter	memory_allocated;	/* Current allocated memory. */
 	struct percpu_counter	sockets_allocated;	/* Current number of sockets. */
 	int			memory_pressure;
@@ -1155,8 +1154,7 @@ static inline void sk_leave_memory_pressure(struct sock *sk)
 		struct proto *prot = sk->sk_prot;
 
 		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			if (cg_proto->memory_pressure)
-				cg_proto->memory_pressure = 0;
+			cg_proto->memory_pressure = 0;
 	}
 
 }
@@ -1171,7 +1169,7 @@ static inline void sk_enter_memory_pressure(struct sock *sk)
 		struct proto *prot = sk->sk_prot;
 
 		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			cg_proto->enter_memory_pressure(sk);
+			cg_proto->memory_pressure = 1;
 	}
 
 	sk->sk_prot->enter_memory_pressure(sk);

commit 35b87f6c135b7ded8ab1a44e46d792f7688f3608
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Wed Oct 23 12:49:21 2013 -0700

    net: Dereference pointer-value of sk_prot->memory_pressure
    
    2e685cad57 (tcp_memcontrol: Kill struct tcp_memcontrol) falsly modified
    the access to memory_pressure of sk->sk_prot->memory_pressure. The patch
    did modify the memory_pressure-field of struct cg_proto, but not the one
    of struct proto.
    
    So, the access to sk_prot->memory_pressure should not be changed.
    
    Acked-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c93542f92420..e3a18ff0c38b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1137,7 +1137,7 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
 		return !!sk->sk_cgrp->memory_pressure;
 
-	return !!sk->sk_prot->memory_pressure;
+	return !!*sk->sk_prot->memory_pressure;
 }
 
 static inline void sk_leave_memory_pressure(struct sock *sk)

commit 0a6957e7d47096bbeedda4e1d926359eb487dcfc
Author: ZHAO Gang <gamerh2o@gmail.com>
Date:   Tue Oct 22 16:23:38 2013 +0800

    net: remove function sk_reset_txq()
    
    What sk_reset_txq() does is just calls function sk_tx_queue_reset(),
    and sk_reset_txq() is used only in sock.h, by dst_negative_advice().
    Let dst_negative_advice() calls sk_tx_queue_reset() directly so we
    can remove unneeded sk_reset_txq().
    
    Signed-off-by: ZHAO Gang <gamerh2o@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 86bb0668c581..c93542f92420 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1746,8 +1746,6 @@ sk_dst_get(struct sock *sk)
 	return dst;
 }
 
-void sk_reset_txq(struct sock *sk);
-
 static inline void dst_negative_advice(struct sock *sk)
 {
 	struct dst_entry *ndst, *dst = __sk_dst_get(sk);
@@ -1757,7 +1755,7 @@ static inline void dst_negative_advice(struct sock *sk)
 
 		if (ndst != dst) {
 			rcu_assign_pointer(sk->sk_dst_cache, ndst);
-			sk_reset_txq(sk);
+			sk_tx_queue_clear(sk);
 		}
 	}
 }

commit 2e685cad57906e19add7189b5ff49dfb6aaa21d3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Oct 19 16:26:19 2013 -0700

    tcp_memcontrol: Kill struct tcp_memcontrol
    
    Replace the pointers in struct cg_proto with actual data fields and kill
    struct tcp_memcontrol as it is not fully redundant.
    
    This removes a confusing, unnecessary layer of abstraction.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7e50df5c71d4..86bb0668c581 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1036,10 +1036,10 @@ enum cg_proto_flags {
 
 struct cg_proto {
 	void			(*enter_memory_pressure)(struct sock *sk);
-	struct res_counter	*memory_allocated;	/* Current allocated memory. */
-	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
-	int			*memory_pressure;
-	long			*sysctl_mem;
+	struct res_counter	memory_allocated;	/* Current allocated memory. */
+	struct percpu_counter	sockets_allocated;	/* Current number of sockets. */
+	int			memory_pressure;
+	long			sysctl_mem[3];
 	unsigned long		flags;
 	/*
 	 * memcg field is used to find which memcg we belong directly
@@ -1135,9 +1135,9 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 		return false;
 
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return !!*sk->sk_cgrp->memory_pressure;
+		return !!sk->sk_cgrp->memory_pressure;
 
-	return !!*sk->sk_prot->memory_pressure;
+	return !!sk->sk_prot->memory_pressure;
 }
 
 static inline void sk_leave_memory_pressure(struct sock *sk)
@@ -1155,8 +1155,8 @@ static inline void sk_leave_memory_pressure(struct sock *sk)
 		struct proto *prot = sk->sk_prot;
 
 		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			if (*cg_proto->memory_pressure)
-				*cg_proto->memory_pressure = 0;
+			if (cg_proto->memory_pressure)
+				cg_proto->memory_pressure = 0;
 	}
 
 }
@@ -1192,7 +1192,7 @@ static inline void memcg_memory_allocated_add(struct cg_proto *prot,
 	struct res_counter *fail;
 	int ret;
 
-	ret = res_counter_charge_nofail(prot->memory_allocated,
+	ret = res_counter_charge_nofail(&prot->memory_allocated,
 					amt << PAGE_SHIFT, &fail);
 	if (ret < 0)
 		*parent_status = OVER_LIMIT;
@@ -1201,13 +1201,13 @@ static inline void memcg_memory_allocated_add(struct cg_proto *prot,
 static inline void memcg_memory_allocated_sub(struct cg_proto *prot,
 					      unsigned long amt)
 {
-	res_counter_uncharge(prot->memory_allocated, amt << PAGE_SHIFT);
+	res_counter_uncharge(&prot->memory_allocated, amt << PAGE_SHIFT);
 }
 
 static inline u64 memcg_memory_allocated_read(struct cg_proto *prot)
 {
 	u64 ret;
-	ret = res_counter_read_u64(prot->memory_allocated, RES_USAGE);
+	ret = res_counter_read_u64(&prot->memory_allocated, RES_USAGE);
 	return ret >> PAGE_SHIFT;
 }
 
@@ -1255,7 +1255,7 @@ static inline void sk_sockets_allocated_dec(struct sock *sk)
 		struct cg_proto *cg_proto = sk->sk_cgrp;
 
 		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			percpu_counter_dec(cg_proto->sockets_allocated);
+			percpu_counter_dec(&cg_proto->sockets_allocated);
 	}
 
 	percpu_counter_dec(prot->sockets_allocated);
@@ -1269,7 +1269,7 @@ static inline void sk_sockets_allocated_inc(struct sock *sk)
 		struct cg_proto *cg_proto = sk->sk_cgrp;
 
 		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
-			percpu_counter_inc(cg_proto->sockets_allocated);
+			percpu_counter_inc(&cg_proto->sockets_allocated);
 	}
 
 	percpu_counter_inc(prot->sockets_allocated);
@@ -1281,7 +1281,7 @@ sk_sockets_allocated_read_positive(struct sock *sk)
 	struct proto *prot = sk->sk_prot;
 
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return percpu_counter_read_positive(sk->sk_cgrp->sockets_allocated);
+		return percpu_counter_read_positive(&sk->sk_cgrp->sockets_allocated);
 
 	return percpu_counter_read_positive(prot->sockets_allocated);
 }

commit efe4208f47f907b86f528788da711e8ab9dea44d
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 3 15:42:29 2013 -0700

    ipv6: make lookups simpler and faster
    
    TCP listener refactoring, part 4 :
    
    To speed up inet lookups, we moved IPv4 addresses from inet to struct
    sock_common
    
    Now is time to do the same for IPv6, because it permits us to have fast
    lookups for all kind of sockets, including upcoming SYN_RECV.
    
    Getting IPv6 addresses in TCP lookups currently requires two extra cache
    lines, plus a dereference (and memory stall).
    
    inet6_sk(sk) does the dereference of inet_sk(__sk)->pinet6
    
    This patch is way bigger than its IPv4 counter part, because for IPv4,
    we could add aliases (inet_daddr, inet_rcv_saddr), while on IPv6,
    it's not doable easily.
    
    inet6_sk(sk)->daddr becomes sk->sk_v6_daddr
    inet6_sk(sk)->rcv_saddr becomes sk->sk_v6_rcv_saddr
    
    And timewait socket also have tw->tw_v6_daddr & tw->tw_v6_rcv_saddr
    at the same offset.
    
    We get rid of INET6_TW_MATCH() as INET6_MATCH() is now the generic
    macro.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3f3e48c4704d..7e50df5c71d4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -191,6 +191,12 @@ struct sock_common {
 #ifdef CONFIG_NET_NS
 	struct net	 	*skc_net;
 #endif
+
+#if IS_ENABLED(CONFIG_IPV6)
+	struct in6_addr		skc_v6_daddr;
+	struct in6_addr		skc_v6_rcv_saddr;
+#endif
+
 	/*
 	 * fields between dontcopy_begin/dontcopy_end
 	 * are not copied in sock_copy()
@@ -314,6 +320,9 @@ struct sock {
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_prot			__sk_common.skc_prot
 #define sk_net			__sk_common.skc_net
+#define sk_v6_daddr		__sk_common.skc_v6_daddr
+#define sk_v6_rcv_saddr	__sk_common.skc_v6_rcv_saddr
+
 	socket_lock_t		sk_lock;
 	struct sk_buff_head	sk_receive_queue;
 	/*

commit 05dbc7b59481ca891bbcfe6799a562d48159fbf7
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 3 00:22:02 2013 -0700

    tcp/dccp: remove twchain
    
    TCP listener refactoring, part 3 :
    
    Our goal is to hash SYN_RECV sockets into main ehash for fast lookup,
    and parallel SYN processing.
    
    Current inet_ehash_bucket contains two chains, one for ESTABLISH (and
    friend states) sockets, another for TIME_WAIT sockets only.
    
    As the hash table is sized to get at most one socket per bucket, it
    makes little sense to have separate twchain, as it makes the lookup
    slightly more complicated, and doubles hash table memory usage.
    
    If we make sure all socket types have the lookup keys at the same
    offsets, we can use a generic and faster lookup. It turns out TIME_WAIT
    and ESTABLISHED sockets already have common lookup fields for IPv4.
    
    [ INET_TW_MATCH() is no longer needed ]
    
    I'll provide a follow-up to factorize IPv6 lookup as well, to remove
    INET6_TW_MATCH()
    
    This way, SYN_RECV pseudo sockets will be supported the same.
    
    A new sock_gen_put() helper is added, doing either a sock_put() or
    inet_twsk_put() [ and will support SYN_RECV later ].
    
    Note this helper should only be called in real slow path, when rcu
    lookup found a socket that was moved to another identity (freed/reused
    immediately), but could eventually be used in other contexts, like
    sock_edemux()
    
    Before patch :
    
    dmesg | grep "TCP established"
    
    TCP established hash table entries: 524288 (order: 11, 8388608 bytes)
    
    After patch :
    
    TCP established hash table entries: 524288 (order: 10, 4194304 bytes)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7cf8d2331afb..3f3e48c4704d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -156,7 +156,7 @@ typedef __u64 __bitwise __addrpair;
  */
 struct sock_common {
 	/* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned
-	 * address on 64bit arches : cf INET_MATCH() and INET_TW_MATCH()
+	 * address on 64bit arches : cf INET_MATCH()
 	 */
 	union {
 		__addrpair	skc_addrpair;
@@ -301,6 +301,8 @@ struct sock {
 #define sk_dontcopy_end		__sk_common.skc_dontcopy_end
 #define sk_hash			__sk_common.skc_hash
 #define sk_portpair		__sk_common.skc_portpair
+#define sk_num			__sk_common.skc_num
+#define sk_dport		__sk_common.skc_dport
 #define sk_addrpair		__sk_common.skc_addrpair
 #define sk_daddr		__sk_common.skc_daddr
 #define sk_rcv_saddr		__sk_common.skc_rcv_saddr
@@ -1653,6 +1655,10 @@ static inline void sock_put(struct sock *sk)
 	if (atomic_dec_and_test(&sk->sk_refcnt))
 		sk_free(sk);
 }
+/* Generic version of sock_put(), dealing with all sockets
+ * (TCP_TIMEWAIT, ESTABLISHED...)
+ */
+void sock_gen_put(struct sock *sk);
 
 int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested);
 

commit 53af53ae83fe960ceb9ef74cac7915e9088f4266
Merge: b343ca84b4e3 9684d7b0dab3
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Oct 8 23:07:53 2013 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            include/linux/netdevice.h
            net/core/sock.c
    
    Trivial merge issues.
    
    Removal of "extern" for functions declaration in netdevice.h
    at the same time "const" was added to an argument.
    
    Two parallel line additions in net/core/sock.c
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 421b3885bf6d56391297844f43fb7154a6396e12
Author: Shawn Bohrer <sbohrer@rgmadvisors.com>
Date:   Mon Oct 7 11:01:39 2013 -0500

    udp: ipv4: Add udp early demux
    
    The removal of the routing cache introduced a performance regression for
    some UDP workloads since a dst lookup must be done for each packet.
    This change caches the dst per socket in a similar manner to what we do
    for TCP by implementing early_demux.
    
    For UDP multicast we can only cache the dst if there is only one
    receiving socket on the host.  Since caching only works when there is
    one receiving socket we do the multicast socket lookup using RCU.
    
    For UDP unicast we only demux sockets with an exact match in order to
    not break forwarding setups.  Additionally since the hash chains may be
    long we only check the first socket to see if it is a match and not
    waste extra time searching the whole chain when we might not find an
    exact match.
    
    Benchmark results from a netperf UDP_RR test:
    Before 87961.22 transactions/s
    After  89789.68 transactions/s
    
    Benchmark results from a fio 1 byte UDP multicast pingpong test
    (Multicast one way unicast response):
    Before 12.97us RTT
    After  12.63us RTT
    
    Signed-off-by: Shawn Bohrer <sbohrer@rgmadvisors.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e3bf213be625..79532540201b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -218,7 +218,7 @@ struct cg_proto;
   *	@sk_lock:	synchronizer
   *	@sk_rcvbuf: size of receive buffer in bytes
   *	@sk_wq: sock wait queue and async head
-  *	@sk_rx_dst: receive input route used by early tcp demux
+  *	@sk_rx_dst: receive input route used by early demux
   *	@sk_dst_cache: destination cache
   *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy

commit d45ed4a4e33ae103053c0a53d280014e7101bb5c
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Oct 4 00:14:06 2013 -0700

    net: fix unsafe set_memory_rw from softirq
    
    on x86 system with net.core.bpf_jit_enable = 1
    
    sudo tcpdump -i eth1 'tcp port 22'
    
    causes the warning:
    [   56.766097]  Possible unsafe locking scenario:
    [   56.766097]
    [   56.780146]        CPU0
    [   56.786807]        ----
    [   56.793188]   lock(&(&vb->lock)->rlock);
    [   56.799593]   <Interrupt>
    [   56.805889]     lock(&(&vb->lock)->rlock);
    [   56.812266]
    [   56.812266]  *** DEADLOCK ***
    [   56.812266]
    [   56.830670] 1 lock held by ksoftirqd/1/13:
    [   56.836838]  #0:  (rcu_read_lock){.+.+..}, at: [<ffffffff8118f44c>] vm_unmap_aliases+0x8c/0x380
    [   56.849757]
    [   56.849757] stack backtrace:
    [   56.862194] CPU: 1 PID: 13 Comm: ksoftirqd/1 Not tainted 3.12.0-rc3+ #45
    [   56.868721] Hardware name: System manufacturer System Product Name/P8Z77 WS, BIOS 3007 07/26/2012
    [   56.882004]  ffffffff821944c0 ffff88080bbdb8c8 ffffffff8175a145 0000000000000007
    [   56.895630]  ffff88080bbd5f40 ffff88080bbdb928 ffffffff81755b14 0000000000000001
    [   56.909313]  ffff880800000001 ffff880800000000 ffffffff8101178f 0000000000000001
    [   56.923006] Call Trace:
    [   56.929532]  [<ffffffff8175a145>] dump_stack+0x55/0x76
    [   56.936067]  [<ffffffff81755b14>] print_usage_bug+0x1f7/0x208
    [   56.942445]  [<ffffffff8101178f>] ? save_stack_trace+0x2f/0x50
    [   56.948932]  [<ffffffff810cc0a0>] ? check_usage_backwards+0x150/0x150
    [   56.955470]  [<ffffffff810ccb52>] mark_lock+0x282/0x2c0
    [   56.961945]  [<ffffffff810ccfed>] __lock_acquire+0x45d/0x1d50
    [   56.968474]  [<ffffffff810cce6e>] ? __lock_acquire+0x2de/0x1d50
    [   56.975140]  [<ffffffff81393bf5>] ? cpumask_next_and+0x55/0x90
    [   56.981942]  [<ffffffff810cef72>] lock_acquire+0x92/0x1d0
    [   56.988745]  [<ffffffff8118f52a>] ? vm_unmap_aliases+0x16a/0x380
    [   56.995619]  [<ffffffff817628f1>] _raw_spin_lock+0x41/0x50
    [   57.002493]  [<ffffffff8118f52a>] ? vm_unmap_aliases+0x16a/0x380
    [   57.009447]  [<ffffffff8118f52a>] vm_unmap_aliases+0x16a/0x380
    [   57.016477]  [<ffffffff8118f44c>] ? vm_unmap_aliases+0x8c/0x380
    [   57.023607]  [<ffffffff810436b0>] change_page_attr_set_clr+0xc0/0x460
    [   57.030818]  [<ffffffff810cfb8d>] ? trace_hardirqs_on+0xd/0x10
    [   57.037896]  [<ffffffff811a8330>] ? kmem_cache_free+0xb0/0x2b0
    [   57.044789]  [<ffffffff811b59c3>] ? free_object_rcu+0x93/0xa0
    [   57.051720]  [<ffffffff81043d9f>] set_memory_rw+0x2f/0x40
    [   57.058727]  [<ffffffff8104e17c>] bpf_jit_free+0x2c/0x40
    [   57.065577]  [<ffffffff81642cba>] sk_filter_release_rcu+0x1a/0x30
    [   57.072338]  [<ffffffff811108e2>] rcu_process_callbacks+0x202/0x7c0
    [   57.078962]  [<ffffffff81057f17>] __do_softirq+0xf7/0x3f0
    [   57.085373]  [<ffffffff81058245>] run_ksoftirqd+0x35/0x70
    
    cannot reuse jited filter memory, since it's readonly,
    so use original bpf insns memory to hold work_struct
    
    defer kfree of sk_filter until jit completed freeing
    
    tested on x86_64 and i386
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1d37a8086bed..808cbc2ec6c1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1630,16 +1630,14 @@ static inline void sk_filter_release(struct sk_filter *fp)
 
 static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
 {
-	unsigned int size = sk_filter_len(fp);
-
-	atomic_sub(size, &sk->sk_omem_alloc);
+	atomic_sub(sk_filter_size(fp->len), &sk->sk_omem_alloc);
 	sk_filter_release(fp);
 }
 
 static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)
 {
 	atomic_inc(&fp->refcnt);
-	atomic_add(sk_filter_len(fp), &sk->sk_omem_alloc);
+	atomic_add(sk_filter_size(fp->len), &sk->sk_omem_alloc);
 }
 
 /*

commit 5080546682bae3d32734b18e281091684f0ebbe4
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 2 04:29:50 2013 -0700

    inet: consolidate INET_TW_MATCH
    
    TCP listener refactoring, part 2 :
    
    We can use a generic lookup, sockets being in whatever state, if
    we are sure all relevant fields are at the same place in all socket
    types (ESTABLISH, TIME_WAIT, SYN_RECV)
    
    This patch removes these macros :
    
     inet_addrpair, inet_addrpair, tw_addrpair, tw_portpair
    
    And adds :
    
     sk_portpair, sk_addrpair, sk_daddr, sk_rcv_saddr
    
    Then, INET_TW_MATCH() is really the same than INET_MATCH()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f0a44cc1ff9d..e3bf213be625 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -300,6 +300,10 @@ struct sock {
 #define sk_dontcopy_begin	__sk_common.skc_dontcopy_begin
 #define sk_dontcopy_end		__sk_common.skc_dontcopy_end
 #define sk_hash			__sk_common.skc_hash
+#define sk_portpair		__sk_common.skc_portpair
+#define sk_addrpair		__sk_common.skc_addrpair
+#define sk_daddr		__sk_common.skc_daddr
+#define sk_rcv_saddr		__sk_common.skc_rcv_saddr
 #define sk_family		__sk_common.skc_family
 #define sk_state		__sk_common.skc_state
 #define sk_reuse		__sk_common.skc_reuse

commit 4fbef95af4e62d4aada6c1728e04d3b1c828abe0
Merge: 5229432f15e6 c31eeaced22c
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Oct 1 17:06:14 2013 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/emulex/benet/be.h
            drivers/net/usb/qmi_wwan.c
            drivers/net/wireless/brcm80211/brcmfmac/dhd_bus.h
            include/net/netfilter/nf_conntrack_synproxy.h
            include/net/secure_seq.h
    
    The conflicts are of two varieties:
    
    1) Conflicts with Joe Perches's 'extern' removal from header file
       function declarations.  Usually it's an argument signature change
       or a function being added/removed.  The resolutions are trivial.
    
    2) Some overlapping changes in qmi_wwan.c and be.h, one commit adds
       a new value, another changes an existing value.  That sort of
       thing.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c3f40d7c04152c6f168db2f9b43438015cf092c4
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Sep 29 01:12:40 2013 -0700

    net: add missing sk_max_pacing_rate doc
    
    Warning(include/net/sock.h:411): No description found for parameter
    'sk_max_pacing_rate'
    
    Lets please "make htmldocs" and kbuild bot.
    
    Reported-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 240aa3f08cd6..cf91c8edcfc4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -233,6 +233,7 @@ struct cg_proto;
   *	@sk_ll_usec: usecs to busypoll when there is no data
   *	@sk_allocation: allocation mode
   *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
+  *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
   *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings

commit 559835ea7292e2f09304d81eda16f4209433245e
Author: Pravin B Shelar <pshelar@nicira.com>
Date:   Tue Sep 24 10:25:40 2013 -0700

    vxlan: Use RCU apis to access sk_user_data.
    
    Use of RCU api makes vxlan code easier to understand.  It also
    fixes bug due to missing ACCESS_ONCE() on sk_user_data dereference.
    In rare case without ACCESS_ONCE() compiler might omit vs on
    sk_user_data dereference.
    Compiler can use vs as alias for sk->sk_user_data, resulting in
    multiple sk_user_data dereference in rcu read context which
    could change.
    
    CC: Jesse Gross <jesse@nicira.com>
    Signed-off-by: Pravin B Shelar <pshelar@nicira.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6ba2e7b0e2b1..1d37a8086bed 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -409,6 +409,11 @@ struct sock {
 	void                    (*sk_destruct)(struct sock *sk);
 };
 
+#define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))
+
+#define rcu_dereference_sk_user_data(sk)	rcu_dereference(__sk_user_data((sk)))
+#define rcu_assign_sk_user_data(sk, ptr)	rcu_assign_pointer(__sk_user_data((sk)), ptr)
+
 /*
  * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK
  * or not whether his port will be reused by someone else. SK_FORCE_REUSE

commit 62748f32d501f5d3712a7c372bbb92abc7c62bc7
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 24 08:20:52 2013 -0700

    net: introduce SO_MAX_PACING_RATE
    
    As mentioned in commit afe4fd062416b ("pkt_sched: fq: Fair Queue packet
    scheduler"), this patch adds a new socket option.
    
    SO_MAX_PACING_RATE offers the application the ability to cap the
    rate computed by transport layer. Value is in bytes per second.
    
    u32 val = 1000000;
    setsockopt(sockfd, SOL_SOCKET, SO_MAX_PACING_RATE, &val, sizeof(val));
    
    To be effectively paced, a flow must use FQ packet scheduler.
    
    Note that a packet scheduler takes into account the headers for its
    computations. The effective payload rate depends on MSS and retransmits
    if any.
    
    I chose to make this pacing rate a SOL_SOCKET option instead of a
    TCP one because this can be used by other protocols.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Steinar H. Gunderson <sesse@google.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4625d2eff461..240aa3f08cd6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -363,6 +363,7 @@ struct sock {
 	int			sk_wmem_queued;
 	gfp_t			sk_allocation;
 	u32			sk_pacing_rate; /* bytes per second */
+	u32			sk_max_pacing_rate;
 	netdev_features_t	sk_route_caps;
 	netdev_features_t	sk_route_nocaps;
 	int			sk_gso_type;

commit 69336bd2d3ddce315dc57be894e4b57a91036a14
Author: Joe Perches <joe@perches.com>
Date:   Sun Sep 22 10:32:26 2013 -0700

    sock.h: Remove extern from function prototypes
    
    There are a mix of function prototypes with and without extern
    in the kernel sources.  Standardize on not using extern for
    function prototypes.
    
    Function prototypes don't need to be written with extern.
    extern is assumed by the compiler.  Its use is as unnecessary as
    using auto to declare automatic/local variables in a block.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6ba2e7b0e2b1..4625d2eff461 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -746,7 +746,7 @@ static inline int sk_stream_wspace(const struct sock *sk)
 	return sk->sk_sndbuf - sk->sk_wmem_queued;
 }
 
-extern void sk_stream_write_space(struct sock *sk);
+void sk_stream_write_space(struct sock *sk);
 
 /* OOB backlog add */
 static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
@@ -788,7 +788,7 @@ static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *s
 	return 0;
 }
 
-extern int __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb);
+int __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb);
 
 static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 {
@@ -853,15 +853,15 @@ static inline void sock_rps_reset_rxhash(struct sock *sk)
 		__rc;							\
 	})
 
-extern int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
-extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
-extern void sk_stream_wait_close(struct sock *sk, long timeo_p);
-extern int sk_stream_error(struct sock *sk, int flags, int err);
-extern void sk_stream_kill_queues(struct sock *sk);
-extern void sk_set_memalloc(struct sock *sk);
-extern void sk_clear_memalloc(struct sock *sk);
+int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
+int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
+void sk_stream_wait_close(struct sock *sk, long timeo_p);
+int sk_stream_error(struct sock *sk, int flags, int err);
+void sk_stream_kill_queues(struct sock *sk);
+void sk_set_memalloc(struct sock *sk);
+void sk_clear_memalloc(struct sock *sk);
 
-extern int sk_wait_data(struct sock *sk, long *timeo);
+int sk_wait_data(struct sock *sk, long *timeo);
 
 struct request_sock_ops;
 struct timewait_sock_ops;
@@ -1031,8 +1031,8 @@ struct cg_proto {
 	struct mem_cgroup	*memcg;
 };
 
-extern int proto_register(struct proto *prot, int alloc_slab);
-extern void proto_unregister(struct proto *prot);
+int proto_register(struct proto *prot, int alloc_slab);
+void proto_unregister(struct proto *prot);
 
 static inline bool memcg_proto_active(struct cg_proto *cg_proto)
 {
@@ -1287,8 +1287,8 @@ proto_memory_pressure(struct proto *prot)
 
 #ifdef CONFIG_PROC_FS
 /* Called with local bh disabled */
-extern void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
-extern int sock_prot_inuse_get(struct net *net, struct proto *proto);
+void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
+int sock_prot_inuse_get(struct net *net, struct proto *proto);
 #else
 static inline void sock_prot_inuse_add(struct net *net, struct proto *prot,
 		int inc)
@@ -1364,8 +1364,8 @@ static inline struct inode *SOCK_INODE(struct socket *socket)
 /*
  * Functions for memory accounting
  */
-extern int __sk_mem_schedule(struct sock *sk, int size, int kind);
-extern void __sk_mem_reclaim(struct sock *sk);
+int __sk_mem_schedule(struct sock *sk, int size, int kind);
+void __sk_mem_reclaim(struct sock *sk);
 
 #define SK_MEM_QUANTUM ((int)PAGE_SIZE)
 #define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
@@ -1473,14 +1473,14 @@ do {									\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
-extern void lock_sock_nested(struct sock *sk, int subclass);
+void lock_sock_nested(struct sock *sk, int subclass);
 
 static inline void lock_sock(struct sock *sk)
 {
 	lock_sock_nested(sk, 0);
 }
 
-extern void release_sock(struct sock *sk);
+void release_sock(struct sock *sk);
 
 /* BH context may only use the following locking interface. */
 #define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
@@ -1489,7 +1489,7 @@ extern void release_sock(struct sock *sk);
 				SINGLE_DEPTH_NESTING)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
-extern bool lock_sock_fast(struct sock *sk);
+bool lock_sock_fast(struct sock *sk);
 /**
  * unlock_sock_fast - complement of lock_sock_fast
  * @sk: socket
@@ -1507,108 +1507,84 @@ static inline void unlock_sock_fast(struct sock *sk, bool slow)
 }
 
 
-extern struct sock		*sk_alloc(struct net *net, int family,
-					  gfp_t priority,
-					  struct proto *prot);
-extern void			sk_free(struct sock *sk);
-extern void			sk_release_kernel(struct sock *sk);
-extern struct sock		*sk_clone_lock(const struct sock *sk,
-					       const gfp_t priority);
-
-extern struct sk_buff		*sock_wmalloc(struct sock *sk,
-					      unsigned long size, int force,
-					      gfp_t priority);
-extern struct sk_buff		*sock_rmalloc(struct sock *sk,
-					      unsigned long size, int force,
-					      gfp_t priority);
-extern void			sock_wfree(struct sk_buff *skb);
-extern void			skb_orphan_partial(struct sk_buff *skb);
-extern void			sock_rfree(struct sk_buff *skb);
-extern void			sock_edemux(struct sk_buff *skb);
-
-extern int			sock_setsockopt(struct socket *sock, int level,
-						int op, char __user *optval,
-						unsigned int optlen);
-
-extern int			sock_getsockopt(struct socket *sock, int level,
-						int op, char __user *optval,
-						int __user *optlen);
-extern struct sk_buff		*sock_alloc_send_skb(struct sock *sk,
-						     unsigned long size,
-						     int noblock,
-						     int *errcode);
-extern struct sk_buff		*sock_alloc_send_pskb(struct sock *sk,
-						      unsigned long header_len,
-						      unsigned long data_len,
-						      int noblock,
-						      int *errcode,
-						      int max_page_order);
-extern void *sock_kmalloc(struct sock *sk, int size,
-			  gfp_t priority);
-extern void sock_kfree_s(struct sock *sk, void *mem, int size);
-extern void sk_send_sigurg(struct sock *sk);
+struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
+		      struct proto *prot);
+void sk_free(struct sock *sk);
+void sk_release_kernel(struct sock *sk);
+struct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);
+
+struct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,
+			     gfp_t priority);
+struct sk_buff *sock_rmalloc(struct sock *sk, unsigned long size, int force,
+			     gfp_t priority);
+void sock_wfree(struct sk_buff *skb);
+void skb_orphan_partial(struct sk_buff *skb);
+void sock_rfree(struct sk_buff *skb);
+void sock_edemux(struct sk_buff *skb);
+
+int sock_setsockopt(struct socket *sock, int level, int op,
+		    char __user *optval, unsigned int optlen);
+
+int sock_getsockopt(struct socket *sock, int level, int op,
+		    char __user *optval, int __user *optlen);
+struct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,
+				    int noblock, int *errcode);
+struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
+				     unsigned long data_len, int noblock,
+				     int *errcode, int max_page_order);
+void *sock_kmalloc(struct sock *sk, int size, gfp_t priority);
+void sock_kfree_s(struct sock *sk, void *mem, int size);
+void sk_send_sigurg(struct sock *sk);
 
 /*
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.
  */
-extern int                      sock_no_bind(struct socket *,
-					     struct sockaddr *, int);
-extern int                      sock_no_connect(struct socket *,
-						struct sockaddr *, int, int);
-extern int                      sock_no_socketpair(struct socket *,
-						   struct socket *);
-extern int                      sock_no_accept(struct socket *,
-					       struct socket *, int);
-extern int                      sock_no_getname(struct socket *,
-						struct sockaddr *, int *, int);
-extern unsigned int             sock_no_poll(struct file *, struct socket *,
-					     struct poll_table_struct *);
-extern int                      sock_no_ioctl(struct socket *, unsigned int,
-					      unsigned long);
-extern int			sock_no_listen(struct socket *, int);
-extern int                      sock_no_shutdown(struct socket *, int);
-extern int			sock_no_getsockopt(struct socket *, int , int,
-						   char __user *, int __user *);
-extern int			sock_no_setsockopt(struct socket *, int, int,
-						   char __user *, unsigned int);
-extern int                      sock_no_sendmsg(struct kiocb *, struct socket *,
-						struct msghdr *, size_t);
-extern int                      sock_no_recvmsg(struct kiocb *, struct socket *,
-						struct msghdr *, size_t, int);
-extern int			sock_no_mmap(struct file *file,
-					     struct socket *sock,
-					     struct vm_area_struct *vma);
-extern ssize_t			sock_no_sendpage(struct socket *sock,
-						struct page *page,
-						int offset, size_t size,
-						int flags);
+int sock_no_bind(struct socket *, struct sockaddr *, int);
+int sock_no_connect(struct socket *, struct sockaddr *, int, int);
+int sock_no_socketpair(struct socket *, struct socket *);
+int sock_no_accept(struct socket *, struct socket *, int);
+int sock_no_getname(struct socket *, struct sockaddr *, int *, int);
+unsigned int sock_no_poll(struct file *, struct socket *,
+			  struct poll_table_struct *);
+int sock_no_ioctl(struct socket *, unsigned int, unsigned long);
+int sock_no_listen(struct socket *, int);
+int sock_no_shutdown(struct socket *, int);
+int sock_no_getsockopt(struct socket *, int , int, char __user *, int __user *);
+int sock_no_setsockopt(struct socket *, int, int, char __user *, unsigned int);
+int sock_no_sendmsg(struct kiocb *, struct socket *, struct msghdr *, size_t);
+int sock_no_recvmsg(struct kiocb *, struct socket *, struct msghdr *, size_t,
+		    int);
+int sock_no_mmap(struct file *file, struct socket *sock,
+		 struct vm_area_struct *vma);
+ssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset,
+			 size_t size, int flags);
 
 /*
  * Functions to fill in entries in struct proto_ops when a protocol
  * uses the inet style.
  */
-extern int sock_common_getsockopt(struct socket *sock, int level, int optname,
+int sock_common_getsockopt(struct socket *sock, int level, int optname,
 				  char __user *optval, int __user *optlen);
-extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
+int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
 			       struct msghdr *msg, size_t size, int flags);
-extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
+int sock_common_setsockopt(struct socket *sock, int level, int optname,
 				  char __user *optval, unsigned int optlen);
-extern int compat_sock_common_getsockopt(struct socket *sock, int level,
+int compat_sock_common_getsockopt(struct socket *sock, int level,
 		int optname, char __user *optval, int __user *optlen);
-extern int compat_sock_common_setsockopt(struct socket *sock, int level,
+int compat_sock_common_setsockopt(struct socket *sock, int level,
 		int optname, char __user *optval, unsigned int optlen);
 
-extern void sk_common_release(struct sock *sk);
+void sk_common_release(struct sock *sk);
 
 /*
  *	Default socket callbacks and setup code
  */
 
 /* Initialise core socket variables */
-extern void sock_init_data(struct socket *sock, struct sock *sk);
+void sock_init_data(struct socket *sock, struct sock *sk);
 
-extern void sk_filter_release_rcu(struct rcu_head *rcu);
+void sk_filter_release_rcu(struct rcu_head *rcu);
 
 /**
  *	sk_filter_release - release a socket filter
@@ -1669,8 +1645,7 @@ static inline void sock_put(struct sock *sk)
 		sk_free(sk);
 }
 
-extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
-			  const int nested);
+int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested);
 
 static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
 {
@@ -1724,8 +1699,8 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
-extern kuid_t sock_i_uid(struct sock *sk);
-extern unsigned long sock_i_ino(struct sock *sk);
+kuid_t sock_i_uid(struct sock *sk);
+unsigned long sock_i_ino(struct sock *sk);
 
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
@@ -1747,7 +1722,7 @@ sk_dst_get(struct sock *sk)
 	return dst;
 }
 
-extern void sk_reset_txq(struct sock *sk);
+void sk_reset_txq(struct sock *sk);
 
 static inline void dst_negative_advice(struct sock *sk)
 {
@@ -1800,16 +1775,16 @@ sk_dst_reset(struct sock *sk)
 	spin_unlock(&sk->sk_dst_lock);
 }
 
-extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
+struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
-extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
+struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
 static inline bool sk_can_gso(const struct sock *sk)
 {
 	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
 }
 
-extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
+void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
 
 static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
 {
@@ -2022,14 +1997,14 @@ static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 	sk_mem_charge(sk, skb->truesize);
 }
 
-extern void sk_reset_timer(struct sock *sk, struct timer_list *timer,
-			   unsigned long expires);
+void sk_reset_timer(struct sock *sk, struct timer_list *timer,
+		    unsigned long expires);
 
-extern void sk_stop_timer(struct sock *sk, struct timer_list *timer);
+void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
-extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
+int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
-extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
+int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
 
 /*
  *	Recover an error report and clear atomically
@@ -2097,7 +2072,7 @@ static inline struct page_frag *sk_page_frag(struct sock *sk)
 	return &sk->sk_frag;
 }
 
-extern bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
+bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
 
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
@@ -2135,10 +2110,10 @@ static inline int sock_intr_errno(long timeo)
 	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
 }
 
-extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
-	struct sk_buff *skb);
-extern void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
-	struct sk_buff *skb);
+void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
+			   struct sk_buff *skb);
+void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
+			     struct sk_buff *skb);
 
 static inline void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
@@ -2171,8 +2146,8 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 		__sock_recv_wifi_status(msg, sk, skb);
 }
 
-extern void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
-				     struct sk_buff *skb);
+void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
+			      struct sk_buff *skb);
 
 static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 					  struct sk_buff *skb)
@@ -2197,7 +2172,7 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
  *
  * Currently only depends on SOCK_TIMESTAMPING* flags.
  */
-extern void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
+void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed
@@ -2261,11 +2236,11 @@ static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 	return NULL;
 }
 
-extern void sock_enable_timestamp(struct sock *sk, int flag);
-extern int sock_get_timestamp(struct sock *, struct timeval __user *);
-extern int sock_get_timestampns(struct sock *, struct timespec __user *);
-extern int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len,
-			      int level, int type);
+void sock_enable_timestamp(struct sock *sk, int flag);
+int sock_get_timestamp(struct sock *, struct timeval __user *);
+int sock_get_timestampns(struct sock *, struct timespec __user *);
+int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len, int level,
+		       int type);
 
 /*
  *	Enable debug/info messages

commit 95bd09eb27507691520d39ee1044d6ad831c1168
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Aug 27 05:46:32 2013 -0700

    tcp: TSO packets automatic sizing
    
    After hearing many people over past years complaining against TSO being
    bursty or even buggy, we are proud to present automatic sizing of TSO
    packets.
    
    One part of the problem is that tcp_tso_should_defer() uses an heuristic
    relying on upcoming ACKS instead of a timer, but more generally, having
    big TSO packets makes little sense for low rates, as it tends to create
    micro bursts on the network, and general consensus is to reduce the
    buffering amount.
    
    This patch introduces a per socket sk_pacing_rate, that approximates
    the current sending rate, and allows us to size the TSO packets so
    that we try to send one packet every ms.
    
    This field could be set by other transports.
    
    Patch has no impact for high speed flows, where having large TSO packets
    makes sense to reach line rate.
    
    For other flows, this helps better packet scheduling and ACK clocking.
    
    This patch increases performance of TCP flows in lossy environments.
    
    A new sysctl (tcp_min_tso_segs) is added, to specify the
    minimal size of a TSO packet (default being 2).
    
    A follow-up patch will provide a new packet scheduler (FQ), using
    sk_pacing_rate as an input to perform optional per flow pacing.
    
    This explains why we chose to set sk_pacing_rate to twice the current
    rate, allowing 'slow start' ramp up.
    
    sk_pacing_rate = 2 * cwnd * mss / srtt
    
    v2: Neal Cardwell reported a suspect deferring of last two segments on
    initial write of 10 MSS, I had to change tcp_tso_should_defer() to take
    into account tp->xmit_size_goal_segs
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Van Jacobson <vanj@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e4bbcbfd07ea..6ba2e7b0e2b1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -232,6 +232,7 @@ struct cg_proto;
   *	@sk_napi_id: id of the last napi context to receive data for sk
   *	@sk_ll_usec: usecs to busypoll when there is no data
   *	@sk_allocation: allocation mode
+  *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
   *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
@@ -361,6 +362,7 @@ struct sock {
 	kmemcheck_bitfield_end(flags);
 	int			sk_wmem_queued;
 	gfp_t			sk_allocation;
+	u32			sk_pacing_rate; /* bytes per second */
 	netdev_features_t	sk_route_caps;
 	netdev_features_t	sk_route_nocaps;
 	int			sk_gso_type;

commit 28d6427109d13b0f447cba5761f88d3548e83605
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Aug 8 14:38:47 2013 -0700

    net: attempt high order allocations in sock_alloc_send_pskb()
    
    Adding paged frags skbs to af_unix sockets introduced a performance
    regression on large sends because of additional page allocations, even
    if each skb could carry at least 100% more payload than before.
    
    We can instruct sock_alloc_send_pskb() to attempt high order
    allocations.
    
    Most of the time, it does a single page allocation instead of 8.
    
    I added an additional parameter to sock_alloc_send_pskb() to
    let other users to opt-in for this new feature on followup patches.
    
    Tested:
    
    Before patch :
    
    $ netperf -t STREAM_STREAM
    STREAM STREAM TEST
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/sec
    
     2304  212992  212992    10.00    46861.15
    
    After patch :
    
    $ netperf -t STREAM_STREAM
    STREAM STREAM TEST
    Recv   Send    Send
    Socket Socket  Message  Elapsed
    Size   Size    Size     Time     Throughput
    bytes  bytes   bytes    secs.    10^6bits/sec
    
     2304  212992  212992    10.00    57981.11
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ab6a8b75207e..e4bbcbfd07ea 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1539,7 +1539,8 @@ extern struct sk_buff		*sock_alloc_send_pskb(struct sock *sk,
 						      unsigned long header_len,
 						      unsigned long data_len,
 						      int noblock,
-						      int *errcode);
+						      int *errcode,
+						      int max_page_order);
 extern void *sock_kmalloc(struct sock *sk, int size,
 			  gfp_t priority);
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);

commit 0e76a3a587fc7abda2badf249053b427baad255e
Merge: fba3679d3451 72a67a94bcba
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Aug 3 21:36:46 2013 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Merge net into net-next to setup some infrastructure Eric
    Dumazet needs for usbnet changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e0d1095ae3405404d247afb00233ef837d58da83
Author: Cong Wang <amwang@redhat.com>
Date:   Thu Aug 1 11:10:25 2013 +0800

    net: rename CONFIG_NET_LL_RX_POLL to CONFIG_NET_RX_BUSY_POLL
    
    Eliezer renames several *ll_poll to *busy_poll, but forgets
    CONFIG_NET_LL_RX_POLL, so in case of confusion, rename it too.
    
    Cc: Eliezer Tamir <eliezer.tamir@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Cong Wang <amwang@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 95a5a2c6925a..31d5cfbb51ec 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -327,7 +327,7 @@ struct sock {
 #ifdef CONFIG_RPS
 	__u32			sk_rxhash;
 #endif
-#ifdef CONFIG_NET_LL_RX_POLL
+#ifdef CONFIG_NET_RX_BUSY_POLL
 	unsigned int		sk_napi_id;
 	unsigned int		sk_ll_usec;
 #endif

commit f2f872f9272a79a1048877ea14c15576f46c225e
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jul 30 17:55:08 2013 -0700

    netem: Introduce skb_orphan_partial() helper
    
    Commit 547669d483e578 ("tcp: xps: fix reordering issues") added
    unexpected reorders in case netem is used in a MQ setup for high
    performance test bed.
    
    ETH=eth0
    tc qd del dev $ETH root 2>/dev/null
    tc qd add dev $ETH root handle 1: mq
    for i in `seq 1 32`
    do
     tc qd add dev $ETH parent 1:$i netem delay 100ms
    done
    
    As all tcp packets are orphaned by netem, TCP stack believes it can
    set skb->ooo_okay on all packets.
    
    In order to allow producers to send more packets, we want to
    keep sk_wmem_alloc from reaching sk_sndbuf limit.
    
    We can do that by accounting one byte per skb in netem queues,
    so that TCP stack is not fooled too much.
    
    Tested:
    
    With above MQ/netem setup, scaling number of concurrent flows gives
    linear results and no reorders/retransmits
    
    lpq83:~# for n in 1 10 20 30 40 50 60 70 80 90 100
     do echo -n "n:$n " ; ./super_netperf $n -H 10.7.7.84; done
    n:1 198.46
    n:10 2002.69
    n:20 4000.98
    n:30 6006.35
    n:40 8020.93
    n:50 10032.3
    n:60 12081.9
    n:70 13971.3
    n:80 16009.7
    n:90 17117.3
    n:100 17425.5
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b9f2b095b1ab..53d4714709a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1520,6 +1520,7 @@ extern struct sk_buff		*sock_rmalloc(struct sock *sk,
 					      unsigned long size, int force,
 					      gfp_t priority);
 extern void			sock_wfree(struct sk_buff *skb);
+extern void			skb_orphan_partial(struct sk_buff *skb);
 extern void			sock_rfree(struct sk_buff *skb);
 extern void			sock_edemux(struct sk_buff *skb);
 

commit c9bee3b7fdecb0c1d070c7b54113b3bdfb9a3d36
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 22 20:27:07 2013 -0700

    tcp: TCP_NOTSENT_LOWAT socket option
    
    Idea of this patch is to add optional limitation of number of
    unsent bytes in TCP sockets, to reduce usage of kernel memory.
    
    TCP receiver might announce a big window, and TCP sender autotuning
    might allow a large amount of bytes in write queue, but this has little
    performance impact if a large part of this buffering is wasted :
    
    Write queue needs to be large only to deal with large BDP, not
    necessarily to cope with scheduling delays (incoming ACKS make room
    for the application to queue more bytes)
    
    For most workloads, using a value of 128 KB or less is OK to give
    applications enough time to react to POLLOUT events in time
    (or being awaken in a blocking sendmsg())
    
    This patch adds two ways to set the limit :
    
    1) Per socket option TCP_NOTSENT_LOWAT
    
    2) A sysctl (/proc/sys/net/ipv4/tcp_notsent_lowat) for sockets
    not using TCP_NOTSENT_LOWAT socket option (or setting a zero value)
    Default value being UINT_MAX (0xFFFFFFFF), meaning this has no effect.
    
    This changes poll()/select()/epoll() to report POLLOUT
    only if number of unsent bytes is below tp->nosent_lowat
    
    Note this might increase number of sendmsg()/sendfile() calls
    when using non blocking sockets,
    and increase number of context switches for blocking sockets.
    
    Note this is not related to SO_SNDLOWAT (as SO_SNDLOWAT is
    defined as :
     Specify the minimum number of bytes in the buffer until
     the socket layer will pass the data to the protocol)
    
    Tested:
    
    netperf sessions, and watching /proc/net/protocols "memory" column for TCP
    
    With 200 concurrent netperf -t TCP_STREAM sessions, amount of kernel memory
    used by TCP buffers shrinks by ~55 % (20567 pages instead of 45458)
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   45458   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   45458   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# (super_netperf 200 -t TCP_STREAM -H remote -l 90 &); sleep 60 ; grep TCP /proc/net/protocols
    TCPv6     1880      2   20567   no     208   yes  ipv6        y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    TCP       1696    508   20567   no     208   yes  kernel      y  y  y  y  y  y  y  y  y  y  y  y  y  n  y  y  y  y  y
    
    Using 128KB has no bad effect on the throughput or cpu usage
    of a single flow, although there is an increase of context switches.
    
    A bonus is that we hold socket lock for a shorter amount
    of time and should improve latencies of ACK processing.
    
    lpq83:~# echo -1 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1651584     6291456     16384  20.00   17447.90   10^6bits/s  3.13  S      -1.00  U      0.353   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
               412,514 context-switches
    
         200.034645535 seconds time elapsed
    
    lpq83:~# echo 131072 >/proc/sys/net/ipv4/tcp_notsent_lowat
    lpq83:~# perf stat -e context-switches ./netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3
    OMNI Send TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 7.7.7.84 () port 0 AF_INET : +/-2.500% @ 99% conf.
    Local       Remote      Local  Elapsed Throughput Throughput  Local Local  Remote Remote Local   Remote  Service
    Send Socket Recv Socket Send   Time               Units       CPU   CPU    CPU    CPU    Service Service Demand
    Size        Size        Size   (sec)                          Util  Util   Util   Util   Demand  Demand  Units
    Final       Final                                             %     Method %      Method
    1593240     6291456     16384  20.00   17321.16   10^6bits/s  3.35  S      -1.00  U      0.381   -1.000  usec/KB
    
     Performance counter stats for './netperf -H 7.7.7.84 -t omni -l 20 -c -i10,3':
    
             2,675,818 context-switches
    
         200.029651391 seconds time elapsed
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-By: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d0b5fdee50a2..b9f2b095b1ab 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -746,11 +746,6 @@ static inline int sk_stream_wspace(const struct sock *sk)
 
 extern void sk_stream_write_space(struct sock *sk);
 
-static inline bool sk_stream_memory_free(const struct sock *sk)
-{
-	return sk->sk_wmem_queued < sk->sk_sndbuf;
-}
-
 /* OOB backlog add */
 static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
@@ -950,6 +945,7 @@ struct proto {
 	unsigned int		inuse_idx;
 #endif
 
+	bool			(*stream_memory_free)(const struct sock *sk);
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
 	atomic_long_t		*memory_allocated;	/* Current allocated memory. */
@@ -1088,11 +1084,22 @@ static inline struct cg_proto *parent_cg_proto(struct proto *proto,
 }
 #endif
 
+static inline bool sk_stream_memory_free(const struct sock *sk)
+{
+	if (sk->sk_wmem_queued >= sk->sk_sndbuf)
+		return false;
+
+	return sk->sk_prot->stream_memory_free ?
+		sk->sk_prot->stream_memory_free(sk) : true;
+}
+
 static inline bool sk_stream_is_writeable(const struct sock *sk)
 {
-	return sk_stream_wspace(sk) >= sk_stream_min_wspace(sk);
+	return sk_stream_wspace(sk) >= sk_stream_min_wspace(sk) &&
+	       sk_stream_memory_free(sk);
 }
 
+
 static inline bool sk_has_memory_pressure(const struct sock *sk)
 {
 	return sk->sk_prot->memory_pressure != NULL;

commit 64dc61306ce7da370833289739e2f52dfc6b37ba
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 22 20:26:31 2013 -0700

    net: add sk_stream_is_writeable() helper
    
    Several call sites use the hardcoded following condition :
    
    sk_stream_wspace(sk) >= sk_stream_min_wspace(sk)
    
    Lets use a helper because TCP_NOTSENT_LOWAT support will change this
    condition for TCP sockets.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e0473f61693f..d0b5fdee50a2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1088,6 +1088,10 @@ static inline struct cg_proto *parent_cg_proto(struct proto *proto,
 }
 #endif
 
+static inline bool sk_stream_is_writeable(const struct sock *sk)
+{
+	return sk_stream_wspace(sk) >= sk_stream_min_wspace(sk);
+}
 
 static inline bool sk_has_memory_pressure(const struct sock *sk)
 {

commit cb820f8e4b7f73d1a32175e6591735b25bb5398d
Author: Richard Cochran <richardcochran@gmail.com>
Date:   Fri Jul 19 19:40:09 2013 +0200

    net: Provide a generic socket error queue delivery method for Tx time stamps.
    
    This patch moves the private error queue delivery function from the
    af_packet code to the core socket method. In this way, network layers
    only needing the error queue for transmit time stamping can share common
    code.
    
    Signed-off-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 95a5a2c6925a..e0473f61693f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2249,6 +2249,8 @@ static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 extern void sock_enable_timestamp(struct sock *sk, int flag);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);
+extern int sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len,
+			      int level, int type);
 
 /*
  *	Enable debug/info messages

commit 9eb5bf838d06aa6ddebe4aca6b5cedcf2eb53b86
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jul 3 05:02:22 2013 -0700

    net: sock: fix TCP_SKB_MIN_TRUESIZE
    
    commit eea86af6b1e18d ("net: sock: adapt SOCK_MIN_RCVBUF and
    SOCK_MIN_SNDBUF") forgot the sk_buff alignment taken into account
    in __alloc_skb() : skb->truesize = SKB_TRUESIZE(size);
    
    While above commit fixed the sender issue, the receiver is still
    dropping the second packet (on loopback device), because the receiver
    socket can not really hold two skbs :
    First packet truesize already is above sk_rcvbuf, so even TCP coalescing
    cannot help.
    
    On a typical 64bit build, each tcp skb truesize is 2304, instead of 2272
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Tested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ea6206ccc896..95a5a2c6925a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2052,7 +2052,7 @@ static inline void sk_wake_async(struct sock *sk, int how, int band)
  * Note: for send buffers, TCP works better if we can build two skbs at
  * minimum.
  */
-#define TCP_SKB_MIN_TRUESIZE	(2048 + sizeof(struct sk_buff))
+#define TCP_SKB_MIN_TRUESIZE	(2048 + SKB_DATA_ALIGN(sizeof(struct sk_buff)))
 
 #define SOCK_MIN_SNDBUF		(TCP_SKB_MIN_TRUESIZE * 2)
 #define SOCK_MIN_RCVBUF		 TCP_SKB_MIN_TRUESIZE

commit eea86af6b1e18d6fa8dc959e3ddc0100f27aff9f
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Wed Jun 19 12:51:20 2013 +0200

    net: sock: adapt SOCK_MIN_RCVBUF and SOCK_MIN_SNDBUF
    
    The current situation is that SOCK_MIN_RCVBUF is 2048 + sizeof(struct sk_buff))
    while SOCK_MIN_SNDBUF is 2048. Since in both cases, skb->truesize is used for
    sk_{r,w}mem_alloc accounting, we should have both sizes adjusted via defining a
    TCP_SKB_MIN_TRUESIZE.
    
    Further, as Eric Dumazet points out, the minimal skb truesize in transmit path is
    SKB_TRUESIZE(2048) after commit f07d960df33c5 ("tcp: avoid frag allocation for
    small frames"), and tcp_sendmsg() tries to limit skb size to half the congestion
    window, meaning we try to build two skbs at minimum. Thus, having SOCK_MIN_SNDBUF
    as 2048 can hit a small regression for some applications setting to low
    SO_SNDBUF / SO_RCVBUF. Note that we define a TCP_SKB_MIN_TRUESIZE, because
    SKB_TRUESIZE(2048) adds SKB_DATA_ALIGN(sizeof(struct skb_shared_info)), but in
    case of TCP skbs, the skb_shared_info is part of the 2048 bytes allocation for
    skb->head.
    
    The minor adaption in sk_stream_moderate_sndbuf() is to silence a warning by
    using a typed max macro, as similarly done in SOCK_MIN_RCVBUF occurences, that
    would appear otherwise.
    
    Suggested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 21db792bffa5..ea6206ccc896 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2047,18 +2047,21 @@ static inline void sk_wake_async(struct sock *sk, int how, int band)
 		sock_wake_async(sk->sk_socket, how, band);
 }
 
-#define SOCK_MIN_SNDBUF 2048
-/*
- * Since sk_rmem_alloc sums skb->truesize, even a small frame might need
- * sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak
+/* Since sk_{r,w}mem_alloc sums skb->truesize, even a small frame might
+ * need sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak.
+ * Note: for send buffers, TCP works better if we can build two skbs at
+ * minimum.
  */
-#define SOCK_MIN_RCVBUF (2048 + sizeof(struct sk_buff))
+#define TCP_SKB_MIN_TRUESIZE	(2048 + sizeof(struct sk_buff))
+
+#define SOCK_MIN_SNDBUF		(TCP_SKB_MIN_TRUESIZE * 2)
+#define SOCK_MIN_RCVBUF		 TCP_SKB_MIN_TRUESIZE
 
 static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 {
 	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
 		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
-		sk->sk_sndbuf = max(sk->sk_sndbuf, SOCK_MIN_SNDBUF);
+		sk->sk_sndbuf = max_t(u32, sk->sk_sndbuf, SOCK_MIN_SNDBUF);
 	}
 }
 

commit dafcc4380deec21d160c31411f33c8813f67f517
Author: Eliezer Tamir <eliezer.tamir@linux.intel.com>
Date:   Fri Jun 14 16:33:57 2013 +0300

    net: add socket option for low latency polling
    
    adds a socket option for low latency polling.
    This allows overriding the global sysctl value with a per-socket one.
    Unexport sysctl_net_ll_poll since for now it's not needed in modules.
    
    Signed-off-by: Eliezer Tamir <eliezer.tamir@linux.intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ac8e1818380c..21db792bffa5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -230,6 +230,7 @@ struct cg_proto;
   *	@sk_wmem_queued: persistent queue size
   *	@sk_forward_alloc: space allocated forward
   *	@sk_napi_id: id of the last napi context to receive data for sk
+  *	@sk_ll_usec: usecs to busypoll when there is no data
   *	@sk_allocation: allocation mode
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
@@ -328,6 +329,7 @@ struct sock {
 #endif
 #ifdef CONFIG_NET_LL_RX_POLL
 	unsigned int		sk_napi_id;
+	unsigned int		sk_ll_usec;
 #endif
 	atomic_t		sk_drops;
 	int			sk_rcvbuf;

commit 060212928670593fb89243640bf05cf89560b023
Author: Eliezer Tamir <eliezer.tamir@linux.intel.com>
Date:   Mon Jun 10 11:39:50 2013 +0300

    net: add low latency socket poll
    
    Adds an ndo_ll_poll method and the code that supports it.
    This method can be used by low latency applications to busy-poll
    Ethernet device queues directly from the socket code.
    sysctl_net_ll_poll controls how many microseconds to poll.
    Default is zero (disabled).
    Individual protocol support will be added by subsequent patches.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Signed-off-by: Eliezer Tamir <eliezer.tamir@linux.intel.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 66772cf8c3c5..ac8e1818380c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -229,6 +229,7 @@ struct cg_proto;
   *	@sk_omem_alloc: "o" is "option" or "other"
   *	@sk_wmem_queued: persistent queue size
   *	@sk_forward_alloc: space allocated forward
+  *	@sk_napi_id: id of the last napi context to receive data for sk
   *	@sk_allocation: allocation mode
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
@@ -324,6 +325,9 @@ struct sock {
 	int			sk_forward_alloc;
 #ifdef CONFIG_RPS
 	__u32			sk_rxhash;
+#endif
+#ifdef CONFIG_NET_LL_RX_POLL
+	unsigned int		sk_napi_id;
 #endif
 	atomic_t		sk_drops;
 	int			sk_rcvbuf;

commit f77d602124d865c38705df7fa25c03de9c284ad2
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu May 9 10:28:16 2013 +0000

    ipv6: do not clear pinet6 field
    
    We have seen multiple NULL dereferences in __inet6_lookup_established()
    
    After analysis, I found that inet6_sk() could be NULL while the
    check for sk_family == AF_INET6 was true.
    
    Bug was added in linux-2.6.29 when RCU lookups were introduced in UDP
    and TCP stacks.
    
    Once an IPv6 socket, using SLAB_DESTROY_BY_RCU is inserted in a hash
    table, we no longer can clear pinet6 field.
    
    This patch extends logic used in commit fcbdf09d9652c891
    ("net: fix nulls list corruptions in sk_prot_alloc")
    
    TCP/UDP/UDPLite IPv6 protocols provide their own .clear_sk() method
    to make sure we do not clear pinet6 field.
    
    At socket clone phase, we do not really care, as cloning the parent (non
    NULL) pinet6 is not adding a fatal race.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5c97b0fc5623..66772cf8c3c5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -866,6 +866,18 @@ struct inet_hashinfo;
 struct raw_hashinfo;
 struct module;
 
+/*
+ * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes
+ * un-modified. Special care is taken when initializing object to zero.
+ */
+static inline void sk_prot_clear_nulls(struct sock *sk, int size)
+{
+	if (offsetof(struct sock, sk_node.next) != 0)
+		memset(sk, 0, offsetof(struct sock, sk_node.next));
+	memset(&sk->sk_node.pprev, 0,
+	       size - offsetof(struct sock, sk_node.pprev));
+}
+
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
  * transport -> network interface is defined by struct inet_proto

commit bf84a01063eaab2f1a37d72d1b903445b3a25a4e
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Sun Apr 14 08:08:13 2013 +0000

    net: sock: make sock_tx_timestamp void
    
    Currently, sock_tx_timestamp() always returns 0. The comment that
    describes the sock_tx_timestamp() function wrongly says that it
    returns an error when an invalid argument is passed (from commit
    20d4947353be, ``net: socket infrastructure for SO_TIMESTAMPING'').
    Make the function void, so that we can also remove all the unneeded
    if conditions that check for such a _non-existant_ error case in the
    output path.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 08f05f964737..5c97b0fc5623 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2159,10 +2159,9 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
  * @sk:		socket sending this packet
  * @tx_flags:	filled with instructions for time stamping
  *
- * Currently only depends on SOCK_TIMESTAMPING* flags. Returns error code if
- * parameters are invalid.
+ * Currently only depends on SOCK_TIMESTAMPING* flags.
  */
-extern int sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
+extern void sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed

commit 7d4c04fc170087119727119074e72445f2bb192b
Author: Keller, Jacob E <jacob.e.keller@intel.com>
Date:   Thu Mar 28 11:19:25 2013 +0000

    net: add option to enable error queue packets waking select
    
    Currently, when a socket receives something on the error queue it only wakes up
    the socket on select if it is in the "read" list, that is the socket has
    something to read. It is useful also to wake the socket if it is in the error
    list, which would enable software to wait on error queue packets without waking
    up for regular data on the socket. The main use case is for receiving
    timestamped transmit packets which return the timestamp to the socket via the
    error queue. This enables an application to select on the socket for the error
    queue only instead of for the regular traffic.
    
    -v2-
    * Added the SO_SELECT_ERR_QUEUE socket option to every architechture specific file
    * Modified every socket poll function that checks error queue
    
    Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
    Cc: Jeffrey Kirsher <jeffrey.t.kirsher@intel.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Matthew Vick <matthew.vick@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 14f6e9d19dc7..08f05f964737 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -667,6 +667,7 @@ enum sock_flags {
 		     * user-space instead.
 		     */
 	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
+	SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)

commit b67bfe0d42cac56c512dd5da4b1b347a23f4b70a
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Wed Feb 27 17:06:00 2013 -0800

    hlist: drop the node parameter from iterators
    
    I'm not sure why, but the hlist for each entry iterators were conceived
    
            list_for_each_entry(pos, head, member)
    
    The hlist ones were greedy and wanted an extra parameter:
    
            hlist_for_each_entry(tpos, pos, head, member)
    
    Why did they need an extra pos parameter? I'm not quite sure. Not only
    they don't really need it, it also prevents the iterator from looking
    exactly like the list iterator, which is unfortunate.
    
    Besides the semantic patch, there was some manual work required:
    
     - Fix up the actual hlist iterators in linux/list.h
     - Fix up the declaration of other iterators based on the hlist ones.
     - A very small amount of places were using the 'node' parameter, this
     was modified to use 'obj->member' instead.
     - Coccinelle didn't handle the hlist_for_each_entry_safe iterator
     properly, so those had to be fixed up manually.
    
    The semantic patch which is mostly the work of Peter Senna Tschudin is here:
    
    @@
    iterator name hlist_for_each_entry, hlist_for_each_entry_continue, hlist_for_each_entry_from, hlist_for_each_entry_rcu, hlist_for_each_entry_rcu_bh, hlist_for_each_entry_continue_rcu_bh, for_each_busy_worker, ax25_uid_for_each, ax25_for_each, inet_bind_bucket_for_each, sctp_for_each_hentry, sk_for_each, sk_for_each_rcu, sk_for_each_from, sk_for_each_safe, sk_for_each_bound, hlist_for_each_entry_safe, hlist_for_each_entry_continue_rcu, nr_neigh_for_each, nr_neigh_for_each_safe, nr_node_for_each, nr_node_for_each_safe, for_each_gfn_indirect_valid_sp, for_each_gfn_sp, for_each_host;
    
    type T;
    expression a,c,d,e;
    identifier b;
    statement S;
    @@
    
    -T b;
        <+... when != b
    (
    hlist_for_each_entry(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue(a,
    - b,
    c) S
    |
    hlist_for_each_entry_from(a,
    - b,
    c) S
    |
    hlist_for_each_entry_rcu(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_rcu_bh(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue_rcu_bh(a,
    - b,
    c) S
    |
    for_each_busy_worker(a, c,
    - b,
    d) S
    |
    ax25_uid_for_each(a,
    - b,
    c) S
    |
    ax25_for_each(a,
    - b,
    c) S
    |
    inet_bind_bucket_for_each(a,
    - b,
    c) S
    |
    sctp_for_each_hentry(a,
    - b,
    c) S
    |
    sk_for_each(a,
    - b,
    c) S
    |
    sk_for_each_rcu(a,
    - b,
    c) S
    |
    sk_for_each_from
    -(a, b)
    +(a)
    S
    + sk_for_each_from(a) S
    |
    sk_for_each_safe(a,
    - b,
    c, d) S
    |
    sk_for_each_bound(a,
    - b,
    c) S
    |
    hlist_for_each_entry_safe(a,
    - b,
    c, d, e) S
    |
    hlist_for_each_entry_continue_rcu(a,
    - b,
    c) S
    |
    nr_neigh_for_each(a,
    - b,
    c) S
    |
    nr_neigh_for_each_safe(a,
    - b,
    c, d) S
    |
    nr_node_for_each(a,
    - b,
    c) S
    |
    nr_node_for_each_safe(a,
    - b,
    c, d) S
    |
    - for_each_gfn_sp(a, c, d, b) S
    + for_each_gfn_sp(a, c, d) S
    |
    - for_each_gfn_indirect_valid_sp(a, c, d, b) S
    + for_each_gfn_indirect_valid_sp(a, c, d) S
    |
    for_each_host(a,
    - b,
    c) S
    |
    for_each_host_safe(a,
    - b,
    c, d) S
    |
    for_each_mesh_entry(a,
    - b,
    c, d) S
    )
        ...+>
    
    [akpm@linux-foundation.org: drop bogus change from net/ipv4/raw.c]
    [akpm@linux-foundation.org: drop bogus hunk from net/ipv6/raw.c]
    [akpm@linux-foundation.org: checkpatch fixes]
    [akpm@linux-foundation.org: fix warnings]
    [akpm@linux-foudnation.org: redo intrusive kvm changes]
    Tested-by: Peter Senna Tschudin <peter.senna@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index a66caa223d18..14f6e9d19dc7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -606,24 +606,23 @@ static inline void sk_add_bind_node(struct sock *sk,
 	hlist_add_head(&sk->sk_bind_node, list);
 }
 
-#define sk_for_each(__sk, node, list) \
-	hlist_for_each_entry(__sk, node, list, sk_node)
-#define sk_for_each_rcu(__sk, node, list) \
-	hlist_for_each_entry_rcu(__sk, node, list, sk_node)
+#define sk_for_each(__sk, list) \
+	hlist_for_each_entry(__sk, list, sk_node)
+#define sk_for_each_rcu(__sk, list) \
+	hlist_for_each_entry_rcu(__sk, list, sk_node)
 #define sk_nulls_for_each(__sk, node, list) \
 	hlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)
 #define sk_nulls_for_each_rcu(__sk, node, list) \
 	hlist_nulls_for_each_entry_rcu(__sk, node, list, sk_nulls_node)
-#define sk_for_each_from(__sk, node) \
-	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
-		hlist_for_each_entry_from(__sk, node, sk_node)
+#define sk_for_each_from(__sk) \
+	hlist_for_each_entry_from(__sk, sk_node)
 #define sk_nulls_for_each_from(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \
 		hlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)
-#define sk_for_each_safe(__sk, node, tmp, list) \
-	hlist_for_each_entry_safe(__sk, node, tmp, list, sk_node)
-#define sk_for_each_bound(__sk, node, list) \
-	hlist_for_each_entry(__sk, node, list, sk_bind_node)
+#define sk_for_each_safe(__sk, tmp, list) \
+	hlist_for_each_entry_safe(__sk, tmp, list, sk_node)
+#define sk_for_each_bound(__sk, list) \
+	hlist_for_each_entry(__sk, list, sk_bind_node)
 
 static inline struct user_namespace *sk_user_ns(struct sock *sk)
 {

commit 6338a53a2bd02d5878ab449371323364b7cc7694
Merge: 8064b3cf750e 18cf0d0784b4
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Feb 18 23:32:49 2013 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net into net
    
    Pull in 'net' to take in the bug fixes that didn't make it into
    3.8-final.
    
    Also, deal with the semantic conflict of the change made to
    net/ipv6/xfrm6_policy.c   A missing rt6->n neighbour release
    was added to 'net', but in 'net-next' we no longer cache the
    neighbour entries in the ipv6 routes so that change is not
    appropriate there.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit dec34fb0f5b7873de45132a84a3af29e61084a6b
Author: Ying Xue <ying.xue@windriver.com>
Date:   Fri Feb 15 22:28:25 2013 +0000

    net: fix a compile error when SOCK_REFCNT_DEBUG is enabled
    
    When SOCK_REFCNT_DEBUG is enabled, below build error is met:
    
    kernel/sysctl_binary.o: In function `sk_refcnt_debug_release':
    include/net/sock.h:1025: multiple definition of `sk_refcnt_debug_release'
    kernel/sysctl.o:include/net/sock.h:1025: first defined here
    kernel/audit.o: In function `sk_refcnt_debug_release':
    include/net/sock.h:1025: multiple definition of `sk_refcnt_debug_release'
    kernel/sysctl.o:include/net/sock.h:1025: first defined here
    make[1]: *** [kernel/built-in.o] Error 1
    make: *** [kernel] Error 2
    
    So we decide to make sk_refcnt_debug_release static to eliminate
    the error.
    
    Signed-off-by: Ying Xue <ying.xue@windriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 182ca99405ad..25afaa013320 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1037,7 +1037,7 @@ static inline void sk_refcnt_debug_dec(struct sock *sk)
 	       sk->sk_prot->name, sk, atomic_read(&sk->sk_prot->socks));
 }
 
-inline void sk_refcnt_debug_release(const struct sock *sk)
+static inline void sk_refcnt_debug_release(const struct sock *sk)
 {
 	if (atomic_read(&sk->sk_refcnt) != 1)
 		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",

commit 0e36cbb344575e481167e090f0926701f83207d6
Author: Cong Wang <amwang@redhat.com>
Date:   Tue Jan 22 21:09:51 2013 +0000

    net: add RCU annotation to sk_dst_cache field
    
    sock->sk_dst_cache is protected by RCU.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Cong Wang <amwang@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 581dc6bd7dc6..a340ab46b41c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -340,7 +340,7 @@ struct sock {
 #endif
 	unsigned long 		sk_flags;
 	struct dst_entry	*sk_rx_dst;
-	struct dst_entry	*sk_dst_cache;
+	struct dst_entry __rcu	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;

commit 055dc21a1d1d219608cd4baac7d0683fb2cbbe8a
Author: Tom Herbert <therbert@google.com>
Date:   Tue Jan 22 09:49:50 2013 +0000

    soreuseport: infrastructure
    
    Definitions and macros for implementing soreusport.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5a34e2f03657..581dc6bd7dc6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -140,6 +140,7 @@ typedef __u64 __bitwise __addrpair;
  *	@skc_family: network address family
  *	@skc_state: Connection state
  *	@skc_reuse: %SO_REUSEADDR setting
+ *	@skc_reuseport: %SO_REUSEPORT setting
  *	@skc_bound_dev_if: bound device index if != 0
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
  *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
@@ -179,7 +180,8 @@ struct sock_common {
 
 	unsigned short		skc_family;
 	volatile unsigned char	skc_state;
-	unsigned char		skc_reuse;
+	unsigned char		skc_reuse:4;
+	unsigned char		skc_reuseport:4;
 	int			skc_bound_dev_if;
 	union {
 		struct hlist_node	skc_bind_node;
@@ -297,6 +299,7 @@ struct sock {
 #define sk_family		__sk_common.skc_family
 #define sk_state		__sk_common.skc_state
 #define sk_reuse		__sk_common.skc_reuse
+#define sk_reuseport		__sk_common.skc_reuseport
 #define sk_bound_dev_if		__sk_common.skc_bound_dev_if
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_prot			__sk_common.skc_prot

commit d59577b6ffd313d0ab3be39cb1ab47e29bdc9182
Author: Vincent Bernat <bernat@luffy.cx>
Date:   Wed Jan 16 22:55:49 2013 +0100

    sk-filter: Add ability to lock a socket filter program
    
    While a privileged program can open a raw socket, attach some
    restrictive filter and drop its privileges (or send the socket to an
    unprivileged program through some Unix socket), the filter can still
    be removed or modified by the unprivileged program. This commit adds a
    socket option to lock the filter (SO_LOCK_FILTER) preventing any
    modification of a socket filter program.
    
    This is similar to OpenBSD BIOCLOCK ioctl on bpf sockets, except even
    root is not allowed change/drop the filter.
    
    The state of the lock can be read with getsockopt(). No error is
    triggered if the state is not changed. -EPERM is returned when a user
    tries to remove the lock or to change/remove the filter while the lock
    is active. The check is done directly in sk_attach_filter() and
    sk_detach_filter() and does not affect only setsockopt() syscall.
    
    Signed-off-by: Vincent Bernat <bernat@luffy.cx>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 182ca99405ad..5a34e2f03657 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -664,6 +664,7 @@ enum sock_flags {
 		     * Will use last 4 bytes of packet sent from
 		     * user-space instead.
 		     */
+	SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)

commit 3d0dcfbd8fa2a1e63fabb5f8edac8b8a27860d98
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Dec 25 20:48:24 2012 +0000

    netprio_cgroup: define sk_cgrp_prioidx only if NETPRIO_CGROUP is enabled
    
    sock->sk_cgrp_prioidx won't be used at all if CONFIG_NETPRIO_CGROUP=n.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 93a6745bfdb2..182ca99405ad 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -367,7 +367,7 @@ struct sock {
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
-#ifdef CONFIG_CGROUPS
+#if IS_ENABLED(CONFIG_NETPRIO_CGROUP)
 	__u32			sk_cgrp_prioidx;
 #endif
 	struct pid		*sk_peer_pid;

commit a2013a13e68354e0c8f3696b69701803e13fb737
Merge: dadfab487325 106f9d9337f6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 12:00:02 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial branch from Jiri Kosina:
     "Usual stuff -- comment/printk typo fixes, documentation updates, dead
      code elimination."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (39 commits)
      HOWTO: fix double words typo
      x86 mtrr: fix comment typo in mtrr_bp_init
      propagate name change to comments in kernel source
      doc: Update the name of profiling based on sysfs
      treewide: Fix typos in various drivers
      treewide: Fix typos in various Kconfig
      wireless: mwifiex: Fix typo in wireless/mwifiex driver
      messages: i2o: Fix typo in messages/i2o
      scripts/kernel-doc: check that non-void fcts describe their return value
      Kernel-doc: Convention: Use a "Return" section to describe return values
      radeon: Fix typo and copy/paste error in comments
      doc: Remove unnecessary declarations from Documentation/accounting/getdelays.c
      various: Fix spelling of "asynchronous" in comments.
      Fix misspellings of "whether" in comments.
      eisa: Fix spelling of "asynchronous".
      various: Fix spelling of "registered" in comments.
      doc: fix quite a few typos within Documentation
      target: iscsi: fix comment typos in target/iscsi drivers
      treewide: fix typo of "suport" in various comments and Kconfig
      treewide: fix typo of "suppport" in various comments
      ...

commit 077b393d05915f04e2629bfc47c6fce95cae7d3f
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Dec 2 07:33:10 2012 +0000

    net: fix sparse endianness warnings on sock_common
    
    # make C=2 CF=-D__CHECK_ENDIAN__ net/ipv4/inet_hashtables.o
    ...
    net/ipv4/inet_hashtables.c:242:7: warning: restricted __portpair degrades to integer
    net/ipv4/inet_hashtables.c:242:7: warning: restricted __addrpair degrades to integer
    ...
    
    Move __portpair/__addrpair from include/net/inet_hashtables.h
    to include/net/sock.h where we need them in struct sock_common
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ling Ma <ling.ma.program@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c4132c1b63a8..0a9a01a5b0d7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -126,6 +126,9 @@ struct sock;
 struct proto;
 struct net;
 
+typedef __u32 __bitwise __portpair;
+typedef __u64 __bitwise __addrpair;
+
 /**
  *	struct sock_common - minimal network layer representation of sockets
  *	@skc_daddr: Foreign IPv4 addr
@@ -155,7 +158,7 @@ struct sock_common {
 	 * address on 64bit arches : cf INET_MATCH() and INET_TW_MATCH()
 	 */
 	union {
-		unsigned long	skc_addrpair;
+		__addrpair	skc_addrpair;
 		struct {
 			__be32	skc_daddr;
 			__be32	skc_rcv_saddr;
@@ -167,7 +170,7 @@ struct sock_common {
 	};
 	/* skc_dport && skc_num must be grouped as well */
 	union {
-		u32		skc_portpair;
+		__portpair	skc_portpair;
 		struct {
 			__be16	skc_dport;
 			__u16	skc_num;

commit ce43b03e8889475817d427b1f3724c7e294b76eb
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Nov 30 09:49:27 2012 +0000

    net: move inet_dport/inet_num in sock_common
    
    commit 68835aba4d9b (net: optimize INET input path further)
    moved some fields used for tcp/udp sockets lookup in the first cache
    line of struct sock_common.
    
    This patch moves inet_dport/inet_num as well, filling a 32bit hole
    on 64 bit arches and reducing number of cache line misses in lookups.
    
    Also change INET_MATCH()/INET_TW_MATCH() to perform the ports match
    before addresses match, as this check is more discriminant.
    
    Remove the hash check from MATCH() macros because we dont need to
    re validate the hash value after taking a refcount on socket, and
    use likely/unlikely compiler hints, as the sk_hash/hash check
    makes the following conditional tests 100% predicted by cpu.
    
    Introduce skc_addrpair/skc_portpair pair values to better
    document the alignment requirements of the port/addr pairs
    used in the various MATCH() macros, and remove some casts.
    
    The namespace check can also be done at last.
    
    This slightly improves TCP/UDP lookup times.
    
    IP/TCP early demux needs inet->rx_dst_ifindex and
    TCP needs inet->min_ttl, lets group them together in same cache line.
    
    With help from Ben Hutchings & Joe Perches.
    
    Idea of this patch came after Ling Ma proposal to move skc_hash
    to the beginning of struct sock_common, and should allow him
    to submit a final version of his patch. My tests show an improvement
    doing so.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Ling Ma <ling.ma.program@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c945fba4f543..c4132c1b63a8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -132,6 +132,8 @@ struct net;
  *	@skc_rcv_saddr: Bound local IPv4 addr
  *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
+ *	@skc_dport: placeholder for inet_dport/tw_dport
+ *	@skc_num: placeholder for inet_num/tw_num
  *	@skc_family: network address family
  *	@skc_state: Connection state
  *	@skc_reuse: %SO_REUSEADDR setting
@@ -149,16 +151,29 @@ struct net;
  *	for struct sock and struct inet_timewait_sock.
  */
 struct sock_common {
-	/* skc_daddr and skc_rcv_saddr must be grouped :
-	 * cf INET_MATCH() and INET_TW_MATCH()
+	/* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned
+	 * address on 64bit arches : cf INET_MATCH() and INET_TW_MATCH()
 	 */
-	__be32			skc_daddr;
-	__be32			skc_rcv_saddr;
-
+	union {
+		unsigned long	skc_addrpair;
+		struct {
+			__be32	skc_daddr;
+			__be32	skc_rcv_saddr;
+		};
+	};
 	union  {
 		unsigned int	skc_hash;
 		__u16		skc_u16hashes[2];
 	};
+	/* skc_dport && skc_num must be grouped as well */
+	union {
+		u32		skc_portpair;
+		struct {
+			__be16	skc_dport;
+			__u16	skc_num;
+		};
+	};
+
 	unsigned short		skc_family;
 	volatile unsigned char	skc_state;
 	unsigned char		skc_reuse;

commit 48fc7f7e787dd65ffe88521bce31f4062ba273eb
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Wed Sep 19 21:48:00 2012 -0400

    Fix misspellings of "whether" in comments.
    
    "Whether" is misspelled in various comments across the tree; this
    fixes them. No code changes.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/net/sock.h b/include/net/sock.h
index c945fba4f543..a95e0756e56e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -213,7 +213,7 @@ struct cg_proto;
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
   *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
-  *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
+  *	@sk_no_check: %SO_NO_CHECK setting, whether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)

commit aecdc33e111b2c447b622e287c6003726daa1426
Merge: a20acf99f75e a3a6cab5ea10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 13:38:27 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking changes from David Miller:
    
     1) GRE now works over ipv6, from Dmitry Kozlov.
    
     2) Make SCTP more network namespace aware, from Eric Biederman.
    
     3) TEAM driver now works with non-ethernet devices, from Jiri Pirko.
    
     4) Make openvswitch network namespace aware, from Pravin B Shelar.
    
     5) IPV6 NAT implementation, from Patrick McHardy.
    
     6) Server side support for TCP Fast Open, from Jerry Chu and others.
    
     7) Packet BPF filter supports MOD and XOR, from Eric Dumazet and Daniel
        Borkmann.
    
     8) Increate the loopback default MTU to 64K, from Eric Dumazet.
    
     9) Use a per-task rather than per-socket page fragment allocator for
        outgoing networking traffic.  This benefits processes that have very
        many mostly idle sockets, which is quite common.
    
        From Eric Dumazet.
    
    10) Use up to 32K for page fragment allocations, with fallbacks to
        smaller sizes when higher order page allocations fail.  Benefits are
        a) less segments for driver to process b) less calls to page
        allocator c) less waste of space.
    
        From Eric Dumazet.
    
    11) Allow GRO to be used on GRE tunnels, from Eric Dumazet.
    
    12) VXLAN device driver, one way to handle VLAN issues such as the
        limitation of 4096 VLAN IDs yet still have some level of isolation.
        From Stephen Hemminger.
    
    13) As usual there is a large boatload of driver changes, with the scale
        perhaps tilted towards the wireless side this time around.
    
    Fix up various fairly trivial conflicts, mostly caused by the user
    namespace changes.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1012 commits)
      hyperv: Add buffer for extended info after the RNDIS response message.
      hyperv: Report actual status in receive completion packet
      hyperv: Remove extra allocated space for recv_pkt_list elements
      hyperv: Fix page buffer handling in rndis_filter_send_request()
      hyperv: Fix the missing return value in rndis_filter_set_packet_filter()
      hyperv: Fix the max_xfer_size in RNDIS initialization
      vxlan: put UDP socket in correct namespace
      vxlan: Depend on CONFIG_INET
      sfc: Fix the reported priorities of different filter types
      sfc: Remove EFX_FILTER_FLAG_RX_OVERRIDE_IP
      sfc: Fix loopback self-test with separate_tx_channels=1
      sfc: Fix MCDI structure field lookup
      sfc: Add parentheses around use of bitfield macro arguments
      sfc: Fix null function pointer in efx_sriov_channel_type
      vxlan: virtual extensible lan
      igmp: export symbol ip_mc_leave_group
      netlink: add attributes to fdb interface
      tg3: unconditionally select HWMON support when tg3 is enabled.
      Revert "net: ti cpsw ethernet: allow reading phy interface mode from DT"
      gre: fix sparse warning
      ...

commit 437589a74b6a590d175f86cf9f7b2efcee7765e7
Merge: 68d47a137c3b 72235465864d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 11:11:09 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace changes from Eric Biederman:
     "This is a mostly modest set of changes to enable basic user namespace
      support.  This allows the code to code to compile with user namespaces
      enabled and removes the assumption there is only the initial user
      namespace.  Everything is converted except for the most complex of the
      filesystems: autofs4, 9p, afs, ceph, cifs, coda, fuse, gfs2, ncpfs,
      nfs, ocfs2 and xfs as those patches need a bit more review.
    
      The strategy is to push kuid_t and kgid_t values are far down into
      subsystems and filesystems as reasonable.  Leaving the make_kuid and
      from_kuid operations to happen at the edge of userspace, as the values
      come off the disk, and as the values come in from the network.
      Letting compile type incompatible compile errors (present when user
      namespaces are enabled) guide me to find the issues.
    
      The most tricky areas have been the places where we had an implicit
      union of uid and gid values and were storing them in an unsigned int.
      Those places were converted into explicit unions.  I made certain to
      handle those places with simple trivial patches.
    
      Out of that work I discovered we have generic interfaces for storing
      quota by projid.  I had never heard of the project identifiers before.
      Adding full user namespace support for project identifiers accounts
      for most of the code size growth in my git tree.
    
      Ultimately there will be work to relax privlige checks from
      "capable(FOO)" to "ns_capable(user_ns, FOO)" where it is safe allowing
      root in a user names to do those things that today we only forbid to
      non-root users because it will confuse suid root applications.
    
      While I was pushing kuid_t and kgid_t changes deep into the audit code
      I made a few other cleanups.  I capitalized on the fact we process
      netlink messages in the context of the message sender.  I removed
      usage of NETLINK_CRED, and started directly using current->tty.
    
      Some of these patches have also made it into maintainer trees, with no
      problems from identical code from different trees showing up in
      linux-next.
    
      After reading through all of this code I feel like I might be able to
      win a game of kernel trivial pursuit."
    
    Fix up some fairly trivial conflicts in netfilter uid/git logging code.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (107 commits)
      userns: Convert the ufs filesystem to use kuid/kgid where appropriate
      userns: Convert the udf filesystem to use kuid/kgid where appropriate
      userns: Convert ubifs to use kuid/kgid
      userns: Convert squashfs to use kuid/kgid where appropriate
      userns: Convert reiserfs to use kuid and kgid where appropriate
      userns: Convert jfs to use kuid/kgid where appropriate
      userns: Convert jffs2 to use kuid and kgid where appropriate
      userns: Convert hpfs to use kuid and kgid where appropriate
      userns: Convert btrfs to use kuid/kgid where appropriate
      userns: Convert bfs to use kuid/kgid where appropriate
      userns: Convert affs to use kuid/kgid wherwe appropriate
      userns: On alpha modify linux_to_osf_stat to use convert from kuids and kgids
      userns: On ia64 deal with current_uid and current_gid being kuid and kgid
      userns: On ppc convert current_uid from a kuid before printing.
      userns: Convert s390 getting uid and gid system calls to use kuid and kgid
      userns: Convert s390 hypfs to use kuid and kgid where appropriate
      userns: Convert binder ipc to use kuids
      userns: Teach security_path_chown to take kuids and kgids
      userns: Add user namespace support to IMA
      userns: Convert EVM to deal with kuids and kgids in it's hmac computation
      ...

commit c0e8a139a5bb8add02b4111e9e1957d810d7285e
Merge: 033d9959ed2d a6f00298b2ce
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 10:50:47 2012 -0700

    Merge branch 'for-3.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - xattr support added.  The implementation is shared with tmpfs.  The
       usage is restricted and intended to be used to manage per-cgroup
       metadata by system software.  tmpfs changes are routed through this
       branch with Hugh's permission.
    
     - cgroup subsystem ID handling simplified.
    
    * 'for-3.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: Define CGROUP_SUBSYS_COUNT according the configuration
      cgroup: Assign subsystem IDs during compile time
      cgroup: Do not depend on a given order when populating the subsys array
      cgroup: Wrap subsystem selection macro
      cgroup: Remove CGROUP_BUILTIN_SUBSYS_COUNT
      cgroup: net_prio: Do not define task_netpioidx() when not selected
      cgroup: net_cls: Do not define task_cls_classid() when not selected
      cgroup: net_cls: Move sock_update_classid() declaration to cls_cgroup.h
      cgroup: trivial fixes for Documentation/cgroups/cgroups.txt
      xattr: mark variable as uninitialized to make both gcc and smatch happy
      fs: add missing documentation to simple_xattr functions
      cgroup: add documentation on extended attributes usage
      cgroup: rename subsys_bits to subsys_mask
      cgroup: add xattr support
      cgroup: revise how we re-populate root directory
      xattr: extract simple_xattr code from tmpfs

commit 6a06e5e1bb217be077e1f8ee2745b4c5b1aa02db
Merge: d9f72f359e00 6672d90fe779
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 28 14:40:49 2012 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/team/team.c
            drivers/net/usb/qmi_wwan.c
            net/batman-adv/bat_iv_ogm.c
            net/ipv4/fib_frontend.c
            net/ipv4/route.c
            net/l2tp/l2tp_netlink.c
    
    The team, fib_frontend, route, and l2tp_netlink conflicts were simply
    overlapping changes.
    
    qmi_wwan and bat_iv_ogm were of the "use HEAD" variety.
    
    With help from Antonio Quartulli.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e2bcabec6ea5ba30dd2097dc1566e9957d14117c
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 25 11:32:13 2012 +0000

    net: remove sk_init() helper
    
    It seems sk_init() has no value today and even does strange things :
    
    # grep . /proc/sys/net/core/?mem_*
    /proc/sys/net/core/rmem_default:212992
    /proc/sys/net/core/rmem_max:131071
    /proc/sys/net/core/wmem_default:212992
    /proc/sys/net/core/wmem_max:131071
    
    We can remove it completely.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Shan Wei <davidshan@tencent.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f036493b9a61..bc476a19f28e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2229,8 +2229,6 @@ extern int net_msg_warn;
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 
-extern void sk_init(void);
-
 extern int sysctl_optmem_max;
 
 extern __u32 sysctl_wmem_default;

commit 5640f7685831e088fe6c2e1f863a6805962f8e81
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Sep 23 23:04:42 2012 +0000

    net: use a per task frag allocator
    
    We currently use a per socket order-0 page cache for tcp_sendmsg()
    operations.
    
    This page is used to build fragments for skbs.
    
    Its done to increase probability of coalescing small write() into
    single segments in skbs still in write queue (not yet sent)
    
    But it wastes a lot of memory for applications handling many mostly
    idle sockets, since each socket holds one page in sk->sk_sndmsg_page
    
    Its also quite inefficient to build TSO 64KB packets, because we need
    about 16 pages per skb on arches where PAGE_SIZE = 4096, so we hit
    page allocator more than wanted.
    
    This patch adds a per task frag allocator and uses bigger pages,
    if available. An automatic fallback is done in case of memory pressure.
    
    (up to 32768 bytes per frag, thats order-3 pages on x86)
    
    This increases TCP stream performance by 20% on loopback device,
    but also benefits on other network devices, since 8x less frags are
    mapped on transmit and unmapped on tx completion. Alexander Duyck
    mentioned a probable performance win on systems with IOMMU enabled.
    
    Its possible some SG enabled hardware cant cope with bigger fragments,
    but their ndo_start_xmit() should already handle this, splitting a
    fragment in sub fragments, since some arches have PAGE_SIZE=65536
    
    Successfully tested on various ethernet devices.
    (ixgbe, igb, bnx2x, tg3, mellanox mlx4)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Vijay Subramanian <subramanian.vijay@gmail.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 84bdaeca1314..f036493b9a61 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -247,8 +247,7 @@ struct cg_proto;
   *	@sk_stamp: time stamp of last packet received
   *	@sk_socket: Identd and reporting IO signals
   *	@sk_user_data: RPC layer private data
-  *	@sk_sndmsg_page: cached page for sendmsg
-  *	@sk_sndmsg_off: cached offset for sendmsg
+  *	@sk_frag: cached page frag
   *	@sk_peek_off: current peek_offset value
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
@@ -362,9 +361,8 @@ struct sock {
 	ktime_t			sk_stamp;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
-	struct page		*sk_sndmsg_page;
+	struct page_frag	sk_frag;
 	struct sk_buff		*sk_send_head;
-	__u32			sk_sndmsg_off;
 	__s32			sk_peek_off;
 	int			sk_write_pending;
 #ifdef CONFIG_SECURITY
@@ -2034,18 +2032,23 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 
 struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp);
 
-static inline struct page *sk_stream_alloc_page(struct sock *sk)
+/**
+ * sk_page_frag - return an appropriate page_frag
+ * @sk: socket
+ *
+ * If socket allocation mode allows current thread to sleep, it means its
+ * safe to use the per task page_frag instead of the per socket one.
+ */
+static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
-	struct page *page = NULL;
+	if (sk->sk_allocation & __GFP_WAIT)
+		return &current->task_frag;
 
-	page = alloc_pages(sk->sk_allocation, 0);
-	if (!page) {
-		sk_enter_memory_pressure(sk);
-		sk_stream_moderate_sndbuf(sk);
-	}
-	return page;
+	return &sk->sk_frag;
 }
 
+extern bool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);
+
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */

commit 35c448a8a3471b95ebc0ebcf91eb1183401b4274
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Sep 17 14:09:11 2012 -0700

    include/net/sock.h: squelch compiler warning in sk_rmem_schedule()
    
    This warning:
    
      In file included from linux/include/linux/tcp.h:227:0,
                       from linux/include/linux/ipv6.h:221,
                       from linux/include/net/ipv6.h:16,
                       from linux/include/linux/sunrpc/clnt.h:26,
                       from linux/net/sunrpc/stats.c:22:
      linux/include/net/sock.h: In function `sk_rmem_schedule':
      linux/nfs-2.6/include/net/sock.h:1339:13: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
    
    is seen with gcc (GCC) 4.6.3 20120306 (Red Hat 4.6.3-2) using the
    -Wextra option.
    
    Commit c76562b6709f ("netvm: prevent a stream-specific deadlock")
    accidentally replaced the "size" parameter of sk_rmem_schedule() with an
    unsigned int.  This changes the semantics of the comparison in the
    return statement.
    
    In sk_wmem_schedule we have syntactically the same comparison, but
    "size" is a signed integer.  In addition, __sk_mem_schedule() takes a
    signed integer for its "size" parameter, so there is an implicit type
    conversion in sk_rmem_schedule() anyway.
    
    Revert the "size" parameter back to a signed integer so that the
    semantics of the expressions in both sk_[rw]mem_schedule() are exactly
    the same.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 72132aef53fc..adb7da20b5a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1332,7 +1332,7 @@ static inline bool sk_wmem_schedule(struct sock *sk, int size)
 }
 
 static inline bool
-sk_rmem_schedule(struct sock *sk, struct sk_buff *skb, unsigned int size)
+sk_rmem_schedule(struct sock *sk, struct sk_buff *skb, int size)
 {
 	if (!sk_has_account(sk))
 		return true;

commit f3419807716be503c06f399b2bcbc68823be3a78
Author: Daniel Wagner <daniel.wagner@bmw-carit.de>
Date:   Wed Sep 12 16:12:01 2012 +0200

    cgroup: net_cls: Move sock_update_classid() declaration to cls_cgroup.h
    
    The only user of sock_update_classid() is net/socket.c which happens
    to include cls_cgroup.h directly.
    
    tj: Fix build breakage due to missing cls_cgroup.h inclusion in
        drivers/net/tun.c reported in linux-next by Stephen.
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Gao feng <gaofeng@cn.fujitsu.com>
    Cc: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: John Fastabend <john.r.fastabend@intel.com>
    Cc: netdev@vger.kernel.org
    Cc: cgroups@vger.kernel.org

diff --git a/include/net/sock.h b/include/net/sock.h
index 72132aef53fc..160a68093dbc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1486,14 +1486,6 @@ extern void *sock_kmalloc(struct sock *sk, int size,
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);
 extern void sk_send_sigurg(struct sock *sk);
 
-#ifdef CONFIG_CGROUPS
-extern void sock_update_classid(struct sock *sk);
-#else
-static inline void sock_update_classid(struct sock *sk)
-{
-}
-#endif
-
 /*
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.

commit e6acb384807406c1a6ad3ddc91191f7658e63b7a
Merge: 255e87657a84 898132ae76d1
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Aug 24 18:54:37 2012 -0400

    Merge branch 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    This is an initial merge in of Eric Biederman's work to start adding
    user namespace support to the networking.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c336d148adc4181f31741ae066df41429be64b67
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 24 17:56:43 2012 -0600

    userns: Implement sk_user_ns
    
    Add a helper sk_user_ns to make it easy to find the user namespace
    of the process that opened a socket.
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 65c3d62bfa5a..9d43736a869d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -604,6 +604,15 @@ static inline void sk_add_bind_node(struct sock *sk,
 #define sk_for_each_bound(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_bind_node)
 
+static inline struct user_namespace *sk_user_ns(struct sock *sk)
+{
+	/* Careful only use this in a context where these parameters
+	 * can not change and must all be valid, such as recvmsg from
+	 * userspace.
+	 */
+	return sk->sk_socket->file->f_cred->user_ns;
+}
+
 /* Sock flags */
 enum sock_flags {
 	SOCK_DEAD,

commit 976d020150456fccbd34103fd117fab910eed09c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed May 23 17:16:53 2012 -0600

    userns: Convert sock_i_uid to return a kuid_t
    
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index b3730239bf18..65c3d62bfa5a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1668,7 +1668,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
-extern int sock_i_uid(struct sock *sk);
+extern kuid_t sock_i_uid(struct sock *sk);
 extern unsigned long sock_i_ino(struct sock *sk);
 
 static inline struct dst_entry *

commit 1485348d2424e1131ea42efc033cbd9366462b01
Author: Ben Hutchings <bhutchings@solarflare.com>
Date:   Mon Jul 30 16:11:42 2012 +0000

    tcp: Apply device TSO segment limit earlier
    
    Cache the device gso_max_segs in sock::sk_gso_max_segs and use it to
    limit the size of TSO skbs.  This avoids the need to fall back to
    software GSO for local TCP senders.
    
    Signed-off-by: Ben Hutchings <bhutchings@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b3730239bf18..72132aef53fc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -218,6 +218,7 @@ struct cg_proto;
   *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
   *	@sk_gso_max_size: Maximum GSO segment size to build
+  *	@sk_gso_max_segs: Maximum number of GSO segments
   *	@sk_lingertime: %SO_LINGER l_linger setting
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
@@ -338,6 +339,7 @@ struct sock {
 	netdev_features_t	sk_route_nocaps;
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
+	u16			sk_gso_max_segs;
 	int			sk_rcvlowat;
 	unsigned long	        sk_lingertime;
 	struct sk_buff_head	sk_error_queue;

commit c76562b6709fee5eff8a6a779be41c0bce661fd7
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:41 2012 -0700

    netvm: prevent a stream-specific deadlock
    
    This patch series is based on top of "Swap-over-NBD without deadlocking
    v15" as it depends on the same reservation of PF_MEMALLOC reserves logic.
    
    When a user or administrator requires swap for their application, they
    create a swap partition and file, format it with mkswap and activate it
    with swapon.  In diskless systems this is not an option so if swap if
    required then swapping over the network is considered.  The two likely
    scenarios are when blade servers are used as part of a cluster where the
    form factor or maintenance costs do not allow the use of disks and thin
    clients.
    
    The Linux Terminal Server Project recommends the use of the Network Block
    Device (NBD) for swap but this is not always an option.  There is no
    guarantee that the network attached storage (NAS) device is running Linux
    or supports NBD.  However, it is likely that it supports NFS so there are
    users that want support for swapping over NFS despite any performance
    concern.  Some distributions currently carry patches that support swapping
    over NFS but it would be preferable to support it in the mainline kernel.
    
    Patch 1 avoids a stream-specific deadlock that potentially affects TCP.
    
    Patch 2 is a small modification to SELinux to avoid using PFMEMALLOC
            reserves.
    
    Patch 3 adds three helpers for filesystems to handle swap cache pages.
            For example, page_file_mapping() returns page->mapping for
            file-backed pages and the address_space of the underlying
            swap file for swap cache pages.
    
    Patch 4 adds two address_space_operations to allow a filesystem
            to pin all metadata relevant to a swapfile in memory. Upon
            successful activation, the swapfile is marked SWP_FILE and
            the address space operation ->direct_IO is used for writing
            and ->readpage for reading in swap pages.
    
    Patch 5 notes that patch 3 is bolting
            filesystem-specific-swapfile-support onto the side and that
            the default handlers have different information to what
            is available to the filesystem. This patch refactors the
            code so that there are generic handlers for each of the new
            address_space operations.
    
    Patch 6 adds an API to allow a vector of kernel addresses to be
            translated to struct pages and pinned for IO.
    
    Patch 7 adds support for using highmem pages for swap by kmapping
            the pages before calling the direct_IO handler.
    
    Patch 8 updates NFS to use the helpers from patch 3 where necessary.
    
    Patch 9 avoids setting PF_private on PG_swapcache pages within NFS.
    
    Patch 10 implements the new swapfile-related address_space operations
            for NFS and teaches the direct IO handler how to manage
            kernel addresses.
    
    Patch 11 prevents page allocator recursions in NFS by using GFP_NOIO
            where appropriate.
    
    Patch 12 fixes a NULL pointer dereference that occurs when using
            swap-over-NFS.
    
    With the patches applied, it is possible to mount a swapfile that is on an
    NFS filesystem.  Swap performance is not great with a swap stress test
    taking roughly twice as long to complete than if the swap device was
    backed by NBD.
    
    This patch: netvm: prevent a stream-specific deadlock
    
    It could happen that all !SOCK_MEMALLOC sockets have buffered so much data
    that we're over the global rmem limit.  This will prevent SOCK_MEMALLOC
    buffers from receiving data, which will prevent userspace from running,
    which is needed to reduce the buffered data.
    
    Fix this by exempting the SOCK_MEMALLOC sockets from the rmem limit.  Once
    this change it applied, it is important that sockets that set
    SOCK_MEMALLOC do not clear the flag until the socket is being torn down.
    If this happens, a warning is generated and the tokens reclaimed to avoid
    accounting errors until the bug is fixed.
    
    [davem@davemloft.net: Warning about clearing SOCK_MEMALLOC]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 43a470d40d76..b3730239bf18 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1329,12 +1329,14 @@ static inline bool sk_wmem_schedule(struct sock *sk, int size)
 		__sk_mem_schedule(sk, size, SK_MEM_SEND);
 }
 
-static inline bool sk_rmem_schedule(struct sock *sk, int size)
+static inline bool
+sk_rmem_schedule(struct sock *sk, struct sk_buff *skb, unsigned int size)
 {
 	if (!sk_has_account(sk))
 		return true;
-	return size <= sk->sk_forward_alloc ||
-		__sk_mem_schedule(sk, size, SK_MEM_RECV);
+	return size<= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_RECV) ||
+		skb_pfmemalloc(skb);
 }
 
 static inline void sk_mem_reclaim(struct sock *sk)

commit b4b9e3558508980fc0cd161a545ffb55a1f13ee9
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:26 2012 -0700

    netvm: set PF_MEMALLOC as appropriate during SKB processing
    
    In order to make sure pfmemalloc packets receive all memory needed to
    proceed, ensure processing of pfmemalloc SKBs happens under PF_MEMALLOC.
    This is limited to a subset of protocols that are expected to be used for
    writing to swap.  Taps are not allowed to use PF_MEMALLOC as these are
    expected to communicate with userspace processes which could be paged out.
    
    [a.p.zijlstra@chello.nl: Ideas taken from various patches]
    [jslaby@suse.cz: Lock imbalance fix]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 81198632ac2a..43a470d40d76 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -754,8 +754,13 @@ static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *s
 	return 0;
 }
 
+extern int __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb);
+
 static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 {
+	if (sk_memalloc_socks() && skb_pfmemalloc(skb))
+		return __sk_backlog_rcv(sk, skb);
+
 	return sk->sk_backlog_rcv(sk, skb);
 }
 

commit c93bdd0e03e848555d144eb44a1f275b871a8dd5
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:19 2012 -0700

    netvm: allow skb allocation to use PFMEMALLOC reserves
    
    Change the skb allocation API to indicate RX usage and use this to fall
    back to the PFMEMALLOC reserve when needed.  SKBs allocated from the
    reserve are tagged in skb->pfmemalloc.  If an SKB is allocated from the
    reserve and the socket is later found to be unrelated to page reclaim, the
    packet is dropped so that the memory remains available for page reclaim.
    Network protocols are expected to recover from this packet loss.
    
    [a.p.zijlstra@chello.nl: Ideas taken from various patches]
    [davem@davemloft.net: Use static branches, coding style corrections]
    [sebastian@breakpoint.cc: Avoid unnecessary cast, fix !CONFIG_NET build]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index bfa7d20e6646..81198632ac2a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -659,6 +659,21 @@ static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
 	return test_bit(flag, &sk->sk_flags);
 }
 
+#ifdef CONFIG_NET
+extern struct static_key memalloc_socks;
+static inline int sk_memalloc_socks(void)
+{
+	return static_key_false(&memalloc_socks);
+}
+#else
+
+static inline int sk_memalloc_socks(void)
+{
+	return 0;
+}
+
+#endif
+
 static inline gfp_t sk_gfp_atomic(struct sock *sk, gfp_t gfp_mask)
 {
 	return GFP_ATOMIC | (sk->sk_allocation & __GFP_MEMALLOC);

commit 7cb0240492caea2f6467f827313478f41877e6ef
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:16 2012 -0700

    netvm: allow the use of __GFP_MEMALLOC by specific sockets
    
    Allow specific sockets to be tagged SOCK_MEMALLOC and use __GFP_MEMALLOC
    for their allocations.  These sockets will be able to go below watermarks
    and allocate from the emergency reserve.  Such sockets are to be used to
    service the VM (iow.  to swap over).  They must be handled kernel side,
    exposing such a socket to user-space is a bug.
    
    There is a risk that the reserves be depleted so for now, the
    administrator is responsible for increasing min_free_kbytes as necessary
    to prevent deadlock for their workloads.
    
    [a.p.zijlstra@chello.nl: Original patches]
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 11ccde65c4cf..bfa7d20e6646 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -621,6 +621,7 @@ enum sock_flags {
 	SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+	SOCK_MEMALLOC, /* VM depends on this socket for swapping */
 	SOCK_TIMESTAMPING_TX_HARDWARE,  /* %SOF_TIMESTAMPING_TX_HARDWARE */
 	SOCK_TIMESTAMPING_TX_SOFTWARE,  /* %SOF_TIMESTAMPING_TX_SOFTWARE */
 	SOCK_TIMESTAMPING_RX_HARDWARE,  /* %SOF_TIMESTAMPING_RX_HARDWARE */
@@ -660,7 +661,7 @@ static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
 
 static inline gfp_t sk_gfp_atomic(struct sock *sk, gfp_t gfp_mask)
 {
-	return GFP_ATOMIC;
+	return GFP_ATOMIC | (sk->sk_allocation & __GFP_MEMALLOC);
 }
 
 static inline void sk_acceptq_removed(struct sock *sk)
@@ -803,6 +804,8 @@ extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
 extern void sk_stream_wait_close(struct sock *sk, long timeo_p);
 extern int sk_stream_error(struct sock *sk, int flags, int err);
 extern void sk_stream_kill_queues(struct sock *sk);
+extern void sk_set_memalloc(struct sock *sk);
+extern void sk_clear_memalloc(struct sock *sk);
 
 extern int sk_wait_data(struct sock *sk, long *timeo);
 

commit 99a1dec70d5acbd8c6b3928cdebb4a2d1da676c8
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:14 2012 -0700

    net: introduce sk_gfp_atomic() to allow addition of GFP flags depending on the individual socket
    
    Introduce sk_gfp_atomic(), this function allows to inject sock specific
    flags to each sock related allocation.  It is only used on allocation
    paths that may be required for writing pages back to network storage.
    
    [davem@davemloft.net: Use sk_gfp_atomic only when necessary]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index cee528c119ca..11ccde65c4cf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -658,6 +658,11 @@ static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
 	return test_bit(flag, &sk->sk_flags);
 }
 
+static inline gfp_t sk_gfp_atomic(struct sock *sk, gfp_t gfp_mask)
+{
+	return GFP_ATOMIC;
+}
+
 static inline void sk_acceptq_removed(struct sock *sk)
 {
 	sk->sk_ack_backlog--;

commit c255a458055e459f65eb7b7f51dc5dbdd0caf1d8
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Jul 31 16:43:02 2012 -0700

    memcg: rename config variables
    
    Sanity:
    
    CONFIG_CGROUP_MEM_RES_CTLR -> CONFIG_MEMCG
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP -> CONFIG_MEMCG_SWAP
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP_ENABLED -> CONFIG_MEMCG_SWAP_ENABLED
    CONFIG_CGROUP_MEM_RES_CTLR_KMEM -> CONFIG_MEMCG_KMEM
    
    [mhocko@suse.cz: fix missed bits]
    Cc: Glauber Costa <glommer@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index e067f8c18f88..cee528c119ca 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -913,7 +913,7 @@ struct proto {
 #ifdef SOCK_REFCNT_DEBUG
 	atomic_t		socks;
 #endif
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM
+#ifdef CONFIG_MEMCG_KMEM
 	/*
 	 * cgroup specific init/deinit functions. Called once for all
 	 * protocols that implement it, from cgroups populate function.
@@ -994,7 +994,7 @@ inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
-#if defined(CONFIG_CGROUP_MEM_RES_CTLR_KMEM) && defined(CONFIG_NET)
+#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_NET)
 extern struct static_key memcg_socket_limit_enabled;
 static inline struct cg_proto *parent_cg_proto(struct proto *proto,
 					       struct cg_proto *cg_proto)

commit 563d34d05786263893ba4a1042eb9b9374127cf5
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Jul 23 09:48:52 2012 +0200

    tcp: dont drop MTU reduction indications
    
    ICMP messages generated in output path if frame length is bigger than
    mtu are actually lost because socket is owned by user (doing the xmit)
    
    One example is the ipgre_tunnel_xmit() calling
    icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
    
    We had a similar case fixed in commit a34a101e1e6 (ipv6: disable GSO on
    sockets hitting dst_allfrag).
    
    Problem of such fix is that it relied on retransmit timers, so short tcp
    sessions paid a too big latency increase price.
    
    This patch uses the tcp_release_cb() infrastructure so that MTU
    reduction messages (ICMP messages) are not lost, and no extra delay
    is added in TCP transmits.
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Diagnosed-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Tore Anderson <tore@fud.no>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 88de092df50f..e067f8c18f88 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -859,6 +859,7 @@ struct proto {
 						struct sk_buff *skb);
 
 	void		(*release_cb)(struct sock *sk);
+	void		(*mtu_reduced)(struct sock *sk);
 
 	/* Keeping track of sk's, looking them up, and port selection methods. */
 	void			(*hash)(struct sock *sk);

commit 46d3ceabd8d98ed0ad10f20c595ca784e34786c5
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Jul 11 05:50:31 2012 +0000

    tcp: TCP Small Queues
    
    This introduce TSQ (TCP Small Queues)
    
    TSQ goal is to reduce number of TCP packets in xmit queues (qdisc &
    device queues), to reduce RTT and cwnd bias, part of the bufferbloat
    problem.
    
    sk->sk_wmem_alloc not allowed to grow above a given limit,
    allowing no more than ~128KB [1] per tcp socket in qdisc/dev layers at a
    given time.
    
    TSO packets are sized/capped to half the limit, so that we have two
    TSO packets in flight, allowing better bandwidth use.
    
    As a side effect, setting the limit to 40000 automatically reduces the
    standard gso max limit (65536) to 40000/2 : It can help to reduce
    latencies of high prio packets, having smaller TSO packets.
    
    This means we divert sock_wfree() to a tcp_wfree() handler, to
    queue/send following frames when skb_orphan() [2] is called for the
    already queued skbs.
    
    Results on my dev machines (tg3/ixgbe nics) are really impressive,
    using standard pfifo_fast, and with or without TSO/GSO.
    
    Without reduction of nominal bandwidth, we have reduction of buffering
    per bulk sender :
    < 1ms on Gbit (instead of 50ms with TSO)
    < 8ms on 100Mbit (instead of 132 ms)
    
    I no longer have 4 MBytes backlogged in qdisc by a single netperf
    session, and both side socket autotuning no longer use 4 Mbytes.
    
    As skb destructor cannot restart xmit itself ( as qdisc lock might be
    taken at this point ), we delegate the work to a tasklet. We use one
    tasklest per cpu for performance reasons.
    
    If tasklet finds a socket owned by the user, it sets TSQ_OWNED flag.
    This flag is tested in a new protocol method called from release_sock(),
    to eventually send new segments.
    
    [1] New /proc/sys/net/ipv4/tcp_limit_output_bytes tunable
    [2] skb_orphan() is usually called at TX completion time,
      but some drivers call it in their start_xmit() handler.
      These drivers should at least use BQL, or else a single TCP
      session can still fill the whole NIC TX ring, since TSQ will
      have no effect.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Dave Taht <dave.taht@bufferbloat.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Matt Mathis <mattmathis@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Nandita Dukkipati <nanditad@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index dcb54a0793ec..88de092df50f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -858,6 +858,8 @@ struct proto {
 	int			(*backlog_rcv) (struct sock *sk,
 						struct sk_buff *skb);
 
+	void		(*release_cb)(struct sock *sk);
+
 	/* Keeping track of sk's, looking them up, and port selection methods. */
 	void			(*hash)(struct sock *sk);
 	void			(*unhash)(struct sock *sk);

commit deaa58542b21d2b395db816952c202034319cbb4
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Jun 24 20:22:49 2012 +0000

    net: struct sock cleanups
    
    Add missing kernel doc for sk_rx_dst
    
    Move sk_rx_dst to avoid two 32bit holes on 64bit arches
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 21086036e348..dcb54a0793ec 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -198,6 +198,7 @@ struct cg_proto;
   *	@sk_lock:	synchronizer
   *	@sk_rcvbuf: size of receive buffer in bytes
   *	@sk_wq: sock wait queue and async head
+  *	@sk_rx_dst: receive input route used by early tcp demux
   *	@sk_dst_cache: destination cache
   *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy
@@ -317,9 +318,9 @@ struct sock {
 	struct xfrm_policy	*sk_policy[2];
 #endif
 	unsigned long 		sk_flags;
+	struct dst_entry	*sk_rx_dst;
 	struct dst_entry	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;
-	struct dst_entry	*sk_rx_dst;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;

commit efc27f8ceebe5eb147fa31d6c995706d327ad855
Author: Vijay Subramanian <subramanian.vijay@gmail.com>
Date:   Sun Jun 24 13:03:07 2012 +0000

    net: Remove 'unlikely' qualifier in skb_steal_sock()
    
    With early demux enabled by default for TCP flows, there is high chance that
    skb->sk will be non-null. 'unlikely()' was removed from __inet_lookup_skb() but
    maybe it can be removed from skb_steal_sock() as well.
    
    Note: skb_steal_sock() is also called by __inet6_lookup_skb() and
    __udp4_lib_lookup_skb() but they are protected by their own 'unlikely' calls.
    
    Signed-off-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 87b424ae750a..21086036e348 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2154,7 +2154,7 @@ static inline void sk_change_net(struct sock *sk, struct net *net)
 
 static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 {
-	if (unlikely(skb->sk)) {
+	if (skb->sk) {
 		struct sock *sk = skb->sk;
 
 		skb->destructor = NULL;

commit 41063e9dd11956f2d285e12e4342e1d232ba0ea2
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 19 21:22:05 2012 -0700

    ipv4: Early TCP socket demux.
    
    Input packet processing for local sockets involves two major demuxes.
    One for the route and one for the socket.
    
    But we can optimize this down to one demux for certain kinds of local
    sockets.
    
    Currently we only do this for established TCP sockets, but it could
    at least in theory be expanded to other kinds of connections.
    
    If a TCP socket is established then it's identity is fully specified.
    
    This means that whatever input route was used during the three-way
    handshake must work equally well for the rest of the connection since
    the keys will not change.
    
    Once we move to established state, we cache the receive packet's input
    route to use later.
    
    Like the existing cached route in sk->sk_dst_cache used for output
    packets, we have to check for route invalidations using dst->obsolete
    and dst->ops->check().
    
    Early demux occurs outside of a socket locked section, so when a route
    invalidation occurs we defer the fixup of sk->sk_rx_dst until we are
    actually inside of established state packet processing and thus have
    the socket locked.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4a4521699563..87b424ae750a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -319,6 +319,7 @@ struct sock {
 	unsigned long 		sk_flags;
 	struct dst_entry	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;
+	struct dst_entry	*sk_rx_dst;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;
@@ -1426,6 +1427,7 @@ extern struct sk_buff		*sock_rmalloc(struct sock *sk,
 					      gfp_t priority);
 extern void			sock_wfree(struct sk_buff *skb);
 extern void			sock_rfree(struct sk_buff *skb);
+extern void			sock_edemux(struct sk_buff *skb);
 
 extern int			sock_setsockopt(struct socket *sock, int level,
 						int op, char __user *optval,

commit 3f134619393cb6c6dfab7890a617d0ceca6d05d7
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue May 29 15:07:11 2012 -0700

    memcg: decrement static keys at real destroy time
    
    We call the destroy function when a cgroup starts to be removed, such as
    by a rmdir event.
    
    However, because of our reference counters, some objects are still
    inflight.  Right now, we are decrementing the static_keys at destroy()
    time, meaning that if we get rid of the last static_key reference, some
    objects will still have charges, but the code to properly uncharge them
    won't be run.
    
    This becomes a problem specially if it is ever enabled again, because now
    new charges will be added to the staled charges making keeping it pretty
    much impossible.
    
    We just need to be careful with the static branch activation: since there
    is no particular preferred order of their activation, we need to make sure
    that we only start using it after all call sites are active.  This is
    achieved by having a per-memcg flag that is only updated after
    static_key_slow_inc() returns.  At this time, we are sure all sites are
    active.
    
    This is made per-memcg, not global, for a reason: it also has the effect
    of making socket accounting more consistent.  The first memcg to be
    limited will trigger static_key() activation, therefore, accounting.  But
    all the others will then be accounted no matter what.  After this patch,
    only limited memcgs will have its sockets accounted.
    
    [akpm@linux-foundation.org: move enum sock_flag_bits into sock.h,
                                document enum sock_flag_bits,
                                convert memcg_proto_active() and memcg_proto_activated() to test_bit(),
                                redo tcp_update_limit() comment to 80 cols]
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index d89f0582b6b6..4a4521699563 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -46,6 +46,7 @@
 #include <linux/list_nulls.h>
 #include <linux/timer.h>
 #include <linux/cache.h>
+#include <linux/bitops.h>
 #include <linux/lockdep.h>
 #include <linux/netdevice.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
@@ -921,12 +922,23 @@ struct proto {
 #endif
 };
 
+/*
+ * Bits in struct cg_proto.flags
+ */
+enum cg_proto_flags {
+	/* Currently active and new sockets should be assigned to cgroups */
+	MEMCG_SOCK_ACTIVE,
+	/* It was ever activated; we must disarm static keys on destruction */
+	MEMCG_SOCK_ACTIVATED,
+};
+
 struct cg_proto {
 	void			(*enter_memory_pressure)(struct sock *sk);
 	struct res_counter	*memory_allocated;	/* Current allocated memory. */
 	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
 	int			*memory_pressure;
 	long			*sysctl_mem;
+	unsigned long		flags;
 	/*
 	 * memcg field is used to find which memcg we belong directly
 	 * Each memcg struct can hold more than one cg_proto, so container_of
@@ -942,6 +954,16 @@ struct cg_proto {
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
 
+static inline bool memcg_proto_active(struct cg_proto *cg_proto)
+{
+	return test_bit(MEMCG_SOCK_ACTIVE, &cg_proto->flags);
+}
+
+static inline bool memcg_proto_activated(struct cg_proto *cg_proto)
+{
+	return test_bit(MEMCG_SOCK_ACTIVATED, &cg_proto->flags);
+}
+
 #ifdef SOCK_REFCNT_DEBUG
 static inline void sk_refcnt_debug_inc(struct sock *sk)
 {

commit 88d6ae8dc33af12fe1c7941b1fae2767374046fd
Merge: f5c101892fbd 0d4dde1ac9a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 22 17:40:19 2012 -0700

    Merge branch 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "cgroup file type addition / removal is updated so that file types are
      added and removed instead of individual files so that dynamic file
      type addition / removal can be implemented by cgroup and used by
      controllers.  blkio controller changes which will come through block
      tree are dependent on this.  Other changes include res_counter cleanup
      and disallowing kthread / PF_THREAD_BOUND threads to be attached to
      non-root cgroups.
    
      There's a reported bug with the file type addition / removal handling
      which can lead to oops on cgroup umount.  The issue is being looked
      into.  It shouldn't cause problems for most setups and isn't a
      security concern."
    
    Fix up trivial conflict in Documentation/feature-removal-schedule.txt
    
    * 'for-3.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (21 commits)
      res_counter: Account max_usage when calling res_counter_charge_nofail()
      res_counter: Merge res_counter_charge and res_counter_charge_nofail
      cgroups: disallow attaching kthreadd or PF_THREAD_BOUND threads
      cgroup: remove cgroup_subsys->populate()
      cgroup: get rid of populate for memcg
      cgroup: pass struct mem_cgroup instead of struct cgroup to socket memcg
      cgroup: make css->refcnt clearing on cgroup removal optional
      cgroup: use negative bias on css->refcnt to block css_tryget()
      cgroup: implement cgroup_rm_cftypes()
      cgroup: introduce struct cfent
      cgroup: relocate __d_cgrp() and __d_cft()
      cgroup: remove cgroup_add_file[s]()
      cgroup: convert memcg controller to the new cftype interface
      memcg: always create memsw files if CONFIG_CGROUP_MEM_RES_CTLR_SWAP
      cgroup: convert all non-memcg controllers to the new cftype interface
      cgroup: relocate cftype and cgroup_subsys definitions in controllers
      cgroup: merge cft_release_agent cftype array into the base files array
      cgroup: implement cgroup_add_cftypes() and friends
      cgroup: build list of all cgroups under a given cgroupfs_root
      cgroup: move cgroup_clear_directory() call out of cgroup_populate_dir()
      ...

commit dc6b9b78234fecdc6d2ca5e1629185718202bcf5
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 16 22:48:15 2012 +0000

    net: include/net/sock.h cleanup
    
    bool/const conversions where possible
    
    __inline__ -> inline
    
    space cleanups
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 036f5069b6e0..da931555e000 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -97,7 +97,7 @@ void mem_cgroup_sockets_destroy(struct cgroup *cgrp)
 #else
 /* Validate arguments and do nothing */
 static inline __printf(2, 3)
-void SOCK_DEBUG(struct sock *sk, const char *msg, ...)
+void SOCK_DEBUG(const struct sock *sk, const char *msg, ...)
 {
 }
 #endif
@@ -372,8 +372,8 @@ struct sock {
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
 	void			(*sk_write_space)(struct sock *sk);
 	void			(*sk_error_report)(struct sock *sk);
-  	int			(*sk_backlog_rcv)(struct sock *sk,
-						  struct sk_buff *skb);  
+	int			(*sk_backlog_rcv)(struct sock *sk,
+						  struct sk_buff *skb);
 	void                    (*sk_destruct)(struct sock *sk);
 };
 
@@ -454,40 +454,40 @@ static inline struct sock *sk_nulls_next(const struct sock *sk)
 		NULL;
 }
 
-static inline int sk_unhashed(const struct sock *sk)
+static inline bool sk_unhashed(const struct sock *sk)
 {
 	return hlist_unhashed(&sk->sk_node);
 }
 
-static inline int sk_hashed(const struct sock *sk)
+static inline bool sk_hashed(const struct sock *sk)
 {
 	return !sk_unhashed(sk);
 }
 
-static __inline__ void sk_node_init(struct hlist_node *node)
+static inline void sk_node_init(struct hlist_node *node)
 {
 	node->pprev = NULL;
 }
 
-static __inline__ void sk_nulls_node_init(struct hlist_nulls_node *node)
+static inline void sk_nulls_node_init(struct hlist_nulls_node *node)
 {
 	node->pprev = NULL;
 }
 
-static __inline__ void __sk_del_node(struct sock *sk)
+static inline void __sk_del_node(struct sock *sk)
 {
 	__hlist_del(&sk->sk_node);
 }
 
 /* NB: equivalent to hlist_del_init_rcu */
-static __inline__ int __sk_del_node_init(struct sock *sk)
+static inline bool __sk_del_node_init(struct sock *sk)
 {
 	if (sk_hashed(sk)) {
 		__sk_del_node(sk);
 		sk_node_init(&sk->sk_node);
-		return 1;
+		return true;
 	}
-	return 0;
+	return false;
 }
 
 /* Grab socket reference count. This operation is valid only
@@ -509,9 +509,9 @@ static inline void __sock_put(struct sock *sk)
 	atomic_dec(&sk->sk_refcnt);
 }
 
-static __inline__ int sk_del_node_init(struct sock *sk)
+static inline bool sk_del_node_init(struct sock *sk)
 {
-	int rc = __sk_del_node_init(sk);
+	bool rc = __sk_del_node_init(sk);
 
 	if (rc) {
 		/* paranoid for a while -acme */
@@ -522,18 +522,18 @@ static __inline__ int sk_del_node_init(struct sock *sk)
 }
 #define sk_del_node_init_rcu(sk)	sk_del_node_init(sk)
 
-static __inline__ int __sk_nulls_del_node_init_rcu(struct sock *sk)
+static inline bool __sk_nulls_del_node_init_rcu(struct sock *sk)
 {
 	if (sk_hashed(sk)) {
 		hlist_nulls_del_init_rcu(&sk->sk_nulls_node);
-		return 1;
+		return true;
 	}
-	return 0;
+	return false;
 }
 
-static __inline__ int sk_nulls_del_node_init_rcu(struct sock *sk)
+static inline bool sk_nulls_del_node_init_rcu(struct sock *sk)
 {
-	int rc = __sk_nulls_del_node_init_rcu(sk);
+	bool rc = __sk_nulls_del_node_init_rcu(sk);
 
 	if (rc) {
 		/* paranoid for a while -acme */
@@ -543,40 +543,40 @@ static __inline__ int sk_nulls_del_node_init_rcu(struct sock *sk)
 	return rc;
 }
 
-static __inline__ void __sk_add_node(struct sock *sk, struct hlist_head *list)
+static inline void __sk_add_node(struct sock *sk, struct hlist_head *list)
 {
 	hlist_add_head(&sk->sk_node, list);
 }
 
-static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
+static inline void sk_add_node(struct sock *sk, struct hlist_head *list)
 {
 	sock_hold(sk);
 	__sk_add_node(sk, list);
 }
 
-static __inline__ void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+static inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
 {
 	sock_hold(sk);
 	hlist_add_head_rcu(&sk->sk_node, list);
 }
 
-static __inline__ void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+static inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
 }
 
-static __inline__ void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
+static inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	sock_hold(sk);
 	__sk_nulls_add_node_rcu(sk, list);
 }
 
-static __inline__ void __sk_del_bind_node(struct sock *sk)
+static inline void __sk_del_bind_node(struct sock *sk)
 {
 	__hlist_del(&sk->sk_bind_node);
 }
 
-static __inline__ void sk_add_bind_node(struct sock *sk,
+static inline void sk_add_bind_node(struct sock *sk,
 					struct hlist_head *list)
 {
 	hlist_add_head(&sk->sk_bind_node, list);
@@ -665,7 +665,7 @@ static inline void sk_acceptq_added(struct sock *sk)
 	sk->sk_ack_backlog++;
 }
 
-static inline int sk_acceptq_is_full(struct sock *sk)
+static inline bool sk_acceptq_is_full(const struct sock *sk)
 {
 	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
 }
@@ -673,19 +673,19 @@ static inline int sk_acceptq_is_full(struct sock *sk)
 /*
  * Compute minimal free write space needed to queue new packets.
  */
-static inline int sk_stream_min_wspace(struct sock *sk)
+static inline int sk_stream_min_wspace(const struct sock *sk)
 {
 	return sk->sk_wmem_queued >> 1;
 }
 
-static inline int sk_stream_wspace(struct sock *sk)
+static inline int sk_stream_wspace(const struct sock *sk)
 {
 	return sk->sk_sndbuf - sk->sk_wmem_queued;
 }
 
 extern void sk_stream_write_space(struct sock *sk);
 
-static inline int sk_stream_memory_free(struct sock *sk)
+static inline bool sk_stream_memory_free(const struct sock *sk)
 {
 	return sk->sk_wmem_queued < sk->sk_sndbuf;
 }
@@ -809,26 +809,26 @@ struct module;
  * transport -> network interface is defined by struct inet_proto
  */
 struct proto {
-	void			(*close)(struct sock *sk, 
+	void			(*close)(struct sock *sk,
 					long timeout);
 	int			(*connect)(struct sock *sk,
-				        struct sockaddr *uaddr, 
+					struct sockaddr *uaddr,
 					int addr_len);
 	int			(*disconnect)(struct sock *sk, int flags);
 
-	struct sock *		(*accept) (struct sock *sk, int flags, int *err);
+	struct sock *		(*accept)(struct sock *sk, int flags, int *err);
 
 	int			(*ioctl)(struct sock *sk, int cmd,
 					 unsigned long arg);
 	int			(*init)(struct sock *sk);
 	void			(*destroy)(struct sock *sk);
 	void			(*shutdown)(struct sock *sk, int how);
-	int			(*setsockopt)(struct sock *sk, int level, 
+	int			(*setsockopt)(struct sock *sk, int level,
 					int optname, char __user *optval,
 					unsigned int optlen);
-	int			(*getsockopt)(struct sock *sk, int level, 
-					int optname, char __user *optval, 
-					int __user *option);  	 
+	int			(*getsockopt)(struct sock *sk, int level,
+					int optname, char __user *optval,
+					int __user *option);
 #ifdef CONFIG_COMPAT
 	int			(*compat_setsockopt)(struct sock *sk,
 					int level,
@@ -845,14 +845,14 @@ struct proto {
 					   struct msghdr *msg, size_t len);
 	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
 					   struct msghdr *msg,
-					size_t len, int noblock, int flags, 
-					int *addr_len);
+					   size_t len, int noblock, int flags,
+					   int *addr_len);
 	int			(*sendpage)(struct sock *sk, struct page *page,
 					int offset, size_t size, int flags);
-	int			(*bind)(struct sock *sk, 
+	int			(*bind)(struct sock *sk,
 					struct sockaddr *uaddr, int addr_len);
 
-	int			(*backlog_rcv) (struct sock *sk, 
+	int			(*backlog_rcv) (struct sock *sk,
 						struct sk_buff *skb);
 
 	/* Keeping track of sk's, looking them up, and port selection methods. */
@@ -1173,7 +1173,7 @@ proto_memory_pressure(struct proto *prot)
 extern void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
 extern int sock_prot_inuse_get(struct net *net, struct proto *proto);
 #else
-static void inline sock_prot_inuse_add(struct net *net, struct proto *prot,
+static inline void sock_prot_inuse_add(struct net *net, struct proto *prot,
 		int inc)
 {
 }
@@ -1260,24 +1260,24 @@ static inline int sk_mem_pages(int amt)
 	return (amt + SK_MEM_QUANTUM - 1) >> SK_MEM_QUANTUM_SHIFT;
 }
 
-static inline int sk_has_account(struct sock *sk)
+static inline bool sk_has_account(struct sock *sk)
 {
 	/* return true if protocol supports memory accounting */
 	return !!sk->sk_prot->memory_allocated;
 }
 
-static inline int sk_wmem_schedule(struct sock *sk, int size)
+static inline bool sk_wmem_schedule(struct sock *sk, int size)
 {
 	if (!sk_has_account(sk))
-		return 1;
+		return true;
 	return size <= sk->sk_forward_alloc ||
 		__sk_mem_schedule(sk, size, SK_MEM_SEND);
 }
 
-static inline int sk_rmem_schedule(struct sock *sk, int size)
+static inline bool sk_rmem_schedule(struct sock *sk, int size)
 {
 	if (!sk_has_account(sk))
-		return 1;
+		return true;
 	return size <= sk->sk_forward_alloc ||
 		__sk_mem_schedule(sk, size, SK_MEM_RECV);
 }
@@ -1342,7 +1342,7 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
  * Mark both the sk_lock and the sk_lock.slock as a
  * per-address-family lock class.
  */
-#define sock_lock_init_class_and_name(sk, sname, skey, name, key) 	\
+#define sock_lock_init_class_and_name(sk, sname, skey, name, key)	\
 do {									\
 	sk->sk_lock.owned = 0;						\
 	init_waitqueue_head(&sk->sk_lock.wq);				\
@@ -1350,7 +1350,7 @@ do {									\
 	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\
 			sizeof((sk)->sk_lock));				\
 	lockdep_set_class_and_name(&(sk)->sk_lock.slock,		\
-		       	(skey), (sname));				\
+				(skey), (sname));				\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
@@ -1410,13 +1410,13 @@ extern int			sock_setsockopt(struct socket *sock, int level,
 						unsigned int optlen);
 
 extern int			sock_getsockopt(struct socket *sock, int level,
-						int op, char __user *optval, 
+						int op, char __user *optval,
 						int __user *optlen);
-extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
+extern struct sk_buff		*sock_alloc_send_skb(struct sock *sk,
 						     unsigned long size,
 						     int noblock,
 						     int *errcode);
-extern struct sk_buff 		*sock_alloc_send_pskb(struct sock *sk,
+extern struct sk_buff		*sock_alloc_send_pskb(struct sock *sk,
 						      unsigned long header_len,
 						      unsigned long data_len,
 						      int noblock,
@@ -1438,7 +1438,7 @@ static inline void sock_update_classid(struct sock *sk)
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.
  */
-extern int                      sock_no_bind(struct socket *, 
+extern int                      sock_no_bind(struct socket *,
 					     struct sockaddr *, int);
 extern int                      sock_no_connect(struct socket *,
 						struct sockaddr *, int, int);
@@ -1467,7 +1467,7 @@ extern int			sock_no_mmap(struct file *file,
 					     struct vm_area_struct *vma);
 extern ssize_t			sock_no_sendpage(struct socket *sock,
 						struct page *page,
-						int offset, size_t size, 
+						int offset, size_t size,
 						int flags);
 
 /*
@@ -1490,7 +1490,7 @@ extern void sk_common_release(struct sock *sk);
 /*
  *	Default socket callbacks and setup code
  */
- 
+
 /* Initialise core socket variables */
 extern void sock_init_data(struct socket *sock, struct sock *sk);
 
@@ -1690,7 +1690,7 @@ extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
 extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
-static inline int sk_can_gso(const struct sock *sk)
+static inline bool sk_can_gso(const struct sock *sk)
 {
 	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
 }
@@ -1807,7 +1807,7 @@ static inline int sk_rmem_alloc_get(const struct sock *sk)
  *
  * Returns true if socket has write or read allocations
  */
-static inline int sk_has_allocations(const struct sock *sk)
+static inline bool sk_has_allocations(const struct sock *sk)
 {
 	return sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);
 }
@@ -1846,9 +1846,7 @@ static inline int sk_has_allocations(const struct sock *sk)
  */
 static inline bool wq_has_sleeper(struct socket_wq *wq)
 {
-
-	/*
-	 * We need to be sure we are in sync with the
+	/* We need to be sure we are in sync with the
 	 * add_wait_queue modifications to the wait queue.
 	 *
 	 * This memory barrier is paired in the sock_poll_wait.
@@ -1870,22 +1868,21 @@ static inline void sock_poll_wait(struct file *filp,
 {
 	if (!poll_does_not_wait(p) && wait_address) {
 		poll_wait(filp, wait_address, p);
-		/*
-		 * We need to be sure we are in sync with the
+		/* We need to be sure we are in sync with the
 		 * socket flags modification.
 		 *
 		 * This memory barrier is paired in the wq_has_sleeper.
-		*/
+		 */
 		smp_mb();
 	}
 }
 
 /*
- * 	Queue a received datagram if it will fit. Stream and sequenced
+ *	Queue a received datagram if it will fit. Stream and sequenced
  *	protocols can't normally use this as they need to fit buffers in
  *	and play with them.
  *
- * 	Inlined as it's very short and called for pretty much every
+ *	Inlined as it's very short and called for pretty much every
  *	packet ever received.
  */
 
@@ -1911,10 +1908,10 @@ static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 	sk_mem_charge(sk, skb->truesize);
 }
 
-extern void sk_reset_timer(struct sock *sk, struct timer_list* timer,
+extern void sk_reset_timer(struct sock *sk, struct timer_list *timer,
 			   unsigned long expires);
 
-extern void sk_stop_timer(struct sock *sk, struct timer_list* timer);
+extern void sk_stop_timer(struct sock *sk, struct timer_list *timer);
 
 extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
@@ -1923,7 +1920,7 @@ extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
 /*
  *	Recover an error report and clear atomically
  */
- 
+
 static inline int sock_error(struct sock *sk)
 {
 	int err;
@@ -1939,7 +1936,7 @@ static inline unsigned long sock_wspace(struct sock *sk)
 
 	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
 		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
-		if (amt < 0) 
+		if (amt < 0)
 			amt = 0;
 	}
 	return amt;
@@ -1983,7 +1980,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */
-static inline int sock_writeable(const struct sock *sk) 
+static inline bool sock_writeable(const struct sock *sk)
 {
 	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
 }
@@ -1993,12 +1990,12 @@ static inline gfp_t gfp_any(void)
 	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
 }
 
-static inline long sock_rcvtimeo(const struct sock *sk, int noblock)
+static inline long sock_rcvtimeo(const struct sock *sk, bool noblock)
 {
 	return noblock ? 0 : sk->sk_rcvtimeo;
 }
 
-static inline long sock_sndtimeo(const struct sock *sk, int noblock)
+static inline long sock_sndtimeo(const struct sock *sk, bool noblock)
 {
 	return noblock ? 0 : sk->sk_sndtimeo;
 }
@@ -2021,7 +2018,7 @@ extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 extern void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
 	struct sk_buff *skb);
 
-static __inline__ void
+static inline void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 {
 	ktime_t kt = skb->tstamp;
@@ -2062,7 +2059,7 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 			   (1UL << SOCK_RCVTSTAMP)			| \
 			   (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE)	| \
 			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
-			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE) 	| \
+			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE)	| \
 			   (1UL << SOCK_TIMESTAMPING_SYS_HARDWARE))
 
 	if (sk->sk_flags & FLAGS_TS_OR_DROPS)
@@ -2091,7 +2088,7 @@ extern int sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
  * locked so that the sk_buff queue operation is ok.
 */
 #ifdef CONFIG_NET_DMA
-static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, bool copied_early)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
 	if (!copied_early)
@@ -2100,7 +2097,7 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
 		__skb_queue_tail(&sk->sk_async_wait_queue, skb);
 }
 #else
-static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, bool copied_early)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
 	__kfree_skb(skb);
@@ -2147,8 +2144,8 @@ extern void sock_enable_timestamp(struct sock *sk, int flag);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);
 
-/* 
- *	Enable debug/info messages 
+/*
+ *	Enable debug/info messages
  */
 extern int net_msg_warn;
 #define NETDEBUG(fmt, args...) \

commit 1b23a5dfc20469d4a4bb8a552dd224ac693c407c
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed May 16 05:57:07 2012 +0000

    net: sock_flag() cleanup
    
    - sock_flag() accepts a const pointer
    
    - sock_flag() returns a boolean
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e613704e9d1c..036f5069b6e0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -650,7 +650,7 @@ static inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)
 	__clear_bit(flag, &sk->sk_flags);
 }
 
-static inline int sock_flag(struct sock *sk, enum sock_flags flag)
+static inline bool sock_flag(const struct sock *sk, enum sock_flags flag)
 {
 	return test_bit(flag, &sk->sk_flags);
 }

commit 0d6c4a2e4641bbc556dd74d3aa158c413a972492
Merge: 6e06c0e2347e 1c430a727fa5
Author: David S. Miller <davem@davemloft.net>
Date:   Mon May 7 23:35:40 2012 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/intel/e1000e/param.c
            drivers/net/wireless/iwlwifi/iwl-agn-rx.c
            drivers/net/wireless/iwlwifi/iwl-trans-pcie-rx.c
            drivers/net/wireless/iwlwifi/iwl-trans.h
    
    Resolved the iwlwifi conflict with mainline using 3-way diff posted
    by John Linville and Stephen Rothwell.  In 'net' we added a bug
    fix to make iwlwifi report a more accurate skb->truesize but this
    conflicted with RX path changes that happened meanwhile in net-next.
    
    In e1000e a conflict arose in the validation code for settings of
    adapter->itr.  'net-next' had more sophisticated logic so that
    logic was used.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 518fbf9cdf17875d808596afd77fc115a6f942ca
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Apr 28 23:21:56 2012 +0000

    net: fix sk_sockets_allocated_read_positive
    
    Denys Fedoryshchenko reported frequent crashes on a proxy server and kindly
    provided a lockdep report that explains it all :
    
      [  762.903868]
      [  762.903880] =================================
      [  762.903890] [ INFO: inconsistent lock state ]
      [  762.903903] 3.3.4-build-0061 #8 Not tainted
      [  762.904133] ---------------------------------
      [  762.904344] inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-W} usage.
      [  762.904542] squid/1603 [HC0[0]:SC0[0]:HE1:SE1] takes:
      [  762.904542]  (key#3){+.?...}, at: [<c0232cc4>]
    __percpu_counter_sum+0xd/0x58
      [  762.904542] {IN-SOFTIRQ-W} state was registered at:
      [  762.904542]   [<c0158b84>] __lock_acquire+0x284/0xc26
      [  762.904542]   [<c01598e8>] lock_acquire+0x71/0x85
      [  762.904542]   [<c0349765>] _raw_spin_lock+0x33/0x40
      [  762.904542]   [<c0232c93>] __percpu_counter_add+0x58/0x7c
      [  762.904542]   [<c02cfde1>] sk_clone_lock+0x1e5/0x200
      [  762.904542]   [<c0303ee4>] inet_csk_clone_lock+0xe/0x78
      [  762.904542]   [<c0315778>] tcp_create_openreq_child+0x1b/0x404
      [  762.904542]   [<c031339c>] tcp_v4_syn_recv_sock+0x32/0x1c1
      [  762.904542]   [<c031615a>] tcp_check_req+0x1fd/0x2d7
      [  762.904542]   [<c0313f77>] tcp_v4_do_rcv+0xab/0x194
      [  762.904542]   [<c03153bb>] tcp_v4_rcv+0x3b3/0x5cc
      [  762.904542]   [<c02fc0c4>] ip_local_deliver_finish+0x13a/0x1e9
      [  762.904542]   [<c02fc539>] NF_HOOK.clone.11+0x46/0x4d
      [  762.904542]   [<c02fc652>] ip_local_deliver+0x41/0x45
      [  762.904542]   [<c02fc4d1>] ip_rcv_finish+0x31a/0x33c
      [  762.904542]   [<c02fc539>] NF_HOOK.clone.11+0x46/0x4d
      [  762.904542]   [<c02fc857>] ip_rcv+0x201/0x23e
      [  762.904542]   [<c02daa3a>] __netif_receive_skb+0x319/0x368
      [  762.904542]   [<c02dac07>] netif_receive_skb+0x4e/0x7d
      [  762.904542]   [<c02dacf6>] napi_skb_finish+0x1e/0x34
      [  762.904542]   [<c02db122>] napi_gro_receive+0x20/0x24
      [  762.904542]   [<f85d1743>] e1000_receive_skb+0x3f/0x45 [e1000e]
      [  762.904542]   [<f85d3464>] e1000_clean_rx_irq+0x1f9/0x284 [e1000e]
      [  762.904542]   [<f85d3926>] e1000_clean+0x62/0x1f4 [e1000e]
      [  762.904542]   [<c02db228>] net_rx_action+0x90/0x160
      [  762.904542]   [<c012a445>] __do_softirq+0x7b/0x118
      [  762.904542] irq event stamp: 156915469
      [  762.904542] hardirqs last  enabled at (156915469): [<c019b4f4>]
    __slab_alloc.clone.58.clone.63+0xc4/0x2de
      [  762.904542] hardirqs last disabled at (156915468): [<c019b452>]
    __slab_alloc.clone.58.clone.63+0x22/0x2de
      [  762.904542] softirqs last  enabled at (156915466): [<c02ce677>]
    lock_sock_nested+0x64/0x6c
      [  762.904542] softirqs last disabled at (156915464): [<c0349914>]
    _raw_spin_lock_bh+0xe/0x45
      [  762.904542]
      [  762.904542] other info that might help us debug this:
      [  762.904542]  Possible unsafe locking scenario:
      [  762.904542]
      [  762.904542]        CPU0
      [  762.904542]        ----
      [  762.904542]   lock(key#3);
      [  762.904542]   <Interrupt>
      [  762.904542]     lock(key#3);
      [  762.904542]
      [  762.904542]  *** DEADLOCK ***
      [  762.904542]
      [  762.904542] 1 lock held by squid/1603:
      [  762.904542]  #0:  (sk_lock-AF_INET){+.+.+.}, at: [<c03055c0>]
    lock_sock+0xa/0xc
      [  762.904542]
      [  762.904542] stack backtrace:
      [  762.904542] Pid: 1603, comm: squid Not tainted 3.3.4-build-0061 #8
      [  762.904542] Call Trace:
      [  762.904542]  [<c0347b73>] ? printk+0x18/0x1d
      [  762.904542]  [<c015873a>] valid_state+0x1f6/0x201
      [  762.904542]  [<c0158816>] mark_lock+0xd1/0x1bb
      [  762.904542]  [<c015876b>] ? mark_lock+0x26/0x1bb
      [  762.904542]  [<c015805d>] ? check_usage_forwards+0x77/0x77
      [  762.904542]  [<c0158bf8>] __lock_acquire+0x2f8/0xc26
      [  762.904542]  [<c0159b8e>] ? mark_held_locks+0x5d/0x7b
      [  762.904542]  [<c0159cf6>] ? trace_hardirqs_on+0xb/0xd
      [  762.904542]  [<c0158dd4>] ? __lock_acquire+0x4d4/0xc26
      [  762.904542]  [<c01598e8>] lock_acquire+0x71/0x85
      [  762.904542]  [<c0232cc4>] ? __percpu_counter_sum+0xd/0x58
      [  762.904542]  [<c0349765>] _raw_spin_lock+0x33/0x40
      [  762.904542]  [<c0232cc4>] ? __percpu_counter_sum+0xd/0x58
      [  762.904542]  [<c0232cc4>] __percpu_counter_sum+0xd/0x58
      [  762.904542]  [<c02cebc4>] __sk_mem_schedule+0xdd/0x1c7
      [  762.904542]  [<c02d178d>] ? __alloc_skb+0x76/0x100
      [  762.904542]  [<c0305e8e>] sk_wmem_schedule+0x21/0x2d
      [  762.904542]  [<c0306370>] sk_stream_alloc_skb+0x42/0xaa
      [  762.904542]  [<c0306567>] tcp_sendmsg+0x18f/0x68b
      [  762.904542]  [<c031f3dc>] ? ip_fast_csum+0x30/0x30
      [  762.904542]  [<c0320193>] inet_sendmsg+0x53/0x5a
      [  762.904542]  [<c02cb633>] sock_aio_write+0xd2/0xda
      [  762.904542]  [<c015876b>] ? mark_lock+0x26/0x1bb
      [  762.904542]  [<c01a1017>] do_sync_write+0x9f/0xd9
      [  762.904542]  [<c01a2111>] ? file_free_rcu+0x2f/0x2f
      [  762.904542]  [<c01a17a1>] vfs_write+0x8f/0xab
      [  762.904542]  [<c01a284d>] ? fget_light+0x75/0x7c
      [  762.904542]  [<c01a1900>] sys_write+0x3d/0x5e
      [  762.904542]  [<c0349ec9>] syscall_call+0x7/0xb
      [  762.904542]  [<c0340000>] ? rp_sidt+0x41/0x83
    
    Bug is that sk_sockets_allocated_read_positive() calls
    percpu_counter_sum_positive() without BH being disabled.
    
    This bug was added in commit 180d8cd942ce33
    (foundations of per-cgroup memory pressure controlling.), since previous
    code was using percpu_counter_read_positive() which is IRQ safe.
    
    In __sk_mem_schedule() we dont need the precise count of allocated
    sockets and can revert to previous behavior.
    
    Reported-by: Denys Fedoryshchenko <denys@visp.net.lb>
    Sined-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 188532ee88b6..5a0a58ac4126 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1129,9 +1129,9 @@ sk_sockets_allocated_read_positive(struct sock *sk)
 	struct proto *prot = sk->sk_prot;
 
 	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
-		return percpu_counter_sum_positive(sk->sk_cgrp->sockets_allocated);
+		return percpu_counter_read_positive(sk->sk_cgrp->sockets_allocated);
 
-	return percpu_counter_sum_positive(prot->sockets_allocated);
+	return percpu_counter_read_positive(prot->sockets_allocated);
 }
 
 static inline int

commit f24001941c99776f41bd3f09c07d91205c2ad9d4
Merge: a108d5f35adc 4d634ca35a8b
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Apr 23 23:14:36 2012 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Fix merge between commit 3adadc08cc1e ("net ax25: Reorder ax25_exit to
    remove races") and commit 0ca7a4c87d27 ("net ax25: Simplify and
    cleanup the ax25 sysctl handling")
    
    The former moved around the sysctl register/unregister calls, the
    later simply removed them.
    
    With help from Stephen Rothwell.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f545a38f74584cc7424cb74f792a00c6d2589485
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Apr 22 23:34:26 2012 +0000

    net: add a limit parameter to sk_add_backlog()
    
    sk_add_backlog() & sk_rcvqueues_full() hard coded sk_rcvbuf as the
    memory limit. We need to make this limit a parameter for TCP use.
    
    No functional change expected in this patch, all callers still using the
    old sk_rcvbuf limit.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Maciej Żenczykowski <maze@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Cc: Rick Jones <rick.jones2@hp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4cdb9b3050f4..4e9d01e491d5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -709,17 +709,19 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
  * Do not take into account this skb truesize,
  * to allow even a single big packet to come.
  */
-static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb)
+static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb,
+				     unsigned int limit)
 {
 	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
 
-	return qsize > sk->sk_rcvbuf;
+	return qsize > limit;
 }
 
 /* The per-socket spinlock must be held here. */
-static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,
+					      unsigned int limit)
 {
-	if (sk_rcvqueues_full(sk, skb))
+	if (sk_rcvqueues_full(sk, skb, limit))
 		return -ENOBUFS;
 
 	__sk_add_backlog(sk, skb);

commit 4a17fd5229c1b6066aa478f6b690f8293ce811a1
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Thu Apr 19 03:39:36 2012 +0000

    sock: Introduce named constants for sk_reuse
    
    Name them in a "backward compatible" manner, i.e. reuse or not
    are still 1 and 0 respectively. The reuse value of 2 means that
    the socket with it will forcibly reuse everyone else's port.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6ba1f8871fd..4cdb9b3050f4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -376,6 +376,17 @@ struct sock {
 	void                    (*sk_destruct)(struct sock *sk);
 };
 
+/*
+ * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK
+ * or not whether his port will be reused by someone else. SK_FORCE_REUSE
+ * on a socket means that the socket will reuse everybody else's port
+ * without looking at the other's sk_reuse value.
+ */
+
+#define SK_NO_REUSE	0
+#define SK_CAN_REUSE	1
+#define SK_FORCE_REUSE	2
+
 static inline int sk_peek_offset(struct sock *sk, int flags)
 {
 	if ((flags & MSG_PEEK) && (sk->sk_peek_off >= 0))

commit d3d4f0a025e621b82da08a76df4036d4267739dd
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Tue Apr 17 14:03:53 2012 +0000

    net/sock.h: fix sk_peek_off kernel-doc warning
    
    Fix kernel-doc warning in net/sock.h:
    
    Warning(include/net/sock.h:377): No description found for parameter 'sk_peek_off'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6ba1f8871fd..188532ee88b6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -246,6 +246,7 @@ struct cg_proto;
   *	@sk_user_data: RPC layer private data
   *	@sk_sndmsg_page: cached page for sendmsg
   *	@sk_sndmsg_off: cached offset for sendmsg
+  *	@sk_peek_off: current peek_offset value
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark

commit 1d62e43657c63a858560c98069706c705d20505d
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Apr 9 19:36:33 2012 -0300

    cgroup: pass struct mem_cgroup instead of struct cgroup to socket memcg
    
    The only reason cgroup was used, was to be consistent with the populate()
    interface. Now that we're getting rid of it, not only we no longer need
    it, but we also *can't* call it this way.
    
    Since we will no longer rely on populate(), this will be called from
    create(). During create, the association between struct mem_cgroup
    and struct cgroup does not yet exist, since cgroup internals hasn't
    yet initialized its bookkeeping. This means we would not be able
    to draw the memcg pointer from the cgroup pointer in these
    functions, which is highly undesirable.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    CC: Li Zefan <lizefan@huawei.com>
    CC: Johannes Weiner <hannes@cmpxchg.org>
    CC: Michal Hocko <mhocko@suse.cz>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6ba1f8871fd..b3ebe6b3e7db 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -70,16 +70,16 @@
 struct cgroup;
 struct cgroup_subsys;
 #ifdef CONFIG_NET
-int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss);
-void mem_cgroup_sockets_destroy(struct cgroup *cgrp);
+int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss);
+void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg);
 #else
 static inline
-int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)
+int mem_cgroup_sockets_init(struct mem_cgroup *memcg, struct cgroup_subsys *ss)
 {
 	return 0;
 }
 static inline
-void mem_cgroup_sockets_destroy(struct cgroup *cgrp)
+void mem_cgroup_sockets_destroy(struct mem_cgroup *memcg)
 {
 }
 #endif
@@ -900,9 +900,9 @@ struct proto {
 	 * This function has to setup any files the protocol want to
 	 * appear in the kmem cgroup filesystem.
 	 */
-	int			(*init_cgroup)(struct cgroup *cgrp,
+	int			(*init_cgroup)(struct mem_cgroup *memcg,
 					       struct cgroup_subsys *ss);
-	void			(*destroy_cgroup)(struct cgroup *cgrp);
+	void			(*destroy_cgroup)(struct mem_cgroup *memcg);
 	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
 #endif
 };

commit 626cf236608505d376e4799adb4f7eb00a8594af
Author: Hans Verkuil <hans.verkuil@cisco.com>
Date:   Fri Mar 23 15:02:27 2012 -0700

    poll: add poll_requested_events() and poll_does_not_wait() functions
    
    In some cases the poll() implementation in a driver has to do different
    things depending on the events the caller wants to poll for.  An example
    is when a driver needs to start a DMA engine if the caller polls for
    POLLIN, but doesn't want to do that if POLLIN is not requested but instead
    only POLLOUT or POLLPRI is requested.  This is something that can happen
    in the video4linux subsystem among others.
    
    Unfortunately, the current epoll/poll/select implementation doesn't
    provide that information reliably.  The poll_table_struct does have it: it
    has a key field with the event mask.  But once a poll() call matches one
    or more bits of that mask any following poll() calls are passed a NULL
    poll_table pointer.
    
    Also, the eventpoll implementation always left the key field at ~0 instead
    of using the requested events mask.
    
    This was changed in eventpoll.c so the key field now contains the actual
    events that should be polled for as set by the caller.
    
    The solution to the NULL poll_table pointer is to set the qproc field to
    NULL in poll_table once poll() matches the events, not the poll_table
    pointer itself.  That way drivers can obtain the mask through a new
    poll_requested_events inline.
    
    The poll_table_struct can still be NULL since some kernel code calls it
    internally (netfs_state_poll() in ./drivers/staging/pohmelfs/netfs.h).  In
    that case poll_requested_events() returns ~0 (i.e.  all events).
    
    Very rarely drivers might want to know whether poll_wait will actually
    wait.  If another earlier file descriptor in the set already matched the
    events the caller wanted to wait for, then the kernel will return from the
    select() call without waiting.  This might be useful information in order
    to avoid doing expensive work.
    
    A new helper function poll_does_not_wait() is added that drivers can use
    to detect this situation.  This is now used in sock_poll_wait() in
    include/net/sock.h.  This was the only place in the kernel that needed
    this information.
    
    Drivers should no longer access any of the poll_table internals, but use
    the poll_requested_events() and poll_does_not_wait() access functions
    instead.  In order to enforce that the poll_table fields are now prepended
    with an underscore and a comment was added warning against using them
    directly.
    
    This required a change in unix_dgram_poll() in unix/af_unix.c which used
    the key field to get the requested events.  It's been replaced by a call
    to poll_requested_events().
    
    For qproc it was especially important to change its name since the
    behavior of that field changes with this patch since this function pointer
    can now be NULL when that wasn't possible in the past.
    
    Any driver accessing the qproc or key fields directly will now fail to compile.
    
    Some notes regarding the correctness of this patch: the driver's poll()
    function is called with a 'struct poll_table_struct *wait' argument.  This
    pointer may or may not be NULL, drivers can never rely on it being one or
    the other as that depends on whether or not an earlier file descriptor in
    the select()'s fdset matched the requested events.
    
    There are only three things a driver can do with the wait argument:
    
    1) obtain the key field:
    
            events = wait ? wait->key : ~0;
    
       This will still work although it should be replaced with the new
       poll_requested_events() function (which does exactly the same).
       This will now even work better, since wait is no longer set to NULL
       unnecessarily.
    
    2) use the qproc callback. This could be deadly since qproc can now be
       NULL. Renaming qproc should prevent this from happening. There are no
       kernel drivers that actually access this callback directly, BTW.
    
    3) test whether wait == NULL to determine whether poll would return without
       waiting. This is no longer sufficient as the correct test is now
       wait == NULL || wait->_qproc == NULL.
    
       However, the worst that can happen here is a slight performance hit in
       the case where wait != NULL and wait->_qproc == NULL. In that case the
       driver will assume that poll_wait() will actually add the fd to the set
       of waiting file descriptors. Of course, poll_wait() will not do that
       since it tests for wait->_qproc. This will not break anything, though.
    
       There is only one place in the whole kernel where this happens
       (sock_poll_wait() in include/net/sock.h) and that code will be replaced
       by a call to poll_does_not_wait() in the next patch.
    
       Note that even if wait->_qproc != NULL drivers cannot rely on poll_wait()
       actually waiting. The next file descriptor from the set might match the
       event mask and thus any possible waits will never happen.
    
    Signed-off-by: Hans Verkuil <hans.verkuil@cisco.com>
    Reviewed-by: Jonathan Corbet <corbet@lwn.net>
    Reviewed-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Hans de Goede <hdegoede@redhat.com>
    Cc: Mauro Carvalho Chehab <mchehab@infradead.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 04bc0b30e9e9..a6ba1f8871fd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1854,7 +1854,7 @@ static inline bool wq_has_sleeper(struct socket_wq *wq)
 static inline void sock_poll_wait(struct file *filp,
 		wait_queue_head_t *wait_address, poll_table *p)
 {
-	if (p && wait_address) {
+	if (!poll_does_not_wait(p) && wait_address) {
 		poll_wait(filp, wait_address, p);
 		/*
 		 * We need to be sure we are in sync with the

commit 3556485f1595e3964ba539e39ea682acbb835cee
Merge: b8716614a7cc 09f61cdbb32a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 21 13:25:04 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates for 3.4 from James Morris:
     "The main addition here is the new Yama security module from Kees Cook,
      which was discussed at the Linux Security Summit last year.  Its
      purpose is to collect miscellaneous DAC security enhancements in one
      place.  This also marks a departure in policy for LSM modules, which
      were previously limited to being standalone access control systems.
      Chromium OS is using Yama, and I believe there are plans for Ubuntu,
      at least.
    
      This patchset also includes maintenance updates for AppArmor, TOMOYO
      and others."
    
    Fix trivial conflict in <net/sock.h> due to the jumo_label->static_key
    rename.
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (38 commits)
      AppArmor: Fix location of const qualifier on generated string tables
      TOMOYO: Return error if fails to delete a domain
      AppArmor: add const qualifiers to string arrays
      AppArmor: Add ability to load extended policy
      TOMOYO: Return appropriate value to poll().
      AppArmor: Move path failure information into aa_get_name and rename
      AppArmor: Update dfa matching routines.
      AppArmor: Minor cleanup of d_namespace_path to consolidate error handling
      AppArmor: Retrieve the dentry_path for error reporting when path lookup fails
      AppArmor: Add const qualifiers to generated string tables
      AppArmor: Fix oops in policy unpack auditing
      AppArmor: Fix error returned when a path lookup is disconnected
      KEYS: testing wrong bit for KEY_FLAG_REVOKED
      TOMOYO: Fix mount flags checking order.
      security: fix ima kconfig warning
      AppArmor: Fix the error case for chroot relative path name lookup
      AppArmor: fix mapping of META_READ to audit and quiet flags
      AppArmor: Fix underflow in xindex calculation
      AppArmor: Fix dropping of allowed operations that are force audited
      AppArmor: Add mising end of structure test to caps unpacking
      ...

commit 3b59bf081622b6446db77ad06c93fe23677bc533
Merge: e45836fafe15 bbdb32cb5b73
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 21:04:47 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking merge from David Miller:
     "1) Move ixgbe driver over to purely page based buffering on receive.
         From Alexander Duyck.
    
      2) Add receive packet steering support to e1000e, from Bruce Allan.
    
      3) Convert TCP MD5 support over to RCU, from Eric Dumazet.
    
      4) Reduce cpu usage in handling out-of-order TCP packets on modern
         systems, also from Eric Dumazet.
    
      5) Support the IP{,V6}_UNICAST_IF socket options, making the wine
         folks happy, from Erich Hoover.
    
      6) Support VLAN trunking from guests in hyperv driver, from Haiyang
         Zhang.
    
      7) Support byte-queue-limtis in r8169, from Igor Maravic.
    
      8) Outline code intended for IP_RECVTOS in IP_PKTOPTIONS existed but
         was never properly implemented, Jiri Benc fixed that.
    
      9) 64-bit statistics support in r8169 and 8139too, from Junchang Wang.
    
      10) Support kernel side dump filtering by ctmark in netfilter
          ctnetlink, from Pablo Neira Ayuso.
    
      11) Support byte-queue-limits in gianfar driver, from Paul Gortmaker.
    
      12) Add new peek socket options to assist with socket migration, from
          Pavel Emelyanov.
    
      13) Add sch_plug packet scheduler whose queue is controlled by
          userland daemons using explicit freeze and release commands.  From
          Shriram Rajagopalan.
    
      14) Fix FCOE checksum offload handling on transmit, from Yi Zou."
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1846 commits)
      Fix pppol2tp getsockname()
      Remove printk from rds_sendmsg
      ipv6: fix incorrent ipv6 ipsec packet fragment
      cpsw: Hook up default ndo_change_mtu.
      net: qmi_wwan: fix build error due to cdc-wdm dependecy
      netdev: driver: ethernet: Add TI CPSW driver
      netdev: driver: ethernet: add cpsw address lookup engine support
      phy: add am79c874 PHY support
      mlx4_core: fix race on comm channel
      bonding: send igmp report for its master
      fs_enet: Add MPC5125 FEC support and PHY interface selection
      net: bpf_jit: fix BPF_S_LDX_B_MSH compilation
      net: update the usage of CHECKSUM_UNNECESSARY
      fcoe: use CHECKSUM_UNNECESSARY instead of CHECKSUM_PARTIAL on tx
      net: do not do gso for CHECKSUM_UNNECESSARY in netif_needs_gso
      ixgbe: Fix issues with SR-IOV loopback when flow control is disabled
      net/hyperv: Fix the code handling tx busy
      ixgbe: fix namespace issues when FCoE/DCB is not enabled
      rtlwifi: Remove unused ETH_ADDR_LEN defines
      igbvf: Use ETH_ALEN
      ...
    
    Fix up fairly trivial conflicts in drivers/isdn/gigaset/interface.c and
    drivers/net/usb/{Kconfig,qmi_wwan.c} as per David.

commit 0d9cabdccedb79ee5f27b77ff51f29a9e7d23275
Merge: 701085b21901 3ce3230a0cff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 18:11:21 2012 -0700

    Merge branch 'for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup changes from Tejun Heo:
     "Out of the 8 commits, one fixes a long-standing locking issue around
      tasklist walking and others are cleanups."
    
    * 'for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: Walk task list under tasklist_lock in cgroup_enable_task_cg_list
      cgroup: Remove wrong comment on cgroup_enable_task_cg_list()
      cgroup: remove cgroup_subsys argument from callbacks
      cgroup: remove extra calls to find_existing_css_set
      cgroup: replace tasklist_lock with rcu_read_lock
      cgroup: simplify double-check locking in cgroup_attach_proc
      cgroup: move struct cgroup_pidlist out from the header file
      cgroup: remove cgroup_attach_task_current_cg()

commit 3bdc0eba0b8b47797f4a76e377dd8360f317450f
Author: Ben Greear <greearb@candelatech.com>
Date:   Sat Feb 11 15:39:30 2012 +0000

    net: Add framework to allow sending packets with customized CRC.
    
    This is useful for testing RX handling of frames with bad
    CRCs.
    
    Requires driver support to actually put the packet on the
    wire properly.
    
    Signed-off-by: Ben Greear <greearb@candelatech.com>
    Tested-by: Aaron Brown <aaron.f.brown@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9c0553b9e451..ba761e7de252 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -615,6 +615,10 @@ enum sock_flags {
 	SOCK_RXQ_OVFL,
 	SOCK_ZEROCOPY, /* buffers from userspace */
 	SOCK_WIFI_STATUS, /* push wifi status to userspace */
+	SOCK_NOFCS, /* Tell NIC not to do the Ethernet FCS.
+		     * Will use last 4 bytes of packet sent from
+		     * user-space instead.
+		     */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)

commit c5905afb0ee6550b42c49213da1c22d67316c194
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 24 08:31:31 2012 +0100

    static keys: Introduce 'struct static_key', static_key_true()/false() and static_key_slow_[inc|dec]()
    
    So here's a boot tested patch on top of Jason's series that does
    all the cleanups I talked about and turns jump labels into a
    more intuitive to use facility. It should also address the
    various misconceptions and confusions that surround jump labels.
    
    Typical usage scenarios:
    
            #include <linux/static_key.h>
    
            struct static_key key = STATIC_KEY_INIT_TRUE;
    
            if (static_key_false(&key))
                    do unlikely code
            else
                    do likely code
    
    Or:
    
            if (static_key_true(&key))
                    do likely code
            else
                    do unlikely code
    
    The static key is modified via:
    
            static_key_slow_inc(&key);
            ...
            static_key_slow_dec(&key);
    
    The 'slow' prefix makes it abundantly clear that this is an
    expensive operation.
    
    I've updated all in-kernel code to use this everywhere. Note
    that I (intentionally) have not pushed through the rename
    blindly through to the lowest levels: the actual jump-label
    patching arch facility should be named like that, so we want to
    decouple jump labels from the static-key facility a bit.
    
    On non-jump-label enabled architectures static keys default to
    likely()/unlikely() branches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: a.p.zijlstra@chello.nl
    Cc: mathieu.desnoyers@efficios.com
    Cc: davem@davemloft.net
    Cc: ddaney.cavm@gmail.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120222085809.GA26397@elte.hu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91c1c8baf020..dcde2d9268cd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -55,7 +55,7 @@
 #include <linux/uaccess.h>
 #include <linux/memcontrol.h>
 #include <linux/res_counter.h>
-#include <linux/jump_label.h>
+#include <linux/static_key.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -924,13 +924,13 @@ inline void sk_refcnt_debug_release(const struct sock *sk)
 #endif /* SOCK_REFCNT_DEBUG */
 
 #if defined(CONFIG_CGROUP_MEM_RES_CTLR_KMEM) && defined(CONFIG_NET)
-extern struct jump_label_key memcg_socket_limit_enabled;
+extern struct static_key memcg_socket_limit_enabled;
 static inline struct cg_proto *parent_cg_proto(struct proto *proto,
 					       struct cg_proto *cg_proto)
 {
 	return proto->proto_cgroup(parent_mem_cgroup(cg_proto->memcg));
 }
-#define mem_cgroup_sockets_enabled static_branch(&memcg_socket_limit_enabled)
+#define mem_cgroup_sockets_enabled static_key_false(&memcg_socket_limit_enabled)
 #else
 #define mem_cgroup_sockets_enabled 0
 static inline struct cg_proto *parent_cg_proto(struct proto *proto,

commit ef64a54f6e558155b4f149bb10666b9e914b6c54
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Tue Feb 21 07:31:34 2012 +0000

    sock: Introduce the SO_PEEK_OFF sock option
    
    This one specifies where to start MSG_PEEK-ing queue data from. When
    set to negative value means that MSG_PEEK works as ususally -- peeks
    from the head of the queue always.
    
    When some bytes are peeked from queue and the peeking offset is non
    negative it is moved forward so that the next peek will return next
    portion of data.
    
    When non-peeking recvmsg occurs and the peeking offset is non negative
    is is moved backward so that the next peek will still peek the proper
    data (i.e. the one that would have been picked if there were no non
    peeking recv in between).
    
    The offset is set using per-proto opteration to let the protocol handle
    the locking issues and to check whether the peeking offset feature is
    supported by the protocol the socket belongs to.
    
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91c1c8baf020..9c0553b9e451 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -357,6 +357,7 @@ struct sock {
 	struct page		*sk_sndmsg_page;
 	struct sk_buff		*sk_send_head;
 	__u32			sk_sndmsg_off;
+	__s32			sk_peek_off;
 	int			sk_write_pending;
 #ifdef CONFIG_SECURITY
 	void			*sk_security;
@@ -373,6 +374,30 @@ struct sock {
 	void                    (*sk_destruct)(struct sock *sk);
 };
 
+static inline int sk_peek_offset(struct sock *sk, int flags)
+{
+	if ((flags & MSG_PEEK) && (sk->sk_peek_off >= 0))
+		return sk->sk_peek_off;
+	else
+		return 0;
+}
+
+static inline void sk_peek_offset_bwd(struct sock *sk, int val)
+{
+	if (sk->sk_peek_off >= 0) {
+		if (sk->sk_peek_off >= val)
+			sk->sk_peek_off -= val;
+		else
+			sk->sk_peek_off = 0;
+	}
+}
+
+static inline void sk_peek_offset_fwd(struct sock *sk, int val)
+{
+	if (sk->sk_peek_off >= 0)
+		sk->sk_peek_off += val;
+}
+
 /*
  * Hashed lists helper routines
  */

commit 4040153087478993cbf0809f444400a3c808074c
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Mon Feb 13 03:58:52 2012 +0000

    security: trim security.h
    
    Trim security.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91c1c8baf020..27508f07eada 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -56,6 +56,8 @@
 #include <linux/memcontrol.h>
 #include <linux/res_counter.h>
 #include <linux/jump_label.h>
+#include <linux/aio.h>
+#include <linux/sched.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>

commit 761b3ef50e1c2649cffbfa67a4dcb2dcdb7982ed
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Jan 31 13:47:36 2012 +0800

    cgroup: remove cgroup_subsys argument from callbacks
    
    The argument is not used at all, and it's not necessary, because
    a specific callback handler of course knows which subsys it
    belongs to.
    
    Now only ->pupulate() takes this argument, because the handlers of
    this callback always call cgroup_add_file()/cgroup_add_files().
    
    So we reduce a few lines of code, though the shrinking of object size
    is minimal.
    
     16 files changed, 113 insertions(+), 162 deletions(-)
    
       text    data     bss     dec     hex filename
    5486240  656987 7039960 13183187         c928d3 vmlinux.o.orig
    5486170  656987 7039960 13183117         c9288d vmlinux.o
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index bb972d254dff..705d1add19a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -68,7 +68,7 @@ struct cgroup;
 struct cgroup_subsys;
 #ifdef CONFIG_NET
 int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss);
-void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss);
+void mem_cgroup_sockets_destroy(struct cgroup *cgrp);
 #else
 static inline
 int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)
@@ -76,7 +76,7 @@ int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)
 	return 0;
 }
 static inline
-void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss)
+void mem_cgroup_sockets_destroy(struct cgroup *cgrp)
 {
 }
 #endif
@@ -869,8 +869,7 @@ struct proto {
 	 */
 	int			(*init_cgroup)(struct cgroup *cgrp,
 					       struct cgroup_subsys *ss);
-	void			(*destroy_cgroup)(struct cgroup *cgrp,
-						  struct cgroup_subsys *ss);
+	void			(*destroy_cgroup)(struct cgroup *cgrp);
 	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
 #endif
 };

commit 9018e93948c6f8f95fbcc9fa05f6c403d6adb406
Author: Glauber Costa <glommer@parallels.com>
Date:   Thu Jan 26 12:09:28 2012 +0000

    net: explicitly add jump_label.h header to sock.h
    
    Commit 36a1211970193ce215de50ed1e4e1272bc814df1 removed linux/module.h
    include statement from one of the headers that end up in net/sock.h.
    It was providing us with static_branch() definition implicitly, so
    after its removal the build got broken.
    
    To fix this, and avoid having this happening in the future,
    let me do the right thing and include linux/jump_label.h
    explicitly in sock.h.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reported-by: Randy Dunlap <rdunlap@xenotime.net>
    CC: David S. Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4c69ac165e6b..91c1c8baf020 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -55,6 +55,7 @@
 #include <linux/uaccess.h>
 #include <linux/memcontrol.h>
 #include <linux/res_counter.h>
+#include <linux/jump_label.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>

commit 0e90b31f4ba77027a7c21cbfc66404df0851ca21
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Jan 20 04:57:16 2012 +0000

    net: introduce res_counter_charge_nofail() for socket allocations
    
    There is a case in __sk_mem_schedule(), where an allocation
    is beyond the maximum, but yet we are allowed to proceed.
    It happens under the following condition:
    
            sk->sk_wmem_queued + size >= sk->sk_sndbuf
    
    The network code won't revert the allocation in this case,
    meaning that at some point later it'll try to do it. Since
    this is never communicated to the underlying res_counter
    code, there is an inbalance in res_counter uncharge operation.
    
    I see two ways of fixing this:
    
    1) storing the information about those allocations somewhere
       in memcg, and then deducting from that first, before
       we start draining the res_counter,
    2) providing a slightly different allocation function for
       the res_counter, that matches the original behavior of
       the network code more closely.
    
    I decided to go for #2 here, believing it to be more elegant,
    since #1 would require us to do basically that, but in a more
    obscure way.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    CC: Tejun Heo <tj@kernel.org>
    CC: Li Zefan <lizf@cn.fujitsu.com>
    CC: Laurent Chavey <chavey@google.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0e7a9b05f92b..4c69ac165e6b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1008,9 +1008,8 @@ static inline void memcg_memory_allocated_add(struct cg_proto *prot,
 	struct res_counter *fail;
 	int ret;
 
-	ret = res_counter_charge(prot->memory_allocated,
-				 amt << PAGE_SHIFT, &fail);
-
+	ret = res_counter_charge_nofail(prot->memory_allocated,
+					amt << PAGE_SHIFT, &fail);
 	if (ret < 0)
 		*parent_status = OVER_LIMIT;
 }
@@ -1054,12 +1053,11 @@ sk_memory_allocated_add(struct sock *sk, int amt, int *parent_status)
 }
 
 static inline void
-sk_memory_allocated_sub(struct sock *sk, int amt, int parent_status)
+sk_memory_allocated_sub(struct sock *sk, int amt)
 {
 	struct proto *prot = sk->sk_prot;
 
-	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
-	    parent_status != OVER_LIMIT) /* Otherwise was uncharged already */
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
 		memcg_memory_allocated_sub(sk->sk_cgrp, amt);
 
 	atomic_long_sub(amt, prot->memory_allocated);

commit 376be5ff8a6a36efadd131860cf26841f366d44c
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Jan 20 04:57:14 2012 +0000

    net: fix socket memcg build with !CONFIG_NET
    
    There is still a build bug with the sock memcg code, that triggers
    with !CONFIG_NET, that survived my series of randconfig builds.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reported-by: Randy Dunlap <rdunlap@xenotime.net>
    CC: Hiroyouki Kamezawa <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 97fc0ad47da0..0e7a9b05f92b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -922,7 +922,7 @@ inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM
+#if defined(CONFIG_CGROUP_MEM_RES_CTLR_KMEM) && defined(CONFIG_NET)
 extern struct jump_label_key memcg_socket_limit_enabled;
 static inline struct cg_proto *parent_cg_proto(struct proto *proto,
 					       struct cg_proto *cg_proto)

commit 1a3bc369ba547c11ca8b3ed079d7584f27499e70
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Sat Jan 21 09:03:10 2012 +0000

    kernel-doc: fix new warning in net/sock.h
    
    Fix new kernel-doc warning:
    
    Warning(include/net/sock.h:372): No description found for parameter 'sk_cgrp_prioidx'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bb972d254dff..97fc0ad47da0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -226,6 +226,7 @@ struct cg_proto;
   *	@sk_ack_backlog: current listen backlog
   *	@sk_max_ack_backlog: listen backlog set in listen()
   *	@sk_priority: %SO_PRIORITY setting
+  *	@sk_cgrp_prioidx: socket group's priority map index
   *	@sk_type: socket type (%SOCK_STREAM, etc)
   *	@sk_protocol: which protocol this socket belongs in this network family
   *	@sk_peer_pid: &struct pid for this socket's peer

commit abb434cb0539fb355c1c921f8fd761efbbac3462
Merge: 2494654d4890 6350323ad8de
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 23 17:13:56 2011 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            net/bluetooth/l2cap_core.c
    
    Just two overlapping changes, one added an initialization of
    a local variable, and another change added a new local variable.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0fd7bac6b6157eed6cf0cb86a1e88ba29e57c033
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Dec 21 07:11:44 2011 +0000

    net: relax rcvbuf limits
    
    skb->truesize might be big even for a small packet.
    
    Its even bigger after commit 87fb4b7b533 (net: more accurate skb
    truesize) and big MTU.
    
    We should allow queueing at least one packet per receiver, even with a
    low RCVBUF setting.
    
    Reported-by: Michal Simek <monstr@monstr.eu>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index abb6e0f0c3c3..32e39371fba6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -637,12 +637,14 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 
 /*
  * Take into account size of receive queue and backlog queue
+ * Do not take into account this skb truesize,
+ * to allow even a single big packet to come.
  */
 static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb)
 {
 	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
 
-	return qsize + skb->truesize > sk->sk_rcvbuf;
+	return qsize > sk->sk_rcvbuf;
 }
 
 /* The per-socket spinlock must be held here. */

commit c607b2ed84929e143d9fb5653c4b5d0109147cde
Author: Glauber Costa <glommer@parallels.com>
Date:   Fri Dec 16 00:52:00 2011 +0000

    net: fix compilation with !CONFIG_NET
    
    Reported-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    CC: Hiroyouki Kamezawa <kamezawa.hiroyu@jp.fujitsu.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6fe0dae81451..3144c7950649 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -66,8 +66,20 @@
 
 struct cgroup;
 struct cgroup_subsys;
+#ifdef CONFIG_NET
 int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss);
 void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss);
+#else
+static inline
+int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)
+{
+	return 0;
+}
+static inline
+void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss)
+{
+}
+#endif
 /*
  * This structure really needs to be cleaned up.
  * Most of it is for TCP, and not used by any of

commit 9f048bfba15a22d1d1ce0c1f44567fa16bed4d25
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Dec 13 03:59:08 2011 +0000

    net: fix build error if CONFIG_CGROUPS=n
    
    Reported-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 18ecc9919d29..6fe0dae81451 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -64,6 +64,8 @@
 #include <net/dst.h>
 #include <net/checksum.h>
 
+struct cgroup;
+struct cgroup_subsys;
 int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss);
 void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss);
 /*

commit d1a4c0b37c296e600ffe08edb0db2dc1b8f550d7
Author: Glauber Costa <glommer@parallels.com>
Date:   Sun Dec 11 21:47:04 2011 +0000

    tcp memory pressure controls
    
    This patch introduces memory pressure controls for the tcp
    protocol. It uses the generic socket memory pressure code
    introduced in earlier patches, and fills in the
    necessary data in cg_proto struct.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtisu.com>
    CC: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d5eab256167c..18ecc9919d29 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -64,6 +64,8 @@
 #include <net/dst.h>
 #include <net/checksum.h>
 
+int mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss);
+void mem_cgroup_sockets_destroy(struct cgroup *cgrp, struct cgroup_subsys *ss);
 /*
  * This structure really needs to be cleaned up.
  * Most of it is for TCP, and not used by any of

commit e1aab161e0135aafcd439be20b4f35e4b0922d95
Author: Glauber Costa <glommer@parallels.com>
Date:   Sun Dec 11 21:47:03 2011 +0000

    socket: initial cgroup code.
    
    The goal of this work is to move the memory pressure tcp
    controls to a cgroup, instead of just relying on global
    conditions.
    
    To avoid excessive overhead in the network fast paths,
    the code that accounts allocated memory to a cgroup is
    hidden inside a static_branch(). This branch is patched out
    until the first non-root cgroup is created. So when nobody
    is using cgroups, even if it is mounted, no significant performance
    penalty should be seen.
    
    This patch handles the generic part of the code, and has nothing
    tcp-specific.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujtsu.com>
    CC: Kirill A. Shutemov <kirill@shutemov.name>
    CC: David S. Miller <davem@davemloft.net>
    CC: Eric W. Biederman <ebiederm@xmission.com>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ed0dbf034539..d5eab256167c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -54,6 +54,7 @@
 #include <linux/slab.h>
 #include <linux/uaccess.h>
 #include <linux/memcontrol.h>
+#include <linux/res_counter.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -168,6 +169,7 @@ struct sock_common {
 	/* public: */
 };
 
+struct cg_proto;
 /**
   *	struct sock - network layer representation of sockets
   *	@__sk_common: shared layout with inet_timewait_sock
@@ -228,6 +230,7 @@ struct sock_common {
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
   *	@sk_classid: this socket's cgroup classid
+  *	@sk_cgrp: this socket's cgroup-specific proto data
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
   *	@sk_data_ready: callback to indicate there is data to be processed
@@ -342,6 +345,7 @@ struct sock {
 #endif
 	__u32			sk_mark;
 	u32			sk_classid;
+	struct cg_proto		*sk_cgrp;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
 	void			(*sk_write_space)(struct sock *sk);
@@ -838,6 +842,37 @@ struct proto {
 #ifdef SOCK_REFCNT_DEBUG
 	atomic_t		socks;
 #endif
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM
+	/*
+	 * cgroup specific init/deinit functions. Called once for all
+	 * protocols that implement it, from cgroups populate function.
+	 * This function has to setup any files the protocol want to
+	 * appear in the kmem cgroup filesystem.
+	 */
+	int			(*init_cgroup)(struct cgroup *cgrp,
+					       struct cgroup_subsys *ss);
+	void			(*destroy_cgroup)(struct cgroup *cgrp,
+						  struct cgroup_subsys *ss);
+	struct cg_proto		*(*proto_cgroup)(struct mem_cgroup *memcg);
+#endif
+};
+
+struct cg_proto {
+	void			(*enter_memory_pressure)(struct sock *sk);
+	struct res_counter	*memory_allocated;	/* Current allocated memory. */
+	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
+	int			*memory_pressure;
+	long			*sysctl_mem;
+	/*
+	 * memcg field is used to find which memcg we belong directly
+	 * Each memcg struct can hold more than one cg_proto, so container_of
+	 * won't really cut.
+	 *
+	 * The elegant solution would be having an inverse function to
+	 * proto_cgroup in struct proto, but that means polluting the structure
+	 * for everybody, instead of just for memcg users.
+	 */
+	struct mem_cgroup	*memcg;
 };
 
 extern int proto_register(struct proto *prot, int alloc_slab);
@@ -856,7 +891,7 @@ static inline void sk_refcnt_debug_dec(struct sock *sk)
 	       sk->sk_prot->name, sk, atomic_read(&sk->sk_prot->socks));
 }
 
-static inline void sk_refcnt_debug_release(const struct sock *sk)
+inline void sk_refcnt_debug_release(const struct sock *sk)
 {
 	if (atomic_read(&sk->sk_refcnt) != 1)
 		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",
@@ -868,6 +903,24 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM
+extern struct jump_label_key memcg_socket_limit_enabled;
+static inline struct cg_proto *parent_cg_proto(struct proto *proto,
+					       struct cg_proto *cg_proto)
+{
+	return proto->proto_cgroup(parent_mem_cgroup(cg_proto->memcg));
+}
+#define mem_cgroup_sockets_enabled static_branch(&memcg_socket_limit_enabled)
+#else
+#define mem_cgroup_sockets_enabled 0
+static inline struct cg_proto *parent_cg_proto(struct proto *proto,
+					       struct cg_proto *cg_proto)
+{
+	return NULL;
+}
+#endif
+
+
 static inline bool sk_has_memory_pressure(const struct sock *sk)
 {
 	return sk->sk_prot->memory_pressure != NULL;
@@ -877,6 +930,10 @@ static inline bool sk_under_memory_pressure(const struct sock *sk)
 {
 	if (!sk->sk_prot->memory_pressure)
 		return false;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return !!*sk->sk_cgrp->memory_pressure;
+
 	return !!*sk->sk_prot->memory_pressure;
 }
 
@@ -884,52 +941,136 @@ static inline void sk_leave_memory_pressure(struct sock *sk)
 {
 	int *memory_pressure = sk->sk_prot->memory_pressure;
 
-	if (memory_pressure && *memory_pressure)
+	if (!memory_pressure)
+		return;
+
+	if (*memory_pressure)
 		*memory_pressure = 0;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+		struct proto *prot = sk->sk_prot;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			if (*cg_proto->memory_pressure)
+				*cg_proto->memory_pressure = 0;
+	}
+
 }
 
 static inline void sk_enter_memory_pressure(struct sock *sk)
 {
-	if (sk->sk_prot->enter_memory_pressure)
-		sk->sk_prot->enter_memory_pressure(sk);
+	if (!sk->sk_prot->enter_memory_pressure)
+		return;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+		struct proto *prot = sk->sk_prot;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			cg_proto->enter_memory_pressure(sk);
+	}
+
+	sk->sk_prot->enter_memory_pressure(sk);
 }
 
 static inline long sk_prot_mem_limits(const struct sock *sk, int index)
 {
 	long *prot = sk->sk_prot->sysctl_mem;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		prot = sk->sk_cgrp->sysctl_mem;
 	return prot[index];
 }
 
+static inline void memcg_memory_allocated_add(struct cg_proto *prot,
+					      unsigned long amt,
+					      int *parent_status)
+{
+	struct res_counter *fail;
+	int ret;
+
+	ret = res_counter_charge(prot->memory_allocated,
+				 amt << PAGE_SHIFT, &fail);
+
+	if (ret < 0)
+		*parent_status = OVER_LIMIT;
+}
+
+static inline void memcg_memory_allocated_sub(struct cg_proto *prot,
+					      unsigned long amt)
+{
+	res_counter_uncharge(prot->memory_allocated, amt << PAGE_SHIFT);
+}
+
+static inline u64 memcg_memory_allocated_read(struct cg_proto *prot)
+{
+	u64 ret;
+	ret = res_counter_read_u64(prot->memory_allocated, RES_USAGE);
+	return ret >> PAGE_SHIFT;
+}
+
 static inline long
 sk_memory_allocated(const struct sock *sk)
 {
 	struct proto *prot = sk->sk_prot;
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return memcg_memory_allocated_read(sk->sk_cgrp);
+
 	return atomic_long_read(prot->memory_allocated);
 }
 
 static inline long
-sk_memory_allocated_add(struct sock *sk, int amt)
+sk_memory_allocated_add(struct sock *sk, int amt, int *parent_status)
 {
 	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		memcg_memory_allocated_add(sk->sk_cgrp, amt, parent_status);
+		/* update the root cgroup regardless */
+		atomic_long_add_return(amt, prot->memory_allocated);
+		return memcg_memory_allocated_read(sk->sk_cgrp);
+	}
+
 	return atomic_long_add_return(amt, prot->memory_allocated);
 }
 
 static inline void
-sk_memory_allocated_sub(struct sock *sk, int amt)
+sk_memory_allocated_sub(struct sock *sk, int amt, int parent_status)
 {
 	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp &&
+	    parent_status != OVER_LIMIT) /* Otherwise was uncharged already */
+		memcg_memory_allocated_sub(sk->sk_cgrp, amt);
+
 	atomic_long_sub(amt, prot->memory_allocated);
 }
 
 static inline void sk_sockets_allocated_dec(struct sock *sk)
 {
 	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			percpu_counter_dec(cg_proto->sockets_allocated);
+	}
+
 	percpu_counter_dec(prot->sockets_allocated);
 }
 
 static inline void sk_sockets_allocated_inc(struct sock *sk)
 {
 	struct proto *prot = sk->sk_prot;
+
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp) {
+		struct cg_proto *cg_proto = sk->sk_cgrp;
+
+		for (; cg_proto; cg_proto = parent_cg_proto(prot, cg_proto))
+			percpu_counter_inc(cg_proto->sockets_allocated);
+	}
+
 	percpu_counter_inc(prot->sockets_allocated);
 }
 
@@ -938,6 +1079,9 @@ sk_sockets_allocated_read_positive(struct sock *sk)
 {
 	struct proto *prot = sk->sk_prot;
 
+	if (mem_cgroup_sockets_enabled && sk->sk_cgrp)
+		return percpu_counter_sum_positive(sk->sk_cgrp->sockets_allocated);
+
 	return percpu_counter_sum_positive(prot->sockets_allocated);
 }
 

commit 180d8cd942ce336b2c869d324855c40c5db478ad
Author: Glauber Costa <glommer@parallels.com>
Date:   Sun Dec 11 21:47:02 2011 +0000

    foundations of per-cgroup memory pressure controlling.
    
    This patch replaces all uses of struct sock fields' memory_pressure,
    memory_allocated, sockets_allocated, and sysctl_mem to acessor
    macros. Those macros can either receive a socket argument, or a mem_cgroup
    argument, depending on the context they live in.
    
    Since we're only doing a macro wrapping here, no performance impact at all is
    expected in the case where we don't have cgroups disabled.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Reviewed-by: Hiroyouki Kamezawa <kamezawa.hiroyu@jp.fujitsu.com>
    CC: David S. Miller <davem@davemloft.net>
    CC: Eric W. Biederman <ebiederm@xmission.com>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8ac338cb39ce..ed0dbf034539 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -53,6 +53,7 @@
 #include <linux/security.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
+#include <linux/memcontrol.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -867,6 +868,99 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
+static inline bool sk_has_memory_pressure(const struct sock *sk)
+{
+	return sk->sk_prot->memory_pressure != NULL;
+}
+
+static inline bool sk_under_memory_pressure(const struct sock *sk)
+{
+	if (!sk->sk_prot->memory_pressure)
+		return false;
+	return !!*sk->sk_prot->memory_pressure;
+}
+
+static inline void sk_leave_memory_pressure(struct sock *sk)
+{
+	int *memory_pressure = sk->sk_prot->memory_pressure;
+
+	if (memory_pressure && *memory_pressure)
+		*memory_pressure = 0;
+}
+
+static inline void sk_enter_memory_pressure(struct sock *sk)
+{
+	if (sk->sk_prot->enter_memory_pressure)
+		sk->sk_prot->enter_memory_pressure(sk);
+}
+
+static inline long sk_prot_mem_limits(const struct sock *sk, int index)
+{
+	long *prot = sk->sk_prot->sysctl_mem;
+	return prot[index];
+}
+
+static inline long
+sk_memory_allocated(const struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+	return atomic_long_read(prot->memory_allocated);
+}
+
+static inline long
+sk_memory_allocated_add(struct sock *sk, int amt)
+{
+	struct proto *prot = sk->sk_prot;
+	return atomic_long_add_return(amt, prot->memory_allocated);
+}
+
+static inline void
+sk_memory_allocated_sub(struct sock *sk, int amt)
+{
+	struct proto *prot = sk->sk_prot;
+	atomic_long_sub(amt, prot->memory_allocated);
+}
+
+static inline void sk_sockets_allocated_dec(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+	percpu_counter_dec(prot->sockets_allocated);
+}
+
+static inline void sk_sockets_allocated_inc(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+	percpu_counter_inc(prot->sockets_allocated);
+}
+
+static inline int
+sk_sockets_allocated_read_positive(struct sock *sk)
+{
+	struct proto *prot = sk->sk_prot;
+
+	return percpu_counter_sum_positive(prot->sockets_allocated);
+}
+
+static inline int
+proto_sockets_allocated_sum_positive(struct proto *prot)
+{
+	return percpu_counter_sum_positive(prot->sockets_allocated);
+}
+
+static inline long
+proto_memory_allocated(struct proto *prot)
+{
+	return atomic_long_read(prot->memory_allocated);
+}
+
+static inline bool
+proto_memory_pressure(struct proto *prot)
+{
+	if (!prot->memory_pressure)
+		return false;
+	return !!*prot->memory_pressure;
+}
+
 
 #ifdef CONFIG_PROC_FS
 /* Called with local bh disabled */
@@ -1674,7 +1768,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 
 	page = alloc_pages(sk->sk_allocation, 0);
 	if (!page) {
-		sk->sk_prot->enter_memory_pressure(sk);
+		sk_enter_memory_pressure(sk);
 		sk_stream_moderate_sndbuf(sk);
 	}
 	return page;

commit 5bc1421e34ecfe0bd4b26dc3232b7d5e25179144
Author: Neil Horman <nhorman@tuxdriver.com>
Date:   Tue Nov 22 05:10:51 2011 +0000

    net: add network priority cgroup infrastructure (v4)
    
    This patch adds in the infrastructure code to create the network priority
    cgroup.  The cgroup, in addition to the standard processes file creates two
    control files:
    
    1) prioidx - This is a read-only file that exports the index of this cgroup.
    This is a value that is both arbitrary and unique to a cgroup in this subsystem,
    and is used to index the per-device priority map
    
    2) priomap - This is a writeable file.  On read it reports a table of 2-tuples
    <name:priority> where name is the name of a network interface and priority is
    indicates the priority assigned to frames egresessing on the named interface and
    originating from a pid in this cgroup
    
    This cgroup allows for skb priority to be set prior to a root qdisc getting
    selected. This is benenficial for DCB enabled systems, in that it allows for any
    application to use dcb configured priorities so without application modification
    
    Signed-off-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: John Fastabend <john.r.fastabend@intel.com>
    CC: Robert Love <robert.w.love@intel.com>
    CC: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1c28f394d8ec..8ac338cb39ce 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -320,6 +320,9 @@ struct sock {
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
+#ifdef CONFIG_CGROUPS
+	__u32			sk_cgrp_prioidx;
+#endif
 	struct pid		*sk_peer_pid;
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;

commit e11c259f745889b55bc5596ca78271f2f5cf08d2
Merge: 8d26784cf0d0 b4487c2d0eda
Author: John W. Linville <linville@tuxdriver.com>
Date:   Thu Nov 17 13:11:43 2011 -0500

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/linville/wireless-next into for-davem
    
    Conflicts:
            include/net/bluetooth/bluetooth.h

commit c8f44affb7244f2ac3e703cab13d55ede27621bb
Author: Michał Mirosław <mirq-linux@rere.qmqm.pl>
Date:   Tue Nov 15 15:29:55 2011 +0000

    net: introduce and use netdev_features_t for device features sets
    
    v2:     add couple missing conversions in drivers
            split unexporting netdev_fix_features()
            implemented %pNF
            convert sock::sk_route_(no?)caps
    
    Signed-off-by: Michał Mirosław <mirq-linux@rere.qmqm.pl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 67cd4581b6da..1331008ad885 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -306,8 +306,8 @@ struct sock {
 	kmemcheck_bitfield_end(flags);
 	int			sk_wmem_queued;
 	gfp_t			sk_allocation;
-	int			sk_route_caps;
-	int			sk_route_nocaps;
+	netdev_features_t	sk_route_caps;
+	netdev_features_t	sk_route_nocaps;
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
@@ -1393,7 +1393,7 @@ static inline int sk_can_gso(const struct sock *sk)
 
 extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
 
-static inline void sk_nocaps_add(struct sock *sk, int flags)
+static inline void sk_nocaps_add(struct sock *sk, netdev_features_t flags)
 {
 	sk->sk_route_nocaps |= flags;
 	sk->sk_route_caps &= ~flags;

commit 6e3e939f3b1bf8534b32ad09ff199d88800835a0
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Wed Nov 9 10:15:42 2011 +0100

    net: add wireless TX status socket option
    
    The 802.1X EAPOL handshake hostapd does requires
    knowing whether the frame was ack'ed by the peer.
    Currently, we fudge this pretty badly by not even
    transmitting the frame as a normal data frame but
    injecting it with radiotap and getting the status
    out of radiotap monitor as well. This is rather
    complex, confuses users (mon.wlan0 presence) and
    doesn't work with all hardware.
    
    To get rid of that hack, introduce a real wifi TX
    status option for data frame transmissions.
    
    This works similar to the existing TX timestamping
    in that it reflects the SKB back to the socket's
    error queue with a SCM_WIFI_STATUS cmsg that has
    an int indicating ACK status (0/1).
    
    Since it is possible that at some point we will
    want to have TX timestamping and wifi status in a
    single errqueue SKB (there's little point in not
    doing that), redefine SO_EE_ORIGIN_TIMESTAMPING
    to SO_EE_ORIGIN_TXSTATUS which can collect more
    than just the timestamp; keep the old constant
    as an alias of course. Currently the internal APIs
    don't make that possible, but it wouldn't be hard
    to split them up in a way that makes it possible.
    
    Thanks to Neil Horman for helping me figure out
    the functions that add the control messages.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5ac682f73d63..fa6f5381c5d6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -564,6 +564,7 @@ enum sock_flags {
 	SOCK_FASYNC, /* fasync() active */
 	SOCK_RXQ_OVFL,
 	SOCK_ZEROCOPY, /* buffers from userspace */
+	SOCK_WIFI_STATUS, /* push wifi status to userspace */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
@@ -1714,6 +1715,8 @@ static inline int sock_intr_errno(long timeo)
 
 extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
 	struct sk_buff *skb);
+extern void __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,
+	struct sk_buff *skb);
 
 static __inline__ void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
@@ -1741,6 +1744,9 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 		__sock_recv_timestamp(msg, sk, skb);
 	else
 		sk->sk_stamp = kt;
+
+	if (sock_flag(sk, SOCK_WIFI_STATUS) && skb->wifi_acked_valid)
+		__sock_recv_wifi_status(msg, sk, skb);
 }
 
 extern void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,

commit e56c57d0d3fdbbdf583d3af96bfb803b8dfa713e
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 8 17:07:07 2011 -0500

    net: rename sk_clone to sk_clone_lock
    
    Make clear that sk_clone() and inet_csk_clone() return a locked socket.
    
    Add _lock() prefix and kerneldoc.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index abb6e0f0c3c3..67cd4581b6da 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1089,8 +1089,8 @@ extern struct sock		*sk_alloc(struct net *net, int family,
 					  struct proto *prot);
 extern void			sk_free(struct sock *sk);
 extern void			sk_release_kernel(struct sock *sk);
-extern struct sock		*sk_clone(const struct sock *sk,
-					  const gfp_t priority);
+extern struct sock		*sk_clone_lock(const struct sock *sk,
+					       const gfp_t priority);
 
 extern struct sk_buff		*sock_wmalloc(struct sock *sk,
 					      unsigned long size, int force,

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit b9075fa968a0a4347aef35e235e2995c0e57dddd
Author: Joe Perches <joe@perches.com>
Date:   Mon Oct 31 17:11:33 2011 -0700

    treewide: use __printf not __attribute__((format(printf,...)))
    
    Standardize the style for compiler based printf format verification.
    Standardized the location of __printf too.
    
    Done via script and a little typing.
    
    $ grep -rPl --include=*.[ch] -w "__attribute__" * | \
      grep -vP "^(tools|scripts|include/linux/compiler-gcc.h)" | \
      xargs perl -n -i -e 'local $/; while (<>) { s/\b__attribute__\s*\(\s*\(\s*format\s*\(\s*printf\s*,\s*(.+)\s*,\s*(.+)\s*\)\s*\)\s*\)/__printf($1, $2)/g ; print; }'
    
    [akpm@linux-foundation.org: revert arch bits]
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5ac682f73d63..c6658bef7f32 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -76,8 +76,8 @@
 					printk(KERN_DEBUG msg); } while (0)
 #else
 /* Validate arguments and do nothing */
-static inline void __attribute__ ((format (printf, 2, 3)))
-SOCK_DEBUG(struct sock *sk, const char *msg, ...)
+static inline __printf(2, 3)
+void SOCK_DEBUG(struct sock *sk, const char *msg, ...)
 {
 }
 #endif

commit de47725421ad5627a5c905f4e40bb844ebc06d29
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu May 26 13:46:22 2011 -0400

    include: replace linux/module.h with "struct module" wherever possible
    
    The <linux/module.h> pretty much brings in the kitchen sink along
    with it, so it should be avoided wherever reasonably possible in
    terms of being included from other commonly used <linux/something.h>
    files, as it results in a measureable increase on compile times.
    
    The worst culprit was probably device.h since it is used everywhere.
    This file also had an implicit dependency/usage of mutex.h which was
    masked by module.h, and is also fixed here at the same time.
    
    There are over a dozen other headers that simply declare the
    struct instead of pulling in the whole file, so follow their lead
    and simply make it a few more.
    
    Most of the implicit dependencies on module.h being present by
    these headers pulling it in have been now weeded out, so we can
    finally make this change with hopefully minimal breakage.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5ac682f73d63..beb1a911acbb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -46,7 +46,6 @@
 #include <linux/list_nulls.h>
 #include <linux/timer.h>
 #include <linux/cache.h>
-#include <linux/module.h>
 #include <linux/lockdep.h>
 #include <linux/netdevice.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
@@ -729,6 +728,7 @@ struct request_sock_ops;
 struct timewait_sock_ops;
 struct inet_hashinfo;
 struct raw_hashinfo;
+struct module;
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface

commit bdeab991918663aed38757904219e8398214334c
Author: Tom Herbert <therbert@google.com>
Date:   Sun Aug 14 19:45:55 2011 +0000

    rps: Add flag to skb to indicate rxhash is based on L4 tuple
    
    The l4_rxhash flag was added to the skb structure to indicate
    that the rxhash value was computed over the 4 tuple for the
    packet which includes the port information in the encapsulated
    transport packet.  This is used by the stack to preserve the
    rxhash value in __skb_rx_tunnel.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8e4062f165b8..5ac682f73d63 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -686,16 +686,25 @@ static inline void sock_rps_reset_flow(const struct sock *sk)
 #endif
 }
 
-static inline void sock_rps_save_rxhash(struct sock *sk, u32 rxhash)
+static inline void sock_rps_save_rxhash(struct sock *sk,
+					const struct sk_buff *skb)
 {
 #ifdef CONFIG_RPS
-	if (unlikely(sk->sk_rxhash != rxhash)) {
+	if (unlikely(sk->sk_rxhash != skb->rxhash)) {
 		sock_rps_reset_flow(sk);
-		sk->sk_rxhash = rxhash;
+		sk->sk_rxhash = skb->rxhash;
 	}
 #endif
 }
 
+static inline void sock_rps_reset_rxhash(struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	sock_rps_reset_flow(sk);
+	sk->sk_rxhash = 0;
+#endif
+}
+
 #define sk_wait_event(__sk, __timeo, __condition)			\
 	({	int __rc;						\
 		release_sock(__sk);					\

commit d3ec4844d449cf7af9e749f73ba2052fb7b72fc2
Merge: 0003230e8200 df2e301fee3c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:56:39 2011 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (43 commits)
      fs: Merge split strings
      treewide: fix potentially dangerous trailing ';' in #defined values/expressions
      uwb: Fix misspelling of neighbourhood in comment
      net, netfilter: Remove redundant goto in ebt_ulog_packet
      trivial: don't touch files that are removed in the staging tree
      lib/vsprintf: replace link to Draft by final RFC number
      doc: Kconfig: `to be' -> `be'
      doc: Kconfig: Typo: square -> squared
      doc: Konfig: Documentation/power/{pm => apm-acpi}.txt
      drivers/net: static should be at beginning of declaration
      drivers/media: static should be at beginning of declaration
      drivers/i2c: static should be at beginning of declaration
      XTENSA: static should be at beginning of declaration
      SH: static should be at beginning of declaration
      MIPS: static should be at beginning of declaration
      ARM: static should be at beginning of declaration
      rcu: treewide: Do not use rcu_read_lock_held when calling rcu_dereference_check
      Update my e-mail address
      PCIe ASPM: forcedly -> forcibly
      gma500: push through device driver tree
      ...
    
    Fix up trivial conflicts:
     - arch/arm/mach-ep93xx/dma-m2p.c (deleted)
     - drivers/gpio/gpio-ep93xx.c (renamed and context nearby)
     - drivers/net/r8169.c (just context changes)

commit d8bf4ca9ca9576548628344c9725edd3786e90b1
Author: Michal Hocko <mhocko@suse.cz>
Date:   Fri Jul 8 14:39:41 2011 +0200

    rcu: treewide: Do not use rcu_read_lock_held when calling rcu_dereference_check
    
    Since ca5ecddf (rcu: define __rcu address space modifier for sparse)
    rcu_dereference_check use rcu_read_lock_held as a part of condition
    automatically so callers do not have to do that as well.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/net/sock.h b/include/net/sock.h
index c0b938cb4b1a..d5b65c19a8e3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1301,8 +1301,7 @@ extern unsigned long sock_i_ino(struct sock *sk);
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
-	return rcu_dereference_check(sk->sk_dst_cache, rcu_read_lock_held() ||
-						       sock_owned_by_user(sk) ||
+	return rcu_dereference_check(sk->sk_dst_cache, sock_owned_by_user(sk) ||
 						       lockdep_is_held(&sk->sk_lock.slock));
 }
 

commit 1cdebb423202e255366a321814fc6df079802a0d
Author: Shirley Ma <mashirle@us.ibm.com>
Date:   Wed Jul 6 12:17:30 2011 +0000

    sock.h: Add a new sock zero-copy flag
    
    Signed-off-by: Shirley Ma <xma@us.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ae56da6ec958..396f735e0cd5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -563,6 +563,7 @@ enum sock_flags {
 	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
 	SOCK_FASYNC, /* fasync() active */
 	SOCK_RXQ_OVFL,
+	SOCK_ZEROCOPY, /* buffers from userspace */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)

commit e12fe68ce34d60c04bb1ddb1d3cc5c3022388fe4
Merge: 7329f0d58de0 712ae51afd55
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 23:23:37 2011 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6

commit e1f91f82b8bb031fe1b7731fb3666fa68c97fd38
Author: Vitaliy Ivanov <vitalivanov@gmail.com>
Date:   Mon Jun 27 19:07:08 2011 +0300

    treewide: fix kernel-doc warnings
    
    Fix 'make htmldocs' warnings:
    
    Warning(/include/linux/hrtimer.h:153): No description found for
    parameter 'clockid'
    Warning(/include/linux/device.h:604): Excess struct/union/enum/typedef
    member 'of_match' description in 'device'
    Warning(/include/net/sock.h:349): Excess struct/union/enum/typedef
    member 'sk_rmem_alloc' description in 'sock'
    
    Signed-off-by: Vitaliy Ivanov <vitalivanov@gmail.com>
    Acked-by: Grant Likely <grant.likely@secretlab.ca>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/include/net/sock.h b/include/net/sock.h
index f2046e404a61..c0b938cb4b1a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -178,7 +178,6 @@ struct sock_common {
   *	@sk_dst_cache: destination cache
   *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy
-  *	@sk_rmem_alloc: receive queue bytes committed
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
   *	@sk_write_queue: Packet sending queue

commit 4d258b25d947521c8b913154db61ec55198243f8
Author: Vitaliy Ivanov <vitalivanov@gmail.com>
Date:   Mon Jun 27 19:07:08 2011 +0300

    Fix some kernel-doc warnings
    
    Fix 'make htmldocs' warnings:
    
      Warning(/include/linux/hrtimer.h:153): No description found for parameter 'clockid'
      Warning(/include/linux/device.h:604): Excess struct/union/enum/typedef member 'of_match' description in 'device'
      Warning(/include/net/sock.h:349): Excess struct/union/enum/typedef member 'sk_rmem_alloc' description in 'sock'
    
    Signed-off-by: Vitaliy Ivanov <vitalivanov@gmail.com>
    Acked-by: Grant Likely <grant.likely@secretlab.ca>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index f2046e404a61..c0b938cb4b1a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -178,7 +178,6 @@ struct sock_common {
   *	@sk_dst_cache: destination cache
   *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy
-  *	@sk_rmem_alloc: receive queue bytes committed
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
   *	@sk_write_queue: Packet sending queue

commit a6b7a407865aab9f849dd99a71072b7cd1175116
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jun 6 10:43:46 2011 +0000

    net: remove interrupt.h inclusion from netdevice.h
    
    * remove interrupt.g inclusion from netdevice.h -- not needed
    * fixup fallout, add interrupt.h and hardirq.h back where needed.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f2046e404a61..ebbc0bafe661 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -40,6 +40,7 @@
 #ifndef _SOCK_H
 #define _SOCK_H
 
+#include <linux/hardirq.h>
 #include <linux/kernel.h>
 #include <linux/list.h>
 #include <linux/list_nulls.h>

commit 1c01a80cfec6f806246f31ff2680cd3639b30e67
Merge: c44d79950b2d 4a9f65f6304a
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Apr 11 13:44:25 2011 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/net/smsc911x.c

commit 912d398d28b4359c2fb1f3763f1ce4f86de8350e
Author: Wei Yongjun <yjwei@cn.fujitsu.com>
Date:   Wed Apr 6 18:40:12 2011 +0000

    net: fix skb_add_data_nocache() to calc csum correctly
    
    commit c6e1a0d12ca7b4f22c58e55a16beacfb7d3d8462 broken the calc
     (net: Allow no-cache copy from user on transmit)
    of checksum, which may cause some tcp packets be dropped because
    incorrect checksum. ssh does not work under today's net-next-2.6
    tree.
    
    Signed-off-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Acked-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 43bd515e92fd..9cbf23c815f5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1392,14 +1392,14 @@ static inline void sk_nocaps_add(struct sock *sk, int flags)
 
 static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
 					   char __user *from, char *to,
-					   int copy)
+					   int copy, int offset)
 {
 	if (skb->ip_summed == CHECKSUM_NONE) {
 		int err = 0;
 		__wsum csum = csum_and_copy_from_user(from, to, copy, 0, &err);
 		if (err)
 			return err;
-		skb->csum = csum_block_add(skb->csum, csum, skb->len);
+		skb->csum = csum_block_add(skb->csum, csum, offset);
 	} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {
 		if (!access_ok(VERIFY_READ, from, copy) ||
 		    __copy_from_user_nocache(to, from, copy))
@@ -1413,11 +1413,12 @@ static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
 static inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,
 				       char __user *from, int copy)
 {
-	int err;
+	int err, offset = skb->len;
 
-	err = skb_do_copy_data_nocache(sk, skb, from, skb_put(skb, copy), copy);
+	err = skb_do_copy_data_nocache(sk, skb, from, skb_put(skb, copy),
+				       copy, offset);
 	if (err)
-		__skb_trim(skb, skb->len);
+		__skb_trim(skb, offset);
 
 	return err;
 }
@@ -1429,8 +1430,8 @@ static inline int skb_copy_to_page_nocache(struct sock *sk, char __user *from,
 {
 	int err;
 
-	err = skb_do_copy_data_nocache(sk, skb, from,
-				       page_address(page) + off, copy);
+	err = skb_do_copy_data_nocache(sk, skb, from, page_address(page) + off,
+				       copy, skb->len);
 	if (err)
 		return err;
 

commit c6e1a0d12ca7b4f22c58e55a16beacfb7d3d8462
Author: Tom Herbert <therbert@google.com>
Date:   Mon Apr 4 22:30:30 2011 -0700

    net: Allow no-cache copy from user on transmit
    
    This patch uses __copy_from_user_nocache on transmit to bypass data
    cache for a performance improvement.  skb_add_data_nocache and
    skb_copy_to_page_nocache can be called by sendmsg functions to use
    this feature, initial support is in tcp_sendmsg.  This functionality is
    configurable per device using ethtool.
    
    Presumably, this feature would only be useful when the driver does
    not touch the data.  The feature is turned on by default if a device
    indicates that it does some form of checksum offload; it is off by
    default for devices that do no checksum offload or indicate no checksum
    is necessary.  For the former case copy-checksum is probably done
    anyway, in the latter case the device is likely loopback in which case
    the no cache copy is probably not beneficial.
    
    This patch was tested using 200 instances of netperf TCP_RR with
    1400 byte request and one byte reply.  Platform is 16 core AMD x86.
    
    No-cache copy disabled:
       672703 tps, 97.13% utilization
       50/90/99% latency:244.31 484.205 1028.41
    
    No-cache copy enabled:
       702113 tps, 96.16% utilization,
       50/90/99% latency 238.56 467.56 956.955
    
    Using 14000 byte request and response sizes demonstrate the
    effects more dramatically:
    
    No-cache copy disabled:
       79571 tps, 34.34 %utlization
       50/90/95% latency 1584.46 2319.59 5001.76
    
    No-cache copy enabled:
       83856 tps, 34.81% utilization
       50/90/95% latency 2508.42 2622.62 2735.88
    
    Note especially the effect on latency tail (95th percentile).
    
    This seems to provide a nice performance improvement and is
    consistent in the tests I ran.  Presumably, this would provide
    the greatest benfits in the presence of an application workload
    stressing the cache and a lot of transmit data happening.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index da0534d3401c..43bd515e92fd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -52,6 +52,7 @@
 #include <linux/mm.h>
 #include <linux/security.h>
 #include <linux/slab.h>
+#include <linux/uaccess.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
@@ -1389,6 +1390,58 @@ static inline void sk_nocaps_add(struct sock *sk, int flags)
 	sk->sk_route_caps &= ~flags;
 }
 
+static inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,
+					   char __user *from, char *to,
+					   int copy)
+{
+	if (skb->ip_summed == CHECKSUM_NONE) {
+		int err = 0;
+		__wsum csum = csum_and_copy_from_user(from, to, copy, 0, &err);
+		if (err)
+			return err;
+		skb->csum = csum_block_add(skb->csum, csum, skb->len);
+	} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {
+		if (!access_ok(VERIFY_READ, from, copy) ||
+		    __copy_from_user_nocache(to, from, copy))
+			return -EFAULT;
+	} else if (copy_from_user(to, from, copy))
+		return -EFAULT;
+
+	return 0;
+}
+
+static inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,
+				       char __user *from, int copy)
+{
+	int err;
+
+	err = skb_do_copy_data_nocache(sk, skb, from, skb_put(skb, copy), copy);
+	if (err)
+		__skb_trim(skb, skb->len);
+
+	return err;
+}
+
+static inline int skb_copy_to_page_nocache(struct sock *sk, char __user *from,
+					   struct sk_buff *skb,
+					   struct page *page,
+					   int off, int copy)
+{
+	int err;
+
+	err = skb_do_copy_data_nocache(sk, skb, from,
+				       page_address(page) + off, copy);
+	if (err)
+		return err;
+
+	skb->len	     += copy;
+	skb->data_len	     += copy;
+	skb->truesize	     += copy;
+	sk->sk_wmem_queued   += copy;
+	sk_mem_charge(sk, copy);
+	return 0;
+}
+
 static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 				   struct sk_buff *skb, struct page *page,
 				   int off, int copy)

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/include/net/sock.h b/include/net/sock.h
index da0534d3401c..01810a3f19df 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1749,7 +1749,7 @@ void sock_net_set(struct sock *sk, struct net *net)
 
 /*
  * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
- * They should not hold a referrence to a namespace in order to allow
+ * They should not hold a reference to a namespace in order to allow
  * to stop it.
  * Sockets after sk_change_net should be released using sk_release_kernel
  */

commit eaefd1105bc431ef329599e307a07f2a36ae7872
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Feb 18 03:26:36 2011 +0000

    net: add __rcu annotations to sk_wq and wq
    
    Add proper RCU annotations/verbs to sk_wq and wq members
    
    Fix __sctp_write_space() sk_sleep() abuse (and sock->wq access)
    
    Fix sunrpc sk_sleep() abuse too
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e3893a2b5d25..da0534d3401c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -281,7 +281,7 @@ struct sock {
 	int			sk_rcvbuf;
 
 	struct sk_filter __rcu	*sk_filter;
-	struct socket_wq	*sk_wq;
+	struct socket_wq __rcu	*sk_wq;
 
 #ifdef CONFIG_NET_DMA
 	struct sk_buff_head	sk_async_wait_queue;
@@ -1266,7 +1266,8 @@ static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 
 static inline wait_queue_head_t *sk_sleep(struct sock *sk)
 {
-	return &sk->sk_wq->wait;
+	BUILD_BUG_ON(offsetof(struct socket_wq, wait) != 0);
+	return &rcu_dereference_raw(sk->sk_wq)->wait;
 }
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
@@ -1287,7 +1288,7 @@ static inline void sock_orphan(struct sock *sk)
 static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
 	write_lock_bh(&sk->sk_callback_lock);
-	rcu_assign_pointer(sk->sk_wq, parent->wq);
+	sk->sk_wq = parent->wq;
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
 	security_sock_graft(sk, parent);

commit 5403c8a29521a6eb02f9283dbbe0184527f8f42b
Merge: c79b9e493614 c4c93106741b
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jan 31 13:13:24 2011 -0800

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6

commit 709b46e8d90badda1898caea50483c12af178e96
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jan 29 16:15:56 2011 +0000

    net: Add compat ioctl support for the ipv4 multicast ioctl SIOCGETSGCNT
    
    SIOCGETSGCNT is not a unique ioctl value as it it maps tio SIOCPROTOPRIVATE +1,
    which unfortunately means the existing infrastructure for compat networking
    ioctls is insufficient.  A trivial compact ioctl implementation would conflict
    with:
    
    SIOCAX25ADDUID
    SIOCAIPXPRISLT
    SIOCGETSGCNT_IN6
    SIOCGETSGCNT
    SIOCRSSCAUSE
    SIOCX25SSUBSCRIP
    SIOCX25SDTEFACILITIES
    
    To make this work I have updated the compat_ioctl decode path to mirror the
    the normal ioctl decode path.  I have added an ipv4 inet_compat_ioctl function
    so that I can have ipv4 specific compat ioctls.   I have added a compat_ioctl
    function into struct proto so I can break out ioctls by which kind of ip socket
    I am using.  I have added a compat_raw_ioctl function because SIOCGETSGCNT only
    works on raw sockets.  I have added a ipmr_compat_ioctl that mirrors the normal
    ipmr_ioctl.
    
    This was necessary because unfortunately the struct layout for the SIOCGETSGCNT
    has unsigned longs in it so changes between 32bit and 64bit kernels.
    
    This change was sufficient to run a 32bit ip multicast routing daemon on a
    64bit kernel.
    
    Reported-by: Bill Fenner <fenner@aristanetworks.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d884d268c704..bc1cf7d88ccb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -753,6 +753,8 @@ struct proto {
 					int level,
 					int optname, char __user *optval,
 					int __user *option);
+	int			(*compat_ioctl)(struct sock *sk,
+					unsigned int cmd, unsigned long arg);
 #endif
 	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
 					   struct msghdr *msg, size_t len);

commit 80f8f1027b99660897bdeaeae73002185d829906
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jan 18 07:46:52 2011 +0000

    net: filter: dont block softirqs in sk_run_filter()
    
    Packet filter (BPF) doesnt need to disable softirqs, being fully
    re-entrant and lock-less.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d884d268c704..ba6465bf7c7a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1189,7 +1189,7 @@ extern void sk_filter_release_rcu(struct rcu_head *rcu);
 static inline void sk_filter_release(struct sk_filter *fp)
 {
 	if (atomic_dec_and_test(&fp->refcnt))
-		call_rcu_bh(&fp->rcu, sk_filter_release_rcu);
+		call_rcu(&fp->rcu, sk_filter_release_rcu);
 }
 
 static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)

commit 928c41e7a15d1164bb725f3445575f4651b5b9f0
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Sat Jan 8 17:39:21 2011 +0000

    net/sock.h: make some fields private to fix kernel-doc warning(s)
    
    Fix new kernel-doc notation warning in sock.h by annotating skc_dontcopy_*
    as private fields.
    
    Warning(include/net/sock.h:163): No description found for parameter 'skc_dontcopy_end[0]'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 21a02f7e4f45..d884d268c704 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -152,14 +152,18 @@ struct sock_common {
 	 * fields between dontcopy_begin/dontcopy_end
 	 * are not copied in sock_copy()
 	 */
+	/* private: */
 	int			skc_dontcopy_begin[0];
+	/* public: */
 	union {
 		struct hlist_node	skc_node;
 		struct hlist_nulls_node skc_nulls_node;
 	};
 	int			skc_tx_queue_mapping;
 	atomic_t		skc_refcnt;
+	/* private: */
 	int                     skc_dontcopy_end[0];
+	/* public: */
 };
 
 /**

commit b4aa9e05a61b845541fa6f5b1d246976922601f0
Merge: 1dc0f3c54ce1 4b8fe66300ac
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Dec 17 12:27:22 2010 -0800

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/net/bnx2x/bnx2x.h
            drivers/net/wireless/iwlwifi/iwl-1000.c
            drivers/net/wireless/iwlwifi/iwl-6000.c
            drivers/net/wireless/iwlwifi/iwl-core.h
            drivers/vhost/vhost.c

commit fcbdf09d9652c8919dcf47072e3ae7dcb4eb98ac
Author: Octavian Purdila <opurdila@ixiacom.com>
Date:   Thu Dec 16 14:26:56 2010 -0800

    net: fix nulls list corruptions in sk_prot_alloc
    
    Special care is taken inside sk_port_alloc to avoid overwriting
    skc_node/skc_nulls_node. We should also avoid overwriting
    skc_bind_node/skc_portaddr_node.
    
    The patch fixes the following crash:
    
     BUG: unable to handle kernel paging request at fffffffffffffff0
     IP: [<ffffffff812ec6dd>] udp4_lib_lookup2+0xad/0x370
     [<ffffffff812ecc22>] __udp4_lib_lookup+0x282/0x360
     [<ffffffff812ed63e>] __udp4_lib_rcv+0x31e/0x700
     [<ffffffff812bba45>] ? ip_local_deliver_finish+0x65/0x190
     [<ffffffff812bbbf8>] ? ip_local_deliver+0x88/0xa0
     [<ffffffff812eda35>] udp_rcv+0x15/0x20
     [<ffffffff812bba45>] ip_local_deliver_finish+0x65/0x190
     [<ffffffff812bbbf8>] ip_local_deliver+0x88/0xa0
     [<ffffffff812bb2cd>] ip_rcv_finish+0x32d/0x6f0
     [<ffffffff8128c14c>] ? netif_receive_skb+0x99c/0x11c0
     [<ffffffff812bb94b>] ip_rcv+0x2bb/0x350
     [<ffffffff8128c14c>] netif_receive_skb+0x99c/0x11c0
    
    Signed-off-by: Leonard Crestez <lcrestez@ixiacom.com>
    Signed-off-by: Octavian Purdila <opurdila@ixiacom.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 659d968d95c5..7d3f7ce239b5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -754,6 +754,7 @@ struct proto {
 	void			(*unhash)(struct sock *sk);
 	void			(*rehash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
+	void			(*clear_sk)(struct sock *sk, int size);
 
 	/* Keeping track of sockets in use */
 #ifdef CONFIG_PROC_FS
@@ -852,6 +853,8 @@ static inline void __sk_prot_rehash(struct sock *sk)
 	sk->sk_prot->hash(sk);
 }
 
+void sk_prot_clear_portaddr_nulls(struct sock *sk, int size);
+
 /* About 10 seconds */
 #define SOCK_DESTROY_TIME (10*HZ)
 

commit 68835aba4d9b74e2f94106d13b6a4bddc447c4c8
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 30 19:04:07 2010 +0000

    net: optimize INET input path further
    
    Followup of commit b178bb3dfc30 (net: reorder struct sock fields)
    
    Optimize INET input path a bit further, by :
    
    1) moving sk_refcnt close to sk_lock.
    
    This reduces number of dirtied cache lines by one on 64bit arches (and
    64 bytes cache line size).
    
    2) moving inet_daddr & inet_rcv_saddr at the beginning of sk
    
    (same cache line than hash / family / bound_dev_if / nulls_node)
    
    This reduces number of accessed cache lines in lookups by one, and dont
    increase size of inet and timewait socks.
    inet and tw sockets now share same place-holder for these fields.
    
    Before patch :
    
    offsetof(struct sock, sk_refcnt) = 0x10
    offsetof(struct sock, sk_lock) = 0x40
    offsetof(struct sock, sk_receive_queue) = 0x60
    offsetof(struct inet_sock, inet_daddr) = 0x270
    offsetof(struct inet_sock, inet_rcv_saddr) = 0x274
    
    After patch :
    
    offsetof(struct sock, sk_refcnt) = 0x44
    offsetof(struct sock, sk_lock) = 0x48
    offsetof(struct sock, sk_receive_queue) = 0x68
    offsetof(struct inet_sock, inet_daddr) = 0x0
    offsetof(struct inet_sock, inet_rcv_saddr) = 0x4
    
    compute_score() (udp or tcp) now use a single cache line per ignored
    item, instead of two.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3482004e5c29..82e86034702f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -105,10 +105,8 @@ struct net;
 
 /**
  *	struct sock_common - minimal network layer representation of sockets
- *	@skc_node: main hash linkage for various protocol lookup tables
- *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
- *	@skc_refcnt: reference count
- *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_daddr: Foreign IPv4 addr
+ *	@skc_rcv_saddr: Bound local IPv4 addr
  *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
  *	@skc_family: network address family
@@ -119,20 +117,20 @@ struct net;
  *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
  *	@skc_prot: protocol handlers inside a network family
  *	@skc_net: reference to the network namespace of this socket
+ *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
+ *	@skc_tx_queue_mapping: tx queue number for this connection
+ *	@skc_refcnt: reference count
  *
  *	This is the minimal network layer representation of sockets, the header
  *	for struct sock and struct inet_timewait_sock.
  */
 struct sock_common {
-	/*
-	 * first fields are not copied in sock_copy()
+	/* skc_daddr and skc_rcv_saddr must be grouped :
+	 * cf INET_MATCH() and INET_TW_MATCH()
 	 */
-	union {
-		struct hlist_node	skc_node;
-		struct hlist_nulls_node skc_nulls_node;
-	};
-	atomic_t		skc_refcnt;
-	int			skc_tx_queue_mapping;
+	__be32			skc_daddr;
+	__be32			skc_rcv_saddr;
 
 	union  {
 		unsigned int	skc_hash;
@@ -150,6 +148,18 @@ struct sock_common {
 #ifdef CONFIG_NET_NS
 	struct net	 	*skc_net;
 #endif
+	/*
+	 * fields between dontcopy_begin/dontcopy_end
+	 * are not copied in sock_copy()
+	 */
+	int			skc_dontcopy_begin[0];
+	union {
+		struct hlist_node	skc_node;
+		struct hlist_nulls_node skc_nulls_node;
+	};
+	int			skc_tx_queue_mapping;
+	atomic_t		skc_refcnt;
+	int                     skc_dontcopy_end[0];
 };
 
 /**
@@ -232,7 +242,8 @@ struct sock {
 #define sk_refcnt		__sk_common.skc_refcnt
 #define sk_tx_queue_mapping	__sk_common.skc_tx_queue_mapping
 
-#define sk_copy_start		__sk_common.skc_hash
+#define sk_dontcopy_begin	__sk_common.skc_dontcopy_begin
+#define sk_dontcopy_end		__sk_common.skc_dontcopy_end
 #define sk_hash			__sk_common.skc_hash
 #define sk_family		__sk_common.skc_family
 #define sk_state		__sk_common.skc_state

commit fe6c791570efe717946ea7b7dd50aec96b70d551
Merge: f8bf5681cf15 f19872575ff7
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Dec 8 13:15:38 2010 -0800

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/net/wireless/ath/ath9k/ar9003_eeprom.c
            net/llc/af_llc.c

commit 46bcf14f44d8f31ecfdc8b6708ec15a3b33316d9
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Dec 6 09:29:43 2010 -0800

    filter: fix sk_filter rcu handling
    
    Pavel Emelyanov tried to fix a race between sk_filter_(de|at)tach and
    sk_clone() in commit 47e958eac280c263397
    
    Problem is we can have several clones sharing a common sk_filter, and
    these clones might want to sk_filter_attach() their own filters at the
    same time, and can overwrite old_filter->rcu, corrupting RCU queues.
    
    We can not use filter->rcu without being sure no other thread could do
    the same thing.
    
    Switch code to a more conventional ref-counting technique : Do the
    atomic decrement immediately and queue one rcu call back when last
    reference is released.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6338d039857..659d968d95c5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1155,6 +1155,8 @@ extern void sk_common_release(struct sock *sk);
 /* Initialise core socket variables */
 extern void sock_init_data(struct socket *sock, struct sock *sk);
 
+extern void sk_filter_release_rcu(struct rcu_head *rcu);
+
 /**
  *	sk_filter_release - release a socket filter
  *	@fp: filter to remove
@@ -1165,7 +1167,7 @@ extern void sock_init_data(struct socket *sock, struct sock *sk);
 static inline void sk_filter_release(struct sk_filter *fp)
 {
 	if (atomic_dec_and_test(&fp->refcnt))
-		kfree(fp);
+		call_rcu_bh(&fp->rcu, sk_filter_release_rcu);
 }
 
 static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)

commit dca9b2404a6d6579828da2425c051462701efd3f
Author: Shan Wei <shanwei@cn.fujitsu.com>
Date:   Wed Dec 1 18:05:17 2010 +0000

    net: kill unused macros from head file
    
    These macros have been defined for several years since v2.6.12-rc2（tracing by git）,
    but never be used. So remove them.
    
    Signed-off-by: Shan Wei <shanwei@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5557dfb3dd68..717cfbf649df 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -516,9 +516,6 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 #define sk_nulls_for_each_from(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \
 		hlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)
-#define sk_for_each_continue(__sk, node) \
-	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
-		hlist_for_each_entry_continue(__sk, node, sk_node)
 #define sk_for_each_safe(__sk, node, tmp, list) \
 	hlist_for_each_entry_safe(__sk, node, tmp, list, sk_node)
 #define sk_for_each_bound(__sk, node, list) \

commit b178bb3dfc30d9555bdd2401e95af98e23e83e10
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 16 05:56:04 2010 +0000

    net: reorder struct sock fields
    
    Right now, fields in struct sock are not optimally ordered, because each
    path (RX softirq, TX completion, RX user,  TX user) has to touch fields
    that are contained in many different cache lines.
    
    The really critical thing is to shrink number of cache lines that are
    used at RX softirq time : CPU handling softirqs for a device can receive
    many frames per second for many sockets. If load is too big, we can drop
    frames at NIC level. RPS or multiqueue cards can help, but better reduce
    latency if possible.
    
    This patch starts with UDP protocol, then additional patches will try to
    reduce latencies of other ones as well.
    
    At RX softirq time, fields of interest for UDP protocol are :
    (not counting ones in inet struct for the lookup)
    
    Read/Written:
    sk_refcnt   (atomic increment/decrement)
    sk_rmem_alloc & sk_backlog.len (to check if there is room in queues)
    sk_receive_queue
    sk_backlog (if socket locked by user program)
    sk_rxhash
    sk_forward_alloc
    sk_drops
    
    Read only:
    sk_rcvbuf (sk_rcvqueues_full())
    sk_filter
    sk_wq
    sk_policy[0]
    sk_flags
    
    Additional notes :
    
    - sk_backlog has one hole on 64bit arches. We can fill it to save 8
    bytes.
    - sk_backlog is used only if RX sofirq handler finds the socket while
    locked by user.
    - sk_rxhash is written only once per flow.
    - sk_drops is written only if queues are full
    
    Final layout :
    
    [1] One section grouping all read/write fields, but placing rxhash and
    sk_backlog at the end of this section.
    
    [2] One section grouping all read fields in RX handler
       (sk_filter, sk_rcv_buf, sk_wq)
    
    [3] Section used by other paths
    
    I'll post a patch on its own to put sk_refcnt at the end of struct
    sock_common so that it shares same cache line than section [1]
    
    New offsets on 64bit arch :
    
    sizeof(struct sock)=0x268
    offsetof(struct sock, sk_refcnt)  =0x10
    offsetof(struct sock, sk_lock)    =0x48
    offsetof(struct sock, sk_receive_queue)=0x68
    offsetof(struct sock, sk_backlog)=0x80
    offsetof(struct sock, sk_rmem_alloc)=0x80
    offsetof(struct sock, sk_forward_alloc)=0x98
    offsetof(struct sock, sk_rxhash)=0x9c
    offsetof(struct sock, sk_rcvbuf)=0xa4
    offsetof(struct sock, sk_drops) =0xa0
    offsetof(struct sock, sk_filter)=0xa8
    offsetof(struct sock, sk_wq)=0xb0
    offsetof(struct sock, sk_policy)=0xd0
    offsetof(struct sock, sk_flags) =0xe0
    
    Instead of :
    
    sizeof(struct sock)=0x270
    offsetof(struct sock, sk_refcnt)  =0x10
    offsetof(struct sock, sk_lock)    =0x50
    offsetof(struct sock, sk_receive_queue)=0xc0
    offsetof(struct sock, sk_backlog)=0x70
    offsetof(struct sock, sk_rmem_alloc)=0xac
    offsetof(struct sock, sk_forward_alloc)=0x10c
    offsetof(struct sock, sk_rxhash)=0x128
    offsetof(struct sock, sk_rcvbuf)=0x4c
    offsetof(struct sock, sk_drops) =0x16c
    offsetof(struct sock, sk_filter)=0x198
    offsetof(struct sock, sk_wq)=0x88
    offsetof(struct sock, sk_policy)=0x98
    offsetof(struct sock, sk_flags) =0x130
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index eb0c1f504678..5557dfb3dd68 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -241,59 +241,67 @@ struct sock {
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_prot			__sk_common.skc_prot
 #define sk_net			__sk_common.skc_net
-	kmemcheck_bitfield_begin(flags);
-	unsigned int		sk_shutdown  : 2,
-				sk_no_check  : 2,
-				sk_userlocks : 4,
-				sk_protocol  : 8,
-				sk_type      : 16;
-	kmemcheck_bitfield_end(flags);
-	int			sk_rcvbuf;
 	socket_lock_t		sk_lock;
+	struct sk_buff_head	sk_receive_queue;
 	/*
 	 * The backlog queue is special, it is always used with
 	 * the per-socket spinlock held and requires low latency
 	 * access. Therefore we special case it's implementation.
+	 * Note : rmem_alloc is in this structure to fill a hole
+	 * on 64bit arches, not because its logically part of
+	 * backlog.
 	 */
 	struct {
-		struct sk_buff *head;
-		struct sk_buff *tail;
-		int len;
+		atomic_t	rmem_alloc;
+		int		len;
+		struct sk_buff	*head;
+		struct sk_buff	*tail;
 	} sk_backlog;
+#define sk_rmem_alloc sk_backlog.rmem_alloc
+	int			sk_forward_alloc;
+#ifdef CONFIG_RPS
+	__u32			sk_rxhash;
+#endif
+	atomic_t		sk_drops;
+	int			sk_rcvbuf;
+
+	struct sk_filter __rcu	*sk_filter;
 	struct socket_wq	*sk_wq;
-	struct dst_entry	*sk_dst_cache;
+
+#ifdef CONFIG_NET_DMA
+	struct sk_buff_head	sk_async_wait_queue;
+#endif
+
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
+	unsigned long 		sk_flags;
+	struct dst_entry	*sk_dst_cache;
 	spinlock_t		sk_dst_lock;
-	atomic_t		sk_rmem_alloc;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
 	int			sk_sndbuf;
-	struct sk_buff_head	sk_receive_queue;
 	struct sk_buff_head	sk_write_queue;
-#ifdef CONFIG_NET_DMA
-	struct sk_buff_head	sk_async_wait_queue;
-#endif
+	kmemcheck_bitfield_begin(flags);
+	unsigned int		sk_shutdown  : 2,
+				sk_no_check  : 2,
+				sk_userlocks : 4,
+				sk_protocol  : 8,
+				sk_type      : 16;
+	kmemcheck_bitfield_end(flags);
 	int			sk_wmem_queued;
-	int			sk_forward_alloc;
 	gfp_t			sk_allocation;
 	int			sk_route_caps;
 	int			sk_route_nocaps;
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
-#ifdef CONFIG_RPS
-	__u32			sk_rxhash;
-#endif
-	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
 	struct sk_buff_head	sk_error_queue;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
 				sk_err_soft;
-	atomic_t		sk_drops;
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
@@ -301,7 +309,6 @@ struct sock {
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
 	long			sk_sndtimeo;
-	struct sk_filter __rcu	*sk_filter;
 	void			*sk_protinfo;
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;

commit c31504dc0d1dc853dcee509d9999169a9097a717
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Nov 15 19:58:26 2010 +0000

    udp: use atomic_inc_not_zero_hint
    
    UDP sockets refcount is usually 2, unless an incoming frame is going to
    be queued in receive or backlog queue.
    
    Using atomic_inc_not_zero_hint() permits to reduce latency, because
    processor issues less memory transactions.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a6338d039857..eb0c1f504678 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -57,7 +57,7 @@
 #include <linux/rculist_nulls.h>
 #include <linux/poll.h>
 
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <net/dst.h>
 #include <net/checksum.h>
 

commit 8d987e5c75107ca7515fa19e857cfa24aab6ec8f
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 9 23:24:26 2010 +0000

    net: avoid limits overflow
    
    Robin Holt tried to boot a 16TB machine and found some limits were
    reached : sysctl_tcp_mem[2], sysctl_udp_mem[2]
    
    We can switch infrastructure to use long "instead" of "int", now
    atomic_long_t primitives are available for free.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Reported-by: Robin Holt <holt@sgi.com>
    Reviewed-by: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c7a736228ca2..a6338d039857 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -762,7 +762,7 @@ struct proto {
 
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
-	atomic_t		*memory_allocated;	/* Current allocated memory. */
+	atomic_long_t		*memory_allocated;	/* Current allocated memory. */
 	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
 	/*
 	 * Pressure flag: try to collapse.
@@ -771,7 +771,7 @@ struct proto {
 	 * is strict, actions are advisory and have some latency.
 	 */
 	int			*memory_pressure;
-	int			*sysctl_mem;
+	long			*sysctl_mem;
 	int			*sysctl_wmem;
 	int			*sysctl_rmem;
 	int			max_header;

commit 0d7da9ddd9a4eb7808698d04b98bf9d62d02649b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Oct 25 03:47:05 2010 +0000

    net: add __rcu annotation to sk_filter
    
    Add __rcu annotation to :
            (struct sock)->sk_filter
    
    And use appropriate rcu primitives to reduce sparse warnings if
    CONFIG_SPARSE_RCU_POINTER=y
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 73a4f9702a65..c7a736228ca2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -301,7 +301,7 @@ struct sock {
 	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
 	long			sk_sndtimeo;
-	struct sk_filter      	*sk_filter;
+	struct sk_filter __rcu	*sk_filter;
 	void			*sk_protinfo;
 	struct timer_list	sk_timer;
 	ktime_t			sk_stamp;

commit 7a91b434e2bad554b709265db7603b1aa52dd92e
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun Sep 26 18:53:07 2010 -0700

    net: update SOCK_MIN_RCVBUF
    
    SOCK_MIN_RCVBUF current value is 256 bytes
    
    It doesnt permit to receive the smallest possible frame, considering
    socket sk_rmem_alloc/sk_rcvbuf account skb truesizes. On 64bit arches,
    sizeof(struct sk_buff) is 240 bytes. Add the typical 64 bytes of
    headroom, and we go over the limit.
    
    With old kernels and 32bit arches, we were under the limit, if netdriver
    was doing copybreak.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8ae97c4970df..73a4f9702a65 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1558,7 +1558,11 @@ static inline void sk_wake_async(struct sock *sk, int how, int band)
 }
 
 #define SOCK_MIN_SNDBUF 2048
-#define SOCK_MIN_RCVBUF 256
+/*
+ * Since sk_rmem_alloc sums skb->truesize, even a small frame might need
+ * sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak
+ */
+#define SOCK_MIN_RCVBUF (2048 + sizeof(struct sk_buff))
 
 static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 {

commit e548833df83c3554229eff0672900bfe958b45fd
Merge: cbd9da7be869 053d8f662270
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Sep 9 22:27:33 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            net/mac80211/main.c

commit 719f835853a92f6090258114a72ffe41f09155cd
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Sep 8 05:08:44 2010 +0000

    udp: add rehash on connect()
    
    commit 30fff923 introduced in linux-2.6.33 (udp: bind() optimisation)
    added a secondary hash on UDP, hashed on (local addr, local port).
    
    Problem is that following sequence :
    
    fd = socket(...)
    connect(fd, &remote, ...)
    
    not only selects remote end point (address and port), but also sets
    local address, while UDP stack stored in secondary hash table the socket
    while its local address was INADDR_ANY (or ipv6 equivalent)
    
    Sequence is :
     - autobind() : choose a random local port, insert socket in hash tables
                  [while local address is INADDR_ANY]
     - connect() : set remote address and port, change local address to IP
                  given by a route lookup.
    
    When an incoming UDP frame comes, if more than 10 sockets are found in
    primary hash table, we switch to secondary table, and fail to find
    socket because its local address changed.
    
    One solution to this problem is to rehash datagram socket if needed.
    
    We add a new rehash(struct socket *) method in "struct proto", and
    implement this method for UDP v4 & v6, using a common helper.
    
    This rehashing only takes care of secondary hash table, since primary
    hash (based on local port only) is not changed.
    
    Reported-by: Krzysztof Piotr Oledzki <ole@ans.pl>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Tested-by: Krzysztof Piotr Oledzki <ole@ans.pl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ac53bfbdfe16..adab9dc58183 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -752,6 +752,7 @@ struct proto {
 	/* Keeping track of sk's, looking them up, and port selection methods. */
 	void			(*hash)(struct sock *sk);
 	void			(*unhash)(struct sock *sk);
+	void			(*rehash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
 
 	/* Keeping track of sockets in use */

commit 2244d07bfa2097cb00600da91c715a8aa547917e
Author: Oliver Hartkopp <socketcan@hartkopp.net>
Date:   Tue Aug 17 08:59:14 2010 +0000

    net: simplify flags for tx timestamping
    
    This patch removes the abstraction introduced by the union skb_shared_tx in
    the shared skb data.
    
    The access of the different union elements at several places led to some
    confusion about accessing the shared tx_flags e.g. in skb_orphan_try().
    
        http://marc.info/?l=linux-netdev&m=128084897415886&w=2
    
    Signed-off-by: Oliver Hartkopp <socketcan@hartkopp.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ac53bfbdfe16..100e43bf95fb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1669,17 +1669,13 @@ static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
 
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
- * @msg:	outgoing packet
  * @sk:		socket sending this packet
- * @shtx:	filled with instructions for time stamping
+ * @tx_flags:	filled with instructions for time stamping
  *
  * Currently only depends on SOCK_TIMESTAMPING* flags. Returns error code if
  * parameters are invalid.
  */
-extern int sock_tx_timestamp(struct msghdr *msg,
-			     struct sock *sk,
-			     union skb_shared_tx *shtx);
-
+extern int sock_tx_timestamp(struct sock *sk, __u8 *tx_flags);
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed

commit 53c3fa206415d8a3f8b2a4f77689ea044c4a9c65
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon Aug 9 13:41:07 2010 +0000

    net/sock.h: add missing kernel-doc notation
    
    Add missing kernel-doc notation to struct sock:
    
    Warning(include/net/sock.h:324): No description found for parameter 'sk_peer_pid'
    Warning(include/net/sock.h:324): No description found for parameter 'sk_peer_cred'
    Warning(include/net/sock.h:324): No description found for parameter 'sk_classid'
    Warning(include/net/sock.h:324): Excess struct/union/enum/typedef member 'sk_peercred' description in 'sock'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a441c9cdd625..ac53bfbdfe16 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -195,7 +195,8 @@ struct sock_common {
   *	@sk_priority: %SO_PRIORITY setting
   *	@sk_type: socket type (%SOCK_STREAM, etc)
   *	@sk_protocol: which protocol this socket belongs in this network family
-  *	@sk_peercred: %SO_PEERCRED setting
+  *	@sk_peer_pid: &struct pid for this socket's peer
+  *	@sk_peer_cred: %SO_PEERCRED setting
   *	@sk_rcvlowat: %SO_RCVLOWAT setting
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
@@ -211,6 +212,7 @@ struct sock_common {
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
   *	@sk_mark: generic packet mark
+  *	@sk_classid: this socket's cgroup classid
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
   *	@sk_data_ready: callback to indicate there is data to be processed

commit 11fe883936980fe242869d671092a466cf1db3e3
Merge: 70d4bf6d467a 573201f36fd9
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 20 18:25:24 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/vhost/net.c
            net/bridge/br_device.c
    
    Fix merge conflict in drivers/vhost/net.c with guidance from
    Stephen Rothwell.
    
    Revert the effects of net-2.6 commit 573201f36fd9c7c6d5218cdcd9948cee700b277d
    since net-next-2.6 has fixes that make bridge netpoll work properly thus
    we don't need it disabled.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b0f77d0eae0c58a5a9691a067ada112ceeae2d00
Author: Tom Herbert <therbert@google.com>
Date:   Wed Jul 14 20:50:29 2010 -0700

    net: fix problem in reading sock TX queue
    
    Fix problem in reading the tx_queue recorded in a socket.  In
    dev_pick_tx, the TX queue is read by doing a check with
    sk_tx_queue_recorded on the socket, followed by a sk_tx_queue_get.
    The problem is that there is not mutual exclusion across these
    calls in the socket so it it is possible that the queue in the
    sock can be invalidated after sk_tx_queue_recorded is called so
    that sk_tx_queue get returns -1, which sets 65535 in queue_index
    and thus dev_pick_tx returns 65536 which is a bogus queue and
    can cause crash in dev_queue_xmit.
    
    We fix this by only calling sk_tx_queue_get which does the proper
    checks.  The interface is that sk_tx_queue_get returns the TX queue
    if the sock argument is non-NULL and TX queue is recorded, else it
    returns -1.  sk_tx_queue_recorded is no longer used so it can be
    completely removed.
    
    Signed-off-by: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 731150d52799..0a691ea7654a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1224,12 +1224,7 @@ static inline void sk_tx_queue_clear(struct sock *sk)
 
 static inline int sk_tx_queue_get(const struct sock *sk)
 {
-	return sk->sk_tx_queue_mapping;
-}
-
-static inline bool sk_tx_queue_recorded(const struct sock *sk)
-{
-	return (sk && sk->sk_tx_queue_mapping >= 0);
+	return sk ? sk->sk_tx_queue_mapping : -1;
 }
 
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)

commit 7ba42910073f8432934d61a6c08b1023c408fb62
Author: Changli Gao <xiaosuo@gmail.com>
Date:   Sat Jul 10 20:41:55 2010 +0000

    inet, inet6: make tcp_sendmsg() and tcp_sendpage() through inet_sendmsg() and inet_sendpage()
    
    a new boolean flag no_autobind is added to structure proto to avoid the autobind
    calls when the protocol is TCP. Then sock_rps_record_flow() is called int the
    TCP's sendmsg() and sendpage() pathes.
    
    Signed-off-by: Changli Gao <xiaosuo@gmail.com>
    ----
     include/net/inet_common.h |    4 ++++
     include/net/sock.h        |    1 +
     include/net/tcp.h         |    8 ++++----
     net/ipv4/af_inet.c        |   15 +++++++++------
     net/ipv4/tcp.c            |   11 +++++------
     net/ipv4/tcp_ipv4.c       |    3 +++
     net/ipv6/af_inet6.c       |    8 ++++----
     net/ipv6/tcp_ipv6.c       |    3 +++
     8 files changed, 33 insertions(+), 20 deletions(-)
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4f26f2f83be9..3100e71f0c3d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -772,6 +772,7 @@ struct proto {
 	int			*sysctl_wmem;
 	int			*sysctl_rmem;
 	int			max_header;
+	bool			no_autobind;
 
 	struct kmem_cache	*slab;
 	unsigned int		obj_size;

commit 109f6e39fa07c48f580125f531f46cb7c245b528
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jun 13 03:30:14 2010 +0000

    af_unix: Allow SO_PEERCRED to work across namespaces.
    
    Use struct pid and struct cred to store the peer credentials on struct
    sock.  This gives enough information to convert the peer credential
    information to a value relative to whatever namespace the socket is in
    at the time.
    
    This removes nasty surprises when using SO_PEERCRED on socket
    connetions where the processes on either side are in different pid and
    user namespaces.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@free.fr>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f8acf38f092f..4f26f2f83be9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -295,7 +295,8 @@ struct sock {
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
-	struct ucred		sk_peercred;
+	struct pid		*sk_peer_pid;
+	const struct cred	*sk_peer_cred;
 	long			sk_rcvtimeo;
 	long			sk_sndtimeo;
 	struct sk_filter      	*sk_filter;

commit eedc765ca4b19a41cf0b921a492ac08d640060d1
Merge: e59d44df46ed 024a07bacf82
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Jun 6 17:42:02 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
            drivers/net/sfc/net_driver.h
            drivers/net/sfc/siena.c

commit c2d9ba9bce8d7323ca96f239e1f505c14d6244fb
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jun 1 06:51:19 2010 +0000

    net: CONFIG_NET_NS reduction
    
    Use read_pnet() and write_pnet() to reduce number of ifdef CONFIG_NET_NS
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ca241ea14875..3461e5d1e9ad 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1724,19 +1724,13 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
 static inline
 struct net *sock_net(const struct sock *sk)
 {
-#ifdef CONFIG_NET_NS
-	return sk->sk_net;
-#else
-	return &init_net;
-#endif
+	return read_pnet(&sk->sk_net);
 }
 
 static inline
 void sock_net_set(struct sock *sk, struct net *net)
 {
-#ifdef CONFIG_NET_NS
-	sk->sk_net = net;
-#endif
+	write_pnet(&sk->sk_net, net);
 }
 
 /*

commit b1faf5666438090a4dc4fceac8502edc7788b7e3
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon May 31 23:44:05 2010 -0700

    net: sock_queue_err_skb() dont mess with sk_forward_alloc
    
    Correct sk_forward_alloc handling for error_queue would need to use a
    backlog of frames that softirq handler could not deliver because socket
    is owned by user thread. Or extend backlog processing to be able to
    process normal and error packets.
    
    Another possibility is to not use mem charge for error queue, this is
    what I implemented in this patch.
    
    Note: this reverts commit 29030374
    (net: fix sk_forward_alloc corruptions), since we dont need to lock
    socket anymore.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ca241ea14875..731150d52799 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1524,20 +1524,7 @@ extern void sk_stop_timer(struct sock *sk, struct timer_list* timer);
 
 extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
-static inline int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)
-{
-	/* Cast skb->rcvbuf to unsigned... It's pointless, but reduces
-	   number of warnings when compiling with -W --ANK
-	 */
-	if (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=
-	    (unsigned)sk->sk_rcvbuf)
-		return -ENOMEM;
-	skb_set_owner_r(skb, sk);
-	skb_queue_tail(&sk->sk_error_queue, skb);
-	if (!sock_flag(sk, SOCK_DEAD))
-		sk->sk_data_ready(sk, skb->len);
-	return 0;
-}
+extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
 
 /*
  *	Recover an error report and clear atomically

commit 8a74ad60a546b13bd1096b2a61a7a5c6fd9ae17c
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed May 26 19:20:18 2010 +0000

    net: fix lock_sock_bh/unlock_sock_bh
    
    This new sock lock primitive was introduced to speedup some user context
    socket manipulation. But it is unsafe to protect two threads, one using
    regular lock_sock/release_sock, one using lock_sock_bh/unlock_sock_bh
    
    This patch changes lock_sock_bh to be careful against 'owned' state.
    If owned is found to be set, we must take the slow path.
    lock_sock_bh() now returns a boolean to say if the slow path was taken,
    and this boolean is used at unlock_sock_bh time to call the appropriate
    unlock function.
    
    After this change, BH are either disabled or enabled during the
    lock_sock_bh/unlock_sock_bh protected section. This might be misleading,
    so we rename these functions to lock_sock_fast()/unlock_sock_fast().
    
    Reported-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Tested-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d2a71b04a5ae..ca241ea14875 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1026,15 +1026,23 @@ extern void release_sock(struct sock *sk);
 				SINGLE_DEPTH_NESTING)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
-static inline void lock_sock_bh(struct sock *sk)
+extern bool lock_sock_fast(struct sock *sk);
+/**
+ * unlock_sock_fast - complement of lock_sock_fast
+ * @sk: socket
+ * @slow: slow mode
+ *
+ * fast unlock socket for user context.
+ * If slow mode is on, we call regular release_sock()
+ */
+static inline void unlock_sock_fast(struct sock *sk, bool slow)
 {
-	spin_lock_bh(&sk->sk_lock.slock);
+	if (slow)
+		release_sock(sk);
+	else
+		spin_unlock_bh(&sk->sk_lock.slock);
 }
 
-static inline void unlock_sock_bh(struct sock *sk)
-{
-	spin_unlock_bh(&sk->sk_lock.slock);
-}
 
 extern struct sock		*sk_alloc(struct net *net, int family,
 					  gfp_t priority,

commit acfbe96a3035639619a6533e04d88ed4ef9ccb61
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon May 24 23:54:18 2010 -0700

    sock.h: fix kernel-doc warning
    
    Fix sock.h kernel-doc warning:
    Warning(include/net/sock.h:1438): No description found for parameter 'wq'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d24f382cb712..d2a71b04a5ae 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1412,7 +1412,7 @@ static inline int sk_has_allocations(const struct sock *sk)
 
 /**
  * wq_has_sleeper - check if there are any waiting processes
- * @sk: struct socket_wq
+ * @wq: struct socket_wq
  *
  * Returns true if socket_wq has waiting processes
  *

commit f845172531fb7410c7fb7780b1a6e51ee6df7d52
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon May 24 00:12:34 2010 -0700

    cls_cgroup: Store classid in struct sock
    
    Up until now cls_cgroup has relied on fetching the classid out of
    the current executing thread.  This runs into trouble when a packet
    processing is delayed in which case it may execute out of another
    thread's context.
    
    Furthermore, even when a packet is not delayed we may fail to
    classify it if soft IRQs have been disabled, because this scenario
    is indistinguishable from one where a packet unrelated to the
    current thread is processed by a real soft IRQ.
    
    In fact, the current semantics is inherently broken, as a single
    skb may be constructed out of the writes of two different tasks.
    A different manifestation of this problem is when the TCP stack
    transmits in response of an incoming ACK.  This is currently
    unclassified.
    
    As we already have a concept of packet ownership for accounting
    purposes in the skb->sk pointer, this is a natural place to store
    the classid in a persistent manner.
    
    This patch adds the cls_cgroup classid in struct sock, filling up
    an existing hole on 64-bit :)
    
    The value is set at socket creation time.  So all sockets created
    via socket(2) automatically gains the ID of the thread creating it.
    Whenever another process touches the socket by either reading or
    writing to it, we will change the socket classid to that of the
    process if it has a valid (non-zero) classid.
    
    For sockets created on inbound connections through accept(2), we
    inherit the classid of the original listening socket through
    sk_clone, possibly preceding the actual accept(2) call.
    
    In order to minimise risks, I have not made this the authoritative
    classid.  For now it is only used as a backup when we execute
    with soft IRQs disabled.  Once we're completely happy with its
    semantics we can use it as the sole classid.
    
    Footnote: I have rearranged the error path on cls_group module
    creation.  If we didn't do this, then there is a window where
    someone could create a tc rule using cls_group before the cgroup
    subsystem has been registered.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5697caf8cc76..d24f382cb712 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -312,7 +312,7 @@ struct sock {
 	void			*sk_security;
 #endif
 	__u32			sk_mark;
-	/* XXX 4 bytes hole on 64 bit */
+	u32			sk_classid;
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
 	void			(*sk_write_space)(struct sock *sk);
@@ -1074,6 +1074,14 @@ extern void *sock_kmalloc(struct sock *sk, int size,
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);
 extern void sk_send_sigurg(struct sock *sk);
 
+#ifdef CONFIG_CGROUPS
+extern void sock_update_classid(struct sock *sk);
+#else
+static inline void sock_update_classid(struct sock *sk)
+{
+}
+#endif
+
 /*
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.

commit 7fee226ad2397b635e2fd565a59ca3ae08a164cd
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue May 11 23:19:48 2010 +0000

    net: add a noref bit on skb dst
    
    Use low order bit of skb->_skb_dst to tell dst is not refcounted.
    
    Change _skb_dst to _skb_refdst to make sure all uses are catched.
    
    skb_dst() returns the dst, regardless of noref bit set or not, but
    with a lockdep check to make sure a noref dst is not given if current
    user is not rcu protected.
    
    New skb_dst_set_noref() helper to set an notrefcounted dst on a skb.
    (with lockdep check)
    
    skb_dst_drop() drops a reference only if skb dst was refcounted.
    
    skb_dst_force() helper is used to force a refcount on dst, when skb
    is queued and not anymore RCU protected.
    
    Use skb_dst_force() in __sk_add_backlog(), __dev_xmit_skb() if
    !IFF_XMIT_DST_RELEASE or skb enqueued on qdisc queue, in
    sock_queue_rcv_skb(), in __nf_queue().
    
    Use skb_dst_force() in dev_requeue_skb().
    
    Note: dst_use_noref() still dirties dst, we might transform it
    later to do one dirtying per jiffies.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index aed16eb9db4b..5697caf8cc76 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -600,12 +600,15 @@ static inline int sk_stream_memory_free(struct sock *sk)
 /* OOB backlog add */
 static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
-	if (!sk->sk_backlog.tail) {
-		sk->sk_backlog.head = sk->sk_backlog.tail = skb;
-	} else {
+	/* dont let skb dst not refcounted, we are going to leave rcu lock */
+	skb_dst_force(skb);
+
+	if (!sk->sk_backlog.tail)
+		sk->sk_backlog.head = skb;
+	else
 		sk->sk_backlog.tail->next = skb;
-		sk->sk_backlog.tail = skb;
-	}
+
+	sk->sk_backlog.tail = skb;
 	skb->next = NULL;
 }
 

commit a465419b1febb603821f924805529cff89cafeed
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun May 16 00:36:33 2010 -0700

    net: Introduce sk_route_nocaps
    
    TCP-MD5 sessions have intermittent failures, when route cache is
    invalidated. ip_queue_xmit() has to find a new route, calls
    sk_setup_caps(sk, &rt->u.dst), destroying the
    
    sk->sk_route_caps &= ~NETIF_F_GSO_MASK
    
    that MD5 desperately try to make all over its way (from
    tcp_transmit_skb() for example)
    
    So we send few bad packets, and everything is fine when
    tcp_transmit_skb() is called again for this socket.
    
    Since ip_queue_xmit() is at a lower level than TCP-MD5, I chose to use a
    socket field, sk_route_nocaps, containing bits to mask on sk_route_caps.
    
    Reported-by: Bhaskar Dutta <bhaskie@gmail.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 328e03f47dd1..aed16eb9db4b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -177,6 +177,7 @@ struct sock_common {
   *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
   *	@sk_gso_max_size: Maximum GSO segment size to build
   *	@sk_lingertime: %SO_LINGER l_linger setting
@@ -276,6 +277,7 @@ struct sock {
 	int			sk_forward_alloc;
 	gfp_t			sk_allocation;
 	int			sk_route_caps;
+	int			sk_route_nocaps;
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
@@ -1335,6 +1337,12 @@ static inline int sk_can_gso(const struct sock *sk)
 
 extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
 
+static inline void sk_nocaps_add(struct sock *sk, int flags)
+{
+	sk->sk_route_nocaps |= flags;
+	sk->sk_route_caps &= ~flags;
+}
+
 static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 				   struct sk_buff *skb, struct page *page,
 				   int off, int copy)

commit 7ef527377b88ff05fb122a47619ea506c631c914
Merge: 47d29646a2c1 1183f3838c58
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 2 21:43:40 2010 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6

commit 1183f3838c588545592c042c0ce15015661ce7f2
Author: Jan Engelhardt <jengelh@medozas.de>
Date:   Sun May 2 13:42:39 2010 -0700

    net: fix compile error due to double return type in SOCK_DEBUG
    
    Fix this one:
    include/net/sock.h: error: two or more data types in declaration specifiers
    
    Signed-off-by: Jan Engelhardt <jengelh@medozas.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b4603cd54fcd..1ad6435f252e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -74,7 +74,7 @@
 					printk(KERN_DEBUG msg); } while (0)
 #else
 /* Validate arguments and do nothing */
-static void inline int __attribute__ ((format (printf, 2, 3)))
+static inline void __attribute__ ((format (printf, 2, 3)))
 SOCK_DEBUG(struct sock *sk, const char *msg, ...)
 {
 }

commit 43815482370c510c569fd18edb57afcb0fa8cab6
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Apr 29 11:01:49 2010 +0000

    net: sock_def_readable() and friends RCU conversion
    
    sk_callback_lock rwlock actually protects sk->sk_sleep pointer, so we
    need two atomic operations (and associated dirtying) per incoming
    packet.
    
    RCU conversion is pretty much needed :
    
    1) Add a new structure, called "struct socket_wq" to hold all fields
    that will need rcu_read_lock() protection (currently: a
    wait_queue_head_t and a struct fasync_struct pointer).
    
    [Future patch will add a list anchor for wakeup coalescing]
    
    2) Attach one of such structure to each "struct socket" created in
    sock_alloc_inode().
    
    3) Respect RCU grace period when freeing a "struct socket_wq"
    
    4) Change sk_sleep pointer in "struct sock" by sk_wq, pointer to "struct
    socket_wq"
    
    5) Change sk_sleep() function to use new sk->sk_wq instead of
    sk->sk_sleep
    
    6) Change sk_has_sleeper() to wq_has_sleeper() that must be used inside
    a rcu_read_lock() section.
    
    7) Change all sk_has_sleeper() callers to :
      - Use rcu_read_lock() instead of read_lock(&sk->sk_callback_lock)
      - Use wq_has_sleeper() to eventually wakeup tasks.
      - Use rcu_read_unlock() instead of read_unlock(&sk->sk_callback_lock)
    
    8) sock_wake_async() is modified to use rcu protection as well.
    
    9) Exceptions :
      macvtap, drivers/net/tun.c, af_unix use integrated "struct socket_wq"
    instead of dynamically allocated ones. They dont need rcu freeing.
    
    Some cleanups or followups are probably needed, (possible
    sk_callback_lock conversion to a spinlock for example...).
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e1777db5b9ab..cc7f91ec972c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -159,7 +159,7 @@ struct sock_common {
   *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
   *	@sk_lock:	synchronizer
   *	@sk_rcvbuf: size of receive buffer in bytes
-  *	@sk_sleep: sock wait queue
+  *	@sk_wq: sock wait queue and async head
   *	@sk_dst_cache: destination cache
   *	@sk_dst_lock: destination cache lock
   *	@sk_policy: flow policy
@@ -257,7 +257,7 @@ struct sock {
 		struct sk_buff *tail;
 		int len;
 	} sk_backlog;
-	wait_queue_head_t	*sk_sleep;
+	struct socket_wq	*sk_wq;
 	struct dst_entry	*sk_dst_cache;
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
@@ -1219,7 +1219,7 @@ static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 
 static inline wait_queue_head_t *sk_sleep(struct sock *sk)
 {
-	return sk->sk_sleep;
+	return &sk->sk_wq->wait;
 }
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
@@ -1233,14 +1233,14 @@ static inline void sock_orphan(struct sock *sk)
 	write_lock_bh(&sk->sk_callback_lock);
 	sock_set_flag(sk, SOCK_DEAD);
 	sk_set_socket(sk, NULL);
-	sk->sk_sleep  = NULL;
+	sk->sk_wq  = NULL;
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
 static inline void sock_graft(struct sock *sk, struct socket *parent)
 {
 	write_lock_bh(&sk->sk_callback_lock);
-	sk->sk_sleep = &parent->wait;
+	rcu_assign_pointer(sk->sk_wq, parent->wq);
 	parent->sk = sk;
 	sk_set_socket(sk, parent);
 	security_sock_graft(sk, parent);
@@ -1392,12 +1392,12 @@ static inline int sk_has_allocations(const struct sock *sk)
 }
 
 /**
- * sk_has_sleeper - check if there are any waiting processes
- * @sk: socket
+ * wq_has_sleeper - check if there are any waiting processes
+ * @sk: struct socket_wq
  *
- * Returns true if socket has waiting processes
+ * Returns true if socket_wq has waiting processes
  *
- * The purpose of the sk_has_sleeper and sock_poll_wait is to wrap the memory
+ * The purpose of the wq_has_sleeper and sock_poll_wait is to wrap the memory
  * barrier call. They were added due to the race found within the tcp code.
  *
  * Consider following tcp code paths:
@@ -1410,9 +1410,10 @@ static inline int sk_has_allocations(const struct sock *sk)
  *   ...                 ...
  *   tp->rcv_nxt check   sock_def_readable
  *   ...                 {
- *   schedule               ...
- *                          if (sk_sleep(sk) && waitqueue_active(sk_sleep(sk)))
- *                              wake_up_interruptible(sk_sleep(sk))
+ *   schedule               rcu_read_lock();
+ *                          wq = rcu_dereference(sk->sk_wq);
+ *                          if (wq && waitqueue_active(&wq->wait))
+ *                              wake_up_interruptible(&wq->wait)
  *                          ...
  *                       }
  *
@@ -1421,19 +1422,18 @@ static inline int sk_has_allocations(const struct sock *sk)
  * could then endup calling schedule and sleep forever if there are no more
  * data on the socket.
  *
- * The sk_has_sleeper is always called right after a call to read_lock, so we
- * can use smp_mb__after_lock barrier.
  */
-static inline int sk_has_sleeper(struct sock *sk)
+static inline bool wq_has_sleeper(struct socket_wq *wq)
 {
+
 	/*
 	 * We need to be sure we are in sync with the
 	 * add_wait_queue modifications to the wait queue.
 	 *
 	 * This memory barrier is paired in the sock_poll_wait.
 	 */
-	smp_mb__after_lock();
-	return sk_sleep(sk) && waitqueue_active(sk_sleep(sk));
+	smp_mb();
+	return wq && waitqueue_active(&wq->wait);
 }
 
 /**
@@ -1442,7 +1442,7 @@ static inline int sk_has_sleeper(struct sock *sk)
  * @wait_address:   socket wait queue
  * @p:              poll_table
  *
- * See the comments in the sk_has_sleeper function.
+ * See the comments in the wq_has_sleeper function.
  */
 static inline void sock_poll_wait(struct file *filp,
 		wait_queue_head_t *wait_address, poll_table *p)
@@ -1453,7 +1453,7 @@ static inline void sock_poll_wait(struct file *filp,
 		 * We need to be sure we are in sync with the
 		 * socket flags modification.
 		 *
-		 * This memory barrier is paired in the sk_has_sleeper.
+		 * This memory barrier is paired in the wq_has_sleeper.
 		*/
 		smp_mb();
 	}

commit 767dd03369ac18af58efdef0383d6eb986eab426
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Apr 28 19:14:43 2010 +0000

    net: speedup sock_recv_ts_and_drops()
    
    sock_recv_ts_and_drops() is fat and slow (~ 4% of cpu time on some
    profiles)
    
    We can test all socket flags at once to make fast path fast again.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d361c7769fe0..e1777db5b9ab 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1635,7 +1635,24 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 		sk->sk_stamp = kt;
 }
 
-extern void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk, struct sk_buff *skb);
+extern void __sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
+				     struct sk_buff *skb);
+
+static inline void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk,
+					  struct sk_buff *skb)
+{
+#define FLAGS_TS_OR_DROPS ((1UL << SOCK_RXQ_OVFL)			| \
+			   (1UL << SOCK_RCVTSTAMP)			| \
+			   (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE)	| \
+			   (1UL << SOCK_TIMESTAMPING_SOFTWARE)		| \
+			   (1UL << SOCK_TIMESTAMPING_RAW_HARDWARE) 	| \
+			   (1UL << SOCK_TIMESTAMPING_SYS_HARDWARE))
+
+	if (sk->sk_flags & FLAGS_TS_OR_DROPS)
+		__sock_recv_ts_and_drops(msg, sk, skb);
+	else
+		sk->sk_stamp = skb->tstamp;
+}
 
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped

commit 4b0b72f7dd617b13abd1b04c947e15873e011a24
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Apr 28 14:35:48 2010 -0700

    net: speedup udp receive path
    
    Since commit 95766fff ([UDP]: Add memory accounting.),
    each received packet needs one extra sock_lock()/sock_release() pair.
    
    This added latency because of possible backlog handling. Then later,
    ticket spinlocks added yet another latency source in case of DDOS.
    
    This patch introduces lock_sock_bh() and unlock_sock_bh()
    synchronization primitives, avoiding one atomic operation and backlog
    processing.
    
    skb_free_datagram_locked() uses them instead of full blown
    lock_sock()/release_sock(). skb is orphaned inside locked section for
    proper socket memory reclaim, and finally freed outside of it.
    
    UDP receive path now take the socket spinlock only once.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cf12b1e61fa6..d361c7769fe0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1021,6 +1021,16 @@ extern void release_sock(struct sock *sk);
 				SINGLE_DEPTH_NESTING)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
+static inline void lock_sock_bh(struct sock *sk)
+{
+	spin_lock_bh(&sk->sk_lock.slock);
+}
+
+static inline void unlock_sock_bh(struct sock *sk)
+{
+	spin_unlock_bh(&sk->sk_lock.slock);
+}
+
 extern struct sock		*sk_alloc(struct net *net, int family,
 					  gfp_t priority,
 					  struct proto *prot);

commit c377411f2494a931ff7facdbb3a6839b1266bcf6
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Apr 27 15:13:20 2010 -0700

    net: sk_add_backlog() take rmem_alloc into account
    
    Current socket backlog limit is not enough to really stop DDOS attacks,
    because user thread spend many time to process a full backlog each
    round, and user might crazy spin on socket lock.
    
    We should add backlog size and receive_queue size (aka rmem_alloc) to
    pace writers, and let user run without being slow down too much.
    
    Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in
    stress situations.
    
    Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp
    receiver can now process ~200.000 pps (instead of ~100 pps before the
    patch) on a 8 core machine.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 07822280d953..cf12b1e61fa6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -256,7 +256,6 @@ struct sock {
 		struct sk_buff *head;
 		struct sk_buff *tail;
 		int len;
-		int limit;
 	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
@@ -608,10 +607,20 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb->next = NULL;
 }
 
+/*
+ * Take into account size of receive queue and backlog queue
+ */
+static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb)
+{
+	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
+
+	return qsize + skb->truesize > sk->sk_rcvbuf;
+}
+
 /* The per-socket spinlock must be held here. */
 static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
-	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
+	if (sk_rcvqueues_full(sk, skb))
 		return -ENOBUFS;
 
 	__sk_add_backlog(sk, skb);

commit c58dc01babfd58ec9e71a6ce080150dc27755d88
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Apr 27 15:05:31 2010 -0700

    net: Make RFS socket operations not be inet specific.
    
    Idea from Eric Dumazet.
    
    As for placement inside of struct sock, I tried to choose a place
    that otherwise has a 32-bit hole on 64-bit systems.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4081db86a352..07822280d953 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -198,6 +198,7 @@ struct sock_common {
   *	@sk_rcvlowat: %SO_RCVLOWAT setting
   *	@sk_rcvtimeo: %SO_RCVTIMEO setting
   *	@sk_sndtimeo: %SO_SNDTIMEO setting
+  *	@sk_rxhash: flow hash received from netif layer
   *	@sk_filter: socket filtering instructions
   *	@sk_protinfo: private area, net family specific, when not using slab
   *	@sk_timer: sock cleanup timer
@@ -279,6 +280,9 @@ struct sock {
 	int			sk_gso_type;
 	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
+#ifdef CONFIG_RPS
+	__u32			sk_rxhash;
+#endif
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
 	struct sk_buff_head	sk_error_queue;
@@ -620,6 +624,40 @@ static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 	return sk->sk_backlog_rcv(sk, skb);
 }
 
+static inline void sock_rps_record_flow(const struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	struct rps_sock_flow_table *sock_flow_table;
+
+	rcu_read_lock();
+	sock_flow_table = rcu_dereference(rps_sock_flow_table);
+	rps_record_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rcu_read_unlock();
+#endif
+}
+
+static inline void sock_rps_reset_flow(const struct sock *sk)
+{
+#ifdef CONFIG_RPS
+	struct rps_sock_flow_table *sock_flow_table;
+
+	rcu_read_lock();
+	sock_flow_table = rcu_dereference(rps_sock_flow_table);
+	rps_reset_sock_flow(sock_flow_table, sk->sk_rxhash);
+	rcu_read_unlock();
+#endif
+}
+
+static inline void sock_rps_save_rxhash(struct sock *sk, u32 rxhash)
+{
+#ifdef CONFIG_RPS
+	if (unlikely(sk->sk_rxhash != rxhash)) {
+		sock_rps_reset_flow(sk);
+		sk->sk_rxhash = rxhash;
+	}
+#endif
+}
+
 #define sk_wait_event(__sk, __timeo, __condition)			\
 	({	int __rc;						\
 		release_sock(__sk);					\

commit 0b53ff2eadb1db6818894435f85989fb05d7e718
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Mon Apr 26 20:40:43 2010 +0000

    net: fix a lockdep rcu warning in __sk_dst_set()
    
    __sk_dst_set() might be called while no state can be integrated in a
    rcu_dereference_check() condition.
    
    So use rcu_dereference_raw() to shutup lockdep warnings (if
    CONFIG_PROVE_RCU is set)
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 86a8ca177a29..4081db86a352 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1236,8 +1236,11 @@ __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 	struct dst_entry *old_dst;
 
 	sk_tx_queue_clear(sk);
-	old_dst = rcu_dereference_check(sk->sk_dst_cache,
-					lockdep_is_held(&sk->sk_dst_lock));
+	/*
+	 * This can be called while sk is owned by the caller only,
+	 * with no state that can be checked in a rcu_dereference_check() cond
+	 */
+	old_dst = rcu_dereference_raw(sk->sk_dst_cache);
 	rcu_assign_pointer(sk->sk_dst_cache, dst);
 	dst_release(old_dst);
 }

commit f68c224fedff2157f3fad7f7da674cbc96567c84
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Apr 22 16:06:59 2010 -0700

    dst: rcu check refinement
    
    __sk_dst_get() might be called from softirq, with socket lock held.
    
    [  159.026180] include/net/sock.h:1200 invoked rcu_dereference_check()
    without protection!
    [  159.026261]
    [  159.026261] other info that might help us debug this:
    [  159.026263]
    [  159.026425]
    [  159.026426] rcu_scheduler_active = 1, debug_locks = 0
    [  159.026552] 2 locks held by swapper/0:
    [  159.026609]  #0:  (&icsk->icsk_retransmit_timer){+.-...}, at:
    [<ffffffff8104fc15>] run_timer_softirq+0x105/0x350
    [  159.026839]  #1:  (slock-AF_INET){+.-...}, at: [<ffffffff81392b8f>]
    tcp_write_timer+0x2f/0x1e0
    [  159.027063]
    [  159.027064] stack backtrace:
    [  159.027172] Pid: 0, comm: swapper Not tainted
    2.6.34-rc5-03707-gde498c8-dirty #36
    [  159.027252] Call Trace:
    [  159.027306]  <IRQ>  [<ffffffff810718ef>] lockdep_rcu_dereference
    +0xaf/0xc0
    [  159.027411]  [<ffffffff8138e4f7>] tcp_current_mss+0xa7/0xb0
    [  159.027537]  [<ffffffff8138fa49>] tcp_write_wakeup+0x89/0x190
    [  159.027600]  [<ffffffff81391936>] tcp_send_probe0+0x16/0x100
    [  159.027726]  [<ffffffff81392cd9>] tcp_write_timer+0x179/0x1e0
    [  159.027790]  [<ffffffff8104fca1>] run_timer_softirq+0x191/0x350
    [  159.027980]  [<ffffffff810477ed>] __do_softirq+0xcd/0x200
    
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8ab05146a447..86a8ca177a29 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1197,7 +1197,8 @@ static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
 	return rcu_dereference_check(sk->sk_dst_cache, rcu_read_lock_held() ||
-						       sock_owned_by_user(sk));
+						       sock_owned_by_user(sk) ||
+						       lockdep_is_held(&sk->sk_lock.slock));
 }
 
 static inline struct dst_entry *

commit aa395145165cb06a0d0885221bbe0ce4a564391d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Apr 20 13:03:51 2010 +0000

    net: sk_sleep() helper
    
    Define a new function to return the waitqueue of a "struct sock".
    
    static inline wait_queue_head_t *sk_sleep(struct sock *sk)
    {
            return sk->sk_sleep;
    }
    
    Change all read occurrences of sk_sleep by a call to this function.
    
    Needed for a future RCU conversion. sk_sleep wont be a field directly
    available.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 56df440a950b..8ab05146a447 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1160,6 +1160,10 @@ static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 	sk->sk_socket = sock;
 }
 
+static inline wait_queue_head_t *sk_sleep(struct sock *sk)
+{
+	return sk->sk_sleep;
+}
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
  * Note that parent inode held reference count on this struct sock,
@@ -1346,8 +1350,8 @@ static inline int sk_has_allocations(const struct sock *sk)
  *   tp->rcv_nxt check   sock_def_readable
  *   ...                 {
  *   schedule               ...
- *                          if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
- *                              wake_up_interruptible(sk->sk_sleep)
+ *                          if (sk_sleep(sk) && waitqueue_active(sk_sleep(sk)))
+ *                              wake_up_interruptible(sk_sleep(sk))
  *                          ...
  *                       }
  *
@@ -1368,7 +1372,7 @@ static inline int sk_has_sleeper(struct sock *sk)
 	 * This memory barrier is paired in the sock_poll_wait.
 	 */
 	smp_mb__after_lock();
-	return sk->sk_sleep && waitqueue_active(sk->sk_sleep);
+	return sk_sleep(sk) && waitqueue_active(sk_sleep(sk));
 }
 
 /**

commit b6c6712a42ca3f9fa7f4a3d7c40e3a9dd1fd9e03
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Apr 8 23:03:29 2010 +0000

    net: sk_dst_cache RCUification
    
    With latest CONFIG_PROVE_RCU stuff, I felt more comfortable to make this
    work.
    
    sk->sk_dst_cache is currently protected by a rwlock (sk_dst_lock)
    
    This rwlock is readlocked for a very small amount of time, and dst
    entries are already freed after RCU grace period. This calls for RCU
    again :)
    
    This patch converts sk_dst_lock to a spinlock, and use RCU for readers.
    
    __sk_dst_get() is supposed to be called with rcu_read_lock() or if
    socket locked by user, so use appropriate rcu_dereference_check()
    condition (rcu_read_lock_held() || sock_owned_by_user(sk))
    
    This patch avoids two atomic ops per tx packet on UDP connected sockets,
    for example, and permits sk_dst_lock to be much less dirtied.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b4603cd54fcd..56df440a950b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -262,7 +262,7 @@ struct sock {
 #ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
 #endif
-	rwlock_t		sk_dst_lock;
+	spinlock_t		sk_dst_lock;
 	atomic_t		sk_rmem_alloc;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
@@ -1192,7 +1192,8 @@ extern unsigned long sock_i_ino(struct sock *sk);
 static inline struct dst_entry *
 __sk_dst_get(struct sock *sk)
 {
-	return sk->sk_dst_cache;
+	return rcu_dereference_check(sk->sk_dst_cache, rcu_read_lock_held() ||
+						       sock_owned_by_user(sk));
 }
 
 static inline struct dst_entry *
@@ -1200,50 +1201,62 @@ sk_dst_get(struct sock *sk)
 {
 	struct dst_entry *dst;
 
-	read_lock(&sk->sk_dst_lock);
-	dst = sk->sk_dst_cache;
+	rcu_read_lock();
+	dst = rcu_dereference(sk->sk_dst_cache);
 	if (dst)
 		dst_hold(dst);
-	read_unlock(&sk->sk_dst_lock);
+	rcu_read_unlock();
 	return dst;
 }
 
+extern void sk_reset_txq(struct sock *sk);
+
+static inline void dst_negative_advice(struct sock *sk)
+{
+	struct dst_entry *ndst, *dst = __sk_dst_get(sk);
+
+	if (dst && dst->ops->negative_advice) {
+		ndst = dst->ops->negative_advice(dst);
+
+		if (ndst != dst) {
+			rcu_assign_pointer(sk->sk_dst_cache, ndst);
+			sk_reset_txq(sk);
+		}
+	}
+}
+
 static inline void
 __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 {
 	struct dst_entry *old_dst;
 
 	sk_tx_queue_clear(sk);
-	old_dst = sk->sk_dst_cache;
-	sk->sk_dst_cache = dst;
+	old_dst = rcu_dereference_check(sk->sk_dst_cache,
+					lockdep_is_held(&sk->sk_dst_lock));
+	rcu_assign_pointer(sk->sk_dst_cache, dst);
 	dst_release(old_dst);
 }
 
 static inline void
 sk_dst_set(struct sock *sk, struct dst_entry *dst)
 {
-	write_lock(&sk->sk_dst_lock);
+	spin_lock(&sk->sk_dst_lock);
 	__sk_dst_set(sk, dst);
-	write_unlock(&sk->sk_dst_lock);
+	spin_unlock(&sk->sk_dst_lock);
 }
 
 static inline void
 __sk_dst_reset(struct sock *sk)
 {
-	struct dst_entry *old_dst;
-
-	sk_tx_queue_clear(sk);
-	old_dst = sk->sk_dst_cache;
-	sk->sk_dst_cache = NULL;
-	dst_release(old_dst);
+	__sk_dst_set(sk, NULL);
 }
 
 static inline void
 sk_dst_reset(struct sock *sk)
 {
-	write_lock(&sk->sk_dst_lock);
+	spin_lock(&sk->sk_dst_lock);
 	__sk_dst_reset(sk);
-	write_unlock(&sk->sk_dst_lock);
+	spin_unlock(&sk->sk_dst_lock);
 }
 
 extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 092b0551e77f..b4603cd54fcd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -51,6 +51,7 @@
 #include <linux/skbuff.h>	/* struct sk_buff */
 #include <linux/mm.h>
 #include <linux/security.h>
+#include <linux/slab.h>
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>

commit 4045635318538d3ddd2007720412fdc4b08f6a62
Author: Zhu Yi <yi.zhu@intel.com>
Date:   Sun Mar 7 16:21:39 2010 +0000

    net: add __must_check to sk_add_backlog
    
    Add the "__must_check" tag to sk_add_backlog() so that any failure to
    check and drop packets will be warned about.
    
    Signed-off-by: Zhu Yi <yi.zhu@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 170353dd9570..092b0551e77f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -604,7 +604,7 @@ static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 }
 
 /* The per-socket spinlock must be held here. */
-static inline int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
 		return -ENOBUFS;

commit a3a858ff18a72a8d388e31ab0d98f7e944841a62
Author: Zhu Yi <yi.zhu@intel.com>
Date:   Thu Mar 4 18:01:47 2010 +0000

    net: backlog functions rename
    
    sk_add_backlog -> __sk_add_backlog
    sk_add_backlog_limited -> sk_add_backlog
    
    Signed-off-by: Zhu Yi <yi.zhu@intel.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2516d76f043c..170353dd9570 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -592,7 +592,7 @@ static inline int sk_stream_memory_free(struct sock *sk)
 }
 
 /* OOB backlog add */
-static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	if (!sk->sk_backlog.tail) {
 		sk->sk_backlog.head = sk->sk_backlog.tail = skb;
@@ -604,12 +604,12 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 }
 
 /* The per-socket spinlock must be held here. */
-static inline int sk_add_backlog_limited(struct sock *sk, struct sk_buff *skb)
+static inline int sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
 		return -ENOBUFS;
 
-	sk_add_backlog(sk, skb);
+	__sk_add_backlog(sk, skb);
 	sk->sk_backlog.len += skb->truesize;
 	return 0;
 }

commit 8eae939f1400326b06d0c9afe53d2a484a326871
Author: Zhu Yi <yi.zhu@intel.com>
Date:   Thu Mar 4 18:01:40 2010 +0000

    net: add limit for socket backlog
    
    We got system OOM while running some UDP netperf testing on the loopback
    device. The case is multiple senders sent stream UDP packets to a single
    receiver via loopback on local host. Of course, the receiver is not able
    to handle all the packets in time. But we surprisingly found that these
    packets were not discarded due to the receiver's sk->sk_rcvbuf limit.
    Instead, they are kept queuing to sk->sk_backlog and finally ate up all
    the memory. We believe this is a secure hole that a none privileged user
    can crash the system.
    
    The root cause for this problem is, when the receiver is doing
    __release_sock() (i.e. after userspace recv, kernel udp_recvmsg ->
    skb_free_datagram_locked -> release_sock), it moves skbs from backlog to
    sk_receive_queue with the softirq enabled. In the above case, multiple
    busy senders will almost make it an endless loop. The skbs in the
    backlog end up eat all the system memory.
    
    The issue is not only for UDP. Any protocols using socket backlog is
    potentially affected. The patch adds limit for socket backlog so that
    the backlog size cannot be expanded endlessly.
    
    Reported-by: Alex Shi <alex.shi@intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru
    Cc: "Pekka Savola (ipv6)" <pekkas@netcore.fi>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: Vlad Yasevich <vladislav.yasevich@hp.com>
    Cc: Sridhar Samudrala <sri@us.ibm.com>
    Cc: Jon Maloy <jon.maloy@ericsson.com>
    Cc: Allan Stephens <allan.stephens@windriver.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Signed-off-by: Zhu Yi <yi.zhu@intel.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6cb1676e409a..2516d76f043c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -253,6 +253,8 @@ struct sock {
 	struct {
 		struct sk_buff *head;
 		struct sk_buff *tail;
+		int len;
+		int limit;
 	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
@@ -589,7 +591,7 @@ static inline int sk_stream_memory_free(struct sock *sk)
 	return sk->sk_wmem_queued < sk->sk_sndbuf;
 }
 
-/* The per-socket spinlock must be held here. */
+/* OOB backlog add */
 static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
 	if (!sk->sk_backlog.tail) {
@@ -601,6 +603,17 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb->next = NULL;
 }
 
+/* The per-socket spinlock must be held here. */
+static inline int sk_add_backlog_limited(struct sock *sk, struct sk_buff *skb)
+{
+	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
+		return -ENOBUFS;
+
+	sk_add_backlog(sk, skb);
+	sk->sk_backlog.len += skb->truesize;
+	return 0;
+}
+
 static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
 {
 	return sk->sk_backlog_rcv(sk, skb);

commit 808f5114a9206fee855117d416440e1071ab375c
Author: stephen hemminger <shemminger@vyatta.com>
Date:   Mon Feb 22 07:57:18 2010 +0000

    packet: convert socket list to RCU (v3)
    
    Convert AF_PACKET to use RCU, eliminating one more reader/writer lock.
    
    There is no need for a real sk_del_node_init_rcu(), because sk_del_node_init
    is doing the equivalent thing to hlst_del_init_rcu already; but added
    some comments to try and make that obvious.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 580d51fa28e9..6cb1676e409a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -381,6 +381,7 @@ static __inline__ void __sk_del_node(struct sock *sk)
 	__hlist_del(&sk->sk_node);
 }
 
+/* NB: equivalent to hlist_del_init_rcu */
 static __inline__ int __sk_del_node_init(struct sock *sk)
 {
 	if (sk_hashed(sk)) {
@@ -421,6 +422,7 @@ static __inline__ int sk_del_node_init(struct sock *sk)
 	}
 	return rc;
 }
+#define sk_del_node_init_rcu(sk)	sk_del_node_init(sk)
 
 static __inline__ int __sk_nulls_del_node_init_rcu(struct sock *sk)
 {
@@ -454,6 +456,12 @@ static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
 	__sk_add_node(sk, list);
 }
 
+static __inline__ void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	hlist_add_head_rcu(&sk->sk_node, list);
+}
+
 static __inline__ void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
@@ -478,6 +486,8 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 
 #define sk_for_each(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_node)
+#define sk_for_each_rcu(__sk, node, list) \
+	hlist_for_each_entry_rcu(__sk, node, list, sk_node)
 #define sk_nulls_for_each(__sk, node, list) \
 	hlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)
 #define sk_nulls_for_each_rcu(__sk, node, list) \

commit 1a5778aa000ebfec7f07eed0ffa2852ffb5d16bb
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Sun Feb 14 22:35:47 2010 -0800

    net: Fix first line of kernel-doc for a few functions
    
    The function name must be followed by a space, hypen, space, and a
    short description.
    
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c8d400063c16..580d51fa28e9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1049,7 +1049,7 @@ extern void sk_common_release(struct sock *sk);
 extern void sock_init_data(struct socket *sock, struct sock *sk);
 
 /**
- *	sk_filter_release: Release a socket filter
+ *	sk_filter_release - release a socket filter
  *	@fp: filter to remove
  *
  *	Remove a filter from a socket and release its resources.

commit c4146644a56b1f213c4c5567c75771883bec33c7
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Feb 8 23:18:45 2010 +0000

    net: add a wrapper sk_entry()
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3f1a4804bb3f..c8d400063c16 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -317,6 +317,11 @@ struct sock {
 /*
  * Hashed lists helper routines
  */
+static inline struct sock *sk_entry(const struct hlist_node *node)
+{
+	return hlist_entry(node, struct sock, sk_node);
+}
+
 static inline struct sock *__sk_head(const struct hlist_head *head)
 {
 	return hlist_entry(head->first, struct sock, sk_node);

commit 512615b6b843ff3ff5ad583f34c39b3f302f5f26
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun Nov 8 10:17:58 2009 +0000

    udp: secondary hash on (local port, local address)
    
    Extends udp_table to contain a secondary hash table.
    
    socket anchor for this second hash is free, because UDP
    doesnt use skc_bind_node : We define an union to hold
    both skc_bind_node & a new hlist_nulls_node udp_portaddr_node
    
    udp_lib_get_port() inserts sockets into second hash chain
    (additional cost of one atomic op)
    
    udp_lib_unhash() deletes socket from second hash chain
    (additional cost of one atomic op)
    
    Note : No spinlock lockdep annotation is needed, because
    lock for the secondary hash chain is always get after
    lock for primary hash chain.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 827366b62680..3f1a4804bb3f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -105,7 +105,7 @@ struct net;
 /**
  *	struct sock_common - minimal network layer representation of sockets
  *	@skc_node: main hash linkage for various protocol lookup tables
- *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
+ *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
  *	@skc_refcnt: reference count
  *	@skc_tx_queue_mapping: tx queue number for this connection
  *	@skc_hash: hash value used with various protocol lookup tables
@@ -115,6 +115,7 @@ struct net;
  *	@skc_reuse: %SO_REUSEADDR setting
  *	@skc_bound_dev_if: bound device index if != 0
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
+ *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
  *	@skc_prot: protocol handlers inside a network family
  *	@skc_net: reference to the network namespace of this socket
  *
@@ -140,7 +141,10 @@ struct sock_common {
 	volatile unsigned char	skc_state;
 	unsigned char		skc_reuse;
 	int			skc_bound_dev_if;
-	struct hlist_node	skc_bind_node;
+	union {
+		struct hlist_node	skc_bind_node;
+		struct hlist_nulls_node skc_portaddr_node;
+	};
 	struct proto		*skc_prot;
 #ifdef CONFIG_NET_NS
 	struct net	 	*skc_net;

commit d4cada4ae1c012815f95fa507eb86a0ae9d607d7
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Sun Nov 8 10:17:30 2009 +0000

    udp: split sk_hash into two u16 hashes
    
    Union sk_hash with two u16 hashes for udp (no extra memory taken)
    
    One 16 bits hash on (local port) value (the previous udp 'hash')
    
    One 16 bits hash on (local address, local port) values, initialized
    but not yet used. This second hash is using jenkin hash for better
    distribution.
    
    Because the 'port' is xored later, a partial hash is performed
    on local address + net_hash_mix(net)
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 55de3bd719a5..827366b62680 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -109,6 +109,7 @@ struct net;
  *	@skc_refcnt: reference count
  *	@skc_tx_queue_mapping: tx queue number for this connection
  *	@skc_hash: hash value used with various protocol lookup tables
+ *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
  *	@skc_family: network address family
  *	@skc_state: Connection state
  *	@skc_reuse: %SO_REUSEADDR setting
@@ -131,7 +132,10 @@ struct sock_common {
 	atomic_t		skc_refcnt;
 	int			skc_tx_queue_mapping;
 
-	unsigned int		skc_hash;
+	union  {
+		unsigned int	skc_hash;
+		__u16		skc_u16hashes[2];
+	};
 	unsigned short		skc_family;
 	volatile unsigned char	skc_state;
 	unsigned char		skc_reuse;

commit e022f0b4a03f4fff9323b509df023b8af635716e
Author: Krishna Kumar <krkumar2@in.ibm.com>
Date:   Mon Oct 19 23:46:20 2009 +0000

    net: Introduce sk_tx_queue_mapping
    
    Introduce sk_tx_queue_mapping; and functions that set, test and
    get this value. Reset sk_tx_queue_mapping to -1 whenever the dst
    cache is set/reset, and in socket alloc. Setting txq to -1 and
    using valid txq=<0 to n-1> allows the tx path to use the value
    of sk_tx_queue_mapping directly instead of subtracting 1 on every
    tx.
    
    Signed-off-by: Krishna Kumar <krkumar2@in.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1364428f53f2..55de3bd719a5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -107,6 +107,7 @@ struct net;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
  *	@skc_refcnt: reference count
+ *	@skc_tx_queue_mapping: tx queue number for this connection
  *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_family: network address family
  *	@skc_state: Connection state
@@ -128,6 +129,7 @@ struct sock_common {
 		struct hlist_nulls_node skc_nulls_node;
 	};
 	atomic_t		skc_refcnt;
+	int			skc_tx_queue_mapping;
 
 	unsigned int		skc_hash;
 	unsigned short		skc_family;
@@ -215,6 +217,7 @@ struct sock {
 #define sk_node			__sk_common.skc_node
 #define sk_nulls_node		__sk_common.skc_nulls_node
 #define sk_refcnt		__sk_common.skc_refcnt
+#define sk_tx_queue_mapping	__sk_common.skc_tx_queue_mapping
 
 #define sk_copy_start		__sk_common.skc_hash
 #define sk_hash			__sk_common.skc_hash
@@ -1094,8 +1097,29 @@ static inline void sock_put(struct sock *sk)
 extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
 			  const int nested);
 
+static inline void sk_tx_queue_set(struct sock *sk, int tx_queue)
+{
+	sk->sk_tx_queue_mapping = tx_queue;
+}
+
+static inline void sk_tx_queue_clear(struct sock *sk)
+{
+	sk->sk_tx_queue_mapping = -1;
+}
+
+static inline int sk_tx_queue_get(const struct sock *sk)
+{
+	return sk->sk_tx_queue_mapping;
+}
+
+static inline bool sk_tx_queue_recorded(const struct sock *sk)
+{
+	return (sk && sk->sk_tx_queue_mapping >= 0);
+}
+
 static inline void sk_set_socket(struct sock *sk, struct socket *sock)
 {
+	sk_tx_queue_clear(sk);
 	sk->sk_socket = sock;
 }
 
@@ -1152,6 +1176,7 @@ __sk_dst_set(struct sock *sk, struct dst_entry *dst)
 {
 	struct dst_entry *old_dst;
 
+	sk_tx_queue_clear(sk);
 	old_dst = sk->sk_dst_cache;
 	sk->sk_dst_cache = dst;
 	dst_release(old_dst);
@@ -1170,6 +1195,7 @@ __sk_dst_reset(struct sock *sk)
 {
 	struct dst_entry *old_dst;
 
+	sk_tx_queue_clear(sk);
 	old_dst = sk->sk_dst_cache;
 	sk->sk_dst_cache = NULL;
 	dst_release(old_dst);

commit 421355de876b9f3fcc7e4cb6026e416fb12a5068
Merge: aace495933a9 0fe7463a35aa
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Oct 13 12:55:20 2009 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6

commit 3b885787ea4112eaa80945999ea0901bf742707f
Author: Neil Horman <nhorman@tuxdriver.com>
Date:   Mon Oct 12 13:26:31 2009 -0700

    net: Generalize socket rx gap / receive queue overflow cmsg
    
    Create a new socket level option to report number of queue overflows
    
    Recently I augmented the AF_PACKET protocol to report the number of frames lost
    on the socket receive queue between any two enqueued frames.  This value was
    exported via a SOL_PACKET level cmsg.  AFter I completed that work it was
    requested that this feature be generalized so that any datagram oriented socket
    could make use of this option.  As such I've created this patch, It creates a
    new SOL_SOCKET level option called SO_RXQ_OVFL, which when enabled exports a
    SOL_SOCKET level cmsg that reports the nubmer of times the sk_receive_queue
    overflowed between any two given frames.  It also augments the AF_PACKET
    protocol to take advantage of this new feature (as it previously did not touch
    sk->sk_drops, which this patch uses to record the overflow count).  Tested
    successfully by me.
    
    Notes:
    
    1) Unlike my previous patch, this patch simply records the sk_drops value, which
    is not a number of drops between packets, but rather a total number of drops.
    Deltas must be computed in user space.
    
    2) While this patch currently works with datagram oriented protocols, it will
    also be accepted by non-datagram oriented protocols. I'm not sure if thats
    agreeable to everyone, but my argument in favor of doing so is that, for those
    protocols which aren't applicable to this option, sk_drops will always be zero,
    and reporting no drops on a receive queue that isn't used for those
    non-participating protocols seems reasonable to me.  This also saves us having
    to code in a per-protocol opt in mechanism.
    
    3) This applies cleanly to net-next assuming that commit
    977750076d98c7ff6cbda51858bb5a5894a9d9ab (my af packet cmsg patch) is reverted
    
    Signed-off-by: Neil Horman <nhorman@tuxdriver.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 98398bdec57d..10669b01eeab 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -505,6 +505,7 @@ enum sock_flags {
 	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
 	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
 	SOCK_FASYNC, /* fasync() active */
+	SOCK_RXQ_OVFL,
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
@@ -1493,6 +1494,8 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 		sk->sk_stamp = kt;
 }
 
+extern void sock_recv_ts_and_drops(struct msghdr *msg, struct sock *sk, struct sk_buff *skb);
+
 /**
  * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
  * @msg:	outgoing packet

commit 5fdb9973c10c2d2e046da0976782ece25e78dc8b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Oct 8 22:50:25 2009 +0000

    net: Fix struct sock bitfield annotation
    
    Since commit a98b65a3 (net: annotate struct sock bitfield), we lost
    8 bytes in struct sock on 64bit arches because of
    kmemcheck_bitfield_end(flags) misplacement.
    
    Fix this by putting together sk_shutdown, sk_no_check, sk_userlocks,
    sk_protocol and sk_type in the 'flags' 32bits bitfield
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1621935aad5b..9f96394f694e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -226,12 +226,12 @@ struct sock {
 #define sk_prot			__sk_common.skc_prot
 #define sk_net			__sk_common.skc_net
 	kmemcheck_bitfield_begin(flags);
-	unsigned char		sk_shutdown : 2,
-				sk_no_check : 2,
-				sk_userlocks : 4;
+	unsigned int		sk_shutdown  : 2,
+				sk_no_check  : 2,
+				sk_userlocks : 4,
+				sk_protocol  : 8,
+				sk_type      : 16;
 	kmemcheck_bitfield_end(flags);
-	unsigned char		sk_protocol;
-	unsigned short		sk_type;
 	int			sk_rcvbuf;
 	socket_lock_t		sk_lock;
 	/*

commit bcdce7195e0eab55b37dbd53be53057f38006380
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Oct 6 17:28:29 2009 -0700

    net: speedup sk_wake_async()
    
    An incoming datagram must bring into cpu cache *lot* of cache lines,
    in particular : (other parts omitted (hash chains, ip route cache...))
    
    On 32bit arches :
    
    offsetof(struct sock, sk_rcvbuf)       =0x30    (read)
    offsetof(struct sock, sk_lock)         =0x34   (rw)
    
    offsetof(struct sock, sk_sleep)        =0x50 (read)
    offsetof(struct sock, sk_rmem_alloc)   =0x64   (rw)
    offsetof(struct sock, sk_receive_queue)=0x74   (rw)
    
    offsetof(struct sock, sk_forward_alloc)=0x98   (rw)
    
    offsetof(struct sock, sk_callback_lock)=0xcc    (rw)
    offsetof(struct sock, sk_drops)        =0xd8 (read if we add dropcount support, rw if frame dropped)
    offsetof(struct sock, sk_filter)       =0xf8    (read)
    
    offsetof(struct sock, sk_socket)       =0x138 (read)
    
    offsetof(struct sock, sk_data_ready)   =0x15c   (read)
    
    
    We can avoid sk->sk_socket and socket->fasync_list referencing on sockets
    with no fasync() structures. (socket->fasync_list ptr is probably already in cache
    because it shares a cache line with socket->wait, ie location pointed by sk->sk_sleep)
    
    This avoids one cache line load per incoming packet for common cases (no fasync())
    
    We can leave (or even move in a future patch) sk->sk_socket in a cold location
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1621935aad5b..98398bdec57d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -504,6 +504,7 @@ enum sock_flags {
 	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
 	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
 	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
+	SOCK_FASYNC, /* fasync() active */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
@@ -1396,7 +1397,7 @@ static inline unsigned long sock_wspace(struct sock *sk)
 
 static inline void sk_wake_async(struct sock *sk, int how, int band)
 {
-	if (sk->sk_socket && sk->sk_socket->fasync_list)
+	if (sock_flag(sk, SOCK_FASYNC))
 		sock_wake_async(sk->sk_socket, how, band);
 }
 

commit b7058842c940ad2c08dd829b21e5c92ebe3b8758
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Sep 30 16:12:20 2009 -0700

    net: Make setsockopt() optlen be unsigned.
    
    This provides safety against negative optlen at the type
    level instead of depending upon (sometimes non-trivial)
    checks against this sprinkled all over the the place, in
    each and every implementation.
    
    Based upon work done by Arjan van de Ven and feedback
    from Linus Torvalds.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 950409dcec3d..1621935aad5b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -624,7 +624,7 @@ struct proto {
 	void			(*shutdown)(struct sock *sk, int how);
 	int			(*setsockopt)(struct sock *sk, int level, 
 					int optname, char __user *optval,
-					int optlen);
+					unsigned int optlen);
 	int			(*getsockopt)(struct sock *sk, int level, 
 					int optname, char __user *optval, 
 					int __user *option);  	 
@@ -632,7 +632,7 @@ struct proto {
 	int			(*compat_setsockopt)(struct sock *sk,
 					int level,
 					int optname, char __user *optval,
-					int optlen);
+					unsigned int optlen);
 	int			(*compat_getsockopt)(struct sock *sk,
 					int level,
 					int optname, char __user *optval,
@@ -951,7 +951,7 @@ extern void			sock_rfree(struct sk_buff *skb);
 
 extern int			sock_setsockopt(struct socket *sock, int level,
 						int op, char __user *optval,
-						int optlen);
+						unsigned int optlen);
 
 extern int			sock_getsockopt(struct socket *sock, int level,
 						int op, char __user *optval, 
@@ -993,7 +993,7 @@ extern int                      sock_no_shutdown(struct socket *, int);
 extern int			sock_no_getsockopt(struct socket *, int , int,
 						   char __user *, int __user *);
 extern int			sock_no_setsockopt(struct socket *, int, int,
-						   char __user *, int);
+						   char __user *, unsigned int);
 extern int                      sock_no_sendmsg(struct kiocb *, struct socket *,
 						struct msghdr *, size_t);
 extern int                      sock_no_recvmsg(struct kiocb *, struct socket *,
@@ -1015,11 +1015,11 @@ extern int sock_common_getsockopt(struct socket *sock, int level, int optname,
 extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
 			       struct msghdr *msg, size_t size, int flags);
 extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
-				  char __user *optval, int optlen);
+				  char __user *optval, unsigned int optlen);
 extern int compat_sock_common_getsockopt(struct socket *sock, int level,
 		int optname, char __user *optval, int __user *optlen);
 extern int compat_sock_common_setsockopt(struct socket *sock, int level,
-		int optname, char __user *optval, int optlen);
+		int optname, char __user *optval, unsigned int optlen);
 
 extern void sk_common_release(struct sock *sk);
 

commit 4dc6dc7162c08b9965163c9ab3f9375d4adff2c7
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Jul 15 23:13:10 2009 +0000

    net: sock_copy() fixes
    
    Commit e912b1142be8f1e2c71c71001dc992c6e5eb2ec1
    (net: sk_prot_alloc() should not blindly overwrite memory)
    took care of not zeroing whole new socket at allocation time.
    
    sock_copy() is another spot where we should be very careful.
    We should not set refcnt to a non null value, until
    we are sure other fields are correctly setup, or
    a lockless reader could catch this socket by mistake,
    while not fully (re)initialized.
    
    This patch puts sk_node & sk_refcnt to the very beginning
    of struct sock to ease sock_copy() & sk_prot_alloc() job.
    
    We add appropriate smp_wmb() before sk_refcnt initializations
    to match our RCU requirements (changes to sock keys should
    be committed to memory before sk_refcnt setting)
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c0da9239b95..950409dcec3d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -104,15 +104,15 @@ struct net;
 
 /**
  *	struct sock_common - minimal network layer representation of sockets
+ *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
+ *	@skc_refcnt: reference count
+ *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_family: network address family
  *	@skc_state: Connection state
  *	@skc_reuse: %SO_REUSEADDR setting
  *	@skc_bound_dev_if: bound device index if != 0
- *	@skc_node: main hash linkage for various protocol lookup tables
- *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
- *	@skc_refcnt: reference count
- *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_prot: protocol handlers inside a network family
  *	@skc_net: reference to the network namespace of this socket
  *
@@ -120,17 +120,21 @@ struct net;
  *	for struct sock and struct inet_timewait_sock.
  */
 struct sock_common {
-	unsigned short		skc_family;
-	volatile unsigned char	skc_state;
-	unsigned char		skc_reuse;
-	int			skc_bound_dev_if;
+	/*
+	 * first fields are not copied in sock_copy()
+	 */
 	union {
 		struct hlist_node	skc_node;
 		struct hlist_nulls_node skc_nulls_node;
 	};
-	struct hlist_node	skc_bind_node;
 	atomic_t		skc_refcnt;
+
 	unsigned int		skc_hash;
+	unsigned short		skc_family;
+	volatile unsigned char	skc_state;
+	unsigned char		skc_reuse;
+	int			skc_bound_dev_if;
+	struct hlist_node	skc_bind_node;
 	struct proto		*skc_prot;
 #ifdef CONFIG_NET_NS
 	struct net	 	*skc_net;
@@ -208,15 +212,17 @@ struct sock {
 	 * don't add nothing before this first member (__sk_common) --acme
 	 */
 	struct sock_common	__sk_common;
+#define sk_node			__sk_common.skc_node
+#define sk_nulls_node		__sk_common.skc_nulls_node
+#define sk_refcnt		__sk_common.skc_refcnt
+
+#define sk_copy_start		__sk_common.skc_hash
+#define sk_hash			__sk_common.skc_hash
 #define sk_family		__sk_common.skc_family
 #define sk_state		__sk_common.skc_state
 #define sk_reuse		__sk_common.skc_reuse
 #define sk_bound_dev_if		__sk_common.skc_bound_dev_if
-#define sk_node			__sk_common.skc_node
-#define sk_nulls_node		__sk_common.skc_nulls_node
 #define sk_bind_node		__sk_common.skc_bind_node
-#define sk_refcnt		__sk_common.skc_refcnt
-#define sk_hash			__sk_common.skc_hash
 #define sk_prot			__sk_common.skc_prot
 #define sk_net			__sk_common.skc_net
 	kmemcheck_bitfield_begin(flags);

commit ad46276952f1af34cd91d46d49ba13d347d56367
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Jul 8 12:10:31 2009 +0000

    memory barrier: adding smp_mb__after_lock
    
    Adding smp_mb__after_lock define to be used as a smp_mb call after
    a lock.
    
    Making it nop for x86, since {read|write|spin}_lock() on x86 are
    full memory barriers.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4eb8409249f6..2c0da9239b95 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1271,6 +1271,9 @@ static inline int sk_has_allocations(const struct sock *sk)
  * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
  * could then endup calling schedule and sleep forever if there are no more
  * data on the socket.
+ *
+ * The sk_has_sleeper is always called right after a call to read_lock, so we
+ * can use smp_mb__after_lock barrier.
  */
 static inline int sk_has_sleeper(struct sock *sk)
 {
@@ -1280,7 +1283,7 @@ static inline int sk_has_sleeper(struct sock *sk)
 	 *
 	 * This memory barrier is paired in the sock_poll_wait.
 	 */
-	smp_mb();
+	smp_mb__after_lock();
 	return sk->sk_sleep && waitqueue_active(sk->sk_sleep);
 }
 

commit a57de0b4336e48db2811a2030bb68dba8dd09d88
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Jul 8 12:09:13 2009 +0000

    net: adding memory barrier to the poll and receive callbacks
    
    Adding memory barrier after the poll_wait function, paired with
    receive callbacks. Adding fuctions sock_poll_wait and sk_has_sleeper
    to wrap the memory barrier.
    
    Without the memory barrier, following race can happen.
    The race fires, when following code paths meet, and the tp->rcv_nxt
    and __add_wait_queue updates stay in CPU caches.
    
    CPU1                         CPU2
    
    sys_select                   receive packet
      ...                        ...
      __add_wait_queue           update tp->rcv_nxt
      ...                        ...
      tp->rcv_nxt check          sock_def_readable
      ...                        {
      schedule                      ...
                                    if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
                                            wake_up_interruptible(sk->sk_sleep)
                                    ...
                                 }
    
    If there was no cache the code would work ok, since the wait_queue and
    rcv_nxt are opposit to each other.
    
    Meaning that once tp->rcv_nxt is updated by CPU2, the CPU1 either already
    passed the tp->rcv_nxt check and sleeps, or will get the new value for
    tp->rcv_nxt and will return with new data mask.
    In both cases the process (CPU1) is being added to the wait queue, so the
    waitqueue_active (CPU2) call cannot miss and will wake up CPU1.
    
    The bad case is when the __add_wait_queue changes done by CPU1 stay in its
    cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1 will then
    endup calling schedule and sleep forever if there are no more data on the
    socket.
    
    Calls to poll_wait in following modules were ommited:
            net/bluetooth/af_bluetooth.c
            net/irda/af_irda.c
            net/irda/irnet/irnet_ppp.c
            net/mac80211/rc80211_pid_debugfs.c
            net/phonet/socket.c
            net/rds/af_rds.c
            net/rfkill/core.c
            net/sunrpc/cache.c
            net/sunrpc/rpc_pipe.c
            net/tipc/socket.c
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 352f06bbd7a9..4eb8409249f6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -54,6 +54,7 @@
 
 #include <linux/filter.h>
 #include <linux/rculist_nulls.h>
+#include <linux/poll.h>
 
 #include <asm/atomic.h>
 #include <net/dst.h>
@@ -1241,6 +1242,71 @@ static inline int sk_has_allocations(const struct sock *sk)
 	return sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);
 }
 
+/**
+ * sk_has_sleeper - check if there are any waiting processes
+ * @sk: socket
+ *
+ * Returns true if socket has waiting processes
+ *
+ * The purpose of the sk_has_sleeper and sock_poll_wait is to wrap the memory
+ * barrier call. They were added due to the race found within the tcp code.
+ *
+ * Consider following tcp code paths:
+ *
+ * CPU1                  CPU2
+ *
+ * sys_select            receive packet
+ *   ...                 ...
+ *   __add_wait_queue    update tp->rcv_nxt
+ *   ...                 ...
+ *   tp->rcv_nxt check   sock_def_readable
+ *   ...                 {
+ *   schedule               ...
+ *                          if (sk->sk_sleep && waitqueue_active(sk->sk_sleep))
+ *                              wake_up_interruptible(sk->sk_sleep)
+ *                          ...
+ *                       }
+ *
+ * The race for tcp fires when the __add_wait_queue changes done by CPU1 stay
+ * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
+ * could then endup calling schedule and sleep forever if there are no more
+ * data on the socket.
+ */
+static inline int sk_has_sleeper(struct sock *sk)
+{
+	/*
+	 * We need to be sure we are in sync with the
+	 * add_wait_queue modifications to the wait queue.
+	 *
+	 * This memory barrier is paired in the sock_poll_wait.
+	 */
+	smp_mb();
+	return sk->sk_sleep && waitqueue_active(sk->sk_sleep);
+}
+
+/**
+ * sock_poll_wait - place memory barrier behind the poll_wait call.
+ * @filp:           file
+ * @wait_address:   socket wait queue
+ * @p:              poll_table
+ *
+ * See the comments in the sk_has_sleeper function.
+ */
+static inline void sock_poll_wait(struct file *filp,
+		wait_queue_head_t *wait_address, poll_table *p)
+{
+	if (p && wait_address) {
+		poll_wait(filp, wait_address, p);
+		/*
+		 * We need to be sure we are in sync with the
+		 * socket flags modification.
+		 *
+		 * This memory barrier is paired in the sk_has_sleeper.
+		*/
+		smp_mb();
+	}
+}
+
 /*
  * 	Queue a received datagram if it will fit. Stream and sequenced
  *	protocols can't normally use this as they need to fit buffers in

commit 09ce42d3167e3f20b501fa780c2415332330fac5
Merge: d7ed9c05ebf5 7959ea254ed1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:01:12 2009 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next-2.6:
      bnx2: Fix the behavior of ethtool when ONBOOT=no
      qla3xxx: Don't sleep while holding lock.
      qla3xxx: Give the PHY time to come out of reset.
      ipv4 routing: Ensure that route cache entries are usable and reclaimable with caching is off
      net: Move rx skb_orphan call to where needed
      ipv6: Use correct data types for ICMPv6 type and code
      net: let KS8842 driver depend on HAS_IOMEM
      can: let SJA1000 driver depend on HAS_IOMEM
      netxen: fix firmware init handshake
      netxen: fix build with without CONFIG_PM
      netfilter: xt_rateest: fix comparison with self
      netfilter: xt_quota: fix incomplete initialization
      netfilter: nf_log: fix direct userspace memory access in proc handler
      netfilter: fix some sparse endianess warnings
      netfilter: nf_conntrack: fix conntrack lookup race
      netfilter: nf_conntrack: fix confirmation race condition
      netfilter: nf_conntrack: death_by_timeout() fix

commit d55d87fdff8252d0e2f7c28c2d443aee17e9d70f
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Jun 22 02:25:25 2009 +0000

    net: Move rx skb_orphan call to where needed
    
    In order to get the tun driver to account packets, we need to be
    able to receive packets with destructors set.  To be on the safe
    side, I added an skb_orphan call for all protocols by default since
    some of them (IP in particular) cannot handle receiving packets
    destructors properly.
    
    Now it seems that at least one protocol (CAN) expects to be able
    to pass skb->sk through the rx path without getting clobbered.
    
    So this patch attempts to fix this properly by moving the skb_orphan
    call to where it's actually needed.  In particular, I've added it
    to skb_set_owner_[rw] which is what most users of skb->destructor
    call.
    
    This is actually an improvement for tun too since it means that
    we only give back the amount charged to the socket when the skb
    is passed to another socket that will also be charged accordingly.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Tested-by: Oliver Hartkopp <olver@hartkopp.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 570c7a12b54e..7f5c41cc45a9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1250,6 +1250,7 @@ static inline int sk_has_allocations(const struct sock *sk)
 
 static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
 {
+	skb_orphan(skb);
 	skb->sk = sk;
 	skb->destructor = sock_wfree;
 	/*
@@ -1262,6 +1263,7 @@ static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
 
 static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 {
+	skb_orphan(skb);
 	skb->sk = sk;
 	skb->destructor = sock_rfree;
 	atomic_add(skb->truesize, &sk->sk_rmem_alloc);

commit d2aa4550379f92e929af7ed1dd4f55e6a1e331f8
Merge: 9e3e4b1d2d13 cb2107be43d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 18 14:07:15 2009 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next-2.6
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next-2.6: (55 commits)
      netxen: fix tx ring accounting
      netxen: fix detection of cut-thru firmware mode
      forcedeth: fix dma api mismatches
      atm: sk_wmem_alloc initial value is one
      net: correct off-by-one write allocations reports
      via-velocity : fix no link detection on boot
      Net / e100: Fix suspend of devices that cannot be power managed
      TI DaVinci EMAC : Fix rmmod error
      net: group address list and its count
      ipv4: Fix fib_trie rebalancing, part 2
      pkt_sched: Update drops stats in act_police
      sky2: version 1.23
      sky2: add GRO support
      sky2: skb recycling
      sky2: reduce default transmit ring
      sky2: receive counter update
      sky2: fix shutdown synchronization
      sky2: PCI irq issues
      sky2: more receive shutdown
      sky2: turn off pause during shutdown
      ...
    
    Manually fix trivial conflict in net/core/skbuff.c due to kmemcheck

commit c564039fd83ea16a86a96d52632794b24849e507
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Jun 16 10:12:03 2009 +0000

    net: sk_wmem_alloc has initial value of one, not zero
    
    commit 2b85a34e911bf483c27cfdd124aeb1605145dc80
    (net: No more expensive sock_hold()/sock_put() on each tx)
    changed initial sk_wmem_alloc value.
    
    Some protocols check sk_wmem_alloc value to determine if a timer
    must delay socket deallocation. We must take care of the sk_wmem_alloc
    value being one instead of zero when no write allocations are pending.
    
    Reported by Ingo Molnar, and full diagnostic from David Miller.
    
    This patch introduces three helpers to get read/write allocations
    and a followup patch will use these helpers to report correct
    write allocations to user.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 010e14a93c92..570c7a12b54e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1206,6 +1206,39 @@ static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 	return 0;
 }
 
+/**
+ * sk_wmem_alloc_get - returns write allocations
+ * @sk: socket
+ *
+ * Returns sk_wmem_alloc minus initial offset of one
+ */
+static inline int sk_wmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_wmem_alloc) - 1;
+}
+
+/**
+ * sk_rmem_alloc_get - returns read allocations
+ * @sk: socket
+ *
+ * Returns sk_rmem_alloc
+ */
+static inline int sk_rmem_alloc_get(const struct sock *sk)
+{
+	return atomic_read(&sk->sk_rmem_alloc);
+}
+
+/**
+ * sk_has_allocations - check if allocations are outstanding
+ * @sk: socket
+ *
+ * Returns true if socket has write or read allocations
+ */
+static inline int sk_has_allocations(const struct sock *sk)
+{
+	return sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);
+}
+
 /*
  * 	Queue a received datagram if it will fit. Stream and sequenced
  *	protocols can't normally use this as they need to fit buffers in

commit b3fec0fe35a4ff048484f1408385a27695d4273b
Merge: e1f5b94fd0c9 722f2a6c87f3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 16 13:09:51 2009 -0700

    Merge branch 'for-linus2' of git://git.kernel.org/pub/scm/linux/kernel/git/vegard/kmemcheck
    
    * 'for-linus2' of git://git.kernel.org/pub/scm/linux/kernel/git/vegard/kmemcheck: (39 commits)
      signal: fix __send_signal() false positive kmemcheck warning
      fs: fix do_mount_root() false positive kmemcheck warning
      fs: introduce __getname_gfp()
      trace: annotate bitfields in struct ring_buffer_event
      net: annotate struct sock bitfield
      c2port: annotate bitfield for kmemcheck
      net: annotate inet_timewait_sock bitfields
      ieee1394/csr1212: fix false positive kmemcheck report
      ieee1394: annotate bitfield
      net: annotate bitfields in struct inet_sock
      net: use kmemcheck bitfields API for skbuff
      kmemcheck: introduce bitfield API
      kmemcheck: add opcode self-testing at boot
      x86: unify pte_hidden
      x86: make _PAGE_HIDDEN conditional
      kmemcheck: make kconfig accessible for other architectures
      kmemcheck: enable in the x86 Kconfig
      kmemcheck: add hooks for the page allocator
      kmemcheck: add hooks for page- and sg-dma-mappings
      kmemcheck: don't track page tables
      ...

commit a98b65a3ad71e702e760bc63f57684301628e837
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Thu Feb 26 14:46:57 2009 +0100

    net: annotate struct sock bitfield
    
    2009/2/24 Ingo Molnar <mingo@elte.hu>:
    > ok, this is the last warning i have from today's overnight -tip
    > testruns - a 32-bit system warning in sock_init_data():
    >
    > [    2.610389] NET: Registered protocol family 16
    > [    2.616138] initcall netlink_proto_init+0x0/0x170 returned 0 after 7812 usecs
    > [    2.620010] WARNING: kmemcheck: Caught 32-bit read from uninitialized memory (f642c184)
    > [    2.624002] 010000000200000000000000604990c000000000000000000000000000000000
    > [    2.634076]  i i i i i i u u i i i i i i i i i i i i i i i i i i i i i i i i
    > [    2.641038]          ^
    > [    2.643376]
    > [    2.644004] Pid: 1, comm: swapper Not tainted (2.6.29-rc6-tip-01751-g4d1c22c-dirty #885)
    > [    2.648003] EIP: 0060:[<c07141a1>] EFLAGS: 00010282 CPU: 0
    > [    2.652008] EIP is at sock_init_data+0xa1/0x190
    > [    2.656003] EAX: 0001a800 EBX: f6836c00 ECX: 00463000 EDX: c0e46fe0
    > [    2.660003] ESI: f642c180 EDI: c0b83088 EBP: f6863ed8 ESP: c0c412ec
    > [    2.664003]  DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
    > [    2.668003] CR0: 8005003b CR2: f682c400 CR3: 00b91000 CR4: 000006f0
    > [    2.672003] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    > [    2.676003] DR6: ffff4ff0 DR7: 00000400
    > [    2.680002]  [<c07423e5>] __netlink_create+0x35/0xa0
    > [    2.684002]  [<c07443cc>] netlink_kernel_create+0x4c/0x140
    > [    2.688002]  [<c072755e>] rtnetlink_net_init+0x1e/0x40
    > [    2.696002]  [<c071b601>] register_pernet_operations+0x11/0x30
    > [    2.700002]  [<c071b72c>] register_pernet_subsys+0x1c/0x30
    > [    2.704002]  [<c0bf3c8c>] rtnetlink_init+0x4c/0x100
    > [    2.708002]  [<c0bf4669>] netlink_proto_init+0x159/0x170
    > [    2.712002]  [<c0101124>] do_one_initcall+0x24/0x150
    > [    2.716002]  [<c0bbf3c7>] do_initcalls+0x27/0x40
    > [    2.723201]  [<c0bbf3fc>] do_basic_setup+0x1c/0x20
    > [    2.728002]  [<c0bbfb8a>] kernel_init+0x5a/0xa0
    > [    2.732002]  [<c0103e47>] kernel_thread_helper+0x7/0x10
    > [    2.736002]  [<ffffffff>] 0xffffffff
    
    We fix this false positive by annotating the bitfield in struct
    sock.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4bb1ff9fd15b..d933da00d505 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -218,9 +218,11 @@ struct sock {
 #define sk_hash			__sk_common.skc_hash
 #define sk_prot			__sk_common.skc_prot
 #define sk_net			__sk_common.skc_net
+	kmemcheck_bitfield_begin(flags);
 	unsigned char		sk_shutdown : 2,
 				sk_no_check : 2,
 				sk_userlocks : 4;
+	kmemcheck_bitfield_end(flags);
 	unsigned char		sk_protocol;
 	unsigned short		sk_type;
 	int			sk_rcvbuf;

commit 2b85a34e911bf483c27cfdd124aeb1605145dc80
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Jun 11 02:55:43 2009 -0700

    net: No more expensive sock_hold()/sock_put() on each tx
    
    One of the problem with sock memory accounting is it uses
    a pair of sock_hold()/sock_put() for each transmitted packet.
    
    This slows down bidirectional flows because the receive path
    also needs to take a refcount on socket and might use a different
    cpu than transmit path or transmit completion path. So these
    two atomic operations also trigger cache line bounces.
    
    We can see this in tx or tx/rx workloads (media gateways for example),
    where sock_wfree() can be in top five functions in profiles.
    
    We use this sock_hold()/sock_put() so that sock freeing
    is delayed until all tx packets are completed.
    
    As we also update sk_wmem_alloc, we could offset sk_wmem_alloc
    by one unit at init time, until sk_free() is called.
    Once sk_free() is called, we atomic_dec_and_test(sk_wmem_alloc)
    to decrement initial offset and atomicaly check if any packets
    are in flight.
    
    skb_set_owner_w() doesnt call sock_hold() anymore
    
    sock_wfree() doesnt call sock_put() anymore, but check if sk_wmem_alloc
    reached 0 to perform the final freeing.
    
    Drawback is that a skb->truesize error could lead to unfreeable sockets, or
    even worse, prematurely calling __sk_free() on a live socket.
    
    Nice speedups on SMP. tbench for example, going from 2691 MB/s to 2711 MB/s
    on my 8 cpu dev machine, even if tbench was not really hitting sk_refcnt
    contention point. 5 % speedup on a UDP transmit workload (depends
    on number of flows), lowering TX completion cpu usage.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4bb1ff9fd15b..010e14a93c92 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1217,9 +1217,13 @@ static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 
 static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
 {
-	sock_hold(sk);
 	skb->sk = sk;
 	skb->destructor = sock_wfree;
+	/*
+	 * We used to take a refcount on sk, but following operation
+	 * is enough to guarantee sk_free() wont free this sock until
+	 * all in-flight packets are completed
+	 */
 	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
 }
 

commit e70049b9e74267dd47e1ffa62302073487afcb48
Merge: d18921a0e319 f7e603ad8f78
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Feb 24 03:50:29 2009 -0800

    Merge branch 'master' of /home/davem/src/GIT/linux-2.6/

commit 92a0acce186cde8ead56c6915d9479773673ea1a
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Feb 17 21:24:05 2009 -0800

    net: Kill skb_truesize_check(), it only catches false-positives.
    
    A long time ago we had bugs, primarily in TCP, where we would modify
    skb->truesize (for TSO queue collapsing) in ways which would corrupt
    the socket memory accounting.
    
    skb_truesize_check() was added in order to try and catch this error
    more systematically.
    
    However this debugging check has morphed into a Frankenstein of sorts
    and these days it does nothing other than catch false-positives.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ce3b5b622683..eefeeaf7fc46 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -860,7 +860,6 @@ static inline void sk_mem_uncharge(struct sock *sk, int size)
 
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
-	skb_truesize_check(skb);
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
 	sk->sk_wmem_queued -= skb->truesize;
 	sk_mem_uncharge(sk, skb->truesize);

commit 20d4947353be60e909e6b1a79d241457edd6833f
Author: Patrick Ohly <patrick.ohly@intel.com>
Date:   Thu Feb 12 05:03:38 2009 +0000

    net: socket infrastructure for SO_TIMESTAMPING
    
    The overlap with the old SO_TIMESTAMP[NS] options is handled so
    that time stamping in software (net_enable_timestamp()) is
    enabled when SO_TIMESTAMP[NS] and/or SO_TIMESTAMPING_RX_SOFTWARE
    is set.  It's disabled if all of these are off.
    
    Signed-off-by: Patrick Ohly <patrick.ohly@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ded6854e3e4a..cc9d5bcb06f7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -158,7 +158,7 @@ struct sock_common {
   *	@sk_allocation: allocation mode
   *	@sk_sndbuf: size of send buffer in bytes
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
-  *		   %SO_OOBINLINE settings
+  *		   %SO_OOBINLINE settings, %SO_TIMESTAMPING settings
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
@@ -488,6 +488,13 @@ enum sock_flags {
 	SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+	SOCK_TIMESTAMPING_TX_HARDWARE,  /* %SOF_TIMESTAMPING_TX_HARDWARE */
+	SOCK_TIMESTAMPING_TX_SOFTWARE,  /* %SOF_TIMESTAMPING_TX_SOFTWARE */
+	SOCK_TIMESTAMPING_RX_HARDWARE,  /* %SOF_TIMESTAMPING_RX_HARDWARE */
+	SOCK_TIMESTAMPING_RX_SOFTWARE,  /* %SOF_TIMESTAMPING_RX_SOFTWARE */
+	SOCK_TIMESTAMPING_SOFTWARE,     /* %SOF_TIMESTAMPING_SOFTWARE */
+	SOCK_TIMESTAMPING_RAW_HARDWARE, /* %SOF_TIMESTAMPING_RAW_HARDWARE */
+	SOCK_TIMESTAMPING_SYS_HARDWARE, /* %SOF_TIMESTAMPING_SYS_HARDWARE */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
@@ -1346,13 +1353,44 @@ static __inline__ void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 {
 	ktime_t kt = skb->tstamp;
+	struct skb_shared_hwtstamps *hwtstamps = skb_hwtstamps(skb);
 
-	if (sock_flag(sk, SOCK_RCVTSTAMP))
+	/*
+	 * generate control messages if
+	 * - receive time stamping in software requested (SOCK_RCVTSTAMP
+	 *   or SOCK_TIMESTAMPING_RX_SOFTWARE)
+	 * - software time stamp available and wanted
+	 *   (SOCK_TIMESTAMPING_SOFTWARE)
+	 * - hardware time stamps available and wanted
+	 *   (SOCK_TIMESTAMPING_SYS_HARDWARE or
+	 *   SOCK_TIMESTAMPING_RAW_HARDWARE)
+	 */
+	if (sock_flag(sk, SOCK_RCVTSTAMP) ||
+	    sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE) ||
+	    (kt.tv64 && sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE)) ||
+	    (hwtstamps->hwtstamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE)) ||
+	    (hwtstamps->syststamp.tv64 &&
+	     sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE)))
 		__sock_recv_timestamp(msg, sk, skb);
 	else
 		sk->sk_stamp = kt;
 }
 
+/**
+ * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
+ * @msg:	outgoing packet
+ * @sk:		socket sending this packet
+ * @shtx:	filled with instructions for time stamping
+ *
+ * Currently only depends on SOCK_TIMESTAMPING* flags. Returns error code if
+ * parameters are invalid.
+ */
+extern int sock_tx_timestamp(struct msghdr *msg,
+			     struct sock *sk,
+			     union skb_shared_tx *shtx);
+
+
 /**
  * sk_eat_skb - Release a skb if it is no longer needed
  * @sk: socket to eat this skb from
@@ -1421,7 +1459,7 @@ static inline struct sock *skb_steal_sock(struct sk_buff *skb)
 	return NULL;
 }
 
-extern void sock_enable_timestamp(struct sock *sk);
+extern void sock_enable_timestamp(struct sock *sk, int flag);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);
 

commit 5e30589521518bff36fd2638b3c3d69679c50436
Merge: ac178ef0ae9e d2f8d7ee1a9b
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Feb 14 23:12:00 2009 -0800

    Merge branch 'master' of /home/davem/src/GIT/linux-2.6/
    
    Conflicts:
            drivers/net/wireless/iwlwifi/iwl-agn.c
            drivers/net/wireless/iwlwifi/iwl3945-base.c

commit 99709372736a216f99eb32b76fba835a2bfc93a8
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Feb 12 16:43:17 2009 -0800

    net: don't use in_atomic() in gfp_any()
    
    The problem is that in_atomic() will return false inside spinlocks if
    CONFIG_PREEMPT=n.  This will lead to deadlockable GFP_KERNEL allocations
    from spinlocked regions.
    
    Secondly, if CONFIG_PREEMPT=y, this bug solves itself because networking
    will instead use GFP_ATOMIC from this callsite.  Hence we won't get the
    might_sleep() debugging warnings which would have informed us of the buggy
    callsites.
    
    Solve both these problems by switching to in_interrupt().  Now, if someone
    runs a gfp_any() allocation from inside spinlock we will get the warning
    if CONFIG_PREEMPT=y.
    
    I reviewed all callsites and most of them were too complex for my little
    brain and none of them documented their interface requirements.  I have no
    idea what this patch will do.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5a3a151bd730..ce3b5b622683 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1308,7 +1308,7 @@ static inline int sock_writeable(const struct sock *sk)
 
 static inline gfp_t gfp_any(void)
 {
-	return in_atomic() ? GFP_ATOMIC : GFP_KERNEL;
+	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
 }
 
 static inline long sock_rcvtimeo(const struct sock *sk, int noblock)

commit 4cc7f68d65558f683c702d4fe3a5aac4c5227b97
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Feb 4 16:55:54 2009 -0800

    net: Reexport sock_alloc_send_pskb
    
    The function sock_alloc_send_pskb is completely useless if not
    exported since most of the code in it won't be used as is.  In
    fact, this code has already been duplicated in the tun driver.
    
    Now that we need accounting in the tun driver, we can in fact
    use this function as is.  So this patch marks it for export again.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5a3a151bd730..c047def9cf10 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -945,6 +945,11 @@ extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
 						     unsigned long size,
 						     int noblock,
 						     int *errcode);
+extern struct sk_buff 		*sock_alloc_send_pskb(struct sock *sk,
+						      unsigned long header_len,
+						      unsigned long data_len,
+						      int noblock,
+						      int *errcode);
 extern void *sock_kmalloc(struct sock *sk, int size,
 			  gfp_t priority);
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);

commit dd24c00191d5e4a1ae896aafe33c6b8095ab4bd1
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Nov 25 21:17:14 2008 -0800

    net: Use a percpu_counter for orphan_count
    
    Instead of using one atomic_t per protocol, use a percpu_counter
    for "orphan_count", to reduce cache line contention on
    heavy duty network servers.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a2a3890959c4..5a3a151bd730 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -666,7 +666,7 @@ struct proto {
 	unsigned int		obj_size;
 	int			slab_flags;
 
-	atomic_t		*orphan_count;
+	struct percpu_counter	*orphan_count;
 
 	struct request_sock_ops	*rsk_prot;
 	struct timewait_sock_ops *twsk_prot;

commit 1748376b6626acf59c24e9592ac67b3fe2a0e026
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Nov 25 21:16:35 2008 -0800

    net: Use a percpu_counter for sockets_allocated
    
    Instead of using one atomic_t per protocol, use a percpu_counter
    for "sockets_allocated", to reduce cache line contention on
    heavy duty network servers.
    
    Note : We revert commit (248969ae31e1b3276fc4399d67ce29a5d81e6fd9
    net: af_unix can make unix_nr_socks visbile in /proc),
    since it is not anymore used after sock_prot_inuse_add() addition
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 00cd486d362f..a2a3890959c4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -649,7 +649,7 @@ struct proto {
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(struct sock *sk);
 	atomic_t		*memory_allocated;	/* Current allocated memory. */
-	atomic_t		*sockets_allocated;	/* Current number of sockets. */
+	struct percpu_counter	*sockets_allocated;	/* Current number of sockets. */
 	/*
 	 * Pressure flag: try to collapse.
 	 * Technical note: it is used by multiple contexts non atomically.

commit 198d6ba4d7f48c94f990f4604f0b3d73925e0ded
Merge: 9a57f7fabd38 7f0f598a0069
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Nov 18 23:38:23 2008 -0800

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
    
            drivers/isdn/i4l/isdn_net.c
            fs/cifs/connect.c

commit 88ab1932eac721c6e7336708558fa5ed02c85c80
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Sun Nov 16 19:39:21 2008 -0800

    udp: Use hlist_nulls in UDP RCU code
    
    This is a straightforward patch, using hlist_nulls infrastructure.
    
    RCUification already done on UDP two weeks ago.
    
    Using hlist_nulls permits us to avoid some memory barriers, both
    at lookup time and delete time.
    
    Patch is large because it adds new macros to include/net/sock.h.
    These macros will be used by TCP & DCCP in next patch.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8b2b82131b67..0a638948868d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -42,6 +42,7 @@
 
 #include <linux/kernel.h>
 #include <linux/list.h>
+#include <linux/list_nulls.h>
 #include <linux/timer.h>
 #include <linux/cache.h>
 #include <linux/module.h>
@@ -52,6 +53,7 @@
 #include <linux/security.h>
 
 #include <linux/filter.h>
+#include <linux/rculist_nulls.h>
 
 #include <asm/atomic.h>
 #include <net/dst.h>
@@ -106,6 +108,7 @@ struct net;
  *	@skc_reuse: %SO_REUSEADDR setting
  *	@skc_bound_dev_if: bound device index if != 0
  *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_nulls_node: main hash linkage for UDP/UDP-Lite protocol
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
  *	@skc_refcnt: reference count
  *	@skc_hash: hash value used with various protocol lookup tables
@@ -120,7 +123,10 @@ struct sock_common {
 	volatile unsigned char	skc_state;
 	unsigned char		skc_reuse;
 	int			skc_bound_dev_if;
-	struct hlist_node	skc_node;
+	union {
+		struct hlist_node	skc_node;
+		struct hlist_nulls_node skc_nulls_node;
+	};
 	struct hlist_node	skc_bind_node;
 	atomic_t		skc_refcnt;
 	unsigned int		skc_hash;
@@ -206,6 +212,7 @@ struct sock {
 #define sk_reuse		__sk_common.skc_reuse
 #define sk_bound_dev_if		__sk_common.skc_bound_dev_if
 #define sk_node			__sk_common.skc_node
+#define sk_nulls_node		__sk_common.skc_nulls_node
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_refcnt		__sk_common.skc_refcnt
 #define sk_hash			__sk_common.skc_hash
@@ -300,12 +307,30 @@ static inline struct sock *sk_head(const struct hlist_head *head)
 	return hlist_empty(head) ? NULL : __sk_head(head);
 }
 
+static inline struct sock *__sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_entry(head->first, struct sock, sk_nulls_node);
+}
+
+static inline struct sock *sk_nulls_head(const struct hlist_nulls_head *head)
+{
+	return hlist_nulls_empty(head) ? NULL : __sk_nulls_head(head);
+}
+
 static inline struct sock *sk_next(const struct sock *sk)
 {
 	return sk->sk_node.next ?
 		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
 }
 
+static inline struct sock *sk_nulls_next(const struct sock *sk)
+{
+	return (!is_a_nulls(sk->sk_nulls_node.next)) ?
+		hlist_nulls_entry(sk->sk_nulls_node.next,
+				  struct sock, sk_nulls_node) :
+		NULL;
+}
+
 static inline int sk_unhashed(const struct sock *sk)
 {
 	return hlist_unhashed(&sk->sk_node);
@@ -321,6 +346,11 @@ static __inline__ void sk_node_init(struct hlist_node *node)
 	node->pprev = NULL;
 }
 
+static __inline__ void sk_nulls_node_init(struct hlist_nulls_node *node)
+{
+	node->pprev = NULL;
+}
+
 static __inline__ void __sk_del_node(struct sock *sk)
 {
 	__hlist_del(&sk->sk_node);
@@ -367,18 +397,18 @@ static __inline__ int sk_del_node_init(struct sock *sk)
 	return rc;
 }
 
-static __inline__ int __sk_del_node_init_rcu(struct sock *sk)
+static __inline__ int __sk_nulls_del_node_init_rcu(struct sock *sk)
 {
 	if (sk_hashed(sk)) {
-		hlist_del_init_rcu(&sk->sk_node);
+		hlist_nulls_del_init_rcu(&sk->sk_nulls_node);
 		return 1;
 	}
 	return 0;
 }
 
-static __inline__ int sk_del_node_init_rcu(struct sock *sk)
+static __inline__ int sk_nulls_del_node_init_rcu(struct sock *sk)
 {
-	int rc = __sk_del_node_init_rcu(sk);
+	int rc = __sk_nulls_del_node_init_rcu(sk);
 
 	if (rc) {
 		/* paranoid for a while -acme */
@@ -399,15 +429,15 @@ static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
 	__sk_add_node(sk, list);
 }
 
-static __inline__ void __sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+static __inline__ void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
-	hlist_add_head_rcu(&sk->sk_node, list);
+	hlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);
 }
 
-static __inline__ void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+static __inline__ void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)
 {
 	sock_hold(sk);
-	__sk_add_node_rcu(sk, list);
+	__sk_nulls_add_node_rcu(sk, list);
 }
 
 static __inline__ void __sk_del_bind_node(struct sock *sk)
@@ -423,11 +453,16 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 
 #define sk_for_each(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_node)
-#define sk_for_each_rcu_safenext(__sk, node, list, next) \
-	hlist_for_each_entry_rcu_safenext(__sk, node, list, sk_node, next)
+#define sk_nulls_for_each(__sk, node, list) \
+	hlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)
+#define sk_nulls_for_each_rcu(__sk, node, list) \
+	hlist_nulls_for_each_entry_rcu(__sk, node, list, sk_nulls_node)
 #define sk_for_each_from(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
 		hlist_for_each_entry_from(__sk, node, sk_node)
+#define sk_nulls_for_each_from(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \
+		hlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)
 #define sk_for_each_continue(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
 		hlist_for_each_entry_continue(__sk, node, sk_node)

commit e8f6fbf62de37cbc2e179176ac7010d5f4396b67
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 12 01:38:36 2008 +0000

    lockdep: include/linux/lockdep.h - fix warning in net/bluetooth/af_bluetooth.c
    
    fix this warning:
    
      net/bluetooth/af_bluetooth.c:60: warning: ‘bt_key_strings’ defined but not used
      net/bluetooth/af_bluetooth.c:71: warning: ‘bt_slock_key_strings’ defined but not used
    
    this is a lockdep macro problem in the !LOCKDEP case.
    
    We cannot convert it to an inline because the macro works on multiple types,
    but we can mark the parameter used.
    
    [ also clean up a misaligned tab in sock_lock_init_class_and_name() ]
    
    [ also remove #ifdefs from around af_family_clock_key strings - which
      were certainly added to get rid of the ugly build warnings. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c04f9e18ea22..2f47107f6d0f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -815,7 +815,7 @@ static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
  */
 #define sock_lock_init_class_and_name(sk, sname, skey, name, key) 	\
 do {									\
-	sk->sk_lock.owned = 0;					\
+	sk->sk_lock.owned = 0;						\
 	init_waitqueue_head(&sk->sk_lock.wq);				\
 	spin_lock_init(&(sk)->sk_lock.slock);				\
 	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\

commit 2378982487c492541d17adc0a870e7e83b07ba43
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 12 23:25:32 2008 -0800

    net: ifdef struct sock::sk_async_wait_queue
    
    Every user is under CONFIG_NET_DMA already, so ifdef field as well.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 08291c1be41e..8b2b82131b67 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -239,7 +239,9 @@ struct sock {
 	int			sk_sndbuf;
 	struct sk_buff_head	sk_receive_queue;
 	struct sk_buff_head	sk_write_queue;
+#ifdef CONFIG_NET_DMA
 	struct sk_buff_head	sk_async_wait_queue;
+#endif
 	int			sk_wmem_queued;
 	int			sk_forward_alloc;
 	gfp_t			sk_allocation;

commit d5f642384e9da75393160350f75bbb9a527f7c58
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Nov 4 14:45:58 2008 -0800

    net: #ifdef ->sk_security
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 941ad7c830a3..08291c1be41e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -271,7 +271,9 @@ struct sock {
 	struct sk_buff		*sk_send_head;
 	__u32			sk_sndmsg_off;
 	int			sk_write_pending;
+#ifdef CONFIG_SECURITY
 	void			*sk_security;
+#endif
 	__u32			sk_mark;
 	/* XXX 4 bytes hole on 64 bit */
 	void			(*sk_state_change)(struct sock *sk);

commit a1744d3bee19d3b9cbfb825ab316a101b9c9f109
Merge: 275f165fa970 a432226614c5
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 31 00:17:34 2008 -0700

    Merge branch 'master' of master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    Conflicts:
    
            drivers/net/wireless/p54/p54common.c

commit ad1d967c88e349c7e822ad75dd3247a2a50d2ea3
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Thu Oct 30 23:54:35 2008 -0700

    net: delete excess kernel-doc notation
    
    Remove excess kernel-doc function parameters from networking header
    & driver files:
    
    Warning(include/net/sock.h:946): Excess function parameter or struct member 'sk' description in 'sk_filter_release'
    Warning(include/linux/netdevice.h:1545): Excess function parameter or struct member 'cpu' description in 'netif_tx_lock'
    Warning(drivers/net/wan/z85230.c:712): Excess function parameter or struct member 'regs' description in 'z8530_interrupt'
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ada50c04d09f..c04f9e18ea22 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -936,7 +936,6 @@ extern void sock_init_data(struct socket *sock, struct sock *sk);
 
 /**
  *	sk_filter_release: Release a socket filter
- *	@sk: socket
  *	@fp: filter to remove
  *
  *	Remove a filter from a socket and release its resources.

commit 96631ed16c514cf8b28fab991a076985ce378c26
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed Oct 29 11:19:58 2008 -0700

    udp: introduce sk_for_each_rcu_safenext()
    
    Corey Minyard found a race added in commit 271b72c7fa82c2c7a795bc16896149933110672d
    (udp: RCU handling for Unicast packets.)
    
     "If the socket is moved from one list to another list in-between the
     time the hash is calculated and the next field is accessed, and the
     socket has moved to the end of the new list, the traversal will not
     complete properly on the list it should have, since the socket will
     be on the end of the new list and there's not a way to tell it's on a
     new list and restart the list traversal.  I think that this can be
     solved by pre-fetching the "next" field (with proper barriers) before
     checking the hash."
    
    This patch corrects this problem, introducing a new
    sk_for_each_rcu_safenext() macro.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0bea25db5471..a4f6d3fc0470 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -419,8 +419,8 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 
 #define sk_for_each(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_node)
-#define sk_for_each_rcu(__sk, node, list) \
-	hlist_for_each_entry_rcu(__sk, node, list, sk_node)
+#define sk_for_each_rcu_safenext(__sk, node, list, next) \
+	hlist_for_each_entry_rcu_safenext(__sk, node, list, sk_node, next)
 #define sk_for_each_from(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
 		hlist_for_each_entry_from(__sk, node, sk_node)

commit 271b72c7fa82c2c7a795bc16896149933110672d
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed Oct 29 02:11:14 2008 -0700

    udp: RCU handling for Unicast packets.
    
    Goals are :
    
    1) Optimizing handling of incoming Unicast UDP frames, so that no memory
     writes should happen in the fast path.
    
     Note: Multicasts and broadcasts still will need to take a lock,
     because doing a full lockless lookup in this case is difficult.
    
    2) No expensive operations in the socket bind/unhash phases :
      - No expensive synchronize_rcu() calls.
    
      - No added rcu_head in socket structure, increasing memory needs,
      but more important, forcing us to use call_rcu() calls,
      that have the bad property of making sockets structure cold.
      (rcu grace period between socket freeing and its potential reuse
       make this socket being cold in CPU cache).
      David did a previous patch using call_rcu() and noticed a 20%
      impact on TCP connection rates.
      Quoting Cristopher Lameter :
       "Right. That results in cacheline cooldown. You'd want to recycle
        the object as they are cache hot on a per cpu basis. That is screwed
        up by the delayed regular rcu processing. We have seen multiple
        regressions due to cacheline cooldown.
        The only choice in cacheline hot sensitive areas is to deal with the
        complexity that comes with SLAB_DESTROY_BY_RCU or give up on RCU."
    
      - Because udp sockets are allocated from dedicated kmem_cache,
      use of SLAB_DESTROY_BY_RCU can help here.
    
    Theory of operation :
    ---------------------
    
    As the lookup is lockfree (using rcu_read_lock()/rcu_read_unlock()),
    special attention must be taken by readers and writers.
    
    Use of SLAB_DESTROY_BY_RCU is tricky too, because a socket can be freed,
    reused, inserted in a different chain or in worst case in the same chain
    while readers could do lookups in the same time.
    
    In order to avoid loops, a reader must check each socket found in a chain
    really belongs to the chain the reader was traversing. If it finds a
    mismatch, lookup must start again at the begining. This *restart* loop
    is the reason we had to use rdlock for the multicast case, because
    we dont want to send same message several times to the same socket.
    
    We use RCU only for fast path.
    Thus, /proc/net/udp still takes spinlocks.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d200dfbe1ef6..0bea25db5471 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -363,6 +363,27 @@ static __inline__ int sk_del_node_init(struct sock *sk)
 	return rc;
 }
 
+static __inline__ int __sk_del_node_init_rcu(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		hlist_del_init_rcu(&sk->sk_node);
+		return 1;
+	}
+	return 0;
+}
+
+static __inline__ int sk_del_node_init_rcu(struct sock *sk)
+{
+	int rc = __sk_del_node_init_rcu(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+
 static __inline__ void __sk_add_node(struct sock *sk, struct hlist_head *list)
 {
 	hlist_add_head(&sk->sk_node, list);
@@ -374,6 +395,17 @@ static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
 	__sk_add_node(sk, list);
 }
 
+static __inline__ void __sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+{
+	hlist_add_head_rcu(&sk->sk_node, list);
+}
+
+static __inline__ void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	__sk_add_node_rcu(sk, list);
+}
+
 static __inline__ void __sk_del_bind_node(struct sock *sk)
 {
 	__hlist_del(&sk->sk_bind_node);
@@ -387,6 +419,8 @@ static __inline__ void sk_add_bind_node(struct sock *sk,
 
 #define sk_for_each(__sk, node, list) \
 	hlist_for_each_entry(__sk, node, list, sk_node)
+#define sk_for_each_rcu(__sk, node, list) \
+	hlist_for_each_entry_rcu(__sk, node, list, sk_node)
 #define sk_for_each_from(__sk, node) \
 	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
 		hlist_for_each_entry_from(__sk, node, sk_node)
@@ -589,8 +623,9 @@ struct proto {
 	int			*sysctl_rmem;
 	int			max_header;
 
-	struct kmem_cache		*slab;
+	struct kmem_cache	*slab;
 	unsigned int		obj_size;
+	int			slab_flags;
 
 	atomic_t		*orphan_count;
 

commit 645ca708f936b2fbeb79e52d7823e3eb2c0905f8
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed Oct 29 01:41:45 2008 -0700

    udp: introduce struct udp_table and multiple spinlocks
    
    UDP sockets are hashed in a 128 slots hash table.
    
    This hash table is protected by *one* rwlock.
    
    This rwlock is readlocked each time an incoming UDP message is handled.
    
    This rwlock is writelocked each time a socket must be inserted in
    hash table (bind time), or deleted from this table (close time)
    
    This is not scalable on SMP machines :
    
    1) Even in read mode, lock() and unlock() are atomic operations and
     must dirty a contended cache line, shared by all cpus.
    
    2) A writer might be starved if many readers are 'in flight'. This can
     happen on a machine with some NIC receiving many UDP messages. User
     process can be delayed a long time at socket creation/dismantle time.
    
    This patch prepares RCU migration, by introducing 'struct udp_table
    and struct udp_hslot', and using one spinlock per chain, to reduce
    contention on central rwlock.
    
    Introducing one spinlock per chain reduces latencies, for port
    randomization on heavily loaded UDP servers. This also speedup
    bindings to specific ports.
    
    udp_lib_unhash() was uninlined, becoming to big.
    
    Some cleanups were done to ease review of following patch
    (RCUification of UDP Unicast lookups)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d6b750a25078..d200dfbe1ef6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -599,7 +599,7 @@ struct proto {
 
 	union {
 		struct inet_hashinfo	*hashinfo;
-		struct hlist_head	*udp_hash;
+		struct udp_table	*udp_table;
 		struct raw_hashinfo	*raw_hash;
 	} h;
 

commit def8b4faff5ca349beafbbfeb2c51f3602a6ef3a
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Oct 28 13:24:06 2008 -0700

    net: reduce structures when XFRM=n
    
    ifdef out
    * struct sk_buff::sp            (pointer)
    * struct dst_entry::xfrm        (pointer)
    * struct sock::sk_policy        (2 pointers)
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ada50c04d09f..d6b750a25078 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -229,7 +229,9 @@ struct sock {
 	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
+#ifdef CONFIG_XFRM
 	struct xfrm_policy	*sk_policy[2];
+#endif
 	rwlock_t		sk_dst_lock;
 	atomic_t		sk_rmem_alloc;
 	atomic_t		sk_wmem_alloc;

commit c57943a1c96214ee68f3890bb6772841ffbfd606
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 7 14:18:42 2008 -0700

    net: wrap sk->sk_backlog_rcv()
    
    Wrap calling sk->sk_backlog_rcv() in a function. This will allow extending the
    generic sk_backlog_rcv behaviour.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 18f96708f3a6..ada50c04d09f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -482,6 +482,11 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb->next = NULL;
 }
 
+static inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	return sk->sk_backlog_rcv(sk, skb);
+}
+
 #define sk_wait_event(__sk, __timeo, __condition)			\
 	({	int __rc;						\
 		release_sock(__sk);					\

commit 23542618deb77cfed312842fe8c41ed19fb16470
Author: KOVACS Krisztian <hidden@sch.bme.hu>
Date:   Tue Oct 7 12:41:01 2008 -0700

    inet: Don't lookup the socket if there's a socket attached to the skb
    
    Use the socket cached in the skb if it's present.
    
    Signed-off-by: KOVACS Krisztian <hidden@sch.bme.hu>
    Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 75a312d3888a..18f96708f3a6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1324,6 +1324,18 @@ static inline void sk_change_net(struct sock *sk, struct net *net)
 	sock_net_set(sk, hold_net(net));
 }
 
+static inline struct sock *skb_steal_sock(struct sk_buff *skb)
+{
+	if (unlikely(skb->sk)) {
+		struct sock *sk = skb->sk;
+
+		skb->destructor = NULL;
+		skb->sk = NULL;
+		return sk;
+	}
+	return NULL;
+}
+
 extern void sock_enable_timestamp(struct sock *sk);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);

commit af01d537463714e36e2c96d2da35902b76cd6827
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Aug 28 02:53:51 2008 -0700

    net: more #ifdef CONFIG_COMPAT
    
    All users of struct proto::compat_[gs]etsockopt and
    struct inet_connection_sock_af_ops::compat_[gs]etsockopt are under
    #ifdef already, so use it in structure definition too.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 06c5259aff30..75a312d3888a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -532,6 +532,7 @@ struct proto {
 	int			(*getsockopt)(struct sock *sk, int level, 
 					int optname, char __user *optval, 
 					int __user *option);  	 
+#ifdef CONFIG_COMPAT
 	int			(*compat_setsockopt)(struct sock *sk,
 					int level,
 					int optname, char __user *optval,
@@ -540,6 +541,7 @@ struct proto {
 					int level,
 					int optname, char __user *optval,
 					int __user *option);
+#endif
 	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
 					   struct msghdr *msg, size_t len);
 	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,

commit 5c52ba170f8167511bdb65b981f4582100c40675
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Jul 16 20:28:10 2008 -0700

    sock: add net to prot->enter_memory_pressure callback
    
    The tcp_enter_memory_pressure calls NET_INC_STATS, but doesn't
    have where to get the net from.
    
    I decided to add a sk argument, not the net itself, only to factor
    all the required sock_net(sk) calls inside the enter_memory_pressure
    callback itself.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3f4897ab432e..06c5259aff30 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -565,7 +565,7 @@ struct proto {
 #endif
 
 	/* Memory pressure */
-	void			(*enter_memory_pressure)(void);
+	void			(*enter_memory_pressure)(struct sock *sk);
 	atomic_t		*memory_allocated;	/* Current allocated memory. */
 	atomic_t		*sockets_allocated;	/* Current number of sockets. */
 	/*
@@ -1210,7 +1210,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 
 	page = alloc_pages(sk->sk_allocation, 0);
 	if (!page) {
-		sk->sk_prot->enter_memory_pressure();
+		sk->sk_prot->enter_memory_pressure(sk);
 		sk_stream_moderate_sndbuf(sk);
 	}
 	return page;

commit 972692e0db9b0a62329ca394062b58917ddbd03c
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 17 22:41:38 2008 -0700

    net: Add sk_set_socket() helper.
    
    In order to more easily grep for all things that set
    sk->sk_socket, add sk_set_socket() helper inline function.
    
    Suggested (although only half-seriously) by Evgeniy Polyakov.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a7c30412de66..3f4897ab432e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -990,6 +990,11 @@ static inline void sock_put(struct sock *sk)
 extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
 			  const int nested);
 
+static inline void sk_set_socket(struct sock *sk, struct socket *sock)
+{
+	sk->sk_socket = sock;
+}
+
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
  * Note that parent inode held reference count on this struct sock,
@@ -1001,7 +1006,7 @@ static inline void sock_orphan(struct sock *sk)
 {
 	write_lock_bh(&sk->sk_callback_lock);
 	sock_set_flag(sk, SOCK_DEAD);
-	sk->sk_socket = NULL;
+	sk_set_socket(sk, NULL);
 	sk->sk_sleep  = NULL;
 	write_unlock_bh(&sk->sk_callback_lock);
 }
@@ -1011,7 +1016,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	write_lock_bh(&sk->sk_callback_lock);
 	sk->sk_sleep = &parent->wait;
 	parent->sk = sk;
-	sk->sk_socket = parent;
+	sk_set_socket(sk, parent);
 	security_sock_graft(sk, parent);
 	write_unlock_bh(&sk->sk_callback_lock);
 }

commit cb61cb9b8b5ef6c2697d84e5015e314626eb2fba
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Jun 17 21:04:56 2008 -0700

    udp: sk_drops handling
    
    In commits 33c732c36169d7022ad7d6eb474b0c9be43a2dc1 ([IPV4]: Add raw
    drops counter) and a92aa318b4b369091fd80433c80e62838db8bc1c ([IPV6]:
    Add raw drops counter), Wang Chen added raw drops counter for
    /proc/net/raw & /proc/net/raw6
    
    This patch adds this capability to UDP sockets too (/proc/net/udp &
    /proc/net/udp6).
    
    This means that 'RcvbufErrors' errors found in /proc/net/snmp can be also
    be examined for each udp socket.
    
    # grep Udp: /proc/net/snmp
    Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors
    Udp: 23971006 75 899420 16390693 146348 0
    
    # cat /proc/net/udp
     sl  local_address rem_address   st tx_queue rx_queue tr tm->when retrnsmt  ---
    uid  timeout inode ref pointer drops
     75: 00000000:02CB 00000000:0000 07 00000000:00000000 00:00000000 00000000  ---
      0        0 2358 2 ffff81082a538c80 0
    111: 00000000:006F 00000000:0000 07 00000000:00000000 00:00000000 00000000  ---
      0        0 2286 2 ffff81042dd35c80 146348
    
    In this example, only port 111 (0x006F) was flooded by messages that
    user program could not read fast enough. 146348 messages were lost.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 83f74b11d09a..a7c30412de66 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -166,7 +166,7 @@ struct sock_common {
   *	@sk_err: last error
   *	@sk_err_soft: errors that don't cause failure but are the cause of a
   *		      persistent failure not just 'timed out'
-  *	@sk_drops: raw drops counter
+  *	@sk_drops: raw/udp drops counter
   *	@sk_ack_backlog: current listen backlog
   *	@sk_max_ack_backlog: listen backlog set in listen()
   *	@sk_priority: %SO_PRIORITY setting

commit 338db085518a8436cdecd33f7b52a06ec16d9ec1
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jun 17 01:09:00 2008 -0700

    net: Kill SOCK_SLEEP_PRE and SOCK_SLEEP_POST, no users.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0a80961a83c0..83f74b11d09a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1331,30 +1331,6 @@ extern int net_msg_warn;
 #define LIMIT_NETDEBUG(fmt, args...) \
 	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
 
-/*
- * Macros for sleeping on a socket. Use them like this:
- *
- * SOCK_SLEEP_PRE(sk)
- * if (condition)
- * 	schedule();
- * SOCK_SLEEP_POST(sk)
- *
- * N.B. These are now obsolete and were, afaik, only ever used in DECnet
- * and when the last use of them in DECnet has gone, I'm intending to
- * remove them.
- */
-
-#define SOCK_SLEEP_PRE(sk) 	{ struct task_struct *tsk = current; \
-				DECLARE_WAITQUEUE(wait, tsk); \
-				tsk->state = TASK_INTERRUPTIBLE; \
-				add_wait_queue((sk)->sk_sleep, &wait); \
-				release_sock(sk);
-
-#define SOCK_SLEEP_POST(sk)	tsk->state = TASK_RUNNING; \
-				remove_wait_queue((sk)->sk_sleep, &wait); \
-				lock_sock(sk); \
-				}
-
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 

commit 7d06b2e053d2d536348e3a0f6bb02982a41bea37
Author: Brian Haley <brian.haley@hp.com>
Date:   Sat Jun 14 17:04:49 2008 -0700

    net: change proto destroy method to return void
    
    Change struct proto destroy function pointer to return void.  Noticed
    by Al Viro.
    
    Signed-off-by: Brian Haley <brian.haley@hp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index dc42b44c2aa1..0a80961a83c0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -524,7 +524,7 @@ struct proto {
 	int			(*ioctl)(struct sock *sk, int cmd,
 					 unsigned long arg);
 	int			(*init)(struct sock *sk);
-	int			(*destroy)(struct sock *sk);
+	void			(*destroy)(struct sock *sk);
 	void			(*shutdown)(struct sock *sk, int how);
 	int			(*setsockopt)(struct sock *sk, int level, 
 					int optname, char __user *optval,

commit 65a18ec58e5e6186103f62f720acea94dfb26f4e
Author: Denis V. Lunev <den@openvz.org>
Date:   Wed Apr 16 01:59:46 2008 -0700

    [NETNS]: Add netns refcnt debug for kernel sockets.
    
    Protocol control sockets and netlink kernel sockets should not prevent the
    namespace stop request. They are initialized and disposed in a special way by
    sk_change_net/sk_release_kernel.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 09255eae93e9..dc42b44c2aa1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1314,7 +1314,7 @@ void sock_net_set(struct sock *sk, struct net *net)
 static inline void sk_change_net(struct sock *sk, struct net *net)
 {
 	put_net(sock_net(sk));
-	sock_net_set(sk, net);
+	sock_net_set(sk, hold_net(net));
 }
 
 extern void sock_enable_timestamp(struct sock *sk);

commit 43db6d65e0ef943a361cb91f8baa49132009227b
Author: Stephen Hemminger <shemminger@vyatta.com>
Date:   Thu Apr 10 01:43:09 2008 -0700

    socket: sk_filter deinline
    
    The sk_filter function is too big to be inlined. This saves 2296 bytes
    of text on allyesconfig.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f4fdd101c9a2..09255eae93e9 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -927,41 +927,6 @@ extern void sk_common_release(struct sock *sk);
 /* Initialise core socket variables */
 extern void sock_init_data(struct socket *sock, struct sock *sk);
 
-/**
- *	sk_filter - run a packet through a socket filter
- *	@sk: sock associated with &sk_buff
- *	@skb: buffer to filter
- *	@needlock: set to 1 if the sock is not locked by caller.
- *
- * Run the filter code and then cut skb->data to correct size returned by
- * sk_run_filter. If pkt_len is 0 we toss packet. If skb->len is smaller
- * than pkt_len we keep whole skb->data. This is the socket level
- * wrapper to sk_run_filter. It returns 0 if the packet should
- * be accepted or -EPERM if the packet should be tossed.
- *
- */
-
-static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
-{
-	int err;
-	struct sk_filter *filter;
-	
-	err = security_sock_rcv_skb(sk, skb);
-	if (err)
-		return err;
-	
-	rcu_read_lock_bh();
-	filter = rcu_dereference(sk->sk_filter);
-	if (filter) {
-		unsigned int pkt_len = sk_run_filter(skb, filter->insns,
-				filter->len);
-		err = pkt_len ? pskb_trim(skb, pkt_len) : -EPERM;
-	}
- 	rcu_read_unlock_bh();
-
-	return err;
-}
-
 /**
  *	sk_filter_release: Release a socket filter
  *	@sk: socket

commit c29a0bc4dfc4d833eb702b1929cec96a3eeb9f7a
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Mon Mar 31 19:41:46 2008 -0700

    [SOCK][NETNS]: Add a struct net argument to sock_prot_inuse_add and _get.
    
    This counter is about to become per-proto-and-per-net, so we'll need
    two arguments to determine which cell in this "table" to work with.
    
    All the places, but proc already pass proper net to it - proc will be
    tuned a bit later.
    
    Some indentation with spaces in proc files is done to keep the file
    coding style consistent.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2a3344f666aa..f4fdd101c9a2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -635,10 +635,11 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 
 #ifdef CONFIG_PROC_FS
 /* Called with local bh disabled */
-extern void sock_prot_inuse_add(struct proto *prot, int inc);
-extern int sock_prot_inuse_get(struct proto *proto);
+extern void sock_prot_inuse_add(struct net *net, struct proto *prot, int inc);
+extern int sock_prot_inuse_get(struct net *net, struct proto *proto);
 #else
-static void inline sock_prot_inuse_add(struct proto *prot, int inc)
+static void inline sock_prot_inuse_add(struct net *net, struct proto *prot,
+		int inc)
 {
 }
 #endif

commit bdcde3d71a67e97f25e851f3ca97c9bb5ef03e7f
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Mar 28 16:39:33 2008 -0700

    [SOCK]: Drop inuse pcounter from struct proto (v2).
    
    An uppercut - do not use the pcounter on struct proto.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1f4294252dd7..2a3344f666aa 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -47,7 +47,6 @@
 #include <linux/module.h>
 #include <linux/lockdep.h>
 #include <linux/netdevice.h>
-#include <linux/pcounter.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
 #include <linux/mm.h>
 #include <linux/security.h>
@@ -563,7 +562,6 @@ struct proto {
 	/* Keeping track of sockets in use */
 #ifdef CONFIG_PROC_FS
 	unsigned int		inuse_idx;
-	struct pcounter		inuse;
 #endif
 
 	/* Memory pressure */
@@ -636,14 +634,10 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 
 
 #ifdef CONFIG_PROC_FS
-# define DEFINE_PROTO_INUSE(NAME) DEFINE_PCOUNTER(NAME)
-# define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
 /* Called with local bh disabled */
 extern void sock_prot_inuse_add(struct proto *prot, int inc);
 extern int sock_prot_inuse_get(struct proto *proto);
 #else
-# define DEFINE_PROTO_INUSE(NAME)
-# define REF_PROTO_INUSE(NAME)
 static void inline sock_prot_inuse_add(struct proto *prot, int inc)
 {
 }

commit 60e7663d462af3994f292cb3691ea4f7371a9220
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Mar 28 16:39:10 2008 -0700

    [SOCK]: Drop per-proto inuse init and fre functions (v2).
    
    Constructive part of the set is finished here. We have to remove the
    pcounter, so start with its init and free functions.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ebf9552664b2..1f4294252dd7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -640,31 +640,13 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 # define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
 /* Called with local bh disabled */
 extern void sock_prot_inuse_add(struct proto *prot, int inc);
-
-static inline int sock_prot_inuse_init(struct proto *proto)
-{
-	return pcounter_alloc(&proto->inuse);
-}
-
 extern int sock_prot_inuse_get(struct proto *proto);
-
-static inline void sock_prot_inuse_free(struct proto *proto)
-{
-	pcounter_free(&proto->inuse);
-}
 #else
 # define DEFINE_PROTO_INUSE(NAME)
 # define REF_PROTO_INUSE(NAME)
 static void inline sock_prot_inuse_add(struct proto *prot, int inc)
 {
 }
-static int inline sock_prot_inuse_init(struct proto *proto)
-{
-	return 0;
-}
-static void inline sock_prot_inuse_free(struct proto *proto)
-{
-}
 #endif
 
 

commit 1338d466d9c3f8a65cc6d83c629cd906f2a989f8
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Mar 28 16:38:43 2008 -0700

    [SOCK]: Introduce a percpu inuse counters array (v2).
    
    And redirect sock_prot_inuse_add and _get to use one.
    
    As far as the dereferences are concerned. Before the patch we made
    1 dereference to proto->inuse.add call, the call itself and then
    called the __get_cpu_var() on a static variable. After the patch we
    make a direct call, then one dereference to proto->inuse_idx and
    then the same __get_cpu_var() on a still static variable. So this
    patch doesn't seem to produce performance penalty on SMP.
    
    This is not per-net yet, but I will deliberately make NET_NS=y case
    separated from NET_NS=n one, since it'll cost us one-or-two more
    dereferences to get the struct net and the inuse counter.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index abc6341f536f..ebf9552664b2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -639,18 +639,15 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 # define DEFINE_PROTO_INUSE(NAME) DEFINE_PCOUNTER(NAME)
 # define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
 /* Called with local bh disabled */
-static inline void sock_prot_inuse_add(struct proto *prot, int inc)
-{
-	pcounter_add(&prot->inuse, inc);
-}
+extern void sock_prot_inuse_add(struct proto *prot, int inc);
+
 static inline int sock_prot_inuse_init(struct proto *proto)
 {
 	return pcounter_alloc(&proto->inuse);
 }
-static inline int sock_prot_inuse_get(struct proto *proto)
-{
-	return pcounter_getval(&proto->inuse);
-}
+
+extern int sock_prot_inuse_get(struct proto *proto);
+
 static inline void sock_prot_inuse_free(struct proto *proto)
 {
 	pcounter_free(&proto->inuse);

commit 13ff3d6fa4e6d8b6ee7c64245a0078e6a0e6f977
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Mar 28 16:38:17 2008 -0700

    [SOCK]: Enumerate struct proto-s to facilitate percpu inuse accounting (v2).
    
    The inuse counters are going to become a per-cpu array.  Introduce an
    index for this array on the struct proto.
    
    To handle the case of proto register-unregister-register loop the
    bitmap is used. All its bits manipulations are protected with
    proto_list_lock and a sanity check for the bitmap being exhausted is
    also added.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1c9d059223ee..abc6341f536f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -562,6 +562,7 @@ struct proto {
 
 	/* Keeping track of sockets in use */
 #ifdef CONFIG_PROC_FS
+	unsigned int		inuse_idx;
 	struct pcounter		inuse;
 #endif
 

commit f5aa23fd49063745f85644dd7a9330acd706add6
Author: Denis V. Lunev <den@openvz.org>
Date:   Wed Mar 26 00:48:17 2008 -0700

    [NETNS]: Compilation warnings under CONFIG_NET_NS.
    
    Recent commits from YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    have been introduced a several compilation warnings
    'assignment discards qualifiers from pointer target type'
    due to extra const modifier in the inline call parameters of
    {dev|sock|twsk}_net_set.
    
    Drop it.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7e0d4a0c4d12..1c9d059223ee 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1358,7 +1358,7 @@ struct net *sock_net(const struct sock *sk)
 }
 
 static inline
-void sock_net_set(struct sock *sk, const struct net *net)
+void sock_net_set(struct sock *sk, struct net *net)
 {
 #ifdef CONFIG_NET_NS
 	sk->sk_net = net;

commit 3b1e0a655f8eba44ab1ee2a1068d169ccfb853b9
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Wed Mar 26 02:26:21 2008 +0900

    [NET] NETNS: Omit sock->sk_net without CONFIG_NET_NS.
    
    Introduce per-sock inlines: sock_net(), sock_net_set()
    and per-inet_timewait_sock inlines: twsk_net(), twsk_net_set().
    Without CONFIG_NET_NS, no namespace other than &init_net exists.
    Let's explicitly define them to help compiler optimizations.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index b433b1ed203d..7e0d4a0c4d12 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -126,7 +126,9 @@ struct sock_common {
 	atomic_t		skc_refcnt;
 	unsigned int		skc_hash;
 	struct proto		*skc_prot;
+#ifdef CONFIG_NET_NS
 	struct net	 	*skc_net;
+#endif
 };
 
 /**
@@ -1345,6 +1347,24 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
 }
 #endif
 
+static inline
+struct net *sock_net(const struct sock *sk)
+{
+#ifdef CONFIG_NET_NS
+	return sk->sk_net;
+#else
+	return &init_net;
+#endif
+}
+
+static inline
+void sock_net_set(struct sock *sk, const struct net *net)
+{
+#ifdef CONFIG_NET_NS
+	sk->sk_net = net;
+#endif
+}
+
 /*
  * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
  * They should not hold a referrence to a namespace in order to allow
@@ -1353,8 +1373,8 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
  */
 static inline void sk_change_net(struct sock *sk, struct net *net)
 {
-	put_net(sk->sk_net);
-	sk->sk_net = net;
+	put_net(sock_net(sk));
+	sock_net_set(sk, net);
 }
 
 extern void sock_enable_timestamp(struct sock *sk);

commit fc8717baa8f52dd8d1b90df9008300ef3ec794ed
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Sat Mar 22 16:56:51 2008 -0700

    [RAW]: Add raw_hashinfo member on struct proto.
    
    Sorry for the patch sequence confusion :| but I found that the similar
    thing can be done for raw sockets easily too late.
    
    Expand the proto.h union with the raw_hashinfo member and use it in
    raw_prot and rawv6_prot. This allows to drop the protocol specific
    versions of hash and unhash callbacks.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c3175c400b79..b433b1ed203d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -504,6 +504,7 @@ extern int sk_wait_data(struct sock *sk, long *timeo);
 struct request_sock_ops;
 struct timewait_sock_ops;
 struct inet_hashinfo;
+struct raw_hashinfo;
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
@@ -589,6 +590,7 @@ struct proto {
 	union {
 		struct inet_hashinfo	*hashinfo;
 		struct hlist_head	*udp_hash;
+		struct raw_hashinfo	*raw_hash;
 	} h;
 
 	struct module		*owner;

commit 39d8cda76cfb1178455f9d196b39e773878e6c05
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Sat Mar 22 16:50:58 2008 -0700

    [SOCK]: Add udp_hash member to struct proto.
    
    Inspired by the commit ab1e0a13 ([SOCK] proto: Add hashinfo member to
    struct proto) from Arnaldo, I made similar thing for UDP/-Lite IPv4
    and -v6 protocols.
    
    The result is not that exciting, but it removes some levels of
    indirection in udpxxx_get_port and saves some space in code and text.
    
    The first step is to union existing hashinfo and new udp_hash on the
    struct proto and give a name to this union, since future initialization
    of tcpxxx_prot, dccp_vx_protinfo and udpxxx_protinfo will cause gcc
    warning about inability to initialize anonymous member this way.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b89680d2693b..c3175c400b79 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -586,7 +586,10 @@ struct proto {
 	struct request_sock_ops	*rsk_prot;
 	struct timewait_sock_ops *twsk_prot;
 
-	struct inet_hashinfo	*hashinfo;
+	union {
+		struct inet_hashinfo	*hashinfo;
+		struct hlist_head	*udp_hash;
+	} h;
 
 	struct module		*owner;
 

commit 4cd9029d25f6612302f82634620f107c65f790b1
Author: Stephen Hemminger <shemminger@vyatta.com>
Date:   Fri Mar 21 15:54:53 2008 -0700

    socket: SOCK_DEBUG type checking
    
    Use the inline trick (same as pr_debug) to get checking of debug
    statements even if no code is generated.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8358fff002eb..b89680d2693b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -70,7 +70,11 @@
 #define SOCK_DEBUG(sk, msg...) do { if ((sk) && sock_flag((sk), SOCK_DBG)) \
 					printk(KERN_DEBUG msg); } while (0)
 #else
-#define SOCK_DEBUG(sk, msg...) do { } while (0)
+/* Validate arguments and do nothing */
+static void inline int __attribute__ ((format (printf, 2, 3)))
+SOCK_DEBUG(struct sock *sk, const char *msg, ...)
+{
+}
 #endif
 
 /* This is the per-socket lock.  The spinlock provides a synchronization

commit 82cc1a7a56872056af0ead6c7d695aa223f36695
Author: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
Date:   Fri Mar 21 03:43:19 2008 -0700

    [NET]: Add per-connection option to set max TSO frame size
    
    Update: My mailer ate one of Jarek's feedback mails...  Fixed the
    parameter in netif_set_gso_max_size() to be u32, not u16.  Fixed the
    whitespace issue due to a patch import botch.  Changed the types from
    u32 to unsigned int to be more consistent with other variables in the
    area.  Also brought the patch up to the latest net-2.6.26 tree.
    
    Update: Made gso_max_size container 32 bits, not 16.  Moved the
    location of gso_max_size within netdev to be less hotpath.  Made more
    consistent names between the sock and netdev layers, and added a
    define for the max GSO size.
    
    Update: Respun for net-2.6.26 tree.
    
    Update: changed max_gso_frame_size and sk_gso_max_size from signed to
    unsigned - thanks Stephen!
    
    This patch adds the ability for device drivers to control the size of
    the TSO frames being sent to them, per TCP connection.  By setting the
    netdevice's gso_max_size value, the socket layer will set the GSO
    frame size based on that value.  This will propogate into the TCP
    layer, and send TSO's of that size to the hardware.
    
    This can be desirable to help tune the bursty nature of TSO on a
    per-adapter basis, where one may have 1 GbE and 10 GbE devices
    coexisting in a system, one running multiqueue and the other not, etc.
    
    This can also be desirable for devices that cannot support full 64 KB
    TSO's, but still want to benefit from some level of segmentation
    offloading.
    
    Signed-off-by: Peter P Waskiewicz Jr <peter.p.waskiewicz.jr@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 39112e75411c..8358fff002eb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -151,6 +151,7 @@ struct sock_common {
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
+  *	@sk_gso_max_size: Maximum GSO segment size to build
   *	@sk_lingertime: %SO_LINGER l_linger setting
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
@@ -237,6 +238,7 @@ struct sock {
 	gfp_t			sk_allocation;
 	int			sk_route_caps;
 	int			sk_gso_type;
+	unsigned int		sk_gso_max_size;
 	int			sk_rcvlowat;
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;

commit edf0208702007ec1f6a36756fdd005f771a4cf17
Author: Denis V. Lunev <den@openvz.org>
Date:   Fri Feb 29 11:18:32 2008 -0800

    [NET]: Make netlink_kernel_release publically available as sk_release_kernel.
    
    This staff will be needed for non-netlink kernel sockets, which should
    also not pin a namespace like tcp_socket and icmp_socket.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Acked-by: Daniel Lezcano <dlezcano@fr.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index fd9876087651..39112e75411c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -850,6 +850,7 @@ extern struct sock		*sk_alloc(struct net *net, int family,
 					  gfp_t priority,
 					  struct proto *prot);
 extern void			sk_free(struct sock *sk);
+extern void			sk_release_kernel(struct sock *sk);
 extern struct sock		*sk_clone(const struct sock *sk,
 					  const gfp_t priority);
 
@@ -1333,6 +1334,18 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
 }
 #endif
 
+/*
+ * Kernel sockets, f.e. rtnl or icmp_socket, are a part of a namespace.
+ * They should not hold a referrence to a namespace in order to allow
+ * to stop it.
+ * Sockets after sk_change_net should be released using sk_release_kernel
+ */
+static inline void sk_change_net(struct sock *sk, struct net *net)
+{
+	put_net(sk->sk_net);
+	sk->sk_net = net;
+}
+
 extern void sock_enable_timestamp(struct sock *sk);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);

commit 31729363418ea25b01aa9410838c38e36792e44c
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon Feb 18 20:52:13 2008 -0800

    net: fix kernel-doc warnings in header files
    
    Add missing structure kernel-doc descriptions to sock.h & skbuff.h
    to fix kernel-doc warnings.
    
    (I think that Stephen H. sent a similar patch, but I can't find it.
    I just want to kill the warnings, with either patch.)
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8a7889b35810..fd9876087651 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -180,6 +180,7 @@ struct sock_common {
   *	@sk_sndmsg_off: cached offset for sendmsg
   *	@sk_send_head: front of stuff to transmit
   *	@sk_security: used by security modules
+  *	@sk_mark: generic packet mark
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
   *	@sk_data_ready: callback to indicate there is data to be processed

commit ab1e0a13d70299e792fd0527cefd070c1405fa5b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sun Feb 3 04:06:04 2008 -0800

    [SOCK] proto: Add hashinfo member to struct proto
    
    This way we can remove TCP and DCCP specific versions of
    
    sk->sk_prot->get_port: both v4 and v6 use inet_csk_get_port
    sk->sk_prot->hash:     inet_hash is directly used, only v6 need
                           a specific version to deal with mapped sockets
    sk->sk_prot->unhash:   both v4 and v6 use inet_hash directly
    
    struct inet_connection_sock_af_ops also gets a new member, bind_conflict, so
    that inet_csk_get_port can find the per family routine.
    
    Now only the lookup routines receive as a parameter a struct inet_hashtable.
    
    With this we further reuse code, reducing the difference among INET transport
    protocols.
    
    Eventually work has to be done on UDP and SCTP to make them share this
    infrastructure and get as a bonus inet_diag interfaces so that iproute can be
    used with these protocols.
    
    net-2.6/net/ipv4/inet_hashtables.c:
      struct proto                       |   +8
      struct inet_connection_sock_af_ops |   +8
     2 structs changed
      __inet_hash_nolisten               |  +18
      __inet_hash                        | -210
      inet_put_port                      |   +8
      inet_bind_bucket_create            |   +1
      __inet_hash_connect                |   -8
     5 functions changed, 27 bytes added, 218 bytes removed, diff: -191
    
    net-2.6/net/core/sock.c:
      proto_seq_show                     |   +3
     1 function changed, 3 bytes added, diff: +3
    
    net-2.6/net/ipv4/inet_connection_sock.c:
      inet_csk_get_port                  |  +15
     1 function changed, 15 bytes added, diff: +15
    
    net-2.6/net/ipv4/tcp.c:
      tcp_set_state                      |   -7
     1 function changed, 7 bytes removed, diff: -7
    
    net-2.6/net/ipv4/tcp_ipv4.c:
      tcp_v4_get_port                    |  -31
      tcp_v4_hash                        |  -48
      tcp_v4_destroy_sock                |   -7
      tcp_v4_syn_recv_sock               |   -2
      tcp_unhash                         | -179
     5 functions changed, 267 bytes removed, diff: -267
    
    net-2.6/net/ipv6/inet6_hashtables.c:
      __inet6_hash |   +8
     1 function changed, 8 bytes added, diff: +8
    
    net-2.6/net/ipv4/inet_hashtables.c:
      inet_unhash                        | +190
      inet_hash                          | +242
     2 functions changed, 432 bytes added, diff: +432
    
    vmlinux:
     16 functions changed, 485 bytes added, 492 bytes removed, diff: -7
    
    /home/acme/git/net-2.6/net/ipv6/tcp_ipv6.c:
      tcp_v6_get_port                    |  -31
      tcp_v6_hash                        |   -7
      tcp_v6_syn_recv_sock               |   -9
     3 functions changed, 47 bytes removed, diff: -47
    
    /home/acme/git/net-2.6/net/dccp/proto.c:
      dccp_destroy_sock                  |   -7
      dccp_unhash                        | -179
      dccp_hash                          |  -49
      dccp_set_state                     |   -7
      dccp_done                          |   +1
     5 functions changed, 1 bytes added, 242 bytes removed, diff: -241
    
    /home/acme/git/net-2.6/net/dccp/ipv4.c:
      dccp_v4_get_port                   |  -31
      dccp_v4_request_recv_sock          |   -2
     2 functions changed, 33 bytes removed, diff: -33
    
    /home/acme/git/net-2.6/net/dccp/ipv6.c:
      dccp_v6_get_port                   |  -31
      dccp_v6_hash                       |   -7
      dccp_v6_request_recv_sock          |   +5
     3 functions changed, 5 bytes added, 38 bytes removed, diff: -33
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e3fb4c047f4c..8a7889b35810 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -496,6 +496,7 @@ extern int sk_wait_data(struct sock *sk, long *timeo);
 
 struct request_sock_ops;
 struct timewait_sock_ops;
+struct inet_hashinfo;
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
@@ -578,6 +579,8 @@ struct proto {
 	struct request_sock_ops	*rsk_prot;
 	struct timewait_sock_ops *twsk_prot;
 
+	struct inet_hashinfo	*hashinfo;
+
 	struct module		*owner;
 
 	char			name[32];

commit 4a19ec5800fc3bb64e2d87c4d9fdd9e636086fe0
Author: Laszlo Attila Toth <panther@balabit.hu>
Date:   Wed Jan 30 19:08:16 2008 -0800

    [NET]: Introducing socket mark socket option.
    
    A userspace program may wish to set the mark for each packets its send
    without using the netfilter MARK target. Changing the mark can be used
    for mark based routing without netfilter or for packet filtering.
    
    It requires CAP_NET_ADMIN capability.
    
    Signed-off-by: Laszlo Attila Toth <panther@balabit.hu>
    Acked-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 902324488d0f..e3fb4c047f4c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -262,6 +262,8 @@ struct sock {
 	__u32			sk_sndmsg_off;
 	int			sk_write_pending;
 	void			*sk_security;
+	__u32			sk_mark;
+	/* XXX 4 bytes hole on 64 bit */
 	void			(*sk_state_change)(struct sock *sk);
 	void			(*sk_data_ready)(struct sock *sk, int bytes);
 	void			(*sk_write_space)(struct sock *sk);

commit 9993e7d313e80bdc005d09c7def91903e0068f07
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jan 10 21:56:38 2008 -0800

    [TCP]: Do not purge sk_forward_alloc entirely in tcp_delack_timer().
    
    Otherwise we beat heavily on the global tcp_memory atomics
    when all of the sockets in the system are slowly sending
    perioding packet clumps.
    
    Noticed and suggested by Eric Dumazet.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 786fae858e77..902324488d0f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -759,6 +759,14 @@ static inline void sk_mem_reclaim(struct sock *sk)
 		__sk_mem_reclaim(sk);
 }
 
+static inline void sk_mem_reclaim_partial(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc > SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
 static inline void sk_mem_charge(struct sock *sk, int size)
 {
 	if (!sk_has_account(sk))

commit 65f7651788e18fadb2fbb7276af935d7871e1803
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Jan 3 20:46:48 2008 -0800

    [NET]: prot_inuse cleanups and optimizations
    
    1) Cleanups (all functions are prefixed by sock_prot_inuse)
    
    sock_prot_inc_use(prot) -> sock_prot_inuse_add(prot,-1)
    sock_prot_dec_use(prot) -> sock_prot_inuse_add(prot,-1)
    sock_prot_inuse()       -> sock_prot_inuse_get()
    
    New functions :
    
    sock_prot_inuse_init() and sock_prot_inuse_free() to abstract pcounter use.
    
    2) if CONFIG_PROC_FS=n, we can zap 'inuse' member from "struct proto",
    since nobody wants to read the inuse value.
    
    This saves 1372 bytes on i386/SMP and some cpu cycles.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 3d938f6c6725..786fae858e77 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -548,7 +548,9 @@ struct proto {
 	int			(*get_port)(struct sock *sk, unsigned short snum);
 
 	/* Keeping track of sockets in use */
+#ifdef CONFIG_PROC_FS
 	struct pcounter		inuse;
+#endif
 
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(void);
@@ -584,9 +586,6 @@ struct proto {
 #endif
 };
 
-#define DEFINE_PROTO_INUSE(NAME) DEFINE_PCOUNTER(NAME)
-#define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
-
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
 
@@ -615,21 +614,42 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 #define sk_refcnt_debug_release(sk) do { } while (0)
 #endif /* SOCK_REFCNT_DEBUG */
 
+
+#ifdef CONFIG_PROC_FS
+# define DEFINE_PROTO_INUSE(NAME) DEFINE_PCOUNTER(NAME)
+# define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
 /* Called with local bh disabled */
-static __inline__ void sock_prot_inc_use(struct proto *prot)
+static inline void sock_prot_inuse_add(struct proto *prot, int inc)
 {
-	pcounter_add(&prot->inuse, 1);
+	pcounter_add(&prot->inuse, inc);
 }
-
-static __inline__ void sock_prot_dec_use(struct proto *prot)
+static inline int sock_prot_inuse_init(struct proto *proto)
 {
-	pcounter_add(&prot->inuse, -1);
+	return pcounter_alloc(&proto->inuse);
 }
-
-static __inline__ int sock_prot_inuse(struct proto *proto)
+static inline int sock_prot_inuse_get(struct proto *proto)
 {
 	return pcounter_getval(&proto->inuse);
 }
+static inline void sock_prot_inuse_free(struct proto *proto)
+{
+	pcounter_free(&proto->inuse);
+}
+#else
+# define DEFINE_PROTO_INUSE(NAME)
+# define REF_PROTO_INUSE(NAME)
+static void inline sock_prot_inuse_add(struct proto *prot, int inc)
+{
+}
+static int inline sock_prot_inuse_init(struct proto *proto)
+{
+	return 0;
+}
+static void inline sock_prot_inuse_free(struct proto *proto)
+{
+}
+#endif
+
 
 /* With per-bucket locks this operation is not-atomic, so that
  * this version is not worse.

commit 3ab224be6d69de912ee21302745ea45a99274dbc
Author: Hideo Aoki <haoki@redhat.com>
Date:   Mon Dec 31 00:11:19 2007 -0800

    [NET] CORE: Introducing new memory accounting interface.
    
    This patch introduces new memory accounting functions for each network
    protocol. Most of them are renamed from memory accounting functions
    for stream protocols. At the same time, some stream memory accounting
    functions are removed since other functions do same thing.
    
    Renaming:
            sk_stream_free_skb()            ->      sk_wmem_free_skb()
            __sk_stream_mem_reclaim()       ->      __sk_mem_reclaim()
            sk_stream_mem_reclaim()         ->      sk_mem_reclaim()
            sk_stream_mem_schedule          ->      __sk_mem_schedule()
            sk_stream_pages()               ->      sk_mem_pages()
            sk_stream_rmem_schedule()       ->      sk_rmem_schedule()
            sk_stream_wmem_schedule()       ->      sk_wmem_schedule()
            sk_charge_skb()                 ->      sk_mem_charge()
    
    Removeing
            sk_stream_rfree():      consolidates into sock_rfree()
            sk_stream_set_owner_r(): consolidates into skb_set_owner_r()
            sk_stream_mem_schedule()
    
    The following functions are added.
            sk_has_account(): check if the protocol supports accounting
            sk_mem_uncharge(): do the opposite of sk_mem_charge()
    
    In addition, to achieve consolidation, updating sk_wmem_queued is
    removed from sk_mem_charge().
    
    Next, to consolidate memory accounting functions, this patch adds
    memory accounting calls to network core functions. Moreover, present
    memory accounting call is renamed to new accounting call.
    
    Finally we replace present memory accounting calls with new interface
    in TCP and SCTP.
    
    Signed-off-by: Takahiro Yasui <tyasui@redhat.com>
    Signed-off-by: Hideo Aoki <haoki@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d27ba6fdd039..3d938f6c6725 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -460,25 +460,6 @@ static inline int sk_stream_memory_free(struct sock *sk)
 	return sk->sk_wmem_queued < sk->sk_sndbuf;
 }
 
-extern void sk_stream_rfree(struct sk_buff *skb);
-
-static inline void sk_stream_set_owner_r(struct sk_buff *skb, struct sock *sk)
-{
-	skb->sk = sk;
-	skb->destructor = sk_stream_rfree;
-	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
-	sk->sk_forward_alloc -= skb->truesize;
-}
-
-static inline void sk_stream_free_skb(struct sock *sk, struct sk_buff *skb)
-{
-	skb_truesize_check(skb);
-	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
-	sk->sk_wmem_queued   -= skb->truesize;
-	sk->sk_forward_alloc += skb->truesize;
-	__kfree_skb(skb);
-}
-
 /* The per-socket spinlock must be held here. */
 static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 {
@@ -576,7 +557,7 @@ struct proto {
 	/*
 	 * Pressure flag: try to collapse.
 	 * Technical note: it is used by multiple contexts non atomically.
-	 * All the sk_stream_mem_schedule() is of this nature: accounting
+	 * All the __sk_mem_schedule() is of this nature: accounting
 	 * is strict, actions are advisory and have some latency.
 	 */
 	int			*memory_pressure;
@@ -712,33 +693,73 @@ static inline struct inode *SOCK_INODE(struct socket *socket)
 	return &container_of(socket, struct socket_alloc, socket)->vfs_inode;
 }
 
-extern void __sk_stream_mem_reclaim(struct sock *sk);
-extern int sk_stream_mem_schedule(struct sock *sk, int size, int kind);
+/*
+ * Functions for memory accounting
+ */
+extern int __sk_mem_schedule(struct sock *sk, int size, int kind);
+extern void __sk_mem_reclaim(struct sock *sk);
 
-#define SK_STREAM_MEM_QUANTUM ((int)PAGE_SIZE)
-#define SK_STREAM_MEM_QUANTUM_SHIFT ilog2(SK_STREAM_MEM_QUANTUM)
+#define SK_MEM_QUANTUM ((int)PAGE_SIZE)
+#define SK_MEM_QUANTUM_SHIFT ilog2(SK_MEM_QUANTUM)
+#define SK_MEM_SEND	0
+#define SK_MEM_RECV	1
 
-static inline int sk_stream_pages(int amt)
+static inline int sk_mem_pages(int amt)
 {
-	return (amt + SK_STREAM_MEM_QUANTUM - 1) >> SK_STREAM_MEM_QUANTUM_SHIFT;
+	return (amt + SK_MEM_QUANTUM - 1) >> SK_MEM_QUANTUM_SHIFT;
 }
 
-static inline void sk_stream_mem_reclaim(struct sock *sk)
+static inline int sk_has_account(struct sock *sk)
 {
-	if (sk->sk_forward_alloc >= SK_STREAM_MEM_QUANTUM)
-		__sk_stream_mem_reclaim(sk);
+	/* return true if protocol supports memory accounting */
+	return !!sk->sk_prot->memory_allocated;
 }
 
-static inline int sk_stream_rmem_schedule(struct sock *sk, struct sk_buff *skb)
+static inline int sk_wmem_schedule(struct sock *sk, int size)
 {
-	return (int)skb->truesize <= sk->sk_forward_alloc ||
-		sk_stream_mem_schedule(sk, skb->truesize, 1);
+	if (!sk_has_account(sk))
+		return 1;
+	return size <= sk->sk_forward_alloc ||
+		__sk_mem_schedule(sk, size, SK_MEM_SEND);
 }
 
-static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
+static inline int sk_rmem_schedule(struct sock *sk, int size)
 {
+	if (!sk_has_account(sk))
+		return 1;
 	return size <= sk->sk_forward_alloc ||
-	       sk_stream_mem_schedule(sk, size, 0);
+		__sk_mem_schedule(sk, size, SK_MEM_RECV);
+}
+
+static inline void sk_mem_reclaim(struct sock *sk)
+{
+	if (!sk_has_account(sk))
+		return;
+	if (sk->sk_forward_alloc >= SK_MEM_QUANTUM)
+		__sk_mem_reclaim(sk);
+}
+
+static inline void sk_mem_charge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc -= size;
+}
+
+static inline void sk_mem_uncharge(struct sock *sk, int size)
+{
+	if (!sk_has_account(sk))
+		return;
+	sk->sk_forward_alloc += size;
+}
+
+static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
+{
+	skb_truesize_check(skb);
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued -= skb->truesize;
+	sk_mem_uncharge(sk, skb->truesize);
+	__kfree_skb(skb);
 }
 
 /* Used by processes to "lock" a socket state, so that
@@ -1076,12 +1097,6 @@ static inline int sk_can_gso(const struct sock *sk)
 
 extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
 
-static inline void sk_charge_skb(struct sock *sk, struct sk_buff *skb)
-{
-	sk->sk_wmem_queued   += skb->truesize;
-	sk->sk_forward_alloc -= skb->truesize;
-}
-
 static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 				   struct sk_buff *skb, struct page *page,
 				   int off, int copy)
@@ -1101,7 +1116,7 @@ static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 	skb->data_len	     += copy;
 	skb->truesize	     += copy;
 	sk->sk_wmem_queued   += copy;
-	sk->sk_forward_alloc -= copy;
+	sk_mem_charge(sk, copy);
 	return 0;
 }
 
@@ -1127,6 +1142,7 @@ static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
 	skb->sk = sk;
 	skb->destructor = sock_rfree;
 	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+	sk_mem_charge(sk, skb->truesize);
 }
 
 extern void sk_reset_timer(struct sock *sk, struct timer_list* timer,

commit 21371f768bf7127ee45bfaadd17899df6a439e8f
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Mon Dec 24 20:57:56 2007 -0800

    [SOCK] Avoid divides in sk_stream_pages() and __sk_stream_mem_reclaim()
    
    sk_forward_alloc being signed, we should take care of divides by
    SK_STREAM_MEM_QUANTUM we do in sk_stream_pages() and
    __sk_stream_mem_reclaim()
    
    This patchs introduces SK_STREAM_MEM_QUANTUM_SHIFT, defined
    as ilog2(SK_STREAM_MEM_QUANTUM), to be able to use right
    shifts instead of plain divides.
    
    This should help compiler to choose right shifts instead of
    expensive divides (as seen with CONFIG_CC_OPTIMIZE_FOR_SIZE=y on x86)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e178b49eefbb..d27ba6fdd039 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -716,10 +716,11 @@ extern void __sk_stream_mem_reclaim(struct sock *sk);
 extern int sk_stream_mem_schedule(struct sock *sk, int size, int kind);
 
 #define SK_STREAM_MEM_QUANTUM ((int)PAGE_SIZE)
+#define SK_STREAM_MEM_QUANTUM_SHIFT ilog2(SK_STREAM_MEM_QUANTUM)
 
 static inline int sk_stream_pages(int amt)
 {
-	return DIV_ROUND_UP(amt, SK_STREAM_MEM_QUANTUM);
+	return (amt + SK_STREAM_MEM_QUANTUM - 1) >> SK_STREAM_MEM_QUANTUM_SHIFT;
 }
 
 static inline void sk_stream_mem_reclaim(struct sock *sk)

commit 8df09ea3b8ccfe0c94844102d33fa46f57c08d9e
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Fri Dec 21 03:07:41 2007 -0800

    [SOCK] Avoid integer divides where not necessary in include/net/sock.h
    
    Because sk_wmem_queued, sk_sndbuf are signed, a divide per two
    may force compiler to use an integer divide.
    
    We can instead use a right shift.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 416bc994adad..e178b49eefbb 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -445,7 +445,7 @@ static inline int sk_acceptq_is_full(struct sock *sk)
  */
 static inline int sk_stream_min_wspace(struct sock *sk)
 {
-	return sk->sk_wmem_queued / 2;
+	return sk->sk_wmem_queued >> 1;
 }
 
 static inline int sk_stream_wspace(struct sock *sk)
@@ -1187,7 +1187,7 @@ static inline void sk_wake_async(struct sock *sk, int how, int band)
 static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 {
 	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
-		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued / 2);
+		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);
 		sk->sk_sndbuf = max(sk->sk_sndbuf, SOCK_MIN_SNDBUF);
 	}
 }
@@ -1211,7 +1211,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
  */
 static inline int sock_writeable(const struct sock *sk) 
 {
-	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf / 2);
+	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
 }
 
 static inline gfp_t gfp_any(void)

commit 41380930d2cbdc0abf7513a675864258b7ac973d
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Wed Dec 12 10:46:51 2007 -0800

    [NET]: Remove FASTCALL macro
    
    X86_32 was the last user of the FASTCALL macro, now that it
    uses regparm(3) by default, this macro expands to nothing.
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 4ce37ce8c411..416bc994adad 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -774,14 +774,14 @@ do {									\
 	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
 } while (0)
 
-extern void FASTCALL(lock_sock_nested(struct sock *sk, int subclass));
+extern void lock_sock_nested(struct sock *sk, int subclass);
 
 static inline void lock_sock(struct sock *sk)
 {
 	lock_sock_nested(sk, 0);
 }
 
-extern void FASTCALL(release_sock(struct sock *sk));
+extern void release_sock(struct sock *sk);
 
 /* BH context may only use the following locking interface. */
 #define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))

commit 33eb9cfc700ae9ce621d47d6ca6d6b4ad7cd97f3
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Dec 5 01:37:34 2007 -0800

    [NET]: Isolate the net/core/ sysctl table
    
    Using ctl paths we can put all the stuff, related to net/core/
    sysctl table, into one file and remove all the references on it.
    
    As a good side effect this hides the "core_table" name from
    the global scope :)
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 800e73a62d8f..4ce37ce8c411 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1325,10 +1325,6 @@ extern __u32 sysctl_rmem_max;
 
 extern void sk_init(void);
 
-#ifdef CONFIG_SYSCTL
-extern struct ctl_table core_table[];
-#endif
-
 extern int sysctl_optmem_max;
 
 extern __u32 sysctl_wmem_default;

commit 0eeb8ffcfeaa0d909ce39147f7b8fdd6cef1aacd
Author: Denis V. Lunev <den@openvz.org>
Date:   Tue Dec 4 01:15:45 2007 -0800

    [NET]: netns compilation speedup
    
    This patch speedups compilation when net_namespace.h is changed.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bcbf8891dde4..800e73a62d8f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -57,7 +57,6 @@
 #include <asm/atomic.h>
 #include <net/dst.h>
 #include <net/checksum.h>
-#include <net/net_namespace.h>
 
 /*
  * This structure really needs to be cleaned up.
@@ -95,6 +94,7 @@ typedef struct {
 
 struct sock;
 struct proto;
+struct net;
 
 /**
  *	struct sock_common - minimal network layer representation of sockets

commit df97c708d5e6eebdd9ded1fa588eae09acf53793
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 29 21:22:33 2007 +1100

    [NET]: Eliminate unused argument from sk_stream_alloc_pskb
    
    The 3rd argument is always zero (according to grep :) Eliminate
    it and merge the function with sk_stream_alloc_skb.
    
    This saves 44 more bytes, and together with the previous patch
    we have:
    
    add/remove: 1/0 grow/shrink: 0/8 up/down: 183/-751 (-568)
    function                                     old     new   delta
    sk_stream_alloc_skb                            -     183    +183
    ip_rt_init                                   529     525      -4
    arp_ignore                                   112     107      -5
    __inet_lookup_listener                       284     274     -10
    tcp_sendmsg                                 2583    2481    -102
    tcp_sendpage                                1449    1300    -149
    tso_fragment                                 417     258    -159
    tcp_fragment                                1149     988    -161
    __tcp_push_pending_frames                   1998    1837    -161
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7d500a825e4e..bcbf8891dde4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1192,15 +1192,7 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 	}
 }
 
-struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
-		int size, int mem, gfp_t gfp);
-
-static inline struct sk_buff *sk_stream_alloc_skb(struct sock *sk,
-						  int size,
-						  gfp_t gfp)
-{
-	return sk_stream_alloc_pskb(sk, size, 0, gfp);
-}
+struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp);
 
 static inline struct page *sk_stream_alloc_page(struct sock *sk)
 {

commit f561d0f27d6283c49359bb96048f8ac3728c812c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 29 20:28:50 2007 +1100

    [NET]: Uninline the sk_stream_alloc_pskb
    
    This function seems too big for inlining. Indeed, it saves
    half-a-kilo when uninlined:
    
    add/remove: 1/0 grow/shrink: 0/7 up/down: 195/-719 (-524)
    function                                     old     new   delta
    sk_stream_alloc_pskb                           -     195    +195
    ip_rt_init                                   529     525      -4
    __inet_lookup_listener                       284     274     -10
    tcp_sendmsg                                 2583    2486     -97
    tcp_sendpage                                1449    1305    -144
    tso_fragment                                 417     267    -150
    tcp_fragment                                1149     992    -157
    __tcp_push_pending_frames                   1998    1841    -157
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e329d05f7995..7d500a825e4e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1192,33 +1192,8 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 	}
 }
 
-static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
-						   int size, int mem,
-						   gfp_t gfp)
-{
-	struct sk_buff *skb;
-
-	/* The TCP header must be at least 32-bit aligned.  */
-	size = ALIGN(size, 4);
-
-	skb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);
-	if (skb) {
-		skb->truesize += mem;
-		if (sk_stream_wmem_schedule(sk, skb->truesize)) {
-			/*
-			 * Make sure that we have exactly size bytes
-			 * available to the caller, no more, no less.
-			 */
-			skb_reserve(skb, skb_tailroom(skb) - size);
-			return skb;
-		}
-		__kfree_skb(skb);
-	} else {
-		sk->sk_prot->enter_memory_pressure();
-		sk_stream_moderate_sndbuf(sk);
-	}
-	return NULL;
-}
+struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
+		int size, int mem, gfp_t gfp);
 
 static inline struct sk_buff *sk_stream_alloc_skb(struct sock *sk,
 						  int size,

commit ebb53d75657f86587ac8cf3e38ab0c860a8e3d4f
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Wed Nov 21 22:08:50 2007 +0800

    [NET] proto: Use pcounters for the inuse field
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9c55af8e5f81..e329d05f7995 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -47,6 +47,7 @@
 #include <linux/module.h>
 #include <linux/lockdep.h>
 #include <linux/netdevice.h>
+#include <linux/pcounter.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
 #include <linux/mm.h>
 #include <linux/security.h>
@@ -565,14 +566,9 @@ struct proto {
 	void			(*unhash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
 
-#ifdef CONFIG_SMP
 	/* Keeping track of sockets in use */
-	void			(*inuse_add)(struct proto *prot, int inc);
-	int			(*inuse_getval)(const struct proto *prot);
-	int			*inuse_ptr;
-#else
-	int			inuse;
-#endif
+	struct pcounter		inuse;
+
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(void);
 	atomic_t		*memory_allocated;	/* Current allocated memory. */
@@ -607,35 +603,8 @@ struct proto {
 #endif
 };
 
-/*
- * Special macros to let protos use a fast version of inuse{get|add}
- * using a static percpu variable per proto instead of an allocated one,
- * saving one dereference.
- * This might be changed if/when dynamic percpu vars become fast.
- */
-#ifdef CONFIG_SMP
-# define DEFINE_PROTO_INUSE(NAME)			\
-static DEFINE_PER_CPU(int, NAME##_inuse);		\
-static void NAME##_inuse_add(struct proto *prot, int inc)	\
-{							\
-	__get_cpu_var(NAME##_inuse) += inc;		\
-}							\
-							\
-static int NAME##_inuse_getval(const struct proto *prot)\
-{							\
-	int res = 0, cpu;				\
-							\
-	for_each_possible_cpu(cpu)			\
-		res += per_cpu(NAME##_inuse, cpu);	\
-	return res;					\
-}
-# define REF_PROTO_INUSE(NAME)				\
-	.inuse_add = NAME##_inuse_add,			\
-	.inuse_getval = NAME##_inuse_getval,
-#else
-# define DEFINE_PROTO_INUSE(NAME)
-# define REF_PROTO_INUSE(NAME)
-#endif
+#define DEFINE_PROTO_INUSE(NAME) DEFINE_PCOUNTER(NAME)
+#define REF_PROTO_INUSE(NAME) PCOUNTER_MEMBER_INITIALIZER(NAME, .inuse)
 
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
@@ -668,29 +637,17 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 /* Called with local bh disabled */
 static __inline__ void sock_prot_inc_use(struct proto *prot)
 {
-#ifdef CONFIG_SMP
-	prot->inuse_add(prot, 1);
-#else
-	prot->inuse++;
-#endif
+	pcounter_add(&prot->inuse, 1);
 }
 
 static __inline__ void sock_prot_dec_use(struct proto *prot)
 {
-#ifdef CONFIG_SMP
-	prot->inuse_add(prot, -1);
-#else
-	prot->inuse--;
-#endif
+	pcounter_add(&prot->inuse, -1);
 }
 
 static __inline__ int sock_prot_inuse(struct proto *proto)
 {
-#ifdef CONFIG_SMP
-	return proto->inuse_getval(proto);
-#else
-	return proto->inuse;
-#endif
+	return pcounter_getval(&proto->inuse);
 }
 
 /* With per-bucket locks this operation is not-atomic, so that

commit c0ef877b2c9f543e9fb7953bfe1a0cd3a4eae362
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 15 03:03:19 2007 -0800

    [NET]: Move sock_valbool_flag to socket.c
    
    The sock_valbool_flag() helper is used in setsockopt to
    set or reset some flag on the sock. This helper is required
    in the net/socket.c only, so move it there.
    
    Besides, patch two places in sys_setsockopt() that repeat
    this helper functionality manually.
    
    Since this is not a bugfix, but a trivial cleanup, I
    prepared this patch against net-2.6.25, but it also
    applies (with a single offset) to the latest net-2.6.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f5b643714131..9c55af8e5f81 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1396,14 +1396,6 @@ extern int net_msg_warn;
 				lock_sock(sk); \
 				}
 
-static inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)
-{
-	if (valbool)
-		sock_set_flag(sk, bit);
-	else
-		sock_reset_flag(sk, bit);
-}
-
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 

commit 33c732c36169d7022ad7d6eb474b0c9be43a2dc1
Author: Wang Chen <wangchen@cn.fujitsu.com>
Date:   Tue Nov 13 20:30:01 2007 -0800

    [IPV4]: Add raw drops counter.
    
    Add raw drops counter for IPv4 in /proc/net/raw .
    
    Signed-off-by: Wang Chen <wangchen@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6e1542da33a1..f5b643714131 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -145,7 +145,8 @@ struct sock_common {
   *	@sk_forward_alloc: space allocated forward
   *	@sk_allocation: allocation mode
   *	@sk_sndbuf: size of send buffer in bytes
-  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE, %SO_OOBINLINE settings
+  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
+  *		   %SO_OOBINLINE settings
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
@@ -153,9 +154,12 @@ struct sock_common {
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
   *	@sk_error_queue: rarely used
-  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt, IPV6_ADDRFORM for instance)
+  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
+  *			  IPV6_ADDRFORM for instance)
   *	@sk_err: last error
-  *	@sk_err_soft: errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
+  *	@sk_err_soft: errors that don't cause failure but are the cause of a
+  *		      persistent failure not just 'timed out'
+  *	@sk_drops: raw drops counter
   *	@sk_ack_backlog: current listen backlog
   *	@sk_max_ack_backlog: listen backlog set in listen()
   *	@sk_priority: %SO_PRIORITY setting
@@ -239,6 +243,7 @@ struct sock {
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
 				sk_err_soft;
+	atomic_t		sk_drops;
 	unsigned short		sk_ack_backlog;
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;

commit 9d3e44425e3498eb33f25d94392b4fd0d56a5176
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Jan 8 23:41:28 2008 -0800

    [SOCK]: Adds a rcu_dereference() in sk_filter
    
    It seems commit fda9ef5d679b07c9d9097aaf6ef7f069d794a8f9 introduced a RCU
    protection for sk_filter(), without a rcu_dereference()
    
    Either we need a rcu_dereference(), either a comment should explain why we
    dont need it. I vote for the former.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 67e35c7e230c..6e1542da33a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -944,7 +944,7 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 		return err;
 	
 	rcu_read_lock_bh();
-	filter = sk->sk_filter;
+	filter = rcu_dereference(sk->sk_filter);
 	if (filter) {
 		unsigned int pkt_len = sk_run_filter(skb, filter->insns,
 				filter->len);

commit 21df56c6e2372e09c916111efb6c14c372a5ab2e
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Nov 18 18:48:08 2007 -0800

    [TCP]: Fix TCP header misalignment
    
    Indeed my previous change to alloc_pskb has made it possible
    for the TCP header to be misaligned iff the MTU is not a multiple
    of 4 (and less than a page).  So I suspect the optimised IPsec
    MTU calculation is giving you just such an MTU :)
    
    This patch fixes it by changing alloc_pskb to make sure that
    the size is at least 32-bit aligned.  This does not cause the
    problem fixed by the previous patch because max_header is always
    32-bit aligned which means that in the SG/NOTSO case this will
    be a no-op.
    
    I thought about putting this in the callers but all the current
    callers are from TCP.  If and when we get a non-TCP caller we
    can always create a TCP wrapper for this function and move the
    alignment over there.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 567e468d7492..67e35c7e230c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1236,6 +1236,9 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 {
 	struct sk_buff *skb;
 
+	/* The TCP header must be at least 32-bit aligned.  */
+	size = ALIGN(size, 4);
+
 	skb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);
 	if (skb) {
 		skb->truesize += mem;

commit fb93134dfc2a6e6fbedc7c270a31da03fce88db9
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Nov 14 15:45:21 2007 -0800

    [TCP]: Fix size calculation in sk_stream_alloc_pskb
    
    We round up the header size in sk_stream_alloc_pskb so that
    TSO packets get zero tail room.  Unfortunately this rounding
    up is not coordinated with the select_size() function used by
    TCP to calculate the second parameter of sk_stream_alloc_pskb.
    
    As a result, we may allocate more than a page of data in the
    non-TSO case when exactly one page is desired.
    
    In fact, rounding up the head room is detrimental in the non-TSO
    case because it makes memory that would otherwise be available to
    the payload head room.  TSO doesn't need this either, all it wants
    is the guarantee that there is no tail room.
    
    So this patch fixes this by adjusting the skb_reserve call so that
    exactly the requested amount (which all callers have calculated in
    a precise way) is made available as tail room.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5504fb9fa88a..567e468d7492 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1235,14 +1235,16 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 						   gfp_t gfp)
 {
 	struct sk_buff *skb;
-	int hdr_len;
 
-	hdr_len = SKB_DATA_ALIGN(sk->sk_prot->max_header);
-	skb = alloc_skb_fclone(size + hdr_len, gfp);
+	skb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);
 	if (skb) {
 		skb->truesize += mem;
 		if (sk_stream_wmem_schedule(sk, skb->truesize)) {
-			skb_reserve(skb, hdr_len);
+			/*
+			 * Make sure that we have exactly size bytes
+			 * available to the caller, no more, no less.
+			 */
+			skb_reserve(skb, skb_tailroom(skb) - size);
 			return skb;
 		}
 		__kfree_skb(skb);

commit 286ab3d46058840d68e5d7d52e316c1f7e98c59f
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Mon Nov 5 23:38:39 2007 -0800

    [NET]: Define infrastructure to keep 'inuse' changes in an efficent SMP/NUMA way.
    
    "struct proto" currently uses an array stats[NR_CPUS] to track change on
    'inuse' sockets per protocol.
    
    If NR_CPUS is big, this means we use a big memory area for this.
    Moreover, all this memory area is located on a single node on NUMA
    machines, increasing memory pressure on the boot node.
    
    In this patch, I tried to :
    
    - Keep a fast !CONFIG_SMP implementation
    - Keep a fast CONFIG_SMP implementation for often used protocols
    (tcp,udp,raw,...)
    - Introduce a NUMA efficient implementation
    
    Some helper macros are defined in include/net/sock.h
    These macros take into account CONFIG_SMP
    
    If a "struct proto" is declared without using DEFINE_PROTO_INUSE /
    REF_PROTO_INUSE
    macros, it will automatically use a default implementation, using a
    dynamically allocated percpu zone.
    This default implementation will be NUMA efficient, but might use 32/64
    bytes per possible cpu
    because of current alloc_percpu() implementation.
    However it still should be better than previous implementation based on
    stats[NR_CPUS] field.
    
    When a "struct proto" is changed to use the new macros, we use a single
    static "int" percpu variable,
    lowering the memory and cpu costs, still preserving NUMA efficiency.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 20de3fa7ae40..5504fb9fa88a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -560,6 +560,14 @@ struct proto {
 	void			(*unhash)(struct sock *sk);
 	int			(*get_port)(struct sock *sk, unsigned short snum);
 
+#ifdef CONFIG_SMP
+	/* Keeping track of sockets in use */
+	void			(*inuse_add)(struct proto *prot, int inc);
+	int			(*inuse_getval)(const struct proto *prot);
+	int			*inuse_ptr;
+#else
+	int			inuse;
+#endif
 	/* Memory pressure */
 	void			(*enter_memory_pressure)(void);
 	atomic_t		*memory_allocated;	/* Current allocated memory. */
@@ -592,12 +600,38 @@ struct proto {
 #ifdef SOCK_REFCNT_DEBUG
 	atomic_t		socks;
 #endif
-	struct {
-		int inuse;
-		u8  __pad[SMP_CACHE_BYTES - sizeof(int)];
-	} stats[NR_CPUS];
 };
 
+/*
+ * Special macros to let protos use a fast version of inuse{get|add}
+ * using a static percpu variable per proto instead of an allocated one,
+ * saving one dereference.
+ * This might be changed if/when dynamic percpu vars become fast.
+ */
+#ifdef CONFIG_SMP
+# define DEFINE_PROTO_INUSE(NAME)			\
+static DEFINE_PER_CPU(int, NAME##_inuse);		\
+static void NAME##_inuse_add(struct proto *prot, int inc)	\
+{							\
+	__get_cpu_var(NAME##_inuse) += inc;		\
+}							\
+							\
+static int NAME##_inuse_getval(const struct proto *prot)\
+{							\
+	int res = 0, cpu;				\
+							\
+	for_each_possible_cpu(cpu)			\
+		res += per_cpu(NAME##_inuse, cpu);	\
+	return res;					\
+}
+# define REF_PROTO_INUSE(NAME)				\
+	.inuse_add = NAME##_inuse_add,			\
+	.inuse_getval = NAME##_inuse_getval,
+#else
+# define DEFINE_PROTO_INUSE(NAME)
+# define REF_PROTO_INUSE(NAME)
+#endif
+
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
 
@@ -629,12 +663,29 @@ static inline void sk_refcnt_debug_release(const struct sock *sk)
 /* Called with local bh disabled */
 static __inline__ void sock_prot_inc_use(struct proto *prot)
 {
-	prot->stats[smp_processor_id()].inuse++;
+#ifdef CONFIG_SMP
+	prot->inuse_add(prot, 1);
+#else
+	prot->inuse++;
+#endif
 }
 
 static __inline__ void sock_prot_dec_use(struct proto *prot)
 {
-	prot->stats[smp_processor_id()].inuse--;
+#ifdef CONFIG_SMP
+	prot->inuse_add(prot, -1);
+#else
+	prot->inuse--;
+#endif
+}
+
+static __inline__ int sock_prot_inuse(struct proto *proto)
+{
+#ifdef CONFIG_SMP
+	return proto->inuse_getval(proto);
+#else
+	return proto->inuse;
+#endif
 }
 
 /* With per-bucket locks this operation is not-atomic, so that

commit 6257ff2177ff02d7f260a7a501876aa41cb9a9f6
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 1 00:39:31 2007 -0700

    [NET]: Forget the zero_it argument of sk_alloc()
    
    Finally, the zero_it argument can be completely removed from
    the callers and from the function prototype.
    
    Besides, fix the checkpatch.pl warnings about using the
    assignments inside if-s.
    
    This patch is rather big, and it is a part of the previous one.
    I splitted it wishing to make the patches more readable. Hope
    this particular split helped.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ecad7b4e2a63..20de3fa7ae40 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -779,7 +779,7 @@ extern void FASTCALL(release_sock(struct sock *sk));
 
 extern struct sock		*sk_alloc(struct net *net, int family,
 					  gfp_t priority,
-					  struct proto *prot, int zero_it);
+					  struct proto *prot);
 extern void			sk_free(struct sock *sk);
 extern struct sock		*sk_clone(const struct sock *sk,
 					  const gfp_t priority);

commit f1a6c4da14c365d3ee0b5de43a93f7470982637c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 1 00:29:45 2007 -0700

    [NET]: Move the sock_copy() from the header
    
    The sock_copy() call is not used outside the sock.c file,
    so just move it into a sock.c
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 43fc3fa50d62..ecad7b4e2a63 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -993,20 +993,6 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
-static inline void sock_copy(struct sock *nsk, const struct sock *osk)
-{
-#ifdef CONFIG_SECURITY_NETWORK
-	void *sptr = nsk->sk_security;
-#endif
-
-	memcpy(nsk, osk, osk->sk_prot->obj_size);
-	get_net(nsk->sk_net);
-#ifdef CONFIG_SECURITY_NETWORK
-	nsk->sk_security = sptr;
-	security_sk_clone(osk, nsk);
-#endif
-}
-
 extern int sock_i_uid(struct sock *sk);
 extern unsigned long sock_i_ino(struct sock *sk);
 

commit 47e958eac280c263397582d5581e868c3227a1bd
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Oct 17 21:22:42 2007 -0700

    [NET]: Fix the race between sk_filter_(de|at)tach and sk_clone()
    
    The proposed fix is to delay the reference counter decrement
    until the quiescent state pass. This will give sk_clone() a
    chance to get the reference on the cloned filter.
    
    Regular sk_filter_uncharge can happen from the sk_free() only
    and there's no need in delaying the put - the socket is dead
    anyway and is to be release itself.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index b9cfe125c9e6..43fc3fa50d62 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -904,16 +904,6 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 	return err;
 }
 
-/**
- * 	sk_filter_rcu_free: Free a socket filter
- *	@rcu: rcu_head that contains the sk_filter to free
- */
-static inline void sk_filter_rcu_free(struct rcu_head *rcu)
-{
-	struct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);
-	kfree(fp);
-}
-
 /**
  *	sk_filter_release: Release a socket filter
  *	@sk: socket
@@ -925,7 +915,7 @@ static inline void sk_filter_rcu_free(struct rcu_head *rcu)
 static inline void sk_filter_release(struct sk_filter *fp)
 {
 	if (atomic_dec_and_test(&fp->refcnt))
-		call_rcu_bh(&fp->rcu, sk_filter_rcu_free);
+		kfree(fp);
 }
 
 static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)

commit 309dd5fc872448e35634d510049642312ebc170d
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Oct 17 21:21:51 2007 -0700

    [NET]: Move the filter releasing into a separate call
    
    This is done merely as a preparation for the fix.
    
    The sk_filter_uncharge() unaccounts the filter memory and calls
    the sk_filter_release(), which in turn decrements the refcount
    anf frees the filter.
    
    The latter function will be required separately.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 453c79d0915b..b9cfe125c9e6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -922,14 +922,18 @@ static inline void sk_filter_rcu_free(struct rcu_head *rcu)
  *	Remove a filter from a socket and release its resources.
  */
 
-static inline void sk_filter_release(struct sock *sk, struct sk_filter *fp)
+static inline void sk_filter_release(struct sk_filter *fp)
+{
+	if (atomic_dec_and_test(&fp->refcnt))
+		call_rcu_bh(&fp->rcu, sk_filter_rcu_free);
+}
+
+static inline void sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)
 {
 	unsigned int size = sk_filter_len(fp);
 
 	atomic_sub(size, &sk->sk_omem_alloc);
-
-	if (atomic_dec_and_test(&fp->refcnt))
-		call_rcu_bh(&fp->rcu, sk_filter_rcu_free);
+	sk_filter_release(fp);
 }
 
 static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)

commit cfcabdcc2d5a810208e5bb3974121b7ed60119aa
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Tue Oct 9 01:59:42 2007 -0700

    [NET]: sparse warning fixes
    
    Fix a bunch of sparse warnings. Mostly about 0 used as
    NULL pointer, and shadowed variable declarations.
    One notable case was that hash size should have been unsigned.
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 74e1f7d90d73..453c79d0915b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -485,17 +485,17 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb->next = NULL;
 }
 
-#define sk_wait_event(__sk, __timeo, __condition)		\
-({	int rc;							\
-	release_sock(__sk);					\
-	rc = __condition;					\
-	if (!rc) {						\
-		*(__timeo) = schedule_timeout(*(__timeo));	\
-	}							\
-	lock_sock(__sk);					\
-	rc = __condition;					\
-	rc;							\
-})
+#define sk_wait_event(__sk, __timeo, __condition)			\
+	({	int __rc;						\
+		release_sock(__sk);					\
+		__rc = __condition;					\
+		if (!__rc) {						\
+			*(__timeo) = schedule_timeout(*(__timeo));	\
+		}							\
+		lock_sock(__sk);					\
+		__rc = __condition;					\
+		__rc;							\
+	})
 
 extern int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
 extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);

commit 1b8d7ae42d02e483ad94035cca851e4f7fbecb40
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Oct 8 23:24:22 2007 -0700

    [NET]: Make socket creation namespace safe.
    
    This patch passes in the namespace a new socket should be created in
    and has the socket code do the appropriate reference counting.  By
    virtue of this all socket create methods are touched.  In addition
    the socket create methods are modified so that they will fail if
    you attempt to create a socket in a non-default network namespace.
    
    Failing if we attempt to create a socket outside of the default
    network namespace ensures that as we incrementally make the network stack
    network namespace aware we will not export functionality that someone
    has not audited and made certain is network namespace safe.
    Allowing us to partially enable network namespaces before all of the
    exotic protocols are supported.
    
    Any protocol layers I have missed will fail to compile because I now
    pass an extra parameter into the socket creation code.
    
    [ Integrated AF_IUCV build fixes from Andrew Morton... -DaveM ]
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9ef8b5fb7936..74e1f7d90d73 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -56,6 +56,7 @@
 #include <asm/atomic.h>
 #include <net/dst.h>
 #include <net/checksum.h>
+#include <net/net_namespace.h>
 
 /*
  * This structure really needs to be cleaned up.
@@ -776,7 +777,7 @@ extern void FASTCALL(release_sock(struct sock *sk));
 				SINGLE_DEPTH_NESTING)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
-extern struct sock		*sk_alloc(int family,
+extern struct sock		*sk_alloc(struct net *net, int family,
 					  gfp_t priority,
 					  struct proto *prot, int zero_it);
 extern void			sk_free(struct sock *sk);
@@ -1005,6 +1006,7 @@ static inline void sock_copy(struct sock *nsk, const struct sock *osk)
 #endif
 
 	memcpy(nsk, osk, osk->sk_prot->obj_size);
+	get_net(nsk->sk_net);
 #ifdef CONFIG_SECURITY_NETWORK
 	nsk->sk_security = sptr;
 	security_sk_clone(osk, nsk);

commit 07feaebfcc10cd35e745c7073667935246494bee
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Sep 12 11:58:02 2007 +0200

    [NET]: Add a network namespace parameter to struct sock
    
    Sockets need to get a reference to their network namespace,
    or possibly a simple hold if someone registers on the network
    namespace notifier and will free the sockets when the namespace
    is going to be destroyed.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5ed9fa42b6e8..9ef8b5fb7936 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -105,6 +105,7 @@ struct proto;
  *	@skc_refcnt: reference count
  *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_prot: protocol handlers inside a network family
+ *	@skc_net: reference to the network namespace of this socket
  *
  *	This is the minimal network layer representation of sockets, the header
  *	for struct sock and struct inet_timewait_sock.
@@ -119,6 +120,7 @@ struct sock_common {
 	atomic_t		skc_refcnt;
 	unsigned int		skc_hash;
 	struct proto		*skc_prot;
+	struct net	 	*skc_net;
 };
 
 /**
@@ -195,6 +197,7 @@ struct sock {
 #define sk_refcnt		__sk_common.skc_refcnt
 #define sk_hash			__sk_common.skc_hash
 #define sk_prot			__sk_common.skc_prot
+#define sk_net			__sk_common.skc_net
 	unsigned char		sk_shutdown : 2,
 				sk_no_check : 2,
 				sk_userlocks : 4;

commit d2e9117c7aa9544d910634e17e3519fd67155229
Author: John Heffner <jheffner@psc.edu>
Date:   Wed Sep 12 10:44:19 2007 +0200

    [NET]: Change type of owner in sock_lock_t to int, rename
    
    The type of owner in sock_lock_t is currently (struct sock_iocb *),
    presumably for historical reasons.  It is never used as this type, only
    tested as NULL or set to (void *)1.  For clarity, this changes it to type
    int, and renames to owned, to avoid any possible type casting errors.
    
    Signed-off-by: John Heffner <jheffner@psc.edu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 802c670ba820..5ed9fa42b6e8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -76,10 +76,9 @@
  * between user contexts and software interrupt processing, whereas the
  * mini-semaphore synchronizes multiple users amongst themselves.
  */
-struct sock_iocb;
 typedef struct {
 	spinlock_t		slock;
-	struct sock_iocb	*owner;
+	int			owned;
 	wait_queue_head_t	wq;
 	/*
 	 * We express the mutex-alike socket_lock semantics
@@ -737,7 +736,7 @@ static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
  * Since ~2.3.5 it is also exclusive sleep lock serializing
  * accesses from user process context.
  */
-#define sock_owned_by_user(sk)	((sk)->sk_lock.owner)
+#define sock_owned_by_user(sk)	((sk)->sk_lock.owned)
 
 /*
  * Macro so as to not evaluate some arguments when
@@ -748,7 +747,7 @@ static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
  */
 #define sock_lock_init_class_and_name(sk, sname, skey, name, key) 	\
 do {									\
-	sk->sk_lock.owner = NULL;					\
+	sk->sk_lock.owned = 0;					\
 	init_waitqueue_head(&sk->sk_lock.wq);				\
 	spin_lock_init(&(sk)->sk_lock.slock);				\
 	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\

commit 172589ccdde41b59861c92c4a971b95514ef24e3
Author: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
Date:   Tue Aug 28 15:50:33 2007 -0700

    [NET]: DIV_ROUND_UP cleanup (part two)
    
    Hopefully captured all single statement cases under net/. I'm
    not too sure if there is some policy about #includes that are
    "guaranteed" (ie., in the current tree) to be available through
    some other #included header, so I just added linux/kernel.h to
    each changed file that didn't #include it previously.
    
    Signed-off-by: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index dfeb8b13024f..802c670ba820 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -40,6 +40,7 @@
 #ifndef _SOCK_H
 #define _SOCK_H
 
+#include <linux/kernel.h>
 #include <linux/list.h>
 #include <linux/timer.h>
 #include <linux/cache.h>
@@ -702,7 +703,7 @@ extern int sk_stream_mem_schedule(struct sock *sk, int size, int kind);
 
 static inline int sk_stream_pages(int amt)
 {
-	return (amt + SK_STREAM_MEM_QUANTUM - 1) / SK_STREAM_MEM_QUANTUM;
+	return DIV_ROUND_UP(amt, SK_STREAM_MEM_QUANTUM);
 }
 
 static inline void sk_stream_mem_reclaim(struct sock *sk)

commit 4e07a91c37c69ec1647c218214591ee4fe3408fe
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue May 29 13:17:47 2007 -0700

    [SOCK]: Shrink struct sock by 8 bytes on 64-bit.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 689b886038da..dfeb8b13024f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -218,13 +218,13 @@ struct sock {
 	atomic_t		sk_rmem_alloc;
 	atomic_t		sk_wmem_alloc;
 	atomic_t		sk_omem_alloc;
+	int			sk_sndbuf;
 	struct sk_buff_head	sk_receive_queue;
 	struct sk_buff_head	sk_write_queue;
 	struct sk_buff_head	sk_async_wait_queue;
 	int			sk_wmem_queued;
 	int			sk_forward_alloc;
 	gfp_t			sk_allocation;
-	int			sk_sndbuf;
 	int			sk_route_caps;
 	int			sk_gso_type;
 	int			sk_rcvlowat;

commit 6272e2667965dfb5b59199f462cd0f001fb304a6
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 8 00:29:21 2007 -0700

    cleanup compat ioctl handling
    
    Merge all compat ioctl handling into compat_ioctl.c instead of splitting it
    over compat.c and compat_ioctl.c.  This also allows to get rid of ioctl32.h
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Looks-good-to: Andi Kleen <ak@suse.de>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 25c37e34bfdc..689b886038da 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1361,15 +1361,6 @@ static inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)
 extern __u32 sysctl_wmem_max;
 extern __u32 sysctl_rmem_max;
 
-#ifdef CONFIG_NET
-int siocdevprivate_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg);
-#else
-static inline int siocdevprivate_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg)
-{
-	return -ENODEV;
-}
-#endif
-
 extern void sk_init(void);
 
 #ifdef CONFIG_SYSCTL

commit 9958089a43ae8a9af07402461c0b2b7548c7341e
Author: Andi Kleen <ak@suse.de>
Date:   Fri Apr 20 17:12:43 2007 -0700

    [NET]: Move sk_setup_caps() out of line.
    
    It is far too large to be an inline and not in any hot paths.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 390c04700590..25c37e34bfdc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1075,19 +1075,7 @@ static inline int sk_can_gso(const struct sock *sk)
 	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
 }
 
-static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
-{
-	__sk_dst_set(sk, dst);
-	sk->sk_route_caps = dst->dev->features;
-	if (sk->sk_route_caps & NETIF_F_GSO)
-		sk->sk_route_caps |= NETIF_F_GSO_MASK;
-	if (sk_can_gso(sk)) {
-		if (dst->header_len)
-			sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
-		else 
-			sk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;
-	}
-}
+extern void sk_setup_caps(struct sock *sk, struct dst_entry *dst);
 
 static inline void sk_charge_skb(struct sock *sk, struct sk_buff *skb)
 {

commit 92f37fd2ee805aa77925c1e64fd56088b46094fc
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Sun Mar 25 22:14:49 2007 -0700

    [NET]: Adding SO_TIMESTAMPNS / SCM_TIMESTAMPNS support
    
    Now that network timestamps use ktime_t infrastructure, we can add a new
    SOL_SOCKET sockopt  SO_TIMESTAMPNS.
    
    This command is similar to SO_TIMESTAMP, but permits transmission of
    a 'timespec struct' instead of a 'timeval struct' control message.
    (nanosecond resolution instead of microsecond)
    
    Control message is labelled SCM_TIMESTAMPNS instead of SCM_TIMESTAMP
    
    A socket cannot mix SO_TIMESTAMP and SO_TIMESTAMPNS : the two modes are
    mutually exclusive.
    
    sock_recv_timestamp() became too big to be fully inlined so I added a
    __sock_recv_timestamp() helper function.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    CC: linux-arch@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 51246579592e..390c04700590 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -390,6 +390,7 @@ enum sock_flags {
 	SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
 	SOCK_DBG, /* %SO_DEBUG setting */
 	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
+	SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 };
@@ -1283,21 +1284,17 @@ static inline int sock_intr_errno(long timeo)
 	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
 }
 
+extern void __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,
+	struct sk_buff *skb);
+
 static __inline__ void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 {
 	ktime_t kt = skb->tstamp;
 
-	if (sock_flag(sk, SOCK_RCVTSTAMP)) {
-		struct timeval tv;
-		/* Race occurred between timestamp enabling and packet
-		   receiving.  Fill in the current time for now. */
-		if (kt.tv64 == 0)
-			kt = ktime_get_real();
-		skb->tstamp = kt;
-		tv = ktime_to_timeval(kt);
-		put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP, sizeof(tv), &tv);
-	} else
+	if (sock_flag(sk, SOCK_RCVTSTAMP))
+		__sock_recv_timestamp(msg, sk, skb);
+	else
 		sk->sk_stamp = kt;
 }
 

commit a2a316fd068c455c609ecc155dcfaa7e208d29fe
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Thu Mar 8 20:41:08 2007 -0800

    [NET]: Replace CONFIG_NET_DEBUG with sysctl.
    
    Covert network warning messages from a compile time to runtime choice.
    Removes kernel config option and replaces it with new /proc/sys/net/core/warnings.
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d093e49fdc85..51246579592e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1334,14 +1334,12 @@ extern int sock_get_timestampns(struct sock *, struct timespec __user *);
 /* 
  *	Enable debug/info messages 
  */
+extern int net_msg_warn;
+#define NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn) printk(fmt,##args); } while (0)
 
-#ifdef CONFIG_NETDEBUG
-#define NETDEBUG(fmt, args...)	printk(fmt,##args)
-#define LIMIT_NETDEBUG(fmt, args...) do { if (net_ratelimit()) printk(fmt,##args); } while(0)
-#else
-#define NETDEBUG(fmt, args...)	do { } while (0)
-#define LIMIT_NETDEBUG(fmt, args...) do { } while(0)
-#endif
+#define LIMIT_NETDEBUG(fmt, args...) \
+	do { if (net_msg_warn && net_ratelimit()) printk(fmt,##args); } while(0)
 
 /*
  * Macros for sleeping on a socket. Use them like this:

commit ae40eb1ef30ab4120bd3c8b7e3da99ee53d27a23
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Sun Mar 18 17:33:16 2007 -0700

    [NET]: Introduce SIOCGSTAMPNS ioctl to get timestamps with nanosec resolution
    
    Now network timestamps use ktime_t infrastructure, we can add a new
    ioctl() SIOCGSTAMPNS command to get timestamps in 'struct timespec'.
    User programs can thus access to nanosecond resolution.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    CC: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2974bacc8850..d093e49fdc85 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1329,6 +1329,7 @@ static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_e
 
 extern void sock_enable_timestamp(struct sock *sk);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
+extern int sock_get_timestampns(struct sock *, struct timespec __user *);
 
 /* 
  *	Enable debug/info messages 

commit fe067e8ab5e0dc5ca3c54634924c628da92090b4
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Wed Mar 7 12:12:44 2007 -0800

    [TCP]: Abstract out all write queue operations.
    
    This allows the write queue implementation to be changed,
    for example, to one which allows fast interval searching.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 9583639090d2..2974bacc8850 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -710,15 +710,6 @@ static inline void sk_stream_mem_reclaim(struct sock *sk)
 		__sk_stream_mem_reclaim(sk);
 }
 
-static inline void sk_stream_writequeue_purge(struct sock *sk)
-{
-	struct sk_buff *skb;
-
-	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
-		sk_stream_free_skb(sk, skb);
-	sk_stream_mem_reclaim(sk);
-}
-
 static inline int sk_stream_rmem_schedule(struct sock *sk, struct sk_buff *skb)
 {
 	return (int)skb->truesize <= sk->sk_forward_alloc ||
@@ -1256,18 +1247,6 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 	return page;
 }
 
-#define sk_stream_for_retrans_queue(skb, sk)				\
-		for (skb = (sk)->sk_write_queue.next;			\
-		     (skb != (sk)->sk_send_head) &&			\
-		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
-		     skb = skb->next)
-
-/*from STCP for fast SACK Process*/
-#define sk_stream_for_retrans_queue_from(skb, sk)			\
-		for (; (skb != (sk)->sk_send_head) &&                   \
-		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
-		     skb = skb->next)
-
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */

commit b7aa0bf70c4afb9e38be25f5c0922498d0f8684c
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Apr 19 16:16:32 2007 -0700

    [NET]: convert network timestamps to ktime_t
    
    We currently use a special structure (struct skb_timeval) and plain
    'struct timeval' to store packet timestamps in sk_buffs and struct
    sock.
    
    This has some drawbacks :
    - Fixed resolution of micro second.
    - Waste of space on 64bit platforms where sizeof(struct timeval)=16
    
    I suggest using ktime_t that is a nice abstraction of high resolution
    time services, currently capable of nanosecond resolution.
    
    As sizeof(ktime_t) is 8 bytes, using ktime_t in 'struct sock' permits
    a 8 byte shrink of this structure on 64bit architectures. Some other
    structures also benefit from this size reduction (struct ipq in
    ipv4/ip_fragment.c, struct frag_queue in ipv6/reassembly.c, ...)
    
    Once this ktime infrastructure adopted, we can more easily provide
    nanosecond resolution on top of it. (ioctl SIOCGSTAMPNS and/or
    SO_TIMESTAMPNS/SCM_TIMESTAMPNS)
    
    Note : this patch includes a bug correction in
    compat_sock_get_timestamp() where a "err = 0;" was missing (so this
    syscall returned -ENOENT instead of 0)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    CC: Stephen Hemminger <shemminger@linux-foundation.org>
    CC: John find <linux.kernel@free.fr>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a3366c3c837a..9583639090d2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -244,7 +244,7 @@ struct sock {
 	struct sk_filter      	*sk_filter;
 	void			*sk_protinfo;
 	struct timer_list	sk_timer;
-	struct timeval		sk_stamp;
+	ktime_t			sk_stamp;
 	struct socket		*sk_socket;
 	void			*sk_user_data;
 	struct page		*sk_sndmsg_page;
@@ -1307,19 +1307,19 @@ static inline int sock_intr_errno(long timeo)
 static __inline__ void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 {
-	struct timeval stamp;
+	ktime_t kt = skb->tstamp;
 
-	skb_get_timestamp(skb, &stamp);
 	if (sock_flag(sk, SOCK_RCVTSTAMP)) {
+		struct timeval tv;
 		/* Race occurred between timestamp enabling and packet
 		   receiving.  Fill in the current time for now. */
-		if (stamp.tv_sec == 0)
-			do_gettimeofday(&stamp);
-		skb_set_timestamp(skb, &stamp);
-		put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP, sizeof(struct timeval),
-			 &stamp);
+		if (kt.tv64 == 0)
+			kt = ktime_get_real();
+		skb->tstamp = kt;
+		tv = ktime_to_timeval(kt);
+		put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP, sizeof(tv), &tv);
 	} else
-		sk->sk_stamp = stamp;
+		sk->sk_stamp = kt;
 }
 
 /**

commit fa438ccfdfd3f6db02c13b61b21454eb81cd6a13
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Sun Mar 4 16:05:44 2007 -0800

    [NET]: Keep sk_backlog near sk_lock
    
    sk_backlog is a critical field of struct sock. (known famous words)
    
    It is (ab)used in hot paths, in particular in release_sock(), tcp_recvmsg(),
    tcp_v4_rcv(), sk_receive_skb().
    
    It really makes sense to place it next to sk_lock, because sk_backlog is only
    used after sk_lock locked (and thus memory cache line in L1 cache). This
    should reduce cache misses and sk_lock acquisition time.
    
    (In theory, we could only move the head pointer near sk_lock, and leaving tail
    far away, because 'tail' is normally not so hot, but keep it simple :) )
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c7d60ca3548..a3366c3c837a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -202,6 +202,15 @@ struct sock {
 	unsigned short		sk_type;
 	int			sk_rcvbuf;
 	socket_lock_t		sk_lock;
+	/*
+	 * The backlog queue is special, it is always used with
+	 * the per-socket spinlock held and requires low latency
+	 * access. Therefore we special case it's implementation.
+	 */
+	struct {
+		struct sk_buff *head;
+		struct sk_buff *tail;
+	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
 	struct xfrm_policy	*sk_policy[2];
@@ -221,15 +230,6 @@ struct sock {
 	int			sk_rcvlowat;
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
-	/*
-	 * The backlog queue is special, it is always used with
-	 * the per-socket spinlock held and requires low latency
-	 * access. Therefore we special case it's implementation.
-	 */
-	struct {
-		struct sk_buff *head;
-		struct sk_buff *tail;
-	} sk_backlog;
 	struct sk_buff_head	sk_error_queue;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;

commit 64a146513f8f12ba204b7bf5cb7e9505594ead42
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Tue Mar 6 11:21:05 2007 -0800

    [NET]: Revert incorrect accept queue backlog changes.
    
    This reverts two changes:
    
    8488df894d05d6fa41c2bd298c335f944bb0e401
    248f06726e866942b3d8ca8f411f9067713b7ff8
    
    A backlog value of N really does mean allow "N + 1" connections
    to queue to a listening socket.  This allows one to specify
    "0" as the backlog and still get 1 connection.
    
    Noticed by Gerrit Renker and Rick Jones.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 849c7df23181..2c7d60ca3548 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -426,7 +426,7 @@ static inline void sk_acceptq_added(struct sock *sk)
 
 static inline int sk_acceptq_is_full(struct sock *sk)
 {
-	return sk->sk_ack_backlog >= sk->sk_max_ack_backlog;
+	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
 }
 
 /*

commit 8488df894d05d6fa41c2bd298c335f944bb0e401
Author: Wei Dong <weid@np.css.fujitsu.com>
Date:   Fri Mar 2 12:37:26 2007 -0800

    [NET]: Fix bugs in "Whether sock accept queue is full" checking
    
            when I use linux TCP socket, and find there is a bug in function
    sk_acceptq_is_full().
    
            When a new SYN comes, TCP module first checks its validation. If valid,
    send SYN,ACK to the client and add the sock to the syn hash table. Next
    time if received the valid ACK for SYN,ACK from the client. server will
    accept this connection and increase the sk->sk_ack_backlog -- which is
    done in function tcp_check_req().We check wether acceptq is full in
    function tcp_v4_syn_recv_sock().
    
    Consider an example:
    
     After listen(sockfd, 1) system call, sk->sk_max_ack_backlog is set to
    1. As we know, sk->sk_ack_backlog is initialized to 0. Assuming accept()
    system call is not invoked now.
    
    1. 1st connection comes. invoke sk_acceptq_is_full(). sk-
    >sk_ack_backlog=0 sk->sk_max_ack_backlog=1, function return 0 accept
    this connection. Increase the sk->sk_ack_backlog
    2. 2nd connection comes. invoke sk_acceptq_is_full(). sk-
    >sk_ack_backlog=1 sk->sk_max_ack_backlog=1, function return 0 accept
    this connection. Increase the sk->sk_ack_backlog
    3. 3rd connection comes. invoke sk_acceptq_is_full(). sk-
    >sk_ack_backlog=2 sk->sk_max_ack_backlog=1, function return 1. Refuse
    this connection.
    
    I think it has bugs. after listen system call. sk->sk_max_ack_backlog=1
    but now it can accept 2 connections.
    
    Signed-off-by: Wei Dong <weid@np.css.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2c7d60ca3548..849c7df23181 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -426,7 +426,7 @@ static inline void sk_acceptq_added(struct sock *sk)
 
 static inline int sk_acceptq_is_full(struct sock *sk)
 {
-	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
+	return sk->sk_ack_backlog >= sk->sk_max_ack_backlog;
 }
 
 /*

commit 4498121ca3acbf928681b71261227d28dc29b6f6
Author: Patrick McHardy <kaber@trash.net>
Date:   Tue Feb 27 09:56:42 2007 -0800

    [NET]: Handle disabled preemption in gfp_any()
    
    ctnetlink uses netlink_unicast from an atomic_notifier_chain
    (which is called within a RCU read side critical section)
    without holding further locks. netlink_unicast calls netlink_trim
    with the result of gfp_any() for the gfp flags, which are passed
    down to pskb_expand_header. gfp_any() only checks for softirq
    context and returns GFP_KERNEL, resulting in this warning:
    
    BUG: sleeping function called from invalid context at mm/slab.c:3032
    in_atomic():1, irqs_disabled():0
    no locks held by rmmod/7010.
    
    Call Trace:
     [<ffffffff8109467f>] debug_show_held_locks+0x9/0xb
     [<ffffffff8100b0b4>] __might_sleep+0xd9/0xdb
     [<ffffffff810b5082>] __kmalloc+0x68/0x110
     [<ffffffff811ba8f2>] pskb_expand_head+0x4d/0x13b
     [<ffffffff81053147>] netlink_broadcast+0xa5/0x2e0
     [<ffffffff881cd1d7>] :nfnetlink:nfnetlink_send+0x83/0x8a
     [<ffffffff8834f6a6>] :nf_conntrack_netlink:ctnetlink_conntrack_event+0x94c/0x96a
     [<ffffffff810624d6>] notifier_call_chain+0x29/0x3e
     [<ffffffff8106251d>] atomic_notifier_call_chain+0x32/0x60
     [<ffffffff881d266d>] :nf_conntrack:destroy_conntrack+0xa5/0x1d3
     [<ffffffff881d194e>] :nf_conntrack:nf_ct_cleanup+0x8c/0x12c
     [<ffffffff881d4614>] :nf_conntrack:kill_l3proto+0x0/0x13
     [<ffffffff881d482a>] :nf_conntrack:nf_conntrack_l3proto_unregister+0x90/0x94
     [<ffffffff883551b3>] :nf_conntrack_ipv4:nf_conntrack_l3proto_ipv4_fini+0x2b/0x5d
     [<ffffffff8109d44f>] sys_delete_module+0x1b5/0x1e6
     [<ffffffff8105f245>] trace_hardirqs_on_thunk+0x35/0x37
     [<ffffffff8105911e>] system_call+0x7e/0x83
    
    Since netlink_unicast is supposed to be callable from within RCU
    read side critical sections, make gfp_any() check for in_atomic()
    instead of in_softirq().
    
    Additionally nfnetlink_send needs to use gfp_any() as well for the
    call to netlink_broadcast).
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 03684e702d13..2c7d60ca3548 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1278,7 +1278,7 @@ static inline int sock_writeable(const struct sock *sk)
 
 static inline gfp_t gfp_any(void)
 {
-	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
+	return in_atomic() ? GFP_ATOMIC : GFP_KERNEL;
 }
 
 static inline long sock_rcvtimeo(const struct sock *sk, int noblock)

commit ed07536ed6731775219c1df7fa26a7588753e693
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 6 20:35:24 2006 -0800

    [PATCH] lockdep: annotate nfs/nfsd in-kernel sockets
    
    Stick NFS sockets in their own class to avoid some lockdep warnings.  NFS
    sockets are never exposed to user-space, and will hence not trigger certain
    code paths that would otherwise pose deadlock scenarios.
    
    [akpm@osdl.org: cleanups]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Dickson <SteveD@redhat.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Acked-by: Neil Brown <neilb@suse.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    [ Fixed patch corruption by quilt, pointed out by Peter Zijlstra ]
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 730899ce5162..03684e702d13 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -746,6 +746,25 @@ static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
  */
 #define sock_owned_by_user(sk)	((sk)->sk_lock.owner)
 
+/*
+ * Macro so as to not evaluate some arguments when
+ * lockdep is not enabled.
+ *
+ * Mark both the sk_lock and the sk_lock.slock as a
+ * per-address-family lock class.
+ */
+#define sock_lock_init_class_and_name(sk, sname, skey, name, key) 	\
+do {									\
+	sk->sk_lock.owner = NULL;					\
+	init_waitqueue_head(&sk->sk_lock.wq);				\
+	spin_lock_init(&(sk)->sk_lock.slock);				\
+	debug_check_no_locks_freed((void *)&(sk)->sk_lock,		\
+			sizeof((sk)->sk_lock));				\
+	lockdep_set_class_and_name(&(sk)->sk_lock.slock,		\
+		       	(skey), (sname));				\
+	lockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);	\
+} while (0)
+
 extern void FASTCALL(lock_sock_nested(struct sock *sk, int subclass));
 
 static inline void lock_sock(struct sock *sk)

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index fe3a33fad03f..730899ce5162 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -571,7 +571,7 @@ struct proto {
 	int			*sysctl_rmem;
 	int			max_header;
 
-	kmem_cache_t		*slab;
+	struct kmem_cache		*slab;
 	unsigned int		obj_size;
 
 	atomic_t		*orphan_count;

commit d7fe0f241dceade9c8d4af75498765c5ff7f27e6
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Dec 3 23:15:30 2006 -0500

    [PATCH] severing skbuff.h -> mm.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/include/net/sock.h b/include/net/sock.h
index 26fc0b16bc0c..fe3a33fad03f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -47,6 +47,7 @@
 #include <linux/lockdep.h>
 #include <linux/netdevice.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
+#include <linux/mm.h>
 #include <linux/security.h>
 
 #include <linux/filter.h>

commit 58a5a7b9555ea231b557ebef5cabeaf8e951df0b
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Thu Nov 16 14:06:06 2006 -0200

    [NET]: Conditionally use bh_lock_sock_nested in sk_receive_skb
    
    Spotted by Ian McDonald, tentatively fixed by Gerrit Renker:
    
    http://www.mail-archive.com/dccp%40vger.kernel.org/msg00599.html
    
    Rewritten not to unroll sk_receive_skb, in the common case, i.e. no lock
    debugging, its optimized away.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>

diff --git a/include/net/sock.h b/include/net/sock.h
index dc4b92b8abea..26fc0b16bc0c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -954,7 +954,8 @@ static inline void sock_put(struct sock *sk)
 		sk_free(sk);
 }
 
-extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb);
+extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb,
+			  const int nested);
 
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.

commit 5084205faf45384fff25c4cf77dd5c96279283ad
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Nov 14 21:36:34 2006 -0800

    [NET]: Annotate callers of csum_partial_copy_...() and csum_and_copy...() in net/*
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 35ffbdd35d3e..dc4b92b8abea 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1088,7 +1088,7 @@ static inline int skb_copy_to_page(struct sock *sk, char __user *from,
 {
 	if (skb->ip_summed == CHECKSUM_NONE) {
 		int err = 0;
-		unsigned int csum = csum_and_copy_from_user(from,
+		__wsum csum = csum_and_copy_from_user(from,
 						     page_address(page) + off,
 							    copy, 0, &err);
 		if (err)

commit fcc70d5fdc9b0bd3e99c9dacb8198224af2b4b42
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Nov 8 22:44:35 2006 -0800

    [BLUETOOTH] lockdep: annotate sk_lock nesting in AF_BLUETOOTH
    
    =============================================
    [ INFO: possible recursive locking detected ]
    2.6.18-1.2726.fc6 #1

diff --git a/include/net/sock.h b/include/net/sock.h
index 9cdbae2a53a3..35ffbdd35d3e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -745,7 +745,13 @@ static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
  */
 #define sock_owned_by_user(sk)	((sk)->sk_lock.owner)
 
-extern void FASTCALL(lock_sock(struct sock *sk));
+extern void FASTCALL(lock_sock_nested(struct sock *sk, int subclass));
+
+static inline void lock_sock(struct sock *sk)
+{
+	lock_sock_nested(sk, 0);
+}
+
 extern void FASTCALL(release_sock(struct sock *sk));
 
 /* BH context may only use the following locking interface. */

commit dc9b334622bff6d22456917a034c2e2d194b9328
Author: Paul Bonser <misterpib@gmail.com>
Date:   Thu Nov 23 17:56:13 2006 -0800

    [NET]: Re-fix of doc-comment in sock.h
    
    Restoring old, correct comment for sk_filter_release, moving it to
    where it should actually be, and changing new comment into proper
    comment for sk_filter_rcu_free, where it actually makes sense.
    
    The original fix submitted for this on Oct 23 mistakenly documented
    the wrong function.
    
    Signed-off-by: Paul Bonser <misterpib@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ac286a353032..9cdbae2a53a3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -883,18 +883,23 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 }
 
 /**
- *	sk_filter_release: Release a socket filter
- *	@rcu: rcu_head that contains the sk_filter info to remove
- *
- *	Remove a filter from a socket and release its resources.
+ * 	sk_filter_rcu_free: Free a socket filter
+ *	@rcu: rcu_head that contains the sk_filter to free
  */
- 
 static inline void sk_filter_rcu_free(struct rcu_head *rcu)
 {
 	struct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);
 	kfree(fp);
 }
 
+/**
+ *	sk_filter_release: Release a socket filter
+ *	@sk: socket
+ *	@fp: filter to remove
+ *
+ *	Remove a filter from a socket and release its resources.
+ */
+
 static inline void sk_filter_release(struct sock *sk, struct sk_filter *fp)
 {
 	unsigned int size = sk_filter_len(fp);

commit 6a43487f43fbd4e03c606dcb62b98374a3af88fc
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Sun Oct 22 20:38:00 2006 -0700

    [NET]: kernel-doc fix for sock.h
    
    Fix kernel-doc warning in include/net/sock.h:
    Warning(/var/linsrc/linux-2619-rc1-pv//include/net/sock.h:894): No description found for parameter 'rcu'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 40bb90ebb2d1..ac286a353032 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -884,8 +884,7 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 
 /**
  *	sk_filter_release: Release a socket filter
- *	@sk: socket
- *	@fp: filter to remove
+ *	@rcu: rcu_head that contains the sk_filter info to remove
  *
  *	Remove a filter from a socket and release its resources.
  */

commit 027445c37282bc1ed26add45e573ad2d3e4860a5
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Sat Sep 30 23:28:46 2006 -0700

    [PATCH] Vectorize aio_read/aio_write fileop methods
    
    This patch vectorizes aio_read() and aio_write() methods to prepare for
    collapsing all aio & vectored operations into one interface - which is
    aio_read()/aio_write().
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Michael Holzheu <HOLZHEU@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index edd4d73ce7f5..40bb90ebb2d1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -665,7 +665,6 @@ struct sock_iocb {
 	struct sock		*sk;
 	struct scm_cookie	*scm;
 	struct msghdr		*msg, async_msg;
-	struct iovec		async_iov;
 	struct kiocb		*kiocb;
 };
 

commit fda9ef5d679b07c9d9097aaf6ef7f069d794a8f9
Author: Dmitry Mishin <dim@openvz.org>
Date:   Thu Aug 31 15:28:39 2006 -0700

    [NET]: Fix sk->sk_filter field access
    
    Function sk_filter() is called from tcp_v{4,6}_rcv() functions with arg
    needlock = 0, while socket is not locked at that moment. In order to avoid
    this and similar issues in the future, use rcu for sk->sk_filter field read
    protection.
    
    Signed-off-by: Dmitry Mishin <dim@openvz.org>
    Signed-off-by: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 337ebec84c70..edd4d73ce7f5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -862,30 +862,24 @@ extern void sock_init_data(struct socket *sock, struct sock *sk);
  *
  */
 
-static inline int sk_filter(struct sock *sk, struct sk_buff *skb, int needlock)
+static inline int sk_filter(struct sock *sk, struct sk_buff *skb)
 {
 	int err;
+	struct sk_filter *filter;
 	
 	err = security_sock_rcv_skb(sk, skb);
 	if (err)
 		return err;
 	
-	if (sk->sk_filter) {
-		struct sk_filter *filter;
-		
-		if (needlock)
-			bh_lock_sock(sk);
-		
-		filter = sk->sk_filter;
-		if (filter) {
-			unsigned int pkt_len = sk_run_filter(skb, filter->insns,
-							     filter->len);
-			err = pkt_len ? pskb_trim(skb, pkt_len) : -EPERM;
-		}
-
-		if (needlock)
-			bh_unlock_sock(sk);
+	rcu_read_lock_bh();
+	filter = sk->sk_filter;
+	if (filter) {
+		unsigned int pkt_len = sk_run_filter(skb, filter->insns,
+				filter->len);
+		err = pkt_len ? pskb_trim(skb, pkt_len) : -EPERM;
 	}
+ 	rcu_read_unlock_bh();
+
 	return err;
 }
 
@@ -897,6 +891,12 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb, int needlock)
  *	Remove a filter from a socket and release its resources.
  */
  
+static inline void sk_filter_rcu_free(struct rcu_head *rcu)
+{
+	struct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);
+	kfree(fp);
+}
+
 static inline void sk_filter_release(struct sock *sk, struct sk_filter *fp)
 {
 	unsigned int size = sk_filter_len(fp);
@@ -904,7 +904,7 @@ static inline void sk_filter_release(struct sock *sk, struct sk_filter *fp)
 	atomic_sub(size, &sk->sk_omem_alloc);
 
 	if (atomic_dec_and_test(&fp->refcnt))
-		kfree(fp);
+		call_rcu_bh(&fp->rcu, sk_filter_rcu_free);
 }
 
 static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)

commit 4237c75c0a35535d7f9f2bfeeb4b4df1e068a0bf
Author: Venkat Yekkirala <vyekkirala@TrustedCS.com>
Date:   Mon Jul 24 23:32:50 2006 -0700

    [MLSXFRM]: Auto-labeling of child sockets
    
    This automatically labels the TCP, Unix stream, and dccp child sockets
    as well as openreqs to be at the same MLS level as the peer. This will
    result in the selection of appropriately labeled IPSec Security
    Associations.
    
    This also uses the sock's sid (as opposed to the isec sid) in SELinux
    enforcement of secmark in rcv_skb and postroute_last hooks.
    
    Signed-off-by: Venkat Yekkirala <vyekkirala@TrustedCS.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91cdceb3c028..337ebec84c70 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -969,6 +969,7 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	sk->sk_sleep = &parent->wait;
 	parent->sk = sk;
 	sk->sk_socket = parent;
+	security_sock_graft(sk, parent);
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 

commit 892c141e62982272b9c738b5520ad0e5e1ad7b42
Author: Venkat Yekkirala <vyekkirala@TrustedCS.com>
Date:   Fri Aug 4 23:08:56 2006 -0700

    [MLSXFRM]: Add security sid to sock
    
    This adds security for IP sockets at the sock level. Security at the
    sock level is needed to enforce the SELinux security policy for
    security associations even when a sock is orphaned (such as in the TCP
    LAST_ACK state).
    
    This will also be used to enforce SELinux controls over data arriving
    at or leaving a child socket while it's still waiting to be accepted.
    
    Signed-off-by: Venkat Yekkirala <vyekkirala@TrustedCS.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 324b3ea233d6..91cdceb3c028 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -972,6 +972,19 @@ static inline void sock_graft(struct sock *sk, struct socket *parent)
 	write_unlock_bh(&sk->sk_callback_lock);
 }
 
+static inline void sock_copy(struct sock *nsk, const struct sock *osk)
+{
+#ifdef CONFIG_SECURITY_NETWORK
+	void *sptr = nsk->sk_security;
+#endif
+
+	memcpy(nsk, osk, osk->sk_prot->obj_size);
+#ifdef CONFIG_SECURITY_NETWORK
+	nsk->sk_security = sptr;
+	security_sk_clone(osk, nsk);
+#endif
+}
+
 extern int sock_i_uid(struct sock *sk);
 extern unsigned long sock_i_ino(struct sock *sk);
 

commit a5b5bb9a053a973c23b867738c074acb3e80c0a0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:35 2006 -0700

    [PATCH] lockdep: annotate sk_locks
    
    Teach sk_lock semantics to the lock validator.  In the softirq path the
    slock has mutex_trylock()+mutex_unlock() semantics, in the process context
    sock_lock() case it has mutex_lock()/mutex_unlock() semantics.
    
    Thus we treat sock_owned_by_user() flagged areas as an exclusion area too,
    not just those areas covered by a held sk_lock.slock.
    
    Effect on non-lockdep kernels: minimal, sk_lock_sock_init() has been turned
    into an inline function.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0969fb60d6ea..324b3ea233d6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -44,6 +44,7 @@
 #include <linux/timer.h>
 #include <linux/cache.h>
 #include <linux/module.h>
+#include <linux/lockdep.h>
 #include <linux/netdevice.h>
 #include <linux/skbuff.h>	/* struct sk_buff */
 #include <linux/security.h>
@@ -78,18 +79,17 @@ typedef struct {
 	spinlock_t		slock;
 	struct sock_iocb	*owner;
 	wait_queue_head_t	wq;
+	/*
+	 * We express the mutex-alike socket_lock semantics
+	 * to the lock validator by explicitly managing
+	 * the slock as a lock variant (in addition to
+	 * the slock itself):
+	 */
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
 } socket_lock_t;
 
-extern struct lock_class_key af_family_keys[AF_MAX];
-
-#define sock_lock_init(__sk) \
-do {	spin_lock_init(&((__sk)->sk_lock.slock)); \
-	lockdep_set_class(&(__sk)->sk_lock.slock, \
-			  af_family_keys + (__sk)->sk_family); \
-	(__sk)->sk_lock.owner = NULL; \
-	init_waitqueue_head(&((__sk)->sk_lock.wq)); \
-} while(0)
-
 struct sock;
 struct proto;
 

commit c63661848581a9842dfc72d9a400285dd284fc47
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:13 2006 -0700

    [PATCH] lockdep: annotate bh_lock_sock()
    
    Teach special (recursive) locking code to the lock validator.  Has no effect
    on non-lockdep kernels.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 83805feea880..0969fb60d6ea 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -751,6 +751,9 @@ extern void FASTCALL(release_sock(struct sock *sk));
 
 /* BH context may only use the following locking interface. */
 #define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
+#define bh_lock_sock_nested(__sk) \
+				spin_lock_nested(&((__sk)->sk_lock.slock), \
+				SINGLE_DEPTH_NESTING)
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
 extern struct sock		*sk_alloc(int family,

commit da21f24dd73954c2ed0cd39a698e2c9916c05d71
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:12 2006 -0700

    [PATCH] lockdep: annotate sock_lock_init()
    
    Teach special (multi-initialized, per-address-family) locking code to the lock
    validator.  Has no effect on non-lockdep kernels.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7b3d6b856946..83805feea880 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -80,8 +80,12 @@ typedef struct {
 	wait_queue_head_t	wq;
 } socket_lock_t;
 
+extern struct lock_class_key af_family_keys[AF_MAX];
+
 #define sock_lock_init(__sk) \
 do {	spin_lock_init(&((__sk)->sk_lock.slock)); \
+	lockdep_set_class(&(__sk)->sk_lock.slock, \
+			  af_family_keys + (__sk)->sk_family); \
 	(__sk)->sk_lock.owner = NULL; \
 	init_waitqueue_head(&((__sk)->sk_lock.wq)); \
 } while(0)

commit bcd76111178ebccedd46a9b3eaff65c78e5a70af
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Jun 30 13:36:35 2006 -0700

    [NET]: Generalise TSO-specific bits from skb_setup_caps
    
    This patch generalises the TSO-specific bits from sk_setup_caps by adding
    the sk_gso_type member to struct sock.  This makes sk_setup_caps generic
    so that it can be used by TCPv6 or UFO.
    
    The only catch is that whoever uses this must provide a GSO implementation
    for their protocol which I think is a fair deal :) For now UFO continues to
    live without a GSO implementation which is OK since it doesn't use the sock
    caps field at the moment.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7136bae48c2f..7b3d6b856946 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -140,6 +140,7 @@ struct sock_common {
   *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE, %SO_OOBINLINE settings
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
   *	@sk_lingertime: %SO_LINGER l_linger setting
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
@@ -211,6 +212,7 @@ struct sock {
 	gfp_t			sk_allocation;
 	int			sk_sndbuf;
 	int			sk_route_caps;
+	int			sk_gso_type;
 	int			sk_rcvlowat;
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
@@ -1025,15 +1027,20 @@ extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
 extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
+static inline int sk_can_gso(const struct sock *sk)
+{
+	return net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);
+}
+
 static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
 {
 	__sk_dst_set(sk, dst);
 	sk->sk_route_caps = dst->dev->features;
 	if (sk->sk_route_caps & NETIF_F_GSO)
-		sk->sk_route_caps |= NETIF_F_TSO;
-	if (sk->sk_route_caps & NETIF_F_TSO) {
+		sk->sk_route_caps |= NETIF_F_GSO_MASK;
+	if (sk_can_gso(sk)) {
 		if (dst->header_len)
-			sk->sk_route_caps &= ~NETIF_F_TSO;
+			sk->sk_route_caps &= ~NETIF_F_GSO_MASK;
 		else 
 			sk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;
 	}

commit b0da8537037f337103348f239ad901477e907aa8
Author: Michael Chan <mchan@broadcom.com>
Date:   Thu Jun 29 12:30:00 2006 -0700

    [NET]: Add ECN support for TSO
    
    In the current TSO implementation, NETIF_F_TSO and ECN cannot be
    turned on together in a TCP connection.  The problem is that most
    hardware that supports TSO does not handle CWR correctly if it is set
    in the TSO packet.  Correct handling requires CWR to be set in the
    first packet only if it is set in the TSO header.
    
    This patch adds the ability to turn on NETIF_F_TSO and ECN using
    GSO if necessary to handle TSO packets with CWR set.  Hardware
    that handles CWR correctly can turn on NETIF_F_TSO_ECN in the dev->
    features flag.
    
    All TSO packets with CWR set will have the SKB_GSO_TCPV4_ECN set.  If
    the output device does not have the NETIF_F_TSO_ECN feature set, GSO
    will split the packet up correctly with CWR only set in the first
    segment.
    
    With help from Herbert Xu <herbert@gondor.apana.org.au>.
    
    Since ECN can always be enabled with TSO, the SOCK_NO_LARGESEND sock
    flag is completely removed.
    
    Signed-off-by: Michael Chan <mchan@broadcom.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2d8d6adf1616..7136bae48c2f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -383,7 +383,6 @@ enum sock_flags {
 	SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
 	SOCK_DBG, /* %SO_DEBUG setting */
 	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
-	SOCK_NO_LARGESEND, /* whether to sent large segments or not */
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 };
@@ -1033,7 +1032,7 @@ static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
 	if (sk->sk_route_caps & NETIF_F_GSO)
 		sk->sk_route_caps |= NETIF_F_TSO;
 	if (sk->sk_route_caps & NETIF_F_TSO) {
-		if (sock_flag(sk, SOCK_NO_LARGESEND) || dst->header_len)
+		if (dst->header_len)
 			sk->sk_route_caps &= ~NETIF_F_TSO;
 		else 
 			sk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;

commit f4b8ea7849544114e9d3d682df4d400180854677
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Thu Jun 22 16:00:11 2006 -0700

    [NET]: fix net-core kernel-doc
    
    Warning(/var/linsrc/linux-2617-g4//include/linux/skbuff.h:304): No description found for parameter 'dma_cookie'
    Warning(/var/linsrc/linux-2617-g4//include/net/sock.h:1274): No description found for parameter 'copied_early'
    Warning(/var/linsrc/linux-2617-g4//net/core/dev.c:3309): No description found for parameter 'chan'
    Warning(/var/linsrc/linux-2617-g4//net/core/dev.c:3309): No description found for parameter 'event'
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a897f05de3b5..2d8d6adf1616 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1269,6 +1269,7 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
  * sk_eat_skb - Release a skb if it is no longer needed
  * @sk: socket to eat this skb from
  * @skb: socket buffer to eat
+ * @copied_early: flag indicating whether DMA operations copied this data early
  *
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.

commit 37c3185a02d4b85fbe134bf5204535405dd2c957
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Jun 22 03:07:29 2006 -0700

    [NET]: Added GSO toggle
    
    This patch adds a generic segmentation offload toggle that can be turned
    on/off for each net device.  For now it only supports in TCPv4.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d10dfecb6cbd..a897f05de3b5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1030,9 +1030,13 @@ static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
 {
 	__sk_dst_set(sk, dst);
 	sk->sk_route_caps = dst->dev->features;
+	if (sk->sk_route_caps & NETIF_F_GSO)
+		sk->sk_route_caps |= NETIF_F_TSO;
 	if (sk->sk_route_caps & NETIF_F_TSO) {
 		if (sock_flag(sk, SOCK_NO_LARGESEND) || dst->header_len)
 			sk->sk_route_caps &= ~NETIF_F_TSO;
+		else 
+			sk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;
 	}
 }
 

commit cee4cca740d209bcb4b9857baa2253d5ba4e3fbe
Merge: 2edc322d420a 9348f0de2d2b
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Jun 20 15:10:08 2006 -0700

    Merge git://git.infradead.org/hdrcleanup-2.6
    
    * git://git.infradead.org/hdrcleanup-2.6: (63 commits)
      [S390] __FD_foo definitions.
      Switch to __s32 types in joystick.h instead of C99 types for consistency.
      Add <sys/types.h> to headers included for userspace in <linux/input.h>
      Move inclusion of <linux/compat.h> out of user scope in asm-x86_64/mtrr.h
      Remove struct fddi_statistics from user view in <linux/if_fddi.h>
      Move user-visible parts of drivers/s390/crypto/z90crypt.h to include/asm-s390
      Revert include/media changes: Mauro says those ioctls are only used in-kernel(!)
      Include <linux/types.h> and use __uXX types in <linux/cramfs_fs.h>
      Use __uXX types in <linux/i2o_dev.h>, include <linux/ioctl.h> too
      Remove private struct dx_hash_info from public view in <linux/ext3_fs.h>
      Include <linux/types.h> and use __uXX types in <linux/affs_hardblocks.h>
      Use __uXX types in <linux/divert.h> for struct divert_blk et al.
      Use __u32 for elf_addr_t in <asm-powerpc/elf.h>, not u32. It's user-visible.
      Remove PPP_FCS from user view in <linux/ppp_defs.h>, remove __P mess entirely
      Use __uXX types in user-visible structures in <linux/nbd.h>
      Don't use 'u32' in user-visible struct ip_conntrack_old_tuple.
      Use __uXX types for S390 DASD volume label definitions which are user-visible
      S390 BIODASDREADCMB ioctl should use __u64 not u64 type.
      Remove unneeded inclusion of <linux/time.h> from <linux/ufs_fs.h>
      Fix private integer types used in V4L2 ioctls.
      ...
    
    Manually resolve conflict in include/linux/mtd/physmap.h

commit b38dfee3d616ffadb58d4215e3ff9d1d7921031e
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Jun 9 16:13:01 2006 -0700

    [NET]: skb_trim audit
    
    I found a few more spots where pskb_trim_rcsum could be used but were not.
    This patch changes them to use it.
    
    Also, sk_filter can get paged skb data.  Therefore we must use pskb_trim
    instead of skb_trim.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 75b0e97ed93d..96565ff0de6a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -873,10 +873,7 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb, int needlock)
 		if (filter) {
 			unsigned int pkt_len = sk_run_filter(skb, filter->insns,
 							     filter->len);
-			if (!pkt_len)
-				err = -EPERM;
-			else
-				skb_trim(skb, pkt_len);
+			err = pkt_len ? pskb_trim(skb, pkt_len) : -EPERM;
 		}
 
 		if (needlock)

commit 624d1164730d58a494cc5aa4afa37d02c41e83a7
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 18:01:28 2006 -0700

    [I/OAT]: Make sk_eat_skb I/OAT aware.
    
    Add an extra argument to sk_eat_skb, and make it move early copied
    packets to the async_wait_queue instead of freeing them.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 90c65cb091a8..75b0e97ed93d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1273,11 +1273,22 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.
 */
-static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
+#ifdef CONFIG_NET_DMA
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
+{
+	__skb_unlink(skb, &sk->sk_receive_queue);
+	if (!copied_early)
+		__kfree_skb(skb);
+	else
+		__skb_queue_tail(&sk->sk_async_wait_queue, skb);
+}
+#else
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb, int copied_early)
 {
 	__skb_unlink(skb, &sk->sk_receive_queue);
 	__kfree_skb(skb);
 }
+#endif
 
 extern void sock_enable_timestamp(struct sock *sk);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);

commit 97fc2f0848c928c63c2ae619deee61a0b1107b69
Author: Chris Leech <christopher.leech@intel.com>
Date:   Tue May 23 17:55:33 2006 -0700

    [I/OAT]: Structure changes for TCP recv offload to I/OAT
    
    Adds an async_wait_queue and some additional fields to tcp_sock, and a
    dma_cookie_t to sk_buff.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c9fad6fb629b..90c65cb091a8 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -132,6 +132,7 @@ struct sock_common {
   *	@sk_receive_queue: incoming packets
   *	@sk_wmem_alloc: transmit queue bytes committed
   *	@sk_write_queue: Packet sending queue
+  *	@sk_async_wait_queue: DMA copied packets
   *	@sk_omem_alloc: "o" is "option" or "other"
   *	@sk_wmem_queued: persistent queue size
   *	@sk_forward_alloc: space allocated forward
@@ -205,6 +206,7 @@ struct sock {
 	atomic_t		sk_omem_alloc;
 	struct sk_buff_head	sk_receive_queue;
 	struct sk_buff_head	sk_write_queue;
+	struct sk_buff_head	sk_async_wait_queue;
 	int			sk_wmem_queued;
 	int			sk_forward_alloc;
 	gfp_t			sk_allocation;

commit 5047f09b56d0bc3c21aec9cb16de60283da645c6
Merge: c0f1fe00c392 5528e568a760
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Sat May 6 19:59:18 2006 +0100

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

commit da753beaeb1446aa87bcca7e8a0026633a8914f0
Author: Akinobu Mita <mita@miraclelinux.com>
Date:   Fri Apr 28 15:21:23 2006 -0700

    [NET]: use hlist_unhashed()
    
    Use hlist_unhashed() rather than accessing inside data structure.
    
    Signed-off-by: Akinobu Mita <mita@miraclelinux.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ff8b0dad7b0f..c9fad6fb629b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -279,7 +279,7 @@ static inline int sk_unhashed(const struct sock *sk)
 
 static inline int sk_hashed(const struct sock *sk)
 {
-	return sk->sk_node.pprev != NULL;
+	return !sk_unhashed(sk);
 }
 
 static __inline__ void sk_node_init(struct hlist_node *node)

commit 62c4f0a2d5a188f73a94f2cb8ea0dba3e7cf0a7f
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Wed Apr 26 12:56:16 2006 +0100

    Don't include linux/config.h from anywhere else in include/
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index ff8b0dad7b0f..d8a5d87ad145 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -40,7 +40,6 @@
 #ifndef _SOCK_H
 #define _SOCK_H
 
-#include <linux/config.h>
 #include <linux/list.h>
 #include <linux/timer.h>
 #include <linux/cache.h>

commit dc6de33674608f978ec29f5c2f7e3af458c06f78
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Thu Apr 20 00:10:50 2006 -0700

    [NET]: Add skb->truesize assertion checking.
    
    Add some sanity checking.  truesize should be at least sizeof(struct
    sk_buff) plus the current packet length.  If not, then truesize is
    seriously mangled and deserves a kernel log message.
    
    Currently we'll do the check for release of stream socket buffers.
    
    But we can add checks to more spots over time.
    
    Incorporating ideas from Herbert Xu.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index af2b0544586e..ff8b0dad7b0f 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -454,6 +454,7 @@ static inline void sk_stream_set_owner_r(struct sk_buff *skb, struct sock *sk)
 
 static inline void sk_stream_free_skb(struct sock *sk, struct sk_buff *skb)
 {
+	skb_truesize_check(skb);
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
 	sk->sk_wmem_queued   -= skb->truesize;
 	sk->sk_forward_alloc += skb->truesize;

commit f0088a50e7c49d1ba285c88fe06345f223652fd3
Author: Denis Vlasenko <vda@ilport.com.ua>
Date:   Tue Mar 28 01:08:21 2006 -0800

    [NET]: deinline 200+ byte inlines in sock.h
    
    Sizes in bytes (allyesconfig, i386) and files where those inlines
    are used:
    
    238 sock_queue_rcv_skb 2.6.16/net/x25/x25_in.o
    238 sock_queue_rcv_skb 2.6.16/net/rose/rose_in.o
    238 sock_queue_rcv_skb 2.6.16/net/packet/af_packet.o
    238 sock_queue_rcv_skb 2.6.16/net/netrom/nr_in.o
    238 sock_queue_rcv_skb 2.6.16/net/llc/llc_sap.o
    238 sock_queue_rcv_skb 2.6.16/net/llc/llc_conn.o
    238 sock_queue_rcv_skb 2.6.16/net/irda/af_irda.o
    238 sock_queue_rcv_skb 2.6.16/net/ipx/af_ipx.o
    238 sock_queue_rcv_skb 2.6.16/net/ipv6/udp.o
    238 sock_queue_rcv_skb 2.6.16/net/ipv6/raw.o
    238 sock_queue_rcv_skb 2.6.16/net/ipv4/udp.o
    238 sock_queue_rcv_skb 2.6.16/net/ipv4/raw.o
    238 sock_queue_rcv_skb 2.6.16/net/ipv4/ipmr.o
    238 sock_queue_rcv_skb 2.6.16/net/econet/econet.o
    238 sock_queue_rcv_skb 2.6.16/net/econet/af_econet.o
    238 sock_queue_rcv_skb 2.6.16/net/bluetooth/sco.o
    238 sock_queue_rcv_skb 2.6.16/net/bluetooth/l2cap.o
    238 sock_queue_rcv_skb 2.6.16/net/bluetooth/hci_sock.o
    238 sock_queue_rcv_skb 2.6.16/net/ax25/ax25_in.o
    238 sock_queue_rcv_skb 2.6.16/net/ax25/af_ax25.o
    238 sock_queue_rcv_skb 2.6.16/net/appletalk/ddp.o
    238 sock_queue_rcv_skb 2.6.16/drivers/net/pppoe.o
    
    276 sk_receive_skb 2.6.16/net/decnet/dn_nsp_in.o
    276 sk_receive_skb 2.6.16/net/dccp/ipv6.o
    276 sk_receive_skb 2.6.16/net/dccp/ipv4.o
    276 sk_receive_skb 2.6.16/net/dccp/dccp_ipv6.o
    276 sk_receive_skb 2.6.16/drivers/net/pppoe.o
    
    209 sk_dst_check 2.6.16/net/ipv6/ip6_output.o
    209 sk_dst_check 2.6.16/net/ipv4/udp.o
    209 sk_dst_check 2.6.16/net/decnet/dn_nsp_out.o
    
    Large inlines with multiple callers:
    Size  Uses Wasted Name and definition
    ===== ==== ====== ================================================
      238   21   4360 sock_queue_rcv_skb    include/net/sock.h
      109   10    801 sock_recv_timestamp   include/net/sock.h
      276    4    768 sk_receive_skb        include/net/sock.h
       94    8    518 __sk_dst_check        include/net/sock.h
      209    3    378 sk_dst_check  include/net/sock.h
      131    4    333 sk_setup_caps include/net/sock.h
      152    2    132 sk_stream_alloc_pskb  include/net/sock.h
      125    2    105 sk_stream_writequeue_purge    include/net/sock.h
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 2aa73c0ec6c2..af2b0544586e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -938,28 +938,7 @@ static inline void sock_put(struct sock *sk)
 		sk_free(sk);
 }
 
-static inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb)
-{
-	int rc = NET_RX_SUCCESS;
-
-	if (sk_filter(sk, skb, 0))
-		goto discard_and_relse;
-
-	skb->dev = NULL;
-
-	bh_lock_sock(sk);
-	if (!sock_owned_by_user(sk))
-		rc = sk->sk_backlog_rcv(sk, skb);
-	else
-		sk_add_backlog(sk, skb);
-	bh_unlock_sock(sk);
-out:
-	sock_put(sk);
-	return rc;
-discard_and_relse:
-	kfree_skb(skb);
-	goto out;
-}
+extern int sk_receive_skb(struct sock *sk, struct sk_buff *skb);
 
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
@@ -1044,33 +1023,9 @@ sk_dst_reset(struct sock *sk)
 	write_unlock(&sk->sk_dst_lock);
 }
 
-static inline struct dst_entry *
-__sk_dst_check(struct sock *sk, u32 cookie)
-{
-	struct dst_entry *dst = sk->sk_dst_cache;
-
-	if (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {
-		sk->sk_dst_cache = NULL;
-		dst_release(dst);
-		return NULL;
-	}
-
-	return dst;
-}
-
-static inline struct dst_entry *
-sk_dst_check(struct sock *sk, u32 cookie)
-{
-	struct dst_entry *dst = sk_dst_get(sk);
+extern struct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);
 
-	if (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {
-		sk_dst_reset(sk);
-		dst_release(dst);
-		return NULL;
-	}
-
-	return dst;
-}
+extern struct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);
 
 static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
 {
@@ -1140,45 +1095,7 @@ extern void sk_reset_timer(struct sock *sk, struct timer_list* timer,
 
 extern void sk_stop_timer(struct sock *sk, struct timer_list* timer);
 
-static inline int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
-{
-	int err = 0;
-	int skb_len;
-
-	/* Cast skb->rcvbuf to unsigned... It's pointless, but reduces
-	   number of warnings when compiling with -W --ANK
-	 */
-	if (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=
-	    (unsigned)sk->sk_rcvbuf) {
-		err = -ENOMEM;
-		goto out;
-	}
-
-	/* It would be deadlock, if sock_queue_rcv_skb is used
-	   with socket lock! We assume that users of this
-	   function are lock free.
-	*/
-	err = sk_filter(sk, skb, 1);
-	if (err)
-		goto out;
-
-	skb->dev = NULL;
-	skb_set_owner_r(skb, sk);
-
-	/* Cache the SKB length before we tack it onto the receive
-	 * queue.  Once it is added it no longer belongs to us and
-	 * may be freed by other threads of control pulling packets
-	 * from the queue.
-	 */
-	skb_len = skb->len;
-
-	skb_queue_tail(&sk->sk_receive_queue, skb);
-
-	if (!sock_flag(sk, SOCK_DEAD))
-		sk->sk_data_ready(sk, skb_len);
-out:
-	return err;
-}
+extern int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 
 static inline int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)
 {

commit 9932cf954691f20d548cd6010d89d1d48597e104
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Fri Mar 24 15:12:37 2006 -0800

    [NET]: Fill in a 32-bit hole in struct sock on 64-bit platforms.
    
    This makes struct sock 8 bytes smaller on 64-bit.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ec226f31dc2a..2aa73c0ec6c2 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -210,6 +210,7 @@ struct sock {
 	gfp_t			sk_allocation;
 	int			sk_sndbuf;
 	int			sk_route_caps;
+	int			sk_rcvlowat;
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
 	/*
@@ -230,7 +231,6 @@ struct sock {
 	unsigned short		sk_max_ack_backlog;
 	__u32			sk_priority;
 	struct ucred		sk_peercred;
-	int			sk_rcvlowat;
 	long			sk_rcvtimeo;
 	long			sk_sndtimeo;
 	struct sk_filter      	*sk_filter;

commit 3fdadf7d27e3fbcf72930941884387d1f4936f04
Author: Dmitry Mishin <dim@openvz.org>
Date:   Mon Mar 20 22:45:21 2006 -0800

    [NET]: {get|set}sockopt compatibility layer
    
    This patch extends {get|set}sockopt compatibility layer in order to
    move protocol specific parts to their place and avoid huge universal
    net/compat.c file in the future.
    
    Signed-off-by: Dmitry Mishin <dim@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f63d0d56712c..ec226f31dc2a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -520,6 +520,14 @@ struct proto {
 	int			(*getsockopt)(struct sock *sk, int level, 
 					int optname, char __user *optval, 
 					int __user *option);  	 
+	int			(*compat_setsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					int optlen);
+	int			(*compat_getsockopt)(struct sock *sk,
+					int level,
+					int optname, char __user *optval,
+					int __user *option);
 	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
 					   struct msghdr *msg, size_t len);
 	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
@@ -816,6 +824,10 @@ extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
 			       struct msghdr *msg, size_t size, int flags);
 extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
 				  char __user *optval, int optlen);
+extern int compat_sock_common_getsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, int __user *optlen);
+extern int compat_sock_common_setsockopt(struct socket *sock, int level,
+		int optname, char __user *optval, int optlen);
 
 extern void sk_common_release(struct sock *sk);
 

commit 265a92856b17524c87da0258ac0d3cec80ae1d35
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Fri Mar 17 16:05:43 2006 -0800

    [NET]: Fix race condition in sk_wait_event().
    
    It is broken, the condition is checked out of socket lock. It is
    wonderful the bug survived for so long time.
    
    [ This fixes bugzilla #6233:
      race condition in tcp_sendmsg when connection became established ]
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 30758035d616..f63d0d56712c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -478,9 +478,9 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	rc = __condition;					\
 	if (!rc) {						\
 		*(__timeo) = schedule_timeout(*(__timeo));	\
-		rc = __condition;				\
 	}							\
 	lock_sock(__sk);					\
+	rc = __condition;					\
 	rc;							\
 })
 

commit 0dec456d1fe73e0539625f0973ee8ef8fb805943
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Feb 2 20:40:09 2006 -0800

    [NET]: Add CONFIG_NETDEBUG to suppress bad packet messages.
    
    If you are on a hostile network, or are running protocol tests, you can
    easily get the logged swamped by messages about bad UDP and ICMP packets.
    This turns those messages off unless a config option is enabled.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Acked-by: Dave Jones <davej@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 1806e5b61419..30758035d616 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1354,12 +1354,12 @@ extern int sock_get_timestamp(struct sock *, struct timeval __user *);
  *	Enable debug/info messages 
  */
 
-#if 0
-#define NETDEBUG(fmt, args...)	do { } while (0)
-#define LIMIT_NETDEBUG(fmt, args...) do { } while(0)
-#else
+#ifdef CONFIG_NETDEBUG
 #define NETDEBUG(fmt, args...)	printk(fmt,##args)
 #define LIMIT_NETDEBUG(fmt, args...) do { if (net_ratelimit()) printk(fmt,##args); } while(0)
+#else
+#define NETDEBUG(fmt, args...)	do { } while (0)
+#define LIMIT_NETDEBUG(fmt, args...) do { } while(0)
 #endif
 
 /*

commit 4bad4dc919573dbe9a5b41dd9edff279e99822d7
Author: Kris Katterjohn <kjak@ispwest.com>
Date:   Fri Jan 6 13:08:20 2006 -0800

    [NET]: Change sk_run_filter()'s return type in net/core/filter.c
    
    It should return an unsigned value, and fix sk_filter() as well.
    
    Signed-off-by: Kris Katterjohn <kjak@ispwest.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6961700ff3a0..1806e5b61419 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -856,8 +856,8 @@ static inline int sk_filter(struct sock *sk, struct sk_buff *skb, int needlock)
 		
 		filter = sk->sk_filter;
 		if (filter) {
-			int pkt_len = sk_run_filter(skb, filter->insns,
-						    filter->len);
+			unsigned int pkt_len = sk_run_filter(skb, filter->insns,
+							     filter->len);
 			if (!pkt_len)
 				err = -EPERM;
 			else

commit 25995ff577675b58dbd848b7758e7bad87411947
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 27 02:42:22 2005 -0200

    [SOCK]: Introduce sk_receive_skb
    
    Its common enough to to justify that, TCP still can't use it as it has the
    prequeueing stuff, still to be made generic in the not so distant future :-)
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 91d28957dc10..6961700ff3a0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -926,6 +926,29 @@ static inline void sock_put(struct sock *sk)
 		sk_free(sk);
 }
 
+static inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb)
+{
+	int rc = NET_RX_SUCCESS;
+
+	if (sk_filter(sk, skb, 0))
+		goto discard_and_relse;
+
+	skb->dev = NULL;
+
+	bh_lock_sock(sk);
+	if (!sock_owned_by_user(sk))
+		rc = sk->sk_backlog_rcv(sk, skb);
+	else
+		sk_add_backlog(sk, skb);
+	bh_unlock_sock(sk);
+out:
+	sock_put(sk);
+	return rc;
+discard_and_relse:
+	kfree_skb(skb);
+	goto out;
+}
+
 /* Detach socket from process context.
  * Announce socket dead, detach it from wait queue and inode.
  * Note that parent inode held reference count on this struct sock,

commit 6d6ee43e0b8b8d4847627fd43739b98ec2b9404f
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:25:19 2005 -0800

    [TWSK]: Introduce struct timewait_sock_ops
    
    So that we can share several timewait sockets related functions and
    make the timewait mini sockets infrastructure closer to the request
    mini sockets one.
    
    Next changesets will take advantage of this, moving more code out of
    TCP and DCCP v4 and v6 to common infrastructure.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 0fbae85c6d55..91d28957dc10 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -493,6 +493,7 @@ extern void sk_stream_kill_queues(struct sock *sk);
 extern int sk_wait_data(struct sock *sk, long *timeo);
 
 struct request_sock_ops;
+struct timewait_sock_ops;
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
@@ -557,11 +558,10 @@ struct proto {
 	kmem_cache_t		*slab;
 	unsigned int		obj_size;
 
-	kmem_cache_t		*twsk_slab;
-	unsigned int		twsk_obj_size;
 	atomic_t		*orphan_count;
 
 	struct request_sock_ops	*rsk_prot;
+	struct timewait_sock_ops *twsk_prot;
 
 	struct module		*owner;
 

commit c1cbe4b7ad0bc4b1d98ea708a3fecb7362aa4088
Author: Benjamin LaHaise <benjamin.c.lahaise@intel.com>
Date:   Tue Dec 13 23:22:19 2005 -0800

    [NET]: Avoid atomic xchg() for non-error case
    
    It also looks like there were 2 places where the test on sk_err was
    missing from the event wait logic (in sk_stream_wait_connect and
    sk_stream_wait_memory), while the rest of the sock_error() users look
    to be doing the right thing.  This version of the patch fixes those,
    and cleans up a few places that were testing ->sk_err directly.
    
    Signed-off-by: Benjamin LaHaise <benjamin.c.lahaise@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 982b4ecd187b..0fbae85c6d55 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1166,7 +1166,10 @@ static inline int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)
  
 static inline int sock_error(struct sock *sk)
 {
-	int err = xchg(&sk->sk_err, 0);
+	int err;
+	if (likely(!sk->sk_err))
+		return 0;
+	err = xchg(&sk->sk_err, 0);
 	return -err;
 }
 

commit 6a438bbe68c7013a42d9c5aee5a40d7dafdbe6ec
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Thu Nov 10 17:14:59 2005 -0800

    [TCP]: speed up SACK processing
    
    Use "hints" to speed up the SACK processing. Various forms
    of this have been used by TCP developers (Web100, STCP, BIC)
    to avoid the 2x linear search of outstanding segments.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index ff13c4cc287a..982b4ecd187b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1247,6 +1247,12 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
 		     skb = skb->next)
 
+/*from STCP for fast SACK Process*/
+#define sk_stream_for_retrans_queue_from(skb, sk)			\
+		for (; (skb != (sk)->sk_send_head) &&                   \
+		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
+		     skb = skb->next)
+
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */

commit 9ee6b535af4c2c97b4e3b88f37f244bf1004ebd4
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Tue Nov 8 09:39:42 2005 -0800

    [NET]: sk_add_backlog convert from macro to inline
    
    There is no reason for sk_add_backlog to be a macro. It can
    just be an inline function and get type checking.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e0498bd36004..ff13c4cc287a 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -461,16 +461,16 @@ static inline void sk_stream_free_skb(struct sock *sk, struct sk_buff *skb)
 }
 
 /* The per-socket spinlock must be held here. */
-#define sk_add_backlog(__sk, __skb)				\
-do {	if (!(__sk)->sk_backlog.tail) {				\
-		(__sk)->sk_backlog.head =			\
-		     (__sk)->sk_backlog.tail = (__skb);		\
-	} else {						\
-		((__sk)->sk_backlog.tail)->next = (__skb);	\
-		(__sk)->sk_backlog.tail = (__skb);		\
-	}							\
-	(__skb)->next = NULL;					\
-} while(0)
+static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
+{
+	if (!sk->sk_backlog.tail) {
+		sk->sk_backlog.head = sk->sk_backlog.tail = skb;
+	} else {
+		sk->sk_backlog.tail->next = skb;
+		sk->sk_backlog.tail = skb;
+	}
+	skb->next = NULL;
+}
 
 #define sk_wait_event(__sk, __timeo, __condition)		\
 ({	int rc;							\

commit 7d877f3bda870ab5f001bd92528654471d5966b3
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 21 03:20:43 2005 -0400

    [PATCH] gfp_t: net/*
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index ecb75526cba0..e0498bd36004 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -207,7 +207,7 @@ struct sock {
 	struct sk_buff_head	sk_write_queue;
 	int			sk_wmem_queued;
 	int			sk_forward_alloc;
-	unsigned int		sk_allocation;
+	gfp_t			sk_allocation;
 	int			sk_sndbuf;
 	int			sk_route_caps;
 	unsigned long 		sk_flags;

commit dd0fc66fb33cd610bc1a5db8a5e232d34879b4d7
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Fri Oct 7 07:46:04 2005 +0100

    [PATCH] gfp flags annotations - part 1
    
     - added typedef unsigned int __nocast gfp_t;
    
     - replaced __nocast uses for gfp flags with gfp_t - it gives exactly
       the same warnings as far as sparse is concerned, doesn't change
       generated code (from gcc point of view we replaced unsigned int with
       typedef) and documents what's going on far better.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index b6440805c420..ecb75526cba0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -739,18 +739,18 @@ extern void FASTCALL(release_sock(struct sock *sk));
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
 extern struct sock		*sk_alloc(int family,
-					  unsigned int __nocast priority,
+					  gfp_t priority,
 					  struct proto *prot, int zero_it);
 extern void			sk_free(struct sock *sk);
 extern struct sock		*sk_clone(const struct sock *sk,
-					  const unsigned int __nocast priority);
+					  const gfp_t priority);
 
 extern struct sk_buff		*sock_wmalloc(struct sock *sk,
 					      unsigned long size, int force,
-					      unsigned int __nocast priority);
+					      gfp_t priority);
 extern struct sk_buff		*sock_rmalloc(struct sock *sk,
 					      unsigned long size, int force,
-					      unsigned int __nocast priority);
+					      gfp_t priority);
 extern void			sock_wfree(struct sk_buff *skb);
 extern void			sock_rfree(struct sk_buff *skb);
 
@@ -766,7 +766,7 @@ extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
 						     int noblock,
 						     int *errcode);
 extern void *sock_kmalloc(struct sock *sk, int size,
-			  unsigned int __nocast priority);
+			  gfp_t priority);
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);
 extern void sk_send_sigurg(struct sock *sk);
 
@@ -1201,7 +1201,7 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 
 static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 						   int size, int mem,
-						   unsigned int __nocast gfp)
+						   gfp_t gfp)
 {
 	struct sk_buff *skb;
 	int hdr_len;
@@ -1224,7 +1224,7 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 
 static inline struct sk_buff *sk_stream_alloc_skb(struct sock *sk,
 						  int size,
-						  unsigned int __nocast gfp)
+						  gfp_t gfp)
 {
 	return sk_stream_alloc_pskb(sk, size, 0, gfp);
 }
@@ -1255,7 +1255,7 @@ static inline int sock_writeable(const struct sock *sk)
 	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf / 2);
 }
 
-static inline unsigned int __nocast gfp_any(void)
+static inline gfp_t gfp_any(void)
 {
 	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
 }

commit 81c3d5470ecc70564eb9209946730fe2be93ad06
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Mon Oct 3 14:13:38 2005 -0700

    [INET]: speedup inet (tcp/dccp) lookups
    
    Arnaldo and I agreed it could be applied now, because I have other
    pending patches depending on this one (Thank you Arnaldo)
    
    (The other important patch moves skc_refcnt in a separate cache line,
    so that the SMP/NUMA performance doesnt suffer from cache line ping pongs)
    
    1) First some performance data :
    --------------------------------
    
    tcp_v4_rcv() wastes a *lot* of time in __inet_lookup_established()
    
    The most time critical code is :
    
    sk_for_each(sk, node, &head->chain) {
         if (INET_MATCH(sk, acookie, saddr, daddr, ports, dif))
             goto hit; /* You sunk my battleship! */
    }
    
    The sk_for_each() does use prefetch() hints but only the begining of
    "struct sock" is prefetched.
    
    As INET_MATCH first comparison uses inet_sk(__sk)->daddr, wich is far
    away from the begining of "struct sock", it has to bring into CPU
    cache cold cache line. Each iteration has to use at least 2 cache
    lines.
    
    This can be problematic if some chains are very long.
    
    2) The goal
    -----------
    
    The idea I had is to change things so that INET_MATCH() may return
    FALSE in 99% of cases only using the data already in the CPU cache,
    using one cache line per iteration.
    
    3) Description of the patch
    ---------------------------
    
    Adds a new 'unsigned int skc_hash' field in 'struct sock_common',
    filling a 32 bits hole on 64 bits platform.
    
    struct sock_common {
            unsigned short          skc_family;
            volatile unsigned char  skc_state;
            unsigned char           skc_reuse;
            int                     skc_bound_dev_if;
            struct hlist_node       skc_node;
            struct hlist_node       skc_bind_node;
            atomic_t                skc_refcnt;
    +       unsigned int            skc_hash;
            struct proto            *skc_prot;
    };
    
    Store in this 32 bits field the full hash, not masked by (ehash_size -
    1) Using this full hash as the first comparison done in INET_MATCH
    permits us immediatly skip the element without touching a second cache
    line in case of a miss.
    
    Suppress the sk_hashent/tw_hashent fields since skc_hash (aliased to
    sk_hash and tw_hash) already contains the slot number if we mask with
    (ehash_size - 1)
    
    File include/net/inet_hashtables.h
    
    64 bits platforms :
    #define INET_MATCH(__sk, __hash, __cookie, __saddr, __daddr, __ports, __dif)\
         (((__sk)->sk_hash == (__hash))
         ((*((__u64 *)&(inet_sk(__sk)->daddr)))== (__cookie))   &&  \
         ((*((__u32 *)&(inet_sk(__sk)->dport))) == (__ports))   &&  \
         (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
    
    32bits platforms:
    #define TCP_IPV4_MATCH(__sk, __hash, __cookie, __saddr, __daddr, __ports, __dif)\
         (((__sk)->sk_hash == (__hash))                 &&  \
         (inet_sk(__sk)->daddr          == (__saddr))   &&  \
         (inet_sk(__sk)->rcv_saddr      == (__daddr))   &&  \
         (!((__sk)->sk_bound_dev_if) || ((__sk)->sk_bound_dev_if == (__dif))))
    
    
    - Adds a prefetch(head->chain.first) in
    __inet_lookup_established()/__tcp_v4_check_established() and
    __inet6_lookup_established()/__tcp_v6_check_established() and
    __dccp_v4_check_established() to bring into cache the first element of the
    list, before the {read|write}_lock(&head->lock);
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Acked-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8c48fbecb7cf..b6440805c420 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -99,6 +99,7 @@ struct proto;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
  *	@skc_refcnt: reference count
+ *	@skc_hash: hash value used with various protocol lookup tables
  *	@skc_prot: protocol handlers inside a network family
  *
  *	This is the minimal network layer representation of sockets, the header
@@ -112,6 +113,7 @@ struct sock_common {
 	struct hlist_node	skc_node;
 	struct hlist_node	skc_bind_node;
 	atomic_t		skc_refcnt;
+	unsigned int		skc_hash;
 	struct proto		*skc_prot;
 };
 
@@ -139,7 +141,6 @@ struct sock_common {
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_lingertime: %SO_LINGER l_linger setting
-  *	@sk_hashent: hash entry in several tables (e.g. inet_hashinfo.ehash)
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
   *	@sk_error_queue: rarely used
@@ -186,6 +187,7 @@ struct sock {
 #define sk_node			__sk_common.skc_node
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_refcnt		__sk_common.skc_refcnt
+#define sk_hash			__sk_common.skc_hash
 #define sk_prot			__sk_common.skc_prot
 	unsigned char		sk_shutdown : 2,
 				sk_no_check : 2,
@@ -208,7 +210,6 @@ struct sock {
 	unsigned int		sk_allocation;
 	int			sk_sndbuf;
 	int			sk_route_caps;
-	int			sk_hashent;
 	unsigned long 		sk_flags;
 	unsigned long	        sk_lingertime;
 	/*

commit 6baf1f417d092bd2de7c8892cecad456024c993f
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Mon Sep 5 18:14:11 2005 -0700

    [NET]: Do not protect sysctl_optmem_max with CONFIG_SYSCTL
    
    The ipv4 and ipv6 protocols need to access it unconditionally.
    SYSCTL=n build failure reported by Russell King.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cf628261da52..8c48fbecb7cf 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1377,9 +1377,10 @@ extern void sk_init(void);
 
 #ifdef CONFIG_SYSCTL
 extern struct ctl_table core_table[];
-extern int sysctl_optmem_max;
 #endif
 
+extern int sysctl_optmem_max;
+
 extern __u32 sysctl_wmem_default;
 extern __u32 sysctl_rmem_default;
 

commit ef015786152adaff5a6a8bf0c8ea2f70cee8059d
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Sep 1 17:48:59 2005 -0700

    [TCP]: Fix sk_forward_alloc underflow in tcp_sendmsg
    
    I've finally found a potential cause of the sk_forward_alloc underflows
    that people have been reporting sporadically.
    
    When tcp_sendmsg tacks on extra bits to an existing TCP_PAGE we don't
    check sk_forward_alloc even though a large amount of time may have
    elapsed since we allocated the page.  In the mean time someone could've
    come along and liberated packets and reclaimed sk_forward_alloc memory.
    
    This patch makes tcp_sendmsg check sk_forward_alloc every time as we
    do in do_tcp_sendpages.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e51e626e9af1..cf628261da52 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1232,9 +1232,8 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 {
 	struct page *page = NULL;
 
-	if (sk_stream_wmem_schedule(sk, PAGE_SIZE))
-		page = alloc_pages(sk->sk_allocation, 0);
-	else {
+	page = alloc_pages(sk->sk_allocation, 0);
+	if (!page) {
 		sk->sk_prot->enter_memory_pressure();
 		sk_stream_moderate_sndbuf(sk);
 	}

commit d80d99d643090c3cf2b1f9fb3fadd1256f7e384f
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Sep 1 17:48:23 2005 -0700

    [NET]: Add sk_stream_wmem_schedule
    
    This patch introduces sk_stream_wmem_schedule as a short-hand for
    the sk_forward_alloc checking on egress.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 312cb25cbd18..e51e626e9af1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -709,6 +709,12 @@ static inline int sk_stream_rmem_schedule(struct sock *sk, struct sk_buff *skb)
 		sk_stream_mem_schedule(sk, skb->truesize, 1);
 }
 
+static inline int sk_stream_wmem_schedule(struct sock *sk, int size)
+{
+	return size <= sk->sk_forward_alloc ||
+	       sk_stream_mem_schedule(sk, size, 0);
+}
+
 /* Used by processes to "lock" a socket state, so that
  * interrupts and bottom half handlers won't change it
  * from under us. It essentially blocks any incoming
@@ -1203,8 +1209,7 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 	skb = alloc_skb_fclone(size + hdr_len, gfp);
 	if (skb) {
 		skb->truesize += mem;
-		if (sk->sk_forward_alloc >= (int)skb->truesize ||
-		    sk_stream_mem_schedule(sk, skb->truesize, 0)) {
+		if (sk_stream_wmem_schedule(sk, skb->truesize)) {
 			skb_reserve(skb, hdr_len);
 			return skb;
 		}
@@ -1227,8 +1232,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 {
 	struct page *page = NULL;
 
-	if (sk->sk_forward_alloc >= (int)PAGE_SIZE ||
-	    sk_stream_mem_schedule(sk, PAGE_SIZE, 0))
+	if (sk_stream_wmem_schedule(sk, PAGE_SIZE))
 		page = alloc_pages(sk->sk_allocation, 0);
 	else {
 		sk->sk_prot->enter_memory_pressure();

commit 8cd25c1fcfbf6460983e99091d278187421c1a1d
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sat Aug 20 17:14:11 2005 -0700

    [NET]: fix PROC_FS=n compile
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d57aece9492c..312cb25cbd18 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1377,9 +1377,7 @@ extern struct ctl_table core_table[];
 extern int sysctl_optmem_max;
 #endif
 
-#ifdef CONFIG_PROC_FS
 extern __u32 sysctl_wmem_default;
 extern __u32 sysctl_rmem_default;
-#endif
 
 #endif	/* _SOCK_H */

commit d179cd12928443f3ec29cfbc3567439644bd0afc
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Aug 17 14:57:30 2005 -0700

    [NET]: Implement SKB fast cloning.
    
    Protocols that make extensive use of SKB cloning,
    for example TCP, eat at least 2 allocations per
    packet sent as a result.
    
    To cut the kmalloc() count in half, we implement
    a pre-allocation scheme wherein we allocate
    2 sk_buff objects in advance, then use a simple
    reference count to free up the memory at the
    correct time.
    
    Based upon an initial patch by Thomas Graf and
    suggestions from Herbert Xu.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 14183883e8e6..d57aece9492c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1200,7 +1200,7 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 	int hdr_len;
 
 	hdr_len = SKB_DATA_ALIGN(sk->sk_prot->max_header);
-	skb = alloc_skb(size + hdr_len, gfp);
+	skb = alloc_skb_fclone(size + hdr_len, gfp);
 	if (skb) {
 		skb->truesize += mem;
 		if (sk->sk_forward_alloc >= (int)skb->truesize ||

commit 20380731bc2897f2952ae055420972ded4cd786e
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Aug 16 02:18:02 2005 -0300

    [NET]: Fix sparse warnings
    
    Of this type, mostly:
    
    CHECK   net/ipv6/netfilter.c
    net/ipv6/netfilter.c:96:12: warning: symbol 'ipv6_netfilter_init' was not declared. Should it be static?
    net/ipv6/netfilter.c:101:6: warning: symbol 'ipv6_netfilter_fini' was not declared. Should it be static?
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index d59428877078..14183883e8e6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1370,4 +1370,16 @@ static inline int siocdevprivate_ioctl(unsigned int fd, unsigned int cmd, unsign
 }
 #endif
 
+extern void sk_init(void);
+
+#ifdef CONFIG_SYSCTL
+extern struct ctl_table core_table[];
+extern int sysctl_optmem_max;
+#endif
+
+#ifdef CONFIG_PROC_FS
+extern __u32 sysctl_wmem_default;
+extern __u32 sysctl_rmem_default;
+#endif
+
 #endif	/* _SOCK_H */

commit a61bbcf28a8cb0ba56f8193d512f7222e711a294
Author: Patrick McHardy <kaber@trash.net>
Date:   Sun Aug 14 17:24:31 2005 -0700

    [NET]: Store skb->timestamp as offset to a base timestamp
    
    Reduces skb size by 8 bytes on 64-bit.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 065df67b6422..d59428877078 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1282,16 +1282,19 @@ static inline int sock_intr_errno(long timeo)
 static __inline__ void
 sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 {
-	struct timeval *stamp = &skb->stamp;
+	struct timeval stamp;
+
+	skb_get_timestamp(skb, &stamp);
 	if (sock_flag(sk, SOCK_RCVTSTAMP)) {
 		/* Race occurred between timestamp enabling and packet
 		   receiving.  Fill in the current time for now. */
-		if (stamp->tv_sec == 0)
-			do_gettimeofday(stamp);
+		if (stamp.tv_sec == 0)
+			do_gettimeofday(&stamp);
+		skb_set_timestamp(skb, &stamp);
 		put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP, sizeof(struct timeval),
-			 stamp);
+			 &stamp);
 	} else
-		sk->sk_stamp = *stamp;
+		sk->sk_stamp = stamp;
 }
 
 /**

commit 64ce207306debd7157f47282be94770407bec01c
Author: Patrick McHardy <kaber@trash.net>
Date:   Tue Aug 9 20:50:53 2005 -0700

    [NET]: Make NETDEBUG pure printk wrappers
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 8678313a22b4..065df67b6422 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1316,11 +1316,11 @@ extern int sock_get_timestamp(struct sock *, struct timeval __user *);
  */
 
 #if 0
-#define NETDEBUG(x)	do { } while (0)
-#define LIMIT_NETDEBUG(x) do {} while(0)
+#define NETDEBUG(fmt, args...)	do { } while (0)
+#define LIMIT_NETDEBUG(fmt, args...) do { } while(0)
 #else
-#define NETDEBUG(x)	do { x; } while (0)
-#define LIMIT_NETDEBUG(x) do { if (net_ratelimit()) { x; } } while(0)
+#define NETDEBUG(fmt, args...)	printk(fmt,##args)
+#define LIMIT_NETDEBUG(fmt, args...) do { if (net_ratelimit()) printk(fmt,##args); } while(0)
 #endif
 
 /*

commit 0a5578cf8e5e045aaa68643c17ce885426697c6b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:41 2005 -0700

    [ICSK]: Generalise tcp_listen_{start,stop}
    
    This also moved inet_iif from tcp to inet_hashtables.h, as it is
    needed by the inet_lookup callers, perhaps this needs a bit of
    polishing, but for now seems fine.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 48cc337a6566..8678313a22b4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -558,6 +558,7 @@ struct proto {
 
 	kmem_cache_t		*twsk_slab;
 	unsigned int		twsk_obj_size;
+	atomic_t		*orphan_count;
 
 	struct request_sock_ops	*rsk_prot;
 

commit 463c84b97f24010a67cd871746d6a7e4c925a5f9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:10:42 2005 -0700

    [NET]: Introduce inet_connection_sock
    
    This creates struct inet_connection_sock, moving members out of struct
    tcp_sock that are shareable with other INET connection oriented
    protocols, such as DCCP, that in my private tree already uses most of
    these members.
    
    The functions that operate on these members were renamed, using a
    inet_csk_ prefix while not being moved yet to a new file, so as to
    ease the review of these changes.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 828dc082fcb7..48cc337a6566 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -493,9 +493,6 @@ extern int sk_wait_data(struct sock *sk, long *timeo);
 
 struct request_sock_ops;
 
-/* Here is the right place to enable sock refcounting debugging */
-//#define SOCK_REFCNT_DEBUG
-
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
  * transport -> network interface is defined by struct inet_proto

commit 87d11ceb9deb7a3f13fdee6e89d9bb6be7d27a71
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:10:12 2005 -0700

    [SOCK]: Introduce sk_clone
    
    Out of tcp_create_openreq_child, will be used in
    dccp_create_openreq_child, and is a nice sock function anyway.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index bdae0a5eadf5..828dc082fcb7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -737,6 +737,8 @@ extern struct sock		*sk_alloc(int family,
 					  unsigned int __nocast priority,
 					  struct proto *prot, int zero_it);
 extern void			sk_free(struct sock *sk);
+extern struct sock		*sk_clone(const struct sock *sk,
+					  const unsigned int __nocast priority);
 
 extern struct sk_buff		*sock_wmalloc(struct sock *sk,
 					      unsigned long size, int force,

commit e48c414ee61f4ac8d5cff2973e66a7cbc8a93aa5
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:09:46 2005 -0700

    [INET]: Generalise the TCP sock ID lookup routines
    
    And also some TIME_WAIT functions.
    
    [acme@toy net-2.6.14]$ grep built-in /tmp/before.size /tmp/after.size
    /tmp/before.size: 282955   13122    9312  305389   4a8ed net/ipv4/built-in.o
    /tmp/after.size:  281566   13122    9312  304000   4a380 net/ipv4/built-in.o
    [acme@toy net-2.6.14]$
    
    I kept them still inlined, will uninline at some point to see what
    would be the performance difference.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index c902c57bf2b7..bdae0a5eadf5 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -255,28 +255,28 @@ struct sock {
 /*
  * Hashed lists helper routines
  */
-static inline struct sock *__sk_head(struct hlist_head *head)
+static inline struct sock *__sk_head(const struct hlist_head *head)
 {
 	return hlist_entry(head->first, struct sock, sk_node);
 }
 
-static inline struct sock *sk_head(struct hlist_head *head)
+static inline struct sock *sk_head(const struct hlist_head *head)
 {
 	return hlist_empty(head) ? NULL : __sk_head(head);
 }
 
-static inline struct sock *sk_next(struct sock *sk)
+static inline struct sock *sk_next(const struct sock *sk)
 {
 	return sk->sk_node.next ?
 		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
 }
 
-static inline int sk_unhashed(struct sock *sk)
+static inline int sk_unhashed(const struct sock *sk)
 {
 	return hlist_unhashed(&sk->sk_node);
 }
 
-static inline int sk_hashed(struct sock *sk)
+static inline int sk_hashed(const struct sock *sk)
 {
 	return sk->sk_node.pprev != NULL;
 }
@@ -494,7 +494,7 @@ extern int sk_wait_data(struct sock *sk, long *timeo);
 struct request_sock_ops;
 
 /* Here is the right place to enable sock refcounting debugging */
-#define SOCK_REFCNT_DEBUG
+//#define SOCK_REFCNT_DEBUG
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface

commit 8feaf0c0a5488b3d898a9c207eb6678f44ba3f26
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:09:30 2005 -0700

    [INET]: Generalise tcp_tw_bucket, aka TIME_WAIT sockets
    
    This paves the way to generalise the rest of the sock ID lookup
    routines and saves some bytes in TCPv4 TIME_WAIT sockets on distro
    kernels (where IPv6 is always built as a module):
    
    [root@qemu ~]# grep tw_sock /proc/slabinfo
    tw_sock_TCPv6  0  0  128  31  1
    tw_sock_TCP    0  0   96  41  1
    [root@qemu ~]#
    
    Now if a protocol wants to use the TIME_WAIT generic infrastructure it
    only has to set the sk_prot->twsk_obj_size field with the size of its
    inet_timewait_sock derived sock and proto_register will create
    sk_prot->twsk_slab, for now its only for INET sockets, but we can
    introduce timewait_sock later if some non INET transport protocolo
    wants to use this stuff.
    
    Next changesets will take advantage of this new infrastructure to
    generalise even more TCP code.
    
    [acme@toy net-2.6.14]$ grep built-in /tmp/before.size /tmp/after.size
    /tmp/before.size: 188646   11764    5068  205478   322a6 net/ipv4/built-in.o
    /tmp/after.size:  188144   11764    5068  204976   320b0 net/ipv4/built-in.o
    [acme@toy net-2.6.14]$
    
    Tested with both IPv4 & IPv6 (::1 (localhost) & ::ffff:172.20.0.1
    (qemu host)).
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 391d00b5b7b4..c902c57bf2b7 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -88,6 +88,7 @@ do {	spin_lock_init(&((__sk)->sk_lock.slock)); \
 } while(0)
 
 struct sock;
+struct proto;
 
 /**
  *	struct sock_common - minimal network layer representation of sockets
@@ -98,10 +99,11 @@ struct sock;
  *	@skc_node: main hash linkage for various protocol lookup tables
  *	@skc_bind_node: bind hash linkage for various protocol lookup tables
  *	@skc_refcnt: reference count
+ *	@skc_prot: protocol handlers inside a network family
  *
  *	This is the minimal network layer representation of sockets, the header
- *	for struct sock and struct tcp_tw_bucket.
-  */
+ *	for struct sock and struct inet_timewait_sock.
+ */
 struct sock_common {
 	unsigned short		skc_family;
 	volatile unsigned char	skc_state;
@@ -110,11 +112,12 @@ struct sock_common {
 	struct hlist_node	skc_node;
 	struct hlist_node	skc_bind_node;
 	atomic_t		skc_refcnt;
+	struct proto		*skc_prot;
 };
 
 /**
   *	struct sock - network layer representation of sockets
-  *	@__sk_common: shared layout with tcp_tw_bucket
+  *	@__sk_common: shared layout with inet_timewait_sock
   *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
   *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
   *	@sk_lock:	synchronizer
@@ -140,7 +143,6 @@ struct sock_common {
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
   *	@sk_error_queue: rarely used
-  *	@sk_prot: protocol handlers inside a network family
   *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt, IPV6_ADDRFORM for instance)
   *	@sk_err: last error
   *	@sk_err_soft: errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
@@ -173,7 +175,7 @@ struct sock_common {
  */
 struct sock {
 	/*
-	 * Now struct tcp_tw_bucket also uses sock_common, so please just
+	 * Now struct inet_timewait_sock also uses sock_common, so please just
 	 * don't add nothing before this first member (__sk_common) --acme
 	 */
 	struct sock_common	__sk_common;
@@ -184,6 +186,7 @@ struct sock {
 #define sk_node			__sk_common.skc_node
 #define sk_bind_node		__sk_common.skc_bind_node
 #define sk_refcnt		__sk_common.skc_refcnt
+#define sk_prot			__sk_common.skc_prot
 	unsigned char		sk_shutdown : 2,
 				sk_no_check : 2,
 				sk_userlocks : 4;
@@ -218,7 +221,6 @@ struct sock {
 		struct sk_buff *tail;
 	} sk_backlog;
 	struct sk_buff_head	sk_error_queue;
-	struct proto		*sk_prot;
 	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
@@ -557,6 +559,9 @@ struct proto {
 	kmem_cache_t		*slab;
 	unsigned int		obj_size;
 
+	kmem_cache_t		*twsk_slab;
+	unsigned int		twsk_obj_size;
+
 	struct request_sock_ops	*rsk_prot;
 
 	struct module		*owner;

commit 6e04e02165a7209a71db553b7bc48d68421e5ebf
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:07:35 2005 -0700

    [INET]: Move tcp_port_rover to inet_hashinfo
    
    Also expose all of the tcp_hashinfo members, i.e. killing those
    tcp_ehash, etc macros, this will more clearly expose already generic
    functions and some that need just a bit of work to become generic, as
    we'll see in the upcoming changesets.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 69d869e41c35..391d00b5b7b4 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -136,7 +136,7 @@ struct sock_common {
   *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
   *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
   *	@sk_lingertime: %SO_LINGER l_linger setting
-  *	@sk_hashent: hash entry in several tables (e.g. tcp_ehash)
+  *	@sk_hashent: hash entry in several tables (e.g. inet_hashinfo.ehash)
   *	@sk_backlog: always used with the per-socket spinlock held
   *	@sk_callback_lock: used with the callbacks in the end of this struct
   *	@sk_error_queue: rarely used

commit 6cbb0df788b90777a7ed0f9d8261260353f48076
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:49:02 2005 -0700

    [SOCK]: Introduce sk_setup_caps
    
    From tcp_v4_setup_caps, that always is preceded by a call to
    __sk_dst_set, so coalesce this sequence into sk_setup_caps, removing
    one call to a TCP function in the IP layer.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index f91ee82522ff..69d869e41c35 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1025,6 +1025,16 @@ sk_dst_check(struct sock *sk, u32 cookie)
 	return dst;
 }
 
+static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
+{
+	__sk_dst_set(sk, dst);
+	sk->sk_route_caps = dst->dev->features;
+	if (sk->sk_route_caps & NETIF_F_TSO) {
+		if (sock_flag(sk, SOCK_NO_LARGESEND) || dst->header_len)
+			sk->sk_route_caps &= ~NETIF_F_TSO;
+	}
+}
+
 static inline void sk_charge_skb(struct sock *sk, struct sk_buff *skb)
 {
 	sk->sk_wmem_queued   += skb->truesize;

commit 614c6cb4f225a7da9f13e5dd0fac3b531078eb9f
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:47:37 2005 -0700

    [SOCK]: Rename __tcp_v4_rehash to __sk_prot_rehash
    
    This operation was already generic and DCCP will use it.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 11b81551041e..f91ee82522ff 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -612,6 +612,15 @@ static __inline__ void sock_prot_dec_use(struct proto *prot)
 	prot->stats[smp_processor_id()].inuse--;
 }
 
+/* With per-bucket locks this operation is not-atomic, so that
+ * this version is not worse.
+ */
+static inline void __sk_prot_rehash(struct sock *sk)
+{
+	sk->sk_prot->unhash(sk);
+	sk->sk_prot->hash(sk);
+}
+
 /* About 10 seconds */
 #define SOCK_DESTROY_TIME (10*HZ)
 

commit e6848976b721eeb5551cd94673faafeef78d9f35
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:45:38 2005 -0700

    [NET]: Cleanup INET_REFCNT_DEBUG code
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e9b1dbab90d0..11b81551041e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -491,6 +491,9 @@ extern int sk_wait_data(struct sock *sk, long *timeo);
 
 struct request_sock_ops;
 
+/* Here is the right place to enable sock refcounting debugging */
+#define SOCK_REFCNT_DEBUG
+
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
  * transport -> network interface is defined by struct inet_proto
@@ -561,7 +564,9 @@ struct proto {
 	char			name[32];
 
 	struct list_head	node;
-
+#ifdef SOCK_REFCNT_DEBUG
+	atomic_t		socks;
+#endif
 	struct {
 		int inuse;
 		u8  __pad[SMP_CACHE_BYTES - sizeof(int)];
@@ -571,6 +576,31 @@ struct proto {
 extern int proto_register(struct proto *prot, int alloc_slab);
 extern void proto_unregister(struct proto *prot);
 
+#ifdef SOCK_REFCNT_DEBUG
+static inline void sk_refcnt_debug_inc(struct sock *sk)
+{
+	atomic_inc(&sk->sk_prot->socks);
+}
+
+static inline void sk_refcnt_debug_dec(struct sock *sk)
+{
+	atomic_dec(&sk->sk_prot->socks);
+	printk(KERN_DEBUG "%s socket %p released, %d are still alive\n",
+	       sk->sk_prot->name, sk, atomic_read(&sk->sk_prot->socks));
+}
+
+static inline void sk_refcnt_debug_release(const struct sock *sk)
+{
+	if (atomic_read(&sk->sk_refcnt) != 1)
+		printk(KERN_DEBUG "Destruction of the %s socket %p delayed, refcnt=%d\n",
+		       sk->sk_prot->name, sk, atomic_read(&sk->sk_refcnt));
+}
+#else /* SOCK_REFCNT_DEBUG */
+#define sk_refcnt_debug_inc(sk) do { } while (0)
+#define sk_refcnt_debug_dec(sk) do { } while (0)
+#define sk_refcnt_debug_release(sk) do { } while (0)
+#endif /* SOCK_REFCNT_DEBUG */
+
 /* Called with local bh disabled */
 static __inline__ void sock_prot_inc_use(struct proto *prot)
 {

commit 53b924b31fa53ac3007df3fef6870d5074a9adf8
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Aug 23 10:11:30 2005 -0700

    [NET]: Fix socket bitop damage
    
    The socket flag cleanups that went into 2.6.12-rc1 are basically oring
    the flags of an old socket into the socket just being created.
    Unfortunately that one was just initialized by sock_init_data(), so already
    has SOCK_ZAPPED set.  As the result zapped sockets are created and all
    incoming connection will fail due to this bug which again was carefully
    replicated to at least AX.25, NET/ROM or ROSE.
    
    In order to keep the abstraction alive I've introduced sock_copy_flags()
    to copy the socket flags from one sockets to another and used that
    instead of the bitwise copy thing.  Anyway, the idea here has probably
    been to copy all flags, so sock_copy_flags() should be the right thing.
    With this the ham radio protocols are usable again, so I hope this will
    make it into 2.6.13.
    
    Signed-off-by: Ralf Baechle DL5RB <ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a1042d08becd..e9b1dbab90d0 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -384,6 +384,11 @@ enum sock_flags {
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 };
 
+static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
+{
+	nsk->sk_flags = osk->sk_flags;
+}
+
 static inline void sock_set_flag(struct sock *sk, enum sock_flags flag)
 {
 	__set_bit(flag, &sk->sk_flags);

commit 86a76caf8705e3524e15f343f3c4806939a06dc8
Author: Victor Fusco <victor@cetuc.puc-rio.br>
Date:   Fri Jul 8 14:57:47 2005 -0700

    [NET]: Fix sparse warnings
    
    From: Victor Fusco <victor@cetuc.puc-rio.br>
    
    Fix the sparse warning "implicit cast to nocast type"
    
    Signed-off-by: Victor Fusco <victor@cetuc.puc-rio.br>
    Signed-off-by: Domen Puncer <domen@coderock.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 7b76f891ae2d..a1042d08becd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -684,16 +684,17 @@ extern void FASTCALL(release_sock(struct sock *sk));
 #define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
 #define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
 
-extern struct sock		*sk_alloc(int family, int priority,
+extern struct sock		*sk_alloc(int family,
+					  unsigned int __nocast priority,
 					  struct proto *prot, int zero_it);
 extern void			sk_free(struct sock *sk);
 
 extern struct sk_buff		*sock_wmalloc(struct sock *sk,
 					      unsigned long size, int force,
-					      int priority);
+					      unsigned int __nocast priority);
 extern struct sk_buff		*sock_rmalloc(struct sock *sk,
 					      unsigned long size, int force,
-					      int priority);
+					      unsigned int __nocast priority);
 extern void			sock_wfree(struct sk_buff *skb);
 extern void			sock_rfree(struct sk_buff *skb);
 
@@ -708,7 +709,8 @@ extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
 						     unsigned long size,
 						     int noblock,
 						     int *errcode);
-extern void *sock_kmalloc(struct sock *sk, int size, int priority);
+extern void *sock_kmalloc(struct sock *sk, int size,
+			  unsigned int __nocast priority);
 extern void sock_kfree_s(struct sock *sk, void *mem, int size);
 extern void sk_send_sigurg(struct sock *sk);
 
@@ -1132,7 +1134,8 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 }
 
 static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
-						   int size, int mem, int gfp)
+						   int size, int mem,
+						   unsigned int __nocast gfp)
 {
 	struct sk_buff *skb;
 	int hdr_len;
@@ -1155,7 +1158,8 @@ static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 }
 
 static inline struct sk_buff *sk_stream_alloc_skb(struct sock *sk,
-						  int size, int gfp)
+						  int size,
+						  unsigned int __nocast gfp)
 {
 	return sk_stream_alloc_pskb(sk, size, 0, gfp);
 }
@@ -1188,7 +1192,7 @@ static inline int sock_writeable(const struct sock *sk)
 	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf / 2);
 }
 
-static inline int gfp_any(void)
+static inline unsigned int __nocast gfp_any(void)
 {
 	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
 }

commit c65f7f00c587828e3d50737805a78f74804972de
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 5 15:17:25 2005 -0700

    [TCP]: Simplify SKB data portion allocation with NETIF_F_SG.
    
    The ideal and most optimal layout for an SKB when doing
    scatter-gather is to put all the headers at skb->data, and
    all the user data in the page array.
    
    This makes SKB splitting and combining extremely simple,
    especially before a packet goes onto the wire the first
    time.
    
    So, when sk_stream_alloc_pskb() is given a zero size, make
    sure there is no skb_tailroom().  This is achieved by applying
    SKB_DATA_ALIGN() to the header length used here.
    
    Next, make select_size() in TCP output segmentation use a
    length of zero when NETIF_F_SG is true on the outgoing
    interface.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index e593af5b1ecc..7b76f891ae2d 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1134,13 +1134,16 @@ static inline void sk_stream_moderate_sndbuf(struct sock *sk)
 static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
 						   int size, int mem, int gfp)
 {
-	struct sk_buff *skb = alloc_skb(size + sk->sk_prot->max_header, gfp);
+	struct sk_buff *skb;
+	int hdr_len;
 
+	hdr_len = SKB_DATA_ALIGN(sk->sk_prot->max_header);
+	skb = alloc_skb(size + hdr_len, gfp);
 	if (skb) {
 		skb->truesize += mem;
 		if (sk->sk_forward_alloc >= (int)skb->truesize ||
 		    sk_stream_mem_schedule(sk, skb->truesize, 0)) {
-			skb_reserve(skb, sk->sk_prot->max_header);
+			skb_reserve(skb, hdr_len);
 			return skb;
 		}
 		__kfree_skb(skb);

commit 60236fdd08b2169045a3bbfc5ffe1576e6c3c17b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:21 2005 -0700

    [NET] Rename open_request to request_sock
    
    Ok, this one just renames some stuff to have a better namespace and to
    dissassociate it from TCP:
    
    struct open_request  -> struct request_sock
    tcp_openreq_alloc    -> reqsk_alloc
    tcp_openreq_free     -> reqsk_free
    tcp_openreq_fastfree -> __reqsk_free
    
    With this most of the infrastructure closely resembles a struct
    sock methods subset.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index 6919276af8af..e593af5b1ecc 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -484,7 +484,7 @@ extern void sk_stream_kill_queues(struct sock *sk);
 
 extern int sk_wait_data(struct sock *sk, long *timeo);
 
-struct or_calltable;
+struct request_sock_ops;
 
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
@@ -549,7 +549,7 @@ struct proto {
 	kmem_cache_t		*slab;
 	unsigned int		obj_size;
 
-	struct or_calltable	*rsk_prot;
+	struct request_sock_ops	*rsk_prot;
 
 	struct module		*owner;
 

commit 2e6599cb899ba4b133f42cbf9d2b1883d2dc583a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:46:52 2005 -0700

    [NET] Generalise TCP's struct open_request minisock infrastructure
    
    Kept this first changeset minimal, without changing existing names to
    ease peer review.
    
    Basicaly tcp_openreq_alloc now receives the or_calltable, that in turn
    has two new members:
    
    ->slab, that replaces tcp_openreq_cachep
    ->obj_size, to inform the size of the openreq descendant for
      a specific protocol
    
    The protocol specific fields in struct open_request were moved to a
    class hierarchy, with the things that are common to all connection
    oriented PF_INET protocols in struct inet_request_sock, the TCP ones
    in tcp_request_sock, that is an inet_request_sock, that is an
    open_request.
    
    I.e. this uses the same approach used for the struct sock class
    hierarchy, with sk_prot indicating if the protocol wants to use the
    open_request infrastructure by filling in sk_prot->rsk_prot with an
    or_calltable.
    
    Results? Performance is improved and TCP v4 now uses only 64 bytes per
    open request minisock, down from 96 without this patch :-)
    
    Next changeset will rename some of the structs, fields and functions
    mentioned above, struct or_calltable is way unclear, better name it
    struct request_sock_ops, s/struct open_request/struct request_sock/g,
    etc.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index a9ef3a6a13f3..6919276af8af 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -484,6 +484,8 @@ extern void sk_stream_kill_queues(struct sock *sk);
 
 extern int sk_wait_data(struct sock *sk, long *timeo);
 
+struct or_calltable;
+
 /* Networking protocol blocks we attach to sockets.
  * socket layer -> transport layer interface
  * transport -> network interface is defined by struct inet_proto
@@ -547,6 +549,8 @@ struct proto {
 	kmem_cache_t		*slab;
 	unsigned int		obj_size;
 
+	struct or_calltable	*rsk_prot;
+
 	struct module		*owner;
 
 	char			name[32];

commit 02c30a84e6298b6b20a56f0896ac80b47839e134
Author: Jesper Juhl <juhl-lkml@dif.dk>
Date:   Thu May 5 16:16:16 2005 -0700

    [PATCH] update Ross Biro bouncing email address
    
    Ross moved.  Remove the bad email address so people will find the correct
    one in ./CREDITS.
    
    Signed-off-by: Jesper Juhl <juhl-lkml@dif.dk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 77f02f86346e..a9ef3a6a13f3 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -7,7 +7,7 @@
  *
  * Version:	@(#)sock.h	1.0.4	05/13/93
  *
- * Authors:	Ross Biro, <bir7@leland.Stanford.Edu>
+ * Authors:	Ross Biro
  *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
  *		Corey Minyard <wf-rch!minyard@relay.EU.net>
  *		Florian La Roche <flla@stud.uni-sb.de>

commit 476e19cfa131e2b6eedc4017b627cdc4ca419ffb
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Thu May 5 13:35:15 2005 -0700

    [IPV6]: Fix OOPS when using IPV6_ADDRFORM
    
    This causes sk->sk_prot to change, which makes the socket
    release free the sock into the wrong SLAB cache.  Fix this
    by introducing sk_prot_creator so that we always remember
    where the sock came from.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock.h b/include/net/sock.h
index cc4c9190b7fd..77f02f86346e 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -141,6 +141,7 @@ struct sock_common {
   *	@sk_callback_lock: used with the callbacks in the end of this struct
   *	@sk_error_queue: rarely used
   *	@sk_prot: protocol handlers inside a network family
+  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt, IPV6_ADDRFORM for instance)
   *	@sk_err: last error
   *	@sk_err_soft: errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
   *	@sk_ack_backlog: current listen backlog
@@ -218,6 +219,7 @@ struct sock {
 	} sk_backlog;
 	struct sk_buff_head	sk_error_queue;
 	struct proto		*sk_prot;
+	struct proto		*sk_prot_creator;
 	rwlock_t		sk_callback_lock;
 	int			sk_err,
 				sk_err_soft;

commit 67be2dd1bace0ec7ce2dbc1bba3f8df3d7be597e
Author: Martin Waitz <tali@admingilde.org>
Date:   Sun May 1 08:59:26 2005 -0700

    [PATCH] DocBook: fix some descriptions
    
    Some KernelDoc descriptions are updated to match the current code.
    No code changes.
    
    Signed-off-by: Martin Waitz <tali@admingilde.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index 5bc180adfb14..cc4c9190b7fd 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -161,6 +161,7 @@ struct sock_common {
   *	@sk_sndmsg_page: cached page for sendmsg
   *	@sk_sndmsg_off: cached offset for sendmsg
   *	@sk_send_head: front of stuff to transmit
+  *	@sk_security: used by security modules
   *	@sk_write_pending: a write to stream socket waits to start
   *	@sk_state_change: callback to indicate change in the state of the sock
   *	@sk_data_ready: callback to indicate there is data to be processed

commit 4dc3b16ba18c0f967ad100c52fa65b01a4f76ff0
Author: Pavel Pisa <pisa@cmp.felk.cvut.cz>
Date:   Sun May 1 08:59:25 2005 -0700

    [PATCH] DocBook: changes and extensions to the kernel documentation
    
    I have recompiled Linux kernel 2.6.11.5 documentation for me and our
    university students again.  The documentation could be extended for more
    sources which are equipped by structured comments for recent 2.6 kernels.  I
    have tried to proceed with that task.  I have done that more times from 2.6.0
    time and it gets boring to do same changes again and again.  Linux kernel
    compiles after changes for i386 and ARM targets.  I have added references to
    some more files into kernel-api book, I have added some section names as well.
     So please, check that changes do not break something and that categories are
    not too much skewed.
    
    I have changed kernel-doc to accept "fastcall" and "asmlinkage" words reserved
    by kernel convention.  Most of the other changes are modifications in the
    comments to make kernel-doc happy, accept some parameters description and do
    not bail out on errors.  Changed <pid> to @pid in the description, moved some
    #ifdef before comments to correct function to comments bindings, etc.
    
    You can see result of the modified documentation build at
      http://cmp.felk.cvut.cz/~pisa/linux/lkdb-2.6.11.tar.gz
    
    Some more sources are ready to be included into kernel-doc generated
    documentation.  Sources has been added into kernel-api for now.  Some more
    section names added and probably some more chaos introduced as result of quick
    cleanup work.
    
    Signed-off-by: Pavel Pisa <pisa@cmp.felk.cvut.cz>
    Signed-off-by: Martin Waitz <tali@admingilde.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/sock.h b/include/net/sock.h
index be81cabd0da3..5bc180adfb14 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -90,17 +90,17 @@ do {	spin_lock_init(&((__sk)->sk_lock.slock)); \
 struct sock;
 
 /**
-  *	struct sock_common - minimal network layer representation of sockets
-  *	@skc_family - network address family
-  *	@skc_state - Connection state
-  *	@skc_reuse - %SO_REUSEADDR setting
-  *	@skc_bound_dev_if - bound device index if != 0
-  *	@skc_node - main hash linkage for various protocol lookup tables
-  *	@skc_bind_node - bind hash linkage for various protocol lookup tables
-  *	@skc_refcnt - reference count
-  *
-  *	This is the minimal network layer representation of sockets, the header
-  *	for struct sock and struct tcp_tw_bucket.
+ *	struct sock_common - minimal network layer representation of sockets
+ *	@skc_family: network address family
+ *	@skc_state: Connection state
+ *	@skc_reuse: %SO_REUSEADDR setting
+ *	@skc_bound_dev_if: bound device index if != 0
+ *	@skc_node: main hash linkage for various protocol lookup tables
+ *	@skc_bind_node: bind hash linkage for various protocol lookup tables
+ *	@skc_refcnt: reference count
+ *
+ *	This is the minimal network layer representation of sockets, the header
+ *	for struct sock and struct tcp_tw_bucket.
   */
 struct sock_common {
 	unsigned short		skc_family;
@@ -114,60 +114,60 @@ struct sock_common {
 
 /**
   *	struct sock - network layer representation of sockets
-  *	@__sk_common - shared layout with tcp_tw_bucket
-  *	@sk_shutdown - mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
-  *	@sk_userlocks - %SO_SNDBUF and %SO_RCVBUF settings
-  *	@sk_lock -	synchronizer
-  *	@sk_rcvbuf - size of receive buffer in bytes
-  *	@sk_sleep - sock wait queue
-  *	@sk_dst_cache - destination cache
-  *	@sk_dst_lock - destination cache lock
-  *	@sk_policy - flow policy
-  *	@sk_rmem_alloc - receive queue bytes committed
-  *	@sk_receive_queue - incoming packets
-  *	@sk_wmem_alloc - transmit queue bytes committed
-  *	@sk_write_queue - Packet sending queue
-  *	@sk_omem_alloc - "o" is "option" or "other"
-  *	@sk_wmem_queued - persistent queue size
-  *	@sk_forward_alloc - space allocated forward
-  *	@sk_allocation - allocation mode
-  *	@sk_sndbuf - size of send buffer in bytes
-  *	@sk_flags - %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE, %SO_OOBINLINE settings
-  *	@sk_no_check - %SO_NO_CHECK setting, wether or not checkup packets
-  *	@sk_route_caps - route capabilities (e.g. %NETIF_F_TSO)
-  *	@sk_lingertime - %SO_LINGER l_linger setting
-  *	@sk_hashent - hash entry in several tables (e.g. tcp_ehash)
-  *	@sk_backlog - always used with the per-socket spinlock held
-  *	@sk_callback_lock - used with the callbacks in the end of this struct
-  *	@sk_error_queue - rarely used
-  *	@sk_prot - protocol handlers inside a network family
-  *	@sk_err - last error
-  *	@sk_err_soft - errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
-  *	@sk_ack_backlog - current listen backlog
-  *	@sk_max_ack_backlog - listen backlog set in listen()
-  *	@sk_priority - %SO_PRIORITY setting
-  *	@sk_type - socket type (%SOCK_STREAM, etc)
-  *	@sk_protocol - which protocol this socket belongs in this network family
-  *	@sk_peercred - %SO_PEERCRED setting
-  *	@sk_rcvlowat - %SO_RCVLOWAT setting
-  *	@sk_rcvtimeo - %SO_RCVTIMEO setting
-  *	@sk_sndtimeo - %SO_SNDTIMEO setting
-  *	@sk_filter - socket filtering instructions
-  *	@sk_protinfo - private area, net family specific, when not using slab
-  *	@sk_timer - sock cleanup timer
-  *	@sk_stamp - time stamp of last packet received
-  *	@sk_socket - Identd and reporting IO signals
-  *	@sk_user_data - RPC layer private data
-  *	@sk_sndmsg_page - cached page for sendmsg
-  *	@sk_sndmsg_off - cached offset for sendmsg
-  *	@sk_send_head - front of stuff to transmit
-  *	@sk_write_pending - a write to stream socket waits to start
-  *	@sk_state_change - callback to indicate change in the state of the sock
-  *	@sk_data_ready - callback to indicate there is data to be processed
-  *	@sk_write_space - callback to indicate there is bf sending space available
-  *	@sk_error_report - callback to indicate errors (e.g. %MSG_ERRQUEUE)
-  *	@sk_backlog_rcv - callback to process the backlog
-  *	@sk_destruct - called at sock freeing time, i.e. when all refcnt == 0
+  *	@__sk_common: shared layout with tcp_tw_bucket
+  *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
+  *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
+  *	@sk_lock:	synchronizer
+  *	@sk_rcvbuf: size of receive buffer in bytes
+  *	@sk_sleep: sock wait queue
+  *	@sk_dst_cache: destination cache
+  *	@sk_dst_lock: destination cache lock
+  *	@sk_policy: flow policy
+  *	@sk_rmem_alloc: receive queue bytes committed
+  *	@sk_receive_queue: incoming packets
+  *	@sk_wmem_alloc: transmit queue bytes committed
+  *	@sk_write_queue: Packet sending queue
+  *	@sk_omem_alloc: "o" is "option" or "other"
+  *	@sk_wmem_queued: persistent queue size
+  *	@sk_forward_alloc: space allocated forward
+  *	@sk_allocation: allocation mode
+  *	@sk_sndbuf: size of send buffer in bytes
+  *	@sk_flags: %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE, %SO_OOBINLINE settings
+  *	@sk_no_check: %SO_NO_CHECK setting, wether or not checkup packets
+  *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_lingertime: %SO_LINGER l_linger setting
+  *	@sk_hashent: hash entry in several tables (e.g. tcp_ehash)
+  *	@sk_backlog: always used with the per-socket spinlock held
+  *	@sk_callback_lock: used with the callbacks in the end of this struct
+  *	@sk_error_queue: rarely used
+  *	@sk_prot: protocol handlers inside a network family
+  *	@sk_err: last error
+  *	@sk_err_soft: errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
+  *	@sk_ack_backlog: current listen backlog
+  *	@sk_max_ack_backlog: listen backlog set in listen()
+  *	@sk_priority: %SO_PRIORITY setting
+  *	@sk_type: socket type (%SOCK_STREAM, etc)
+  *	@sk_protocol: which protocol this socket belongs in this network family
+  *	@sk_peercred: %SO_PEERCRED setting
+  *	@sk_rcvlowat: %SO_RCVLOWAT setting
+  *	@sk_rcvtimeo: %SO_RCVTIMEO setting
+  *	@sk_sndtimeo: %SO_SNDTIMEO setting
+  *	@sk_filter: socket filtering instructions
+  *	@sk_protinfo: private area, net family specific, when not using slab
+  *	@sk_timer: sock cleanup timer
+  *	@sk_stamp: time stamp of last packet received
+  *	@sk_socket: Identd and reporting IO signals
+  *	@sk_user_data: RPC layer private data
+  *	@sk_sndmsg_page: cached page for sendmsg
+  *	@sk_sndmsg_off: cached offset for sendmsg
+  *	@sk_send_head: front of stuff to transmit
+  *	@sk_write_pending: a write to stream socket waits to start
+  *	@sk_state_change: callback to indicate change in the state of the sock
+  *	@sk_data_ready: callback to indicate there is data to be processed
+  *	@sk_write_space: callback to indicate there is bf sending space available
+  *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
+  *	@sk_backlog_rcv: callback to process the backlog
+  *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
  */
 struct sock {
 	/*
@@ -1223,8 +1223,8 @@ sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
 
 /**
  * sk_eat_skb - Release a skb if it is no longer needed
- * @sk - socket to eat this skb from
- * @skb - socket buffer to eat
+ * @sk: socket to eat this skb from
+ * @skb: socket buffer to eat
  *
  * This routine must be called with interrupts disabled or with the socket
  * locked so that the sk_buff queue operation is ok.

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/include/net/sock.h b/include/net/sock.h
new file mode 100644
index 000000000000..be81cabd0da3
--- /dev/null
+++ b/include/net/sock.h
@@ -0,0 +1,1297 @@
+/*
+ * INET		An implementation of the TCP/IP protocol suite for the LINUX
+ *		operating system.  INET is implemented using the  BSD Socket
+ *		interface as the means of communication with the user level.
+ *
+ *		Definitions for the AF_INET socket handler.
+ *
+ * Version:	@(#)sock.h	1.0.4	05/13/93
+ *
+ * Authors:	Ross Biro, <bir7@leland.Stanford.Edu>
+ *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
+ *		Corey Minyard <wf-rch!minyard@relay.EU.net>
+ *		Florian La Roche <flla@stud.uni-sb.de>
+ *
+ * Fixes:
+ *		Alan Cox	:	Volatiles in skbuff pointers. See
+ *					skbuff comments. May be overdone,
+ *					better to prove they can be removed
+ *					than the reverse.
+ *		Alan Cox	:	Added a zapped field for tcp to note
+ *					a socket is reset and must stay shut up
+ *		Alan Cox	:	New fields for options
+ *	Pauline Middelink	:	identd support
+ *		Alan Cox	:	Eliminate low level recv/recvfrom
+ *		David S. Miller	:	New socket lookup architecture.
+ *              Steve Whitehouse:       Default routines for sock_ops
+ *              Arnaldo C. Melo :	removed net_pinfo, tp_pinfo and made
+ *              			protinfo be just a void pointer, as the
+ *              			protocol specific parts were moved to
+ *              			respective headers and ipv4/v6, etc now
+ *              			use private slabcaches for its socks
+ *              Pedro Hortas	:	New flags field for socket options
+ *
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _SOCK_H
+#define _SOCK_H
+
+#include <linux/config.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+#include <linux/cache.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>	/* struct sk_buff */
+#include <linux/security.h>
+
+#include <linux/filter.h>
+
+#include <asm/atomic.h>
+#include <net/dst.h>
+#include <net/checksum.h>
+
+/*
+ * This structure really needs to be cleaned up.
+ * Most of it is for TCP, and not used by any of
+ * the other protocols.
+ */
+
+/* Define this to get the SOCK_DBG debugging facility. */
+#define SOCK_DEBUGGING
+#ifdef SOCK_DEBUGGING
+#define SOCK_DEBUG(sk, msg...) do { if ((sk) && sock_flag((sk), SOCK_DBG)) \
+					printk(KERN_DEBUG msg); } while (0)
+#else
+#define SOCK_DEBUG(sk, msg...) do { } while (0)
+#endif
+
+/* This is the per-socket lock.  The spinlock provides a synchronization
+ * between user contexts and software interrupt processing, whereas the
+ * mini-semaphore synchronizes multiple users amongst themselves.
+ */
+struct sock_iocb;
+typedef struct {
+	spinlock_t		slock;
+	struct sock_iocb	*owner;
+	wait_queue_head_t	wq;
+} socket_lock_t;
+
+#define sock_lock_init(__sk) \
+do {	spin_lock_init(&((__sk)->sk_lock.slock)); \
+	(__sk)->sk_lock.owner = NULL; \
+	init_waitqueue_head(&((__sk)->sk_lock.wq)); \
+} while(0)
+
+struct sock;
+
+/**
+  *	struct sock_common - minimal network layer representation of sockets
+  *	@skc_family - network address family
+  *	@skc_state - Connection state
+  *	@skc_reuse - %SO_REUSEADDR setting
+  *	@skc_bound_dev_if - bound device index if != 0
+  *	@skc_node - main hash linkage for various protocol lookup tables
+  *	@skc_bind_node - bind hash linkage for various protocol lookup tables
+  *	@skc_refcnt - reference count
+  *
+  *	This is the minimal network layer representation of sockets, the header
+  *	for struct sock and struct tcp_tw_bucket.
+  */
+struct sock_common {
+	unsigned short		skc_family;
+	volatile unsigned char	skc_state;
+	unsigned char		skc_reuse;
+	int			skc_bound_dev_if;
+	struct hlist_node	skc_node;
+	struct hlist_node	skc_bind_node;
+	atomic_t		skc_refcnt;
+};
+
+/**
+  *	struct sock - network layer representation of sockets
+  *	@__sk_common - shared layout with tcp_tw_bucket
+  *	@sk_shutdown - mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
+  *	@sk_userlocks - %SO_SNDBUF and %SO_RCVBUF settings
+  *	@sk_lock -	synchronizer
+  *	@sk_rcvbuf - size of receive buffer in bytes
+  *	@sk_sleep - sock wait queue
+  *	@sk_dst_cache - destination cache
+  *	@sk_dst_lock - destination cache lock
+  *	@sk_policy - flow policy
+  *	@sk_rmem_alloc - receive queue bytes committed
+  *	@sk_receive_queue - incoming packets
+  *	@sk_wmem_alloc - transmit queue bytes committed
+  *	@sk_write_queue - Packet sending queue
+  *	@sk_omem_alloc - "o" is "option" or "other"
+  *	@sk_wmem_queued - persistent queue size
+  *	@sk_forward_alloc - space allocated forward
+  *	@sk_allocation - allocation mode
+  *	@sk_sndbuf - size of send buffer in bytes
+  *	@sk_flags - %SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE, %SO_OOBINLINE settings
+  *	@sk_no_check - %SO_NO_CHECK setting, wether or not checkup packets
+  *	@sk_route_caps - route capabilities (e.g. %NETIF_F_TSO)
+  *	@sk_lingertime - %SO_LINGER l_linger setting
+  *	@sk_hashent - hash entry in several tables (e.g. tcp_ehash)
+  *	@sk_backlog - always used with the per-socket spinlock held
+  *	@sk_callback_lock - used with the callbacks in the end of this struct
+  *	@sk_error_queue - rarely used
+  *	@sk_prot - protocol handlers inside a network family
+  *	@sk_err - last error
+  *	@sk_err_soft - errors that don't cause failure but are the cause of a persistent failure not just 'timed out'
+  *	@sk_ack_backlog - current listen backlog
+  *	@sk_max_ack_backlog - listen backlog set in listen()
+  *	@sk_priority - %SO_PRIORITY setting
+  *	@sk_type - socket type (%SOCK_STREAM, etc)
+  *	@sk_protocol - which protocol this socket belongs in this network family
+  *	@sk_peercred - %SO_PEERCRED setting
+  *	@sk_rcvlowat - %SO_RCVLOWAT setting
+  *	@sk_rcvtimeo - %SO_RCVTIMEO setting
+  *	@sk_sndtimeo - %SO_SNDTIMEO setting
+  *	@sk_filter - socket filtering instructions
+  *	@sk_protinfo - private area, net family specific, when not using slab
+  *	@sk_timer - sock cleanup timer
+  *	@sk_stamp - time stamp of last packet received
+  *	@sk_socket - Identd and reporting IO signals
+  *	@sk_user_data - RPC layer private data
+  *	@sk_sndmsg_page - cached page for sendmsg
+  *	@sk_sndmsg_off - cached offset for sendmsg
+  *	@sk_send_head - front of stuff to transmit
+  *	@sk_write_pending - a write to stream socket waits to start
+  *	@sk_state_change - callback to indicate change in the state of the sock
+  *	@sk_data_ready - callback to indicate there is data to be processed
+  *	@sk_write_space - callback to indicate there is bf sending space available
+  *	@sk_error_report - callback to indicate errors (e.g. %MSG_ERRQUEUE)
+  *	@sk_backlog_rcv - callback to process the backlog
+  *	@sk_destruct - called at sock freeing time, i.e. when all refcnt == 0
+ */
+struct sock {
+	/*
+	 * Now struct tcp_tw_bucket also uses sock_common, so please just
+	 * don't add nothing before this first member (__sk_common) --acme
+	 */
+	struct sock_common	__sk_common;
+#define sk_family		__sk_common.skc_family
+#define sk_state		__sk_common.skc_state
+#define sk_reuse		__sk_common.skc_reuse
+#define sk_bound_dev_if		__sk_common.skc_bound_dev_if
+#define sk_node			__sk_common.skc_node
+#define sk_bind_node		__sk_common.skc_bind_node
+#define sk_refcnt		__sk_common.skc_refcnt
+	unsigned char		sk_shutdown : 2,
+				sk_no_check : 2,
+				sk_userlocks : 4;
+	unsigned char		sk_protocol;
+	unsigned short		sk_type;
+	int			sk_rcvbuf;
+	socket_lock_t		sk_lock;
+	wait_queue_head_t	*sk_sleep;
+	struct dst_entry	*sk_dst_cache;
+	struct xfrm_policy	*sk_policy[2];
+	rwlock_t		sk_dst_lock;
+	atomic_t		sk_rmem_alloc;
+	atomic_t		sk_wmem_alloc;
+	atomic_t		sk_omem_alloc;
+	struct sk_buff_head	sk_receive_queue;
+	struct sk_buff_head	sk_write_queue;
+	int			sk_wmem_queued;
+	int			sk_forward_alloc;
+	unsigned int		sk_allocation;
+	int			sk_sndbuf;
+	int			sk_route_caps;
+	int			sk_hashent;
+	unsigned long 		sk_flags;
+	unsigned long	        sk_lingertime;
+	/*
+	 * The backlog queue is special, it is always used with
+	 * the per-socket spinlock held and requires low latency
+	 * access. Therefore we special case it's implementation.
+	 */
+	struct {
+		struct sk_buff *head;
+		struct sk_buff *tail;
+	} sk_backlog;
+	struct sk_buff_head	sk_error_queue;
+	struct proto		*sk_prot;
+	rwlock_t		sk_callback_lock;
+	int			sk_err,
+				sk_err_soft;
+	unsigned short		sk_ack_backlog;
+	unsigned short		sk_max_ack_backlog;
+	__u32			sk_priority;
+	struct ucred		sk_peercred;
+	int			sk_rcvlowat;
+	long			sk_rcvtimeo;
+	long			sk_sndtimeo;
+	struct sk_filter      	*sk_filter;
+	void			*sk_protinfo;
+	struct timer_list	sk_timer;
+	struct timeval		sk_stamp;
+	struct socket		*sk_socket;
+	void			*sk_user_data;
+	struct page		*sk_sndmsg_page;
+	struct sk_buff		*sk_send_head;
+	__u32			sk_sndmsg_off;
+	int			sk_write_pending;
+	void			*sk_security;
+	void			(*sk_state_change)(struct sock *sk);
+	void			(*sk_data_ready)(struct sock *sk, int bytes);
+	void			(*sk_write_space)(struct sock *sk);
+	void			(*sk_error_report)(struct sock *sk);
+  	int			(*sk_backlog_rcv)(struct sock *sk,
+						  struct sk_buff *skb);  
+	void                    (*sk_destruct)(struct sock *sk);
+};
+
+/*
+ * Hashed lists helper routines
+ */
+static inline struct sock *__sk_head(struct hlist_head *head)
+{
+	return hlist_entry(head->first, struct sock, sk_node);
+}
+
+static inline struct sock *sk_head(struct hlist_head *head)
+{
+	return hlist_empty(head) ? NULL : __sk_head(head);
+}
+
+static inline struct sock *sk_next(struct sock *sk)
+{
+	return sk->sk_node.next ?
+		hlist_entry(sk->sk_node.next, struct sock, sk_node) : NULL;
+}
+
+static inline int sk_unhashed(struct sock *sk)
+{
+	return hlist_unhashed(&sk->sk_node);
+}
+
+static inline int sk_hashed(struct sock *sk)
+{
+	return sk->sk_node.pprev != NULL;
+}
+
+static __inline__ void sk_node_init(struct hlist_node *node)
+{
+	node->pprev = NULL;
+}
+
+static __inline__ void __sk_del_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_node);
+}
+
+static __inline__ int __sk_del_node_init(struct sock *sk)
+{
+	if (sk_hashed(sk)) {
+		__sk_del_node(sk);
+		sk_node_init(&sk->sk_node);
+		return 1;
+	}
+	return 0;
+}
+
+/* Grab socket reference count. This operation is valid only
+   when sk is ALREADY grabbed f.e. it is found in hash table
+   or a list and the lookup is made under lock preventing hash table
+   modifications.
+ */
+
+static inline void sock_hold(struct sock *sk)
+{
+	atomic_inc(&sk->sk_refcnt);
+}
+
+/* Ungrab socket in the context, which assumes that socket refcnt
+   cannot hit zero, f.e. it is true in context of any socketcall.
+ */
+static inline void __sock_put(struct sock *sk)
+{
+	atomic_dec(&sk->sk_refcnt);
+}
+
+static __inline__ int sk_del_node_init(struct sock *sk)
+{
+	int rc = __sk_del_node_init(sk);
+
+	if (rc) {
+		/* paranoid for a while -acme */
+		WARN_ON(atomic_read(&sk->sk_refcnt) == 1);
+		__sock_put(sk);
+	}
+	return rc;
+}
+
+static __inline__ void __sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_node, list);
+}
+
+static __inline__ void sk_add_node(struct sock *sk, struct hlist_head *list)
+{
+	sock_hold(sk);
+	__sk_add_node(sk, list);
+}
+
+static __inline__ void __sk_del_bind_node(struct sock *sk)
+{
+	__hlist_del(&sk->sk_bind_node);
+}
+
+static __inline__ void sk_add_bind_node(struct sock *sk,
+					struct hlist_head *list)
+{
+	hlist_add_head(&sk->sk_bind_node, list);
+}
+
+#define sk_for_each(__sk, node, list) \
+	hlist_for_each_entry(__sk, node, list, sk_node)
+#define sk_for_each_from(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
+		hlist_for_each_entry_from(__sk, node, sk_node)
+#define sk_for_each_continue(__sk, node) \
+	if (__sk && ({ node = &(__sk)->sk_node; 1; })) \
+		hlist_for_each_entry_continue(__sk, node, sk_node)
+#define sk_for_each_safe(__sk, node, tmp, list) \
+	hlist_for_each_entry_safe(__sk, node, tmp, list, sk_node)
+#define sk_for_each_bound(__sk, node, list) \
+	hlist_for_each_entry(__sk, node, list, sk_bind_node)
+
+/* Sock flags */
+enum sock_flags {
+	SOCK_DEAD,
+	SOCK_DONE,
+	SOCK_URGINLINE,
+	SOCK_KEEPOPEN,
+	SOCK_LINGER,
+	SOCK_DESTROY,
+	SOCK_BROADCAST,
+	SOCK_TIMESTAMP,
+	SOCK_ZAPPED,
+	SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
+	SOCK_DBG, /* %SO_DEBUG setting */
+	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
+	SOCK_NO_LARGESEND, /* whether to sent large segments or not */
+	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
+	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+};
+
+static inline void sock_set_flag(struct sock *sk, enum sock_flags flag)
+{
+	__set_bit(flag, &sk->sk_flags);
+}
+
+static inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)
+{
+	__clear_bit(flag, &sk->sk_flags);
+}
+
+static inline int sock_flag(struct sock *sk, enum sock_flags flag)
+{
+	return test_bit(flag, &sk->sk_flags);
+}
+
+static inline void sk_acceptq_removed(struct sock *sk)
+{
+	sk->sk_ack_backlog--;
+}
+
+static inline void sk_acceptq_added(struct sock *sk)
+{
+	sk->sk_ack_backlog++;
+}
+
+static inline int sk_acceptq_is_full(struct sock *sk)
+{
+	return sk->sk_ack_backlog > sk->sk_max_ack_backlog;
+}
+
+/*
+ * Compute minimal free write space needed to queue new packets.
+ */
+static inline int sk_stream_min_wspace(struct sock *sk)
+{
+	return sk->sk_wmem_queued / 2;
+}
+
+static inline int sk_stream_wspace(struct sock *sk)
+{
+	return sk->sk_sndbuf - sk->sk_wmem_queued;
+}
+
+extern void sk_stream_write_space(struct sock *sk);
+
+static inline int sk_stream_memory_free(struct sock *sk)
+{
+	return sk->sk_wmem_queued < sk->sk_sndbuf;
+}
+
+extern void sk_stream_rfree(struct sk_buff *skb);
+
+static inline void sk_stream_set_owner_r(struct sk_buff *skb, struct sock *sk)
+{
+	skb->sk = sk;
+	skb->destructor = sk_stream_rfree;
+	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+	sk->sk_forward_alloc -= skb->truesize;
+}
+
+static inline void sk_stream_free_skb(struct sock *sk, struct sk_buff *skb)
+{
+	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
+	sk->sk_wmem_queued   -= skb->truesize;
+	sk->sk_forward_alloc += skb->truesize;
+	__kfree_skb(skb);
+}
+
+/* The per-socket spinlock must be held here. */
+#define sk_add_backlog(__sk, __skb)				\
+do {	if (!(__sk)->sk_backlog.tail) {				\
+		(__sk)->sk_backlog.head =			\
+		     (__sk)->sk_backlog.tail = (__skb);		\
+	} else {						\
+		((__sk)->sk_backlog.tail)->next = (__skb);	\
+		(__sk)->sk_backlog.tail = (__skb);		\
+	}							\
+	(__skb)->next = NULL;					\
+} while(0)
+
+#define sk_wait_event(__sk, __timeo, __condition)		\
+({	int rc;							\
+	release_sock(__sk);					\
+	rc = __condition;					\
+	if (!rc) {						\
+		*(__timeo) = schedule_timeout(*(__timeo));	\
+		rc = __condition;				\
+	}							\
+	lock_sock(__sk);					\
+	rc;							\
+})
+
+extern int sk_stream_wait_connect(struct sock *sk, long *timeo_p);
+extern int sk_stream_wait_memory(struct sock *sk, long *timeo_p);
+extern void sk_stream_wait_close(struct sock *sk, long timeo_p);
+extern int sk_stream_error(struct sock *sk, int flags, int err);
+extern void sk_stream_kill_queues(struct sock *sk);
+
+extern int sk_wait_data(struct sock *sk, long *timeo);
+
+/* Networking protocol blocks we attach to sockets.
+ * socket layer -> transport layer interface
+ * transport -> network interface is defined by struct inet_proto
+ */
+struct proto {
+	void			(*close)(struct sock *sk, 
+					long timeout);
+	int			(*connect)(struct sock *sk,
+				        struct sockaddr *uaddr, 
+					int addr_len);
+	int			(*disconnect)(struct sock *sk, int flags);
+
+	struct sock *		(*accept) (struct sock *sk, int flags, int *err);
+
+	int			(*ioctl)(struct sock *sk, int cmd,
+					 unsigned long arg);
+	int			(*init)(struct sock *sk);
+	int			(*destroy)(struct sock *sk);
+	void			(*shutdown)(struct sock *sk, int how);
+	int			(*setsockopt)(struct sock *sk, int level, 
+					int optname, char __user *optval,
+					int optlen);
+	int			(*getsockopt)(struct sock *sk, int level, 
+					int optname, char __user *optval, 
+					int __user *option);  	 
+	int			(*sendmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg, size_t len);
+	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
+					   struct msghdr *msg,
+					size_t len, int noblock, int flags, 
+					int *addr_len);
+	int			(*sendpage)(struct sock *sk, struct page *page,
+					int offset, size_t size, int flags);
+	int			(*bind)(struct sock *sk, 
+					struct sockaddr *uaddr, int addr_len);
+
+	int			(*backlog_rcv) (struct sock *sk, 
+						struct sk_buff *skb);
+
+	/* Keeping track of sk's, looking them up, and port selection methods. */
+	void			(*hash)(struct sock *sk);
+	void			(*unhash)(struct sock *sk);
+	int			(*get_port)(struct sock *sk, unsigned short snum);
+
+	/* Memory pressure */
+	void			(*enter_memory_pressure)(void);
+	atomic_t		*memory_allocated;	/* Current allocated memory. */
+	atomic_t		*sockets_allocated;	/* Current number of sockets. */
+	/*
+	 * Pressure flag: try to collapse.
+	 * Technical note: it is used by multiple contexts non atomically.
+	 * All the sk_stream_mem_schedule() is of this nature: accounting
+	 * is strict, actions are advisory and have some latency.
+	 */
+	int			*memory_pressure;
+	int			*sysctl_mem;
+	int			*sysctl_wmem;
+	int			*sysctl_rmem;
+	int			max_header;
+
+	kmem_cache_t		*slab;
+	unsigned int		obj_size;
+
+	struct module		*owner;
+
+	char			name[32];
+
+	struct list_head	node;
+
+	struct {
+		int inuse;
+		u8  __pad[SMP_CACHE_BYTES - sizeof(int)];
+	} stats[NR_CPUS];
+};
+
+extern int proto_register(struct proto *prot, int alloc_slab);
+extern void proto_unregister(struct proto *prot);
+
+/* Called with local bh disabled */
+static __inline__ void sock_prot_inc_use(struct proto *prot)
+{
+	prot->stats[smp_processor_id()].inuse++;
+}
+
+static __inline__ void sock_prot_dec_use(struct proto *prot)
+{
+	prot->stats[smp_processor_id()].inuse--;
+}
+
+/* About 10 seconds */
+#define SOCK_DESTROY_TIME (10*HZ)
+
+/* Sockets 0-1023 can't be bound to unless you are superuser */
+#define PROT_SOCK	1024
+
+#define SHUTDOWN_MASK	3
+#define RCV_SHUTDOWN	1
+#define SEND_SHUTDOWN	2
+
+#define SOCK_SNDBUF_LOCK	1
+#define SOCK_RCVBUF_LOCK	2
+#define SOCK_BINDADDR_LOCK	4
+#define SOCK_BINDPORT_LOCK	8
+
+/* sock_iocb: used to kick off async processing of socket ios */
+struct sock_iocb {
+	struct list_head	list;
+
+	int			flags;
+	int			size;
+	struct socket		*sock;
+	struct sock		*sk;
+	struct scm_cookie	*scm;
+	struct msghdr		*msg, async_msg;
+	struct iovec		async_iov;
+	struct kiocb		*kiocb;
+};
+
+static inline struct sock_iocb *kiocb_to_siocb(struct kiocb *iocb)
+{
+	return (struct sock_iocb *)iocb->private;
+}
+
+static inline struct kiocb *siocb_to_kiocb(struct sock_iocb *si)
+{
+	return si->kiocb;
+}
+
+struct socket_alloc {
+	struct socket socket;
+	struct inode vfs_inode;
+};
+
+static inline struct socket *SOCKET_I(struct inode *inode)
+{
+	return &container_of(inode, struct socket_alloc, vfs_inode)->socket;
+}
+
+static inline struct inode *SOCK_INODE(struct socket *socket)
+{
+	return &container_of(socket, struct socket_alloc, socket)->vfs_inode;
+}
+
+extern void __sk_stream_mem_reclaim(struct sock *sk);
+extern int sk_stream_mem_schedule(struct sock *sk, int size, int kind);
+
+#define SK_STREAM_MEM_QUANTUM ((int)PAGE_SIZE)
+
+static inline int sk_stream_pages(int amt)
+{
+	return (amt + SK_STREAM_MEM_QUANTUM - 1) / SK_STREAM_MEM_QUANTUM;
+}
+
+static inline void sk_stream_mem_reclaim(struct sock *sk)
+{
+	if (sk->sk_forward_alloc >= SK_STREAM_MEM_QUANTUM)
+		__sk_stream_mem_reclaim(sk);
+}
+
+static inline void sk_stream_writequeue_purge(struct sock *sk)
+{
+	struct sk_buff *skb;
+
+	while ((skb = __skb_dequeue(&sk->sk_write_queue)) != NULL)
+		sk_stream_free_skb(sk, skb);
+	sk_stream_mem_reclaim(sk);
+}
+
+static inline int sk_stream_rmem_schedule(struct sock *sk, struct sk_buff *skb)
+{
+	return (int)skb->truesize <= sk->sk_forward_alloc ||
+		sk_stream_mem_schedule(sk, skb->truesize, 1);
+}
+
+/* Used by processes to "lock" a socket state, so that
+ * interrupts and bottom half handlers won't change it
+ * from under us. It essentially blocks any incoming
+ * packets, so that we won't get any new data or any
+ * packets that change the state of the socket.
+ *
+ * While locked, BH processing will add new packets to
+ * the backlog queue.  This queue is processed by the
+ * owner of the socket lock right before it is released.
+ *
+ * Since ~2.3.5 it is also exclusive sleep lock serializing
+ * accesses from user process context.
+ */
+#define sock_owned_by_user(sk)	((sk)->sk_lock.owner)
+
+extern void FASTCALL(lock_sock(struct sock *sk));
+extern void FASTCALL(release_sock(struct sock *sk));
+
+/* BH context may only use the following locking interface. */
+#define bh_lock_sock(__sk)	spin_lock(&((__sk)->sk_lock.slock))
+#define bh_unlock_sock(__sk)	spin_unlock(&((__sk)->sk_lock.slock))
+
+extern struct sock		*sk_alloc(int family, int priority,
+					  struct proto *prot, int zero_it);
+extern void			sk_free(struct sock *sk);
+
+extern struct sk_buff		*sock_wmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      int priority);
+extern struct sk_buff		*sock_rmalloc(struct sock *sk,
+					      unsigned long size, int force,
+					      int priority);
+extern void			sock_wfree(struct sk_buff *skb);
+extern void			sock_rfree(struct sk_buff *skb);
+
+extern int			sock_setsockopt(struct socket *sock, int level,
+						int op, char __user *optval,
+						int optlen);
+
+extern int			sock_getsockopt(struct socket *sock, int level,
+						int op, char __user *optval, 
+						int __user *optlen);
+extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
+						     unsigned long size,
+						     int noblock,
+						     int *errcode);
+extern void *sock_kmalloc(struct sock *sk, int size, int priority);
+extern void sock_kfree_s(struct sock *sk, void *mem, int size);
+extern void sk_send_sigurg(struct sock *sk);
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * does not implement a particular function.
+ */
+extern int                      sock_no_bind(struct socket *, 
+					     struct sockaddr *, int);
+extern int                      sock_no_connect(struct socket *,
+						struct sockaddr *, int, int);
+extern int                      sock_no_socketpair(struct socket *,
+						   struct socket *);
+extern int                      sock_no_accept(struct socket *,
+					       struct socket *, int);
+extern int                      sock_no_getname(struct socket *,
+						struct sockaddr *, int *, int);
+extern unsigned int             sock_no_poll(struct file *, struct socket *,
+					     struct poll_table_struct *);
+extern int                      sock_no_ioctl(struct socket *, unsigned int,
+					      unsigned long);
+extern int			sock_no_listen(struct socket *, int);
+extern int                      sock_no_shutdown(struct socket *, int);
+extern int			sock_no_getsockopt(struct socket *, int , int,
+						   char __user *, int __user *);
+extern int			sock_no_setsockopt(struct socket *, int, int,
+						   char __user *, int);
+extern int                      sock_no_sendmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t);
+extern int                      sock_no_recvmsg(struct kiocb *, struct socket *,
+						struct msghdr *, size_t, int);
+extern int			sock_no_mmap(struct file *file,
+					     struct socket *sock,
+					     struct vm_area_struct *vma);
+extern ssize_t			sock_no_sendpage(struct socket *sock,
+						struct page *page,
+						int offset, size_t size, 
+						int flags);
+
+/*
+ * Functions to fill in entries in struct proto_ops when a protocol
+ * uses the inet style.
+ */
+extern int sock_common_getsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, int __user *optlen);
+extern int sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,
+			       struct msghdr *msg, size_t size, int flags);
+extern int sock_common_setsockopt(struct socket *sock, int level, int optname,
+				  char __user *optval, int optlen);
+
+extern void sk_common_release(struct sock *sk);
+
+/*
+ *	Default socket callbacks and setup code
+ */
+ 
+/* Initialise core socket variables */
+extern void sock_init_data(struct socket *sock, struct sock *sk);
+
+/**
+ *	sk_filter - run a packet through a socket filter
+ *	@sk: sock associated with &sk_buff
+ *	@skb: buffer to filter
+ *	@needlock: set to 1 if the sock is not locked by caller.
+ *
+ * Run the filter code and then cut skb->data to correct size returned by
+ * sk_run_filter. If pkt_len is 0 we toss packet. If skb->len is smaller
+ * than pkt_len we keep whole skb->data. This is the socket level
+ * wrapper to sk_run_filter. It returns 0 if the packet should
+ * be accepted or -EPERM if the packet should be tossed.
+ *
+ */
+
+static inline int sk_filter(struct sock *sk, struct sk_buff *skb, int needlock)
+{
+	int err;
+	
+	err = security_sock_rcv_skb(sk, skb);
+	if (err)
+		return err;
+	
+	if (sk->sk_filter) {
+		struct sk_filter *filter;
+		
+		if (needlock)
+			bh_lock_sock(sk);
+		
+		filter = sk->sk_filter;
+		if (filter) {
+			int pkt_len = sk_run_filter(skb, filter->insns,
+						    filter->len);
+			if (!pkt_len)
+				err = -EPERM;
+			else
+				skb_trim(skb, pkt_len);
+		}
+
+		if (needlock)
+			bh_unlock_sock(sk);
+	}
+	return err;
+}
+
+/**
+ *	sk_filter_release: Release a socket filter
+ *	@sk: socket
+ *	@fp: filter to remove
+ *
+ *	Remove a filter from a socket and release its resources.
+ */
+ 
+static inline void sk_filter_release(struct sock *sk, struct sk_filter *fp)
+{
+	unsigned int size = sk_filter_len(fp);
+
+	atomic_sub(size, &sk->sk_omem_alloc);
+
+	if (atomic_dec_and_test(&fp->refcnt))
+		kfree(fp);
+}
+
+static inline void sk_filter_charge(struct sock *sk, struct sk_filter *fp)
+{
+	atomic_inc(&fp->refcnt);
+	atomic_add(sk_filter_len(fp), &sk->sk_omem_alloc);
+}
+
+/*
+ * Socket reference counting postulates.
+ *
+ * * Each user of socket SHOULD hold a reference count.
+ * * Each access point to socket (an hash table bucket, reference from a list,
+ *   running timer, skb in flight MUST hold a reference count.
+ * * When reference count hits 0, it means it will never increase back.
+ * * When reference count hits 0, it means that no references from
+ *   outside exist to this socket and current process on current CPU
+ *   is last user and may/should destroy this socket.
+ * * sk_free is called from any context: process, BH, IRQ. When
+ *   it is called, socket has no references from outside -> sk_free
+ *   may release descendant resources allocated by the socket, but
+ *   to the time when it is called, socket is NOT referenced by any
+ *   hash tables, lists etc.
+ * * Packets, delivered from outside (from network or from another process)
+ *   and enqueued on receive/error queues SHOULD NOT grab reference count,
+ *   when they sit in queue. Otherwise, packets will leak to hole, when
+ *   socket is looked up by one cpu and unhasing is made by another CPU.
+ *   It is true for udp/raw, netlink (leak to receive and error queues), tcp
+ *   (leak to backlog). Packet socket does all the processing inside
+ *   BR_NETPROTO_LOCK, so that it has not this race condition. UNIX sockets
+ *   use separate SMP lock, so that they are prone too.
+ */
+
+/* Ungrab socket and destroy it, if it was the last reference. */
+static inline void sock_put(struct sock *sk)
+{
+	if (atomic_dec_and_test(&sk->sk_refcnt))
+		sk_free(sk);
+}
+
+/* Detach socket from process context.
+ * Announce socket dead, detach it from wait queue and inode.
+ * Note that parent inode held reference count on this struct sock,
+ * we do not release it in this function, because protocol
+ * probably wants some additional cleanups or even continuing
+ * to work with this socket (TCP).
+ */
+static inline void sock_orphan(struct sock *sk)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sock_set_flag(sk, SOCK_DEAD);
+	sk->sk_socket = NULL;
+	sk->sk_sleep  = NULL;
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+static inline void sock_graft(struct sock *sk, struct socket *parent)
+{
+	write_lock_bh(&sk->sk_callback_lock);
+	sk->sk_sleep = &parent->wait;
+	parent->sk = sk;
+	sk->sk_socket = parent;
+	write_unlock_bh(&sk->sk_callback_lock);
+}
+
+extern int sock_i_uid(struct sock *sk);
+extern unsigned long sock_i_ino(struct sock *sk);
+
+static inline struct dst_entry *
+__sk_dst_get(struct sock *sk)
+{
+	return sk->sk_dst_cache;
+}
+
+static inline struct dst_entry *
+sk_dst_get(struct sock *sk)
+{
+	struct dst_entry *dst;
+
+	read_lock(&sk->sk_dst_lock);
+	dst = sk->sk_dst_cache;
+	if (dst)
+		dst_hold(dst);
+	read_unlock(&sk->sk_dst_lock);
+	return dst;
+}
+
+static inline void
+__sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	struct dst_entry *old_dst;
+
+	old_dst = sk->sk_dst_cache;
+	sk->sk_dst_cache = dst;
+	dst_release(old_dst);
+}
+
+static inline void
+sk_dst_set(struct sock *sk, struct dst_entry *dst)
+{
+	write_lock(&sk->sk_dst_lock);
+	__sk_dst_set(sk, dst);
+	write_unlock(&sk->sk_dst_lock);
+}
+
+static inline void
+__sk_dst_reset(struct sock *sk)
+{
+	struct dst_entry *old_dst;
+
+	old_dst = sk->sk_dst_cache;
+	sk->sk_dst_cache = NULL;
+	dst_release(old_dst);
+}
+
+static inline void
+sk_dst_reset(struct sock *sk)
+{
+	write_lock(&sk->sk_dst_lock);
+	__sk_dst_reset(sk);
+	write_unlock(&sk->sk_dst_lock);
+}
+
+static inline struct dst_entry *
+__sk_dst_check(struct sock *sk, u32 cookie)
+{
+	struct dst_entry *dst = sk->sk_dst_cache;
+
+	if (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {
+		sk->sk_dst_cache = NULL;
+		dst_release(dst);
+		return NULL;
+	}
+
+	return dst;
+}
+
+static inline struct dst_entry *
+sk_dst_check(struct sock *sk, u32 cookie)
+{
+	struct dst_entry *dst = sk_dst_get(sk);
+
+	if (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {
+		sk_dst_reset(sk);
+		dst_release(dst);
+		return NULL;
+	}
+
+	return dst;
+}
+
+static inline void sk_charge_skb(struct sock *sk, struct sk_buff *skb)
+{
+	sk->sk_wmem_queued   += skb->truesize;
+	sk->sk_forward_alloc -= skb->truesize;
+}
+
+static inline int skb_copy_to_page(struct sock *sk, char __user *from,
+				   struct sk_buff *skb, struct page *page,
+				   int off, int copy)
+{
+	if (skb->ip_summed == CHECKSUM_NONE) {
+		int err = 0;
+		unsigned int csum = csum_and_copy_from_user(from,
+						     page_address(page) + off,
+							    copy, 0, &err);
+		if (err)
+			return err;
+		skb->csum = csum_block_add(skb->csum, csum, skb->len);
+	} else if (copy_from_user(page_address(page) + off, from, copy))
+		return -EFAULT;
+
+	skb->len	     += copy;
+	skb->data_len	     += copy;
+	skb->truesize	     += copy;
+	sk->sk_wmem_queued   += copy;
+	sk->sk_forward_alloc -= copy;
+	return 0;
+}
+
+/*
+ * 	Queue a received datagram if it will fit. Stream and sequenced
+ *	protocols can't normally use this as they need to fit buffers in
+ *	and play with them.
+ *
+ * 	Inlined as it's very short and called for pretty much every
+ *	packet ever received.
+ */
+
+static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
+{
+	sock_hold(sk);
+	skb->sk = sk;
+	skb->destructor = sock_wfree;
+	atomic_add(skb->truesize, &sk->sk_wmem_alloc);
+}
+
+static inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)
+{
+	skb->sk = sk;
+	skb->destructor = sock_rfree;
+	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+}
+
+extern void sk_reset_timer(struct sock *sk, struct timer_list* timer,
+			   unsigned long expires);
+
+extern void sk_stop_timer(struct sock *sk, struct timer_list* timer);
+
+static inline int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)
+{
+	int err = 0;
+	int skb_len;
+
+	/* Cast skb->rcvbuf to unsigned... It's pointless, but reduces
+	   number of warnings when compiling with -W --ANK
+	 */
+	if (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=
+	    (unsigned)sk->sk_rcvbuf) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* It would be deadlock, if sock_queue_rcv_skb is used
+	   with socket lock! We assume that users of this
+	   function are lock free.
+	*/
+	err = sk_filter(sk, skb, 1);
+	if (err)
+		goto out;
+
+	skb->dev = NULL;
+	skb_set_owner_r(skb, sk);
+
+	/* Cache the SKB length before we tack it onto the receive
+	 * queue.  Once it is added it no longer belongs to us and
+	 * may be freed by other threads of control pulling packets
+	 * from the queue.
+	 */
+	skb_len = skb->len;
+
+	skb_queue_tail(&sk->sk_receive_queue, skb);
+
+	if (!sock_flag(sk, SOCK_DEAD))
+		sk->sk_data_ready(sk, skb_len);
+out:
+	return err;
+}
+
+static inline int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb)
+{
+	/* Cast skb->rcvbuf to unsigned... It's pointless, but reduces
+	   number of warnings when compiling with -W --ANK
+	 */
+	if (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=
+	    (unsigned)sk->sk_rcvbuf)
+		return -ENOMEM;
+	skb_set_owner_r(skb, sk);
+	skb_queue_tail(&sk->sk_error_queue, skb);
+	if (!sock_flag(sk, SOCK_DEAD))
+		sk->sk_data_ready(sk, skb->len);
+	return 0;
+}
+
+/*
+ *	Recover an error report and clear atomically
+ */
+ 
+static inline int sock_error(struct sock *sk)
+{
+	int err = xchg(&sk->sk_err, 0);
+	return -err;
+}
+
+static inline unsigned long sock_wspace(struct sock *sk)
+{
+	int amt = 0;
+
+	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
+		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
+		if (amt < 0) 
+			amt = 0;
+	}
+	return amt;
+}
+
+static inline void sk_wake_async(struct sock *sk, int how, int band)
+{
+	if (sk->sk_socket && sk->sk_socket->fasync_list)
+		sock_wake_async(sk->sk_socket, how, band);
+}
+
+#define SOCK_MIN_SNDBUF 2048
+#define SOCK_MIN_RCVBUF 256
+
+static inline void sk_stream_moderate_sndbuf(struct sock *sk)
+{
+	if (!(sk->sk_userlocks & SOCK_SNDBUF_LOCK)) {
+		sk->sk_sndbuf = min(sk->sk_sndbuf, sk->sk_wmem_queued / 2);
+		sk->sk_sndbuf = max(sk->sk_sndbuf, SOCK_MIN_SNDBUF);
+	}
+}
+
+static inline struct sk_buff *sk_stream_alloc_pskb(struct sock *sk,
+						   int size, int mem, int gfp)
+{
+	struct sk_buff *skb = alloc_skb(size + sk->sk_prot->max_header, gfp);
+
+	if (skb) {
+		skb->truesize += mem;
+		if (sk->sk_forward_alloc >= (int)skb->truesize ||
+		    sk_stream_mem_schedule(sk, skb->truesize, 0)) {
+			skb_reserve(skb, sk->sk_prot->max_header);
+			return skb;
+		}
+		__kfree_skb(skb);
+	} else {
+		sk->sk_prot->enter_memory_pressure();
+		sk_stream_moderate_sndbuf(sk);
+	}
+	return NULL;
+}
+
+static inline struct sk_buff *sk_stream_alloc_skb(struct sock *sk,
+						  int size, int gfp)
+{
+	return sk_stream_alloc_pskb(sk, size, 0, gfp);
+}
+
+static inline struct page *sk_stream_alloc_page(struct sock *sk)
+{
+	struct page *page = NULL;
+
+	if (sk->sk_forward_alloc >= (int)PAGE_SIZE ||
+	    sk_stream_mem_schedule(sk, PAGE_SIZE, 0))
+		page = alloc_pages(sk->sk_allocation, 0);
+	else {
+		sk->sk_prot->enter_memory_pressure();
+		sk_stream_moderate_sndbuf(sk);
+	}
+	return page;
+}
+
+#define sk_stream_for_retrans_queue(skb, sk)				\
+		for (skb = (sk)->sk_write_queue.next;			\
+		     (skb != (sk)->sk_send_head) &&			\
+		     (skb != (struct sk_buff *)&(sk)->sk_write_queue);	\
+		     skb = skb->next)
+
+/*
+ *	Default write policy as shown to user space via poll/select/SIGIO
+ */
+static inline int sock_writeable(const struct sock *sk) 
+{
+	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf / 2);
+}
+
+static inline int gfp_any(void)
+{
+	return in_softirq() ? GFP_ATOMIC : GFP_KERNEL;
+}
+
+static inline long sock_rcvtimeo(const struct sock *sk, int noblock)
+{
+	return noblock ? 0 : sk->sk_rcvtimeo;
+}
+
+static inline long sock_sndtimeo(const struct sock *sk, int noblock)
+{
+	return noblock ? 0 : sk->sk_sndtimeo;
+}
+
+static inline int sock_rcvlowat(const struct sock *sk, int waitall, int len)
+{
+	return (waitall ? len : min_t(int, sk->sk_rcvlowat, len)) ? : 1;
+}
+
+/* Alas, with timeout socket operations are not restartable.
+ * Compare this to poll().
+ */
+static inline int sock_intr_errno(long timeo)
+{
+	return timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;
+}
+
+static __inline__ void
+sock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)
+{
+	struct timeval *stamp = &skb->stamp;
+	if (sock_flag(sk, SOCK_RCVTSTAMP)) {
+		/* Race occurred between timestamp enabling and packet
+		   receiving.  Fill in the current time for now. */
+		if (stamp->tv_sec == 0)
+			do_gettimeofday(stamp);
+		put_cmsg(msg, SOL_SOCKET, SO_TIMESTAMP, sizeof(struct timeval),
+			 stamp);
+	} else
+		sk->sk_stamp = *stamp;
+}
+
+/**
+ * sk_eat_skb - Release a skb if it is no longer needed
+ * @sk - socket to eat this skb from
+ * @skb - socket buffer to eat
+ *
+ * This routine must be called with interrupts disabled or with the socket
+ * locked so that the sk_buff queue operation is ok.
+*/
+static inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)
+{
+	__skb_unlink(skb, &sk->sk_receive_queue);
+	__kfree_skb(skb);
+}
+
+extern void sock_enable_timestamp(struct sock *sk);
+extern int sock_get_timestamp(struct sock *, struct timeval __user *);
+
+/* 
+ *	Enable debug/info messages 
+ */
+
+#if 0
+#define NETDEBUG(x)	do { } while (0)
+#define LIMIT_NETDEBUG(x) do {} while(0)
+#else
+#define NETDEBUG(x)	do { x; } while (0)
+#define LIMIT_NETDEBUG(x) do { if (net_ratelimit()) { x; } } while(0)
+#endif
+
+/*
+ * Macros for sleeping on a socket. Use them like this:
+ *
+ * SOCK_SLEEP_PRE(sk)
+ * if (condition)
+ * 	schedule();
+ * SOCK_SLEEP_POST(sk)
+ *
+ * N.B. These are now obsolete and were, afaik, only ever used in DECnet
+ * and when the last use of them in DECnet has gone, I'm intending to
+ * remove them.
+ */
+
+#define SOCK_SLEEP_PRE(sk) 	{ struct task_struct *tsk = current; \
+				DECLARE_WAITQUEUE(wait, tsk); \
+				tsk->state = TASK_INTERRUPTIBLE; \
+				add_wait_queue((sk)->sk_sleep, &wait); \
+				release_sock(sk);
+
+#define SOCK_SLEEP_POST(sk)	tsk->state = TASK_RUNNING; \
+				remove_wait_queue((sk)->sk_sleep, &wait); \
+				lock_sock(sk); \
+				}
+
+static inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)
+{
+	if (valbool)
+		sock_set_flag(sk, bit);
+	else
+		sock_reset_flag(sk, bit);
+}
+
+extern __u32 sysctl_wmem_max;
+extern __u32 sysctl_rmem_max;
+
+#ifdef CONFIG_NET
+int siocdevprivate_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg);
+#else
+static inline int siocdevprivate_ioctl(unsigned int fd, unsigned int cmd, unsigned long arg)
+{
+	return -ENODEV;
+}
+#endif
+
+#endif	/* _SOCK_H */
