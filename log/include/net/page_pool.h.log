commit 458de8a97f107e1b6120d608b93ae9e3de019a2e
Author: Ilias Apalodimas <ilias.apalodimas@linaro.org>
Date:   Thu Feb 20 09:41:55 2020 +0200

    net: page_pool: API cleanup and comments
    
    Functions starting with __ usually indicate those which are exported,
    but should not be called directly. Update some of those declared in the
    API and make it more readable.
    
    page_pool_unmap_page() and page_pool_release_page() were doing
    exactly the same thing calling __page_pool_clean_page().  Let's
    rename __page_pool_clean_page() to page_pool_release_page() and
    export it in order to show up on perf logs and get rid of
    page_pool_unmap_page().
    
    Finally rename __page_pool_put_page() to page_pool_put_page() since we
    can now directly call it from drivers and rename the existing
    page_pool_put_page() to page_pool_put_full_page() since they do the same
    thing but the latter is trying to sync the full DMA area.
    
    This patch also updates netsec, mvneta and stmmac drivers which use
    those functions.
    
    Suggested-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index cfbed00ba7ee..81d7773f96cd 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -151,6 +151,7 @@ struct page_pool *page_pool_create(const struct page_pool_params *params);
 #ifdef CONFIG_PAGE_POOL
 void page_pool_destroy(struct page_pool *pool);
 void page_pool_use_xdp_mem(struct page_pool *pool, void (*disconnect)(void *));
+void page_pool_release_page(struct page_pool *pool, struct page *page);
 #else
 static inline void page_pool_destroy(struct page_pool *pool)
 {
@@ -160,41 +161,32 @@ static inline void page_pool_use_xdp_mem(struct page_pool *pool,
 					 void (*disconnect)(void *))
 {
 }
+static inline void page_pool_release_page(struct page_pool *pool,
+					  struct page *page)
+{
+}
 #endif
 
-/* Never call this directly, use helpers below */
-void __page_pool_put_page(struct page_pool *pool, struct page *page,
-			  unsigned int dma_sync_size, bool allow_direct);
+void page_pool_put_page(struct page_pool *pool, struct page *page,
+			unsigned int dma_sync_size, bool allow_direct);
 
-static inline void page_pool_put_page(struct page_pool *pool,
-				      struct page *page, bool allow_direct)
+/* Same as above but will try to sync the entire area pool->max_len */
+static inline void page_pool_put_full_page(struct page_pool *pool,
+					   struct page *page, bool allow_direct)
 {
 	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
 	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
 	 */
 #ifdef CONFIG_PAGE_POOL
-	__page_pool_put_page(pool, page, -1, allow_direct);
+	page_pool_put_page(pool, page, -1, allow_direct);
 #endif
 }
-/* Very limited use-cases allow recycle direct */
+
+/* Same as above but the caller must guarantee safe context. e.g NAPI */
 static inline void page_pool_recycle_direct(struct page_pool *pool,
 					    struct page *page)
 {
-	__page_pool_put_page(pool, page, -1, true);
-}
-
-/* Disconnects a page (from a page_pool).  API users can have a need
- * to disconnect a page (from a page_pool), to allow it to be used as
- * a regular page (that will eventually be returned to the normal
- * page-allocator via put_page).
- */
-void page_pool_unmap_page(struct page_pool *pool, struct page *page);
-static inline void page_pool_release_page(struct page_pool *pool,
-					  struct page *page)
-{
-#ifdef CONFIG_PAGE_POOL
-	page_pool_unmap_page(pool, page);
-#endif
+	page_pool_put_full_page(pool, page, true);
 }
 
 static inline dma_addr_t page_pool_get_dma_addr(struct page *page)

commit e68bc75691cc3de608c2c7505057c948d13ae587
Author: Lorenzo Bianconi <lorenzo@kernel.org>
Date:   Wed Nov 20 16:54:18 2019 +0200

    net: page_pool: add the possibility to sync DMA memory for device
    
    Introduce the following parameters in order to add the possibility to sync
    DMA memory for device before putting allocated pages in the page_pool
    caches:
    - PP_FLAG_DMA_SYNC_DEV: if set in page_pool_params flags, all pages that
      the driver gets from page_pool will be DMA-synced-for-device according
      to the length provided by the device driver. Please note DMA-sync-for-CPU
      is still device driver responsibility
    - offset: DMA address offset where the DMA engine starts copying rx data
    - max_len: maximum DMA memory size page_pool is allowed to flush. This
      is currently used in __page_pool_alloc_pages_slow routine when pages
      are allocated from page allocator
    These parameters are supposed to be set by device drivers.
    
    This optimization reduces the length of the DMA-sync-for-device.
    The optimization is valid because pages are initially
    DMA-synced-for-device as defined via max_len. At RX time, the driver
    will perform a DMA-sync-for-CPU on the memory for the packet length.
    What is important is the memory occupied by packet payload, because
    this is the area CPU is allowed to read and modify. As we don't track
    cache-lines written into by the CPU, simply use the packet payload length
    as dma_sync_size at page_pool recycle time. This also take into account
    any tail-extend.
    
    Tested-by: Matteo Croce <mcroce@redhat.com>
    Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index e2e1b7b1e8ba..cfbed00ba7ee 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -34,8 +34,18 @@
 #include <linux/ptr_ring.h>
 #include <linux/dma-direction.h>
 
-#define PP_FLAG_DMA_MAP 1 /* Should page_pool do the DMA map/unmap */
-#define PP_FLAG_ALL	PP_FLAG_DMA_MAP
+#define PP_FLAG_DMA_MAP		BIT(0) /* Should page_pool do the DMA
+					* map/unmap
+					*/
+#define PP_FLAG_DMA_SYNC_DEV	BIT(1) /* If set all pages that the driver gets
+					* from page_pool will be
+					* DMA-synced-for-device according to
+					* the length provided by the device
+					* driver.
+					* Please note DMA-sync-for-CPU is still
+					* device driver responsibility
+					*/
+#define PP_FLAG_ALL		(PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV)
 
 /*
  * Fast allocation side cache array/stack
@@ -65,6 +75,8 @@ struct page_pool_params {
 	int		nid;  /* Numa node id to allocate from pages from */
 	struct device	*dev; /* device, for DMA pre-mapping purposes */
 	enum dma_data_direction dma_dir; /* DMA mapping direction */
+	unsigned int	max_len; /* max DMA sync memory size */
+	unsigned int	offset;  /* DMA addr offset */
 };
 
 struct page_pool {
@@ -151,8 +163,8 @@ static inline void page_pool_use_xdp_mem(struct page_pool *pool,
 #endif
 
 /* Never call this directly, use helpers below */
-void __page_pool_put_page(struct page_pool *pool,
-			  struct page *page, bool allow_direct);
+void __page_pool_put_page(struct page_pool *pool, struct page *page,
+			  unsigned int dma_sync_size, bool allow_direct);
 
 static inline void page_pool_put_page(struct page_pool *pool,
 				      struct page *page, bool allow_direct)
@@ -161,14 +173,14 @@ static inline void page_pool_put_page(struct page_pool *pool,
 	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
 	 */
 #ifdef CONFIG_PAGE_POOL
-	__page_pool_put_page(pool, page, allow_direct);
+	__page_pool_put_page(pool, page, -1, allow_direct);
 #endif
 }
 /* Very limited use-cases allow recycle direct */
 static inline void page_pool_recycle_direct(struct page_pool *pool,
 					    struct page *page)
 {
-	__page_pool_put_page(pool, page, true);
+	__page_pool_put_page(pool, page, -1, true);
 }
 
 /* Disconnects a page (from a page_pool).  API users can have a need

commit bc836748707cf6b8b1a948b61149278f109107da
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Nov 20 00:15:17 2019 +0000

    page_pool: Add API to update numa node
    
    Add page_pool_update_nid() to be called by page pool consumers when they
    detect numa node changes.
    
    It will update the page pool nid value to start allocating from the new
    effective numa node.
    
    This is to mitigate page pool allocating pages from a wrong numa node,
    where the pool was originally allocated, and holding on to pages that
    belong to a different numa node, which causes performance degradation.
    
    For pages that are already being consumed and could be returned to the
    pool by the consumer, in next patch we will add a check per page to avoid
    recycling them back to the pool and return them to the page allocator.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Reviewed-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index ace881c15dcb..e2e1b7b1e8ba 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -204,4 +204,11 @@ static inline bool page_pool_put(struct page_pool *pool)
 	return refcount_dec_and_test(&pool->user_cnt);
 }
 
+/* Caller must provide appropriate safe context, e.g. NAPI. */
+void page_pool_update_nid(struct page_pool *pool, int new_nid);
+static inline void page_pool_nid_changed(struct page_pool *pool, int new_nid)
+{
+	if (unlikely(pool->p.nid != new_nid))
+		page_pool_update_nid(pool, new_nid);
+}
 #endif /* _NET_PAGE_POOL_H */

commit 7c9e69428da39ed761c9d903c4850368fa4ef7bf
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Sat Nov 16 12:22:43 2019 +0100

    page_pool: add destroy attempts counter and rename tracepoint
    
    When Jonathan change the page_pool to become responsible to its
    own shutdown via deferred work queue, then the disconnect_cnt
    counter was removed from xdp memory model tracepoint.
    
    This patch change the page_pool_inflight tracepoint name to
    page_pool_release, because it reflects the new responsability
    better.  And it reintroduces a counter that reflect the number of
    times page_pool_release have been tried.
    
    The counter is also used by the code, to only empty the alloc
    cache once.  With a stuck work queue running every second and
    counter being 64-bit, it will overrun in approx 584 billion
    years. For comparison, Earth lifetime expectancy is 7.5 billion
    years, before the Sun will engulf, and destroy, the Earth.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 1121faa99c12..ace881c15dcb 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -112,6 +112,8 @@ struct page_pool {
 	 * refcnt serves purpose is to simplify drivers error handling.
 	 */
 	refcount_t user_cnt;
+
+	u64 destroy_cnt;
 };
 
 struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp);

commit c3f812cea0d7006469d1cf33a4a9f0a12bb4b3a3
Author: Jonathan Lemon <jonathan.lemon@gmail.com>
Date:   Thu Nov 14 14:13:00 2019 -0800

    page_pool: do not release pool until inflight == 0.
    
    The page pool keeps track of the number of pages in flight, and
    it isn't safe to remove the pool until all pages are returned.
    
    Disallow removing the pool until all pages are back, so the pool
    is always available for page producers.
    
    Make the page pool responsible for its own delayed destruction
    instead of relying on XDP, so the page pool can be used without
    the xdp memory model.
    
    When all pages are returned, free the pool and notify xdp if the
    pool is registered with the xdp memory system.  Have the callback
    perform a table walk since some drivers (cpsw) may share the pool
    among multiple xdp_rxq_info.
    
    Note that the increment of pages_state_release_cnt may result in
    inflight == 0, resulting in the pool being released.
    
    Fixes: d956a048cd3f ("xdp: force mem allocator removal and periodic warning")
    Signed-off-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 2cbcdbdec254..1121faa99c12 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -70,7 +70,12 @@ struct page_pool_params {
 struct page_pool {
 	struct page_pool_params p;
 
-        u32 pages_state_hold_cnt;
+	struct delayed_work release_dw;
+	void (*disconnect)(void *);
+	unsigned long defer_start;
+	unsigned long defer_warn;
+
+	u32 pages_state_hold_cnt;
 
 	/*
 	 * Data structure for allocation side
@@ -129,25 +134,19 @@ inline enum dma_data_direction page_pool_get_dma_dir(struct page_pool *pool)
 
 struct page_pool *page_pool_create(const struct page_pool_params *params);
 
-void __page_pool_free(struct page_pool *pool);
-static inline void page_pool_free(struct page_pool *pool)
-{
-	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
-	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
-	 */
 #ifdef CONFIG_PAGE_POOL
-	__page_pool_free(pool);
-#endif
-}
-
-/* Drivers use this instead of page_pool_free */
+void page_pool_destroy(struct page_pool *pool);
+void page_pool_use_xdp_mem(struct page_pool *pool, void (*disconnect)(void *));
+#else
 static inline void page_pool_destroy(struct page_pool *pool)
 {
-	if (!pool)
-		return;
+}
 
-	page_pool_free(pool);
+static inline void page_pool_use_xdp_mem(struct page_pool *pool,
+					 void (*disconnect)(void *))
+{
 }
+#endif
 
 /* Never call this directly, use helpers below */
 void __page_pool_put_page(struct page_pool *pool,
@@ -170,24 +169,6 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
-/* API user MUST have disconnected alloc-side (not allowed to call
- * page_pool_alloc_pages()) before calling this.  The free-side can
- * still run concurrently, to handle in-flight packet-pages.
- *
- * A request to shutdown can fail (with false) if there are still
- * in-flight packet-pages.
- */
-bool __page_pool_request_shutdown(struct page_pool *pool);
-static inline bool page_pool_request_shutdown(struct page_pool *pool)
-{
-	bool safe_to_remove = false;
-
-#ifdef CONFIG_PAGE_POOL
-	safe_to_remove = __page_pool_request_shutdown(pool);
-#endif
-	return safe_to_remove;
-}
-
 /* Disconnects a page (from a page_pool).  API users can have a need
  * to disconnect a page (from a page_pool), to allow it to be used as
  * a regular page (that will eventually be returned to the normal
@@ -216,11 +197,6 @@ static inline bool is_page_pool_compiled_in(void)
 #endif
 }
 
-static inline void page_pool_get(struct page_pool *pool)
-{
-	refcount_inc(&pool->user_cnt);
-}
-
 static inline bool page_pool_put(struct page_pool *pool)
 {
 	return refcount_dec_and_test(&pool->user_cnt);

commit 1da4bbeffe41ba318812d7590955faee8636668b
Author: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
Date:   Tue Jul 9 00:34:28 2019 +0300

    net: core: page_pool: add user refcnt and reintroduce page_pool_destroy
    
    Jesper recently removed page_pool_destroy() (from driver invocation)
    and moved shutdown and free of page_pool into xdp_rxq_info_unreg(),
    in-order to handle in-flight packets/pages. This created an asymmetry
    in drivers create/destroy pairs.
    
    This patch reintroduce page_pool_destroy and add page_pool user
    refcnt. This serves the purpose to simplify drivers error handling as
    driver now drivers always calls page_pool_destroy() and don't need to
    track if xdp_rxq_info_reg_mem_model() was unsuccessful.
    
    This could be used for a special cases where a single RX-queue (with a
    single page_pool) provides packets for two net_device'es, and thus
    needs to register the same page_pool twice with two xdp_rxq_info
    structures.
    
    This patch is primarily to ease API usage for drivers. The recently
    merged netsec driver, actually have a bug in this area, which is
    solved by this API change.
    
    This patch is a modified version of Ivan Khoronzhuk's original patch.
    
    Link: https://lore.kernel.org/netdev/20190625175948.24771-2-ivan.khoronzhuk@linaro.org/
    Fixes: 5c67bf0ec4d0 ("net: netsec: Use page_pool API")
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index ee9c871d2043..2cbcdbdec254 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -101,6 +101,12 @@ struct page_pool {
 	struct ptr_ring ring;
 
 	atomic_t pages_state_release_cnt;
+
+	/* A page_pool is strictly tied to a single RX-queue being
+	 * protected by NAPI, due to above pp_alloc_cache. This
+	 * refcnt serves purpose is to simplify drivers error handling.
+	 */
+	refcount_t user_cnt;
 };
 
 struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp);
@@ -134,6 +140,15 @@ static inline void page_pool_free(struct page_pool *pool)
 #endif
 }
 
+/* Drivers use this instead of page_pool_free */
+static inline void page_pool_destroy(struct page_pool *pool)
+{
+	if (!pool)
+		return;
+
+	page_pool_free(pool);
+}
+
 /* Never call this directly, use helpers below */
 void __page_pool_put_page(struct page_pool *pool,
 			  struct page *page, bool allow_direct);
@@ -201,4 +216,14 @@ static inline bool is_page_pool_compiled_in(void)
 #endif
 }
 
+static inline void page_pool_get(struct page_pool *pool)
+{
+	refcount_inc(&pool->user_cnt);
+}
+
+static inline bool page_pool_put(struct page_pool *pool)
+{
+	return refcount_dec_and_test(&pool->user_cnt);
+}
+
 #endif /* _NET_PAGE_POOL_H */

commit bb005f2a70742d66f8fe44d57e217c696e805d97
Author: Ilias Apalodimas <ilias.apalodimas@linaro.org>
Date:   Sat Jun 29 08:23:24 2019 +0300

    net: page_pool: add helper function for retrieving dma direction
    
    Since the dma direction is stored in page pool params, offer an API
    helper for driver that choose not to keep track of it locally
    
    Signed-off-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index f07c518ef8a5..ee9c871d2043 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -112,6 +112,15 @@ static inline struct page *page_pool_dev_alloc_pages(struct page_pool *pool)
 	return page_pool_alloc_pages(pool, gfp);
 }
 
+/* get the stored dma direction. A driver might decide to treat this locally and
+ * avoid the extra cache line from page_pool to determine the direction
+ */
+static
+inline enum dma_data_direction page_pool_get_dma_dir(struct page_pool *pool)
+{
+	return pool->p.dma_dir;
+}
+
 struct page_pool *page_pool_create(const struct page_pool_params *params);
 
 void __page_pool_free(struct page_pool *pool);

commit 497ad9f5b2dc86b733761b9afa44ecfa2f17be65
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Thu Jun 20 00:15:52 2019 +0200

    page_pool: fix compile warning when CONFIG_PAGE_POOL is disabled
    
    Kbuild test robot reported compile warning:
     warning: no return statement in function returning non-void
    in function page_pool_request_shutdown, when CONFIG_PAGE_POOL is disabled.
    
    The fix makes the code a little more verbose, with a descriptive variable.
    
    Fixes: 99c07c43c4ea ("xdp: tracking page_pool resources and safe removal")
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index f09b3f1994e6..f07c518ef8a5 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -156,12 +156,12 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 bool __page_pool_request_shutdown(struct page_pool *pool);
 static inline bool page_pool_request_shutdown(struct page_pool *pool)
 {
-	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
-	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
-	 */
+	bool safe_to_remove = false;
+
 #ifdef CONFIG_PAGE_POOL
-	return __page_pool_request_shutdown(pool);
+	safe_to_remove = __page_pool_request_shutdown(pool);
 #endif
+	return safe_to_remove;
 }
 
 /* Disconnects a page (from a page_pool).  API users can have a need

commit 99c07c43c4ea0bc101331401a0fabfc51933c6a3
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Jun 18 15:05:47 2019 +0200

    xdp: tracking page_pool resources and safe removal
    
    This patch is needed before we can allow drivers to use page_pool for
    DMA-mappings. Today with page_pool and XDP return API, it is possible to
    remove the page_pool object (from rhashtable), while there are still
    in-flight packet-pages. This is safely handled via RCU and failed lookups in
    __xdp_return() fallback to call put_page(), when page_pool object is gone.
    In-case page is still DMA mapped, this will result in page note getting
    correctly DMA unmapped.
    
    To solve this, the page_pool is extended with tracking in-flight pages. And
    XDP disconnect system queries page_pool and waits, via workqueue, for all
    in-flight pages to be returned.
    
    To avoid killing performance when tracking in-flight pages, the implement
    use two (unsigned) counters, that in placed on different cache-lines, and
    can be used to deduct in-flight packets. This is done by mapping the
    unsigned "sequence" counters onto signed Two's complement arithmetic
    operations. This is e.g. used by kernel's time_after macros, described in
    kernel commit 1ba3aab3033b and 5a581b367b5, and also explained in RFC1982.
    
    The trick is these two incrementing counters only need to be read and
    compared, when checking if it's safe to free the page_pool structure. Which
    will only happen when driver have disconnected RX/alloc side. Thus, on a
    non-fast-path.
    
    It is chosen that page_pool tracking is also enabled for the non-DMA
    use-case, as this can be used for statistics later.
    
    After this patch, using page_pool requires more strict resource "release",
    e.g. via page_pool_release_page() that was introduced in this patchset, and
    previous patches implement/fix this more strict requirement.
    
    Drivers no-longer call page_pool_destroy(). Drivers already call
    xdp_rxq_info_unreg() which call xdp_rxq_info_unreg_mem_model(), which will
    attempt to disconnect the mem id, and if attempt fails schedule the
    disconnect for later via delayed workqueue.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 754d980700df..f09b3f1994e6 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -16,14 +16,16 @@
  * page_pool_alloc_pages() call.  Drivers should likely use
  * page_pool_dev_alloc_pages() replacing dev_alloc_pages().
  *
- * If page_pool handles DMA mapping (use page->private), then API user
- * is responsible for invoking page_pool_put_page() once.  In-case of
- * elevated refcnt, the DMA state is released, assuming other users of
- * the page will eventually call put_page().
+ * API keeps track of in-flight pages, in-order to let API user know
+ * when it is safe to dealloactor page_pool object.  Thus, API users
+ * must make sure to call page_pool_release_page() when a page is
+ * "leaving" the page_pool.  Or call page_pool_put_page() where
+ * appropiate.  For maintaining correct accounting.
  *
- * If no DMA mapping is done, then it can act as shim-layer that
- * fall-through to alloc_page.  As no state is kept on the page, the
- * regular put_page() call is sufficient.
+ * API user must only call page_pool_put_page() once on a page, as it
+ * will either recycle the page, or in case of elevated refcnt, it
+ * will release the DMA mapping and in-flight state accounting.  We
+ * hope to lift this requirement in the future.
  */
 #ifndef _NET_PAGE_POOL_H
 #define _NET_PAGE_POOL_H
@@ -66,9 +68,10 @@ struct page_pool_params {
 };
 
 struct page_pool {
-	struct rcu_head rcu;
 	struct page_pool_params p;
 
+        u32 pages_state_hold_cnt;
+
 	/*
 	 * Data structure for allocation side
 	 *
@@ -96,6 +99,8 @@ struct page_pool {
 	 * TODO: Implement bulk return pages into this structure.
 	 */
 	struct ptr_ring ring;
+
+	atomic_t pages_state_release_cnt;
 };
 
 struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp);
@@ -109,8 +114,6 @@ static inline struct page *page_pool_dev_alloc_pages(struct page_pool *pool)
 
 struct page_pool *page_pool_create(const struct page_pool_params *params);
 
-void page_pool_destroy(struct page_pool *pool);
-
 void __page_pool_free(struct page_pool *pool);
 static inline void page_pool_free(struct page_pool *pool)
 {
@@ -143,6 +146,24 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
+/* API user MUST have disconnected alloc-side (not allowed to call
+ * page_pool_alloc_pages()) before calling this.  The free-side can
+ * still run concurrently, to handle in-flight packet-pages.
+ *
+ * A request to shutdown can fail (with false) if there are still
+ * in-flight packet-pages.
+ */
+bool __page_pool_request_shutdown(struct page_pool *pool);
+static inline bool page_pool_request_shutdown(struct page_pool *pool)
+{
+	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
+	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
+	 */
+#ifdef CONFIG_PAGE_POOL
+	return __page_pool_request_shutdown(pool);
+#endif
+}
+
 /* Disconnects a page (from a page_pool).  API users can have a need
  * to disconnect a page (from a page_pool), to allow it to be used as
  * a regular page (that will eventually be returned to the normal

commit e54cfd7e1745e52eb6c67ee9c77aefb8e4666a88
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Jun 18 15:05:37 2019 +0200

    page_pool: introduce page_pool_free and use in mlx5
    
    In case driver fails to register the page_pool with XDP return API (via
    xdp_rxq_info_reg_mem_model()), then the driver can free the page_pool
    resources more directly than calling page_pool_destroy(), which does a
    unnecessarily RCU free procedure.
    
    This patch is preparing for removing page_pool_destroy(), from driver
    invocation.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index e240fac4c5b9..754d980700df 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -111,6 +111,17 @@ struct page_pool *page_pool_create(const struct page_pool_params *params);
 
 void page_pool_destroy(struct page_pool *pool);
 
+void __page_pool_free(struct page_pool *pool);
+static inline void page_pool_free(struct page_pool *pool)
+{
+	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
+	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
+	 */
+#ifdef CONFIG_PAGE_POOL
+	__page_pool_free(pool);
+#endif
+}
+
 /* Never call this directly, use helpers below */
 void __page_pool_put_page(struct page_pool *pool,
 			  struct page *page, bool allow_direct);

commit 6bf071bf09d4b2ff3ee8783531e2ce814f0870cb
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Jun 18 15:05:27 2019 +0200

    xdp: page_pool related fix to cpumap
    
    When converting an xdp_frame into an SKB, and sending this into the network
    stack, then the underlying XDP memory model need to release associated
    resources, because the network stack don't have callbacks for XDP memory
    models.  The only memory model that needs this is page_pool, when a driver
    use the DMA-mapping feature.
    
    Introduce page_pool_release_page(), which basically does the same as
    page_pool_unmap_page(). Add xdp_release_frame() as the XDP memory model
    interface for calling it, if the memory model match MEM_TYPE_PAGE_POOL, to
    save the function call overhead for others. Have cpumap call
    xdp_release_frame() before xdp_scrub_frame().
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index ad218cef88c5..e240fac4c5b9 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -110,7 +110,6 @@ static inline struct page *page_pool_dev_alloc_pages(struct page_pool *pool)
 struct page_pool *page_pool_create(const struct page_pool_params *params);
 
 void page_pool_destroy(struct page_pool *pool);
-void page_pool_unmap_page(struct page_pool *pool, struct page *page);
 
 /* Never call this directly, use helpers below */
 void __page_pool_put_page(struct page_pool *pool,
@@ -133,6 +132,20 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
+/* Disconnects a page (from a page_pool).  API users can have a need
+ * to disconnect a page (from a page_pool), to allow it to be used as
+ * a regular page (that will eventually be returned to the normal
+ * page-allocator via put_page).
+ */
+void page_pool_unmap_page(struct page_pool *pool, struct page *page);
+static inline void page_pool_release_page(struct page_pool *pool,
+					  struct page *page)
+{
+#ifdef CONFIG_PAGE_POOL
+	page_pool_unmap_page(pool, page);
+#endif
+}
+
 static inline dma_addr_t page_pool_get_dma_addr(struct page *page)
 {
 	return page->dma_addr;

commit a25d50bfe645b3ed6b2cb3773e7025db14a608f3
Author: Ilias Apalodimas <ilias.apalodimas@linaro.org>
Date:   Tue Jun 18 15:05:17 2019 +0200

    net: page_pool: add helper function to unmap dma addresses
    
    On a previous patch dma addr was stored in 'struct page'.
    Use that to unmap DMA addresses used by network drivers
    
    Signed-off-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index b885d86cb7a1..ad218cef88c5 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -110,6 +110,7 @@ static inline struct page *page_pool_dev_alloc_pages(struct page_pool *pool)
 struct page_pool *page_pool_create(const struct page_pool_params *params);
 
 void page_pool_destroy(struct page_pool *pool);
+void page_pool_unmap_page(struct page_pool *pool, struct page *page);
 
 /* Never call this directly, use helpers below */
 void __page_pool_put_page(struct page_pool *pool,

commit 0afdeeed08b5f59148069e9f7a52d2ab0eba5d49
Author: Ilias Apalodimas <ilias.apalodimas@linaro.org>
Date:   Tue Jun 18 15:05:12 2019 +0200

    net: page_pool: add helper function to retrieve dma addresses
    
    On a previous patch dma addr was stored in 'struct page'.
    Use that to retrieve DMA addresses used by network drivers
    
    Signed-off-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 694d055e01ef..b885d86cb7a1 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -132,6 +132,11 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
+static inline dma_addr_t page_pool_get_dma_addr(struct page *page)
+{
+	return page->dma_addr;
+}
+
 static inline bool is_page_pool_compiled_in(void)
 {
 #ifdef CONFIG_PAGE_POOL

commit 389ab7f01af988c2a1ec5617eb0c7e220df1ef1c
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Thu May 24 16:46:07 2018 +0200

    xdp: introduce xdp_return_frame_rx_napi
    
    When sending an xdp_frame through xdp_do_redirect call, then error
    cases can happen where the xdp_frame needs to be dropped, and
    returning an -errno code isn't sufficient/possible any-longer
    (e.g. for cpumap case). This is already fully supported, by simply
    calling xdp_return_frame.
    
    This patch is an optimization, which provides xdp_return_frame_rx_napi,
    which is a faster variant for these error cases.  It take advantage of
    the protection provided by XDP RX running under NAPI protection.
    
    This change is mostly relevant for drivers using the page_pool
    allocator as it can take advantage of this. (Tested with mlx5).
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index c79087153148..694d055e01ef 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -115,13 +115,14 @@ void page_pool_destroy(struct page_pool *pool);
 void __page_pool_put_page(struct page_pool *pool,
 			  struct page *page, bool allow_direct);
 
-static inline void page_pool_put_page(struct page_pool *pool, struct page *page)
+static inline void page_pool_put_page(struct page_pool *pool,
+				      struct page *page, bool allow_direct)
 {
 	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
 	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
 	 */
 #ifdef CONFIG_PAGE_POOL
-	__page_pool_put_page(pool, page, false);
+	__page_pool_put_page(pool, page, allow_direct);
 #endif
 }
 /* Very limited use-cases allow recycle direct */

commit 57d0a1c1ac9e6a836bbab4698ba2a2e03f64bf1b
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Apr 17 16:46:22 2018 +0200

    xdp: allow page_pool as an allocator type in xdp_return_frame
    
    New allocator type MEM_TYPE_PAGE_POOL for page_pool usage.
    
    The registered allocator page_pool pointer is not available directly
    from xdp_rxq_info, but it could be (if needed).  For now, the driver
    should keep separate track of the page_pool pointer, which it should
    use for RX-ring page allocation.
    
    As suggested by Saeed, to maintain a symmetric API it is the drivers
    responsibility to allocate/create and free/destroy the page_pool.
    Thus, after the driver have called xdp_rxq_info_unreg(), it is drivers
    responsibility to free the page_pool, but with a RCU free call.  This
    is done easily via the page_pool helper page_pool_destroy() (which
    avoids touching any driver code during the RCU callback, which could
    happen after the driver have been unloaded).
    
    V8: address issues found by kbuild test robot
     - Address sparse should be static warnings
     - Allow xdp.o to be compiled without page_pool.o
    
    V9: Remove inline from .c file, compiler knows best
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
index 1fe77db59518..c79087153148 100644
--- a/include/net/page_pool.h
+++ b/include/net/page_pool.h
@@ -117,7 +117,12 @@ void __page_pool_put_page(struct page_pool *pool,
 
 static inline void page_pool_put_page(struct page_pool *pool, struct page *page)
 {
+	/* When page_pool isn't compiled-in, net/core/xdp.c doesn't
+	 * allow registering MEM_TYPE_PAGE_POOL, but shield linker.
+	 */
+#ifdef CONFIG_PAGE_POOL
 	__page_pool_put_page(pool, page, false);
+#endif
 }
 /* Very limited use-cases allow recycle direct */
 static inline void page_pool_recycle_direct(struct page_pool *pool,
@@ -126,4 +131,13 @@ static inline void page_pool_recycle_direct(struct page_pool *pool,
 	__page_pool_put_page(pool, page, true);
 }
 
+static inline bool is_page_pool_compiled_in(void)
+{
+#ifdef CONFIG_PAGE_POOL
+	return true;
+#else
+	return false;
+#endif
+}
+
 #endif /* _NET_PAGE_POOL_H */

commit ff7d6b27f894f1469dc51ccb828b7363ccd9799f
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Apr 17 16:46:17 2018 +0200

    page_pool: refurbish version of page_pool code
    
    Need a fast page recycle mechanism for ndo_xdp_xmit API for returning
    pages on DMA-TX completion time, which have good cross CPU
    performance, given DMA-TX completion time can happen on a remote CPU.
    
    Refurbish my page_pool code, that was presented[1] at MM-summit 2016.
    Adapted page_pool code to not depend the page allocator and
    integration into struct page.  The DMA mapping feature is kept,
    even-though it will not be activated/used in this patchset.
    
    [1] http://people.netfilter.org/hawk/presentations/MM-summit2016/generic_page_pool_mm_summit2016.pdf
    
    V2: Adjustments requested by Tariq
     - Changed page_pool_create return codes, don't return NULL, only
       ERR_PTR, as this simplifies err handling in drivers.
    
    V4: many small improvements and cleanups
    - Add DOC comment section, that can be used by kernel-doc
    - Improve fallback mode, to work better with refcnt based recycling
      e.g. remove a WARN as pointed out by Tariq
      e.g. quicker fallback if ptr_ring is empty.
    
    V5: Fixed SPDX license as pointed out by Alexei
    
    V6: Adjustments requested by Eric Dumazet
     - Adjust ____cacheline_aligned_in_smp usage/placement
     - Move rcu_head in struct page_pool
     - Free pages quicker on destroy, minimize resources delayed an RCU period
     - Remove code for forward/backward compat ABI interface
    
    V8: Issues found by kbuild test robot
     - Address sparse should be static warnings
     - Only compile+link when a driver use/select page_pool,
       mlx5 selects CONFIG_PAGE_POOL, although its first used in two patches
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/page_pool.h b/include/net/page_pool.h
new file mode 100644
index 000000000000..1fe77db59518
--- /dev/null
+++ b/include/net/page_pool.h
@@ -0,0 +1,129 @@
+/* SPDX-License-Identifier: GPL-2.0
+ *
+ * page_pool.h
+ *	Author:	Jesper Dangaard Brouer <netoptimizer@brouer.com>
+ *	Copyright (C) 2016 Red Hat, Inc.
+ */
+
+/**
+ * DOC: page_pool allocator
+ *
+ * This page_pool allocator is optimized for the XDP mode that
+ * uses one-frame-per-page, but have fallbacks that act like the
+ * regular page allocator APIs.
+ *
+ * Basic use involve replacing alloc_pages() calls with the
+ * page_pool_alloc_pages() call.  Drivers should likely use
+ * page_pool_dev_alloc_pages() replacing dev_alloc_pages().
+ *
+ * If page_pool handles DMA mapping (use page->private), then API user
+ * is responsible for invoking page_pool_put_page() once.  In-case of
+ * elevated refcnt, the DMA state is released, assuming other users of
+ * the page will eventually call put_page().
+ *
+ * If no DMA mapping is done, then it can act as shim-layer that
+ * fall-through to alloc_page.  As no state is kept on the page, the
+ * regular put_page() call is sufficient.
+ */
+#ifndef _NET_PAGE_POOL_H
+#define _NET_PAGE_POOL_H
+
+#include <linux/mm.h> /* Needed by ptr_ring */
+#include <linux/ptr_ring.h>
+#include <linux/dma-direction.h>
+
+#define PP_FLAG_DMA_MAP 1 /* Should page_pool do the DMA map/unmap */
+#define PP_FLAG_ALL	PP_FLAG_DMA_MAP
+
+/*
+ * Fast allocation side cache array/stack
+ *
+ * The cache size and refill watermark is related to the network
+ * use-case.  The NAPI budget is 64 packets.  After a NAPI poll the RX
+ * ring is usually refilled and the max consumed elements will be 64,
+ * thus a natural max size of objects needed in the cache.
+ *
+ * Keeping room for more objects, is due to XDP_DROP use-case.  As
+ * XDP_DROP allows the opportunity to recycle objects directly into
+ * this array, as it shares the same softirq/NAPI protection.  If
+ * cache is already full (or partly full) then the XDP_DROP recycles
+ * would have to take a slower code path.
+ */
+#define PP_ALLOC_CACHE_SIZE	128
+#define PP_ALLOC_CACHE_REFILL	64
+struct pp_alloc_cache {
+	u32 count;
+	void *cache[PP_ALLOC_CACHE_SIZE];
+};
+
+struct page_pool_params {
+	unsigned int	flags;
+	unsigned int	order;
+	unsigned int	pool_size;
+	int		nid;  /* Numa node id to allocate from pages from */
+	struct device	*dev; /* device, for DMA pre-mapping purposes */
+	enum dma_data_direction dma_dir; /* DMA mapping direction */
+};
+
+struct page_pool {
+	struct rcu_head rcu;
+	struct page_pool_params p;
+
+	/*
+	 * Data structure for allocation side
+	 *
+	 * Drivers allocation side usually already perform some kind
+	 * of resource protection.  Piggyback on this protection, and
+	 * require driver to protect allocation side.
+	 *
+	 * For NIC drivers this means, allocate a page_pool per
+	 * RX-queue. As the RX-queue is already protected by
+	 * Softirq/BH scheduling and napi_schedule. NAPI schedule
+	 * guarantee that a single napi_struct will only be scheduled
+	 * on a single CPU (see napi_schedule).
+	 */
+	struct pp_alloc_cache alloc ____cacheline_aligned_in_smp;
+
+	/* Data structure for storing recycled pages.
+	 *
+	 * Returning/freeing pages is more complicated synchronization
+	 * wise, because free's can happen on remote CPUs, with no
+	 * association with allocation resource.
+	 *
+	 * Use ptr_ring, as it separates consumer and producer
+	 * effeciently, it a way that doesn't bounce cache-lines.
+	 *
+	 * TODO: Implement bulk return pages into this structure.
+	 */
+	struct ptr_ring ring;
+};
+
+struct page *page_pool_alloc_pages(struct page_pool *pool, gfp_t gfp);
+
+static inline struct page *page_pool_dev_alloc_pages(struct page_pool *pool)
+{
+	gfp_t gfp = (GFP_ATOMIC | __GFP_NOWARN);
+
+	return page_pool_alloc_pages(pool, gfp);
+}
+
+struct page_pool *page_pool_create(const struct page_pool_params *params);
+
+void page_pool_destroy(struct page_pool *pool);
+
+/* Never call this directly, use helpers below */
+void __page_pool_put_page(struct page_pool *pool,
+			  struct page *page, bool allow_direct);
+
+static inline void page_pool_put_page(struct page_pool *pool, struct page *page)
+{
+	__page_pool_put_page(pool, page, false);
+}
+/* Very limited use-cases allow recycle direct */
+static inline void page_pool_recycle_direct(struct page_pool *pool,
+					    struct page *page)
+{
+	__page_pool_put_page(pool, page, true);
+}
+
+#endif /* _NET_PAGE_POOL_H */
