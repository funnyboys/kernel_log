commit e91de6afa81c10e9f855c5695eb9a53168d96b73
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri May 29 16:06:59 2020 -0700

    bpf: Fix running sk_skb program types with ktls
    
    KTLS uses a stream parser to collect TLS messages and send them to
    the upper layer tls receive handler. This ensures the tls receiver
    has a full TLS header to parse when it is run. However, when a
    socket has BPF_SK_SKB_STREAM_VERDICT program attached before KTLS
    is enabled we end up with two stream parsers running on the same
    socket.
    
    The result is both try to run on the same socket. First the KTLS
    stream parser runs and calls read_sock() which will tcp_read_sock
    which in turn calls tcp_rcv_skb(). This dequeues the skb from the
    sk_receive_queue. When this is done KTLS code then data_ready()
    callback which because we stacked KTLS on top of the bpf stream
    verdict program has been replaced with sk_psock_start_strp(). This
    will in turn kick the stream parser again and eventually do the
    same thing KTLS did above calling into tcp_rcv_skb() and dequeuing
    a skb from the sk_receive_queue.
    
    At this point the data stream is broke. Part of the stream was
    handled by the KTLS side some other bytes may have been handled
    by the BPF side. Generally this results in either missing data
    or more likely a "Bad Message" complaint from the kTLS receive
    handler as the BPF program steals some bytes meant to be in a
    TLS header and/or the TLS header length is no longer correct.
    
    We've already broke the idealized model where we can stack ULPs
    in any order with generic callbacks on the TX side to handle this.
    So in this patch we do the same thing but for RX side. We add
    a sk_psock_strp_enabled() helper so TLS can learn a BPF verdict
    program is running and add a tls_sw_has_ctx_rx() helper so BPF
    side can learn there is a TLS ULP on the socket.
    
    Then on BPF side we omit calling our stream parser to avoid
    breaking the data stream for the KTLS receiver. Then on the
    KTLS side we call BPF_SK_SKB_STREAM_VERDICT once the KTLS
    receiver is done with the packet but before it posts the
    msg to userspace. This gives us symmetry between the TX and
    RX halfs and IMO makes it usable again. On the TX side we
    process packets in this order BPF -> TLS -> TCP and on
    the receive side in the reverse order TCP -> TLS -> BPF.
    
    Discovered while testing OpenSSL 3.0 Alpha2.0 release.
    
    Fixes: d829e9c4112b5 ("tls: convert to generic sk_msg interface")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/159079361946.5745.605854335665044485.stgit@john-Precision-5820-Tower
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3e7b44cae0d9..3212d3c214a9 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -571,6 +571,15 @@ static inline bool tls_sw_has_ctx_tx(const struct sock *sk)
 	return !!tls_sw_ctx_tx(ctx);
 }
 
+static inline bool tls_sw_has_ctx_rx(const struct sock *sk)
+{
+	struct tls_context *ctx = tls_get_ctx(sk);
+
+	if (!ctx)
+		return false;
+	return !!tls_sw_ctx_rx(ctx);
+}
+
 void tls_sw_write_space(struct sock *sk, struct tls_context *ctx);
 void tls_device_write_space(struct sock *sk, struct tls_context *ctx);
 

commit 1806c13dc2532090d742ce03847b22367fb20ad6
Merge: 1079a34c56c5 bdc48fa11e46
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 31 17:48:46 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    xdp_umem.c had overlapping changes between the 64-bit math fix
    for the calculation of npgs and the removal of the zerocopy
    memory type which got rid of the chunk_size_nohdr member.
    
    The mlx5 Kconfig conflict is a case where we just take the
    net-next copy of the Kconfig entry dependency as it takes on
    the ESWITCH dependency by one level of indirection which is
    what the 'net' conflicting change is trying to ensure.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b3ae2459f89773adcbf16fef4b68deaaa3be1929
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed May 27 12:25:26 2020 +0300

    net/tls: Add force_resync for driver resync
    
    This patch adds a field to the tls rx offload context which enables
    drivers to force a send_resync call.
    
    This field can be used by drivers to request a resync at the next
    possible tls record. It is beneficial for hardware that provides the
    resync sequence number asynchronously. In such cases, the packet that
    triggered the resync does not contain the information required for a
    resync. Instead, the driver requests resync for all the following
    TLS record until the asynchronous notification with the resync request
    TCP sequence arrives.
    
    A following series for mlx5e ConnectX-6DX TLS RX offload support will
    use this mechanism.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index bf9eb4823933..cf9ec152fbb7 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -594,12 +594,22 @@ tls_driver_ctx(const struct sock *sk, enum tls_offload_ctx_dir direction)
 #endif
 
 /* The TLS context is valid until sk_destruct is called */
+#define RESYNC_REQ (1 << 0)
+#define RESYNC_REQ_FORCE (1 << 1)
 static inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)
 {
 	struct tls_context *tls_ctx = tls_get_ctx(sk);
 	struct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);
 
-	atomic64_set(&rx_ctx->resync_req, ((u64)ntohl(seq) << 32) | 1);
+	atomic64_set(&rx_ctx->resync_req, ((u64)ntohl(seq) << 32) | RESYNC_REQ);
+}
+
+static inline void tls_offload_rx_force_resync_request(struct sock *sk)
+{
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+	struct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);
+
+	atomic64_set(&rx_ctx->resync_req, RESYNC_REQ | RESYNC_REQ_FORCE);
 }
 
 static inline void

commit 0cada33241d9de205522e3858b18e506ca5cce2c
Author: Vinay Kumar Yadav <vinay.yadav@chelsio.com>
Date:   Sat May 23 01:40:31 2020 +0530

    net/tls: fix race condition causing kernel panic
    
    tls_sw_recvmsg() and tls_decrypt_done() can be run concurrently.
    // tls_sw_recvmsg()
            if (atomic_read(&ctx->decrypt_pending))
                    crypto_wait_req(-EINPROGRESS, &ctx->async_wait);
            else
                    reinit_completion(&ctx->async_wait.completion);
    
    //tls_decrypt_done()
            pending = atomic_dec_return(&ctx->decrypt_pending);
    
            if (!pending && READ_ONCE(ctx->async_notify))
                    complete(&ctx->async_wait.completion);
    
    Consider the scenario tls_decrypt_done() is about to run complete()
    
            if (!pending && READ_ONCE(ctx->async_notify))
    
    and tls_sw_recvmsg() reads decrypt_pending == 0, does reinit_completion(),
    then tls_decrypt_done() runs complete(). This sequence of execution
    results in wrong completion. Consequently, for next decrypt request,
    it will not wait for completion, eventually on connection close, crypto
    resources freed, there is no way to handle pending decrypt response.
    
    This race condition can be avoided by having atomic_read() mutually
    exclusive with atomic_dec_return(),complete().Intoduced spin lock to
    ensure the mutual exclution.
    
    Addressed similar problem in tx direction.
    
    v1->v2:
    - More readable commit message.
    - Corrected the lock to fix new race scenario.
    - Removed barrier which is not needed now.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    Signed-off-by: Vinay Kumar Yadav <vinay.yadav@chelsio.com>
    Reviewed-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index bf9eb4823933..18cd4f418464 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -135,6 +135,8 @@ struct tls_sw_context_tx {
 	struct tls_rec *open_rec;
 	struct list_head tx_list;
 	atomic_t encrypt_pending;
+	/* protect crypto_wait with encrypt_pending */
+	spinlock_t encrypt_compl_lock;
 	int async_notify;
 	u8 async_capable:1;
 
@@ -155,6 +157,8 @@ struct tls_sw_context_rx {
 	u8 async_capable:1;
 	u8 decrypted:1;
 	atomic_t decrypt_pending;
+	/* protect crypto_wait with decrypt_pending*/
+	spinlock_t decrypt_compl_lock;
 	bool async_notify;
 };
 

commit 8d5a49e9e31ba1ddd34a54b2351d068a90c78707
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Dec 17 14:12:01 2019 -0800

    net/tls: add helper for testing if socket is RX offloaded
    
    There is currently no way for driver to reliably check that
    the socket it has looked up is in fact RX offloaded. Add
    a helper. This allows drivers to catch misbehaving firmware.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index df630f5fc723..bf9eb4823933 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -641,6 +641,7 @@ int tls_sw_fallback_init(struct sock *sk,
 #ifdef CONFIG_TLS_DEVICE
 void tls_device_init(void);
 void tls_device_cleanup(void);
+void tls_device_sk_destruct(struct sock *sk);
 int tls_set_device_offload(struct sock *sk, struct tls_context *ctx);
 void tls_device_free_resources_tx(struct sock *sk);
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
@@ -649,6 +650,14 @@ void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
 void tls_offload_tx_resync_request(struct sock *sk, u32 got_seq, u32 exp_seq);
 int tls_device_decrypted(struct sock *sk, struct tls_context *tls_ctx,
 			 struct sk_buff *skb, struct strp_msg *rxm);
+
+static inline bool tls_is_sk_rx_device_offloaded(struct sock *sk)
+{
+	if (!sk_fullsock(sk) ||
+	    smp_load_acquire(&sk->sk_destruct) != tls_device_sk_destruct)
+		return false;
+	return tls_get_ctx(sk)->rx_conf == TLS_HW;
+}
 #else
 static inline void tls_device_init(void) {}
 static inline void tls_device_cleanup(void) {}

commit c5daa6cccdc2f94aca2c9b3fa5f94e4469997293
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Nov 27 12:16:44 2019 -0800

    net/tls: use sg_next() to walk sg entries
    
    Partially sent record cleanup path increments an SG entry
    directly instead of using sg_next(). This should not be a
    problem today, as encrypted messages should be always
    allocated as arrays. But given this is a cleanup path it's
    easy to miss was this ever to change. Use sg_next(), and
    simplify the code.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9d32f7ce6b31..df630f5fc723 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -376,7 +376,7 @@ int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 		int flags);
 int tls_push_partial_record(struct sock *sk, struct tls_context *ctx,
 			    int flags);
-bool tls_free_partial_record(struct sock *sk, struct tls_context *ctx);
+void tls_free_partial_record(struct sock *sk, struct tls_context *ctx);
 
 static inline struct tls_msg *tls_msg(struct sk_buff *skb)
 {

commit 9e5ffed37df68d0ccfb2fdc528609e23a1e70ebe
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Nov 27 12:16:43 2019 -0800

    net/tls: remove the dead inplace_crypto code
    
    Looks like when BPF support was added by commit d3b18ad31f93
    ("tls: add bpf support to sk_msg handling") and
    commit d829e9c4112b ("tls: convert to generic sk_msg interface")
    it broke/removed the support for in-place crypto as added by
    commit 4e6d47206c32 ("tls: Add support for inplace records
    encryption").
    
    The inplace_crypto member of struct tls_rec is dead, inited
    to zero, and sometimes set to zero again. It used to be
    set to 1 when record was allocated, but the skmsg code doesn't
    seem to have been written with the idea of in-place crypto
    in mind.
    
    Since non trivial effort is required to bring the feature back
    and we don't really have the HW to measure the benefit just
    remove the left over support for now to avoid confusing readers.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 6ed91e82edd0..9d32f7ce6b31 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -100,7 +100,6 @@ struct tls_rec {
 	struct list_head list;
 	int tx_ready;
 	int tx_flags;
-	int inplace_crypto;
 
 	struct sk_msg msg_plaintext;
 	struct sk_msg msg_encrypted;

commit a9f852e92e40992c4ff09ac3940f7725e016317a
Merge: 3243e04ab1c0 34c36f4564b8
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 22 16:27:24 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Minor conflict in drivers/s390/net/qeth_l2_main.c, kept the lock
    from commit c8183f548902 ("s390/qeth: fix potential deadlock on
    workqueue flush"), removed the code which was removed by commit
    9897d583b015 ("s390/qeth: consolidate some duplicated HW cmd code").
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

commit d4ffb02dee2fcb20e0c8086a8d1305bf885820bb
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Nov 18 10:40:51 2019 -0500

    net/tls: enable sk_msg redirect to tls socket egress
    
    Bring back tls_sw_sendpage_locked. sk_msg redirection into a socket
    with TLS_TX takes the following path:
    
      tcp_bpf_sendmsg_redir
        tcp_bpf_push_locked
          tcp_bpf_push
            kernel_sendpage_locked
              sock->ops->sendpage_locked
    
    Also update the flags test in tls_sw_sendpage_locked to allow flag
    MSG_NO_SHARED_FRAGS. bpf_tcp_sendmsg sets this.
    
    Link: https://lore.kernel.org/netdev/CA+FuTSdaAawmZ2N8nfDDKu3XLpXBbMtcCT0q4FntDD2gn8ASUw@mail.gmail.com/T/#t
    Link: https://github.com/wdebruij/kerneltools/commits/icept.2
    Fixes: 0608c69c9a80 ("bpf: sk_msg, sock{map|hash} redirect through ULP")
    Fixes: f3de19af0f5b ("Revert \"net/tls: remove unused function tls_sw_sendpage_locked\"")
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 794e297483ea..f4ad831eaa02 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -356,6 +356,8 @@ int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx);
 void tls_sw_strparser_arm(struct sock *sk, struct tls_context *ctx);
 void tls_sw_strparser_done(struct tls_context *tls_ctx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
+int tls_sw_sendpage_locked(struct sock *sk, struct page *page,
+			   int offset, size_t size, int flags);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_cancel_work_tx(struct tls_context *tls_ctx);

commit 14684b93019a2d2ece0df5acaf921924541b928d
Merge: 92da362c07d4 0058b0a506e4
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 9 11:04:37 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    One conflict in the BPF samples Makefile, some fixes in 'net' whilst
    we were converting over to Makefile.target rules in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 79ffe6087e9145d2377385cac48d0d6a6b4225a5
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Nov 5 14:24:35 2019 -0800

    net/tls: add a TX lock
    
    TLS TX needs to release and re-acquire the socket lock if send buffer
    fills up.
    
    TLS SW TX path currently depends on only allowing one thread to enter
    the function by the abuse of sk_write_pending. If another writer is
    already waiting for memory no new ones are allowed in.
    
    This has two problems:
     - writers don't wake other threads up when they leave the kernel;
       meaning that this scheme works for single extra thread (second
       application thread or delayed work) because memory becoming
       available will send a wake up request, but as Mallesham and
       Pooja report with larger number of threads it leads to threads
       being put to sleep indefinitely;
     - the delayed work does not get _scheduled_ but it may _run_ when
       other writers are present leading to crashes as writers don't
       expect state to change under their feet (same records get pushed
       and freed multiple times); it's hard to reliably bail from the
       work, however, because the mere presence of a writer does not
       guarantee that the writer will push pending records before exiting.
    
    Ensuring wakeups always happen will make the code basically open
    code a mutex. Just use a mutex.
    
    The TLS HW TX path does not have any locking (not even the
    sk_write_pending hack), yet it uses a per-socket sg_tx_data
    array to push records.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    Reported-by: Mallesham  Jatharakonda <mallesh537@gmail.com>
    Reported-by: Pooja Trivedi <poojatrivedi@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index c664e6dba0d1..794e297483ea 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -40,6 +40,7 @@
 #include <linux/socket.h>
 #include <linux/tcp.h>
 #include <linux/skmsg.h>
+#include <linux/mutex.h>
 #include <linux/netdevice.h>
 #include <linux/rcupdate.h>
 
@@ -269,6 +270,10 @@ struct tls_context {
 
 	bool in_tcp_sendpages;
 	bool pending_open_record_frags;
+
+	struct mutex tx_lock; /* protects partially_sent_* fields and
+			       * per-type TX fields
+			       */
 	unsigned long flags;
 
 	/* cache cold stuff */

commit bc76e5bb1229ede1f26317b813099b0e983e4009
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Sun Oct 6 21:09:32 2019 -0700

    net/tls: store decrypted on a single bit
    
    Use a single bit instead of boolean to remember if packet
    was already decrypted.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 97eae7271a67..41265e542e71 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -153,7 +153,7 @@ struct tls_sw_context_rx {
 	struct sk_buff *recv_pkt;
 	u8 control;
 	u8 async_capable:1;
-	bool decrypted;
+	u8 decrypted:1;
 	atomic_t decrypt_pending;
 	bool async_notify;
 };

commit 5c5458ec9d631fbca29f53a944168265e18aa77a
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Sun Oct 6 21:09:31 2019 -0700

    net/tls: store async_capable on a single bit
    
    Store async_capable on a single bit instead of a full integer
    to save space.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index b809f2362049..97eae7271a67 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -136,7 +136,7 @@ struct tls_sw_context_tx {
 	struct list_head tx_list;
 	atomic_t encrypt_pending;
 	int async_notify;
-	int async_capable;
+	u8 async_capable:1;
 
 #define BIT_TX_SCHEDULED	0
 #define BIT_TX_CLOSING		1
@@ -152,7 +152,7 @@ struct tls_sw_context_rx {
 
 	struct sk_buff *recv_pkt;
 	u8 control;
-	int async_capable;
+	u8 async_capable:1;
 	bool decrypted;
 	atomic_t decrypt_pending;
 	bool async_notify;

commit 4de30a8d58c90e18140342cdcb74903d2e4fbb62
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Sun Oct 6 21:09:30 2019 -0700

    net/tls: pass context to tls_device_decrypted()
    
    Avoid unnecessary pointer chasing and calculations, callers already
    have most of the state tls_device_decrypted() needs.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 24c37bffc961..b809f2362049 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -641,7 +641,8 @@ int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
 void tls_device_offload_cleanup_rx(struct sock *sk);
 void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
 void tls_offload_tx_resync_request(struct sock *sk, u32 got_seq, u32 exp_seq);
-int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
+int tls_device_decrypted(struct sock *sk, struct tls_context *tls_ctx,
+			 struct sk_buff *skb, struct strp_msg *rxm);
 #else
 static inline void tls_device_init(void) {}
 static inline void tls_device_cleanup(void) {}
@@ -664,7 +665,9 @@ static inline void tls_device_offload_cleanup_rx(struct sock *sk) {}
 static inline void
 tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq) {}
 
-static inline int tls_device_decrypted(struct sock *sk, struct sk_buff *skb)
+static inline int
+tls_device_decrypted(struct sock *sk, struct tls_context *tls_ctx,
+		     struct sk_buff *skb, struct strp_msg *rxm)
 {
 	return 0;
 }

commit d26b698dd3cd52f5a3277446a87e5e0198c99cd0
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Oct 4 16:19:24 2019 -0700

    net/tls: add skeleton of MIB statistics
    
    Add a skeleton structure for adding TLS statistics.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 38086ade65ce..24c37bffc961 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -43,6 +43,7 @@
 #include <linux/netdevice.h>
 #include <linux/rcupdate.h>
 
+#include <net/net_namespace.h>
 #include <net/tcp.h>
 #include <net/strparser.h>
 #include <crypto/aead.h>
@@ -73,6 +74,15 @@
  */
 #define TLS_AES_CCM_IV_B0_BYTE		2
 
+#define __TLS_INC_STATS(net, field)				\
+	__SNMP_INC_STATS((net)->mib.tls_statistics, field)
+#define TLS_INC_STATS(net, field)				\
+	SNMP_INC_STATS((net)->mib.tls_statistics, field)
+#define __TLS_DEC_STATS(net, field)				\
+	__SNMP_DEC_STATS((net)->mib.tls_statistics, field)
+#define TLS_DEC_STATS(net, field)				\
+	SNMP_DEC_STATS((net)->mib.tls_statistics, field)
+
 enum {
 	TLS_BASE,
 	TLS_SW,
@@ -605,6 +615,9 @@ static inline bool tls_offload_tx_resync_pending(struct sock *sk)
 	return ret;
 }
 
+int __net_init tls_proc_init(struct net *net);
+void __net_exit tls_proc_fini(struct net *net);
+
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
 int decrypt_skb(struct sock *sk, struct sk_buff *skb,

commit 8538d29cea9530f114159e06bfa31b2358161493
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Oct 4 16:19:22 2019 -0700

    net/tls: add tracing for device/offload events
    
    Add tracing of device-related interaction to aid performance
    analysis, especially around resync:
    
     tls:tls_device_offload_set
     tls:tls_device_rx_resync_send
     tls:tls_device_rx_resync_nh_schedule
     tls:tls_device_rx_resync_nh_delay
     tls:tls_device_tx_resync_req
     tls:tls_device_tx_resync_send
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 5c48cb9e0c18..38086ade65ce 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -594,13 +594,6 @@ tls_offload_rx_resync_set_type(struct sock *sk, enum tls_offload_sync_type type)
 	tls_offload_ctx_rx(tls_ctx)->resync_type = type;
 }
 
-static inline void tls_offload_tx_resync_request(struct sock *sk)
-{
-	struct tls_context *tls_ctx = tls_get_ctx(sk);
-
-	WARN_ON(test_and_set_bit(TLS_TX_SYNC_SCHED, &tls_ctx->flags));
-}
-
 /* Driver's seq tracking has to be disabled until resync succeeded */
 static inline bool tls_offload_tx_resync_pending(struct sock *sk)
 {
@@ -634,6 +627,7 @@ void tls_device_free_resources_tx(struct sock *sk);
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
 void tls_device_offload_cleanup_rx(struct sock *sk);
 void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
+void tls_offload_tx_resync_request(struct sock *sk, u32 got_seq, u32 exp_seq);
 int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
 #else
 static inline void tls_device_init(void) {}

commit 08700dab816847d5e600ef263155fb04ea4b312d
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Oct 3 11:18:57 2019 -0700

    net/tls: move TOE-related code to a separate file
    
    Move tls_hw_* functions to a new, separate source file
    to avoid confusion with normal, non-TOE offload.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 57865c944095..5c48cb9e0c18 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -308,7 +308,10 @@ struct tls_offload_context_rx {
 #define TLS_OFFLOAD_CONTEXT_SIZE_RX					\
 	(sizeof(struct tls_offload_context_rx) + TLS_DRIVER_STATE_SIZE_RX)
 
+struct tls_context *tls_ctx_create(struct sock *sk);
 void tls_ctx_free(struct sock *sk, struct tls_context *ctx);
+void update_sk_prot(struct sock *sk, struct tls_context *ctx);
+
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,
 		int __user *optlen);

commit 25a3cd8189c8832c04225e6f1d41228fd6cc64cc
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Oct 3 11:18:54 2019 -0700

    net/tls: move TOE-related structures to a separate header
    
    Move tls_device structure and register/unregister functions
    to a new header to avoid confusion with normal, non-TOE offload.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index c664e6dba0d1..57865c944095 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -60,7 +60,6 @@
 #define TLS_RECORD_TYPE_DATA		0x17
 
 #define TLS_AAD_SPACE_SIZE		13
-#define TLS_DEVICE_NAME_MAX		32
 
 #define MAX_IV_SIZE			16
 #define TLS_MAX_REC_SEQ_SIZE		8
@@ -74,37 +73,6 @@
  */
 #define TLS_AES_CCM_IV_B0_BYTE		2
 
-/*
- * This structure defines the routines for Inline TLS driver.
- * The following routines are optional and filled with a
- * null pointer if not defined.
- *
- * @name: Its the name of registered Inline tls device
- * @dev_list: Inline tls device list
- * int (*feature)(struct tls_device *device);
- *     Called to return Inline TLS driver capability
- *
- * int (*hash)(struct tls_device *device, struct sock *sk);
- *     This function sets Inline driver for listen and program
- *     device specific functioanlity as required
- *
- * void (*unhash)(struct tls_device *device, struct sock *sk);
- *     This function cleans listen state set by Inline TLS driver
- *
- * void (*release)(struct kref *kref);
- *     Release the registered device and allocated resources
- * @kref: Number of reference to tls_device
- */
-struct tls_device {
-	char name[TLS_DEVICE_NAME_MAX];
-	struct list_head dev_list;
-	int  (*feature)(struct tls_device *device);
-	int  (*hash)(struct tls_device *device, struct sock *sk);
-	void (*unhash)(struct tls_device *device, struct sock *sk);
-	void (*release)(struct kref *kref);
-	struct kref kref;
-};
-
 enum {
 	TLS_BASE,
 	TLS_SW,
@@ -643,8 +611,6 @@ static inline bool tls_offload_tx_resync_pending(struct sock *sk)
 
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
-void tls_register_device(struct tls_device *device);
-void tls_unregister_device(struct tls_device *device);
 int decrypt_skb(struct sock *sk, struct sk_buff *skb,
 		struct scatterlist *sgout);
 struct sk_buff *tls_encrypt_skb(struct sk_buff *skb);

commit be2fbc155fc8c0ff6e499753354d965cd9cf1bb0
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Sep 2 21:31:05 2019 -0700

    net/tls: clean up the number of #ifdefs for CONFIG_TLS_DEVICE
    
    TLS code has a number of #ifdefs which make the code a little
    harder to follow. Recent fixes removed the ifdef around the
    TLS_HW define, so we can switch to the often used pattern
    of defining tls_device functions as empty static inlines
    in the header when CONFIG_TLS_DEVICE=n.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 6dab6683e42f..c664e6dba0d1 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -366,13 +366,9 @@ ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
 			   struct pipe_inode_info *pipe,
 			   size_t len, unsigned int flags);
 
-int tls_set_device_offload(struct sock *sk, struct tls_context *ctx);
 int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_device_sendpage(struct sock *sk, struct page *page,
 			int offset, size_t size, int flags);
-void tls_device_free_resources_tx(struct sock *sk);
-void tls_device_init(void);
-void tls_device_cleanup(void);
 int tls_tx_records(struct sock *sk, int flags);
 
 struct tls_record_info *tls_get_record(struct tls_offload_context_tx *context,
@@ -649,7 +645,6 @@ int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
 void tls_register_device(struct tls_device *device);
 void tls_unregister_device(struct tls_device *device);
-int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
 int decrypt_skb(struct sock *sk, struct sk_buff *skb,
 		struct scatterlist *sgout);
 struct sk_buff *tls_encrypt_skb(struct sk_buff *skb);
@@ -662,9 +657,40 @@ int tls_sw_fallback_init(struct sock *sk,
 			 struct tls_offload_context_tx *offload_ctx,
 			 struct tls_crypto_info *crypto_info);
 
+#ifdef CONFIG_TLS_DEVICE
+void tls_device_init(void);
+void tls_device_cleanup(void);
+int tls_set_device_offload(struct sock *sk, struct tls_context *ctx);
+void tls_device_free_resources_tx(struct sock *sk);
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
-
 void tls_device_offload_cleanup_rx(struct sock *sk);
 void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
+int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
+#else
+static inline void tls_device_init(void) {}
+static inline void tls_device_cleanup(void) {}
 
+static inline int
+tls_set_device_offload(struct sock *sk, struct tls_context *ctx)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void tls_device_free_resources_tx(struct sock *sk) {}
+
+static inline int
+tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void tls_device_offload_cleanup_rx(struct sock *sk) {}
+static inline void
+tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq) {}
+
+static inline int tls_device_decrypted(struct sock *sk, struct sk_buff *skb)
+{
+	return 0;
+}
+#endif
 #endif /* _TLS_OFFLOAD_H */

commit be7bbea114d6ab2688b9e59cd24a306d21e51c27
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Sep 2 21:31:02 2019 -0700

    net/tls: use the full sk_proto pointer
    
    Since we already have the pointer to the full original sk_proto
    stored use that instead of storing all individual callback
    pointers as well.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: John Hurley <john.hurley@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index ec3c3ed2c6c3..6dab6683e42f 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -275,16 +275,6 @@ struct tls_context {
 	struct proto *sk_proto;
 
 	void (*sk_destruct)(struct sock *sk);
-	void (*sk_proto_close)(struct sock *sk, long timeout);
-
-	int  (*setsockopt)(struct sock *sk, int level,
-			   int optname, char __user *optval,
-			   unsigned int optlen);
-	int  (*getsockopt)(struct sock *sk, int level,
-			   int optname, char __user *optval,
-			   int __user *optlen);
-	int  (*hash)(struct sock *sk);
-	void (*unhash)(struct sock *sk);
 
 	union tls_crypto_context crypto_send;
 	union tls_crypto_context crypto_recv;

commit 26811cc9f55acf835f7fdadc5ff2bbd6f06bc3ac
Author: Davide Caratti <dcaratti@redhat.com>
Date:   Fri Aug 30 12:25:49 2019 +0200

    net: tls: export protocol version, cipher, tx_conf/rx_conf to socket diag
    
    When an application configures kernel TLS on top of a TCP socket, it's
    now possible for inet_diag_handler() to collect information regarding the
    protocol version, the cipher type and TX / RX configuration, in case
    INET_DIAG_INFO is requested.
    
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 4997742475cd..ec3c3ed2c6c3 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -431,6 +431,23 @@ static inline bool is_tx_ready(struct tls_sw_context_tx *ctx)
 	return READ_ONCE(rec->tx_ready);
 }
 
+static inline u16 tls_user_config(struct tls_context *ctx, bool tx)
+{
+	u16 config = tx ? ctx->tx_conf : ctx->rx_conf;
+
+	switch (config) {
+	case TLS_BASE:
+		return TLS_CONF_BASE;
+	case TLS_SW:
+		return TLS_CONF_SW;
+	case TLS_HW:
+		return TLS_CONF_HW;
+	case TLS_HW_RECORD:
+		return TLS_CONF_HW_RECORD;
+	}
+	return 0;
+}
+
 struct sk_buff *
 tls_validate_xmit_skb(struct sock *sk, struct net_device *dev,
 		      struct sk_buff *skb);

commit 15a7dea750e0162f273c6e61a94f96944b75b31e
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Aug 30 12:25:47 2019 +0200

    net/tls: use RCU protection on icsk->icsk_ulp_data
    
    We need to make sure context does not get freed while diag
    code is interrogating it. Free struct tls_context with
    kfree_rcu().
    
    We add the __rcu annotation directly in icsk, and cast it
    away in the datapath accessor. Presumably all ULPs will
    do a similar thing.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 41b2d41bb1b8..4997742475cd 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -41,6 +41,7 @@
 #include <linux/tcp.h>
 #include <linux/skmsg.h>
 #include <linux/netdevice.h>
+#include <linux/rcupdate.h>
 
 #include <net/tcp.h>
 #include <net/strparser.h>
@@ -290,6 +291,7 @@ struct tls_context {
 
 	struct list_head list;
 	refcount_t refcount;
+	struct rcu_head rcu;
 };
 
 enum tls_offload_ctx_dir {
@@ -348,7 +350,7 @@ struct tls_offload_context_rx {
 #define TLS_OFFLOAD_CONTEXT_SIZE_RX					\
 	(sizeof(struct tls_offload_context_rx) + TLS_DRIVER_STATE_SIZE_RX)
 
-void tls_ctx_free(struct tls_context *ctx);
+void tls_ctx_free(struct sock *sk, struct tls_context *ctx);
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,
 		int __user *optlen);
@@ -467,7 +469,10 @@ static inline struct tls_context *tls_get_ctx(const struct sock *sk)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
-	return icsk->icsk_ulp_data;
+	/* Use RCU on icsk_ulp_data only for sock diag code,
+	 * TLS data path doesn't need rcu_dereference().
+	 */
+	return (__force void *)icsk->icsk_ulp_data;
 }
 
 static inline void tls_advance_record_sn(struct sock *sk,

commit 5d92e631b8be8965a90c144320f06e096081a551
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Aug 1 14:36:01 2019 -0700

    net/tls: partially revert fix transition through disconnect with close
    
    Looks like we were slightly overzealous with the shutdown()
    cleanup. Even though the sock->sk_state can reach CLOSED again,
    socket->state will not got back to SS_UNCONNECTED once
    connections is ESTABLISHED. Meaning we will see EISCONN if
    we try to reconnect, and EINVAL if we try to listen.
    
    Only listen sockets can be shutdown() and reused, but since
    ESTABLISHED sockets can never be re-connected() or used for
    listen() we don't need to try to clean up the ULP state early.
    
    Fixes: 32857cf57f92 ("net/tls: fix transition through disconnect with close")
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9e425ac2de45..41b2d41bb1b8 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -290,8 +290,6 @@ struct tls_context {
 
 	struct list_head list;
 	refcount_t refcount;
-
-	struct work_struct gc;
 };
 
 enum tls_offload_ctx_dir {

commit 32857cf57f920cdc03b5095f08febec94cf9c36b
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Jul 19 10:29:18 2019 -0700

    net/tls: fix transition through disconnect with close
    
    It is possible (via shutdown()) for TCP socks to go through TCP_CLOSE
    state via tcp_disconnect() without actually calling tcp_close which
    would then call the tls close callback. Because of this a user could
    disconnect a socket then put it in a LISTEN state which would break
    our assumptions about sockets always being ESTABLISHED state.
    
    More directly because close() can call unhash() and unhash is
    implemented by sockmap if a sockmap socket has TLS enabled we can
    incorrectly destroy the psock from unhash() and then call its close
    handler again. But because the psock (sockmap socket representation)
    is already destroyed we call close handler in sk->prot. However,
    in some cases (TLS BASE/BASE case) this will still point at the
    sockmap close handler resulting in a circular call and crash reported
    by syzbot.
    
    To fix both above issues implement the unhash() routine for TLS.
    
    v4:
     - add note about tls offload still needing the fix;
     - move sk_proto to the cold cache line;
     - split TX context free into "release" and "free",
       otherwise the GC work itself is in already freed
       memory;
     - more TX before RX for consistency;
     - reuse tls_ctx_free();
     - schedule the GC work after we're done with context
       to avoid UAF;
     - don't set the unhash in all modes, all modes "inherit"
       TLS_BASE's callbacks anyway;
     - disable the unhash hook for TLS_HW.
    
    Fixes: 3c4d7559159bf ("tls: kernel TLS support")
    Reported-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 235508e35fd4..9e425ac2de45 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -271,6 +271,8 @@ struct tls_context {
 	unsigned long flags;
 
 	/* cache cold stuff */
+	struct proto *sk_proto;
+
 	void (*sk_destruct)(struct sock *sk);
 	void (*sk_proto_close)(struct sock *sk, long timeout);
 
@@ -288,6 +290,8 @@ struct tls_context {
 
 	struct list_head list;
 	refcount_t refcount;
+
+	struct work_struct gc;
 };
 
 enum tls_offload_ctx_dir {
@@ -359,7 +363,6 @@ void tls_sw_strparser_done(struct tls_context *tls_ctx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
-void tls_sw_close(struct sock *sk, long timeout);
 void tls_sw_cancel_work_tx(struct tls_context *tls_ctx);
 void tls_sw_release_resources_tx(struct sock *sk);
 void tls_sw_free_ctx_tx(struct tls_context *tls_ctx);

commit 313ab004805cf52a42673b15852b3842474ccd87
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Jul 19 10:29:17 2019 -0700

    net/tls: remove sock unlock/lock around strp_done()
    
    The tls close() callback currently drops the sock lock to call
    strp_done(). Split up the RX cleanup into stopping the strparser
    and releasing most resources, syncing strparser and finally
    freeing the context.
    
    To avoid the need for a strp_done() call on the cleanup path
    of device offload make sure we don't arm the strparser until
    we are sure init will be successful.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index d4276cb6de53..235508e35fd4 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -107,9 +107,7 @@ struct tls_device {
 enum {
 	TLS_BASE,
 	TLS_SW,
-#ifdef CONFIG_TLS_DEVICE
 	TLS_HW,
-#endif
 	TLS_HW_RECORD,
 	TLS_NUM_CONFIG,
 };
@@ -357,14 +355,17 @@ int tls_sk_attach(struct sock *sk, int optname, char __user *optval,
 
 int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx);
 void tls_sw_strparser_arm(struct sock *sk, struct tls_context *ctx);
+void tls_sw_strparser_done(struct tls_context *tls_ctx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_close(struct sock *sk, long timeout);
 void tls_sw_cancel_work_tx(struct tls_context *tls_ctx);
-void tls_sw_free_resources_tx(struct sock *sk);
+void tls_sw_release_resources_tx(struct sock *sk);
+void tls_sw_free_ctx_tx(struct tls_context *tls_ctx);
 void tls_sw_free_resources_rx(struct sock *sk);
 void tls_sw_release_resources_rx(struct sock *sk);
+void tls_sw_free_ctx_rx(struct tls_context *tls_ctx);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
 bool tls_sw_stream_read(const struct sock *sk);

commit f87e62d45e51b12d48d2cb46b5cde8f83b866bc4
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Jul 19 10:29:16 2019 -0700

    net/tls: remove close callback sock unlock/lock around TX work flush
    
    The tls close() callback currently drops the sock lock, makes a
    cancel_delayed_work_sync() call, and then relocks the sock.
    
    By restructuring the code we can avoid droping lock and then
    reclaiming it. To simplify this we do the following,
    
     tls_sk_proto_close
     set_bit(CLOSING)
     set_bit(SCHEDULE)
     cancel_delay_work_sync() <- cancel workqueue
     lock_sock(sk)
     ...
     release_sock(sk)
     strp_done()
    
    Setting the CLOSING bit prevents the SCHEDULE bit from being
    cleared by any workqueue items e.g. if one happens to be
    scheduled and run between when we set SCHEDULE bit and cancel
    work. Then because SCHEDULE bit is set now no new work will
    be scheduled.
    
    Tested with net selftests and bpf selftests.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 43f551cd508b..d4276cb6de53 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -162,6 +162,7 @@ struct tls_sw_context_tx {
 	int async_capable;
 
 #define BIT_TX_SCHEDULED	0
+#define BIT_TX_CLOSING		1
 	unsigned long tx_bitmask;
 };
 
@@ -360,6 +361,7 @@ int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_close(struct sock *sk, long timeout);
+void tls_sw_cancel_work_tx(struct tls_context *tls_ctx);
 void tls_sw_free_resources_tx(struct sock *sk);
 void tls_sw_free_resources_rx(struct sock *sk);
 void tls_sw_release_resources_rx(struct sock *sk);

commit 318892ac068397f40ff81d9155898da01493b1d2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Jul 19 10:29:14 2019 -0700

    net/tls: don't arm strparser immediately in tls_set_sw_offload()
    
    In tls_set_device_offload_rx() we prepare the software context
    for RX fallback and proceed to add the connection to the device.
    Unfortunately, software context prep includes arming strparser
    so in case of a later error we have to release the socket lock
    to call strp_done().
    
    In preparation for not releasing the socket lock half way through
    callbacks move arming strparser into a separate function.
    Following patches will make use of that.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 584609174fe0..43f551cd508b 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -355,6 +355,7 @@ int tls_sk_attach(struct sock *sk, int optname, char __user *optval,
 		  unsigned int optlen);
 
 int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx);
+void tls_sw_strparser_arm(struct sock *sk, struct tls_context *ctx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);

commit b5d9a834f4fd1b6abfa527ec351c871084dd23a3
Author: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
Date:   Mon Jul 8 19:53:13 2019 -0700

    net/tls: don't clear TX resync flag on error
    
    Introduce a return code for the tls_dev_resync callback.
    
    When the driver TX resync fails, kernel can retry the resync again
    until it succeeds.  This prevents drivers from attempting to offload
    TLS packets if the connection is known to be out of sync.
    
    We don't worry about the RX resync since they will be retried naturally
    as more encrypted records get received.
    
    Signed-off-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 176d0b039f32..584609174fe0 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -304,9 +304,9 @@ struct tlsdev_ops {
 	void (*tls_dev_del)(struct net_device *netdev,
 			    struct tls_context *ctx,
 			    enum tls_offload_ctx_dir direction);
-	void (*tls_dev_resync)(struct net_device *netdev,
-			       struct sock *sk, u32 seq, u8 *rcd_sn,
-			       enum tls_offload_ctx_dir direction);
+	int (*tls_dev_resync)(struct net_device *netdev,
+			      struct sock *sk, u32 seq, u8 *rcd_sn,
+			      enum tls_offload_ctx_dir direction);
 };
 
 enum tls_offload_sync_type {

commit af144a983402f7fd324ce556d9f9011a8b3e01fe
Merge: 6413139dfc64 e858faf556d4
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jul 8 19:48:57 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Two cases of overlapping changes, nothing fancy.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit acd3e96d53a24d219f720ed4012b62723ae05da1
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Jun 28 16:11:39 2019 -0700

    net/tls: make sure offload also gets the keys wiped
    
    Commit 86029d10af18 ("tls: zero the crypto information from tls_context
    before freeing") added memzero_explicit() calls to clear the key material
    before freeing struct tls_context, but it missed tls_device.c has its
    own way of freeing this structure. Replace the missing free.
    
    Fixes: 86029d10af18 ("tls: zero the crypto information from tls_context before freeing")
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 53d96bca220d..889df0312cd1 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -313,6 +313,7 @@ struct tls_offload_context_rx {
 	(ALIGN(sizeof(struct tls_offload_context_rx), sizeof(void *)) + \
 	 TLS_DRIVER_STATE_SIZE)
 
+void tls_ctx_free(struct tls_context *ctx);
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,
 		int __user *optlen);

commit d96ff269a04be286989ead13bf8b4be55bdee8ee
Merge: 3a49584477ff 556e2f6020bf
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 27 21:06:39 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The new route handling in ip_mc_finish_output() from 'net' overlapped
    with the new support for returning congestion notifications from BPF
    programs.
    
    In order to handle this I had to take the dev_loopback_xmit() calls
    out of the switch statement.
    
    The aquantia driver conflicts were simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9354544cbccf68da1b047f8fb7b47630e3c8a59d
Author: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
Date:   Sun Jun 23 21:26:58 2019 -0700

    net/tls: fix page double free on TX cleanup
    
    With commit 94850257cf0f ("tls: Fix tls_device handling of partial records")
    a new path was introduced to cleanup partial records during sk_proto_close.
    This path does not handle the SW KTLS tx_list cleanup.
    
    This is unnecessary though since the free_resources calls for both
    SW and offload paths will cleanup a partial record.
    
    The visible effect is the following warning, but this bug also causes
    a page double free.
    
        WARNING: CPU: 7 PID: 4000 at net/core/stream.c:206 sk_stream_kill_queues+0x103/0x110
        RIP: 0010:sk_stream_kill_queues+0x103/0x110
        RSP: 0018:ffffb6df87e07bd0 EFLAGS: 00010206
        RAX: 0000000000000000 RBX: ffff8c21db4971c0 RCX: 0000000000000007
        RDX: ffffffffffffffa0 RSI: 000000000000001d RDI: ffff8c21db497270
        RBP: ffff8c21db497270 R08: ffff8c29f4748600 R09: 000000010020001a
        R10: ffffb6df87e07aa0 R11: ffffffff9a445600 R12: 0000000000000007
        R13: 0000000000000000 R14: ffff8c21f03f2900 R15: ffff8c21f03b8df0
        Call Trace:
         inet_csk_destroy_sock+0x55/0x100
         tcp_close+0x25d/0x400
         ? tcp_check_oom+0x120/0x120
         tls_sk_proto_close+0x127/0x1c0
         inet_release+0x3c/0x60
         __sock_release+0x3d/0xb0
         sock_close+0x11/0x20
         __fput+0xd8/0x210
         task_work_run+0x84/0xa0
         do_exit+0x2dc/0xb90
         ? release_sock+0x43/0x90
         do_group_exit+0x3a/0xa0
         get_signal+0x295/0x720
         do_signal+0x36/0x610
         ? SYSC_recvfrom+0x11d/0x130
         exit_to_usermode_loop+0x69/0xb0
         do_syscall_64+0x173/0x180
         entry_SYSCALL_64_after_hwframe+0x3d/0xa2
        RIP: 0033:0x7fe9b9abc10d
        RSP: 002b:00007fe9b19a1d48 EFLAGS: 00000246 ORIG_RAX: 00000000000000ca
        RAX: fffffffffffffe00 RBX: 0000000000000006 RCX: 00007fe9b9abc10d
        RDX: 0000000000000002 RSI: 0000000000000080 RDI: 00007fe948003430
        RBP: 00007fe948003410 R08: 00007fe948003430 R09: 0000000000000000
        R10: 0000000000000000 R11: 0000000000000246 R12: 00005603739d9080
        R13: 00007fe9b9ab9f90 R14: 00007fe948003430 R15: 0000000000000000
    
    Fixes: 94850257cf0f ("tls: Fix tls_device handling of partial records")
    Signed-off-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 4a55ce6a303f..53d96bca220d 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -373,21 +373,6 @@ static inline bool tls_is_partially_sent_record(struct tls_context *ctx)
 	return !!ctx->partially_sent_record;
 }
 
-static inline int tls_complete_pending_work(struct sock *sk,
-					    struct tls_context *ctx,
-					    int flags, long *timeo)
-{
-	int rc = 0;
-
-	if (unlikely(sk->sk_write_pending))
-		rc = wait_on_pending_writer(sk, timeo);
-
-	if (!rc && tls_is_partially_sent_record(ctx))
-		rc = tls_push_partial_record(sk, ctx, flags);
-
-	return rc;
-}
-
 static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 {
 	return tls_ctx->pending_open_record_frags;

commit 50180074099fcda752d9d56282d23242b126ebc9
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 10 21:40:09 2019 -0700

    net/tls: add kernel-driven resync mechanism for TX
    
    TLS offload drivers keep track of TCP seq numbers to make sure
    the packets are fed into the HW in order.
    
    When packets get dropped on the way through the stack, the driver
    will get out of sync and have to use fallback encryption, but unless
    TCP seq number is resynced it will never match the packets correctly
    (or even worse - use incorrect record sequence number after TCP seq
    wraps).
    
    Existing drivers (mlx5) feed the entire record on every out-of-order
    event, allowing FW/HW to always be in sync.
    
    This patch adds an alternative, more akin to the RX resync.  When
    driver sees a frame which is past its expected sequence number the
    stream must have gotten out of order (if the sequence number is
    smaller than expected its likely a retransmission which doesn't
    require resync).  Driver will ask the stack to perform TX sync
    before it submits the next full record, and fall back to software
    crypto until stack has performed the sync.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9b49baecc4a8..63e473420b00 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -212,6 +212,11 @@ struct tls_offload_context_tx {
 
 enum tls_context_flags {
 	TLS_RX_SYNC_RUNNING = 0,
+	/* Unlike RX where resync is driven entirely by the core in TX only
+	 * the driver knows when things went out of sync, so we need the flag
+	 * to be atomic.
+	 */
+	TLS_TX_SYNC_SCHED = 1,
 };
 
 struct cipher_context {
@@ -619,6 +624,24 @@ tls_offload_rx_resync_set_type(struct sock *sk, enum tls_offload_sync_type type)
 	tls_offload_ctx_rx(tls_ctx)->resync_type = type;
 }
 
+static inline void tls_offload_tx_resync_request(struct sock *sk)
+{
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+
+	WARN_ON(test_and_set_bit(TLS_TX_SYNC_SCHED, &tls_ctx->flags));
+}
+
+/* Driver's seq tracking has to be disabled until resync succeeded */
+static inline bool tls_offload_tx_resync_pending(struct sock *sk)
+{
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+	bool ret;
+
+	ret = test_bit(TLS_TX_SYNC_SCHED, &tls_ctx->flags);
+	smp_mb__after_atomic();
+	return ret;
+}
+
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
 void tls_register_device(struct tls_device *device);

commit eeb2efaf36c75753f9028de3500669bddfac81a8
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 10 21:40:08 2019 -0700

    net/tls: generalize the resync callback
    
    Currently only RX direction is ever resynced, however, TX may
    also get out of sequence if packets get dropped on the way to
    the driver.  Rename the resync callback and add a direction
    parameter.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 28eca6a3b615..9b49baecc4a8 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -299,8 +299,9 @@ struct tlsdev_ops {
 	void (*tls_dev_del)(struct net_device *netdev,
 			    struct tls_context *ctx,
 			    enum tls_offload_ctx_dir direction);
-	void (*tls_dev_resync_rx)(struct net_device *netdev,
-				  struct sock *sk, u32 seq, u8 *rcd_sn);
+	void (*tls_dev_resync)(struct net_device *netdev,
+			       struct sock *sk, u32 seq, u8 *rcd_sn,
+			       enum tls_offload_ctx_dir direction);
 };
 
 enum tls_offload_sync_type {

commit f953d33ba1225d68cf8790b4706d8c4410b15926
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 10 21:40:02 2019 -0700

    net/tls: add kernel-driven TLS RX resync
    
    TLS offload device may lose sync with the TCP stream if packets
    arrive out of order.  Drivers can currently request a resync at
    a specific TCP sequence number.  When a record is found starting
    at that sequence number kernel will inform the device of the
    corresponding record number.
    
    This requires the device to constantly scan the stream for a
    known pattern (constant bytes of the header) after sync is lost.
    
    This patch adds an alternative approach which is entirely under
    the control of the kernel.  Kernel tracks records it had to fully
    decrypt, even though TLS socket is in TLS_HW mode.  If multiple
    records did not have any decrypted parts - it's a pretty strong
    indication that the device is out of sync.
    
    We choose the min number of fully encrypted records to be 2,
    which should hopefully be more than will get retransmitted at
    a time.
    
    After kernel decides the device is out of sync it schedules a
    resync request.  If the TCP socket is empty the resync gets
    performed immediately.  If socket is not empty we leave the
    record parser to resync when next record comes.
    
    Before resync in message parser we peek at the TCP socket and
    don't attempt the sync if the socket already has some of the
    next record queued.
    
    On resync failure (encrypted data continues to flow in) we
    retry with exponential backoff, up to once every 128 records
    (with a 16k record thats at most once every 2M of data).
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 1c512da5e4f4..28eca6a3b615 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -303,10 +303,33 @@ struct tlsdev_ops {
 				  struct sock *sk, u32 seq, u8 *rcd_sn);
 };
 
+enum tls_offload_sync_type {
+	TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ = 0,
+	TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT = 1,
+};
+
+#define TLS_DEVICE_RESYNC_NH_START_IVAL		2
+#define TLS_DEVICE_RESYNC_NH_MAX_IVAL		128
+
 struct tls_offload_context_rx {
 	/* sw must be the first member of tls_offload_context_rx */
 	struct tls_sw_context_rx sw;
-	atomic64_t resync_req;
+	enum tls_offload_sync_type resync_type;
+	/* this member is set regardless of resync_type, to avoid branches */
+	u8 resync_nh_reset:1;
+	/* CORE_NEXT_HINT-only member, but use the hole here */
+	u8 resync_nh_do_now:1;
+	union {
+		/* TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ */
+		struct {
+			atomic64_t resync_req;
+		};
+		/* TLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT */
+		struct {
+			u32 decrypted_failed;
+			u32 decrypted_tgt;
+		} resync_nh;
+	};
 	u8 driver_state[] __aligned(8);
 	/* The TLS layer reserves room for driver specific state
 	 * Currently the belief is that there is not enough
@@ -587,6 +610,13 @@ static inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)
 	atomic64_set(&rx_ctx->resync_req, ((u64)ntohl(seq) << 32) | 1);
 }
 
+static inline void
+tls_offload_rx_resync_set_type(struct sock *sk, enum tls_offload_sync_type type)
+{
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+
+	tls_offload_ctx_rx(tls_ctx)->resync_type = type;
+}
 
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
@@ -608,6 +638,6 @@ int tls_sw_fallback_init(struct sock *sk,
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
 
 void tls_device_offload_cleanup_rx(struct sock *sk);
-void tls_device_rx_resync_new_rec(struct sock *sk, u32 seq);
+void tls_device_rx_resync_new_rec(struct sock *sk, u32 rcd_len, u32 seq);
 
 #endif /* _TLS_OFFLOAD_H */

commit fe58a5a02cd9f49d5868539b4146ec1e5e5176e4
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 10 21:40:01 2019 -0700

    net/tls: rename handle_device_resync()
    
    handle_device_resync() doesn't describe the function very well.
    The function checks if resync should be issued upon parsing of
    a new record.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 25641e2f5b96..1c512da5e4f4 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -608,6 +608,6 @@ int tls_sw_fallback_init(struct sock *sk,
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
 
 void tls_device_offload_cleanup_rx(struct sock *sk);
-void handle_device_resync(struct sock *sk, u32 seq);
+void tls_device_rx_resync_new_rec(struct sock *sk, u32 seq);
 
 #endif /* _TLS_OFFLOAD_H */

commit 89fec474fa1ab2c754e48d29e1081a2c2bd22dc6
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 10 21:40:00 2019 -0700

    net/tls: pass record number as a byte array
    
    TLS offload code casts record number to a u64.  The buffer
    should be aligned to 8 bytes, but its actually a __be64, and
    the rest of the TLS code treats it as big int.  Make the
    offload callbacks take a byte array, drivers can make the
    choice to do the ugly cast if they want to.
    
    Prepare for copying the record number onto the stack by
    defining a constant for max size of the byte array.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3ecf45adb707..25641e2f5b96 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -62,6 +62,7 @@
 #define TLS_DEVICE_NAME_MAX		32
 
 #define MAX_IV_SIZE			16
+#define TLS_MAX_REC_SEQ_SIZE		8
 
 /* For AES-CCM, the full 16-bytes of IV is made of '4' fields of given sizes.
  *
@@ -299,7 +300,7 @@ struct tlsdev_ops {
 			    struct tls_context *ctx,
 			    enum tls_offload_ctx_dir direction);
 	void (*tls_dev_resync_rx)(struct net_device *netdev,
-				  struct sock *sk, u32 seq, u64 rcd_sn);
+				  struct sock *sk, u32 seq, u8 *rcd_sn);
 };
 
 struct tls_offload_context_rx {
@@ -607,6 +608,6 @@ int tls_sw_fallback_init(struct sock *sk,
 int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
 
 void tls_device_offload_cleanup_rx(struct sock *sk);
-void handle_device_resync(struct sock *sk, u32 seq, u64 rcd_sn);
+void handle_device_resync(struct sock *sk, u32 seq);
 
 #endif /* _TLS_OFFLOAD_H */

commit a6cdeeb16bff89c8486324f53577db058cbe81ba
Merge: 96524ea4be04 1e1d92636954
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 7 11:00:14 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Some ISDN files that got removed in net-next had some changes
    done in mainline, take the removals.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b9727d7f957d68febb3b5f68d4be270ee6fb57e7
Author: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
Date:   Wed Jun 5 14:11:40 2019 -0700

    net/tls: export TLS per skb encryption
    
    While offloading TLS connections, drivers need to handle the case where
    out of order packets need to be transmitted.
    
    Other drivers obtain the entire TLS record for the specific skb to
    provide as context to hardware for encryption. However, other designs
    may also want to keep the hardware state intact and perform the
    out of order encryption entirely on the host.
    
    To achieve this, export the already existing software encryption
    fallback path so drivers could access this.
    
    Signed-off-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3da0d941e729..d1a4f365d6be 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -590,6 +590,7 @@ void tls_unregister_device(struct tls_device *device);
 int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
 int decrypt_skb(struct sock *sk, struct sk_buff *skb,
 		struct scatterlist *sgout);
+struct sk_buff *tls_encrypt_skb(struct sk_buff *skb);
 
 struct sk_buff *tls_validate_xmit_skb(struct sock *sk,
 				      struct net_device *dev,

commit 2e361176ea01c4b2f90b17c2264a2a794050c3f3
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Jun 5 14:11:39 2019 -0700

    net/tls: simplify driver context retrieval
    
    Currently drivers have to ensure the alignment of their tls state
    structure, which leads to unnecessary layers of getters and
    encapsulated structures in each driver.
    
    Simplify all this by marking the driver state as aligned (driver_state
    members are currently aligned, so no hole is added, besides ALIGN in
    TLS_OFFLOAD_CONTEXT_SIZE_RX/TX would reserve this extra space, anyway.)
    With that we can add a common accessor to the core.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3094db5398a9..3da0d941e729 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -40,6 +40,7 @@
 #include <linux/socket.h>
 #include <linux/tcp.h>
 #include <linux/skmsg.h>
+#include <linux/netdevice.h>
 
 #include <net/tcp.h>
 #include <net/strparser.h>
@@ -197,7 +198,7 @@ struct tls_offload_context_tx {
 
 	struct scatterlist sg_tx_data[MAX_SKB_FRAGS];
 	void (*sk_destruct)(struct sock *sk);
-	u8 driver_state[];
+	u8 driver_state[] __aligned(8);
 	/* The TLS layer reserves room for driver specific state
 	 * Currently the belief is that there is not enough
 	 * driver specific state to justify another layer of indirection
@@ -206,8 +207,7 @@ struct tls_offload_context_tx {
 };
 
 #define TLS_OFFLOAD_CONTEXT_SIZE_TX                                            \
-	(ALIGN(sizeof(struct tls_offload_context_tx), sizeof(void *)) +        \
-	 TLS_DRIVER_STATE_SIZE_TX)
+	(sizeof(struct tls_offload_context_tx) + TLS_DRIVER_STATE_SIZE_TX)
 
 struct cipher_context {
 	char *iv;
@@ -302,7 +302,7 @@ struct tls_offload_context_rx {
 	/* sw must be the first member of tls_offload_context_rx */
 	struct tls_sw_context_rx sw;
 	atomic64_t resync_req;
-	u8 driver_state[];
+	u8 driver_state[] __aligned(8);
 	/* The TLS layer reserves room for driver specific state
 	 * Currently the belief is that there is not enough
 	 * driver specific state to justify another layer of indirection
@@ -311,8 +311,7 @@ struct tls_offload_context_rx {
 };
 
 #define TLS_OFFLOAD_CONTEXT_SIZE_RX					\
-	(ALIGN(sizeof(struct tls_offload_context_rx), sizeof(void *)) + \
-	 TLS_DRIVER_STATE_SIZE_RX)
+	(sizeof(struct tls_offload_context_rx) + TLS_DRIVER_STATE_SIZE_RX)
 
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,
@@ -557,6 +556,23 @@ tls_offload_ctx_rx(const struct tls_context *tls_ctx)
 	return (struct tls_offload_context_rx *)tls_ctx->priv_ctx_rx;
 }
 
+#if IS_ENABLED(CONFIG_TLS_DEVICE)
+static inline void *__tls_driver_ctx(struct tls_context *tls_ctx,
+				     enum tls_offload_ctx_dir direction)
+{
+	if (direction == TLS_OFFLOAD_CTX_DIR_TX)
+		return tls_offload_ctx_tx(tls_ctx)->driver_state;
+	else
+		return tls_offload_ctx_rx(tls_ctx)->driver_state;
+}
+
+static inline void *
+tls_driver_ctx(const struct sock *sk, enum tls_offload_ctx_dir direction)
+{
+	return __tls_driver_ctx(tls_get_ctx(sk), direction);
+}
+#endif
+
 /* The TLS context is valid until sk_destruct is called */
 static inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)
 {

commit 2d6b51c6924c1bba8e4948dc4a2dbc96bf685b97
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Jun 5 14:11:38 2019 -0700

    net/tls: split the TLS_DRIVER_STATE_SIZE and bump TX to 16 bytes
    
    8 bytes of driver state has been enough so far, but for drivers
    which have to store 8 byte handle it's no longer practical to
    store the state directly in the context.
    
    Drivers generally don't need much extra state on RX side, while
    TX side has to be tracking TCP sequence numbers.  Split the
    lengths of max driver state size on RX and TX.
    
    The struct tls_offload_context_tx currently stands at 616 bytes and
    struct tls_offload_context_rx stands at 368 bytes.  Upcoming work
    will consume extra 8 bytes in both for kernel-driven resync.
    This means that we can bump TX side to 16 bytes and still fit
    into the same number of cache lines but on RX side we would be 8
    bytes over.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 0a0072636009..3094db5398a9 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -202,12 +202,12 @@ struct tls_offload_context_tx {
 	 * Currently the belief is that there is not enough
 	 * driver specific state to justify another layer of indirection
 	 */
-#define TLS_DRIVER_STATE_SIZE (max_t(size_t, 8, sizeof(void *)))
+#define TLS_DRIVER_STATE_SIZE_TX	16
 };
 
 #define TLS_OFFLOAD_CONTEXT_SIZE_TX                                            \
 	(ALIGN(sizeof(struct tls_offload_context_tx), sizeof(void *)) +        \
-	 TLS_DRIVER_STATE_SIZE)
+	 TLS_DRIVER_STATE_SIZE_TX)
 
 struct cipher_context {
 	char *iv;
@@ -307,11 +307,12 @@ struct tls_offload_context_rx {
 	 * Currently the belief is that there is not enough
 	 * driver specific state to justify another layer of indirection
 	 */
+#define TLS_DRIVER_STATE_SIZE_RX	8
 };
 
 #define TLS_OFFLOAD_CONTEXT_SIZE_RX					\
 	(ALIGN(sizeof(struct tls_offload_context_rx), sizeof(void *)) + \
-	 TLS_DRIVER_STATE_SIZE)
+	 TLS_DRIVER_STATE_SIZE_RX)
 
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,

commit fb0f886fa265f265ad126fc7cd7e8ec51e2f770f
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 3 15:17:05 2019 -0700

    net/tls: don't pass version to tls_advance_record_sn()
    
    All callers pass prot->version as the last parameter
    of tls_advance_record_sn(), yet tls_advance_record_sn()
    itself needs a pointer to prot.  Pass prot from callers.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a463a6074e5d..0a0072636009 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -446,19 +446,15 @@ static inline struct tls_context *tls_get_ctx(const struct sock *sk)
 }
 
 static inline void tls_advance_record_sn(struct sock *sk,
-					 struct cipher_context *ctx,
-					 int version)
+					 struct tls_prot_info *prot,
+					 struct cipher_context *ctx)
 {
-	struct tls_context *tls_ctx = tls_get_ctx(sk);
-	struct tls_prot_info *prot = &tls_ctx->prot_info;
-
 	if (tls_bigint_increment(ctx->rec_seq, prot->rec_seq_size))
 		tls_err_abort(sk, EBADMSG);
 
-	if (version != TLS_1_3_VERSION) {
+	if (prot->version != TLS_1_3_VERSION)
 		tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
 				     prot->iv_size);
-	}
 }
 
 static inline void tls_fill_prepend(struct tls_context *ctx,

commit f0aaa2c975617da78b80feebc87e74dba9ec1f53
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jun 3 15:17:04 2019 -0700

    net/tls: reorganize struct tls_context
    
    struct tls_context is slightly badly laid out.  If we reorder things
    right we can save 16 bytes (320 -> 304) but also make all fast path
    data fit into two cache lines (one read only and one read/write,
    down from four cache lines).
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 39ea62f0c1f6..a463a6074e5d 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -236,34 +236,32 @@ struct tls_prot_info {
 };
 
 struct tls_context {
+	/* read-only cache line */
 	struct tls_prot_info prot_info;
 
-	union tls_crypto_context crypto_send;
-	union tls_crypto_context crypto_recv;
+	u8 tx_conf:3;
+	u8 rx_conf:3;
 
-	struct list_head list;
-	struct net_device *netdev;
-	refcount_t refcount;
+	int (*push_pending_record)(struct sock *sk, int flags);
+	void (*sk_write_space)(struct sock *sk);
 
 	void *priv_ctx_tx;
 	void *priv_ctx_rx;
 
-	u8 tx_conf:3;
-	u8 rx_conf:3;
+	struct net_device *netdev;
 
+	/* rw cache line */
 	struct cipher_context tx;
 	struct cipher_context rx;
 
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
 
-	unsigned long flags;
 	bool in_tcp_sendpages;
 	bool pending_open_record_frags;
+	unsigned long flags;
 
-	int (*push_pending_record)(struct sock *sk, int flags);
-
-	void (*sk_write_space)(struct sock *sk);
+	/* cache cold stuff */
 	void (*sk_destruct)(struct sock *sk);
 	void (*sk_proto_close)(struct sock *sk, long timeout);
 
@@ -275,6 +273,12 @@ struct tls_context {
 			   int __user *optlen);
 	int  (*hash)(struct sock *sk);
 	void (*unhash)(struct sock *sk);
+
+	union tls_crypto_context crypto_send;
+	union tls_crypto_context crypto_recv;
+
+	struct list_head list;
+	refcount_t refcount;
 };
 
 enum tls_offload_ctx_dir {

commit e52972c11d6b1262964db96d65934196db621685
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Tue Jun 4 12:00:12 2019 -0700

    net/tls: replace the sleeping lock around RX resync with a bit lock
    
    Commit 38030d7cb779 ("net/tls: avoid NULL-deref on resync during device removal")
    tried to fix a potential NULL-dereference by taking the
    context rwsem.  Unfortunately the RX resync may get called
    from soft IRQ, so we can't use the rwsem to protect from
    the device disappearing.  Because we are guaranteed there
    can be only one resync at a time (it's called from strparser)
    use a bit to indicate resync is busy and make device
    removal wait for the bit to get cleared.
    
    Note that there is a leftover "flags" field in struct
    tls_context already.
    
    Fixes: 4799ac81e52a ("tls: Add rx inline crypto offload")
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 39ea62f0c1f6..4a55ce6a303f 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -209,6 +209,10 @@ struct tls_offload_context_tx {
 	(ALIGN(sizeof(struct tls_offload_context_tx), sizeof(void *)) +        \
 	 TLS_DRIVER_STATE_SIZE)
 
+enum tls_context_flags {
+	TLS_RX_SYNC_RUNNING = 0,
+};
+
 struct cipher_context {
 	char *iv;
 	char *rec_seq;

commit 63a1c95f3fe48b4e9fe0c261b376e5e527b71b25
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Apr 25 12:32:04 2019 -0700

    net/tls: byte swap device req TCP seq no upon setting
    
    To avoid a sparse warning byteswap the be32 sequence number
    before it's stored in the atomic value.  While at it drop
    unnecessary brackets and use kernel's u64 type.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 41a2ee643fc5..39ea62f0c1f6 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -562,7 +562,7 @@ static inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)
 	struct tls_context *tls_ctx = tls_get_ctx(sk);
 	struct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);
 
-	atomic64_set(&rx_ctx->resync_req, ((((uint64_t)seq) << 32) | 1));
+	atomic64_set(&rx_ctx->resync_req, ((u64)ntohl(seq) << 32) | 1);
 }
 
 

commit da68b4ad02343862fee1e3e8c6315984f16a4778
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Apr 25 12:32:03 2019 -0700

    net/tls: move definition of tls ops into net/tls.h
    
    There seems to be no reason for tls_ops to be defined in netdevice.h
    which is included in a lot of places.  Don't wrap the struct/enum
    declaration in ifdefs, it trickles down unnecessary ifdefs into
    driver code.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 20196cb31ecc..41a2ee643fc5 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -277,6 +277,23 @@ struct tls_context {
 	void (*unhash)(struct sock *sk);
 };
 
+enum tls_offload_ctx_dir {
+	TLS_OFFLOAD_CTX_DIR_RX,
+	TLS_OFFLOAD_CTX_DIR_TX,
+};
+
+struct tlsdev_ops {
+	int (*tls_dev_add)(struct net_device *netdev, struct sock *sk,
+			   enum tls_offload_ctx_dir direction,
+			   struct tls_crypto_info *crypto_info,
+			   u32 start_offload_tcp_sn);
+	void (*tls_dev_del)(struct net_device *netdev,
+			    struct tls_context *ctx,
+			    enum tls_offload_ctx_dir direction);
+	void (*tls_dev_resync_rx)(struct net_device *netdev,
+				  struct sock *sk, u32 seq, u64 rcd_sn);
+};
+
 struct tls_offload_context_rx {
 	/* sw must be the first member of tls_offload_context_rx */
 	struct tls_sw_context_rx sw;

commit 9e9957973c7785b1f8fa77f099cac661cc5e7e5b
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Apr 25 12:32:02 2019 -0700

    net/tls: remove old exports of sk_destruct functions
    
    tls_device_sk_destruct being set on a socket used to indicate
    that socket is a kTLS device one.  That is no longer true -
    now we use sk_validate_xmit_skb pointer for that purpose.
    Remove the export.  tls_device_attach() needs to be moved.
    
    While at it, remove the dead declaration of tls_sk_destruct().
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index d9d0ac66f040..20196cb31ecc 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -317,7 +317,6 @@ int tls_set_device_offload(struct sock *sk, struct tls_context *ctx);
 int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_device_sendpage(struct sock *sk, struct page *page,
 			int offset, size_t size, int flags);
-void tls_device_sk_destruct(struct sock *sk);
 void tls_device_free_resources_tx(struct sock *sk);
 void tls_device_init(void);
 void tls_device_cleanup(void);
@@ -336,7 +335,6 @@ static inline u32 tls_record_start_seq(struct tls_record_info *rec)
 	return rec->end_seq - rec->len;
 }
 
-void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
 int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 		struct scatterlist *sg, u16 first_offset,
 		int flags);

commit 6b0a7f84ea1fe248df96ccc4dd86e817e32ef65b
Merge: cea0aa9cbd5a fe5cdef29e41
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 17 11:26:25 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflict resolution of af_smc.c from Stephen Rothwell.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b4f47f3848eb70986f75d06112af7b48b7f5f462
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Apr 8 17:59:50 2019 -0700

    net/tls: prevent bad memory access in tls_is_sk_tx_device_offloaded()
    
    Unlike '&&' operator, the '&' does not have short-circuit
    evaluation semantics.  IOW both sides of the operator always
    get evaluated.  Fix the wrong operator in
    tls_is_sk_tx_device_offloaded(), which would lead to
    out-of-bounds access for for non-full sockets.
    
    Fixes: 4799ac81e52a ("tls: Add rx inline crypto offload")
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index c7f7dc344e73..5934246b2c6f 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -381,7 +381,7 @@ tls_validate_xmit_skb(struct sock *sk, struct net_device *dev,
 static inline bool tls_is_sk_tx_device_offloaded(struct sock *sk)
 {
 #ifdef CONFIG_SOCK_VALIDATE_XMIT
-	return sk_fullsock(sk) &
+	return sk_fullsock(sk) &&
 	       (smp_load_acquire(&sk->sk_validate_xmit_skb) ==
 	       &tls_validate_xmit_skb);
 #else

commit 35b71a34ada62c9573847a324bf06a133fe11b11
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Apr 10 11:04:31 2019 -0700

    net/tls: don't leak partially sent record in device mode
    
    David reports that tls triggers warnings related to
    sk->sk_forward_alloc not being zero at destruction time:
    
    WARNING: CPU: 5 PID: 6831 at net/core/stream.c:206 sk_stream_kill_queues+0x103/0x110
    WARNING: CPU: 5 PID: 6831 at net/ipv4/af_inet.c:160 inet_sock_destruct+0x15b/0x170
    
    When sender fills up the write buffer and dies from
    SIGPIPE.  This is due to the device implementation
    not cleaning up the partially_sent_record.
    
    This is because commit a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    moved the partial record cleanup to the SW-only path.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    Reported-by: David Beckett <david.beckett@netronome.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Dirk van der Merwe <dirk.vandermerwe@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a5a938583295..c7f7dc344e73 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -307,6 +307,7 @@ int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_device_sendpage(struct sock *sk, struct page *page,
 			int offset, size_t size, int flags);
 void tls_device_sk_destruct(struct sock *sk);
+void tls_device_free_resources_tx(struct sock *sk);
 void tls_device_init(void);
 void tls_device_cleanup(void);
 int tls_tx_records(struct sock *sk, int flags);
@@ -330,6 +331,7 @@ int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 		int flags);
 int tls_push_partial_record(struct sock *sk, struct tls_context *ctx,
 			    int flags);
+bool tls_free_partial_record(struct sock *sk, struct tls_context *ctx);
 
 static inline struct tls_msg *tls_msg(struct sk_buff *skb)
 {

commit f295b3ae9f5927e084bd5decdff82390e3471801
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Wed Mar 20 02:03:36 2019 +0000

    net/tls: Add support of AES128-CCM based ciphers
    
    Added support for AES128-CCM based record encryption. AES128-CCM is
    similar to AES128-GCM. Both of them have same salt/iv/mac size. The
    notable difference between the two is that while invoking AES128-CCM
    operation, the salt||nonce (which is passed as IV) has to be prefixed
    with a hardcoded value '2'. Further, CCM implementation in kernel
    requires IV passed in crypto_aead_request() to be full '16' bytes.
    Therefore, the record structure 'struct tls_rec' has been modified to
    reserve '16' bytes for IV. This works for both GCM and CCM based cipher.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a5a938583295..3ce71d78414c 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -60,6 +60,17 @@
 #define TLS_AAD_SPACE_SIZE		13
 #define TLS_DEVICE_NAME_MAX		32
 
+#define MAX_IV_SIZE			16
+
+/* For AES-CCM, the full 16-bytes of IV is made of '4' fields of given sizes.
+ *
+ * IV[16] = b0[1] || implicit nonce[4] || explicit nonce[8] || length[3]
+ *
+ * The field 'length' is encoded in field 'b0' as '(length width - 1)'.
+ * Hence b0 contains (3 - 1) = 2.
+ */
+#define TLS_AES_CCM_IV_B0_BYTE		2
+
 /*
  * This structure defines the routines for Inline TLS driver.
  * The following routines are optional and filled with a
@@ -123,8 +134,7 @@ struct tls_rec {
 	struct scatterlist sg_content_type;
 
 	char aad_space[TLS_AAD_SPACE_SIZE];
-	u8 iv_data[TLS_CIPHER_AES_GCM_128_IV_SIZE +
-		   TLS_CIPHER_AES_GCM_128_SALT_SIZE];
+	u8 iv_data[MAX_IV_SIZE];
 	struct aead_request aead_req;
 	u8 aead_req_ctx[];
 };
@@ -219,6 +229,7 @@ struct tls_prot_info {
 	u16 tag_size;
 	u16 overhead_size;
 	u16 iv_size;
+	u16 salt_size;
 	u16 rec_seq_size;
 	u16 aad_size;
 	u16 tail_size;

commit 7463d3a2db0efea3701aab5eeb310e0d8157aff7
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Wed Feb 27 17:38:04 2019 +0200

    tls: Fix write space handling
    
    TLS device cannot use the sw context. This patch returns the original
    tls device write space handler and moves the sw/device specific portions
    to the relevant files.
    
    Also, we remove the write_space call for the tls_sw flow, because it
    handles partial records in its delayed tx work handler.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a528a082da73..a5a938583295 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -519,6 +519,9 @@ static inline bool tls_sw_has_ctx_tx(const struct sock *sk)
 	return !!tls_sw_ctx_tx(ctx);
 }
 
+void tls_sw_write_space(struct sock *sk, struct tls_context *ctx);
+void tls_device_write_space(struct sock *sk, struct tls_context *ctx);
+
 static inline struct tls_offload_context_rx *
 tls_offload_ctx_rx(const struct tls_context *tls_ctx)
 {

commit 94850257cf0f88b20db7644f28bfedc7d284de15
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Wed Feb 27 17:38:03 2019 +0200

    tls: Fix tls_device handling of partial records
    
    Cleanup the handling of partial records while fixing a bug where the
    tls_push_pending_closed_record function is using the software tls
    context instead of the hardware context.
    
    The bug resulted in the following crash:
    [   88.791229] BUG: unable to handle kernel NULL pointer dereference at 0000000000000000
    [   88.793271] #PF error: [normal kernel read fault]
    [   88.794449] PGD 800000022a426067 P4D 800000022a426067 PUD 22a156067 PMD 0
    [   88.795958] Oops: 0000 [#1] SMP PTI
    [   88.796884] CPU: 2 PID: 4973 Comm: openssl Not tainted 5.0.0-rc4+ #3
    [   88.798314] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    [   88.800067] RIP: 0010:tls_tx_records+0xef/0x1d0 [tls]
    [   88.801256] Code: 00 02 48 89 43 08 e8 a0 0b 96 d9 48 89 df e8 48 dd
    4d d9 4c 89 f8 4d 8b bf 98 00 00 00 48 05 98 00 00 00 48 89 04 24 49 39
    c7 <49> 8b 1f 4d 89 fd 0f 84 af 00 00 00 41 8b 47 10 85 c0 0f 85 8d 00
    [   88.805179] RSP: 0018:ffffbd888186fca8 EFLAGS: 00010213
    [   88.806458] RAX: ffff9af1ed657c98 RBX: ffff9af1e88a1980 RCX: 0000000000000000
    [   88.808050] RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff9af1e88a1980
    [   88.809724] RBP: ffff9af1e88a1980 R08: 0000000000000017 R09: ffff9af1ebeeb700
    [   88.811294] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
    [   88.812917] R13: ffff9af1e88a1980 R14: ffff9af1ec13f800 R15: 0000000000000000
    [   88.814506] FS:  00007fcad2240740(0000) GS:ffff9af1f7880000(0000) knlGS:0000000000000000
    [   88.816337] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   88.817717] CR2: 0000000000000000 CR3: 0000000228b3e000 CR4: 00000000001406e0
    [   88.819328] Call Trace:
    [   88.820123]  tls_push_data+0x628/0x6a0 [tls]
    [   88.821283]  ? remove_wait_queue+0x20/0x60
    [   88.822383]  ? n_tty_read+0x683/0x910
    [   88.823363]  tls_device_sendmsg+0x53/0xa0 [tls]
    [   88.824505]  sock_sendmsg+0x36/0x50
    [   88.825492]  sock_write_iter+0x87/0x100
    [   88.826521]  __vfs_write+0x127/0x1b0
    [   88.827499]  vfs_write+0xad/0x1b0
    [   88.828454]  ksys_write+0x52/0xc0
    [   88.829378]  do_syscall_64+0x5b/0x180
    [   88.830369]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [   88.831603] RIP: 0033:0x7fcad1451680
    
    [ 1248.470626] BUG: unable to handle kernel NULL pointer dereference at 0000000000000000
    [ 1248.472564] #PF error: [normal kernel read fault]
    [ 1248.473790] PGD 0 P4D 0
    [ 1248.474642] Oops: 0000 [#1] SMP PTI
    [ 1248.475651] CPU: 3 PID: 7197 Comm: openssl Tainted: G           OE 5.0.0-rc4+ #3
    [ 1248.477426] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
    [ 1248.479310] RIP: 0010:tls_tx_records+0x110/0x1f0 [tls]
    [ 1248.480644] Code: 00 02 48 89 43 08 e8 4f cb 63 d7 48 89 df e8 f7 9c
    1b d7 4c 89 f8 4d 8b bf 98 00 00 00 48 05 98 00 00 00 48 89 04 24 49 39
    c7 <49> 8b 1f 4d 89 fd 0f 84 af 00 00 00 41 8b 47 10 85 c0 0f 85 8d 00
    [ 1248.484825] RSP: 0018:ffffaa0a41543c08 EFLAGS: 00010213
    [ 1248.486154] RAX: ffff955a2755dc98 RBX: ffff955a36031980 RCX: 0000000000000006
    [ 1248.487855] RDX: 0000000000000000 RSI: 000000000000002b RDI: 0000000000000286
    [ 1248.489524] RBP: ffff955a36031980 R08: 0000000000000000 R09: 00000000000002b1
    [ 1248.491394] R10: 0000000000000003 R11: 00000000ad55ad55 R12: 0000000000000000
    [ 1248.493162] R13: 0000000000000000 R14: ffff955a2abe6c00 R15: 0000000000000000
    [ 1248.494923] FS:  0000000000000000(0000) GS:ffff955a378c0000(0000) knlGS:0000000000000000
    [ 1248.496847] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 1248.498357] CR2: 0000000000000000 CR3: 000000020c40e000 CR4: 00000000001406e0
    [ 1248.500136] Call Trace:
    [ 1248.500998]  ? tcp_check_oom+0xd0/0xd0
    [ 1248.502106]  tls_sk_proto_close+0x127/0x1e0 [tls]
    [ 1248.503411]  inet_release+0x3c/0x60
    [ 1248.504530]  __sock_release+0x3d/0xb0
    [ 1248.505611]  sock_close+0x11/0x20
    [ 1248.506612]  __fput+0xb4/0x220
    [ 1248.507559]  task_work_run+0x88/0xa0
    [ 1248.508617]  do_exit+0x2cb/0xbc0
    [ 1248.509597]  ? core_sys_select+0x17a/0x280
    [ 1248.510740]  do_group_exit+0x39/0xb0
    [ 1248.511789]  get_signal+0x1d0/0x630
    [ 1248.512823]  do_signal+0x36/0x620
    [ 1248.513822]  exit_to_usermode_loop+0x5c/0xc6
    [ 1248.515003]  do_syscall_64+0x157/0x180
    [ 1248.516094]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [ 1248.517456] RIP: 0033:0x7fb398bd3f53
    [ 1248.518537] Code: Bad RIP value.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption of records for performance")
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9f4117ae2297..a528a082da73 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -199,10 +199,6 @@ struct tls_offload_context_tx {
 	(ALIGN(sizeof(struct tls_offload_context_tx), sizeof(void *)) +        \
 	 TLS_DRIVER_STATE_SIZE)
 
-enum {
-	TLS_PENDING_CLOSED_RECORD
-};
-
 struct cipher_context {
 	char *iv;
 	char *rec_seq;
@@ -335,17 +331,14 @@ int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 int tls_push_partial_record(struct sock *sk, struct tls_context *ctx,
 			    int flags);
 
-int tls_push_pending_closed_record(struct sock *sk, struct tls_context *ctx,
-				   int flags, long *timeo);
-
 static inline struct tls_msg *tls_msg(struct sk_buff *skb)
 {
 	return (struct tls_msg *)strp_msg(skb);
 }
 
-static inline bool tls_is_pending_closed_record(struct tls_context *ctx)
+static inline bool tls_is_partially_sent_record(struct tls_context *ctx)
 {
-	return test_bit(TLS_PENDING_CLOSED_RECORD, &ctx->flags);
+	return !!ctx->partially_sent_record;
 }
 
 static inline int tls_complete_pending_work(struct sock *sk,
@@ -357,17 +350,12 @@ static inline int tls_complete_pending_work(struct sock *sk,
 	if (unlikely(sk->sk_write_pending))
 		rc = wait_on_pending_writer(sk, timeo);
 
-	if (!rc && tls_is_pending_closed_record(ctx))
-		rc = tls_push_pending_closed_record(sk, ctx, flags, timeo);
+	if (!rc && tls_is_partially_sent_record(ctx))
+		rc = tls_push_partial_record(sk, ctx, flags);
 
 	return rc;
 }
 
-static inline bool tls_is_partially_sent_record(struct tls_context *ctx)
-{
-	return !!ctx->partially_sent_record;
-}
-
 static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 {
 	return tls_ctx->pending_open_record_frags;

commit 2b794c4098b525836e37d16045abee3091fdfe18
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Sat Feb 23 08:42:37 2019 +0000

    tls: Return type of non-data records retrieved using MSG_PEEK in recvmsg
    
    The patch enables returning 'type' in msghdr for records that are
    retrieved with MSG_PEEK in recvmsg. Further it prevents records peeked
    from socket from getting clubbed with any other record of different
    type when records are subsequently dequeued from strparser.
    
    For each record, we now retain its type in sk_buff's control buffer
    cb[]. Inside control buffer, record's full length and offset are already
    stored by strparser in 'struct strp_msg'. We store record type after
    'struct strp_msg' inside 'struct tls_msg'. For tls1.2, the type is
    stored just after record dequeue. For tls1.3, the type is stored after
    record has been decrypted.
    
    Inside process_rx_list(), before processing a non-data record, we check
    that we must be able to return back the record type to the user
    application. If not, the decrypted records in tls context's rx_list is
    left there without consuming any data.
    
    Fixes: 692d7b5d1f912 ("tls: Fix recvmsg() to be able to peek across multiple records")
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a8b37226a287..9f4117ae2297 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -129,6 +129,11 @@ struct tls_rec {
 	u8 aead_req_ctx[];
 };
 
+struct tls_msg {
+	struct strp_msg rxm;
+	u8 control;
+};
+
 struct tx_work {
 	struct delayed_work work;
 	struct sock *sk;
@@ -333,6 +338,11 @@ int tls_push_partial_record(struct sock *sk, struct tls_context *ctx,
 int tls_push_pending_closed_record(struct sock *sk, struct tls_context *ctx,
 				   int flags, long *timeo);
 
+static inline struct tls_msg *tls_msg(struct sk_buff *skb)
+{
+	return (struct tls_msg *)strp_msg(skb);
+}
+
 static inline bool tls_is_pending_closed_record(struct tls_context *ctx)
 {
 	return test_bit(TLS_PENDING_CLOSED_RECORD, &ctx->flags);

commit 4509de14680084141d3514c3b87bd9d070fc366d
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Thu Feb 14 07:11:35 2019 +0000

    net/tls: Move protocol constants from cipher context to tls context
    
    Each tls context maintains two cipher contexts (one each for tx and rx
    directions). For each tls session, the constants such as protocol
    version, ciphersuite, iv size, associated data size etc are same for
    both the directions and need to be stored only once per tls context.
    Hence these are moved from 'struct cipher_context' to 'struct
    tls_prot_info' and stored only once in 'struct tls_context'.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index a93a8ed8f716..a8b37226a287 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -199,15 +199,8 @@ enum {
 };
 
 struct cipher_context {
-	u16 prepend_size;
-	u16 tag_size;
-	u16 overhead_size;
-	u16 iv_size;
 	char *iv;
-	u16 rec_seq_size;
 	char *rec_seq;
-	u16 aad_size;
-	u16 tail_size;
 };
 
 union tls_crypto_context {
@@ -218,7 +211,21 @@ union tls_crypto_context {
 	};
 };
 
+struct tls_prot_info {
+	u16 version;
+	u16 cipher_type;
+	u16 prepend_size;
+	u16 tag_size;
+	u16 overhead_size;
+	u16 iv_size;
+	u16 rec_seq_size;
+	u16 aad_size;
+	u16 tail_size;
+};
+
 struct tls_context {
+	struct tls_prot_info prot_info;
+
 	union tls_crypto_context crypto_send;
 	union tls_crypto_context crypto_recv;
 
@@ -401,16 +408,26 @@ static inline bool tls_bigint_increment(unsigned char *seq, int len)
 	return (i == -1);
 }
 
+static inline struct tls_context *tls_get_ctx(const struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+
+	return icsk->icsk_ulp_data;
+}
+
 static inline void tls_advance_record_sn(struct sock *sk,
 					 struct cipher_context *ctx,
 					 int version)
 {
-	if (tls_bigint_increment(ctx->rec_seq, ctx->rec_seq_size))
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+	struct tls_prot_info *prot = &tls_ctx->prot_info;
+
+	if (tls_bigint_increment(ctx->rec_seq, prot->rec_seq_size))
 		tls_err_abort(sk, EBADMSG);
 
 	if (version != TLS_1_3_VERSION) {
 		tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
-				     ctx->iv_size);
+				     prot->iv_size);
 	}
 }
 
@@ -420,9 +437,10 @@ static inline void tls_fill_prepend(struct tls_context *ctx,
 			     unsigned char record_type,
 			     int version)
 {
-	size_t pkt_len, iv_size = ctx->tx.iv_size;
+	struct tls_prot_info *prot = &ctx->prot_info;
+	size_t pkt_len, iv_size = prot->iv_size;
 
-	pkt_len = plaintext_len + ctx->tx.tag_size;
+	pkt_len = plaintext_len + prot->tag_size;
 	if (version != TLS_1_3_VERSION) {
 		pkt_len += iv_size;
 
@@ -475,12 +493,6 @@ static inline void xor_iv_with_seq(int version, char *iv, char *seq)
 	}
 }
 
-static inline struct tls_context *tls_get_ctx(const struct sock *sk)
-{
-	struct inet_connection_sock *icsk = inet_csk(sk);
-
-	return icsk->icsk_ulp_data;
-}
 
 static inline struct tls_sw_context_rx *tls_sw_ctx_rx(
 		const struct tls_context *tls_ctx)

commit 5b053e121ffdec851dc3a7046e9bece287a3c5b1
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jan 30 22:08:21 2019 +0000

    net: tls: Set async_capable for tls zerocopy only if we see EINPROGRESS
    
    Currently we don't zerocopy if the crypto framework async bit is set.
    However some crypto algorithms (such as x86 AESNI) support async,
    but in the context of sendmsg, will never run asynchronously.  Instead,
    check for actual EINPROGRESS return code before assuming algorithm is
    async.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 004bf01ce868..a93a8ed8f716 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -142,6 +142,7 @@ struct tls_sw_context_tx {
 	struct list_head tx_list;
 	atomic_t encrypt_pending;
 	int async_notify;
+	int async_capable;
 
 #define BIT_TX_SCHEDULED	0
 	unsigned long tx_bitmask;

commit 130b392c6cd6b2aed1b7eb32253d4920babb4891
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jan 30 21:58:31 2019 +0000

    net: tls: Add tls 1.3 support
    
    TLS 1.3 has minor changes from TLS 1.2 at the record layer.
    
    * Header now hardcodes the same version and application content type in
      the header.
    * The real content type is appended after the data, before encryption (or
      after decryption).
    * The IV is xored with the sequence number, instead of concatinating four
      bytes of IV with the explicit IV.
    * Zero-padding:  No exlicit length is given, we search backwards from the
      end of the decrypted data for the first non-zero byte, which is the
      content type.  Currently recv supports reading zero-padding, but there
      is no way for send to add zero padding.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 754b130672f0..004bf01ce868 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -119,6 +119,9 @@ struct tls_rec {
 	/* AAD | msg_encrypted.sg.data (data contains overhead for hdr & iv & tag) */
 	struct scatterlist sg_aead_out[2];
 
+	char content_type;
+	struct scatterlist sg_content_type;
+
 	char aad_space[TLS_AAD_SPACE_SIZE];
 	u8 iv_data[TLS_CIPHER_AES_GCM_128_IV_SIZE +
 		   TLS_CIPHER_AES_GCM_128_SALT_SIZE];
@@ -203,6 +206,7 @@ struct cipher_context {
 	u16 rec_seq_size;
 	char *rec_seq;
 	u16 aad_size;
+	u16 tail_size;
 };
 
 union tls_crypto_context {
@@ -397,49 +401,77 @@ static inline bool tls_bigint_increment(unsigned char *seq, int len)
 }
 
 static inline void tls_advance_record_sn(struct sock *sk,
-					 struct cipher_context *ctx)
+					 struct cipher_context *ctx,
+					 int version)
 {
 	if (tls_bigint_increment(ctx->rec_seq, ctx->rec_seq_size))
 		tls_err_abort(sk, EBADMSG);
-	tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
-			     ctx->iv_size);
+
+	if (version != TLS_1_3_VERSION) {
+		tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
+				     ctx->iv_size);
+	}
 }
 
 static inline void tls_fill_prepend(struct tls_context *ctx,
 			     char *buf,
 			     size_t plaintext_len,
-			     unsigned char record_type)
+			     unsigned char record_type,
+			     int version)
 {
 	size_t pkt_len, iv_size = ctx->tx.iv_size;
 
-	pkt_len = plaintext_len + iv_size + ctx->tx.tag_size;
+	pkt_len = plaintext_len + ctx->tx.tag_size;
+	if (version != TLS_1_3_VERSION) {
+		pkt_len += iv_size;
+
+		memcpy(buf + TLS_NONCE_OFFSET,
+		       ctx->tx.iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
+	}
 
 	/* we cover nonce explicit here as well, so buf should be of
 	 * size KTLS_DTLS_HEADER_SIZE + KTLS_DTLS_NONCE_EXPLICIT_SIZE
 	 */
-	buf[0] = record_type;
-	buf[1] = TLS_VERSION_MINOR(ctx->crypto_send.info.version);
-	buf[2] = TLS_VERSION_MAJOR(ctx->crypto_send.info.version);
+	buf[0] = version == TLS_1_3_VERSION ?
+		   TLS_RECORD_TYPE_DATA : record_type;
+	/* Note that VERSION must be TLS_1_2 for both TLS1.2 and TLS1.3 */
+	buf[1] = TLS_1_2_VERSION_MINOR;
+	buf[2] = TLS_1_2_VERSION_MAJOR;
 	/* we can use IV for nonce explicit according to spec */
 	buf[3] = pkt_len >> 8;
 	buf[4] = pkt_len & 0xFF;
-	memcpy(buf + TLS_NONCE_OFFSET,
-	       ctx->tx.iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
 }
 
 static inline void tls_make_aad(char *buf,
 				size_t size,
 				char *record_sequence,
 				int record_sequence_size,
-				unsigned char record_type)
+				unsigned char record_type,
+				int version)
+{
+	if (version != TLS_1_3_VERSION) {
+		memcpy(buf, record_sequence, record_sequence_size);
+		buf += 8;
+	} else {
+		size += TLS_CIPHER_AES_GCM_128_TAG_SIZE;
+	}
+
+	buf[0] = version == TLS_1_3_VERSION ?
+		  TLS_RECORD_TYPE_DATA : record_type;
+	buf[1] = TLS_1_2_VERSION_MAJOR;
+	buf[2] = TLS_1_2_VERSION_MINOR;
+	buf[3] = size >> 8;
+	buf[4] = size & 0xFF;
+}
+
+static inline void xor_iv_with_seq(int version, char *iv, char *seq)
 {
-	memcpy(buf, record_sequence, record_sequence_size);
+	int i;
 
-	buf[8] = record_type;
-	buf[9] = TLS_1_2_VERSION_MAJOR;
-	buf[10] = TLS_1_2_VERSION_MINOR;
-	buf[11] = size >> 8;
-	buf[12] = size & 0xFF;
+	if (version == TLS_1_3_VERSION) {
+		for (i = 0; i < 8; i++)
+			iv[i + 4] ^= seq[i];
+	}
 }
 
 static inline struct tls_context *tls_get_ctx(const struct sock *sk)

commit a2ef9b6a22bd22841bde53e52cc50476fb4d1a5d
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jan 30 21:58:12 2019 +0000

    net: tls: Refactor tls aad space size calculation
    
    TLS 1.3 has a different AAD size, use a variable in the code to
    make TLS 1.3 support easy.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index da616db48413..754b130672f0 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -202,6 +202,7 @@ struct cipher_context {
 	char *iv;
 	u16 rec_seq_size;
 	char *rec_seq;
+	u16 aad_size;
 };
 
 union tls_crypto_context {

commit fb99bce7120014307dde57b3d7def6977a9a62a1
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jan 30 21:58:05 2019 +0000

    net: tls: Support 256 bit keys
    
    Wire up support for 256 bit keys from the setsockopt to the crypto
    framework
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 4592606e136a..da616db48413 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -206,7 +206,10 @@ struct cipher_context {
 
 union tls_crypto_context {
 	struct tls_crypto_info info;
-	struct tls12_crypto_info_aes_gcm_128 aes_gcm_128;
+	union {
+		struct tls12_crypto_info_aes_gcm_128 aes_gcm_128;
+		struct tls12_crypto_info_aes_gcm_256 aes_gcm_256;
+	};
 };
 
 struct tls_context {

commit eaf2a47f40f01ef793d2fc50cf03d199846013ca
Merge: c829f5f52db9 62967898789d
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jan 29 21:18:54 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 32eb67b93c9e3cd62cb423e30b090cdd4aa8d275
Author: Dave Watson <davejwatson@fb.com>
Date:   Sun Jan 27 00:57:38 2019 +0000

    net: tls: Save iv in tls_rec for async crypto requests
    
    aead_request_set_crypt takes an iv pointer, and we change the iv
    soon after setting it.  Some async crypto algorithms don't save the iv,
    so we need to save it in the tls_rec for async requests.
    
    Found by hardcoding x64 aesni to use async crypto manager (to test the async
    codepath), however I don't think this combination can happen in the wild.
    Presumably other hardware offloads will need this fix, but there have been
    no user reports.
    
    Fixes: a42055e8d2c30 ("Add support for async encryption of records...")
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 2a6ac8d642af..1486b60c4de8 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -120,6 +120,8 @@ struct tls_rec {
 	struct scatterlist sg_aead_out[2];
 
 	char aad_space[TLS_AAD_SPACE_SIZE];
+	u8 iv_data[TLS_CIPHER_AES_GCM_128_IV_SIZE +
+		   TLS_CIPHER_AES_GCM_128_SALT_SIZE];
 	struct aead_request aead_req;
 	u8 aead_req_ctx[];
 };

commit 692d7b5d1f9125a1cf0595e979e3b5fb7210547e
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Wed Jan 16 10:40:16 2019 +0000

    tls: Fix recvmsg() to be able to peek across multiple records
    
    This fixes recvmsg() to be able to peek across multiple tls records.
    Without this patch, the tls's selftests test case
    'recv_peek_large_buf_mult_recs' fails. Each tls receive context now
    maintains a 'rx_list' to retain incoming skb carrying tls records. If a
    tls record needs to be retained e.g. for peek case or for the case when
    the buffer passed to recvmsg() has a length smaller than decrypted
    record length, then it is added to 'rx_list'. Additionally, records are
    added in 'rx_list' if the crypto operation runs in async mode. The
    records are dequeued from 'rx_list' after the decrypted data is consumed
    by copying into the buffer passed to recvmsg(). In case, the MSG_PEEK
    flag is used in recvmsg(), then records are not consumed or removed
    from the 'rx_list'.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 2a6ac8d642af..90bf52db573e 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -145,12 +145,13 @@ struct tls_sw_context_tx {
 struct tls_sw_context_rx {
 	struct crypto_aead *aead_recv;
 	struct crypto_wait async_wait;
-
 	struct strparser strp;
+	struct sk_buff_head rx_list;	/* list of decrypted 'data' records */
 	void (*saved_data_ready)(struct sock *sk);
 
 	struct sk_buff *recv_pkt;
 	u8 control;
+	int async_capable;
 	bool decrypted;
 	atomic_t decrypt_pending;
 	bool async_notify;

commit 339bbff2d6e005a5586adeffc3d69a0eea50a764
Merge: e770454fabde 1cf4a0ccc506
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 20 17:31:36 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-12-21
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    There is a merge conflict in test_verifier.c. Result looks as follows:
    
            [...]
            },
            {
                    "calls: cross frame pruning",
                    .insns = {
                    [...]
                    .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
                    .errstr_unpriv = "function calls to other bpf functions are allowed for root only",
                    .result_unpriv = REJECT,
                    .errstr = "!read_ok",
                    .result = REJECT,
            },
            {
                    "jset: functional",
                    .insns = {
            [...]
            {
                    "jset: unknown const compare not taken",
                    .insns = {
                            BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,
                                         BPF_FUNC_get_prandom_u32),
                            BPF_JMP_IMM(BPF_JSET, BPF_REG_0, 1, 1),
                            BPF_LDX_MEM(BPF_B, BPF_REG_8, BPF_REG_9, 0),
                            BPF_EXIT_INSN(),
                    },
                    .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
                    .errstr_unpriv = "!read_ok",
                    .result_unpriv = REJECT,
                    .errstr = "!read_ok",
                    .result = REJECT,
            },
            [...]
            {
                    "jset: range",
                    .insns = {
                    [...]
                    },
                    .prog_type = BPF_PROG_TYPE_SOCKET_FILTER,
                    .result_unpriv = ACCEPT,
                    .result = ACCEPT,
            },
    
    The main changes are:
    
    1) Various BTF related improvements in order to get line info
       working. Meaning, verifier will now annotate the corresponding
       BPF C code to the error log, from Martin and Yonghong.
    
    2) Implement support for raw BPF tracepoints in modules, from Matt.
    
    3) Add several improvements to verifier state logic, namely speeding
       up stacksafe check, optimizations for stack state equivalence
       test and safety checks for liveness analysis, from Alexei.
    
    4) Teach verifier to make use of BPF_JSET instruction, add several
       test cases to kselftests and remove nfp specific JSET optimization
       now that verifier has awareness, from Jakub.
    
    5) Improve BPF verifier's slot_type marking logic in order to
       allow more stack slot sharing, from Jiong.
    
    6) Add sk_msg->size member for context access and add set of fixes
       and improvements to make sock_map with kTLS usable with openssl
       based applications, from John.
    
    7) Several cleanups and documentation updates in bpftool as well as
       auto-mount of tracefs for "bpftool prog tracelog" command,
       from Quentin.
    
    8) Include sub-program tags from now on in bpf_prog_info in order to
       have a reliable way for user space to get all tags of the program
       e.g. needed for kallsyms correlation, from Song.
    
    9) Add BTF annotations for cgroup_local_storage BPF maps and
       implement bpf fs pretty print support, from Roman.
    
    10) Fix bpftool in order to allow for cross-compilation, from Ivan.
    
    11) Update of bpftool license to GPLv2-only + BSD-2-Clause in order
        to be compatible with libbfd and allow for Debian packaging,
        from Jakub.
    
    12) Remove an obsolete prog->aux sanitation in dump and get rid of
        version check for prog load, from Daniel.
    
    13) Fix a memory leak in libbpf's line info handling, from Prashant.
    
    14) Fix cpumap's frame alignment for build_skb() so that skb_shared_info
        does not get unaligned, from Jesper.
    
    15) Fix test_progs kselftest to work with older compilers which are less
        smart in optimizing (and thus throwing build error), from Stanislav.
    
    16) Cleanup and simplify AF_XDP socket teardown, from Bjrn.
    
    17) Fix sk lookup in BPF kselftest's test_sock_addr with regards
        to netns_id argument, from Andrey.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0608c69c9a805c6264689d7eab4203eab88cf1da
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Dec 20 11:35:35 2018 -0800

    bpf: sk_msg, sock{map|hash} redirect through ULP
    
    A sockmap program that redirects through a kTLS ULP enabled socket
    will not work correctly because the ULP layer is skipped. This
    fixes the behavior to call through the ULP layer on redirect to
    ensure any operations required on the data stream at the ULP layer
    continue to be applied.
    
    To do this we add an internal flag MSG_SENDPAGE_NOPOLICY to avoid
    calling the BPF layer on a redirected message. This is
    required to avoid calling the BPF layer multiple times (possibly
    recursively) which is not the current/expected behavior without
    ULPs. In the future we may add a redirect flag if users _do_
    want the policy applied again but this would need to work for both
    ULP and non-ULP sockets and be opt-in to avoid breaking existing
    programs.
    
    Also to avoid polluting the flag space with an internal flag we
    reuse the flag space overlapping MSG_SENDPAGE_NOPOLICY with
    MSG_WAITFORONE. Here WAITFORONE is specific to recv path and
    SENDPAGE_NOPOLICY is only used for sendpage hooks. The last thing
    to verify is user space API is masked correctly to ensure the flag
    can not be set by user. (Note this needs to be true regardless
    because we have internal flags already in-use that user space
    should not be able to set). But for completeness we have two UAPI
    paths into sendpage, sendfile and splice.
    
    In the sendfile case the function do_sendfile() zero's flags,
    
    ./fs/read_write.c:
     static ssize_t do_sendfile(int out_fd, int in_fd, loff_t *ppos,
                                size_t count, loff_t max)
     {
       ...
       fl = 0;
    #if 0
       /*
        * We need to debate whether we can enable this or not. The
        * man page documents EAGAIN return for the output at least,
        * and the application is arguably buggy if it doesn't expect
        * EAGAIN on a non-blocking file descriptor.
        */
        if (in.file->f_flags & O_NONBLOCK)
            fl = SPLICE_F_NONBLOCK;
    #endif
        file_start_write(out.file);
        retval = do_splice_direct(in.file, &pos, out.file, &out_pos, count, fl);
     }
    
    In the splice case the pipe_to_sendpage "actor" is used which
    masks flags with SPLICE_F_MORE.
    
    ./fs/splice.c:
     static int pipe_to_sendpage(struct pipe_inode_info *pipe,
                                struct pipe_buffer *buf, struct splice_desc *sd)
     {
       ...
       more = (sd->flags & SPLICE_F_MORE) ? MSG_MORE : 0;
       ...
     }
    
    Confirming what we expect that internal flags  are in fact internal
    to socket side.
    
    Fixes: d3b18ad31f93 ("tls: add bpf support to sk_msg handling")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index bab5627ff5e3..23601f3b02ee 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -454,6 +454,15 @@ tls_offload_ctx_tx(const struct tls_context *tls_ctx)
 	return (struct tls_offload_context_tx *)tls_ctx->priv_ctx_tx;
 }
 
+static inline bool tls_sw_has_ctx_tx(const struct sock *sk)
+{
+	struct tls_context *ctx = tls_get_ctx(sk);
+
+	if (!ctx)
+		return false;
+	return !!tls_sw_ctx_tx(ctx);
+}
+
 static inline struct tls_offload_context_rx *
 tls_offload_ctx_rx(const struct tls_context *tls_ctx)
 {

commit df9d4a1780223c3ddd4bf0810079b5a75251554e
Author: Atul Gupta <atul.gupta@chelsio.com>
Date:   Tue Dec 11 02:20:09 2018 -0800

    net/tls: sleeping function from invalid context
    
    HW unhash within mutex for registered tls devices cause sleep
    when called from tcp_set_state for TCP_CLOSE. Release lock and
    re-acquire after function call with ref count incr/dec.
    defined kref and fp release for tls_device to ensure device
    is not released outside lock.
    
    BUG: sleeping function called from invalid context at
    kernel/locking/mutex.c:748
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/7
    INFO: lockdep is turned off.
    CPU: 7 PID: 0 Comm: swapper/7 Tainted: G        W  O
    Call Trace:
     <IRQ>
     dump_stack+0x5e/0x8b
     ___might_sleep+0x222/0x260
     __mutex_lock+0x5c/0xa50
     ? vprintk_emit+0x1f3/0x440
     ? kmem_cache_free+0x22d/0x2a0
     ? tls_hw_unhash+0x2f/0x80
     ? printk+0x52/0x6e
     ? tls_hw_unhash+0x2f/0x80
     tls_hw_unhash+0x2f/0x80
     tcp_set_state+0x5f/0x180
     tcp_done+0x2e/0xe0
     tcp_rcv_state_process+0x92c/0xdd3
     ? lock_acquire+0xf5/0x1f0
     ? tcp_v4_rcv+0xa7c/0xbe0
     ? tcp_v4_do_rcv+0x70/0x1e0
    
    Signed-off-by: Atul Gupta <atul.gupta@chelsio.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index bab5627ff5e3..3cbcd12303fd 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -76,6 +76,10 @@
  *
  * void (*unhash)(struct tls_device *device, struct sock *sk);
  *     This function cleans listen state set by Inline TLS driver
+ *
+ * void (*release)(struct kref *kref);
+ *     Release the registered device and allocated resources
+ * @kref: Number of reference to tls_device
  */
 struct tls_device {
 	char name[TLS_DEVICE_NAME_MAX];
@@ -83,6 +87,8 @@ struct tls_device {
 	int  (*feature)(struct tls_device *device);
 	int  (*hash)(struct tls_device *device, struct sock *sk);
 	void (*unhash)(struct tls_device *device, struct sock *sk);
+	void (*release)(struct kref *kref);
+	struct kref kref;
 };
 
 enum {

commit 924ad65ed01ee0eec5d2a3280c01c394343d6df7
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Oct 13 02:46:00 2018 +0200

    tls: replace poll implementation with read hook
    
    Instead of re-implementing poll routine use the poll callback to
    trigger read from kTLS, we reuse the stream_memory_read callback
    which is simpler and achieves the same. This helps to align sockmap
    and kTLS so we can more easily embed BPF in kTLS.
    
    Joint work with Daniel.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3d22d8a59be7..bab5627ff5e3 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -142,8 +142,7 @@ struct tls_sw_context_rx {
 
 	struct strparser strp;
 	void (*saved_data_ready)(struct sock *sk);
-	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
-				struct poll_table_struct *wait);
+
 	struct sk_buff *recv_pkt;
 	u8 control;
 	bool decrypted;
@@ -272,8 +271,7 @@ void tls_sw_free_resources_rx(struct sock *sk);
 void tls_sw_release_resources_rx(struct sock *sk);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
-unsigned int tls_sw_poll(struct file *file, struct socket *sock,
-			 struct poll_table_struct *wait);
+bool tls_sw_stream_read(const struct sock *sk);
 ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
 			   struct pipe_inode_info *pipe,
 			   size_t len, unsigned int flags);

commit d829e9c4112b52f4f00195900fd4c685f61365ab
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:59 2018 +0200

    tls: convert to generic sk_msg interface
    
    Convert kTLS over to make use of sk_msg interface for plaintext and
    encrypted scattergather data, so it reuses all the sk_msg helpers
    and data structure which later on in a second step enables to glue
    this to BPF.
    
    This also allows to remove quite a bit of open coded helpers which
    are covered by the sk_msg API. Recent changes in kTLs 80ece6a03aaf
    ("tls: Remove redundant vars from tls record structure") and
    4e6d47206c32 ("tls: Add support for inplace records encryption")
    changed the data path handling a bit; while we've kept the latter
    optimization intact, we had to undo the former change to better
    fit the sk_msg model, hence the sg_aead_in and sg_aead_out have
    been brought back and are linked into the sk_msg sgs. Now the kTLS
    record contains a msg_plaintext and msg_encrypted sk_msg each.
    
    In the original code, the zerocopy_from_iter() has been used out
    of TX but also RX path. For the strparser skb-based RX path,
    we've left the zerocopy_from_iter() in decrypt_internal() mostly
    untouched, meaning it has been moved into tls_setup_from_iter()
    with charging logic removed (as not used from RX). Given RX path
    is not based on sk_msg objects, we haven't pursued setting up a
    dummy sk_msg to call into sk_msg_zerocopy_from_iter(), but it
    could be an option to prusue in a later step.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/include/net/tls.h b/include/net/tls.h
index 5e853835597e..3d22d8a59be7 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -39,6 +39,8 @@
 #include <linux/crypto.h>
 #include <linux/socket.h>
 #include <linux/tcp.h>
+#include <linux/skmsg.h>
+
 #include <net/tcp.h>
 #include <net/strparser.h>
 #include <crypto/aead.h>
@@ -103,15 +105,13 @@ struct tls_rec {
 	int tx_flags;
 	int inplace_crypto;
 
-	/* AAD | sg_plaintext_data | sg_tag */
-	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS + 1];
-	/* AAD | sg_encrypted_data (data contain overhead for hdr&iv&tag) */
-	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS + 1];
+	struct sk_msg msg_plaintext;
+	struct sk_msg msg_encrypted;
 
-	unsigned int sg_plaintext_size;
-	unsigned int sg_encrypted_size;
-	int sg_plaintext_num_elem;
-	int sg_encrypted_num_elem;
+	/* AAD | msg_plaintext.sg.data | sg_tag */
+	struct scatterlist sg_aead_in[2];
+	/* AAD | msg_encrypted.sg.data (data contains overhead for hdr & iv & tag) */
+	struct scatterlist sg_aead_out[2];
 
 	char aad_space[TLS_AAD_SPACE_SIZE];
 	struct aead_request aead_req;
@@ -223,8 +223,8 @@ struct tls_context {
 
 	unsigned long flags;
 	bool in_tcp_sendpages;
+	bool pending_open_record_frags;
 
-	u16 pending_open_record_frags;
 	int (*push_pending_record)(struct sock *sk, int flags);
 
 	void (*sk_write_space)(struct sock *sk);

commit 4e6d47206c32d1bbb4931f1d851dae3870e0df81
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Sun Sep 30 08:04:35 2018 +0530

    tls: Add support for inplace records encryption
    
    Presently, for non-zero copy case, separate pages are allocated for
    storing plaintext and encrypted text of records. These pages are stored
    in sg_plaintext_data and sg_encrypted_data scatterlists inside record
    structure. Further, sg_plaintext_data & sg_encrypted_data are passed
    to cryptoapis for record encryption. Allocating separate pages for
    plaintext and encrypted text is inefficient from both required memory
    and performance point of view.
    
    This patch adds support of inplace encryption of records. For non-zero
    copy case, we reuse the pages from sg_encrypted_data scatterlist to
    copy the application's plaintext data. For the movement of pages from
    sg_encrypted_data to sg_plaintext_data scatterlists, we introduce a new
    function move_to_plaintext_sg(). This function add pages into
    sg_plaintext_data from sg_encrypted_data scatterlists.
    
    tls_do_encryption() is modified to pass the same scatterlist as both
    source and destination into aead_request_set_crypt() if inplace crypto
    has been enabled. A new ariable 'inplace_crypto' has been introduced in
    record structure to signify whether the same scatterlist can be used.
    By default, the inplace_crypto is enabled in get_rec(). If zero-copy is
    used (i.e. plaintext data is not copied), inplace_crypto is set to '0'.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Reviewed-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 262420cdad10..5e853835597e 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -101,6 +101,7 @@ struct tls_rec {
 	struct list_head list;
 	int tx_ready;
 	int tx_flags;
+	int inplace_crypto;
 
 	/* AAD | sg_plaintext_data | sg_tag */
 	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS + 1];

commit 80ece6a03aaf3f3215475826bdd2bb9f326bccfd
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Wed Sep 26 16:22:08 2018 +0530

    tls: Remove redundant vars from tls record structure
    
    Structure 'tls_rec' contains sg_aead_in and sg_aead_out which point
    to a aad_space and then chain scatterlists sg_plaintext_data,
    sg_encrypted_data respectively. Rather than using chained scatterlists
    for plaintext and encrypted data in aead_req, it is efficient to store
    aad_space in sg_encrypted_data and sg_plaintext_data itself in the
    first index and get rid of sg_aead_in, sg_aead_in and further chaining.
    
    This requires increasing size of sg_encrypted_data & sg_plaintext_data
    arrarys by 1 to accommodate entry for aad_space. The code which uses
    sg_encrypted_data and sg_plaintext_data has been modified to skip first
    index as it points to aad_space.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 1615fb5ea114..262420cdad10 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -101,13 +101,11 @@ struct tls_rec {
 	struct list_head list;
 	int tx_ready;
 	int tx_flags;
-	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS];
-	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS];
 
 	/* AAD | sg_plaintext_data | sg_tag */
-	struct scatterlist sg_aead_in[2];
+	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS + 1];
 	/* AAD | sg_encrypted_data (data contain overhead for hdr&iv&tag) */
-	struct scatterlist sg_aead_out[2];
+	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS + 1];
 
 	unsigned int sg_plaintext_size;
 	unsigned int sg_encrypted_size;

commit 9932a29ab1be1427a2ccbdf852a0f131f2849685
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Mon Sep 24 15:35:56 2018 +0530

    net/tls: Fixed race condition in async encryption
    
    On processors with multi-engine crypto accelerators, it is possible that
    multiple records get encrypted in parallel and their encryption
    completion is notified to different cpus in multicore processor. This
    leads to the situation where tls_encrypt_done() starts executing in
    parallel on different cores. In current implementation, encrypted
    records are queued to tx_ready_list in tls_encrypt_done(). This requires
    addition to linked list 'tx_ready_list' to be protected. As
    tls_decrypt_done() could be executing in irq content, it is not possible
    to protect linked list addition operation using a lock.
    
    To fix the problem, we remove linked list addition operation from the
    irq context. We do tx_ready_list addition/removal operation from
    application context only and get rid of possible multiple access to
    the linked list. Before starting encryption on the record, we add it to
    the tail of tx_ready_list. To prevent tls_tx_records() from transmitting
    it, we mark the record with a new flag 'tx_ready' in 'struct tls_rec'.
    When record encryption gets completed, tls_encrypt_done() has to only
    update the 'tx_ready' flag to true & linked list add operation is not
    required.
    
    The changed logic brings some other side benefits. Since the records
    are always submitted in tls sequence number order for encryption, the
    tx_ready_list always remains sorted and addition of new records to it
    does not have to traverse the linked list.
    
    Lastly, we renamed tx_ready_list in 'struct tls_sw_context_tx' to
    'tx_list'. This is because now, the some of the records at the tail are
    not ready to transmit.
    
    Fixes: a42055e8d2c3 ("net/tls: Add support for async encryption")
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3aa73e2d8823..1615fb5ea114 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -99,6 +99,7 @@ enum {
  */
 struct tls_rec {
 	struct list_head list;
+	int tx_ready;
 	int tx_flags;
 	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS];
 	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS];
@@ -128,7 +129,7 @@ struct tls_sw_context_tx {
 	struct crypto_wait async_wait;
 	struct tx_work tx_work;
 	struct tls_rec *open_rec;
-	struct list_head tx_ready_list;
+	struct list_head tx_list;
 	atomic_t encrypt_pending;
 	int async_notify;
 
@@ -220,7 +221,6 @@ struct tls_context {
 
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
-	u64 tx_seq_number;	/* Next TLS seqnum to be transmitted */
 
 	unsigned long flags;
 	bool in_tcp_sendpages;
@@ -341,21 +341,15 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 	return tls_ctx->pending_open_record_frags;
 }
 
-static inline bool is_tx_ready(struct tls_context *tls_ctx,
-			       struct tls_sw_context_tx *ctx)
+static inline bool is_tx_ready(struct tls_sw_context_tx *ctx)
 {
 	struct tls_rec *rec;
-	u64 seq;
 
-	rec = list_first_entry(&ctx->tx_ready_list, struct tls_rec, list);
+	rec = list_first_entry(&ctx->tx_list, struct tls_rec, list);
 	if (!rec)
 		return false;
 
-	seq = be64_to_cpup((const __be64 *)&rec->aad_space);
-	if (seq == tls_ctx->tx_seq_number)
-		return true;
-	else
-		return false;
+	return READ_ONCE(rec->tx_ready);
 }
 
 struct sk_buff *

commit a42055e8d2c30d4decfc13ce943d09c7b9dad221
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Fri Sep 21 09:46:13 2018 +0530

    net/tls: Add support for async encryption of records for performance
    
    In current implementation, tls records are encrypted & transmitted
    serially. Till the time the previously submitted user data is encrypted,
    the implementation waits and on finish starts transmitting the record.
    This approach of encrypt-one record at a time is inefficient when
    asynchronous crypto accelerators are used. For each record, there are
    overheads of interrupts, driver softIRQ scheduling etc. Also the crypto
    accelerator sits idle most of time while an encrypted record's pages are
    handed over to tcp stack for transmission.
    
    This patch enables encryption of multiple records in parallel when an
    async capable crypto accelerator is present in system. This is achieved
    by allowing the user space application to send more data using sendmsg()
    even while previously issued data is being processed by crypto
    accelerator. This requires returning the control back to user space
    application after submitting encryption request to accelerator. This
    also means that zero-copy mode of encryption cannot be used with async
    accelerator as we must be done with user space application buffer before
    returning from sendmsg().
    
    There can be multiple records in flight to/from the accelerator. Each of
    the record is represented by 'struct tls_rec'. This is used to store the
    memory pages for the record.
    
    After the records are encrypted, they are added in a linked list called
    tx_ready_list which contains encrypted tls records sorted as per tls
    sequence number. The records from tx_ready_list are transmitted using a
    newly introduced function called tls_tx_records(). The tx_ready_list is
    polled for any record ready to be transmitted in sendmsg(), sendpage()
    after initiating encryption of new tls records. This achieves parallel
    encryption and transmission of records when async accelerator is
    present.
    
    There could be situation when crypto accelerator completes encryption
    later than polling of tx_ready_list by sendmsg()/sendpage(). Therefore
    we need a deferred work context to be able to transmit records from
    tx_ready_list. The deferred work context gets scheduled if applications
    are not sending much data through the socket. If the applications issue
    sendmsg()/sendpage() in quick succession, then the scheduling of
    tx_work_handler gets cancelled as the tx_ready_list would be polled from
    application's context itself. This saves scheduling overhead of deferred
    work.
    
    The patch also brings some side benefit. We are able to get rid of the
    concept of CLOSED record. This is because the records once closed are
    either encrypted and then placed into tx_ready_list or if encryption
    fails, the socket error is set. This simplifies the kernel tls
    sendpath. However since tls_device.c is still using macros, accessory
    functions for CLOSED records have been retained.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9f3c4ea9ad6f..3aa73e2d8823 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -41,7 +41,7 @@
 #include <linux/tcp.h>
 #include <net/tcp.h>
 #include <net/strparser.h>
-
+#include <crypto/aead.h>
 #include <uapi/linux/tls.h>
 
 
@@ -93,24 +93,47 @@ enum {
 	TLS_NUM_CONFIG,
 };
 
-struct tls_sw_context_tx {
-	struct crypto_aead *aead_send;
-	struct crypto_wait async_wait;
-
-	char aad_space[TLS_AAD_SPACE_SIZE];
-
-	unsigned int sg_plaintext_size;
-	int sg_plaintext_num_elem;
+/* TLS records are maintained in 'struct tls_rec'. It stores the memory pages
+ * allocated or mapped for each TLS record. After encryption, the records are
+ * stores in a linked list.
+ */
+struct tls_rec {
+	struct list_head list;
+	int tx_flags;
 	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS];
-
-	unsigned int sg_encrypted_size;
-	int sg_encrypted_num_elem;
 	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS];
 
 	/* AAD | sg_plaintext_data | sg_tag */
 	struct scatterlist sg_aead_in[2];
 	/* AAD | sg_encrypted_data (data contain overhead for hdr&iv&tag) */
 	struct scatterlist sg_aead_out[2];
+
+	unsigned int sg_plaintext_size;
+	unsigned int sg_encrypted_size;
+	int sg_plaintext_num_elem;
+	int sg_encrypted_num_elem;
+
+	char aad_space[TLS_AAD_SPACE_SIZE];
+	struct aead_request aead_req;
+	u8 aead_req_ctx[];
+};
+
+struct tx_work {
+	struct delayed_work work;
+	struct sock *sk;
+};
+
+struct tls_sw_context_tx {
+	struct crypto_aead *aead_send;
+	struct crypto_wait async_wait;
+	struct tx_work tx_work;
+	struct tls_rec *open_rec;
+	struct list_head tx_ready_list;
+	atomic_t encrypt_pending;
+	int async_notify;
+
+#define BIT_TX_SCHEDULED	0
+	unsigned long tx_bitmask;
 };
 
 struct tls_sw_context_rx {
@@ -197,6 +220,8 @@ struct tls_context {
 
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
+	u64 tx_seq_number;	/* Next TLS seqnum to be transmitted */
+
 	unsigned long flags;
 	bool in_tcp_sendpages;
 
@@ -261,6 +286,7 @@ int tls_device_sendpage(struct sock *sk, struct page *page,
 void tls_device_sk_destruct(struct sock *sk);
 void tls_device_init(void);
 void tls_device_cleanup(void);
+int tls_tx_records(struct sock *sk, int flags);
 
 struct tls_record_info *tls_get_record(struct tls_offload_context_tx *context,
 				       u32 seq, u64 *p_record_sn);
@@ -279,6 +305,9 @@ void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
 int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 		struct scatterlist *sg, u16 first_offset,
 		int flags);
+int tls_push_partial_record(struct sock *sk, struct tls_context *ctx,
+			    int flags);
+
 int tls_push_pending_closed_record(struct sock *sk, struct tls_context *ctx,
 				   int flags, long *timeo);
 
@@ -312,6 +341,23 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 	return tls_ctx->pending_open_record_frags;
 }
 
+static inline bool is_tx_ready(struct tls_context *tls_ctx,
+			       struct tls_sw_context_tx *ctx)
+{
+	struct tls_rec *rec;
+	u64 seq;
+
+	rec = list_first_entry(&ctx->tx_ready_list, struct tls_rec, list);
+	if (!rec)
+		return false;
+
+	seq = be64_to_cpup((const __be64 *)&rec->aad_space);
+	if (seq == tls_ctx->tx_seq_number)
+		return true;
+	else
+		return false;
+}
+
 struct sk_buff *
 tls_validate_xmit_skb(struct sock *sk, struct net_device *dev,
 		      struct sk_buff *skb);

commit e366fa435032db1ce1538a2c029714666985dd48
Merge: cf7d97e1e54d 5211da9ca526
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 18 09:33:27 2018 -0700

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/davem/net
    
    Two new tls tests added in parallel in both net and net-next.
    
    Used Stephen Rothwell's linux-next resolution.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7a3dd8c8979ce48b99cb0e9b7435a97f0716138a
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Sep 14 13:01:46 2018 -0700

    tls: async support causes out-of-bounds access in crypto APIs
    
    When async support was added it needed to access the sk from the async
    callback to report errors up the stack. The patch tried to use space
    after the aead request struct by directly setting the reqsize field in
    aead_request. This is an internal field that should not be used
    outside the crypto APIs. It is used by the crypto code to define extra
    space for private structures used in the crypto context. Users of the
    API then use crypto_aead_reqsize() and add the returned amount of
    bytes to the end of the request memory allocation before posting the
    request to encrypt/decrypt APIs.
    
    So this breaks (with general protection fault and KASAN error, if
    enabled) because the request sent to decrypt is shorter than required
    causing the crypto API out-of-bounds errors. Also it seems unlikely the
    sk is even valid by the time it gets to the callback because of memset
    in crypto layer.
    
    Anyways, fix this by holding the sk in the skb->sk field when the
    callback is set up and because the skb is already passed through to
    the callback handler via void* we can access it in the handler. Then
    in the handler we need to be careful to NULL the pointer again before
    kfree_skb. I added comments on both the setup (in tls_do_decryption)
    and when we clear it from the crypto callback handler
    tls_decrypt_done(). After this selftests pass again and fixes KASAN
    errors/warnings.
    
    Fixes: 94524d8fc965 ("net/tls: Add support for async decryption of tls records")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Reviewed-by: Vakul Garg <Vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index cd0a65bd92f9..8630d28bd951 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -128,10 +128,6 @@ struct tls_sw_context_rx {
 	bool async_notify;
 };
 
-struct decrypt_req_ctx {
-	struct sock *sk;
-};
-
 struct tls_record_info {
 	struct list_head list;
 	u32 end_seq;

commit 86029d10af18381814881d6cce2dd6872163b59f
Author: Sabrina Dubroca <sd@queasysnail.net>
Date:   Wed Sep 12 17:44:42 2018 +0200

    tls: zero the crypto information from tls_context before freeing
    
    This contains key material in crypto_send_aes_gcm_128 and
    crypto_recv_aes_gcm_128.
    
    Introduce union tls_crypto_context, and replace the two identical
    unions directly embedded in struct tls_context with it. We can then
    use this union to clean up the memory in the new tls_ctx_free()
    function.
    
    Fixes: 3c4d7559159b ("tls: kernel TLS support")
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index d5c683e8bb22..0a769cf2f5f3 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -171,15 +171,14 @@ struct cipher_context {
 	char *rec_seq;
 };
 
+union tls_crypto_context {
+	struct tls_crypto_info info;
+	struct tls12_crypto_info_aes_gcm_128 aes_gcm_128;
+};
+
 struct tls_context {
-	union {
-		struct tls_crypto_info crypto_send;
-		struct tls12_crypto_info_aes_gcm_128 crypto_send_aes_gcm_128;
-	};
-	union {
-		struct tls_crypto_info crypto_recv;
-		struct tls12_crypto_info_aes_gcm_128 crypto_recv_aes_gcm_128;
-	};
+	union tls_crypto_context crypto_send;
+	union tls_crypto_context crypto_recv;
 
 	struct list_head list;
 	struct net_device *netdev;
@@ -367,8 +366,8 @@ static inline void tls_fill_prepend(struct tls_context *ctx,
 	 * size KTLS_DTLS_HEADER_SIZE + KTLS_DTLS_NONCE_EXPLICIT_SIZE
 	 */
 	buf[0] = record_type;
-	buf[1] = TLS_VERSION_MINOR(ctx->crypto_send.version);
-	buf[2] = TLS_VERSION_MAJOR(ctx->crypto_send.version);
+	buf[1] = TLS_VERSION_MINOR(ctx->crypto_send.info.version);
+	buf[2] = TLS_VERSION_MAJOR(ctx->crypto_send.info.version);
 	/* we can use IV for nonce explicit according to spec */
 	buf[3] = pkt_len >> 8;
 	buf[4] = pkt_len & 0xFF;

commit 94524d8fc965a7a0facdef6d1b01d5ef6d71a802
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Wed Aug 29 15:26:55 2018 +0530

    net/tls: Add support for async decryption of tls records
    
    When tls records are decrypted using asynchronous acclerators such as
    NXP CAAM engine, the crypto apis return -EINPROGRESS. Presently, on
    getting -EINPROGRESS, the tls record processing stops till the time the
    crypto accelerator finishes off and returns the result. This incurs a
    context switch and is not an efficient way of accessing the crypto
    accelerators. Crypto accelerators work efficient when they are queued
    with multiple crypto jobs without having to wait for the previous ones
    to complete.
    
    The patch submits multiple crypto requests without having to wait for
    for previous ones to complete. This has been implemented for records
    which are decrypted in zero-copy mode. At the end of recvmsg(), we wait
    for all the asynchronous decryption requests to complete.
    
    The references to records which have been sent for async decryption are
    dropped. For cases where record decryption is not possible in zero-copy
    mode, asynchronous decryption is not used and we wait for decryption
    crypto api to complete.
    
    For crypto requests executing in async fashion, the memory for
    aead_request, sglists and skb etc is freed from the decryption
    completion handler. The decryption completion handler wakesup the
    sleeping user context when recvmsg() flags that it has done sending
    all the decryption requests and there are no more decryption requests
    pending to be completed.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Reviewed-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index d5c683e8bb22..cd0a65bd92f9 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -124,6 +124,12 @@ struct tls_sw_context_rx {
 	struct sk_buff *recv_pkt;
 	u8 control;
 	bool decrypted;
+	atomic_t decrypt_pending;
+	bool async_notify;
+};
+
+struct decrypt_req_ctx {
+	struct sock *sk;
 };
 
 struct tls_record_info {

commit 0b243d004ea640875115d1500ec429a3e9f9fae9
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Fri Aug 10 20:46:41 2018 +0530

    net/tls: Combined memory allocation for decryption request
    
    For preparing decryption request, several memory chunks are required
    (aead_req, sgin, sgout, iv, aad). For submitting the decrypt request to
    an accelerator, it is required that the buffers which are read by the
    accelerator must be dma-able and not come from stack. The buffers for
    aad and iv can be separately kmalloced each, but it is inefficient.
    This patch does a combined allocation for preparing decryption request
    and then segments into aead_req || sgin || sgout || iv || aad.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index d8b3b6578c01..d5c683e8bb22 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -124,10 +124,6 @@ struct tls_sw_context_rx {
 	struct sk_buff *recv_pkt;
 	u8 control;
 	bool decrypted;
-
-	char rx_aad_ciphertext[TLS_AAD_SPACE_SIZE];
-	char rx_aad_plaintext[TLS_AAD_SPACE_SIZE];
-
 };
 
 struct tls_record_info {

commit 4799ac81e52a72a6404827bf2738337bb581a174
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Fri Jul 13 14:33:43 2018 +0300

    tls: Add rx inline crypto offload
    
    This patch completes the generic infrastructure to offload TLS crypto to a
    network device. It enables the kernel to skip decryption and
    authentication of some skbs marked as decrypted by the NIC. In the fast
    path, all packets received are decrypted by the NIC and the performance
    is comparable to plain TCP.
    
    This infrastructure doesn't require a TCP offload engine. Instead, the
    NIC only decrypts packets that contain the expected TCP sequence number.
    Out-Of-Order TCP packets are provided unmodified. As a result, at the
    worst case a received TLS record consists of both plaintext and ciphertext
    packets. These partially decrypted records must be reencrypted,
    only to be decrypted.
    
    The notable differences between SW KTLS Rx and this offload are as
    follows:
    1. Partial decryption - Software must handle the case of a TLS record
    that was only partially decrypted by HW. This can happen due to packet
    reordering.
    2. Resynchronization - tls_read_size calls the device driver to
    resynchronize HW after HW lost track of TLS record framing in
    the TCP stream.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 7a485de25646..d8b3b6578c01 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -83,6 +83,16 @@ struct tls_device {
 	void (*unhash)(struct tls_device *device, struct sock *sk);
 };
 
+enum {
+	TLS_BASE,
+	TLS_SW,
+#ifdef CONFIG_TLS_DEVICE
+	TLS_HW,
+#endif
+	TLS_HW_RECORD,
+	TLS_NUM_CONFIG,
+};
+
 struct tls_sw_context_tx {
 	struct crypto_aead *aead_send;
 	struct crypto_wait async_wait;
@@ -197,6 +207,7 @@ struct tls_context {
 	int (*push_pending_record)(struct sock *sk, int flags);
 
 	void (*sk_write_space)(struct sock *sk);
+	void (*sk_destruct)(struct sock *sk);
 	void (*sk_proto_close)(struct sock *sk, long timeout);
 
 	int  (*setsockopt)(struct sock *sk, int level,
@@ -209,13 +220,27 @@ struct tls_context {
 	void (*unhash)(struct sock *sk);
 };
 
+struct tls_offload_context_rx {
+	/* sw must be the first member of tls_offload_context_rx */
+	struct tls_sw_context_rx sw;
+	atomic64_t resync_req;
+	u8 driver_state[];
+	/* The TLS layer reserves room for driver specific state
+	 * Currently the belief is that there is not enough
+	 * driver specific state to justify another layer of indirection
+	 */
+};
+
+#define TLS_OFFLOAD_CONTEXT_SIZE_RX					\
+	(ALIGN(sizeof(struct tls_offload_context_rx), sizeof(void *)) + \
+	 TLS_DRIVER_STATE_SIZE)
+
 int wait_on_pending_writer(struct sock *sk, long *timeo);
 int tls_sk_query(struct sock *sk, int optname, char __user *optval,
 		int __user *optlen);
 int tls_sk_attach(struct sock *sk, int optname, char __user *optval,
 		  unsigned int optlen);
 
-
 int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
@@ -290,11 +315,19 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 	return tls_ctx->pending_open_record_frags;
 }
 
+struct sk_buff *
+tls_validate_xmit_skb(struct sock *sk, struct net_device *dev,
+		      struct sk_buff *skb);
+
 static inline bool tls_is_sk_tx_device_offloaded(struct sock *sk)
 {
-	return sk_fullsock(sk) &&
-	       /* matches smp_store_release in tls_set_device_offload */
-	       smp_load_acquire(&sk->sk_destruct) == &tls_device_sk_destruct;
+#ifdef CONFIG_SOCK_VALIDATE_XMIT
+	return sk_fullsock(sk) &
+	       (smp_load_acquire(&sk->sk_validate_xmit_skb) ==
+	       &tls_validate_xmit_skb);
+#else
+	return false;
+#endif
 }
 
 static inline void tls_err_abort(struct sock *sk, int err)
@@ -387,10 +420,27 @@ tls_offload_ctx_tx(const struct tls_context *tls_ctx)
 	return (struct tls_offload_context_tx *)tls_ctx->priv_ctx_tx;
 }
 
+static inline struct tls_offload_context_rx *
+tls_offload_ctx_rx(const struct tls_context *tls_ctx)
+{
+	return (struct tls_offload_context_rx *)tls_ctx->priv_ctx_rx;
+}
+
+/* The TLS context is valid until sk_destruct is called */
+static inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)
+{
+	struct tls_context *tls_ctx = tls_get_ctx(sk);
+	struct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);
+
+	atomic64_set(&rx_ctx->resync_req, ((((uint64_t)seq) << 32) | 1));
+}
+
+
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
 void tls_register_device(struct tls_device *device);
 void tls_unregister_device(struct tls_device *device);
+int tls_device_decrypted(struct sock *sk, struct sk_buff *skb);
 int decrypt_skb(struct sock *sk, struct sk_buff *skb,
 		struct scatterlist *sgout);
 
@@ -402,4 +452,9 @@ int tls_sw_fallback_init(struct sock *sk,
 			 struct tls_offload_context_tx *offload_ctx,
 			 struct tls_crypto_info *crypto_info);
 
+int tls_set_device_offload_rx(struct sock *sk, struct tls_context *ctx);
+
+void tls_device_offload_cleanup_rx(struct sock *sk);
+void handle_device_resync(struct sock *sk, u32 seq, u64 rcd_sn);
+
 #endif /* _TLS_OFFLOAD_H */

commit 39f56e1a78d647316db330c3b6f4c5637a895e3b
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Fri Jul 13 14:33:41 2018 +0300

    tls: Split tls_sw_release_resources_rx
    
    This patch splits tls_sw_release_resources_rx into two functions one
    which releases all inner software tls structures and another that also
    frees the containing structure.
    
    In TLS_DEVICE we will need to release the software structures without
    freeeing the containing structure, which contains other information.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 49b89221db43..7a485de25646 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -223,6 +223,7 @@ int tls_sw_sendpage(struct sock *sk, struct page *page,
 void tls_sw_close(struct sock *sk, long timeout);
 void tls_sw_free_resources_tx(struct sock *sk);
 void tls_sw_free_resources_rx(struct sock *sk);
+void tls_sw_release_resources_rx(struct sock *sk);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
 unsigned int tls_sw_poll(struct file *file, struct socket *sock,

commit dafb67f3bb4a58a45fe92c1e362ea6429831688a
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Fri Jul 13 14:33:40 2018 +0300

    tls: Split decrypt_skb to two functions
    
    Previously, decrypt_skb also updated the TLS context.
    Now, decrypt_skb only decrypts the payload using the current context,
    while decrypt_skb_update also updates the state.
    
    Later, in the tls_device Rx flow, we will use decrypt_skb directly.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 5dcd808236a7..49b89221db43 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -390,6 +390,8 @@ int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
 void tls_register_device(struct tls_device *device);
 void tls_unregister_device(struct tls_device *device);
+int decrypt_skb(struct sock *sk, struct sk_buff *skb,
+		struct scatterlist *sgout);
 
 struct sk_buff *tls_validate_xmit_skb(struct sock *sk,
 				      struct net_device *dev,

commit d80a1b9d186057ddb0d384ba601cf2b7d214539c
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Fri Jul 13 14:33:39 2018 +0300

    tls: Refactor tls_offload variable names
    
    For symmetry, we rename tls_offload_context to
    tls_offload_context_tx before we add tls_offload_context_rx.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 70c273777fe9..5dcd808236a7 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -128,7 +128,7 @@ struct tls_record_info {
 	skb_frag_t frags[MAX_SKB_FRAGS];
 };
 
-struct tls_offload_context {
+struct tls_offload_context_tx {
 	struct crypto_aead *aead_send;
 	spinlock_t lock;	/* protects records list */
 	struct list_head records_list;
@@ -147,8 +147,8 @@ struct tls_offload_context {
 #define TLS_DRIVER_STATE_SIZE (max_t(size_t, 8, sizeof(void *)))
 };
 
-#define TLS_OFFLOAD_CONTEXT_SIZE                                               \
-	(ALIGN(sizeof(struct tls_offload_context), sizeof(void *)) +           \
+#define TLS_OFFLOAD_CONTEXT_SIZE_TX                                            \
+	(ALIGN(sizeof(struct tls_offload_context_tx), sizeof(void *)) +        \
 	 TLS_DRIVER_STATE_SIZE)
 
 enum {
@@ -239,7 +239,7 @@ void tls_device_sk_destruct(struct sock *sk);
 void tls_device_init(void);
 void tls_device_cleanup(void);
 
-struct tls_record_info *tls_get_record(struct tls_offload_context *context,
+struct tls_record_info *tls_get_record(struct tls_offload_context_tx *context,
 				       u32 seq, u64 *p_record_sn);
 
 static inline bool tls_record_is_start_marker(struct tls_record_info *rec)
@@ -380,10 +380,10 @@ static inline struct tls_sw_context_tx *tls_sw_ctx_tx(
 	return (struct tls_sw_context_tx *)tls_ctx->priv_ctx_tx;
 }
 
-static inline struct tls_offload_context *tls_offload_ctx(
-		const struct tls_context *tls_ctx)
+static inline struct tls_offload_context_tx *
+tls_offload_ctx_tx(const struct tls_context *tls_ctx)
 {
-	return (struct tls_offload_context *)tls_ctx->priv_ctx_tx;
+	return (struct tls_offload_context_tx *)tls_ctx->priv_ctx_tx;
 }
 
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
@@ -396,7 +396,7 @@ struct sk_buff *tls_validate_xmit_skb(struct sock *sk,
 				      struct sk_buff *skb);
 
 int tls_sw_fallback_init(struct sock *sk,
-			 struct tls_offload_context *offload_ctx,
+			 struct tls_offload_context_tx *offload_ctx,
 			 struct tls_crypto_info *crypto_info);
 
 #endif /* _TLS_OFFLOAD_H */

commit a11e1d432b51f63ba698d044441284a661f01144
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 28 09:43:44 2018 -0700

    Revert changes to convert to ->poll_mask() and aio IOCB_CMD_POLL
    
    The poll() changes were not well thought out, and completely
    unexplained.  They also caused a huge performance regression, because
    "->poll()" was no longer a trivial file operation that just called down
    to the underlying file operations, but instead did at least two indirect
    calls.
    
    Indirect calls are sadly slow now with the Spectre mitigation, but the
    performance problem could at least be largely mitigated by changing the
    "->get_poll_head()" operation to just have a per-file-descriptor pointer
    to the poll head instead.  That gets rid of one of the new indirections.
    
    But that doesn't fix the new complexity that is completely unwarranted
    for the regular case.  The (undocumented) reason for the poll() changes
    was some alleged AIO poll race fixing, but we don't make the common case
    slower and more complex for some uncommon special case, so this all
    really needs way more explanations and most likely a fundamental
    redesign.
    
    [ This revert is a revert of about 30 different commits, not reverted
      individually because that would just be unnecessarily messy  - Linus ]
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/net/tls.h b/include/net/tls.h
index 7f84ea3e217c..70c273777fe9 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -109,7 +109,8 @@ struct tls_sw_context_rx {
 
 	struct strparser strp;
 	void (*saved_data_ready)(struct sock *sk);
-	__poll_t (*sk_poll_mask)(struct socket *sock, __poll_t events);
+	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
+				struct poll_table_struct *wait);
 	struct sk_buff *recv_pkt;
 	u8 control;
 	bool decrypted;
@@ -224,7 +225,8 @@ void tls_sw_free_resources_tx(struct sock *sk);
 void tls_sw_free_resources_rx(struct sock *sk);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
-__poll_t tls_sw_poll_mask(struct socket *sock, __poll_t events);
+unsigned int tls_sw_poll(struct file *file, struct socket *sock,
+			 struct poll_table_struct *wait);
 ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
 			   struct pipe_inode_info *pipe,
 			   size_t len, unsigned int flags);

commit f6fadff33e8b09373eedf99822b89d9dd84545b8
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Mon Jun 11 23:22:04 2018 +0200

    tls: fix NULL pointer dereference on poll
    
    While hacking on kTLS, I ran into the following panic from an
    unprivileged netserver / netperf TCP session:
    
      BUG: unable to handle kernel NULL pointer dereference at 0000000000000000
      PGD 800000037f378067 P4D 800000037f378067 PUD 3c0e61067 PMD 0
      Oops: 0010 [#1] SMP KASAN PTI
      CPU: 1 PID: 2289 Comm: netserver Not tainted 4.17.0+ #139
      Hardware name: LENOVO 20FBCTO1WW/20FBCTO1WW, BIOS N1FET47W (1.21 ) 11/28/2016
      RIP: 0010:          (null)
      Code: Bad RIP value.
      RSP: 0018:ffff88036abcf740 EFLAGS: 00010246
      RAX: dffffc0000000000 RBX: ffff88036f5f6800 RCX: 1ffff1006debed26
      RDX: ffff88036abcf920 RSI: ffff8803cb1a4f00 RDI: ffff8803c258c280
      RBP: ffff8803c258c280 R08: ffff8803c258c280 R09: ffffed006f559d48
      R10: ffff88037aacea43 R11: ffffed006f559d49 R12: ffff8803c258c280
      R13: ffff8803cb1a4f20 R14: 00000000000000db R15: ffffffffc168a350
      FS:  00007f7e631f4700(0000) GS:ffff8803d1c80000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: ffffffffffffffd6 CR3: 00000003ccf64005 CR4: 00000000003606e0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      Call Trace:
       ? tls_sw_poll+0xa4/0x160 [tls]
       ? sock_poll+0x20a/0x680
       ? do_select+0x77b/0x11a0
       ? poll_schedule_timeout.constprop.12+0x130/0x130
       ? pick_link+0xb00/0xb00
       ? read_word_at_a_time+0x13/0x20
       ? vfs_poll+0x270/0x270
       ? deref_stack_reg+0xad/0xe0
       ? __read_once_size_nocheck.constprop.6+0x10/0x10
      [...]
    
    Debugging further, it turns out that calling into ctx->sk_poll() is
    invalid since sk_poll itself is NULL which was saved from the original
    TCP socket in order for tls_sw_poll() to invoke it.
    
    Looks like the recent conversion from poll to poll_mask callback started
    in 152524231023 ("net: add support for ->poll_mask in proto_ops") missed
    to eventually convert kTLS, too: TCP's ->poll was converted over to the
    ->poll_mask in commit 2c7d3dacebd4 ("net/tcp: convert to ->poll_mask")
    and therefore kTLS wrongly saved the ->poll old one which is now NULL.
    
    Convert kTLS over to use ->poll_mask instead. Also instead of POLLIN |
    POLLRDNORM use the proper EPOLLIN | EPOLLRDNORM bits as the case in
    tcp_poll_mask() as well that is mangled here.
    
    Fixes: 2c7d3dacebd4 ("net/tcp: convert to ->poll_mask")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Watson <davejwatson@fb.com>
    Tested-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 70c273777fe9..7f84ea3e217c 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -109,8 +109,7 @@ struct tls_sw_context_rx {
 
 	struct strparser strp;
 	void (*saved_data_ready)(struct sock *sk);
-	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
-				struct poll_table_struct *wait);
+	__poll_t (*sk_poll_mask)(struct socket *sock, __poll_t events);
 	struct sk_buff *recv_pkt;
 	u8 control;
 	bool decrypted;
@@ -225,8 +224,7 @@ void tls_sw_free_resources_tx(struct sock *sk);
 void tls_sw_free_resources_rx(struct sock *sk);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
-unsigned int tls_sw_poll(struct file *file, struct socket *sock,
-			 struct poll_table_struct *wait);
+__poll_t tls_sw_poll_mask(struct socket *sock, __poll_t events);
 ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
 			   struct pipe_inode_info *pipe,
 			   size_t len, unsigned int flags);

commit 6f6e434aa267a6030477876d89444fe3a6b7a48d
Merge: 44c752fe584d 6741c4bb389d
Author: David S. Miller <davem@davemloft.net>
Date:   Mon May 21 16:01:54 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    S390 bpf_jit.S is removed in net-next and had changes in 'net',
    since that code isn't used any more take the removal.
    
    TLS data structures split the TX and RX components in 'net-next',
    put the new struct members from the bug fix in 'net' into the RX
    part.
    
    The 'net-next' tree had some reworking of how the ERSPAN code works in
    the GRE tunneling code, overlapping with a one-line headroom
    calculation fix in 'net'.
    
    Overlapping changes in __sock_map_ctx_update_elem(), keep the bits
    that read the prog members via READ_ONCE() into local variables
    before using them.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8ab6ffba14a466c7298cb3fd5066d774d2977ad1
Author: Matt Mullins <mmullins@fb.com>
Date:   Wed May 16 10:48:40 2018 -0700

    tls: don't use stack memory in a scatterlist
    
    scatterlist code expects virt_to_page() to work, which fails with
    CONFIG_VMAP_STACK=y.
    
    Fixes: c46234ebb4d1e ("tls: RX path for ktls")
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index b400d0bb7448..f5fb16da3860 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -97,6 +97,9 @@ struct tls_sw_context {
 	u8 control;
 	bool decrypted;
 
+	char rx_aad_ciphertext[TLS_AAD_SPACE_SIZE];
+	char rx_aad_plaintext[TLS_AAD_SPACE_SIZE];
+
 	/* Sending context */
 	char aad_space[TLS_AAD_SPACE_SIZE];
 

commit a7b15ab887e5b8e9803136b5a4a0008d7a3dea86
Merge: b05f03b232ab 150426981426
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 4 09:58:56 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Overlapping changes in selftests Makefile.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c212d2c7fc4736d49be102fb7a1a545cdc2f1fea
Author: Dave Watson <davejwatson@fb.com>
Date:   Tue May 1 13:05:39 2018 -0700

    net/tls: Don't recursively call push_record during tls_write_space callbacks
    
    It is reported that in some cases, write_space may be called in
    do_tcp_sendpages, such that we recursively invoke do_tcp_sendpages again:
    
    [  660.468802]  ? do_tcp_sendpages+0x8d/0x580
    [  660.468826]  ? tls_push_sg+0x74/0x130 [tls]
    [  660.468852]  ? tls_push_record+0x24a/0x390 [tls]
    [  660.468880]  ? tls_write_space+0x6a/0x80 [tls]
    ...
    
    tls_push_sg already does a loop over all sending sg's, so ignore
    any tls_write_space notifications until we are done sending.
    We then have to call the previous write_space to wake up
    poll() waiters after we are done with the send loop.
    
    Reported-by: Andre Tomt <andre@tomt.net>
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3da8e13a6d96..b400d0bb7448 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -148,6 +148,7 @@ struct tls_context {
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
 	unsigned long flags;
+	bool in_tcp_sendpages;
 
 	u16 pending_open_record_frags;
 	int (*push_pending_record)(struct sock *sk, int flags);

commit e8f69799810c32dd40c6724d829eccc70baad07f
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:16 2018 +0300

    net/tls: Add generic NIC offload infrastructure
    
    This patch adds a generic infrastructure to offload TLS crypto to a
    network device. It enables the kernel TLS socket to skip encryption
    and authentication operations on the transmit side of the data path.
    Leaving those computationally expensive operations to the NIC.
    
    The NIC offload infrastructure builds TLS records and pushes them to
    the TCP layer just like the SW KTLS implementation and using the same
    API.
    TCP segmentation is mostly unaffected. Currently the only exception is
    that we prevent mixed SKBs where only part of the payload requires
    offload. In the future we are likely to add a similar restriction
    following a change cipher spec record.
    
    The notable differences between SW KTLS and NIC offloaded TLS
    implementations are as follows:
    1. The offloaded implementation builds "plaintext TLS record", those
    records contain plaintext instead of ciphertext and place holder bytes
    instead of authentication tags.
    2. The offloaded implementation maintains a mapping from TCP sequence
    number to TLS records. Thus given a TCP SKB sent from a NIC offloaded
    TLS socket, we can use the tls NIC offload infrastructure to obtain
    enough context to encrypt the payload of the SKB.
    A TLS record is released when the last byte of the record is ack'ed,
    this is done through the new icsk_clean_acked callback.
    
    The infrastructure should be extendable to support various NIC offload
    implementations.  However it is currently written with the
    implementation below in mind:
    The NIC assumes that packets from each offloaded stream are sent as
    plaintext and in-order. It keeps track of the TLS records in the TCP
    stream. When a packet marked for offload is transmitted, the NIC
    encrypts the payload in-place and puts authentication tags in the
    relevant place holders.
    
    The responsibility for handling out-of-order packets (i.e. TCP
    retransmission, qdisc drops) falls on the netdev driver.
    
    The netdev driver keeps track of the expected TCP SN from the NIC's
    perspective.  If the next packet to transmit matches the expected TCP
    SN, the driver advances the expected TCP SN, and transmits the packet
    with TLS offload indication.
    
    If the next packet to transmit does not match the expected TCP SN. The
    driver calls the TLS layer to obtain the TLS record that includes the
    TCP of the packet for transmission. Using this TLS record, the driver
    posts a work entry on the transmit queue to reconstruct the NIC TLS
    state required for the offload of the out-of-order packet. It updates
    the expected TCP SN accordingly and transmits the now in-order packet.
    The same queue is used for packet transmission and TLS context
    reconstruction to avoid the need for flushing the transmit queue before
    issuing the context reconstruction request.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 95a8c60b36be..8c56809eb384 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -116,6 +116,37 @@ struct tls_sw_context_rx {
 	bool decrypted;
 };
 
+struct tls_record_info {
+	struct list_head list;
+	u32 end_seq;
+	int len;
+	int num_frags;
+	skb_frag_t frags[MAX_SKB_FRAGS];
+};
+
+struct tls_offload_context {
+	struct crypto_aead *aead_send;
+	spinlock_t lock;	/* protects records list */
+	struct list_head records_list;
+	struct tls_record_info *open_record;
+	struct tls_record_info *retransmit_hint;
+	u64 hint_record_sn;
+	u64 unacked_record_sn;
+
+	struct scatterlist sg_tx_data[MAX_SKB_FRAGS];
+	void (*sk_destruct)(struct sock *sk);
+	u8 driver_state[];
+	/* The TLS layer reserves room for driver specific state
+	 * Currently the belief is that there is not enough
+	 * driver specific state to justify another layer of indirection
+	 */
+#define TLS_DRIVER_STATE_SIZE (max_t(size_t, 8, sizeof(void *)))
+};
+
+#define TLS_OFFLOAD_CONTEXT_SIZE                                               \
+	(ALIGN(sizeof(struct tls_offload_context), sizeof(void *)) +           \
+	 TLS_DRIVER_STATE_SIZE)
+
 enum {
 	TLS_PENDING_CLOSED_RECORD
 };
@@ -195,9 +226,28 @@ ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
 			   struct pipe_inode_info *pipe,
 			   size_t len, unsigned int flags);
 
-void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
-void tls_icsk_clean_acked(struct sock *sk);
+int tls_set_device_offload(struct sock *sk, struct tls_context *ctx);
+int tls_device_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
+int tls_device_sendpage(struct sock *sk, struct page *page,
+			int offset, size_t size, int flags);
+void tls_device_sk_destruct(struct sock *sk);
+void tls_device_init(void);
+void tls_device_cleanup(void);
+
+struct tls_record_info *tls_get_record(struct tls_offload_context *context,
+				       u32 seq, u64 *p_record_sn);
+
+static inline bool tls_record_is_start_marker(struct tls_record_info *rec)
+{
+	return rec->len == 0;
+}
+
+static inline u32 tls_record_start_seq(struct tls_record_info *rec)
+{
+	return rec->end_seq - rec->len;
+}
 
+void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
 int tls_push_sg(struct sock *sk, struct tls_context *ctx,
 		struct scatterlist *sg, u16 first_offset,
 		int flags);
@@ -234,6 +284,13 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 	return tls_ctx->pending_open_record_frags;
 }
 
+static inline bool tls_is_sk_tx_device_offloaded(struct sock *sk)
+{
+	return sk_fullsock(sk) &&
+	       /* matches smp_store_release in tls_set_device_offload */
+	       smp_load_acquire(&sk->sk_destruct) == &tls_device_sk_destruct;
+}
+
 static inline void tls_err_abort(struct sock *sk, int err)
 {
 	sk->sk_err = err;
@@ -329,4 +386,12 @@ int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 void tls_register_device(struct tls_device *device);
 void tls_unregister_device(struct tls_device *device);
 
+struct sk_buff *tls_validate_xmit_skb(struct sock *sk,
+				      struct net_device *dev,
+				      struct sk_buff *skb);
+
+int tls_sw_fallback_init(struct sock *sk,
+			 struct tls_offload_context *offload_ctx,
+			 struct tls_crypto_info *crypto_info);
+
 #endif /* _TLS_OFFLOAD_H */

commit f66de3ee2c161fcc2d66974e9671f8a2a471ab20
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Mon Apr 30 10:16:15 2018 +0300

    net/tls: Split conf to rx + tx
    
    In TLS inline crypto, we can have one direction in software
    and another in hardware. Thus, we split the TLS configuration to separate
    structures for receive and transmit.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 3da8e13a6d96..95a8c60b36be 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -83,21 +83,10 @@ struct tls_device {
 	void (*unhash)(struct tls_device *device, struct sock *sk);
 };
 
-struct tls_sw_context {
+struct tls_sw_context_tx {
 	struct crypto_aead *aead_send;
-	struct crypto_aead *aead_recv;
 	struct crypto_wait async_wait;
 
-	/* Receive context */
-	struct strparser strp;
-	void (*saved_data_ready)(struct sock *sk);
-	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
-				struct poll_table_struct *wait);
-	struct sk_buff *recv_pkt;
-	u8 control;
-	bool decrypted;
-
-	/* Sending context */
 	char aad_space[TLS_AAD_SPACE_SIZE];
 
 	unsigned int sg_plaintext_size;
@@ -114,6 +103,19 @@ struct tls_sw_context {
 	struct scatterlist sg_aead_out[2];
 };
 
+struct tls_sw_context_rx {
+	struct crypto_aead *aead_recv;
+	struct crypto_wait async_wait;
+
+	struct strparser strp;
+	void (*saved_data_ready)(struct sock *sk);
+	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
+				struct poll_table_struct *wait);
+	struct sk_buff *recv_pkt;
+	u8 control;
+	bool decrypted;
+};
+
 enum {
 	TLS_PENDING_CLOSED_RECORD
 };
@@ -138,9 +140,15 @@ struct tls_context {
 		struct tls12_crypto_info_aes_gcm_128 crypto_recv_aes_gcm_128;
 	};
 
-	void *priv_ctx;
+	struct list_head list;
+	struct net_device *netdev;
+	refcount_t refcount;
+
+	void *priv_ctx_tx;
+	void *priv_ctx_rx;
 
-	u8 conf:3;
+	u8 tx_conf:3;
+	u8 rx_conf:3;
 
 	struct cipher_context tx;
 	struct cipher_context rx;
@@ -177,7 +185,8 @@ int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_close(struct sock *sk, long timeout);
-void tls_sw_free_resources(struct sock *sk);
+void tls_sw_free_resources_tx(struct sock *sk);
+void tls_sw_free_resources_rx(struct sock *sk);
 int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		   int nonblock, int flags, int *addr_len);
 unsigned int tls_sw_poll(struct file *file, struct socket *sock,
@@ -297,16 +306,22 @@ static inline struct tls_context *tls_get_ctx(const struct sock *sk)
 	return icsk->icsk_ulp_data;
 }
 
-static inline struct tls_sw_context *tls_sw_ctx(
+static inline struct tls_sw_context_rx *tls_sw_ctx_rx(
+		const struct tls_context *tls_ctx)
+{
+	return (struct tls_sw_context_rx *)tls_ctx->priv_ctx_rx;
+}
+
+static inline struct tls_sw_context_tx *tls_sw_ctx_tx(
 		const struct tls_context *tls_ctx)
 {
-	return (struct tls_sw_context *)tls_ctx->priv_ctx;
+	return (struct tls_sw_context_tx *)tls_ctx->priv_ctx_tx;
 }
 
 static inline struct tls_offload_context *tls_offload_ctx(
 		const struct tls_context *tls_ctx)
 {
-	return (struct tls_offload_context *)tls_ctx->priv_ctx;
+	return (struct tls_offload_context *)tls_ctx->priv_ctx_tx;
 }
 
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,

commit dd0bed1665d6ca17efd747a90a0bb804b4bf2005
Author: Atul Gupta <atul.gupta@chelsio.com>
Date:   Sat Mar 31 21:41:52 2018 +0530

    tls: support for Inline tls record
    
    Facility to register Inline TLS drivers to net/tls. Setup
    TLS_HW_RECORD prot to listen on offload device.
    
    Cases handled
    - Inline TLS device exists, setup prot for TLS_HW_RECORD
    - Atleast one Inline TLS exists, sets TLS_HW_RECORD.
    - If non-inline device establish connection, move to TLS_SW_TX
    
    Signed-off-by: Atul Gupta <atul.gupta@chelsio.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 437a746300bf..3da8e13a6d96 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -56,6 +56,32 @@
 #define TLS_RECORD_TYPE_DATA		0x17
 
 #define TLS_AAD_SPACE_SIZE		13
+#define TLS_DEVICE_NAME_MAX		32
+
+/*
+ * This structure defines the routines for Inline TLS driver.
+ * The following routines are optional and filled with a
+ * null pointer if not defined.
+ *
+ * @name: Its the name of registered Inline tls device
+ * @dev_list: Inline tls device list
+ * int (*feature)(struct tls_device *device);
+ *     Called to return Inline TLS driver capability
+ *
+ * int (*hash)(struct tls_device *device, struct sock *sk);
+ *     This function sets Inline driver for listen and program
+ *     device specific functioanlity as required
+ *
+ * void (*unhash)(struct tls_device *device, struct sock *sk);
+ *     This function cleans listen state set by Inline TLS driver
+ */
+struct tls_device {
+	char name[TLS_DEVICE_NAME_MAX];
+	struct list_head dev_list;
+	int  (*feature)(struct tls_device *device);
+	int  (*hash)(struct tls_device *device, struct sock *sk);
+	void (*unhash)(struct tls_device *device, struct sock *sk);
+};
 
 struct tls_sw_context {
 	struct crypto_aead *aead_send;
@@ -114,7 +140,7 @@ struct tls_context {
 
 	void *priv_ctx;
 
-	u8 conf:2;
+	u8 conf:3;
 
 	struct cipher_context tx;
 	struct cipher_context rx;
@@ -135,6 +161,8 @@ struct tls_context {
 	int  (*getsockopt)(struct sock *sk, int level,
 			   int optname, char __user *optval,
 			   int __user *optlen);
+	int  (*hash)(struct sock *sk);
+	void (*unhash)(struct sock *sk);
 };
 
 int wait_on_pending_writer(struct sock *sk, long *timeo);
@@ -283,5 +311,7 @@ static inline struct tls_offload_context *tls_offload_ctx(
 
 int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
 		      unsigned char *record_type);
+void tls_register_device(struct tls_device *device);
+void tls_unregister_device(struct tls_device *device);
 
 #endif /* _TLS_OFFLOAD_H */

commit c46234ebb4d1eee5e09819f49169e51cfc6eb909
Author: Dave Watson <davejwatson@fb.com>
Date:   Thu Mar 22 10:10:35 2018 -0700

    tls: RX path for ktls
    
    Add rx path for tls software implementation.
    
    recvmsg, splice_read, and poll implemented.
    
    An additional sockopt TLS_RX is added, with the same interface as
    TLS_TX.  Either TLX_RX or TLX_TX may be provided separately, or
    together (with two different setsockopt calls with appropriate keys).
    
    Control messages are passed via CMSG in a similar way to transmit.
    If no cmsg buffer is passed, then only application data records
    will be passed to userspace, and EIO is returned for other types of
    alerts.
    
    EBADMSG is passed for decryption errors, and EMSGSIZE is passed for
    framing too big, and EBADMSG for framing too small (matching openssl
    semantics). EINVAL is returned for TLS versions that do not match the
    original setsockopt call.  All are unrecoverable.
    
    strparser is used to parse TLS framing.   Decryption is done directly
    in to userspace buffers if they are large enough to support it, otherwise
    sk_cow_data is called (similar to ipsec), and buffers are decrypted in
    place and copied.  splice_read always decrypts in place, since no
    buffers are provided to decrypt in to.
    
    sk_poll is overridden, and only returns POLLIN if a full TLS message is
    received.  Otherwise we wait for strparser to finish reading a full frame.
    Actual decryption is only done during recvmsg or splice_read calls.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 095b72283861..437a746300bf 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -40,6 +40,7 @@
 #include <linux/socket.h>
 #include <linux/tcp.h>
 #include <net/tcp.h>
+#include <net/strparser.h>
 
 #include <uapi/linux/tls.h>
 
@@ -58,8 +59,18 @@
 
 struct tls_sw_context {
 	struct crypto_aead *aead_send;
+	struct crypto_aead *aead_recv;
 	struct crypto_wait async_wait;
 
+	/* Receive context */
+	struct strparser strp;
+	void (*saved_data_ready)(struct sock *sk);
+	unsigned int (*sk_poll)(struct file *file, struct socket *sock,
+				struct poll_table_struct *wait);
+	struct sk_buff *recv_pkt;
+	u8 control;
+	bool decrypted;
+
 	/* Sending context */
 	char aad_space[TLS_AAD_SPACE_SIZE];
 
@@ -96,12 +107,17 @@ struct tls_context {
 		struct tls_crypto_info crypto_send;
 		struct tls12_crypto_info_aes_gcm_128 crypto_send_aes_gcm_128;
 	};
+	union {
+		struct tls_crypto_info crypto_recv;
+		struct tls12_crypto_info_aes_gcm_128 crypto_recv_aes_gcm_128;
+	};
 
 	void *priv_ctx;
 
 	u8 conf:2;
 
 	struct cipher_context tx;
+	struct cipher_context rx;
 
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
@@ -128,12 +144,19 @@ int tls_sk_attach(struct sock *sk, int optname, char __user *optval,
 		  unsigned int optlen);
 
 
-int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx);
+int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx, int tx);
 int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_close(struct sock *sk, long timeout);
-void tls_sw_free_tx_resources(struct sock *sk);
+void tls_sw_free_resources(struct sock *sk);
+int tls_sw_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
+		   int nonblock, int flags, int *addr_len);
+unsigned int tls_sw_poll(struct file *file, struct socket *sock,
+			 struct poll_table_struct *wait);
+ssize_t tls_sw_splice_read(struct socket *sock, loff_t *ppos,
+			   struct pipe_inode_info *pipe,
+			   size_t len, unsigned int flags);
 
 void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
 void tls_icsk_clean_acked(struct sock *sk);

commit 583715853a25b4f2720b847e4fb8e37727299152
Author: Dave Watson <davejwatson@fb.com>
Date:   Thu Mar 22 10:10:26 2018 -0700

    tls: Refactor variable names
    
    Several config variables are prefixed with tx, drop the prefix
    since these will be used for both tx and rx.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 6b44875a78e5..095b72283861 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -99,7 +99,7 @@ struct tls_context {
 
 	void *priv_ctx;
 
-	u8 tx_conf:2;
+	u8 conf:2;
 
 	struct cipher_context tx;
 

commit f4a8e43f1f0abc0e93ed5ee132288ee4142afde1
Author: Dave Watson <davejwatson@fb.com>
Date:   Thu Mar 22 10:10:15 2018 -0700

    tls: Pass error code explicitly to tls_err_abort
    
    Pass EBADMSG explicitly to tls_err_abort.  Receive path will
    pass additional codes - EMSGSIZE if framing is larger than max
    TLS record size, EINVAL if TLS version mismatch.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 019e52db1817..6b44875a78e5 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -174,9 +174,9 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 	return tls_ctx->pending_open_record_frags;
 }
 
-static inline void tls_err_abort(struct sock *sk)
+static inline void tls_err_abort(struct sock *sk, int err)
 {
-	sk->sk_err = EBADMSG;
+	sk->sk_err = err;
 	sk->sk_error_report(sk);
 }
 
@@ -197,7 +197,7 @@ static inline void tls_advance_record_sn(struct sock *sk,
 					 struct cipher_context *ctx)
 {
 	if (tls_bigint_increment(ctx->rec_seq, ctx->rec_seq_size))
-		tls_err_abort(sk);
+		tls_err_abort(sk, EBADMSG);
 	tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
 			     ctx->iv_size);
 }

commit dbe425599ba05c7415f632e6f5f018453098eb69
Author: Dave Watson <davejwatson@fb.com>
Date:   Thu Mar 22 10:10:06 2018 -0700

    tls: Move cipher info to a separate struct
    
    Separate tx crypto parameters to a separate cipher_context struct.
    The same parameters will be used for rx using the same struct.
    
    tls_advance_record_sn is modified to only take the cipher info.
    
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 4913430ab807..019e52db1817 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -81,6 +81,16 @@ enum {
 	TLS_PENDING_CLOSED_RECORD
 };
 
+struct cipher_context {
+	u16 prepend_size;
+	u16 tag_size;
+	u16 overhead_size;
+	u16 iv_size;
+	char *iv;
+	u16 rec_seq_size;
+	char *rec_seq;
+};
+
 struct tls_context {
 	union {
 		struct tls_crypto_info crypto_send;
@@ -91,13 +101,7 @@ struct tls_context {
 
 	u8 tx_conf:2;
 
-	u16 prepend_size;
-	u16 tag_size;
-	u16 overhead_size;
-	u16 iv_size;
-	char *iv;
-	u16 rec_seq_size;
-	char *rec_seq;
+	struct cipher_context tx;
 
 	struct scatterlist *partially_sent_record;
 	u16 partially_sent_offset;
@@ -190,7 +194,7 @@ static inline bool tls_bigint_increment(unsigned char *seq, int len)
 }
 
 static inline void tls_advance_record_sn(struct sock *sk,
-					 struct tls_context *ctx)
+					 struct cipher_context *ctx)
 {
 	if (tls_bigint_increment(ctx->rec_seq, ctx->rec_seq_size))
 		tls_err_abort(sk);
@@ -203,9 +207,9 @@ static inline void tls_fill_prepend(struct tls_context *ctx,
 			     size_t plaintext_len,
 			     unsigned char record_type)
 {
-	size_t pkt_len, iv_size = ctx->iv_size;
+	size_t pkt_len, iv_size = ctx->tx.iv_size;
 
-	pkt_len = plaintext_len + iv_size + ctx->tag_size;
+	pkt_len = plaintext_len + iv_size + ctx->tx.tag_size;
 
 	/* we cover nonce explicit here as well, so buf should be of
 	 * size KTLS_DTLS_HEADER_SIZE + KTLS_DTLS_NONCE_EXPLICIT_SIZE
@@ -217,7 +221,7 @@ static inline void tls_fill_prepend(struct tls_context *ctx,
 	buf[3] = pkt_len >> 8;
 	buf[4] = pkt_len & 0xFF;
 	memcpy(buf + TLS_NONCE_OFFSET,
-	       ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
+	       ctx->tx.iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
 }
 
 static inline void tls_make_aad(char *buf,

commit a54667f6728c2714a400f3c884727da74b6d1717
Author: Vakul Garg <vakul.garg@nxp.com>
Date:   Wed Jan 31 21:34:37 2018 +0530

    tls: Add support for encryption using async offload accelerator
    
    Async crypto accelerators (e.g. drivers/crypto/caam) support offloading
    GCM operation. If they are enabled, crypto_aead_encrypt() return error
    code -EINPROGRESS. In this case tls_do_encryption() needs to wait on a
    completion till the time the response for crypto offload request is
    received.
    
    Signed-off-by: Vakul Garg <vakul.garg@nxp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 9185e53a743c..4913430ab807 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -36,6 +36,7 @@
 
 #include <linux/types.h>
 #include <asm/byteorder.h>
+#include <linux/crypto.h>
 #include <linux/socket.h>
 #include <linux/tcp.h>
 #include <net/tcp.h>
@@ -57,6 +58,7 @@
 
 struct tls_sw_context {
 	struct crypto_aead *aead_send;
+	struct crypto_wait async_wait;
 
 	/* Sending context */
 	char aad_space[TLS_AAD_SPACE_SIZE];

commit 30be8f8dba1bd2aff73e8447d59228471233a3d4
Author: r.hering@avm.de <r.hering@avm.de>
Date:   Fri Jan 12 15:42:06 2018 +0100

    net/tls: Fix inverted error codes to avoid endless loop
    
    sendfile() calls can hang endless with using Kernel TLS if a socket error occurs.
    Socket error codes must be inverted by Kernel TLS before returning because
    they are stored with positive sign. If returned non-inverted they are
    interpreted as number of bytes sent, causing endless looping of the
    splice mechanic behind sendfile().
    
    Signed-off-by: Robert Hering <r.hering@avm.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 936cfc5cab7d..9185e53a743c 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -170,7 +170,7 @@ static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
 
 static inline void tls_err_abort(struct sock *sk)
 {
-	sk->sk_err = -EBADMSG;
+	sk->sk_err = EBADMSG;
 	sk->sk_error_report(sk);
 }
 

commit b9f3eb499d84f8d4adcb2f9212ec655700b28228
Author: Dmitry V. Levin <ldv@altlinux.org>
Date:   Tue Nov 14 06:30:11 2017 +0300

    uapi: fix linux/tls.h userspace compilation error
    
    Move inclusion of a private kernel header <net/tcp.h>
    from uapi/linux/tls.h to its only user - net/tls.h,
    to fix the following linux/tls.h userspace compilation error:
    
    /usr/include/linux/tls.h:41:21: fatal error: net/tcp.h: No such file or directory
    
    As to this point uapi/linux/tls.h was totaly unusuable for userspace,
    cleanup this header file further by moving other redundant includes
    to net/tls.h.
    
    Fixes: 3c4d7559159b ("tls: kernel TLS support")
    Cc: <stable@vger.kernel.org> # v4.13+
    Signed-off-by: Dmitry V. Levin <ldv@altlinux.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 70becd0a9299..936cfc5cab7d 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -35,6 +35,10 @@
 #define _TLS_OFFLOAD_H
 
 #include <linux/types.h>
+#include <asm/byteorder.h>
+#include <linux/socket.h>
+#include <linux/tcp.h>
+#include <net/tcp.h>
 
 #include <uapi/linux/tls.h>
 

commit 213ef6e7c9c063c482d77f12cc438872628d48ec
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Nov 13 10:22:47 2017 +0200

    tls: Move tls_make_aad to header to allow sharing
    
    move tls_make_aad as it is going to be reused
    by the device offload code and rx path.
    Remove unused recv parameter.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index 7cb58a6b8fd0..70becd0a9299 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -214,6 +214,21 @@ static inline void tls_fill_prepend(struct tls_context *ctx,
 	       ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
 }
 
+static inline void tls_make_aad(char *buf,
+				size_t size,
+				char *record_sequence,
+				int record_sequence_size,
+				unsigned char record_type)
+{
+	memcpy(buf, record_sequence, record_sequence_size);
+
+	buf[8] = record_type;
+	buf[9] = TLS_1_2_VERSION_MAJOR;
+	buf[10] = TLS_1_2_VERSION_MINOR;
+	buf[11] = size >> 8;
+	buf[12] = size & 0xFF;
+}
+
 static inline struct tls_context *tls_get_ctx(const struct sock *sk)
 {
 	struct inet_connection_sock *icsk = inet_csk(sk);

commit ff45d820a2df163957ad8ab459b6eb6976144c18
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Nov 13 10:22:46 2017 +0200

    tls: Fix TLS ulp context leak, when TLS_TX setsockopt is not used.
    
    Previously the TLS ulp context would leak if we attached a TLS ulp
    to a socket but did not use the TLS_TX setsockopt,
    or did use it but it failed.
    This patch solves the issue by overriding prot[TLS_BASE_TX].close
    and fixing tls_sk_proto_close to work properly
    when its called with ctx->tx_conf == TLS_BASE_TX.
    This patch also removes ctx->free_resources as we can use ctx->tx_conf
    to obtain the relevant information.
    
    Fixes: 3c4d7559159b ('tls: kernel TLS support')
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index f058a6e08eaa..7cb58a6b8fd0 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -99,7 +99,6 @@ struct tls_context {
 
 	u16 pending_open_record_frags;
 	int (*push_pending_record)(struct sock *sk, int flags);
-	void (*free_resources)(struct sock *sk);
 
 	void (*sk_write_space)(struct sock *sk);
 	void (*sk_proto_close)(struct sock *sk, long timeout);
@@ -124,6 +123,7 @@ int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int tls_sw_sendpage(struct sock *sk, struct page *page,
 		    int offset, size_t size, int flags);
 void tls_sw_close(struct sock *sk, long timeout);
+void tls_sw_free_tx_resources(struct sock *sk);
 
 void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
 void tls_icsk_clean_acked(struct sock *sk);

commit 6d88207fcfddc002afe3e2e4a455e5201089d5d9
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Nov 13 10:22:45 2017 +0200

    tls: Add function to update the TLS socket configuration
    
    The tx configuration is now stored in ctx->tx_conf.
    And sk->sk_prot is updated trough a function
    This will simplify things when we add rx
    and support for different possible
    tx and rx cross configurations.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
index b89d397dd62f..f058a6e08eaa 100644
--- a/include/net/tls.h
+++ b/include/net/tls.h
@@ -83,6 +83,8 @@ struct tls_context {
 
 	void *priv_ctx;
 
+	u8 tx_conf:2;
+
 	u16 prepend_size;
 	u16 tag_size;
 	u16 overhead_size;

commit 3c4d7559159bfe1e3b94df3a657b2cda3a34e218
Author: Dave Watson <davejwatson@fb.com>
Date:   Wed Jun 14 11:37:39 2017 -0700

    tls: kernel TLS support
    
    Software implementation of transport layer security, implemented using ULP
    infrastructure.  tcp proto_ops are replaced with tls equivalents of sendmsg and
    sendpage.
    
    Only symmetric crypto is done in the kernel, keys are passed by setsockopt
    after the handshake is complete.  All control messages are supported via CMSG
    data - the actual symmetric encryption is the same, just the message type needs
    to be passed separately.
    
    For user API, please see Documentation patch.
    
    Pieces that can be shared between hw and sw implementation
    are in tls_main.c
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: Dave Watson <davejwatson@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/tls.h b/include/net/tls.h
new file mode 100644
index 000000000000..b89d397dd62f
--- /dev/null
+++ b/include/net/tls.h
@@ -0,0 +1,237 @@
+/*
+ * Copyright (c) 2016-2017, Mellanox Technologies. All rights reserved.
+ * Copyright (c) 2016-2017, Dave Watson <davejwatson@fb.com>. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _TLS_OFFLOAD_H
+#define _TLS_OFFLOAD_H
+
+#include <linux/types.h>
+
+#include <uapi/linux/tls.h>
+
+
+/* Maximum data size carried in a TLS record */
+#define TLS_MAX_PAYLOAD_SIZE		((size_t)1 << 14)
+
+#define TLS_HEADER_SIZE			5
+#define TLS_NONCE_OFFSET		TLS_HEADER_SIZE
+
+#define TLS_CRYPTO_INFO_READY(info)	((info)->cipher_type)
+
+#define TLS_RECORD_TYPE_DATA		0x17
+
+#define TLS_AAD_SPACE_SIZE		13
+
+struct tls_sw_context {
+	struct crypto_aead *aead_send;
+
+	/* Sending context */
+	char aad_space[TLS_AAD_SPACE_SIZE];
+
+	unsigned int sg_plaintext_size;
+	int sg_plaintext_num_elem;
+	struct scatterlist sg_plaintext_data[MAX_SKB_FRAGS];
+
+	unsigned int sg_encrypted_size;
+	int sg_encrypted_num_elem;
+	struct scatterlist sg_encrypted_data[MAX_SKB_FRAGS];
+
+	/* AAD | sg_plaintext_data | sg_tag */
+	struct scatterlist sg_aead_in[2];
+	/* AAD | sg_encrypted_data (data contain overhead for hdr&iv&tag) */
+	struct scatterlist sg_aead_out[2];
+};
+
+enum {
+	TLS_PENDING_CLOSED_RECORD
+};
+
+struct tls_context {
+	union {
+		struct tls_crypto_info crypto_send;
+		struct tls12_crypto_info_aes_gcm_128 crypto_send_aes_gcm_128;
+	};
+
+	void *priv_ctx;
+
+	u16 prepend_size;
+	u16 tag_size;
+	u16 overhead_size;
+	u16 iv_size;
+	char *iv;
+	u16 rec_seq_size;
+	char *rec_seq;
+
+	struct scatterlist *partially_sent_record;
+	u16 partially_sent_offset;
+	unsigned long flags;
+
+	u16 pending_open_record_frags;
+	int (*push_pending_record)(struct sock *sk, int flags);
+	void (*free_resources)(struct sock *sk);
+
+	void (*sk_write_space)(struct sock *sk);
+	void (*sk_proto_close)(struct sock *sk, long timeout);
+
+	int  (*setsockopt)(struct sock *sk, int level,
+			   int optname, char __user *optval,
+			   unsigned int optlen);
+	int  (*getsockopt)(struct sock *sk, int level,
+			   int optname, char __user *optval,
+			   int __user *optlen);
+};
+
+int wait_on_pending_writer(struct sock *sk, long *timeo);
+int tls_sk_query(struct sock *sk, int optname, char __user *optval,
+		int __user *optlen);
+int tls_sk_attach(struct sock *sk, int optname, char __user *optval,
+		  unsigned int optlen);
+
+
+int tls_set_sw_offload(struct sock *sk, struct tls_context *ctx);
+int tls_sw_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
+int tls_sw_sendpage(struct sock *sk, struct page *page,
+		    int offset, size_t size, int flags);
+void tls_sw_close(struct sock *sk, long timeout);
+
+void tls_sk_destruct(struct sock *sk, struct tls_context *ctx);
+void tls_icsk_clean_acked(struct sock *sk);
+
+int tls_push_sg(struct sock *sk, struct tls_context *ctx,
+		struct scatterlist *sg, u16 first_offset,
+		int flags);
+int tls_push_pending_closed_record(struct sock *sk, struct tls_context *ctx,
+				   int flags, long *timeo);
+
+static inline bool tls_is_pending_closed_record(struct tls_context *ctx)
+{
+	return test_bit(TLS_PENDING_CLOSED_RECORD, &ctx->flags);
+}
+
+static inline int tls_complete_pending_work(struct sock *sk,
+					    struct tls_context *ctx,
+					    int flags, long *timeo)
+{
+	int rc = 0;
+
+	if (unlikely(sk->sk_write_pending))
+		rc = wait_on_pending_writer(sk, timeo);
+
+	if (!rc && tls_is_pending_closed_record(ctx))
+		rc = tls_push_pending_closed_record(sk, ctx, flags, timeo);
+
+	return rc;
+}
+
+static inline bool tls_is_partially_sent_record(struct tls_context *ctx)
+{
+	return !!ctx->partially_sent_record;
+}
+
+static inline bool tls_is_pending_open_record(struct tls_context *tls_ctx)
+{
+	return tls_ctx->pending_open_record_frags;
+}
+
+static inline void tls_err_abort(struct sock *sk)
+{
+	sk->sk_err = -EBADMSG;
+	sk->sk_error_report(sk);
+}
+
+static inline bool tls_bigint_increment(unsigned char *seq, int len)
+{
+	int i;
+
+	for (i = len - 1; i >= 0; i--) {
+		++seq[i];
+		if (seq[i] != 0)
+			break;
+	}
+
+	return (i == -1);
+}
+
+static inline void tls_advance_record_sn(struct sock *sk,
+					 struct tls_context *ctx)
+{
+	if (tls_bigint_increment(ctx->rec_seq, ctx->rec_seq_size))
+		tls_err_abort(sk);
+	tls_bigint_increment(ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE,
+			     ctx->iv_size);
+}
+
+static inline void tls_fill_prepend(struct tls_context *ctx,
+			     char *buf,
+			     size_t plaintext_len,
+			     unsigned char record_type)
+{
+	size_t pkt_len, iv_size = ctx->iv_size;
+
+	pkt_len = plaintext_len + iv_size + ctx->tag_size;
+
+	/* we cover nonce explicit here as well, so buf should be of
+	 * size KTLS_DTLS_HEADER_SIZE + KTLS_DTLS_NONCE_EXPLICIT_SIZE
+	 */
+	buf[0] = record_type;
+	buf[1] = TLS_VERSION_MINOR(ctx->crypto_send.version);
+	buf[2] = TLS_VERSION_MAJOR(ctx->crypto_send.version);
+	/* we can use IV for nonce explicit according to spec */
+	buf[3] = pkt_len >> 8;
+	buf[4] = pkt_len & 0xFF;
+	memcpy(buf + TLS_NONCE_OFFSET,
+	       ctx->iv + TLS_CIPHER_AES_GCM_128_SALT_SIZE, iv_size);
+}
+
+static inline struct tls_context *tls_get_ctx(const struct sock *sk)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+
+	return icsk->icsk_ulp_data;
+}
+
+static inline struct tls_sw_context *tls_sw_ctx(
+		const struct tls_context *tls_ctx)
+{
+	return (struct tls_sw_context *)tls_ctx->priv_ctx;
+}
+
+static inline struct tls_offload_context *tls_offload_ctx(
+		const struct tls_context *tls_ctx)
+{
+	return (struct tls_offload_context *)tls_ctx->priv_ctx;
+}
+
+int tls_proccess_cmsg(struct sock *sk, struct msghdr *msg,
+		      unsigned char *record_type);
+
+#endif /* _TLS_OFFLOAD_H */
