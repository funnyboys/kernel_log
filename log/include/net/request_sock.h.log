commit 60b173ca3d1cd1782bd0096dc17298ec242f6fb1
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 14:51:20 2019 -0700

    net: add {READ|WRITE}_ONCE() annotations on ->rskq_accept_head
    
    reqsk_queue_empty() is called from inet_csk_listen_poll() while
    other cpus might write ->rskq_accept_head value.
    
    Use {READ|WRITE}_ONCE() to avoid compiler tricks
    and potential KCSAN splats.
    
    Fixes: fff1f3001cc5 ("tcp: add a spinlock to protect struct request_sock_queue")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index fd178d58fa84..cf8b33213bbc 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -185,7 +185,7 @@ void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
 
 static inline bool reqsk_queue_empty(const struct request_sock_queue *queue)
 {
-	return queue->rskq_accept_head == NULL;
+	return READ_ONCE(queue->rskq_accept_head) == NULL;
 }
 
 static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue *queue,
@@ -197,7 +197,7 @@ static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue
 	req = queue->rskq_accept_head;
 	if (req) {
 		sk_acceptq_removed(parent);
-		queue->rskq_accept_head = req->dl_next;
+		WRITE_ONCE(queue->rskq_accept_head, req->dl_next);
 		if (queue->rskq_accept_head == NULL)
 			queue->rskq_accept_tail = NULL;
 	}

commit 85f9aa7565bd79b039325f2c01af7ffa717924df
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jun 19 09:38:38 2019 -0700

    inet: clear num_timeout reqsk_alloc()
    
    KMSAN caught uninit-value in tcp_create_openreq_child() [1]
    This is caused by a recent change, combined by the fact
    that TCP cleared num_timeout, num_retrans and sk fields only
    when a request socket was about to be queued.
    
    Under syncookie mode, a temporary request socket is used,
    and req->num_timeout could contain garbage.
    
    Lets clear these three fields sooner, there is really no
    point trying to defer this and risk other bugs.
    
    [1]
    
    BUG: KMSAN: uninit-value in tcp_create_openreq_child+0x157f/0x1cc0 net/ipv4/tcp_minisocks.c:526
    CPU: 1 PID: 13357 Comm: syz-executor591 Not tainted 5.2.0-rc4+ #3
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x191/0x1f0 lib/dump_stack.c:113
     kmsan_report+0x162/0x2d0 mm/kmsan/kmsan.c:611
     __msan_warning+0x75/0xe0 mm/kmsan/kmsan_instr.c:304
     tcp_create_openreq_child+0x157f/0x1cc0 net/ipv4/tcp_minisocks.c:526
     tcp_v6_syn_recv_sock+0x761/0x2d80 net/ipv6/tcp_ipv6.c:1152
     tcp_get_cookie_sock+0x16e/0x6b0 net/ipv4/syncookies.c:209
     cookie_v6_check+0x27e0/0x29a0 net/ipv6/syncookies.c:252
     tcp_v6_cookie_check net/ipv6/tcp_ipv6.c:1039 [inline]
     tcp_v6_do_rcv+0xf1c/0x1ce0 net/ipv6/tcp_ipv6.c:1344
     tcp_v6_rcv+0x60b7/0x6a30 net/ipv6/tcp_ipv6.c:1554
     ip6_protocol_deliver_rcu+0x1433/0x22f0 net/ipv6/ip6_input.c:397
     ip6_input_finish net/ipv6/ip6_input.c:438 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ip6_input+0x2af/0x340 net/ipv6/ip6_input.c:447
     dst_input include/net/dst.h:439 [inline]
     ip6_rcv_finish net/ipv6/ip6_input.c:76 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ipv6_rcv+0x683/0x710 net/ipv6/ip6_input.c:272
     __netif_receive_skb_one_core net/core/dev.c:4981 [inline]
     __netif_receive_skb net/core/dev.c:5095 [inline]
     process_backlog+0x721/0x1410 net/core/dev.c:5906
     napi_poll net/core/dev.c:6329 [inline]
     net_rx_action+0x738/0x1940 net/core/dev.c:6395
     __do_softirq+0x4ad/0x858 kernel/softirq.c:293
     do_softirq_own_stack+0x49/0x80 arch/x86/entry/entry_64.S:1052
     </IRQ>
     do_softirq kernel/softirq.c:338 [inline]
     __local_bh_enable_ip+0x199/0x1e0 kernel/softirq.c:190
     local_bh_enable+0x36/0x40 include/linux/bottom_half.h:32
     rcu_read_unlock_bh include/linux/rcupdate.h:682 [inline]
     ip6_finish_output2+0x213f/0x2670 net/ipv6/ip6_output.c:117
     ip6_finish_output+0xae4/0xbc0 net/ipv6/ip6_output.c:150
     NF_HOOK_COND include/linux/netfilter.h:294 [inline]
     ip6_output+0x5d3/0x720 net/ipv6/ip6_output.c:167
     dst_output include/net/dst.h:433 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ip6_xmit+0x1f53/0x2650 net/ipv6/ip6_output.c:271
     inet6_csk_xmit+0x3df/0x4f0 net/ipv6/inet6_connection_sock.c:135
     __tcp_transmit_skb+0x4076/0x5b40 net/ipv4/tcp_output.c:1156
     tcp_transmit_skb net/ipv4/tcp_output.c:1172 [inline]
     tcp_write_xmit+0x39a9/0xa730 net/ipv4/tcp_output.c:2397
     __tcp_push_pending_frames+0x124/0x4e0 net/ipv4/tcp_output.c:2573
     tcp_send_fin+0xd43/0x1540 net/ipv4/tcp_output.c:3118
     tcp_close+0x16ba/0x1860 net/ipv4/tcp.c:2403
     inet_release+0x1f7/0x270 net/ipv4/af_inet.c:427
     inet6_release+0xaf/0x100 net/ipv6/af_inet6.c:470
     __sock_release net/socket.c:601 [inline]
     sock_close+0x156/0x490 net/socket.c:1273
     __fput+0x4c9/0xba0 fs/file_table.c:280
     ____fput+0x37/0x40 fs/file_table.c:313
     task_work_run+0x22e/0x2a0 kernel/task_work.c:113
     tracehook_notify_resume include/linux/tracehook.h:185 [inline]
     exit_to_usermode_loop arch/x86/entry/common.c:168 [inline]
     prepare_exit_to_usermode+0x39d/0x4d0 arch/x86/entry/common.c:199
     syscall_return_slowpath+0x90/0x5c0 arch/x86/entry/common.c:279
     do_syscall_64+0xe2/0xf0 arch/x86/entry/common.c:305
     entry_SYSCALL_64_after_hwframe+0x63/0xe7
    RIP: 0033:0x401d50
    Code: 01 f0 ff ff 0f 83 40 0d 00 00 c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 83 3d dd 8d 2d 00 00 75 14 b8 03 00 00 00 0f 05 <48> 3d 01 f0 ff ff 0f 83 14 0d 00 00 c3 48 83 ec 08 e8 7a 02 00 00
    RSP: 002b:00007fff1cf58cf8 EFLAGS: 00000246 ORIG_RAX: 0000000000000003
    RAX: 0000000000000000 RBX: 0000000000000004 RCX: 0000000000401d50
    RDX: 000000000000001c RSI: 0000000000000000 RDI: 0000000000000003
    RBP: 00000000004a9050 R08: 0000000020000040 R09: 000000000000001c
    R10: 0000000020004004 R11: 0000000000000246 R12: 0000000000402ef0
    R13: 0000000000402f80 R14: 0000000000000000 R15: 0000000000000000
    
    Uninit was created at:
     kmsan_save_stack_with_flags mm/kmsan/kmsan.c:201 [inline]
     kmsan_internal_poison_shadow+0x53/0xa0 mm/kmsan/kmsan.c:160
     kmsan_kmalloc+0xa4/0x130 mm/kmsan/kmsan_hooks.c:177
     kmem_cache_alloc+0x534/0xb00 mm/slub.c:2781
     reqsk_alloc include/net/request_sock.h:84 [inline]
     inet_reqsk_alloc+0xa8/0x600 net/ipv4/tcp_input.c:6384
     cookie_v6_check+0xadb/0x29a0 net/ipv6/syncookies.c:173
     tcp_v6_cookie_check net/ipv6/tcp_ipv6.c:1039 [inline]
     tcp_v6_do_rcv+0xf1c/0x1ce0 net/ipv6/tcp_ipv6.c:1344
     tcp_v6_rcv+0x60b7/0x6a30 net/ipv6/tcp_ipv6.c:1554
     ip6_protocol_deliver_rcu+0x1433/0x22f0 net/ipv6/ip6_input.c:397
     ip6_input_finish net/ipv6/ip6_input.c:438 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ip6_input+0x2af/0x340 net/ipv6/ip6_input.c:447
     dst_input include/net/dst.h:439 [inline]
     ip6_rcv_finish net/ipv6/ip6_input.c:76 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ipv6_rcv+0x683/0x710 net/ipv6/ip6_input.c:272
     __netif_receive_skb_one_core net/core/dev.c:4981 [inline]
     __netif_receive_skb net/core/dev.c:5095 [inline]
     process_backlog+0x721/0x1410 net/core/dev.c:5906
     napi_poll net/core/dev.c:6329 [inline]
     net_rx_action+0x738/0x1940 net/core/dev.c:6395
     __do_softirq+0x4ad/0x858 kernel/softirq.c:293
     do_softirq_own_stack+0x49/0x80 arch/x86/entry/entry_64.S:1052
     do_softirq kernel/softirq.c:338 [inline]
     __local_bh_enable_ip+0x199/0x1e0 kernel/softirq.c:190
     local_bh_enable+0x36/0x40 include/linux/bottom_half.h:32
     rcu_read_unlock_bh include/linux/rcupdate.h:682 [inline]
     ip6_finish_output2+0x213f/0x2670 net/ipv6/ip6_output.c:117
     ip6_finish_output+0xae4/0xbc0 net/ipv6/ip6_output.c:150
     NF_HOOK_COND include/linux/netfilter.h:294 [inline]
     ip6_output+0x5d3/0x720 net/ipv6/ip6_output.c:167
     dst_output include/net/dst.h:433 [inline]
     NF_HOOK include/linux/netfilter.h:305 [inline]
     ip6_xmit+0x1f53/0x2650 net/ipv6/ip6_output.c:271
     inet6_csk_xmit+0x3df/0x4f0 net/ipv6/inet6_connection_sock.c:135
     __tcp_transmit_skb+0x4076/0x5b40 net/ipv4/tcp_output.c:1156
     tcp_transmit_skb net/ipv4/tcp_output.c:1172 [inline]
     tcp_write_xmit+0x39a9/0xa730 net/ipv4/tcp_output.c:2397
     __tcp_push_pending_frames+0x124/0x4e0 net/ipv4/tcp_output.c:2573
     tcp_send_fin+0xd43/0x1540 net/ipv4/tcp_output.c:3118
     tcp_close+0x16ba/0x1860 net/ipv4/tcp.c:2403
     inet_release+0x1f7/0x270 net/ipv4/af_inet.c:427
     inet6_release+0xaf/0x100 net/ipv6/af_inet6.c:470
     __sock_release net/socket.c:601 [inline]
     sock_close+0x156/0x490 net/socket.c:1273
     __fput+0x4c9/0xba0 fs/file_table.c:280
     ____fput+0x37/0x40 fs/file_table.c:313
     task_work_run+0x22e/0x2a0 kernel/task_work.c:113
     tracehook_notify_resume include/linux/tracehook.h:185 [inline]
     exit_to_usermode_loop arch/x86/entry/common.c:168 [inline]
     prepare_exit_to_usermode+0x39d/0x4d0 arch/x86/entry/common.c:199
     syscall_return_slowpath+0x90/0x5c0 arch/x86/entry/common.c:279
     do_syscall_64+0xe2/0xf0 arch/x86/entry/common.c:305
     entry_SYSCALL_64_after_hwframe+0x63/0xe7
    
    Fixes: 336c39a03151 ("tcp: undo init congestion window on false SYNACK timeout")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b3ea21f2732e..fd178d58fa84 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -97,6 +97,9 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 	sk_node_init(&req_to_sk(req)->sk_node);
 	sk_tx_queue_clear(req_to_sk(req));
 	req->saved_syn = NULL;
+	req->num_timeout = 0;
+	req->num_retrans = 0;
+	req->sk = NULL;
 	refcount_set(&req->rsk_refcnt, 0);
 
 	return req;

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 9dfd7960d90a..b3ea21f2732e 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * NET		Generic infrastructure for Network protocols.
  *
@@ -6,11 +7,6 @@
  * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
  *
  * 		From code originally in include/net/tcp.h
- *
- *		This program is free software; you can redistribute it and/or
- *		modify it under the terms of the GNU General Public License
- *		as published by the Free Software Foundation; either version
- *		2 of the License, or (at your option) any later version.
  */
 #ifndef _REQUEST_SOCK_H
 #define _REQUEST_SOCK_H

commit 9403cf2302588022d06f1878b072d3f6933021f0
Author: Guillaume Nault <gnault@redhat.com>
Date:   Tue Mar 19 16:05:44 2019 +0100

    tcp: free request sock directly upon TFO or syncookies error
    
    Since the request socket is created locally, it'd make more sense to
    use reqsk_free() instead of reqsk_put() in TFO and syncookies' error
    path.
    
    However, tcp_get_cookie_sock() may set ->rsk_refcnt before freeing the
    socket; tcp_conn_request() may also have non-null ->rsk_refcnt because
    of tcp_try_fastopen(). In both cases 'req' hasn't been exposed
    to the outside world and is safe to free immediately, but that'd
    trigger the WARN_ON_ONCE in reqsk_free().
    
    Define __reqsk_free() for these situations where we know nobody's
    referencing the socket, even though ->rsk_refcnt might be non-null.
    Now we can consolidate the error path of tcp_get_cookie_sock() and
    tcp_conn_request().
    
    Signed-off-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 21a5243fecd1..9dfd7960d90a 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -106,10 +106,8 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 	return req;
 }
 
-static inline void reqsk_free(struct request_sock *req)
+static inline void __reqsk_free(struct request_sock *req)
 {
-	WARN_ON_ONCE(refcount_read(&req->rsk_refcnt) != 0);
-
 	req->rsk_ops->destructor(req);
 	if (req->rsk_listener)
 		sock_put(req->rsk_listener);
@@ -117,6 +115,12 @@ static inline void reqsk_free(struct request_sock *req)
 	kmem_cache_free(req->rsk_ops->slab, req);
 }
 
+static inline void reqsk_free(struct request_sock *req)
+{
+	WARN_ON_ONCE(refcount_read(&req->rsk_refcnt) != 0);
+	__reqsk_free(req);
+}
+
 static inline void reqsk_put(struct request_sock *req)
 {
 	if (refcount_dec_and_test(&req->rsk_refcnt))

commit 1039c6e1936ef2be0f342bd56fcf0fb25c1df5fe
Author: Guillaume Nault <gnault@redhat.com>
Date:   Sat Mar 9 10:26:53 2019 +0100

    net: keep refcount warning in reqsk_free()
    
    As Eric Dumazet said, "We do not have a way to tell if the req was ever
    inserted in a hash table, so better play safe.".
    Let's remove this comment, so that nobody will be tempted to drop the
    WARN_ON_ONCE() line.
    
    Signed-off-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 347015515a7d..21a5243fecd1 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -108,7 +108,6 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 
 static inline void reqsk_free(struct request_sock *req)
 {
-	/* temporary debugging */
 	WARN_ON_ONCE(refcount_read(&req->rsk_refcnt) != 0);
 
 	req->rsk_ops->destructor(req);

commit 1fba70e5b6bed53496ba1f1f16127f5be01b5fb6
Author: Yuchung Cheng <ycheng@google.com>
Date:   Wed Oct 18 11:22:51 2017 -0700

    tcp: socket option to set TCP fast open key
    
    New socket option TCP_FASTOPEN_KEY to allow different keys per
    listener.  The listener by default uses the global key until the
    socket option is set.  The key is a 16 bytes long binary data. This
    option has no effect on regular non-listener TCP sockets.
    
    Signed-off-by: Yuchung Cheng <ycheng@google.com>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Reviewed-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 23e22054aa60..347015515a7d 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -150,6 +150,8 @@ struct fastopen_queue {
 	spinlock_t	lock;
 	int		qlen;		/* # of pending (TCP_SYN_RECV) reqs */
 	int		max_qlen;	/* != 0 iff TFO is currently enabled */
+
+	struct tcp_fastopen_context __rcu *ctx; /* cipher context for cookie */
 };
 
 /** struct request_sock_queue - queue of request_socks

commit 41c6d650f6537e55a1b53438c646fbc3f49176bf
Author: Reshetova, Elena <elena.reshetova@intel.com>
Date:   Fri Jun 30 13:08:01 2017 +0300

    net: convert sock.sk_refcnt from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    This patch uses refcount_inc_not_zero() instead of
    atomic_inc_not_zero_hint() due to absense of a _hint()
    version of refcount API. If the hint() version must
    be used, we might need to revisit API.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 53ced67c4ae9..23e22054aa60 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -19,6 +19,7 @@
 #include <linux/spinlock.h>
 #include <linux/types.h>
 #include <linux/bug.h>
+#include <linux/refcount.h>
 
 #include <net/sock.h>
 
@@ -89,7 +90,7 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 		return NULL;
 	req->rsk_listener = NULL;
 	if (attach_listener) {
-		if (unlikely(!atomic_inc_not_zero(&sk_listener->sk_refcnt))) {
+		if (unlikely(!refcount_inc_not_zero(&sk_listener->sk_refcnt))) {
 			kmem_cache_free(ops->slab, req);
 			return NULL;
 		}
@@ -100,7 +101,7 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 	sk_node_init(&req_to_sk(req)->sk_node);
 	sk_tx_queue_clear(req_to_sk(req));
 	req->saved_syn = NULL;
-	atomic_set(&req->rsk_refcnt, 0);
+	refcount_set(&req->rsk_refcnt, 0);
 
 	return req;
 }
@@ -108,7 +109,7 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 static inline void reqsk_free(struct request_sock *req)
 {
 	/* temporary debugging */
-	WARN_ON_ONCE(atomic_read(&req->rsk_refcnt) != 0);
+	WARN_ON_ONCE(refcount_read(&req->rsk_refcnt) != 0);
 
 	req->rsk_ops->destructor(req);
 	if (req->rsk_listener)
@@ -119,7 +120,7 @@ static inline void reqsk_free(struct request_sock *req)
 
 static inline void reqsk_put(struct request_sock *req)
 {
-	if (atomic_dec_and_test(&req->rsk_refcnt))
+	if (refcount_dec_and_test(&req->rsk_refcnt))
 		reqsk_free(req);
 }
 

commit 417ccf6b5bc3f1a390505d5ef65ec17f10e8b29a
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue May 23 00:21:39 2017 +0300

    net: make struct request_sock_ops::obj_size unsigned
    
    This field is sizeof of corresponding kmem_cache so it can't be negative.
    
    Space will be saved after 32-bit kmem_cache_create() patch.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index a12a5d25b27e..53ced67c4ae9 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -29,7 +29,7 @@ struct proto;
 
 struct request_sock_ops {
 	int		family;
-	int		obj_size;
+	unsigned int	obj_size;
 	struct kmem_cache	*slab;
 	char		*slab_name;
 	int		(*rtx_syn_ack)(const struct sock *sk,

commit fee83d097b1620530f23bf6063f4ea251ba9c8c7
Author: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
Date:   Wed Dec 28 17:52:33 2016 +0800

    ipv4: Namespaceify tcp_max_syn_backlog knob
    
    Different namespace application might require different maximal
    number of remembered connection requests.
    
    Signed-off-by: Haishuang Yan <yanhaishuang@cmss.chinamobile.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 6ebe13eb1c4c..a12a5d25b27e 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -1,7 +1,7 @@
 /*
  * NET		Generic infrastructure for Network protocols.
  *
- *		Definitions for request_sock 
+ *		Definitions for request_sock
  *
  * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
  *
@@ -123,8 +123,6 @@ static inline void reqsk_put(struct request_sock *req)
 		reqsk_free(req);
 }
 
-extern int sysctl_max_syn_backlog;
-
 /*
  * For a TCP Fast Open listener -
  *	lock - protects the access to all the reqsk, which is co-owned by

commit 3a5d1c0e7cb5ba91aabbd7e28626e3cc925f8093
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 1 08:52:16 2016 -0700

    inet: reqsk_alloc() needs to take care of dead listeners
    
    We'll soon no longer take a refcount on listeners,
    so reqsk_alloc() can not assume a listener refcount is not
    zero. We need to use atomic_inc_not_zero()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index f49759decb28..6ebe13eb1c4c 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -85,24 +85,23 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
 	struct request_sock *req;
 
 	req = kmem_cache_alloc(ops->slab, GFP_ATOMIC | __GFP_NOWARN);
-
-	if (req) {
-		req->rsk_ops = ops;
-		if (attach_listener) {
-			sock_hold(sk_listener);
-			req->rsk_listener = sk_listener;
-		} else {
-			req->rsk_listener = NULL;
+	if (!req)
+		return NULL;
+	req->rsk_listener = NULL;
+	if (attach_listener) {
+		if (unlikely(!atomic_inc_not_zero(&sk_listener->sk_refcnt))) {
+			kmem_cache_free(ops->slab, req);
+			return NULL;
 		}
-		req_to_sk(req)->sk_prot = sk_listener->sk_prot;
-		sk_node_init(&req_to_sk(req)->sk_node);
-		sk_tx_queue_clear(req_to_sk(req));
-		req->saved_syn = NULL;
-		/* Following is temporary. It is coupled with debugging
-		 * helpers in reqsk_put() & reqsk_free()
-		 */
-		atomic_set(&req->rsk_refcnt, 0);
+		req->rsk_listener = sk_listener;
 	}
+	req->rsk_ops = ops;
+	req_to_sk(req)->sk_prot = sk_listener->sk_prot;
+	sk_node_init(&req_to_sk(req)->sk_node);
+	sk_tx_queue_clear(req_to_sk(req));
+	req->saved_syn = NULL;
+	atomic_set(&req->rsk_refcnt, 0);
+
 	return req;
 }
 

commit b1f0a0e99c58fbd7ea053ca36ba623718272b618
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Dec 21 21:29:24 2015 +0100

    net: add inet_sk_transparent() helper
    
    Avoids cluttering tcp_v4_send_reset when followup patch extends
    it to deal with timewait sockets.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index a0dde04eb178..f49759decb28 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -68,7 +68,7 @@ struct request_sock {
 	u32				peer_secid;
 };
 
-static inline struct request_sock *inet_reqsk(struct sock *sk)
+static inline struct request_sock *inet_reqsk(const struct sock *sk)
 {
 	return (struct request_sock *)sk;
 }

commit ebb516af60e18258aac8e80bbe068740ef1579ed
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 14 11:16:28 2015 -0700

    tcp/dccp: fix race at listener dismantle phase
    
    Under stress, a close() on a listener can trigger the
    WARN_ON(sk->sk_ack_backlog) in inet_csk_listen_stop()
    
    We need to test if listener is still active before queueing
    a child in inet_csk_reqsk_queue_add()
    
    Create a common inet_child_forget() helper, and use it
    from inet_csk_reqsk_queue_add() and inet_csk_listen_stop()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 2e73748956d5..a0dde04eb178 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -186,25 +186,6 @@ static inline bool reqsk_queue_empty(const struct request_sock_queue *queue)
 	return queue->rskq_accept_head == NULL;
 }
 
-static inline void reqsk_queue_add(struct request_sock_queue *queue,
-				   struct request_sock *req,
-				   struct sock *parent,
-				   struct sock *child)
-{
-	spin_lock(&queue->rskq_lock);
-	req->sk = child;
-	sk_acceptq_added(parent);
-
-	if (queue->rskq_accept_head == NULL)
-		queue->rskq_accept_head = req;
-	else
-		queue->rskq_accept_tail->dl_next = req;
-
-	queue->rskq_accept_tail = req;
-	req->dl_next = NULL;
-	spin_unlock(&queue->rskq_lock);
-}
-
 static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue *queue,
 						      struct sock *parent)
 {

commit ed53d0ab761f5c71d77c8dc05fd19c0a851200db
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:23 2015 -0700

    net: shrink struct sock and request_sock by 8 bytes
    
    One 32bit hole is following skc_refcnt, use it.
    skc_incoming_cpu can also be an union for request_sock rcv_wnd.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 6b818b77d5e5..2e73748956d5 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -51,15 +51,14 @@ struct request_sock {
 #define rsk_refcnt			__req_common.skc_refcnt
 #define rsk_hash			__req_common.skc_hash
 #define rsk_listener			__req_common.skc_listener
+#define rsk_window_clamp		__req_common.skc_window_clamp
+#define rsk_rcv_wnd			__req_common.skc_rcv_wnd
 
 	struct request_sock		*dl_next;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */
 	u8				cookie_ts:1; /* syncookie: encode tcpopts in timestamp */
 	u8				num_timeout:7; /* number of timeouts */
-	/* The following two fields can be easily recomputed I think -AK */
-	u32				window_clamp; /* window clamp at creation time */
-	u32				rcv_wnd;	  /* rcv_wnd offered first time */
 	u32				ts_recent;
 	struct timer_list		rsk_timer;
 	const struct request_sock_ops	*rsk_ops;

commit 8e5eb54d303b7cb1174977ca79030e135728c95e
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Oct 8 19:33:22 2015 -0700

    net: align sk_refcnt on 128 bytes boundary
    
    sk->sk_refcnt is dirtied for every TCP/UDP incoming packet.
    This is a performance issue if multiple cpus hit a common socket,
    or multiple sockets are chained due to SO_REUSEPORT.
    
    By moving sk_refcnt 8 bytes further, first 128 bytes of sockets
    are mostly read. As they contain the lookup keys, this has
    a considerable performance impact, as cpus can cache them.
    
    These 8 bytes are not wasted, we use them as a place holder
    for various fields, depending on the socket type.
    
    Tested:
     SYN flood hitting a 16 RX queues NIC.
     TCP listener using 16 sockets and SO_REUSEPORT
     and SO_INCOMING_CPU for proper siloing.
    
     Could process 6.0 Mpps SYN instead of 4.2 Mpps
    
     Kernel profile looked like :
        11.68%  [kernel]  [k] sha_transform
         6.51%  [kernel]  [k] __inet_lookup_listener
         5.07%  [kernel]  [k] __inet_lookup_established
         4.15%  [kernel]  [k] memcpy_erms
         3.46%  [kernel]  [k] ipt_do_table
         2.74%  [kernel]  [k] fib_table_lookup
         2.54%  [kernel]  [k] tcp_make_synack
         2.34%  [kernel]  [k] tcp_conn_request
         2.05%  [kernel]  [k] __netif_receive_skb_core
         2.03%  [kernel]  [k] kmem_cache_alloc
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 95ab5d7aab96..6b818b77d5e5 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -50,9 +50,9 @@ struct request_sock {
 	struct sock_common		__req_common;
 #define rsk_refcnt			__req_common.skc_refcnt
 #define rsk_hash			__req_common.skc_hash
+#define rsk_listener			__req_common.skc_listener
 
 	struct request_sock		*dl_next;
-	struct sock			*rsk_listener;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */
 	u8				cookie_ts:1; /* syncookie: encode tcpopts in timestamp */

commit a1a5344ddbe8fd3e080013b317ac9a664490cfdf
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Oct 4 21:08:11 2015 -0700

    tcp: avoid two atomic ops for syncookies
    
    inet_reqsk_alloc() is used to allocate a temporary request
    in order to generate a SYNACK with a cookie. Then later,
    syncookie validation also uses a temporary request.
    
    These paths already took a reference on listener refcount,
    we can avoid a couple of atomic operations.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index f83669460d82..95ab5d7aab96 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -80,7 +80,8 @@ static inline struct sock *req_to_sk(struct request_sock *req)
 }
 
 static inline struct request_sock *
-reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
+reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener,
+	    bool attach_listener)
 {
 	struct request_sock *req;
 
@@ -88,8 +89,12 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 
 	if (req) {
 		req->rsk_ops = ops;
-		sock_hold(sk_listener);
-		req->rsk_listener = sk_listener;
+		if (attach_listener) {
+			sock_hold(sk_listener);
+			req->rsk_listener = sk_listener;
+		} else {
+			req->rsk_listener = NULL;
+		}
 		req_to_sk(req)->sk_prot = sk_listener->sk_prot;
 		sk_node_init(&req_to_sk(req)->sk_node);
 		sk_tx_queue_clear(req_to_sk(req));

commit 004a5d0140ce1d05c1f5fce5df4baa2717a330e0
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Oct 4 21:08:10 2015 -0700

    net: use sk_fullsock() in __netdev_pick_tx()
    
    SYN_RECV & TIMEWAIT sockets are not full blown, they do not have a
    sk_dst_cache pointer.
    
    Fixes: ca6fb0651883 ("tcp: attach SYNACK messages to request sockets instead of listener")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index dd423d840852..f83669460d82 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -92,6 +92,7 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 		req->rsk_listener = sk_listener;
 		req_to_sk(req)->sk_prot = sk_listener->sk_prot;
 		sk_node_init(&req_to_sk(req)->sk_node);
+		sk_tx_queue_clear(req_to_sk(req));
 		req->saved_syn = NULL;
 		/* Following is temporary. It is coupled with debugging
 		 * helpers in reqsk_put() & reqsk_free()

commit e96f78ab2703f3b0d512f6b469bc685d2ef20475
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Oct 3 06:27:28 2015 -0700

    tcp/dccp: add SLAB_DESTROY_BY_RCU flag for request sockets
    
    Before letting request sockets being put in TCP/DCCP regular
    ehash table, we need to add either :
    
    - SLAB_DESTROY_BY_RCU flag to their kmem_cache
    - add RCU grace period before freeing them.
    
    Since we carefully respected the SLAB_DESTROY_BY_RCU protocol
    like ESTABLISH and TIMEWAIT sockets, use it here.
    
    req_prot_init() being only used by TCP and DCCP, I did not add
    a new slab_flags into their rsk_prot, but reuse prot->slab_flags
    
    Since all reqsk_alloc() users are correctly dealing with a failure,
    add the __GFP_NOWARN flag to avoid traces under pressure.
    
    Fixes: 079096f103fa ("tcp/dccp: install syn_recv requests into ehash table")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index bae6936d75c4..dd423d840852 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -82,7 +82,9 @@ static inline struct sock *req_to_sk(struct request_sock *req)
 static inline struct request_sock *
 reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 {
-	struct request_sock *req = kmem_cache_alloc(ops->slab, GFP_ATOMIC);
+	struct request_sock *req;
+
+	req = kmem_cache_alloc(ops->slab, GFP_ATOMIC | __GFP_NOWARN);
 
 	if (req) {
 		req->rsk_ops = ops;

commit ef547f2ac16bd9d77a780a0e7c70857e69e8f23f
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:37 2015 -0700

    tcp: remove max_qlen_log
    
    This control variable was set at first listen(fd, backlog)
    call, but not updated if application tried to increase or decrease
    backlog. It made sense at the time listener had a non resizeable
    hash table.
    
    Also rounding to powers of two was not very friendly.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index a66ab1345373..bae6936d75c4 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -157,7 +157,7 @@ struct fastopen_queue {
 struct request_sock_queue {
 	spinlock_t		rskq_lock;
 	u8			rskq_defer_accept;
-	u8			max_qlen_log;
+
 	u32			synflood_warned;
 	atomic_t		qlen;
 	atomic_t		young;
@@ -169,8 +169,7 @@ struct request_sock_queue {
 					     */
 };
 
-void reqsk_queue_alloc(struct request_sock_queue *queue,
-		       unsigned int nr_table_entries);
+void reqsk_queue_alloc(struct request_sock_queue *queue);
 
 void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
 			   bool reset);
@@ -240,9 +239,4 @@ static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
 	return atomic_read(&queue->young);
 }
 
-static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
-{
-	return reqsk_queue_len(queue) >> queue->max_qlen_log;
-}
-
 #endif /* _REQUEST_SOCK_H */

commit 10cbc8f179177c1a6d5f56a46ebddc8f602ce5ac
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:36 2015 -0700

    tcp/dccp: remove struct listen_sock
    
    It is enough to check listener sk_state, no need for an extra
    condition.
    
    max_qlen_log can be moved into struct request_sock_queue
    
    We can remove syn_wait_lock and the alignment it enforced.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 353cb61bb399..a66ab1345373 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -119,14 +119,6 @@ static inline void reqsk_put(struct request_sock *req)
 
 extern int sysctl_max_syn_backlog;
 
-/** struct listen_sock - listen state
- *
- * @max_qlen_log - log_2 of maximal queued SYNs/REQUESTs
- */
-struct listen_sock {
-	u32			max_qlen_log;
-};
-
 /*
  * For a TCP Fast Open listener -
  *	lock - protects the access to all the reqsk, which is co-owned by
@@ -160,36 +152,26 @@ struct fastopen_queue {
  * @rskq_accept_head - FIFO head of established children
  * @rskq_accept_tail - FIFO tail of established children
  * @rskq_defer_accept - User waits for some data after accept()
- * @syn_wait_lock - serializer
- *
- * %syn_wait_lock is necessary only to avoid proc interface having to grab the main
- * lock sock while browsing the listening hash (otherwise it's deadlock prone).
  *
  */
 struct request_sock_queue {
 	spinlock_t		rskq_lock;
 	u8			rskq_defer_accept;
+	u8			max_qlen_log;
 	u32			synflood_warned;
-
 	atomic_t		qlen;
 	atomic_t		young;
 
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
-	struct listen_sock	*listen_opt;
 	struct fastopen_queue	fastopenq;  /* Check max_qlen != 0 to determine
 					     * if TFO is enabled.
 					     */
-
-	/* temporary alignment, our goal is to get rid of this lock */
-	spinlock_t		syn_wait_lock ____cacheline_aligned_in_smp;
 };
 
-int reqsk_queue_alloc(struct request_sock_queue *queue,
-		      unsigned int nr_table_entries);
+void reqsk_queue_alloc(struct request_sock_queue *queue,
+		       unsigned int nr_table_entries);
 
-void __reqsk_queue_destroy(struct request_sock_queue *queue);
-void reqsk_queue_destroy(struct request_sock_queue *queue);
 void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
 			   bool reset);
 
@@ -260,7 +242,7 @@ static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
 
 static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
 {
-	return reqsk_queue_len(queue) >> queue->listen_opt->max_qlen_log;
+	return reqsk_queue_len(queue) >> queue->max_qlen_log;
 }
 
 #endif /* _REQUEST_SOCK_H */

commit 81b496b31a4331415b6a644b485a329ec0b45155
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:33 2015 -0700

    tcp/dccp: shrink struct listen_sock
    
    We no longer use hash_rnd, nr_table_entries and syn_table[]
    
    For a listener with a backlog of 10 millions sockets, this
    saves 80 MBytes of vmalloced memory.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index e1850923c4f5..353cb61bb399 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -125,9 +125,6 @@ extern int sysctl_max_syn_backlog;
  */
 struct listen_sock {
 	u32			max_qlen_log;
-	u32			hash_rnd;
-	u32			nr_table_entries;
-	struct request_sock	*syn_table[0];
 };
 
 /*

commit 079096f103faca2dd87342cca6f23d4b34da8871
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:32 2015 -0700

    tcp/dccp: install syn_recv requests into ehash table
    
    In this patch, we insert request sockets into TCP/DCCP
    regular ehash table (where ESTABLISHED and TIMEWAIT sockets
    are) instead of using the per listener hash table.
    
    ACK packets find SYN_RECV pseudo sockets without having
    to find and lock the listener.
    
    In nominal conditions, this halves pressure on listener lock.
    
    Note that this will allow for SO_REUSEPORT refinements,
    so that we can select a listener using cpu/numa affinities instead
    of the prior 'consistent hash', since only SYN packets will
    apply this selection logic.
    
    We will shrink listen_sock in the following patch to ease
    code review.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ying Cai <ycai@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 97c1ba61ed2d..e1850923c4f5 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -266,8 +266,4 @@ static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
 	return reqsk_queue_len(queue) >> queue->listen_opt->max_qlen_log;
 }
 
-void reqsk_queue_hash_req(struct request_sock_queue *queue,
-			  u32 hash, struct request_sock *req,
-			  unsigned long timeout);
-
 #endif /* _REQUEST_SOCK_H */

commit b267cdd1075d28501b7c05c7aeb8466775505e8d
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:27 2015 -0700

    tcp/dccp: init sk_prot and call sk_node_init() in reqsk_alloc()
    
    We plan to use generic functions to insert request sockets
    into ehash table.
    
    sk_prot needs to be set (to retrieve sk_prot->h.hashinfo)
    sk_node needs to be cleared.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 273fb7235ce3..97c1ba61ed2d 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -69,6 +69,16 @@ struct request_sock {
 	u32				peer_secid;
 };
 
+static inline struct request_sock *inet_reqsk(struct sock *sk)
+{
+	return (struct request_sock *)sk;
+}
+
+static inline struct sock *req_to_sk(struct request_sock *req)
+{
+	return (struct sock *)req;
+}
+
 static inline struct request_sock *
 reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 {
@@ -78,6 +88,8 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 		req->rsk_ops = ops;
 		sock_hold(sk_listener);
 		req->rsk_listener = sk_listener;
+		req_to_sk(req)->sk_prot = sk_listener->sk_prot;
+		sk_node_init(&req_to_sk(req)->sk_node);
 		req->saved_syn = NULL;
 		/* Following is temporary. It is coupled with debugging
 		 * helpers in reqsk_put() & reqsk_free()
@@ -87,16 +99,6 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 	return req;
 }
 
-static inline struct request_sock *inet_reqsk(struct sock *sk)
-{
-	return (struct request_sock *)sk;
-}
-
-static inline struct sock *req_to_sk(struct request_sock *req)
-{
-	return (struct sock *)req;
-}
-
 static inline void reqsk_free(struct request_sock *req)
 {
 	/* temporary debugging */

commit 8d2675f1e464aa5cedda63849adecffd8d33fead
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:25 2015 -0700

    tcp: move synflood_warned into struct request_sock_queue
    
    long term plan is to remove struct listen_sock when its hash
    table is no longer there.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index d128e7f89042..273fb7235ce3 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -123,7 +123,6 @@ extern int sysctl_max_syn_backlog;
  */
 struct listen_sock {
 	u32			max_qlen_log;
-	u32			synflood_warned;
 	u32			hash_rnd;
 	u32			nr_table_entries;
 	struct request_sock	*syn_table[0];
@@ -171,6 +170,7 @@ struct fastopen_queue {
 struct request_sock_queue {
 	spinlock_t		rskq_lock;
 	u8			rskq_defer_accept;
+	u32			synflood_warned;
 
 	atomic_t		qlen;
 	atomic_t		young;

commit aac065c50aba0c534a929aeb687eb68c58e523b8
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:24 2015 -0700

    tcp: move qlen/young out of struct listen_sock
    
    qlen_inc & young_inc were protected by listener lock,
    while qlen_dec & young_dec were atomic fields.
    
    Everything needs to be atomic for upcoming lockless listener.
    
    Also move qlen/young in request_sock_queue as we'll get rid
    of struct listen_sock eventually.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 202e36163ae3..d128e7f89042 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -122,14 +122,7 @@ extern int sysctl_max_syn_backlog;
  * @max_qlen_log - log_2 of maximal queued SYNs/REQUESTs
  */
 struct listen_sock {
-	int			qlen_inc; /* protected by listener lock */
-	int			young_inc;/* protected by listener lock */
-
-	/* following fields can be updated by timer */
-	atomic_t		qlen_dec; /* qlen = qlen_inc - qlen_dec */
-	atomic_t		young_dec;
-
-	u32			max_qlen_log ____cacheline_aligned_in_smp;
+	u32			max_qlen_log;
 	u32			synflood_warned;
 	u32			hash_rnd;
 	u32			nr_table_entries;
@@ -179,6 +172,9 @@ struct request_sock_queue {
 	spinlock_t		rskq_lock;
 	u8			rskq_defer_accept;
 
+	atomic_t		qlen;
+	atomic_t		young;
+
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
 	struct listen_sock	*listen_opt;
@@ -242,41 +238,25 @@ static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue
 static inline void reqsk_queue_removed(struct request_sock_queue *queue,
 				       const struct request_sock *req)
 {
-	struct listen_sock *lopt = queue->listen_opt;
-
 	if (req->num_timeout == 0)
-		atomic_inc(&lopt->young_dec);
-	atomic_inc(&lopt->qlen_dec);
+		atomic_dec(&queue->young);
+	atomic_dec(&queue->qlen);
 }
 
 static inline void reqsk_queue_added(struct request_sock_queue *queue)
 {
-	struct listen_sock *lopt = queue->listen_opt;
-
-	lopt->young_inc++;
-	lopt->qlen_inc++;
-}
-
-static inline int listen_sock_qlen(const struct listen_sock *lopt)
-{
-	return lopt->qlen_inc - atomic_read(&lopt->qlen_dec);
-}
-
-static inline int listen_sock_young(const struct listen_sock *lopt)
-{
-	return lopt->young_inc - atomic_read(&lopt->young_dec);
+	atomic_inc(&queue->young);
+	atomic_inc(&queue->qlen);
 }
 
 static inline int reqsk_queue_len(const struct request_sock_queue *queue)
 {
-	const struct listen_sock *lopt = queue->listen_opt;
-
-	return lopt ? listen_sock_qlen(lopt) : 0;
+	return atomic_read(&queue->qlen);
 }
 
 static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
 {
-	return listen_sock_young(queue->listen_opt);
+	return atomic_read(&queue->young);
 }
 
 static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)

commit fff1f3001cc58b5064a0f1154a7ac09b76f29c44
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Oct 2 11:43:23 2015 -0700

    tcp: add a spinlock to protect struct request_sock_queue
    
    struct request_sock_queue fields are currently protected
    by the listener 'lock' (not a real spinlock)
    
    We need to add a private spinlock instead, so that softirq handlers
    creating children do not have to worry with backlog notion
    that the listener 'lock' carries.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index d2544de329bd..202e36163ae3 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -176,9 +176,11 @@ struct fastopen_queue {
  *
  */
 struct request_sock_queue {
+	spinlock_t		rskq_lock;
+	u8			rskq_defer_accept;
+
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
-	u8			rskq_defer_accept;
 	struct listen_sock	*listen_opt;
 	struct fastopen_queue	fastopenq;  /* Check max_qlen != 0 to determine
 					     * if TFO is enabled.
@@ -196,16 +198,7 @@ void reqsk_queue_destroy(struct request_sock_queue *queue);
 void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
 			   bool reset);
 
-static inline struct request_sock *
-	reqsk_queue_yank_acceptq(struct request_sock_queue *queue)
-{
-	struct request_sock *req = queue->rskq_accept_head;
-
-	queue->rskq_accept_head = NULL;
-	return req;
-}
-
-static inline int reqsk_queue_empty(struct request_sock_queue *queue)
+static inline bool reqsk_queue_empty(const struct request_sock_queue *queue)
 {
 	return queue->rskq_accept_head == NULL;
 }
@@ -215,6 +208,7 @@ static inline void reqsk_queue_add(struct request_sock_queue *queue,
 				   struct sock *parent,
 				   struct sock *child)
 {
+	spin_lock(&queue->rskq_lock);
 	req->sk = child;
 	sk_acceptq_added(parent);
 
@@ -225,18 +219,23 @@ static inline void reqsk_queue_add(struct request_sock_queue *queue,
 
 	queue->rskq_accept_tail = req;
 	req->dl_next = NULL;
+	spin_unlock(&queue->rskq_lock);
 }
 
-static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue *queue)
+static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue *queue,
+						      struct sock *parent)
 {
-	struct request_sock *req = queue->rskq_accept_head;
-
-	WARN_ON(req == NULL);
-
-	queue->rskq_accept_head = req->dl_next;
-	if (queue->rskq_accept_head == NULL)
-		queue->rskq_accept_tail = NULL;
+	struct request_sock *req;
 
+	spin_lock_bh(&queue->rskq_lock);
+	req = queue->rskq_accept_head;
+	if (req) {
+		sk_acceptq_removed(parent);
+		queue->rskq_accept_head = req->dl_next;
+		if (queue->rskq_accept_head == NULL)
+			queue->rskq_accept_tail = NULL;
+	}
+	spin_unlock_bh(&queue->rskq_lock);
 	return req;
 }
 

commit 0536fcc039a8926ec12ec587f41a83f7acafeb82
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:52 2015 -0700

    tcp: prepare fastopen code for upcoming listener changes
    
    While auditing TCP stack for upcoming 'lockless' listener changes,
    I found I had to change fastopen_init_queue() to properly init the object
    before publishing it.
    
    Otherwise an other cpu could try to lock the spinlock before it gets
    properly initialized.
    
    Instead of adding appropriate barriers, just remove dynamic memory
    allocations :
    - Structure is 28 bytes on 64bit arches. Using additional 8 bytes
      for holding a pointer seems overkill.
    - Two listeners can share same cache line and performance would suffer.
    
    If we really want to save few bytes, we would instead dynamically allocate
    whole struct request_sock_queue in the future.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c146b5284786..d2544de329bd 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -180,11 +180,8 @@ struct request_sock_queue {
 	struct request_sock	*rskq_accept_tail;
 	u8			rskq_defer_accept;
 	struct listen_sock	*listen_opt;
-	struct fastopen_queue	*fastopenq; /* This is non-NULL iff TFO has been
-					     * enabled on this listener. Check
-					     * max_qlen != 0 in fastopen_queue
-					     * to determine if TFO is enabled
-					     * right at this moment.
+	struct fastopen_queue	fastopenq;  /* Check max_qlen != 0 to determine
+					     * if TFO is enabled.
 					     */
 
 	/* temporary alignment, our goal is to get rid of this lock */

commit 2985aaac010ebd5e562ce1a22cc61acbb0e40cf2
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:51 2015 -0700

    tcp: constify tcp_syn_flood_action() socket argument
    
    tcp_syn_flood_action() will soon be called with unlocked socket.
    In order to avoid SYN flood warning being emitted multiple times,
    use xchg().
    Extend max_qlen_log and synflood_warned fields in struct listen_sock
    to u32
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 90247ec7955b..c146b5284786 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -129,9 +129,8 @@ struct listen_sock {
 	atomic_t		qlen_dec; /* qlen = qlen_inc - qlen_dec */
 	atomic_t		young_dec;
 
-	u8			max_qlen_log ____cacheline_aligned_in_smp;
-	u8			synflood_warned;
-	/* 2 bytes hole, try to use */
+	u32			max_qlen_log ____cacheline_aligned_in_smp;
+	u32			synflood_warned;
 	u32			hash_rnd;
 	u32			nr_table_entries;
 	struct request_sock	*syn_table[0];

commit a00e74442bac5ad19a929d097370da7e07540ea6
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 29 07:42:39 2015 -0700

    tcp/dccp: constify send_synack and send_reset socket argument
    
    None of these functions need to change the socket, make it
    const.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 181f97f9fe1c..90247ec7955b 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -34,9 +34,9 @@ struct request_sock_ops {
 	char		*slab_name;
 	int		(*rtx_syn_ack)(const struct sock *sk,
 				       struct request_sock *req);
-	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,
+	void		(*send_ack)(const struct sock *sk, struct sk_buff *skb,
 				    struct request_sock *req);
-	void		(*send_reset)(struct sock *sk,
+	void		(*send_reset)(const struct sock *sk,
 				      struct sk_buff *skb);
 	void		(*destructor)(struct request_sock *req);
 	void		(*syn_ack_timeout)(const struct request_sock *req);

commit 1b70e977cef6ce7e7411c9bbec21f9adc8e29097
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:24 2015 -0700

    inet: constify inet_rtx_syn_ack() sock argument
    
    SYNACK packets are sent on behalf on unlocked listeners
    or fastopen sockets. Mark socket as const to catch future changes
    that might break the assumption.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index ff7ce1e53ed4..181f97f9fe1c 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -42,7 +42,7 @@ struct request_sock_ops {
 	void		(*syn_ack_timeout)(const struct request_sock *req);
 };
 
-int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
+int inet_rtx_syn_ack(const struct sock *parent, struct request_sock *req);
 
 /* struct request_sock - mini sock to represent a connection request
  */

commit ea3bea3a1d38aab1542176b2ff11a99ce3db9656
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Sep 25 07:39:23 2015 -0700

    tcp/dccp: constify rtx_synack() and friends
    
    This is done to make sure we do not change listener socket
    while sending SYNACK packets while socket lock is not held.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 87935cad2f7b..ff7ce1e53ed4 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -32,7 +32,7 @@ struct request_sock_ops {
 	int		obj_size;
 	struct kmem_cache	*slab;
 	char		*slab_name;
-	int		(*rtx_syn_ack)(struct sock *sk,
+	int		(*rtx_syn_ack)(const struct sock *sk,
 				       struct request_sock *req);
 	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,
 				    struct request_sock *req);

commit cd8ae85299d54155702a56811b2e035e63064d3d
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun May 3 21:34:46 2015 -0700

    tcp: provide SYN headers for passive connections
    
    This patch allows a server application to get the TCP SYN headers for
    its passive connections.  This is useful if the server is doing
    fingerprinting of clients based on SYN packet contents.
    
    Two socket options are added: TCP_SAVE_SYN and TCP_SAVED_SYN.
    
    The first is used on a socket to enable saving the SYN headers
    for child connections. This can be set before or after the listen()
    call.
    
    The latter is used to retrieve the SYN headers for passive connections,
    if the parent listener has enabled TCP_SAVE_SYN.
    
    TCP_SAVED_SYN is read once, it frees the saved SYN headers.
    
    The data returned in TCP_SAVED_SYN are network (IPv4/IPv6) and TCP
    headers.
    
    Original patch was written by Tom Herbert, I changed it to not hold
    a full skb (and associated dst and conntracking reference).
    
    We have used such patch for about 3 years at Google.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Tested-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 9f4265ce8892..87935cad2f7b 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -64,6 +64,7 @@ struct request_sock {
 	struct timer_list		rsk_timer;
 	const struct request_sock_ops	*rsk_ops;
 	struct sock			*sk;
+	u32				*saved_syn;
 	u32				secid;
 	u32				peer_secid;
 };
@@ -77,7 +78,7 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 		req->rsk_ops = ops;
 		sock_hold(sk_listener);
 		req->rsk_listener = sk_listener;
-
+		req->saved_syn = NULL;
 		/* Following is temporary. It is coupled with debugging
 		 * helpers in reqsk_put() & reqsk_free()
 		 */
@@ -104,6 +105,7 @@ static inline void reqsk_free(struct request_sock *req)
 	req->rsk_ops->destructor(req);
 	if (req->rsk_listener)
 		sock_put(req->rsk_listener);
+	kfree(req->saved_syn);
 	kmem_cache_free(req->rsk_ops->slab, req);
 }
 

commit b357a364c57c940ddb932224542494363df37378
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Apr 23 18:03:44 2015 -0700

    inet: fix possible panic in reqsk_queue_unlink()
    
    [ 3897.923145] BUG: unable to handle kernel NULL pointer dereference at
     0000000000000080
    [ 3897.931025] IP: [<ffffffffa9f27686>] reqsk_timer_handler+0x1a6/0x243
    
    There is a race when reqsk_timer_handler() and tcp_check_req() call
    inet_csk_reqsk_queue_unlink() on the same req at the same time.
    
    Before commit fa76ce7328b2 ("inet: get rid of central tcp/dccp listener
    timer"), listener spinlock was held and race could not happen.
    
    To solve this bug, we change reqsk_queue_unlink() to not assume req
    must be found, and we return a status, to conditionally release a
    refcount on the request sock.
    
    This also means tcp_check_req() in non fastopen case might or not
    consume req refcount, so tcp_v6_hnd_req() & tcp_v4_hnd_req() have
    to properly handle this.
    
    (Same remark for dccp_check_req() and its callers)
    
    inet_csk_reqsk_queue_drop() is now too big to be inlined, as it is
    called 4 times in tcp and 3 times in dccp.
    
    Fixes: fa76ce7328b2 ("inet: get rid of central tcp/dccp listener timer")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index fe41f3ceb008..9f4265ce8892 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -212,24 +212,6 @@ static inline int reqsk_queue_empty(struct request_sock_queue *queue)
 	return queue->rskq_accept_head == NULL;
 }
 
-static inline void reqsk_queue_unlink(struct request_sock_queue *queue,
-				      struct request_sock *req)
-{
-	struct listen_sock *lopt = queue->listen_opt;
-	struct request_sock **prev;
-
-	spin_lock(&queue->syn_wait_lock);
-
-	prev = &lopt->syn_table[req->rsk_hash];
-	while (*prev != req)
-		prev = &(*prev)->dl_next;
-	*prev = req->dl_next;
-
-	spin_unlock(&queue->syn_wait_lock);
-	if (del_timer(&req->rsk_timer))
-		reqsk_put(req);
-}
-
 static inline void reqsk_queue_add(struct request_sock_queue *queue,
 				   struct request_sock *req,
 				   struct sock *parent,

commit b282705336e03fc7b9377a278939594870a40f96
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 22 10:22:21 2015 -0700

    net: convert syn_wait_lock to a spinlock
    
    This is a low hanging fruit, as we'll get rid of syn_wait_lock eventually.
    
    We hold syn_wait_lock for such small sections, that it makes no sense to use
    a read/write lock. A spin lock is simply faster.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 8603c350fad0..fe41f3ceb008 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -173,11 +173,6 @@ struct fastopen_queue {
  * %syn_wait_lock is necessary only to avoid proc interface having to grab the main
  * lock sock while browsing the listening hash (otherwise it's deadlock prone).
  *
- * This lock is acquired in read mode only from listening_get_next() seq_file
- * op and it's acquired in write mode _only_ from code that is actively
- * changing rskq_accept_head. All readers that are holding the master sock lock
- * don't need to grab this lock in read mode too as rskq_accept_head. writes
- * are always protected from the main sock lock.
  */
 struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
@@ -192,7 +187,7 @@ struct request_sock_queue {
 					     */
 
 	/* temporary alignment, our goal is to get rid of this lock */
-	rwlock_t		syn_wait_lock ____cacheline_aligned_in_smp;
+	spinlock_t		syn_wait_lock ____cacheline_aligned_in_smp;
 };
 
 int reqsk_queue_alloc(struct request_sock_queue *queue,
@@ -223,14 +218,14 @@ static inline void reqsk_queue_unlink(struct request_sock_queue *queue,
 	struct listen_sock *lopt = queue->listen_opt;
 	struct request_sock **prev;
 
-	write_lock(&queue->syn_wait_lock);
+	spin_lock(&queue->syn_wait_lock);
 
 	prev = &lopt->syn_table[req->rsk_hash];
 	while (*prev != req)
 		prev = &(*prev)->dl_next;
 	*prev = req->dl_next;
 
-	write_unlock(&queue->syn_wait_lock);
+	spin_unlock(&queue->syn_wait_lock);
 	if (del_timer(&req->rsk_timer))
 		reqsk_put(req);
 }

commit 42cb80a2353f42913ae78074ffa1f1b4a49e5436
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 22 10:22:19 2015 -0700

    inet: remove sk_listener parameter from syn_ack_timeout()
    
    It is not needed, and req->sk_listener points to the listener anyway.
    request_sock argument can be const.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 6a91261d9b7b..8603c350fad0 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -39,8 +39,7 @@ struct request_sock_ops {
 	void		(*send_reset)(struct sock *sk,
 				      struct sk_buff *skb);
 	void		(*destructor)(struct request_sock *req);
-	void		(*syn_ack_timeout)(struct sock *sk,
-					   struct request_sock *req);
+	void		(*syn_ack_timeout)(const struct request_sock *req);
 };
 
 int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);

commit fa76ce7328b289b6edd476e24eb52fd634261720
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 19 19:04:20 2015 -0700

    inet: get rid of central tcp/dccp listener timer
    
    One of the major issue for TCP is the SYNACK rtx handling,
    done by inet_csk_reqsk_queue_prune(), fired by the keepalive
    timer of a TCP_LISTEN socket.
    
    This function runs for awful long times, with socket lock held,
    meaning that other cpus needing this lock have to spin for hundred of ms.
    
    SYNACK are sent in huge bursts, likely to cause severe drops anyway.
    
    This model was OK 15 years ago when memory was very tight.
    
    We now can afford to have a timer per request sock.
    
    Timer invocations no longer need to lock the listener,
    and can be run from all cpus in parallel.
    
    With following patch increasing somaxconn width to 32 bits,
    I tested a listener with more than 4 million active request sockets,
    and a steady SYNFLOOD of ~200,000 SYN per second.
    Host was sending ~830,000 SYNACK per second.
    
    This is ~100 times more what we could achieve before this patch.
    
    Later, we will get rid of the listener hash and use ehash instead.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 65223905d139..6a91261d9b7b 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -62,7 +62,7 @@ struct request_sock {
 	u32				window_clamp; /* window clamp at creation time */
 	u32				rcv_wnd;	  /* rcv_wnd offered first time */
 	u32				ts_recent;
-	unsigned long			expires;
+	struct timer_list		rsk_timer;
 	const struct request_sock_ops	*rsk_ops;
 	struct sock			*sk;
 	u32				secid;
@@ -110,9 +110,6 @@ static inline void reqsk_free(struct request_sock *req)
 
 static inline void reqsk_put(struct request_sock *req)
 {
-	/* temporary debugging, until req sock are put into ehash table */
-	WARN_ON_ONCE(atomic_read(&req->rsk_refcnt) != 1);
-
 	if (atomic_dec_and_test(&req->rsk_refcnt))
 		reqsk_free(req);
 }
@@ -124,12 +121,16 @@ extern int sysctl_max_syn_backlog;
  * @max_qlen_log - log_2 of maximal queued SYNs/REQUESTs
  */
 struct listen_sock {
-	u8			max_qlen_log;
+	int			qlen_inc; /* protected by listener lock */
+	int			young_inc;/* protected by listener lock */
+
+	/* following fields can be updated by timer */
+	atomic_t		qlen_dec; /* qlen = qlen_inc - qlen_dec */
+	atomic_t		young_dec;
+
+	u8			max_qlen_log ____cacheline_aligned_in_smp;
 	u8			synflood_warned;
 	/* 2 bytes hole, try to use */
-	int			qlen;
-	int			qlen_young;
-	int			clock_hand;
 	u32			hash_rnd;
 	u32			nr_table_entries;
 	struct request_sock	*syn_table[0];
@@ -182,9 +183,7 @@ struct fastopen_queue {
 struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
-	rwlock_t		syn_wait_lock;
 	u8			rskq_defer_accept;
-	/* 3 bytes hole, try to pack */
 	struct listen_sock	*listen_opt;
 	struct fastopen_queue	*fastopenq; /* This is non-NULL iff TFO has been
 					     * enabled on this listener. Check
@@ -192,6 +191,9 @@ struct request_sock_queue {
 					     * to determine if TFO is enabled
 					     * right at this moment.
 					     */
+
+	/* temporary alignment, our goal is to get rid of this lock */
+	rwlock_t		syn_wait_lock ____cacheline_aligned_in_smp;
 };
 
 int reqsk_queue_alloc(struct request_sock_queue *queue,
@@ -223,11 +225,15 @@ static inline void reqsk_queue_unlink(struct request_sock_queue *queue,
 	struct request_sock **prev;
 
 	write_lock(&queue->syn_wait_lock);
+
 	prev = &lopt->syn_table[req->rsk_hash];
 	while (*prev != req)
 		prev = &(*prev)->dl_next;
 	*prev = req->dl_next;
+
 	write_unlock(&queue->syn_wait_lock);
+	if (del_timer(&req->rsk_timer))
+		reqsk_put(req);
 }
 
 static inline void reqsk_queue_add(struct request_sock_queue *queue,
@@ -260,64 +266,53 @@ static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue
 	return req;
 }
 
-static inline int reqsk_queue_removed(struct request_sock_queue *queue,
-				      struct request_sock *req)
+static inline void reqsk_queue_removed(struct request_sock_queue *queue,
+				       const struct request_sock *req)
 {
 	struct listen_sock *lopt = queue->listen_opt;
 
 	if (req->num_timeout == 0)
-		--lopt->qlen_young;
-
-	return --lopt->qlen;
+		atomic_inc(&lopt->young_dec);
+	atomic_inc(&lopt->qlen_dec);
 }
 
-static inline int reqsk_queue_added(struct request_sock_queue *queue)
+static inline void reqsk_queue_added(struct request_sock_queue *queue)
 {
 	struct listen_sock *lopt = queue->listen_opt;
-	const int prev_qlen = lopt->qlen;
 
-	lopt->qlen_young++;
-	lopt->qlen++;
-	return prev_qlen;
+	lopt->young_inc++;
+	lopt->qlen_inc++;
 }
 
-static inline int reqsk_queue_len(const struct request_sock_queue *queue)
+static inline int listen_sock_qlen(const struct listen_sock *lopt)
 {
-	return queue->listen_opt != NULL ? queue->listen_opt->qlen : 0;
+	return lopt->qlen_inc - atomic_read(&lopt->qlen_dec);
 }
 
-static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
+static inline int listen_sock_young(const struct listen_sock *lopt)
 {
-	return queue->listen_opt->qlen_young;
+	return lopt->young_inc - atomic_read(&lopt->young_dec);
 }
 
-static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
+static inline int reqsk_queue_len(const struct request_sock_queue *queue)
 {
-	return queue->listen_opt->qlen >> queue->listen_opt->max_qlen_log;
+	const struct listen_sock *lopt = queue->listen_opt;
+
+	return lopt ? listen_sock_qlen(lopt) : 0;
 }
 
-static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
-					u32 hash, struct request_sock *req,
-					unsigned long timeout)
+static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
 {
-	struct listen_sock *lopt = queue->listen_opt;
-
-	req->expires = jiffies + timeout;
-	req->num_retrans = 0;
-	req->num_timeout = 0;
-	req->sk = NULL;
-
-	/* before letting lookups find us, make sure all req fields
-	 * are committed to memory and refcnt initialized.
-	 */
-	smp_wmb();
-	atomic_set(&req->rsk_refcnt, 1);
+	return listen_sock_young(queue->listen_opt);
+}
 
-	req->rsk_hash = hash;
-	write_lock(&queue->syn_wait_lock);
-	req->dl_next = lopt->syn_table[hash];
-	lopt->syn_table[hash] = req;
-	write_unlock(&queue->syn_wait_lock);
+static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
+{
+	return reqsk_queue_len(queue) >> queue->listen_opt->max_qlen_log;
 }
 
+void reqsk_queue_hash_req(struct request_sock_queue *queue,
+			  u32 hash, struct request_sock *req,
+			  unsigned long timeout);
+
 #endif /* _REQUEST_SOCK_H */

commit 52452c542559ac980b48dbf22a30ee7fa0af507c
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 19 19:04:19 2015 -0700

    inet: drop prev pointer handling in request sock
    
    When request sock are put in ehash table, the whole notion
    of having a previous request to update dl_next is pointless.
    
    Also, following patch will get rid of big purge timer,
    so we want to delete a request sock without holding listener lock.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index e7ef86340514..65223905d139 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -50,6 +50,7 @@ int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
 struct request_sock {
 	struct sock_common		__req_common;
 #define rsk_refcnt			__req_common.skc_refcnt
+#define rsk_hash			__req_common.skc_hash
 
 	struct request_sock		*dl_next;
 	struct sock			*rsk_listener;
@@ -216,11 +217,16 @@ static inline int reqsk_queue_empty(struct request_sock_queue *queue)
 }
 
 static inline void reqsk_queue_unlink(struct request_sock_queue *queue,
-				      struct request_sock *req,
-				      struct request_sock **prev_req)
+				      struct request_sock *req)
 {
+	struct listen_sock *lopt = queue->listen_opt;
+	struct request_sock **prev;
+
 	write_lock(&queue->syn_wait_lock);
-	*prev_req = req->dl_next;
+	prev = &lopt->syn_table[req->rsk_hash];
+	while (*prev != req)
+		prev = &(*prev)->dl_next;
+	*prev = req->dl_next;
 	write_unlock(&queue->syn_wait_lock);
 }
 
@@ -300,7 +306,6 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	req->num_retrans = 0;
 	req->num_timeout = 0;
 	req->sk = NULL;
-	req->dl_next = lopt->syn_table[hash];
 
 	/* before letting lookups find us, make sure all req fields
 	 * are committed to memory and refcnt initialized.
@@ -308,7 +313,9 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	smp_wmb();
 	atomic_set(&req->rsk_refcnt, 1);
 
+	req->rsk_hash = hash;
 	write_lock(&queue->syn_wait_lock);
+	req->dl_next = lopt->syn_table[hash];
 	lopt->syn_table[hash] = req;
 	write_unlock(&queue->syn_wait_lock);
 }

commit 08d2cc3b26554cae21f279b520ae5c2a3b2be421
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Mar 18 14:05:38 2015 -0700

    inet: request sock should init IPv6/IPv4 addresses
    
    In order to be able to use sk_ehashfn() for request socks,
    we need to initialize their IPv6/IPv4 addresses.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 3fa4f824900a..e7ef86340514 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -91,6 +91,11 @@ static inline struct request_sock *inet_reqsk(struct sock *sk)
 	return (struct request_sock *)sk;
 }
 
+static inline struct sock *req_to_sk(struct request_sock *req)
+{
+	return (struct sock *)req;
+}
+
 static inline void reqsk_free(struct request_sock *req)
 {
 	/* temporary debugging */

commit 0470c8ca1d57927f2cc3e1d5add1fb2834609447
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 17 18:32:31 2015 -0700

    inet: fix request sock refcounting
    
    While testing last patch series, I found req sock refcounting was wrong.
    
    We must set skc_refcnt to 1 for all request socks added in hashes,
    but also on request sockets created by FastOpen or syncookies.
    
    It is tricky because we need to defer this initialization so that
    future RCU lookups do not try to take a refcount on a not yet
    fully initialized request socket.
    
    Also get rid of ireq_refcnt alias.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Fixes: 13854e5a6046 ("inet: add proper refcounting to request sock")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 723d1cbdf20e..3fa4f824900a 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -77,6 +77,11 @@ reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 		req->rsk_ops = ops;
 		sock_hold(sk_listener);
 		req->rsk_listener = sk_listener;
+
+		/* Following is temporary. It is coupled with debugging
+		 * helpers in reqsk_put() & reqsk_free()
+		 */
+		atomic_set(&req->rsk_refcnt, 0);
 	}
 	return req;
 }
@@ -292,6 +297,12 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	req->sk = NULL;
 	req->dl_next = lopt->syn_table[hash];
 
+	/* before letting lookups find us, make sure all req fields
+	 * are committed to memory and refcnt initialized.
+	 */
+	smp_wmb();
+	atomic_set(&req->rsk_refcnt, 1);
+
 	write_lock(&queue->syn_wait_lock);
 	lopt->syn_table[hash] = req;
 	write_unlock(&queue->syn_wait_lock);

commit 4e9a578e5b6bdfa8b7fed7a41f28a86a7cffc85f
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 17 18:32:28 2015 -0700

    inet: add rsk_listener field to struct request_sock
    
    Once we'll be able to lookup request sockets in ehash table,
    we'll need to get access to listener which created this request.
    
    This avoid doing a lookup to find the listener, which benefits
    for a more solid SO_REUSEPORT, and is needed once we no
    longer queue request sock into a listener private queue.
    
    Note that 'struct tcp_request_sock'->listener could be reduced
    to a single bit, as TFO listener should match req->rsk_listener.
    TFO will no longer need to hold a reference on the listener.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 56dc2faba47e..723d1cbdf20e 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -52,6 +52,7 @@ struct request_sock {
 #define rsk_refcnt			__req_common.skc_refcnt
 
 	struct request_sock		*dl_next;
+	struct sock			*rsk_listener;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */
 	u8				cookie_ts:1; /* syncookie: encode tcpopts in timestamp */
@@ -67,13 +68,16 @@ struct request_sock {
 	u32				peer_secid;
 };
 
-static inline struct request_sock *reqsk_alloc(const struct request_sock_ops *ops)
+static inline struct request_sock *
+reqsk_alloc(const struct request_sock_ops *ops, struct sock *sk_listener)
 {
 	struct request_sock *req = kmem_cache_alloc(ops->slab, GFP_ATOMIC);
 
-	if (req != NULL)
+	if (req) {
 		req->rsk_ops = ops;
-
+		sock_hold(sk_listener);
+		req->rsk_listener = sk_listener;
+	}
 	return req;
 }
 
@@ -88,6 +92,8 @@ static inline void reqsk_free(struct request_sock *req)
 	WARN_ON_ONCE(atomic_read(&req->rsk_refcnt) != 0);
 
 	req->rsk_ops->destructor(req);
+	if (req->rsk_listener)
+		sock_put(req->rsk_listener);
 	kmem_cache_free(req->rsk_ops->slab, req);
 }
 

commit 13854e5a60461daee08ce99842b7f4d37553d911
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Mar 15 21:12:16 2015 -0700

    inet: add proper refcounting to request sock
    
    reqsk_put() is the generic function that should be used
    to release a refcount (and automatically call reqsk_free())
    
    reqsk_free() might be called if refcount is known to be 0
    or undefined.
    
    refcnt is set to one in inet_csk_reqsk_queue_add()
    
    As request socks are not yet in global ehash table,
    I added temporary debugging checks in reqsk_put() and reqsk_free()
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 3275cf31f731..56dc2faba47e 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -82,19 +82,20 @@ static inline struct request_sock *inet_reqsk(struct sock *sk)
 	return (struct request_sock *)sk;
 }
 
-static inline void __reqsk_free(struct request_sock *req)
-{
-	kmem_cache_free(req->rsk_ops->slab, req);
-}
-
 static inline void reqsk_free(struct request_sock *req)
 {
+	/* temporary debugging */
+	WARN_ON_ONCE(atomic_read(&req->rsk_refcnt) != 0);
+
 	req->rsk_ops->destructor(req);
-	__reqsk_free(req);
+	kmem_cache_free(req->rsk_ops->slab, req);
 }
 
 static inline void reqsk_put(struct request_sock *req)
 {
+	/* temporary debugging, until req sock are put into ehash table */
+	WARN_ON_ONCE(atomic_read(&req->rsk_refcnt) != 1);
+
 	if (atomic_dec_and_test(&req->rsk_refcnt))
 		reqsk_free(req);
 }

commit 41b822c59e21414d829bcfd00df0c8f7f13b1b95
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 12 16:44:08 2015 -0700

    inet: prepare sock_edemux() & sock_gen_put() for new SYN_RECV state
    
    sock_edemux() & sock_gen_put() should be ready to cope with request socks.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index e255ecf8bb40..3275cf31f731 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -77,6 +77,11 @@ static inline struct request_sock *reqsk_alloc(const struct request_sock_ops *op
 	return req;
 }
 
+static inline struct request_sock *inet_reqsk(struct sock *sk)
+{
+	return (struct request_sock *)sk;
+}
+
 static inline void __reqsk_free(struct request_sock *req)
 {
 	kmem_cache_free(req->rsk_ops->slab, req);

commit 1e2e01172fd11b4dbfee746c0c8fbcaa9dbf22a0
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Mar 12 16:44:06 2015 -0700

    inet: add rsk_refcnt/ireq_refcnt to request socks
    
    When request socks will be in ehash, they'll need to be refcounted.
    
    This patch adds rsk_refcnt/ireq_refcnt macros, and adds
    reqsk_put() function, but nothing yet use them.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 7f830ff67f08..e255ecf8bb40 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -49,6 +49,8 @@ int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
  */
 struct request_sock {
 	struct sock_common		__req_common;
+#define rsk_refcnt			__req_common.skc_refcnt
+
 	struct request_sock		*dl_next;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */
@@ -86,6 +88,12 @@ static inline void reqsk_free(struct request_sock *req)
 	__reqsk_free(req);
 }
 
+static inline void reqsk_put(struct request_sock *req)
+{
+	if (atomic_dec_and_test(&req->rsk_refcnt))
+		reqsk_free(req);
+}
+
 extern int sysctl_max_syn_backlog;
 
 /** struct listen_sock - listen state

commit 634fb979e8f3a70f04c1f2f519d0cd1142eb5c1a
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Oct 9 15:21:29 2013 -0700

    inet: includes a sock_common in request_sock
    
    TCP listener refactoring, part 5 :
    
    We want to be able to insert request sockets (SYN_RECV) into main
    ehash table instead of the per listener hash table to allow RCU
    lookups and remove listener lock contention.
    
    This patch includes the needed struct sock_common in front
    of struct request_sock
    
    This means there is no more inet6_request_sock IPv6 specific
    structure.
    
    Following inet_request_sock fields were renamed as they became
    macros to reference fields from struct sock_common.
    Prefix ir_ was chosen to avoid name collisions.
    
    loc_port   -> ir_loc_port
    loc_addr   -> ir_loc_addr
    rmt_addr   -> ir_rmt_addr
    rmt_port   -> ir_rmt_port
    iif        -> ir_iif
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 65c3e5164a5c..7f830ff67f08 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -48,6 +48,7 @@ int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
 /* struct request_sock - mini sock to represent a connection request
  */
 struct request_sock {
+	struct sock_common		__req_common;
 	struct request_sock		*dl_next;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */

commit c0f4502a8700832bb3a917fdcfe080f5ccf1c617
Author: Joe Perches <joe@perches.com>
Date:   Sun Sep 22 10:32:20 2013 -0700

    request_sock.h: Remove extern from function prototypes
    
    There are a mix of function prototypes with and without extern
    in the kernel sources.  Standardize on not using extern for
    function prototypes.
    
    Function prototypes don't need to be written with extern.
    extern is assumed by the compiler.  Its use is as unnecessary as
    using auto to declare automatic/local variables in a block.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 59795e42c8b6..65c3e5164a5c 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -43,7 +43,7 @@ struct request_sock_ops {
 					   struct request_sock *req);
 };
 
-extern int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
+int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
 
 /* struct request_sock - mini sock to represent a connection request
  */
@@ -162,13 +162,13 @@ struct request_sock_queue {
 					     */
 };
 
-extern int reqsk_queue_alloc(struct request_sock_queue *queue,
-			     unsigned int nr_table_entries);
+int reqsk_queue_alloc(struct request_sock_queue *queue,
+		      unsigned int nr_table_entries);
 
-extern void __reqsk_queue_destroy(struct request_sock_queue *queue);
-extern void reqsk_queue_destroy(struct request_sock_queue *queue);
-extern void reqsk_fastopen_remove(struct sock *sk,
-				  struct request_sock *req, bool reset);
+void __reqsk_queue_destroy(struct request_sock_queue *queue);
+void reqsk_queue_destroy(struct request_sock_queue *queue);
+void reqsk_fastopen_remove(struct sock *sk, struct request_sock *req,
+			   bool reset);
 
 static inline struct request_sock *
 	reqsk_queue_yank_acceptq(struct request_sock_queue *queue)

commit 3fb62c5d3fc1821f50c6003e582713857a520f6b
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Apr 19 14:29:25 2013 -0700

    net: remove a stale comment for dl_next
    
    dl_next member in struct request_sock doesn't need to be first.
    
    We expect to insert a "struct common_sock" or a subset of it,
    so this claim had to be verified.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 9069e65c1c56..59795e42c8b6 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -48,7 +48,7 @@ extern int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
 /* struct request_sock - mini sock to represent a connection request
  */
 struct request_sock {
-	struct request_sock		*dl_next; /* Must be first member! */
+	struct request_sock		*dl_next;
 	u16				mss;
 	u8				num_retrans; /* number of retransmits */
 	u8				cookie_ts:1; /* syncookie: encode tcpopts in timestamp */

commit 1a2c6181c4a1922021b4d7df373bba612c3e5f04
Author: Christoph Paasch <christoph.paasch@uclouvain.be>
Date:   Sun Mar 17 08:23:34 2013 +0000

    tcp: Remove TCPCT
    
    TCPCT uses option-number 253, reserved for experimental use and should
    not be used in production environments.
    Further, TCPCT does not fully implement RFC 6013.
    
    As a nice side-effect, removing TCPCT increases TCP's performance for
    very short flows:
    
    Doing an apache-benchmark with -c 100 -n 100000, sending HTTP-requests
    for files of 1KB size.
    
    before this patch:
            average (among 7 runs) of 20845.5 Requests/Second
    after:
            average (among 7 runs) of 21403.6 Requests/Second
    
    Signed-off-by: Christoph Paasch <christoph.paasch@uclouvain.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index a51dbd17c2de..9069e65c1c56 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -27,19 +27,13 @@ struct sk_buff;
 struct dst_entry;
 struct proto;
 
-/* empty to "strongly type" an otherwise void parameter.
- */
-struct request_values {
-};
-
 struct request_sock_ops {
 	int		family;
 	int		obj_size;
 	struct kmem_cache	*slab;
 	char		*slab_name;
 	int		(*rtx_syn_ack)(struct sock *sk,
-				       struct request_sock *req,
-				       struct request_values *rvp);
+				       struct request_sock *req);
 	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,
 				    struct request_sock *req);
 	void		(*send_reset)(struct sock *sk,

commit e6c022a4fa2d2d9ca9d0a7ac3b05ad988f39fc30
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Oct 27 23:16:46 2012 +0000

    tcp: better retrans tracking for defer-accept
    
    For passive TCP connections using TCP_DEFER_ACCEPT facility,
    we incorrectly increment req->retrans each time timeout triggers
    while no SYNACK is sent.
    
    SYNACK are not sent for TCP_DEFER_ACCEPT that were established (for
    which we received the ACK from client). Only the last SYNACK is sent
    so that we can receive again an ACK from client, to move the req into
    accept queue. We plan to change this later to avoid the useless
    retransmit (and potential problem as this SYNACK could be lost)
    
    TCP_INFO later gives wrong information to user, claiming imaginary
    retransmits.
    
    Decouple req->retrans field into two independent fields :
    
    num_retrans : number of retransmit
    num_timeout : number of timeouts
    
    num_timeout is the counter that is incremented at each timeout,
    regardless of actual SYNACK being sent or not, and used to
    compute the exponential timeout.
    
    Introduce inet_rtx_syn_ack() helper to increment num_retrans
    only if ->rtx_syn_ack() succeeded.
    
    Use inet_rtx_syn_ack() from tcp_check_req() to increment num_retrans
    when we re-send a SYNACK in answer to a (retransmitted) SYN.
    Prior to this patch, we were not counting these retransmits.
    
    Change tcp_v[46]_rtx_synack() to increment TCP_MIB_RETRANSSEGS
    only if a synack packet was successfully queued.
    
    Reported-by: Yuchung Cheng <ycheng@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Julian Anastasov <ja@ssi.bg>
    Cc: Vijay Subramanian <subramanian.vijay@gmail.com>
    Cc: Elliott Hughes <enh@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b01d8dd9ee7c..a51dbd17c2de 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -49,13 +49,16 @@ struct request_sock_ops {
 					   struct request_sock *req);
 };
 
+extern int inet_rtx_syn_ack(struct sock *parent, struct request_sock *req);
+
 /* struct request_sock - mini sock to represent a connection request
  */
 struct request_sock {
 	struct request_sock		*dl_next; /* Must be first member! */
 	u16				mss;
-	u8				retrans;
-	u8				cookie_ts; /* syncookie: encode tcpopts in timestamp */
+	u8				num_retrans; /* number of retransmits */
+	u8				cookie_ts:1; /* syncookie: encode tcpopts in timestamp */
+	u8				num_timeout:7; /* number of timeouts */
 	/* The following two fields can be easily recomputed I think -AK */
 	u32				window_clamp; /* window clamp at creation time */
 	u32				rcv_wnd;	  /* rcv_wnd offered first time */
@@ -231,7 +234,7 @@ static inline int reqsk_queue_removed(struct request_sock_queue *queue,
 {
 	struct listen_sock *lopt = queue->listen_opt;
 
-	if (req->retrans == 0)
+	if (req->num_timeout == 0)
 		--lopt->qlen_young;
 
 	return --lopt->qlen;
@@ -269,7 +272,8 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	struct listen_sock *lopt = queue->listen_opt;
 
 	req->expires = jiffies + timeout;
-	req->retrans = 0;
+	req->num_retrans = 0;
+	req->num_timeout = 0;
 	req->sk = NULL;
 	req->dl_next = lopt->syn_table[hash];
 

commit 8336886f786fdacbc19b719c1f7ea91eb70706d4
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 31 12:29:12 2012 +0000

    tcp: TCP Fast Open Server - support TFO listeners
    
    This patch builds on top of the previous patch to add the support
    for TFO listeners. This includes -
    
    1. allocating, properly initializing, and managing the per listener
    fastopen_queue structure when TFO is enabled
    
    2. changes to the inet_csk_accept code to support TFO. E.g., the
    request_sock can no longer be freed upon accept(), not until 3WHS
    finishes
    
    3. allowing a TCP_SYN_RECV socket to properly poll() and sendmsg()
    if it's a TFO socket
    
    4. properly closing a TFO listener, and a TFO socket before 3WHS
    finishes
    
    5. supporting TCP_FASTOPEN socket option
    
    6. modifying tcp_check_req() to use to check a TFO socket as well
    as request_sock
    
    7. supporting TCP's TFO cookie option
    
    8. adding a new SYN-ACK retransmit handler to use the timer directly
    off the TFO socket rather than the listener socket. Note that TFO
    server side will not retransmit anything other than SYN-ACK until
    the 3WHS is completed.
    
    The patch also contains an important function
    "reqsk_fastopen_remove()" to manage the somewhat complex relation
    between a listener, its request_sock, and the corresponding child
    socket. See the comment above the function for the detail.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c3cdd6c9f448..b01d8dd9ee7c 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -226,19 +226,6 @@ static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue
 	return req;
 }
 
-static inline struct sock *reqsk_queue_get_child(struct request_sock_queue *queue,
-						 struct sock *parent)
-{
-	struct request_sock *req = reqsk_queue_remove(queue);
-	struct sock *child = req->sk;
-
-	WARN_ON(child == NULL);
-
-	sk_acceptq_removed(parent);
-	__reqsk_free(req);
-	return child;
-}
-
 static inline int reqsk_queue_removed(struct request_sock_queue *queue,
 				      struct request_sock *req)
 {

commit 1046716368979dee857a2b8a91c4a8833f21b9cb
Author: Jerry Chu <hkchu@google.com>
Date:   Fri Aug 31 12:29:11 2012 +0000

    tcp: TCP Fast Open Server - header & support functions
    
    This patch adds all the necessary data structure and support
    functions to implement TFO server side. It also documents a number
    of flags for the sysctl_tcp_fastopen knob, and adds a few Linux
    extension MIBs.
    
    In addition, it includes the following:
    
    1. a new TCP_FASTOPEN socket option an application must call to
    supply a max backlog allowed in order to enable TFO on its listener.
    
    2. A number of key data structures:
    "fastopen_rsk" in tcp_sock - for a big socket to access its
    request_sock for retransmission and ack processing purpose. It is
    non-NULL iff 3WHS not completed.
    
    "fastopenq" in request_sock_queue - points to a per Fast Open
    listener data structure "fastopen_queue" to keep track of qlen (# of
    outstanding Fast Open requests) and max_qlen, among other things.
    
    "listener" in tcp_request_sock - to point to the original listener
    for book-keeping purpose, i.e., to maintain qlen against max_qlen
    as part of defense against IP spoofing attack.
    
    3. various data structure and functions, many in tcp_fastopen.c, to
    support server side Fast Open cookie operations, including
    /proc/sys/net/ipv4/tcp_fastopen_key to allow manual rekeying.
    
    Signed-off-by: H.K. Jerry Chu <hkchu@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 4c0766e201e3..c3cdd6c9f448 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -106,6 +106,34 @@ struct listen_sock {
 	struct request_sock	*syn_table[0];
 };
 
+/*
+ * For a TCP Fast Open listener -
+ *	lock - protects the access to all the reqsk, which is co-owned by
+ *		the listener and the child socket.
+ *	qlen - pending TFO requests (still in TCP_SYN_RECV).
+ *	max_qlen - max TFO reqs allowed before TFO is disabled.
+ *
+ *	XXX (TFO) - ideally these fields can be made as part of "listen_sock"
+ *	structure above. But there is some implementation difficulty due to
+ *	listen_sock being part of request_sock_queue hence will be freed when
+ *	a listener is stopped. But TFO related fields may continue to be
+ *	accessed even after a listener is closed, until its sk_refcnt drops
+ *	to 0 implying no more outstanding TFO reqs. One solution is to keep
+ *	listen_opt around until	sk_refcnt drops to 0. But there is some other
+ *	complexity that needs to be resolved. E.g., a listener can be disabled
+ *	temporarily through shutdown()->tcp_disconnect(), and re-enabled later.
+ */
+struct fastopen_queue {
+	struct request_sock	*rskq_rst_head; /* Keep track of past TFO */
+	struct request_sock	*rskq_rst_tail; /* requests that caused RST.
+						 * This is part of the defense
+						 * against spoofing attack.
+						 */
+	spinlock_t	lock;
+	int		qlen;		/* # of pending (TCP_SYN_RECV) reqs */
+	int		max_qlen;	/* != 0 iff TFO is currently enabled */
+};
+
 /** struct request_sock_queue - queue of request_socks
  *
  * @rskq_accept_head - FIFO head of established children
@@ -129,6 +157,12 @@ struct request_sock_queue {
 	u8			rskq_defer_accept;
 	/* 3 bytes hole, try to pack */
 	struct listen_sock	*listen_opt;
+	struct fastopen_queue	*fastopenq; /* This is non-NULL iff TFO has been
+					     * enabled on this listener. Check
+					     * max_qlen != 0 in fastopen_queue
+					     * to determine if TFO is enabled
+					     * right at this moment.
+					     */
 };
 
 extern int reqsk_queue_alloc(struct request_sock_queue *queue,
@@ -136,6 +170,8 @@ extern int reqsk_queue_alloc(struct request_sock_queue *queue,
 
 extern void __reqsk_queue_destroy(struct request_sock_queue *queue);
 extern void reqsk_queue_destroy(struct request_sock_queue *queue);
+extern void reqsk_fastopen_remove(struct sock *sk,
+				  struct request_sock *req, bool reset);
 
 static inline struct request_sock *
 	reqsk_queue_yank_acceptq(struct request_sock_queue *queue)

commit 946cedccbd7387488d2cee5da92cdfeb28d2e670
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Aug 30 03:21:44 2011 +0000

    tcp: Change possible SYN flooding messages
    
    "Possible SYN flooding on port xxxx " messages can fill logs on servers.
    
    Change logic to log the message only once per listener, and add two new
    SNMP counters to track :
    
    TCPReqQFullDoCookies : number of times a SYNCOOKIE was replied to client
    
    TCPReqQFullDrop : number of times a SYN request was dropped because
    syncookies were not enabled.
    
    Based on a prior patch from Tom Herbert, and suggestions from David.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Tom Herbert <therbert@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 99e6e19b57c2..4c0766e201e3 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -96,7 +96,8 @@ extern int sysctl_max_syn_backlog;
  */
 struct listen_sock {
 	u8			max_qlen_log;
-	/* 3 bytes hole, try to use */
+	u8			synflood_warned;
+	/* 2 bytes hole, try to use */
 	int			qlen;
 	int			qlen_young;
 	int			clock_hand;

commit 72659ecce68588b74f6c46862c2b4cec137d7a5a
Author: Octavian Purdila <opurdila@ixiacom.com>
Date:   Sun Jan 17 19:09:39 2010 -0800

    tcp: account SYN-ACK timeouts & retransmissions
    
    Currently we don't increment SYN-ACK timeouts & retransmissions
    although we do increment the same stats for SYN. We seem to have lost
    the SYN-ACK accounting with the introduction of tcp_syn_recv_timer
    (commit 2248761e in the netdev-vger-cvs tree).
    
    This patch fixes this issue. In the process we also rename the v4/v6
    syn/ack retransmit functions for clarity. We also add a new
    request_socket operations (syn_ack_timeout) so we can keep code in
    inet_connection_sock.c protocol agnostic.
    
    Signed-off-by: Octavian Purdila <opurdila@ixiacom.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c9b50ebd9ce9..99e6e19b57c2 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -45,6 +45,8 @@ struct request_sock_ops {
 	void		(*send_reset)(struct sock *sk,
 				      struct sk_buff *skb);
 	void		(*destructor)(struct request_sock *req);
+	void		(*syn_ack_timeout)(struct sock *sk,
+					   struct request_sock *req);
 };
 
 /* struct request_sock - mini sock to represent a connection request

commit e6b4d11367519bc71729c09d05a126b133c755be
Author: William Allen Simpson <william.allen.simpson@gmail.com>
Date:   Wed Dec 2 18:07:39 2009 +0000

    TCPCT part 1a: add request_values parameter for sending SYNACK
    
    Add optional function parameters associated with sending SYNACK.
    These parameters are not needed after sending SYNACK, and are not
    used for retransmission.  Avoids extending struct tcp_request_sock,
    and avoids allocating kernel memory.
    
    Also affects DCCP as it uses common struct request_sock_ops,
    but this parameter is currently reserved for future use.
    
    Signed-off-by: William.Allen.Simpson@gmail.com
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c7190846e128..c9b50ebd9ce9 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -27,13 +27,19 @@ struct sk_buff;
 struct dst_entry;
 struct proto;
 
+/* empty to "strongly type" an otherwise void parameter.
+ */
+struct request_values {
+};
+
 struct request_sock_ops {
 	int		family;
 	int		obj_size;
 	struct kmem_cache	*slab;
 	char		*slab_name;
 	int		(*rtx_syn_ack)(struct sock *sk,
-				       struct request_sock *req);
+				       struct request_sock *req,
+				       struct request_values *rvp);
 	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,
 				    struct request_sock *req);
 	void		(*send_reset)(struct sock *sk,

commit 7e56b5d698707a9934833c47b24d78fb0bcaf764
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 21 16:45:22 2008 -0800

    net: Fix memory leak in the proto_register function
    
    If the slub allocator is used, kmem_cache_create() may merge two or more
    kmem_cache's into one but the cache name pointer is not updated and
    kmem_cache_name() is no longer guaranteed to return the pointer passed
    to the former function. This patch stores the kmalloc'ed pointers in the
    corresponding request_sock_ops and timewait_sock_ops structures.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index cac811e51f6d..c7190846e128 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -31,6 +31,7 @@ struct request_sock_ops {
 	int		family;
 	int		obj_size;
 	struct kmem_cache	*slab;
+	char		*slab_name;
 	int		(*rtx_syn_ack)(struct sock *sk,
 				       struct request_sock *req);
 	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,

commit 6edafaaf6f5e70ef1e620ff01bd6bacebe1e0718
Author: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
Date:   Wed Aug 6 23:50:04 2008 -0700

    tcp: Fix kernel panic when calling tcp_v(4/6)_md5_do_lookup
    
    If the following packet flow happen, kernel will panic.
    MathineA                        MathineB
                    SYN
            ---------------------->
                    SYN+ACK
            <----------------------
                    ACK(bad seq)
            ---------------------->
    When a bad seq ACK is received, tcp_v4_md5_do_lookup(skb->sk, ip_hdr(skb)->daddr))
    is finally called by tcp_v4_reqsk_send_ack(), but the first parameter(skb->sk) is
    NULL at that moment, so kernel panic happens.
    This patch fixes this bug.
    
    OOPS output is as following:
    [  302.812793] IP: [<c05cfaa6>] tcp_v4_md5_do_lookup+0x12/0x42
    [  302.817075] Oops: 0000 [#1] SMP
    [  302.819815] Modules linked in: ipv6 loop dm_multipath rtc_cmos rtc_core rtc_lib pcspkr pcnet32 mii i2c_piix4 parport_pc i2c_core parport ac button ata_piix libata dm_mod mptspi mptscsih mptbase scsi_transport_spi sd_mod scsi_mod crc_t10dif ext3 jbd mbcache uhci_hcd ohci_hcd ehci_hcd [last unloaded: scsi_wait_scan]
    [  302.849946]
    [  302.851198] Pid: 0, comm: swapper Not tainted (2.6.27-rc1-guijf #5)
    [  302.855184] EIP: 0060:[<c05cfaa6>] EFLAGS: 00010296 CPU: 0
    [  302.858296] EIP is at tcp_v4_md5_do_lookup+0x12/0x42
    [  302.861027] EAX: 0000001e EBX: 00000000 ECX: 00000046 EDX: 00000046
    [  302.864867] ESI: ceb69e00 EDI: 1467a8c0 EBP: cf75f180 ESP: c0792e54
    [  302.868333]  DS: 007b ES: 007b FS: 00d8 GS: 0000 SS: 0068
    [  302.871287] Process swapper (pid: 0, ti=c0792000 task=c0712340 task.ti=c0746000)
    [  302.875592] Stack: c06f413a 00000000 cf75f180 ceb69e00 00000000 c05d0d86 000016d0 ceac5400
    [  302.883275]        c05d28f8 000016d0 ceb69e00 ceb69e20 681bf6e3 00001000 00000000 0a67a8c0
    [  302.890971]        ceac5400 c04250a3 c06f413a c0792eb0 c0792edc cf59a620 cf59a620 cf59a634
    [  302.900140] Call Trace:
    [  302.902392]  [<c05d0d86>] tcp_v4_reqsk_send_ack+0x17/0x35
    [  302.907060]  [<c05d28f8>] tcp_check_req+0x156/0x372
    [  302.910082]  [<c04250a3>] printk+0x14/0x18
    [  302.912868]  [<c05d0aa1>] tcp_v4_do_rcv+0x1d3/0x2bf
    [  302.917423]  [<c05d26be>] tcp_v4_rcv+0x563/0x5b9
    [  302.920453]  [<c05bb20f>] ip_local_deliver_finish+0xe8/0x183
    [  302.923865]  [<c05bb10a>] ip_rcv_finish+0x286/0x2a3
    [  302.928569]  [<c059e438>] dev_alloc_skb+0x11/0x25
    [  302.931563]  [<c05a211f>] netif_receive_skb+0x2d6/0x33a
    [  302.934914]  [<d0917941>] pcnet32_poll+0x333/0x680 [pcnet32]
    [  302.938735]  [<c05a3b48>] net_rx_action+0x5c/0xfe
    [  302.941792]  [<c042856b>] __do_softirq+0x5d/0xc1
    [  302.944788]  [<c042850e>] __do_softirq+0x0/0xc1
    [  302.948999]  [<c040564b>] do_softirq+0x55/0x88
    [  302.951870]  [<c04501b1>] handle_fasteoi_irq+0x0/0xa4
    [  302.954986]  [<c04284da>] irq_exit+0x35/0x69
    [  302.959081]  [<c0405717>] do_IRQ+0x99/0xae
    [  302.961896]  [<c040422b>] common_interrupt+0x23/0x28
    [  302.966279]  [<c040819d>] default_idle+0x2a/0x3d
    [  302.969212]  [<c0402552>] cpu_idle+0xb2/0xd2
    [  302.972169]  =======================
    [  302.974274] Code: fc ff 84 d2 0f 84 df fd ff ff e9 34 fe ff ff 83 c4 0c 5b 5e 5f 5d c3 90 90 57 89 d7 56 53 89 c3 50 68 3a 41 6f c0 e8 e9 55 e5 ff <8b> 93 9c 04 00 00 58 85 d2 59 74 1e 8b 72 10 31 db 31 c9 85 f6
    [  303.011610] EIP: [<c05cfaa6>] tcp_v4_md5_do_lookup+0x12/0x42 SS:ESP 0068:c0792e54
    [  303.018360] Kernel panic - not syncing: Fatal exception in interrupt
    
    Signed-off-by: Gui Jianfeng <guijianfeng@cn.fujitsu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 8d6e991ef4df..cac811e51f6d 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -33,7 +33,7 @@ struct request_sock_ops {
 	struct kmem_cache	*slab;
 	int		(*rtx_syn_ack)(struct sock *sk,
 				       struct request_sock *req);
-	void		(*send_ack)(struct sk_buff *skb,
+	void		(*send_ack)(struct sock *sk, struct sk_buff *skb,
 				    struct request_sock *req);
 	void		(*send_reset)(struct sock *sk,
 				      struct sk_buff *skb);

commit 547b792cac0a038b9dbf958d3c120df3740b5572
Author: Ilpo Jrvinen <ilpo.jarvinen@helsinki.fi>
Date:   Fri Jul 25 21:43:18 2008 -0700

    net: convert BUG_TRAP to generic WARN_ON
    
    Removes legacy reinvent-the-wheel type thing. The generic
    machinery integrates much better to automated debugging aids
    such as kerneloops.org (and others), and is unambiguous due to
    better naming. Non-intuively BUG_TRAP() is actually equal to
    WARN_ON() rather than BUG_ON() though some might actually be
    promoted to BUG_ON() but I left that to future.
    
    I could make at least one BUILD_BUG_ON conversion.
    
    Signed-off-by: Ilpo Jrvinen <ilpo.jarvinen@helsinki.fi>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 0c96e7bed5db..8d6e991ef4df 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -18,6 +18,7 @@
 #include <linux/slab.h>
 #include <linux/spinlock.h>
 #include <linux/types.h>
+#include <linux/bug.h>
 
 #include <net/sock.h>
 
@@ -170,7 +171,7 @@ static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue
 {
 	struct request_sock *req = queue->rskq_accept_head;
 
-	BUG_TRAP(req != NULL);
+	WARN_ON(req == NULL);
 
 	queue->rskq_accept_head = req->dl_next;
 	if (queue->rskq_accept_head == NULL)
@@ -185,7 +186,7 @@ static inline struct sock *reqsk_queue_get_child(struct request_sock_queue *queu
 	struct request_sock *req = reqsk_queue_remove(queue);
 	struct sock *child = req->sk;
 
-	BUG_TRAP(child != NULL);
+	WARN_ON(child == NULL);
 
 	sk_acceptq_removed(parent);
 	__reqsk_free(req);

commit ec0a196626bd12e0ba108d7daa6d95a4fb25c2c5
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 12 16:31:35 2008 -0700

    tcp: Revert 'process defer accept as established' changes.
    
    This reverts two changesets, ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
    ("[TCP]: TCP_DEFER_ACCEPT updates - process as established") and
    the follow-on bug fix 9ae27e0adbf471c7a6b80102e38e1d5a346b3b38
    ("tcp: Fix slab corruption with ipv6 and tcp6fuzz").
    
    This change causes several problems, first reported by Ingo Molnar
    as a distcc-over-loopback regression where connections were getting
    stuck.
    
    Ilpo Jrvinen first spotted the locking problems.  The new function
    added by this code, tcp_defer_accept_check(), only has the
    child socket locked, yet it is modifying state of the parent
    listening socket.
    
    Fixing that is non-trivial at best, because we can't simply just grab
    the parent listening socket lock at this point, because it would
    create an ABBA deadlock.  The normal ordering is parent listening
    socket --> child socket, but this code path would require the
    reverse lock ordering.
    
    Next is a problem noticed by Vitaliy Gusev, he noted:
    
    ----------------------------------------
    >--- a/net/ipv4/tcp_timer.c
    >+++ b/net/ipv4/tcp_timer.c
    >@@ -481,6 +481,11 @@ static void tcp_keepalive_timer (unsigned long data)
    >               goto death;
    >       }
    >
    >+      if (tp->defer_tcp_accept.request && sk->sk_state == TCP_ESTABLISHED) {
    >+              tcp_send_active_reset(sk, GFP_ATOMIC);
    >+              goto death;
    
    Here socket sk is not attached to listening socket's request queue. tcp_done()
    will not call inet_csk_destroy_sock() (and tcp_v4_destroy_sock() which should
    release this sk) as socket is not DEAD. Therefore socket sk will be lost for
    freeing.
    ----------------------------------------
    
    Finally, Alexey Kuznetsov argues that there might not even be any
    real value or advantage to these new semantics even if we fix all
    of the bugs:
    
    ----------------------------------------
    Hiding from accept() sockets with only out-of-order data only
    is the only thing which is impossible with old approach. Is this really
    so valuable? My opinion: no, this is nothing but a new loophole
    to consume memory without control.
    ----------------------------------------
    
    So revert this thing for now.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b220b5f624de..0c96e7bed5db 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -115,8 +115,8 @@ struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
 	rwlock_t		syn_wait_lock;
-	u16			rskq_defer_accept;
-	/* 2 bytes hole, try to pack */
+	u8			rskq_defer_accept;
+	/* 3 bytes hole, try to pack */
 	struct listen_sock	*listen_opt;
 };
 

commit 4dfc2817025965a2fc78a18c50f540736a6b5c24
Author: Florian Westphal <fw@strlen.de>
Date:   Thu Apr 10 03:12:40 2008 -0700

    [Syncookies]: Add support for TCP options via timestamps.
    
    Allow the use of SACK and window scaling when syncookies are used
    and the client supports tcp timestamps. Options are encoded into
    the timestamp sent in the syn-ack and restored from the timestamp
    echo when the ack is received.
    
    Based on earlier work by Glenn Griffin.
    This patch avoids increasing the size of structs by encoding TCP
    options into the least significant bits of the timestamp and
    by not using any 'timestamp offset'.
    
    The downside is that the timestamp sent in the packet after the synack
    will increase by several seconds.
    
    changes since v1:
     don't duplicate timestamp echo decoding function, put it into ipv4/syncookie.c
     and have ipv6/syncookies.c use it.
     Feedback from Glenn Griffin: fix line indented with spaces, kill redundant if ()
    
    Reviewed-by: Hagen Paul Pfeifer <hagen@jauu.net>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 0369f98e9f3a..b220b5f624de 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -45,7 +45,7 @@ struct request_sock {
 	struct request_sock		*dl_next; /* Must be first member! */
 	u16				mss;
 	u8				retrans;
-	u8				__pad;
+	u8				cookie_ts; /* syncookie: encode tcpopts in timestamp */
 	/* The following two fields can be easily recomputed I think -AK */
 	u32				window_clamp; /* window clamp at creation time */
 	u32				rcv_wnd;	  /* rcv_wnd offered first time */

commit ec3c0982a2dd1e671bad8e9d26c28dcba0039d87
Author: Patrick McManus <mcmanus@ducksong.com>
Date:   Fri Mar 21 16:33:01 2008 -0700

    [TCP]: TCP_DEFER_ACCEPT updates - process as established
    
    Change TCP_DEFER_ACCEPT implementation so that it transitions a
    connection to ESTABLISHED after handshake is complete instead of
    leaving it in SYN-RECV until some data arrvies. Place connection in
    accept queue when first data packet arrives from slow path.
    
    Benefits:
      - established connection is now reset if it never makes it
       to the accept queue
    
     - diagnostic state of established matches with the packet traces
       showing completed handshake
    
     - TCP_DEFER_ACCEPT timeouts are expressed in seconds and can now be
       enforced with reasonable accuracy instead of rounding up to next
       exponential back-off of syn-ack retry.
    
    Signed-off-by: Patrick McManus <mcmanus@ducksong.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 040780add355..0369f98e9f3a 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -115,8 +115,8 @@ struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
 	rwlock_t		syn_wait_lock;
-	u8			rskq_defer_accept;
-	/* 3 bytes hole, try to pack */
+	u16			rskq_defer_accept;
+	/* 2 bytes hole, try to pack */
 	struct listen_sock	*listen_opt;
 };
 

commit fd80eb942ad9761f241c9b287b3b9a342b20690d
Author: Denis V. Lunev <den@openvz.org>
Date:   Fri Feb 29 11:43:03 2008 -0800

    [INET]: Remove struct dst_entry *dst from request_sock_ops.rtx_syn_ack.
    
    It looks like dst parameter is used in this API due to historical
    reasons.  Actually, it is really used in the direct call to
    tcp_v4_send_synack only.  So, create a wrapper for tcp_v4_send_synack
    and remove dst from rtx_syn_ack.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index cff4608179c1..040780add355 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -31,8 +31,7 @@ struct request_sock_ops {
 	int		obj_size;
 	struct kmem_cache	*slab;
 	int		(*rtx_syn_ack)(struct sock *sk,
-				       struct request_sock *req,
-				       struct dst_entry *dst);
+				       struct request_sock *req);
 	void		(*send_ack)(struct sk_buff *skb,
 				    struct request_sock *req);
 	void		(*send_reset)(struct sock *sk,

commit dab6ba36888a12f3e3edff71eeef968fc159178a
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Nov 15 02:57:06 2007 -0800

    [INET]: Fix potential kfree on vmalloc-ed area of request_sock_queue
    
    The request_sock_queue's listen_opt is either vmalloc-ed or
    kmalloc-ed depending on the number of table entries. Thus it
    is expected to be handled properly on free, which is done in
    the reqsk_queue_destroy().
    
    However the error path in inet_csk_listen_start() calls
    the lite version of reqsk_queue_destroy, called
    __reqsk_queue_destroy, which calls the kfree unconditionally.
    
    Fix this and move the __reqsk_queue_destroy into a .c file as
    it looks too big to be inline.
    
    As David also noticed, this is an error recovery path only,
    so no locking is required and the lopt is known to be not NULL.
    
    reqsk_queue_yank_listen_sk is also now only used in
    net/core/request_sock.c so we should move it there too.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 7aed02ce2b65..cff4608179c1 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -124,23 +124,7 @@ struct request_sock_queue {
 extern int reqsk_queue_alloc(struct request_sock_queue *queue,
 			     unsigned int nr_table_entries);
 
-static inline struct listen_sock *reqsk_queue_yank_listen_sk(struct request_sock_queue *queue)
-{
-	struct listen_sock *lopt;
-
-	write_lock_bh(&queue->syn_wait_lock);
-	lopt = queue->listen_opt;
-	queue->listen_opt = NULL;
-	write_unlock_bh(&queue->syn_wait_lock);
-
-	return lopt;
-}
-
-static inline void __reqsk_queue_destroy(struct request_sock_queue *queue)
-{
-	kfree(reqsk_queue_yank_listen_sk(queue));
-}
-
+extern void __reqsk_queue_destroy(struct request_sock_queue *queue);
 extern void reqsk_queue_destroy(struct request_sock_queue *queue);
 
 static inline struct request_sock *

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 426f0fe774ef..7aed02ce2b65 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -29,7 +29,7 @@ struct proto;
 struct request_sock_ops {
 	int		family;
 	int		obj_size;
-	kmem_cache_t	*slab;
+	struct kmem_cache	*slab;
 	int		(*rtx_syn_ack)(struct sock *sk,
 				       struct request_sock *req,
 				       struct dst_entry *dst);

commit 54e6ecb23951b195d02433a741c7f7cb0b796c78
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:16 2006 -0800

    [PATCH] slab: remove SLAB_ATOMIC
    
    SLAB_ATOMIC is an alias of GFP_ATOMIC
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index e37baaf2080b..426f0fe774ef 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -60,7 +60,7 @@ struct request_sock {
 
 static inline struct request_sock *reqsk_alloc(const struct request_sock_ops *ops)
 {
-	struct request_sock *req = kmem_cache_alloc(ops->slab, SLAB_ATOMIC);
+	struct request_sock *req = kmem_cache_alloc(ops->slab, GFP_ATOMIC);
 
 	if (req != NULL)
 		req->rsk_ops = ops;

commit cfb6eeb4c860592edd123fdea908d23c6ad1c7dc
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Tue Nov 14 19:07:45 2006 -0800

    [TCP]: MD5 Signature Option (RFC2385) support.
    
    Based on implementation by Rick Payne.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b5b023e79e5f..e37baaf2080b 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -35,7 +35,8 @@ struct request_sock_ops {
 				       struct dst_entry *dst);
 	void		(*send_ack)(struct sk_buff *skb,
 				    struct request_sock *req);
-	void		(*send_reset)(struct sk_buff *skb);
+	void		(*send_reset)(struct sock *sk,
+				      struct sk_buff *skb);
 	void		(*destructor)(struct request_sock *req);
 };
 

commit 72a3effaf633bcae9034b7e176bdbd78d64a71db
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Nov 16 02:30:37 2006 -0800

    [NET]: Size listen hash tables using backlog hint
    
    We currently allocate a fixed size (TCP_SYNQ_HSIZE=512) slots hash table for
    each LISTEN socket, regardless of various parameters (listen backlog for
    example)
    
    On x86_64, this means order-1 allocations (might fail), even for 'small'
    sockets, expecting few connections. On the contrary, a huge server wanting a
    backlog of 50000 is slowed down a bit because of this fixed limit.
    
    This patch makes the sizing of listen hash table a dynamic parameter,
    depending of :
    - net.core.somaxconn tunable (default is 128)
    - net.ipv4.tcp_max_syn_backlog tunable (default : 256, 1024 or 128)
    - backlog value given by user application  (2nd parameter of listen())
    
    For large allocations (bigger than PAGE_SIZE), we use vmalloc() instead of
    kmalloc().
    
    We still limit memory allocation with the two existing tunables (somaxconn &
    tcp_max_syn_backlog). So for standard setups, this patch actually reduce RAM
    usage.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index f743a941a4f2..b5b023e79e5f 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -28,8 +28,8 @@ struct proto;
 
 struct request_sock_ops {
 	int		family;
-	kmem_cache_t	*slab;
 	int		obj_size;
+	kmem_cache_t	*slab;
 	int		(*rtx_syn_ack)(struct sock *sk,
 				       struct request_sock *req,
 				       struct dst_entry *dst);
@@ -51,13 +51,13 @@ struct request_sock {
 	u32				rcv_wnd;	  /* rcv_wnd offered first time */
 	u32				ts_recent;
 	unsigned long			expires;
-	struct request_sock_ops		*rsk_ops;
+	const struct request_sock_ops	*rsk_ops;
 	struct sock			*sk;
 	u32				secid;
 	u32				peer_secid;
 };
 
-static inline struct request_sock *reqsk_alloc(struct request_sock_ops *ops)
+static inline struct request_sock *reqsk_alloc(const struct request_sock_ops *ops)
 {
 	struct request_sock *req = kmem_cache_alloc(ops->slab, SLAB_ATOMIC);
 
@@ -121,7 +121,7 @@ struct request_sock_queue {
 };
 
 extern int reqsk_queue_alloc(struct request_sock_queue *queue,
-			     const int nr_table_entries);
+			     unsigned int nr_table_entries);
 
 static inline struct listen_sock *reqsk_queue_yank_listen_sk(struct request_sock_queue *queue)
 {

commit 6b877699c6f1efede4545bcecc367786a472eedb
Author: Venkat Yekkirala <vyekkirala@trustedcs.com>
Date:   Wed Nov 8 17:04:09 2006 -0600

    SELinux: Return correct context for SO_PEERSEC
    
    Fix SO_PEERSEC for tcp sockets to return the security context of
    the peer (as represented by the SA from the peer) as opposed to the
    SA used by the local/source socket.
    
    Signed-off-by: Venkat Yekkirala <vyekkirala@TrustedCS.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 8e165ca16bd8..f743a941a4f2 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -54,6 +54,7 @@ struct request_sock {
 	struct request_sock_ops		*rsk_ops;
 	struct sock			*sk;
 	u32				secid;
+	u32				peer_secid;
 };
 
 static inline struct request_sock *reqsk_alloc(struct request_sock_ops *ops)

commit 4237c75c0a35535d7f9f2bfeeb4b4df1e068a0bf
Author: Venkat Yekkirala <vyekkirala@TrustedCS.com>
Date:   Mon Jul 24 23:32:50 2006 -0700

    [MLSXFRM]: Auto-labeling of child sockets
    
    This automatically labels the TCP, Unix stream, and dccp child sockets
    as well as openreqs to be at the same MLS level as the peer. This will
    result in the selection of appropriately labeled IPSec Security
    Associations.
    
    This also uses the sock's sid (as opposed to the isec sid) in SELinux
    enforcement of secmark in rcv_skb and postroute_last hooks.
    
    Signed-off-by: Venkat Yekkirala <vyekkirala@TrustedCS.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index c5d7f920c352..8e165ca16bd8 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -53,6 +53,7 @@ struct request_sock {
 	unsigned long			expires;
 	struct request_sock_ops		*rsk_ops;
 	struct sock			*sk;
+	u32				secid;
 };
 
 static inline struct request_sock *reqsk_alloc(struct request_sock_ops *ops)

commit 3eb4801d7bde42b82f05137392a1ee0ece090bad
Author: Norbert Kiesel <nkiesel@tbdnetworks.com>
Date:   Sun Mar 26 17:39:55 2006 -0800

    [NET]: drop duplicate assignment in request_sock
    
    Just noticed that request_sock.[ch] contain a useless assignment of
    rskq_accept_head to itself.  I assume this is a typo and the 2nd one
    was supposed to be _tail.  However, setting _tail to NULL is not
    needed, so the patch below just drops the 2nd assignment.
    
    Signed-off-By: Norbert Kiesel <nkiesel@tbdnetworks.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 11641c9384f7..c5d7f920c352 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -145,7 +145,7 @@ static inline struct request_sock *
 {
 	struct request_sock *req = queue->rskq_accept_head;
 
-	queue->rskq_accept_head = queue->rskq_accept_head = NULL;
+	queue->rskq_accept_head = NULL;
 	return req;
 }
 

commit 8129765ac07c2455c927051e3a8b048b619b56ee
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Tue Dec 13 23:15:24 2005 -0800

    [IPV6]: Generalise tcp_v6_search_req & tcp_v6_synq_add
    
    More work is needed tho to introduce inet6_request_sock from
    tcp6_request_sock, in the same layout considerations as ipv6_pinfo in
    inet_sock, next changeset will do that.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b52cc52ffe39..11641c9384f7 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -244,7 +244,7 @@ static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
 
 static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 					u32 hash, struct request_sock *req,
-					unsigned timeout)
+					unsigned long timeout)
 {
 	struct listen_sock *lopt = queue->listen_opt;
 

commit a019d6fe2b9da68ea4ba6cf3c4e86fc1dbf554c3
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:15:09 2005 -0700

    [ICSK]: Move generalised functions from tcp to inet_connection_sock
    
    This also improves reqsk_queue_prune and renames it to
    inet_csk_reqsk_queue_prune, as it deals with both inet_connection_sock
    and inet_request_sock objects, not just with request_sock ones thus
    belonging to inet_request_sock.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 447d287a38fd..b52cc52ffe39 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -258,8 +258,4 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	write_unlock(&queue->syn_wait_lock);
 }
 
-extern void reqsk_queue_prune(struct request_sock_queue *queue, struct sock *parent,
-			      const unsigned long interval, const unsigned long timeout,
-			      const unsigned long max_rto, int max_retries);
-
 #endif /* _REQUEST_SOCK_H */

commit 295f7324ff8d9ea58b4d3ec93b1aaa1d80e048a9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:11:56 2005 -0700

    [ICSK]: Introduce reqsk_queue_prune from code in tcp_synack_timer
    
    With this we're very close to getting all of the current TCP
    refactorings in my dccp-2.6 tree merged, next changeset will export
    some functions needed by the current DCCP code and then dccp-2.6.git
    will be born!
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index b7c7eecbe64d..447d287a38fd 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -97,6 +97,7 @@ struct listen_sock {
  *
  * @rskq_accept_head - FIFO head of established children
  * @rskq_accept_tail - FIFO tail of established children
+ * @rskq_defer_accept - User waits for some data after accept()
  * @syn_wait_lock - serializer
  *
  * %syn_wait_lock is necessary only to avoid proc interface having to grab the main
@@ -112,6 +113,8 @@ struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
 	rwlock_t		syn_wait_lock;
+	u8			rskq_defer_accept;
+	/* 3 bytes hole, try to pack */
 	struct listen_sock	*listen_opt;
 };
 
@@ -255,4 +258,8 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 	write_unlock(&queue->syn_wait_lock);
 }
 
+extern void reqsk_queue_prune(struct request_sock_queue *queue, struct sock *parent,
+			      const unsigned long interval, const unsigned long timeout,
+			      const unsigned long max_rto, int max_retries);
+
 #endif /* _REQUEST_SOCK_H */

commit 463c84b97f24010a67cd871746d6a7e4c925a5f9
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 20:10:42 2005 -0700

    [NET]: Introduce inet_connection_sock
    
    This creates struct inet_connection_sock, moving members out of struct
    tcp_sock that are shareable with other INET connection oriented
    protocols, such as DCCP, that in my private tree already uses most of
    these members.
    
    The functions that operate on these members were renamed, using a
    inet_csk_ prefix while not being moved yet to a new file, so as to
    ease the review of these changes.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 334717bf9ef6..b7c7eecbe64d 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -224,17 +224,17 @@ static inline int reqsk_queue_added(struct request_sock_queue *queue)
 	return prev_qlen;
 }
 
-static inline int reqsk_queue_len(struct request_sock_queue *queue)
+static inline int reqsk_queue_len(const struct request_sock_queue *queue)
 {
 	return queue->listen_opt != NULL ? queue->listen_opt->qlen : 0;
 }
 
-static inline int reqsk_queue_len_young(struct request_sock_queue *queue)
+static inline int reqsk_queue_len_young(const struct request_sock_queue *queue)
 {
 	return queue->listen_opt->qlen_young;
 }
 
-static inline int reqsk_queue_is_full(struct request_sock_queue *queue)
+static inline int reqsk_queue_is_full(const struct request_sock_queue *queue)
 {
 	return queue->listen_opt->qlen >> queue->listen_opt->max_qlen_log;
 }

commit 83e3609eba3818f6e18b8bf9442195169ac306b7
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Tue Aug 9 19:33:31 2005 -0700

    [REQSK]: Move the syn_table destroy from tcp_listen_stop to reqsk_queue_destroy
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 72fd6f5e86b1..334717bf9ef6 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -89,6 +89,7 @@ struct listen_sock {
 	int			qlen_young;
 	int			clock_hand;
 	u32			hash_rnd;
+	u32			nr_table_entries;
 	struct request_sock	*syn_table[0];
 };
 
@@ -129,11 +130,13 @@ static inline struct listen_sock *reqsk_queue_yank_listen_sk(struct request_sock
 	return lopt;
 }
 
-static inline void reqsk_queue_destroy(struct request_sock_queue *queue)
+static inline void __reqsk_queue_destroy(struct request_sock_queue *queue)
 {
 	kfree(reqsk_queue_yank_listen_sk(queue));
 }
 
+extern void reqsk_queue_destroy(struct request_sock_queue *queue);
+
 static inline struct request_sock *
 	reqsk_queue_yank_acceptq(struct request_sock_queue *queue)
 {

commit 2ad69c55a282315e6119cf7fd744f26a925bdfd2
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:48:55 2005 -0700

    [NET] rename struct tcp_listen_opt to struct listen_sock
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 38943ed04e73..72fd6f5e86b1 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -78,11 +78,11 @@ static inline void reqsk_free(struct request_sock *req)
 
 extern int sysctl_max_syn_backlog;
 
-/** struct tcp_listen_opt - listen state
+/** struct listen_sock - listen state
  *
  * @max_qlen_log - log_2 of maximal queued SYNs/REQUESTs
  */
-struct tcp_listen_opt {
+struct listen_sock {
 	u8			max_qlen_log;
 	/* 3 bytes hole, try to use */
 	int			qlen;
@@ -111,15 +111,15 @@ struct request_sock_queue {
 	struct request_sock	*rskq_accept_head;
 	struct request_sock	*rskq_accept_tail;
 	rwlock_t		syn_wait_lock;
-	struct tcp_listen_opt	*listen_opt;
+	struct listen_sock	*listen_opt;
 };
 
 extern int reqsk_queue_alloc(struct request_sock_queue *queue,
 			     const int nr_table_entries);
 
-static inline struct tcp_listen_opt *reqsk_queue_yank_listen_sk(struct request_sock_queue *queue)
+static inline struct listen_sock *reqsk_queue_yank_listen_sk(struct request_sock_queue *queue)
 {
-	struct tcp_listen_opt *lopt;
+	struct listen_sock *lopt;
 
 	write_lock_bh(&queue->syn_wait_lock);
 	lopt = queue->listen_opt;
@@ -203,7 +203,7 @@ static inline struct sock *reqsk_queue_get_child(struct request_sock_queue *queu
 static inline int reqsk_queue_removed(struct request_sock_queue *queue,
 				      struct request_sock *req)
 {
-	struct tcp_listen_opt *lopt = queue->listen_opt;
+	struct listen_sock *lopt = queue->listen_opt;
 
 	if (req->retrans == 0)
 		--lopt->qlen_young;
@@ -213,7 +213,7 @@ static inline int reqsk_queue_removed(struct request_sock_queue *queue,
 
 static inline int reqsk_queue_added(struct request_sock_queue *queue)
 {
-	struct tcp_listen_opt *lopt = queue->listen_opt;
+	struct listen_sock *lopt = queue->listen_opt;
 	const int prev_qlen = lopt->qlen;
 
 	lopt->qlen_young++;
@@ -240,7 +240,7 @@ static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
 					u32 hash, struct request_sock *req,
 					unsigned timeout)
 {
-	struct tcp_listen_opt *lopt = queue->listen_opt;
+	struct listen_sock *lopt = queue->listen_opt;
 
 	req->expires = jiffies + timeout;
 	req->retrans = 0;

commit 0e87506fcc734647c7b2497eee4eb81e785c857a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:59 2005 -0700

    [NET] Generalise tcp_listen_opt
    
    This chunks out the accept_queue and tcp_listen_opt code and moves
    them to net/core/request_sock.c and include/net/request_sock.h, to
    make it useful for other transport protocols, DCCP being the first one
    to use it.
    
    Next patches will rename tcp_listen_opt to accept_sock and remove the
    inline tcp functions that just call a reqsk_queue_ function.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 08a8fd1d1610..38943ed04e73 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -16,7 +16,9 @@
 #define _REQUEST_SOCK_H
 
 #include <linux/slab.h>
+#include <linux/spinlock.h>
 #include <linux/types.h>
+
 #include <net/sock.h>
 
 struct request_sock;
@@ -74,4 +76,180 @@ static inline void reqsk_free(struct request_sock *req)
 	__reqsk_free(req);
 }
 
+extern int sysctl_max_syn_backlog;
+
+/** struct tcp_listen_opt - listen state
+ *
+ * @max_qlen_log - log_2 of maximal queued SYNs/REQUESTs
+ */
+struct tcp_listen_opt {
+	u8			max_qlen_log;
+	/* 3 bytes hole, try to use */
+	int			qlen;
+	int			qlen_young;
+	int			clock_hand;
+	u32			hash_rnd;
+	struct request_sock	*syn_table[0];
+};
+
+/** struct request_sock_queue - queue of request_socks
+ *
+ * @rskq_accept_head - FIFO head of established children
+ * @rskq_accept_tail - FIFO tail of established children
+ * @syn_wait_lock - serializer
+ *
+ * %syn_wait_lock is necessary only to avoid proc interface having to grab the main
+ * lock sock while browsing the listening hash (otherwise it's deadlock prone).
+ *
+ * This lock is acquired in read mode only from listening_get_next() seq_file
+ * op and it's acquired in write mode _only_ from code that is actively
+ * changing rskq_accept_head. All readers that are holding the master sock lock
+ * don't need to grab this lock in read mode too as rskq_accept_head. writes
+ * are always protected from the main sock lock.
+ */
+struct request_sock_queue {
+	struct request_sock	*rskq_accept_head;
+	struct request_sock	*rskq_accept_tail;
+	rwlock_t		syn_wait_lock;
+	struct tcp_listen_opt	*listen_opt;
+};
+
+extern int reqsk_queue_alloc(struct request_sock_queue *queue,
+			     const int nr_table_entries);
+
+static inline struct tcp_listen_opt *reqsk_queue_yank_listen_sk(struct request_sock_queue *queue)
+{
+	struct tcp_listen_opt *lopt;
+
+	write_lock_bh(&queue->syn_wait_lock);
+	lopt = queue->listen_opt;
+	queue->listen_opt = NULL;
+	write_unlock_bh(&queue->syn_wait_lock);
+
+	return lopt;
+}
+
+static inline void reqsk_queue_destroy(struct request_sock_queue *queue)
+{
+	kfree(reqsk_queue_yank_listen_sk(queue));
+}
+
+static inline struct request_sock *
+	reqsk_queue_yank_acceptq(struct request_sock_queue *queue)
+{
+	struct request_sock *req = queue->rskq_accept_head;
+
+	queue->rskq_accept_head = queue->rskq_accept_head = NULL;
+	return req;
+}
+
+static inline int reqsk_queue_empty(struct request_sock_queue *queue)
+{
+	return queue->rskq_accept_head == NULL;
+}
+
+static inline void reqsk_queue_unlink(struct request_sock_queue *queue,
+				      struct request_sock *req,
+				      struct request_sock **prev_req)
+{
+	write_lock(&queue->syn_wait_lock);
+	*prev_req = req->dl_next;
+	write_unlock(&queue->syn_wait_lock);
+}
+
+static inline void reqsk_queue_add(struct request_sock_queue *queue,
+				   struct request_sock *req,
+				   struct sock *parent,
+				   struct sock *child)
+{
+	req->sk = child;
+	sk_acceptq_added(parent);
+
+	if (queue->rskq_accept_head == NULL)
+		queue->rskq_accept_head = req;
+	else
+		queue->rskq_accept_tail->dl_next = req;
+
+	queue->rskq_accept_tail = req;
+	req->dl_next = NULL;
+}
+
+static inline struct request_sock *reqsk_queue_remove(struct request_sock_queue *queue)
+{
+	struct request_sock *req = queue->rskq_accept_head;
+
+	BUG_TRAP(req != NULL);
+
+	queue->rskq_accept_head = req->dl_next;
+	if (queue->rskq_accept_head == NULL)
+		queue->rskq_accept_tail = NULL;
+
+	return req;
+}
+
+static inline struct sock *reqsk_queue_get_child(struct request_sock_queue *queue,
+						 struct sock *parent)
+{
+	struct request_sock *req = reqsk_queue_remove(queue);
+	struct sock *child = req->sk;
+
+	BUG_TRAP(child != NULL);
+
+	sk_acceptq_removed(parent);
+	__reqsk_free(req);
+	return child;
+}
+
+static inline int reqsk_queue_removed(struct request_sock_queue *queue,
+				      struct request_sock *req)
+{
+	struct tcp_listen_opt *lopt = queue->listen_opt;
+
+	if (req->retrans == 0)
+		--lopt->qlen_young;
+
+	return --lopt->qlen;
+}
+
+static inline int reqsk_queue_added(struct request_sock_queue *queue)
+{
+	struct tcp_listen_opt *lopt = queue->listen_opt;
+	const int prev_qlen = lopt->qlen;
+
+	lopt->qlen_young++;
+	lopt->qlen++;
+	return prev_qlen;
+}
+
+static inline int reqsk_queue_len(struct request_sock_queue *queue)
+{
+	return queue->listen_opt != NULL ? queue->listen_opt->qlen : 0;
+}
+
+static inline int reqsk_queue_len_young(struct request_sock_queue *queue)
+{
+	return queue->listen_opt->qlen_young;
+}
+
+static inline int reqsk_queue_is_full(struct request_sock_queue *queue)
+{
+	return queue->listen_opt->qlen >> queue->listen_opt->max_qlen_log;
+}
+
+static inline void reqsk_queue_hash_req(struct request_sock_queue *queue,
+					u32 hash, struct request_sock *req,
+					unsigned timeout)
+{
+	struct tcp_listen_opt *lopt = queue->listen_opt;
+
+	req->expires = jiffies + timeout;
+	req->retrans = 0;
+	req->sk = NULL;
+	req->dl_next = lopt->syn_table[hash];
+
+	write_lock(&queue->syn_wait_lock);
+	lopt->syn_table[hash] = req;
+	write_unlock(&queue->syn_wait_lock);
+}
+
 #endif /* _REQUEST_SOCK_H */

commit 60236fdd08b2169045a3bbfc5ffe1576e6c3c17b
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:47:21 2005 -0700

    [NET] Rename open_request to request_sock
    
    Ok, this one just renames some stuff to have a better namespace and to
    dissassociate it from TCP:
    
    struct open_request  -> struct request_sock
    tcp_openreq_alloc    -> reqsk_alloc
    tcp_openreq_free     -> reqsk_free
    tcp_openreq_fastfree -> __reqsk_free
    
    With this most of the infrastructure closely resembles a struct
    sock methods subset.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
index 9502f5587931..08a8fd1d1610 100644
--- a/include/net/request_sock.h
+++ b/include/net/request_sock.h
@@ -19,28 +19,28 @@
 #include <linux/types.h>
 #include <net/sock.h>
 
-struct open_request;
+struct request_sock;
 struct sk_buff;
 struct dst_entry;
 struct proto;
 
-struct or_calltable {
+struct request_sock_ops {
 	int		family;
 	kmem_cache_t	*slab;
 	int		obj_size;
 	int		(*rtx_syn_ack)(struct sock *sk,
-				       struct open_request *req,
+				       struct request_sock *req,
 				       struct dst_entry *dst);
 	void		(*send_ack)(struct sk_buff *skb,
-				    struct open_request *req);
+				    struct request_sock *req);
 	void		(*send_reset)(struct sk_buff *skb);
-	void		(*destructor)(struct open_request *req);
+	void		(*destructor)(struct request_sock *req);
 };
 
-/* struct open_request - mini sock to represent a connection request
+/* struct request_sock - mini sock to represent a connection request
  */
-struct open_request {
-	struct open_request		*dl_next; /* Must be first member! */
+struct request_sock {
+	struct request_sock		*dl_next; /* Must be first member! */
 	u16				mss;
 	u8				retrans;
 	u8				__pad;
@@ -49,29 +49,29 @@ struct open_request {
 	u32				rcv_wnd;	  /* rcv_wnd offered first time */
 	u32				ts_recent;
 	unsigned long			expires;
-	struct or_calltable		*class;
+	struct request_sock_ops		*rsk_ops;
 	struct sock			*sk;
 };
 
-static inline struct open_request *tcp_openreq_alloc(struct or_calltable *class)
+static inline struct request_sock *reqsk_alloc(struct request_sock_ops *ops)
 {
-	struct open_request *req = kmem_cache_alloc(class->slab, SLAB_ATOMIC);
+	struct request_sock *req = kmem_cache_alloc(ops->slab, SLAB_ATOMIC);
 
 	if (req != NULL)
-		req->class = class;
+		req->rsk_ops = ops;
 
 	return req;
 }
 
-static inline void tcp_openreq_fastfree(struct open_request *req)
+static inline void __reqsk_free(struct request_sock *req)
 {
-	kmem_cache_free(req->class->slab, req);
+	kmem_cache_free(req->rsk_ops->slab, req);
 }
 
-static inline void tcp_openreq_free(struct open_request *req)
+static inline void reqsk_free(struct request_sock *req)
 {
-	req->class->destructor(req);
-	tcp_openreq_fastfree(req);
+	req->rsk_ops->destructor(req);
+	__reqsk_free(req);
 }
 
 #endif /* _REQUEST_SOCK_H */

commit 2e6599cb899ba4b133f42cbf9d2b1883d2dc583a
Author: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
Date:   Sat Jun 18 22:46:52 2005 -0700

    [NET] Generalise TCP's struct open_request minisock infrastructure
    
    Kept this first changeset minimal, without changing existing names to
    ease peer review.
    
    Basicaly tcp_openreq_alloc now receives the or_calltable, that in turn
    has two new members:
    
    ->slab, that replaces tcp_openreq_cachep
    ->obj_size, to inform the size of the openreq descendant for
      a specific protocol
    
    The protocol specific fields in struct open_request were moved to a
    class hierarchy, with the things that are common to all connection
    oriented PF_INET protocols in struct inet_request_sock, the TCP ones
    in tcp_request_sock, that is an inet_request_sock, that is an
    open_request.
    
    I.e. this uses the same approach used for the struct sock class
    hierarchy, with sk_prot indicating if the protocol wants to use the
    open_request infrastructure by filling in sk_prot->rsk_prot with an
    or_calltable.
    
    Results? Performance is improved and TCP v4 now uses only 64 bytes per
    open request minisock, down from 96 without this patch :-)
    
    Next changeset will rename some of the structs, fields and functions
    mentioned above, struct or_calltable is way unclear, better name it
    struct request_sock_ops, s/struct open_request/struct request_sock/g,
    etc.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/request_sock.h b/include/net/request_sock.h
new file mode 100644
index 000000000000..9502f5587931
--- /dev/null
+++ b/include/net/request_sock.h
@@ -0,0 +1,77 @@
+/*
+ * NET		Generic infrastructure for Network protocols.
+ *
+ *		Definitions for request_sock 
+ *
+ * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
+ *
+ * 		From code originally in include/net/tcp.h
+ *
+ *		This program is free software; you can redistribute it and/or
+ *		modify it under the terms of the GNU General Public License
+ *		as published by the Free Software Foundation; either version
+ *		2 of the License, or (at your option) any later version.
+ */
+#ifndef _REQUEST_SOCK_H
+#define _REQUEST_SOCK_H
+
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <net/sock.h>
+
+struct open_request;
+struct sk_buff;
+struct dst_entry;
+struct proto;
+
+struct or_calltable {
+	int		family;
+	kmem_cache_t	*slab;
+	int		obj_size;
+	int		(*rtx_syn_ack)(struct sock *sk,
+				       struct open_request *req,
+				       struct dst_entry *dst);
+	void		(*send_ack)(struct sk_buff *skb,
+				    struct open_request *req);
+	void		(*send_reset)(struct sk_buff *skb);
+	void		(*destructor)(struct open_request *req);
+};
+
+/* struct open_request - mini sock to represent a connection request
+ */
+struct open_request {
+	struct open_request		*dl_next; /* Must be first member! */
+	u16				mss;
+	u8				retrans;
+	u8				__pad;
+	/* The following two fields can be easily recomputed I think -AK */
+	u32				window_clamp; /* window clamp at creation time */
+	u32				rcv_wnd;	  /* rcv_wnd offered first time */
+	u32				ts_recent;
+	unsigned long			expires;
+	struct or_calltable		*class;
+	struct sock			*sk;
+};
+
+static inline struct open_request *tcp_openreq_alloc(struct or_calltable *class)
+{
+	struct open_request *req = kmem_cache_alloc(class->slab, SLAB_ATOMIC);
+
+	if (req != NULL)
+		req->class = class;
+
+	return req;
+}
+
+static inline void tcp_openreq_fastfree(struct open_request *req)
+{
+	kmem_cache_free(req->class->slab, req);
+}
+
+static inline void tcp_openreq_free(struct open_request *req)
+{
+	req->class->destructor(req);
+	tcp_openreq_fastfree(req);
+}
+
+#endif /* _REQUEST_SOCK_H */
