commit 2603c29e6c12135c1ef248ddaccf91de32567454
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Fri Feb 28 18:11:02 2020 -0600

    net: sock_reuseport: Replace zero-length array with flexible-array member
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 3ecaa15d1850..505f1e18e9bf 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -24,7 +24,7 @@ struct sock_reuseport {
 	unsigned int		bind_inany:1;
 	unsigned int		has_conns:1;
 	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
-	struct sock		*socks[0];	/* array of sock pointers */
+	struct sock		*socks[];	/* array of sock pointers */
 };
 
 extern int reuseport_alloc(struct sock *sk, bool bind_inany);

commit 035ff358f2d9e2f5e1639ba4defe4dc40ac642dd
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Tue Feb 18 17:10:21 2020 +0000

    net: Generate reuseport group ID on group creation
    
    Commit 736b46027eb4 ("net: Add ID (if needed) to sock_reuseport and expose
    reuseport_lock") has introduced lazy generation of reuseport group IDs that
    survive group resize.
    
    By comparing the identifier we check if BPF reuseport program is not trying
    to select a socket from a BPF map that belongs to a different reuseport
    group than the one the packet is for.
    
    Because SOCKARRAY used to be the only BPF map type that can be used with
    reuseport BPF, it was possible to delay the generation of reuseport group
    ID until a socket from the group was inserted into BPF map for the first
    time.
    
    Now that SOCK{MAP,HASH} can be used with reuseport BPF we have two options,
    either generate the reuseport ID on map update, like SOCKARRAY does, or
    allocate an ID from the start when reuseport group gets created.
    
    This patch takes the latter approach to keep sockmap free of calls into
    reuseport code. This streamlines the reuseport_id access as its lifetime
    now matches the longevity of reuseport object.
    
    The cost of this simplification, however, is that we allocate reuseport IDs
    for all SO_REUSEPORT users. Even those that don't use SOCKARRAY in their
    setups. With the way identifiers are currently generated, we can have at
    most S32_MAX reuseport groups, which hopefully is sufficient. If we ever
    get close to the limit, we can switch an u64 counter like sk_cookie.
    
    Another change is that we now always call into SOCKARRAY logic to unlink
    the socket from the map when unhashing or closing the socket. Previously we
    did it only when at least one socket from the group was in a BPF map.
    
    It is worth noting that this doesn't conflict with sockmap tear-down in
    case a socket is in a SOCK{MAP,HASH} and belongs to a reuseport
    group. sockmap tear-down happens first:
    
      prot->unhash
      `- tcp_bpf_unhash
         |- tcp_bpf_remove
         |  `- while (sk_psock_link_pop(psock))
         |     `- sk_psock_unlink
         |        `- sock_map_delete_from_link
         |           `- __sock_map_delete
         |              `- sock_map_unref
         |                 `- sk_psock_put
         |                    `- sk_psock_drop
         |                       `- rcu_assign_sk_user_data(sk, NULL)
         `- inet_unhash
            `- reuseport_detach_sock
               `- bpf_sk_reuseport_detach
                  `- WRITE_ONCE(sk->sk_user_data, NULL)
    
    Suggested-by: Martin Lau <kafai@fb.com>
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200218171023.844439-10-jakub@cloudflare.com

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 43f4a818d88f..3ecaa15d1850 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -55,6 +55,4 @@ static inline bool reuseport_has_conns(struct sock *sk, bool set)
 	return ret;
 }
 
-int reuseport_get_id(struct sock_reuseport *reuse);
-
 #endif  /* _SOCK_REUSEPORT_H */

commit acdcecc61285faed359f1a3568c32089cc3a8329
Author: Willem de Bruijn <willemb@google.com>
Date:   Thu Sep 12 21:16:39 2019 -0400

    udp: correct reuseport selection with connected sockets
    
    UDP reuseport groups can hold a mix unconnected and connected sockets.
    Ensure that connections only receive all traffic to their 4-tuple.
    
    Fast reuseport returns on the first reuseport match on the assumption
    that all matches are equal. Only if connections are present, return to
    the previous behavior of scoring all sockets.
    
    Record if connections are present and if so (1) treat such connected
    sockets as an independent match from the group, (2) only return
    2-tuple matches from reuseport and (3) do not return on the first
    2-tuple reuseport match to allow for a higher scoring match later.
    
    New field has_conns is set without locks. No other fields in the
    bitmap are modified at runtime and the field is only ever set
    unconditionally, so an RMW cannot miss a change.
    
    Fixes: e32ea7e74727 ("soreuseport: fast reuseport UDP socket selection")
    Link: http://lkml.kernel.org/r/CA+FuTSfRP09aJNYRt04SS6qj22ViiOEWaWmLAwX0psk8-PGNxw@mail.gmail.com
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Paolo Abeni <pabeni@redhat.com>
    Acked-by: Craig Gallek <kraig@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index d9112de85261..43f4a818d88f 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -21,7 +21,8 @@ struct sock_reuseport {
 	unsigned int		synq_overflow_ts;
 	/* ID stays the same even after the size of socks[] grows. */
 	unsigned int		reuseport_id;
-	bool			bind_inany;
+	unsigned int		bind_inany:1;
+	unsigned int		has_conns:1;
 	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
 	struct sock		*socks[0];	/* array of sock pointers */
 };
@@ -37,6 +38,23 @@ extern struct sock *reuseport_select_sock(struct sock *sk,
 extern int reuseport_attach_prog(struct sock *sk, struct bpf_prog *prog);
 extern int reuseport_detach_prog(struct sock *sk);
 
+static inline bool reuseport_has_conns(struct sock *sk, bool set)
+{
+	struct sock_reuseport *reuse;
+	bool ret = false;
+
+	rcu_read_lock();
+	reuse = rcu_dereference(sk->sk_reuseport_cb);
+	if (reuse) {
+		if (set)
+			reuse->has_conns = 1;
+		ret = reuse->has_conns;
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
 int reuseport_get_id(struct sock_reuseport *reuse);
 
 #endif  /* _SOCK_REUSEPORT_H */

commit 99f3a064bc2e4bd5fe50218646c5be342f2ad18c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Jun 13 15:00:01 2019 -0700

    bpf: net: Add SO_DETACH_REUSEPORT_BPF
    
    There is SO_ATTACH_REUSEPORT_[CE]BPF but there is no DETACH.
    This patch adds SO_DETACH_REUSEPORT_BPF sockopt.  The same
    sockopt can be used to undo both SO_ATTACH_REUSEPORT_[CE]BPF.
    
    reseport_detach_prog() is added and it is mostly a mirror
    of the existing reuseport_attach_prog().  The differences are,
    it does not call reuseport_alloc() and returns -ENOENT when
    there is no old prog.
    
    Cc: Craig Gallek <kraig@google.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Reviewed-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 8a5f70c7cdf2..d9112de85261 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -35,6 +35,8 @@ extern struct sock *reuseport_select_sock(struct sock *sk,
 					  struct sk_buff *skb,
 					  int hdr_len);
 extern int reuseport_attach_prog(struct sock *sk, struct bpf_prog *prog);
+extern int reuseport_detach_prog(struct sock *sk);
+
 int reuseport_get_id(struct sock_reuseport *reuse);
 
 #endif  /* _SOCK_REUSEPORT_H */

commit 8217ca653ec601246832d562207bc24bdf652d2f
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:26 2018 -0700

    bpf: Enable BPF_PROG_TYPE_SK_REUSEPORT bpf prog in reuseport selection
    
    This patch allows a BPF_PROG_TYPE_SK_REUSEPORT bpf prog to select a
    SO_REUSEPORT sk from a BPF_MAP_TYPE_REUSEPORT_ARRAY introduced in
    the earlier patch.  "bpf_run_sk_reuseport()" will return -ECONNREFUSED
    when the BPF_PROG_TYPE_SK_REUSEPORT prog returns SK_DROP.
    The callers, in inet[6]_hashtable.c and ipv[46]/udp.c, are modified to
    handle this case and return NULL immediately instead of continuing the
    sk search from its hashtable.
    
    It re-uses the existing SO_ATTACH_REUSEPORT_EBPF setsockopt to attach
    BPF_PROG_TYPE_SK_REUSEPORT.  The "sk_reuseport_attach_bpf()" will check
    if the attaching bpf prog is in the new SK_REUSEPORT or the existing
    SOCKET_FILTER type and then check different things accordingly.
    
    One level of "__reuseport_attach_prog()" call is removed.  The
    "sk_unhashed() && ..." and "sk->sk_reuseport_cb" tests are pushed
    back to "reuseport_attach_prog()" in sock_reuseport.c.  sock_reuseport.c
    seems to have more knowledge on those test requirements than filter.c.
    In "reuseport_attach_prog()", after new_prog is attached to reuse->prog,
    the old_prog (if any) is also directly freed instead of returning the
    old_prog to the caller and asking the caller to free.
    
    The sysctl_optmem_max check is moved back to the
    "sk_reuseport_attach_filter()" and "sk_reuseport_attach_bpf()".
    As of other bpf prog types, the new BPF_PROG_TYPE_SK_REUSEPORT is only
    bounded by the usual "bpf_prog_charge_memlock()" during load time
    instead of bounded by both bpf_prog_charge_memlock and sysctl_optmem_max.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 73b569556be6..8a5f70c7cdf2 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -34,8 +34,7 @@ extern struct sock *reuseport_select_sock(struct sock *sk,
 					  u32 hash,
 					  struct sk_buff *skb,
 					  int hdr_len);
-extern struct bpf_prog *reuseport_attach_prog(struct sock *sk,
-					      struct bpf_prog *prog);
+extern int reuseport_attach_prog(struct sock *sk, struct bpf_prog *prog);
 int reuseport_get_id(struct sock_reuseport *reuse);
 
 #endif  /* _SOCK_REUSEPORT_H */

commit 2dbb9b9e6df67d444fbe425c7f6014858d337adf
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:25 2018 -0700

    bpf: Introduce BPF_PROG_TYPE_SK_REUSEPORT
    
    This patch adds a BPF_PROG_TYPE_SK_REUSEPORT which can select
    a SO_REUSEPORT sk from a BPF_MAP_TYPE_REUSEPORT_ARRAY.  Like other
    non SK_FILTER/CGROUP_SKB program, it requires CAP_SYS_ADMIN.
    
    BPF_PROG_TYPE_SK_REUSEPORT introduces "struct sk_reuseport_kern"
    to store the bpf context instead of using the skb->cb[48].
    
    At the SO_REUSEPORT sk lookup time, it is in the middle of transiting
    from a lower layer (ipv4/ipv6) to a upper layer (udp/tcp).  At this
    point,  it is not always clear where the bpf context can be appended
    in the skb->cb[48] to avoid saving-and-restoring cb[].  Even putting
    aside the difference between ipv4-vs-ipv6 and udp-vs-tcp.  It is not
    clear if the lower layer is only ipv4 and ipv6 in the future and
    will it not touch the cb[] again before transiting to the upper
    layer.
    
    For example, in udp_gro_receive(), it uses the 48 byte NAPI_GRO_CB
    instead of IP[6]CB and it may still modify the cb[] after calling
    the udp[46]_lib_lookup_skb().  Because of the above reason, if
    sk->cb is used for the bpf ctx, saving-and-restoring is needed
    and likely the whole 48 bytes cb[] has to be saved and restored.
    
    Instead of saving, setting and restoring the cb[], this patch opts
    to create a new "struct sk_reuseport_kern" and setting the needed
    values in there.
    
    The new BPF_PROG_TYPE_SK_REUSEPORT and "struct sk_reuseport_(kern|md)"
    will serve all ipv4/ipv6 + udp/tcp combinations.  There is no protocol
    specific usage at this point and it is also inline with the current
    sock_reuseport.c implementation (i.e. no protocol specific requirement).
    
    In "struct sk_reuseport_md", this patch exposes data/data_end/len
    with semantic similar to other existing usages.  Together
    with "bpf_skb_load_bytes()" and "bpf_skb_load_bytes_relative()",
    the bpf prog can peek anywhere in the skb.  The "bind_inany" tells
    the bpf prog that the reuseport group is bind-ed to a local
    INANY address which cannot be learned from skb.
    
    The new "bind_inany" is added to "struct sock_reuseport" which will be
    used when running the new "BPF_PROG_TYPE_SK_REUSEPORT" bpf prog in order
    to avoid repeating the "bind INANY" test on
    "sk_v6_rcv_saddr/sk->sk_rcv_saddr" every time a bpf prog is run.  It can
    only be properly initialized when a "sk->sk_reuseport" enabled sk is
    adding to a hashtable (i.e. during "reuseport_alloc()" and
    "reuseport_add_sock()").
    
    The new "sk_select_reuseport()" is the main helper that the
    bpf prog will use to select a SO_REUSEPORT sk.  It is the only function
    that can use the new BPF_MAP_TYPE_REUSEPORT_ARRAY.  As mentioned in
    the earlier patch, the validity of a selected sk is checked in
    run time in "sk_select_reuseport()".  Doing the check in
    verification time is difficult and inflexible (consider the map-in-map
    use case).  The runtime check is to compare the selected sk's reuseport_id
    with the reuseport_id that we want.  This helper will return -EXXX if the
    selected sk cannot serve the incoming request (e.g. reuseport_id
    not match).  The bpf prog can decide if it wants to do SK_DROP as its
    discretion.
    
    When the bpf prog returns SK_PASS, the kernel will check if a
    valid sk has been selected (i.e. "reuse_kern->selected_sk != NULL").
    If it does , it will use the selected sk.  If not, the kernel
    will select one from "reuse->socks[]" (as before this patch).
    
    The SK_DROP and SK_PASS handling logic will be in the next patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index e1a7681856f7..73b569556be6 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -21,12 +21,14 @@ struct sock_reuseport {
 	unsigned int		synq_overflow_ts;
 	/* ID stays the same even after the size of socks[] grows. */
 	unsigned int		reuseport_id;
+	bool			bind_inany;
 	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
 	struct sock		*socks[0];	/* array of sock pointers */
 };
 
-extern int reuseport_alloc(struct sock *sk);
-extern int reuseport_add_sock(struct sock *sk, struct sock *sk2);
+extern int reuseport_alloc(struct sock *sk, bool bind_inany);
+extern int reuseport_add_sock(struct sock *sk, struct sock *sk2,
+			      bool bind_inany);
 extern void reuseport_detach_sock(struct sock *sk);
 extern struct sock *reuseport_select_sock(struct sock *sk,
 					  u32 hash,

commit 736b46027eb4a4c602d3b8b93d2f48c9facbd915
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:22 2018 -0700

    net: Add ID (if needed) to sock_reuseport and expose reuseport_lock
    
    A later patch will introduce a BPF_MAP_TYPE_REUSEPORT_ARRAY which
    allows a SO_REUSEPORT sk to be added to a bpf map.  When a sk
    is removed from reuse->socks[], it also needs to be removed from
    the bpf map.  Also, when adding a sk to a bpf map, the bpf
    map needs to ensure it is indeed in a reuse->socks[].
    Hence, reuseport_lock is needed by the bpf map to ensure its
    map_update_elem() and map_delete_elem() operations are in-sync with
    the reuse->socks[].  The BPF_MAP_TYPE_REUSEPORT_ARRAY map will only
    acquire the reuseport_lock after ensuring the adding sk is already
    in a reuseport group (i.e. reuse->socks[]).  The map_lookup_elem()
    will be lockless.
    
    This patch also adds an ID to sock_reuseport.  A later patch
    will introduce BPF_PROG_TYPE_SK_REUSEPORT which allows
    a bpf prog to select a sk from a bpf map.  It is inflexible to
    statically enforce a bpf map can only contain the sk belonging to
    a particular reuse->socks[] (i.e. same IP:PORT) during the bpf
    verification time. For example, think about the the map-in-map situation
    where the inner map can be dynamically changed in runtime and the outer
    map may have inner maps belonging to different reuseport groups.
    Hence, when the bpf prog (in the new BPF_PROG_TYPE_SK_REUSEPORT
    type) selects a sk,  this selected sk has to be checked to ensure it
    belongs to the requesting reuseport group (i.e. the group serving
    that IP:PORT).
    
    The "sk->sk_reuseport_cb" pointer cannot be used for this checking
    purpose because the pointer value will change after reuseport_grow().
    Instead of saving all checking conditions like the ones
    preced calling "reuseport_add_sock()" and compare them everytime a
    bpf_prog is run, a 32bits ID is introduced to survive the
    reuseport_grow().  The ID is only acquired if any of the
    reuse->socks[] is added to the newly introduced
    "BPF_MAP_TYPE_REUSEPORT_ARRAY" map.
    
    If "BPF_MAP_TYPE_REUSEPORT_ARRAY" is not used,  the changes in this
    patch is a no-op.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 6bef7a0052f2..e1a7681856f7 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -5,8 +5,11 @@
 #include <linux/filter.h>
 #include <linux/skbuff.h>
 #include <linux/types.h>
+#include <linux/spinlock.h>
 #include <net/sock.h>
 
+extern spinlock_t reuseport_lock;
+
 struct sock_reuseport {
 	struct rcu_head		rcu;
 
@@ -16,6 +19,8 @@ struct sock_reuseport {
 	 * reuse->socks[] group.
 	 */
 	unsigned int		synq_overflow_ts;
+	/* ID stays the same even after the size of socks[] grows. */
+	unsigned int		reuseport_id;
 	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
 	struct sock		*socks[0];	/* array of sock pointers */
 };
@@ -29,5 +34,6 @@ extern struct sock *reuseport_select_sock(struct sock *sk,
 					  int hdr_len);
 extern struct bpf_prog *reuseport_attach_prog(struct sock *sk,
 					      struct bpf_prog *prog);
+int reuseport_get_id(struct sock_reuseport *reuse);
 
 #endif  /* _SOCK_REUSEPORT_H */

commit 40a1227ea845a37ab197dd1caffb60b047fa36b1
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:21 2018 -0700

    tcp: Avoid TCP syncookie rejected by SO_REUSEPORT socket
    
    Although the actual cookie check "__cookie_v[46]_check()" does
    not involve sk specific info, it checks whether the sk has recent
    synq overflow event in "tcp_synq_no_recent_overflow()".  The
    tcp_sk(sk)->rx_opt.ts_recent_stamp is updated every second
    when it has sent out a syncookie (through "tcp_synq_overflow()").
    
    The above per sk "recent synq overflow event timestamp" works well
    for non SO_REUSEPORT use case.  However, it may cause random
    connection request reject/discard when SO_REUSEPORT is used with
    syncookie because it fails the "tcp_synq_no_recent_overflow()"
    test.
    
    When SO_REUSEPORT is used, it usually has multiple listening
    socks serving TCP connection requests destinated to the same local IP:PORT.
    There are cases that the TCP-ACK-COOKIE may not be received
    by the same sk that sent out the syncookie.  For example,
    if reuse->socks[] began with {sk0, sk1},
    1) sk1 sent out syncookies and tcp_sk(sk1)->rx_opt.ts_recent_stamp
       was updated.
    2) the reuse->socks[] became {sk1, sk2} later.  e.g. sk0 was first closed
       and then sk2 was added.  Here, sk2 does not have ts_recent_stamp set.
       There are other ordering that will trigger the similar situation
       below but the idea is the same.
    3) When the TCP-ACK-COOKIE comes back, sk2 was selected.
       "tcp_synq_no_recent_overflow(sk2)" returns true. In this case,
       all syncookies sent by sk1 will be handled (and rejected)
       by sk2 while sk1 is still alive.
    
    The userspace may create and remove listening SO_REUSEPORT sockets
    as it sees fit.  E.g. Adding new thread (and SO_REUSEPORT sock) to handle
    incoming requests, old process stopping and new process starting...etc.
    With or without SO_ATTACH_REUSEPORT_[CB]BPF,
    the sockets leaving and joining a reuseport group makes picking
    the same sk to check the syncookie very difficult (if not impossible).
    
    The later patches will allow bpf prog more flexibility in deciding
    where a sk should be located in a bpf map and selecting a particular
    SO_REUSEPORT sock as it sees fit.  e.g. Without closing any sock,
    replace the whole bpf reuseport_array in one map_update() by using
    map-in-map.  Getting the syncookie check working smoothly across
    socks in the same "reuse->socks[]" is important.
    
    A partial solution is to set the newly added sk's ts_recent_stamp
    to the max ts_recent_stamp of a reuseport group but that will require
    to iterate through reuse->socks[]  OR
    pessimistically set it to "now - TCP_SYNCOOKIE_VALID" when a sk is
    joining a reuseport group.  However, neither of them will solve the
    existing sk getting moved around the reuse->socks[] and that
    sk may not have ts_recent_stamp updated, unlikely under continuous
    synflood but not impossible.
    
    This patch opts to treat the reuseport group as a whole when
    considering the last synq overflow timestamp since
    they are serving the same IP:PORT from the userspace
    (and BPF program) perspective.
    
    "synq_overflow_ts" is added to "struct sock_reuseport".
    The tcp_synq_overflow() and tcp_synq_no_recent_overflow()
    will update/check reuse->synq_overflow_ts if the sk is
    in a reuseport group.  Similar to the reuseport decision in
    __inet_lookup_listener(), both sk->sk_reuseport and
    sk->sk_reuseport_cb are tested for SO_REUSEPORT usage.
    Update on "synq_overflow_ts" happens at roughly once
    every second.
    
    A synflood test was done with a 16 rx-queues and 16 reuseport sockets.
    No meaningful performance change is observed.  Before and
    after the change is ~9Mpps in IPv4.
    
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 0054b3a9b923..6bef7a0052f2 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -12,6 +12,10 @@ struct sock_reuseport {
 
 	u16			max_socks;	/* length of socks */
 	u16			num_socks;	/* elements in socks */
+	/* The last synq overflow event timestamp of this
+	 * reuse->socks[] group.
+	 */
+	unsigned int		synq_overflow_ts;
 	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
 	struct sock		*socks[0];	/* array of sock pointers */
 };

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index aecd30308d50..0054b3a9b923 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _SOCK_REUSEPORT_H
 #define _SOCK_REUSEPORT_H
 

commit b4ace4f1ae07691b4f6ea9f3e92efbec083df058
Author: Craig Gallek <kraig@google.com>
Date:   Tue Jan 19 14:27:08 2016 -0500

    soreuseport: fix NULL ptr dereference SO_REUSEPORT after bind
    
    Marc Dionne discovered a NULL pointer dereference when setting
    SO_REUSEPORT on a socket after it is bound.
    This patch removes the assumption that at least one socket in the
    reuseport group is bound with the SO_REUSEPORT option before other
    bind calls occur.
    
    Fixes: e32ea7e74727 ("soreuseport: fast reuseport UDP socket selection")
    Reported-by: Marc Dionne <marc.c.dionne@gmail.com>
    Signed-off-by: Craig Gallek <kraig@google.com>
    Tested-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 7dda3d7adba8..aecd30308d50 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -16,7 +16,7 @@ struct sock_reuseport {
 };
 
 extern int reuseport_alloc(struct sock *sk);
-extern int reuseport_add_sock(struct sock *sk, const struct sock *sk2);
+extern int reuseport_add_sock(struct sock *sk, struct sock *sk2);
 extern void reuseport_detach_sock(struct sock *sk);
 extern struct sock *reuseport_select_sock(struct sock *sk,
 					  u32 hash,

commit 538950a1b7527a0a52ccd9337e3fcd304f027f13
Author: Craig Gallek <kraig@google.com>
Date:   Mon Jan 4 17:41:47 2016 -0500

    soreuseport: setsockopt SO_ATTACH_REUSEPORT_[CE]BPF
    
    Expose socket options for setting a classic or extended BPF program
    for use when selecting sockets in an SO_REUSEPORT group.  These options
    can be used on the first socket to belong to a group before bind or
    on any socket in the group after bind.
    
    This change includes refactoring of the existing sk_filter code to
    allow reuse of the existing BPF filter validation checks.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
index 67d1eb8fd7af..7dda3d7adba8 100644
--- a/include/net/sock_reuseport.h
+++ b/include/net/sock_reuseport.h
@@ -1,6 +1,8 @@
 #ifndef _SOCK_REUSEPORT_H
 #define _SOCK_REUSEPORT_H
 
+#include <linux/filter.h>
+#include <linux/skbuff.h>
 #include <linux/types.h>
 #include <net/sock.h>
 
@@ -9,12 +11,18 @@ struct sock_reuseport {
 
 	u16			max_socks;	/* length of socks */
 	u16			num_socks;	/* elements in socks */
+	struct bpf_prog __rcu	*prog;		/* optional BPF sock selector */
 	struct sock		*socks[0];	/* array of sock pointers */
 };
 
 extern int reuseport_alloc(struct sock *sk);
 extern int reuseport_add_sock(struct sock *sk, const struct sock *sk2);
 extern void reuseport_detach_sock(struct sock *sk);
-extern struct sock *reuseport_select_sock(struct sock *sk, u32 hash);
+extern struct sock *reuseport_select_sock(struct sock *sk,
+					  u32 hash,
+					  struct sk_buff *skb,
+					  int hdr_len);
+extern struct bpf_prog *reuseport_attach_prog(struct sock *sk,
+					      struct bpf_prog *prog);
 
 #endif  /* _SOCK_REUSEPORT_H */

commit ef456144da8ef507c8cf504284b6042e9201a05c
Author: Craig Gallek <kraig@google.com>
Date:   Mon Jan 4 17:41:45 2016 -0500

    soreuseport: define reuseport groups
    
    struct sock_reuseport is an optional shared structure referenced by each
    socket belonging to a reuseport group.  When a socket is bound to an
    address/port not yet in use and the reuseport flag has been set, the
    structure will be allocated and attached to the newly bound socket.
    When subsequent calls to bind are made for the same address/port, the
    shared structure will be updated to include the new socket and the
    newly bound socket will reference the group structure.
    
    Usually, when an incoming packet was destined for a reuseport group,
    all sockets in the same group needed to be considered before a
    dispatching decision was made.  With this structure, an appropriate
    socket can be found after looking up just one socket in the group.
    
    This shared structure will also allow for more complicated decisions to
    be made when selecting a socket (eg a BPF filter).
    
    This work is based off a similar implementation written by
    Ying Cai <ycai@google.com> for implementing policy-based reuseport
    selection.
    
    Signed-off-by: Craig Gallek <kraig@google.com>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/net/sock_reuseport.h b/include/net/sock_reuseport.h
new file mode 100644
index 000000000000..67d1eb8fd7af
--- /dev/null
+++ b/include/net/sock_reuseport.h
@@ -0,0 +1,20 @@
+#ifndef _SOCK_REUSEPORT_H
+#define _SOCK_REUSEPORT_H
+
+#include <linux/types.h>
+#include <net/sock.h>
+
+struct sock_reuseport {
+	struct rcu_head		rcu;
+
+	u16			max_socks;	/* length of socks */
+	u16			num_socks;	/* elements in socks */
+	struct sock		*socks[0];	/* array of sock pointers */
+};
+
+extern int reuseport_alloc(struct sock *sk);
+extern int reuseport_add_sock(struct sock *sk, const struct sock *sk2);
+extern void reuseport_detach_sock(struct sock *sk);
+extern struct sock *reuseport_select_sock(struct sock *sk, u32 hash);
+
+#endif  /* _SOCK_REUSEPORT_H */
