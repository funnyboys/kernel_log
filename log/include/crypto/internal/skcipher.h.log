commit aacd5b4cfb87306888eb9e3612cb90afbb2ecba5
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 2 19:59:00 2020 -0800

    crypto: skcipher - use crypto_grab_cipher() and simplify error paths
    
    Make skcipher_alloc_instance_simple() use the new function
    crypto_grab_cipher() to initialize its cipher spawn.
    
    This is needed to make all spawns be initialized in a consistent way.
    
    Also simplify the error handling by taking advantage of crypto_drop_*()
    now accepting (as a no-op) spawns that haven't been initialized yet, and
    by taking advantage of crypto_grab_*() now handling ERR_PTR() names.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index e387424f6247..10226c12c5df 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -214,9 +214,9 @@ struct skcipher_instance *skcipher_alloc_instance_simple(
 static inline struct crypto_alg *skcipher_ialg_simple(
 	struct skcipher_instance *inst)
 {
-	struct crypto_spawn *spawn = skcipher_instance_ctx(inst);
+	struct crypto_cipher_spawn *spawn = skcipher_instance_ctx(inst);
 
-	return spawn->alg;
+	return crypto_spawn_cipher_alg(spawn);
 }
 
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */

commit b9f76dddb1f9f70e008b982381bbc9a67c9b8c66
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 2 19:58:45 2020 -0800

    crypto: skcipher - pass instance to crypto_grab_skcipher()
    
    Initializing a crypto_skcipher_spawn currently requires:
    
    1. Set spawn->base.inst to point to the instance.
    2. Call crypto_grab_skcipher().
    
    But there's no reason for these steps to be separate, and in fact this
    unneeded complication has caused at least one bug, the one fixed by
    commit 6db43410179b ("crypto: adiantum - initialize crypto_spawn::inst")
    
    So just make crypto_grab_skcipher() take the instance as an argument.
    
    To keep the function calls from getting too unwieldy due to this extra
    argument, also introduce a 'mask' variable into the affected places
    which weren't already using one.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index df4fdeaa13f3..e387424f6247 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -88,14 +88,9 @@ static inline void skcipher_request_complete(struct skcipher_request *req, int e
 	req->base.complete(&req->base, err);
 }
 
-static inline void crypto_set_skcipher_spawn(
-	struct crypto_skcipher_spawn *spawn, struct crypto_instance *inst)
-{
-	crypto_set_spawn(&spawn->base, inst);
-}
-
-int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
-			 u32 type, u32 mask);
+int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn,
+			 struct crypto_instance *inst,
+			 const char *name, u32 type, u32 mask);
 
 static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
 {

commit 70ffa8fd72b8df7ddfedc0b7db042eea75182fd5
Author: Eric Biggers <ebiggers@google.com>
Date:   Mon Dec 30 13:41:15 2019 -0600

    crypto: skcipher - remove skcipher_walk_aead()
    
    skcipher_walk_aead() is unused and is identical to
    skcipher_walk_aead_encrypt(), so remove it.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index ad4a6330ff53..df4fdeaa13f3 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -140,8 +140,6 @@ int skcipher_walk_virt(struct skcipher_walk *walk,
 void skcipher_walk_atomise(struct skcipher_walk *walk);
 int skcipher_walk_async(struct skcipher_walk *walk,
 			struct skcipher_request *req);
-int skcipher_walk_aead(struct skcipher_walk *walk, struct aead_request *req,
-		       bool atomic);
 int skcipher_walk_aead_encrypt(struct skcipher_walk *walk,
 			       struct aead_request *req, bool atomic);
 int skcipher_walk_aead_decrypt(struct skcipher_walk *walk,

commit b3c16bfc6a79ae517ec3c44be615aed0ffa52c53
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Dec 20 13:29:40 2019 +0800

    crypto: skcipher - Add skcipher_ialg_simple helper
    
    This patch introduces the skcipher_ialg_simple helper which fetches
    the crypto_alg structure from a simple skcipher instance's spawn.
    
    This allows us to remove the third argument from the function
    skcipher_alloc_instance_simple.
    
    In doing so the reference count to the algorithm is now maintained
    by the Crypto API and the caller no longer needs to drop the alg
    refcount.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 921c409fe1b1..ad4a6330ff53 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -214,9 +214,17 @@ skcipher_cipher_simple(struct crypto_skcipher *tfm)
 
 	return ctx->cipher;
 }
-struct skcipher_instance *
-skcipher_alloc_instance_simple(struct crypto_template *tmpl, struct rtattr **tb,
-			       struct crypto_alg **cipher_alg_ret);
+
+struct skcipher_instance *skcipher_alloc_instance_simple(
+	struct crypto_template *tmpl, struct rtattr **tb);
+
+static inline struct crypto_alg *skcipher_ialg_simple(
+	struct skcipher_instance *inst)
+{
+	struct crypto_spawn *spawn = skcipher_instance_ctx(inst);
+
+	return spawn->alg;
+}
 
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit d63007eb954e425f45766482813738f41db9af45
Author: Ard Biesheuvel <ardb@kernel.org>
Date:   Sat Nov 9 18:09:53 2019 +0100

    crypto: ablkcipher - remove deprecated and unused ablkcipher support
    
    Now that all users of the deprecated ablkcipher interface have been
    moved to the skcipher interface, ablkcipher is no longer used and
    can be removed.
    
    Reviewed-by: Eric Biggers <ebiggers@kernel.org>
    Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 454e898d5f5f..921c409fe1b1 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -153,17 +153,6 @@ static inline void skcipher_walk_abort(struct skcipher_walk *walk)
 	skcipher_walk_done(walk, -ECANCELED);
 }
 
-static inline void ablkcipher_request_complete(struct ablkcipher_request *req,
-					       int err)
-{
-	req->base.complete(&req->base, err);
-}
-
-static inline u32 ablkcipher_request_flags(struct ablkcipher_request *req)
-{
-	return req->base.flags;
-}
-
 static inline void *crypto_skcipher_ctx(struct crypto_skcipher *tfm)
 {
 	return crypto_tfm_ctx(&tfm->base);
@@ -182,27 +171,18 @@ static inline u32 skcipher_request_flags(struct skcipher_request *req)
 static inline unsigned int crypto_skcipher_alg_min_keysize(
 	struct skcipher_alg *alg)
 {
-	if (alg->base.cra_ablkcipher.encrypt)
-		return alg->base.cra_ablkcipher.min_keysize;
-
 	return alg->min_keysize;
 }
 
 static inline unsigned int crypto_skcipher_alg_max_keysize(
 	struct skcipher_alg *alg)
 {
-	if (alg->base.cra_ablkcipher.encrypt)
-		return alg->base.cra_ablkcipher.max_keysize;
-
 	return alg->max_keysize;
 }
 
 static inline unsigned int crypto_skcipher_alg_walksize(
 	struct skcipher_alg *alg)
 {
-	if (alg->base.cra_ablkcipher.encrypt)
-		return alg->base.cra_blocksize;
-
 	return alg->walksize;
 }
 

commit c65058b7587fd3d001c57a50285477be521f5350
Author: Eric Biggers <ebiggers@google.com>
Date:   Fri Oct 25 12:41:12 2019 -0700

    crypto: skcipher - remove the "blkcipher" algorithm type
    
    Now that all "blkcipher" algorithms have been converted to "skcipher",
    remove the blkcipher algorithm type.
    
    The skcipher (symmetric key cipher) algorithm type was introduced a few
    years ago to replace both blkcipher and ablkcipher (synchronous and
    asynchronous block cipher).  The advantages of skcipher include:
    
      - A much less confusing name, since none of these algorithm types have
        ever actually been for raw block ciphers, but rather for all
        length-preserving encryption modes including block cipher modes of
        operation, stream ciphers, and other length-preserving modes.
    
      - It unified blkcipher and ablkcipher into a single algorithm type
        which supports both synchronous and asynchronous implementations.
        Note, blkcipher already operated only on scatterlists, so the fact
        that skcipher does too isn't a regression in functionality.
    
      - Better type safety by using struct skcipher_alg, struct
        crypto_skcipher, etc. instead of crypto_alg, crypto_tfm, etc.
    
      - It sometimes simplifies the implementations of algorithms.
    
    Also, the blkcipher API was no longer being tested.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 3175dfeaed2c..454e898d5f5f 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -182,10 +182,6 @@ static inline u32 skcipher_request_flags(struct skcipher_request *req)
 static inline unsigned int crypto_skcipher_alg_min_keysize(
 	struct skcipher_alg *alg)
 {
-	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
-	    CRYPTO_ALG_TYPE_BLKCIPHER)
-		return alg->base.cra_blkcipher.min_keysize;
-
 	if (alg->base.cra_ablkcipher.encrypt)
 		return alg->base.cra_ablkcipher.min_keysize;
 
@@ -195,10 +191,6 @@ static inline unsigned int crypto_skcipher_alg_min_keysize(
 static inline unsigned int crypto_skcipher_alg_max_keysize(
 	struct skcipher_alg *alg)
 {
-	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
-	    CRYPTO_ALG_TYPE_BLKCIPHER)
-		return alg->base.cra_blkcipher.max_keysize;
-
 	if (alg->base.cra_ablkcipher.encrypt)
 		return alg->base.cra_ablkcipher.max_keysize;
 
@@ -208,10 +200,6 @@ static inline unsigned int crypto_skcipher_alg_max_keysize(
 static inline unsigned int crypto_skcipher_alg_walksize(
 	struct skcipher_alg *alg)
 {
-	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
-	    CRYPTO_ALG_TYPE_BLKCIPHER)
-		return alg->base.cra_blocksize;
-
 	if (alg->base.cra_ablkcipher.encrypt)
 		return alg->base.cra_blocksize;
 

commit 5b0fe9552336338acb52756daf65dd7a4eeca73f
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Sep 10 11:42:05 2019 +1000

    crypto: algif_skcipher - Use chunksize instead of blocksize
    
    When algif_skcipher does a partial operation it always process data
    that is a multiple of blocksize.  However, for algorithms such as
    CTR this is wrong because even though it can process any number of
    bytes overall, the partial block must come at the very end and not
    in the middle.
    
    This is exactly what chunksize is meant to describe so this patch
    changes blocksize to chunksize.
    
    Fixes: 8ff590903d5f ("crypto: algif_skcipher - User-space...")
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 734b6f7081b8..3175dfeaed2c 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -205,19 +205,6 @@ static inline unsigned int crypto_skcipher_alg_max_keysize(
 	return alg->max_keysize;
 }
 
-static inline unsigned int crypto_skcipher_alg_chunksize(
-	struct skcipher_alg *alg)
-{
-	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
-	    CRYPTO_ALG_TYPE_BLKCIPHER)
-		return alg->base.cra_blocksize;
-
-	if (alg->base.cra_ablkcipher.encrypt)
-		return alg->base.cra_blocksize;
-
-	return alg->chunksize;
-}
-
 static inline unsigned int crypto_skcipher_alg_walksize(
 	struct skcipher_alg *alg)
 {
@@ -231,23 +218,6 @@ static inline unsigned int crypto_skcipher_alg_walksize(
 	return alg->walksize;
 }
 
-/**
- * crypto_skcipher_chunksize() - obtain chunk size
- * @tfm: cipher handle
- *
- * The block size is set to one for ciphers such as CTR.  However,
- * you still need to provide incremental updates in multiples of
- * the underlying block size as the IV does not have sub-block
- * granularity.  This is known in this API as the chunk size.
- *
- * Return: chunk size in bytes
- */
-static inline unsigned int crypto_skcipher_chunksize(
-	struct crypto_skcipher *tfm)
-{
-	return crypto_skcipher_alg_chunksize(crypto_skcipher_alg(tfm));
-}
-
 /**
  * crypto_skcipher_walksize() - obtain walk size
  * @tfm: cipher handle

commit 6017826b494008895632e092cc49c0bdb3019eab
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 3 09:43:30 2019 -0700

    crypto: skcipher - add the ability to abort a skcipher walk
    
    After starting a skcipher walk, the only way to ensure that all
    resources it has tied up are released is to complete it. In some
    cases, it will be useful to be able to abort a walk cleanly after
    it has started, so add this ability to the skcipher walk API.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index d68faa5759ad..734b6f7081b8 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -148,6 +148,11 @@ int skcipher_walk_aead_decrypt(struct skcipher_walk *walk,
 			       struct aead_request *req, bool atomic);
 void skcipher_walk_complete(struct skcipher_walk *walk, int err);
 
+static inline void skcipher_walk_abort(struct skcipher_walk *walk)
+{
+	skcipher_walk_done(walk, -ECANCELED);
+}
+
 static inline void ablkcipher_request_complete(struct ablkcipher_request *req,
 					       int err)
 {

commit 4d2fa8b44b891f0da5ceda3e5a1402ccf0ab6f26
Merge: 8b68150883ca f3880a23564e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 20:57:08 2019 -0700

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto updates from Herbert Xu:
     "Here is the crypto update for 5.3:
    
      API:
       - Test shash interface directly in testmgr
       - cra_driver_name is now mandatory
    
      Algorithms:
       - Replace arc4 crypto_cipher with library helper
       - Implement 5 way interleave for ECB, CBC and CTR on arm64
       - Add xxhash
       - Add continuous self-test on noise source to drbg
       - Update jitter RNG
    
      Drivers:
       - Add support for SHA204A random number generator
       - Add support for 7211 in iproc-rng200
       - Fix fuzz test failures in inside-secure
       - Fix fuzz test failures in talitos
       - Fix fuzz test failures in qat"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (143 commits)
      crypto: stm32/hash - remove interruptible condition for dma
      crypto: stm32/hash - Fix hmac issue more than 256 bytes
      crypto: stm32/crc32 - rename driver file
      crypto: amcc - remove memset after dma_alloc_coherent
      crypto: ccp - Switch to SPDX license identifiers
      crypto: ccp - Validate the the error value used to index error messages
      crypto: doc - Fix formatting of new crypto engine content
      crypto: doc - Add parameter documentation
      crypto: arm64/aes-ce - implement 5 way interleave for ECB, CBC and CTR
      crypto: arm64/aes-ce - add 5 way interleave routines
      crypto: talitos - drop icv_ool
      crypto: talitos - fix hash on SEC1.
      crypto: talitos - move struct talitos_edesc into talitos.h
      lib/scatterlist: Fix mapping iterator when sg->offset is greater than PAGE_SIZE
      crypto/NX: Set receive window credits to max number of CRBs in RxFIFO
      crypto: asymmetric_keys - select CRYPTO_HASH where needed
      crypto: serpent - mark __serpent_setkey_sbox noinline
      crypto: testmgr - dynamically allocate crypto_shash
      crypto: testmgr - dynamically allocate testvec_config
      crypto: talitos - eliminate unneeded 'done' functions at build time
      ...

commit 314d0f0ea69f2fb8e6fc4cb5d5ae6b1e3418e36c
Author: Eric Biggers <ebiggers@google.com>
Date:   Sun Jun 2 22:46:11 2019 -0700

    crypto: skcipher - make chunksize and walksize accessors internal
    
    The 'chunksize' and 'walksize' properties of skcipher algorithms are
    implementation details that users of the skcipher API should not be
    looking at.  So move their accessor functions from <crypto/skcipher.h>
    to <crypto/internal/skcipher.h>.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 9de6032209cb..abb1096495c2 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -205,6 +205,66 @@ static inline unsigned int crypto_skcipher_alg_max_keysize(
 	return alg->max_keysize;
 }
 
+static inline unsigned int crypto_skcipher_alg_chunksize(
+	struct skcipher_alg *alg)
+{
+	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
+	    CRYPTO_ALG_TYPE_BLKCIPHER)
+		return alg->base.cra_blocksize;
+
+	if (alg->base.cra_ablkcipher.encrypt)
+		return alg->base.cra_blocksize;
+
+	return alg->chunksize;
+}
+
+static inline unsigned int crypto_skcipher_alg_walksize(
+	struct skcipher_alg *alg)
+{
+	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
+	    CRYPTO_ALG_TYPE_BLKCIPHER)
+		return alg->base.cra_blocksize;
+
+	if (alg->base.cra_ablkcipher.encrypt)
+		return alg->base.cra_blocksize;
+
+	return alg->walksize;
+}
+
+/**
+ * crypto_skcipher_chunksize() - obtain chunk size
+ * @tfm: cipher handle
+ *
+ * The block size is set to one for ciphers such as CTR.  However,
+ * you still need to provide incremental updates in multiples of
+ * the underlying block size as the IV does not have sub-block
+ * granularity.  This is known in this API as the chunk size.
+ *
+ * Return: chunk size in bytes
+ */
+static inline unsigned int crypto_skcipher_chunksize(
+	struct crypto_skcipher *tfm)
+{
+	return crypto_skcipher_alg_chunksize(crypto_skcipher_alg(tfm));
+}
+
+/**
+ * crypto_skcipher_walksize() - obtain walk size
+ * @tfm: cipher handle
+ *
+ * In some cases, algorithms can only perform optimally when operating on
+ * multiple blocks in parallel. This is reflected by the walksize, which
+ * must be a multiple of the chunksize (or equal if the concern does not
+ * apply)
+ *
+ * Return: walk size in bytes
+ */
+static inline unsigned int crypto_skcipher_walksize(
+	struct crypto_skcipher *tfm)
+{
+	return crypto_skcipher_alg_walksize(crypto_skcipher_alg(tfm));
+}
+
 /* Helpers for simple block cipher modes of operation */
 struct skcipher_ctx_simple {
 	struct crypto_cipher *cipher;	/* underlying block cipher */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 9de6032209cb..fe0376d5a471 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -1,13 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Symmetric key ciphers.
  * 
  * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License as published by the Free
- * Software Foundation; either version 2 of the License, or (at your option) 
- * any later version.
- *
  */
 
 #ifndef _CRYPTO_INTERNAL_SKCIPHER_H

commit 0872da16dd632e5d1d3f80388f7ae6fbeb17ad53
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Jan 3 20:16:14 2019 -0800

    crypto: skcipher - add helper for simple block cipher modes
    
    The majority of skcipher templates (including both the existing ones and
    the ones remaining to be converted from the "blkcipher" API) just wrap a
    single block cipher algorithm.  This includes cbc, cfb, ctr, ecb, kw,
    ofb, and pcbc.  Add a helper function skcipher_alloc_instance_simple()
    that handles allocating an skcipher instance for this common case.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Stephan Mueller <smueller@chronox.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 453e867b4bd9..9de6032209cb 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -205,5 +205,20 @@ static inline unsigned int crypto_skcipher_alg_max_keysize(
 	return alg->max_keysize;
 }
 
+/* Helpers for simple block cipher modes of operation */
+struct skcipher_ctx_simple {
+	struct crypto_cipher *cipher;	/* underlying block cipher */
+};
+static inline struct crypto_cipher *
+skcipher_cipher_simple(struct crypto_skcipher *tfm)
+{
+	struct skcipher_ctx_simple *ctx = crypto_skcipher_ctx(tfm);
+
+	return ctx->cipher;
+}
+struct skcipher_instance *
+skcipher_alloc_instance_simple(struct crypto_template *tmpl, struct rtattr **tb,
+			       struct crypto_alg **cipher_alg_ret);
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit c79b411eaa7257204f89c30651c45cea22278769
Author: Eric Biggers <ebiggers@google.com>
Date:   Sun Dec 16 15:55:06 2018 -0800

    crypto: skcipher - remove remnants of internal IV generators
    
    Remove dead code related to internal IV generators, which are no longer
    used since they've been replaced with the "seqiv" and "echainiv"
    templates.  The removed code includes:
    
    - The "givcipher" (GIVCIPHER) algorithm type.  No algorithms are
      registered with this type anymore, so it's unneeded.
    
    - The "const char *geniv" member of aead_alg, ablkcipher_alg, and
      blkcipher_alg.  A few algorithms still set this, but it isn't used
      anymore except to show via /proc/crypto and CRYPTO_MSG_GETALG.
      Just hardcode "<default>" or "<none>" in those cases.
    
    - The 'skcipher_givcrypt_request' structure, which is never used.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index e42f7063f245..453e867b4bd9 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -70,8 +70,6 @@ struct skcipher_walk {
 	unsigned int alignmask;
 };
 
-extern const struct crypto_type crypto_givcipher_type;
-
 static inline struct crypto_instance *skcipher_crypto_instance(
 	struct skcipher_instance *inst)
 {

commit c821f6ab2e47946f35ee2f30781c5185e5d07f65
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Dec 29 14:09:08 2016 +0000

    crypto: skcipher - introduce walksize attribute for SIMD algos
    
    In some cases, SIMD algorithms can only perform optimally when
    allowed to operate on multiple input blocks in parallel. This is
    especially true for bit slicing algorithms, which typically take
    the same amount of time processing a single block or 8 blocks in
    parallel. However, other SIMD algorithms may benefit as well from
    bigger strides.
    
    So add a walksize attribute to the skcipher algorithm definition, and
    wire it up to the skcipher walk API. To avoid confusion between the
    skcipher and AEAD attributes, rename the skcipher_walk chunksize
    attribute to 'stride', and set it from the walksize (in the skcipher
    case) or from the chunksize (in the AEAD case).
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 8735979ed341..e42f7063f245 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -66,7 +66,7 @@ struct skcipher_walk {
 
 	int flags;
 	unsigned int blocksize;
-	unsigned int chunksize;
+	unsigned int stride;
 	unsigned int alignmask;
 };
 

commit 34bc085c839cef85e3e795b1cee29514f69c3081
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Nov 30 21:14:07 2016 +0800

    crypto: skcipher - Add separate walker for AEAD decryption
    
    The AEAD decrypt interface includes the authentication tag in
    req->cryptlen.  Therefore we need to exlucde that when doing
    a walk over it.
    
    This patch adds separate walker functions for AEAD encryption
    and decryption.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index d55041f45899..8735979ed341 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -149,6 +149,10 @@ int skcipher_walk_async(struct skcipher_walk *walk,
 			struct skcipher_request *req);
 int skcipher_walk_aead(struct skcipher_walk *walk, struct aead_request *req,
 		       bool atomic);
+int skcipher_walk_aead_encrypt(struct skcipher_walk *walk,
+			       struct aead_request *req, bool atomic);
+int skcipher_walk_aead_decrypt(struct skcipher_walk *walk,
+			       struct aead_request *req, bool atomic);
 void skcipher_walk_complete(struct skcipher_walk *walk, int err);
 
 static inline void ablkcipher_request_complete(struct ablkcipher_request *req,

commit b286d8b1a690667e99a89d22245832b6898c6279
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Nov 22 20:08:12 2016 +0800

    crypto: skcipher - Add skcipher walk interface
    
    This patch adds the skcipher walk interface which replaces both
    blkcipher walk and ablkcipher walk.  Just like blkcipher walk it
    can also be used for AEAD algorithms.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 7a7e815a1701..d55041f45899 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -15,8 +15,10 @@
 
 #include <crypto/algapi.h>
 #include <crypto/skcipher.h>
+#include <linux/list.h>
 #include <linux/types.h>
 
+struct aead_request;
 struct rtattr;
 
 struct skcipher_instance {
@@ -34,6 +36,40 @@ struct crypto_skcipher_spawn {
 	struct crypto_spawn base;
 };
 
+struct skcipher_walk {
+	union {
+		struct {
+			struct page *page;
+			unsigned long offset;
+		} phys;
+
+		struct {
+			u8 *page;
+			void *addr;
+		} virt;
+	} src, dst;
+
+	struct scatter_walk in;
+	unsigned int nbytes;
+
+	struct scatter_walk out;
+	unsigned int total;
+
+	struct list_head buffers;
+
+	u8 *page;
+	u8 *buffer;
+	u8 *oiv;
+	void *iv;
+
+	unsigned int ivsize;
+
+	int flags;
+	unsigned int blocksize;
+	unsigned int chunksize;
+	unsigned int alignmask;
+};
+
 extern const struct crypto_type crypto_givcipher_type;
 
 static inline struct crypto_instance *skcipher_crypto_instance(
@@ -104,6 +140,17 @@ void crypto_unregister_skciphers(struct skcipher_alg *algs, int count);
 int skcipher_register_instance(struct crypto_template *tmpl,
 			       struct skcipher_instance *inst);
 
+int skcipher_walk_done(struct skcipher_walk *walk, int err);
+int skcipher_walk_virt(struct skcipher_walk *walk,
+		       struct skcipher_request *req,
+		       bool atomic);
+void skcipher_walk_atomise(struct skcipher_walk *walk);
+int skcipher_walk_async(struct skcipher_walk *walk,
+			struct skcipher_request *req);
+int skcipher_walk_aead(struct skcipher_walk *walk, struct aead_request *req,
+		       bool atomic);
+void skcipher_walk_complete(struct skcipher_walk *walk, int err);
+
 static inline void ablkcipher_request_complete(struct ablkcipher_request *req,
 					       int err)
 {

commit 60425a8bad3995ed06704f2561aace906a429358
Author: Eric Biggers <ebiggers@google.com>
Date:   Fri Oct 28 09:52:19 2016 -0700

    crypto: skcipher - Get rid of crypto_spawn_skcipher2()
    
    Since commit 3a01d0ee2b99 ("crypto: skcipher - Remove top-level
    givcipher interface"), crypto_spawn_skcipher2() and
    crypto_spawn_skcipher() are equivalent.  So switch callers of
    crypto_spawn_skcipher2() to crypto_spawn_skcipher() and remove it.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 558f5c9a8bd9..7a7e815a1701 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -91,12 +91,6 @@ static inline struct crypto_skcipher *crypto_spawn_skcipher(
 	return crypto_spawn_tfm2(&spawn->base);
 }
 
-static inline struct crypto_skcipher *crypto_spawn_skcipher2(
-	struct crypto_skcipher_spawn *spawn)
-{
-	return crypto_spawn_skcipher(spawn);
-}
-
 static inline void crypto_skcipher_set_reqsize(
 	struct crypto_skcipher *skcipher, unsigned int reqsize)
 {

commit a35528eca0977482b240c29cad5e1cf10e03a6a9
Author: Eric Biggers <ebiggers@google.com>
Date:   Fri Oct 28 09:51:13 2016 -0700

    crypto: skcipher - Get rid of crypto_grab_skcipher2()
    
    Since commit 3a01d0ee2b99 ("crypto: skcipher - Remove top-level
    givcipher interface"), crypto_grab_skcipher2() and
    crypto_grab_skcipher() are equivalent.  So switch callers of
    crypto_grab_skcipher2() to crypto_grab_skcipher() and remove it.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 95d2a1822796..558f5c9a8bd9 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -68,12 +68,6 @@ static inline void crypto_set_skcipher_spawn(
 int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
 			 u32 type, u32 mask);
 
-static inline int crypto_grab_skcipher2(struct crypto_skcipher_spawn *spawn,
-					const char *name, u32 type, u32 mask)
-{
-	return crypto_grab_skcipher(spawn, name, type, mask);
-}
-
 static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
 {
 	crypto_drop_spawn(&spawn->base);

commit afb5a0a947f68dd5a5829db6c975063a0acc07d1
Author: Eric Biggers <ebiggers@google.com>
Date:   Fri Oct 7 14:13:34 2016 -0700

    crypto: skcipher - Remove unused crypto_lookup_skcipher() declaration
    
    The definition of crypto_lookup_skcipher() was already removed in
    commit 3a01d0ee2b99 ("crypto: skcipher - Remove top-level givcipher
    interface").  So the declaration should be removed too.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index a21a95e1a375..95d2a1822796 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -74,8 +74,6 @@ static inline int crypto_grab_skcipher2(struct crypto_skcipher_spawn *spawn,
 	return crypto_grab_skcipher(spawn, name, type, mask);
 }
 
-struct crypto_alg *crypto_lookup_skcipher(const char *name, u32 type, u32 mask);
-
 static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
 {
 	crypto_drop_spawn(&spawn->base);

commit 3a01d0ee2b991c8c267620e63a4ab47cd8c30cc4
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Jul 12 13:17:50 2016 +0800

    crypto: skcipher - Remove top-level givcipher interface
    
    This patch removes the old crypto_grab_skcipher helper and replaces
    it with crypto_grab_skcipher2.
    
    As this is the final entry point into givcipher this patch also
    removes all traces of the top-level givcipher interface, including
    all implicit IV generators such as chainiv.
    
    The bottom-level givcipher interface remains until the drivers
    using it are converted.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index ce6619c339fe..a21a95e1a375 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -67,8 +67,12 @@ static inline void crypto_set_skcipher_spawn(
 
 int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
 			 u32 type, u32 mask);
-int crypto_grab_skcipher2(struct crypto_skcipher_spawn *spawn,
-			  const char *name, u32 type, u32 mask);
+
+static inline int crypto_grab_skcipher2(struct crypto_skcipher_spawn *spawn,
+					const char *name, u32 type, u32 mask)
+{
+	return crypto_grab_skcipher(spawn, name, type, mask);
+}
 
 struct crypto_alg *crypto_lookup_skcipher(const char *name, u32 type, u32 mask);
 
@@ -77,30 +81,28 @@ static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
 	crypto_drop_spawn(&spawn->base);
 }
 
-static inline struct crypto_alg *crypto_skcipher_spawn_alg(
+static inline struct skcipher_alg *crypto_skcipher_spawn_alg(
 	struct crypto_skcipher_spawn *spawn)
 {
-	return spawn->base.alg;
+	return container_of(spawn->base.alg, struct skcipher_alg, base);
 }
 
 static inline struct skcipher_alg *crypto_spawn_skcipher_alg(
 	struct crypto_skcipher_spawn *spawn)
 {
-	return container_of(spawn->base.alg, struct skcipher_alg, base);
+	return crypto_skcipher_spawn_alg(spawn);
 }
 
-static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
+static inline struct crypto_skcipher *crypto_spawn_skcipher(
 	struct crypto_skcipher_spawn *spawn)
 {
-	return __crypto_ablkcipher_cast(
-		crypto_spawn_tfm(&spawn->base, crypto_skcipher_type(0),
-				 crypto_skcipher_mask(0)));
+	return crypto_spawn_tfm2(&spawn->base);
 }
 
 static inline struct crypto_skcipher *crypto_spawn_skcipher2(
 	struct crypto_skcipher_spawn *spawn)
 {
-	return crypto_spawn_tfm2(&spawn->base);
+	return crypto_spawn_skcipher(spawn);
 }
 
 static inline void crypto_skcipher_set_reqsize(
@@ -116,53 +118,12 @@ void crypto_unregister_skciphers(struct skcipher_alg *algs, int count);
 int skcipher_register_instance(struct crypto_template *tmpl,
 			       struct skcipher_instance *inst);
 
-int skcipher_null_givencrypt(struct skcipher_givcrypt_request *req);
-int skcipher_null_givdecrypt(struct skcipher_givcrypt_request *req);
-const char *crypto_default_geniv(const struct crypto_alg *alg);
-
-struct crypto_instance *skcipher_geniv_alloc(struct crypto_template *tmpl,
-					     struct rtattr **tb, u32 type,
-					     u32 mask);
-void skcipher_geniv_free(struct crypto_instance *inst);
-int skcipher_geniv_init(struct crypto_tfm *tfm);
-void skcipher_geniv_exit(struct crypto_tfm *tfm);
-
-static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
-	struct crypto_ablkcipher *geniv)
-{
-	return crypto_ablkcipher_crt(geniv)->base;
-}
-
-static inline int skcipher_enqueue_givcrypt(
-	struct crypto_queue *queue, struct skcipher_givcrypt_request *request)
-{
-	return ablkcipher_enqueue_request(queue, &request->creq);
-}
-
-static inline struct skcipher_givcrypt_request *skcipher_dequeue_givcrypt(
-	struct crypto_queue *queue)
-{
-	return skcipher_givcrypt_cast(crypto_dequeue_request(queue));
-}
-
-static inline void *skcipher_givcrypt_reqctx(
-	struct skcipher_givcrypt_request *req)
-{
-	return ablkcipher_request_ctx(&req->creq);
-}
-
 static inline void ablkcipher_request_complete(struct ablkcipher_request *req,
 					       int err)
 {
 	req->base.complete(&req->base, err);
 }
 
-static inline void skcipher_givcrypt_complete(
-	struct skcipher_givcrypt_request *req, int err)
-{
-	ablkcipher_request_complete(&req->creq, err);
-}
-
 static inline u32 ablkcipher_request_flags(struct ablkcipher_request *req)
 {
 	return req->base.flags;

commit 4e6c3df4d729f85997cbf276bfa8ffd8579b8e77
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Jul 12 13:17:31 2016 +0800

    crypto: skcipher - Add low-level skcipher interface
    
    This patch allows skcipher algorithms and instances to be created
    and registered with the crypto API.  They are accessible through
    the top-level skcipher interface, along with ablkcipher/blkcipher
    algorithms and instances.
    
    This patch also introduces a new parameter called chunk size
    which is meant for ciphers such as CTR and CTS which ostensibly
    can handle arbitrary lengths, but still behave like block ciphers
    in that you can only process a partial block at the very end.
    
    For these ciphers the block size will continue to be set to 1
    as it is now while the chunk size will be set to the underlying
    block size.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 2cf7a61ece59..ce6619c339fe 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -19,12 +19,46 @@
 
 struct rtattr;
 
+struct skcipher_instance {
+	void (*free)(struct skcipher_instance *inst);
+	union {
+		struct {
+			char head[offsetof(struct skcipher_alg, base)];
+			struct crypto_instance base;
+		} s;
+		struct skcipher_alg alg;
+	};
+};
+
 struct crypto_skcipher_spawn {
 	struct crypto_spawn base;
 };
 
 extern const struct crypto_type crypto_givcipher_type;
 
+static inline struct crypto_instance *skcipher_crypto_instance(
+	struct skcipher_instance *inst)
+{
+	return &inst->s.base;
+}
+
+static inline struct skcipher_instance *skcipher_alg_instance(
+	struct crypto_skcipher *skcipher)
+{
+	return container_of(crypto_skcipher_alg(skcipher),
+			    struct skcipher_instance, alg);
+}
+
+static inline void *skcipher_instance_ctx(struct skcipher_instance *inst)
+{
+	return crypto_instance_ctx(skcipher_crypto_instance(inst));
+}
+
+static inline void skcipher_request_complete(struct skcipher_request *req, int err)
+{
+	req->base.complete(&req->base, err);
+}
+
 static inline void crypto_set_skcipher_spawn(
 	struct crypto_skcipher_spawn *spawn, struct crypto_instance *inst)
 {
@@ -33,6 +67,8 @@ static inline void crypto_set_skcipher_spawn(
 
 int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
 			 u32 type, u32 mask);
+int crypto_grab_skcipher2(struct crypto_skcipher_spawn *spawn,
+			  const char *name, u32 type, u32 mask);
 
 struct crypto_alg *crypto_lookup_skcipher(const char *name, u32 type, u32 mask);
 
@@ -47,6 +83,12 @@ static inline struct crypto_alg *crypto_skcipher_spawn_alg(
 	return spawn->base.alg;
 }
 
+static inline struct skcipher_alg *crypto_spawn_skcipher_alg(
+	struct crypto_skcipher_spawn *spawn)
+{
+	return container_of(spawn->base.alg, struct skcipher_alg, base);
+}
+
 static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
 	struct crypto_skcipher_spawn *spawn)
 {
@@ -55,6 +97,25 @@ static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
 				 crypto_skcipher_mask(0)));
 }
 
+static inline struct crypto_skcipher *crypto_spawn_skcipher2(
+	struct crypto_skcipher_spawn *spawn)
+{
+	return crypto_spawn_tfm2(&spawn->base);
+}
+
+static inline void crypto_skcipher_set_reqsize(
+	struct crypto_skcipher *skcipher, unsigned int reqsize)
+{
+	skcipher->reqsize = reqsize;
+}
+
+int crypto_register_skcipher(struct skcipher_alg *alg);
+void crypto_unregister_skcipher(struct skcipher_alg *alg);
+int crypto_register_skciphers(struct skcipher_alg *algs, int count);
+void crypto_unregister_skciphers(struct skcipher_alg *algs, int count);
+int skcipher_register_instance(struct crypto_template *tmpl,
+			       struct skcipher_instance *inst);
+
 int skcipher_null_givencrypt(struct skcipher_givcrypt_request *req);
 int skcipher_null_givdecrypt(struct skcipher_givcrypt_request *req);
 const char *crypto_default_geniv(const struct crypto_alg *alg);
@@ -122,5 +183,31 @@ static inline u32 skcipher_request_flags(struct skcipher_request *req)
 	return req->base.flags;
 }
 
+static inline unsigned int crypto_skcipher_alg_min_keysize(
+	struct skcipher_alg *alg)
+{
+	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
+	    CRYPTO_ALG_TYPE_BLKCIPHER)
+		return alg->base.cra_blkcipher.min_keysize;
+
+	if (alg->base.cra_ablkcipher.encrypt)
+		return alg->base.cra_ablkcipher.min_keysize;
+
+	return alg->min_keysize;
+}
+
+static inline unsigned int crypto_skcipher_alg_max_keysize(
+	struct skcipher_alg *alg)
+{
+	if ((alg->base.cra_flags & CRYPTO_ALG_TYPE_MASK) ==
+	    CRYPTO_ALG_TYPE_BLKCIPHER)
+		return alg->base.cra_blkcipher.max_keysize;
+
+	if (alg->base.cra_ablkcipher.encrypt)
+		return alg->base.cra_ablkcipher.max_keysize;
+
+	return alg->max_keysize;
+}
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit 7a7ffe65c8c5fbf272b132d8980b2511d5e5fc98
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Aug 20 15:21:45 2015 +0800

    crypto: skcipher - Add top-level skcipher interface
    
    This patch introduces the crypto skcipher interface which aims
    to replace both blkcipher and ablkcipher.
    
    It's very similar to the existing ablkcipher interface.  The
    main difference is the removal of the givcrypt interface.  In
    order to make the transition easier for blkcipher users, there
    is a helper SKCIPHER_REQUEST_ON_STACK which can be used to place
    a request on the stack for synchronous transforms.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index b3a46c515d1b..2cf7a61ece59 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -107,5 +107,20 @@ static inline u32 ablkcipher_request_flags(struct ablkcipher_request *req)
 	return req->base.flags;
 }
 
+static inline void *crypto_skcipher_ctx(struct crypto_skcipher *tfm)
+{
+	return crypto_tfm_ctx(&tfm->base);
+}
+
+static inline void *skcipher_request_ctx(struct skcipher_request *req)
+{
+	return req->__ctx;
+}
+
+static inline u32 skcipher_request_flags(struct skcipher_request *req)
+{
+	return req->base.flags;
+}
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit d656c180de4d3eacd610c414fbcfe2509cf64a99
Author: Marek Vasut <marex@denx.de>
Date:   Mon May 26 18:51:48 2014 +0200

    crypto: skcipher - Don't use __crypto_dequeue_request()
    
    Use skcipher_givcrypt_cast(crypto_dequeue_request(queue)) instead, which
    does the same thing in much cleaner way. The skcipher_givcrypt_cast()
    actually uses container_of() instead of messing around with offsetof()
    too.
    
    Signed-off-by: Marek Vasut <marex@denx.de>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Pantelis Antoniou <panto@antoniou-consulting.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 06e8b32d541c..b3a46c515d1b 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -81,8 +81,7 @@ static inline int skcipher_enqueue_givcrypt(
 static inline struct skcipher_givcrypt_request *skcipher_dequeue_givcrypt(
 	struct crypto_queue *queue)
 {
-	return __crypto_dequeue_request(
-		queue, offsetof(struct skcipher_givcrypt_request, creq.base));
+	return skcipher_givcrypt_cast(crypto_dequeue_request(queue));
 }
 
 static inline void *skcipher_givcrypt_reqctx(

commit 1e1229940045a537c61fb69f86010a8774e576d0
Author: Steffen Klassert <steffen.klassert@secunet.com>
Date:   Thu Mar 29 09:03:47 2012 +0200

    crypto: user - Fix lookup of algorithms with IV generator
    
    We lookup algorithms with crypto_alg_mod_lookup() when instantiating via
    crypto_add_alg(). However, algorithms that are wrapped by an IV genearator
    (e.g. aead or genicv type algorithms) need special care. The userspace
    process hangs until it gets a timeout when we use crypto_alg_mod_lookup()
    to lookup these algorithms. So export the lookup functions for these
    algorithms and use them in crypto_add_alg().
    
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 3a748a6bf772..06e8b32d541c 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -34,6 +34,8 @@ static inline void crypto_set_skcipher_spawn(
 int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
 			 u32 type, u32 mask);
 
+struct crypto_alg *crypto_lookup_skcipher(const char *name, u32 type, u32 mask);
+
 static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
 {
 	crypto_drop_spawn(&spawn->base);

commit 0c7d400fafaeab6014504a6a6249f01bac7f7db4
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sat Aug 29 20:44:04 2009 +1000

    crypto: skcipher - Fix skcipher_dequeue_givcrypt NULL test
    
    As struct skcipher_givcrypt_request includes struct crypto_request
    at a non-zero offset, testing for NULL after converting the pointer
    returned by crypto_dequeue_request does not work.  This can result
    in IPsec crashes when the queue is depleted.
    
    This patch fixes it by doing the pointer conversion only when the
    return value is non-NULL.  In particular, we create a new function
    __crypto_dequeue_request that does the pointer conversion.
    
    Reported-by: Brad Bosch <bradbosch@comcast.net>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 2ba42cd7d6aa..3a748a6bf772 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -79,8 +79,8 @@ static inline int skcipher_enqueue_givcrypt(
 static inline struct skcipher_givcrypt_request *skcipher_dequeue_givcrypt(
 	struct crypto_queue *queue)
 {
-	return container_of(ablkcipher_dequeue_request(queue),
-			    struct skcipher_givcrypt_request, creq);
+	return __crypto_dequeue_request(
+		queue, offsetof(struct skcipher_givcrypt_request, creq.base));
 }
 
 static inline void *skcipher_givcrypt_reqctx(

commit 5be5e667a9a5d8d5553e009e67bc692d95e5916a
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sun Aug 17 18:04:30 2008 +1000

    crypto: skcipher - Move IV generators into their own modules
    
    This patch moves the default IV generators into their own modules
    in order to break a dependency loop between cryptomgr, rng, and
    blkcipher.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index ccc32bad9a89..2ba42cd7d6aa 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -15,7 +15,6 @@
 
 #include <crypto/algapi.h>
 #include <crypto/skcipher.h>
-#include <linux/init.h>
 #include <linux/types.h>
 
 struct rtattr;
@@ -65,11 +64,6 @@ void skcipher_geniv_free(struct crypto_instance *inst);
 int skcipher_geniv_init(struct crypto_tfm *tfm);
 void skcipher_geniv_exit(struct crypto_tfm *tfm);
 
-int __init eseqiv_module_init(void);
-void __exit eseqiv_module_exit(void);
-int __init chainiv_module_init(void);
-void chainiv_module_exit(void);
-
 static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
 	struct crypto_ablkcipher *geniv)
 {

commit f13ba2f7d3a877967477ec8f64e1dae7a967c7e2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sat Mar 8 20:29:43 2008 +0800

    [CRYPTO] skcipher: Fix section mismatches
    
    The previous patch to move chainiv and eseqiv into blkcipher created
    a section mismatch for the chainiv exit function which was also called
    from __init.  This patch removes the __exit marking on it.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index a8f12644a13c..ccc32bad9a89 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -68,7 +68,7 @@ void skcipher_geniv_exit(struct crypto_tfm *tfm);
 int __init eseqiv_module_init(void);
 void __exit eseqiv_module_exit(void);
 int __init chainiv_module_init(void);
-void __exit chainiv_module_exit(void);
+void chainiv_module_exit(void);
 
 static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
 	struct crypto_ablkcipher *geniv)

commit 76fc60a2e3c6aa6e98cd3a5cb81a1855c637b274
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Sat Feb 23 11:12:06 2008 +0800

    [CRYPTO] skcipher: Move chainiv/seqiv into crypto_blkcipher module
    
    For compatibility with dm-crypt initramfs setups it is useful to merge
    chainiv/seqiv into the crypto_blkcipher module.  Since they're required
    by most algorithms anyway this is an acceptable trade-off.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 2ba42cd7d6aa..a8f12644a13c 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -15,6 +15,7 @@
 
 #include <crypto/algapi.h>
 #include <crypto/skcipher.h>
+#include <linux/init.h>
 #include <linux/types.h>
 
 struct rtattr;
@@ -64,6 +65,11 @@ void skcipher_geniv_free(struct crypto_instance *inst);
 int skcipher_geniv_init(struct crypto_tfm *tfm);
 void skcipher_geniv_exit(struct crypto_tfm *tfm);
 
+int __init eseqiv_module_init(void);
+void __exit eseqiv_module_exit(void);
+int __init chainiv_module_init(void);
+void __exit chainiv_module_exit(void);
+
 static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
 	struct crypto_ablkcipher *geniv)
 {

commit e7cd2514ea506f06bd4f7b13a9b62afd60f9c73b
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Dec 14 22:28:14 2007 +0800

    [CRYPTO] chainiv: Avoid lock spinning where possible
    
    This patch makes chainiv avoid spinning by postponing requests on lock
    contention if the user allows the use of asynchronous algorithms.  If
    a synchronous algorithm is requested then we behave as before.
    
    This should improve IPsec performance on SMP when two CPUs attempt to
    transmit over the same SA.  Currently one of them will spin doing nothing
    waiting for the other CPU to finish its encryption.  This patch makes it
    postpone the request and get on with other work.
    
    If only one CPU is transmitting for a given SA, then we will process
    the request synchronously as before.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 0053f34764ff..2ba42cd7d6aa 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -70,6 +70,19 @@ static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
 	return crypto_ablkcipher_crt(geniv)->base;
 }
 
+static inline int skcipher_enqueue_givcrypt(
+	struct crypto_queue *queue, struct skcipher_givcrypt_request *request)
+{
+	return ablkcipher_enqueue_request(queue, &request->creq);
+}
+
+static inline struct skcipher_givcrypt_request *skcipher_dequeue_givcrypt(
+	struct crypto_queue *queue)
+{
+	return container_of(ablkcipher_dequeue_request(queue),
+			    struct skcipher_givcrypt_request, creq);
+}
+
 static inline void *skcipher_givcrypt_reqctx(
 	struct skcipher_givcrypt_request *req)
 {

commit 3631c650c495d61b1dabf32eb26b46873636e918
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Dec 13 22:28:59 2007 +0800

    [CRYPTO] null: Add null blkcipher algorithm
    
    This patch adds a null blkcipher algorithm called ecb(cipher_null) for
    backwards compatibility.  Previously the null algorithm when used by
    IPsec copied the data byte by byte.  This new algorithm optimises that
    to a straight memcpy which lets us better measure inherent overheads in
    our IPsec code.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 2071999d4b5e..0053f34764ff 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -88,5 +88,10 @@ static inline void skcipher_givcrypt_complete(
 	ablkcipher_request_complete(&req->creq, err);
 }
 
+static inline u32 ablkcipher_request_flags(struct ablkcipher_request *req)
+{
+	return req->base.flags;
+}
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit b9c55aa475599183d0eab6833ea23e70c52dd24b
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue Dec 4 12:46:48 2007 +1100

    [CRYPTO] skcipher: Create default givcipher instances
    
    This patch makes crypto_alloc_ablkcipher/crypto_grab_skcipher always
    return algorithms that are capable of generating their own IVs through
    givencrypt and givdecrypt.  Each algorithm may specify its default IV
    generator through the geniv field.
    
    For algorithms that do not set the geniv field, the blkcipher layer will
    pick a default.  Currently it's chainiv for synchronous algorithms and
    eseqiv for asynchronous algorithms.  Note that if these wrappers do not
    work on an algorithm then that algorithm must specify its own geniv or
    it can't be used at all.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 80c5bfb14a60..2071999d4b5e 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -53,6 +53,8 @@ static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
 				 crypto_skcipher_mask(0)));
 }
 
+int skcipher_null_givencrypt(struct skcipher_givcrypt_request *req);
+int skcipher_null_givdecrypt(struct skcipher_givcrypt_request *req);
 const char *crypto_default_geniv(const struct crypto_alg *alg);
 
 struct crypto_instance *skcipher_geniv_alloc(struct crypto_template *tmpl,

commit 15c67286685cddce207b646306e8819ec8268ede
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Fri Nov 30 20:17:28 2007 +1100

    [CRYPTO] skcipher: Added skcipher_givcrypt_complete
    
    This patch adds the helper skcipher_givcrypt_complete which should be
    called when an ablkcipher algorithm has completed a givcrypt request.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 07e7c82324ad..80c5bfb14a60 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -74,5 +74,17 @@ static inline void *skcipher_givcrypt_reqctx(
 	return ablkcipher_request_ctx(&req->creq);
 }
 
+static inline void ablkcipher_request_complete(struct ablkcipher_request *req,
+					       int err)
+{
+	req->base.complete(&req->base, err);
+}
+
+static inline void skcipher_givcrypt_complete(
+	struct skcipher_givcrypt_request *req, int err)
+{
+	ablkcipher_request_complete(&req->creq, err);
+}
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit ecfc43292f68566c144afca966b46b371c26d56c
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Dec 5 21:08:36 2007 +1100

    [CRYPTO] skcipher: Add skcipher_geniv_alloc/skcipher_geniv_free
    
    This patch creates the infrastructure to help the construction of givcipher
    templates that wrap around existing blkcipher/ablkcipher algorithms by adding
    an IV generator to them.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index c9402dd12d03..07e7c82324ad 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -15,6 +15,9 @@
 
 #include <crypto/algapi.h>
 #include <crypto/skcipher.h>
+#include <linux/types.h>
+
+struct rtattr;
 
 struct crypto_skcipher_spawn {
 	struct crypto_spawn base;
@@ -50,6 +53,21 @@ static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
 				 crypto_skcipher_mask(0)));
 }
 
+const char *crypto_default_geniv(const struct crypto_alg *alg);
+
+struct crypto_instance *skcipher_geniv_alloc(struct crypto_template *tmpl,
+					     struct rtattr **tb, u32 type,
+					     u32 mask);
+void skcipher_geniv_free(struct crypto_instance *inst);
+int skcipher_geniv_init(struct crypto_tfm *tfm);
+void skcipher_geniv_exit(struct crypto_tfm *tfm);
+
+static inline struct crypto_ablkcipher *skcipher_geniv_cipher(
+	struct crypto_ablkcipher *geniv)
+{
+	return crypto_ablkcipher_crt(geniv)->base;
+}
+
 static inline void *skcipher_givcrypt_reqctx(
 	struct skcipher_givcrypt_request *req)
 {

commit 61da88e2b800eed2b03834a73c46cc89ad48716d
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Dec 17 21:51:27 2007 +0800

    [CRYPTO] skcipher: Add givcrypt operations and givcipher type
    
    Different block cipher modes have different requirements for intialisation
    vectors.  For example, CBC can use a simple randomly generated IV while
    modes such as CTR must use an IV generation mechanisms that give a stronger
    guarantee on the lack of collisions.  Furthermore, disk encryption modes
    have their own IV generation algorithms.
    
    Up until now IV generation has been left to the users of the symmetric
    key cipher API.  This is inconvenient as the number of block cipher modes
    increase because the user needs to be aware of which mode is supposed to
    be paired with which IV generation algorithm.
    
    Therefore it makes sense to integrate the IV generation into the crypto
    API.  This patch takes the first step in that direction by creating two
    new ablkcipher operations, givencrypt and givdecrypt that generates an
    IV before performing the actual encryption or decryption.
    
    The operations are currently not exposed to the user.  That will be done
    once the underlying functionality has actually been implemented.
    
    It also creates the underlying givcipher type.  Algorithms that directly
    generate IVs would use it instead of ablkcipher.  All other algorithms
    (including all existing ones) would generate a givcipher algorithm upon
    registration.  This givcipher algorithm will be constructed from the geniv
    string that's stored in every algorithm.  That string will locate a template
    which is instantiated by the blkcipher/ablkcipher algorithm in question to
    give a givcipher algorithm.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
index 87879e64ff4c..c9402dd12d03 100644
--- a/include/crypto/internal/skcipher.h
+++ b/include/crypto/internal/skcipher.h
@@ -14,11 +14,14 @@
 #define _CRYPTO_INTERNAL_SKCIPHER_H
 
 #include <crypto/algapi.h>
+#include <crypto/skcipher.h>
 
 struct crypto_skcipher_spawn {
 	struct crypto_spawn base;
 };
 
+extern const struct crypto_type crypto_givcipher_type;
+
 static inline void crypto_set_skcipher_spawn(
 	struct crypto_skcipher_spawn *spawn, struct crypto_instance *inst)
 {
@@ -47,5 +50,11 @@ static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
 				 crypto_skcipher_mask(0)));
 }
 
+static inline void *skcipher_givcrypt_reqctx(
+	struct skcipher_givcrypt_request *req)
+{
+	return ablkcipher_request_ctx(&req->creq);
+}
+
 #endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
 

commit 378f4f51f9fdd8df80ea875320e2bf1d7c6e6e77
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Mon Dec 17 20:07:31 2007 +0800

    [CRYPTO] skcipher: Add crypto_grab_skcipher interface
    
    Note: From now on the collective of ablkcipher/blkcipher/givcipher will
    be known as skcipher, i.e., symmetric key cipher.  The name blkcipher has
    always been much of a misnomer since it supports stream ciphers too.
    
    This patch adds the function crypto_grab_skcipher as a new way of getting
    an ablkcipher spawn.  The problem is that previously we did this in two
    steps, first getting the algorithm and then calling crypto_init_spawn.
    
    This meant that each spawn user had to be aware of what type and mask to
    use for these two steps.  This is difficult and also presents a problem
    when the type/mask changes as they're about to be for IV generators.
    
    The new interface does both steps together just like crypto_alloc_ablkcipher.
    
    As a side-effect this also allows us to be stronger on type enforcement
    for spawns.  For now this is only done for ablkcipher but it's trivial
    to extend for other types.
    
    This patch also moves the type/mask logic for skcipher into the helpers
    crypto_skcipher_type and crypto_skcipher_mask.
    
    Finally this patch introduces the function crypto_require_sync to determine
    whether the user is specifically requesting a sync algorithm.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/include/crypto/internal/skcipher.h b/include/crypto/internal/skcipher.h
new file mode 100644
index 000000000000..87879e64ff4c
--- /dev/null
+++ b/include/crypto/internal/skcipher.h
@@ -0,0 +1,51 @@
+/*
+ * Symmetric key ciphers.
+ * 
+ * Copyright (c) 2007 Herbert Xu <herbert@gondor.apana.org.au>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; either version 2 of the License, or (at your option) 
+ * any later version.
+ *
+ */
+
+#ifndef _CRYPTO_INTERNAL_SKCIPHER_H
+#define _CRYPTO_INTERNAL_SKCIPHER_H
+
+#include <crypto/algapi.h>
+
+struct crypto_skcipher_spawn {
+	struct crypto_spawn base;
+};
+
+static inline void crypto_set_skcipher_spawn(
+	struct crypto_skcipher_spawn *spawn, struct crypto_instance *inst)
+{
+	crypto_set_spawn(&spawn->base, inst);
+}
+
+int crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn, const char *name,
+			 u32 type, u32 mask);
+
+static inline void crypto_drop_skcipher(struct crypto_skcipher_spawn *spawn)
+{
+	crypto_drop_spawn(&spawn->base);
+}
+
+static inline struct crypto_alg *crypto_skcipher_spawn_alg(
+	struct crypto_skcipher_spawn *spawn)
+{
+	return spawn->base.alg;
+}
+
+static inline struct crypto_ablkcipher *crypto_spawn_skcipher(
+	struct crypto_skcipher_spawn *spawn)
+{
+	return __crypto_ablkcipher_cast(
+		crypto_spawn_tfm(&spawn->base, crypto_skcipher_type(0),
+				 crypto_skcipher_mask(0)));
+}
+
+#endif	/* _CRYPTO_INTERNAL_SKCIPHER_H */
+
