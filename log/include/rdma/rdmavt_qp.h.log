commit 84e3b19a27f8f37c8cf98f8b7cdf3f8674bf8e97
Author: Gary Leshner <Gary.S.Leshner@intel.com>
Date:   Mon May 11 12:06:00 2020 -0400

    IB/hfi1: Remove module parameter for KDETH qpns
    
    The module parameter for KDETH qpns is being removed in favor
    of always using the default value of 0x80 as the qpn prefix.
    Defines have been added for various KDETH values including
    the prefix of 0x80.
    The reserved range now starts at the base value for KDETH
    qpns (0x80) and extends up to and including the last qpn for
    other reserved QP prefixed types.
    Adjust other QP prefixed define names to match KDETH defined
    names.
    
    Link: https://lore.kernel.org/r/20200511160600.173205.27508.stgit@awfm-01.aw.intel.com
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Gary Leshner <Gary.S.Leshner@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 982bf2340840..c4369a6c2951 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -2,7 +2,7 @@
 #define DEF_RDMAVT_INCQP_H
 
 /*
- * Copyright(c) 2016 - 2019 Intel Corporation.
+ * Copyright(c) 2016 - 2020 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.
@@ -68,6 +68,33 @@
 #define RVT_R_RSP_SEND  0x08
 #define RVT_R_COMM_EST  0x10
 
+/*
+ * If a packet's QP[23:16] bits match this value, then it is
+ * a PSM packet and the hardware will expect a KDETH header
+ * following the BTH.
+ */
+#define RVT_KDETH_QP_PREFIX       0x80
+#define RVT_KDETH_QP_SUFFIX       0xffff
+#define RVT_KDETH_QP_PREFIX_MASK  0x00ff0000
+#define RVT_KDETH_QP_PREFIX_SHIFT 16
+#define RVT_KDETH_QP_BASE         (u32)(RVT_KDETH_QP_PREFIX << \
+					RVT_KDETH_QP_PREFIX_SHIFT)
+#define RVT_KDETH_QP_MAX          (u32)(RVT_KDETH_QP_BASE + RVT_KDETH_QP_SUFFIX)
+
+/*
+ * If a packet's LNH == BTH and DEST QPN[23:16] in the BTH match this
+ * prefix value, then it is an AIP packet with a DETH containing the entropy
+ * value in byte 4 following the BTH.
+ */
+#define RVT_AIP_QP_PREFIX       0x81
+#define RVT_AIP_QP_SUFFIX       0xffff
+#define RVT_AIP_QP_PREFIX_MASK  0x00ff0000
+#define RVT_AIP_QP_PREFIX_SHIFT 16
+#define RVT_AIP_QP_BASE         (u32)(RVT_AIP_QP_PREFIX << \
+				      RVT_AIP_QP_PREFIX_SHIFT)
+#define RVT_AIP_QPN_MAX         BIT(RVT_AIP_QP_PREFIX_SHIFT)
+#define RVT_AIP_QP_MAX          (u32)(RVT_AIP_QP_BASE + RVT_AIP_QPN_MAX - 1)
+
 /*
  * Bit definitions for s_flags.
  *

commit 0cb9e4f9e98af4b969b9140e964c646ad6ed7d40
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Thu May 7 13:53:42 2020 -0500

    IB/rdmavt: Replace zero-length array with flexible-array
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    sizeof(flexible-array-member) triggers a warning because flexible array
    members have incomplete type[1]. There are some instances of code in
    which the sizeof operator is being incorrectly/erroneously applied to
    zero-length arrays and the result is zero. Such instances may be hiding
    some bugs. So, this work (flexible-array member conversions) will also
    help to get completely rid of those sorts of issues.
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Link: https://lore.kernel.org/r/20200507185342.GA14476@embeddedor
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 5fc10108703a..982bf2340840 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -440,7 +440,7 @@ struct rvt_qp {
 	/*
 	 * This sge list MUST be last. Do not add anything below here.
 	 */
-	struct rvt_sge r_sg_list[0] /* verified SGEs */
+	struct rvt_sge r_sg_list[] /* verified SGEs */
 		____cacheline_aligned_in_smp;
 };
 

commit 5b361328ca649534d721e4eae20c96ccbe702ce7
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Wed Feb 12 19:04:25 2020 -0600

    RDMA: Replace zero-length array with flexible-array member
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Link: https://lore.kernel.org/r/20200213010425.GA13068@embeddedor.com
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com> # added a few more

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0d5c70e2d8ab..5fc10108703a 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -191,7 +191,7 @@ struct rvt_swqe {
 	u32 ssn;                /* send sequence number */
 	u32 length;             /* total length of data in sg_list */
 	void *priv;             /* driver dependent field */
-	struct rvt_sge sg_list[0];
+	struct rvt_sge sg_list[];
 };
 
 /**

commit 4ad6429d2713924afe10cd960535600b241560a3
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu Dec 19 16:19:34 2019 -0500

    IB/rdmavt: Correct comments in rdmavt_qp.h header
    
    Comments need to be with the definition of rvt_restart_sge().
    
    Other comments were duplicated in sw/rdmavt/rc.c and were removed.
    
    Fixes: 385156c5f2a6 ("IB/hfi: Move RC functions into a header file")
    Link: https://lore.kernel.org/r/20191219211934.58387.88014.stgit@awfm-01.aw.intel.com
    Reviewed-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index b550ae89bf85..0d5c70e2d8ab 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -640,34 +640,14 @@ static inline int rvt_cmp_msn(u32 a, u32 b)
 	return (((int)a) - ((int)b)) << 8;
 }
 
-/**
- * rvt_compute_aeth - compute the AETH (syndrome + MSN)
- * @qp: the queue pair to compute the AETH for
- *
- * Returns the AETH.
- */
 __be32 rvt_compute_aeth(struct rvt_qp *qp);
 
-/**
- * rvt_get_credit - flush the send work queue of a QP
- * @qp: the qp who's send work queue to flush
- * @aeth: the Acknowledge Extended Transport Header
- *
- * The QP s_lock should be held.
- */
 void rvt_get_credit(struct rvt_qp *qp, u32 aeth);
 
-/**
- * rvt_restart_sge - rewind the sge state for a wqe
- * @ss: the sge state pointer
- * @wqe: the wqe to rewind
- * @len: the data length from the start of the wqe in bytes
- *
- * Returns the remaining data length.
- */
 u32 rvt_restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe, u32 len);
 
 /**
+ * rvt_div_round_up_mtu - round up divide
  * @qp - the qp pair
  * @len - the length
  *

commit 7199435414868bd656284349edc1a1f528fe3662
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Sep 11 07:30:47 2019 -0400

    IB/{rdmavt, hfi1, qib}: Add a counter for credit waits
    
    This patch adds a counter for credit waits to assist field debugging.
    
    Link: https://lore.kernel.org/r/20190911113047.126040.10857.stgit@awfm-01.aw.intel.com
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index e06c77d76463..b550ae89bf85 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -973,6 +973,41 @@ static inline void rvt_free_rq(struct rvt_rq *rq)
 	rq->wq = NULL;
 }
 
+/**
+ * rvt_to_iport - Get the ibport pointer
+ * @qp: the qp pointer
+ *
+ * This function returns the ibport pointer from the qp pointer.
+ */
+static inline struct rvt_ibport *rvt_to_iport(struct rvt_qp *qp)
+{
+	struct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);
+
+	return rdi->ports[qp->port_num - 1];
+}
+
+/**
+ * rvt_rc_credit_avail - Check if there are enough RC credits for the request
+ * @qp: the qp
+ * @wqe: the request
+ *
+ * This function returns false when there are not enough credits for the given
+ * request and true otherwise.
+ */
+static inline bool rvt_rc_credit_avail(struct rvt_qp *qp, struct rvt_swqe *wqe)
+{
+	lockdep_assert_held(&qp->s_lock);
+	if (!(qp->s_flags & RVT_S_UNLIMITED_CREDIT) &&
+	    rvt_cmp_msn(wqe->ssn, qp->s_lsn + 1) > 0) {
+		struct rvt_ibport *rvp = rvt_to_iport(qp);
+
+		qp->s_flags |= RVT_S_WAIT_SSN_CREDIT;
+		rvp->n_rc_crwaits++;
+		return false;
+	}
+	return true;
+}
+
 struct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,
 				     u64 v,
 				     void (*cb)(struct rvt_qp *qp, u64 v));

commit 2b74c878b0eae4c32629c2d5ba69a29f69048313
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Mon Jul 15 12:45:28 2019 -0400

    IB/hfi1: Unreserve a flushed OPFN request
    
    When an OPFN request is flushed, the request is completed without
    unreserving itself from the send queue. Subsequently, when a new
    request is post sent, the following warning will be triggered:
    
    WARNING: CPU: 4 PID: 8130 at rdmavt/qp.c:1761 rvt_post_send+0x72a/0x880 [rdmavt]
    Call Trace:
    [<ffffffffbbb61e41>] dump_stack+0x19/0x1b
    [<ffffffffbb497688>] __warn+0xd8/0x100
    [<ffffffffbb4977cd>] warn_slowpath_null+0x1d/0x20
    [<ffffffffc01c941a>] rvt_post_send+0x72a/0x880 [rdmavt]
    [<ffffffffbb4dcabe>] ? account_entity_dequeue+0xae/0xd0
    [<ffffffffbb61d645>] ? __kmalloc+0x55/0x230
    [<ffffffffc04e1a4c>] ib_uverbs_post_send+0x37c/0x5d0 [ib_uverbs]
    [<ffffffffc04e5e36>] ? rdma_lookup_put_uobject+0x26/0x60 [ib_uverbs]
    [<ffffffffc04dbce6>] ib_uverbs_write+0x286/0x460 [ib_uverbs]
    [<ffffffffbb6f9457>] ? security_file_permission+0x27/0xa0
    [<ffffffffbb641650>] vfs_write+0xc0/0x1f0
    [<ffffffffbb64246f>] SyS_write+0x7f/0xf0
    [<ffffffffbbb74ddb>] system_call_fastpath+0x22/0x27
    
    This patch fixes the problem by moving rvt_qp_wqe_unreserve() into
    rvt_qp_complete_swqe() to simplify the code and make it less
    error-prone.
    
    Fixes: ca95f802ef51 ("IB/hfi1: Unreserve a reserved request when it is completed")
    Link: https://lore.kernel.org/r/20190715164528.74174.31364.stgit@awfm-01.aw.intel.com
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0eeea520a853..e06c77d76463 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -608,7 +608,7 @@ static inline void rvt_qp_wqe_reserve(
 /**
  * rvt_qp_wqe_unreserve - clean reserved operation
  * @qp - the rvt qp
- * @wqe - the send wqe
+ * @flags - send wqe flags
  *
  * This decrements the reserve use count.
  *
@@ -620,11 +620,9 @@ static inline void rvt_qp_wqe_reserve(
  * the compiler does not juggle the order of the s_last
  * ring index and the decrementing of s_reserved_used.
  */
-static inline void rvt_qp_wqe_unreserve(
-	struct rvt_qp *qp,
-	struct rvt_swqe *wqe)
+static inline void rvt_qp_wqe_unreserve(struct rvt_qp *qp, int flags)
 {
-	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED)) {
+	if (unlikely(flags & RVT_SEND_RESERVE_USED)) {
 		atomic_dec(&qp->s_reserved_used);
 		/* insure no compiler re-order up to s_last change */
 		smp_mb__after_atomic();
@@ -853,6 +851,7 @@ rvt_qp_complete_swqe(struct rvt_qp *qp,
 	u32 byte_len, last;
 	int flags = wqe->wr.send_flags;
 
+	rvt_qp_wqe_unreserve(qp, flags);
 	rvt_put_qp_swqe(qp, wqe);
 
 	need_completion =

commit 2b0ad2da8fd4c32f63d9142f2de43a4d34fdd679
Author: Michael J. Ruhl <michael.j.ruhl@intel.com>
Date:   Fri Jun 28 14:22:11 2019 -0400

    IB/{rdmavt, hfi1, qib}: Add helpers to hide SWQE WR details
    
    Add some helper functions to hide struct rvt_swqe details.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 9531de2fabe2..0eeea520a853 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -219,6 +219,56 @@ struct rvt_krwq {
 	struct rvt_rwqe wq[];
 };
 
+/*
+ * rvt_get_swqe_ah - Return the pointer to the struct rvt_ah
+ * @swqe: valid Send WQE
+ *
+ */
+static inline struct rvt_ah *rvt_get_swqe_ah(struct rvt_swqe *swqe)
+{
+	return ibah_to_rvtah(swqe->ud_wr.wr.ah);
+}
+
+/**
+ * rvt_get_swqe_ah_attr - Return the cached ah attribute information
+ * @swqe: valid Send WQE
+ *
+ */
+static inline struct rdma_ah_attr *rvt_get_swqe_ah_attr(struct rvt_swqe *swqe)
+{
+	return swqe->ud_wr.attr;
+}
+
+/**
+ * rvt_get_swqe_remote_qpn - Access the remote QPN value
+ * @swqe: valid Send WQE
+ *
+ */
+static inline u32 rvt_get_swqe_remote_qpn(struct rvt_swqe *swqe)
+{
+	return swqe->ud_wr.wr.remote_qpn;
+}
+
+/**
+ * rvt_get_swqe_remote_qkey - Acces the remote qkey value
+ * @swqe: valid Send WQE
+ *
+ */
+static inline u32 rvt_get_swqe_remote_qkey(struct rvt_swqe *swqe)
+{
+	return swqe->ud_wr.wr.remote_qkey;
+}
+
+/**
+ * rvt_get_swqe_pkey_index - Access the pkey index
+ * @swqe: valid Send WQE
+ *
+ */
+static inline u16 rvt_get_swqe_pkey_index(struct rvt_swqe *swqe)
+{
+	return swqe->ud_wr.wr.pkey_index;
+}
+
 struct rvt_rq {
 	struct rvt_rwq *wq;
 	struct rvt_krwq *kwq;

commit d310c4bf8aeacc0256091feb6a0337b8fef763ac
Author: Michael J. Ruhl <michael.j.ruhl@intel.com>
Date:   Fri Jun 28 14:22:04 2019 -0400

    IB/{rdmavt, hfi1, qib}: Remove AH refcount for UD QPs
    
    Historically rdmavt destroy_ah() has returned an -EBUSY when the AH has a
    non-zero reference count.  IBTA 11.2.2 notes no such return value or error
    case:
    
            Output Modifiers:
            - Verb results:
            - Operation completed successfully.
            - Invalid HCA handle.
            - Invalid address handle.
    
    ULPs never test for this error and this will leak memory.
    
    The reference count exists to allow for driver independent progress
    mechanisms to process UD SWQEs in parallel with post sends.  The SWQE will
    hold a reference count until the UD SWQE completes and then drops the
    reference.
    
    Fix by removing need to reference count the AH.  Add a UD specific
    allocation to each SWQE entry to cache the necessary information for
    independent progress.  Copy the information during the post send
    processing.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index e4be869c4f21..9531de2fabe2 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -2,7 +2,7 @@
 #define DEF_RDMAVT_INCQP_H
 
 /*
- * Copyright(c) 2016 - 2018 Intel Corporation.
+ * Copyright(c) 2016 - 2019 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.
@@ -157,6 +157,22 @@
 #define RVT_SEND_RESERVE_USED           IB_SEND_RESERVED_START
 #define RVT_SEND_COMPLETION_ONLY	(IB_SEND_RESERVED_START << 1)
 
+/**
+ * rvt_ud_wr - IB UD work plus AH cache
+ * @wr: valid IB work request
+ * @attr: pointer to an allocated AH attribute
+ *
+ * Special case the UD WR so we can keep track of the AH attributes.
+ *
+ * NOTE: This data structure is stricly ordered wr then attr. I.e the attr
+ * MUST come after wr.  The ib_ud_wr is sized and copied in rvt_post_one_wr.
+ * The copy assumes that wr is first.
+ */
+struct rvt_ud_wr {
+	struct ib_ud_wr wr;
+	struct rdma_ah_attr *attr;
+};
+
 /*
  * Send work request queue entry.
  * The size of the sg_list is determined when the QP is created and stored
@@ -165,7 +181,7 @@
 struct rvt_swqe {
 	union {
 		struct ib_send_wr wr;   /* don't use wr.sg_list */
-		struct ib_ud_wr ud_wr;
+		struct rvt_ud_wr ud_wr;
 		struct ib_reg_wr reg_wr;
 		struct ib_rdma_wr rdma_wr;
 		struct ib_atomic_wr atomic_wr;
@@ -700,7 +716,7 @@ static inline void rvt_put_qp_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)
 {
 	rvt_put_swqe(wqe);
 	if (qp->allowed_ops == IB_OPCODE_UD)
-		atomic_dec(&ibah_to_rvtah(wqe->ud_wr.ah)->refcount);
+		rdma_destroy_ah_attr(wqe->ud_wr.attr);
 }
 
 /**

commit 5136bfea7e79b333af77594fac5bc70282a95313
Author: Kamenee Arumugam <kamenee.arumugam@intel.com>
Date:   Fri Jun 28 14:21:52 2019 -0400

    IB/{hfi1, qib, rdmavt}: Put qp in error state when cq is full
    
    When a completion queue is full, the associated queue pairs are not put
    into the error state. According to the IBTA specification, this is a
    violation.
    
    Quote from IBTA spec:
    C9-218: A Requester Class F error occurs when the CQ is inaccessible or
    full and an attempt is made to complete a WQE.  The Affected QP shall be
    moved to the error state and affiliated asynchronous errors generated as
    described in 11.6.3.1 Affiliated Asynchronous Events on page 678. The
    current WQE and any subsequent WQEs are left in an unknown state.
    
    C11-37: The CI shall generate a CQ Error when a CQ overrun is
    detected. This condition will result in an Affiliated Asynchronous Error
    for any associated Work Queues when they attempt to use that
    CQ. Completions can no longer be added to the CQ. It is not guaranteed
    that completions present in the CQ at the time the error occurred can be
    retrieved. Possible causes include a CQ overrun or a CQ protection error.
    
    Put the qp in error state when cq is full. Implement a state called full
    to continue to put other associated QPs in error state.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Kamenee Arumugam <kamenee.arumugam@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index de5915b244be..e4be869c4f21 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -718,6 +718,48 @@ rvt_qp_swqe_incr(struct rvt_qp *qp, u32 val)
 	return val;
 }
 
+int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
+
+/**
+ * rvt_recv_cq - add a new entry to completion queue
+ *			by receive queue
+ * @qp: receive queue
+ * @wc: work completion entry to add
+ * @solicited: true if @entry is solicited
+ *
+ * This is wrapper function for rvt_enter_cq function call by
+ * receive queue. If rvt_cq_enter return false, it means cq is
+ * full and the qp is put into error state.
+ */
+static inline void rvt_recv_cq(struct rvt_qp *qp, struct ib_wc *wc,
+			       bool solicited)
+{
+	struct rvt_cq *cq = ibcq_to_rvtcq(qp->ibqp.recv_cq);
+
+	if (unlikely(!rvt_cq_enter(cq, wc, solicited)))
+		rvt_error_qp(qp, IB_WC_LOC_QP_OP_ERR);
+}
+
+/**
+ * rvt_send_cq - add a new entry to completion queue
+ *                        by send queue
+ * @qp: send queue
+ * @wc: work completion entry to add
+ * @solicited: true if @entry is solicited
+ *
+ * This is wrapper function for rvt_enter_cq function call by
+ * send queue. If rvt_cq_enter return false, it means cq is
+ * full and the qp is put into error state.
+ */
+static inline void rvt_send_cq(struct rvt_qp *qp, struct ib_wc *wc,
+			       bool solicited)
+{
+	struct rvt_cq *cq = ibcq_to_rvtcq(qp->ibqp.send_cq);
+
+	if (unlikely(!rvt_cq_enter(cq, wc, solicited)))
+		rvt_error_qp(qp, IB_WC_LOC_QP_OP_ERR);
+}
+
 /**
  * rvt_qp_complete_swqe - insert send completion
  * @qp - the qp
@@ -768,9 +810,7 @@ rvt_qp_complete_swqe(struct rvt_qp *qp,
 			.qp = &qp->ibqp,
 			.byte_len = byte_len,
 		};
-
-		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &w,
-			     status != IB_WC_SUCCESS);
+		rvt_send_cq(qp, &w, status != IB_WC_SUCCESS);
 	}
 	return last;
 }
@@ -780,7 +820,6 @@ extern const int  ib_rvt_state_ops[];
 struct rvt_dev_info;
 int rvt_get_rwqe(struct rvt_qp *qp, bool wr_id_only);
 void rvt_comm_est(struct rvt_qp *qp);
-int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
 void rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);
 unsigned long rvt_rnr_tbl_to_usec(u32 index);
 enum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t);

commit f592ae3c999fbe4faeeb90dfde8ff7da49ee4ae6
Author: Kamenee Arumugam <kamenee.arumugam@intel.com>
Date:   Fri Jun 28 14:04:30 2019 -0400

    IB/rdmavt: Fracture single lock used for posting and processing RWQEs
    
    Usage of single lock prevents fetching posted and processing receive work
    queue entries from progressing simultaneously and impacts overall
    performance.
    
    Fracture the single lock used for posting and processing Receive Work
    Queue Entries (RWQEs) to allow the circular buffer to be filled and
    emptied at the same time. Two new spinlocks - one for the producers and
    one for the consumers used for posting and processing RWQEs simultaneously
    and the two indices are define on two different cache lines. The threshold
    count is used to avoid reading other index in different cache line every
    time.
    
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Kamenee Arumugam <kamenee.arumugam@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index ee55fd04f6da..de5915b244be 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -180,7 +180,9 @@ struct rvt_swqe {
 
 /**
  * struct rvt_krwq - kernel struct receive work request
+ * @p_lock: lock to protect producer of the kernel buffer
  * @head: index of next entry to fill
+ * @c_lock:lock to protect consumer of the kernel buffer
  * @tail: index of next entry to pull
  * @count: count is aproximate of total receive enteries posted
  * @rvt_rwqe: struct of receive work request queue entry
@@ -190,8 +192,13 @@ struct rvt_swqe {
  * mode user.
  */
 struct rvt_krwq {
+	spinlock_t p_lock;	/* protect producer */
 	u32 head;               /* new work requests posted to the head */
+
+	/* protect consumer */
+	spinlock_t c_lock ____cacheline_aligned_in_smp;
 	u32 tail;               /* receives pull requests from here. */
+	u32 count;		/* approx count of receive entries posted */
 	struct rvt_rwqe *curr_wq;
 	struct rvt_rwqe wq[];
 };

commit dabac6e460ce8473f1e685432a8ab7818d81a1f1
Author: Kamenee Arumugam <kamenee.arumugam@intel.com>
Date:   Fri Jun 28 14:04:24 2019 -0400

    IB/hfi1: Move receive work queue struct into uapi directory
    
    The rvt_rwqe and rvt_rwq struct elements are shared between rdmavt and the
    providers but are not in uapi directory.  As per the comment in
    https://marc.info/?l=linux-rdma&m=152296522708522&w=2, The hfi1 driver and
    the rdma core driver are not using shared structures in the uapi
    directory.
    
    Move rvt_rwqe and rvt_rwq struct into rvt-abi.h header in uapi directory.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Kamenee Arumugam <kamenee.arumugam@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 7fcd687af278..ee55fd04f6da 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -52,6 +52,7 @@
 #include <rdma/ib_pack.h>
 #include <rdma/ib_verbs.h>
 #include <rdma/rdmavt_cq.h>
+#include <rdma/rvt-abi.h>
 /*
  * Atomic bit definitions for r_aflags.
  */
@@ -177,33 +178,27 @@ struct rvt_swqe {
 	struct rvt_sge sg_list[0];
 };
 
-/*
- * Receive work request queue entry.
- * The size of the sg_list is determined when the QP (or SRQ) is created
- * and stored in qp->r_rq.max_sge (or srq->rq.max_sge).
+/**
+ * struct rvt_krwq - kernel struct receive work request
+ * @head: index of next entry to fill
+ * @tail: index of next entry to pull
+ * @count: count is aproximate of total receive enteries posted
+ * @rvt_rwqe: struct of receive work request queue entry
+ *
+ * This structure is used to contain the head pointer,
+ * tail pointer and receive work queue entries for kernel
+ * mode user.
  */
-struct rvt_rwqe {
-	u64 wr_id;
-	u8 num_sge;
-	struct ib_sge sg_list[0];
-};
-
-/*
- * This structure is used to contain the head pointer, tail pointer,
- * and receive work queue entries as a single memory allocation so
- * it can be mmap'ed into user space.
- * Note that the wq array elements are variable size so you can't
- * just index into the array to get the N'th element;
- * use get_rwqe_ptr() instead.
- */
-struct rvt_rwq {
+struct rvt_krwq {
 	u32 head;               /* new work requests posted to the head */
 	u32 tail;               /* receives pull requests from here. */
-	struct rvt_rwqe wq[0];
+	struct rvt_rwqe *curr_wq;
+	struct rvt_rwqe wq[];
 };
 
 struct rvt_rq {
 	struct rvt_rwq *wq;
+	struct rvt_krwq *kwq;
 	u32 size;               /* size of RWQE array */
 	u8 max_sge;
 	/* protect changes in this struct */
@@ -472,7 +467,7 @@ static inline struct rvt_swqe *rvt_get_swqe_ptr(struct rvt_qp *qp,
 static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 {
 	return (struct rvt_rwqe *)
-		((char *)rq->wq->wq +
+		((char *)rq->kwq->curr_wq +
 		 (sizeof(struct rvt_rwqe) +
 		  rq->max_sge * sizeof(struct ib_sge)) * n);
 }
@@ -852,6 +847,21 @@ static inline u32 ib_cq_head(struct ib_cq *send_cq)
 	       ibcq_to_rvtcq(send_cq)->kqueue->head;
 }
 
+/**
+ * rvt_free_rq - free memory allocated for rvt_rq struct
+ * @rvt_rq: request queue data structure
+ *
+ * This function should only be called if the rvt_mmap_info()
+ * has not succeeded.
+ */
+static inline void rvt_free_rq(struct rvt_rq *rq)
+{
+	kvfree(rq->kwq);
+	rq->kwq = NULL;
+	vfree(rq->wq);
+	rq->wq = NULL;
+}
+
 struct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,
 				     u64 v,
 				     void (*cb)(struct rvt_qp *qp, u64 v));

commit 239b0e52d8aa64d2559c672fd8c29cf1fffc3ec7
Author: Kamenee Arumugam <kamenee.arumugam@intel.com>
Date:   Fri Jun 28 14:04:17 2019 -0400

    IB/hfi1: Move rvt_cq_wc struct into uapi directory
    
    The rvt_cq_wc struct elements are shared between rdmavt and the providers
    but not in uapi directory.  As per the comment in
    https://marc.info/?l=linux-rdma&m=152296522708522&w=2 The hfi1 driver and
    the rdma core driver are not using shared structures in the uapi
    directory.
    
    In that case, move rvt_cq_wc struct into the rvt-abi.h header file and
    create a rvt_k_cq_w for the kernel completion queue.
    
    Signed-off-by: Kamenee Arumugam <kamenee.arumugam@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 84d0f36afc2f..7fcd687af278 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -820,6 +820,38 @@ struct rvt_qp_iter {
 	int n;
 };
 
+/**
+ * ib_cq_tail - Return tail index of cq buffer
+ * @send_cq - The cq for send
+ *
+ * This is called in qp_iter_print to get tail
+ * of cq buffer.
+ */
+static inline u32 ib_cq_tail(struct ib_cq *send_cq)
+{
+	struct rvt_cq *cq = ibcq_to_rvtcq(send_cq);
+
+	return ibcq_to_rvtcq(send_cq)->ip ?
+	       RDMA_READ_UAPI_ATOMIC(cq->queue->tail) :
+	       ibcq_to_rvtcq(send_cq)->kqueue->tail;
+}
+
+/**
+ * ib_cq_head - Return head index of cq buffer
+ * @send_cq - The cq for send
+ *
+ * This is called in qp_iter_print to get head
+ * of cq buffer.
+ */
+static inline u32 ib_cq_head(struct ib_cq *send_cq)
+{
+	struct rvt_cq *cq = ibcq_to_rvtcq(send_cq);
+
+	return ibcq_to_rvtcq(send_cq)->ip ?
+	       RDMA_READ_UAPI_ATOMIC(cq->queue->head) :
+	       ibcq_to_rvtcq(send_cq)->kqueue->head;
+}
+
 struct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,
 				     u64 v,
 				     void (*cb)(struct rvt_qp *qp, u64 v));

commit 4a9ceb7dbadf9e1435644b1f49720ee87431ce26
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu Jun 13 08:30:52 2019 -0400

    IB/{rdmavt, qib, hfi1}: Convert to new completion API
    
    Convert all completions to use the new completion routine that
    fixes a race between post send and completion where fields from
    a SWQE can be read after SWQE has been freed.
    
    This patch also addresses issues reported in
    https://marc.info/?l=linux-kernel&m=155656897409107&w=2.
    
    The reserved operation path has no need for any barrier.
    
    The barrier for the other path is addressed by the
    smp_load_acquire() barrier.
    
    Cc: Andrea Parri <andrea.parri@amarulasolutions.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 6014f1766907..84d0f36afc2f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -565,42 +565,6 @@ static inline void rvt_qp_wqe_unreserve(
 
 extern const enum ib_wc_opcode ib_rvt_wc_opcode[];
 
-/**
- * rvt_qp_swqe_complete() - insert send completion
- * @qp - the qp
- * @wqe - the send wqe
- * @status - completion status
- *
- * Insert a send completion into the completion
- * queue if the qp indicates it should be done.
- *
- * See IBTA 10.7.3.1 for info on completion
- * control.
- */
-static inline void rvt_qp_swqe_complete(
-	struct rvt_qp *qp,
-	struct rvt_swqe *wqe,
-	enum ib_wc_opcode opcode,
-	enum ib_wc_status status)
-{
-	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED))
-		return;
-	if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
-	    (wqe->wr.send_flags & IB_SEND_SIGNALED) ||
-	     status != IB_WC_SUCCESS) {
-		struct ib_wc wc;
-
-		memset(&wc, 0, sizeof(wc));
-		wc.wr_id = wqe->wr.wr_id;
-		wc.status = status;
-		wc.opcode = opcode;
-		wc.qp = &qp->ibqp;
-		wc.byte_len = wqe->length;
-		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc,
-			     status != IB_WC_SUCCESS);
-	}
-}
-
 /*
  * Compare the lower 24 bits of the msn values.
  * Returns an integer <, ==, or > than zero.

commit f56044d686c82bd31713fc0398d68e322813dc62
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu Jun 13 08:30:44 2019 -0400

    IB/rdmavt: Add new completion inline
    
    There is opencoded send completion logic all over all
    the drivers.
    
    We need to convert to this routine to enforce ordering
    issues for completions.  This routine fixes an ordering
    issue where the read of the SWQE fields necessary for creating
    the completion can race with a post send if the post send catches
    a send queue at the edge of being full.  Is is possible in that situation
    to read SWQE fields that are being written.
    
    This new routine insures that SWQE fields are read prior to advancing
    the index that post send uses to determine queue fullness.
    
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 68e38c20afc0..6014f1766907 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -737,6 +737,78 @@ static inline void rvt_put_qp_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)
 		atomic_dec(&ibah_to_rvtah(wqe->ud_wr.ah)->refcount);
 }
 
+/**
+ * rvt_qp_sqwe_incr - increment ring index
+ * @qp: the qp
+ * @val: the starting value
+ *
+ * Return: the new value wrapping as appropriate
+ */
+static inline u32
+rvt_qp_swqe_incr(struct rvt_qp *qp, u32 val)
+{
+	if (++val >= qp->s_size)
+		val = 0;
+	return val;
+}
+
+/**
+ * rvt_qp_complete_swqe - insert send completion
+ * @qp - the qp
+ * @wqe - the send wqe
+ * @opcode - wc operation (driver dependent)
+ * @status - completion status
+ *
+ * Update the s_last information, and then insert a send
+ * completion into the completion
+ * queue if the qp indicates it should be done.
+ *
+ * See IBTA 10.7.3.1 for info on completion
+ * control.
+ *
+ * Return: new last
+ */
+static inline u32
+rvt_qp_complete_swqe(struct rvt_qp *qp,
+		     struct rvt_swqe *wqe,
+		     enum ib_wc_opcode opcode,
+		     enum ib_wc_status status)
+{
+	bool need_completion;
+	u64 wr_id;
+	u32 byte_len, last;
+	int flags = wqe->wr.send_flags;
+
+	rvt_put_qp_swqe(qp, wqe);
+
+	need_completion =
+		!(flags & RVT_SEND_RESERVE_USED) &&
+		(!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
+		(flags & IB_SEND_SIGNALED) ||
+		status != IB_WC_SUCCESS);
+	if (need_completion) {
+		wr_id = wqe->wr.wr_id;
+		byte_len = wqe->length;
+		/* above fields required before writing s_last */
+	}
+	last = rvt_qp_swqe_incr(qp, qp->s_last);
+	/* see rvt_qp_is_avail() */
+	smp_store_release(&qp->s_last, last);
+	if (need_completion) {
+		struct ib_wc w = {
+			.wr_id = wr_id,
+			.status = status,
+			.opcode = opcode,
+			.qp = &qp->ibqp,
+			.byte_len = byte_len,
+		};
+
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &w,
+			     status != IB_WC_SUCCESS);
+	}
+	return last;
+}
+
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;

commit d40f69c9b9dff3e47d9647943db267b5792ae215
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Fri Apr 12 06:41:42 2019 -0700

    IB/{rdmavt, qib, hfi1}: Use new routine to release reference counts
    
    The reference count adjustments on reference count completion
    are open coded throughout.
    
    Add a routine to do all reference count adjustments and use.
    
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index a00c46a4e779..68e38c20afc0 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -723,6 +723,20 @@ static inline void rvt_mod_retry_timer(struct rvt_qp *qp)
 	return rvt_mod_retry_timer_ext(qp, 0);
 }
 
+/**
+ * rvt_put_qp_swqe - drop refs held by swqe
+ * @qp: the send qp
+ * @wqe: the send wqe
+ *
+ * This drops any references held by the swqe
+ */
+static inline void rvt_put_qp_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)
+{
+	rvt_put_swqe(wqe);
+	if (qp->allowed_ops == IB_OPCODE_UD)
+		atomic_dec(&ibah_to_rvtah(wqe->ud_wr.ah)->refcount);
+}
+
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;

commit 715ab1a862c85b08a9881851c7b1fba84b0dc26b
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu Apr 11 07:16:11 2019 -0700

    IB/rdmavt: Fix ab/ba include issues
    
    The currently include file ordering for rdmavt headers has an
    ab/ba include issue the precludes using inlines from rdma_vt.h
    in rdmavt_qp.h.
    
    At the heart of the issue is that rdma_vt.h includes rdmavt_qp.h.
    
    Fix the ordering issue by adjusting rdma_vt.h to not require rdmavt_qp.h
    and move qp related inlines to rdmavt_qp.h.
    
    Additionally, promote rvt_mmap_info to rdma_vt.h since it is shared
    by rdmavt_cq.h and rdmavt_qp.h.
    
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0ad89867b258..a00c46a4e779 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -210,20 +210,6 @@ struct rvt_rq {
 	spinlock_t lock ____cacheline_aligned_in_smp;
 };
 
-/*
- * This structure is used by rvt_mmap() to validate an offset
- * when an mmap() request is made.  The vm_area_struct then uses
- * this as its vm_private_data.
- */
-struct rvt_mmap_info {
-	struct list_head pending_mmaps;
-	struct ib_ucontext *context;
-	void *obj;
-	__u64 offset;
-	struct kref ref;
-	unsigned size;
-};
-
 /*
  * This structure holds the information that the send tasklet needs
  * to send a RDMA read response or atomic operation.
@@ -398,6 +384,16 @@ struct rvt_srq {
 	u32 limit;
 };
 
+static inline struct rvt_srq *ibsrq_to_rvtsrq(struct ib_srq *ibsrq)
+{
+	return container_of(ibsrq, struct rvt_srq, ibsrq);
+}
+
+static inline struct rvt_qp *ibqp_to_rvtqp(struct ib_qp *ibqp)
+{
+	return container_of(ibqp, struct rvt_qp, ibqp);
+}
+
 #define RVT_QPN_MAX                 BIT(24)
 #define RVT_QPNMAP_ENTRIES          (RVT_QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
 #define RVT_BITS_PER_PAGE           (PAGE_SIZE * BITS_PER_BYTE)
@@ -677,6 +673,56 @@ static inline unsigned long rvt_timeout_to_jiffies(u8 timeout)
 	return usecs_to_jiffies(1U << timeout) * 4096UL / 1000UL;
 }
 
+/**
+ * rvt_lookup_qpn - return the QP with the given QPN
+ * @ibp: the ibport
+ * @qpn: the QP number to look up
+ *
+ * The caller must hold the rcu_read_lock(), and keep the lock until
+ * the returned qp is no longer in use.
+ */
+static inline struct rvt_qp *rvt_lookup_qpn(struct rvt_dev_info *rdi,
+					    struct rvt_ibport *rvp,
+					    u32 qpn) __must_hold(RCU)
+{
+	struct rvt_qp *qp = NULL;
+
+	if (unlikely(qpn <= 1)) {
+		qp = rcu_dereference(rvp->qp[qpn]);
+	} else {
+		u32 n = hash_32(qpn, rdi->qp_dev->qp_table_bits);
+
+		for (qp = rcu_dereference(rdi->qp_dev->qp_table[n]); qp;
+			qp = rcu_dereference(qp->next))
+			if (qp->ibqp.qp_num == qpn)
+				break;
+	}
+	return qp;
+}
+
+/**
+ * rvt_mod_retry_timer - mod a retry timer
+ * @qp - the QP
+ * @shift - timeout shift to wait for multiple packets
+ * Modify a potentially already running retry timer
+ */
+static inline void rvt_mod_retry_timer_ext(struct rvt_qp *qp, u8 shift)
+{
+	struct ib_qp *ibqp = &qp->ibqp;
+	struct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);
+
+	lockdep_assert_held(&qp->s_lock);
+	qp->s_flags |= RVT_S_TIMER;
+	/* 4.096 usec. * (1 << qp->timeout) */
+	mod_timer(&qp->s_timer, jiffies + rdi->busy_jiffies +
+		  (qp->timeout_jiffies << shift));
+}
+
+static inline void rvt_mod_retry_timer(struct rvt_qp *qp)
+{
+	return rvt_mod_retry_timer_ext(qp, 0);
+}
+
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;

commit ea752bc5e50a03e337dfa5c8940d357c62300f8a
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Thu Apr 11 07:15:49 2019 -0700

    IB/{rdmavt, hfi1): Miscellaneous comment fixes
    
    This patch fixes miscellaneous comment errors.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f0fbd4063fef..0ad89867b258 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -83,7 +83,6 @@
  * RVT_S_WAIT_DMA - waiting for send DMA queue to drain before generating
  *                  next send completion entry not via send DMA
  * RVT_S_WAIT_PIO - waiting for a send buffer to be available
- * RVT_S_WAIT_PIO_DRAIN - waiting for a qp to drain pio packets
  * RVT_S_WAIT_TX - waiting for a struct verbs_txreq to be available
  * RVT_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
  * RVT_S_WAIT_KMEM - waiting for kernel memory to be available

commit 3c6cb20a0d17d7a75778fb0935d6fa427c8177af
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 21:51:39 2019 -0800

    IB/hfi1: Add TID RDMA WRITE functionality into RDMA verbs
    
    This patch integrates TID RDMA WRITE protocol into normal RDMA verbs
    framework. The TID RDMA WRITE protocol is an end-to-end protocol
    between the hfi1 drivers on two OPA nodes that converts a qualified
    RDMA WRITE request into a TID RDMA WRITE request to avoid data copying
    on the responder side.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 4ee612ab6cb4..f0fbd4063fef 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -246,6 +246,7 @@ struct rvt_ack_entry {
 #define RVT_OPERATION_ATOMIC_SGE  0x00000004
 #define RVT_OPERATION_LOCAL       0x00000008
 #define RVT_OPERATION_USE_RESERVE 0x00000010
+#define RVT_OPERATION_IGN_RNR_CNT 0x00000020
 
 #define RVT_OPERATION_MAX (IB_WR_RESERVED10 + 1)
 

commit 4f9264d156dc6c154a8a6cfae780730bad45c6f8
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 21:48:48 2019 -0800

    IB/hfi1: Add an s_acked_ack_queue pointer
    
    The s_ack_queue is managed by two pointers into the ring:
    r_head_ack_queue and s_tail_ack_queue. r_head_ack_queue is the index of
    where the next received request is going to be placed and s_tail_ack_queue
    is the entry of the request currently being processed. This works
    perfectly fine for normal Verbs as the requests are processed one at a
    time and the s_tail_ack_queue is not moved until the request that it
    points to is fully completed.
    
    In this fashion, s_tail_ack_queue constantly chases r_head_ack_queue and
    the two pointers can easily be used to determine "queue full" and "queue
    empty" conditions.
    
    The detection of these two conditions are imported in determining when an
    old entry can safely be overwritten with a new received request and the
    resources associated with the old request be safely released.
    
    When pipelined TID RDMA WRITE is introduced into this mix, things look
    very different. r_head_ack_queue is still the point at which a newly
    received request will be inserted, s_tail_ack_queue is still the
    currently processed request. However, with pipelined TID RDMA WRITE
    requests, s_tail_ack_queue moves to the next request once all TID RDMA
    WRITE responses for that request have been sent. The rest of the protocol
    for a particular request is managed by other pointers specific to TID RDMA
    - r_tid_tail and r_tid_ack - which point to the entries for which the next
    TID RDMA DATA packets are going to arrive and the request for which
    the next TID RDMA ACK packets are to be generated, respectively.
    
    What this means is that entries in the ring, which are "behind"
    s_tail_ack_queue (entries which s_tail_ack_queue has gone past) are no
    longer considered complete. This is where the problem is - a newly
    received request could potentially overwrite a still active TID RDMA WRITE
    request.
    
    The reason why the TID RDMA pointers trail s_tail_ack_queue is that the
    normal Verbs send engine uses s_tail_ack_queue as the pointer for the next
    response. Since TID RDMA WRITE responses are processed by the normal Verbs
    send engine, s_tail_ack_queue had to be moved to the next entry once all
    TID RDMA WRITE response packets were sent to get the desired pipelining
    between requests. Doing otherwise would mean that the normal Verbs send
    engine would not be able to send the TID RDMA WRITE responses for the next
    TID RDMA request until the current one is fully completed.
    
    This patch introduces the s_acked_ack_queue index to point to the next
    request to complete on the responder side. For requests other than TID
    RDMA WRITE, s_acked_ack_queue should always be kept in sync with
    s_tail_ack_queue. For TID RDMA WRITE request, it may fall behind
    s_tail_ack_queue.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index d8d88d023092..4ee612ab6cb4 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -375,6 +375,7 @@ struct rvt_qp {
 	u8 s_rnr_retry;         /* requester RNR retry counter */
 	u8 s_num_rd_atomic;     /* number of RDMA read/atomic pending */
 	u8 s_tail_ack_queue;    /* index into s_ack_queue[] */
+	u8 s_acked_ack_queue;   /* index into s_ack_queue[] */
 
 	struct rvt_sge_state s_ack_rdma_sge;
 	struct timer_list s_timer;

commit 039cd3daf19b9acbf080054d765cbceac842b6a0
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 19:31:57 2019 -0800

    IB/hfi1: Increment the retry timeout value for TID RDMA READ request
    
    The RC retry timeout value is based on the estimated time for the
    response packet to come back. However, for TID RDMA READ request, due
    to the use of header suppression, the driver is normally not notified
    for each incoming response packet until the last TID RDMA READ response
    packet. Consequently, the retry timeout value should be extended to
    cover the transaction time for the entire length of a segment (default
    256K) instead of that for a single packet. This patch addresses the
    issue by introducing new retry timer functions to account for multiple
    packets and wrapper functions for backward compatibility.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 9095a0b71250..d8d88d023092 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -688,7 +688,11 @@ enum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t);
 void rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth);
 void rvt_del_timers_sync(struct rvt_qp *qp);
 void rvt_stop_rc_timers(struct rvt_qp *qp);
-void rvt_add_retry_timer(struct rvt_qp *qp);
+void rvt_add_retry_timer_ext(struct rvt_qp *qp, u8 shift);
+static inline void rvt_add_retry_timer(struct rvt_qp *qp)
+{
+	rvt_add_retry_timer_ext(qp, 0);
+}
 
 void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
 		  void *data, u32 length,

commit 838b6fd2d9ca29998869e4d1ecf4566efe807666
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 19:30:07 2019 -0800

    IB/hfi1: TID RDMA RcvArray programming and TID allocation
    
    TID entries are used by hfi1 hardware to receive data payload from
    incoming packets directly into a user buffer and thus avoid data copying
    by software. This patch implements the functions for TID allocation,
    freeing, and programming TID RcvArray entries in hardware for kernel
    clients. TID entries are managed via lists of TID groups similar to PSM.
    Furthermore, to track TID resource allocation for each request, software
    flows are also allocated and freed as needed. Since software flows
    consume large amount of memory for tracking TID allocation and freeing,
    it is generally desirable to allocate them dynamically in the send queue
    and only for TID RDMA requests, but pre-allocate them for receive queue
    because the send queue could have thousands of entries while the receive
    queue has only a limited number of entries.
    
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 56a9221378d9..9095a0b71250 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -174,6 +174,7 @@ struct rvt_swqe {
 	u32 lpsn;               /* last packet sequence number */
 	u32 ssn;                /* send sequence number */
 	u32 length;             /* total length of data in sg_list */
+	void *priv;             /* driver dependent field */
 	struct rvt_sge sg_list[0];
 };
 
@@ -235,6 +236,7 @@ struct rvt_ack_entry {
 	u32 lpsn;
 	u8 opcode;
 	u8 sent;
+	void *priv;
 };
 
 #define	RC_QP_SCALING_INTERVAL	5

commit 385156c5f2a61834666f079ee66338f177c65c28
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 19:29:44 2019 -0800

    IB/hfi: Move RC functions into a header file
    
    This patch moves some RC helper functions into a header file so that
    they can be called from both RC and  TID RDMA functions. In addition,
    a common function for rewinding a request is created in rdmavt so that
    it can be shared between qib and hfi1 driver.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index cbafb1878669..56a9221378d9 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -628,6 +628,16 @@ __be32 rvt_compute_aeth(struct rvt_qp *qp);
  */
 void rvt_get_credit(struct rvt_qp *qp, u32 aeth);
 
+/**
+ * rvt_restart_sge - rewind the sge state for a wqe
+ * @ss: the sge state pointer
+ * @wqe: the wqe to rewind
+ * @len: the data length from the start of the wqe in bytes
+ *
+ * Returns the remaining data length.
+ */
+u32 rvt_restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe, u32 len);
+
 /**
  * @qp - the qp pair
  * @len - the length

commit 15703461533a5ffd775722390431625daaae7618
Author: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
Date:   Wed Sep 26 10:44:52 2018 -0700

    IB/{hfi1, qib, rdmavt}: Move ruc_loopback to rdmavt
    
    This patch moves ruc_loopback() from hfi1 into rdmavt for code sharing
    with the qib driver.
    
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 6fd6f2ad9c0f..cbafb1878669 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -683,6 +683,7 @@ void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
 		  bool release, bool copy_last);
 void rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
 		       enum ib_wc_status status);
+void rvt_ruc_loopback(struct rvt_qp *qp);
 
 /**
  * struct rvt_qp_iter - the iterator for QPs

commit 116aa0330ec71b9872dfb3a3cc5202a72b5a1666
Author: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
Date:   Wed Sep 26 10:44:42 2018 -0700

    IB/{hfi1, qib, rdmavt}: Move send completion logic to rdmavt
    
    Moving send completion code into rdmavt in order to have shared logic
    between qib and hfi1 drivers.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index eaf2593ca822..6fd6f2ad9c0f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -681,6 +681,8 @@ void rvt_add_retry_timer(struct rvt_qp *qp);
 void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
 		  void *data, u32 length,
 		  bool release, bool copy_last);
+void rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,
+		       enum ib_wc_status status);
 
 /**
  * struct rvt_qp_iter - the iterator for QPs

commit 019f118b94c895294debfaa394b267638fe2f648
Author: Brian Welty <brian.welty@intel.com>
Date:   Wed Sep 26 10:44:33 2018 -0700

    IB/{hfi1, qib, rdmavt}: Move copy SGE logic into rdmavt
    
    This patch moves hfi1_copy_sge() into rdmavt for sharing with qib.
    This patch also moves all the wss_*() functions into rdmavt as
    several wss_*() functions are called from hfi1_copy_sge()
    
    When SGE copy mode is adaptive, cacheless copy may be done in some cases
    for performance reasons. In those cases, X86 cacheless copy function
    is called since the drivers that use rdmavt and may set SGE copy mode
    to adaptive are X86 only. For this reason, this patch adds
    "depends on X86_64" to rdmavt/Kconfig.
    
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 927f6d5b6d0f..eaf2593ca822 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -678,6 +678,10 @@ void rvt_del_timers_sync(struct rvt_qp *qp);
 void rvt_stop_rc_timers(struct rvt_qp *qp);
 void rvt_add_retry_timer(struct rvt_qp *qp);
 
+void rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,
+		  void *data, u32 length,
+		  bool release, bool copy_last);
+
 /**
  * struct rvt_qp_iter - the iterator for QPs
  * @qp - the current QP

commit 2e2ba09e48b72497a9b69fc49ab693f7f0c2e5cf
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Jun 4 11:44:02 2018 -0700

    IB/rdmavt, IB/hfi1: Create device dependent s_flags
    
    Move some s_flags defines out of rdmavt and into hfi1 because they are
    hfi1 specific and therefore should remain in the driver instead of
    bubbling up to rdmavt.
    
    Document device specific ranges in rdmavt and remap
    those in hfi1.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 1145a4c154b2..927f6d5b6d0f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -2,7 +2,7 @@
 #define DEF_RDMAVT_INCQP_H
 
 /*
- * Copyright(c) 2016, 2017 Intel Corporation.
+ * Copyright(c) 2016 - 2018 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.
@@ -91,6 +91,7 @@
  * RVT_S_WAIT_ACK - waiting for an ACK packet before sending more requests
  * RVT_S_SEND_ONE - send one packet, request ACK, then wait for ACK
  * RVT_S_ECN - a BECN was queued to the send engine
+ * RVT_S_MAX_BIT_MASK - The max bit that can be used by rdmavt
  */
 #define RVT_S_SIGNAL_REQ_WR	0x0001
 #define RVT_S_BUSY		0x0002
@@ -103,23 +104,26 @@
 #define RVT_S_WAIT_SSN_CREDIT	0x0100
 #define RVT_S_WAIT_DMA		0x0200
 #define RVT_S_WAIT_PIO		0x0400
-#define RVT_S_WAIT_PIO_DRAIN    0x0800
-#define RVT_S_WAIT_TX		0x1000
-#define RVT_S_WAIT_DMA_DESC	0x2000
-#define RVT_S_WAIT_KMEM		0x4000
-#define RVT_S_WAIT_PSN		0x8000
-#define RVT_S_WAIT_ACK		0x10000
-#define RVT_S_SEND_ONE		0x20000
-#define RVT_S_UNLIMITED_CREDIT	0x40000
-#define RVT_S_AHG_VALID		0x80000
-#define RVT_S_AHG_CLEAR		0x100000
-#define RVT_S_ECN		0x200000
+#define RVT_S_WAIT_TX		0x0800
+#define RVT_S_WAIT_DMA_DESC	0x1000
+#define RVT_S_WAIT_KMEM		0x2000
+#define RVT_S_WAIT_PSN		0x4000
+#define RVT_S_WAIT_ACK		0x8000
+#define RVT_S_SEND_ONE		0x10000
+#define RVT_S_UNLIMITED_CREDIT	0x20000
+#define RVT_S_ECN		0x40000
+#define RVT_S_MAX_BIT_MASK	0x800000
+
+/*
+ * Drivers should use s_flags starting with bit 31 down to the bit next to
+ * RVT_S_MAX_BIT_MASK
+ */
 
 /*
  * Wait flags that would prevent any packet type from being sent.
  */
 #define RVT_S_ANY_WAIT_IO \
-	(RVT_S_WAIT_PIO | RVT_S_WAIT_PIO_DRAIN | RVT_S_WAIT_TX | \
+	(RVT_S_WAIT_PIO | RVT_S_WAIT_TX | \
 	 RVT_S_WAIT_DMA_DESC | RVT_S_WAIT_KMEM)
 
 /*

commit 832369fa6410c93547264ad449ebbf16567bbccd
Author: Brian Welty <brian.welty@intel.com>
Date:   Wed May 2 06:44:03 2018 -0700

    IB/{hfi1, qib, rdmavt}: Move logic to allocate receive WQE into rdmavt
    
    Moving receive-side WQE allocation logic into rdmavt will allow
    further code reuse between qib and hfi1 drivers.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 89ab88c342b6..1145a4c154b2 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -663,6 +663,7 @@ static inline unsigned long rvt_timeout_to_jiffies(u8 timeout)
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;
+int rvt_get_rwqe(struct rvt_qp *qp, bool wr_id_only);
 void rvt_comm_est(struct rvt_qp *qp);
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
 void rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);

commit 7ebfc93edcefa0c4c09f194ebdace9ddae15efa6
Author: Sebastian Sanchez <sebastian.sanchez@intel.com>
Date:   Mon Oct 2 11:04:41 2017 -0700

    IB/rdmavt: Correct issues with read-mostly and send size cache lines
    
    The s_ahgpsn was incorrectly placed in the read-mostly section of the QP
    and the s_curr_size and s_hdrwords are oversized. The misplaced
    s_ahgpsn will cause the read-mostly cachelines to thrash.
    
    Place s_ahgpsn in the send side cache lines and correctly size and
    s_hdrwords and s_cur_size to keep the send side cachelines at the same
    size.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0eed3d8752fa..89ab88c342b6 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -282,7 +282,6 @@ struct rvt_qp {
 	u32 remote_qpn;
 	u32 qkey;               /* QKEY for this QP (for UD or RD) */
 	u32 s_size;             /* send work queue size */
-	u32 s_ahgpsn;           /* set to the psn in the copy of the header */
 
 	u16 pmtu;		/* decoded from path_mtu */
 	u8 log_pmtu;		/* shift for pmtu */
@@ -344,7 +343,6 @@ struct rvt_qp {
 	struct rvt_swqe *s_wqe;
 	struct rvt_sge_state s_sge;     /* current send request data */
 	struct rvt_mregion *s_rdma_mr;
-	u32 s_cur_size;         /* size of send packet in bytes */
 	u32 s_len;              /* total length of s_sge */
 	u32 s_rdma_read_len;    /* total length of s_rdma_read_sge */
 	u32 s_last_psn;         /* last response PSN processed */
@@ -358,8 +356,10 @@ struct rvt_qp {
 	u32 s_acked;            /* last un-ACK'ed entry */
 	u32 s_last;             /* last completed entry */
 	u32 s_lsn;              /* limit sequence number (credit) */
-	u16 s_hdrwords;         /* size of s_hdr in 32 bit words */
+	u32 s_ahgpsn;           /* set to the psn in the copy of the header */
+	u16 s_cur_size;         /* size of send packet in bytes */
 	u16 s_rdma_ack_cnt;
+	u8 s_hdrwords;         /* size of s_hdr in 32 bit words */
 	s8 s_ahgidx;
 	u8 s_state;             /* opcode of last packet sent */
 	u8 s_ack_state;         /* opcode of packet to ACK */

commit 0208da90def5776cef940f9de4ffe6ecef346207
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Aug 28 11:24:10 2017 -0700

    IB/rdmavt: Handle dereg of inuse MRs properly
    
    A destroy of an MR prior to destroying the QP can cause the following
    diagnostic if the QP is referencing the MR being de-registered:
    
    hfi1 0000:05:00.0: hfi1_0: rvt_dereg_mr timeout mr ffff8808562108
                  00 pd ffff880859b20b00
    
    The solution is to when the a non-zero refcount is encountered when
    the MR is destroyed the QPs needs to be iterated looking for QPs in
    the same PD as the MR.  If rvt_qp_mr_clean() detects any such QP
    references the rkey/lkey, the QP needs to be put into an error state
    via a call to rvt_qp_error() which will trigger the clean up of any
    stuck references.
    
    This solution is as specified in IBTA 1.3 Volume 1 11.2.10.5.
    
    [This is reproduced with the 0.4.9 version of qperf and the rc_bw test]
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index dfeb311c30a1..0eed3d8752fa 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -702,4 +702,5 @@ int rvt_qp_iter_next(struct rvt_qp_iter *iter);
 void rvt_qp_iter(struct rvt_dev_info *rdi,
 		 u64 v,
 		 void (*cb)(struct rvt_qp *qp, u64 v));
+void rvt_qp_mr_clean(struct rvt_qp *qp, u32 lkey);
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 4734b4f417126e8773b3983122ca935d02af80de
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Aug 28 11:23:45 2017 -0700

    IB/rdmavt: Add QP iterator API for QPs
    
    There are currently 3 spots in the qib and hfi1 driver that have
    knowledge of the internal QP hash list that should only be in
    scope to rdmavt QP code.
    
    Add an iterator API for processing all QPs to hide the
    nature of the RCU hashlist.
    
    The API consists of:
    - rvt_qp_iter_init()
      * For iterating QPs one at a time for seq_file semantics
    - rvt_qp_iter_next()
      * For iterating QPs one at a time for seq_file semantics
    - rvt_qp_iter()
      * For iterating all QPs
    
    The first two are used for things like seq_file prints.
    
    The last is for code that just needs to iterate all QPs
    in the system.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 8fbafb0ce674..dfeb311c30a1 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -673,4 +673,33 @@ void rvt_del_timers_sync(struct rvt_qp *qp);
 void rvt_stop_rc_timers(struct rvt_qp *qp);
 void rvt_add_retry_timer(struct rvt_qp *qp);
 
+/**
+ * struct rvt_qp_iter - the iterator for QPs
+ * @qp - the current QP
+ *
+ * This structure defines the current iterator
+ * state for sequenced access to all QPs relative
+ * to an rvt_dev_info.
+ */
+struct rvt_qp_iter {
+	struct rvt_qp *qp;
+	/* private: backpointer */
+	struct rvt_dev_info *rdi;
+	/* private: callback routine */
+	void (*cb)(struct rvt_qp *qp, u64 v);
+	/* private: for arg to callback routine */
+	u64 v;
+	/* private: number of SMI,GSI QPs for device */
+	int specials;
+	/* private: current iterator index */
+	int n;
+};
+
+struct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,
+				     u64 v,
+				     void (*cb)(struct rvt_qp *qp, u64 v));
+int rvt_qp_iter_next(struct rvt_qp_iter *iter);
+void rvt_qp_iter(struct rvt_dev_info *rdi,
+		 u64 v,
+		 void (*cb)(struct rvt_qp *qp, u64 v));
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 16570d3da0938e0c46c31e5f97c9c8452025d2e7
Author: Sebastian Sanchez <sebastian.sanchez@intel.com>
Date:   Fri Aug 4 13:52:20 2017 -0700

    IB/hfi1: Remove pmtu from the QP structure
    
    The pmtu field doens't have be stored in the QP structure
    as it can easily be calculated when needed.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 07e2fffa6de6..8fbafb0ce674 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -277,7 +277,6 @@ struct rvt_qp {
 
 	unsigned long timeout_jiffies;  /* computed from timeout */
 
-	enum ib_mtu path_mtu;
 	int srate_mbps;		/* s_srate (below) converted to Mbit/s */
 	pid_t pid;		/* pid for user mode QPs */
 	u32 remote_qpn;

commit 03da084ed8804ddc5918883be84245b53393b8fb
Merge: 520eccdfe187 bc5214ee2922
Author: Doug Ledford <dledford@redhat.com>
Date:   Mon Jul 24 08:33:43 2017 -0400

    Merge branch 'hfi1' into k.o/for-4.14

commit a25ce4270bfdd522207b02f81a594c7d1746b697
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Sat Jun 17 10:37:26 2017 -0700

    IB/rdmavt: Setting of QP timeout can overflow jiffies computation
    
    Current computation of qp->timeout_jiffies in rvt_modify_qp() will cause
    overflow due to the fact that the input to the function usecs_to_jiffies
    is only 32-bit ( unsigned int). Overflow will occur when attr->timeout is
    equal to or greater than 30. The consequence is unnecessarily excessive
    retry and thus degradation of the system performance.
    
    This patch fixes the problem by limiting the input to 5-bit and calling
    usecs_to_jiffies() before multiplying the scaling factor.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index be6472e5b06b..d664d2e76280 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -647,6 +647,20 @@ static inline u32 rvt_div_mtu(struct rvt_qp *qp, u32 len)
 	return len >> qp->log_pmtu;
 }
 
+/**
+ * rvt_timeout_to_jiffies - Convert a ULP timeout input into jiffies
+ * @timeout - timeout input(0 - 31).
+ *
+ * Return a timeout value in jiffies.
+ */
+static inline unsigned long rvt_timeout_to_jiffies(u8 timeout)
+{
+	if (timeout > 31)
+		timeout = 31;
+
+	return usecs_to_jiffies(1U << timeout) * 4096UL / 1000UL;
+}
+
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;

commit 7dafbab3753fcf59bc81748e5b2c5bf04e1c62c7
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Fri May 12 09:19:55 2017 -0700

    IB/hfi1: Add functions to parse BTH/IB headers
    
    Improve code readablity by adding inline functions
    to read specific BTH/IB fields without knowledge of
    byte offsets.
    
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Reviewed-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index be6472e5b06b..13f43b3527a8 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -396,7 +396,7 @@ struct rvt_srq {
 #define RVT_QPNMAP_ENTRIES          (RVT_QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
 #define RVT_BITS_PER_PAGE           (PAGE_SIZE * BITS_PER_BYTE)
 #define RVT_BITS_PER_PAGE_MASK      (RVT_BITS_PER_PAGE - 1)
-#define RVT_QPN_MASK		    0xFFFFFF
+#define RVT_QPN_MASK		    IB_QPN_MASK
 
 /*
  * QPN-map pages start out as NULL, they get allocated upon

commit 688f21c0be9e3ddd6c5b2241daeb9fe978c0e081
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu May 4 05:14:04 2017 -0700

    IB/hfi1, IB/rdmavt: Move r_adefered to r_lock cache line
    
    This field is causing excessive cache line bouncing.
    
    There are spare bytes in the r_lock cache line so the best approach
    is to make an rvt QP field and remove from the hfi1 priv field.
    
    Signed-off-by: Sebastian Sanchez <sebastian.sanchez@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 1d8141a88d3c..be6472e5b06b 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -324,6 +324,7 @@ struct rvt_qp {
 	u8 r_state;             /* opcode of last packet received */
 	u8 r_flags;
 	u8 r_head_ack_queue;    /* index into s_ack_queue[] */
+	u8 r_adefered;          /* defered ack count */
 
 	struct list_head rspwait;       /* link for waiting to respond */
 

commit 90898850ec4e7b3ba0f9a35cc7169ff19ff367a6
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:18 2017 -0400

    IB/core: Rename struct ib_ah_attr to rdma_ah_attr
    
    This patch simply renames struct ib_ah_attr to
    rdma_ah_attr as these fields specify attributes that are
    not necessarily specific to IB.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 75a077c59742..1d8141a88d3c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -269,8 +269,8 @@ struct rvt_qp {
 	struct ib_qp ibqp;
 	void *priv; /* Driver private data */
 	/* read mostly fields above and below */
-	struct ib_ah_attr remote_ah_attr;
-	struct ib_ah_attr alt_ah_attr;
+	struct rdma_ah_attr remote_ah_attr;
+	struct rdma_ah_attr alt_ah_attr;
 	struct rvt_qp __rcu *next;           /* link list for QPN hash table */
 	struct rvt_swqe *s_wq;  /* send work queue */
 	struct rvt_mmap_info *ip;

commit aad9ff97ddd774f03f5e055ade2b2d27a9c82ddb
Author: Michael J. Ruhl <michael.j.ruhl@intel.com>
Date:   Sun Apr 9 10:15:57 2017 -0700

    IB/rdmavt/hfi1/qib: Use the MGID and MLID for multicast addressing
    
    The Infiniband spec defines "A multicast address is defined by a
    MGID and a MLID" (section 10.5).
    
    The current code only uses the MGID for identifying multicast groups.
    Update the driver to be compliant with this definition.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index e3bb312a2ffa..75a077c59742 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -435,9 +435,14 @@ struct rvt_mcast_qp {
 	struct rvt_qp *qp;
 };
 
+struct rvt_mcast_addr {
+	union ib_gid mgid;
+	u16 lid;
+};
+
 struct rvt_mcast {
 	struct rb_node rb_node;
-	union ib_gid mgid;
+	struct rvt_mcast_addr mcast_addr;
 	struct list_head qp_list;
 	wait_queue_head_t wait;
 	atomic_t refcount;

commit 44dcfa4b182dffed0e1e6535220fcdd12620b9ec
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Mar 20 17:26:01 2017 -0700

    IB/rdmavt: Avoid reseting wqe send_flags in unreserve
    
    The wqe should be read only and in fact the superfluous reset of the
    RVT_SEND_RESERVE_USED flag causes an issue where reserved operations
    elicit a bad completion to the ULP.
    
    The maintenance of the flag is now entirely within rvt_post_one_wr()
    where a reserved operation will set the flag and a non-reserved operation
    will insure the operation that is about to be posted has the flag reset.
    
    Fixes: Commit 856cc4c237ad ("IB/hfi1: Add the capability for reserved operations")
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 3cdd9e26e96e..e3bb312a2ffa 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -2,7 +2,7 @@
 #define DEF_RDMAVT_INCQP_H
 
 /*
- * Copyright(c) 2016 Intel Corporation.
+ * Copyright(c) 2016, 2017 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.
@@ -526,7 +526,6 @@ static inline void rvt_qp_wqe_reserve(
 	struct rvt_qp *qp,
 	struct rvt_swqe *wqe)
 {
-	wqe->wr.send_flags |= RVT_SEND_RESERVE_USED;
 	atomic_inc(&qp->s_reserved_used);
 }
 
@@ -550,7 +549,6 @@ static inline void rvt_qp_wqe_unreserve(
 	struct rvt_swqe *wqe)
 {
 	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED)) {
-		wqe->wr.send_flags &= ~RVT_SEND_RESERVE_USED;
 		atomic_dec(&qp->s_reserved_used);
 		/* insure no compiler re-order up to s_last change */
 		smp_mb__after_atomic();

commit 43a474aadbd55252cea2036bac36e3ad159344b2
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Mar 20 17:25:04 2017 -0700

    IB/rdmavt, IB/hfi1, IB/qib: Make wc opcode translation driver dependent
    
    The work to create a completion helper moved the translation of send
    wqe operations to completion opcodes to rdmvat.
    
    This precludes having driver dependent operations.  Make the translation
    driver dependent by doing the translation in the driver prior to the
    rvt_qp_swqe_complete() call using restored translation tables.
    
    Fixes: Commit f2dc9cdce83c ("IB/rdmavt: Add a send completion helper")
    Fixes: Commit 0771da5a6e9d ("IB/hfi1,IB/qib: Use new send completion helper")
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f3816396c76a..3cdd9e26e96e 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -574,6 +574,7 @@ extern const enum ib_wc_opcode ib_rvt_wc_opcode[];
 static inline void rvt_qp_swqe_complete(
 	struct rvt_qp *qp,
 	struct rvt_swqe *wqe,
+	enum ib_wc_opcode opcode,
 	enum ib_wc_status status)
 {
 	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED))
@@ -586,7 +587,7 @@ static inline void rvt_qp_swqe_complete(
 		memset(&wc, 0, sizeof(wc));
 		wc.wr_id = wqe->wr.wr_id;
 		wc.status = status;
-		wc.opcode = ib_rvt_wc_opcode[wqe->wr.opcode];
+		wc.opcode = opcode;
 		wc.qp = &qp->ibqp;
 		wc.byte_len = wqe->length;
 		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc,

commit 832666c163f04306fa6823b8974bccf7bb5e5ad3
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Wed Feb 8 05:28:25 2017 -0800

    IB/hfi1, qib, rdmavt: Move AETH defines to rdma/ib_hdrs.h
    
    Rename RVT AETH defines and export in rdma/ib_hdrs.h
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 9767549ab42c..f3816396c76a 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -594,11 +594,6 @@ static inline void rvt_qp_swqe_complete(
 	}
 }
 
-#define RVT_AETH_CREDIT_SHIFT	24
-#define RVT_AETH_CREDIT_MASK	0x1F
-#define RVT_AETH_NAK_SHIFT	29
-#define RVT_MSN_MASK		0xFFFFFF
-
 /*
  * Compare the lower 24 bits of the msn values.
  * Returns an integer <, ==, or > than zero.

commit 881fccb8640bdd3533f5a98a99991aae23a5106b
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Wed Feb 8 05:28:19 2017 -0800

    IB/hfi1: Add rvt_rnr_tbl_to_usec function
    
    Return usec from an index into ib_rvt_rnr_table.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 2f91fae8d23f..9767549ab42c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -653,6 +653,7 @@ struct rvt_dev_info;
 void rvt_comm_est(struct rvt_qp *qp);
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
 void rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);
+unsigned long rvt_rnr_tbl_to_usec(u32 index);
 enum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t);
 void rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth);
 void rvt_del_timers_sync(struct rvt_qp *qp);

commit f9215b5e536b0c598dff4041fc7d6136cd599981
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Wed Feb 8 05:27:49 2017 -0800

    IB/rdmavt, IB/hfi1, IB/qib: Correct ack count for passive (RTR) QPs
    
    The send complete for RC QPs mismanages the ack count when the
    responder side is only in RTR.
    
    A QP in that state cannot send requests, but it can be the target
    for operations that elicit responses.
    
    Adjust the RC completion logic to correct the count maintenance
    by reflecting RECV_OK in a new state test.
    
    Reviewed-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 561b6c876086..2f91fae8d23f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -144,6 +144,8 @@
 #define RVT_FLUSH_RECV			0x40
 #define RVT_PROCESS_OR_FLUSH_SEND \
 	(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND)
+#define RVT_SEND_OR_FLUSH_OR_RECV_OK \
+	(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND | RVT_PROCESS_RECV_OK)
 
 /*
  * Internal send flags

commit 0128fceaf934dbfca4537d4eb8c3a5f7e84562c8
Author: Brian Welty <brian.welty@intel.com>
Date:   Wed Feb 8 05:27:31 2017 -0800

    IB/hfi1, rdmavt: Update copy_sge to use boolean arguments
    
    Convert copy_sge and related SGE state functions to use boolean.
    For determining if QP is in user mode, add helper function in rdmavt_qp.h.
    This is used to determine if QP needs the last byte ordering.
    While here, change rvt_pd.user to a boolean.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dean Luick <dean.luick@intel.com>
    Signed-off-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0b1cbffb967c..561b6c876086 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -467,6 +467,15 @@ static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 		  rq->max_sge * sizeof(struct ib_sge)) * n);
 }
 
+/**
+ * rvt_is_user_qp - return if this is user mode QP
+ * @qp - the target QP
+ */
+static inline bool rvt_is_user_qp(struct rvt_qp *qp)
+{
+	return !!qp->pid;
+}
+
 /**
  * rvt_get_qp - get a QP reference
  * @qp - the QP to hold

commit 11a10d4bc7b2640da1fce27586a617411b70f5c5
Author: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
Date:   Wed Feb 8 05:27:13 2017 -0800

    IB/rdmavt: Adding timer logic to rdmavt
    
    To move common code across target to rdmavt for code reuse.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index a92e7dcfb5f5..0b1cbffb967c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -370,6 +370,7 @@ struct rvt_qp {
 
 	struct rvt_sge_state s_ack_rdma_sge;
 	struct timer_list s_timer;
+	struct hrtimer s_rnr_timer;
 
 	atomic_t local_ops_pending; /* number of fast_reg/local_inv reqs */
 
@@ -641,5 +642,10 @@ struct rvt_dev_info;
 void rvt_comm_est(struct rvt_qp *qp);
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
 void rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);
+enum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t);
+void rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth);
+void rvt_del_timers_sync(struct rvt_qp *qp);
+void rvt_stop_rc_timers(struct rvt_qp *qp);
+void rvt_add_retry_timer(struct rvt_qp *qp);
 
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 696513e8cf39ccb8ee8010fd2157f095b3af6a91
Author: Brian Welty <brian.welty@intel.com>
Date:   Wed Feb 8 05:27:07 2017 -0800

    IB/hfi1, qib, rdmavt: Move AETH credit functions into rdmavt
    
    Add rvt_compute_aeth() and rvt_get_credit() as shared functions in
    rdmavt, moved from hfi1/qib logic.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index eaaba1b6cc57..a92e7dcfb5f5 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -582,6 +582,37 @@ static inline void rvt_qp_swqe_complete(
 	}
 }
 
+#define RVT_AETH_CREDIT_SHIFT	24
+#define RVT_AETH_CREDIT_MASK	0x1F
+#define RVT_AETH_NAK_SHIFT	29
+#define RVT_MSN_MASK		0xFFFFFF
+
+/*
+ * Compare the lower 24 bits of the msn values.
+ * Returns an integer <, ==, or > than zero.
+ */
+static inline int rvt_cmp_msn(u32 a, u32 b)
+{
+	return (((int)a) - ((int)b)) << 8;
+}
+
+/**
+ * rvt_compute_aeth - compute the AETH (syndrome + MSN)
+ * @qp: the queue pair to compute the AETH for
+ *
+ * Returns the AETH.
+ */
+__be32 rvt_compute_aeth(struct rvt_qp *qp);
+
+/**
+ * rvt_get_credit - flush the send work queue of a QP
+ * @qp: the qp who's send work queue to flush
+ * @aeth: the Acknowledge Extended Transport Header
+ *
+ * The QP s_lock should be held.
+ */
+void rvt_get_credit(struct rvt_qp *qp, u32 aeth);
+
 /**
  * @qp - the qp pair
  * @len - the length

commit beb5a0426794c9698c4e0349c626d819b5f3b2c7
Author: Brian Welty <brian.welty@intel.com>
Date:   Wed Feb 8 05:27:01 2017 -0800

    IB/hfi1, qib, rdmavt: Move two IB event functions into rdmavt
    
    Add rvt_rc_error() and rvt_comm_est() as shared functions in
    rdmavt, moved from hfi1/qib logic.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f3dbd157ae5c..eaaba1b6cc57 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -607,6 +607,8 @@ static inline u32 rvt_div_mtu(struct rvt_qp *qp, u32 len)
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;
+void rvt_comm_est(struct rvt_qp *qp);
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
+void rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);
 
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 5dc806052a9fc8c44e111646f9a9f436877749d0
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Wed Dec 7 19:34:37 2016 -0800

    IB/rdmavt, IB/hfi1, IB/qib: Add inlines for mtu division
    
    Add rvt_div_round_up_mtu() and rvt_div_mtu() routines to
    do the computation based on the pmtu and the log_pmtu.
    
    Change divides in qib, hfi1 to use the new inlines.
    
    Reviewed-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 04facda681e0..f3dbd157ae5c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -582,7 +582,29 @@ static inline void rvt_qp_swqe_complete(
 	}
 }
 
-extern const int ib_rvt_state_ops[];
+/**
+ * @qp - the qp pair
+ * @len - the length
+ *
+ * Perform a shift based mtu round up divide
+ */
+static inline u32 rvt_div_round_up_mtu(struct rvt_qp *qp, u32 len)
+{
+	return (len + qp->pmtu - 1) >> qp->log_pmtu;
+}
+
+/**
+ * @qp - the qp pair
+ * @len - the length
+ *
+ * Perform a shift based mtu divide
+ */
+static inline u32 rvt_div_mtu(struct rvt_qp *qp, u32 len)
+{
+	return len >> qp->log_pmtu;
+}
+
+extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);

commit f6475223b12b3a38298976506edb042e13158798
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Wed Dec 7 19:34:25 2016 -0800

    IB/rdmavt: Add swqe mr deref helper
    
    Add a helper to release mr references held by
    an swqe.
    
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index d78e99cf6c11..04facda681e0 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -485,6 +485,23 @@ static inline void rvt_put_qp(struct rvt_qp *qp)
 		wake_up(&qp->wait);
 }
 
+/**
+ * rvt_put_swqe - drop mr refs held by swqe
+ * @wqe - the send wqe
+ *
+ * This drops any mr references held by the swqe
+ */
+static inline void rvt_put_swqe(struct rvt_swqe *wqe)
+{
+	int i;
+
+	for (i = 0; i < wqe->wr.num_sge; i++) {
+		struct rvt_sge *sge = &wqe->sg_list[i];
+
+		rvt_put_mr(sge->mr);
+	}
+}
+
 /**
  * rvt_qp_wqe_reserve - reserve operation
  * @qp - the rvt qp

commit f2dc9cdce83c4aac2f8b6c803b0327df1c7d44a6
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Wed Dec 7 19:34:06 2016 -0800

    IB/rdmavt: Add a send completion helper
    
    This is for use by client drivers to drive
    send completions into a CQ.
    
    A new exported table allows for the mapping
    of ib_wr_opcode into a ib_wc_opcode.
    
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 2c5183ef0243..d78e99cf6c11 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -51,6 +51,7 @@
 #include <rdma/rdma_vt.h>
 #include <rdma/ib_pack.h>
 #include <rdma/ib_verbs.h>
+#include <rdma/rdmavt_cq.h>
 /*
  * Atomic bit definitions for r_aflags.
  */
@@ -527,7 +528,44 @@ static inline void rvt_qp_wqe_unreserve(
 	}
 }
 
-extern const int  ib_rvt_state_ops[];
+extern const enum ib_wc_opcode ib_rvt_wc_opcode[];
+
+/**
+ * rvt_qp_swqe_complete() - insert send completion
+ * @qp - the qp
+ * @wqe - the send wqe
+ * @status - completion status
+ *
+ * Insert a send completion into the completion
+ * queue if the qp indicates it should be done.
+ *
+ * See IBTA 10.7.3.1 for info on completion
+ * control.
+ */
+static inline void rvt_qp_swqe_complete(
+	struct rvt_qp *qp,
+	struct rvt_swqe *wqe,
+	enum ib_wc_status status)
+{
+	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED))
+		return;
+	if (!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||
+	    (wqe->wr.send_flags & IB_SEND_SIGNALED) ||
+	     status != IB_WC_SUCCESS) {
+		struct ib_wc wc;
+
+		memset(&wc, 0, sizeof(wc));
+		wc.wr_id = wqe->wr.wr_id;
+		wc.status = status;
+		wc.opcode = ib_rvt_wc_opcode[wqe->wr.opcode];
+		wc.qp = &qp->ibqp;
+		wc.byte_len = wqe->length;
+		rvt_cq_enter(ibcq_to_rvtcq(qp->ibqp.send_cq), &wc,
+			     status != IB_WC_SUCCESS);
+	}
+}
+
+extern const int ib_rvt_state_ops[];
 
 struct rvt_dev_info;
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);

commit 4107b8a029995e4a2c928b395a12ebc67d8e3689
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Tue Sep 6 04:34:21 2016 -0700

    IB/rdmavt: Add functions to get and release QP references
    
    This centralizes the function and improves code readability.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index bd34d0b56bf7..2c5183ef0243 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -465,6 +465,25 @@ static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 		  rq->max_sge * sizeof(struct ib_sge)) * n);
 }
 
+/**
+ * rvt_get_qp - get a QP reference
+ * @qp - the QP to hold
+ */
+static inline void rvt_get_qp(struct rvt_qp *qp)
+{
+	atomic_inc(&qp->refcount);
+}
+
+/**
+ * rvt_put_qp - release a QP reference
+ * @qp - the QP to release
+ */
+static inline void rvt_put_qp(struct rvt_qp *qp)
+{
+	if (qp && atomic_dec_and_test(&qp->refcount))
+		wake_up(&qp->wait);
+}
+
 /**
  * rvt_qp_wqe_reserve - reserve operation
  * @qp - the rvt qp

commit fe508272c963d62de4183c32b6883c3d54c557ef
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Wed Jul 27 21:07:36 2016 -0400

    IB/rdmavt: Eliminate redundant opcode test in mr ref clear
    
    The use of the specific opcode test is redundant since
    all ack entry users correctly manipulate the mr pointer
    to selectively trigger the reference clearing.
    
    The overly specific test hinders the use of implementation
    specific operations.
    
    The change needs to get rid of the union to insure that
    an atomic value is not seen as an MR pointer.
    
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 13902dd319a9..bd34d0b56bf7 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -222,14 +222,12 @@ struct rvt_mmap_info {
  * to send a RDMA read response or atomic operation.
  */
 struct rvt_ack_entry {
-	u8 opcode;
-	u8 sent;
+	struct rvt_sge rdma_sge;
+	u64 atomic_data;
 	u32 psn;
 	u32 lpsn;
-	union {
-		struct rvt_sge rdma_sge;
-		u64 atomic_data;
-	};
+	u8 opcode;
+	u8 sent;
 };
 
 #define	RC_QP_SCALING_INTERVAL	5

commit d9b13c203003cfb78c1f216049a204d385ccaeff
Author: Jianxin Xiong <jianxin.xiong@intel.com>
Date:   Mon Jul 25 13:39:45 2016 -0700

    IB/rdmavt, hfi1: Fix NFSoRDMA failure with FRMR enabled
    
    Hanging has been observed while writing a file over NFSoRDMA. Dmesg on
    the server contains messages like these:
    
    [  931.992501] svcrdma: Error -22 posting RDMA_READ
    [  952.076879] svcrdma: Error -22 posting RDMA_READ
    [  982.154127] svcrdma: Error -22 posting RDMA_READ
    [ 1012.235884] svcrdma: Error -22 posting RDMA_READ
    [ 1042.319194] svcrdma: Error -22 posting RDMA_READ
    
    Here is why:
    
    With the base memory management extension enabled, FRMR is used instead
    of FMR. The xprtrdma server issues each RDMA read request as the following
    bundle:
    
    (1)IB_WR_REG_MR, signaled;
    (2)IB_WR_RDMA_READ, signaled;
    (3)IB_WR_LOCAL_INV, signaled & fencing.
    
    These requests are signaled. In order to generate completion, the fast
    register work request is processed by the hfi1 send engine after being
    posted to the work queue, and the corresponding lkey is not valid until
    the request is processed. However, the rdmavt driver validates lkey when
    the RDMA read request is posted and thus it fails immediately with error
    -EINVAL (-22).
    
    This patch changes the work flow of local operations (fast register and
    local invalidate) so that fast register work requests are always
    processed immediately to ensure that the corresponding lkey is valid
    when subsequent work requests are posted. Local invalidate requests are
    processed immediately if fencing is not required and no previous local
    invalidate request is pending.
    
    To allow completion generation for signaled local operations that have
    been processed before posting to the work queue, an internal send flag
    RVT_SEND_COMPLETION_ONLY is added. The hfi1 send engine checks this flag
    and only generates completion for such requests.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Jianxin Xiong <jianxin.xiong@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 56adcfcabe0b..13902dd319a9 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -148,6 +148,7 @@
  * Internal send flags
  */
 #define RVT_SEND_RESERVE_USED           IB_SEND_RESERVED_START
+#define RVT_SEND_COMPLETION_ONLY	(IB_SEND_RESERVED_START << 1)
 
 /*
  * Send work request queue entry.

commit 856cc4c237add46510c8ae91764f4eda31a9e1cf
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Jul 25 13:39:39 2016 -0700

    IB/hfi1: Add the capability for reserved operations
    
    This fix allows for support of in-kernel reserved operations
    without impacting the ULP user.
    
    The low level driver can register a non-zero value which
    will be transparently added to the send queue size and hidden
    from the ULP in every respect.
    
    ULP post sends will never see a full queue due to a reserved
    post send and reserved operations will never exceed that
    registered value.
    
    The s_avail will continue to track the ULP swqe availability
    and the difference between the reserved value and the reserved
    in use will track reserved availabity.
    
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index b0ab12b30f1e..56adcfcabe0b 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -144,6 +144,11 @@
 #define RVT_PROCESS_OR_FLUSH_SEND \
 	(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND)
 
+/*
+ * Internal send flags
+ */
+#define RVT_SEND_RESERVE_USED           IB_SEND_RESERVED_START
+
 /*
  * Send work request queue entry.
  * The size of the sg_list is determined when the QP is created and stored
@@ -232,6 +237,7 @@ struct rvt_ack_entry {
 #define RVT_OPERATION_ATOMIC      0x00000002
 #define RVT_OPERATION_ATOMIC_SGE  0x00000004
 #define RVT_OPERATION_LOCAL       0x00000008
+#define RVT_OPERATION_USE_RESERVE 0x00000010
 
 #define RVT_OPERATION_MAX (IB_WR_RESERVED10 + 1)
 
@@ -328,6 +334,7 @@ struct rvt_qp {
 	u32 s_next_psn;         /* PSN for next request */
 	u32 s_avail;            /* number of entries avail */
 	u32 s_ssn;              /* SSN of tail entry */
+	atomic_t s_reserved_used; /* reserved entries in use */
 
 	spinlock_t s_lock ____cacheline_aligned_in_smp;
 	u32 s_flags;
@@ -459,6 +466,49 @@ static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 		  rq->max_sge * sizeof(struct ib_sge)) * n);
 }
 
+/**
+ * rvt_qp_wqe_reserve - reserve operation
+ * @qp - the rvt qp
+ * @wqe - the send wqe
+ *
+ * This routine used in post send to record
+ * a wqe relative reserved operation use.
+ */
+static inline void rvt_qp_wqe_reserve(
+	struct rvt_qp *qp,
+	struct rvt_swqe *wqe)
+{
+	wqe->wr.send_flags |= RVT_SEND_RESERVE_USED;
+	atomic_inc(&qp->s_reserved_used);
+}
+
+/**
+ * rvt_qp_wqe_unreserve - clean reserved operation
+ * @qp - the rvt qp
+ * @wqe - the send wqe
+ *
+ * This decrements the reserve use count.
+ *
+ * This call MUST precede the change to
+ * s_last to insure that post send sees a stable
+ * s_avail.
+ *
+ * An smp_mp__after_atomic() is used to insure
+ * the compiler does not juggle the order of the s_last
+ * ring index and the decrementing of s_reserved_used.
+ */
+static inline void rvt_qp_wqe_unreserve(
+	struct rvt_qp *qp,
+	struct rvt_swqe *wqe)
+{
+	if (unlikely(wqe->wr.send_flags & RVT_SEND_RESERVE_USED)) {
+		wqe->wr.send_flags &= ~RVT_SEND_RESERVE_USED;
+		atomic_dec(&qp->s_reserved_used);
+		/* insure no compiler re-order up to s_last change */
+		smp_mb__after_atomic();
+	}
+}
+
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;

commit d9f8723924d5955979d05cb7f4f10d9ebac39b7d
Author: Jianxin Xiong <jianxin.xiong@intel.com>
Date:   Mon Jul 25 13:38:25 2016 -0700

    IB/rdmavt: Handle local operations in post send
    
    Some work requests are local operations, such as IB_WR_REG_MR and
    IB_WR_LOCAL_INV. They differ from non-local operations in that:
    
    (1) Local operations can be processed immediately without being posted
    to the send queue if neither fencing nor completion generation is needed.
    However, to ensure correct ordering, once a local operation is posted to
    the work queue due to fencing or completion requiement, all subsequent
    local operations must also be posted to the work queue until all the
    local operations on the work queue have completed.
    
    (2) Local operations don't send packets over the wire and thus don't
    need (and shouldn't update) the packet sequence numbers.
    
    Define a new a flag bit for the post send table to identify local
    operations.
    
    Add a new field to the QP structure to track the number of local
    operations on the send queue to determine if direct processing of new
    local operations should be enabled/disabled.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jianxin Xiong <jianxin.xiong@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index a90d1e941504..b0ab12b30f1e 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -231,6 +231,7 @@ struct rvt_ack_entry {
 #define RVT_OPERATION_PRIV        0x00000001
 #define RVT_OPERATION_ATOMIC      0x00000002
 #define RVT_OPERATION_ATOMIC_SGE  0x00000004
+#define RVT_OPERATION_LOCAL       0x00000008
 
 #define RVT_OPERATION_MAX (IB_WR_RESERVED10 + 1)
 
@@ -363,6 +364,8 @@ struct rvt_qp {
 	struct rvt_sge_state s_ack_rdma_sge;
 	struct timer_list s_timer;
 
+	atomic_t local_ops_pending; /* number of fast_reg/local_inv reqs */
+
 	/*
 	 * This sge list MUST be last. Do not add anything below here.
 	 */

commit afcf8f7647780aa147ad68be48d223cd50311b9a
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Fri Jul 1 16:02:07 2016 -0700

    IB/rdmavt: Add data structures and routines for table driven post send
    
    Add flexibility for driver dependent operations in post send
    because different drivers will have differing post send
    operation support.
    
    This includes data structure definitions to support a table
    driven scheme along with the necessary validation routine
    using the new table.
    
    Reviewed-by: Ashutosh Dixit <ashutosh.dixit@intel.com>
    Reviewed-by: Jianxin Xiong <jianxin.xiong@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 6d23b879416a..a90d1e941504 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -228,11 +228,31 @@ struct rvt_ack_entry {
 
 #define	RC_QP_SCALING_INTERVAL	5
 
-/*
- * Variables prefixed with s_ are for the requester (sender).
- * Variables prefixed with r_ are for the responder (receiver).
- * Variables prefixed with ack_ are for responder replies.
+#define RVT_OPERATION_PRIV        0x00000001
+#define RVT_OPERATION_ATOMIC      0x00000002
+#define RVT_OPERATION_ATOMIC_SGE  0x00000004
+
+#define RVT_OPERATION_MAX (IB_WR_RESERVED10 + 1)
+
+/**
+ * rvt_operation_params - op table entry
+ * @length - the length to copy into the swqe entry
+ * @qpt_support - a bit mask indicating QP type support
+ * @flags - RVT_OPERATION flags (see above)
+ *
+ * This supports table driven post send so that
+ * the driver can have differing an potentially
+ * different sets of operations.
  *
+ **/
+
+struct rvt_operation_params {
+	size_t length;
+	u32 qpt_support;
+	u32 flags;
+};
+
+/*
  * Common variables are protected by both r_rq.lock and s_lock in that order
  * which only happens in modify_qp() or changing the QP 'state'.
  */

commit 8b103e9cdee5f381bc20a8a9f9bb5be11de8e68f
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Tue May 24 12:50:40 2016 -0700

    IB/rdamvt: Fix rdmavt s_ack_queue sizing
    
    rdmavt allows the driver to specify the size of the ack queue, but
    only uses it for the modify QP limit testing for setting the atomic
    limit value.
    
    The driver dependent size is now used to size the s_ack_queue ring
    dynamicially.
    
    Since the driver knows its size, the driver will use its define
    for any ring size dependent code.
    
    Reviewed-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 0e1ff2abfe92..6d23b879416a 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -211,8 +211,6 @@ struct rvt_mmap_info {
 	unsigned size;
 };
 
-#define RVT_MAX_RDMA_ATOMIC	16
-
 /*
  * This structure holds the information that the send tasklet needs
  * to send a RDMA read response or atomic operation.
@@ -282,8 +280,7 @@ struct rvt_qp {
 	atomic_t refcount ____cacheline_aligned_in_smp;
 	wait_queue_head_t wait;
 
-	struct rvt_ack_entry s_ack_queue[RVT_MAX_RDMA_ATOMIC + 1]
-		____cacheline_aligned_in_smp;
+	struct rvt_ack_entry *s_ack_queue;
 	struct rvt_sge_state s_rdma_read_sge;
 
 	spinlock_t r_lock ____cacheline_aligned_in_smp;      /* used for APM */

commit f39cc34df7ed919c5c7630a8a7a701939356c37c
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Tue Apr 12 10:45:51 2016 -0700

    IB/rdmavt: Fix adaptive pio hang
    
    The RVT_S_WAIT_PIO_DRAIN flag was missing from
    the set of flags indicating a qp is waiting
    on a resource.
    
    This caused the sleep/wakeup for adaptive pio
    drain to lose a wakeup "hanging" a QP.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 497e59065c2c..0e1ff2abfe92 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -117,8 +117,9 @@
 /*
  * Wait flags that would prevent any packet type from being sent.
  */
-#define RVT_S_ANY_WAIT_IO (RVT_S_WAIT_PIO | RVT_S_WAIT_TX | \
-	RVT_S_WAIT_DMA_DESC | RVT_S_WAIT_KMEM)
+#define RVT_S_ANY_WAIT_IO \
+	(RVT_S_WAIT_PIO | RVT_S_WAIT_PIO_DRAIN | RVT_S_WAIT_TX | \
+	 RVT_S_WAIT_DMA_DESC | RVT_S_WAIT_KMEM)
 
 /*
  * Wait flags that would prevent send work requests from making progress.

commit ef086c0d5dd9a151578c72b6f257e5b0e77d65eb
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Mar 7 11:35:08 2016 -0800

    IB/hfi1: Report pid in qp_stats to aid debug
    
    Tracking user/QP ownership is needed to debug issues with
    user ULPs like OpenMPI.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f2f4df023aaa..497e59065c2c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -251,6 +251,7 @@ struct rvt_qp {
 
 	enum ib_mtu path_mtu;
 	int srate_mbps;		/* s_srate (below) converted to Mbit/s */
+	pid_t pid;		/* pid for user mode QPs */
 	u32 remote_qpn;
 	u32 qkey;               /* QKEY for this QP (for UD or RD) */
 	u32 s_size;             /* send work queue size */

commit 14553ca11039732bcba3c160a26d702dbe71dd49
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Sun Feb 14 12:45:36 2016 -0800

    staging/rdma/hfi1: Adaptive PIO for short messages
    
    The change requires a new pio_busy field in the iowait structure to
    track the number of outstanding pios.  The new counter together
    with the sdma counter serve as the basis for a packet by packet decision
    as to which egress mechanism to use.  Since packets given to different
    egress mechanisms are not ordered, this scheme will preserve the order.
    
    The iowait drain/wait mechanisms are extended for a pio case.  An
    additional qp wait flag is added for the PIO drain wait case.
    
    Currently the only pio wait is for buffers, so the no_bufs_available()
    routine name is changed to pio_wait() and a third argument is passed
    with one of the two pio wait flags to generalize the routine.  A module
    parameter is added to hold a configurable threshold. For now, the
    module parameter is zero.
    
    A heuristic routine is added to return the func pointer of the proper
    egress routine to use.
    
    The heuristic is as follows:
    - SMI always uses pio
    - GSI,UD qps <= threshold use pio
    - UD qps > threadhold use sdma
      o No coordination with sdma is required because order is not required
        and this qp pio count is not maintained for UD
    - RC/UC ONLY packets <= threshold chose as follows:
      o If sdmas pending, use SDMA
      o Otherwise use pio and enable the pio tracking count at
        the time the pio buffer is allocated
    - RC/UC ONLY packets > threshold use SDMA
      o If pio's are pending the pio_wait with the new wait flag is
        called to delay for pios to drain
    
    The threshold is potentially reduced by the QP's mtu.
    
    The sc_buffer_alloc() has two additional args (a callback, a void *)
    which are exploited by the RC/UC cases to pass a new complete routine
    and a qp *.
    
    When the shadow ring completes the credit associated with a packet,
    the new complete routine is called.  The verbs_pio_complete() will then
    decrement the busy count and trigger any drain waiters in qp destroy
    or reset.
    
    Reviewed-by: Jubin John <jubin.john@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 5c307ed4d195..f2f4df023aaa 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -82,6 +82,7 @@
  * RVT_S_WAIT_DMA - waiting for send DMA queue to drain before generating
  *                  next send completion entry not via send DMA
  * RVT_S_WAIT_PIO - waiting for a send buffer to be available
+ * RVT_S_WAIT_PIO_DRAIN - waiting for a qp to drain pio packets
  * RVT_S_WAIT_TX - waiting for a struct verbs_txreq to be available
  * RVT_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
  * RVT_S_WAIT_KMEM - waiting for kernel memory to be available
@@ -101,16 +102,17 @@
 #define RVT_S_WAIT_SSN_CREDIT	0x0100
 #define RVT_S_WAIT_DMA		0x0200
 #define RVT_S_WAIT_PIO		0x0400
-#define RVT_S_WAIT_TX		0x0800
-#define RVT_S_WAIT_DMA_DESC	0x1000
-#define RVT_S_WAIT_KMEM		0x2000
-#define RVT_S_WAIT_PSN		0x4000
-#define RVT_S_WAIT_ACK		0x8000
-#define RVT_S_SEND_ONE		0x10000
-#define RVT_S_UNLIMITED_CREDIT	0x20000
-#define RVT_S_AHG_VALID		0x40000
-#define RVT_S_AHG_CLEAR		0x80000
-#define RVT_S_ECN		0x100000
+#define RVT_S_WAIT_PIO_DRAIN    0x0800
+#define RVT_S_WAIT_TX		0x1000
+#define RVT_S_WAIT_DMA_DESC	0x2000
+#define RVT_S_WAIT_KMEM		0x4000
+#define RVT_S_WAIT_PSN		0x8000
+#define RVT_S_WAIT_ACK		0x10000
+#define RVT_S_SEND_ONE		0x20000
+#define RVT_S_UNLIMITED_CREDIT	0x40000
+#define RVT_S_AHG_VALID		0x80000
+#define RVT_S_AHG_CLEAR		0x100000
+#define RVT_S_ECN		0x200000
 
 /*
  * Wait flags that would prevent any packet type from being sent.

commit d2421a82f6d8ad407d3f4acdbacedfb06d9f47f5
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Sun Feb 14 12:44:26 2016 -0800

    IB/rdmvt: close send engine struct holes
    
    pahole noted the wasted 4 bytes after s_lock and r_lock.
    
    Move s_flags and r_psn to fill the holes.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 933f14f92da6..5c307ed4d195 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -283,12 +283,12 @@ struct rvt_qp {
 	struct rvt_sge_state s_rdma_read_sge;
 
 	spinlock_t r_lock ____cacheline_aligned_in_smp;      /* used for APM */
+	u32 r_psn;              /* expected rcv packet sequence number */
 	unsigned long r_aflags;
 	u64 r_wr_id;            /* ID for current receive WQE */
 	u32 r_ack_psn;          /* PSN for next ACK or atomic ACK */
 	u32 r_len;              /* total length of r_sge */
 	u32 r_rcv_len;          /* receive data len processed */
-	u32 r_psn;              /* expected rcv packet sequence number */
 	u32 r_msn;              /* message sequence number */
 
 	u8 r_state;             /* opcode of last packet received */
@@ -308,8 +308,8 @@ struct rvt_qp {
 	u32 s_ssn;              /* SSN of tail entry */
 
 	spinlock_t s_lock ____cacheline_aligned_in_smp;
-	struct rvt_sge_state *s_cur_sge;
 	u32 s_flags;
+	struct rvt_sge_state *s_cur_sge;
 	struct rvt_swqe *s_wqe;
 	struct rvt_sge_state s_sge;     /* current send request data */
 	struct rvt_mregion *s_rdma_mr;

commit 79a225be38932b17707009767e85d6edf450e7cc
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Sun Feb 14 12:11:20 2016 -0800

    IB/rdmavt: Remove unnecessary exported functions
    
    Remove exported functions which are no longer required as the
    functionality has moved into rdmavt. This also requires re-ordering some
    of the functions since their prototype no longer appears in a header
    file. Rather than add forward declarations it is just cleaner to
    re-order some of the functions.
    
    Reviewed-by: Jubin John <jubin.john@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 1066b5d1b4d2..933f14f92da6 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -438,10 +438,6 @@ static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
 extern const int  ib_rvt_state_ops[];
 
 struct rvt_dev_info;
-void rvt_remove_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp);
-void rvt_clear_mr_refs(struct rvt_qp *qp, int clr_sends);
 int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
-void rvt_free_qpn(struct rvt_qpn_table *qpt, u32 qpn);
-void rvt_dec_qp_cnt(struct rvt_dev_info *rdi);
 
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 46a80d62e6e0ccfc9d8a05c5b773405b84a4afd7
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Sun Feb 14 12:10:04 2016 -0800

    IB/qib, staging/rdma/hfi1: add s_hlock for use in post send
    
    This patch adds an additional lock to reduce contention on the s_lock.
    
    This lock is used in post_send() so that the post_send is not
    serialized with the send engine and other send related processing.
    
    To do this the s_next_psn is now maintained on post_send() while
    post_send() related fields are moved to a new cache line.  There is
    an s_avail maintained for the post_send() to mitigate trading cache
    lines with the send engine.  The lock is released/acquired around
    releasing the just built packet to the egress mechanism.
    
    Reviewed-by: Jubin John <jubin.john@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Dean Luick <dean.luick@intel.com>
    Signed-off-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index b3ea74579316..1066b5d1b4d2 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -250,11 +250,12 @@ struct rvt_qp {
 	enum ib_mtu path_mtu;
 	int srate_mbps;		/* s_srate (below) converted to Mbit/s */
 	u32 remote_qpn;
-	u32 pmtu;		/* decoded from path_mtu */
 	u32 qkey;               /* QKEY for this QP (for UD or RD) */
 	u32 s_size;             /* send work queue size */
 	u32 s_ahgpsn;           /* set to the psn in the copy of the header */
 
+	u16 pmtu;		/* decoded from path_mtu */
+	u8 log_pmtu;		/* shift for pmtu */
 	u8 state;               /* QP state */
 	u8 allowed_ops;		/* high order bits of allowed opcodes */
 	u8 qp_access_flags;
@@ -299,6 +300,13 @@ struct rvt_qp {
 	struct rvt_sge_state r_sge;     /* current receive data */
 	struct rvt_rq r_rq;             /* receive work queue */
 
+	/* post send line */
+	spinlock_t s_hlock ____cacheline_aligned_in_smp;
+	u32 s_head;             /* new entries added here */
+	u32 s_next_psn;         /* PSN for next request */
+	u32 s_avail;            /* number of entries avail */
+	u32 s_ssn;              /* SSN of tail entry */
+
 	spinlock_t s_lock ____cacheline_aligned_in_smp;
 	struct rvt_sge_state *s_cur_sge;
 	u32 s_flags;
@@ -308,19 +316,16 @@ struct rvt_qp {
 	u32 s_cur_size;         /* size of send packet in bytes */
 	u32 s_len;              /* total length of s_sge */
 	u32 s_rdma_read_len;    /* total length of s_rdma_read_sge */
-	u32 s_next_psn;         /* PSN for next request */
 	u32 s_last_psn;         /* last response PSN processed */
 	u32 s_sending_psn;      /* lowest PSN that is being sent */
 	u32 s_sending_hpsn;     /* highest PSN that is being sent */
 	u32 s_psn;              /* current packet sequence number */
 	u32 s_ack_rdma_psn;     /* PSN for sending RDMA read responses */
 	u32 s_ack_psn;          /* PSN for acking sends and RDMA writes */
-	u32 s_head;             /* new entries added here */
 	u32 s_tail;             /* next entry to process */
 	u32 s_cur;              /* current work queue entry */
 	u32 s_acked;            /* last un-ACK'ed entry */
 	u32 s_last;             /* last completed entry */
-	u32 s_ssn;              /* SSN of tail entry */
 	u32 s_lsn;              /* limit sequence number (credit) */
 	u16 s_hdrwords;         /* size of s_hdr in 32 bit words */
 	u16 s_rdma_ack_cnt;

commit bfee5e32e701b98634b380a9eef8b5820feb7488
Author: Vennila Megavannan <vennila.megavannan@intel.com>
Date:   Tue Feb 9 14:29:49 2016 -0800

    IB/rdmavt, staging/rdma/hfi1: use qps to dynamically scale timeout value
    
    A busy_jiffies variable is maintained and updated when rc qps are
    created and deleted. busy_jiffies is a scaled value of the number
    of rc qps in the device. busy_jiffies is incremented every rc qp
    scaling interval. busy_jiffies is added to the rc timeout
    in add_retry_timer and mod_retry_timer. The rc qp scaling interval
    is selected based on extensive performance evaluation of targeted
    workloads.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Vennila Megavannan <vennila.megavannan@intel.com>
    Signed-off-by: Jubin John <jubin.john@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index aed13e13591c..b3ea74579316 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -225,6 +225,8 @@ struct rvt_ack_entry {
 	};
 };
 
+#define	RC_QP_SCALING_INTERVAL	5
+
 /*
  * Variables prefixed with s_ are for the requester (sender).
  * Variables prefixed with r_ are for the responder (receiver).

commit 066fad289408e7d48ce00b54a292e7a90602cb30
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Thu Feb 4 11:03:36 2016 -0800

    IB/rdmavt: remove unused qp field
    
    The field is a vestige from ipath.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 91f20fd91e00..aed13e13591c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -251,7 +251,6 @@ struct rvt_qp {
 	u32 pmtu;		/* decoded from path_mtu */
 	u32 qkey;               /* QKEY for this QP (for UD or RD) */
 	u32 s_size;             /* send work queue size */
-	u32 s_rnr_timeout;      /* number of milliseconds for RNR timeout */
 	u32 s_ahgpsn;           /* set to the psn in the copy of the header */
 
 	u8 state;               /* QP state */

commit fe31419501ba133a967da7b7da0d32945ef21840
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 13:04:58 2016 -0800

    IB/rdmavt: Fix copyright date
    
    Update all files added by rdmavt which do not yet have 2016 as the
    copyright year.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f0e24266bdb4..91f20fd91e00 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -2,7 +2,7 @@
 #define DEF_RDMAVT_INCQP_H
 
 /*
- * Copyright(c) 2015 Intel Corporation.
+ * Copyright(c) 2016 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.

commit 2b047ea7a3ceef0322e666782e0a82e98424f6f1
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Fri Jan 22 13:04:32 2016 -0800

    IB/rdmavt: Remove unused variable from Queue Pair
    
    s_sde should be in the low level driver QP private data.
    
    Remove the definition from rvt_qp.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index a97b95ba893f..f0e24266bdb4 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -304,7 +304,6 @@ struct rvt_qp {
 	struct rvt_swqe *s_wqe;
 	struct rvt_sge_state s_sge;     /* current send request data */
 	struct rvt_mregion *s_rdma_mr;
-	struct sdma_engine *s_sde; /* current sde */
 	u32 s_cur_size;         /* size of send packet in bytes */
 	u32 s_len;              /* total length of s_sge */
 	u32 s_rdma_read_len;    /* total length of s_rdma_read_sge */

commit 4e74080b248701c0c2d1af2764bf02f9c531020a
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 13:00:55 2016 -0800

    IB/rdmavt: Add multicast functions
    
    This patch adds in the multicast add and remove functions as well as the
    ancillary infrastructure needed.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index e66bcc96d273..a97b95ba893f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -50,6 +50,7 @@
 
 #include <rdma/rdma_vt.h>
 #include <rdma/ib_pack.h>
+#include <rdma/ib_verbs.h>
 /*
  * Atomic bit definitions for r_aflags.
  */
@@ -385,9 +386,28 @@ struct rvt_qp_ibdev {
 	struct rvt_qpn_table qpn_table;
 };
 
+/*
+ * There is one struct rvt_mcast for each multicast GID.
+ * All attached QPs are then stored as a list of
+ * struct rvt_mcast_qp.
+ */
+struct rvt_mcast_qp {
+	struct list_head list;
+	struct rvt_qp *qp;
+};
+
+struct rvt_mcast {
+	struct rb_node rb_node;
+	union ib_gid mgid;
+	struct list_head qp_list;
+	wait_queue_head_t wait;
+	atomic_t refcount;
+	int n_attached;
+};
+
 /*
  * Since struct rvt_swqe is not a fixed size, we can't simply index into
- * struct hfi1_qp.s_wq.  This function does the array index computation.
+ * struct rvt_qp.s_wq.  This function does the array index computation.
  */
 static inline struct rvt_swqe *rvt_get_swqe_ptr(struct rvt_qp *qp,
 						unsigned n)

commit 3b0b3fb3c1bbf50a2f88ea7345448a41dcba3c57
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 13:00:35 2016 -0800

    IB/rdmavt: Add modify qp
    
    Add modify qp and supporting functions.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 3189f195538c..e66bcc96d273 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -355,6 +355,7 @@ struct rvt_srq {
 #define RVT_QPNMAP_ENTRIES          (RVT_QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
 #define RVT_BITS_PER_PAGE           (PAGE_SIZE * BITS_PER_BYTE)
 #define RVT_BITS_PER_PAGE_MASK      (RVT_BITS_PER_PAGE - 1)
+#define RVT_QPN_MASK		    0xFFFFFF
 
 /*
  * QPN-map pages start out as NULL, they get allocated upon
@@ -397,6 +398,25 @@ static inline struct rvt_swqe *rvt_get_swqe_ptr(struct rvt_qp *qp,
 				      sizeof(struct rvt_sge)) * n);
 }
 
+/*
+ * Since struct rvt_rwqe is not a fixed size, we can't simply index into
+ * struct rvt_rwq.wq.  This function does the array index computation.
+ */
+static inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)
+{
+	return (struct rvt_rwqe *)
+		((char *)rq->wq->wq +
+		 (sizeof(struct rvt_rwqe) +
+		  rq->max_sge * sizeof(struct ib_sge)) * n);
+}
+
 extern const int  ib_rvt_state_ops[];
 
+struct rvt_dev_info;
+void rvt_remove_qp(struct rvt_dev_info *rdi, struct rvt_qp *qp);
+void rvt_clear_mr_refs(struct rvt_qp *qp, int clr_sends);
+int rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);
+void rvt_free_qpn(struct rvt_qpn_table *qpt, u32 qpn);
+void rvt_dec_qp_cnt(struct rvt_dev_info *rdi);
+
 #endif          /* DEF_RDMAVT_INCQP_H */

commit bfbac097b6e8023e10fdadab2527d0a1a3160d7e
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 13:00:22 2016 -0800

    IB/rdmavt: Add post send to rdmavt
    
    Add in a post_send and post_one_send to rdmavt. The ULP will provide a WQE
    to rdmavt which will then walk and queue each element. Rdmavt will then
    queue the work to be done in the driver or kick the driver's progress
    routine.
    
    There needs to be a follow on patch which adds in another lock for the
    head of the queue so that it can be added to and read from in parallel.
    This will touch protocol handlers and require other changes in the
    drivers. This will be done separately.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index bce0a03a7c07..3189f195538c 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -129,6 +129,17 @@
 /* Number of bits to pay attention to in the opcode for checking qp type */
 #define RVT_OPCODE_QP_MASK 0xE0
 
+/* Flags for checking QP state (see ib_rvt_state_ops[]) */
+#define RVT_POST_SEND_OK                0x01
+#define RVT_POST_RECV_OK                0x02
+#define RVT_PROCESS_RECV_OK             0x04
+#define RVT_PROCESS_SEND_OK             0x08
+#define RVT_PROCESS_NEXT_SEND_OK        0x10
+#define RVT_FLUSH_SEND			0x20
+#define RVT_FLUSH_RECV			0x40
+#define RVT_PROCESS_OR_FLUSH_SEND \
+	(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND)
+
 /*
  * Send work request queue entry.
  * The size of the sg_list is determined when the QP is created and stored
@@ -373,4 +384,19 @@ struct rvt_qp_ibdev {
 	struct rvt_qpn_table qpn_table;
 };
 
+/*
+ * Since struct rvt_swqe is not a fixed size, we can't simply index into
+ * struct hfi1_qp.s_wq.  This function does the array index computation.
+ */
+static inline struct rvt_swqe *rvt_get_swqe_ptr(struct rvt_qp *qp,
+						unsigned n)
+{
+	return (struct rvt_swqe *)((char *)qp->s_wq +
+				     (sizeof(struct rvt_swqe) +
+				      qp->s_max_sge *
+				      sizeof(struct rvt_sge)) * n);
+}
+
+extern const int  ib_rvt_state_ops[];
+
 #endif          /* DEF_RDMAVT_INCQP_H */

commit 5a9cf6f27e36ece71cc8a192a4ca39b62a460807
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 12:50:24 2016 -0800

    IB/rdmavt: Export reset_qp in rdmavt
    
    Until all queue pair functionality is moved to rdmavt we need to provide
    access to the reset function. This is only temporary and will be reverted
    back to a static, non exported function in the end.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index 1aa8b5b40f9f..bce0a03a7c07 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -48,6 +48,7 @@
  *
  */
 
+#include <rdma/rdma_vt.h>
 #include <rdma/ib_pack.h>
 /*
  * Atomic bit definitions for r_aflags.

commit 050eb7fbe0ff2bcd95833ff180337116d5907483
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Fri Jan 22 12:50:11 2016 -0800

    IB/rdmavt: Add R and S flags for queue pairs
    
    Use the flags originally provided for hfi1 in the rdmavt driver. These will
    be made available to drivers in the qp header file.
    
    Reviewed-by: Harish Chegondi <harish.chegondi@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index e6a7d17dcd30..1aa8b5b40f9f 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -48,6 +48,86 @@
  *
  */
 
+#include <rdma/ib_pack.h>
+/*
+ * Atomic bit definitions for r_aflags.
+ */
+#define RVT_R_WRID_VALID        0
+#define RVT_R_REWIND_SGE        1
+
+/*
+ * Bit definitions for r_flags.
+ */
+#define RVT_R_REUSE_SGE 0x01
+#define RVT_R_RDMAR_SEQ 0x02
+#define RVT_R_RSP_NAK   0x04
+#define RVT_R_RSP_SEND  0x08
+#define RVT_R_COMM_EST  0x10
+
+/*
+ * Bit definitions for s_flags.
+ *
+ * RVT_S_SIGNAL_REQ_WR - set if QP send WRs contain completion signaled
+ * RVT_S_BUSY - send tasklet is processing the QP
+ * RVT_S_TIMER - the RC retry timer is active
+ * RVT_S_ACK_PENDING - an ACK is waiting to be sent after RDMA read/atomics
+ * RVT_S_WAIT_FENCE - waiting for all prior RDMA read or atomic SWQEs
+ *                         before processing the next SWQE
+ * RVT_S_WAIT_RDMAR - waiting for a RDMA read or atomic SWQE to complete
+ *                         before processing the next SWQE
+ * RVT_S_WAIT_RNR - waiting for RNR timeout
+ * RVT_S_WAIT_SSN_CREDIT - waiting for RC credits to process next SWQE
+ * RVT_S_WAIT_DMA - waiting for send DMA queue to drain before generating
+ *                  next send completion entry not via send DMA
+ * RVT_S_WAIT_PIO - waiting for a send buffer to be available
+ * RVT_S_WAIT_TX - waiting for a struct verbs_txreq to be available
+ * RVT_S_WAIT_DMA_DESC - waiting for DMA descriptors to be available
+ * RVT_S_WAIT_KMEM - waiting for kernel memory to be available
+ * RVT_S_WAIT_PSN - waiting for a packet to exit the send DMA queue
+ * RVT_S_WAIT_ACK - waiting for an ACK packet before sending more requests
+ * RVT_S_SEND_ONE - send one packet, request ACK, then wait for ACK
+ * RVT_S_ECN - a BECN was queued to the send engine
+ */
+#define RVT_S_SIGNAL_REQ_WR	0x0001
+#define RVT_S_BUSY		0x0002
+#define RVT_S_TIMER		0x0004
+#define RVT_S_RESP_PENDING	0x0008
+#define RVT_S_ACK_PENDING	0x0010
+#define RVT_S_WAIT_FENCE	0x0020
+#define RVT_S_WAIT_RDMAR	0x0040
+#define RVT_S_WAIT_RNR		0x0080
+#define RVT_S_WAIT_SSN_CREDIT	0x0100
+#define RVT_S_WAIT_DMA		0x0200
+#define RVT_S_WAIT_PIO		0x0400
+#define RVT_S_WAIT_TX		0x0800
+#define RVT_S_WAIT_DMA_DESC	0x1000
+#define RVT_S_WAIT_KMEM		0x2000
+#define RVT_S_WAIT_PSN		0x4000
+#define RVT_S_WAIT_ACK		0x8000
+#define RVT_S_SEND_ONE		0x10000
+#define RVT_S_UNLIMITED_CREDIT	0x20000
+#define RVT_S_AHG_VALID		0x40000
+#define RVT_S_AHG_CLEAR		0x80000
+#define RVT_S_ECN		0x100000
+
+/*
+ * Wait flags that would prevent any packet type from being sent.
+ */
+#define RVT_S_ANY_WAIT_IO (RVT_S_WAIT_PIO | RVT_S_WAIT_TX | \
+	RVT_S_WAIT_DMA_DESC | RVT_S_WAIT_KMEM)
+
+/*
+ * Wait flags that would prevent send work requests from making progress.
+ */
+#define RVT_S_ANY_WAIT_SEND (RVT_S_WAIT_FENCE | RVT_S_WAIT_RDMAR | \
+	RVT_S_WAIT_RNR | RVT_S_WAIT_SSN_CREDIT | RVT_S_WAIT_DMA | \
+	RVT_S_WAIT_PSN | RVT_S_WAIT_ACK)
+
+#define RVT_S_ANY_WAIT (RVT_S_ANY_WAIT_IO | RVT_S_ANY_WAIT_SEND)
+
+/* Number of bits to pay attention to in the opcode for checking qp type */
+#define RVT_OPCODE_QP_MASK 0xE0
+
 /*
  * Send work request queue entry.
  * The size of the sg_list is determined when the QP is created and stored

commit 0acb0cc7ecc1e4860b056368566c0c2c254ae281
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Wed Jan 6 10:04:46 2016 -0800

    IB/rdmavt: Initialize and teardown of qpn table
    
    Add table init as well as teardown for handling qpn maps. Drivers can still
    provide this functionality by setting the QP_INIT_DRIVER bit.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
index f33fbb0b3824..e6a7d17dcd30 100644
--- a/include/rdma/rdmavt_qp.h
+++ b/include/rdma/rdmavt_qp.h
@@ -259,4 +259,37 @@ struct rvt_srq {
 	u32 limit;
 };
 
+#define RVT_QPN_MAX                 BIT(24)
+#define RVT_QPNMAP_ENTRIES          (RVT_QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)
+#define RVT_BITS_PER_PAGE           (PAGE_SIZE * BITS_PER_BYTE)
+#define RVT_BITS_PER_PAGE_MASK      (RVT_BITS_PER_PAGE - 1)
+
+/*
+ * QPN-map pages start out as NULL, they get allocated upon
+ * first use and are never deallocated. This way,
+ * large bitmaps are not allocated unless large numbers of QPs are used.
+ */
+struct rvt_qpn_map {
+	void *page;
+};
+
+struct rvt_qpn_table {
+	spinlock_t lock; /* protect changes to the qp table */
+	unsigned flags;         /* flags for QP0/1 allocated for each port */
+	u32 last;               /* last QP number allocated */
+	u32 nmaps;              /* size of the map table */
+	u16 limit;
+	u8  incr;
+	/* bit map of free QP numbers other than 0/1 */
+	struct rvt_qpn_map map[RVT_QPNMAP_ENTRIES];
+};
+
+struct rvt_qp_ibdev {
+	u32 qp_table_size;
+	u32 qp_table_bits;
+	struct rvt_qp __rcu **qp_table;
+	spinlock_t qpt_lock; /* qptable lock */
+	struct rvt_qpn_table qpn_table;
+};
+
 #endif          /* DEF_RDMAVT_INCQP_H */

commit b4e64397dabc946b83ffb1defa1215ede84c3b97
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Wed Jan 6 10:04:31 2016 -0800

    IB/rdmavt: Break rdma_vt main include header file up
    
    Until all functionality is moved over to rdmavt drivers still need to
    access a number of fields in data structures that are predominantly
    meant to be used by rdmavt. Once these rdmavt_<ibta_object>.h header
    files are no longer being touched by drivers their content should be
    moved to rdmavt/<ibta_object>.h. While here move a couple #defines
    over to more general IB verbs header files because they fit better.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/rdmavt_qp.h b/include/rdma/rdmavt_qp.h
new file mode 100644
index 000000000000..f33fbb0b3824
--- /dev/null
+++ b/include/rdma/rdmavt_qp.h
@@ -0,0 +1,262 @@
+#ifndef DEF_RDMAVT_INCQP_H
+#define DEF_RDMAVT_INCQP_H
+
+/*
+ * Copyright(c) 2015 Intel Corporation.
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * BSD LICENSE
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *  - Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  - Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *  - Neither the name of Intel Corporation nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+/*
+ * Send work request queue entry.
+ * The size of the sg_list is determined when the QP is created and stored
+ * in qp->s_max_sge.
+ */
+struct rvt_swqe {
+	union {
+		struct ib_send_wr wr;   /* don't use wr.sg_list */
+		struct ib_ud_wr ud_wr;
+		struct ib_reg_wr reg_wr;
+		struct ib_rdma_wr rdma_wr;
+		struct ib_atomic_wr atomic_wr;
+	};
+	u32 psn;                /* first packet sequence number */
+	u32 lpsn;               /* last packet sequence number */
+	u32 ssn;                /* send sequence number */
+	u32 length;             /* total length of data in sg_list */
+	struct rvt_sge sg_list[0];
+};
+
+/*
+ * Receive work request queue entry.
+ * The size of the sg_list is determined when the QP (or SRQ) is created
+ * and stored in qp->r_rq.max_sge (or srq->rq.max_sge).
+ */
+struct rvt_rwqe {
+	u64 wr_id;
+	u8 num_sge;
+	struct ib_sge sg_list[0];
+};
+
+/*
+ * This structure is used to contain the head pointer, tail pointer,
+ * and receive work queue entries as a single memory allocation so
+ * it can be mmap'ed into user space.
+ * Note that the wq array elements are variable size so you can't
+ * just index into the array to get the N'th element;
+ * use get_rwqe_ptr() instead.
+ */
+struct rvt_rwq {
+	u32 head;               /* new work requests posted to the head */
+	u32 tail;               /* receives pull requests from here. */
+	struct rvt_rwqe wq[0];
+};
+
+struct rvt_rq {
+	struct rvt_rwq *wq;
+	u32 size;               /* size of RWQE array */
+	u8 max_sge;
+	/* protect changes in this struct */
+	spinlock_t lock ____cacheline_aligned_in_smp;
+};
+
+/*
+ * This structure is used by rvt_mmap() to validate an offset
+ * when an mmap() request is made.  The vm_area_struct then uses
+ * this as its vm_private_data.
+ */
+struct rvt_mmap_info {
+	struct list_head pending_mmaps;
+	struct ib_ucontext *context;
+	void *obj;
+	__u64 offset;
+	struct kref ref;
+	unsigned size;
+};
+
+#define RVT_MAX_RDMA_ATOMIC	16
+
+/*
+ * This structure holds the information that the send tasklet needs
+ * to send a RDMA read response or atomic operation.
+ */
+struct rvt_ack_entry {
+	u8 opcode;
+	u8 sent;
+	u32 psn;
+	u32 lpsn;
+	union {
+		struct rvt_sge rdma_sge;
+		u64 atomic_data;
+	};
+};
+
+/*
+ * Variables prefixed with s_ are for the requester (sender).
+ * Variables prefixed with r_ are for the responder (receiver).
+ * Variables prefixed with ack_ are for responder replies.
+ *
+ * Common variables are protected by both r_rq.lock and s_lock in that order
+ * which only happens in modify_qp() or changing the QP 'state'.
+ */
+struct rvt_qp {
+	struct ib_qp ibqp;
+	void *priv; /* Driver private data */
+	/* read mostly fields above and below */
+	struct ib_ah_attr remote_ah_attr;
+	struct ib_ah_attr alt_ah_attr;
+	struct rvt_qp __rcu *next;           /* link list for QPN hash table */
+	struct rvt_swqe *s_wq;  /* send work queue */
+	struct rvt_mmap_info *ip;
+
+	unsigned long timeout_jiffies;  /* computed from timeout */
+
+	enum ib_mtu path_mtu;
+	int srate_mbps;		/* s_srate (below) converted to Mbit/s */
+	u32 remote_qpn;
+	u32 pmtu;		/* decoded from path_mtu */
+	u32 qkey;               /* QKEY for this QP (for UD or RD) */
+	u32 s_size;             /* send work queue size */
+	u32 s_rnr_timeout;      /* number of milliseconds for RNR timeout */
+	u32 s_ahgpsn;           /* set to the psn in the copy of the header */
+
+	u8 state;               /* QP state */
+	u8 allowed_ops;		/* high order bits of allowed opcodes */
+	u8 qp_access_flags;
+	u8 alt_timeout;         /* Alternate path timeout for this QP */
+	u8 timeout;             /* Timeout for this QP */
+	u8 s_srate;
+	u8 s_mig_state;
+	u8 port_num;
+	u8 s_pkey_index;        /* PKEY index to use */
+	u8 s_alt_pkey_index;    /* Alternate path PKEY index to use */
+	u8 r_max_rd_atomic;     /* max number of RDMA read/atomic to receive */
+	u8 s_max_rd_atomic;     /* max number of RDMA read/atomic to send */
+	u8 s_retry_cnt;         /* number of times to retry */
+	u8 s_rnr_retry_cnt;
+	u8 r_min_rnr_timer;     /* retry timeout value for RNR NAKs */
+	u8 s_max_sge;           /* size of s_wq->sg_list */
+	u8 s_draining;
+
+	/* start of read/write fields */
+	atomic_t refcount ____cacheline_aligned_in_smp;
+	wait_queue_head_t wait;
+
+	struct rvt_ack_entry s_ack_queue[RVT_MAX_RDMA_ATOMIC + 1]
+		____cacheline_aligned_in_smp;
+	struct rvt_sge_state s_rdma_read_sge;
+
+	spinlock_t r_lock ____cacheline_aligned_in_smp;      /* used for APM */
+	unsigned long r_aflags;
+	u64 r_wr_id;            /* ID for current receive WQE */
+	u32 r_ack_psn;          /* PSN for next ACK or atomic ACK */
+	u32 r_len;              /* total length of r_sge */
+	u32 r_rcv_len;          /* receive data len processed */
+	u32 r_psn;              /* expected rcv packet sequence number */
+	u32 r_msn;              /* message sequence number */
+
+	u8 r_state;             /* opcode of last packet received */
+	u8 r_flags;
+	u8 r_head_ack_queue;    /* index into s_ack_queue[] */
+
+	struct list_head rspwait;       /* link for waiting to respond */
+
+	struct rvt_sge_state r_sge;     /* current receive data */
+	struct rvt_rq r_rq;             /* receive work queue */
+
+	spinlock_t s_lock ____cacheline_aligned_in_smp;
+	struct rvt_sge_state *s_cur_sge;
+	u32 s_flags;
+	struct rvt_swqe *s_wqe;
+	struct rvt_sge_state s_sge;     /* current send request data */
+	struct rvt_mregion *s_rdma_mr;
+	struct sdma_engine *s_sde; /* current sde */
+	u32 s_cur_size;         /* size of send packet in bytes */
+	u32 s_len;              /* total length of s_sge */
+	u32 s_rdma_read_len;    /* total length of s_rdma_read_sge */
+	u32 s_next_psn;         /* PSN for next request */
+	u32 s_last_psn;         /* last response PSN processed */
+	u32 s_sending_psn;      /* lowest PSN that is being sent */
+	u32 s_sending_hpsn;     /* highest PSN that is being sent */
+	u32 s_psn;              /* current packet sequence number */
+	u32 s_ack_rdma_psn;     /* PSN for sending RDMA read responses */
+	u32 s_ack_psn;          /* PSN for acking sends and RDMA writes */
+	u32 s_head;             /* new entries added here */
+	u32 s_tail;             /* next entry to process */
+	u32 s_cur;              /* current work queue entry */
+	u32 s_acked;            /* last un-ACK'ed entry */
+	u32 s_last;             /* last completed entry */
+	u32 s_ssn;              /* SSN of tail entry */
+	u32 s_lsn;              /* limit sequence number (credit) */
+	u16 s_hdrwords;         /* size of s_hdr in 32 bit words */
+	u16 s_rdma_ack_cnt;
+	s8 s_ahgidx;
+	u8 s_state;             /* opcode of last packet sent */
+	u8 s_ack_state;         /* opcode of packet to ACK */
+	u8 s_nak_state;         /* non-zero if NAK is pending */
+	u8 r_nak_state;         /* non-zero if NAK is pending */
+	u8 s_retry;             /* requester retry counter */
+	u8 s_rnr_retry;         /* requester RNR retry counter */
+	u8 s_num_rd_atomic;     /* number of RDMA read/atomic pending */
+	u8 s_tail_ack_queue;    /* index into s_ack_queue[] */
+
+	struct rvt_sge_state s_ack_rdma_sge;
+	struct timer_list s_timer;
+
+	/*
+	 * This sge list MUST be last. Do not add anything below here.
+	 */
+	struct rvt_sge r_sg_list[0] /* verified SGEs */
+		____cacheline_aligned_in_smp;
+};
+
+struct rvt_srq {
+	struct ib_srq ibsrq;
+	struct rvt_rq rq;
+	struct rvt_mmap_info *ip;
+	/* send signal when number of RWQEs < limit */
+	u32 limit;
+};
+
+#endif          /* DEF_RDMAVT_INCQP_H */
