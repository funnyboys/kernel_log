commit ceabef7dd71720aef58bd182943352c9c307a3de
Author: Orson Zhai <orson.zhai@unisoc.com>
Date:   Sun Jun 7 21:40:14 2020 -0700

    dynamic_debug: add an option to enable dynamic debug for modules only
    
    Instead of enabling dynamic debug globally with CONFIG_DYNAMIC_DEBUG,
    CONFIG_DYNAMIC_DEBUG_CORE will only enable core function of dynamic
    debug.  With the DYNAMIC_DEBUG_MODULE defined for any modules, dynamic
    debug will be tied to them.
    
    This is useful for people who only want to enable dynamic debug for
    kernel modules without worrying about kernel image size and memory
    consumption is increasing too much.
    
    [orson.zhai@unisoc.com: v2]
      Link: http://lkml.kernel.org/r/1587408228-10861-1-git-send-email-orson.unisoc@gmail.com
    
    Signed-off-by: Orson Zhai <orson.zhai@unisoc.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Petr Mladek <pmladek@suse.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Link: http://lkml.kernel.org/r/1586521984-5890-1-git-send-email-orson.unisoc@gmail.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 033e7044f29c..ef2f3986c493 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -100,7 +100,8 @@ void ibdev_notice(const struct ib_device *ibdev, const char *format, ...);
 __printf(2, 3) __cold
 void ibdev_info(const struct ib_device *ibdev, const char *format, ...);
 
-#if defined(CONFIG_DYNAMIC_DEBUG)
+#if defined(CONFIG_DYNAMIC_DEBUG) || \
+	(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))
 #define ibdev_dbg(__dev, format, args...)                       \
 	dynamic_ibdev_dbg(__dev, format, ##args)
 #else
@@ -133,7 +134,8 @@ do {                                                                    \
 #define ibdev_info_ratelimited(ibdev, fmt, ...) \
 	ibdev_level_ratelimited(ibdev_info, ibdev, fmt, ##__VA_ARGS__)
 
-#if defined(CONFIG_DYNAMIC_DEBUG)
+#if defined(CONFIG_DYNAMIC_DEBUG) || \
+	(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))
 /* descriptor check is first to prevent flooding with "callbacks suppressed" */
 #define ibdev_dbg_ratelimited(ibdev, fmt, ...)                          \
 do {                                                                    \

commit 4d12c04caa88cd3115f25acd832a7cddb698981b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu May 28 16:45:55 2020 -0300

    RDMA: Remove 'max_map_per_fmr'
    
    Now that FMR support is gone, this attribute can be deleted from all
    places.
    
    Link: https://lore.kernel.org/r/13-v3-f58e6669d5d3+2cf-fmr_removal_jgg@mellanox.com
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c4708b3243f9..033e7044f29c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -430,7 +430,6 @@ struct ib_device_attr {
 	int			max_mcast_qp_attach;
 	int			max_total_mcast_qp_attach;
 	int			max_ah;
-	int			max_map_per_fmr;
 	int			max_srq;
 	int			max_srq_wr;
 	int			max_srq_sge;

commit 649392bf75a423287a9c4936b341677f12e8cf0b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu May 28 16:45:54 2020 -0300

    RDMA: Remove 'max_fmr'
    
    Now that FMR support is gone, this attribute can be deleted from all
    places.
    
    Link: https://lore.kernel.org/r/12-v3-f58e6669d5d3+2cf-fmr_removal_jgg@mellanox.com
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Bernard Metzler <bmt@zurich.ibm.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ff6a8053ec52..c4708b3243f9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -430,7 +430,6 @@ struct ib_device_attr {
 	int			max_mcast_qp_attach;
 	int			max_total_mcast_qp_attach;
 	int			max_ah;
-	int			max_fmr;
 	int			max_map_per_fmr;
 	int			max_srq;
 	int			max_srq_wr;

commit 3a578152a9208bbcd196210be2f5396744cda302
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Thu May 28 16:45:53 2020 -0300

    RDMA/core: Remove FMR device ops
    
    After removing FMR support from all the RDMA ULPs and providers, there
    is no need to keep FMR operation for IB devices.
    
    Link: https://lore.kernel.org/r/11-v3-f58e6669d5d3+2cf-fmr_removal_jgg@mellanox.com
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 19864da78649..ff6a8053ec52 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1475,12 +1475,6 @@ enum ib_mr_rereg_flags {
 	IB_MR_REREG_SUPPORTED	= ((IB_MR_REREG_ACCESS << 1) - 1)
 };
 
-struct ib_fmr_attr {
-	int	max_pages;
-	int	max_maps;
-	u8	page_shift;
-};
-
 struct ib_umem;
 
 enum rdma_remove_reason {
@@ -1855,14 +1849,6 @@ struct ib_mw {
 	enum ib_mw_type         type;
 };
 
-struct ib_fmr {
-	struct ib_device	*device;
-	struct ib_pd		*pd;
-	struct list_head	list;
-	u32			lkey;
-	u32			rkey;
-};
-
 /* Supported steering options */
 enum ib_flow_attr_type {
 	/* steering according to rule specifications */
@@ -2505,12 +2491,6 @@ struct ib_device_ops {
 	struct ib_mw *(*alloc_mw)(struct ib_pd *pd, enum ib_mw_type type,
 				  struct ib_udata *udata);
 	int (*dealloc_mw)(struct ib_mw *mw);
-	struct ib_fmr *(*alloc_fmr)(struct ib_pd *pd, int mr_access_flags,
-				    struct ib_fmr_attr *fmr_attr);
-	int (*map_phys_fmr)(struct ib_fmr *fmr, u64 *page_list, int list_len,
-			    u64 iova);
-	int (*unmap_fmr)(struct list_head *fmr_list);
-	int (*dealloc_fmr)(struct ib_fmr *fmr);
 	int (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	int (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,
@@ -4319,45 +4299,6 @@ static inline u32 ib_inc_rkey(u32 rkey)
 	return ((rkey + 1) & mask) | (rkey & ~mask);
 }
 
-/**
- * ib_alloc_fmr - Allocates a unmapped fast memory region.
- * @pd: The protection domain associated with the unmapped region.
- * @mr_access_flags: Specifies the memory access rights.
- * @fmr_attr: Attributes of the unmapped region.
- *
- * A fast memory region must be mapped before it can be used as part of
- * a work request.
- */
-struct ib_fmr *ib_alloc_fmr(struct ib_pd *pd,
-			    int mr_access_flags,
-			    struct ib_fmr_attr *fmr_attr);
-
-/**
- * ib_map_phys_fmr - Maps a list of physical pages to a fast memory region.
- * @fmr: The fast memory region to associate with the pages.
- * @page_list: An array of physical pages to map to the fast memory region.
- * @list_len: The number of pages in page_list.
- * @iova: The I/O virtual address to use with the mapped region.
- */
-static inline int ib_map_phys_fmr(struct ib_fmr *fmr,
-				  u64 *page_list, int list_len,
-				  u64 iova)
-{
-	return fmr->device->ops.map_phys_fmr(fmr, page_list, list_len, iova);
-}
-
-/**
- * ib_unmap_fmr - Removes the mapping from a list of fast memory regions.
- * @fmr_list: A linked list of fast memory regions to unmap.
- */
-int ib_unmap_fmr(struct list_head *fmr_list);
-
-/**
- * ib_dealloc_fmr - Deallocates a fast memory region.
- * @fmr: The fast memory region to deallocate.
- */
-int ib_dealloc_fmr(struct ib_fmr *fmr);
-
 /**
  * ib_attach_mcast - Attaches the specified QP to a multicast group.
  * @qp: QP to attach to the multicast group.  The QP must be type

commit c7ff819aefea04944dfcec5f0731b97277df6a9c
Author: Yamin Friedman <yaminf@mellanox.com>
Date:   Wed May 27 11:34:53 2020 +0300

    RDMA/core: Introduce shared CQ pool API
    
    Allow a ULP to ask the core to provide a completion queue based on a
    least-used search on a per-device CQ pools. The device CQ pools grow in a
    lazy fashion when more CQs are requested.
    
    This feature reduces the amount of interrupts when using many QPs.  Using
    shared CQs allows for more effcient completion handling. It also reduces
    the amount of overhead needed for CQ contexts.
    
    Test setup:
    Intel(R) Xeon(R) Platinum 8176M CPU @ 2.10GHz servers.
    Running NVMeoF 4KB read IOs over ConnectX-5EX across Spectrum switch.
    TX-depth = 32. The patch was applied in the nvme driver on both the target
    and initiator. Four controllers are accessed from each core. In the
    current test case we have exposed sixteen NVMe namespaces using four
    different subsystems (four namespaces per subsystem) from one NVM port.
    Each controller allocated X queues (RDMA QPs) and attached to Y CQs.
    Before this series we had X == Y, i.e for four controllers we've created
    total of 4X QPs and 4X CQs. In the shared case, we've created 4X QPs and
    only X CQs which means that we have four controllers that share a
    completion queue per core. Until fourteen cores there is no significant
    change in performance and the number of interrupts per second is less than
    a million in the current case.
    ==================================================
    |Cores|Current KIOPs  |Shared KIOPs  |improvement|
    |-----|---------------|--------------|-----------|
    |14   |2332           |2723          |16.7%      |
    |-----|---------------|--------------|-----------|
    |20   |2086           |2712          |30%        |
    |-----|---------------|--------------|-----------|
    |28   |1971           |2669          |35.4%      |
    |=================================================
    |Cores|Current avg lat|Shared avg lat|improvement|
    |-----|---------------|--------------|-----------|
    |14   |767us          |657us         |14.3%      |
    |-----|---------------|--------------|-----------|
    |20   |1225us         |943us         |23%        |
    |-----|---------------|--------------|-----------|
    |28   |1816us         |1341us        |26.1%      |
    ========================================================
    |Cores|Current interrupts|Shared interrupts|improvement|
    |-----|------------------|-----------------|-----------|
    |14   |1.6M/sec          |0.4M/sec         |72%        |
    |-----|------------------|-----------------|-----------|
    |20   |2.8M/sec          |0.6M/sec         |72.4%      |
    |-----|------------------|-----------------|-----------|
    |28   |2.9M/sec          |0.8M/sec         |63.4%      |
    ====================================================================
    |Cores|Current 99.99th PCTL lat|Shared 99.99th PCTL lat|improvement|
    |-----|------------------------|-----------------------|-----------|
    |14   |67ms                    |6ms                    |90.9%      |
    |-----|------------------------|-----------------------|-----------|
    |20   |5ms                     |6ms                    |-10%       |
    |-----|------------------------|-----------------------|-----------|
    |28   |8.7ms                   |6ms                    |25.9%      |
    |===================================================================
    
    Performance improvement with sixteen disks (sixteen CQs per core) is
    comparable.
    
    Link: https://lore.kernel.org/r/1590568495-101621-3-git-send-email-yaminf@mellanox.com
    Signed-off-by: Yamin Friedman <yaminf@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cc515025cbdb..19864da78649 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1588,10 +1588,12 @@ struct ib_ah {
 typedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);
 
 enum ib_poll_context {
-	IB_POLL_DIRECT,		   /* caller context, no hw completions */
 	IB_POLL_SOFTIRQ,	   /* poll from softirq context */
 	IB_POLL_WORKQUEUE,	   /* poll from workqueue */
 	IB_POLL_UNBOUND_WORKQUEUE, /* poll from unbound workqueue */
+	IB_POLL_LAST_POOL_TYPE = IB_POLL_UNBOUND_WORKQUEUE,
+
+	IB_POLL_DIRECT,		   /* caller context, no hw completions */
 };
 
 struct ib_cq {
@@ -1601,9 +1603,11 @@ struct ib_cq {
 	void                  (*event_handler)(struct ib_event *, void *);
 	void                   *cq_context;
 	int               	cqe;
+	unsigned int		cqe_used;
 	atomic_t          	usecnt; /* count number of work queues */
 	enum ib_poll_context	poll_ctx;
 	struct ib_wc		*wc;
+	struct list_head        pool_entry;
 	union {
 		struct irq_poll		iop;
 		struct work_struct	work;
@@ -1615,6 +1619,7 @@ struct ib_cq {
 	ktime_t timestamp;
 	u8 interrupt:1;
 	u8 shared:1;
+	unsigned int comp_vector;
 
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
@@ -2734,6 +2739,10 @@ struct ib_device {
 #endif
 
 	u32                          index;
+
+	spinlock_t                   cq_pools_lock;
+	struct list_head             cq_pools[IB_POLL_LAST_POOL_TYPE + 1];
+
 	struct rdma_restrack_root *res;
 
 	const struct uapi_definition   *driver_def;
@@ -4037,6 +4046,12 @@ static inline int ib_req_notify_cq(struct ib_cq *cq,
 	return cq->device->ops.req_notify_cq(cq, flags);
 }
 
+struct ib_cq *ib_cq_pool_get(struct ib_device *dev, unsigned int nr_cqe,
+			     int comp_vector_hint,
+			     enum ib_poll_context poll_ctx);
+
+void ib_cq_pool_put(struct ib_cq *cq, unsigned int nr_cqe);
+
 /**
  * ib_req_ncomp_notif - Request completion notification when there are
  *   at least the specified number of unreaped completions on the CQ.

commit 3446cbd2d523fdaf37f3772082071d1154c419d9
Author: Yamin Friedman <yaminf@mellanox.com>
Date:   Wed May 27 11:34:52 2020 +0300

    RDMA/core: Add protection for shared CQs used by ULPs
    
    A pre-step for adding shared CQs. Add the infrastructure to prevent shared
    CQ users from altering the CQ configurations. For now all cqs are marked
    as private (non-shared). The core driver should use the new force
    functions to perform resize/destroy/moderation changes that are not
    allowed for users of shared CQs.
    
    Link: https://lore.kernel.org/r/1590568495-101621-2-git-send-email-yaminf@mellanox.com
    Signed-off-by: Yamin Friedman <yaminf@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 94533ae16697..cc515025cbdb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1613,7 +1613,8 @@ struct ib_cq {
 
 	/* updated only by trace points */
 	ktime_t timestamp;
-	bool interrupt;
+	u8 interrupt:1;
+	u8 shared:1;
 
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
@@ -3909,6 +3910,8 @@ static inline struct ib_cq *ib_alloc_cq_any(struct ib_device *dev,
  * ib_free_cq_user - Free kernel/user CQ
  * @cq: The CQ to free
  * @udata: Valid user data or NULL for kernel objects
+ *
+ * NOTE: This function shouldn't be called on shared CQs.
  */
 void ib_free_cq_user(struct ib_cq *cq, struct ib_udata *udata);
 

commit 175ba58d62c84e1216cdf8b4f49f79e55e1ed04b
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue May 19 10:27:08 2020 +0300

    IB/uverbs: Move QP, SRQ, WQ type and flags to UAPI
    
    These constants are going to be used in the ioctl interface in coming
    patches so they are part of the UAPI, place them in the correct header
    for clarity.
    
    Link: https://lore.kernel.org/r/20200519072711.257271-5-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c988e9205cf9..94533ae16697 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1040,9 +1040,9 @@ enum ib_cq_notify_flags {
 };
 
 enum ib_srq_type {
-	IB_SRQT_BASIC,
-	IB_SRQT_XRC,
-	IB_SRQT_TM,
+	IB_SRQT_BASIC = IB_UVERBS_SRQT_BASIC,
+	IB_SRQT_XRC = IB_UVERBS_SRQT_XRC,
+	IB_SRQT_TM = IB_UVERBS_SRQT_TM,
 };
 
 static inline bool ib_srq_has_cq(enum ib_srq_type srq_type)
@@ -1111,16 +1111,16 @@ enum ib_qp_type {
 	IB_QPT_SMI,
 	IB_QPT_GSI,
 
-	IB_QPT_RC,
-	IB_QPT_UC,
-	IB_QPT_UD,
+	IB_QPT_RC = IB_UVERBS_QPT_RC,
+	IB_QPT_UC = IB_UVERBS_QPT_UC,
+	IB_QPT_UD = IB_UVERBS_QPT_UD,
 	IB_QPT_RAW_IPV6,
 	IB_QPT_RAW_ETHERTYPE,
-	IB_QPT_RAW_PACKET = 8,
-	IB_QPT_XRC_INI = 9,
-	IB_QPT_XRC_TGT,
+	IB_QPT_RAW_PACKET = IB_UVERBS_QPT_RAW_PACKET,
+	IB_QPT_XRC_INI = IB_UVERBS_QPT_XRC_INI,
+	IB_QPT_XRC_TGT = IB_UVERBS_QPT_XRC_TGT,
 	IB_QPT_MAX,
-	IB_QPT_DRIVER = 0xFF,
+	IB_QPT_DRIVER = IB_UVERBS_QPT_DRIVER,
 	/* Reserve a range for qp types internal to the low level driver.
 	 * These qp types will not be visible at the IB core layer, so the
 	 * IB_QPT_MAX usages should not be affected in the core layer
@@ -1139,17 +1139,21 @@ enum ib_qp_type {
 
 enum ib_qp_create_flags {
 	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
-	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
+	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	=
+		IB_UVERBS_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,
 	IB_QP_CREATE_CROSS_CHANNEL              = 1 << 2,
 	IB_QP_CREATE_MANAGED_SEND               = 1 << 3,
 	IB_QP_CREATE_MANAGED_RECV               = 1 << 4,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_INTEGRITY_EN		= 1 << 6,
 	IB_QP_CREATE_NETDEV_USE			= 1 << 7,
-	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
-	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
+	IB_QP_CREATE_SCATTER_FCS		=
+		IB_UVERBS_QP_CREATE_SCATTER_FCS,
+	IB_QP_CREATE_CVLAN_STRIPPING		=
+		IB_UVERBS_QP_CREATE_CVLAN_STRIPPING,
 	IB_QP_CREATE_SOURCE_QPN			= 1 << 10,
-	IB_QP_CREATE_PCI_WRITE_END_PADDING	= 1 << 11,
+	IB_QP_CREATE_PCI_WRITE_END_PADDING	=
+		IB_UVERBS_QP_CREATE_PCI_WRITE_END_PADDING,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
@@ -1654,7 +1658,7 @@ enum ib_raw_packet_caps {
 };
 
 enum ib_wq_type {
-	IB_WQT_RQ
+	IB_WQT_RQ = IB_UVERBS_WQT_RQ,
 };
 
 enum ib_wq_state {
@@ -1677,10 +1681,11 @@ struct ib_wq {
 };
 
 enum ib_wq_flags {
-	IB_WQ_FLAGS_CVLAN_STRIPPING	= 1 << 0,
-	IB_WQ_FLAGS_SCATTER_FCS		= 1 << 1,
-	IB_WQ_FLAGS_DELAY_DROP		= 1 << 2,
-	IB_WQ_FLAGS_PCI_WRITE_END_PADDING = 1 << 3,
+	IB_WQ_FLAGS_CVLAN_STRIPPING	= IB_UVERBS_WQ_FLAGS_CVLAN_STRIPPING,
+	IB_WQ_FLAGS_SCATTER_FCS		= IB_UVERBS_WQ_FLAGS_SCATTER_FCS,
+	IB_WQ_FLAGS_DELAY_DROP		= IB_UVERBS_WQ_FLAGS_DELAY_DROP,
+	IB_WQ_FLAGS_PCI_WRITE_END_PADDING =
+				IB_UVERBS_WQ_FLAGS_PCI_WRITE_END_PADDING,
 };
 
 struct ib_wq_init_attr {

commit 0ac8903cbbe618d947b5815d6e0f7b044ee83aa3
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue May 19 10:27:05 2020 +0300

    RDMA/core: Allow the ioctl layer to abort a fully created uobject
    
    While creating a uobject every create reaches a point where the uobject is
    fully initialized. For ioctls that go on to copy_to_user this means they
    need to open code the destruction of a fully created uobject - ie the
    RDMA_REMOVE_DESTROY sort of flow.
    
    Open coding this creates bugs, eg the CQ does not properly flush the
    events list when it does its error unwind.
    
    Provide a uverbs_finalize_uobj_create() function which indicates that the
    uobject is fully initialized and that abort should call to destroy_hw to
    destroy the uobj->object and related.
    
    Methods can call this function if they go on to have error cases after
    setting uobj->object. Once done those error cases can simply do return,
    without an error unwind.
    
    Link: https://lore.kernel.org/r/20200519072711.257271-2-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 641f4751b062..c988e9205cf9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1491,6 +1491,11 @@ enum rdma_remove_reason {
 	RDMA_REMOVE_DRIVER_REMOVE,
 	/* uobj is being cleaned-up before being committed */
 	RDMA_REMOVE_ABORT,
+	/*
+	 * uobj has been fully created, with the uobj->object set, but is being
+	 * cleaned up before being comitted
+	 */
+	RDMA_REMOVE_ABORT_HWOBJ,
 };
 
 struct ib_rdmacg_object {

commit 6d72344cf6c47010cc2055a832e16c7fcdd16f82
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Mon May 11 12:06:18 2020 -0400

    IB/ipoib: Increase ipoib Datagram mode MTU's upper limit
    
    Currently the ipoib UD mtu is restricted to 4K bytes. Remove this
    limitation so that the IPOIB module can potentially use an MTU (in UD
    mode) that is bounded by the MTU of the underlying device. A field is
    added to the ib_port_attr structure to indicate the maximum physical
    MTU the underlying device supports.
    
    Link: https://lore.kernel.org/r/20200511160618.173205.23053.stgit@awfm-01.aw.intel.com
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Sadanand Warrier <sadanand.warrier@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6278e4e040fc..641f4751b062 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -462,6 +462,11 @@ enum ib_mtu {
 	IB_MTU_4096 = 5
 };
 
+enum opa_mtu {
+	OPA_MTU_8192 = 6,
+	OPA_MTU_10240 = 7
+};
+
 static inline int ib_mtu_enum_to_int(enum ib_mtu mtu)
 {
 	switch (mtu) {
@@ -488,6 +493,28 @@ static inline enum ib_mtu ib_mtu_int_to_enum(int mtu)
 		return IB_MTU_256;
 }
 
+static inline int opa_mtu_enum_to_int(enum opa_mtu mtu)
+{
+	switch (mtu) {
+	case OPA_MTU_8192:
+		return 8192;
+	case OPA_MTU_10240:
+		return 10240;
+	default:
+		return(ib_mtu_enum_to_int((enum ib_mtu)mtu));
+	}
+}
+
+static inline enum opa_mtu opa_mtu_int_to_enum(int mtu)
+{
+	if (mtu >= 10240)
+		return OPA_MTU_10240;
+	else if (mtu >= 8192)
+		return OPA_MTU_8192;
+	else
+		return ((enum opa_mtu)ib_mtu_int_to_enum(mtu));
+}
+
 enum ib_port_state {
 	IB_PORT_NOP		= 0,
 	IB_PORT_DOWN		= 1,
@@ -651,6 +678,7 @@ struct ib_port_attr {
 	enum ib_port_state	state;
 	enum ib_mtu		max_mtu;
 	enum ib_mtu		active_mtu;
+	u32                     phys_mtu;
 	int			gid_tbl_len;
 	unsigned int		ip_gids:1;
 	/* This is the value from PortInfo CapabilityMask, defined by IBA */
@@ -3364,6 +3392,55 @@ static inline unsigned int rdma_find_pg_bit(unsigned long addr,
 	return __fls(pgsz);
 }
 
+/**
+ * rdma_core_cap_opa_port - Return whether the RDMA Port is OPA or not.
+ * @device: Device
+ * @port_num: 1 based Port number
+ *
+ * Return true if port is an Intel OPA port , false if not
+ */
+static inline bool rdma_core_cap_opa_port(struct ib_device *device,
+					  u32 port_num)
+{
+	return (device->port_data[port_num].immutable.core_cap_flags &
+		RDMA_CORE_PORT_INTEL_OPA) == RDMA_CORE_PORT_INTEL_OPA;
+}
+
+/**
+ * rdma_mtu_enum_to_int - Return the mtu of the port as an integer value.
+ * @device: Device
+ * @port_num: Port number
+ * @mtu: enum value of MTU
+ *
+ * Return the MTU size supported by the port as an integer value. Will return
+ * -1 if enum value of mtu is not supported.
+ */
+static inline int rdma_mtu_enum_to_int(struct ib_device *device, u8 port,
+				       int mtu)
+{
+	if (rdma_core_cap_opa_port(device, port))
+		return opa_mtu_enum_to_int((enum opa_mtu)mtu);
+	else
+		return ib_mtu_enum_to_int((enum ib_mtu)mtu);
+}
+
+/**
+ * rdma_mtu_from_attr - Return the mtu of the port from the port attribute.
+ * @device: Device
+ * @port_num: Port number
+ * @attr: port attribute
+ *
+ * Return the MTU size supported by the port as an integer value.
+ */
+static inline int rdma_mtu_from_attr(struct ib_device *device, u8 port,
+				     struct ib_port_attr *attr)
+{
+	if (rdma_core_cap_opa_port(device, port))
+		return attr->phys_mtu;
+	else
+		return ib_mtu_enum_to_int(attr->max_mtu);
+}
+
 int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
 			 int state);
 int ib_get_vf_config(struct ib_device *device, int vf, u8 port,

commit 7f90a5a069f8dff9c76505b9853f95667d117c15
Author: Gary Leshner <Gary.S.Leshner@intel.com>
Date:   Mon May 11 12:06:07 2020 -0400

    IB/{rdmavt, hfi1}: Implement creation of accelerated UD QPs
    
    Adds capability to create a qpn to be recognized as an accelerated
    UD QP for ipoib.
    
    This is accomplished by reserving 0x81 in byte[0] of the qpn as the
    prefix for these qp types and reserving qpns between 0x810000 and
    0x81ffff.
    
    The hfi1 capability mask already contained a flag for the VNIC netdev.
    This has been renamed and extended to include both VNIC and ipoib.
    
    The rvt code to allocate qps now recognizes this flag and sets 0x81
    into byte[0] of the qpn.
    
    The code to allocate qpns is modified to reset the qpn numbering when it
    is detected that a value is located in byte[0] for a UD QP and it is a
    qpn being requested for net dev use. If it is a regular UD QP then it is
    allowable to have bits set in byte[0] of the qpn and provide the
    previously normal behavior.
    
    The code to free the qpn now checks for the AIP prefix value of 0x81 and
    removes it from the qpn before being freed so that the lower 16 bit
    number can be reused.
    
    This patch requires minor changes in the IB core and ipoib to facilitate
    the creation of accelerated UP QPs.
    
    Link: https://lore.kernel.org/r/20200511160607.173205.11757.stgit@awfm-01.aw.intel.com
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Gary Leshner <Gary.S.Leshner@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 029541a8faeb..6278e4e040fc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -305,7 +305,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_VIRTUAL_FUNCTION		= (1ULL << 33),
 	/* Deprecated. Please use IB_RAW_PACKET_CAP_SCATTER_FCS. */
 	IB_DEVICE_RAW_SCATTER_FCS		= (1ULL << 34),
-	IB_DEVICE_RDMA_NETDEV_OPA_VNIC		= (1ULL << 35),
+	IB_DEVICE_RDMA_NETDEV_OPA		= (1ULL << 35),
 	/* The device supports padding incoming writes to cacheline. */
 	IB_DEVICE_PCI_WRITE_END_PADDING		= (1ULL << 36),
 	IB_DEVICE_ALLOW_USER_UNREG		= (1ULL << 37),
@@ -1117,7 +1117,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_MANAGED_RECV               = 1 << 4,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_INTEGRITY_EN		= 1 << 6,
-	/* FREE					= 1 << 7, */
+	IB_QP_CREATE_NETDEV_USE			= 1 << 7,
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
 	IB_QP_CREATE_SOURCE_QPN			= 1 << 10,

commit d99dc602e2a55a99940ba9506a7126dfa54d54ea
Author: Gary Leshner <Gary.S.Leshner@intel.com>
Date:   Mon May 11 12:05:48 2020 -0400

    IB/hfi1: Add functions to transmit datagram ipoib packets
    
    This patch implements the mechanism to accelerate the transmit side of
    a multiple transmit queue RDMA netdev by submitting the packets to
    the SDMA engine directly instead of sending through the verbs layer.
    
    This patch also changes the UD/SEND_ONLY op to output the entropy value
    in byte 0 of deth[1]. UD/SEND_ONLY_WITH_IMMEDIATE uses the previous
    behavior with no entropy value being output.
    
    The code in the ipoib rdma netdev which submits tx requests upon
    successful submission will call trace_sdma_output_ibhdr to output
    the ibhdr to the trace buffer.
    
    Link: https://lore.kernel.org/r/20200511160548.173205.45616.stgit@awfm-01.aw.intel.com
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Gary Leshner <Gary.S.Leshner@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index db58f11552f1..029541a8faeb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2205,6 +2205,7 @@ struct rdma_netdev {
 	void              *clnt_priv;
 	struct ib_device  *hca;
 	u8                 port_num;
+	int                mtu;
 
 	/*
 	 * cleanup function must be specified.

commit b0810b037de0b62a3c6e3abfc123fe2734335f53
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed May 6 11:24:39 2020 +0300

    RDMA/core: Consolidate ib_create_srq flows
    
    The uverbs layer largely duplicate the code in ib_create_srq(), with the
    slight difference that it passes in a udata. Move all the code together
    into ib_create_srq_user() and provide an inline for kernel users, similar
    to other create calls.
    
    Link: https://lore.kernel.org/r/20200506082444.14502-6-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4c488cade70f..db58f11552f1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3559,21 +3559,18 @@ static inline int rdma_destroy_ah(struct ib_ah *ah, u32 flags)
 	return rdma_destroy_ah_user(ah, flags, NULL);
 }
 
-/**
- * ib_create_srq - Creates a SRQ associated with the specified protection
- *   domain.
- * @pd: The protection domain associated with the SRQ.
- * @srq_init_attr: A list of initial attributes required to create the
- *   SRQ.  If SRQ creation succeeds, then the attributes are updated to
- *   the actual capabilities of the created SRQ.
- *
- * srq_attr->max_wr and srq_attr->max_sge are read the determine the
- * requested size of the SRQ, and set to the actual values allocated
- * on return.  If ib_create_srq() succeeds, then max_wr and max_sge
- * will always be at least as large as the requested values.
- */
-struct ib_srq *ib_create_srq(struct ib_pd *pd,
-			     struct ib_srq_init_attr *srq_init_attr);
+struct ib_srq *ib_create_srq_user(struct ib_pd *pd,
+				  struct ib_srq_init_attr *srq_init_attr,
+				  struct ib_usrq_object *uobject,
+				  struct ib_udata *udata);
+static inline struct ib_srq *
+ib_create_srq(struct ib_pd *pd, struct ib_srq_init_attr *srq_init_attr)
+{
+	if (!pd->device->ops.create_srq)
+		return ERR_PTR(-EOPNOTSUPP);
+
+	return ib_create_srq_user(pd, srq_init_attr, NULL, NULL);
+}
 
 /**
  * ib_modify_srq - Modifies the attributes for the specified SRQ.

commit d5665a21250efeeb73579a2f8d71ee1820f37952
Author: Mark Zhang <markz@mellanox.com>
Date:   Mon May 4 08:19:31 2020 +0300

    RDMA/core: Add hash functions to calculate RoCEv2 flowlabel and UDP source port
    
    Add two hash functions to distribute RoCE v2 UDP source and Flowlabel
    symmetrically. These are user visible API and any change in the
    implementation needs to be tested for inter-operability between old and
    new variant.
    
    Link: https://lore.kernel.org/r/20200504051935.269708-2-leon@kernel.org
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3d715e2fc66..4c488cade70f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4709,4 +4709,48 @@ static inline struct ib_device *rdma_device_to_ibdev(struct device *device)
 
 bool rdma_dev_access_netns(const struct ib_device *device,
 			   const struct net *net);
+
+#define IB_ROCE_UDP_ENCAP_VALID_PORT_MIN (0xC000)
+#define IB_GRH_FLOWLABEL_MASK (0x000FFFFF)
+
+/**
+ * rdma_flow_label_to_udp_sport - generate a RoCE v2 UDP src port value based
+ *                               on the flow_label
+ *
+ * This function will convert the 20 bit flow_label input to a valid RoCE v2
+ * UDP src port 14 bit value. All RoCE V2 drivers should use this same
+ * convention.
+ */
+static inline u16 rdma_flow_label_to_udp_sport(u32 fl)
+{
+	u32 fl_low = fl & 0x03fff, fl_high = fl & 0xFC000;
+
+	fl_low ^= fl_high >> 14;
+	return (u16)(fl_low | IB_ROCE_UDP_ENCAP_VALID_PORT_MIN);
+}
+
+/**
+ * rdma_calc_flow_label - generate a RDMA symmetric flow label value based on
+ *                        local and remote qpn values
+ *
+ * This function folded the multiplication results of two qpns, 24 bit each,
+ * fields, and converts it to a 20 bit results.
+ *
+ * This function will create symmetric flow_label value based on the local
+ * and remote qpn values. this will allow both the requester and responder
+ * to calculate the same flow_label for a given connection.
+ *
+ * This helper function should be used by driver in case the upper layer
+ * provide a zero flow_label value. This is to improve entropy of RDMA
+ * traffic in the network.
+ */
+static inline u32 rdma_calc_flow_label(u32 lqpn, u32 rqpn)
+{
+	u64 v = (u64)lqpn * rqpn;
+
+	v ^= v >> 20;
+	v ^= v >> 40;
+
+	return (u32)(v & IB_GRH_FLOWLABEL_MASK);
+}
 #endif /* IB_VERBS_H */

commit 11a0ae4c4bff9b2a471b54dbe910fc0f60e58e62
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Apr 21 20:24:40 2020 +0300

    RDMA: Allow ib_client's to fail when add() is called
    
    When a client is added it isn't allowed to fail, but all the client's have
    various failure paths within their add routines.
    
    This creates the very fringe condition where the client was added, failed
    during add and didn't set the client_data. The core code will then still
    call other client_data centric ops like remove(), rename(), get_nl_info(),
    and get_net_dev_by_params() with NULL client_data - which is confusing and
    unexpected.
    
    If the add() callback fails, then do not call any more client ops for the
    device, even remove.
    
    Remove all the now redundant checks for NULL client_data in ops callbacks.
    
    Update all the add() callbacks to return error codes
    appropriately. EOPNOTSUPP is used for cases where the ULP does not support
    the ib_device - eg because it only works with IB.
    
    Link: https://lore.kernel.org/r/20200421172440.387069-1-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Ursula Braun <ubraun@linux.ibm.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8d29f2f79da8..c3d715e2fc66 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2722,7 +2722,7 @@ struct ib_device {
 struct ib_client_nl_info;
 struct ib_client {
 	const char *name;
-	void (*add)   (struct ib_device *);
+	int (*add)(struct ib_device *ibdev);
 	void (*remove)(struct ib_device *, void *client_data);
 	void (*rename)(struct ib_device *dev, void *client_data);
 	int (*get_nl_info)(struct ib_device *ibdev, void *client_data,

commit 51aab12631dd7700385d275846ca49dc0b8c2124
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Apr 30 22:21:44 2020 +0300

    RDMA/core: Get xmit slave for LAG
    
    Add a call to rdma_lag_get_ah_roce_slave() when the address handle is
    created. Lower driver can use it to select the QP's affinity port.
    
    Link: https://lore.kernel.org/r/20200430192146.12863-15-maorg@mellanox.com
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e6c18ec0365a..8d29f2f79da8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -883,6 +883,7 @@ __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 struct rdma_ah_init_attr {
 	struct rdma_ah_attr *ah_attr;
 	u32 flags;
+	struct net_device *xmit_slave;
 };
 
 enum rdma_ah_attr_type {
@@ -1272,6 +1273,7 @@ struct ib_qp_attr {
 	u8			alt_port_num;
 	u8			alt_timeout;
 	u32			rate_limit;
+	struct net_device	*xmit_slave;
 };
 
 enum ib_wr_opcode {

commit bd3920eac133103f0d4aa5fc62290e6df9a6c6da
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Apr 30 22:21:43 2020 +0300

    RDMA/core: Add LAG functionality
    
    Add support to get the RoCE LAG xmit slave by building skb of the RoCE
    packet and call to master_get_xmit_slave.  If driver wants to get the
    slave assume all slaves are available, then need to set
    RDMA_LAG_FLAGS_HASH_ALL_SLAVES in flags.
    
    Link: https://lore.kernel.org/r/20200430192146.12863-14-maorg@mellanox.com
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 20ea26810349..e6c18ec0365a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2714,6 +2714,7 @@ struct ib_device {
 	/* Used by iWarp CM */
 	char iw_ifname[IFNAMSIZ];
 	u32 iw_driver_flags;
+	u32 lag_flags;
 };
 
 struct ib_client_nl_info;

commit fa5d010c5630b143b802e0477e87bba0656829cf
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Apr 30 22:21:42 2020 +0300

    RDMA: Group create AH arguments in struct
    
    Following patch adds additional argument to the create AH function, so it
    make sense to group ah_attr and flags arguments in struct.
    
    Link: https://lore.kernel.org/r/20200430192146.12863-13-maorg@mellanox.com
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Acked-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Acked-by: Weihang Li <liweihang@huawei.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bbc5cfb57cd2..20ea26810349 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -880,6 +880,11 @@ struct ib_mr_status {
  */
 __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 
+struct rdma_ah_init_attr {
+	struct rdma_ah_attr *ah_attr;
+	u32 flags;
+};
+
 enum rdma_ah_attr_type {
 	RDMA_AH_ATTR_TYPE_UNDEFINED,
 	RDMA_AH_ATTR_TYPE_IB,
@@ -2403,8 +2408,8 @@ struct ib_device_ops {
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	int (*alloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	void (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
-	int (*create_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr,
-			 u32 flags, struct ib_udata *udata);
+	int (*create_ah)(struct ib_ah *ah, struct rdma_ah_init_attr *attr,
+			 struct ib_udata *udata);
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	void (*destroy_ah)(struct ib_ah *ah, u32 flags);

commit 5b361328ca649534d721e4eae20c96ccbe702ce7
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Wed Feb 12 19:04:25 2020 -0600

    RDMA: Replace zero-length array with flexible-array member
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Link: https://lore.kernel.org/r/20200213010425.GA13068@embeddedor.com
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com> # added a few more

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5f3a04ead9f5..bbc5cfb57cd2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1876,7 +1876,7 @@ struct ib_flow_eth_filter {
 	__be16	ether_type;
 	__be16	vlan_tag;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_eth {
@@ -1890,7 +1890,7 @@ struct ib_flow_ib_filter {
 	__be16 dlid;
 	__u8   sl;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_ib {
@@ -1915,7 +1915,7 @@ struct ib_flow_ipv4_filter {
 	u8	ttl;
 	u8	flags;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_ipv4 {
@@ -1933,7 +1933,7 @@ struct ib_flow_ipv6_filter {
 	u8	traffic_class;
 	u8	hop_limit;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_ipv6 {
@@ -1947,7 +1947,7 @@ struct ib_flow_tcp_udp_filter {
 	__be16	dst_port;
 	__be16	src_port;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_tcp_udp {
@@ -1959,7 +1959,7 @@ struct ib_flow_spec_tcp_udp {
 
 struct ib_flow_tunnel_filter {
 	__be32	tunnel_id;
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 /* ib_flow_spec_tunnel describes the Vxlan tunnel
@@ -1976,7 +1976,7 @@ struct ib_flow_esp_filter {
 	__be32	spi;
 	__be32  seq;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_esp {
@@ -1991,7 +1991,7 @@ struct ib_flow_gre_filter {
 	__be16 protocol;
 	__be32 key;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_gre {
@@ -2004,7 +2004,7 @@ struct ib_flow_spec_gre {
 struct ib_flow_mpls_filter {
 	__be32 tag;
 	/* Must be last */
-	u8	real_sz[0];
+	u8	real_sz[];
 };
 
 struct ib_flow_spec_mpls {

commit b72bfc965eb5d3475acabb038a1f9f6034c4658d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Feb 13 15:19:11 2020 -0400

    RDMA/core: Get rid of ib_create_qp_user
    
    This function accepts a udata but does nothing with it, and is never
    passed a !NULL udata. Rename it to ib_create_qp which was the only caller
    and remove the udata.
    
    Link: https://lore.kernel.org/r/20200213191911.GA9898@ziepe.ca
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1f779fad3a1e..5f3a04ead9f5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3627,35 +3627,8 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
 					      bad_recv_wr ? : &dummy);
 }
 
-/**
- * ib_create_qp_user - Creates a QP associated with the specified protection
- *   domain.
- * @pd: The protection domain associated with the QP.
- * @qp_init_attr: A list of initial attributes required to create the
- *   QP.  If QP creation succeeds, then the attributes are updated to
- *   the actual capabilities of the created QP.
- * @udata: Valid user data or NULL for kernel objects
- */
-struct ib_qp *ib_create_qp_user(struct ib_pd *pd,
-				struct ib_qp_init_attr *qp_init_attr,
-				struct ib_udata *udata);
-
-/**
- * ib_create_qp - Creates a kernel QP associated with the specified protection
- *   domain.
- * @pd: The protection domain associated with the QP.
- * @qp_init_attr: A list of initial attributes required to create the
- *   QP.  If QP creation succeeds, then the attributes are updated to
- *   the actual capabilities of the created QP.
- * @udata: Valid user data or NULL for kernel objects
- *
- * NOTE: for user qp use ib_create_qp_user with valid udata!
- */
-static inline struct ib_qp *ib_create_qp(struct ib_pd *pd,
-					 struct ib_qp_init_attr *qp_init_attr)
-{
-	return ib_create_qp_user(pd, qp_init_attr, NULL);
-}
+struct ib_qp *ib_create_qp(struct ib_pd *pd,
+			   struct ib_qp_init_attr *qp_init_attr);
 
 /**
  * ib_modify_qp_with_udata - Modifies the attributes for the specified QP.

commit e8b3a426fb4a9e2856a69b6e19de044c7416c316
Merge: eaad647e5cc2 b2dfc6765e45
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jan 21 09:55:04 2020 -0400

    Merge tag 'rds-odp-for-5.5' into rdma.git for-next
    
    From https://git.kernel.org/pub/scm/linux/kernel/git/leon/linux-rdma
    
    Leon Romanovsky says:
    
    ====================
    Use ODP MRs for kernel ULPs
    
    The following series extends MR creation routines to allow creation of
    user MRs through kernel ULPs as a proxy. The immediate use case is to
    allow RDS to work over FS-DAX, which requires ODP (on-demand-paging)
    MRs to be created and such MRs were not possible to create prior this
    series.
    
    The first part of this patchset extends RDMA to have special verb
    ib_reg_user_mr(). The common use case that uses this function is a
    userspace application that allocates memory for HCA access but the
    responsibility to register the memory at the HCA is on an kernel ULP.
    This ULP acts as an agent for the userspace application.
    
    The second part provides advise MR functionality for ULPs. This is
    integral part of ODP flows and used to trigger pagefaults in advance
    to prepare memory before running working set.
    
    The third part is actual user of those in-kernel APIs.
    ====================
    
    * tag 'rds-odp-for-5.5':
      net/rds: Use prefetch for On-Demand-Paging MR
      net/rds: Handle ODP mr registration/unregistration
      net/rds: Detect need of On-Demand-Paging memory registration
      RDMA/mlx5: Fix handling of IOVA != user_va in ODP paths
      IB/mlx5: Mask out unsupported ODP capabilities for kernel QPs
      RDMA/mlx5: Don't fake udata for kernel path
      IB/mlx5: Add ODP WQE handlers for kernel QPs
      IB/core: Add interface to advise_mr for kernel users
      IB/core: Introduce ib_reg_user_mr
      IB: Allow calls to ib_umem_get from kernel ULPs
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 2233c6609c11146ed1a26eec2e4335131077a608
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Wed Jan 8 20:05:38 2020 +0200

    RDMA/uverbs: Add new relaxed ordering memory region access flag
    
    Add a new relaxed ordering access flag for memory regions.  Using memory
    regions with relaxed ordeing set can enhance performance.
    
    This access flag is handled in a best-effort manner, drivers should ignore
    if they don't support setting relaxed ordering.
    
    Link: https://lore.kernel.org/r/1578506740-22188-9-git-send-email-yishaih@mellanox.com
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ed6cf11612b3..6506df9f31ae 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1418,6 +1418,7 @@ enum ib_access_flags {
 	IB_ZERO_BASED = IB_UVERBS_ACCESS_ZERO_BASED,
 	IB_ACCESS_ON_DEMAND = IB_UVERBS_ACCESS_ON_DEMAND,
 	IB_ACCESS_HUGETLB = IB_UVERBS_ACCESS_HUGETLB,
+	IB_ACCESS_RELAXED_ORDERING = IB_UVERBS_ACCESS_RELAXED_ORDERING,
 
 	IB_ACCESS_OPTIONAL = IB_UVERBS_ACCESS_OPTIONAL_RANGE,
 	IB_ACCESS_SUPPORTED =

commit 68d384b906cfc850b65561fd846adbb8b406d9e5
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Wed Jan 8 20:05:36 2020 +0200

    RDMA/core: Add optional access flags range
    
    Define a range of access flags that are defined to be optional, both
    uverbs and drivers should enable getting them and use if they are
    applicable
    
    This will be used, for example, for the relaxed ordering access flag which
    unsupporting drivers can ignore.
    
    Link: https://lore.kernel.org/r/1578506740-22188-7-git-send-email-yishaih@mellanox.com
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b20b89e93c62..ed6cf11612b3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1419,7 +1419,9 @@ enum ib_access_flags {
 	IB_ACCESS_ON_DEMAND = IB_UVERBS_ACCESS_ON_DEMAND,
 	IB_ACCESS_HUGETLB = IB_UVERBS_ACCESS_HUGETLB,
 
-	IB_ACCESS_SUPPORTED = ((IB_ACCESS_HUGETLB << 1) - 1)
+	IB_ACCESS_OPTIONAL = IB_UVERBS_ACCESS_OPTIONAL_RANGE,
+	IB_ACCESS_SUPPORTED =
+		((IB_ACCESS_HUGETLB << 1) - 1) | IB_ACCESS_OPTIONAL,
 };
 
 /*

commit ca95c1411198c2d87217c19d44571052cdc94725
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Wed Jan 8 20:05:35 2020 +0200

    RDMA/uverbs: Verify MR access flags
    
    Verify that MR access flags that are passed from user are all supported
    ones, otherwise an error is returned.
    
    Fixes: 4fca03778351 ("IB/uverbs: Move ib_access_flags and ib_read_counters_flags to uapi")
    Link: https://lore.kernel.org/r/1578506740-22188-6-git-send-email-yishaih@mellanox.com
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d8031f6f327e..b20b89e93c62 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4310,6 +4310,9 @@ static inline int ib_check_mr_access(int flags)
 	    !(flags & IB_ACCESS_LOCAL_WRITE))
 		return -EINVAL;
 
+	if (flags & ~IB_ACCESS_SUPPORTED)
+		return -EINVAL;
+
 	return 0;
 }
 

commit 87d8069f6b028793254ddd0a66df1d7b6d79b450
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Jan 15 14:43:33 2020 +0200

    IB/core: Add interface to advise_mr for kernel users
    
    Allow ULPs to call advise_mr, so they can control ODP regions
    in the same way as user space applications.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 170d5ec95b79..e2cc62217cc2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4159,6 +4159,9 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 struct ib_mr *ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 			     u64 virt_addr, int mr_access_flags);
 
+/* ib_advise_mr -  give an advice about an address range in a memory region */
+int ib_advise_mr(struct ib_pd *pd, enum ib_uverbs_advise_mr_advice advice,
+		 u32 flags, struct ib_sge *sg_list, u32 num_sge);
 /**
  * ib_dereg_mr_user - Deregisters a memory region and removes it from the
  *   HCA translation table.

commit 33006bd4f37f7d2c3d1cf0268b4f327b5fdc2558
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Jan 15 14:43:32 2020 +0200

    IB/core: Introduce ib_reg_user_mr
    
    Add ib_reg_user_mr() for kernel ULPs to register user MRs.
    
    The common use case that uses this function is a userspace application
    that allocates memory for HCA access but the responsibility to register
    the memory at the HCA is on an kernel ULP. This ULP that acts as an agent
    for the userspace application.
    
    This function is intended to be used without a user context so vendor
    drivers need to be aware of calling reg_user_mr() device operation with
    udata equal to NULL.
    
    Among all drivers, i40iw is the only driver which relies on presence
    of udata, so check udata existence for that driver.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5608e14e3aad..170d5ec95b79 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4153,6 +4153,12 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 	dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 
+/* ib_reg_user_mr - register a memory region for virtual addresses from kernel
+ * space. This function should be called when 'current' is the owning MM.
+ */
+struct ib_mr *ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+			     u64 virt_addr, int mr_access_flags);
+
 /**
  * ib_dereg_mr_user - Deregisters a memory region and removes it from the
  *   HCA translation table.

commit e04dd13159b0ddc0ff7f5e110bf99af3c65fabd3
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 8 19:22:01 2020 +0200

    RDMA/core: Do not erase the type of ib_wq.uobject
    
    This is a struct ib_uwq_object pointer, instead of using container_of()
    all over the place just store it with its actual type.
    
    Link: https://lore.kernel.org/r/1578504126-9400-10-git-send-email-yishaih@mellanox.com
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7990b55b1b40..d8031f6f327e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -74,6 +74,7 @@
 struct ib_umem_odp;
 struct ib_uqp_object;
 struct ib_usrq_object;
+struct ib_uwq_object;
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
@@ -1621,7 +1622,7 @@ enum ib_wq_state {
 
 struct ib_wq {
 	struct ib_device       *device;
-	struct ib_uobject      *uobject;
+	struct ib_uwq_object   *uobject;
 	void		    *wq_context;
 	void		    (*event_handler)(struct ib_event *, void *);
 	struct ib_pd	       *pd;

commit 9fbe334c6a67c3c09f187e4b9b0e6eaf0ad31429
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 8 19:22:00 2020 +0200

    RDMA/core: Do not erase the type of ib_srq.uobject
    
    This is a struct ib_usrq_object pointer, instead of using container_of()
    all over the place just store it with its actual type.
    
    Link: https://lore.kernel.org/r/1578504126-9400-9-git-send-email-yishaih@mellanox.com
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b4dbc5f9636a..7990b55b1b40 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -73,6 +73,7 @@
 
 struct ib_umem_odp;
 struct ib_uqp_object;
+struct ib_usrq_object;
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
@@ -1575,7 +1576,7 @@ struct ib_cq {
 struct ib_srq {
 	struct ib_device       *device;
 	struct ib_pd	       *pd;
-	struct ib_uobject      *uobject;
+	struct ib_usrq_object  *uobject;
 	void		      (*event_handler)(struct ib_event *, void *);
 	void		       *srq_context;
 	enum ib_srq_type	srq_type;

commit 620d3f8176cbb3a9c0c7962a05fb15310a9998d4
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 8 19:21:59 2020 +0200

    RDMA/core: Do not erase the type of ib_qp.uobject
    
    This is a struct ib_uqp_object pointer, instead of using container_of()
    all over the place just store it with its actual type.
    
    Link: https://lore.kernel.org/r/1578504126-9400-8-git-send-email-yishaih@mellanox.com
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e9ab986ab323..b4dbc5f9636a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -72,6 +72,7 @@
 #define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
 
 struct ib_umem_odp;
+struct ib_uqp_object;
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
@@ -1735,7 +1736,7 @@ struct ib_qp {
 	atomic_t		usecnt;
 	struct list_head	open_list;
 	struct ib_qp           *real_qp;
-	struct ib_uobject      *uobject;
+	struct ib_uqp_object   *uobject;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
 	/* sgid_attrs associated with the AV's */

commit 5bd48c18c8cea0154800b40ec75201fa71684312
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 8 19:21:58 2020 +0200

    RDMA/core: Do not erase the type of ib_cq.uobject
    
    This is a struct ib_ucq_object pointer, instead of using container_of()
    all over the place just store it with its actual type.
    
    Link: https://lore.kernel.org/r/1578504126-9400-7-git-send-email-yishaih@mellanox.com
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cea4e198701e..e9ab986ab323 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -77,6 +77,8 @@ extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
 extern struct workqueue_struct *ib_comp_unbound_wq;
 
+struct ib_ucq_object;
+
 __printf(3, 4) __cold
 void ibdev_printk(const char *level, const struct ib_device *ibdev,
 		  const char *format, ...);
@@ -1544,7 +1546,7 @@ enum ib_poll_context {
 
 struct ib_cq {
 	struct ib_device       *device;
-	struct ib_uobject      *uobject;
+	struct ib_ucq_object   *uobject;
 	ib_comp_handler   	comp_handler;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void                   *cq_context;

commit 40adf686128856c4add948fb002d43e2c507d1aa
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Dec 12 13:30:24 2019 +0200

    IB/core: Rename event_handler_lock to qp_open_list_lock
    
    This lock is used to protect the qp->open_list linked list. As a side
    effect it seems to also globally serialize the qp event_handler, but it
    isn't clear if that is a deliberate design.
    
    Link: https://lore.kernel.org/r/20191212113024.336702-5-leon@kernel.org
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 37dac147a946..cea4e198701e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2631,7 +2631,7 @@ struct ib_device {
 	struct rw_semaphore event_handler_rwsem;
 
 	/* Protects QP's event_handler calls and open_qp list */
-	spinlock_t event_handler_lock;
+	spinlock_t qp_open_list_lock;
 
 	struct rw_semaphore	      client_data_rwsem;
 	struct xarray                 client_data;

commit 17e1064632512db419cb9bb4555aec1763969b7d
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Dec 12 13:30:23 2019 +0200

    IB/core: Cut down single member ib_cache structure
    
    Given that ib_cache structure has only single member now, merge the cache
    lock directly in the ib_device.
    
    Link: https://lore.kernel.org/r/20191212113024.336702-4-leon@kernel.org
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f36fb657518f..37dac147a946 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2152,10 +2152,6 @@ struct ib_port_cache {
 	enum ib_port_state     port_state;
 };
 
-struct ib_cache {
-	rwlock_t                lock;
-};
-
 struct ib_port_immutable {
 	int                           pkey_tbl_len;
 	int                           gid_tbl_len;
@@ -2641,7 +2637,8 @@ struct ib_device {
 	struct xarray                 client_data;
 	struct mutex                  unregistration_lock;
 
-	struct ib_cache               cache;
+	/* Synchronize GID, Pkey cache entries, subnet prefix, LMC */
+	rwlock_t cache_lock;
 	/**
 	 * port_data is indexed by port number
 	 */

commit 6b57cea9221b0247ad5111b348522625e489a8e4
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Dec 12 13:30:22 2019 +0200

    IB/core: Let IB core distribute cache update events
    
    Currently when the low level driver notifies Pkey, GID, and port change
    events they are notified to the registered handlers in the order they are
    registered.
    
    IB core and other ULPs such as IPoIB are interested in GID, LID, Pkey
    change events.
    
    Since all GID queries done by ULPs are serviced by IB core, and the IB
    core deferes cache updates to a work queue, it is possible for other
    clients to see stale cache data when they handle their own events.
    
    For example, the below call tree shows how ipoib will call
    rdma_query_gid() concurrently with the update to the cache sitting in the
    WQ.
    
    mlx5_ib_handle_event()
      ib_dispatch_event()
        ib_cache_event()
           queue_work() -> slow cache update
    
        [..]
        ipoib_event()
         queue_work()
           [..]
           work handler
             ipoib_ib_dev_flush_light()
               __ipoib_ib_dev_flush()
                  ipoib_dev_addr_changed_valid()
                    rdma_query_gid() <- Returns old GID, cache not updated.
    
    Move all the event dispatch to a work queue so that the cache update is
    always done before any clients are notified.
    
    Fixes: f35faa4ba956 ("IB/core: Simplify ib_query_gid to always refer to cache")
    Link: https://lore.kernel.org/r/20191212113024.336702-3-leon@kernel.org
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 42f28d39f28c..f36fb657518f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2154,7 +2154,6 @@ struct ib_port_cache {
 
 struct ib_cache {
 	rwlock_t                lock;
-	struct ib_event_handler event_handler;
 };
 
 struct ib_port_immutable {
@@ -2632,7 +2631,11 @@ struct ib_device {
 	struct rcu_head rcu_head;
 
 	struct list_head              event_handler_list;
-	spinlock_t                    event_handler_lock;
+	/* Protects event_handler_list */
+	struct rw_semaphore event_handler_rwsem;
+
+	/* Protects QP's event_handler calls and open_qp list */
+	spinlock_t event_handler_lock;
 
 	struct rw_semaphore	      client_data_rwsem;
 	struct xarray                 client_data;
@@ -2947,7 +2950,7 @@ bool ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
 
 void ib_register_event_handler(struct ib_event_handler *event_handler);
 void ib_unregister_event_handler(struct ib_event_handler *event_handler);
-void ib_dispatch_event(struct ib_event *event);
+void ib_dispatch_event(const struct ib_event *event);
 
 int ib_query_port(struct ib_device *device,
 		  u8 port_num, struct ib_port_attr *port_attr);

commit 3e5901cbfcc15da54f6ad148add754e7a2b2a558
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 18 15:18:15 2019 -0500

    RDMA/core: Trace points for diagnosing completion queue issues
    
    Sample trace events:
    
       kworker/u29:0-300   [007]   120.042217: cq_alloc:             cq.id=4 nr_cqe=161 comp_vector=2 poll_ctx=WORKQUEUE
              <idle>-0     [002]   120.056292: cq_schedule:          cq.id=4
        kworker/2:1H-482   [002]   120.056402: cq_process:           cq.id=4 wake-up took 109 [us] from interrupt
        kworker/2:1H-482   [002]   120.056407: cq_poll:              cq.id=4 requested 16, returned 1
              <idle>-0     [002]   120.067503: cq_schedule:          cq.id=4
        kworker/2:1H-482   [002]   120.067537: cq_process:           cq.id=4 wake-up took 34 [us] from interrupt
        kworker/2:1H-482   [002]   120.067541: cq_poll:              cq.id=4 requested 16, returned 1
              <idle>-0     [002]   120.067657: cq_schedule:          cq.id=4
        kworker/2:1H-482   [002]   120.067672: cq_process:           cq.id=4 wake-up took 15 [us] from interrupt
        kworker/2:1H-482   [002]   120.067674: cq_poll:              cq.id=4 requested 16, returned 1
    
     ...
    
             systemd-1     [002]   122.392653: cq_schedule:          cq.id=4
        kworker/2:1H-482   [002]   122.392688: cq_process:           cq.id=4 wake-up took 35 [us] from interrupt
        kworker/2:1H-482   [002]   122.392693: cq_poll:              cq.id=4 requested 16, returned 16
        kworker/2:1H-482   [002]   122.392836: cq_poll:              cq.id=4 requested 16, returned 16
        kworker/2:1H-482   [002]   122.392970: cq_poll:              cq.id=4 requested 16, returned 16
        kworker/2:1H-482   [002]   122.393083: cq_poll:              cq.id=4 requested 16, returned 16
        kworker/2:1H-482   [002]   122.393195: cq_poll:              cq.id=4 requested 16, returned 3
    
    Several features to note in this output:
     - The WCE count and context type are reported at allocation time
     - The CPU and kworker for each CQ is evident
     - The CQ's restracker ID is tagged on each trace event
     - CQ poll scheduling latency is measured
     - Details about how often single completions occur versus multiple
       completions are evident
     - The cost of the ULP's completion handler is recorded
    
    Link: https://lore.kernel.org/r/20191218201815.30584.3481.stgit@manet.1015granger.net
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5608e14e3aad..42f28d39f28c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1558,6 +1558,11 @@ struct ib_cq {
 	};
 	struct workqueue_struct *comp_wq;
 	struct dim *dim;
+
+	/* updated only by trace points */
+	ktime_t timestamp;
+	bool interrupt;
+
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */

commit 7a763d18ff2a75cfd1014800327f4120840bef09
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Dec 12 12:02:36 2019 +0200

    IB/core: Introduce rdma_user_mmap_entry_insert_range() API
    
    Introduce rdma_user_mmap_entry_insert_range() API to be used once the
    required key for the given entry should be in a given range.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20191212100237.330654-2-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cacb48faf670..5608e14e3aad 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2832,6 +2832,11 @@ int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
 int rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,
 				struct rdma_user_mmap_entry *entry,
 				size_t length);
+int rdma_user_mmap_entry_insert_range(struct ib_ucontext *ucontext,
+				      struct rdma_user_mmap_entry *entry,
+				      size_t length, u32 min_pgoff,
+				      u32 max_pgoff);
+
 struct rdma_user_mmap_entry *
 rdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,
 			       unsigned long pgoff);

commit aa32f1169148beb90d71494e2f2a1999ba7b5366
Merge: d5bb349dbbe2 93f4e735b6d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 10:33:14 2019 -0800

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull hmm updates from Jason Gunthorpe:
     "This is another round of bug fixing and cleanup. This time the focus
      is on the driver pattern to use mmu notifiers to monitor a VA range.
      This code is lifted out of many drivers and hmm_mirror directly into
      the mmu_notifier core and written using the best ideas from all the
      driver implementations.
    
      This removes many bugs from the drivers and has a very pleasing
      diffstat. More drivers can still be converted, but that is for another
      cycle.
    
       - A shared branch with RDMA reworking the RDMA ODP implementation
    
       - New mmu_interval_notifier API. This is focused on the use case of
         monitoring a VA and simplifies the process for drivers
    
       - A common seq-count locking scheme built into the
         mmu_interval_notifier API usable by drivers that call
         get_user_pages() or hmm_range_fault() with the VA range
    
       - Conversion of mlx5 ODP, hfi1, radeon, nouveau, AMD GPU, and Xen
         GntDev drivers to the new API. This deletes a lot of wonky driver
         code.
    
       - Two improvements for hmm_range_fault(), from testing done by Ralph"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:
      mm/hmm: remove hmm_range_dma_map and hmm_range_dma_unmap
      mm/hmm: make full use of walk_page_range()
      xen/gntdev: use mmu_interval_notifier_insert
      mm/hmm: remove hmm_mirror and related
      drm/amdgpu: Use mmu_interval_notifier instead of hmm_mirror
      drm/amdgpu: Use mmu_interval_insert instead of hmm_mirror
      drm/amdgpu: Call find_vma under mmap_sem
      nouveau: use mmu_interval_notifier instead of hmm_mirror
      nouveau: use mmu_notifier directly for invalidate_range_start
      drm/radeon: use mmu_interval_notifier_insert
      RDMA/hfi1: Use mmu_interval_notifier_insert for user_exp_rcv
      RDMA/odp: Use mmu_interval_notifier_insert()
      mm/hmm: define the pre-processor related parts of hmm.h even if disabled
      mm/hmm: allow hmm_range to be used with a mmu_interval_notifier or hmm_mirror
      mm/mmu_notifier: add an interval tree notifier
      mm/mmu_notifier: define the header pre-processor parts even if disabled
      mm/hmm: allow snapshot of the special zero page

commit d76886972823ce456c0c61cd2284e85668e2131e
Merge: 0e45384ceccc f295e4cece5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 27 10:17:28 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "Again another fairly quiet cycle with few notable core code changes
      and the usual variety of driver bug fixes and small improvements.
    
       - Various driver updates and bug fixes for siw, bnxt_re, hns, qedr,
         iw_cxgb4, vmw_pvrdma, mlx5
    
       - Improvements in SRPT from working with iWarp
    
       - SRIOV VF support for bnxt_re
    
       - Skeleton kernel-doc files for drivers/infiniband
    
       - User visible counters for events related to ODP
    
       - Common code for tracking of mmap lifetimes so that drivers can link
         HW object liftime to a VMA
    
       - ODP bug fixes and rework
    
       - RDMA READ support for efa
    
       - Removal of the very old cxgb3 driver"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (168 commits)
      RDMA/hns: Delete unnecessary callback functions for cq
      RDMA/hns: Rename the functions used inside creating cq
      RDMA/hns: Redefine the member of hns_roce_cq struct
      RDMA/hns: Redefine interfaces used in creating cq
      RDMA/efa: Expose RDMA read related attributes
      RDMA/efa: Support remote read access in MR registration
      RDMA/efa: Store network attributes in device attributes
      IB/hfi1: remove redundant assignment to variable ret
      RDMA/bnxt_re: Fix missing le16_to_cpu
      RDMA/bnxt_re: Fix stat push into dma buffer on gen p5 devices
      RDMA/bnxt_re: Fix chip number validation Broadcom's Gen P5 series
      RDMA/bnxt_re: Fix Kconfig indentation
      IB/mlx5: Implement callbacks for getting VFs GUID attributes
      IB/ipoib: Add ndo operation for getting VFs GUID attributes
      IB/core: Add interfaces to get VF node and port GUIDs
      net/core: Add support for getting VF GUIDs
      RDMA/qedr: Fix null-pointer dereference when calling rdma_user_mmap_get_offset
      RDMA/cm: Use refcount_t type for refcount variable
      IB/mlx5: Support extended number of strides for Striding RQ
      IB/mlx4: Update HW GID table while adding vlan GID
      ...

commit 3694e41e41517994664518ece6265f0bc04a840d
Merge: a25984f3baaa 9c0015ef0928
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Nov 22 16:08:34 2019 -0400

    Merge branch 'ib-guids' into rdma.git for-next
    
    Danit Goldberg says:
    
    ====================
    This series extends RTNETLINK to provide IB port and node GUIDs, which
    were configured for Infiniband VFs.
    
    The functionality to set VF GUIDs already existed for a long time, and
    here we are adding the missing "get" so that netlink will be symmetric and
    various cloud orchestration tools will be able to manage such VFs more
    naturally.
    
    The iproute2 was extended too to present those GUIDs.
    
    - ip link show <device>
    
    For example:
    - ip link set ib4 vf 0 node_guid 22:44:33:00:33:11:00:33
    - ip link set ib4 vf 0 port_guid 10:21:33:12:00:11:22:10
    - ip link show ib4
        ib4: <BROADCAST,MULTICAST> mtu 4092 qdisc noop state DOWN mode DEFAULT group default qlen 256
        link/infiniband 00:00:0a:2d:fe:80:00:00:00:00:00:00:ec:0d:9a:03:00:44:36:8d brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
        vf 0     link/infiniband 00:00:0a:2d:fe:80:00:00:00:00:00:00:ec:0d:9a:03:00:44:36:8d brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff,
        spoof checking off, NODE_GUID 22:44:33:00:33:11:00:33, PORT_GUID 10:21:33:12:00:11:22:10, link-state disable, trust off, query_rss off
    ====================
    
    Based on the mlx5-next branch from
    git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux for
    dependencies
    
    * branch 'ib-guids': (35 commits)
      IB/mlx5: Implement callbacks for getting VFs GUID attributes
      IB/ipoib: Add ndo operation for getting VFs GUID attributes
      IB/core: Add interfaces to get VF node and port GUIDs
      net/core: Add support for getting VF GUIDs
    
      net/mlx5: Add new chain for netfilter flow table offload
      net/mlx5: Refactor creating fast path prio chains
      net/mlx5: Accumulate levels for chains prio namespaces
      net/mlx5: Define fdb tc levels per prio
      net/mlx5: Rename FDB_* tc related defines to FDB_TC_* defines
      net/mlx5: Simplify fdb chain and prio eswitch defines
      IB/mlx5: Load profile according to RoCE enablement state
      IB/mlx5: Rename profile and init methods
      net/mlx5: Handle "enable_roce" devlink param
      net/mlx5: Document flow_steering_mode devlink param
      devlink: Add new "enable_roce" generic device param
      net/mlx5: fix spelling mistake "metdata" -> "metadata"
      net/mlx5: fix kvfree of uninitialized pointer spec
      IB/mlx5: Introduce and use mlx5_core_is_vf()
      net/mlx5: E-switch, Enable metadata on own vport
      net/mlx5: Refactor ingress acl configuration
      ...
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit f25a546e65292b36f15cca0912450c4944fae031
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Nov 12 16:22:22 2019 -0400

    RDMA/odp: Use mmu_interval_notifier_insert()
    
    Replace the internal interval tree based mmu notifier with the new common
    mmu_interval_notifier_insert() API. This removes a lot of code and fixes a
    deadlock that can be triggered in ODP:
    
     zap_page_range()
      mmu_notifier_invalidate_range_start()
       [..]
        ib_umem_notifier_invalidate_range_start()
           down_read(&per_mm->umem_rwsem)
      unmap_single_vma()
        [..]
          __split_huge_page_pmd()
            mmu_notifier_invalidate_range_start()
            [..]
               ib_umem_notifier_invalidate_range_start()
                  down_read(&per_mm->umem_rwsem)   // DEADLOCK
    
            mmu_notifier_invalidate_range_end()
               up_read(&per_mm->umem_rwsem)
      mmu_notifier_invalidate_range_end()
         up_read(&per_mm->umem_rwsem)
    
    The umem_rwsem is held across the range_start/end as the ODP algorithm for
    invalidate_range_end cannot tolerate changes to the interval
    tree. However, due to the nested invalidation regions the second
    down_read() can deadlock if there are competing writers. The new core code
    provides an alternative scheme to solve this problem.
    
    Fixes: ca748c39ea3f ("RDMA/umem: Get rid of per_mm->notifier_count")
    Link: https://lore.kernel.org/r/20191112202231.3856-6-jgg@ziepe.ca
    Tested-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6a47ba85c54c..2c30c859ae0d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2422,8 +2422,6 @@ struct ib_device_ops {
 			    u64 iova);
 	int (*unmap_fmr)(struct list_head *fmr_list);
 	int (*dealloc_fmr)(struct ib_fmr *fmr);
-	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
-				 unsigned long start, unsigned long end);
 	int (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	int (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,

commit bfcb3c5d14854f001881dc3f5cc29bf186598d9f
Author: Danit Goldberg <danitg@mellanox.com>
Date:   Wed Nov 6 15:08:32 2019 +0200

    IB/core: Add interfaces to get VF node and port GUIDs
    
    Provide ability to get node and port GUIDs of VFs to be symmetrical
    to already existing set option.
    
    Signed-off-by: Danit Goldberg <danitg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6a47ba85c54c..ec7d1a1f8f31 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2448,6 +2448,9 @@ struct ib_device_ops {
 			     struct ifla_vf_info *ivf);
 	int (*get_vf_stats)(struct ib_device *device, int vf, u8 port,
 			    struct ifla_vf_stats *stats);
+	int (*get_vf_guid)(struct ib_device *device, int vf, u8 port,
+			    struct ifla_vf_guid *node_guid,
+			    struct ifla_vf_guid *port_guid);
 	int (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
 			   int type);
 	struct ib_wq *(*create_wq)(struct ib_pd *pd,
@@ -3303,6 +3306,9 @@ int ib_get_vf_config(struct ib_device *device, int vf, u8 port,
 		     struct ifla_vf_info *info);
 int ib_get_vf_stats(struct ib_device *device, int vf, u8 port,
 		    struct ifla_vf_stats *stats);
+int ib_get_vf_guid(struct ib_device *device, int vf, u8 port,
+		    struct ifla_vf_guid *node_guid,
+		    struct ifla_vf_guid *port_guid);
 int ib_set_vf_guid(struct ib_device *device, int vf, u8 port, u64 guid,
 		   int type);
 

commit e26e7b88f6b7482cbff633c6fc9eaee3ecbd41b1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Oct 29 08:27:45 2019 +0200

    RDMA: Change MAD processing function to remove extra casting and parameter
    
    All users of process_mad() converts input pointers from ib_mad_hdr to be
    ib_mad, update the function declaration to use ib_mad directly.
    
    Also remove not used input MAD size parameter.
    
    Link: https://lore.kernel.org/r/20191029062745.7932-17-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Tested-By: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 416e72ea80d9..663fa16caa76 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2123,7 +2123,7 @@ struct ib_flow_action {
 	atomic_t			usecnt;
 };
 
-struct ib_mad_hdr;
+struct ib_mad;
 struct ib_grh;
 
 enum ib_process_mad_flags {
@@ -2301,9 +2301,8 @@ struct ib_device_ops {
 	int (*process_mad)(struct ib_device *device, int process_mad_flags,
 			   u8 port_num, const struct ib_wc *in_wc,
 			   const struct ib_grh *in_grh,
-			   const struct ib_mad_hdr *in_mad, size_t in_mad_size,
-			   struct ib_mad_hdr *out_mad, size_t *out_mad_size,
-			   u16 *out_mad_pkey_index);
+			   const struct ib_mad *in_mad, struct ib_mad *out_mad,
+			   size_t *out_mad_size, u16 *out_mad_pkey_index);
 	int (*query_device)(struct ib_device *device,
 			    struct ib_device_attr *device_attr,
 			    struct ib_udata *udata);

commit c043ff2cfb7f6fdd9a1cb1a7ba3800f19b70bf65
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Wed Oct 30 11:44:12 2019 +0200

    RDMA: Connect between the mmap entry and the umap_priv structure
    
    The rdma_user_mmap_io interface created a common interface for drivers to
    correctly map hw resources and zap them once the ucontext is destroyed
    enabling the drivers to safely free the hw resources.
    
    However, this meant the drivers need to delay freeing the resource to the
    ucontext destroy phase to ensure they were no longer mapped.  The new
    mechanism for a common way of handling user/driver address mapping enabled
    notifying the driver if all umap_priv mappings were removed, and enabled
    freeing the hw resources when they are done with and not delay it until
    ucontext destroy.
    
    Since not all drivers use the mechanism, NULL can be sent to the
    rdma_user_mmap_io interface to continue working as before.  Drivers that
    use the mmap_xa interface can pass the entry being mapped to the
    rdma_user_mmap_io function to be linked together.
    
    Link: https://lore.kernel.org/r/20191030094417.16866-4-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8865ec28180a..416e72ea80d9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2826,18 +2826,9 @@ void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
 void ib_set_device_ops(struct ib_device *device,
 		       const struct ib_device_ops *ops);
 
-#if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
-		      unsigned long pfn, unsigned long size, pgprot_t prot);
-#else
-static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
-				    struct vm_area_struct *vma,
-				    unsigned long pfn, unsigned long size,
-				    pgprot_t prot)
-{
-	return -EINVAL;
-}
-#endif
+		      unsigned long pfn, unsigned long size, pgprot_t prot,
+		      struct rdma_user_mmap_entry *entry);
 int rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,
 				struct rdma_user_mmap_entry *entry,
 				size_t length);

commit 3411f9f01b76bd88aa6e0e013847ab6479cb4f24
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Wed Oct 30 11:44:11 2019 +0200

    RDMA/core: Create mmap database and cookie helper functions
    
    Create some common API's for adding entries to a xa_mmap. Searching for
    an entry and freeing one.
    
    The general approach is copied from the EFA driver and improved to be more
    general and do more to help the drivers. Integration with the core allows
    a reference counted scheme with a free function so that the driver can
    know when its mmaps are all gone.
    
    This significant new functionality will be helpful for drivers to have the
    correct lifetime model for mmap objects.
    
    Link: https://lore.kernel.org/r/20191030094417.16866-3-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0626b62ed107..8865ec28180a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1473,6 +1473,7 @@ struct ib_ucontext {
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
 	struct rdma_restrack_entry res;
+	struct xarray mmap_xa;
 };
 
 struct ib_uobject {
@@ -2258,6 +2259,21 @@ struct iw_cm_conn_param;
 
 #define DECLARE_RDMA_OBJ_SIZE(ib_struct) size_t size_##ib_struct
 
+struct rdma_user_mmap_entry {
+	struct kref ref;
+	struct ib_ucontext *ucontext;
+	unsigned long start_pgoff;
+	size_t npages;
+	bool driver_removed;
+};
+
+/* Return the offset (in bytes) the user should pass to libc's mmap() */
+static inline u64
+rdma_user_mmap_get_offset(const struct rdma_user_mmap_entry *entry)
+{
+	return (u64)entry->start_pgoff << PAGE_SHIFT;
+}
+
 /**
  * struct ib_device_ops - InfiniBand device operations
  * This structure defines all the InfiniBand device operations, providers will
@@ -2370,6 +2386,13 @@ struct ib_device_ops {
 			      struct ib_udata *udata);
 	void (*dealloc_ucontext)(struct ib_ucontext *context);
 	int (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);
+	/**
+	 * This will be called once refcount of an entry in mmap_xa reaches
+	 * zero. The type of the memory that was mapped may differ between
+	 * entries and is opaque to the rdma_user_mmap interface.
+	 * Therefore needs to be implemented by the driver in mmap_free.
+	 */
+	void (*mmap_free)(struct rdma_user_mmap_entry *entry);
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	int (*alloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	void (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
@@ -2815,6 +2838,18 @@ static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
 	return -EINVAL;
 }
 #endif
+int rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,
+				struct rdma_user_mmap_entry *entry,
+				size_t length);
+struct rdma_user_mmap_entry *
+rdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,
+			       unsigned long pgoff);
+struct rdma_user_mmap_entry *
+rdma_user_mmap_entry_get(struct ib_ucontext *ucontext,
+			 struct vm_area_struct *vma);
+void rdma_user_mmap_entry_put(struct rdma_user_mmap_entry *entry);
+
+void rdma_user_mmap_entry_remove(struct rdma_user_mmap_entry *entry);
 
 static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)
 {

commit ecdfdfdbe4d4c74029f2b416b7ee6d0aeb56364a
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Fri Oct 25 15:58:27 2019 -0700

    RDMA/core: Fix ib_dma_max_seg_size()
    
    If dev->dma_device->params == NULL then the maximum DMA segment size is 64
    KB. See also the dma_get_max_seg_size() implementation. This patch fixes
    the following kernel warning:
    
      DMA-API: infiniband rxe0: mapping sg segment longer than device claims to support [len=126976] [max=65536]
      WARNING: CPU: 4 PID: 4848 at kernel/dma/debug.c:1220 debug_dma_map_sg+0x3d9/0x450
      RIP: 0010:debug_dma_map_sg+0x3d9/0x450
      Call Trace:
       srp_queuecommand+0x626/0x18d0 [ib_srp]
       scsi_queue_rq+0xd02/0x13e0 [scsi_mod]
       __blk_mq_try_issue_directly+0x2b3/0x3f0
       blk_mq_request_issue_directly+0xac/0xf0
       blk_insert_cloned_request+0xdf/0x170
       dm_mq_queue_rq+0x43d/0x830 [dm_mod]
       __blk_mq_try_issue_directly+0x2b3/0x3f0
       blk_mq_request_issue_directly+0xac/0xf0
       blk_mq_try_issue_list_directly+0xb8/0x170
       blk_mq_sched_insert_requests+0x23c/0x3b0
       blk_mq_flush_plug_list+0x529/0x730
       blk_flush_plug_list+0x21f/0x260
       blk_mq_make_request+0x56b/0xf20
       generic_make_request+0x196/0x660
       submit_bio+0xae/0x290
       blkdev_direct_IO+0x822/0x900
       generic_file_direct_write+0x110/0x200
       __generic_file_write_iter+0x124/0x2a0
       blkdev_write_iter+0x168/0x270
       aio_write+0x1c4/0x310
       io_submit_one+0x971/0x1390
       __x64_sys_io_submit+0x12a/0x390
       do_syscall_64+0x6f/0x2e0
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Link: https://lore.kernel.org/r/20191025225830.257535-2-bvanassche@acm.org
    Cc: <stable@vger.kernel.org>
    Fixes: 0b5cb3300ae5 ("RDMA/srp: Increase max_segment_size")
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cca9985b4cbc..0626b62ed107 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4057,9 +4057,7 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
  */
 static inline unsigned int ib_dma_max_seg_size(struct ib_device *dev)
 {
-	struct device_dma_parameters *p = dev->dma_device->dma_parms;
-
-	return p ? p->max_segment_size : UINT_MAX;
+	return dma_get_max_seg_size(dev->dma_device);
 }
 
 /**

commit 4061ff7aa379fa770a82da0ed7ec4f9163034518
Author: Erez Alfasi <ereza@mellanox.com>
Date:   Wed Oct 16 09:23:08 2019 +0300

    RDMA/nldev: Provide MR statistics
    
    Add RDMA nldev netlink interface for dumping MR statistics information.
    
    Output example:
    
    $ ./ibv_rc_pingpong -o -P -s 500000000
      local address:  LID 0x0001, QPN 0x00008a, PSN 0xf81096, GID ::
    
    $ rdma stat show mr
    dev mlx5_0 mrn 2 page_faults 122071 page_invalidations 0
    
    Link: https://lore.kernel.org/r/20191016062308.11886-5-leon@kernel.org
    Signed-off-by: Erez Alfasi <ereza@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1ef31b27a41c..cca9985b4cbc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2570,6 +2570,13 @@ struct ib_device_ops {
 	 */
 	int (*counter_update_stats)(struct rdma_counter *counter);
 
+	/**
+	 * Allows rdma drivers to add their own restrack attributes
+	 * dumped via 'rdma stat' iproute2 command.
+	 */
+	int (*fill_stat_entry)(struct sk_buff *msg,
+			       struct rdma_restrack_entry *entry);
+
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_cq);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);

commit a3de94e3d61ec6e6c57ee066ec4d28ebc260dafa
Author: Erez Alfasi <ereza@mellanox.com>
Date:   Wed Oct 16 09:23:05 2019 +0300

    IB/mlx5: Introduce ODP diagnostic counters
    
    Introduce ODP diagnostic counters and count the following
    per MR within IB/mlx5 driver:
     1) Page faults:
            Total number of faulted pages.
     2) Page invalidations:
            Total number of pages invalidated by the OS during all
            invalidation events. The translations can be no longer
            valid due to either non-present pages or mapping changes.
    
    Link: https://lore.kernel.org/r/20191016062308.11886-2-leon@kernel.org
    Signed-off-by: Erez Alfasi <ereza@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a465fcae992b..1ef31b27a41c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2220,6 +2220,11 @@ struct rdma_netdev_alloc_params {
 				      struct net_device *netdev, void *param);
 };
 
+struct ib_odp_counters {
+	atomic64_t faults;
+	atomic64_t invalidations;
+};
+
 struct ib_counters {
 	struct ib_device	*device;
 	struct ib_uobject	*uobject;

commit a9018adfde809d44e71189b984fa61cc89682b5e
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri Oct 11 16:34:19 2019 +0300

    RDMA/uverbs: Prevent potential underflow
    
    The issue is in drivers/infiniband/core/uverbs_std_types_cq.c in the
    UVERBS_HANDLER(UVERBS_METHOD_CQ_CREATE) function.  We check that:
    
            if (attr.comp_vector >= attrs->ufile->device->num_comp_vectors) {
    
    But we don't check if "attr.comp_vector" is negative.  It could
    potentially lead to an array underflow.  My concern would be where
    cq->vector is used in the create_cq() function from the cxgb4 driver.
    
    And really "attr.comp_vector" is appears as a u32 to user space so that's
    the right type to use.
    
    Fixes: 9ee79fce3642 ("IB/core: Add completion queue (cq) object actions")
    Link: https://lore.kernel.org/r/20191011133419.GA22905@mwanda
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6a47ba85c54c..e7e733add99f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -366,7 +366,7 @@ struct ib_tm_caps {
 
 struct ib_cq_init_attr {
 	unsigned int	cqe;
-	int		comp_vector;
+	u32		comp_vector;
 	u32		flags;
 };
 

commit 00bd1439f464cfac3c60f6eabfe209b8a52e8194
Author: Yamin Friedman <yaminf@mellanox.com>
Date:   Mon Oct 7 16:59:32 2019 +0300

    RDMA/rw: Support threshold for registration vs scattering to local pages
    
    If there are more scatter entries than the recommended limit provided by
    the ib device, UMR registration is used. This will provide optimal
    performance when performing large RDMA READs over devices that advertise
    the threshold capability.
    
    With ConnectX-5 running NVMeoF RDMA with FIO single QP 128KB writes:
    Without use of cap: 70Gb/sec
    With use of cap: 84Gb/sec
    
    Link: https://lore.kernel.org/r/20191007135933.12483-3-leon@kernel.org
    Signed-off-by: Yamin Friedman <yaminf@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6a47ba85c54c..a465fcae992b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -445,6 +445,8 @@ struct ib_device_attr {
 	struct ib_tm_caps	tm_caps;
 	struct ib_cq_caps       cq_caps;
 	u64			max_dm_size;
+	/* Max entries for sgl for optimized performance per READ */
+	u32			max_sgl_rd;
 };
 
 enum ib_mtu {

commit 018c6837f3e63b45163d55a1668d9f8e6fdecf6e
Merge: 84da111de0b4 3eca7fc2d8d1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 21 10:26:24 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull RDMA subsystem updates from Jason Gunthorpe:
     "This cycle mainly saw lots of bug fixes and clean up code across the
      core code and several drivers, few new functional changes were made.
    
       - Many cleanup and bug fixes for hns
    
       - Various small bug fixes and cleanups in hfi1, mlx5, usnic, qed,
         bnxt_re, efa
    
       - Share the query_port code between all the iWarp drivers
    
       - General rework and cleanup of the ODP MR umem code to fit better
         with the mmu notifier get/put scheme
    
       - Support rdma netlink in non init_net name spaces
    
       - mlx5 support for XRC devx and DC ODP"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (99 commits)
      RDMA: Fix double-free in srq creation error flow
      RDMA/efa: Fix incorrect error print
      IB/mlx5: Free mpi in mp_slave mode
      IB/mlx5: Use the original address for the page during free_pages
      RDMA/bnxt_re: Fix spelling mistake "missin_resp" -> "missing_resp"
      RDMA/hns: Package operations of rq inline buffer into separate functions
      RDMA/hns: Optimize cmd init and mode selection for hip08
      IB/hfi1: Define variables as unsigned long to fix KASAN warning
      IB/{rdmavt, hfi1, qib}: Add a counter for credit waits
      IB/hfi1: Add traces for TID RDMA READ
      RDMA/siw: Relax from kmap_atomic() use in TX path
      IB/iser: Support up to 16MB data transfer in a single command
      RDMA/siw: Fix page address mapping in TX path
      RDMA: Fix goto target to release the allocated memory
      RDMA/usnic: Avoid overly large buffers on stack
      RDMA/odp: Add missing cast for 32 bit
      RDMA/hns: Use devm_platform_ioremap_resource() to simplify code
      Documentation/infiniband: update name of some functions
      RDMA/cma: Fix false error message
      RDMA/hns: Fix wrong assignment of qp_access_flags
      ...

commit c571feca2dc972dc5afeba9036d08239f1c51af1
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Aug 6 20:15:43 2019 -0300

    RDMA/odp: use mmu_notifier_get/put for 'struct ib_ucontext_per_mm'
    
    This is a significant simplification, no extra list is kept per FD, and
    the interval tree is now shared between all the ucontexts, reducing
    overhead if there are multiple ucontexts active.
    
    Link: https://lore.kernel.org/r/20190806231548.25242-7-jgg@ziepe.ca
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c2b39dda44cc..f659f4a02aa9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1417,9 +1417,6 @@ struct ib_ucontext {
 
 	bool cleanup_retryable;
 
-	struct mutex per_mm_list_lock;
-	struct list_head per_mm_list;
-
 	struct ib_rdmacg_object	cg_obj;
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:

commit 868df536f5e84672c3e002b949e0e44f97cb0f09
Merge: b2299e83815c fba0e448a2c5
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Aug 21 14:10:36 2019 -0300

    Merge branch 'odp_fixes' into rdma.git for-next
    
    Jason Gunthorpe says:
    
    ====================
    This is a collection of general cleanups for ODP to clarify some of the
    flows around umem creation and use of the interval tree.
    ====================
    
    The branch is based on v5.3-rc5 due to dependencies
    
    * odp_fixes:
      RDMA/mlx5: Use odp instead of mr->umem in pagefault_mr
      RDMA/mlx5: Use ib_umem_start instead of umem.address
      RDMA/core: Make invalidate_range a device operation
      RDMA/odp: Use kvcalloc for the dma_list and page_list
      RDMA/odp: Check for overflow when computing the umem_odp end
      RDMA/odp: Provide ib_umem_odp_release() to undo the allocs
      RDMA/odp: Split creating a umem_odp from ib_umem_get
      RDMA/odp: Make the three ways to create a umem_odp clear
      RMDA/odp: Consolidate umem_odp initialization
      RDMA/odp: Make it clearer when a umem is an implicit ODP umem
      RDMA/odp: Iterate over the whole rbtree directly
      RDMA/odp: Use the common interval tree library instead of generic
      RDMA/mlx5: Fix MR npages calculation for IB_ACCESS_HUGETLB
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit ce51346feede2ea41de0ad58af2b514223e11dad
Author: Moni Shoua <monis@mellanox.com>
Date:   Mon Aug 19 14:17:08 2019 +0300

    RDMA/core: Make invalidate_range a device operation
    
    The callback function 'invalidate_range' is implemented in a driver so the
    place for it is in the ib_device_ops structure and not in ib_ucontext.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Link: https://lore.kernel.org/r/20190819111710.18440-11-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4f225175cb91..c2b39dda44cc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1417,8 +1417,6 @@ struct ib_ucontext {
 
 	bool cleanup_retryable;
 
-	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
-				 unsigned long start, unsigned long end);
 	struct mutex per_mm_list_lock;
 	struct list_head per_mm_list;
 
@@ -2378,6 +2376,8 @@ struct ib_device_ops {
 			    u64 iova);
 	int (*unmap_fmr)(struct list_head *fmr_list);
 	int (*dealloc_fmr)(struct ib_fmr *fmr);
+	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
+				 unsigned long start, unsigned long end);
 	int (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	int (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,

commit b2299e83815c59ab59c4ee4fb4842b3b28e5072f
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Aug 19 14:45:47 2019 +0300

    RDMA: Delete DEBUG code
    
    There is no need to keep DEBUG defines for out-of-the tree testing.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190819114547.20704-1-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 391499008a22..08e966c8081a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -98,9 +98,6 @@ void ibdev_info(const struct ib_device *ibdev, const char *format, ...);
 #if defined(CONFIG_DYNAMIC_DEBUG)
 #define ibdev_dbg(__dev, format, args...)                       \
 	dynamic_ibdev_dbg(__dev, format, ##args)
-#elif defined(DEBUG)
-#define ibdev_dbg(__dev, format, args...)                       \
-	ibdev_printk(KERN_DEBUG, __dev, format, ##args)
 #else
 __printf(2, 3) __cold
 static inline

commit 72a7720fca37fec0daf295923f17ac5d88a613e1
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Wed Aug 7 13:31:35 2019 +0300

    RDMA: Introduce ib_port_phys_state enum
    
    In order to improve readability, add ib_port_phys_state enum to replace
    the use of magic numbers.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Reviewed-by: Andrew Boyer <aboyer@tobark.org>
    Acked-by: Michal Kalderon <michal.kalderon@marvell.com>
    Acked-by: Bernard Metzler <bmt@zurich.ibm.com>
    Link: https://lore.kernel.org/r/20190807103138.17219-2-kamalheib1@gmail.com
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0ecda7d15df2..391499008a22 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -493,6 +493,16 @@ enum ib_port_state {
 	IB_PORT_ACTIVE_DEFER	= 5
 };
 
+enum ib_port_phys_state {
+	IB_PORT_PHYS_STATE_SLEEP = 1,
+	IB_PORT_PHYS_STATE_POLLING = 2,
+	IB_PORT_PHYS_STATE_DISABLED = 3,
+	IB_PORT_PHYS_STATE_PORT_CONFIGURATION_TRAINING = 4,
+	IB_PORT_PHYS_STATE_LINK_UP = 5,
+	IB_PORT_PHYS_STATE_LINK_ERROR_RECOVERY = 6,
+	IB_PORT_PHYS_STATE_PHY_TEST = 7,
+};
+
 enum ib_port_width {
 	IB_WIDTH_1X	= 1,
 	IB_WIDTH_2X	= 16,

commit 05bb411ada9508b48044ef5d84dd8bc46cece607
Author: Gal Pressman <galpress@amazon.com>
Date:   Thu Aug 1 20:14:46 2019 +0300

    RDMA/core: Introduce ratelimited ibdev printk functions
    
    Add ratelimited helpers to the ibdev_* printk functions.
    Implementation inspired by counterpart dev_*_ratelimited functions.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190801171447.54440-2-galpress@amazon.com
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2a1523ccd7ab..0ecda7d15df2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -107,6 +107,48 @@ static inline
 void ibdev_dbg(const struct ib_device *ibdev, const char *format, ...) {}
 #endif
 
+#define ibdev_level_ratelimited(ibdev_level, ibdev, fmt, ...)           \
+do {                                                                    \
+	static DEFINE_RATELIMIT_STATE(_rs,                              \
+				      DEFAULT_RATELIMIT_INTERVAL,       \
+				      DEFAULT_RATELIMIT_BURST);         \
+	if (__ratelimit(&_rs))                                          \
+		ibdev_level(ibdev, fmt, ##__VA_ARGS__);                 \
+} while (0)
+
+#define ibdev_emerg_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_emerg, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_alert_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_alert, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_crit_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_crit, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_err_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_err, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_warn_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_warn, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_notice_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_notice, ibdev, fmt, ##__VA_ARGS__)
+#define ibdev_info_ratelimited(ibdev, fmt, ...) \
+	ibdev_level_ratelimited(ibdev_info, ibdev, fmt, ##__VA_ARGS__)
+
+#if defined(CONFIG_DYNAMIC_DEBUG)
+/* descriptor check is first to prevent flooding with "callbacks suppressed" */
+#define ibdev_dbg_ratelimited(ibdev, fmt, ...)                          \
+do {                                                                    \
+	static DEFINE_RATELIMIT_STATE(_rs,                              \
+				      DEFAULT_RATELIMIT_INTERVAL,       \
+				      DEFAULT_RATELIMIT_BURST);         \
+	DEFINE_DYNAMIC_DEBUG_METADATA(descriptor, fmt);                 \
+	if (DYNAMIC_DEBUG_BRANCH(descriptor) && __ratelimit(&_rs))      \
+		__dynamic_ibdev_dbg(&descriptor, ibdev, fmt,            \
+				    ##__VA_ARGS__);                     \
+} while (0)
+#else
+__printf(2, 3) __cold
+static inline
+void ibdev_dbg_ratelimited(const struct ib_device *ibdev, const char *format, ...) {}
+#endif
+
 union ib_gid {
 	u8	raw[16];
 	struct {

commit 20cf4e026730104892fa1268de0371a631cee294
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jul 29 13:22:09 2019 -0400

    rdma: Enable ib_alloc_cq to spread work over a device's comp_vectors
    
    Send and Receive completion is handled on a single CPU selected at
    the time each Completion Queue is allocated. Typically this is when
    an initiator instantiates an RDMA transport, or when a target
    accepts an RDMA connection.
    
    Some ULPs cannot open a connection per CPU to spread completion
    workload across available CPUs and MSI vectors. For such ULPs,
    provide an API that allows the RDMA core to select a completion
    vector based on the device's complement of available comp_vecs.
    
    ULPs that invoke ib_alloc_cq() with only comp_vector 0 are converted
    to use the new API so that their completion workloads interfere less
    with each other.
    
    Suggested-by: Hkon Bugge <haakon.bugge@oracle.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Cc: <linux-cifs@vger.kernel.org>
    Cc: <v9fs-developer@lists.sourceforge.net>
    Link: https://lore.kernel.org/r/20190729171923.13428.52555.stgit@manet.1015granger.net
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c5f8a9f17063..2a1523ccd7ab 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3711,6 +3711,25 @@ static inline struct ib_cq *ib_alloc_cq(struct ib_device *dev, void *private,
 				NULL);
 }
 
+struct ib_cq *__ib_alloc_cq_any(struct ib_device *dev, void *private,
+				int nr_cqe, enum ib_poll_context poll_ctx,
+				const char *caller);
+
+/**
+ * ib_alloc_cq_any: Allocate kernel CQ
+ * @dev: The IB device
+ * @private: Private data attached to the CQE
+ * @nr_cqe: Number of CQEs in the CQ
+ * @poll_ctx: Context used for polling the CQ
+ */
+static inline struct ib_cq *ib_alloc_cq_any(struct ib_device *dev,
+					    void *private, int nr_cqe,
+					    enum ib_poll_context poll_ctx)
+{
+	return __ib_alloc_cq_any(dev, private, nr_cqe, poll_ctx,
+				 KBUILD_MODNAME);
+}
+
 /**
  * ib_free_cq_user - Free kernel/user CQ
  * @cq: The CQ to free

commit 9cd5881719e9555cae300ec8b389eda3c8101339
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 31 11:18:41 2019 +0300

    RDMA/devices: Remove the lock around remove_client_context
    
    Due to the complexity of client->remove() callbacks it is desirable to not
    hold any locks while calling them. Remove the last one by tracking only
    the highest client ID and running backwards from there over the xarray.
    
    Since the only purpose of that lock was to protect the linked list, we can
    drop the lock.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190731081841.32345-3-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7b80ec822043..4f225175cb91 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2650,7 +2650,6 @@ struct ib_client {
 
 	refcount_t uses;
 	struct completion uses_zero;
-	struct list_head list;
 	u32 client_id;
 
 	/* kverbs are not required by the client */

commit 621e55ff5b8e0ab5d1063f0eae0ef3960bef8f6e
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 31 11:18:40 2019 +0300

    RDMA/devices: Do not deadlock during client removal
    
    lockdep reports:
    
       WARNING: possible circular locking dependency detected
    
       modprobe/302 is trying to acquire lock:
       0000000007c8919c ((wq_completion)ib_cm){+.+.}, at: flush_workqueue+0xdf/0x990
    
       but task is already holding lock:
       000000002d3d2ca9 (&device->client_data_rwsem){++++}, at: remove_client_context+0x79/0xd0 [ib_core]
    
       which lock already depends on the new lock.
    
       the existing dependency chain (in reverse order) is:
    
       -> #2 (&device->client_data_rwsem){++++}:
              down_read+0x3f/0x160
              ib_get_net_dev_by_params+0xd5/0x200 [ib_core]
              cma_ib_req_handler+0x5f6/0x2090 [rdma_cm]
              cm_process_work+0x29/0x110 [ib_cm]
              cm_req_handler+0x10f5/0x1c00 [ib_cm]
              cm_work_handler+0x54c/0x311d [ib_cm]
              process_one_work+0x4aa/0xa30
              worker_thread+0x62/0x5b0
              kthread+0x1ca/0x1f0
              ret_from_fork+0x24/0x30
    
       -> #1 ((work_completion)(&(&work->work)->work)){+.+.}:
              process_one_work+0x45f/0xa30
              worker_thread+0x62/0x5b0
              kthread+0x1ca/0x1f0
              ret_from_fork+0x24/0x30
    
       -> #0 ((wq_completion)ib_cm){+.+.}:
              lock_acquire+0xc8/0x1d0
              flush_workqueue+0x102/0x990
              cm_remove_one+0x30e/0x3c0 [ib_cm]
              remove_client_context+0x94/0xd0 [ib_core]
              disable_device+0x10a/0x1f0 [ib_core]
              __ib_unregister_device+0x5a/0xe0 [ib_core]
              ib_unregister_device+0x21/0x30 [ib_core]
              mlx5_ib_stage_ib_reg_cleanup+0x9/0x10 [mlx5_ib]
              __mlx5_ib_remove+0x3d/0x70 [mlx5_ib]
              mlx5_ib_remove+0x12e/0x140 [mlx5_ib]
              mlx5_remove_device+0x144/0x150 [mlx5_core]
              mlx5_unregister_interface+0x3f/0xf0 [mlx5_core]
              mlx5_ib_cleanup+0x10/0x3a [mlx5_ib]
              __x64_sys_delete_module+0x227/0x350
              do_syscall_64+0xc3/0x6a4
              entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Which is due to the read side of the client_data_rwsem being obtained
    recursively through a work queue flush during cm client removal.
    
    The lock is being held across the remove in remove_client_context() so
    that the function is a fence, once it returns the client is removed. This
    is required so that the two callers do not proceed with destruction until
    the client completes removal.
    
    Instead of using client_data_rwsem use the existing device unregistration
    refcount and add a similar client unregistration (client->uses) refcount.
    
    This will fence the two unregistration paths without holding any locks.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 921eab1143aa ("RDMA/devices: Re-organize device.c locking")
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190731081841.32345-2-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c5f8a9f17063..7b80ec822043 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2647,6 +2647,9 @@ struct ib_client {
 			const union ib_gid *gid,
 			const struct sockaddr *addr,
 			void *client_data);
+
+	refcount_t uses;
+	struct completion uses_zero;
 	struct list_head list;
 	u32 client_id;
 

commit da6629793aa6944db6c8a908ca1a52d87f1489aa
Author: Yamin Friedman <yaminf@mellanox.com>
Date:   Mon Jul 8 13:59:03 2019 +0300

    RDMA/core: Provide RDMA DIM support for ULPs
    
    Added the interface in the infiniband driver that applies the rdma_dim
    adaptive moderation. There is now a special function for allocating an
    ib_cq that uses rdma_dim.
    
    Performance improvement (ConnectX-5 100GbE, x86) running FIO benchmark over
    NVMf between two equal end-hosts with 56 cores across a Mellanox switch
    using null_blk device:
    
    READS without DIM:
    blk size | BW       | IOPS | 99th percentile latency  | 99.99th latency
    512B     | 3.8GiB/s | 7.7M | 1401  usec               | 2442  usec
    4k       | 7.0GiB/s | 1.8M | 4817  usec               | 6587  usec
    64k      | 10.7GiB/s| 175k | 9896  usec               | 10028 usec
    
    IO WRITES without DIM:
    blk size | BW       | IOPS | 99th percentile latency  | 99.99th latency
    512B     | 3.6GiB/s | 7.5M | 1434  usec               | 2474  usec
    4k       | 6.3GiB/s | 1.6M | 938   usec               | 1221  usec
    64k      | 10.7GiB/s| 175k | 8979  usec               | 12780 usec
    
    IO READS with DIM:
    blk size | BW       | IOPS | 99th percentile latency  | 99.99th latency
    512B     | 4GiB/s   | 8.2M | 816    usec              | 889   usec
    4k       | 10.1GiB/s| 2.65M| 3359   usec              | 5080  usec
    64k      | 10.7GiB/s| 175k | 9896   usec              | 10028 usec
    
    IO WRITES with DIM:
    blk size | BW       | IOPS  | 99th percentile latency | 99.99th latency
    512B     | 3.9GiB/s | 8.1M  | 799   usec              | 922   usec
    4k       | 9.6GiB/s | 2.5M  | 717   usec              | 1004  usec
    64k      | 10.7GiB/s| 176k  | 8586  usec              | 12256 usec
    
    The rdma_dim algorithm was designed to measure the effectiveness of
    moderation on the flow in a general way and thus should be appropriate
    for all RDMA storage protocols.
    
    rdma_dim is configured to be the default option based on performance
    improvement seen after extensive tests.
    
    Signed-off-by: Yamin Friedman <yaminf@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4053be51b7fa..c5f8a9f17063 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -61,6 +61,7 @@
 #include <linux/cgroup_rdma.h>
 #include <linux/irqflags.h>
 #include <linux/preempt.h>
+#include <linux/dim.h>
 #include <uapi/rdma/ib_user_verbs.h>
 #include <rdma/rdma_counter.h>
 #include <rdma/restrack.h>
@@ -1509,6 +1510,7 @@ struct ib_cq {
 		struct work_struct	work;
 	};
 	struct workqueue_struct *comp_wq;
+	struct dim *dim;
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
@@ -2576,6 +2578,8 @@ struct ib_device {
 	u16                          is_switch:1;
 	/* Indicates kernel verbs support, should not be used in drivers */
 	u16                          kverbs_provider:1;
+	/* CQ adaptive moderation (RDMA DIM) */
+	u16                          use_cq_dim:1;
 	u8                           node_type;
 	u8                           phys_port_cnt;
 	struct ib_device_attr        attrs;

commit 89705e92700170888236555fe91b45e4c1bb0985
Author: Danit Goldberg <danitg@mellanox.com>
Date:   Fri Jul 5 19:21:57 2019 +0300

    IB/mlx5: Report correctly tag matching rendezvous capability
    
    Userspace expects the IB_TM_CAP_RC bit to indicate that the device
    supports RC transport tag matching with rendezvous offload. However the
    firmware splits this into two capabilities for eager and rendezvous tag
    matching.
    
    Only if the FW supports both modes should userspace be told the tag
    matching capability is available.
    
    Cc: <stable@vger.kernel.org> # 4.13
    Fixes: eb761894351d ("IB/mlx5: Fill XRQ capabilities")
    Signed-off-by: Danit Goldberg <danitg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 50806bef9f20..4053be51b7fa 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -307,8 +307,8 @@ struct ib_rss_caps {
 };
 
 enum ib_tm_cap_flags {
-	/*  Support tag matching on RC transport */
-	IB_TM_CAP_RC		    = 1 << 0,
+	/*  Support tag matching with rendezvous offload for RC transport */
+	IB_TM_CAP_RNDV_RC = 1 << 0,
 };
 
 struct ib_tm_caps {

commit 6e7be47a53459ba3d288c3240ccd948fc699c377
Author: Mark Zhang <markz@mellanox.com>
Date:   Tue Jul 2 13:02:46 2019 +0300

    RDMA/nldev: Allow get default counter statistics through RDMA netlink
    
    This patch adds the ability to return the hwstats of per-port default
    counters (which can also be queried through sysfs nodes).
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0c5151a12ae4..50806bef9f20 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2124,6 +2124,7 @@ struct ib_port_data {
 	struct net_device __rcu *netdev;
 	struct hlist_node ndev_hash_link;
 	struct rdma_port_counter port_counter;
+	struct rdma_hw_stats *hw_stats;
 };
 
 /* rdma netdev type - specifies protocol type */

commit c4ffee7c9bdba7b189df3251e375c4c7e93a91ac
Author: Mark Zhang <markz@mellanox.com>
Date:   Tue Jul 2 13:02:40 2019 +0300

    RDMA/netlink: Implement counter dumpit calback
    
    This patch adds the ability to return all available counters together with
    their properties and hwstats.
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0205472eb73a..0c5151a12ae4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2503,6 +2503,16 @@ struct ib_device_ops {
 	 * counter_dealloc -De-allocate the hw counter
 	 */
 	int (*counter_dealloc)(struct rdma_counter *counter);
+	/**
+	 * counter_alloc_stats - Allocate a struct rdma_hw_stats and fill in
+	 * the driver initialized data.
+	 */
+	struct rdma_hw_stats *(*counter_alloc_stats)(
+		struct rdma_counter *counter);
+	/**
+	 * counter_update_stats - Query the stats value of this counter
+	 */
+	int (*counter_update_stats)(struct rdma_counter *counter);
 
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_cq);

commit 99fa331dc8629be55ac7a0cca0dc56492070ddac
Author: Mark Zhang <markz@mellanox.com>
Date:   Tue Jul 2 13:02:35 2019 +0300

    RDMA/counter: Add "auto" configuration mode support
    
    In auto mode all QPs belong to one category are bind automatically to a
    single counter set. Currently only "qp type" is supported.
    
    In this mode the qp counter is set in RST2INIT modification, and when a qp
    is destroyed the counter is unbound.
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3d19c056fbc0..0205472eb73a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1698,6 +1698,9 @@ struct ib_qp {
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
 	struct rdma_restrack_entry     res;
+
+	/* The counter the qp is bind to */
+	struct rdma_counter    *counter;
 };
 
 struct ib_dm {
@@ -2485,6 +2488,21 @@ struct ib_device_ops {
 			 u8 pdata_len);
 	int (*iw_create_listen)(struct iw_cm_id *cm_id, int backlog);
 	int (*iw_destroy_listen)(struct iw_cm_id *cm_id);
+	/**
+	 * counter_bind_qp - Bind a QP to a counter.
+	 * @counter - The counter to be bound. If counter->id is zero then
+	 *   the driver needs to allocate a new counter and set counter->id
+	 */
+	int (*counter_bind_qp)(struct rdma_counter *counter, struct ib_qp *qp);
+	/**
+	 * counter_unbind_qp - Unbind the qp from the dynamically-allocated
+	 *   counter and bind it onto the default one
+	 */
+	int (*counter_unbind_qp)(struct ib_qp *qp);
+	/**
+	 * counter_dealloc -De-allocate the hw counter
+	 */
+	int (*counter_dealloc)(struct rdma_counter *counter);
 
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_cq);

commit 413d3347503bc39e17577eaf16451fd492a68558
Author: Mark Zhang <markz@mellanox.com>
Date:   Tue Jul 2 13:02:34 2019 +0300

    RDMA/counter: Add set/clear per-port auto mode support
    
    Add an API to support set/clear per-port auto mode.
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 26e9c2594913..3d19c056fbc0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -62,6 +62,7 @@
 #include <linux/irqflags.h>
 #include <linux/preempt.h>
 #include <uapi/rdma/ib_user_verbs.h>
+#include <rdma/rdma_counter.h>
 #include <rdma/restrack.h>
 #include <rdma/signature.h>
 #include <uapi/rdma/rdma_user_ioctl.h>
@@ -2119,6 +2120,7 @@ struct ib_port_data {
 	spinlock_t netdev_lock;
 	struct net_device __rcu *netdev;
 	struct hlist_node ndev_hash_link;
+	struct rdma_port_counter port_counter;
 };
 
 /* rdma netdev type - specifies protocol type */

commit 371bb62158d53c1fc33e2fb9b6aeb9522caf6cf4
Merge: 10dcc7448e9e 4b972a01a7da
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jun 28 21:18:23 2019 -0300

    Merge tag 'v5.2-rc6' into rdma.git for-next
    
    For dependencies in next patches.
    
    Resolve conflicts:
    - Use uverbs_get_cleared_udata() with new cq allocation flow
    - Continue to delete nes despite SPDX conflict
    - Resolve list appends in mlx5_command_str()
    - Use u16 for vport_rule stuff
    - Resolve list appends in struct ib_client
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 5c171cbe3ab3d1390290eaa85e7b371cc26b1122
Author: Israel Rukshin <israelr@mellanox.com>
Date:   Tue Jun 11 18:52:54 2019 +0300

    RDMA/mlx5: Remove unused IB_WR_REG_SIG_MR code
    
    IB_WR_REG_SIG_MR is not needed after IB_WR_REG_MR_INTEGRITY
    was used.
    
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 14b5eab76ed8..e2478b74551d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -776,9 +776,6 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  * enum ib_mr_type - memory region type
  * @IB_MR_TYPE_MEM_REG:       memory region that is used for
  *                            normal registration
- * @IB_MR_TYPE_SIGNATURE:     memory region that is used for
- *                            signature operations (data-integrity
- *                            capable regions)
  * @IB_MR_TYPE_SG_GAPS:       memory region that is capable to
  *                            register any arbitrary sg lists (without
  *                            the normal mr constraints - see
@@ -794,7 +791,6 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  */
 enum ib_mr_type {
 	IB_MR_TYPE_MEM_REG,
-	IB_MR_TYPE_SIGNATURE,
 	IB_MR_TYPE_SG_GAPS,
 	IB_MR_TYPE_DM,
 	IB_MR_TYPE_USER,
@@ -1235,7 +1231,6 @@ enum ib_wr_opcode {
 
 	/* These are kernel only and can not be issued by userspace */
 	IB_WR_REG_MR = 0x20,
-	IB_WR_REG_SIG_MR,
 	IB_WR_REG_MR_INTEGRITY,
 
 	/* reserve values for low level drivers' internal use.
@@ -1346,20 +1341,6 @@ static inline const struct ib_reg_wr *reg_wr(const struct ib_send_wr *wr)
 	return container_of(wr, struct ib_reg_wr, wr);
 }
 
-struct ib_sig_handover_wr {
-	struct ib_send_wr	wr;
-	struct ib_sig_attrs    *sig_attrs;
-	struct ib_mr	       *sig_mr;
-	int			access_flags;
-	struct ib_sge	       *prot;
-};
-
-static inline const struct ib_sig_handover_wr *
-sig_handover_wr(const struct ib_send_wr *wr)
-{
-	return container_of(wr, struct ib_sig_handover_wr, wr);
-}
-
 struct ib_recv_wr {
 	struct ib_recv_wr      *next;
 	union {

commit 185eddc45798b9f73e5470964948d79b4c8df4b7
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:51 2019 +0300

    RDMA/core: Validate integrity handover device cap
    
    Protect the case that a ULP tries to allocate a QP with signature
    enabled flag while the LLD doesn't support this feature.
    While we're here, also move integrity_en attribute from mlx5_qp to
    ib_qp as a preparation for adding new integrity API to the rw-API
    (that is part of ib_core module).
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6de0ea1aafc3..14b5eab76ed8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1711,6 +1711,7 @@ struct ib_qp {
 	struct ib_qp_security  *qp_sec;
 	u8			port;
 
+	bool			integrity_en;
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */

commit c0a6cbb9cbccffc249743afa16e64f16c46c80b2
Author: Israel Rukshin <israelr@mellanox.com>
Date:   Tue Jun 11 18:52:50 2019 +0300

    RDMA/core: Rename signature qp create flag and signature device capability
    
    Rename IB_QP_CREATE_SIGNATURE_EN to IB_QP_CREATE_INTEGRITY_EN
    and IB_DEVICE_SIGNATURE_HANDOVER to IB_DEVICE_INTEGRITY_HANDOVER.
    
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 28db256cbdb9..6de0ea1aafc3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -253,7 +253,7 @@ enum ib_device_cap_flags {
 	 */
 	IB_DEVICE_CROSS_CHANNEL			= (1 << 27),
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
-	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
+	IB_DEVICE_INTEGRITY_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1ULL << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 	IB_DEVICE_VIRTUAL_FUNCTION		= (1ULL << 33),
@@ -1056,7 +1056,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_MANAGED_SEND               = 1 << 3,
 	IB_QP_CREATE_MANAGED_RECV               = 1 << 4,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
-	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
+	IB_QP_CREATE_INTEGRITY_EN		= 1 << 6,
 	/* FREE					= 1 << 7, */
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,

commit 38ca87c6f1e514686d4a385246d1afe1e1f2e482
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:46 2019 +0300

    RDMA/mlx5: Introduce and implement new IB_WR_REG_MR_INTEGRITY work request
    
    This new WR will be used to perform PI (protection information) handover
    using the new API. Using the new API, the user will post a single WR that
    will internally perform all the needed actions to complete PI operation.
    This new WR will use a memory region that was allocated as
    IB_MR_TYPE_INTEGRITY and was mapped using ib_map_mr_sg_pi to perform the
    registration. In the old API, in order to perform a signature handover
    operation, each ULP should perform the following:
    1. Map and register the data buffers.
    2. Map and register the protection buffers.
    3. Post a special reg WR to configure the signature handover operation
       layout.
    4. Invalidate the signature memory key.
    5. Invalidate protection buffers memory key.
    6. Invalidate data buffers memory key.
    
    In the new API, the mapping of both data and protection buffers is
    performed using a single call to ib_map_mr_sg_pi function. Also the
    registration of the buffers and the configuration of the signature
    operation layout is done by a single new work request called
    IB_WR_REG_MR_INTEGRITY.
    This patch implements this operation for mlx5 devices that are capable to
    offload data integrity generation/validation while performing the actual
    buffer transfer.
    This patch will not remove the old signature API that is used by the iSER
    initiator and target drivers. This will be done in the future.
    
    In the internal implementation, for each IB_WR_REG_MR_INTEGRITY work
    request, we are using a single UMR operation to register both data and
    protection buffers using KLM's.
    Afterwards, another UMR operation will describe the strided block format.
    These will be followed by 2 SET_PSV operations to set the memory/wire
    domains initial signature parameters passed by the user.
    In the end of the whole transaction, only the signature memory key
    (the one that exposed for the RDMA operation) will be invalidated.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9169e798334f..28db256cbdb9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1236,6 +1236,7 @@ enum ib_wr_opcode {
 	/* These are kernel only and can not be issued by userspace */
 	IB_WR_REG_MR = 0x20,
 	IB_WR_REG_SIG_MR,
+	IB_WR_REG_MR_INTEGRITY,
 
 	/* reserve values for low level drivers' internal use.
 	 * These values will not be used at all in the ib core layer.

commit 62e3c379d4d713dbcb70adc7c65443fd8722b33f
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:43 2019 +0300

    RDMA/mlx5: Add attr for max number page list length for PI operation
    
    PI offload (protection information) is a feature that each RDMA provider
    can implement differently. Thus, introduce new device attribute to define
    the maximal length of the page list for PI fast registration operation. For
    example, mlx5 driver uses a single internal MR to map both data and
    protection SGL's, so it's equal to max_fast_reg_page_list_len / 2.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 995b217a1940..9169e798334f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -390,6 +390,7 @@ struct ib_device_attr {
 	int			max_srq_wr;
 	int			max_srq_sge;
 	unsigned int		max_fast_reg_page_list_len;
+	unsigned int		max_pi_fast_reg_page_list_len;
 	u16			max_pkeys;
 	u8			local_ca_ack_delay;
 	int			sig_prot_cap;

commit 7c717d3aeeaabbfddd0fe949b501595a2e3469e4
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:41 2019 +0300

    RDMA/core: Add signature attrs element for ib_mr structure
    
    This element will describe the needed characteristics for the signature
    operation per signature enabled memory region (type IB_MR_TYPE_INTEGRITY).
    Also add meta_length attribute to ib_sig_attrs structure for saving the
    mapped metadata length (needed for the new API implementation).
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 632e133e7a59..995b217a1940 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1739,7 +1739,7 @@ struct ib_mr {
 	};
 
 	struct ib_dm      *dm;
-
+	struct ib_sig_attrs *sig_attrs; /* only for IB_MR_TYPE_INTEGRITY MRs */
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */

commit 2cdfcdd8677b277b32d32ef8976802dc5d5f883f
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:40 2019 +0300

    RDMA/core: Introduce ib_map_mr_sg_pi to map data/protection sgl's
    
    This function will map the previously dma mapped SG lists for PI
    (protection information) and data to an appropriate memory region for
    future registration.
    The given MR must be allocated as IB_MR_TYPE_INTEGRITY.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 01bc04c8e220..632e133e7a59 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2437,6 +2437,11 @@ struct ib_device_ops {
 	int (*read_counters)(struct ib_counters *counters,
 			     struct ib_counters_read_attr *counters_read_attr,
 			     struct uverbs_attr_bundle *attrs);
+	int (*map_mr_sg_pi)(struct ib_mr *mr, struct scatterlist *data_sg,
+			    int data_sg_nents, unsigned int *data_sg_offset,
+			    struct scatterlist *meta_sg, int meta_sg_nents,
+			    unsigned int *meta_sg_offset);
+
 	/**
 	 * alloc_hw_stats - Allocate a struct rdma_hw_stats and fill in the
 	 *   driver initialized data.  The struct is kfree()'ed by the sysfs
@@ -4236,6 +4241,10 @@ int ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
 
 int ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
 		 unsigned int *sg_offset, unsigned int page_size);
+int ib_map_mr_sg_pi(struct ib_mr *mr, struct scatterlist *data_sg,
+		    int data_sg_nents, unsigned int *data_sg_offset,
+		    struct scatterlist *meta_sg, int meta_sg_nents,
+		    unsigned int *meta_sg_offset, unsigned int page_size);
 
 static inline int
 ib_map_mr_sg_zbva(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,

commit 26bc7eaee94fd904d1817fee4d864f8526807465
Author: Israel Rukshin <israelr@mellanox.com>
Date:   Tue Jun 11 18:52:39 2019 +0300

    RDMA/core: Introduce IB_MR_TYPE_INTEGRITY and ib_alloc_mr_integrity API
    
    This is a preparation for signature verbs API re-design. In the new
    design a single MR with IB_MR_TYPE_INTEGRITY type will be used to perform
    the needed mapping for data integrity operations.
    
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b6ec71ee4d3e..01bc04c8e220 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -788,6 +788,8 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  *                            application
  * @IB_MR_TYPE_DMA:           memory region that is used for DMA operations
  *                            without address translations (VA=PA)
+ * @IB_MR_TYPE_INTEGRITY:     memory region that is used for
+ *                            data integrity operations
  */
 enum ib_mr_type {
 	IB_MR_TYPE_MEM_REG,
@@ -796,6 +798,7 @@ enum ib_mr_type {
 	IB_MR_TYPE_DM,
 	IB_MR_TYPE_USER,
 	IB_MR_TYPE_DMA,
+	IB_MR_TYPE_INTEGRITY,
 };
 
 enum ib_mr_status_check {
@@ -2363,6 +2366,9 @@ struct ib_device_ops {
 	int (*dereg_mr)(struct ib_mr *mr, struct ib_udata *udata);
 	struct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,
 				  u32 max_num_sg, struct ib_udata *udata);
+	struct ib_mr *(*alloc_mr_integrity)(struct ib_pd *pd,
+					    u32 max_num_data_sg,
+					    u32 max_num_meta_sg);
 	int (*advise_mr)(struct ib_pd *pd,
 			 enum ib_uverbs_advise_mr_advice advice, u32 flags,
 			 struct ib_sge *sg_list, u32 num_sge,
@@ -4042,6 +4048,10 @@ static inline struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
 	return ib_alloc_mr_user(pd, mr_type, max_num_sg, NULL);
 }
 
+struct ib_mr *ib_alloc_mr_integrity(struct ib_pd *pd,
+				    u32 max_num_data_sg,
+				    u32 max_num_meta_sg);
+
 /**
  * ib_update_fast_reg_key - updates the key portion of the fast_reg MR
  *   R_Key and L_Key.

commit a0bc099abf7b45b16cb18459f3516af8c2fea781
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:38 2019 +0300

    RDMA/core: Save the MR type in the ib_mr structure
    
    This is a preparation for the signature verbs API change. This change is
    needed since the MR type will define, in the upcoming patches, the need
    for allocating internal resources in LLD for signature handover related
    operations. It will also help to make sure that signature related
    functions are called with an appropriate MR type and fail otherwise.
    Also introduce new mr types IB_MR_TYPE_USER, IB_MR_TYPE_DMA and
    IB_MR_TYPE_DM for correctness.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dc59fa12669a..b6ec71ee4d3e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -782,11 +782,20 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  *                            register any arbitrary sg lists (without
  *                            the normal mr constraints - see
  *                            ib_map_mr_sg)
+ * @IB_MR_TYPE_DM:            memory region that is used for device
+ *                            memory registration
+ * @IB_MR_TYPE_USER:          memory region that is used for the user-space
+ *                            application
+ * @IB_MR_TYPE_DMA:           memory region that is used for DMA operations
+ *                            without address translations (VA=PA)
  */
 enum ib_mr_type {
 	IB_MR_TYPE_MEM_REG,
 	IB_MR_TYPE_SIGNATURE,
 	IB_MR_TYPE_SG_GAPS,
+	IB_MR_TYPE_DM,
+	IB_MR_TYPE_USER,
+	IB_MR_TYPE_DMA,
 };
 
 enum ib_mr_status_check {
@@ -1719,6 +1728,7 @@ struct ib_mr {
 	u64		   iova;
 	u64		   length;
 	unsigned int	   page_size;
+	enum ib_mr_type	   type;
 	bool		   need_inval;
 	union {
 		struct ib_uobject	*uobject;	/* user */

commit 36b1e47ff0c196a95d5e55a05b3f988f827cce7e
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:37 2019 +0300

    RDMA/core: Introduce new header file for signature operations
    
    Ease the exhausted ib_verbs.h file and make the code more readable.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 805148a12660..dc59fa12669a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -63,6 +63,7 @@
 #include <linux/preempt.h>
 #include <uapi/rdma/ib_user_verbs.h>
 #include <rdma/restrack.h>
+#include <rdma/signature.h>
 #include <uapi/rdma/rdma_user_ioctl.h>
 #include <uapi/rdma/ib_user_ioctl_verbs.h>
 
@@ -264,17 +265,6 @@ enum ib_device_cap_flags {
 	IB_DEVICE_ALLOW_USER_UNREG		= (1ULL << 37),
 };
 
-enum ib_signature_prot_cap {
-	IB_PROT_T10DIF_TYPE_1 = 1,
-	IB_PROT_T10DIF_TYPE_2 = 1 << 1,
-	IB_PROT_T10DIF_TYPE_3 = 1 << 2,
-};
-
-enum ib_signature_guard_cap {
-	IB_GUARD_T10DIF_CRC	= 1,
-	IB_GUARD_T10DIF_CSUM	= 1 << 1,
-};
-
 enum ib_atomic_cap {
 	IB_ATOMIC_NONE,
 	IB_ATOMIC_HCA,
@@ -799,106 +789,6 @@ enum ib_mr_type {
 	IB_MR_TYPE_SG_GAPS,
 };
 
-/**
- * Signature types
- * IB_SIG_TYPE_NONE: Unprotected.
- * IB_SIG_TYPE_T10_DIF: Type T10-DIF
- */
-enum ib_signature_type {
-	IB_SIG_TYPE_NONE,
-	IB_SIG_TYPE_T10_DIF,
-};
-
-/**
- * Signature T10-DIF block-guard types
- * IB_T10DIF_CRC: Corresponds to T10-PI mandated CRC checksum rules.
- * IB_T10DIF_CSUM: Corresponds to IP checksum rules.
- */
-enum ib_t10_dif_bg_type {
-	IB_T10DIF_CRC,
-	IB_T10DIF_CSUM
-};
-
-/**
- * struct ib_t10_dif_domain - Parameters specific for T10-DIF
- *     domain.
- * @bg_type: T10-DIF block guard type (CRC|CSUM)
- * @pi_interval: protection information interval.
- * @bg: seed of guard computation.
- * @app_tag: application tag of guard block
- * @ref_tag: initial guard block reference tag.
- * @ref_remap: Indicate wethear the reftag increments each block
- * @app_escape: Indicate to skip block check if apptag=0xffff
- * @ref_escape: Indicate to skip block check if reftag=0xffffffff
- * @apptag_check_mask: check bitmask of application tag.
- */
-struct ib_t10_dif_domain {
-	enum ib_t10_dif_bg_type bg_type;
-	u16			pi_interval;
-	u16			bg;
-	u16			app_tag;
-	u32			ref_tag;
-	bool			ref_remap;
-	bool			app_escape;
-	bool			ref_escape;
-	u16			apptag_check_mask;
-};
-
-/**
- * struct ib_sig_domain - Parameters for signature domain
- * @sig_type: specific signauture type
- * @sig: union of all signature domain attributes that may
- *     be used to set domain layout.
- */
-struct ib_sig_domain {
-	enum ib_signature_type sig_type;
-	union {
-		struct ib_t10_dif_domain dif;
-	} sig;
-};
-
-/**
- * struct ib_sig_attrs - Parameters for signature handover operation
- * @check_mask: bitmask for signature byte check (8 bytes)
- * @mem: memory domain layout desciptor.
- * @wire: wire domain layout desciptor.
- */
-struct ib_sig_attrs {
-	u8			check_mask;
-	struct ib_sig_domain	mem;
-	struct ib_sig_domain	wire;
-};
-
-enum ib_sig_err_type {
-	IB_SIG_BAD_GUARD,
-	IB_SIG_BAD_REFTAG,
-	IB_SIG_BAD_APPTAG,
-};
-
-/**
- * Signature check masks (8 bytes in total) according to the T10-PI standard:
- *  -------- -------- ------------
- * | GUARD  | APPTAG |   REFTAG   |
- * |  2B    |  2B    |    4B      |
- *  -------- -------- ------------
- */
-enum {
-	IB_SIG_CHECK_GUARD	= 0xc0,
-	IB_SIG_CHECK_APPTAG	= 0x30,
-	IB_SIG_CHECK_REFTAG	= 0x0f,
-};
-
-/**
- * struct ib_sig_err - signature error descriptor
- */
-struct ib_sig_err {
-	enum ib_sig_err_type	err_type;
-	u32			expected;
-	u32			actual;
-	u64			sig_err_offset;
-	u32			key;
-};
-
 enum ib_mr_status_check {
 	IB_MR_CHECK_SIG_STATUS = 1,
 };

commit a49b1dc7ae447d7085360cd587fc1c8b9ec6c871
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Jun 12 15:27:41 2019 +0300

    RDMA: Convert destroy_wq to be void
    
    All callers of destroy WQ are always success and there is no need
    to check their return value, so convert destroy_wq to be void.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6f09fcc21d7a..805148a12660 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2509,7 +2509,7 @@ struct ib_device_ops {
 	struct ib_wq *(*create_wq)(struct ib_pd *pd,
 				   struct ib_wq_init_attr *init_attr,
 				   struct ib_udata *udata);
-	int (*destroy_wq)(struct ib_wq *wq, struct ib_udata *udata);
+	void (*destroy_wq)(struct ib_wq *wq, struct ib_udata *udata);
 	int (*modify_wq)(struct ib_wq *wq, struct ib_wq_attr *attr,
 			 u32 wq_attr_mask, struct ib_udata *udata);
 	struct ib_rwq_ind_table *(*create_rwq_ind_table)(

commit 8f71bb0030b8816f57be142f95b3c7189c6eaf4c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jun 13 21:38:19 2019 -0300

    RDMA: Report available cdevs through RDMA_NLDEV_CMD_GET_CHARDEV
    
    Update the struct ib_client for all modules exporting cdevs related to the
    ibdevice to also implement RDMA_NLDEV_CMD_GET_CHARDEV. All cdevs are now
    autoloadable and discoverable by userspace over netlink instead of relying
    on sysfs.
    
    uverbs also exposes the DRIVER_ID for drivers that are able to support
    driver id binding in rdma-core.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a1265e9ce2d1..6f09fcc21d7a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2321,6 +2321,7 @@ struct ib_device_ops {
 	struct module *owner;
 	enum rdma_driver_id driver_id;
 	u32 uverbs_abi_ver;
+	unsigned int uverbs_no_driver_id_binding:1;
 
 	int (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,
 			 const struct ib_send_wr **bad_send_wr);

commit 0e2d00eb6fd45f2a645f4874286bdc5b4b53782b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jun 13 21:38:18 2019 -0300

    RDMA: Add NLDEV_GET_CHARDEV to allow char dev discovery and autoload
    
    Allow userspace to issue a netlink query against the ib_device for
    something like "uverbs" and get back the char dev name, inode major/minor,
    and interface ABI information for "uverbs0".
    
    Since we are now in netlink this can also trigger a module autoload to
    make the uverbs device come into existence.
    
    Largely this will let us replace searching and reading inside sysfs to
    setup devices, and provides an alternative (using driver_id) to device
    name based provider binding for things like rxe.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 973514ea17a7..a1265e9ce2d1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2684,10 +2684,14 @@ struct ib_device {
 	u32 iw_driver_flags;
 };
 
+struct ib_client_nl_info;
 struct ib_client {
 	const char *name;
 	void (*add)   (struct ib_device *);
 	void (*remove)(struct ib_device *, void *client_data);
+	int (*get_nl_info)(struct ib_device *ibdev, void *client_data,
+			   struct ib_client_nl_info *res);
+	int (*get_global_nl_info)(struct ib_client_nl_info *res);
 
 	/* Returns the net_dev belonging to this ib_client and matching the
 	 * given parameters.

commit 5d60c11154116e2127374d4178e952649612b69b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jun 13 21:38:17 2019 -0300

    RDMA: Move rdma_node_type to uapi/
    
    This enum is exposed over the sysfs file 'node_type' and over netlink via
    RDMA_NLDEV_ATTR_DEV_NODE_TYPE, so declare it in the uapi headers.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f357e03a85a6..973514ea17a7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -132,17 +132,6 @@ struct ib_gid_attr {
 	u8			port_num;
 };
 
-enum rdma_node_type {
-	/* IB values map to NodeInfo:NodeType. */
-	RDMA_NODE_IB_CA 	= 1,
-	RDMA_NODE_IB_SWITCH,
-	RDMA_NODE_IB_ROUTER,
-	RDMA_NODE_RNIC,
-	RDMA_NODE_USNIC,
-	RDMA_NODE_USNIC_UDP,
-	RDMA_NODE_UNSPECIFIED,
-};
-
 enum {
 	/* set the local administered indication */
 	IB_SA_WELL_KNOWN_GUID	= BIT_ULL(57) | 2,
@@ -164,7 +153,7 @@ enum rdma_protocol_type {
 };
 
 __attribute_const__ enum rdma_transport_type
-rdma_node_get_transport(enum rdma_node_type node_type);
+rdma_node_get_transport(unsigned int node_type);
 
 enum rdma_network_type {
 	RDMA_NETWORK_IB,

commit e39afe3d6dbd908d8fd189571a3c1561088a86c2
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 28 14:37:29 2019 +0300

    RDMA: Convert CQ allocations to be under core responsibility
    
    Ensure that CQ is allocated and freed by IB/core and not by drivers.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Tested-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bc1d94c9c9ba..f357e03a85a6 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2458,9 +2458,8 @@ struct ib_device_ops {
 	int (*query_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
 			int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr);
 	int (*destroy_qp)(struct ib_qp *qp, struct ib_udata *udata);
-	struct ib_cq *(*create_cq)(struct ib_device *device,
-				   const struct ib_cq_init_attr *attr,
-				   struct ib_udata *udata);
+	int (*create_cq)(struct ib_cq *cq, const struct ib_cq_init_attr *attr,
+			 struct ib_udata *udata);
 	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
 	void (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
 	int (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);
@@ -2601,6 +2600,7 @@ struct ib_device_ops {
 	int (*iw_destroy_listen)(struct iw_cm_id *cm_id);
 
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
+	DECLARE_RDMA_OBJ_SIZE(ib_cq);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 	DECLARE_RDMA_OBJ_SIZE(ib_srq);
 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);

commit a52c8e2469c30cf7ac453d624aed9c168b23d1af
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 28 14:37:28 2019 +0300

    RDMA: Clean destroy CQ in drivers do not return errors
    
    Like all other destroy commands, .destroy_cq() call is not supposed
    to fail. In all flows, the attempt to return earlier caused to memory
    leaks.
    
    This patch converts .destroy_cq() to do not return any errors.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Acked-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d1f16a6c4810..bc1d94c9c9ba 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2462,7 +2462,7 @@ struct ib_device_ops {
 				   const struct ib_cq_init_attr *attr,
 				   struct ib_udata *udata);
 	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
-	int (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
+	void (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
 	int (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);
 	struct ib_mr *(*get_dma_mr)(struct ib_pd *pd, int mr_access_flags);
 	struct ib_mr *(*reg_user_mr)(struct ib_pd *pd, u64 start, u64 length,

commit 7a15414252ae4f1d450462d83f883b2d9d8036ee
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 5 14:39:26 2019 -0300

    RDMA: Move owner into struct ib_device_ops
    
    This more closely follows how other subsytems work, with owner being a
    member of the structure containing the function pointers.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 16405b9bca13..d1f16a6c4810 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2329,6 +2329,7 @@ struct iw_cm_conn_param;
  * need to define the supported operations, otherwise they will be set to null.
  */
 struct ib_device_ops {
+	struct module *owner;
 	enum rdma_driver_id driver_id;
 	u32 uverbs_abi_ver;
 
@@ -2639,7 +2640,6 @@ struct ib_device {
 
 	int			      num_comp_vectors;
 
-	struct module               *owner;
 	union {
 		struct device		dev;
 		struct ib_core_device	coredev;

commit 72c6ec18eb6161c8fc672ae96ec5c77df4d07405
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 5 14:39:25 2019 -0300

    RDMA: Move uverbs_abi_ver into struct ib_device_ops
    
    No reason for every driver to emit code to set this, just make it part of
    the driver's existing static const ops structure.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dacf2b5ad862..16405b9bca13 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2330,6 +2330,7 @@ struct iw_cm_conn_param;
  */
 struct ib_device_ops {
 	enum rdma_driver_id driver_id;
+	u32 uverbs_abi_ver;
 
 	int (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,
 			 const struct ib_send_wr **bad_send_wr);
@@ -2650,7 +2651,6 @@ struct ib_device {
 	 */
 	const struct attribute_group	*groups[3];
 
-	int			     uverbs_abi_ver;
 	u64			     uverbs_cmd_mask;
 	u64			     uverbs_ex_cmd_mask;
 

commit b9560a419bfd498279333387817adcf5faef2825
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 5 14:39:24 2019 -0300

    RDMA: Move driver_id into struct ib_device_ops
    
    No reason for every driver to emit code to set this, just make it part of
    the driver's existing static const ops structure.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ec6446864b08..dacf2b5ad862 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2329,6 +2329,8 @@ struct iw_cm_conn_param;
  * need to define the supported operations, otherwise they will be set to null.
  */
 struct ib_device_ops {
+	enum rdma_driver_id driver_id;
+
 	int (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,
 			 const struct ib_send_wr **bad_send_wr);
 	int (*post_recv)(struct ib_qp *qp, const struct ib_recv_wr *recv_wr,
@@ -2672,7 +2674,6 @@ struct ib_device {
 	struct rdma_restrack_root *res;
 
 	const struct uapi_definition   *driver_def;
-	enum rdma_driver_id		driver_id;
 
 	/*
 	 * Positive refcount indicates that the device is currently

commit 890ac8d97e6722a9e4a66a0bd836d1b028d075fe
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon May 20 09:54:21 2019 +0300

    RDMA/core: Make ib_destroy_cq() void
    
    Kernel destroy CQ flows can't fail and the returned value of
    ib_destroy_cq() is not interested in those flows.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0742095355f2..ec6446864b08 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3858,9 +3858,9 @@ int ib_destroy_cq_user(struct ib_cq *cq, struct ib_udata *udata);
  *
  * NOTE: for user cq use ib_destroy_cq_user with valid udata!
  */
-static inline int ib_destroy_cq(struct ib_cq *cq)
+static inline void ib_destroy_cq(struct ib_cq *cq)
 {
-	return ib_destroy_cq_user(cq, NULL);
+	ib_destroy_cq_user(cq, NULL);
 }
 
 /**

commit dc1435c00fcd102c9803cd6120701ba5547138d5
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Fri May 17 15:43:10 2019 +0300

    RDMA/srp: Rename SRP sysfs name after IB device rename trigger
    
    SRP logic used device name and port index as symlink to relevant
    kobject. If the IB device is renamed then the prior name will be re-used
    by the next device plugged in and sysfs will panic as SRP will try to
    re-use the same name.
    
     mlx5_ib: Mellanox Connect-IB Infiniband driver v5.0-0
     sysfs: cannot create duplicate filename '/class/infiniband_srp/srp-mlx5_0-1'
     CPU: 3 PID: 1107 Comm: modprobe Not tainted 5.1.0-for-upstream-perf-2019-05-12_15-09-52-87 #1
     Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1ubuntu1 04/01/2014
     Call Trace:
      dump_stack+0x5a/0x73
      sysfs_warn_dup+0x58/0x70
      sysfs_do_create_link_sd.isra.2+0xa3/0xb0
      device_add+0x33f/0x660
      srp_add_one+0x301/0x4f0 [ib_srp]
      add_client_context+0x99/0xe0 [ib_core]
      enable_device_and_get+0xd1/0x1b0 [ib_core]
      ib_register_device+0x533/0x710 [ib_core]
      ? mutex_lock+0xe/0x30
      __mlx5_ib_add+0x23/0x70 [mlx5_ib]
      mlx5_add_device+0x4e/0xd0 [mlx5_core]
      mlx5_register_interface+0x85/0xc0 [mlx5_core]
      ? 0xffffffffa0791000
      do_one_initcall+0x4b/0x1cb
      ? kmem_cache_alloc_trace+0xc6/0x1d0
      ? do_init_module+0x22/0x21f
      do_init_module+0x5a/0x21f
      load_module+0x17f2/0x1ca0
      ? m_show+0x1c0/0x1c0
      __do_sys_finit_module+0x94/0xe0
      do_syscall_64+0x48/0x120
      entry_SYSCALL_64_after_hwframe+0x44/0xa9
     RIP: 0033:0x7f157cce10d9
    
    The module load/unload sequence was used to trigger such kernel panic:
     sudo modprobe ib_srp
     sudo modprobe -r mlx5_ib
     sudo modprobe -r mlx5_core
     sudo modprobe mlx5_core
    
    Have SRP track the name of the core device so that it can't have a name
    collision.
    
    Fixes: d21943dd19b5 ("RDMA/core: Implement IB device rename function")
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0742095355f2..54873085f2da 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2698,6 +2698,7 @@ struct ib_client {
 	const char *name;
 	void (*add)   (struct ib_device *);
 	void (*remove)(struct ib_device *, void *client_data);
+	void (*rename)(struct ib_device *dev, void *client_data);
 
 	/* Returns the net_dev belonging to this ib_client and matching the
 	 * given parameters.

commit f95be3d28d891b0c0f339a504e3aa8e382bbd9a6
Author: Gal Pressman <galpress@amazon.com>
Date:   Sun May 5 20:59:21 2019 +0300

    RDMA: Add EFA related definitions
    
    Add EFA driver ID to the IOCTL interface uapi. This patch also adds
    unspecified node/transport type that will be used by EFA (usnic is left
    unchanged as it's already part of our ABI).
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index deb67b21ccb9..0742095355f2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -140,6 +140,7 @@ enum rdma_node_type {
 	RDMA_NODE_RNIC,
 	RDMA_NODE_USNIC,
 	RDMA_NODE_USNIC_UDP,
+	RDMA_NODE_UNSPECIFIED,
 };
 
 enum {
@@ -151,7 +152,8 @@ enum rdma_transport_type {
 	RDMA_TRANSPORT_IB,
 	RDMA_TRANSPORT_IWARP,
 	RDMA_TRANSPORT_USNIC,
-	RDMA_TRANSPORT_USNIC_UDP
+	RDMA_TRANSPORT_USNIC_UDP,
+	RDMA_TRANSPORT_UNSPECIFIED,
 };
 
 enum rdma_protocol_type {

commit a808273a495c657e33281b181fd7fcc2bb28f662
Author: Shiraz Saleem <shiraz.saleem@intel.com>
Date:   Mon May 6 08:53:33 2019 -0500

    RDMA/verbs: Add a DMA iterator to return aligned contiguous memory blocks
    
    This helper iterates over a DMA-mapped SGL and returns contiguous memory
    blocks aligned to a HW supported page size.
    
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 03b07ec6a34b..deb67b21ccb9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2726,6 +2726,21 @@ struct ib_client {
 	u8 no_kverbs_req:1;
 };
 
+/*
+ * IB block DMA iterator
+ *
+ * Iterates the DMA-mapped SGL in contiguous memory blocks aligned
+ * to a HW supported page size.
+ */
+struct ib_block_iter {
+	/* internal states */
+	struct scatterlist *__sg;	/* sg holding the current aligned block */
+	dma_addr_t __dma_addr;		/* unaligned DMA address of this block */
+	unsigned int __sg_nents;	/* number of SG entries */
+	unsigned int __sg_advance;	/* number of bytes to advance in sg in next step */
+	unsigned int __pg_bit;		/* alignment of current block */
+};
+
 struct ib_device *_ib_alloc_device(size_t size);
 #define ib_alloc_device(drv_struct, member)                                    \
 	container_of(_ib_alloc_device(sizeof(struct drv_struct) +              \
@@ -2746,6 +2761,38 @@ void ib_unregister_device_queued(struct ib_device *ib_dev);
 int ib_register_client   (struct ib_client *client);
 void ib_unregister_client(struct ib_client *client);
 
+void __rdma_block_iter_start(struct ib_block_iter *biter,
+			     struct scatterlist *sglist,
+			     unsigned int nents,
+			     unsigned long pgsz);
+bool __rdma_block_iter_next(struct ib_block_iter *biter);
+
+/**
+ * rdma_block_iter_dma_address - get the aligned dma address of the current
+ * block held by the block iterator.
+ * @biter: block iterator holding the memory block
+ */
+static inline dma_addr_t
+rdma_block_iter_dma_address(struct ib_block_iter *biter)
+{
+	return biter->__dma_addr & ~(BIT_ULL(biter->__pg_bit) - 1);
+}
+
+/**
+ * rdma_for_each_block - iterate over contiguous memory blocks of the sg list
+ * @sglist: sglist to iterate over
+ * @biter: block iterator holding the memory block
+ * @nents: maximum number of sg entries to iterate over
+ * @pgsz: best HW supported page size to use
+ *
+ * Callers may use rdma_block_iter_dma_address() to get each
+ * blocks aligned DMA address.
+ */
+#define rdma_for_each_block(sglist, biter, nents, pgsz)		\
+	for (__rdma_block_iter_start(biter, sglist, nents,	\
+				     pgsz);			\
+	     __rdma_block_iter_next(biter);)
+
 /**
  * ib_get_client_data - Get IB client context
  * @device:Device to get context for

commit 4a35339958f16d42a4ca06a8da9d4b5ab39ee8ea
Author: Shiraz Saleem <shiraz.saleem@intel.com>
Date:   Mon May 6 08:53:32 2019 -0500

    RDMA/umem: Add API to find best driver supported page size in an MR
    
    This helper iterates through the SG list to find the best page size to use
    from a bitmap of HW supported page sizes. Drivers that support multiple
    page sizes, but not mixed sizes in an MR can use this API.
    
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8f8965f8ffdb..03b07ec6a34b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3250,6 +3250,30 @@ static inline bool rdma_cap_read_inv(struct ib_device *dev, u32 port_num)
 	return rdma_protocol_iwarp(dev, port_num);
 }
 
+/**
+ * rdma_find_pg_bit - Find page bit given address and HW supported page sizes
+ *
+ * @addr: address
+ * @pgsz_bitmap: bitmap of HW supported page sizes
+ */
+static inline unsigned int rdma_find_pg_bit(unsigned long addr,
+					    unsigned long pgsz_bitmap)
+{
+	unsigned long align;
+	unsigned long pgsz;
+
+	align = addr & -addr;
+
+	/* Find page bit such that addr is aligned to the highest supported
+	 * HW page size
+	 */
+	pgsz = pgsz_bitmap & ~(-align << 1);
+	if (!pgsz)
+		return __ffs(pgsz_bitmap);
+
+	return __fls(pgsz);
+}
+
 int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
 			 int state);
 int ib_get_vf_config(struct ib_device *device, int vf, u8 port,

commit 943bd984b108b3bb778790c2da4ae8d186b547e6
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu May 2 10:48:07 2019 +0300

    RDMA/core: Allow detaching gid attribute netdevice for RoCE
    
    When there is active traffic through a GID, a QP/AH holds reference to
    this GID entry. RoCE GID entry holds reference to its attached
    netdevice. Due to this when netdevice is deleted by admin user, its
    refcount is not dropped.
    
    Therefore, while deleting RoCE GID, wait for all GID attribute's netdev
    users to finish accessing netdev in rcu context.  Once all users done
    accessing it, release the netdev refcount.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4312899231ca..8f8965f8ffdb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -124,7 +124,7 @@ enum ib_gid_type {
 
 #define ROCE_V2_UDP_DPORT      4791
 struct ib_gid_attr {
-	struct net_device	*ndev;
+	struct net_device __rcu	*ndev;
 	struct ib_device	*device;
 	union ib_gid		gid;
 	enum ib_gid_type	gid_type;

commit dd05cb828d0ebecd3d772075fccb85ec3618bedf
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Mon Apr 29 14:59:06 2019 +0300

    RDMA: Get rid of iw_cm_verbs
    
    Integrate iw_cm_verbs data members into ib_device_ops and ib_device
    structs, this is done to achieve the following:
    
    1) Avoid memory related bugs durring error unwind
    2) Make the code more cleaner
    3) Reduce code duplication
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index de8724e5a727..4312899231ca 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2221,8 +2221,6 @@ struct ib_cache {
 	struct ib_event_handler event_handler;
 };
 
-struct iw_cm_verbs;
-
 struct ib_port_immutable {
 	int                           pkey_tbl_len;
 	int                           gid_tbl_len;
@@ -2304,6 +2302,8 @@ struct ib_counters_read_attr {
 };
 
 struct uverbs_attr_bundle;
+struct iw_cm_id;
+struct iw_cm_conn_param;
 
 #define INIT_RDMA_OBJ_SIZE(ib_struct, drv_struct, member)                      \
 	.size_##ib_struct =                                                    \
@@ -2581,6 +2581,19 @@ struct ib_device_ops {
 	 */
 	void (*dealloc_driver)(struct ib_device *dev);
 
+	/* iWarp CM callbacks */
+	void (*iw_add_ref)(struct ib_qp *qp);
+	void (*iw_rem_ref)(struct ib_qp *qp);
+	struct ib_qp *(*iw_get_qp)(struct ib_device *device, int qpn);
+	int (*iw_connect)(struct iw_cm_id *cm_id,
+			  struct iw_cm_conn_param *conn_param);
+	int (*iw_accept)(struct iw_cm_id *cm_id,
+			 struct iw_cm_conn_param *conn_param);
+	int (*iw_reject)(struct iw_cm_id *cm_id, const void *pdata,
+			 u8 pdata_len);
+	int (*iw_create_listen)(struct iw_cm_id *cm_id, int backlog);
+	int (*iw_destroy_listen)(struct iw_cm_id *cm_id);
+
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 	DECLARE_RDMA_OBJ_SIZE(ib_srq);
@@ -2621,8 +2634,6 @@ struct ib_device {
 
 	int			      num_comp_vectors;
 
-	struct iw_cm_verbs	     *iwcm;
-
 	struct module               *owner;
 	union {
 		struct device		dev;
@@ -2675,6 +2686,10 @@ struct ib_device {
 	struct mutex compat_devs_mutex;
 	/* Maintains compat devices for each net namespace */
 	struct xarray compat_devs;
+
+	/* Used by iWarp CM */
+	char iw_ifname[IFNAMSIZ];
+	u32 iw_driver_flags;
 };
 
 struct ib_client {

commit 923abb9d797ba078f4e9eb3734dd71be5f567a2a
Author: Gal Pressman <galpress@amazon.com>
Date:   Wed May 1 13:48:13 2019 +0300

    RDMA/core: Introduce RDMA subsystem ibdev_* print functions
    
    Similarly to dev/netdev/etc printk helpers, add standard printk helpers
    for the RDMA subsystem.
    
    Example output:
    efa 0000:00:06.0 efa_0: Hello World!
    efa_0: Hello World! (no parent device set)
    (NULL ib_device): Hello World! (ibdev is NULL)
    
    Cc: Jason Baron <jbaron@akamai.com>
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Suggested-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 737ef5ed3930..de8724e5a727 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -74,6 +74,36 @@ extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
 extern struct workqueue_struct *ib_comp_unbound_wq;
 
+__printf(3, 4) __cold
+void ibdev_printk(const char *level, const struct ib_device *ibdev,
+		  const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_emerg(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_alert(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_crit(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_err(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_warn(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_notice(const struct ib_device *ibdev, const char *format, ...);
+__printf(2, 3) __cold
+void ibdev_info(const struct ib_device *ibdev, const char *format, ...);
+
+#if defined(CONFIG_DYNAMIC_DEBUG)
+#define ibdev_dbg(__dev, format, args...)                       \
+	dynamic_ibdev_dbg(__dev, format, ##args)
+#elif defined(DEBUG)
+#define ibdev_dbg(__dev, format, args...)                       \
+	ibdev_printk(KERN_DEBUG, __dev, format, ##args)
+#else
+__printf(2, 3) __cold
+static inline
+void ibdev_dbg(const struct ib_device *ibdev, const char *format, ...) {}
+#endif
+
 union ib_gid {
 	u8	raw[16];
 	struct {

commit 449a224c10a48d047c799c5c5d3b22d6aec98c60
Merge: 3c176c9d7244 4eb6ab13b991
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Apr 24 16:20:34 2019 -0300

    Merge branch 'rdma_mmap' into rdma.git for-next
    
    Jason Gunthorpe says:
    
    ====================
    Upon review it turns out there are some long standing problems in BAR
    mapping area:
     * BAR pages intended for read-only can be switched to writable via mprotect.
     * Missing use of rdma_user_mmap_io for the mlx5 clock BAR page.
     * Disassociate causes SIGBUS when touching the pages.
     * CPU pages are being mapped through to the process via remap_pfn_range
       instead of the more appropriate vm_insert_page, causing weird behaviors
       during disassociation.
    
    This series adds the missing VM_* flag manipulation, adds faulting a zero
    page for disassociation and revises the CPU page mappings to use
    vm_insert_page.
    ====================
    
    For dependencies this branch is based on for-rc from
    git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git
    
    * branch 'rdma_mmap':
      RDMA: Remove rdma_user_mmap_page
      RDMA/mlx5: Use get_zeroed_page() for clock_info
      RDMA/ucontext: Fix regression with disassociate
      RDMA/mlx5: Use rdma_user_map_io for mapping BAR pages
      RDMA/mlx5: Do not allow the user to write to the clock page
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 4eb6ab13b99148b5bf9bfdae7977fe139b4452f8
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Apr 16 14:07:30 2019 +0300

    RDMA: Remove rdma_user_mmap_page
    
    Upon further research drivers that want this should simply call the core
    function vm_insert_page(). The VMA holds a reference on the page and it
    will be automatically freed when the last reference drops. No need for
    disassociate to sequence the cleanup.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9b9e17bcc201..7ca908d5c0c3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2705,9 +2705,6 @@ void ib_set_device_ops(struct ib_device *device,
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
 		      unsigned long pfn, unsigned long size, pgprot_t prot);
-int rdma_user_mmap_page(struct ib_ucontext *ucontext,
-			struct vm_area_struct *vma, struct page *page,
-			unsigned long size);
 #else
 static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
 				    struct vm_area_struct *vma,
@@ -2716,12 +2713,6 @@ static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
 {
 	return -EINVAL;
 }
-static inline int rdma_user_mmap_page(struct ib_ucontext *ucontext,
-				struct vm_area_struct *vma, struct page *page,
-				unsigned long size)
-{
-	return -EINVAL;
-}
 #endif
 
 static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)

commit 68e326dea1dba935f6a5299a24343a58b33eed10
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:43 2019 +0300

    RDMA: Handle SRQ allocations by IB/core
    
    Convert SRQ allocation from drivers to be in the IB/core
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3232a84c4fdb..43a75ab8ea8a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2406,14 +2406,14 @@ struct ib_device_ops {
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	void (*destroy_ah)(struct ib_ah *ah, u32 flags);
-	struct ib_srq *(*create_srq)(struct ib_pd *pd,
-				     struct ib_srq_init_attr *srq_init_attr,
-				     struct ib_udata *udata);
+	int (*create_srq)(struct ib_srq *srq,
+			  struct ib_srq_init_attr *srq_init_attr,
+			  struct ib_udata *udata);
 	int (*modify_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr,
 			  enum ib_srq_attr_mask srq_attr_mask,
 			  struct ib_udata *udata);
 	int (*query_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr);
-	int (*destroy_srq)(struct ib_srq *srq, struct ib_udata *udata);
+	void (*destroy_srq)(struct ib_srq *srq, struct ib_udata *udata);
 	struct ib_qp *(*create_qp)(struct ib_pd *pd,
 				   struct ib_qp_init_attr *qp_init_attr,
 				   struct ib_udata *udata);
@@ -2553,6 +2553,7 @@ struct ib_device_ops {
 
 	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
+	DECLARE_RDMA_OBJ_SIZE(ib_srq);
 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
 };
 

commit d345691471b426e540140a4cc431c69f80abfcb6
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:42 2019 +0300

    RDMA: Handle AH allocations by IB/core
    
    Simplify drivers by ensuring lifetime of ib_ah object. The changes
    in .create_ah() go hand in hand with relevant update in .destroy_ah().
    
    We will use this opportunity and convert .destroy_ah() to don't fail, as
    it was suggested a long time ago, because there is nothing to do in case
    of failure during destroy.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7e965bc06477..3232a84c4fdb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2401,12 +2401,11 @@ struct ib_device_ops {
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	int (*alloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	void (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
-	struct ib_ah *(*create_ah)(struct ib_pd *pd,
-				   struct rdma_ah_attr *ah_attr, u32 flags,
-				   struct ib_udata *udata);
+	int (*create_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr,
+			 u32 flags, struct ib_udata *udata);
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
-	int (*destroy_ah)(struct ib_ah *ah, u32 flags, struct ib_udata *udata);
+	void (*destroy_ah)(struct ib_ah *ah, u32 flags);
 	struct ib_srq *(*create_srq)(struct ib_pd *pd,
 				     struct ib_srq_init_attr *srq_init_attr,
 				     struct ib_udata *udata);
@@ -2552,6 +2551,7 @@ struct ib_device_ops {
 	 */
 	void (*dealloc_driver)(struct ib_device *dev);
 
+	DECLARE_RDMA_OBJ_SIZE(ib_ah);
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
 };

commit f6316032fd3243d3544603d94f237b976f90bb73
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Thu Mar 28 15:12:58 2019 +0200

    RDMA/core: Support object allocation in atomic context
    
    AH objects are allocated in atomic context and those allocations should
    be done with GFP_ATOMIC.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0e24f6b6c61d..7e965bc06477 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -59,6 +59,8 @@
 #include <linux/mmu_notifier.h>
 #include <linux/uaccess.h>
 #include <linux/cgroup_rdma.h>
+#include <linux/irqflags.h>
+#include <linux/preempt.h>
 #include <uapi/rdma/ib_user_verbs.h>
 #include <rdma/restrack.h>
 #include <uapi/rdma/rdma_user_ioctl.h>
@@ -2281,8 +2283,11 @@ struct uverbs_attr_bundle;
 			 !__same_type(((struct drv_struct *)NULL)->member,     \
 				      struct ib_struct)))
 
+#define rdma_zalloc_drv_obj_gfp(ib_dev, ib_type, gfp)                         \
+	((struct ib_type *)kzalloc(ib_dev->ops.size_##ib_type, gfp))
+
 #define rdma_zalloc_drv_obj(ib_dev, ib_type)                                   \
-	((struct ib_type *)kzalloc(ib_dev->ops.size_##ib_type, GFP_KERNEL))
+	rdma_zalloc_drv_obj_gfp(ib_dev, ib_type, GFP_KERNEL)
 
 #define DECLARE_RDMA_OBJ_SIZE(ib_struct) size_t size_##ib_struct
 

commit ff23dfa134576e071ace69e91761d229a0f73139
Author: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
Date:   Sun Mar 31 19:10:07 2019 +0300

    IB: Pass only ib_udata in function prototypes
    
    Now when ib_udata is passed to all the driver's object create/destroy APIs
    the ib_udata will carry the ib_ucontext for every user command. There is
    no need to also pass the ib_ucontext via the functions prototypes.
    
    Make ib_udata the only argument psssed.
    
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 54e48dd36644..0e24f6b6c61d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2394,8 +2394,7 @@ struct ib_device_ops {
 	void (*dealloc_ucontext)(struct ib_ucontext *context);
 	int (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
-	int (*alloc_pd)(struct ib_pd *pd, struct ib_ucontext *context,
-			struct ib_udata *udata);
+	int (*alloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	void (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	struct ib_ah *(*create_ah)(struct ib_pd *pd,
 				   struct rdma_ah_attr *ah_attr, u32 flags,
@@ -2421,7 +2420,6 @@ struct ib_device_ops {
 	int (*destroy_qp)(struct ib_qp *qp, struct ib_udata *udata);
 	struct ib_cq *(*create_cq)(struct ib_device *device,
 				   const struct ib_cq_init_attr *attr,
-				   struct ib_ucontext *context,
 				   struct ib_udata *udata);
 	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
 	int (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
@@ -2456,7 +2454,6 @@ struct ib_device_ops {
 	int (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	int (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,
-				      struct ib_ucontext *ucontext,
 				      struct ib_udata *udata);
 	int (*dealloc_xrcd)(struct ib_xrcd *xrcd, struct ib_udata *udata);
 	struct ib_flow *(*create_flow)(struct ib_qp *qp,

commit c4367a26357be501338e41ceae7ebb7ce57064e5
Author: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
Date:   Sun Mar 31 19:10:05 2019 +0300

    IB: Pass uverbs_attr_bundle down ib_x destroy path
    
    The uverbs_attr_bundle with the ucontext is sent down to the drivers ib_x
    destroy path as ib_udata. The next patch will use the ib_udata to free the
    drivers destroy path from the dependency in 'uobject->context' as we
    already did for the create path.
    
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3b6eb646066c..54e48dd36644 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2396,13 +2396,13 @@ struct ib_device_ops {
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	int (*alloc_pd)(struct ib_pd *pd, struct ib_ucontext *context,
 			struct ib_udata *udata);
-	void (*dealloc_pd)(struct ib_pd *pd);
+	void (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);
 	struct ib_ah *(*create_ah)(struct ib_pd *pd,
 				   struct rdma_ah_attr *ah_attr, u32 flags,
 				   struct ib_udata *udata);
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
-	int (*destroy_ah)(struct ib_ah *ah, u32 flags);
+	int (*destroy_ah)(struct ib_ah *ah, u32 flags, struct ib_udata *udata);
 	struct ib_srq *(*create_srq)(struct ib_pd *pd,
 				     struct ib_srq_init_attr *srq_init_attr,
 				     struct ib_udata *udata);
@@ -2410,7 +2410,7 @@ struct ib_device_ops {
 			  enum ib_srq_attr_mask srq_attr_mask,
 			  struct ib_udata *udata);
 	int (*query_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr);
-	int (*destroy_srq)(struct ib_srq *srq);
+	int (*destroy_srq)(struct ib_srq *srq, struct ib_udata *udata);
 	struct ib_qp *(*create_qp)(struct ib_pd *pd,
 				   struct ib_qp_init_attr *qp_init_attr,
 				   struct ib_udata *udata);
@@ -2418,13 +2418,13 @@ struct ib_device_ops {
 			 int qp_attr_mask, struct ib_udata *udata);
 	int (*query_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
 			int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr);
-	int (*destroy_qp)(struct ib_qp *qp);
+	int (*destroy_qp)(struct ib_qp *qp, struct ib_udata *udata);
 	struct ib_cq *(*create_cq)(struct ib_device *device,
 				   const struct ib_cq_init_attr *attr,
 				   struct ib_ucontext *context,
 				   struct ib_udata *udata);
 	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
-	int (*destroy_cq)(struct ib_cq *cq);
+	int (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);
 	int (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);
 	struct ib_mr *(*get_dma_mr)(struct ib_pd *pd, int mr_access_flags);
 	struct ib_mr *(*reg_user_mr)(struct ib_pd *pd, u64 start, u64 length,
@@ -2433,9 +2433,9 @@ struct ib_device_ops {
 	int (*rereg_user_mr)(struct ib_mr *mr, int flags, u64 start, u64 length,
 			     u64 virt_addr, int mr_access_flags,
 			     struct ib_pd *pd, struct ib_udata *udata);
-	int (*dereg_mr)(struct ib_mr *mr);
+	int (*dereg_mr)(struct ib_mr *mr, struct ib_udata *udata);
 	struct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,
-				  u32 max_num_sg);
+				  u32 max_num_sg, struct ib_udata *udata);
 	int (*advise_mr)(struct ib_pd *pd,
 			 enum ib_uverbs_advise_mr_advice advice, u32 flags,
 			 struct ib_sge *sg_list, u32 num_sge,
@@ -2458,7 +2458,7 @@ struct ib_device_ops {
 	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,
 				      struct ib_ucontext *ucontext,
 				      struct ib_udata *udata);
-	int (*dealloc_xrcd)(struct ib_xrcd *xrcd);
+	int (*dealloc_xrcd)(struct ib_xrcd *xrcd, struct ib_udata *udata);
 	struct ib_flow *(*create_flow)(struct ib_qp *qp,
 				       struct ib_flow_attr *flow_attr,
 				       int domain, struct ib_udata *udata);
@@ -2483,7 +2483,7 @@ struct ib_device_ops {
 	struct ib_wq *(*create_wq)(struct ib_pd *pd,
 				   struct ib_wq_init_attr *init_attr,
 				   struct ib_udata *udata);
-	int (*destroy_wq)(struct ib_wq *wq);
+	int (*destroy_wq)(struct ib_wq *wq, struct ib_udata *udata);
 	int (*modify_wq)(struct ib_wq *wq, struct ib_wq_attr *attr,
 			 u32 wq_attr_mask, struct ib_udata *udata);
 	struct ib_rwq_ind_table *(*create_rwq_ind_table)(
@@ -2495,7 +2495,7 @@ struct ib_device_ops {
 				  struct ib_ucontext *context,
 				  struct ib_dm_alloc_attr *attr,
 				  struct uverbs_attr_bundle *attrs);
-	int (*dealloc_dm)(struct ib_dm *dm);
+	int (*dealloc_dm)(struct ib_dm *dm, struct uverbs_attr_bundle *attrs);
 	struct ib_mr *(*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,
 				   struct ib_dm_mr_attr *attr,
 				   struct uverbs_attr_bundle *attrs);
@@ -3252,9 +3252,27 @@ enum ib_pd_flags {
 
 struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,
 		const char *caller);
+
 #define ib_alloc_pd(device, flags) \
 	__ib_alloc_pd((device), (flags), KBUILD_MODNAME)
-void ib_dealloc_pd(struct ib_pd *pd);
+
+/**
+ * ib_dealloc_pd_user - Deallocate kernel/user PD
+ * @pd: The protection domain
+ * @udata: Valid user data or NULL for kernel objects
+ */
+void ib_dealloc_pd_user(struct ib_pd *pd, struct ib_udata *udata);
+
+/**
+ * ib_dealloc_pd - Deallocate kernel PD
+ * @pd: The protection domain
+ *
+ * NOTE: for user PD use ib_dealloc_pd_user with valid udata!
+ */
+static inline void ib_dealloc_pd(struct ib_pd *pd)
+{
+	ib_dealloc_pd_user(pd, NULL);
+}
 
 enum rdma_create_ah_flags {
 	/* In a sleepable context */
@@ -3367,11 +3385,24 @@ enum rdma_destroy_ah_flags {
 };
 
 /**
- * rdma_destroy_ah - Destroys an address handle.
+ * rdma_destroy_ah_user - Destroys an address handle.
  * @ah: The address handle to destroy.
  * @flags: Destroy address handle flags (see enum rdma_destroy_ah_flags).
+ * @udata: Valid user data or NULL for kernel objects
  */
-int rdma_destroy_ah(struct ib_ah *ah, u32 flags);
+int rdma_destroy_ah_user(struct ib_ah *ah, u32 flags, struct ib_udata *udata);
+
+/**
+ * rdma_destroy_ah - Destroys an kernel address handle.
+ * @ah: The address handle to destroy.
+ * @flags: Destroy address handle flags (see enum rdma_destroy_ah_flags).
+ *
+ * NOTE: for user ah use rdma_destroy_ah_user with valid udata!
+ */
+static inline int rdma_destroy_ah(struct ib_ah *ah, u32 flags)
+{
+	return rdma_destroy_ah_user(ah, flags, NULL);
+}
 
 /**
  * ib_create_srq - Creates a SRQ associated with the specified protection
@@ -3415,10 +3446,22 @@ int ib_query_srq(struct ib_srq *srq,
 		 struct ib_srq_attr *srq_attr);
 
 /**
- * ib_destroy_srq - Destroys the specified SRQ.
+ * ib_destroy_srq_user - Destroys the specified SRQ.
+ * @srq: The SRQ to destroy.
+ * @udata: Valid user data or NULL for kernel objects
+ */
+int ib_destroy_srq_user(struct ib_srq *srq, struct ib_udata *udata);
+
+/**
+ * ib_destroy_srq - Destroys the specified kernel SRQ.
  * @srq: The SRQ to destroy.
+ *
+ * NOTE: for user srq use ib_destroy_srq_user with valid udata!
  */
-int ib_destroy_srq(struct ib_srq *srq);
+static inline int ib_destroy_srq(struct ib_srq *srq)
+{
+	return ib_destroy_srq_user(srq, NULL);
+}
 
 /**
  * ib_post_srq_recv - Posts a list of work requests to the specified SRQ.
@@ -3438,15 +3481,34 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
 }
 
 /**
- * ib_create_qp - Creates a QP associated with the specified protection
+ * ib_create_qp_user - Creates a QP associated with the specified protection
  *   domain.
  * @pd: The protection domain associated with the QP.
  * @qp_init_attr: A list of initial attributes required to create the
  *   QP.  If QP creation succeeds, then the attributes are updated to
  *   the actual capabilities of the created QP.
+ * @udata: Valid user data or NULL for kernel objects
  */
-struct ib_qp *ib_create_qp(struct ib_pd *pd,
-			   struct ib_qp_init_attr *qp_init_attr);
+struct ib_qp *ib_create_qp_user(struct ib_pd *pd,
+				struct ib_qp_init_attr *qp_init_attr,
+				struct ib_udata *udata);
+
+/**
+ * ib_create_qp - Creates a kernel QP associated with the specified protection
+ *   domain.
+ * @pd: The protection domain associated with the QP.
+ * @qp_init_attr: A list of initial attributes required to create the
+ *   QP.  If QP creation succeeds, then the attributes are updated to
+ *   the actual capabilities of the created QP.
+ * @udata: Valid user data or NULL for kernel objects
+ *
+ * NOTE: for user qp use ib_create_qp_user with valid udata!
+ */
+static inline struct ib_qp *ib_create_qp(struct ib_pd *pd,
+					 struct ib_qp_init_attr *qp_init_attr)
+{
+	return ib_create_qp_user(pd, qp_init_attr, NULL);
+}
 
 /**
  * ib_modify_qp_with_udata - Modifies the attributes for the specified QP.
@@ -3496,8 +3558,20 @@ int ib_query_qp(struct ib_qp *qp,
 /**
  * ib_destroy_qp - Destroys the specified QP.
  * @qp: The QP to destroy.
+ * @udata: Valid udata or NULL for kernel objects
  */
-int ib_destroy_qp(struct ib_qp *qp);
+int ib_destroy_qp_user(struct ib_qp *qp, struct ib_udata *udata);
+
+/**
+ * ib_destroy_qp - Destroys the specified kernel QP.
+ * @qp: The QP to destroy.
+ *
+ * NOTE: for user qp use ib_destroy_qp_user with valid udata!
+ */
+static inline int ib_destroy_qp(struct ib_qp *qp)
+{
+	return ib_destroy_qp_user(qp, NULL);
+}
 
 /**
  * ib_open_qp - Obtain a reference to an existing sharable QP.
@@ -3557,13 +3631,66 @@ static inline int ib_post_recv(struct ib_qp *qp,
 	return qp->device->ops.post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);
 }
 
-struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
-			    int nr_cqe, int comp_vector,
-			    enum ib_poll_context poll_ctx, const char *caller);
-#define ib_alloc_cq(device, priv, nr_cqe, comp_vect, poll_ctx) \
-	__ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
+struct ib_cq *__ib_alloc_cq_user(struct ib_device *dev, void *private,
+				 int nr_cqe, int comp_vector,
+				 enum ib_poll_context poll_ctx,
+				 const char *caller, struct ib_udata *udata);
+
+/**
+ * ib_alloc_cq_user: Allocate kernel/user CQ
+ * @dev: The IB device
+ * @private: Private data attached to the CQE
+ * @nr_cqe: Number of CQEs in the CQ
+ * @comp_vector: Completion vector used for the IRQs
+ * @poll_ctx: Context used for polling the CQ
+ * @udata: Valid user data or NULL for kernel objects
+ */
+static inline struct ib_cq *ib_alloc_cq_user(struct ib_device *dev,
+					     void *private, int nr_cqe,
+					     int comp_vector,
+					     enum ib_poll_context poll_ctx,
+					     struct ib_udata *udata)
+{
+	return __ib_alloc_cq_user(dev, private, nr_cqe, comp_vector, poll_ctx,
+				  KBUILD_MODNAME, udata);
+}
+
+/**
+ * ib_alloc_cq: Allocate kernel CQ
+ * @dev: The IB device
+ * @private: Private data attached to the CQE
+ * @nr_cqe: Number of CQEs in the CQ
+ * @comp_vector: Completion vector used for the IRQs
+ * @poll_ctx: Context used for polling the CQ
+ *
+ * NOTE: for user cq use ib_alloc_cq_user with valid udata!
+ */
+static inline struct ib_cq *ib_alloc_cq(struct ib_device *dev, void *private,
+					int nr_cqe, int comp_vector,
+					enum ib_poll_context poll_ctx)
+{
+	return ib_alloc_cq_user(dev, private, nr_cqe, comp_vector, poll_ctx,
+				NULL);
+}
+
+/**
+ * ib_free_cq_user - Free kernel/user CQ
+ * @cq: The CQ to free
+ * @udata: Valid user data or NULL for kernel objects
+ */
+void ib_free_cq_user(struct ib_cq *cq, struct ib_udata *udata);
+
+/**
+ * ib_free_cq - Free kernel CQ
+ * @cq: The CQ to free
+ *
+ * NOTE: for user cq use ib_free_cq_user with valid udata!
+ */
+static inline void ib_free_cq(struct ib_cq *cq)
+{
+	ib_free_cq_user(cq, NULL);
+}
 
-void ib_free_cq(struct ib_cq *cq);
 int ib_process_cq_direct(struct ib_cq *cq, int budget);
 
 /**
@@ -3607,10 +3734,22 @@ int ib_resize_cq(struct ib_cq *cq, int cqe);
 int rdma_set_cq_moderation(struct ib_cq *cq, u16 cq_count, u16 cq_period);
 
 /**
- * ib_destroy_cq - Destroys the specified CQ.
+ * ib_destroy_cq_user - Destroys the specified CQ.
  * @cq: The CQ to destroy.
+ * @udata: Valid user data or NULL for kernel objects
  */
-int ib_destroy_cq(struct ib_cq *cq);
+int ib_destroy_cq_user(struct ib_cq *cq, struct ib_udata *udata);
+
+/**
+ * ib_destroy_cq - Destroys the specified kernel CQ.
+ * @cq: The CQ to destroy.
+ *
+ * NOTE: for user cq use ib_destroy_cq_user with valid udata!
+ */
+static inline int ib_destroy_cq(struct ib_cq *cq)
+{
+	return ib_destroy_cq_user(cq, NULL);
+}
 
 /**
  * ib_poll_cq - poll a CQ for completion(s)
@@ -3864,17 +4003,37 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 }
 
 /**
- * ib_dereg_mr - Deregisters a memory region and removes it from the
+ * ib_dereg_mr_user - Deregisters a memory region and removes it from the
+ *   HCA translation table.
+ * @mr: The memory region to deregister.
+ * @udata: Valid user data or NULL for kernel object
+ *
+ * This function can fail, if the memory region has memory windows bound to it.
+ */
+int ib_dereg_mr_user(struct ib_mr *mr, struct ib_udata *udata);
+
+/**
+ * ib_dereg_mr - Deregisters a kernel memory region and removes it from the
  *   HCA translation table.
  * @mr: The memory region to deregister.
  *
  * This function can fail, if the memory region has memory windows bound to it.
+ *
+ * NOTE: for user mr use ib_dereg_mr_user with valid udata!
  */
-int ib_dereg_mr(struct ib_mr *mr);
+static inline int ib_dereg_mr(struct ib_mr *mr)
+{
+	return ib_dereg_mr_user(mr, NULL);
+}
+
+struct ib_mr *ib_alloc_mr_user(struct ib_pd *pd, enum ib_mr_type mr_type,
+			       u32 max_num_sg, struct ib_udata *udata);
 
-struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
-			  enum ib_mr_type mr_type,
-			  u32 max_num_sg);
+static inline struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
+					enum ib_mr_type mr_type, u32 max_num_sg)
+{
+	return ib_alloc_mr_user(pd, mr_type, max_num_sg, NULL);
+}
 
 /**
  * ib_update_fast_reg_key - updates the key portion of the fast_reg MR
@@ -3972,8 +4131,9 @@ struct ib_xrcd *__ib_alloc_xrcd(struct ib_device *device, const char *caller);
 /**
  * ib_dealloc_xrcd - Deallocates an XRC domain.
  * @xrcd: The XRC domain to deallocate.
+ * @udata: Valid user data or NULL for kernel object
  */
-int ib_dealloc_xrcd(struct ib_xrcd *xrcd);
+int ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);
 
 static inline int ib_check_mr_access(int flags)
 {
@@ -4049,7 +4209,7 @@ struct net_device *ib_device_netdev(struct ib_device *dev, u8 port);
 
 struct ib_wq *ib_create_wq(struct ib_pd *pd,
 			   struct ib_wq_init_attr *init_attr);
-int ib_destroy_wq(struct ib_wq *wq);
+int ib_destroy_wq(struct ib_wq *wq, struct ib_udata *udata);
 int ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *attr,
 		 u32 wq_attr_mask);
 struct ib_rwq_ind_table *ib_create_rwq_ind_table(struct ib_device *device,

commit d3243da8e3700eaccb41b93b498d0dfc77c90d37
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Mar 10 17:27:46 2019 +0200

    RDMA/core: Don't compare specific bit after boolean AND
    
    There is no need to perform extra comparison after boolean AND.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 418d17c8b65b..3b6eb646066c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2994,8 +2994,8 @@ static inline bool rdma_cap_ib_mad(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_opa_mad(struct ib_device *device, u8 port_num)
 {
-	return (device->port_data[port_num].immutable.core_cap_flags &
-		RDMA_CORE_CAP_OPA_MAD) == RDMA_CORE_CAP_OPA_MAD;
+	return device->port_data[port_num].immutable.core_cap_flags &
+		RDMA_CORE_CAP_OPA_MAD;
 }
 
 /**

commit 41c6140189afdf67bd07d7bbe2d8f9382b6f9ef7
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Feb 26 14:01:46 2019 +0200

    RDMA: Check net namespace access for uverbs, umad, cma and nldev
    
    Introduce an API rdma_dev_access_netns() to check whether a rdma device
    can be accessed from the specified net namespace or not.
    Use rdma_dev_access_netns() while opening character uverbs, umad network
    device and also check while rdma cm_id binds to rdma device.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d42267e72c4b..418d17c8b65b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4381,4 +4381,7 @@ static inline struct ib_device *rdma_device_to_ibdev(struct device *device)
  */
 #define rdma_device_to_drv_device(dev, drv_dev_struct, ibdev_member)           \
 	container_of(rdma_device_to_ibdev(dev), drv_dev_struct, ibdev_member)
+
+bool rdma_dev_access_netns(const struct ib_device *device,
+			   const struct net *net);
 #endif /* IB_VERBS_H */

commit 4e0f7b9070726a34bbd87a74e407d4cced6d49ab
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Feb 26 13:56:13 2019 +0200

    RDMA/core: Implement compat device/sysfs tree in net namespace
    
    Implement compatibility layer sysfs entries of ib_core so that non
    init_net net namespaces can also discover rdma devices.
    
    Each non init_net net namespace has ib_core_device created in it.
    Such ib_core_device sysfs tree resembles rdma devices found in
    init_net namespace.
    
    This allows discovering rdma devices in multiple non init_net net
    namespaces via sysfs entries and helpful to rdma-core userspace.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5f9f4fcdc4cc..d42267e72c4b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2559,6 +2559,7 @@ struct ib_core_device {
 	 * union of ib_core_device and device exists in ib_device.
 	 */
 	struct device dev;
+	possible_net_t rdma_net;
 	struct kobject *ports_kobj;
 	struct list_head port_list;
 	struct ib_device *owner; /* reach back to owner ib_device */
@@ -2636,6 +2637,11 @@ struct ib_device {
 	struct work_struct unregistration_work;
 
 	const struct rdma_link_ops *link_ops;
+
+	/* Protects compat_devs xarray modifications */
+	struct mutex compat_devs_mutex;
+	/* Maintains compat devices for each net namespace */
+	struct xarray compat_devs;
 };
 
 struct ib_client {

commit cebe556bd755d16559c8bc0d1fe5545db6bbeaf0
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Feb 26 13:56:11 2019 +0200

    RDMA/core: Introduce ib_core_device to hold device
    
    In order to support sysfs entries in multiple net namespaces for a rdma
    device, introduce a ib_core_device whose scope is limited to hold core
    device and per port sysfs related entries.
    
    This is preparation patch so that multiple ib_core_devices in each net
    namespace can be created in subsequent patch who all can share ib_device.
    
    (a) Move sysfs specific fields to ib_core_device.
    (b) Make sysfs and device life cycle related routines to work on
        ib_core_device.
    (c) Introduce and use rdma_init_coredev() helper to initialize
        coredev fields.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9b9e17bcc201..5f9f4fcdc4cc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2554,8 +2554,17 @@ struct ib_device_ops {
 	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
 };
 
-struct rdma_restrack_root;
+struct ib_core_device {
+	/* device must be the first element in structure until,
+	 * union of ib_core_device and device exists in ib_device.
+	 */
+	struct device dev;
+	struct kobject *ports_kobj;
+	struct list_head port_list;
+	struct ib_device *owner; /* reach back to owner ib_device */
+};
 
+struct rdma_restrack_root;
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -2581,16 +2590,17 @@ struct ib_device {
 	struct iw_cm_verbs	     *iwcm;
 
 	struct module               *owner;
-	struct device                dev;
+	union {
+		struct device		dev;
+		struct ib_core_device	coredev;
+	};
+
 	/* First group for device attributes,
 	 * Second group for driver provided attributes (optional).
 	 * It is NULL terminated array.
 	 */
 	const struct attribute_group	*groups[3];
 
-	struct kobject			*ports_kobj;
-	struct list_head             port_list;
-
 	int			     uverbs_abi_ver;
 	u64			     uverbs_cmd_mask;
 	u64			     uverbs_ex_cmd_mask;
@@ -4349,7 +4359,10 @@ rdma_set_device_sysfs_group(struct ib_device *dev,
  */
 static inline struct ib_device *rdma_device_to_ibdev(struct device *device)
 {
-	return container_of(device, struct ib_device, dev);
+	struct ib_core_device *coredev =
+		container_of(device, struct ib_core_device, dev);
+
+	return coredev->owner;
 }
 
 /**

commit a2a074ef396f8738d9ee08ceefa8811381a4fe4f
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Feb 12 20:39:16 2019 +0200

    RDMA: Handle ucontext allocations by IB/core
    
    Following the PD conversion patch, do the same for ucontext allocations.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 225cb76d469f..9b9e17bcc201 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2389,9 +2389,9 @@ struct ib_device_ops {
 	int (*del_gid)(const struct ib_gid_attr *attr, void **context);
 	int (*query_pkey)(struct ib_device *device, u8 port_num, u16 index,
 			  u16 *pkey);
-	struct ib_ucontext *(*alloc_ucontext)(struct ib_device *device,
-					      struct ib_udata *udata);
-	int (*dealloc_ucontext)(struct ib_ucontext *context);
+	int (*alloc_ucontext)(struct ib_ucontext *context,
+			      struct ib_udata *udata);
+	void (*dealloc_ucontext)(struct ib_ucontext *context);
 	int (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	int (*alloc_pd)(struct ib_pd *pd, struct ib_ucontext *context,
@@ -2551,6 +2551,7 @@ struct ib_device_ops {
 	void (*dealloc_driver)(struct ib_device *dev);
 
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
+	DECLARE_RDMA_OBJ_SIZE(ib_ucontext);
 };
 
 struct rdma_restrack_root;

commit 3856ec4b93c9463d36ee39098dde1fbbd29ec6dd
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Fri Feb 15 11:03:53 2019 -0800

    RDMA/core: Add RDMA_NLDEV_CMD_NEWLINK/DELLINK support
    
    Add support for new LINK messages to allow adding and deleting rdma
    interfaces.  This will be used initially for soft rdma drivers which
    instantiate device instances dynamically by the admin specifying a netdev
    device to use.  The rdma_rxe module will be the first user of these
    messages.
    
    The design is modeled after RTNL_NEWLINK/DELLINK: rdma drivers register
    with the rdma core if they provide link add/delete functions.  Each driver
    registers with a unique "type" string, that is used to dispatch messages
    coming from user space.  A new RDMA_NLDEV_ATTR is defined for the "type"
    string.  User mode will pass 3 attributes in a NEWLINK message:
    RDMA_NLDEV_ATTR_DEV_NAME for the desired rdma device name to be created,
    RDMA_NLDEV_ATTR_LINK_TYPE for the "type" of link being added, and
    RDMA_NLDEV_ATTR_NDEV_NAME for the net_device interface to use for this
    link.  The DELLINK message will contain the RDMA_NLDEV_ATTR_DEV_INDEX of
    the device to delete.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 640263289ab9..225cb76d469f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -238,6 +238,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_RDMA_NETDEV_OPA_VNIC		= (1ULL << 35),
 	/* The device supports padding incoming writes to cacheline. */
 	IB_DEVICE_PCI_WRITE_END_PADDING		= (1ULL << 36),
+	IB_DEVICE_ALLOW_USER_UNREG		= (1ULL << 37),
 };
 
 enum ib_signature_prot_cap {
@@ -2622,6 +2623,8 @@ struct ib_device {
 	refcount_t refcount;
 	struct completion unreg_completion;
 	struct work_struct unregistration_work;
+
+	const struct rdma_link_ops *link_ops;
 };
 
 struct ib_client {

commit ca22354b140853b8155692d5b2bc0110aa54e937
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:56 2019 -0700

    RDMA/rxe: Close a race after ib_register_device
    
    Since rxe allows unregistration from other threads the rxe pointer can
    become invalid any moment after ib_register_driver returns. This could
    cause a user triggered use after free.
    
    Add another driver callback to be called right after the device becomes
    registered to complete any device setup required post-registration.  This
    callback has enough core locking to prevent the device from becoming
    unregistered.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ad83f8c38dc8..640263289ab9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2539,6 +2539,11 @@ struct ib_device_ops {
 			      struct rdma_restrack_entry *entry);
 
 	/* Device lifecycle callbacks */
+	/*
+	 * Called after the device becomes registered, before clients are
+	 * attached
+	 */
+	int (*enable_driver)(struct ib_device *dev);
 	/*
 	 * This is called as part of ib_dealloc_device().
 	 */

commit d0899892edd089790eb17943ecf28254a909deae
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:53 2019 -0700

    RDMA/device: Provide APIs from the core code to help unregistration
    
    These APIs are intended to support drivers that exist outside the usual
    driver core probe()/remove() callbacks. Normally the driver core will
    prevent remove() from running concurrently with probe(), once this safety
    is lost drivers need more support to get the locking and lifetimes right.
    
    ib_unregister_driver() is intended to be used during module_exit of a
    driver using these APIs. It unregisters all the associated ib_devices.
    
    ib_unregister_device_and_put() is to be used by a driver-specific removal
    function (ie removal by name, removal from a netdev notifier, removal from
    netlink)
    
    ib_unregister_queued() is to be used from netdev notifier chains where
    RTNL is held.
    
    The locking is tricky here since once things become async it is possible
    to race unregister with registration. This is largely solved by relying on
    the registration refcount, unregistration will only ever work on something
    that has a positive registration refcount - and then an unregistration
    mutex serializes all competing unregistrations of the same device.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3aa802b65cf3..ad83f8c38dc8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2538,6 +2538,12 @@ struct ib_device_ops {
 	int (*fill_res_entry)(struct sk_buff *msg,
 			      struct rdma_restrack_entry *entry);
 
+	/* Device lifecycle callbacks */
+	/*
+	 * This is called as part of ib_dealloc_device().
+	 */
+	void (*dealloc_driver)(struct ib_device *dev);
+
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 };
 
@@ -2555,6 +2561,7 @@ struct ib_device {
 
 	struct rw_semaphore	      client_data_rwsem;
 	struct xarray                 client_data;
+	struct mutex                  unregistration_lock;
 
 	struct ib_cache               cache;
 	/**
@@ -2609,6 +2616,7 @@ struct ib_device {
 	 */
 	refcount_t refcount;
 	struct completion unreg_completion;
+	struct work_struct unregistration_work;
 };
 
 struct ib_client {
@@ -2658,6 +2666,9 @@ void ib_get_device_fw_str(struct ib_device *device, char *str);
 
 int ib_register_device(struct ib_device *device, const char *name);
 void ib_unregister_device(struct ib_device *device);
+void ib_unregister_driver(enum rdma_driver_id driver_id);
+void ib_unregister_device_and_put(struct ib_device *device);
+void ib_unregister_device_queued(struct ib_device *ib_dev);
 
 int ib_register_client   (struct ib_client *client);
 void ib_unregister_client(struct ib_client *client);

commit 324e227ea7c952626abafe72db42ae0d70220a6e
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:51 2019 -0700

    RDMA/device: Add ib_device_get_by_netdev()
    
    Several drivers need to find the ib_device from a given netdev. rxe needs
    this at speed in an unsleepable context, so choose to implement the
    translation using a RCU safe hash table.
    
    The hash table can have a many to one mapping. This is intended to support
    some future case where multiple IB drivers (ie iWarp and RoCE) connect to
    the same netdevs. driver_ids will need to be different to support this.
    
    In the process this makes the struct ib_device and ib_port_data RCU safe
    by deferring their kfrees.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7f81a313c01b..3aa802b65cf3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2198,6 +2198,8 @@ struct ib_port_immutable {
 };
 
 struct ib_port_data {
+	struct ib_device *ib_dev;
+
 	struct ib_port_immutable immutable;
 
 	spinlock_t pkey_list_lock;
@@ -2206,7 +2208,8 @@ struct ib_port_data {
 	struct ib_port_cache cache;
 
 	spinlock_t netdev_lock;
-	struct net_device *netdev;
+	struct net_device __rcu *netdev;
+	struct hlist_node ndev_hash_link;
 };
 
 /* rdma netdev type - specifies protocol type */
@@ -2545,6 +2548,7 @@ struct ib_device {
 	struct device                *dma_device;
 	struct ib_device_ops	     ops;
 	char                          name[IB_DEVICE_NAME_MAX];
+	struct rcu_head rcu_head;
 
 	struct list_head              event_handler_list;
 	spinlock_t                    event_handler_lock;
@@ -3996,6 +4000,10 @@ static inline bool ib_device_try_get(struct ib_device *dev)
 }
 
 void ib_device_put(struct ib_device *device);
+struct ib_device *ib_device_get_by_netdev(struct net_device *ndev,
+					  enum rdma_driver_id driver_id);
+struct ib_device *ib_device_get_by_name(const char *name,
+					enum rdma_driver_id driver_id);
 struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);

commit c2261dd76b549754c14c8ac7cadadd0993b182d6
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:50 2019 -0700

    RDMA/device: Add ib_device_set_netdev() as an alternative to get_netdev
    
    The associated netdev should not actually be very dynamic, so for most
    drivers there is no reason for a callback like this. Provide an API to
    inform the core code about the net dev affiliation and use a core
    maintained data structure instead.
    
    This allows the core code to be more aware of the ndev relationship which
    will allow some new APIs based around this.
    
    This also uses locking that makes some kind of sense, many drivers had a
    confusing RCU lock, or missing locking which isn't right.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 50b7ebc2885e..7f81a313c01b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2204,6 +2204,9 @@ struct ib_port_data {
 	struct list_head pkey_list;
 
 	struct ib_port_cache cache;
+
+	spinlock_t netdev_lock;
+	struct net_device *netdev;
 };
 
 /* rdma netdev type - specifies protocol type */
@@ -3996,6 +3999,10 @@ void ib_device_put(struct ib_device *device);
 struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);
+int ib_device_set_netdev(struct ib_device *ib_dev, struct net_device *ndev,
+			 unsigned int port);
+struct net_device *ib_device_netdev(struct ib_device *dev, u8 port);
+
 struct ib_wq *ib_create_wq(struct ib_pd *pd,
 			   struct ib_wq_init_attr *init_attr);
 int ib_destroy_wq(struct ib_wq *wq);

commit 8faea9fd4a3914f12cd343e10810ec5f4215ddd6
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:49 2019 -0700

    RDMA/cache: Move the cache per-port data into the main ib_port_data
    
    Like the other cases there no real reason to have another array just for
    the cache. This larger conversion gets its own patch.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b42e257814f7..50b7ebc2885e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2186,7 +2186,6 @@ struct ib_port_cache {
 struct ib_cache {
 	rwlock_t                lock;
 	struct ib_event_handler event_handler;
-	struct ib_port_cache   *ports;
 };
 
 struct iw_cm_verbs;
@@ -2203,6 +2202,8 @@ struct ib_port_data {
 
 	spinlock_t pkey_list_lock;
 	struct list_head pkey_list;
+
+	struct ib_port_cache cache;
 };
 
 /* rdma netdev type - specifies protocol type */

commit 8ceb1357b33790193e9d55d2d09bcfd6bd59dd6d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:48 2019 -0700

    RDMA/device: Consolidate ib_device per_port data into one place
    
    There is no reason to have three allocations of per-port data. Combine
    them together and make the lifetime for all the per-port data match the
    struct ib_device.
    
    Following patches will require more port-specific data, now there is a
    good place to put it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fa0edd6ae33c..b42e257814f7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2198,6 +2198,13 @@ struct ib_port_immutable {
 	u32                           max_mad_size;
 };
 
+struct ib_port_data {
+	struct ib_port_immutable immutable;
+
+	spinlock_t pkey_list_lock;
+	struct list_head pkey_list;
+};
+
 /* rdma netdev type - specifies protocol type */
 enum rdma_netdev_t {
 	RDMA_NETDEV_OPA_VNIC,
@@ -2243,12 +2250,6 @@ struct rdma_netdev_alloc_params {
 				      struct net_device *netdev, void *param);
 };
 
-struct ib_port_pkey_list {
-	/* Lock to hold while modifying the list. */
-	spinlock_t                    list_lock;
-	struct list_head              pkey_list;
-};
-
 struct ib_counters {
 	struct ib_device	*device;
 	struct ib_uobject	*uobject;
@@ -2549,14 +2550,12 @@ struct ib_device {
 
 	struct ib_cache               cache;
 	/**
-	 * port_immutable is indexed by port number
+	 * port_data is indexed by port number
 	 */
-	struct ib_port_immutable     *port_immutable;
+	struct ib_port_data *port_data;
 
 	int			      num_comp_vectors;
 
-	struct ib_port_pkey_list     *port_pkey_list;
-
 	struct iw_cm_verbs	     *iwcm;
 
 	struct module               *owner;
@@ -2860,34 +2859,38 @@ static inline int rdma_is_port_valid(const struct ib_device *device,
 static inline bool rdma_is_grh_required(const struct ib_device *device,
 					u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags &
-		RDMA_CORE_PORT_IB_GRH_REQUIRED;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_PORT_IB_GRH_REQUIRED;
 }
 
 static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_IB;
 }
 
 static inline bool rdma_protocol_roce(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags &
-		(RDMA_CORE_CAP_PROT_ROCE | RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP);
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       (RDMA_CORE_CAP_PROT_ROCE | RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP);
 }
 
 static inline bool rdma_protocol_roce_udp_encap(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;
 }
 
 static inline bool rdma_protocol_roce_eth_encap(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_ROCE;
 }
 
 static inline bool rdma_protocol_iwarp(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_IWARP;
 }
 
 static inline bool rdma_ib_or_roce(const struct ib_device *device, u8 port_num)
@@ -2898,12 +2901,14 @@ static inline bool rdma_ib_or_roce(const struct ib_device *device, u8 port_num)
 
 static inline bool rdma_protocol_raw_packet(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_RAW_PACKET;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_RAW_PACKET;
 }
 
 static inline bool rdma_protocol_usnic(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_USNIC;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_PROT_USNIC;
 }
 
 /**
@@ -2920,7 +2925,8 @@ static inline bool rdma_protocol_usnic(const struct ib_device *device, u8 port_n
  */
 static inline bool rdma_cap_ib_mad(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_MAD;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_IB_MAD;
 }
 
 /**
@@ -2944,8 +2950,8 @@ static inline bool rdma_cap_ib_mad(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_opa_mad(struct ib_device *device, u8 port_num)
 {
-	return (device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_OPA_MAD)
-		== RDMA_CORE_CAP_OPA_MAD;
+	return (device->port_data[port_num].immutable.core_cap_flags &
+		RDMA_CORE_CAP_OPA_MAD) == RDMA_CORE_CAP_OPA_MAD;
 }
 
 /**
@@ -2970,7 +2976,8 @@ static inline bool rdma_cap_opa_mad(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_smi(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SMI;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_IB_SMI;
 }
 
 /**
@@ -2990,7 +2997,8 @@ static inline bool rdma_cap_ib_smi(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_cm(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_CM;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_IB_CM;
 }
 
 /**
@@ -3007,7 +3015,8 @@ static inline bool rdma_cap_ib_cm(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_iw_cm(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IW_CM;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_IW_CM;
 }
 
 /**
@@ -3027,7 +3036,8 @@ static inline bool rdma_cap_iw_cm(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_sa(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SA;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_IB_SA;
 }
 
 /**
@@ -3067,7 +3077,8 @@ static inline bool rdma_cap_ib_mcast(const struct ib_device *device, u8 port_num
  */
 static inline bool rdma_cap_af_ib(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_AF_IB;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_AF_IB;
 }
 
 /**
@@ -3088,7 +3099,8 @@ static inline bool rdma_cap_af_ib(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_eth_ah(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_ETH_AH;
+	return device->port_data[port_num].immutable.core_cap_flags &
+	       RDMA_CORE_CAP_ETH_AH;
 }
 
 /**
@@ -3102,7 +3114,7 @@ static inline bool rdma_cap_eth_ah(const struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_opa_ah(struct ib_device *device, u8 port_num)
 {
-	return (device->port_immutable[port_num].core_cap_flags &
+	return (device->port_data[port_num].immutable.core_cap_flags &
 		RDMA_CORE_CAP_OPA_AH) == RDMA_CORE_CAP_OPA_AH;
 }
 
@@ -3120,7 +3132,7 @@ static inline bool rdma_cap_opa_ah(struct ib_device *device, u8 port_num)
  */
 static inline size_t rdma_max_mad_size(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].max_mad_size;
+	return device->port_data[port_num].immutable.max_mad_size;
 }
 
 /**

commit ea1075edcbab7d92f4e4ccf5490043f796bf78be
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:47 2019 -0700

    RDMA: Add and use rdma_for_each_port
    
    We have many loops iterating over all of the end port numbers on a struct
    ib_device, simplify them with a for_each helper.
    
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2a17c2b30073..fa0edd6ae33c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2827,6 +2827,16 @@ static inline u8 rdma_start_port(const struct ib_device *device)
 	return rdma_cap_ib_switch(device) ? 0 : 1;
 }
 
+/**
+ * rdma_for_each_port - Iterate over all valid port numbers of the IB device
+ * @device - The struct ib_device * to iterate over
+ * @iter - The unsigned int to store the port number
+ */
+#define rdma_for_each_port(device, iter)                                       \
+	for (iter = rdma_start_port(device + BUILD_BUG_ON_ZERO(!__same_type(   \
+						     unsigned int, iter)));    \
+	     iter <= rdma_end_port(device); (iter)++)
+
 /**
  * rdma_end_port - Return the last valid port number for the device
  * specified

commit 41eda65c6100930d95bb854a0114f3544593070c
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Feb 18 22:25:47 2019 +0200

    RDMA/restrack: Hide restrack DB from IB/core
    
    There is no need to expose internals of restrack DB to IB/core.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 64ee7c08be22..2a17c2b30073 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2533,6 +2533,8 @@ struct ib_device_ops {
 	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 };
 
+struct rdma_restrack_root;
+
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -2589,10 +2591,7 @@ struct ib_device {
 #endif
 
 	u32                          index;
-	/*
-	 * Implementation details of the RDMA core, don't use in drivers
-	 */
-	struct rdma_restrack_root     res;
+	struct rdma_restrack_root *res;
 
 	const struct uapi_definition   *driver_def;
 	enum rdma_driver_id		driver_id;

commit 3d9dfd060391928bd615db62ecddea5e1255edfd
Author: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
Date:   Thu Feb 7 18:44:47 2019 +0200

    IB/uverbs: Add ib_ucontext to uverbs_attr_bundle sent from ioctl and cmd flows
    
    Add ib_ucontext to the uverbs_attr_bundle sent down the iocl and cmd flows
    as soon as the flow has ib_uobject.
    
    In addition, remove rdma_get_ucontext helper function that is only used by
    ib_umem_get.
    
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 135fab2c016c..64ee7c08be22 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4247,8 +4247,6 @@ void rdma_roce_rescan_device(struct ib_device *ibdev);
 
 struct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile);
 
-struct ib_ucontext *rdma_get_ucontext(struct ib_udata *udata);
-
 int uverbs_destroy_def_handler(struct uverbs_attr_bundle *attrs);
 
 struct net_device *rdma_alloc_netdev(struct ib_device *device, u8 port_num,

commit 921eab1143aadf976a42cac4605b4d35159b355d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Feb 6 22:41:54 2019 -0700

    RDMA/devices: Re-organize device.c locking
    
    The locking here started out with a single lock that covered everything
    and then has lately veered into crazy town.
    
    The fundamental problem is that several places need to iterate over a
    linked list, but also need to drop their locks to avoid deadlock during
    client callbacks.
    
    xarray's restartable iteration offers a simple solution to the
    problem. Once all the lists are xarrays we can drop locks in the places
    that need that and rely on xarray to provide consistency and locking for
    the data structure.
    
    The resulting simplification is that each of the three lists has a
    dedicated rwsem that must be held when working with the list it
    covers. One data structure is no longer covered by multiple locks.
    
    The sleeping semaphore is selected because the read side generally needs
    to be held over something sleeping, and using RCU reader locking in those
    cases is overkill.
    
    In the process this simplifies the entire registration/unregistration flow
    to be the expected list of setups and the reversed list of matching
    teardowns, and the registration lock 'refcount' can now be revised to be
    released after the ULPs are removed, providing a very sane semantic for
    this feature.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8558f31ca46f..135fab2c016c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2542,6 +2542,7 @@ struct ib_device {
 	struct list_head              event_handler_list;
 	spinlock_t                    event_handler_lock;
 
+	struct rw_semaphore	      client_data_rwsem;
 	struct xarray                 client_data;
 
 	struct ib_cache               cache;

commit 0df91bb67334eebaf73d4ba32567e16d55f4f116
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Feb 6 22:41:53 2019 -0700

    RDMA/devices: Use xarray to store the client_data
    
    Now that we have a small ID for each client we can use xarray instead of
    linearly searching linked lists for client data. This will give much
    faster and scalable client data lookup, and will lets us revise the
    locking scheme.
    
    Since xarray can store 'going_down' using a mark just entirely eliminate
    the struct ib_client_data and directly store the client_data value in the
    xarray. However this does require a special iterator as we must still
    iterate over any NULL client_data values.
    
    Also eliminate the client_data_lock in favour of internal xarray locking.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cc15820513cd..8558f31ca46f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2542,12 +2542,7 @@ struct ib_device {
 	struct list_head              event_handler_list;
 	spinlock_t                    event_handler_lock;
 
-	rwlock_t			client_data_lock;
-	struct list_head              core_list;
-	/* Access to the client_data_list is protected by the client_data_lock
-	 * rwlock and the lists_rwsem read-write semaphore
-	 */
-	struct list_head              client_data_list;
+	struct xarray                 client_data;
 
 	struct ib_cache               cache;
 	/**
@@ -2660,7 +2655,21 @@ void ib_unregister_device(struct ib_device *device);
 int ib_register_client   (struct ib_client *client);
 void ib_unregister_client(struct ib_client *client);
 
-void *ib_get_client_data(struct ib_device *device, struct ib_client *client);
+/**
+ * ib_get_client_data - Get IB client context
+ * @device:Device to get context for
+ * @client:Client to get context for
+ *
+ * ib_get_client_data() returns the client context data set with
+ * ib_set_client_data(). This can only be called while the client is
+ * registered to the device, once the ib_client remove() callback returns this
+ * cannot be called.
+ */
+static inline void *ib_get_client_data(struct ib_device *device,
+				       struct ib_client *client)
+{
+	return xa_load(&device->client_data, client->client_id);
+}
 void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
 			 void *data);
 void ib_set_device_ops(struct ib_device *device,

commit e59178d895afa29b671323f8265a1e50afe989e5
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Feb 6 22:41:52 2019 -0700

    RDMA/devices: Use xarray to store the clients
    
    This gives each client a unique ID and will let us move client_data to use
    xarray, and revise the locking scheme.
    
    clients have to be add/removed in strict FIFO/LIFO order as they
    interdepend. To support this the client_ids are assigned to increase in
    FIFO order. The existing linked list is kept to support reverse iteration
    until xarray can get a reverse iteration API.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d8ba987e8b29..cc15820513cd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2610,7 +2610,7 @@ struct ib_device {
 };
 
 struct ib_client {
-	char  *name;
+	const char *name;
 	void (*add)   (struct ib_device *);
 	void (*remove)(struct ib_device *, void *client_data);
 
@@ -2637,6 +2637,7 @@ struct ib_client {
 			const struct sockaddr *addr,
 			void *client_data);
 	struct list_head list;
+	u32 client_id;
 
 	/* kverbs are not required by the client */
 	u8 no_kverbs_req:1;

commit 652432f33c01b2edaa5b2550b423cd894b1c7b9a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Feb 6 22:41:50 2019 -0700

    RDMA/device: Get rid of reg_state
    
    This really has no purpose anymore, refcount can be used to tell if the
    device is still registered. Keeping it around just invites mis-use.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 854d7816787c..d8ba987e8b29 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2572,12 +2572,6 @@ struct ib_device {
 	struct kobject			*ports_kobj;
 	struct list_head             port_list;
 
-	enum {
-		IB_DEV_UNINITIALIZED,
-		IB_DEV_REGISTERED,
-		IB_DEV_UNREGISTERED
-	}                            reg_state;
-
 	int			     uverbs_abi_ver;
 	u64			     uverbs_cmd_mask;
 	u64			     uverbs_ex_cmd_mask;

commit 21a428a019c9a6d133e745b529b9bf18c1187e70
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Feb 3 14:55:51 2019 +0200

    RDMA: Handle PD allocations by IB/core
    
    The PD allocations in IB/core allows us to simplify drivers and their
    error flows in their .alloc_pd() paths. The changes in .alloc_pd() go hand
    in had with relevant update in .dealloc_pd().
    
    We will use this opportunity and convert .dealloc_pd() to don't fail, as
    it was suggested a long time ago, failures are not happening as we have
    never seen a WARN_ON print.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e29eae4aec84..854d7816787c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2385,10 +2385,9 @@ struct ib_device_ops {
 	int (*dealloc_ucontext)(struct ib_ucontext *context);
 	int (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);
 	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
-	struct ib_pd *(*alloc_pd)(struct ib_device *device,
-				  struct ib_ucontext *context,
-				  struct ib_udata *udata);
-	int (*dealloc_pd)(struct ib_pd *pd);
+	int (*alloc_pd)(struct ib_pd *pd, struct ib_ucontext *context,
+			struct ib_udata *udata);
+	void (*dealloc_pd)(struct ib_pd *pd);
 	struct ib_ah *(*create_ah)(struct ib_pd *pd,
 				   struct rdma_ah_attr *ah_attr, u32 flags,
 				   struct ib_udata *udata);
@@ -2530,6 +2529,8 @@ struct ib_device_ops {
 	 */
 	int (*fill_res_entry)(struct sk_buff *msg,
 			      struct rdma_restrack_entry *entry);
+
+	DECLARE_RDMA_OBJ_SIZE(ib_pd);
 };
 
 struct ib_device {

commit 30471d4b20335d9bd9ae9b2382a1e1e97d18d86d
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Feb 3 14:55:50 2019 +0200

    RDMA/core: Share driver structure size with core
    
    Add new macros to be used in drivers while registering ops structure and
    IB/core while calling allocation routines, so drivers won't need to
    perform kzalloc/kfree in their paths.
    
    The change in allocation stage allows us to initialize common fields prior
    to calling to drivers (e.g. restrack).
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2e1f1e885ee5..e29eae4aec84 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2264,6 +2264,19 @@ struct ib_counters_read_attr {
 
 struct uverbs_attr_bundle;
 
+#define INIT_RDMA_OBJ_SIZE(ib_struct, drv_struct, member)                      \
+	.size_##ib_struct =                                                    \
+		(sizeof(struct drv_struct) +                                   \
+		 BUILD_BUG_ON_ZERO(offsetof(struct drv_struct, member)) +      \
+		 BUILD_BUG_ON_ZERO(                                            \
+			 !__same_type(((struct drv_struct *)NULL)->member,     \
+				      struct ib_struct)))
+
+#define rdma_zalloc_drv_obj(ib_dev, ib_type)                                   \
+	((struct ib_type *)kzalloc(ib_dev->ops.size_##ib_type, GFP_KERNEL))
+
+#define DECLARE_RDMA_OBJ_SIZE(ib_struct) size_t size_##ib_struct
+
 /**
  * struct ib_device_ops - InfiniBand device operations
  * This structure defines all the InfiniBand device operations, providers will

commit 6a8a2aa62da2fbe51f5449993fd366398048f465
Merge: a163afc88556 8834f5600cf3
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Feb 4 14:53:42 2019 -0700

    Merge tag 'v5.0-rc5' into rdma.git for-next
    
    Linux 5.0-rc5
    
    Needed to merge the include/uapi changes so we have an up to date
    single-tree for these files. Patches already posted are also expected to
    need this for dependencies.

commit a163afc88556e099271a7b423295bc5176fcecce
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Jan 31 08:30:34 2019 -0800

    IB/core: Remove ib_sg_dma_address() and ib_sg_dma_len()
    
    Keeping single line wrapper functions is not useful. Hence remove the
    ib_sg_dma_address() and ib_sg_dma_len() functions. This patch does not
    change any functionality.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8219c07340a9..f7e8709e48cd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3705,33 +3705,6 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 {
 	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
 }
-/**
- * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
- * @dev: The device for which the DMA addresses were created
- * @sg: The scatter/gather entry
- *
- * Note: this function is obsolete. To do: change all occurrences of
- * ib_sg_dma_address() into sg_dma_address().
- */
-static inline u64 ib_sg_dma_address(struct ib_device *dev,
-				    struct scatterlist *sg)
-{
-	return sg_dma_address(sg);
-}
-
-/**
- * ib_sg_dma_len - Return the DMA length from a scatter/gather entry
- * @dev: The device for which the DMA addresses were created
- * @sg: The scatter/gather entry
- *
- * Note: this function is obsolete. To do: change all occurrences of
- * ib_sg_dma_len() into sg_dma_len().
- */
-static inline unsigned int ib_sg_dma_len(struct ib_device *dev,
-					 struct scatterlist *sg)
-{
-	return sg_dma_len(sg);
-}
 
 /**
  * ib_dma_max_seg_size - Return the size limit of a single DMA transfer

commit 52a72e2a395fa3c5ab5df41058a8511e87215730
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 22 08:48:42 2019 +0200

    IB/uverbs: Expose XRC ODP device capabilities
    
    Expose XRC ODP capabilities as part of the extended device capabilities.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5eefdea62831..8219c07340a9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -277,6 +277,7 @@ struct ib_odp_caps {
 		uint32_t  rc_odp_caps;
 		uint32_t  uc_odp_caps;
 		uint32_t  ud_odp_caps;
+		uint32_t  xrc_odp_caps;
 	} per_transport_caps;
 };
 

commit da82334219bc386ef7ea5b4b185a339a973dd513
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 22 08:48:41 2019 +0200

    IB/core: Allocate a bit for SRQ ODP support
    
    The ODP support matrix is per operation and per transport. The support for
    each transport (RC, UD, etc.) is described with a bit field.
    
    ODP for SRQ WQEs is considered a different kind of support from ODP for RQ
    WQs and therefore needs a different capability bit.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5fc3be884444..5eefdea62831 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -268,6 +268,7 @@ enum ib_odp_transport_cap_bits {
 	IB_ODP_SUPPORT_WRITE	= 1 << 2,
 	IB_ODP_SUPPORT_READ	= 1 << 3,
 	IB_ODP_SUPPORT_ATOMIC	= 1 << 4,
+	IB_ODP_SUPPORT_SRQ_RECV	= 1 << 5,
 };
 
 struct ib_odp_caps {

commit 02da37509705d3ba6a58fe4799a0caf6b4baecb0
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Jan 30 12:49:02 2019 +0200

    RDMA/core: Use the ops infrastructure to keep all callbacks in one place
    
    As preparation to hide rdma_restrack_root, refactor the code to use the
    ops structure instead of a special callback which is hidden in
    rdma_restrack_root.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4183a03b46b5..5fc3be884444 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2510,6 +2510,11 @@ struct ib_device_ops {
 	 */
 	int (*init_port)(struct ib_device *device, u8 port_num,
 			 struct kobject *port_sysfs);
+	/**
+	 * Allows rdma drivers to add their own restrack attributes.
+	 */
+	int (*fill_res_entry)(struct sk_buff *msg,
+			      struct rdma_restrack_entry *entry);
 };
 
 struct ib_device {

commit 6780c4fa9d6e091b2f206ac429a40e2e8d2e45f3
Author: Gal Pressman <galpress@amazon.com>
Date:   Tue Jan 22 10:08:22 2019 +0200

    RDMA: Add indication for in kernel API support to IB device
    
    Drivers that do not provide kernel verbs support should not be used by ib
    kernel clients at all.
    
    In case a device does not implement all mandatory verbs for kverbs usage
    mark it as a non kverbs provider and prevent its usage for all clients
    except for uverbs.
    
    The device is marked as a non kverbs provider using the 'kverbs_provider'
    flag which should only be set by the core code.  The clients can choose
    whether kverbs are requested for its usage using the 'no_kverbs_req' flag
    which is currently set for uverbs only.
    
    This patch allows drivers to remove mandatory verbs stubs and simply set
    the callbacks to NULL. The IB device will be registered as a non-kverbs
    provider. Note that verbs that are required for the device registration
    process must be implemented.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a1a1e710642c..4183a03b46b5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2565,6 +2565,8 @@ struct ib_device {
 	__be64			     node_guid;
 	u32			     local_dma_lkey;
 	u16                          is_switch:1;
+	/* Indicates kernel verbs support, should not be used in drivers */
+	u16                          kverbs_provider:1;
 	u8                           node_type;
 	u8                           phys_port_cnt;
 	struct ib_device_attr        attrs;
@@ -2619,6 +2621,9 @@ struct ib_client {
 			const struct sockaddr *addr,
 			void *client_data);
 	struct list_head list;
+
+	/* kverbs are not required by the client */
+	u8 no_kverbs_req:1;
 };
 
 struct ib_device *_ib_alloc_device(size_t size);

commit 459cc69fa4c17caf21de596693d8a07170820a58
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Jan 30 12:49:11 2019 +0200

    RDMA: Provide safe ib_alloc_device() function
    
    All callers to ib_alloc_device() provide a larger size than struct
    ib_device and rely on the fact that struct ib_device is embedded in their
    driver specific structure as the first member.
    
    Provide a safer variant of ib_alloc_device() that checks and enforces this
    approach to make sure the drivers are using it right.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 71ea144ec823..a1a1e710642c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2621,7 +2621,13 @@ struct ib_client {
 	struct list_head list;
 };
 
-struct ib_device *ib_alloc_device(size_t size);
+struct ib_device *_ib_alloc_device(size_t size);
+#define ib_alloc_device(drv_struct, member)                                    \
+	container_of(_ib_alloc_device(sizeof(struct drv_struct) +              \
+				      BUILD_BUG_ON_ZERO(offsetof(              \
+					      struct drv_struct, member))),    \
+		     struct drv_struct, member)
+
 void ib_dealloc_device(struct ib_device *device);
 
 void ib_get_device_fw_str(struct ib_device *device, char *str);

commit 0b5cb3300ae59ed7e93b465dfa2384a6a4df8eb4
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Tue Jan 22 10:25:20 2019 -0800

    RDMA/srp: Increase max_segment_size
    
    The default behavior of the SCSI core is to set the block layer request
    queue parameter max_segment_size to 64 KB. That means that elements of
    scatterlists are limited to 64 KB. Since RDMA adapters support larger
    sizes, increase max_segment_size for the SRP initiator.
    
    Notes:
    - The SCSI max_segment_size parameter was introduced in kernel v5.0. See
      also commit 50c2e9107f17 ("scsi: introduce a max_segment_size
      host_template parameters").
    - Some other block drivers already set max_segment_size to UINT_MAX,
      e.g. nbd and rbd.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 94b6e1dd4dab..71ea144ec823 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3715,6 +3715,19 @@ static inline unsigned int ib_sg_dma_len(struct ib_device *dev,
 	return sg_dma_len(sg);
 }
 
+/**
+ * ib_dma_max_seg_size - Return the size limit of a single DMA transfer
+ * @dev: The device to query
+ *
+ * The returned value represents a size in bytes.
+ */
+static inline unsigned int ib_dma_max_seg_size(struct ib_device *dev)
+{
+	struct device_dma_parameters *p = dev->dma_device->dma_parms;
+
+	return p ? p->max_segment_size : UINT_MAX;
+}
+
 /**
  * ib_dma_sync_single_for_cpu - Prepare DMA region to be accessed by CPU
  * @dev: The device for which the DMA address was created

commit d79af7242bb237d00e40092810e6828fbb929d2d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jan 10 14:02:24 2019 -0700

    RDMA/device: Expose ib_device_try_get(()
    
    It turns out future patches need this capability quite widely now, not
    just for netlink, so provide two global functions to manage the
    registration lock refcount.
    
    This also moves the point the lock becomes 1 to within
    ib_register_device() so that the semantics of the public API are very sane
    and clear. Calling ib_device_try_get() will fail on devices that are only
    allocated but not yet registered.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a3ceed3a040a..80debf5982ac 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2579,9 +2579,10 @@ struct ib_device {
 
 	const struct uapi_definition   *driver_def;
 	enum rdma_driver_id		driver_id;
+
 	/*
-	 * Provides synchronization between device unregistration and netlink
-	 * commands on a device. To be used only by core.
+	 * Positive refcount indicates that the device is currently
+	 * registered and cannot be unregistered.
 	 */
 	refcount_t refcount;
 	struct completion unreg_completion;
@@ -3926,6 +3927,25 @@ static inline bool ib_access_writable(int access_flags)
 int ib_check_mr_status(struct ib_mr *mr, u32 check_mask,
 		       struct ib_mr_status *mr_status);
 
+/**
+ * ib_device_try_get: Hold a registration lock
+ * device: The device to lock
+ *
+ * A device under an active registration lock cannot become unregistered. It
+ * is only possible to obtain a registration lock on a device that is fully
+ * registered, otherwise this function returns false.
+ *
+ * The registration lock is only necessary for actions which require the
+ * device to still be registered. Uses that only require the device pointer to
+ * be valid should use get_device(&ibdev->dev) to hold the memory.
+ *
+ */
+static inline bool ib_device_try_get(struct ib_device *dev)
+{
+	return refcount_inc_not_zero(&dev->refcount);
+}
+
+void ib_device_put(struct ib_device *device);
 struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);

commit 54747231150f0dddf68f2ee29ec2970fcc433909
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Dec 18 14:15:56 2018 +0200

    RDMA: Introduce and use rdma_device_to_ibdev()
    
    Introduce and use rdma_device_to_ibdev() API for those drivers which are
    registering one sysfs group and also use in ib_core.
    
    In subsequent patch, device->provider_ibdev one-to-one mapping is no
    longer holds true during accessing sysfs entries.
    Therefore, introduce an API rdma_device_to_ibdev() that provides such
    information.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1d1902fd9f87..94b6e1dd4dab 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4241,4 +4241,27 @@ rdma_set_device_sysfs_group(struct ib_device *dev,
 	dev->groups[1] = group;
 }
 
+/**
+ * rdma_device_to_ibdev - Get ib_device pointer from device pointer
+ *
+ * @device:	device pointer for which ib_device pointer to retrieve
+ *
+ * rdma_device_to_ibdev() retrieves ib_device pointer from device.
+ *
+ */
+static inline struct ib_device *rdma_device_to_ibdev(struct device *device)
+{
+	return container_of(device, struct ib_device, dev);
+}
+
+/**
+ * rdma_device_to_drv_device - Helper macro to reach back to driver's
+ *			       ib_device holder structure from device pointer.
+ *
+ * NOTE: New drivers should not make use of this API; This API is only for
+ * existing drivers who have exposed sysfs entries using
+ * rdma_set_device_sysfs_group().
+ */
+#define rdma_device_to_drv_device(dev, drv_dev_struct, ibdev_member)           \
+	container_of(rdma_device_to_ibdev(dev), drv_dev_struct, ibdev_member)
 #endif /* IB_VERBS_H */

commit ea4baf7f116a18382df331db2123d98bc1c3cd83
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Dec 18 14:28:30 2018 +0200

    RDMA: Rename port_callback to init_port
    
    Most provider routines are callback routines which ib core invokes.
    _callback suffix doesn't convey information about when such callback is
    invoked. Therefore, rename port_callback to init_port.
    
    Additionally, store the init_port function pointer in ib_device_ops, so
    that it can be accessed in subsequent patches when binding rdma device to
    net namespace.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index aa1f126d3383..1d1902fd9f87 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2504,6 +2504,12 @@ struct ib_device_ops {
 	 */
 	int (*get_hw_stats)(struct ib_device *device,
 			    struct rdma_hw_stats *stats, u8 port, int index);
+	/*
+	 * This function is called once for each port when a ib device is
+	 * registered.
+	 */
+	int (*init_port)(struct ib_device *device, u8 port_num,
+			 struct kobject *port_sysfs);
 };
 
 struct ib_device {
@@ -2620,9 +2626,7 @@ void ib_dealloc_device(struct ib_device *device);
 
 void ib_get_device_fw_str(struct ib_device *device, char *str);
 
-int ib_register_device(struct ib_device *device, const char *name,
-		       int (*port_callback)(struct ib_device *, u8,
-					    struct kobject *));
+int ib_register_device(struct ib_device *device, const char *name);
 void ib_unregister_device(struct ib_device *device);
 
 int ib_register_client   (struct ib_client *client);

commit b0ea0fa5435f9df7213a9af098558f7dd584d8e8
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 9 11:15:16 2019 +0200

    IB/{core,hw}: Have ib_umem_get extract the ib_ucontext from ib_udata
    
    ib_umem_get() can only be called in a method callback, which always has a
    udata parameter. This allows ib_umem_get() to derive the ucontext pointer
    directly from the udata without requiring the drivers to find it in some
    way or another.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3ddd199ba602..aa1f126d3383 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4200,6 +4200,7 @@ void rdma_roce_rescan_device(struct ib_device *ibdev);
 
 struct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile);
 
+struct ib_ucontext *rdma_get_ucontext(struct ib_udata *udata);
 
 int uverbs_destroy_def_handler(struct uverbs_attr_bundle *attrs);
 

commit 96f87ee1811306d0c8cf94b8c37b0e4f725b01d1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Jan 8 16:07:23 2019 +0200

    RDMA: Clean structures from CONFIG_INFINIBAND_ON_DEMAND_PAGING
    
    CONFIG_INFINIBAND_ON_DEMAND_PAGING is used in general structures to
    micro-optimize the memory footprint. Remove it, so it will allow us to
    simplify various ODP device flows.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a3ceed3a040a..3ddd199ba602 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1504,12 +1504,10 @@ struct ib_ucontext {
 
 	bool cleanup_retryable;
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
 	struct mutex per_mm_list_lock;
 	struct list_head per_mm_list;
-#endif
 
 	struct ib_rdmacg_object	cg_obj;
 	/*

commit 2553ba217eea37dc6291635ecddb883fb5c36a8b
Author: Gal Pressman <galpress@amazon.com>
Date:   Wed Dec 12 11:09:06 2018 +0200

    RDMA: Mark if destroy address handle is in a sleepable context
    
    Introduce a 'flags' field to destroy address handle callback and add a
    flag that marks whether the callback is executed in an atomic context or
    not.
    
    This will allow drivers to wait for completion instead of polling for it
    when it is allowed.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 86ae878e1de4..a3ceed3a040a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2381,7 +2381,7 @@ struct ib_device_ops {
 				   struct ib_udata *udata);
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
-	int (*destroy_ah)(struct ib_ah *ah);
+	int (*destroy_ah)(struct ib_ah *ah, u32 flags);
 	struct ib_srq *(*create_srq)(struct ib_pd *pd,
 				     struct ib_srq_init_attr *srq_init_attr,
 				     struct ib_udata *udata);
@@ -3256,11 +3256,17 @@ int rdma_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
  */
 int rdma_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
+enum rdma_destroy_ah_flags {
+	/* In a sleepable context */
+	RDMA_DESTROY_AH_SLEEPABLE = BIT(0),
+};
+
 /**
  * rdma_destroy_ah - Destroys an address handle.
  * @ah: The address handle to destroy.
+ * @flags: Destroy address handle flags (see enum rdma_destroy_ah_flags).
  */
-int rdma_destroy_ah(struct ib_ah *ah);
+int rdma_destroy_ah(struct ib_ah *ah, u32 flags);
 
 /**
  * ib_create_srq - Creates a SRQ associated with the specified protection

commit b090c4e3a07c33ffdf95fb7601551b38fc2a4bbb
Author: Gal Pressman <galpress@amazon.com>
Date:   Wed Dec 12 11:09:05 2018 +0200

    RDMA: Mark if create address handle is in a sleepable context
    
    Introduce a 'flags' field to create address handle callback and add a flag
    that marks whether the callback is executed in an atomic context or not.
    
    This will allow drivers to wait for completion instead of polling for it
    when it is allowed.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0ec15d673d92..86ae878e1de4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2377,7 +2377,7 @@ struct ib_device_ops {
 				  struct ib_udata *udata);
 	int (*dealloc_pd)(struct ib_pd *pd);
 	struct ib_ah *(*create_ah)(struct ib_pd *pd,
-				   struct rdma_ah_attr *ah_attr,
+				   struct rdma_ah_attr *ah_attr, u32 flags,
 				   struct ib_udata *udata);
 	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
@@ -3151,15 +3151,22 @@ struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,
 	__ib_alloc_pd((device), (flags), KBUILD_MODNAME)
 void ib_dealloc_pd(struct ib_pd *pd);
 
+enum rdma_create_ah_flags {
+	/* In a sleepable context */
+	RDMA_CREATE_AH_SLEEPABLE = BIT(0),
+};
+
 /**
  * rdma_create_ah - Creates an address handle for the given address vector.
  * @pd: The protection domain associated with the address handle.
  * @ah_attr: The attributes of the address vector.
+ * @flags: Create address handle flags (see enum rdma_create_ah_flags).
  *
  * The address handle is used to reference a local or global destination
  * in all UD QP post sends.
  */
-struct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr);
+struct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
+			     u32 flags);
 
 /**
  * rdma_create_user_ah - Creates an address handle for the given address vector.

commit ad8a4496757f6f7344011a20a07195bd27e3989c
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Dec 11 13:37:52 2018 +0200

    IB/uverbs: Add support to advise_mr
    
    Add new ioctl method for the MR object - ADVISE_MR.
    
    This command can be used by users to give an advice or directions to the
    kernel about an address range that belongs to memory regions.
    
    A new ib_device callback, advise_mr(), is introduced here to suupport the
    new command. This command takes the following arguments:
    
    - pd:           The protection domain to which all memory regions belong
    - advice:       The type of the advice
                    * IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH - Pre-fetch a range of
                    an on-demand paging MR
                    * IB_UVERBS_ADVISE_MR_ADVICE_PREFETCH_WRITE - Pre-fetch a range
                    of an on-demand paging MR with write intention
    - flags:        The properties of the advice
                    * IB_UVERBS_ADVISE_MR_FLAG_FLUSH - Operation must end before
                    return to the caller
    - sg_list:      The list of memory ranges
    - num_sge:      The number of memory ranges in the list
    - attrs:        More attributes to be parsed by the provider
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5b3b51f00f48..0ec15d673d92 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2415,6 +2415,10 @@ struct ib_device_ops {
 	int (*dereg_mr)(struct ib_mr *mr);
 	struct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,
 				  u32 max_num_sg);
+	int (*advise_mr)(struct ib_pd *pd,
+			 enum ib_uverbs_advise_mr_advice advice, u32 flags,
+			 struct ib_sge *sg_list, u32 num_sge,
+			 struct uverbs_attr_bundle *attrs);
 	int (*map_mr_sg)(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
 			 unsigned int *sg_offset);
 	int (*check_mr_status)(struct ib_mr *mr, u32 check_mask,

commit 3023a1e93656c02b8d6a3a46e712b815843fa514
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Mon Dec 10 21:09:48 2018 +0200

    RDMA: Start use ib_device_ops
    
    Make all the required change to start use the ib_device_ops structure.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 60315fd05411..5b3b51f00f48 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2507,7 +2507,7 @@ struct ib_device_ops {
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
-
+	struct ib_device_ops	     ops;
 	char                          name[IB_DEVICE_NAME_MAX];
 
 	struct list_head              event_handler_list;
@@ -2532,273 +2532,6 @@ struct ib_device {
 
 	struct iw_cm_verbs	     *iwcm;
 
-	/**
-	 * alloc_hw_stats - Allocate a struct rdma_hw_stats and fill in the
-	 *   driver initialized data.  The struct is kfree()'ed by the sysfs
-	 *   core when the device is removed.  A lifespan of -1 in the return
-	 *   struct tells the core to set a default lifespan.
-	 */
-	struct rdma_hw_stats      *(*alloc_hw_stats)(struct ib_device *device,
-						     u8 port_num);
-	/**
-	 * get_hw_stats - Fill in the counter value(s) in the stats struct.
-	 * @index - The index in the value array we wish to have updated, or
-	 *   num_counters if we want all stats updated
-	 * Return codes -
-	 *   < 0 - Error, no counters updated
-	 *   index - Updated the single counter pointed to by index
-	 *   num_counters - Updated all counters (will reset the timestamp
-	 *     and prevent further calls for lifespan milliseconds)
-	 * Drivers are allowed to update all counters in lieu of just the
-	 *   one given in index at their option
-	 */
-	int		           (*get_hw_stats)(struct ib_device *device,
-						   struct rdma_hw_stats *stats,
-						   u8 port, int index);
-	int		           (*query_device)(struct ib_device *device,
-						   struct ib_device_attr *device_attr,
-						   struct ib_udata *udata);
-	int		           (*query_port)(struct ib_device *device,
-						 u8 port_num,
-						 struct ib_port_attr *port_attr);
-	enum rdma_link_layer	   (*get_link_layer)(struct ib_device *device,
-						     u8 port_num);
-	/* When calling get_netdev, the HW vendor's driver should return the
-	 * net device of device @device at port @port_num or NULL if such
-	 * a net device doesn't exist. The vendor driver should call dev_hold
-	 * on this net device. The HW vendor's device driver must guarantee
-	 * that this function returns NULL before the net device has finished
-	 * NETDEV_UNREGISTER state.
-	 */
-	struct net_device	  *(*get_netdev)(struct ib_device *device,
-						 u8 port_num);
-	/* query_gid should be return GID value for @device, when @port_num
-	 * link layer is either IB or iWarp. It is no-op if @port_num port
-	 * is RoCE link layer.
-	 */
-	int		           (*query_gid)(struct ib_device *device,
-						u8 port_num, int index,
-						union ib_gid *gid);
-	/* When calling add_gid, the HW vendor's driver should add the gid
-	 * of device of port at gid index available at @attr. Meta-info of
-	 * that gid (for example, the network device related to this gid) is
-	 * available at @attr. @context allows the HW vendor driver to store
-	 * extra information together with a GID entry. The HW vendor driver may
-	 * allocate memory to contain this information and store it in @context
-	 * when a new GID entry is written to. Params are consistent until the
-	 * next call of add_gid or delete_gid. The function should return 0 on
-	 * success or error otherwise. The function could be called
-	 * concurrently for different ports. This function is only called when
-	 * roce_gid_table is used.
-	 */
-	int		           (*add_gid)(const struct ib_gid_attr *attr,
-					      void **context);
-	/* When calling del_gid, the HW vendor's driver should delete the
-	 * gid of device @device at gid index gid_index of port port_num
-	 * available in @attr.
-	 * Upon the deletion of a GID entry, the HW vendor must free any
-	 * allocated memory. The caller will clear @context afterwards.
-	 * This function is only called when roce_gid_table is used.
-	 */
-	int		           (*del_gid)(const struct ib_gid_attr *attr,
-					      void **context);
-	int		           (*query_pkey)(struct ib_device *device,
-						 u8 port_num, u16 index, u16 *pkey);
-	int		           (*modify_device)(struct ib_device *device,
-						    int device_modify_mask,
-						    struct ib_device_modify *device_modify);
-	int		           (*modify_port)(struct ib_device *device,
-						  u8 port_num, int port_modify_mask,
-						  struct ib_port_modify *port_modify);
-	struct ib_ucontext *       (*alloc_ucontext)(struct ib_device *device,
-						     struct ib_udata *udata);
-	int                        (*dealloc_ucontext)(struct ib_ucontext *context);
-	int                        (*mmap)(struct ib_ucontext *context,
-					   struct vm_area_struct *vma);
-	struct ib_pd *             (*alloc_pd)(struct ib_device *device,
-					       struct ib_ucontext *context,
-					       struct ib_udata *udata);
-	int                        (*dealloc_pd)(struct ib_pd *pd);
-	struct ib_ah *             (*create_ah)(struct ib_pd *pd,
-						struct rdma_ah_attr *ah_attr,
-						struct ib_udata *udata);
-	int                        (*modify_ah)(struct ib_ah *ah,
-						struct rdma_ah_attr *ah_attr);
-	int                        (*query_ah)(struct ib_ah *ah,
-					       struct rdma_ah_attr *ah_attr);
-	int                        (*destroy_ah)(struct ib_ah *ah);
-	struct ib_srq *            (*create_srq)(struct ib_pd *pd,
-						 struct ib_srq_init_attr *srq_init_attr,
-						 struct ib_udata *udata);
-	int                        (*modify_srq)(struct ib_srq *srq,
-						 struct ib_srq_attr *srq_attr,
-						 enum ib_srq_attr_mask srq_attr_mask,
-						 struct ib_udata *udata);
-	int                        (*query_srq)(struct ib_srq *srq,
-						struct ib_srq_attr *srq_attr);
-	int                        (*destroy_srq)(struct ib_srq *srq);
-	int                        (*post_srq_recv)(struct ib_srq *srq,
-						    const struct ib_recv_wr *recv_wr,
-						    const struct ib_recv_wr **bad_recv_wr);
-	struct ib_qp *             (*create_qp)(struct ib_pd *pd,
-						struct ib_qp_init_attr *qp_init_attr,
-						struct ib_udata *udata);
-	int                        (*modify_qp)(struct ib_qp *qp,
-						struct ib_qp_attr *qp_attr,
-						int qp_attr_mask,
-						struct ib_udata *udata);
-	int                        (*query_qp)(struct ib_qp *qp,
-					       struct ib_qp_attr *qp_attr,
-					       int qp_attr_mask,
-					       struct ib_qp_init_attr *qp_init_attr);
-	int                        (*destroy_qp)(struct ib_qp *qp);
-	int                        (*post_send)(struct ib_qp *qp,
-						const struct ib_send_wr *send_wr,
-						const struct ib_send_wr **bad_send_wr);
-	int                        (*post_recv)(struct ib_qp *qp,
-						const struct ib_recv_wr *recv_wr,
-						const struct ib_recv_wr **bad_recv_wr);
-	struct ib_cq *             (*create_cq)(struct ib_device *device,
-						const struct ib_cq_init_attr *attr,
-						struct ib_ucontext *context,
-						struct ib_udata *udata);
-	int                        (*modify_cq)(struct ib_cq *cq, u16 cq_count,
-						u16 cq_period);
-	int                        (*destroy_cq)(struct ib_cq *cq);
-	int                        (*resize_cq)(struct ib_cq *cq, int cqe,
-						struct ib_udata *udata);
-	int                        (*poll_cq)(struct ib_cq *cq, int num_entries,
-					      struct ib_wc *wc);
-	int                        (*peek_cq)(struct ib_cq *cq, int wc_cnt);
-	int                        (*req_notify_cq)(struct ib_cq *cq,
-						    enum ib_cq_notify_flags flags);
-	int                        (*req_ncomp_notif)(struct ib_cq *cq,
-						      int wc_cnt);
-	struct ib_mr *             (*get_dma_mr)(struct ib_pd *pd,
-						 int mr_access_flags);
-	struct ib_mr *             (*reg_user_mr)(struct ib_pd *pd,
-						  u64 start, u64 length,
-						  u64 virt_addr,
-						  int mr_access_flags,
-						  struct ib_udata *udata);
-	int			   (*rereg_user_mr)(struct ib_mr *mr,
-						    int flags,
-						    u64 start, u64 length,
-						    u64 virt_addr,
-						    int mr_access_flags,
-						    struct ib_pd *pd,
-						    struct ib_udata *udata);
-	int                        (*dereg_mr)(struct ib_mr *mr);
-	struct ib_mr *		   (*alloc_mr)(struct ib_pd *pd,
-					       enum ib_mr_type mr_type,
-					       u32 max_num_sg);
-	int                        (*map_mr_sg)(struct ib_mr *mr,
-						struct scatterlist *sg,
-						int sg_nents,
-						unsigned int *sg_offset);
-	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
-					       enum ib_mw_type type,
-					       struct ib_udata *udata);
-	int                        (*dealloc_mw)(struct ib_mw *mw);
-	struct ib_fmr *	           (*alloc_fmr)(struct ib_pd *pd,
-						int mr_access_flags,
-						struct ib_fmr_attr *fmr_attr);
-	int		           (*map_phys_fmr)(struct ib_fmr *fmr,
-						   u64 *page_list, int list_len,
-						   u64 iova);
-	int		           (*unmap_fmr)(struct list_head *fmr_list);
-	int		           (*dealloc_fmr)(struct ib_fmr *fmr);
-	int                        (*attach_mcast)(struct ib_qp *qp,
-						   union ib_gid *gid,
-						   u16 lid);
-	int                        (*detach_mcast)(struct ib_qp *qp,
-						   union ib_gid *gid,
-						   u16 lid);
-	int                        (*process_mad)(struct ib_device *device,
-						  int process_mad_flags,
-						  u8 port_num,
-						  const struct ib_wc *in_wc,
-						  const struct ib_grh *in_grh,
-						  const struct ib_mad_hdr *in_mad,
-						  size_t in_mad_size,
-						  struct ib_mad_hdr *out_mad,
-						  size_t *out_mad_size,
-						  u16 *out_mad_pkey_index);
-	struct ib_xrcd *	   (*alloc_xrcd)(struct ib_device *device,
-						 struct ib_ucontext *ucontext,
-						 struct ib_udata *udata);
-	int			   (*dealloc_xrcd)(struct ib_xrcd *xrcd);
-	struct ib_flow *	   (*create_flow)(struct ib_qp *qp,
-						  struct ib_flow_attr
-						  *flow_attr,
-						  int domain,
-						  struct ib_udata *udata);
-	int			   (*destroy_flow)(struct ib_flow *flow_id);
-	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
-						      struct ib_mr_status *mr_status);
-	void			   (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
-	void			   (*drain_rq)(struct ib_qp *qp);
-	void			   (*drain_sq)(struct ib_qp *qp);
-	int			   (*set_vf_link_state)(struct ib_device *device, int vf, u8 port,
-							int state);
-	int			   (*get_vf_config)(struct ib_device *device, int vf, u8 port,
-						   struct ifla_vf_info *ivf);
-	int			   (*get_vf_stats)(struct ib_device *device, int vf, u8 port,
-						   struct ifla_vf_stats *stats);
-	int			   (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
-						  int type);
-	struct ib_wq *		   (*create_wq)(struct ib_pd *pd,
-						struct ib_wq_init_attr *init_attr,
-						struct ib_udata *udata);
-	int			   (*destroy_wq)(struct ib_wq *wq);
-	int			   (*modify_wq)(struct ib_wq *wq,
-						struct ib_wq_attr *attr,
-						u32 wq_attr_mask,
-						struct ib_udata *udata);
-	struct ib_rwq_ind_table *  (*create_rwq_ind_table)(struct ib_device *device,
-							   struct ib_rwq_ind_table_init_attr *init_attr,
-							   struct ib_udata *udata);
-	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
-	struct ib_flow_action *	   (*create_flow_action_esp)(struct ib_device *device,
-							     const struct ib_flow_action_attrs_esp *attr,
-							     struct uverbs_attr_bundle *attrs);
-	int			   (*destroy_flow_action)(struct ib_flow_action *action);
-	int			   (*modify_flow_action_esp)(struct ib_flow_action *action,
-							     const struct ib_flow_action_attrs_esp *attr,
-							     struct uverbs_attr_bundle *attrs);
-	struct ib_dm *             (*alloc_dm)(struct ib_device *device,
-					       struct ib_ucontext *context,
-					       struct ib_dm_alloc_attr *attr,
-					       struct uverbs_attr_bundle *attrs);
-	int                        (*dealloc_dm)(struct ib_dm *dm);
-	struct ib_mr *             (*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,
-						struct ib_dm_mr_attr *attr,
-						struct uverbs_attr_bundle *attrs);
-	struct ib_counters *	(*create_counters)(struct ib_device *device,
-						   struct uverbs_attr_bundle *attrs);
-	int	(*destroy_counters)(struct ib_counters	*counters);
-	int	(*read_counters)(struct ib_counters *counters,
-				 struct ib_counters_read_attr *counters_read_attr,
-				 struct uverbs_attr_bundle *attrs);
-
-	/**
-	 * rdma netdev operation
-	 *
-	 * Driver implementing alloc_rdma_netdev or rdma_netdev_get_params
-	 * must return -EOPNOTSUPP if it doesn't support the specified type.
-	 */
-	struct net_device *(*alloc_rdma_netdev)(
-					struct ib_device *device,
-					u8 port_num,
-					enum rdma_netdev_t type,
-					const char *name,
-					unsigned char name_assign_type,
-					void (*setup)(struct net_device *));
-
-	int (*rdma_netdev_get_params)(struct ib_device *device, u8 port_num,
-				      enum rdma_netdev_t type,
-				      struct rdma_netdev_alloc_params *params);
-
 	struct module               *owner;
 	struct device                dev;
 	/* First group for device attributes,
@@ -2840,17 +2573,6 @@ struct ib_device {
 	 */
 	struct rdma_restrack_root     res;
 
-	/**
-	 * The following mandatory functions are used only at device
-	 * registration.  Keep functions such as these at the end of this
-	 * structure to avoid cache line misses when accessing struct ib_device
-	 * in fast paths.
-	 */
-	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
-	void (*get_dev_fw_str)(struct ib_device *, char *str);
-	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
-						     int comp_vector);
-
 	const struct uapi_definition   *driver_def;
 	enum rdma_driver_id		driver_id;
 	/*
@@ -3365,7 +3087,7 @@ static inline bool rdma_cap_roce_gid_table(const struct ib_device *device,
 					   u8 port_num)
 {
 	return rdma_protocol_roce(device, port_num) &&
-		device->add_gid && device->del_gid;
+		device->ops.add_gid && device->ops.del_gid;
 }
 
 /*
@@ -3589,7 +3311,8 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
 {
 	const struct ib_recv_wr *dummy;
 
-	return srq->device->post_srq_recv(srq, recv_wr, bad_recv_wr ? : &dummy);
+	return srq->device->ops.post_srq_recv(srq, recv_wr,
+					      bad_recv_wr ? : &dummy);
 }
 
 /**
@@ -3692,7 +3415,7 @@ static inline int ib_post_send(struct ib_qp *qp,
 {
 	const struct ib_send_wr *dummy;
 
-	return qp->device->post_send(qp, send_wr, bad_send_wr ? : &dummy);
+	return qp->device->ops.post_send(qp, send_wr, bad_send_wr ? : &dummy);
 }
 
 /**
@@ -3709,7 +3432,7 @@ static inline int ib_post_recv(struct ib_qp *qp,
 {
 	const struct ib_recv_wr *dummy;
 
-	return qp->device->post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);
+	return qp->device->ops.post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);
 }
 
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
@@ -3782,7 +3505,7 @@ int ib_destroy_cq(struct ib_cq *cq);
 static inline int ib_poll_cq(struct ib_cq *cq, int num_entries,
 			     struct ib_wc *wc)
 {
-	return cq->device->poll_cq(cq, num_entries, wc);
+	return cq->device->ops.poll_cq(cq, num_entries, wc);
 }
 
 /**
@@ -3815,7 +3538,7 @@ static inline int ib_poll_cq(struct ib_cq *cq, int num_entries,
 static inline int ib_req_notify_cq(struct ib_cq *cq,
 				   enum ib_cq_notify_flags flags)
 {
-	return cq->device->req_notify_cq(cq, flags);
+	return cq->device->ops.req_notify_cq(cq, flags);
 }
 
 /**
@@ -3827,8 +3550,8 @@ static inline int ib_req_notify_cq(struct ib_cq *cq,
  */
 static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
 {
-	return cq->device->req_ncomp_notif ?
-		cq->device->req_ncomp_notif(cq, wc_cnt) :
+	return cq->device->ops.req_ncomp_notif ?
+		cq->device->ops.req_ncomp_notif(cq, wc_cnt) :
 		-ENOSYS;
 }
 
@@ -4092,7 +3815,7 @@ static inline int ib_map_phys_fmr(struct ib_fmr *fmr,
 				  u64 *page_list, int list_len,
 				  u64 iova)
 {
-	return fmr->device->map_phys_fmr(fmr, page_list, list_len, iova);
+	return fmr->device->ops.map_phys_fmr(fmr, page_list, list_len, iova);
 }
 
 /**
@@ -4445,10 +4168,10 @@ static inline const struct cpumask *
 ib_get_vector_affinity(struct ib_device *device, int comp_vector)
 {
 	if (comp_vector < 0 || comp_vector >= device->num_comp_vectors ||
-	    !device->get_vector_affinity)
+	    !device->ops.get_vector_affinity)
 		return NULL;
 
-	return device->get_vector_affinity(device, comp_vector);
+	return device->ops.get_vector_affinity(device, comp_vector);
 
 }
 

commit 521ed0d92ab0db3edd17a5f4716b7f698f4fce61
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Mon Dec 10 21:09:30 2018 +0200

    RDMA/core: Introduce ib_device_ops
    
    This change introduces the ib_device_ops structure that defines all the
    InfiniBand device operations in one place, so the code will be more
    readable and clean, unlike today when the ops are mixed with ib_device
    data members.
    
    The providers will need to define the supported operations and assign them
    using ib_set_device_ops(), that will also make the providers code more
    readable and clean.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 89eead636e68..60315fd05411 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2264,6 +2264,246 @@ struct ib_counters_read_attr {
 
 struct uverbs_attr_bundle;
 
+/**
+ * struct ib_device_ops - InfiniBand device operations
+ * This structure defines all the InfiniBand device operations, providers will
+ * need to define the supported operations, otherwise they will be set to null.
+ */
+struct ib_device_ops {
+	int (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,
+			 const struct ib_send_wr **bad_send_wr);
+	int (*post_recv)(struct ib_qp *qp, const struct ib_recv_wr *recv_wr,
+			 const struct ib_recv_wr **bad_recv_wr);
+	void (*drain_rq)(struct ib_qp *qp);
+	void (*drain_sq)(struct ib_qp *qp);
+	int (*poll_cq)(struct ib_cq *cq, int num_entries, struct ib_wc *wc);
+	int (*peek_cq)(struct ib_cq *cq, int wc_cnt);
+	int (*req_notify_cq)(struct ib_cq *cq, enum ib_cq_notify_flags flags);
+	int (*req_ncomp_notif)(struct ib_cq *cq, int wc_cnt);
+	int (*post_srq_recv)(struct ib_srq *srq,
+			     const struct ib_recv_wr *recv_wr,
+			     const struct ib_recv_wr **bad_recv_wr);
+	int (*process_mad)(struct ib_device *device, int process_mad_flags,
+			   u8 port_num, const struct ib_wc *in_wc,
+			   const struct ib_grh *in_grh,
+			   const struct ib_mad_hdr *in_mad, size_t in_mad_size,
+			   struct ib_mad_hdr *out_mad, size_t *out_mad_size,
+			   u16 *out_mad_pkey_index);
+	int (*query_device)(struct ib_device *device,
+			    struct ib_device_attr *device_attr,
+			    struct ib_udata *udata);
+	int (*modify_device)(struct ib_device *device, int device_modify_mask,
+			     struct ib_device_modify *device_modify);
+	void (*get_dev_fw_str)(struct ib_device *device, char *str);
+	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
+						     int comp_vector);
+	int (*query_port)(struct ib_device *device, u8 port_num,
+			  struct ib_port_attr *port_attr);
+	int (*modify_port)(struct ib_device *device, u8 port_num,
+			   int port_modify_mask,
+			   struct ib_port_modify *port_modify);
+	/**
+	 * The following mandatory functions are used only at device
+	 * registration.  Keep functions such as these at the end of this
+	 * structure to avoid cache line misses when accessing struct ib_device
+	 * in fast paths.
+	 */
+	int (*get_port_immutable)(struct ib_device *device, u8 port_num,
+				  struct ib_port_immutable *immutable);
+	enum rdma_link_layer (*get_link_layer)(struct ib_device *device,
+					       u8 port_num);
+	/**
+	 * When calling get_netdev, the HW vendor's driver should return the
+	 * net device of device @device at port @port_num or NULL if such
+	 * a net device doesn't exist. The vendor driver should call dev_hold
+	 * on this net device. The HW vendor's device driver must guarantee
+	 * that this function returns NULL before the net device has finished
+	 * NETDEV_UNREGISTER state.
+	 */
+	struct net_device *(*get_netdev)(struct ib_device *device, u8 port_num);
+	/**
+	 * rdma netdev operation
+	 *
+	 * Driver implementing alloc_rdma_netdev or rdma_netdev_get_params
+	 * must return -EOPNOTSUPP if it doesn't support the specified type.
+	 */
+	struct net_device *(*alloc_rdma_netdev)(
+		struct ib_device *device, u8 port_num, enum rdma_netdev_t type,
+		const char *name, unsigned char name_assign_type,
+		void (*setup)(struct net_device *));
+
+	int (*rdma_netdev_get_params)(struct ib_device *device, u8 port_num,
+				      enum rdma_netdev_t type,
+				      struct rdma_netdev_alloc_params *params);
+	/**
+	 * query_gid should be return GID value for @device, when @port_num
+	 * link layer is either IB or iWarp. It is no-op if @port_num port
+	 * is RoCE link layer.
+	 */
+	int (*query_gid)(struct ib_device *device, u8 port_num, int index,
+			 union ib_gid *gid);
+	/**
+	 * When calling add_gid, the HW vendor's driver should add the gid
+	 * of device of port at gid index available at @attr. Meta-info of
+	 * that gid (for example, the network device related to this gid) is
+	 * available at @attr. @context allows the HW vendor driver to store
+	 * extra information together with a GID entry. The HW vendor driver may
+	 * allocate memory to contain this information and store it in @context
+	 * when a new GID entry is written to. Params are consistent until the
+	 * next call of add_gid or delete_gid. The function should return 0 on
+	 * success or error otherwise. The function could be called
+	 * concurrently for different ports. This function is only called when
+	 * roce_gid_table is used.
+	 */
+	int (*add_gid)(const struct ib_gid_attr *attr, void **context);
+	/**
+	 * When calling del_gid, the HW vendor's driver should delete the
+	 * gid of device @device at gid index gid_index of port port_num
+	 * available in @attr.
+	 * Upon the deletion of a GID entry, the HW vendor must free any
+	 * allocated memory. The caller will clear @context afterwards.
+	 * This function is only called when roce_gid_table is used.
+	 */
+	int (*del_gid)(const struct ib_gid_attr *attr, void **context);
+	int (*query_pkey)(struct ib_device *device, u8 port_num, u16 index,
+			  u16 *pkey);
+	struct ib_ucontext *(*alloc_ucontext)(struct ib_device *device,
+					      struct ib_udata *udata);
+	int (*dealloc_ucontext)(struct ib_ucontext *context);
+	int (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);
+	void (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
+	struct ib_pd *(*alloc_pd)(struct ib_device *device,
+				  struct ib_ucontext *context,
+				  struct ib_udata *udata);
+	int (*dealloc_pd)(struct ib_pd *pd);
+	struct ib_ah *(*create_ah)(struct ib_pd *pd,
+				   struct rdma_ah_attr *ah_attr,
+				   struct ib_udata *udata);
+	int (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
+	int (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
+	int (*destroy_ah)(struct ib_ah *ah);
+	struct ib_srq *(*create_srq)(struct ib_pd *pd,
+				     struct ib_srq_init_attr *srq_init_attr,
+				     struct ib_udata *udata);
+	int (*modify_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr,
+			  enum ib_srq_attr_mask srq_attr_mask,
+			  struct ib_udata *udata);
+	int (*query_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr);
+	int (*destroy_srq)(struct ib_srq *srq);
+	struct ib_qp *(*create_qp)(struct ib_pd *pd,
+				   struct ib_qp_init_attr *qp_init_attr,
+				   struct ib_udata *udata);
+	int (*modify_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
+			 int qp_attr_mask, struct ib_udata *udata);
+	int (*query_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
+			int qp_attr_mask, struct ib_qp_init_attr *qp_init_attr);
+	int (*destroy_qp)(struct ib_qp *qp);
+	struct ib_cq *(*create_cq)(struct ib_device *device,
+				   const struct ib_cq_init_attr *attr,
+				   struct ib_ucontext *context,
+				   struct ib_udata *udata);
+	int (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);
+	int (*destroy_cq)(struct ib_cq *cq);
+	int (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);
+	struct ib_mr *(*get_dma_mr)(struct ib_pd *pd, int mr_access_flags);
+	struct ib_mr *(*reg_user_mr)(struct ib_pd *pd, u64 start, u64 length,
+				     u64 virt_addr, int mr_access_flags,
+				     struct ib_udata *udata);
+	int (*rereg_user_mr)(struct ib_mr *mr, int flags, u64 start, u64 length,
+			     u64 virt_addr, int mr_access_flags,
+			     struct ib_pd *pd, struct ib_udata *udata);
+	int (*dereg_mr)(struct ib_mr *mr);
+	struct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,
+				  u32 max_num_sg);
+	int (*map_mr_sg)(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
+			 unsigned int *sg_offset);
+	int (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
+			       struct ib_mr_status *mr_status);
+	struct ib_mw *(*alloc_mw)(struct ib_pd *pd, enum ib_mw_type type,
+				  struct ib_udata *udata);
+	int (*dealloc_mw)(struct ib_mw *mw);
+	struct ib_fmr *(*alloc_fmr)(struct ib_pd *pd, int mr_access_flags,
+				    struct ib_fmr_attr *fmr_attr);
+	int (*map_phys_fmr)(struct ib_fmr *fmr, u64 *page_list, int list_len,
+			    u64 iova);
+	int (*unmap_fmr)(struct list_head *fmr_list);
+	int (*dealloc_fmr)(struct ib_fmr *fmr);
+	int (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
+	int (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);
+	struct ib_xrcd *(*alloc_xrcd)(struct ib_device *device,
+				      struct ib_ucontext *ucontext,
+				      struct ib_udata *udata);
+	int (*dealloc_xrcd)(struct ib_xrcd *xrcd);
+	struct ib_flow *(*create_flow)(struct ib_qp *qp,
+				       struct ib_flow_attr *flow_attr,
+				       int domain, struct ib_udata *udata);
+	int (*destroy_flow)(struct ib_flow *flow_id);
+	struct ib_flow_action *(*create_flow_action_esp)(
+		struct ib_device *device,
+		const struct ib_flow_action_attrs_esp *attr,
+		struct uverbs_attr_bundle *attrs);
+	int (*destroy_flow_action)(struct ib_flow_action *action);
+	int (*modify_flow_action_esp)(
+		struct ib_flow_action *action,
+		const struct ib_flow_action_attrs_esp *attr,
+		struct uverbs_attr_bundle *attrs);
+	int (*set_vf_link_state)(struct ib_device *device, int vf, u8 port,
+				 int state);
+	int (*get_vf_config)(struct ib_device *device, int vf, u8 port,
+			     struct ifla_vf_info *ivf);
+	int (*get_vf_stats)(struct ib_device *device, int vf, u8 port,
+			    struct ifla_vf_stats *stats);
+	int (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
+			   int type);
+	struct ib_wq *(*create_wq)(struct ib_pd *pd,
+				   struct ib_wq_init_attr *init_attr,
+				   struct ib_udata *udata);
+	int (*destroy_wq)(struct ib_wq *wq);
+	int (*modify_wq)(struct ib_wq *wq, struct ib_wq_attr *attr,
+			 u32 wq_attr_mask, struct ib_udata *udata);
+	struct ib_rwq_ind_table *(*create_rwq_ind_table)(
+		struct ib_device *device,
+		struct ib_rwq_ind_table_init_attr *init_attr,
+		struct ib_udata *udata);
+	int (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
+	struct ib_dm *(*alloc_dm)(struct ib_device *device,
+				  struct ib_ucontext *context,
+				  struct ib_dm_alloc_attr *attr,
+				  struct uverbs_attr_bundle *attrs);
+	int (*dealloc_dm)(struct ib_dm *dm);
+	struct ib_mr *(*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,
+				   struct ib_dm_mr_attr *attr,
+				   struct uverbs_attr_bundle *attrs);
+	struct ib_counters *(*create_counters)(
+		struct ib_device *device, struct uverbs_attr_bundle *attrs);
+	int (*destroy_counters)(struct ib_counters *counters);
+	int (*read_counters)(struct ib_counters *counters,
+			     struct ib_counters_read_attr *counters_read_attr,
+			     struct uverbs_attr_bundle *attrs);
+	/**
+	 * alloc_hw_stats - Allocate a struct rdma_hw_stats and fill in the
+	 *   driver initialized data.  The struct is kfree()'ed by the sysfs
+	 *   core when the device is removed.  A lifespan of -1 in the return
+	 *   struct tells the core to set a default lifespan.
+	 */
+	struct rdma_hw_stats *(*alloc_hw_stats)(struct ib_device *device,
+						u8 port_num);
+	/**
+	 * get_hw_stats - Fill in the counter value(s) in the stats struct.
+	 * @index - The index in the value array we wish to have updated, or
+	 *   num_counters if we want all stats updated
+	 * Return codes -
+	 *   < 0 - Error, no counters updated
+	 *   index - Updated the single counter pointed to by index
+	 *   num_counters - Updated all counters (will reset the timestamp
+	 *     and prevent further calls for lifespan milliseconds)
+	 * Drivers are allowed to update all counters in leiu of just the
+	 *   one given in index at their option
+	 */
+	int (*get_hw_stats)(struct ib_device *device,
+			    struct rdma_hw_stats *stats, u8 port, int index);
+};
+
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -2667,6 +2907,8 @@ void ib_unregister_client(struct ib_client *client);
 void *ib_get_client_data(struct ib_device *device, struct ib_client *client);
 void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
 			 void *data);
+void ib_set_device_ops(struct ib_device *device,
+		       const struct ib_device_ops *ops);
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,

commit a5a5d1993696419e7d5357fc3128e53d219d382e
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Sun Dec 9 11:49:50 2018 +0200

    IB/core: Add new IB rates
    
    Add the new rates that were added to Infiniband spec as part of HDR and 2x
    support.
    
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a7b839312671..89eead636e68 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -732,7 +732,11 @@ enum ib_rate {
 	IB_RATE_25_GBPS  = 15,
 	IB_RATE_100_GBPS = 16,
 	IB_RATE_200_GBPS = 17,
-	IB_RATE_300_GBPS = 18
+	IB_RATE_300_GBPS = 18,
+	IB_RATE_28_GBPS  = 19,
+	IB_RATE_50_GBPS  = 20,
+	IB_RATE_400_GBPS = 21,
+	IB_RATE_600_GBPS = 22,
 };
 
 /**

commit dbabf68574f96d49cfa4eb6e8e56178874477535
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Sun Dec 9 11:49:49 2018 +0200

    IB/core: Add 2X port width
    
    Add the new 2X port width that is part of IB spec 1.3
    
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b03e24852186..a7b839312671 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -434,6 +434,7 @@ enum ib_port_state {
 
 enum ib_port_width {
 	IB_WIDTH_1X	= 1,
+	IB_WIDTH_2X	= 16,
 	IB_WIDTH_4X	= 2,
 	IB_WIDTH_8X	= 4,
 	IB_WIDTH_12X	= 8
@@ -443,6 +444,7 @@ static inline int ib_width_enum_to_int(enum ib_port_width width)
 {
 	switch (width) {
 	case IB_WIDTH_1X:  return  1;
+	case IB_WIDTH_2X:  return  2;
 	case IB_WIDTH_4X:  return  4;
 	case IB_WIDTH_8X:  return  8;
 	case IB_WIDTH_12X: return 12;

commit 1e8f43b7fb25aa7a85db1e81b4689dd394e23b35
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Sun Dec 9 11:49:48 2018 +0200

    IB/core: Add CapabilityMask2 to port attributes
    
    CapabilityMask2 was added in IB Spec 1.3 under PortInfo attribute.  The
    new Capapbility mask is needed in order to expose the new 2X width and HDR
    speed.
    
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 85021451eee0..b03e24852186 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -592,6 +592,7 @@ struct ib_port_attr {
 	u8			active_width;
 	u8			active_speed;
 	u8                      phys_state;
+	u16			port_cap_flags2;
 };
 
 enum ib_device_modify_flags {

commit 606152107bbdbc0e21f25e0d15ef2787a4ab90fd
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Nov 28 13:16:43 2018 +0200

    RDMA/restrack: Track ucontext
    
    Add ability to track allocated ib_ucontext, which are limited
    resource and worth to be visible by users.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 92633c15125b..85021451eee0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1505,6 +1505,10 @@ struct ib_ucontext {
 #endif
 
 	struct ib_rdmacg_object	cg_obj;
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct rdma_restrack_entry res;
 };
 
 struct ib_uobject {

commit 15a1b4becba886176aa1426604548c34904fd054
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Nov 25 20:51:15 2018 +0200

    RDMA/uverbs: Do not pass ib_uverbs_file to ioctl methods
    
    The uverbs_attr_bundle already contains this pointer, and most methods
    don't actually need it. Get rid of the redundant function argument.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0b7269870571..92633c15125b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4209,8 +4209,8 @@ void rdma_roce_rescan_device(struct ib_device *ibdev);
 
 struct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile);
 
-int uverbs_destroy_def_handler(struct ib_uverbs_file *file,
-			       struct uverbs_attr_bundle *attrs);
+
+int uverbs_destroy_def_handler(struct uverbs_attr_bundle *attrs);
 
 struct net_device *rdma_alloc_netdev(struct ib_device *device, u8 port_num,
 				     enum rdma_netdev_t type, const char *name,

commit 8313c10fa8be032fccc1e757bccc21207f533127
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Nov 25 20:51:13 2018 +0200

    RDMA/uverbs: Replace ib_uverbs_file with uverbs_attr_bundle for write
    
    Now that we can add meta-data to the description of write() methods we
    need to pass the uverbs_attr_bundle into all write based handlers so
    future patches can use it as a container for any new data transferred out
    of the core.
    
    This is the first step to bringing the write() and ioctl() methods to a
    common interface signature.
    
    This is a simple search/replace, and we push the attr down into the uobj
    and other APIs to keep changes minimal.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ee8a6008e222..0b7269870571 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4207,7 +4207,7 @@ ib_get_vector_affinity(struct ib_device *device, int comp_vector)
  */
 void rdma_roce_rescan_device(struct ib_device *ibdev);
 
-struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
+struct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile);
 
 int uverbs_destroy_def_handler(struct ib_uverbs_file *file,
 			       struct uverbs_attr_bundle *attrs);

commit 01b671170d7f82b959dad6d5dbb44d7a915e647d
Author: Parav Pandit <parav@mellanox.com>
Date:   Fri Nov 16 03:50:57 2018 +0200

    RDMA/core: Sync unregistration with netlink commands
    
    When the rdma device is getting removed, get resource info can race with
    device removal, as below:
    
          CPU-0                                  CPU-1
        --------                               --------
        rdma_nl_rcv_msg()
           nldev_res_get_cq_dumpit()
              mutex_lock(device_lock);
              get device reference
              mutex_unlock(device_lock);        [..]
                                                ib_unregister_device()
                                                /* Valid reference to
                                                 * device->dev exists.
                                                 */
                                                 ib_dealloc_device()
    
              [..]
              provider->fill_res_entry();
    
    Even though device object is not freed, fill_res_entry() can get called on
    device which doesn't have a driver anymore. Kernel core device reference
    count is not sufficient, as this only keeps the structure valid, and
    doesn't guarantee the driver is still loaded.
    
    Similar race can occur with device renaming and device removal, where
    device_rename() tries to rename a unregistered device. While this is fine
    for devices of a class which are not net namespace aware, but it is
    incorrect for net namespace aware class coming in subsequent series.  If a
    class is net namespace aware, then the below [1] call trace is observed in
    above situation.
    
    Therefore, to avoid the race, keep a reference count and let device
    unregistration wait until all netlink users drop the reference.
    
    [1] Call trace:
    kernfs: ns required in 'infiniband' for 'mlx5_0'
    WARNING: CPU: 18 PID: 44270 at fs/kernfs/dir.c:842 kernfs_find_ns+0x104/0x120
    libahci i2c_core mlxfw libata dca [last unloaded: devlink]
    RIP: 0010:kernfs_find_ns+0x104/0x120
    Call Trace:
    kernfs_find_and_get_ns+0x2e/0x50
    sysfs_rename_link_ns+0x40/0xb0
    device_rename+0xb2/0xf0
    ib_device_rename+0xb3/0x100 [ib_core]
    nldev_set_doit+0x165/0x190 [ib_core]
    rdma_nl_rcv_msg+0x249/0x250 [ib_core]
    ? netlink_deliver_tap+0x8f/0x3e0
    rdma_nl_rcv+0xd6/0x120 [ib_core]
    netlink_unicast+0x17c/0x230
    netlink_sendmsg+0x2f0/0x3e0
    sock_sendmsg+0x30/0x40
    __sys_sendto+0xdc/0x160
    
    Fixes: da5c85078215 ("RDMA/nldev: add driver-specific resource tracking")
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1af7bf34c04c..ee8a6008e222 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -53,7 +53,7 @@
 #include <linux/string.h>
 #include <linux/slab.h>
 #include <linux/netdevice.h>
-
+#include <linux/refcount.h>
 #include <linux/if_link.h>
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
@@ -2602,6 +2602,12 @@ struct ib_device {
 
 	const struct uapi_definition   *driver_def;
 	enum rdma_driver_id		driver_id;
+	/*
+	 * Provides synchronization between device unregistration and netlink
+	 * commands on a device. To be used only by core.
+	 */
+	refcount_t refcount;
+	struct completion unreg_completion;
 };
 
 struct ib_client {

commit 0cbf432db405289216747a8d31d74bab2452337c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Nov 12 22:59:50 2018 +0200

    RDMA/uverbs: Use a linear list to describe the compiled-in uapi
    
    The 'tree' data structure is very hard to build at compile time, and this
    makes it very limited. The new radix tree based compiler can handle a more
    complex input language that does not require the compiler to perfectly
    group everything into a neat tree structure.
    
    Instead use a simple list to describe to input, where the list elements
    can be of various different 'opcodes' instructing the radix compiler what
    to do. Start out with opcodes chaining to other definition lists and
    chaining to the existing 'tree' definition.
    
    Replace the very top level of the 'object tree' with this list type and
    get rid of struct uverbs_object_tree_def and DECLARE_UVERBS_OBJECT_TREE.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 86313e1e9210..1af7bf34c04c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2600,7 +2600,7 @@ struct ib_device {
 	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
 						     int comp_vector);
 
-	const struct uverbs_object_tree_def *const *driver_specs;
+	const struct uapi_definition   *driver_def;
 	enum rdma_driver_id		driver_id;
 };
 

commit 2dc50c5a543148e2a02926214a786805dc4d947b
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Nov 15 04:03:35 2018 +0200

    RDMA/core: Remove unused header files mm.h, socket.h, scatterlist.h
    
    Structures of ib_verbs.h don't use fields/structures of mm.h, socket.h or
    scatterlist.h.  So remove such header files inclusion.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bb63d9f75fdd..86313e1e9210 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -41,14 +41,11 @@
 
 #include <linux/types.h>
 #include <linux/device.h>
-#include <linux/mm.h>
 #include <linux/dma-mapping.h>
 #include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/rwsem.h>
-#include <linux/scatterlist.h>
 #include <linux/workqueue.h>
-#include <linux/socket.h>
 #include <linux/irq_poll.h>
 #include <uapi/linux/if_ether.h>
 #include <net/ipv6.h>

commit ce1fd6bfb15da135158e93a0ee4c0540cd26b43c
Author: Rami Rosen <ramirose@gmail.com>
Date:   Sun Oct 28 21:04:00 2018 +0200

    IB/uverbs: fix a typo
    
    This patch fixes a typo in include/rdma/ib_verbs.h.
    See: https://www.merriam-webster.com/dictionary/lieu
    
    Signed-off-by: Rami Rosen <ramirose@gmail.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9c0c2132a2d6..bb63d9f75fdd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2301,7 +2301,7 @@ struct ib_device {
 	 *   index - Updated the single counter pointed to by index
 	 *   num_counters - Updated all counters (will reset the timestamp
 	 *     and prevent further calls for lifespan milliseconds)
-	 * Drivers are allowed to update all counters in leiu of just the
+	 * Drivers are allowed to update all counters in lieu of just the
 	 *   one given in index at their option
 	 */
 	int		           (*get_hw_stats)(struct ib_device *device,

commit da19a102ce87bf3e0a7fe277a659d1fc35330d6d
Merge: e5f6d9afa341 a60109dc9a95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 07:38:19 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "This has been a smaller cycle with many of the commits being smallish
      code fixes and improvements across the drivers.
    
       - Driver updates for bnxt_re, cxgb4, hfi1, hns, mlx5, nes, qedr, and
         rxe
    
       - Memory window support in hns
    
       - mlx5 user API 'flow mutate/steering' allows accessing the full
         packet mangling and matching machinery from user space
    
       - Support inter-working with verbs API calls in the 'devx' mlx5 user
         API, and provide options to use devx with less privilege
    
       - Modernize the use of syfs and the device interface to use attribute
         groups and cdev properly for uverbs, and clean up some of the core
         code's device list management
    
       - More progress on net namespaces for RDMA devices
    
       - Consolidate driver BAR mmapping support into core code helpers and
         rework how RDMA holds poitners to mm_struct for get_user_pages
         cases
    
       - First pass to use 'dev_name' instead of ib_device->name
    
       - Device renaming for RDMA devices"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (242 commits)
      IB/mlx5: Add support for extended atomic operations
      RDMA/core: Fix comment for hw stats init for port == 0
      RDMA/core: Refactor ib_register_device() function
      RDMA/core: Fix unwinding flow in case of error to register device
      ib_srp: Remove WARN_ON in srp_terminate_io()
      IB/mlx5: Allow scatter to CQE without global signaled WRs
      IB/mlx5: Verify that driver supports user flags
      IB/mlx5: Support scatter to CQE for DC transport type
      RDMA/drivers: Use core provided API for registering device attributes
      RDMA/core: Allow existing drivers to set one sysfs group per device
      IB/rxe: Remove unnecessary enum values
      RDMA/umad: Use kernel API to allocate umad indexes
      RDMA/uverbs: Use kernel API to allocate uverbs indexes
      RDMA/core: Increase total number of RDMA ports across all devices
      IB/mlx4: Add port and TID to MAD debug print
      IB/mlx4: Enable debug print of SMPs
      RDMA/core: Rename ports_parent to ports_kobj
      RDMA/core: Do not expose unsupported counters
      IB/mlx4: Refer to the device kobject instead of ports_parent
      RDMA/nldev: Allow IB device rename through RDMA netlink
      ...

commit d4122f5abef844112799d2056fdc7bbedbc913f3
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Oct 11 22:31:53 2018 +0300

    RDMA/core: Allow existing drivers to set one sysfs group per device
    
    Currently many rdma drivers are creating device attribute files using
    device_create_file() with device specific attributes.  Device specific
    attributes should be exposed via well defined netlink device attributes in
    future.
    
    Introduce an API rdma_set_device_sysfs_group() for existing drivers to set
    a group for sysfs attributes for legacy.
    
    This API is only for exposing legacy attributes which existed for sometime
    now.  New drivers should not be using this API and rather follow netlink
    path.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7d732cf87886..b17eea0373cb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2539,8 +2539,11 @@ struct ib_device {
 
 	struct module               *owner;
 	struct device                dev;
-	/* First group for device attributes, NULL terminated array */
-	const struct attribute_group	*groups[2];
+	/* First group for device attributes,
+	 * Second group for driver provided attributes (optional).
+	 * It is NULL terminated array.
+	 */
+	const struct attribute_group	*groups[3];
 
 	struct kobject			*ports_kobj;
 	struct list_head             port_list;
@@ -4191,4 +4194,27 @@ struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
 
 int uverbs_destroy_def_handler(struct ib_uverbs_file *file,
 			       struct uverbs_attr_bundle *attrs);
+
+/**
+ * rdma_set_device_sysfs_group - Set device attributes group to have
+ *				 driver specific sysfs entries at
+ *				 for infiniband class.
+ *
+ * @device:	device pointer for which attributes to be created
+ * @group:	Pointer to group which should be added when device
+ *		is registered with sysfs.
+ * rdma_set_device_sysfs_group() allows existing drivers to expose one
+ * group per device to have sysfs attributes.
+ *
+ * NOTE: New drivers should not make use of this API; instead new device
+ * parameter should be exposed via netlink command. This API and mechanism
+ * exist only for existing drivers.
+ */
+static inline void
+rdma_set_device_sysfs_group(struct ib_device *dev,
+			    const struct attribute_group *group)
+{
+	dev->groups[1] = group;
+}
+
 #endif /* IB_VERBS_H */

commit 1ae4cfa03902c83d1d77123e5ac8f0812c61b90e
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:41 2018 +0300

    RDMA/core: Rename ports_parent to ports_kobj
    
    Normally kobj objects have kobj suffix to reflect it.
    Rename ports_parent to ports_kobj.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7ce617d77f8f..7d732cf87886 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2542,7 +2542,7 @@ struct ib_device {
 	/* First group for device attributes, NULL terminated array */
 	const struct attribute_group	*groups[2];
 
-	struct kobject               *ports_parent;
+	struct kobject			*ports_kobj;
 	struct list_head             port_list;
 
 	enum {

commit 5d6b0cb3369df425de75c94c98eb3f1a86659022
Author: Denis Drozdov <denisd@mellanox.com>
Date:   Tue Aug 14 14:22:35 2018 +0300

    RDMA/netdev: Fix netlink support in IPoIB
    
    IPoIB netlink support was broken by the below commit since integrating
    the rdma_netdev support relies on an allocation flow for netdevs that
    was controlled by the ipoib driver while netdev's rtnl_newlink
    implementation assumes that the netdev will be allocated by netlink.
    Such situation leads to crash in __ipoib_device_add, once trying to
    reuse netlink device.
    
    This patch fixes the kernel oops for both mlx4 and mlx5
    devices triggered by the following command:
    
    Fixes: cd565b4b51e5 ("IB/IPoIB: Support acceleration options callbacks")
    Signed-off-by: Denis Drozdov <denisd@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 020216cee8f1..0ed5d913a492 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4198,4 +4198,11 @@ struct net_device *rdma_alloc_netdev(struct ib_device *device, u8 port_num,
 				     enum rdma_netdev_t type, const char *name,
 				     unsigned char name_assign_type,
 				     void (*setup)(struct net_device *));
+
+int rdma_init_netdev(struct ib_device *device, u8 port_num,
+		     enum rdma_netdev_t type, const char *name,
+		     unsigned char name_assign_type,
+		     void (*setup)(struct net_device *),
+		     struct net_device *netdev);
+
 #endif /* IB_VERBS_H */

commit f6a8a19bb11b46d60250ddc4e3e1ba6aa166f488
Author: Denis Drozdov <denisd@mellanox.com>
Date:   Tue Aug 14 14:08:51 2018 +0300

    RDMA/netdev: Hoist alloc_netdev_mqs out of the driver
    
    netdev has several interfaces that expect to call alloc_netdev_mqs from
    the core code, with the driver only providing the arguments.  This is
    incompatible with the rdma_netdev interface that returns the netdev
    directly.
    
    Thus re-organize the API used by ipoib so that the verbs core code calls
    alloc_netdev_mqs for the driver. This is done by allowing the drivers to
    provide the allocation parameters via a 'get_params' callback and then
    initializing an allocated netdev as a second step.
    
    Fixes: cd565b4b51e5 ("IB/IPoIB: Support acceleration options callbacks")
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Denis Drozdov <denisd@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e950c2a68f06..020216cee8f1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2223,6 +2223,16 @@ struct rdma_netdev {
 			    union ib_gid *gid, u16 mlid);
 };
 
+struct rdma_netdev_alloc_params {
+	size_t sizeof_priv;
+	unsigned int txqs;
+	unsigned int rxqs;
+	void *param;
+
+	int (*initialize_rdma_netdev)(struct ib_device *device, u8 port_num,
+				      struct net_device *netdev, void *param);
+};
+
 struct ib_port_pkey_list {
 	/* Lock to hold while modifying the list. */
 	spinlock_t                    list_lock;
@@ -2523,8 +2533,8 @@ struct ib_device {
 	/**
 	 * rdma netdev operation
 	 *
-	 * Driver implementing alloc_rdma_netdev must return -EOPNOTSUPP if it
-	 * doesn't support the specified rdma netdev type.
+	 * Driver implementing alloc_rdma_netdev or rdma_netdev_get_params
+	 * must return -EOPNOTSUPP if it doesn't support the specified type.
 	 */
 	struct net_device *(*alloc_rdma_netdev)(
 					struct ib_device *device,
@@ -2534,6 +2544,10 @@ struct ib_device {
 					unsigned char name_assign_type,
 					void (*setup)(struct net_device *));
 
+	int (*rdma_netdev_get_params)(struct ib_device *device, u8 port_num,
+				      enum rdma_netdev_t type,
+				      struct rdma_netdev_alloc_params *params);
+
 	struct module               *owner;
 	struct device                dev;
 	struct kobject               *ports_parent;
@@ -4179,4 +4193,9 @@ struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
 
 int uverbs_destroy_def_handler(struct ib_uverbs_file *file,
 			       struct uverbs_attr_bundle *attrs);
+
+struct net_device *rdma_alloc_netdev(struct ib_device *device, u8 port_num,
+				     enum rdma_netdev_t type, const char *name,
+				     unsigned char name_assign_type,
+				     void (*setup)(struct net_device *));
 #endif /* IB_VERBS_H */

commit b56511c15713ba6c7572e77a41f7ddba9c1053ec
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Mon Sep 24 12:57:16 2018 -0700

    IB/mlx4: Avoid implicit enumerated type conversion
    
    Clang warns when one enumerated type is implicitly converted to another.
    
    drivers/infiniband/hw/mlx4/mad.c:1811:41: warning: implicit conversion
    from enumeration type 'enum mlx4_ib_qp_flags' to different enumeration
    type 'enum ib_qp_create_flags' [-Wenum-conversion]
                    qp_init_attr.init_attr.create_flags = MLX4_IB_SRIOV_TUNNEL_QP;
                                                        ~ ^~~~~~~~~~~~~~~~~~~~~~~
    
    drivers/infiniband/hw/mlx4/mad.c:1819:41: warning: implicit conversion
    from enumeration type 'enum mlx4_ib_qp_flags' to different enumeration
    type 'enum ib_qp_create_flags' [-Wenum-conversion]
                    qp_init_attr.init_attr.create_flags = MLX4_IB_SRIOV_SQP;
                                                        ~ ^~~~~~~~~~~~~~~~~
    
    The type mlx4_ib_qp_flags explicitly provides supplemental values to the
    type ib_qp_create_flags. Make that clear to Clang by changing the
    create_flags type to u32.
    
    Reported-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f88c1071413a..7ce617d77f8f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1151,7 +1151,7 @@ struct ib_qp_init_attr {
 	struct ib_qp_cap	cap;
 	enum ib_sig_type	sq_sig_type;
 	enum ib_qp_type		qp_type;
-	enum ib_qp_create_flags	create_flags;
+	u32			create_flags;
 
 	/*
 	 * Only needed for special QP types, or when using the RW API.

commit d31131bba5a1630304c55ea775c48cc84912ab59
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Tue Oct 2 16:11:21 2018 +0300

    RDMA: Remove unused parameter from ib_modify_qp_is_ok()
    
    The ll parameter is not used in ib_modify_qp_is_ok(), so remove it.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9897d2329f2c..f88c1071413a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2742,7 +2742,6 @@ static inline int ib_destroy_usecnt(atomic_t *usecnt,
  * @next_state: Next QP state
  * @type: QP type
  * @mask: Mask of supplied QP attributes
- * @ll : link layer of port
  *
  * This function is a helper function that a low-level driver's
  * modify_qp method can use to validate the consumer's input.  It
@@ -2751,8 +2750,7 @@ static inline int ib_destroy_usecnt(atomic_t *usecnt,
  * and that the attribute mask supplied is allowed for the transition.
  */
 bool ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
-			enum ib_qp_type type, enum ib_qp_attr_mask mask,
-			enum rdma_link_layer ll);
+			enum ib_qp_type type, enum ib_qp_attr_mask mask);
 
 void ib_register_event_handler(struct ib_event_handler *event_handler);
 void ib_unregister_event_handler(struct ib_event_handler *event_handler);

commit e349f858d29f300ad9ad327fd57735a1d15e147f
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Sep 25 16:58:09 2018 -0600

    RDMA: Fully setup the device name in ib_register_device
    
    The current code has two copies of the device name, ibdev->dev and
    dev_name(&ibdev->dev), and they are setup at different times, which is
    very confusing.
    
    Set them both up at the same time and make dev_name() the lead name, which
    is the proper use of the driver core APIs. To make it very clear that the
    name is not valid until registration pass it in to the
    ib_register_device() call rather than messing with ibdev->name directly.
    
    Also the reorganization now checks that dev_name is unique even if it does
    not contain a %.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Acked-by: Adit Ranadive <aditr@vmware.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Acked-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Reviewed-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0d822a9db300..9897d2329f2c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2625,9 +2625,9 @@ void ib_dealloc_device(struct ib_device *device);
 
 void ib_get_device_fw_str(struct ib_device *device, char *str);
 
-int ib_register_device(struct ib_device *device,
-		       int (*port_callback)(struct ib_device *,
-					    u8, struct kobject *));
+int ib_register_device(struct ib_device *device, const char *name,
+		       int (*port_callback)(struct ib_device *, u8,
+					    struct kobject *));
 void ib_unregister_device(struct ib_device *device);
 
 int ib_register_client   (struct ib_client *client);

commit 2a3ccfdbeb6a5f832d7203e230799f1ffa46e0fc
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:12 2018 +0300

    RDMA/uverbs: Get rid of ucontext->tgid
    
    Nothing uses this now, just delete it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6437e6af758d..0d822a9db300 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1500,7 +1500,6 @@ struct ib_ucontext {
 
 	bool cleanup_retryable;
 
-	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);

commit f27a0d50a4bc2861b472c2e3740d63a29d1ac460
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:08 2018 +0300

    RDMA/umem: Use umem->owning_mm inside ODP
    
    Since ODP had a single struct mmu_notifier located in the ucontext it
    could only handle a single MM at a time, and this prevented it from using
    the new owning_mm system.
    
    With the prior rework it is now simple to let ODP track multiple MMs per
    ucontext, finish the job so that the per_mm is allocated on a mm by mm
    basis, and freed when the last umem is dropped from the ucontext.
    
    As a side effect the new saner locking removes the lockdep splat about
    nesting the umem_rwsem between mmu_notifier_unregister and
    ib_umem_odp_release.
    
    It also makes ODP work with multiple processes, across, fork, etc.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2cf2cee5a753..6437e6af758d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1488,25 +1488,6 @@ struct ib_rdmacg_object {
 #endif
 };
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-struct ib_ucontext_per_mm {
-	struct ib_ucontext *context;
-
-	struct rb_root_cached umem_tree;
-	/*
-	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
-	 * mmu notifiers registration.
-	 */
-	struct rw_semaphore umem_rwsem;
-
-	struct mmu_notifier mn;
-	atomic_t notifier_count;
-	/* A list of umems that don't have private mmu notifier counters yet. */
-	struct list_head no_private_counters;
-	unsigned int odp_mrs_count;
-};
-#endif
-
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct ib_uverbs_file  *ufile;
@@ -1523,7 +1504,8 @@ struct ib_ucontext {
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
-	struct ib_ucontext_per_mm per_mm;
+	struct mutex per_mm_list_lock;
+	struct list_head per_mm_list;
 #endif
 
 	struct ib_rdmacg_object	cg_obj;

commit c9990ab39b6e911003bab10a6da96e98ab1503a3
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:07 2018 +0300

    RDMA/umem: Move all the ODP related stuff out of ucontext and into per_mm
    
    This is the first step to make ODP use the owning_mm that is now part of
    struct ib_umem.
    
    Each ODP umem is linked to a single per_mm structure, which in turn, is
    linked to a single mm, via the embedded mmu_notifier. This first patch
    introduces the structure and reworks eveything to use it.
    
    This also needs to introduce tgid into the ib_ucontext_per_mm, as
    get_user_pages_remote() requires the originating task for statistics
    tracking.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d611ce9df7fb..2cf2cee5a753 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1488,6 +1488,25 @@ struct ib_rdmacg_object {
 #endif
 };
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+struct ib_ucontext_per_mm {
+	struct ib_ucontext *context;
+
+	struct rb_root_cached umem_tree;
+	/*
+	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
+	 * mmu notifiers registration.
+	 */
+	struct rw_semaphore umem_rwsem;
+
+	struct mmu_notifier mn;
+	atomic_t notifier_count;
+	/* A list of umems that don't have private mmu notifier counters yet. */
+	struct list_head no_private_counters;
+	unsigned int odp_mrs_count;
+};
+#endif
+
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct ib_uverbs_file  *ufile;
@@ -1502,20 +1521,9 @@ struct ib_ucontext {
 
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	struct rb_root_cached   umem_tree;
-	/*
-	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
-	 * mmu notifiers registration.
-	 */
-	struct rw_semaphore	umem_rwsem;
 	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
-
-	struct mmu_notifier	mn;
-	atomic_t		notifier_count;
-	/* A list of umems that don't have private mmu notifier counters yet. */
-	struct list_head	no_private_counters;
-	int                     odp_mrs_count;
+	struct ib_ucontext_per_mm per_mm;
 #endif
 
 	struct ib_rdmacg_object	cg_obj;

commit b5231b019d76521dd8c59a54c174770ec92c767c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:04 2018 +0300

    RDMA/umem: Use ib_umem_odp in all function signatures connected to ODP
    
    All of these functions already require the ODP version of the umem struct,
    make this very clear by having the signature require it. This paves the
    way to using the container_of() pattern to link umem_odp and umem
    together.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a66238d8a2a3..d611ce9df7fb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -69,6 +69,8 @@
 
 #define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
 
+struct ib_umem_odp;
+
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
 extern struct workqueue_struct *ib_comp_unbound_wq;
@@ -1506,7 +1508,7 @@ struct ib_ucontext {
 	 * mmu notifiers registration.
 	 */
 	struct rw_semaphore	umem_rwsem;
-	void (*invalidate_range)(struct ib_umem *umem,
+	void (*invalidate_range)(struct ib_umem_odp *umem_odp,
 				 unsigned long start, unsigned long end);
 
 	struct mmu_notifier	mn;

commit 5f9794dc94f59ad1eb821724a8ae1f8e803ea188
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:43:08 2018 +0300

    RDMA/ucontext: Add a core API for mmaping driver IO memory
    
    To support disassociation and PCI hot unplug, we have to track all the
    VMAs that refer to the device IO memory. When disassociation occurs the
    VMAs have to be revised to point to the zero page, not the IO memory, to
    allow the physical HW to be unplugged.
    
    The three drivers supporting this implemented three different versions
    of this algorithm, all leaving something to be desired. This new common
    implementation has a few differences from the driver versions:
    
    - Track all VMAs, including splitting/truncating/etc. Tie the lifetime of
      the private data allocation to the lifetime of the vma. This avoids any
      tricks with setting vm_ops which Linus didn't like. (see link)
    - Support multiple mms, and support properly tracking mmaps triggered by
      processes other than the one first opening the uverbs fd. This makes
      fork behavior of disassociation enabled drivers the same as fork support
      in normal drivers.
    - Don't use crazy get_task stuff.
    - Simplify the approach for to racing between vm_ops close and
      disassociation, fixing the related bugs most of the driver
      implementations had. Since we are in core code the tracking list can be
      placed in struct ib_uverbs_ufile, which has a lifetime strictly longer
      than any VMAs created by mmap on the uverbs FD.
    
    Link: https://www.spinics.net/lists/stable/msg248747.html
    Link: https://lkml.kernel.org/r/CA+55aFxJTV_g46AQPoPXen-UPiqR1HGMZictt7VpC-SMFbm3Cw@mail.gmail.com
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e463d3007a35..a66238d8a2a3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2646,6 +2646,28 @@ void *ib_get_client_data(struct ib_device *device, struct ib_client *client);
 void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
 			 void *data);
 
+#if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
+int rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,
+		      unsigned long pfn, unsigned long size, pgprot_t prot);
+int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+			struct vm_area_struct *vma, struct page *page,
+			unsigned long size);
+#else
+static inline int rdma_user_mmap_io(struct ib_ucontext *ucontext,
+				    struct vm_area_struct *vma,
+				    unsigned long pfn, unsigned long size,
+				    pgprot_t prot)
+{
+	return -EINVAL;
+}
+static inline int rdma_user_mmap_page(struct ib_ucontext *ucontext,
+				struct vm_area_struct *vma, struct page *page,
+				unsigned long size)
+{
+	return -EINVAL;
+}
+#endif
+
 static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)
 {
 	return copy_from_user(dest, udata->inbuf, len) ? -EFAULT : 0;

commit 9a59739bd01f77db6fbe2955a4fce165f0f43568
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Aug 14 15:33:02 2018 -0700

    IB/rxe: Revise the ib_wr_opcode enum
    
    This enum has become part of the uABI, as both RXE and the
    ib_uverbs_post_send() command expect userspace to supply values from this
    enum. So it should be properly placed in include/uapi/rdma.
    
    In userspace this enum is called 'enum ibv_wr_opcode' as part of
    libibverbs.h. That enum defines different values for IB_WR_LOCAL_INV,
    IB_WR_SEND_WITH_INV, and IB_WR_LSO. These were introduced (incorrectly, it
    turns out) into libiberbs in 2015.
    
    The kernel has changed its mind on the numbering for several of the IB_WC
    values over the years, but has remained stable on IB_WR_LOCAL_INV and
    below.
    
    Based on this we can conclude that there is no real user space user of the
    values beyond IB_WR_ATOMIC_FETCH_AND_ADD, as they have never worked via
    rdma-core. This is confirmed by inspection, only rxe uses the kernel enum
    and implements the latter operations. rxe has clearly never worked with
    these attributes from userspace. Other drivers that support these opcodes
    implement the functionality without calling out to the kernel.
    
    To make IB_WR_SEND_WITH_INV and related work for RXE in userspace we
    choose to renumber the IB_WR enum in the kernel to match the uABI that
    userspace has bee using since before Soft RoCE was merged. This is an
    overall simpler configuration for the whole software stack, and obviously
    can't break anything existing.
    
    Reported-by: Seth Howell <seth.howell@intel.com>
    Tested-by: Seth Howell <seth.howell@intel.com>
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6076c9b72ab9..e463d3007a35 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1281,21 +1281,27 @@ struct ib_qp_attr {
 };
 
 enum ib_wr_opcode {
-	IB_WR_RDMA_WRITE,
-	IB_WR_RDMA_WRITE_WITH_IMM,
-	IB_WR_SEND,
-	IB_WR_SEND_WITH_IMM,
-	IB_WR_RDMA_READ,
-	IB_WR_ATOMIC_CMP_AND_SWP,
-	IB_WR_ATOMIC_FETCH_AND_ADD,
-	IB_WR_LSO,
-	IB_WR_SEND_WITH_INV,
-	IB_WR_RDMA_READ_WITH_INV,
-	IB_WR_LOCAL_INV,
-	IB_WR_REG_MR,
-	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
-	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
+	/* These are shared with userspace */
+	IB_WR_RDMA_WRITE = IB_UVERBS_WR_RDMA_WRITE,
+	IB_WR_RDMA_WRITE_WITH_IMM = IB_UVERBS_WR_RDMA_WRITE_WITH_IMM,
+	IB_WR_SEND = IB_UVERBS_WR_SEND,
+	IB_WR_SEND_WITH_IMM = IB_UVERBS_WR_SEND_WITH_IMM,
+	IB_WR_RDMA_READ = IB_UVERBS_WR_RDMA_READ,
+	IB_WR_ATOMIC_CMP_AND_SWP = IB_UVERBS_WR_ATOMIC_CMP_AND_SWP,
+	IB_WR_ATOMIC_FETCH_AND_ADD = IB_UVERBS_WR_ATOMIC_FETCH_AND_ADD,
+	IB_WR_LSO = IB_UVERBS_WR_TSO,
+	IB_WR_SEND_WITH_INV = IB_UVERBS_WR_SEND_WITH_INV,
+	IB_WR_RDMA_READ_WITH_INV = IB_UVERBS_WR_RDMA_READ_WITH_INV,
+	IB_WR_LOCAL_INV = IB_UVERBS_WR_LOCAL_INV,
+	IB_WR_MASKED_ATOMIC_CMP_AND_SWP =
+		IB_UVERBS_WR_MASKED_ATOMIC_CMP_AND_SWP,
+	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD =
+		IB_UVERBS_WR_MASKED_ATOMIC_FETCH_AND_ADD,
+
+	/* These are kernel only and can not be issued by userspace */
+	IB_WR_REG_MR = 0x20,
 	IB_WR_REG_SIG_MR,
+
 	/* reserve values for low level drivers' internal use.
 	 * These values will not be used at all in the ib core layer.
 	 */

commit 86e1d464a8ccd627b6ea3e9a98a0389b0d27fd1f
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Sep 6 17:27:02 2018 +0300

    RDMA/uverbs: Move flow resources initialization
    
    Use ib_set_flow() when initializing flow related resources.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f687faadf33b..6076c9b72ab9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4162,20 +4162,6 @@ ib_get_vector_affinity(struct ib_device *device, int comp_vector)
 
 }
 
-static inline void ib_set_flow(struct ib_uobject *uobj, struct ib_flow *ibflow,
-			       struct ib_qp *qp, struct ib_device *device)
-{
-	uobj->object = ibflow;
-	ibflow->uobject = uobj;
-
-	if (qp) {
-		atomic_inc(&qp->usecnt);
-		ibflow->qp = qp;
-	}
-
-	ibflow->device = device;
-}
-
 /**
  * rdma_roce_rescan_device - Rescan all of the network devices in the system
  * and add their gids, as needed, to the relevant RoCE devices.

commit eb93c82ed8c77f00955f2891483170194c3be92c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Sep 4 11:45:20 2018 -0400

    RDMA/core: Document QP @event_handler function
    
    Add helpful warning for RDMA consumer implementers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 995f176d4782..f687faadf33b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1138,7 +1138,9 @@ enum ib_qp_create_flags {
  */
 
 struct ib_qp_init_attr {
+	/* Consumer's event_handler callback must not block */
 	void                  (*event_handler)(struct ib_event *, void *);
+
 	void		       *qp_context;
 	struct ib_cq	       *send_cq;
 	struct ib_cq	       *recv_cq;

commit e1f540c3ed0e9634d0f8c4600f3c85df8aff4ae2
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Aug 28 15:08:45 2018 +0300

    RDMA/core: Define client_data_lock as rwlock instead of spinlock
    
    Even though device registration/unregistration and client
    registration/unregistration is not a performance path, define the
    client_data_lock as rwlock for code clarity.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ddc7c317e136..995f176d4782 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2256,10 +2256,11 @@ struct ib_device {
 	struct list_head              event_handler_list;
 	spinlock_t                    event_handler_lock;
 
-	spinlock_t                    client_data_lock;
+	rwlock_t			client_data_lock;
 	struct list_head              core_list;
 	/* Access to the client_data_list is protected by the client_data_lock
-	 * spinlock and the lists_rwsem read-write semaphore */
+	 * rwlock and the lists_rwsem read-write semaphore
+	 */
 	struct list_head              client_data_list;
 
 	struct ib_cache               cache;

commit 2c910cb75e1fe6de52d95c8e32caedd1629a33a5
Merge: 627212c9d49b b53b1c08a23e
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Sep 5 16:21:22 2018 -0600

    Merge branch 'uverbs_dev_cleanups' into rdma.git for-next
    
    For dependencies, branch based on rdma.git 'for-rc' of
    https://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma.git/
    
    Pull 'uverbs_dev_cleanups' from Leon Romanovsky:
    
    ====================
    Reuse the char device code interfaces to simplify ib_uverbs_device
    creation and destruction. As part of this series, we are sending fix to
    cleanup path, which was discovered during internal review,
    
    The fix definitely can go to -rc, but it means that this series will be
    dependent on rdma-rc.
    ====================
    
    * branch 'uverbs_dev_cleanups':
      RDMA/uverbs: Use device.groups to initialize device attributes
      RDMA/uverbs: Use cdev_device_add() instead of cdev_add()
      RDMA/core: Depend on device_add() to add device attributes
      RDMA/uverbs: Fix error cleanup path of ib_uverbs_add_one()
    
    Resolved conflict in ib_device_unregister_sysfs()
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit adee9f3f3bbb317c5469f84deba01eef4b86515b
Author: Parav Pandit <parav@mellanox.com>
Date:   Wed Sep 5 09:47:58 2018 +0300

    RDMA/core: Depend on device_add() to add device attributes
    
    Instead of adding/removing device attribute files, depend on device_add()
    which considers adding these device files based on NULL terminated
    attributes group array.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e950c2a68f06..cd0f935f0bc1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2536,6 +2536,9 @@ struct ib_device {
 
 	struct module               *owner;
 	struct device                dev;
+	/* First group for device attributes, NULL terminated array */
+	const struct attribute_group	*groups[2];
+
 	struct kobject               *ports_parent;
 	struct list_head             port_list;
 

commit 6ceb6331b3291694fb6ceba625219f51447c3fa2
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Sep 3 20:18:03 2018 +0300

    RDMA/uverbs: Declare closing variable as boolean
    
    The "closing" variable is used as boolean and set to "true" in one
    place, update the declaration of that variable and their other
    assignment to proper type.
    
    Fixes: e951747a087a ("IB/uverbs: Rework the locking for cleaning up the ucontext")
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index df8d234a2b56..a4c3a09a91bc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1486,7 +1486,7 @@ struct ib_ucontext {
 	 * it is set when we are closing the file descriptor and indicates
 	 * that mm_sem may be locked.
 	 */
-	int			closing;
+	bool closing;
 
 	bool cleanup_retryable;
 

commit f794809a7259dfaa3d47d90ef5a86007cf48b1ce
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Mon Aug 27 08:35:55 2018 +0300

    IB/core: Add an unbound WQ type to the new CQ API
    
    The upstream kernel commit cited below modified the workqueue in the
    new CQ API to be bound to a specific CPU (instead of being unbound).
    This caused ALL users of the new CQ API to use the same bound WQ.
    
    Specifically, MAD handling was severely delayed when the CPU bound
    to the WQ was busy handling (higher priority) interrupts.
    
    This caused a delay in the MAD "heartbeat" response handling,
    which resulted in ports being incorrectly classified as "down".
    
    To fix this, add a new "unbound" WQ type to the new CQ API, so that users
    have the option to choose either a bound WQ or an unbound WQ.
    
    For MADs, choose the new "unbound" WQ.
    
    Fixes: b7363e67b23e ("IB/device: Convert ib-comp-wq to be CPU-bound")
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.m>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e950c2a68f06..df8d234a2b56 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -71,6 +71,7 @@
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
+extern struct workqueue_struct *ib_comp_unbound_wq;
 
 union ib_gid {
 	u8	raw[16];
@@ -1570,9 +1571,10 @@ struct ib_ah {
 typedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);
 
 enum ib_poll_context {
-	IB_POLL_DIRECT,		/* caller context, no hw completions */
-	IB_POLL_SOFTIRQ,	/* poll from softirq context */
-	IB_POLL_WORKQUEUE,	/* poll from workqueue */
+	IB_POLL_DIRECT,		   /* caller context, no hw completions */
+	IB_POLL_SOFTIRQ,	   /* poll from softirq context */
+	IB_POLL_WORKQUEUE,	   /* poll from workqueue */
+	IB_POLL_UNBOUND_WORKQUEUE, /* poll from unbound workqueue */
 };
 
 struct ib_cq {
@@ -1589,6 +1591,7 @@ struct ib_cq {
 		struct irq_poll		iop;
 		struct work_struct	work;
 	};
+	struct workqueue_struct *comp_wq;
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */

commit 89982f7ccee2fcd8fea7936b81eec6defbf0f131
Merge: a1ceeca679dc 94710cac0ef4
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Aug 16 13:08:18 2018 -0600

    Merge tag 'v4.18' into rdma.git for-next
    
    Resolve merge conflicts from the -rc cycle against the rdma.git tree:
    
    Conflicts:
     drivers/infiniband/core/uverbs_cmd.c
      - New ifs added to ib_uverbs_ex_create_flow in -rc and for-next
      - Merge removal of file->ucontext in for-next with new code in -rc
     drivers/infiniband/core/uverbs_main.c
      - for-next removed code from ib_uverbs_write() that was modified
        in for-rc
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 6b0d08f4a27134e6fb49aa33ceb53356081bc92e
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Aug 9 20:14:37 2018 -0600

    IB/uverbs: Use uverbs_api to manage the object type inside the uobject
    
    Currently the struct uverbs_obj_type stored in the ib_uobject is part of
    the .rodata segment of the module that defines the object. This is a
    problem if drivers define new uapi objects as we will be left with a
    dangling pointer after device disassociation.
    
    Switch the uverbs_obj_type for struct uverbs_api_object, which is
    allocated memory that is part of the uverbs_api and is guaranteed to
    always exist. Further this moves the 'type_class' into this memory which
    means access to the IDR/FD function pointers is also guaranteed. Drivers
    cannot define new types.
    
    This makes it safe to continue to use all uobjects, including driver
    defined ones, after disassociation.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3b07201b9a80..5d404c20b49f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1524,7 +1524,7 @@ struct ib_uobject {
 	atomic_t		usecnt;		/* protects exclusive access */
 	struct rcu_head		rcu;		/* kfree_rcu() overhead */
 
-	const struct uverbs_obj_type *type;
+	const struct uverbs_api_object *uapi_object;
 };
 
 struct ib_udata {

commit 7d96c9b17636b6148534617ddf95dead18617776
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Aug 9 20:14:35 2018 -0600

    IB/uverbs: Have the core code create the uverbs_root_spec
    
    There is no reason for drivers to do this, the core code should take of
    everything. The drivers will provide their information from rodata to
    describe their modifications to the core's base uapi specification.
    
    The core uses this to build up the runtime uapi for each device.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4ffe3e11e8fb..3b07201b9a80 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2580,7 +2580,7 @@ struct ib_device {
 	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
 						     int comp_vector);
 
-	struct uverbs_root_spec		*driver_specs_root;
+	const struct uverbs_object_tree_def *const *driver_specs;
 	enum rdma_driver_id		driver_id;
 };
 

commit 9f49a5b5c21d58aa84e16cfdc5e99e49faefcb7a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Jul 29 11:34:56 2018 +0300

    RDMA/netdev: Use priv_destructor for netdev cleanup
    
    Now that the unregister_netdev flow for IPoIB no longer relies on external
    code we can now introduce the use of priv_destructor and
    needs_free_netdev.
    
    The rdma_netdev flow is switched to use the netdev common priv_destructor
    instead of the special free_rdma_netdev and the IPOIB ULP adjusted:
     - priv_destructor needs to switch to point to the ULP's destructor
       which will then call the rdma_ndev's in the right order
     - We need to be careful around the error unwind of register_netdev
       as it sometimes calls priv_destructor on failure
     - ULPs need to use ndo_init/uninit to ensure proper ordering
       of failures around register_netdev
    
    Switching to priv_destructor is a necessary pre-requisite to using
    the rtnl new_link mechanism.
    
    The VNIC user for rdma_netdev should also be revised, but that is left for
    another patch.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Denis Drozdov <denisd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dea770e5b9ae..4ffe3e11e8fb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2203,7 +2203,11 @@ struct rdma_netdev {
 	struct ib_device  *hca;
 	u8                 port_num;
 
-	/* cleanup function must be specified */
+	/*
+	 * cleanup function must be specified.
+	 * FIXME: This is only used for OPA_VNIC and that usage should be
+	 * removed too.
+	 */
 	void (*free_rdma_netdev)(struct net_device *netdev);
 
 	/* control functions */

commit e83f0ecdc40f2c3d63ff0e7f17462a29d12684a2
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 25 21:40:18 2018 -0600

    IB/uverbs: Do not pass struct ib_device to the ioctl methods
    
    This does the same as the patch before, except for ioctl. The rules are
    the same, but for the ioctl methods the core code handles setting up the
    uobject.
    
    - Retrieve the ib_dev from the uobject->context->device. This is
      safe under ioctl as the core has already done rdma_alloc_begin_uobject
      and so CREATE calls are entirely protected by the rwsem.
    - Retrieve the ib_dev from uobject->object
    - Call ib_uverbs_get_ucontext()
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index be208421f7d3..dea770e5b9ae 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4170,7 +4170,6 @@ void rdma_roce_rescan_device(struct ib_device *ibdev);
 
 struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
 
-int uverbs_destroy_def_handler(struct ib_device *ib_dev,
-			       struct ib_uverbs_file *file,
+int uverbs_destroy_def_handler(struct ib_uverbs_file *file,
 			       struct uverbs_attr_bundle *attrs);
 #endif /* IB_VERBS_H */

commit 87ad80abc70d2d5a4e383bc7e63867c9bc660838
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 25 21:40:12 2018 -0600

    IB/uverbs: Consolidate uobject destruction
    
    There are several flows that can destroy a uobject and each one is
    minimized and sprinkled throughout the code base, making it difficult to
    understand and very hard to modify the destroy path.
    
    Consolidate all of these into uverbs_destroy_uobject() and call it in all
    cases where a uobject has to be destroyed.
    
    This makes one change to the lifecycle, during any abort (eg when
    alloc_commit is not called) we always call out to alloc_abort, even if
    remove_commit needs to be called to delete a HW object.
    
    This also renames RDMA_REMOVE_DURING_CLEANUP to RDMA_REMOVE_ABORT to
    clarify its actual usage and revises some of the comments to reflect what
    the life cycle is for the type implementation.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1de8f0d2797c..be208421f7d3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1467,8 +1467,8 @@ enum rdma_remove_reason {
 	RDMA_REMOVE_CLOSE,
 	/* Driver is being hot-unplugged. This call should delete the actual object itself */
 	RDMA_REMOVE_DRIVER_REMOVE,
-	/* Context is being cleaned-up, but commit was just completed */
-	RDMA_REMOVE_DURING_CLEANUP,
+	/* uobj is being cleaned-up before being committed */
+	RDMA_REMOVE_ABORT,
 };
 
 struct ib_rdmacg_object {

commit d34ac5cd3a73aacd11009c4fc3ba15d7ea62c411
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jul 18 09:25:32 2018 -0700

    RDMA, core and ULPs: Declare ib_post_send() and ib_post_recv() arguments const
    
    Since neither ib_post_send() nor ib_post_recv() modify the data structure
    their second argument points at, declare that argument const. This change
    makes it necessary to declare the 'bad_wr' argument const too and also to
    modify all ULPs that call ib_post_send(), ib_post_recv() or
    ib_post_srq_recv(). This patch does not change any functionality but makes
    it possible for the compiler to verify whether the
    ib_post_(send|recv|srq_recv) really do not modify the posted work request.
    
    To make this possible, only one cast had to be introduce that casts away
    constness, namely in rpcrdma_post_recvs(). The only way I can think of to
    avoid that cast is to introduce an additional loop in that function or to
    change the data type of bad_wr from struct ib_recv_wr ** into int
    (an index that refers to an element in the work request list). However,
    both approaches would require even more extensive changes than this
    patch.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cf38d47fa8f8..1de8f0d2797c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2373,8 +2373,8 @@ struct ib_device {
 						struct ib_srq_attr *srq_attr);
 	int                        (*destroy_srq)(struct ib_srq *srq);
 	int                        (*post_srq_recv)(struct ib_srq *srq,
-						    struct ib_recv_wr *recv_wr,
-						    struct ib_recv_wr **bad_recv_wr);
+						    const struct ib_recv_wr *recv_wr,
+						    const struct ib_recv_wr **bad_recv_wr);
 	struct ib_qp *             (*create_qp)(struct ib_pd *pd,
 						struct ib_qp_init_attr *qp_init_attr,
 						struct ib_udata *udata);
@@ -2388,11 +2388,11 @@ struct ib_device {
 					       struct ib_qp_init_attr *qp_init_attr);
 	int                        (*destroy_qp)(struct ib_qp *qp);
 	int                        (*post_send)(struct ib_qp *qp,
-						struct ib_send_wr *send_wr,
-						struct ib_send_wr **bad_send_wr);
+						const struct ib_send_wr *send_wr,
+						const struct ib_send_wr **bad_send_wr);
 	int                        (*post_recv)(struct ib_qp *qp,
-						struct ib_recv_wr *recv_wr,
-						struct ib_recv_wr **bad_recv_wr);
+						const struct ib_recv_wr *recv_wr,
+						const struct ib_recv_wr **bad_recv_wr);
 	struct ib_cq *             (*create_cq)(struct ib_device *device,
 						const struct ib_cq_init_attr *attr,
 						struct ib_ucontext *context,
@@ -3281,10 +3281,10 @@ int ib_destroy_srq(struct ib_srq *srq);
  *   the work request that failed to be posted on the QP.
  */
 static inline int ib_post_srq_recv(struct ib_srq *srq,
-				   struct ib_recv_wr *recv_wr,
-				   struct ib_recv_wr **bad_recv_wr)
+				   const struct ib_recv_wr *recv_wr,
+				   const struct ib_recv_wr **bad_recv_wr)
 {
-	struct ib_recv_wr *dummy;
+	const struct ib_recv_wr *dummy;
 
 	return srq->device->post_srq_recv(srq, recv_wr, bad_recv_wr ? : &dummy);
 }
@@ -3384,10 +3384,10 @@ int ib_close_qp(struct ib_qp *qp);
  * earlier work requests in the list.
  */
 static inline int ib_post_send(struct ib_qp *qp,
-			       struct ib_send_wr *send_wr,
-			       struct ib_send_wr **bad_send_wr)
+			       const struct ib_send_wr *send_wr,
+			       const struct ib_send_wr **bad_send_wr)
 {
-	struct ib_send_wr *dummy;
+	const struct ib_send_wr *dummy;
 
 	return qp->device->post_send(qp, send_wr, bad_send_wr ? : &dummy);
 }
@@ -3401,10 +3401,10 @@ static inline int ib_post_send(struct ib_qp *qp,
  *   the work request that failed to be posted on the QP.
  */
 static inline int ib_post_recv(struct ib_qp *qp,
-			       struct ib_recv_wr *recv_wr,
-			       struct ib_recv_wr **bad_recv_wr)
+			       const struct ib_recv_wr *recv_wr,
+			       const struct ib_recv_wr **bad_recv_wr)
 {
-	struct ib_recv_wr *dummy;
+	const struct ib_recv_wr *dummy;
 
 	return qp->device->post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);
 }

commit f696bf6d64b195b83ca1bdb7cd33c999c9dcf514
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jul 18 09:25:14 2018 -0700

    RDMA: Constify the argument of the work request conversion functions
    
    When posting a send work request, the work request that is posted is not
    modified by any of the RDMA drivers. Make this explicit by constifying
    most ib_send_wr pointers in RDMA transport drivers.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 42cbf8eabe9d..cf38d47fa8f8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1352,7 +1352,7 @@ struct ib_rdma_wr {
 	u32			rkey;
 };
 
-static inline struct ib_rdma_wr *rdma_wr(struct ib_send_wr *wr)
+static inline const struct ib_rdma_wr *rdma_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct ib_rdma_wr, wr);
 }
@@ -1367,7 +1367,7 @@ struct ib_atomic_wr {
 	u32			rkey;
 };
 
-static inline struct ib_atomic_wr *atomic_wr(struct ib_send_wr *wr)
+static inline const struct ib_atomic_wr *atomic_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct ib_atomic_wr, wr);
 }
@@ -1384,7 +1384,7 @@ struct ib_ud_wr {
 	u8			port_num;   /* valid for DR SMPs on switch only */
 };
 
-static inline struct ib_ud_wr *ud_wr(struct ib_send_wr *wr)
+static inline const struct ib_ud_wr *ud_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct ib_ud_wr, wr);
 }
@@ -1396,7 +1396,7 @@ struct ib_reg_wr {
 	int			access;
 };
 
-static inline struct ib_reg_wr *reg_wr(struct ib_send_wr *wr)
+static inline const struct ib_reg_wr *reg_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct ib_reg_wr, wr);
 }
@@ -1409,7 +1409,8 @@ struct ib_sig_handover_wr {
 	struct ib_sge	       *prot;
 };
 
-static inline struct ib_sig_handover_wr *sig_handover_wr(struct ib_send_wr *wr)
+static inline const struct ib_sig_handover_wr *
+sig_handover_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct ib_sig_handover_wr, wr);
 }

commit e951747a087a8655f467833bb367ebf53d57527c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jul 10 20:55:19 2018 -0600

    IB/uverbs: Rework the locking for cleaning up the ucontext
    
    The locking here has always been a bit crazy and spread out, upon some
    careful analysis we can simplify things.
    
    Create a single function uverbs_destroy_ufile_hw() that internally handles
    all locking. This pulls together pieces of this process that were
    sprinkled all over the places into one place, and covers them with one
    lock.
    
    This eliminates several duplicate/confusing locks and makes the control
    flow in ib_uverbs_close() and ib_uverbs_free_hw_resources() extremely
    simple.
    
    Unfortunately we have to keep an extra mutex, ucontext_lock.  This lock is
    logically part of the rwsem and provides the 'down write, fail if write
    locked, wait if read locked' semantic we require.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 99bcf64a4762..42cbf8eabe9d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1479,6 +1479,11 @@ struct ib_rdmacg_object {
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct ib_uverbs_file  *ufile;
+	/*
+	 * 'closing' can be read by the driver only during a destroy callback,
+	 * it is set when we are closing the file descriptor and indicates
+	 * that mm_sem may be locked.
+	 */
 	int			closing;
 
 	bool cleanup_retryable;

commit bb039a870c0593a4deaa72c2693d02a87723305c
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jul 18 09:25:16 2018 -0700

    IB/core: Allow ULPs to specify NULL as the third ib_post_(send|recv|srq_recv)() argument
    
    This patch does not change the behavior of the modified functions.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b626aa2310c5..99bcf64a4762 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3278,7 +3278,9 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
 				   struct ib_recv_wr *recv_wr,
 				   struct ib_recv_wr **bad_recv_wr)
 {
-	return srq->device->post_srq_recv(srq, recv_wr, bad_recv_wr);
+	struct ib_recv_wr *dummy;
+
+	return srq->device->post_srq_recv(srq, recv_wr, bad_recv_wr ? : &dummy);
 }
 
 /**
@@ -3379,7 +3381,9 @@ static inline int ib_post_send(struct ib_qp *qp,
 			       struct ib_send_wr *send_wr,
 			       struct ib_send_wr **bad_send_wr)
 {
-	return qp->device->post_send(qp, send_wr, bad_send_wr);
+	struct ib_send_wr *dummy;
+
+	return qp->device->post_send(qp, send_wr, bad_send_wr ? : &dummy);
 }
 
 /**
@@ -3394,7 +3398,9 @@ static inline int ib_post_recv(struct ib_qp *qp,
 			       struct ib_recv_wr *recv_wr,
 			       struct ib_recv_wr **bad_recv_wr)
 {
-	return qp->device->post_recv(qp, recv_wr, bad_recv_wr);
+	struct ib_recv_wr *dummy;
+
+	return qp->device->post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);
 }
 
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,

commit 32269441240064c7475241ae28fee787fcdf55b9
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:09 2018 +0300

    IB/mlx5: Introduce driver create and destroy flow methods
    
    Introduce driver create and destroy flow methods on the uverbs flow
    object.
    
    This allows the driver to get its specific device attributes to match the
    underlay specification while still using the generic ib_flow object for
    cleanup and code sharing.
    
    The IB object's attributes are set via the ib_set_flow() helper function.
    
    The specific implementation for the given specification is added in
    downstream patches.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 24d6ec38feea..b626aa2310c5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4134,6 +4134,20 @@ ib_get_vector_affinity(struct ib_device *device, int comp_vector)
 
 }
 
+static inline void ib_set_flow(struct ib_uobject *uobj, struct ib_flow *ibflow,
+			       struct ib_qp *qp, struct ib_device *device)
+{
+	uobj->object = ibflow;
+	ibflow->uobject = uobj;
+
+	if (qp) {
+		atomic_inc(&qp->usecnt);
+		ibflow->qp = qp;
+	}
+
+	ibflow->device = device;
+}
+
 /**
  * rdma_roce_rescan_device - Rescan all of the network devices in the system
  * and add their gids, as needed, to the relevant RoCE devices.

commit 6cd080a674a7adce97c0189c4579cf40782c2770
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:08 2018 +0300

    IB: Support ib_flow creation in drivers
    
    This patch considers the case that ib_flow is created by some device
    driver with its specific parameters using the KABI infrastructure.
    
    In that case both QP and ib_uflow_resources might not be applicable.
    Downstream patches from this series use the above functionality.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 08348e53082c..24d6ec38feea 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2081,6 +2081,7 @@ struct ib_flow_attr {
 
 struct ib_flow {
 	struct ib_qp		*qp;
+	struct ib_device	*device;
 	struct ib_uobject	*uobject;
 };
 

commit 4fca037783512cedfb23a116c66727ce40c8558a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 11 16:20:44 2018 -0600

    IB/uverbs: Move ib_access_flags and ib_read_counters_flags to uapi
    
    These constants are used in the ioctl interface so they are part of the
    uapi, place them in the correct header for clarity.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2696f1d730a1..08348e53082c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1425,14 +1425,16 @@ struct ib_recv_wr {
 };
 
 enum ib_access_flags {
-	IB_ACCESS_LOCAL_WRITE	= 1,
-	IB_ACCESS_REMOTE_WRITE	= (1<<1),
-	IB_ACCESS_REMOTE_READ	= (1<<2),
-	IB_ACCESS_REMOTE_ATOMIC	= (1<<3),
-	IB_ACCESS_MW_BIND	= (1<<4),
-	IB_ZERO_BASED		= (1<<5),
-	IB_ACCESS_ON_DEMAND     = (1<<6),
-	IB_ACCESS_HUGETLB	= (1<<7),
+	IB_ACCESS_LOCAL_WRITE = IB_UVERBS_ACCESS_LOCAL_WRITE,
+	IB_ACCESS_REMOTE_WRITE = IB_UVERBS_ACCESS_REMOTE_WRITE,
+	IB_ACCESS_REMOTE_READ = IB_UVERBS_ACCESS_REMOTE_READ,
+	IB_ACCESS_REMOTE_ATOMIC = IB_UVERBS_ACCESS_REMOTE_ATOMIC,
+	IB_ACCESS_MW_BIND = IB_UVERBS_ACCESS_MW_BIND,
+	IB_ZERO_BASED = IB_UVERBS_ACCESS_ZERO_BASED,
+	IB_ACCESS_ON_DEMAND = IB_UVERBS_ACCESS_ON_DEMAND,
+	IB_ACCESS_HUGETLB = IB_UVERBS_ACCESS_HUGETLB,
+
+	IB_ACCESS_SUPPORTED = ((IB_ACCESS_HUGETLB << 1) - 1)
 };
 
 /*
@@ -2223,11 +2225,6 @@ struct ib_counters {
 	atomic_t	usecnt;
 };
 
-enum ib_read_counters_flags {
-	/* prefer read values from driver cache */
-	IB_READ_COUNTERS_ATTR_PREFER_CACHED = 1 << 0,
-};
-
 struct ib_counters_read_attr {
 	u64	*counters_buff;
 	u32	ncounters;

commit 528922afd41cdd1da6a4b33e2c82e38c1746561c
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jul 8 13:24:39 2018 +0300

    IB: Enable uverbs_destroy_def_handler to be used by drivers
    
    Enable uverbs_destroy_def_handler to be used by drivers and replace
    current code to use it.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b523298d486b..2696f1d730a1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4146,4 +4146,7 @@ void rdma_roce_rescan_device(struct ib_device *ibdev);
 
 struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
 
+int uverbs_destroy_def_handler(struct ib_device *ib_dev,
+			       struct ib_uverbs_file *file,
+			       struct uverbs_attr_bundle *attrs);
 #endif /* IB_VERBS_H */

commit b02289b3d60f79ba0831051a7743d8fdb4110355
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Jul 4 15:57:50 2018 +0300

    RDMA: Validate grh_required when handling AVs
    
    Extend the existing grh_required flag to check when AV's are handled that
    a GRH is present.
    
    Since we don't want to do query_port during the AV checks for performance
    reasons move the flag into the immutable_data.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 98e025759791..b523298d486b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -529,6 +529,7 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 #define RDMA_CORE_CAP_AF_IB             0x00001000
 #define RDMA_CORE_CAP_ETH_AH            0x00002000
 #define RDMA_CORE_CAP_OPA_AH            0x00004000
+#define RDMA_CORE_CAP_IB_GRH_REQUIRED   0x00008000
 
 /* Protocol                             0xFFF00000 */
 #define RDMA_CORE_CAP_PROT_IB           0x00100000
@@ -538,6 +539,10 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 #define RDMA_CORE_CAP_PROT_RAW_PACKET   0x01000000
 #define RDMA_CORE_CAP_PROT_USNIC        0x02000000
 
+#define RDMA_CORE_PORT_IB_GRH_REQUIRED (RDMA_CORE_CAP_IB_GRH_REQUIRED \
+					| RDMA_CORE_CAP_PROT_ROCE     \
+					| RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP)
+
 #define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \
 					| RDMA_CORE_CAP_IB_MAD \
 					| RDMA_CORE_CAP_IB_SMI \
@@ -570,7 +575,6 @@ struct ib_port_attr {
 	enum ib_mtu		max_mtu;
 	enum ib_mtu		active_mtu;
 	int			gid_tbl_len;
-	unsigned int		grh_required:1;
 	unsigned int		ip_gids:1;
 	/* This is the value from PortInfo CapabilityMask, defined by IBA */
 	u32			port_cap_flags;
@@ -2771,6 +2775,13 @@ static inline int rdma_is_port_valid(const struct ib_device *device,
 		port <= rdma_end_port(device));
 }
 
+static inline bool rdma_is_grh_required(const struct ib_device *device,
+					u8 port_num)
+{
+	return device->port_immutable[port_num].core_cap_flags &
+		RDMA_CORE_PORT_IB_GRH_REQUIRED;
+}
+
 static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;

commit 2f944c0fbf58b1f390e5e61affd98ba0061214c6
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 4 15:57:48 2018 +0300

    RDMA: Fix storage of PortInfo CapabilityMask in the kernel
    
    The internal flag IP_BASED_GIDS was added to a field that was being used
    to hold the port Info CapabilityMask without considering the effects this
    will have. Since most drivers just use the value from the HW MAD it means
    IP_BASED_GIDS will also become set on any HW that sets the IBA flag
    IsOtherLocalChangesNoticeSupported - which is not intended.
    
    Fix this by keeping port_cap_flags only for the IBA CapabilityMask value
    and store unrelated flags externally. Move the bit definitions for this to
    ib_mad.h to make it clear what is happening.
    
    To keep the uAPI unchanged define a new set of flags in the uapi header
    that are only used by ib_uverbs_query_port_resp.port_cap_flags which match
    the current flags supported in rdma-core, and the values exposed by the
    current kernel.
    
    Fixes: b4a26a27287a ("IB: Report using RoCE IP based gids in port caps")
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 031d121190fd..98e025759791 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -432,33 +432,6 @@ enum ib_port_state {
 	IB_PORT_ACTIVE_DEFER	= 5
 };
 
-enum ib_port_cap_flags {
-	IB_PORT_SM				= 1 <<  1,
-	IB_PORT_NOTICE_SUP			= 1 <<  2,
-	IB_PORT_TRAP_SUP			= 1 <<  3,
-	IB_PORT_OPT_IPD_SUP                     = 1 <<  4,
-	IB_PORT_AUTO_MIGR_SUP			= 1 <<  5,
-	IB_PORT_SL_MAP_SUP			= 1 <<  6,
-	IB_PORT_MKEY_NVRAM			= 1 <<  7,
-	IB_PORT_PKEY_NVRAM			= 1 <<  8,
-	IB_PORT_LED_INFO_SUP			= 1 <<  9,
-	IB_PORT_SM_DISABLED			= 1 << 10,
-	IB_PORT_SYS_IMAGE_GUID_SUP		= 1 << 11,
-	IB_PORT_PKEY_SW_EXT_PORT_TRAP_SUP	= 1 << 12,
-	IB_PORT_EXTENDED_SPEEDS_SUP             = 1 << 14,
-	IB_PORT_CM_SUP				= 1 << 16,
-	IB_PORT_SNMP_TUNNEL_SUP			= 1 << 17,
-	IB_PORT_REINIT_SUP			= 1 << 18,
-	IB_PORT_DEVICE_MGMT_SUP			= 1 << 19,
-	IB_PORT_VENDOR_CLASS_SUP		= 1 << 20,
-	IB_PORT_DR_NOTICE_SUP			= 1 << 21,
-	IB_PORT_CAP_MASK_NOTICE_SUP		= 1 << 22,
-	IB_PORT_BOOT_MGMT_SUP			= 1 << 23,
-	IB_PORT_LINK_LATENCY_SUP		= 1 << 24,
-	IB_PORT_CLIENT_REG_SUP			= 1 << 25,
-	IB_PORT_IP_BASED_GIDS			= 1 << 26,
-};
-
 enum ib_port_width {
 	IB_WIDTH_1X	= 1,
 	IB_WIDTH_4X	= 2,
@@ -597,6 +570,9 @@ struct ib_port_attr {
 	enum ib_mtu		max_mtu;
 	enum ib_mtu		active_mtu;
 	int			gid_tbl_len;
+	unsigned int		grh_required:1;
+	unsigned int		ip_gids:1;
+	/* This is the value from PortInfo CapabilityMask, defined by IBA */
 	u32			port_cap_flags;
 	u32			max_msg_sz;
 	u32			bad_pkey_cntr;
@@ -612,7 +588,6 @@ struct ib_port_attr {
 	u8			active_width;
 	u8			active_speed;
 	u8                      phys_state;
-	bool			grh_required;
 };
 
 enum ib_device_modify_flags {

commit d0259e82e7d214340aed33732e9a5ce448564921
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 4 11:32:11 2018 +0300

    IB/uverbs: Remove ib_uobject_file
    
    The only purpose for this structure was to hold the ib_uobject_file
    pointer, but now that is part of the standard ib_uobject the structure
    no longer makes any sense, so get rid of it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9c04cb5e4041..031d121190fd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1540,12 +1540,6 @@ struct ib_uobject {
 	const struct uverbs_obj_type *type;
 };
 
-struct ib_uobject_file {
-	struct ib_uobject	uobj;
-	/* ufile contains the lock between context release and file close */
-	struct ib_uverbs_file	*ufile;
-};
-
 struct ib_udata {
 	const void __user *inbuf;
 	void __user *outbuf;

commit 6a5e9c88419828a487204e35291ae4459697a9bd
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 4 11:32:07 2018 +0300

    IB/uverbs: Move non driver related elements from ib_ucontext to ib_ufile
    
    The IDR is part of the ib_ufile so all the machinery to lock it, handle
    closing and disassociation rightly belongs to the ufile not the ucontext.
    
    This changes the lifetime of that data to match the lifetime of the file
    descriptor which is always strictly longer than the lifetime of the
    ucontext.
    
    We need the entire locking machinery to continue to exist after ucontext
    destruction to allow us to return the destroy data after a device has been
    disassociated.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8784d5bfc252..9c04cb5e4041 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1500,12 +1500,6 @@ struct ib_ucontext {
 	struct ib_uverbs_file  *ufile;
 	int			closing;
 
-	/* locking the uobjects_list */
-	struct mutex		uobjects_lock;
-	struct list_head	uobjects;
-	/* protects cleanup process from other actions */
-	struct rw_semaphore	cleanup_rwsem;
-	enum rdma_remove_reason cleanup_reason;
 	bool cleanup_retryable;
 
 	struct pid             *tgid;
@@ -1531,6 +1525,9 @@ struct ib_ucontext {
 
 struct ib_uobject {
 	u64			user_handle;	/* handle given to us by userspace */
+	/* ufile & ucontext owning this object */
+	struct ib_uverbs_file  *ufile;
+	/* FIXME, save memory: ufile->context == context */
 	struct ib_ucontext     *context;	/* associated user context */
 	void		       *object;		/* containing object */
 	struct list_head	list;		/* link to context's list */

commit 87fc2a620a398e970872064841b0db7cc6d0149f
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 4 08:50:23 2018 +0300

    RDMA/uverbs: Store the specs_root in the struct ib_uverbs_device
    
    The specs are required to operate the uverbs file, so they belong inside
    the ib_uverbs_device, not inside the ib_device. The spec passed in the
    ib_device is just a communication from the driver and should not be used
    during runtime.
    
    This also changes the lifetime of the spec memory to match the
    ib_uverbs_device, however at this time the spec_root can still contain
    driver pointers after disassociation, so it cannot be used if ib_dev is
    NULL. This is preparation for another series.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e1130c6c1377..8784d5bfc252 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2602,7 +2602,7 @@ struct ib_device {
 	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
 						     int comp_vector);
 
-	struct uverbs_root_spec		*specs_root;
+	struct uverbs_root_spec		*driver_specs_root;
 	enum rdma_driver_id		driver_id;
 };
 

commit 1c77483e4c50339b0306572167ccbff6b55d051b
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Wed Jun 20 17:11:39 2018 +0300

    IB: Improve uverbs_cleanup_ucontext algorithm
    
    Improve uverbs_cleanup_ucontext algorithm to work properly when the
    topology graph of the objects cannot be determined at compile time.  This
    is the case with objects created via the devx interface in mlx5.
    
    Typically uverbs objects must be created in a strict topologically sorted
    order, so that LIFO ordering will generally cause them to be freed
    properly. There are only a few cases (eg memory windows) where objects can
    point to things out of the strict LIFO order.
    
    Instead of using an explicit ordering scheme where the HW destroy is not
    allowed to fail, go over the list multiple times and allow the destroy
    function to fail. If progress halts then a final, desperate, cleanup is
    done before leaking the memory. This indicates a driver bug.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8e726fff30fe..e1130c6c1377 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1476,7 +1476,10 @@ struct ib_fmr_attr {
 struct ib_umem;
 
 enum rdma_remove_reason {
-	/* Userspace requested uobject deletion. Call could fail */
+	/*
+	 * Userspace requested uobject deletion or initial try
+	 * to remove uobject via cleanup. Call could fail
+	 */
 	RDMA_REMOVE_DESTROY,
 	/* Context deletion. This call should delete the actual object itself */
 	RDMA_REMOVE_CLOSE,
@@ -1503,6 +1506,7 @@ struct ib_ucontext {
 	/* protects cleanup process from other actions */
 	struct rw_semaphore	cleanup_rwsem;
 	enum rdma_remove_reason cleanup_reason;
+	bool cleanup_retryable;
 
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@ -2684,6 +2688,46 @@ static inline bool ib_is_udata_cleared(struct ib_udata *udata,
 	return ib_is_buffer_cleared(udata->inbuf + offset, len);
 }
 
+/**
+ * ib_is_destroy_retryable - Check whether the uobject destruction
+ * is retryable.
+ * @ret: The initial destruction return code
+ * @why: remove reason
+ * @uobj: The uobject that is destroyed
+ *
+ * This function is a helper function that IB layer and low-level drivers
+ * can use to consider whether the destruction of the given uobject is
+ * retry-able.
+ * It checks the original return code, if it wasn't success the destruction
+ * is retryable according to the ucontext state (i.e. cleanup_retryable) and
+ * the remove reason. (i.e. why).
+ * Must be called with the object locked for destroy.
+ */
+static inline bool ib_is_destroy_retryable(int ret, enum rdma_remove_reason why,
+					   struct ib_uobject *uobj)
+{
+	return ret && (why == RDMA_REMOVE_DESTROY ||
+		       uobj->context->cleanup_retryable);
+}
+
+/**
+ * ib_destroy_usecnt - Called during destruction to check the usecnt
+ * @usecnt: The usecnt atomic
+ * @why: remove reason
+ * @uobj: The uobject that is destroyed
+ *
+ * Non-zero usecnts will block destruction unless destruction was triggered by
+ * a ucontext cleanup.
+ */
+static inline int ib_destroy_usecnt(atomic_t *usecnt,
+				    enum rdma_remove_reason why,
+				    struct ib_uobject *uobj)
+{
+	if (atomic_read(usecnt) && ib_is_destroy_retryable(-EBUSY, why, uobj))
+		return -EBUSY;
+	return 0;
+}
+
 /**
  * ib_modify_qp_is_ok - Check that the supplied attribute mask
  * contains all required attributes and no attributes not allowed for

commit 1ccddc42da03876f60fe2d0a1b124c27ed5ff201
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jun 24 11:23:45 2018 +0300

    RDMA/verbs: Drop kernel variant of destroy_flow
    
    Following the removal of ib_create_flow(), adjust the code to get rid of
    ib_destroy_flow() too.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1c72ca81e5fa..8e726fff30fe 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3807,8 +3807,6 @@ struct ib_xrcd *__ib_alloc_xrcd(struct ib_device *device, const char *caller);
  */
 int ib_dealloc_xrcd(struct ib_xrcd *xrcd);
 
-int ib_destroy_flow(struct ib_flow *flow_id);
-
 static inline int ib_check_mr_access(int flags)
 {
 	/*

commit ca576fbbdc80d26ca46dd881944413e7dc05c21d
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jun 24 11:23:44 2018 +0300

    RDMA/verbs: Drop kernel variant of create_flow
    
    There are no kernel users of this interface so lets drop it.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c01e9c6ed666..1c72ca81e5fa 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3807,8 +3807,6 @@ struct ib_xrcd *__ib_alloc_xrcd(struct ib_device *device, const char *caller);
  */
 int ib_dealloc_xrcd(struct ib_xrcd *xrcd);
 
-struct ib_flow *ib_create_flow(struct ib_qp *qp,
-			       struct ib_flow_attr *flow_attr, int domain);
 int ib_destroy_flow(struct ib_flow *flow_id);
 
 static inline int ib_check_mr_access(int flags)

commit b7403217656dcf6c51f09d0bca7a12db0de8934a
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jun 19 10:59:14 2018 +0300

    IB: Make ib_init_ah_attr_from_wc set sgid_attr
    
    The work completion is inspected to determine what dgid table entry was
    used to receieve the packet, produces a sgid_attr that matches and sticks
    it in the ah_attr.
    
    All callers of this function are now required to release the ah_attr on
    success.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 995d517c0a76..c01e9c6ed666 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3150,6 +3150,13 @@ int ib_get_rdma_header_version(const union rdma_network_hdr *hdr);
  *   ignored unless the work completion indicates that the GRH is valid.
  * @ah_attr: Returned attributes that can be used when creating an address
  *   handle for replying to the message.
+ * When ib_init_ah_attr_from_wc() returns success,
+ * (a) for IB link layer it optionally contains a reference to SGID attribute
+ * when GRH is present for IB link layer.
+ * (b) for RoCE link layer it contains a reference to SGID attribute.
+ * User must invoke rdma_cleanup_ah_attr_gid_attr() to release reference to SGID
+ * attributes which are initialized using ib_init_ah_attr_from_wc().
+ *
  */
 int ib_init_ah_attr_from_wc(struct ib_device *device, u8 port_num,
 			    const struct ib_wc *wc, const struct ib_grh *grh,

commit 1abd8a8f39cd9a2925149000056494523c85643a
Merge: d8894a08d91e 375dc53d032f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 21 07:22:30 2018 +0900

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma fixes from Jason Gunthorpe:
     "Here are eight fairly small fixes collected over the last two weeks.
    
      Regression and crashing bug fixes:
    
       - mlx4/5: Fixes for issues found from various checkers
    
       - A resource tracking and uverbs regression in the core code
    
       - qedr: NULL pointer regression found during testing
    
       - rxe: Various small bugs"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:
      IB/rxe: Fix missing completion for mem_reg work requests
      RDMA/core: Save kernel caller name when creating CQ using ib_create_cq()
      IB/uverbs: Fix ordering of ucontext check in ib_uverbs_write
      IB/mlx4: Fix an error handling path in 'mlx4_ib_rereg_user_mr()'
      RDMA/qedr: Fix NULL pointer dereference when running over iWARP without RDMA-CM
      IB/mlx5: Fix return value check in flow_counters_set_data()
      IB/mlx5: Fix memory leak in mlx5_ib_create_flow
      IB/rxe: avoid double kfree skb

commit 7dc08dcfc8c86cb4457e383734ff6844ddaff876
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 17 12:59:59 2018 +0300

    IB/core: Expose ib_ucontext from a given ib_uverbs_file
    
    Drivers that use the IOCTL API may have the ib_uverbs_file and need a
    way to get the related ib_ucontext from it, this is enabled by this
    patch.
    
    Downstream patches from this series will use it.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dc5d262739e5..995d517c0a76 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4120,4 +4120,6 @@ ib_get_vector_affinity(struct ib_device *device, int comp_vector)
  */
 void rdma_roce_rescan_device(struct ib_device *ibdev);
 
+struct ib_ucontext *ib_uverbs_get_ucontext(struct ib_uverbs_file *ufile);
+
 #endif /* IB_VERBS_H */

commit 33023fb85a42b53bf778bc025f9667b582282be4
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jun 18 08:05:26 2018 -0700

    IB/core: add max_send_sge and max_recv_sge attributes
    
    This patch replaces the ib_device_attr.max_sge with max_send_sge and
    max_recv_sge. It allows ulps to take advantage of devices that have very
    different send and recv sge depths.  For example cxgb4 has a max_recv_sge
    of 4, yet a max_send_sge of 16.  Splitting out these attributes allows
    much more efficient use of the SQ for cxgb4 with ulps that use the RDMA_RW
    API. Consider a large RDMA WRITE that has 16 scattergather entries.
    With max_sge of 4, the ulp would send 4 WRITE WRs, but with max_sge of
    16, it can be done with 1 WRITE WR.
    
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Acked-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Acked-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0232c0f9f717..dc5d262739e5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -345,7 +345,8 @@ struct ib_device_attr {
 	int			max_qp;
 	int			max_qp_wr;
 	u64			device_cap_flags;
-	int			max_sge;
+	int			max_send_sge;
+	int			max_recv_sge;
 	int			max_sge_rd;
 	int			max_cq;
 	int			max_cqe;

commit 7350cdd0257e73a37df57253fb9decd8effacd37
Author: Bharat Potnuri <bharat@chelsio.com>
Date:   Fri Jun 15 20:52:33 2018 +0530

    RDMA/core: Save kernel caller name when creating CQ using ib_create_cq()
    
    Few kernel applications like SCST-iSER create CQ using ib_create_cq(),
    where accessing CQ structures using rdma restrack tool leads to below NULL
    pointer dereference. This patch saves caller kernel module name similar to
    ib_alloc_cq().
    
    BUG: unable to handle kernel NULL pointer dereference at           (null)
    IP: [<ffffffff8132ca70>] skip_spaces+0x30/0x30
    PGD 738bac067 PUD 8533f0067 PMD 0
    Oops: 0000 [#1] SMP
    R10: ffff88017fc03300 R11: 0000000000000246 R12: 0000000000000000
    R13: ffff88082fa5a668 R14: ffff88017475a000 R15: 0000000000000000
    FS:  00002b32726582c0(0000) GS:ffff88087fc40000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000000 CR3: 00000008491a1000 CR4: 00000000003607e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     [<ffffffffc05af69c>] ? fill_res_name_pid+0x7c/0x90 [ib_core]
     [<ffffffffc05af79f>] fill_res_cq_entry+0xef/0x170 [ib_core]
     [<ffffffffc05af4c4>] res_get_common_dumpit+0x3c4/0x480 [ib_core]
     [<ffffffffc05af5d3>] nldev_res_get_cq_dumpit+0x13/0x20 [ib_core]
     [<ffffffff815bc1e7>] netlink_dump+0x117/0x2e0
     [<ffffffff815bcb8b>] __netlink_dump_start+0x1ab/0x230
     [<ffffffffc059fead>] ibnl_rcv_msg+0x11d/0x1f0 [ib_core]
     [<ffffffffc05af5c0>] ? nldev_res_get_mr_dumpit+0x20/0x20 [ib_core]
     [<ffffffffc059fd90>] ? rdma_nl_multicast+0x30/0x30 [ib_core]
     [<ffffffff815bea49>] netlink_rcv_skb+0xa9/0xc0
     [<ffffffffc05a0018>] ibnl_rcv+0x98/0xb0 [ib_core]
     [<ffffffff815be132>] netlink_unicast+0xf2/0x1b0
     [<ffffffff815be50f>] netlink_sendmsg+0x31f/0x6a0
     [<ffffffff8156b580>] sock_sendmsg+0xb0/0xf0
     [<ffffffff816ace9e>] ? _raw_spin_unlock_bh+0x1e/0x20
     [<ffffffff8156f998>] ? release_sock+0x118/0x170
     [<ffffffff8156b731>] SYSC_sendto+0x121/0x1c0
     [<ffffffff81568340>] ? sock_alloc_file+0xa0/0x140
     [<ffffffff81221265>] ? __fd_install+0x25/0x60
     [<ffffffff8156c2ce>] SyS_sendto+0xe/0x10
     [<ffffffff816b6c2a>] system_call_fastpath+0x16/0x1b
    RIP  [<ffffffff8132ca70>] skip_spaces+0x30/0x30
    RSP <ffff88072be97760>
    CR2: 0000000000000000
    
    Cc: <stable@vger.kernel.org>
    Fixes: f66c8ba4c9fa ("RDMA/core: Save kernel caller name when creating PD and CQ objects")
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Potnuri Bharat Teja <bharat@chelsio.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2043e1a8f851..4f71d6a073ba 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3394,11 +3394,14 @@ int ib_process_cq_direct(struct ib_cq *cq, int budget);
  *
  * Users can examine the cq structure to determine the actual CQ size.
  */
-struct ib_cq *ib_create_cq(struct ib_device *device,
-			   ib_comp_handler comp_handler,
-			   void (*event_handler)(struct ib_event *, void *),
-			   void *cq_context,
-			   const struct ib_cq_init_attr *cq_attr);
+struct ib_cq *__ib_create_cq(struct ib_device *device,
+			     ib_comp_handler comp_handler,
+			     void (*event_handler)(struct ib_event *, void *),
+			     void *cq_context,
+			     const struct ib_cq_init_attr *cq_attr,
+			     const char *caller);
+#define ib_create_cq(device, cmp_hndlr, evt_hndlr, cq_ctxt, cq_attr) \
+	__ib_create_cq((device), (cmp_hndlr), (evt_hndlr), (cq_ctxt), (cq_attr), KBUILD_MODNAME)
 
 /**
  * ib_resize_cq - Modifies the capacity of the CQ.

commit 1a1f460ff151710289c2f8d4badd8b603b87d610
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 13 10:22:08 2018 +0300

    RDMA: Hold the sgid_attr inside the struct ib_ah/qp
    
    If the AH has a GRH then hold a reference to the sgid_attr inside the
    common struct.
    
    If the QP is modified with an AV that includes a GRH then also hold a
    reference to the sgid_attr inside the common struct.
    
    This informs the cache that the sgid_index is in-use so long as the AH or
    QP using it exists.
    
    This also means that all drivers can access the sgid_attr directly from
    the ah_attr instead of querying the cache during their UD post-send paths.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 65f467d65bff..0232c0f9f717 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1580,6 +1580,7 @@ struct ib_ah {
 	struct ib_device	*device;
 	struct ib_pd		*pd;
 	struct ib_uobject	*uobject;
+	const struct ib_gid_attr *sgid_attr;
 	enum rdma_ah_attr_type	type;
 };
 
@@ -1778,6 +1779,9 @@ struct ib_qp {
 	struct ib_uobject      *uobject;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
+	/* sgid_attrs associated with the AV's */
+	const struct ib_gid_attr *av_sgid_attr;
+	const struct ib_gid_attr *alt_path_sgid_attr;
 	u32			qp_num;
 	u32			max_write_sge;
 	u32			max_read_sge;

commit 47ec38666210485de860ab24675acb3d2e7d4954
Author: Parav Pandit <parav@mellanox.com>
Date:   Wed Jun 13 10:22:06 2018 +0300

    RDMA: Convert drivers to use sgid_attr instead of sgid_index
    
    The core code now ensures that all driver callbacks that receive an
    rdma_ah_attrs will have a sgid_attr's pointer if there is a GRH present.
    
    Drivers can use this pointer instead of calling a query function with
    sgid_index. This simplifies the drivers and also avoids races where a
    gid_index lookup may return different data if it is changed.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1e5c1e8ba282..65f467d65bff 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -149,13 +149,13 @@ static inline enum ib_gid_type ib_network_to_gid_type(enum rdma_network_type net
 	return IB_GID_TYPE_IB;
 }
 
-static inline enum rdma_network_type ib_gid_to_network_type(enum ib_gid_type gid_type,
-							    union ib_gid *gid)
+static inline enum rdma_network_type
+rdma_gid_attr_network_type(const struct ib_gid_attr *attr)
 {
-	if (gid_type == IB_GID_TYPE_IB)
+	if (attr->gid_type == IB_GID_TYPE_IB)
 		return RDMA_NETWORK_IB;
 
-	if (ipv6_addr_v4mapped((struct in6_addr *)gid))
+	if (ipv6_addr_v4mapped((struct in6_addr *)&attr->gid))
 		return RDMA_NETWORK_IPV4;
 	else
 		return RDMA_NETWORK_IPV6;

commit d97099fe53ed9ab8b17d084bed0099feb08a48c1
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 13 10:22:05 2018 +0300

    IB{cm, core}: Introduce and use ah_attr copy, move, replace APIs
    
    Introduce AH attribute copy, move and replace APIs to be used by core and
    provider drivers.
    
    In CM code flow when ah attribute might be re-initialized twice while
    processing incoming request, or initialized once while from path record
    while sending out CM requests. Therefore use rdma_move_ah_attr API to
    handle such scenarios instead of memcpy().
    
    Provider drivers keeps a copy ah_attr during the lifetime of the ah.
    Therefore, use rdma_replace_ah_attr() which conditionally release
    reference to old ah_attr and holds reference to new attribute whose
    referrence is released when the AH is freed.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a3a4b8335668..1e5c1e8ba282 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -4034,6 +4034,11 @@ void rdma_destroy_ah_attr(struct rdma_ah_attr *ah_attr);
 void rdma_move_grh_sgid_attr(struct rdma_ah_attr *attr, union ib_gid *dgid,
 			     u32 flow_label, u8 hop_limit, u8 traffic_class,
 			     const struct ib_gid_attr *sgid_attr);
+void rdma_copy_ah_attr(struct rdma_ah_attr *dest,
+		       const struct rdma_ah_attr *src);
+void rdma_replace_ah_attr(struct rdma_ah_attr *old,
+			  const struct rdma_ah_attr *new);
+void rdma_move_ah_attr(struct rdma_ah_attr *dest, struct rdma_ah_attr *src);
 
 /**
  * rdma_ah_find_type - Return address handle type.

commit 8d9ec9addd6c492a99d3699212653cba92989767
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jun 13 10:22:03 2018 +0300

    IB/core: Add a sgid_attr pointer to struct rdma_ah_attr
    
    The sgid_attr will ultimately replace the sgid_index in the ah_attr.
    This will allow for all layers to have a consistent view of what
    gid table entry was selected as processing runs through all stages of the
    stack.
    
    This commit introduces the pointer and ensures it is set before calling
    any driver callback that includes a struct ah_attr callback, allowing
    future patches to adjust both the drivers and the callers to use
    sgid_attr instead of sgid_index.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 67c458215512..a3a4b8335668 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -690,6 +690,7 @@ struct ib_event_handler {
 	} while (0)
 
 struct ib_global_route {
+	const struct ib_gid_attr *sgid_attr;
 	union ib_gid	dgid;
 	u32		flow_label;
 	u8		sgid_index;
@@ -4026,8 +4027,14 @@ static inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,
 	grh->sgid_index = sgid_index;
 	grh->hop_limit = hop_limit;
 	grh->traffic_class = traffic_class;
+	grh->sgid_attr = NULL;
 }
 
+void rdma_destroy_ah_attr(struct rdma_ah_attr *ah_attr);
+void rdma_move_grh_sgid_attr(struct rdma_ah_attr *attr, union ib_gid *dgid,
+			     u32 flow_label, u8 hop_limit, u8 traffic_class,
+			     const struct ib_gid_attr *sgid_attr);
+
 /**
  * rdma_ah_find_type - Return address handle type.
  *

commit 1dfce294577120ec60399a64094ea00e4247103d
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jun 5 08:40:22 2018 +0300

    IB: Replace ib_query_gid/ib_get_cached_gid with rdma_query_gid
    
    If the gid_attr argument is NULL then the functions behave identically to
    rdma_query_gid. ib_query_gid just calls ib_get_cached_gid, so everything
    can be consolidated to one function.
    
    Now that all callers either use rdma_query_gid() or ib_get_cached_gid(),
    ib_query_gid() API is removed.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1c153cc046ee..67c458215512 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3046,10 +3046,6 @@ static inline bool rdma_cap_read_inv(struct ib_device *dev, u32 port_num)
 	return rdma_protocol_iwarp(dev, port_num);
 }
 
-int ib_query_gid(struct ib_device *device,
-		 u8 port_num, int index, union ib_gid *gid,
-		 struct ib_gid_attr *attr);
-
 int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
 			 int state);
 int ib_get_vf_config(struct ib_device *device, int vf, u8 port,

commit f4df9a7c34d8f9e84af73ce187bcdf6fea65c4cb
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jun 5 08:40:16 2018 +0300

    RDMA: Use GID from the ib_gid_attr during the add_gid() callback
    
    Now that ib_gid_attr contains the GID, make use of that in the add_gid()
    callback functions for the provider drivers to simplify the add_gid()
    implementations.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0a77afedabd0..1c153cc046ee 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2342,8 +2342,7 @@ struct ib_device {
 	 * concurrently for different ports. This function is only called when
 	 * roce_gid_table is used.
 	 */
-	int		           (*add_gid)(const union ib_gid *gid,
-					      const struct ib_gid_attr *attr,
+	int		           (*add_gid)(const struct ib_gid_attr *attr,
 					      void **context);
 	/* When calling del_gid, the HW vendor's driver should delete the
 	 * gid of device @device at gid index gid_index of port port_num

commit b150c3862d21a4a9ce0f26d8067b9dcd41e2050c
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jun 5 08:40:15 2018 +0300

    IB/core: Introduce GID entry reference counts
    
    In order to be able to expose pointers to the ib_gid_attrs in the GID
    table we need to make it so the value of the pointer cannot be
    changed. Thus each GID table entry gets a unique piece of kref'd memory
    that is written only during initialization and remains constant for its
    lifetime.
    
    This eventually will allow the struct ib_gid_attrs to be returned without
    copy from many of query the APIs, but it also provides a way to track when
    all users of a HW table index go away.
    
    For roce we no longer allow an in-use HW table index to be re-used for a
    new an different entry. When a GID table entry needs to be removed it is
    hidden from the find API, but remains as a valid HW index and all
    ib_gid_attr points remain valid. The HW index is not relased until all
    users put the kref.
    
    Later patches will broadly replace the use of the sgid_index integer with
    the kref'd structure.
    
    Ultimately this will prevent security problems where the OS changes the
    properties of a HW GID table entry while an active user object is still
    using the entry.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4c6241bc2039..0a77afedabd0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -94,6 +94,7 @@ enum ib_gid_type {
 struct ib_gid_attr {
 	struct net_device	*ndev;
 	struct ib_device	*device;
+	union ib_gid		gid;
 	enum ib_gid_type	gid_type;
 	u16			index;
 	u8			port_num;

commit 7654cb1ba7d0f312a6841d35d0f576db4723e8a3
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Thu Jun 7 07:57:16 2018 -0700

    Convert infiniband uverbs to struct_size
    
    The flows were hidden from the C compiler; expose them as a zero-length
    array to allow struct_size to work.
    
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2043e1a8f851..4c6241bc2039 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2093,10 +2093,7 @@ struct ib_flow_attr {
 	u32	     flags;
 	u8	     num_of_specs;
 	u8	     port;
-	/* Following are the optional layers according to user request
-	 * struct ib_flow_spec_xxx
-	 * struct ib_flow_spec_yyy
-	 */
+	union ib_flow_spec flows[];
 };
 
 struct ib_flow {

commit ca24da008fd665db125aed889e857ccaa033de84
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Thu May 31 11:05:24 2018 +0300

    RDMA/core: introduce check masks for T10-PI offload
    
    T10-PI offload capability is currently supported in iSER protocol only,
    and the definition of the HCA protection information checks are missing
    from the core layer. Add those definition to avoid code duplication in
    other drivers (such iSER target and NVMeoF).
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2cc04abb6df8..2043e1a8f851 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -860,6 +860,19 @@ enum ib_sig_err_type {
 	IB_SIG_BAD_APPTAG,
 };
 
+/**
+ * Signature check masks (8 bytes in total) according to the T10-PI standard:
+ *  -------- -------- ------------
+ * | GUARD  | APPTAG |   REFTAG   |
+ * |  2B    |  2B    |    4B      |
+ *  -------- -------- ------------
+ */
+enum {
+	IB_SIG_CHECK_GUARD	= 0xc0,
+	IB_SIG_CHECK_APPTAG	= 0x30,
+	IB_SIG_CHECK_REFTAG	= 0x0f,
+};
+
 /**
  * struct ib_sig_err - signature error descriptor
  */

commit 0f45e69d625a423d225968c3b59da7f31c5d70b4
Merge: 27d036e33237 1a1e03dc15cf
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Jun 4 08:48:11 2018 -0600

    Merge tag 'verbs_flow_counters' of git://git.kernel.org/pub/scm/linux/kernel/git/leon/linux-rdma.git into for-next
    
    Pull verbs counters series from Leon Romanovsky:
    
    ====================
    Verbs flow counters support
    
    This series comes to allow user space applications to monitor real time
    traffic activity and events of the verbs objects it manages, e.g.: ibv_qp,
    ibv_wq, ibv_flow.
    
    The API enables generic counters creation and define mapping to
    association with a verbs object, the current mlx5 driver is using this API
    for flow counters.
    
    With this API, an application can monitor the entire life cycle of object
    activity, defined here as a static counters attachment.  This API also
    allows dynamic counters monitoring of measurement points for a partial
    period in the verbs object life cycle.
    
    In addition it presents the implementation of the generic counters
    interface.
    
    This will be achieved by extending flow creation by adding a new flow
    count specification type which allows the user to associate a previously
    created flow counters using the generic verbs counters interface to the
    created flow, once associated the user could read statistics by using the
    read function of the generic counters interface.
    
    The API includes:
    1. create and destroyed API of a new counters objects
    2. read the counters values from HW
    
    Note:
    Attaching API to allow application to define the measurement points per
    objects is a user space only API and this data is passed to kernel when
    the counted object (e.g. flow) is created with the counters object.
    ===================
    
    * tag 'verbs_flow_counters':
      IB/mlx5: Add counters read support
      IB/mlx5: Add flow counters read support
      IB/mlx5: Add flow counters binding support
      IB/mlx5: Add counters create and destroy support
      IB/uverbs: Add support for flow counters
      IB/core: Add support for flow counters
      IB/core: Support passing uhw for create_flow
      IB/uverbs: Add read counters support
      IB/core: Introduce counters read verb
      IB/uverbs: Add create/destroy counters support
      IB/core: Introduce counters object and its create/destroy
      IB/uverbs: Add an ib_uobject getter to ioctl() infrastructure
      net/mlx5: Export flow counter related API
      net/mlx5: Use flow counter pointer as input to the query function

commit 7eea23a5cd51a945700ad44c7ce585dd2a640dea
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:36 2018 +0300

    IB/core: Add support for flow counters
    
    A counters object could be attached to flow on creation by providing the
    counter specification action.
    
    General counters description which count packets and bytes are introduced,
    downstream patches from this series will use them as part of flow counters
    binding.
    
    In addition, increase number of flow specifications supported layers to 10
    upon adding count specification and for the previously added drop
    specification.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ea97b91dd88c..ee8ab9f2d250 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1857,9 +1857,10 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
 	IB_FLOW_SPEC_ACTION_DROP        = 0x1001,
 	IB_FLOW_SPEC_ACTION_HANDLE	= 0x1002,
+	IB_FLOW_SPEC_ACTION_COUNT       = 0x1003,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
-#define IB_FLOW_SPEC_SUPPORT_LAYERS 8
+#define IB_FLOW_SPEC_SUPPORT_LAYERS 10
 
 /* Flow steering rule priority is set according to it's domain.
  * Lower domain value means higher priority.
@@ -2011,6 +2012,17 @@ struct ib_flow_spec_action_handle {
 	struct ib_flow_action	     *act;
 };
 
+enum ib_counters_description {
+	IB_COUNTER_PACKETS,
+	IB_COUNTER_BYTES,
+};
+
+struct ib_flow_spec_action_count {
+	enum ib_flow_spec_type type;
+	u16 size;
+	struct ib_counters *counters;
+};
+
 union ib_flow_spec {
 	struct {
 		u32			type;
@@ -2026,6 +2038,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_action_tag  flow_tag;
 	struct ib_flow_spec_action_drop drop;
 	struct ib_flow_spec_action_handle action;
+	struct ib_flow_spec_action_count flow_count;
 };
 
 struct ib_flow_attr {

commit 59082a327d0145c69b419a0f5bed96b13c5e9ed4
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu May 31 16:43:35 2018 +0300

    IB/core: Support passing uhw for create_flow
    
    This is required when user-space drivers need to pass extra information
    regarding how to handle this flow steering specification.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3769a1cc99b0..ea97b91dd88c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2427,7 +2427,8 @@ struct ib_device {
 	struct ib_flow *	   (*create_flow)(struct ib_qp *qp,
 						  struct ib_flow_attr
 						  *flow_attr,
-						  int domain);
+						  int domain,
+						  struct ib_udata *udata);
 	int			   (*destroy_flow)(struct ib_flow *flow_id);
 	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
 						      struct ib_mr_status *mr_status);

commit 51d7a5387464f1b1fee3f2db9287409189d83d65
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:33 2018 +0300

    IB/core: Introduce counters read verb
    
    The user supplies counters instance and a reference to an output array of
    uint64_t.  The driver reads the hardware counters values and writes them
    to the output index location in the user supplied array.  All counters
    values are represented as uint64_t types.
    
    To be able to successfully read the data the counters must be first bound
    to an IB object.
    
    Downstream patches will present binding method for flow counters.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4f5d9fd4f920..3769a1cc99b0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2187,6 +2187,17 @@ struct ib_counters {
 	atomic_t	usecnt;
 };
 
+enum ib_read_counters_flags {
+	/* prefer read values from driver cache */
+	IB_READ_COUNTERS_ATTR_PREFER_CACHED = 1 << 0,
+};
+
+struct ib_counters_read_attr {
+	u64	*counters_buff;
+	u32	ncounters;
+	u32	flags; /* use enum ib_read_counters_flags */
+};
+
 struct uverbs_attr_bundle;
 
 struct ib_device {
@@ -2461,6 +2472,9 @@ struct ib_device {
 	struct ib_counters *	(*create_counters)(struct ib_device *device,
 						   struct uverbs_attr_bundle *attrs);
 	int	(*destroy_counters)(struct ib_counters	*counters);
+	int	(*read_counters)(struct ib_counters *counters,
+				 struct ib_counters_read_attr *counters_read_attr,
+				 struct uverbs_attr_bundle *attrs);
 
 	/**
 	 * rdma netdev operation

commit fa9b1802d140f4d4a781b3444aadf47fa511cabb
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:31 2018 +0300

    IB/core: Introduce counters object and its create/destroy
    
    A verbs application may need to get statistics and info on various aspects
    of a verb object (e.g. Flow, QP, ...), in general case the application
    will state which object's counters its interested in (we refer to this
    action as attach), bind this new counters object to the appropriate verb
    object and on later stage read their values using the counters object.
    
    This series introduces a general API for counters object that may
    accumulate any ib object counters type, bound and read on demand.
    
    Counters instance is allocated on an IB context and belongs to that
    context.  Upon successful creation the counters can be bound to a verbs
    object so that hardware counter instances can be created and read.
    
    Downstream patches in this series will introduce the attach, bind and the
    read functionality.
    
    Counters instance can be de-allocated, upon successful destruction the
    related hardware resources are released.
    
    Prior to destroy call the user must first make sure that the counters is
    not being used by any IB object, e.g. not attached to any of its counted
    type otherwise an EBUSY error is invoked.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9fc8a825aa28..4f5d9fd4f920 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2180,6 +2180,13 @@ struct ib_port_pkey_list {
 	struct list_head              pkey_list;
 };
 
+struct ib_counters {
+	struct ib_device	*device;
+	struct ib_uobject	*uobject;
+	/* num of objects attached */
+	atomic_t	usecnt;
+};
+
 struct uverbs_attr_bundle;
 
 struct ib_device {
@@ -2451,6 +2458,10 @@ struct ib_device {
 	struct ib_mr *             (*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,
 						struct ib_dm_mr_attr *attr,
 						struct uverbs_attr_bundle *attrs);
+	struct ib_counters *	(*create_counters)(struct ib_device *device,
+						   struct uverbs_attr_bundle *attrs);
+	int	(*destroy_counters)(struct ib_counters	*counters);
+
 	/**
 	 * rdma netdev operation
 	 *

commit 0394808d9ed5ca9d3595ca4d97ce79faf845ac77
Merge: bb42f87e2924 d8f9cc328c88
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon May 28 11:44:35 2018 -0600

    Merge branch 'mr_fix' into git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma for-next
    
    Update mlx4 to support user MR creation against read-only memory, previously
    it required the memory to be writable.
    
    Based on rdma for-rc due to dependencies.
    
    * mr_fix: (2 commits)
      IB/mlx4: Mark user MR as writable if actual virtual memory is writable
      IB/core: Make testing MR flags for writability a static inline function

commit 08bb558ac11ab944e0539e78619d7b4c356278bd
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Wed May 23 15:30:30 2018 +0300

    IB/core: Make testing MR flags for writability a static inline function
    
    Make the MR writability flags check, which is performed in umem.c,
    a static inline function in file ib_verbs.h
    
    This allows the function to be used by low-level infiniband drivers.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9fc8a825aa28..20fa5c591e81 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3734,6 +3734,20 @@ static inline int ib_check_mr_access(int flags)
 	return 0;
 }
 
+static inline bool ib_access_writable(int access_flags)
+{
+	/*
+	 * We have writable memory backing the MR if any of the following
+	 * access flags are set.  "Local write" and "remote write" obviously
+	 * require write access.  "Remote atomic" can do things like fetch and
+	 * add, which will modify memory, and "MW bind" can change permissions
+	 * by binding a window.
+	 */
+	return access_flags &
+		(IB_ACCESS_LOCAL_WRITE   | IB_ACCESS_REMOTE_WRITE |
+		 IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_MW_BIND);
+}
+
 /**
  * ib_check_mr_status: lightweight check of MR status.
  *     This routine may provide status checks on a selected

commit b04f0f036ac17b879e4d8c83f6503a19322ddde4
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Sun May 13 14:33:32 2018 +0300

    IB/uverbs: Introduce a MPLS steering match filter
    
    Add a new MPLS steering match filter that can match against
    a single MPLS tag field.
    
    Since the MPLS header can reside in different locations in the packet's
    protocol stack as well as be encapsulated with a tunnel protocol, it
    is required to know the exact location of the header in the protocol
    stack.
    
    Therefore, when including the MPLS protocol spec in the specs list,
    it is mandatory to provide the list in an ordered manner, so
    that it represents the actual header order in a matching packet.
    
    Drivers that process the spec list and apply the matching rule
    should treat the position of the MPLS spec in the spec list as the
    actual location of the MPLS label in the packet's protocol stack.
    
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e3ec61fe6060..e849bd0fc618 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1853,6 +1853,7 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_UDP		= 0x41,
 	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
 	IB_FLOW_SPEC_GRE		= 0x51,
+	IB_FLOW_SPEC_MPLS		= 0x60,
 	IB_FLOW_SPEC_INNER		= 0x100,
 	/* Actions */
 	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
@@ -2010,6 +2011,19 @@ struct ib_flow_spec_gre {
 	struct ib_flow_gre_filter     mask;
 };
 
+struct ib_flow_mpls_filter {
+	__be32 tag;
+	/* Must be last */
+	u8	real_sz[0];
+};
+
+struct ib_flow_spec_mpls {
+	u32                           type;
+	u16			      size;
+	struct ib_flow_mpls_filter     val;
+	struct ib_flow_mpls_filter     mask;
+};
+
 struct ib_flow_spec_action_tag {
 	enum ib_flow_spec_type	      type;
 	u16			      size;
@@ -2040,6 +2054,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_tunnel      tunnel;
 	struct ib_flow_spec_esp		esp;
 	struct ib_flow_spec_gre		gre;
+	struct ib_flow_spec_mpls	mpls;
 	struct ib_flow_spec_action_tag  flow_tag;
 	struct ib_flow_spec_action_drop drop;
 	struct ib_flow_spec_action_handle action;

commit d90e5e5038ef8e62c82710e3970317ef4eb367cf
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Sun May 13 14:33:30 2018 +0300

    IB/uverbs: Introduce a GRE steering match filter
    
    Adding a new GRE steering match filter that can match against
    key and protocol fields.
    
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9fc8a825aa28..e3ec61fe6060 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1852,6 +1852,7 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_TCP		= 0x40,
 	IB_FLOW_SPEC_UDP		= 0x41,
 	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
+	IB_FLOW_SPEC_GRE		= 0x51,
 	IB_FLOW_SPEC_INNER		= 0x100,
 	/* Actions */
 	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
@@ -1994,6 +1995,21 @@ struct ib_flow_spec_esp {
 	struct ib_flow_esp_filter     mask;
 };
 
+struct ib_flow_gre_filter {
+	__be16 c_ks_res0_ver;
+	__be16 protocol;
+	__be32 key;
+	/* Must be last */
+	u8	real_sz[0];
+};
+
+struct ib_flow_spec_gre {
+	u32                           type;
+	u16			      size;
+	struct ib_flow_gre_filter     val;
+	struct ib_flow_gre_filter     mask;
+};
+
 struct ib_flow_spec_action_tag {
 	enum ib_flow_spec_type	      type;
 	u16			      size;
@@ -2023,6 +2039,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_ipv6        ipv6;
 	struct ib_flow_spec_tunnel      tunnel;
 	struct ib_flow_spec_esp		esp;
+	struct ib_flow_spec_gre		gre;
 	struct ib_flow_spec_action_tag  flow_tag;
 	struct ib_flow_spec_action_drop drop;
 	struct ib_flow_spec_action_handle action;

commit 19fd08b85bc7e0502b55cd726f466df82ee7e777
Merge: 28da7be5ebc0 efc365e7290d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 6 17:35:43 2018 -0700

    Merge tag 'for-linus-unmerged' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "Doug and I are at a conference next week so if another PR is sent I
      expect it to only be bug fixes. Parav noted yesterday that there are
      some fringe case behavior changes in his work that he would like to
      fix, and I see that Intel has a number of rc looking patches for HFI1
      they posted yesterday.
    
      Parav is again the biggest contributor by patch count with his ongoing
      work to enable container support in the RDMA stack, followed by Leon
      doing syzkaller inspired cleanups, though most of the actual fixing
      went to RC.
    
      There is one uncomfortable series here fixing the user ABI to actually
      work as intended in 32 bit mode. There are lots of notes in the commit
      messages, but the basic summary is we don't think there is an actual
      32 bit kernel user of drivers/infiniband for several good reasons.
    
      However we are seeing people want to use a 32 bit user space with 64
      bit kernel, which didn't completely work today. So in fixing it we
      required a 32 bit rxe user to upgrade their userspace. rxe users are
      still already quite rare and we think a 32 bit one is non-existing.
    
       - Fix RDMA uapi headers to actually compile in userspace and be more
         complete
    
       - Three shared with netdev pull requests from Mellanox:
    
          * 7 patches, mostly to net with 1 IB related one at the back).
            This series addresses an IRQ performance issue (patch 1),
            cleanups related to the fix for the IRQ performance problem
            (patches 2-6), and then extends the fragmented completion queue
            support that already exists in the net side of the driver to the
            ib side of the driver (patch 7).
    
          * Mostly IB, with 5 patches to net that are needed to support the
            remaining 10 patches to the IB subsystem. This series extends
            the current 'representor' framework when the mlx5 driver is in
            switchdev mode from being a netdev only construct to being a
            netdev/IB dev construct. The IB dev is limited to raw Eth queue
            pairs only, but by having an IB dev of this type attached to the
            representor for a switchdev port, it enables DPDK to work on the
            switchdev device.
    
          * All net related, but needed as infrastructure for the rdma
            driver
    
       - Updates for the hns, i40iw, bnxt_re, cxgb3, cxgb4, hns drivers
    
       - SRP performance updates
    
       - IB uverbs write path cleanup patch series from Leon
    
       - Add RDMA_CM support to ib_srpt. This is disabled by default. Users
         need to set the port for ib_srpt to listen on in configfs in order
         for it to be enabled
         (/sys/kernel/config/target/srpt/discovery_auth/rdma_cm_port)
    
       - TSO and Scatter FCS support in mlx4
    
       - Refactor of modify_qp routine to resolve problems seen while
         working on new code that is forthcoming
    
       - More refactoring and updates of RDMA CM for containers support from
         Parav
    
       - mlx5 'fine grained packet pacing', 'ipsec offload' and 'device
         memory' user API features
    
       - Infrastructure updates for the new IOCTL interface, based on
         increased usage
    
       - ABI compatibility bug fixes to fully support 32 bit userspace on 64
         bit kernel as was originally intended. See the commit messages for
         extensive details
    
       - Syzkaller bugs and code cleanups motivated by them"
    
    * tag 'for-linus-unmerged' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (199 commits)
      IB/rxe: Fix for oops in rxe_register_device on ppc64le arch
      IB/mlx5: Device memory mr registration support
      net/mlx5: Mkey creation command adjustments
      IB/mlx5: Device memory support in mlx5_ib
      net/mlx5: Query device memory capabilities
      IB/uverbs: Add device memory registration ioctl support
      IB/uverbs: Add alloc/free dm uverbs ioctl support
      IB/uverbs: Add device memory capabilities reporting
      IB/uverbs: Expose device memory capabilities to user
      RDMA/qedr: Fix wmb usage in qedr
      IB/rxe: Removed GID add/del dummy routines
      RDMA/qedr: Zero stack memory before copying to user space
      IB/mlx5: Add ability to hash by IPSEC_SPI when creating a TIR
      IB/mlx5: Add information for querying IPsec capabilities
      IB/mlx5: Add IPsec support for egress and ingress
      {net,IB}/mlx5: Add ipsec helper
      IB/mlx5: Add modify_flow_action_esp verb
      IB/mlx5: Add implementation for create and destroy action_xfrm
      IB/uverbs: Introduce ESP steering match filter
      IB/uverbs: Add modify ESP flow_action
      ...

commit be934cca9e987e73eb20e3c80731a9580d5acc79
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Apr 5 18:53:25 2018 +0300

    IB/uverbs: Add device memory registration ioctl support
    
    Adding new ioctl method for the MR object - REG_DM_MR.
    
    This command can be used by users to register an allocated
    device memory buffer as an MR and receive lkey and rkey
    to be used within work requests.
    
    It is added as a new method under the MR object and using a new
    ib_device callback - reg_dm_mr.
    The command creates a standard ib_mr object which represents the
    registered memory.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6806c4f5657a..4bd24c48b1ad 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -321,6 +321,12 @@ struct ib_cq_caps {
 	u16     max_cq_moderation_period;
 };
 
+struct ib_dm_mr_attr {
+	u64		length;
+	u64		offset;
+	u32		access_flags;
+};
+
 struct ib_dm_alloc_attr {
 	u64	length;
 	u32	alignment;
@@ -1797,6 +1803,8 @@ struct ib_mr {
 		struct list_head	qp_entry;	/* FR */
 	};
 
+	struct ib_dm      *dm;
+
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
@@ -2444,6 +2452,9 @@ struct ib_device {
 					       struct ib_dm_alloc_attr *attr,
 					       struct uverbs_attr_bundle *attrs);
 	int                        (*dealloc_dm)(struct ib_dm *dm);
+	struct ib_mr *             (*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,
+						struct ib_dm_mr_attr *attr,
+						struct uverbs_attr_bundle *attrs);
 	/**
 	 * rdma netdev operation
 	 *

commit bee76d7ab5d270919e80e4764df7cd7e4f06ed24
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Apr 5 18:53:24 2018 +0300

    IB/uverbs: Add alloc/free dm uverbs ioctl support
    
    This change adds uverbs support for allocation/freeing
    of device memory commands.
    
    A new uverbs object is defined of type idr to represent
    and track the new resource type allocation per context.
    
    The API requires provider driver to implement 2 new ib_device
    callbacks - one for allocation and one for deallocation which
    return and accept (respectively) the ib_dm object which represents
    the allocated memory on the device.
    
    The support is added via the ioctl command infrastructure
    only.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ed425627efd8..6806c4f5657a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -321,6 +321,12 @@ struct ib_cq_caps {
 	u16     max_cq_moderation_period;
 };
 
+struct ib_dm_alloc_attr {
+	u64	length;
+	u32	alignment;
+	u32	flags;
+};
+
 struct ib_device_attr {
 	u64			fw_ver;
 	__be64			sys_image_guid;
@@ -1769,6 +1775,14 @@ struct ib_qp {
 	struct rdma_restrack_entry     res;
 };
 
+struct ib_dm {
+	struct ib_device  *device;
+	u32		   length;
+	u32		   flags;
+	struct ib_uobject *uobject;
+	atomic_t	   usecnt;
+};
+
 struct ib_mr {
 	struct ib_device  *device;
 	struct ib_pd	  *pd;
@@ -2425,7 +2439,11 @@ struct ib_device {
 	int			   (*modify_flow_action_esp)(struct ib_flow_action *action,
 							     const struct ib_flow_action_attrs_esp *attr,
 							     struct uverbs_attr_bundle *attrs);
-
+	struct ib_dm *             (*alloc_dm)(struct ib_device *device,
+					       struct ib_ucontext *context,
+					       struct ib_dm_alloc_attr *attr,
+					       struct uverbs_attr_bundle *attrs);
+	int                        (*dealloc_dm)(struct ib_dm *dm);
 	/**
 	 * rdma netdev operation
 	 *

commit 1d8eeb9f6a6e0d8ac43a54fd95126044bf8d6695
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Apr 5 18:53:23 2018 +0300

    IB/uverbs: Add device memory capabilities reporting
    
    This change allows vendors to report device memory capability
    max_dm_size - to user via uverbs command.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a6dba77c1b28..ed425627efd8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -372,6 +372,7 @@ struct ib_device_attr {
 	u32			raw_packet_caps; /* Use ib_raw_packet_caps enum */
 	struct ib_tm_caps	tm_caps;
 	struct ib_cq_caps       cq_caps;
+	u64			max_dm_size;
 };
 
 enum ib_mtu {

commit 56ab0b38b80e5771920e163cc9bd52504b03f539
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Mar 28 09:27:49 2018 +0300

    IB/uverbs: Introduce ESP steering match filter
    
    Adding a new ESP steering match filter that could match against
    spi and seq used in IPSec protocol.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8c3ca073016d..a6dba77c1b28 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1828,6 +1828,7 @@ enum ib_flow_spec_type {
 	/* L3 header*/
 	IB_FLOW_SPEC_IPV4		= 0x30,
 	IB_FLOW_SPEC_IPV6		= 0x31,
+	IB_FLOW_SPEC_ESP                = 0x34,
 	/* L4 headers*/
 	IB_FLOW_SPEC_TCP		= 0x40,
 	IB_FLOW_SPEC_UDP		= 0x41,
@@ -1960,6 +1961,20 @@ struct ib_flow_spec_tunnel {
 	struct ib_flow_tunnel_filter  mask;
 };
 
+struct ib_flow_esp_filter {
+	__be32	spi;
+	__be32  seq;
+	/* Must be last */
+	u8	real_sz[0];
+};
+
+struct ib_flow_spec_esp {
+	u32                           type;
+	u16			      size;
+	struct ib_flow_esp_filter     val;
+	struct ib_flow_esp_filter     mask;
+};
+
 struct ib_flow_spec_action_tag {
 	enum ib_flow_spec_type	      type;
 	u16			      size;
@@ -1988,6 +2003,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_tcp_udp	tcp_udp;
 	struct ib_flow_spec_ipv6        ipv6;
 	struct ib_flow_spec_tunnel      tunnel;
+	struct ib_flow_spec_esp		esp;
 	struct ib_flow_spec_action_tag  flow_tag;
 	struct ib_flow_spec_action_drop drop;
 	struct ib_flow_spec_action_handle action;

commit 7d12f8d5a1645275dd452138bf1fe478be736704
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Mar 28 09:27:48 2018 +0300

    IB/uverbs: Add modify ESP flow_action
    
    flow_actions of ESP type could be modified during runtime. This could be
    common for example when ESN should be changed. Adding a new
    UVERBS_FLOW_ACTION_ESP_MODIFY method for changing ESP parameters of an
    existing ESP flow_action.
    The new method uses the UVERBS_FLOW_ACTION_ESP_CREATE attributes, but
    adds a new IB_FLOW_ACTION_ESP_FLAGS_MOD_ESP_ATTRS which means ESP_ATTRS
    should be changed.
    In addition, we add a new FLOW_ACTION_ESP_REPLAY_NONE replay type that
    could be used when one wants to disable a replay protection over a
    specific flow_action.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c674fc1e596b..8c3ca073016d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2038,6 +2038,7 @@ enum ib_flow_action_attrs_esp_flags {
 
 	/* Kernel flags */
 	IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED	= 1ULL << 32,
+	IB_FLOW_ACTION_ESP_FLAGS_MOD_ESP_ATTRS	= 1ULL << 33,
 };
 
 struct ib_flow_spec_list {
@@ -2404,6 +2405,9 @@ struct ib_device {
 							     const struct ib_flow_action_attrs_esp *attr,
 							     struct uverbs_attr_bundle *attrs);
 	int			   (*destroy_flow_action)(struct ib_flow_action *action);
+	int			   (*modify_flow_action_esp)(struct ib_flow_action *action,
+							     const struct ib_flow_action_attrs_esp *attr,
+							     struct uverbs_attr_bundle *attrs);
 
 	/**
 	 * rdma netdev operation

commit 21e82d3e1dcf9ce61ae387ca1a507cf53665336a
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Wed Mar 28 09:27:47 2018 +0300

    IB/uverbs: Introduce egress flow steering
    
    The egress flag indicates that this flow steering rule is for egress
    traffic. The scope of an egress rule is port-wide, meaning all packets
    originated from that port, which match the steering rule specification
    will be effected by this steering rule's action.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c1b9cba79710..c674fc1e596b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1854,7 +1854,8 @@ enum ib_flow_domain {
 
 enum ib_flow_flags {
 	IB_FLOW_ATTR_FLAGS_DONT_TRAP = 1UL << 1, /* Continue match, no steal */
-	IB_FLOW_ATTR_FLAGS_RESERVED  = 1UL << 2  /* Must be last */
+	IB_FLOW_ATTR_FLAGS_EGRESS = 1UL << 2, /* Egress flow */
+	IB_FLOW_ATTR_FLAGS_RESERVED  = 1UL << 3  /* Must be last */
 };
 
 struct ib_flow_eth_filter {

commit 9b828441976ef719f1008a9855fff95a45e474b8
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Mar 28 09:27:46 2018 +0300

    IB/uverbs: Add action_handle flow steering specification
    
    Binding a flow_action to flow steering rule requires using a new
    specification. Therefore, adding such an IB_FLOW_SPEC_ACTION_HANDLE flow
    specification.
    
    Flow steering rules could use flow_action(s) and as of that we need to
    avoid deleting flow_action(s) as long as they're being used.
    Moreover, when the attached rules are deleted, action_handle reference
    count should be decremented. Introducing a new mechanism of flow
    resources to keep track on the attached action_handle(s). Later on, this
    mechanism should be extended to other attached flow steering resources
    like flow counters.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 49da92143341..c1b9cba79710 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1836,6 +1836,7 @@ enum ib_flow_spec_type {
 	/* Actions */
 	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
 	IB_FLOW_SPEC_ACTION_DROP        = 0x1001,
+	IB_FLOW_SPEC_ACTION_HANDLE	= 0x1002,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
 #define IB_FLOW_SPEC_SUPPORT_LAYERS 8
@@ -1969,6 +1970,12 @@ struct ib_flow_spec_action_drop {
 	u16			      size;
 };
 
+struct ib_flow_spec_action_handle {
+	enum ib_flow_spec_type	      type;
+	u16			      size;
+	struct ib_flow_action	     *act;
+};
+
 union ib_flow_spec {
 	struct {
 		u32			type;
@@ -1982,6 +1989,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_tunnel      tunnel;
 	struct ib_flow_spec_action_tag  flow_tag;
 	struct ib_flow_spec_action_drop drop;
+	struct ib_flow_spec_action_handle action;
 };
 
 struct ib_flow_attr {

commit 2eb9beaee5d73130d28c54e91eecb8a186581e08
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Mar 28 09:27:45 2018 +0300

    IB/uverbs: Add flow_action create and destroy verbs
    
    A verbs application may receive and transmits packets using a data
    path pipeline. Sometimes, the first stage in the receive pipeline or
    the last stage in the transmit pipeline involves transforming a
    packet, either in order to make it easier for later stages to process
    it or to prepare it for transmission over the wire. Such transformation
    could be stripping/encapsulating the packet (i.e. vxlan),
    decrypting/encrypting it (i.e. ipsec), altering headers, doing some
    complex FPGA changes, etc.
    
    Some hardware could do such transformations without software data path
    intervention at all. The flow steering API supports steering a
    packet (either to a QP or dropping it) and some simple packet
    immutable actions (i.e. tagging a packet). Complex actions, that may
    change the packet, could bloat the flow steering API extensively.
    Sometimes the same action should be applied to several flows.
    In this case, it's easier to bind several flows to the same action and
    modify it than change all matching flows.
    
    Introducing a new flow_action object that abstracts any packet
    transformation (out of a standard and well defined set of actions).
    This flow_action object could be tied to a flow steering rule via a
    new specification.
    
    Currently, we support esp flow_action, which encrypts or decrypts a
    packet according to the given parameters. However, we present a
    flexible schema that could be used to other transformation actions tied
    to flow rules.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1e3059ce73b6..49da92143341 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -65,6 +65,7 @@
 #include <uapi/rdma/ib_user_verbs.h>
 #include <rdma/restrack.h>
 #include <uapi/rdma/rdma_user_ioctl.h>
+#include <uapi/rdma/ib_user_ioctl_verbs.h>
 
 #define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
 
@@ -2001,6 +2002,63 @@ struct ib_flow {
 	struct ib_uobject	*uobject;
 };
 
+enum ib_flow_action_type {
+	IB_FLOW_ACTION_UNSPECIFIED,
+	IB_FLOW_ACTION_ESP = 1,
+};
+
+struct ib_flow_action_attrs_esp_keymats {
+	enum ib_uverbs_flow_action_esp_keymat			protocol;
+	union {
+		struct ib_uverbs_flow_action_esp_keymat_aes_gcm aes_gcm;
+	} keymat;
+};
+
+struct ib_flow_action_attrs_esp_replays {
+	enum ib_uverbs_flow_action_esp_replay			protocol;
+	union {
+		struct ib_uverbs_flow_action_esp_replay_bmp	bmp;
+	} replay;
+};
+
+enum ib_flow_action_attrs_esp_flags {
+	/* All user-space flags at the top: Use enum ib_uverbs_flow_action_esp_flags
+	 * This is done in order to share the same flags between user-space and
+	 * kernel and spare an unnecessary translation.
+	 */
+
+	/* Kernel flags */
+	IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED	= 1ULL << 32,
+};
+
+struct ib_flow_spec_list {
+	struct ib_flow_spec_list	*next;
+	union ib_flow_spec		spec;
+};
+
+struct ib_flow_action_attrs_esp {
+	struct ib_flow_action_attrs_esp_keymats		*keymat;
+	struct ib_flow_action_attrs_esp_replays		*replay;
+	struct ib_flow_spec_list			*encap;
+	/* Used only if IB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED is enabled.
+	 * Value of 0 is a valid value.
+	 */
+	u32						esn;
+	u32						spi;
+	u32						seq;
+	u32						tfc_pad;
+	/* Use enum ib_flow_action_attrs_esp_flags */
+	u64						flags;
+	u64						hard_limit_pkts;
+};
+
+struct ib_flow_action {
+	struct ib_device		*device;
+	struct ib_uobject		*uobject;
+	enum ib_flow_action_type	type;
+	atomic_t			usecnt;
+};
+
 struct ib_mad_hdr;
 struct ib_grh;
 
@@ -2077,6 +2135,8 @@ struct ib_port_pkey_list {
 	struct list_head              pkey_list;
 };
 
+struct uverbs_attr_bundle;
+
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -2331,6 +2391,11 @@ struct ib_device {
 							   struct ib_rwq_ind_table_init_attr *init_attr,
 							   struct ib_udata *udata);
 	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
+	struct ib_flow_action *	   (*create_flow_action_esp)(struct ib_device *device,
+							     const struct ib_flow_action_attrs_esp *attr,
+							     struct uverbs_attr_bundle *attrs);
+	int			   (*destroy_flow_action)(struct ib_flow_action *action);
+
 	/**
 	 * rdma netdev operation
 	 *

commit 414448d249d82c9be93b35e61e0303e84ef2f959
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Apr 1 15:08:24 2018 +0300

    RDMA: Use ib_gid_attr during GID modification
    
    Now that ib_gid_attr contains device, port and index, simplify the
    provider APIs add_gid() and del_gid() to use device, port and index
    fields from the ib_gid_attr attributes structure.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dc2541f13d7f..1e3059ce73b6 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2151,34 +2151,29 @@ struct ib_device {
 	int		           (*query_gid)(struct ib_device *device,
 						u8 port_num, int index,
 						union ib_gid *gid);
-	/* When calling add_gid, the HW vendor's driver should
-	 * add the gid of device @device at gid index @index of
-	 * port @port_num to be @gid. Meta-info of that gid (for example,
-	 * the network device related to this gid is available
-	 * at @attr. @context allows the HW vendor driver to store extra
-	 * information together with a GID entry. The HW vendor may allocate
-	 * memory to contain this information and store it in @context when a
-	 * new GID entry is written to. Params are consistent until the next
-	 * call of add_gid or delete_gid. The function should return 0 on
+	/* When calling add_gid, the HW vendor's driver should add the gid
+	 * of device of port at gid index available at @attr. Meta-info of
+	 * that gid (for example, the network device related to this gid) is
+	 * available at @attr. @context allows the HW vendor driver to store
+	 * extra information together with a GID entry. The HW vendor driver may
+	 * allocate memory to contain this information and store it in @context
+	 * when a new GID entry is written to. Params are consistent until the
+	 * next call of add_gid or delete_gid. The function should return 0 on
 	 * success or error otherwise. The function could be called
-	 * concurrently for different ports. This function is only called
-	 * when roce_gid_table is used.
+	 * concurrently for different ports. This function is only called when
+	 * roce_gid_table is used.
 	 */
-	int		           (*add_gid)(struct ib_device *device,
-					      u8 port_num,
-					      unsigned int index,
-					      const union ib_gid *gid,
+	int		           (*add_gid)(const union ib_gid *gid,
 					      const struct ib_gid_attr *attr,
 					      void **context);
 	/* When calling del_gid, the HW vendor's driver should delete the
-	 * gid of device @device at gid index @index of port @port_num.
+	 * gid of device @device at gid index gid_index of port port_num
+	 * available in @attr.
 	 * Upon the deletion of a GID entry, the HW vendor must free any
 	 * allocated memory. The caller will clear @context afterwards.
 	 * This function is only called when roce_gid_table is used.
 	 */
-	int		           (*del_gid)(struct ib_device *device,
-					      u8 port_num,
-					      unsigned int index,
+	int		           (*del_gid)(const struct ib_gid_attr *attr,
 					      void **context);
 	int		           (*query_pkey)(struct ib_device *device,
 						 u8 port_num, u16 index, u16 *pkey);

commit 598ff6bae689453aa894bc38f3f1bb78eb131a61
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Apr 1 15:08:21 2018 +0300

    IB/core: Refactor GID modify code for RoCE
    
    Code is refactored to prepare separate functions for RoCE which can do more
    complex operations related to reference counting, while still
    maintainining code readability. This includes
    (a) Simplification to not perform netdevice checks and modifications
    for IB link layer.
    (b) Do not add RoCE GID entry which has NULL netdevice; instead return
    an error.
    (c) If GID addition fails at provider level add_gid(), do not add the
    entry in the cache and keep the entry marked as INVALID.
    (d) Simplify and reuse the ib_cache_gid_add()/del() routines so that they
    can be used even for modifying default GIDs. This avoid some code
    duplication in modifying default GIDs.
    (e) find_gid() routine refers to the data entry flags to qualify a GID
    as valid or invalid GID rather than depending on attributes and zeroness
    of the GID content.
    (f) gid_table_reserve_default() sets the GID default attribute at
    beginning while setting up the GID table. There is no need to use
    default_gid flag in low level functions such as write_gid(), add_gid(),
    del_gid(), as they never need to update the DEFAULT property of the GID
    entry while during GID table update.
    
    As as result of this refactor, reserved GID 0:0:0:0:0:0:0:0 is no longer
    searchable as described below.
    
    A unicast GID entry of 0:0:0:0:0:0:0:0 is Reserved GID as per the IB
    spec version 1.3 section 4.1.1, point (6) whose snippet is below.
    
    "The unicast GID address 0:0:0:0:0:0:0:0 is reserved - referred to as
    the Reserved GID. It shall never be assigned to any endport. It shall
    not be used as a destination address or in a global routing header
    (GRH)."
    
    GID table cache now only stores valid GID entries. Before this patch,
    Reserved GID 0:0:0:0:0:0:0:0 was searchable in the GID table using
    ib_find_cached_gid_by_port() and other similar find routines.
    
    Zero GID is no longer searchable as it shall not to be present in GRH or
    path recored entry as described in IB spec version 1.3 section 4.1.1,
    point (6), section 12.7.10 and section 12.7.20.
    
    ib_cache_update() is simplified to check link layer once, use unified
    locking scheme for all link layers, removed temporary gid table
    allocation/free logic.
    
    Additionally,
    (a) Expand ib_gid_attr to store port and index so that GID query
    routines can get port and index information from the attribute structure.
    (b) Expand ib_gid_attr to store device as well so that in future code when
    GID reference counting is done, device is used to reach back to the GID
    table entry.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a2f658125795..dc2541f13d7f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -91,8 +91,11 @@ enum ib_gid_type {
 
 #define ROCE_V2_UDP_DPORT      4791
 struct ib_gid_attr {
-	enum ib_gid_type	gid_type;
 	struct net_device	*ndev;
+	struct ib_device	*device;
+	enum ib_gid_type	gid_type;
+	u16			index;
+	u8			port_num;
 };
 
 enum rdma_node_type {

commit 72e1ff0fb7e09c34956e4b3f619481da4d9787c1
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Apr 1 15:08:18 2018 +0300

    RDMA/core: Update query_gid documentation for HCA drivers
    
    query_gid() should return right GID value for iWarp and IB link layers.
    It is a no-op for RoCE link layer.  Update the documentation to reflect
    this.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 48f416fabe0c..a2f658125795 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2141,6 +2141,10 @@ struct ib_device {
 	 */
 	struct net_device	  *(*get_netdev)(struct ib_device *device,
 						 u8 port_num);
+	/* query_gid should be return GID value for @device, when @port_num
+	 * link layer is either IB or iWarp. It is no-op if @port_num port
+	 * is RoCE link layer.
+	 */
 	int		           (*query_gid)(struct ib_device *device,
 						u8 port_num, int index,
 						union ib_gid *gid);

commit e945130b52bea65d15f9bdf54949d4cb7a88db7f
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Mar 27 15:51:05 2018 +0300

    IB/core: Protect against concurrent access to hardware stats
    
    Currently access to hardware stats buffer isn't protected, this can
    result in multiple writes and reads at the same time to the same
    memory location. This can lead to providing an incorrect value to
    the user. Add a mutex to protect against it.
    
    Fixes: b40f4757daa1 ("IB/core: Make device counter infrastructure dynamic")
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e9288d0f627e..48f416fabe0c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -470,6 +470,9 @@ enum ib_port_speed {
 
 /**
  * struct rdma_hw_stats
+ * @lock - Mutex to protect parallel write access to lifespan and values
+ *    of counters, which are 64bits and not guaranteeed to be written
+ *    atomicaly on 32bits systems.
  * @timestamp - Used by the core code to track when the last update was
  * @lifespan - Used by the core code to determine how old the counters
  *   should be before being updated again.  Stored in jiffies, defaults
@@ -485,6 +488,7 @@ enum ib_port_speed {
  *   filled in by the drivers get_stats routine
  */
 struct rdma_hw_stats {
+	struct mutex	lock; /* Protect lifespan and values[] */
 	unsigned long	timestamp;
 	unsigned long	lifespan;
 	const char * const *names;

commit 070f2d7e264acd6316fc24092b7f51a18c75ac9c
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri Mar 23 19:47:39 2018 +0300

    net: Drop NETDEV_UNREGISTER_FINAL
    
    Last user is gone after bdf5bd7f2132 "rds: tcp: remove
    register_netdevice_notifier infrastructure.", so we can
    remove this netdevice command. This allows to delete
    rtnl_lock() in netdev_run_todo(), which is hot path for
    net namespace unregistration.
    
    dev_change_net_namespace() and netdev_wait_allrefs()
    have rcu_barrier() before NETDEV_UNREGISTER_FINAL call,
    and the source commits say they were introduced to
    delemit the call with NETDEV_UNREGISTER, but this patch
    leaves them on the places, since they require additional
    analysis, whether we need in them for something else.
    
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ff3ed435701f..6eb174753acf 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2122,8 +2122,8 @@ struct ib_device {
 	 * net device of device @device at port @port_num or NULL if such
 	 * a net device doesn't exist. The vendor driver should call dev_hold
 	 * on this net device. The HW vendor's device driver must guarantee
-	 * that this function returns NULL before the net device reaches
-	 * NETDEV_UNREGISTER_FINAL state.
+	 * that this function returns NULL before the net device has finished
+	 * NETDEV_UNREGISTER state.
 	 */
 	struct net_device	  *(*get_netdev)(struct ib_device *device,
 						 u8 port_num);

commit c66db31113948ba61682f55265df8d032e793fcc
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Mar 19 15:02:36 2018 +0200

    IB/uverbs: Safely extend existing attributes
    
    Previously, we've used UVERBS_ATTR_SPEC_F_MIN_SZ for extending existing
    attributes. The behavior of this flag was the kernel accepts anything
    bigger than the minimum size it specified. This is unsafe, since in
    order to safely extend an attribute, we need to make sure unknown size
    is zeroed. Replacing UVERBS_ATTR_SPEC_F_MIN_SZ with
    UVERBS_ATTR_SPEC_F_MIN_SZ_OR_ZERO, which essentially checks that the
    unknown size is zero. In addition, attributes are now decorated with
    UVERBS_ATTR_TYPE and UVERBS_ATTR_STRUCT, so we can provide the minimum
    and known length.
    
    Users of this flag needs to use copy_from_or_zero functions/macros.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2357f2b29610..e9288d0f627e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2446,11 +2446,9 @@ static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len
 	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
 }
 
-static inline bool ib_is_udata_cleared(struct ib_udata *udata,
-				       size_t offset,
-				       size_t len)
+static inline bool ib_is_buffer_cleared(const void __user *p,
+					size_t len)
 {
-	const void __user *p = udata->inbuf + offset;
 	bool ret;
 	u8 *buf;
 
@@ -2466,6 +2464,13 @@ static inline bool ib_is_udata_cleared(struct ib_udata *udata,
 	return ret;
 }
 
+static inline bool ib_is_udata_cleared(struct ib_udata *udata,
+				       size_t offset,
+				       size_t len)
+{
+	return ib_is_buffer_cleared(udata->inbuf + offset, len);
+}
+
 /**
  * ib_modify_qp_is_ok - Check that the supplied attribute mask
  * contains all required attributes and no attributes not allowed for

commit 0ede73bc012c98fba244b33efbc42e48dd23ee9a
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Mar 19 15:02:34 2018 +0200

    IB/uverbs: Extend uverbs_ioctl header with driver_id
    
    Extending uverbs_ioctl header with driver_id and another reserved
    field. driver_id should be used in order to identify the driver.
    Since every driver could have its own parsing tree, this is necessary
    for strace support.
    Downstream patches take off the EXPERIMENTAL flag from the ioctl() IB
    support and thus we add some reserved fields for future usage.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3cc48f34e3e4..2357f2b29610 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -64,6 +64,7 @@
 #include <linux/cgroup_rdma.h>
 #include <uapi/rdma/ib_user_verbs.h>
 #include <rdma/restrack.h>
+#include <uapi/rdma/rdma_user_ioctl.h>
 
 #define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
 
@@ -2385,6 +2386,7 @@ struct ib_device {
 						     int comp_vector);
 
 	struct uverbs_root_spec		*specs_root;
+	enum rdma_driver_id		driver_id;
 };
 
 struct ib_client {

commit 80cf79ae4f688ead1dca8878990709c0446c3992
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Mar 19 12:21:43 2018 +0200

    RDMA/verbs: Remove restrack entry from XRCD structure
    
    XRCD object is not implemented in the restrack, so lets remove it.
    
    Fixes: 02d8883f520e ("RDMA/restrack: Add general infrastructure to track RDMA resources")
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73b2387e3f74..ff3ed435701f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1537,10 +1537,6 @@ struct ib_xrcd {
 
 	struct mutex		tgt_qp_mutex;
 	struct list_head	tgt_qp_list;
-	/*
-	 * Implementation details of the RDMA core, don't use in drivers:
-	 */
-	struct rdma_restrack_entry res;
 };
 
 struct ib_ah {

commit b19744e965abed7ad0167c25097f405b88ce5d13
Author: Parav Pandit <parav@mellanox.com>
Date:   Mon Mar 19 08:07:14 2018 +0200

    IB/core: Remove unimplemented ib_peek_cq
    
    ib_peek_cq() verb doesn't seem be implemented in current code.
    There is some past reference to it at [1] about it being unimplemented.
    
    Lot of user documentation created out of kdoc refers to this
    unimplemented API. Therefore, remove unimplemented API.
    
    [1] http://lists.openfabrics.org/pipermail/ofw/2008-May/002465.html
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ac3791e056cf..3cc48f34e3e4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3225,18 +3225,6 @@ static inline int ib_poll_cq(struct ib_cq *cq, int num_entries,
 	return cq->device->poll_cq(cq, num_entries, wc);
 }
 
-/**
- * ib_peek_cq - Returns the number of unreaped completions currently
- *   on the specified CQ.
- * @cq: The CQ to peek.
- * @wc_cnt: A minimum number of unreaped completions to check for.
- *
- * If the number of unreaped completions is greater than or equal to wc_cnt,
- * this function returns wc_cnt, otherwise, it returns the actual number of
- * unreaped completions.
- */
-int ib_peek_cq(struct ib_cq *cq, int wc_cnt);
-
 /**
  * ib_req_notify_cq - Request completion notification on a CQ.
  * @cq: The CQ to generate an event for.

commit b26c4a1138dff34cff507bafa4c87e365f4145a6
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Mar 13 16:06:12 2018 +0200

    IB/{core, ipoib}: Simplify ib_find_gid() for unused ndev
    
    ib_find_gid() is only used by IPoIB driver. For IB link layer, GID table
    entries are not based on netdevice. Netdevice parameter is unused here.
    Therefore, it is removed.
    
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5eb10c2470f0..ac3791e056cf 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2857,7 +2857,7 @@ int ib_modify_port(struct ib_device *device,
 		   struct ib_port_modify *port_modify);
 
 int ib_find_gid(struct ib_device *device, union ib_gid *gid,
-		struct net_device *ndev, u8 *port_num, u16 *index);
+		u8 *port_num, u16 *index);
 
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);

commit 19b1f54099b6ee334acbfbcfbdffd1d1f057216d
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Mar 11 13:51:35 2018 +0200

    RDMA/verbs: Simplify modify QP check
    
    All callers to ib_modify_qp_is_ok() provides enum ib_qp_state
    makes the checks of out-of-scope redundant. Let's remove them
    together with updating function signature to return boolean result.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7df3274818f9..5eb10c2470f0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2480,9 +2480,9 @@ static inline bool ib_is_udata_cleared(struct ib_udata *udata,
  * transition from cur_state to next_state is allowed by the IB spec,
  * and that the attribute mask supplied is allowed for the transition.
  */
-int ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
-		       enum ib_qp_type type, enum ib_qp_attr_mask mask,
-		       enum rdma_link_layer ll);
+bool ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
+			enum ib_qp_type type, enum ib_qp_attr_mask mask,
+			enum rdma_link_layer ll);
 
 void ib_register_event_handler(struct ib_event_handler *event_handler);
 void ib_unregister_event_handler(struct ib_event_handler *event_handler);

commit fccec5b89ac61ebe2f353feecd08a16621f2418b
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Thu Mar 1 13:58:13 2018 -0800

    RDMA/nldev: provide detailed MR information
    
    Implement the RDMA nldev netlink interface for dumping detailed
    MR information.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73b2387e3f74..7df3274818f9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1772,6 +1772,11 @@ struct ib_mr {
 		struct ib_uobject	*uobject;	/* user */
 		struct list_head	qp_entry;	/* FR */
 	};
+
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct rdma_restrack_entry res;
 };
 
 struct ib_mw {

commit 87daac68f77a3e21a1113f816e6a7be0b38bdde8
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Thu Feb 1 10:57:03 2018 -0800

    IB/core: Map iWarp AH type to undefined in rdma_ah_find_type
    
    iWarp devices do not support the creation of address handles
    so return AH_ATTR_TYPE_UNDEFINED for all iWarp devices.
    
    While we are here reduce the size of port_num to u8 and add
    a comment.
    
    Fixes: 44c58487d51a ("IB/core: Define 'ib' and 'roce' rdma_ah_attr types")
    Reported-by: Parav Pandit <parav@mellanox.com>
    CC: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5263c86fd103..73b2387e3f74 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -874,6 +874,7 @@ struct ib_mr_status {
 __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 
 enum rdma_ah_attr_type {
+	RDMA_AH_ATTR_TYPE_UNDEFINED,
 	RDMA_AH_ATTR_TYPE_IB,
 	RDMA_AH_ATTR_TYPE_ROCE,
 	RDMA_AH_ATTR_TYPE_OPA,
@@ -3810,17 +3811,24 @@ static inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,
 	grh->traffic_class = traffic_class;
 }
 
-/*Get AH type */
+/**
+ * rdma_ah_find_type - Return address handle type.
+ *
+ * @dev: Device to be checked
+ * @port_num: Port number
+ */
 static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
-						       u32 port_num)
+						       u8 port_num)
 {
 	if (rdma_protocol_roce(dev, port_num))
 		return RDMA_AH_ATTR_TYPE_ROCE;
-	else if ((rdma_protocol_ib(dev, port_num)) &&
-		 (rdma_cap_opa_ah(dev, port_num)))
-		return RDMA_AH_ATTR_TYPE_OPA;
-	else
+	if (rdma_protocol_ib(dev, port_num)) {
+		if (rdma_cap_opa_ah(dev, port_num))
+			return RDMA_AH_ATTR_TYPE_OPA;
 		return RDMA_AH_ATTR_TYPE_IB;
+	}
+
+	return RDMA_AH_ATTR_TYPE_UNDEFINED;
 }
 
 /**

commit 02d8883f520ee91c4c40c0a31892eb25ea2df2c9
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jan 28 11:17:20 2018 +0200

    RDMA/restrack: Add general infrastructure to track RDMA resources
    
    The RDMA subsystem has very strict set of objects to work with, but it
    completely lacks tracking facilities and has no visibility of resource
    utilization.
    
    The following patch adds such infrastructure to keep track of RDMA
    resources to help with debugging of user space applications. The primary
    user of this infrastructure is RDMA nldev netlink (following patches), to
    be exposed to userspace via rdmatool, but it is not limited too that.
    
    At this stage, the main three objects (PD, CQ and QP) are added, and more
    will be added later.
    
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index df3ab2d967f7..5263c86fd103 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -63,6 +63,7 @@
 #include <linux/uaccess.h>
 #include <linux/cgroup_rdma.h>
 #include <uapi/rdma/ib_user_verbs.h>
+#include <rdma/restrack.h>
 
 #define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
 
@@ -1525,6 +1526,7 @@ struct ib_pd {
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
 	struct ib_mr	       *__internal_mr;
+	struct rdma_restrack_entry res;
 };
 
 struct ib_xrcd {
@@ -1534,6 +1536,10 @@ struct ib_xrcd {
 
 	struct mutex		tgt_qp_mutex;
 	struct list_head	tgt_qp_list;
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct rdma_restrack_entry res;
 };
 
 struct ib_ah {
@@ -1565,6 +1571,10 @@ struct ib_cq {
 		struct irq_poll		iop;
 		struct work_struct	work;
 	};
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct rdma_restrack_entry res;
 };
 
 struct ib_srq {
@@ -1741,6 +1751,11 @@ struct ib_qp {
 	struct ib_rwq_ind_table *rwq_ind_tbl;
 	struct ib_qp_security  *qp_sec;
 	u8			port;
+
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct rdma_restrack_entry     res;
 };
 
 struct ib_mr {
@@ -2347,6 +2362,10 @@ struct ib_device {
 #endif
 
 	u32                          index;
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers
+	 */
+	struct rdma_restrack_root     res;
 
 	/**
 	 * The following mandatory functions are used only at device

commit f66c8ba4c9fa193481fdb8030504287bf5ad4d69
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jan 28 11:17:19 2018 +0200

    RDMA/core: Save kernel caller name when creating PD and CQ objects
    
    The KBUILD_MODNAME variable contains the module name and it is known for
    kernel users during compilation, so let's reuse it to track the owners.
    
    Followup patches will store this for resource tracking.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6ebb46e4ecf5..df3ab2d967f7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3130,8 +3130,12 @@ static inline int ib_post_recv(struct ib_qp *qp,
 	return qp->device->post_recv(qp, recv_wr, bad_recv_wr);
 }
 
-struct ib_cq *ib_alloc_cq(struct ib_device *dev, void *private,
-		int nr_cqe, int comp_vector, enum ib_poll_context poll_ctx);
+struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
+			    int nr_cqe, int comp_vector,
+			    enum ib_poll_context poll_ctx, const char *caller);
+#define ib_alloc_cq(device, priv, nr_cqe, comp_vect, poll_ctx) \
+	__ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
+
 void ib_free_cq(struct ib_cq *cq);
 int ib_process_cq_direct(struct ib_cq *cq, int budget);
 
@@ -3555,8 +3559,11 @@ int ib_detach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 /**
  * ib_alloc_xrcd - Allocates an XRC domain.
  * @device: The device on which to allocate the XRC domain.
+ * @caller: Module name for kernel consumers
  */
-struct ib_xrcd *ib_alloc_xrcd(struct ib_device *device);
+struct ib_xrcd *__ib_alloc_xrcd(struct ib_device *device, const char *caller);
+#define ib_alloc_xrcd(device) \
+	__ib_alloc_xrcd((device), KBUILD_MODNAME)
 
 /**
  * ib_dealloc_xrcd - Deallocates an XRC domain.

commit e449644741af523fa345f41494e19a0914709a69
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jan 28 11:17:18 2018 +0200

    RDMA/core: Use the MODNAME instead of the function name for pd callers
    
    Each of our modules only allocates a PD in one place, so there isn't any
    loss in detail, while MODNAME is more useful and recognizable as something
    to expose to the user.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0b2942586840..6ebb46e4ecf5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2853,7 +2853,7 @@ enum ib_pd_flags {
 struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,
 		const char *caller);
 #define ib_alloc_pd(device, flags) \
-	__ib_alloc_pd((device), (flags), __func__)
+	__ib_alloc_pd((device), (flags), KBUILD_MODNAME)
 void ib_dealloc_pd(struct ib_pd *pd);
 
 /**

commit beb801ac51be3e024edef435333198d59ccfbb8f
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jan 26 15:16:46 2018 -0700

    RDMA: Move enum ib_cq_creation_flags to uapi headers
    
    The flags field the enum is used with comes directly from the uapi
    so it belongs in the uapi headers for clarity and so userspace can
    use it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5e32fe781ca3..0b2942586840 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -300,11 +300,6 @@ struct ib_tm_caps {
 	u32 max_sge;
 };
 
-enum ib_cq_creation_flags {
-	IB_CQ_FLAGS_TIMESTAMP_COMPLETION   = 1 << 0,
-	IB_CQ_FLAGS_IGNORE_OVERRUN	   = 1 << 1,
-};
-
 struct ib_cq_init_attr {
 	unsigned int	cqe;
 	int		comp_vector;

commit a6532e7139660c103dda181aa5b2c734aa26ed6c
Author: Parav Pandit <parav@mellanox.com>
Date:   Fri Jan 12 07:58:42 2018 +0200

    RDMA/core: Clarify rdma_ah_find_type
    
    iWARP does not use rdma_ah_attr_type, and for this reason we do not have a
    RDMA_AH_ATTR_TYPE_IWARP. rdma_ah_find_type should not even be called on iwarp
    ports and for clarity it shouldn't have a special test for iWarp.
    
    This changes the result from RDMA_AH_ATTR_TYPE_ROCE to RDMA_AH_ATTR_TYPE_IB
    when wrongly called on an iWarp port.
    
    Fixes: 44c58487d51a ("IB/core: Define 'ib' and 'roce' rdma_ah_attr types")
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f2b1e2244016..5e32fe781ca3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3793,8 +3793,7 @@ static inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,
 static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
 						       u32 port_num)
 {
-	if ((rdma_protocol_roce(dev, port_num)) ||
-	    (rdma_protocol_iwarp(dev, port_num)))
+	if (rdma_protocol_roce(dev, port_num))
 		return RDMA_AH_ATTR_TYPE_ROCE;
 	else if ((rdma_protocol_ib(dev, port_num)) &&
 		 (rdma_cap_opa_ah(dev, port_num)))

commit cd2a6e7d384b043d5d029e39663061cebc949385
Author: Bodong Wang <bodong@mellanox.com>
Date:   Fri Jan 12 07:58:41 2018 +0200

    IB/core: Fix ib_wc structure size to remain in 64 bytes boundary
    
    The change of slid from u16 to u32 results in sizeof(struct ib_wc)
    cross 64B boundary, which causes more cache misses. This patch
    rearranges the fields and remain the size to 64B.
    
    Pahole output before this change:
    
    struct ib_wc {
            union {
                    u64                wr_id;                /*           8 */
                    struct ib_cqe *    wr_cqe;               /*           8 */
            };                                               /*     0     8 */
            enum ib_wc_status          status;               /*     8     4 */
            enum ib_wc_opcode          opcode;               /*    12     4 */
            u32                        vendor_err;           /*    16     4 */
            u32                        byte_len;             /*    20     4 */
            struct ib_qp *             qp;                   /*    24     8 */
            union {
                    __be32             imm_data;             /*           4 */
                    u32                invalidate_rkey;      /*           4 */
            } ex;                                            /*    32     4 */
            u32                        src_qp;               /*    36     4 */
            int                        wc_flags;             /*    40     4 */
            u16                        pkey_index;           /*    44     2 */
    
            /* XXX 2 bytes hole, try to pack */
    
            u32                        slid;                 /*    48     4 */
            u8                         sl;                   /*    52     1 */
            u8                         dlid_path_bits;       /*    53     1 */
            u8                         port_num;             /*    54     1 */
            u8                         smac[6];              /*    55     6 */
    
            /* XXX 1 byte hole, try to pack */
    
            u16                        vlan_id;              /*    62     2 */
            /* --- cacheline 1 boundary (64 bytes) --- */
            u8                         network_hdr_type;     /*    64     1 */
    
            /* size: 72, cachelines: 2, members: 17 */
            /* sum members: 62, holes: 2, sum holes: 3 */
            /* padding: 7 */
            /* last cacheline: 8 bytes */
    };
    
    Pahole output after this change:
    
    struct ib_wc {
            union {
                    u64                wr_id;                /*           8 */
                    struct ib_cqe *    wr_cqe;               /*           8 */
            };                                               /*     0     8 */
            enum ib_wc_status          status;               /*     8     4 */
            enum ib_wc_opcode          opcode;               /*    12     4 */
            u32                        vendor_err;           /*    16     4 */
            u32                        byte_len;             /*    20     4 */
            struct ib_qp *             qp;                   /*    24     8 */
            union {
                    __be32             imm_data;             /*           4 */
                    u32                invalidate_rkey;      /*           4 */
            } ex;                                            /*    32     4 */
            u32                        src_qp;               /*    36     4 */
            u32                        slid;                 /*    40     4 */
            int                        wc_flags;             /*    44     4 */
            u16                        pkey_index;           /*    48     2 */
            u8                         sl;                   /*    50     1 */
            u8                         dlid_path_bits;       /*    51     1 */
            u8                         port_num;             /*    52     1 */
            u8                         smac[6];              /*    53     6 */
    
            /* XXX 1 byte hole, try to pack */
    
            u16                        vlan_id;              /*    60     2 */
            u8                         network_hdr_type;     /*    62     1 */
    
            /* size: 64, cachelines: 1, members: 17 */
            /* sum members: 62, holes: 1, sum holes: 1 */
            /* padding: 1 */
    };
    
    Cc: <stable@vger.kernel.org> # v4.13
    Fixes: 7db20ecd1d97 ("IB/core: Change wc.slid from 16 to 32 bits")
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f25c03687ee9..f2b1e2244016 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -983,9 +983,9 @@ struct ib_wc {
 		u32		invalidate_rkey;
 	} ex;
 	u32			src_qp;
+	u32			slid;
 	int			wc_flags;
 	u16			pkey_index;
-	u32			slid;
 	u8			sl;
 	u8			dlid_path_bits;
 	u8			port_num;	/* valid only for DR SMPs on switches */

commit 32f69e4be269739c3850cd20f1a3322e95c1145f
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jan 4 17:25:36 2018 +0200

    {net, IB}/mlx5: Manage port association for multiport RoCE
    
    When mlx5_ib_add is called determine if the mlx5 core device being
    added is capable of dual port RoCE operation. If it is, determine
    whether it is a master device or a slave device using the
    num_vhca_ports and affiliate_nic_vport_criteria capabilities.
    
    If the device is a slave, attempt to find a master device to affiliate it
    with. Devices that can be affiliated will share a system image guid. If
    none are found place it on a list of unaffiliated ports. If a master is
    found bind the port to it by configuring the port affiliation in the NIC
    vport context.
    
    Similarly when mlx5_ib_remove is called determine the port type. If it's
    a slave port, unaffiliate it from the master device, otherwise just
    remove it from the unaffiliated port list.
    
    The IB device is registered as a multiport device, even if a 2nd port is
    not available for affiliation. When the 2nd port is affiliated later the
    GID cache must be refreshed in order to get the default GIDs for the 2nd
    port in the cache. Export roce_rescan_device to provide a mechanism to
    refresh the cache after a new port is bound.
    
    In a multiport configuration all IB object (QP, MR, PD, etc) related
    commands should flow through the master mlx5_core_dev, other commands
    must be sent to the slave port mlx5_core_mdev, an interface is provide
    to get the correct mdev for non IB object commands.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e44a8adac677..f25c03687ee9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3850,4 +3850,12 @@ ib_get_vector_affinity(struct ib_device *device, int comp_vector)
 
 }
 
+/**
+ * rdma_roce_rescan_device - Rescan all of the network devices in the system
+ * and add their gids, as needed, to the relevant RoCE devices.
+ *
+ * @device:         the rdma device
+ */
+void rdma_roce_rescan_device(struct ib_device *ibdev);
+
 #endif /* IB_VERBS_H */

commit 8011c1e33626ea7b04f74f648aad7bb2e48f8a81
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 2 16:19:30 2018 +0200

    IB/core: Introduce driver QP type
    
    Vendors can implement type of QPs that are not described in the
    InfiniBand specification. To still be able to use the IB/core layer
    services (e.g. user object management) without tainting this layer with
    driver proprietary logic, a new QP type is added - IB_QPT_DRIVER. This
    will be a general QP type that the core layer doesn't know about its true nature.
    When a command like create_qp() is passed to a hardware driver the extra
    data that is required is taken from the driver channel.
    Downstream patches from this series will use that QP type in the mlx5
    driver.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 60c3268c8c04..e44a8adac677 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1082,6 +1082,7 @@ enum ib_qp_type {
 	IB_QPT_XRC_INI = 9,
 	IB_QPT_XRC_TGT,
 	IB_QPT_MAX,
+	IB_QPT_DRIVER = 0xFF,
 	/* Reserve a range for qp types internal to the low level driver.
 	 * These qp types will not be visible at the IB core layer, so the
 	 * IB_QPT_MAX usages should not be affected in the core layer

commit f6bdb14267ba799296a49a3de9c2811636b768f9
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Nov 14 14:52:17 2017 +0200

    IB/{core, umad, cm}: Rename ib_init_ah_from_wc to ib_init_ah_attr_from_wc
    
    Currently ib_init_ah_from_wc initializes address handle attributes and
    not the address handle object itself.
    To avoid confusion between ah_attr vs ah, ib_init_ah_from_wc is
    renamed to ib_init_ah_attr_from_wc to reflect that its initialzes
    ah_attr.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cfd837049a32..60c3268c8c04 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2904,7 +2904,7 @@ int ib_get_gids_from_rdma_hdr(const union rdma_network_hdr *hdr,
 int ib_get_rdma_header_version(const union rdma_network_hdr *hdr);
 
 /**
- * ib_init_ah_from_wc - Initializes address handle attributes from a
+ * ib_init_ah_attr_from_wc - Initializes address handle attributes from a
  *   work completion.
  * @device: Device on which the received message arrived.
  * @port_num: Port on which the received message arrived.
@@ -2914,9 +2914,9 @@ int ib_get_rdma_header_version(const union rdma_network_hdr *hdr);
  * @ah_attr: Returned attributes that can be used when creating an address
  *   handle for replying to the message.
  */
-int ib_init_ah_from_wc(struct ib_device *device, u8 port_num,
-		       const struct ib_wc *wc, const struct ib_grh *grh,
-		       struct rdma_ah_attr *ah_attr);
+int ib_init_ah_attr_from_wc(struct ib_device *device, u8 port_num,
+			    const struct ib_wc *wc, const struct ib_grh *grh,
+			    struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_create_ah_from_wc - Creates an address handle associated with the

commit dbb12562f7c2377c210ed6b2e79eda5bfe23c30c
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Nov 14 14:52:12 2017 +0200

    IB/{core, ipoib}: Simplify ib_find_gid to search only for IB link layer
    
    Currently there are no users of ib_find_gid for RoCE transport. It is
    only used by IPoIB.
    Therefore its simplified to ignore RoCE ports and GID type check which
    was previously done for every port.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fd84cda5ed7c..cfd837049a32 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2836,8 +2836,7 @@ int ib_modify_port(struct ib_device *device,
 		   struct ib_port_modify *port_modify);
 
 int ib_find_gid(struct ib_device *device, union ib_gid *gid,
-		enum ib_gid_type gid_type, struct net_device *ndev,
-		u8 *port_num, u16 *index);
+		struct net_device *ndev, u8 *port_num, u16 *index);
 
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);

commit 4190b4e96954e2c3597021d29435c3f8db8d3129
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Nov 13 10:51:19 2017 +0200

    RDMA/core: Rename kernel modify_cq to better describe its usage
    
    Current ib_modify_cq() is used to set CQ moderation parameters.
    
    This patch renames ib_modify_cq() to be rdma_set_cq_moderation(),
    because the kernel version of RDMA API doesn't need to follow already
    exposed to user's API pattern (create_XXX/modify_XXX/query_XXX/destroy_XXX)
    and better to have more accurate name which describes the actual usage.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0b484c023fa9..fd84cda5ed7c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3169,13 +3169,13 @@ struct ib_cq *ib_create_cq(struct ib_device *device,
 int ib_resize_cq(struct ib_cq *cq, int cqe);
 
 /**
- * ib_modify_cq - Modifies moderation params of the CQ
+ * rdma_set_cq_moderation - Modifies moderation params of the CQ
  * @cq: The CQ to modify.
  * @cq_count: number of CQEs that will trigger an event
  * @cq_period: max period of time in usec before triggering an event
  *
  */
-int ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
+int rdma_set_cq_moderation(struct ib_cq *cq, u16 cq_count, u16 cq_period);
 
 /**
  * ib_destroy_cq - Destroys the specified CQ.

commit 18bd90729237dc6ddbad01bc9618148224f03590
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Mon Nov 13 10:51:16 2017 +0200

    IB/uverbs: Add CQ moderation capability to query_device
    
    The query_device function can now obtain the maximum values for
    cq_max_count and cq_period, needed for CQ moderation.
    cq_max_count is a 16 bits number that determines the number
    of CQEs to accumulate before generating an event.
    cq_period is a 16 bits number that determines the timeout in micro
    seconds from the last event generated, upon which a new event will
    be generated even if cq_max_count was not reached.
    
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8e0d3780ce4e..0b484c023fa9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -315,6 +315,11 @@ enum ib_cq_attr_mask {
 	IB_CQ_MODERATE = 1 << 0,
 };
 
+struct ib_cq_caps {
+	u16     max_cq_moderation_count;
+	u16     max_cq_moderation_period;
+};
+
 struct ib_device_attr {
 	u64			fw_ver;
 	__be64			sys_image_guid;
@@ -365,6 +370,7 @@ struct ib_device_attr {
 	u32			max_wq_type_rq;
 	u32			raw_packet_caps; /* Use ib_raw_packet_caps enum */
 	struct ib_tm_caps	tm_caps;
+	struct ib_cq_caps       cq_caps;
 };
 
 enum ib_mtu {

commit 869ddcf8b351ace5bf8860f3cd6265dccb382426
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Mon Nov 13 10:51:13 2017 +0200

    IB/uverbs: Allow CQ moderation with modify CQ
    
    Uverbs support in modify_cq for CQ moderation only.
    Gives ability to change cq_max_count and cq_period.
    CQ moderation enhance performance by moderating the number
    of CQEs needed to create an event instead of application
    having to suffer from event per-CQE.
    To achieve CQ moderation the application needs to set cq_max_count
    and cq_period.
    cq_max_count - defines the number of CQEs needed to create an event.
    cq_period - defines the timeout (micro seconds) between last
                event and a new one that will occur even if
                cq_max_count was not satisfied
    
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0b671982bbb3..8e0d3780ce4e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -311,6 +311,10 @@ struct ib_cq_init_attr {
 	u32		flags;
 };
 
+enum ib_cq_attr_mask {
+	IB_CQ_MODERATE = 1 << 0,
+};
+
 struct ib_device_attr {
 	u64			fw_ver;
 	__be64			sys_image_guid;

commit e1d2e88733695038754d3303b180f8005a02b6f1
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Sun Oct 29 13:59:44 2017 +0200

    IB/core: Add PCI write end padding flags for WQ and QP
    
    There are root complexes that are able to optimize their
    performance when incoming data is multiple full cache lines.
    
    PCI write end padding is the device's ability to pad the ending of
    incoming packets (scatter) to full cache line such that the last
    upstream write generated by an incoming packet will be a full cache
    line.
    
    Add a relevant entry to ib_device_cap_flags to report such capability
    of an RDMA device.
    
    Add the QP and WQ create flags:
     * A QP/WQ created with a scatter end padding flag will cause
       HW to pad the last upstream write generated by a packet to cache line.
    
    User should consider several factors before activating this feature:
    - In case of high CPU memory load (which may cause PCI back pressure in
      turn), if a large percent of the writes are partial cache line, this
      feature should be checked as an optional solution.
    - This feature might reduce performance if most packets are between one
      and two cache lines and PCIe throughput has reached its maximum
      capacity. E.g. 65B packet from the network port will lead to 128B
      write on PCIe, which may cause traffic on PCIe to reach high
      throughput.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9810e4568635..0b671982bbb3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -229,6 +229,8 @@ enum ib_device_cap_flags {
 	/* Deprecated. Please use IB_RAW_PACKET_CAP_SCATTER_FCS. */
 	IB_DEVICE_RAW_SCATTER_FCS		= (1ULL << 34),
 	IB_DEVICE_RDMA_NETDEV_OPA_VNIC		= (1ULL << 35),
+	/* The device supports padding incoming writes to cacheline. */
+	IB_DEVICE_PCI_WRITE_END_PADDING		= (1ULL << 36),
 };
 
 enum ib_signature_prot_cap {
@@ -1098,6 +1100,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
 	IB_QP_CREATE_SOURCE_QPN			= 1 << 10,
+	IB_QP_CREATE_PCI_WRITE_END_PADDING	= 1 << 11,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
@@ -1621,6 +1624,7 @@ enum ib_wq_flags {
 	IB_WQ_FLAGS_CVLAN_STRIPPING	= 1 << 0,
 	IB_WQ_FLAGS_SCATTER_FCS		= 1 << 1,
 	IB_WQ_FLAGS_DELAY_DROP		= 1 << 2,
+	IB_WQ_FLAGS_PCI_WRITE_END_PADDING = 1 << 3,
 };
 
 struct ib_wq_init_attr {

commit c0348eb069687a2f27c0cd23dafb35918edf9e75
Author: Parav Pandit <parav@mellanox.com>
Date:   Mon Oct 16 08:45:13 2017 +0300

    IB: Let ib_core resolve destination mac address
    
    Since IB/core resolves the destination mac address for user and kernel
    consumers, avoid resolving in multiple provider drivers.
    
    Only ib_core resolves DMAC now, therefore resolve_eth_dmac is removed as
    exported symbol.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 09c4a695155e..9810e4568635 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3622,8 +3622,6 @@ void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);
 void ib_drain_qp(struct ib_qp *qp);
 
-int ib_resolve_eth_dmac(struct ib_device *device,
-			struct rdma_ah_attr *ah_attr);
 int ib_get_eth_speed(struct ib_device *dev, u8 port_num, u8 *speed, u8 *width);
 
 static inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)

commit 5cda6587feec790a089703dde2e6e1f82de50bbd
Author: Parav Pandit <parav@mellanox.com>
Date:   Mon Oct 16 08:45:12 2017 +0300

    IB/core: Introduce and use rdma_create_user_ah
    
    Introduce rdma_create_user_ah API which allows passing udata to
    provider driver and additionally which resolves DMAC for RoCE.
    
    ib_resolve_eth_dmac() resolves destination mac address for unicast,
    multicast, link local ipv4 mapped ipv6 and ipv6 destination gid entry.
    This allows all RoCE provider drivers to avoid duplicating such code.
    
    Such change brings consistency where IB core always resolves dmac and pass
    it to RoCE provider drivers for user and kernel consumers, with this
    ah_attr->roce.dmac is always an input field for provider drivers.
    
    This uniformity avoids exporting ib_resolve_eth_dmac symbol to providers
    or other modules. Therefore its removed as exported symbol at later in
    the patch series.
    
    Now uverbs and umad both makes use of rdma_create_user_ah API which
    fixes the issue where umad has invalid DMAC for address.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e8608b2dc844..09c4a695155e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2857,6 +2857,21 @@ void ib_dealloc_pd(struct ib_pd *pd);
  */
 struct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr);
 
+/**
+ * rdma_create_user_ah - Creates an address handle for the given address vector.
+ * It resolves destination mac address for ah attribute of RoCE type.
+ * @pd: The protection domain associated with the address handle.
+ * @ah_attr: The attributes of the address vector.
+ * @udata: pointer to user's input output buffer information need by
+ *         provider driver.
+ *
+ * It returns 0 on success and returns appropriate error code on error.
+ * The address handle is used to reference a local or global destination
+ * in all UD QP post sends.
+ */
+struct ib_ah *rdma_create_user_ah(struct ib_pd *pd,
+				  struct rdma_ah_attr *ah_attr,
+				  struct ib_udata *udata);
 /**
  * ib_get_gids_from_rdma_hdr - Get sgid and dgid from GRH or IPv4 header
  *   work completion.

commit edd31551148c09608feee6b8756ad148d550ee3b
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Sep 24 21:46:31 2017 +0300

    IB: Correct MR length field to be 64-bit
    
    The ib_mr->length represents the length of the MR in bytes as per
    the IBTA spec 1.3 section 11.2.10.3 (REGISTER PHYSICAL MEMORY REGION).
    
    Currently ib_mr->length field is defined as only 32-bits field.
    This might result into truncation and failed WRs of consumers who
    registers more than 4GB bytes memory regions and whose WRs accessing
    such MRs.
    
    This patch makes the length 64-bit to avoid such truncation.
    
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: Faisal Latif <faisal.latif@intel.com>
    Fixes: 4c67e2bfc8b7 ("IB/core: Introduce new fast registration API")
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bbb5f54db882..e8608b2dc844 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1739,7 +1739,7 @@ struct ib_mr {
 	u32		   lkey;
 	u32		   rkey;
 	u64		   iova;
-	u32		   length;
+	u64		   length;
 	unsigned int	   page_size;
 	bool		   need_inval;
 	union {

commit 78b1beb0998437107ed144b341fbe1252188916b
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Sep 24 21:46:29 2017 +0300

    IB/core: Fix typo in the name of the tag-matching cap struct
    
    The tag matching functionality is implemented by mlx5 driver
    by extending XRQ, however this internal kernel information was
    exposed to user space applications with *xrq* name instead of *tm*.
    
    This patch renames *xrq* to *tm* to handle that.
    
    Fixes: 8d50505ada72 ("IB/uverbs: Expose XRQ capabilities")
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bdb1279a415b..bbb5f54db882 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -285,7 +285,7 @@ enum ib_tm_cap_flags {
 	IB_TM_CAP_RC		    = 1 << 0,
 };
 
-struct ib_xrq_caps {
+struct ib_tm_caps {
 	/* Max size of RNDV header */
 	u32 max_rndv_hdr_size;
 	/* Max number of entries in tag matching list */
@@ -358,7 +358,7 @@ struct ib_device_attr {
 	struct ib_rss_caps	rss_caps;
 	u32			max_wq_type_rq;
 	u32			raw_packet_caps; /* Use ib_raw_packet_caps enum */
-	struct ib_xrq_caps	xrq_caps;
+	struct ib_tm_caps	tm_caps;
 };
 
 enum ib_mtu {

commit f808c13fd3738948e10196496959871130612b61
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:15:08 2017 -0700

    lib/interval_tree: fast overlap detection
    
    Allow interval trees to quickly check for overlaps to avoid unnecesary
    tree lookups in interval_tree_iter_first().
    
    As of this patch, all interval tree flavors will require using a
    'rb_root_cached' such that we can have the leftmost node easily
    available.  While most users will make use of this feature, those with
    special functions (in addition to the generic insert, delete, search
    calls) will avoid using the cached option as they can do funky things
    with insertions -- for example, vma_interval_tree_insert_after().
    
    [jglisse@redhat.com: fix deadlock from typo vm_lock_anon_vma()]
      Link: http://lkml.kernel.org/r/20170808225719.20723-1-jglisse@redhat.com
    Link: http://lkml.kernel.org/r/20170719014603.19029-12-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Jrme Glisse <jglisse@redhat.com>
    Acked-by: Christian Knig <christian.koenig@amd.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Doug Ledford <dledford@redhat.com>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Christian Benvenuti <benve@cisco.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e6df68048517..bdb1279a415b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1457,7 +1457,7 @@ struct ib_ucontext {
 
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	struct rb_root      umem_tree;
+	struct rb_root_cached   umem_tree;
 	/*
 	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
 	 * mmu notifiers registration.

commit fac9658cabb98afb68ef1630c558864e6f559c07
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Aug 3 16:06:57 2017 +0300

    IB/core: Add new ioctl interface
    
    In this ioctl interface, processing the command starts from
    properties of the command and fetching the appropriate user objects
    before calling the handler.
    
    Parsing and validation is done according to a specifier declared by
    the driver's code. In the driver, all supported objects are declared.
    These objects are separated to different object namepsaces. Dividing
    objects to namespaces is done at initialization by using the higher
    bits of the object ids. This initialization can mix objects declared
    in different places to one parsing tree using in this ioctl interface.
    
    For each object we list all supported methods. Similarly to objects,
    methods are separated to method namespaces too. Namespacing is done
    similarly to the objects case. This could be used in order to add
    methods to an existing object.
    
    Each method has a specific handler, which could be either a default
    handler or a driver specific handler.
    Along with the handler, a bunch of attributes are specified as well.
    Similarly to objects and method, attributes are namespaced and hashed
    by their ids at initialization too. All supported attributes are
    subject to automatic fetching and validation. These attributes include
    the command, response and the method's related objects' ids.
    
    When these entities (objects, methods and attributes) are used, the
    high bits of the entities ids are used in order to calculate the hash
    bucket index. Then, these high bits are masked out in order to have a
    zero based index. Since we use these high bits for both bucketing and
    namespacing, we get a compact representation and O(1) array access.
    This is mandatory for efficient dispatching.
    
    Each attribute has a type (PTR_IN, PTR_OUT, IDR and FD) and a length.
    Attributes could be validated through some attributes, like:
    (*) Minimum size / Exact size
    (*) Fops for FD
    (*) Object type for IDR
    
    If an IDR/fd attribute is specified, the kernel also states the object
    type and the required access (NEW, WRITE, READ or DESTROY).
    All uobject/fd management is done automatically by the infrastructure,
    meaning - the infrastructure will fail concurrent commands that at
    least one of them requires concurrent access (WRITE/DESTROY),
    synchronize actions with device removals (dissociate context events)
    and take care of reference counting (increase/decrease) for concurrent
    actions invocation. The reference counts on the actual kernel objects
    shall be handled by the handlers.
    
     objects
    +--------+
    |        |
    |        |   methods                                                                +--------+
    |        |   ns         method      method_spec                           +-----+   |len     |
    +--------+  +------+[d]+-------+   +----------------+[d]+------------+    |attr1+-> |type    |
    | object +> |method+-> | spec  +-> +  attr_buckets  +-> |default_chain+--> +-----+   |idr_type|
    +--------+  +------+   |handler|   |                |   +------------+    |attr2|   |access  |
    |        |  |      |   +-------+   +----------------+   |driver chain|    +-----+   +--------+
    |        |  |      |                                    +------------+
    |        |  +------+
    |        |
    |        |
    |        |
    |        |
    |        |
    |        |
    |        |
    |        |
    |        |
    |        |
    +--------+
    
    [d] = Hash ids to groups using the high order bits
    
    The right types table is also chosen by using the high bits from
    the ids. Currently we have either default or driver specific groups.
    
    Once validation and object fetching (or creation) completed, we call
    the handler:
    int (*handler)(struct ib_device *ib_dev, struct ib_uverbs_file *ufile,
                   struct uverbs_attr_bundle *ctx);
    
    ctx bundles attributes of different namespaces. Each element there
    is an array of attributes which corresponds to one namespaces of
    attributes. For example, in the usually used case:
    
     ctx                               core
    +----------------------------+     +------------+
    | core:                      +---> | valid      |
    +----------------------------+     | cmd_attr   |
    | driver:                    |     +------------+
    |----------------------------+--+  | valid      |
                                    |  | cmd_attr   |
                                    |  +------------+
                                    |  | valid      |
                                    |  | obj_attr   |
                                    |  +------------+
                                    |
                                    |  drivers
                                    |  +------------+
                                    +> | valid      |
                                       | cmd_attr   |
                                       +------------+
                                       | valid      |
                                       | cmd_attr   |
                                       +------------+
                                       | valid      |
                                       | obj_attr   |
                                       +------------+
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1b4bb8743969..e6df68048517 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2348,6 +2348,8 @@ struct ib_device {
 	void (*get_dev_fw_str)(struct ib_device *, char *str);
 	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
 						     int comp_vector);
+
+	struct uverbs_root_spec		*specs_root;
 };
 
 struct ib_client {

commit 9c2c849625cf779e0fac41c8be3c163df4b80c14
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Thu Aug 17 15:52:05 2017 +0300

    IB/core: Add new SRQ type IB_SRQT_TM
    
    This patch adds new SRQ type - IB_SRQT_TM. The new SRQ type supports tag
    matching and rendezvous offloads for MPI applications.
    
    When SRQ receives a message it will search through the matching list
    for the corresponding posted receive buffer. The process of searching
    the matching list is called tag matching.
    In case the tag matching results in a match, the received message will
    be placed in the address specified by the receive buffer. In case no
    match was found the message will be placed in a generic buffer until the
    corresponding receive buffer will be posted. These messages are called
    unexpected and their set is called an unexpected list.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Yossi Itigin <yosefe@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f0e46757185b..1b4bb8743969 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -991,12 +991,14 @@ enum ib_cq_notify_flags {
 
 enum ib_srq_type {
 	IB_SRQT_BASIC,
-	IB_SRQT_XRC
+	IB_SRQT_XRC,
+	IB_SRQT_TM,
 };
 
 static inline bool ib_srq_has_cq(enum ib_srq_type srq_type)
 {
-	return srq_type == IB_SRQT_XRC;
+	return srq_type == IB_SRQT_XRC ||
+	       srq_type == IB_SRQT_TM;
 }
 
 enum ib_srq_attr_mask {
@@ -1022,6 +1024,10 @@ struct ib_srq_init_attr {
 			struct {
 				struct ib_xrcd *xrcd;
 			} xrc;
+
+			struct {
+				u32		max_num_tags;
+			} tag_matching;
 		};
 	} ext;
 };

commit 1a56ff6daab1e062aadec582eb10e7090f0b370a
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Thu Aug 17 15:52:04 2017 +0300

    IB/core: Separate CQ handle in SRQ context
    
    Before this change CQ attached to SRQ was part of XRC specific extension.
    Moving CQ handle out makes it available to other types extending SRQ
    functionality.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Yossi Itigin <yosefe@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cab0bdcfad51..f0e46757185b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -994,6 +994,11 @@ enum ib_srq_type {
 	IB_SRQT_XRC
 };
 
+static inline bool ib_srq_has_cq(enum ib_srq_type srq_type)
+{
+	return srq_type == IB_SRQT_XRC;
+}
+
 enum ib_srq_attr_mask {
 	IB_SRQ_MAX_WR	= 1 << 0,
 	IB_SRQ_LIMIT	= 1 << 1,
@@ -1011,11 +1016,13 @@ struct ib_srq_init_attr {
 	struct ib_srq_attr	attr;
 	enum ib_srq_type	srq_type;
 
-	union {
-		struct {
-			struct ib_xrcd *xrcd;
-			struct ib_cq   *cq;
-		} xrc;
+	struct {
+		struct ib_cq   *cq;
+		union {
+			struct {
+				struct ib_xrcd *xrcd;
+			} xrc;
+		};
 	} ext;
 };
 
@@ -1554,12 +1561,14 @@ struct ib_srq {
 	enum ib_srq_type	srq_type;
 	atomic_t		usecnt;
 
-	union {
-		struct {
-			struct ib_xrcd *xrcd;
-			struct ib_cq   *cq;
-			u32		srq_num;
-		} xrc;
+	struct {
+		struct ib_cq   *cq;
+		union {
+			struct {
+				struct ib_xrcd *xrcd;
+				u32		srq_num;
+			} xrc;
+		};
 	} ext;
 };
 

commit 6938fc1ee07e54c057430005f8dcaccabce027c3
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Thu Aug 17 15:52:03 2017 +0300

    IB/core: Add XRQ capabilities
    
    This patch adds following TM XRQ capabilities:
    
    * max_rndv_hdr_size - Max size of rendezvous request message
    * max_num_tags - Max number of entries in tag matching list
    * max_ops - Max number of outstanding list operations
    * max_sge - Max number of SGE in tag matching entry
    * flags - the following flags are currently defined:
        - IB_TM_CAP_RC - Support tag matching on RC transport
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Yossi Itigin <yosefe@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 355c7a328e0b..cab0bdcfad51 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -280,6 +280,24 @@ struct ib_rss_caps {
 	u32 max_rwq_indirection_table_size;
 };
 
+enum ib_tm_cap_flags {
+	/*  Support tag matching on RC transport */
+	IB_TM_CAP_RC		    = 1 << 0,
+};
+
+struct ib_xrq_caps {
+	/* Max size of RNDV header */
+	u32 max_rndv_hdr_size;
+	/* Max number of entries in tag matching list */
+	u32 max_num_tags;
+	/* From enum ib_tm_cap_flags */
+	u32 flags;
+	/* Max number of outstanding list operations */
+	u32 max_ops;
+	/* Max number of SGE in tag matching entry */
+	u32 max_sge;
+};
+
 enum ib_cq_creation_flags {
 	IB_CQ_FLAGS_TIMESTAMP_COMPLETION   = 1 << 0,
 	IB_CQ_FLAGS_IGNORE_OVERRUN	   = 1 << 1,
@@ -340,6 +358,7 @@ struct ib_device_attr {
 	struct ib_rss_caps	rss_caps;
 	u32			max_wq_type_rq;
 	u32			raw_packet_caps; /* Use ib_raw_packet_caps enum */
+	struct ib_xrq_caps	xrq_caps;
 };
 
 enum ib_mtu {

commit 78b57f9529225111b440e6e5150f52f5d44e3c60
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Thu Aug 17 15:50:37 2017 +0300

    RDMA/core: Cleanup device capability enum
    
    Cleanup patch prior exporting the ib_device_cap_flags
    to the user space. In this patch, we are aligning the
    indentation, removing IB_DEVICE_INIT_TYPE and IB_DEVICE_RESERVED
    fields, because it is not used in the kernel.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e536a052e5dd..355c7a328e0b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -170,7 +170,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_UD_AV_PORT_ENFORCE		= (1 << 6),
 	IB_DEVICE_CURR_QP_STATE_MOD		= (1 << 7),
 	IB_DEVICE_SHUTDOWN_PORT			= (1 << 8),
-	IB_DEVICE_INIT_TYPE			= (1 << 9),
+	/* Not in use, former INIT_TYPE		= (1 << 9),*/
 	IB_DEVICE_PORT_ACTIVE_EVENT		= (1 << 10),
 	IB_DEVICE_SYS_IMAGE_GUID		= (1 << 11),
 	IB_DEVICE_RC_RNR_NAK_GEN		= (1 << 12),
@@ -185,7 +185,7 @@ enum ib_device_cap_flags {
 	 * which will always contain a usable lkey.
 	 */
 	IB_DEVICE_LOCAL_DMA_LKEY		= (1 << 15),
-	IB_DEVICE_RESERVED /* old SEND_W_INV */	= (1 << 16),
+	/* Reserved, old SEND_W_INV		= (1 << 16),*/
 	IB_DEVICE_MEM_WINDOW			= (1 << 17),
 	/*
 	 * Devices should set IB_DEVICE_UD_IP_SUM if they support
@@ -220,7 +220,7 @@ enum ib_device_cap_flags {
 	 * of I/O operations with single completion queue managed
 	 * by hardware.
 	 */
-	IB_DEVICE_CROSS_CHANNEL		= (1 << 27),
+	IB_DEVICE_CROSS_CHANNEL			= (1 << 27),
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1ULL << 31),

commit dcc9881e6767559c04faf15804ac145a2ea026cb
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Thu Aug 17 15:50:36 2017 +0300

    RDMA/(core, ulp): Convert register/unregister event handler to be void
    
    The functions ib_register_event_handler() and
    ib_unregister_event_handler() always returned success and they can't fail.
    
    Let's convert those functions to be void, remove redundant checks and
    cleanup tons of goto statements.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c155c105589d..e536a052e5dd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2413,8 +2413,8 @@ int ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
 		       enum ib_qp_type type, enum ib_qp_attr_mask mask,
 		       enum rdma_link_layer ll);
 
-int ib_register_event_handler  (struct ib_event_handler *event_handler);
-int ib_unregister_event_handler(struct ib_event_handler *event_handler);
+void ib_register_event_handler(struct ib_event_handler *event_handler);
+void ib_unregister_event_handler(struct ib_event_handler *event_handler);
 void ib_dispatch_event(struct ib_event *event);
 
 int ib_query_port(struct ib_device *device,

commit 732912c7386120179bf8f666febc232922e3ee17
Merge: e3bf14bdc17a ec2558796d25
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Aug 24 15:58:26 2017 -0400

    Merge branch 'k.o/for-4.13-rc' into k.o/for-next
    
    Pick up -rc fixes.
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 498ca3c82a7b11e152a46c253f6b2087c929ce00
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Aug 23 08:35:40 2017 +0300

    IB/core: Avoid accessing non-allocated memory when inferring port type
    
    Commit 44c58487d51a ("IB/core: Define 'ib' and 'roce' rdma_ah_attr types")
    introduced the concept of type in ah_attr:
     * During ib_register_device, each port is checked for its type which
       is stored in ib_device's port_immutable array.
     * During uverbs' modify_qp, the type is inferred using the port number
       in ib_uverbs_qp_dest struct (address vector) by accessing the
       relevant port_immutable array and the type is passed on to
       providers.
    
    IB spec (version 1.3) enforces a valid port value only in Reset to
    Init. During Init to RTR, the address vector must be valid but port
    number is not mentioned as a field in the address vector, so its
    value is not validated, which leads to accesses to a non-allocated
    memory when inferring the port type.
    
    Save the real port number in ib_qp during modify to Init (when the
    comp_mask indicates that the port number is valid) and use this value
    to infer the port type.
    
    Avoid copying the address vector fields if the matching bit is not set
    in the attr_mask. Address vector can't be modified before the port, so
    no valid flow is affected.
    
    Fixes: 44c58487d51a ('IB/core: Define 'ib' and 'roce' rdma_ah_attr types')
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b5732432bb29..88c32aba32f7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1683,6 +1683,7 @@ struct ib_qp {
 	enum ib_qp_type		qp_type;
 	struct ib_rwq_ind_table *rwq_ind_tbl;
 	struct ib_qp_security  *qp_sec;
+	u8			port;
 };
 
 struct ib_mr {

commit d98bb7f7e6fa29d45008370084d5cabac7ac69ed
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Fri Aug 4 13:54:16 2017 -0700

    IB/hfi1: Determine 9B/16B L2 header type based on Address handle
    
    When address handle attributes are initialized, the LIDs are
    transformed to be in the 32 bit LID space.
    When constructing the header, hfi1 driver will look at the LID
    to determine the packet header to be created.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 70a183179224..8f263930c56f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -864,6 +864,7 @@ struct roce_ah_attr {
 struct opa_ah_attr {
 	u32			dlid;
 	u8			src_path_bits;
+	bool			make_grd;
 };
 
 struct rdma_ah_attr {
@@ -3625,6 +3626,20 @@ static inline u8 rdma_ah_get_path_bits(const struct rdma_ah_attr *attr)
 	return 0;
 }
 
+static inline void rdma_ah_set_make_grd(struct rdma_ah_attr *attr,
+					bool make_grd)
+{
+	if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		attr->opa.make_grd = make_grd;
+}
+
+static inline bool rdma_ah_get_make_grd(const struct rdma_ah_attr *attr)
+{
+	if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		return attr->opa.make_grd;
+	return false;
+}
+
 static inline void rdma_ah_set_port_num(struct rdma_ah_attr *attr, u8 port_num)
 {
 	attr->port_num = port_num;

commit 62ede7779904bc75bdd84f1ff0016113956ce3b4
Author: Hiatt, Don <don.hiatt@intel.com>
Date:   Mon Aug 14 14:17:43 2017 -0400

    Add OPA extended LID support
    
    This patch series primarily increases sizes of variables that hold
    lid values from 16 to 32 bits. Additionally, it adds a check in
    the IB mad stack to verify a properly formatted MAD when OPA
    extended LIDs are used.
    
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4db4ad56ace6..70a183179224 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3724,16 +3724,30 @@ static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
 		return RDMA_AH_ATTR_TYPE_IB;
 }
 
-/* Return slid in 16bit CPU encoding */
-static inline u16 ib_slid_cpu16(u32 slid)
+/**
+ * ib_lid_cpu16 - Return lid in 16bit CPU encoding.
+ *     In the current implementation the only way to get
+ *     get the 32bit lid is from other sources for OPA.
+ *     For IB, lids will always be 16bits so cast the
+ *     value accordingly.
+ *
+ * @lid: A 32bit LID
+ */
+static inline u16 ib_lid_cpu16(u32 lid)
 {
-	return (u16)slid;
+	WARN_ON_ONCE(lid & 0xFFFF0000);
+	return (u16)lid;
 }
 
-/* Return slid in 16bit BE encoding */
-static inline u16 ib_slid_be16(u32 slid)
+/**
+ * ib_lid_be16 - Return lid in 16bit BE encoding.
+ *
+ * @lid: A 32bit LID
+ */
+static inline __be16 ib_lid_be16(u32 lid)
 {
-	return cpu_to_be16((u16)slid);
+	WARN_ON_ONCE(lid & 0xFFFF0000);
+	return cpu_to_be16((u16)lid);
 }
 
 /**

commit d0d62c34fb746eaf68df5b3d6f4877c7d1e6320c
Merge: 320438301b85 db14dff1743e
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Aug 10 14:34:18 2017 -0400

    Merge branch 'rdma-netlink' into k.o/merge-test
    
    Conflicts:
            include/rdma/ib_verbs.h - Modified a function signature adjacent
            to a newly added function signature from a previous merge
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 320438301b85038e995b5a40a24c43cbc0ed4909
Merge: 913cc67159bc ac3a949fb2ff 0b36658ca8b1
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Aug 10 14:31:29 2017 -0400

    Merge branches '32bit_lid' and 'irq_affinity' into k.o/merge-test
    
    Conflicts:
            drivers/infiniband/hw/mlx5/main.c - Both add new code
            include/rdma/ib_verbs.h - Both add new code
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 9abb0d1bbd9529c574eacd8586e2bf68d17966cd
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Jun 27 16:49:53 2017 +0300

    RDMA: Simplify get firmware interface
    
    There is a need to forward FW version to user space
    application through RDMA netlink. In order to make it safe, there
    is need to declare nla_policy and limit the size of FW string.
    
    The new define IB_FW_VERSION_NAME_MAX will limit the size of
    FW version string. That define was chosen to be equal to
    ETHTOOL_FWVERS_LEN, because many drivers anyway are limited
    by that value indirectly.
    
    The introduction of this define allows us to remove the string size
    from get_fw_str function signature.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3391df5fdc9c..e0e87a1f66fb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -64,6 +64,8 @@
 #include <linux/cgroup_rdma.h>
 #include <uapi/rdma/ib_user_verbs.h>
 
+#define IB_FW_VERSION_NAME_MAX	ETHTOOL_FWVERS_LEN
+
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
 
@@ -2307,7 +2309,7 @@ struct ib_device {
 	 * in fast paths.
 	 */
 	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
-	void (*get_dev_fw_str)(struct ib_device *, char *str, size_t str_len);
+	void (*get_dev_fw_str)(struct ib_device *, char *str);
 };
 
 struct ib_client {
@@ -2343,7 +2345,7 @@ struct ib_client {
 struct ib_device *ib_alloc_device(size_t size);
 void ib_dealloc_device(struct ib_device *device);
 
-void ib_get_device_fw_str(struct ib_device *device, char *str, size_t str_len);
+void ib_get_device_fw_str(struct ib_device *device, char *str);
 
 int ib_register_device(struct ib_device *device,
 		       int (*port_callback)(struct ib_device *,

commit ecc82c53f9a4ce08ba7df626a4262c86841ced8f
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jun 18 14:39:59 2017 +0300

    RDMA/core: Add and expose static device index
    
    This patch adds static device index in similar fashion to
    already available in netdev world (struct net->ifindex).
    
    In downstream patches, the RDMA nelink will use this idx-to-ib_device
    conversion, so as part of this commit, we are exposing the translation
    function to be visible for IB/core users.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1082b4c81b2c..3391df5fdc9c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2298,6 +2298,8 @@ struct ib_device {
 	struct rdmacg_device         cg_device;
 #endif
 
+	u32                          index;
+
 	/**
 	 * The following mandatory functions are used only at device
 	 * registration.  Keep functions such as these at the end of this

commit c66cd353bbe6869a059869a7a1518ec619afdc9d
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Thu Jul 13 11:09:41 2017 +0300

    RDMA/core: expose affinity mappings per completion vector
    
    This will allow ULPs to intelligently locate threads based
    on completion vector cpu affinity mappings. In case the
    driver does not expose a get_vector_affinity callout, return
    NULL so the caller can maintain a fallback logic.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hkon Bugge <haakon.bugge@oracle.com>
    Acked-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b5732432bb29..73ed2e4e802f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2296,6 +2296,8 @@ struct ib_device {
 	 */
 	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
 	void (*get_dev_fw_str)(struct ib_device *, char *str, size_t str_len);
+	const struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,
+						     int comp_vector);
 };
 
 struct ib_client {
@@ -3706,4 +3708,26 @@ static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
 	else
 		return RDMA_AH_ATTR_TYPE_IB;
 }
+
+/**
+ * ib_get_vector_affinity - Get the affinity mappings of a given completion
+ *   vector
+ * @device:         the rdma device
+ * @comp_vector:    index of completion vector
+ *
+ * Returns NULL on failure, otherwise a corresponding cpu map of the
+ * completion vector (returns all-cpus map if the device driver doesn't
+ * implement get_vector_affinity).
+ */
+static inline const struct cpumask *
+ib_get_vector_affinity(struct ib_device *device, int comp_vector)
+{
+	if (comp_vector < 0 || comp_vector >= device->num_comp_vectors ||
+	    !device->get_vector_affinity)
+		return NULL;
+
+	return device->get_vector_affinity(device, comp_vector);
+
+}
+
 #endif /* IB_VERBS_H */

commit 7db20ecd1d9700e2c240dee505162eb56ab55b5b
Author: Hiatt, Don <don.hiatt@intel.com>
Date:   Thu Jun 8 13:37:49 2017 -0400

    IB/core: Change wc.slid from 16 to 32 bits
    
    slid field in struct ib_wc is increased to 32 bits.
    This enables core components to use larger LIDs if needed.
    The user ABI is unchanged and return 16 bit values when queried.
    
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 620535908118..7eaf7d2ab424 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -948,7 +948,7 @@ struct ib_wc {
 	u32			src_qp;
 	int			wc_flags;
 	u16			pkey_index;
-	u16			slid;
+	u32			slid;
 	u8			sl;
 	u8			dlid_path_bits;
 	u8			port_num;	/* valid only for DR SMPs on switches */
@@ -3706,4 +3706,16 @@ static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
 	else
 		return RDMA_AH_ATTR_TYPE_IB;
 }
+
+/* Return slid in 16bit CPU encoding */
+static inline u16 ib_slid_cpu16(u32 slid)
+{
+	return (u16)slid;
+}
+
+/* Return slid in 16bit BE encoding */
+static inline u16 ib_slid_be16(u32 slid)
+{
+	return cpu_to_be16((u16)slid);
+}
 #endif /* IB_VERBS_H */

commit db58540b021a17e0ede64f761b740556d77f1679
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Thu Jun 8 13:37:48 2017 -0400

    IB/core: Change port_attr.sm_lid from 16 to 32 bits
    
    sm_lid field in struct ib_port_attr is increased to 32 bits. This
    enables core components to use larger LIDs if needed.
    The user ABI is unchanged and return 16 bit values when queried.
    
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4fa94e69b1fc..620535908118 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -549,7 +549,7 @@ struct ib_port_attr {
 	u32			bad_pkey_cntr;
 	u32			qkey_viol_cntr;
 	u16			pkey_tbl_len;
-	u16			sm_lid;
+	u32			sm_lid;
 	u32			lid;
 	u8			lmc;
 	u8			max_vl_num;

commit 582faf3150f57b8364ac9d2aa731d7368ada7a4b
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Thu Jun 8 13:37:47 2017 -0400

    IB/core: Change port_attr.lid size from 16 to 32 bits
    
    lid field in struct ib_port_attr is increased to 32 bits. This enables core
    components to use larger LIDs if needed.
    The user ABI is unchanged and return 16 bit values when queried.
    
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b5732432bb29..4fa94e69b1fc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -549,8 +549,8 @@ struct ib_port_attr {
 	u32			bad_pkey_cntr;
 	u32			qkey_viol_cntr;
 	u16			pkey_tbl_len;
-	u16			lid;
 	u16			sm_lid;
+	u32			lid;
 	u8			lmc;
 	u8			max_vl_num;
 	u8			sm_sl;

commit a5f66725c7748a6831005a091b4aa9d18abb3b03
Merge: f55c1e6608be 67cbe3532c2c
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Jul 27 09:00:38 2017 -0400

    Merge branch 'misc' into k.o/for-next

commit f55c1e6608bed7f583f5be4a74bbca16b288a7e5
Merge: 03da084ed880 b4fbec9673db 6afff1c7152f
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Jul 26 20:13:33 2017 -0400

    Merge branches 'rxe' and 'mlx' into k.o/for-next

commit 02984cc7b3d62418bd72abacaf875c3a9eccdb66
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Jun 8 16:15:06 2017 +0300

    IB/core: Enable QP creation with a given source QP number
    
    Enable QP creation with a given source QP number.
    The created QP will use the source QPN as its wire QP number.
    
    This comes as a pre-patch for downstream patches in this series to
    allow user space applications to accelerate traffic which is typically
    handled by IPoIB ULP.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4a1444456af7..76a74c783c50 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1059,6 +1059,7 @@ enum ib_qp_create_flags {
 	/* FREE					= 1 << 7, */
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
+	IB_QP_CREATE_SOURCE_QPN			= 1 << 10,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
@@ -1086,6 +1087,7 @@ struct ib_qp_init_attr {
 	 */
 	u8			port_num;
 	struct ib_rwq_ind_table *rwq_ind_tbl;
+	u32			source_qpn;
 };
 
 struct ib_qp_open_attr {

commit 7d9336d80b0b35d3537d37ff35e08dcb425073ed
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue May 30 10:29:10 2017 +0300

    IB/core: Introduce delay drop for a WQ
    
    Work queue which is created with IB_WQ_FLAGS_DELAY_DROP won't
    cause packet drops when there aren't receive WQEs, but will wait until
    posting of receive WQEs or for some period of time that the device
    was configured with.
    
    It includes:
     * Add a new creation flag to enable delay drop functionality in a WQ.
     * A new capability was introduced - IB_RAW_PACKET_CAP_DELAY_DROP, which
       is the device's ability to delay packet drops when there aren't receive
       WQEs.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b5732432bb29..4a1444456af7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1546,6 +1546,10 @@ enum ib_raw_packet_caps {
 	IB_RAW_PACKET_CAP_SCATTER_FCS		= (1 << 1),
 	/* Checksum offloads are supported (for both send and receive). */
 	IB_RAW_PACKET_CAP_IP_CSUM		= (1 << 2),
+	/* When a packet is received for an RQ with no receive WQEs, the
+	 * packet processing is delayed.
+	 */
+	IB_RAW_PACKET_CAP_DELAY_DROP		= (1 << 3),
 };
 
 enum ib_wq_type {
@@ -1574,6 +1578,7 @@ struct ib_wq {
 enum ib_wq_flags {
 	IB_WQ_FLAGS_CVLAN_STRIPPING	= 1 << 0,
 	IB_WQ_FLAGS_SCATTER_FCS		= 1 << 1,
+	IB_WQ_FLAGS_DELAY_DROP		= 1 << 2,
 };
 
 struct ib_wq_init_attr {

commit d41861942fc55c14b6280d9568a0d0112037f065
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Wed Jun 14 23:13:34 2017 +0300

    IB/core: Add generic function to extract IB speed from netdev
    
    Logic of retrieving netdev speed from net_device and translating it to
    IB speed is implemented in rxe, in usnic and in bnxt drivers.
    
    Define new function which merges all.
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Christian Benvenuti <benve@cisco.com>
    Reviewed-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b5732432bb29..68d947dac9a2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3555,6 +3555,7 @@ void ib_drain_qp(struct ib_qp *qp);
 
 int ib_resolve_eth_dmac(struct ib_device *device,
 			struct rdma_ah_attr *ah_attr);
+int ib_get_eth_speed(struct ib_device *dev, u8 port_num, u8 *speed, u8 *width);
 
 static inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)
 {

commit 03da084ed8804ddc5918883be84245b53393b8fb
Merge: 520eccdfe187 bc5214ee2922
Author: Doug Ledford <dledford@redhat.com>
Date:   Mon Jul 24 08:33:43 2017 -0400

    Merge branch 'hfi1' into k.o/for-4.14

commit 7855f5842741e5835b9be073079780c444dca898
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 23 14:38:16 2017 +0300

    IB/core: Remove NOIO QP create flag
    
    There are no users for IB_QP_CREATE_USE_GFP_NOIO flag,
    so let's remove it.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6e62c9e2c971..b5732432bb29 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1056,7 +1056,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_MANAGED_RECV               = 1 << 4,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
-	IB_QP_CREATE_USE_GFP_NOIO		= 1 << 7,
+	/* FREE					= 1 << 7, */
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
 	/* reserve bits 26-31 for low level drivers' internal use */

commit a512c2fbef9c700ee1ee0e045b75e140fef8f5ee
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue May 23 11:26:08 2017 +0300

    IB/core: Introduce modify QP operation with udata
    
    This patch adds new function ib_modify_qp_with_udata so that
    uverbs layer can avoid handling L2 mac address at verbs layer
    and depend on the core layer to resolve the mac address consistently
    for all required QPs.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 356953d3dbd1..6e62c9e2c971 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2947,6 +2947,22 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
 struct ib_qp *ib_create_qp(struct ib_pd *pd,
 			   struct ib_qp_init_attr *qp_init_attr);
 
+/**
+ * ib_modify_qp_with_udata - Modifies the attributes for the specified QP.
+ * @qp: The QP to modify.
+ * @attr: On input, specifies the QP attributes to modify.  On output,
+ *   the current values of selected QP attributes are returned.
+ * @attr_mask: A bit-mask used to specify which attributes of the QP
+ *   are being modified.
+ * @udata: pointer to user's input output buffer information
+ *   are being modified.
+ * It returns 0 on success and returns appropriate error code on error.
+ */
+int ib_modify_qp_with_udata(struct ib_qp *qp,
+			    struct ib_qp_attr *attr,
+			    int attr_mask,
+			    struct ib_udata *udata);
+
 /**
  * ib_modify_qp - Modifies the attributes for the specified QP and then
  *   transitions the QP to the given state.

commit 9871ab22f2784b2823b01522772a72ee4fc9d1fa
Merge: ac7b75966c9c 8e959601996d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 6 11:45:08 2017 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma update from Doug Ledford:
     "This includes two bugs against the newly added opa vnic that were
      found by turning on the debug kernel options:
    
       - sleeping while holding a lock, so a one line fix where they
         switched it from GFP_KERNEL allocation to a GFP_ATOMIC allocation
    
       - a case where they had an isolated caller of their code that could
         call them in an atomic context so they had to switch their use of a
         mutex to a spinlock to be safe, so this was considerably more lines
         of diff because all uses of that lock had to be switched
    
      In addition, the bug that was discussed with you already about an out
      of bounds array access in ib_uverbs_modify_qp and ib_uverbs_create_ah
      and is only seven lines of diff.
    
      And finally, one fix to an earlier fix in the -rc cycle that broke
      hfi1 and qib in regards to IPoIB (this one is, unfortunately, larger
      than I would like for a -rc7 submission, but fixing the problem
      required that we not treat all devices as though they had allocated a
      netdev universally because it isn't true, and it took 70 lines of diff
      to resolve the issue, but the final patch has been vetted by Intel and
      Mellanox and they've both given their approval to the fix).
    
      Summary:
    
       - Two fixes for OPA found by debug kernel
       - Fix for user supplied input causing kernel problems
       - Fix for the IPoIB fixes submitted around -rc4"
    
    [ Doug sent this having not noticed the 4.12 release, so I guess I'll be
      getting another rdma pull request with the actuakl merge window
      updates and not just fixes.
    
      Oh well - it would have been nice if this small update had been the
      merge window one.     - Linus ]
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma:
      IB/core, opa_vnic, hfi1, mlx5: Properly free rdma_netdev
      RDMA/uverbs: Check port number supplied by user verbs cmds
      IB/opa_vnic: Use spinlock instead of mutex for stats_lock
      IB/opa_vnic: Use GFP_ATOMIC while sending trap

commit 8e959601996dc645f4ed7004482a1667c27deb39
Author: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
Date:   Fri Jun 30 13:14:46 2017 -0700

    IB/core, opa_vnic, hfi1, mlx5: Properly free rdma_netdev
    
    IPOIB is calling free_rdma_netdev even though alloc_rdma_netdev has
    returned -EOPNOTSUPP.
    Move free_rdma_netdev from ib_device structure to rdma_netdev structure
    thus ensuring proper cleanup function is called for the rdma net device.
    
    Fix the following trace:
    
    ib0: Failed to modify QP to ERROR state
    BUG: unable to handle kernel paging request at 0000000000001d20
    IP: hfi1_vnic_free_rn+0x26/0xb0 [hfi1]
    Call Trace:
     ipoib_remove_one+0xbe/0x160 [ib_ipoib]
     ib_unregister_device+0xd0/0x170 [ib_core]
     rvt_unregister_device+0x29/0x90 [rdmavt]
     hfi1_unregister_ib_device+0x1a/0x100 [hfi1]
     remove_one+0x4b/0x220 [hfi1]
     pci_device_remove+0x39/0xc0
     device_release_driver_internal+0x141/0x200
     driver_detach+0x3f/0x80
     bus_remove_driver+0x55/0xd0
     driver_unregister+0x2c/0x50
     pci_unregister_driver+0x2a/0xa0
     hfi1_mod_cleanup+0x10/0xf65 [hfi1]
     SyS_delete_module+0x171/0x250
     do_syscall_64+0x67/0x150
     entry_SYSCALL64_slow_path+0x25/0x25
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ba8314ec5768..71313d5ca1c8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1927,6 +1927,9 @@ struct rdma_netdev {
 	struct ib_device  *hca;
 	u8                 port_num;
 
+	/* cleanup function must be specified */
+	void (*free_rdma_netdev)(struct net_device *netdev);
+
 	/* control functions */
 	void (*set_id)(struct net_device *netdev, int id);
 	/* send packet */
@@ -2194,7 +2197,7 @@ struct ib_device {
 							   struct ib_udata *udata);
 	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
 	/**
-	 * rdma netdev operations
+	 * rdma netdev operation
 	 *
 	 * Driver implementing alloc_rdma_netdev must return -EOPNOTSUPP if it
 	 * doesn't support the specified rdma netdev type.
@@ -2206,7 +2209,6 @@ struct ib_device {
 					const char *name,
 					unsigned char name_assign_type,
 					void (*setup)(struct net_device *));
-	void (*free_rdma_netdev)(struct net_device *netdev);
 
 	struct module               *owner;
 	struct device                dev;

commit cb49366f3616fdf197893c24a5b2677b8c26ce29
Author: Vishwanathapura, Niranjana <niranjana.vishwanathapura@intel.com>
Date:   Thu Jun 1 17:04:02 2017 -0700

    IB/core,rdmavt,hfi1,opa-vnic: Send OPA cap_mask3 in trap
    
    Provide the ability for IB clients to modify the OPA specific
    capability mask and include this mask in the subsequent trap data.
    
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Michael N. Henry <michael.n.henry@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8f1ce4e27bbd..9d4d2a74c95e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -577,7 +577,8 @@ struct ib_device_modify {
 enum ib_port_modify_flags {
 	IB_PORT_SHUTDOWN		= 1,
 	IB_PORT_INIT_TYPE		= (1<<2),
-	IB_PORT_RESET_QKEY_CNTR		= (1<<3)
+	IB_PORT_RESET_QKEY_CNTR		= (1<<3),
+	IB_PORT_OPA_MASK_CHG		= (1<<4)
 };
 
 struct ib_port_modify {

commit 7dafbab3753fcf59bc81748e5b2c5bf04e1c62c7
Author: Don Hiatt <don.hiatt@intel.com>
Date:   Fri May 12 09:19:55 2017 -0700

    IB/hfi1: Add functions to parse BTH/IB headers
    
    Improve code readablity by adding inline functions
    to read specific BTH/IB fields without knowledge of
    byte offsets.
    
    Reviewed-by: Brian Welty <brian.welty@intel.com>
    Reviewed-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ba8314ec5768..8f1ce4e27bbd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -664,6 +664,8 @@ union rdma_network_hdr {
 	};
 };
 
+#define IB_QPN_MASK		0xFFFFFF
+
 enum {
 	IB_MULTICAST_QPN = 0xffffff
 };

commit d291f1a6523292d916fe1659c67f6db061fbd1b5
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Fri May 19 15:48:52 2017 +0300

    IB/core: Enforce PKey security on QPs
    
    Add new LSM hooks to allocate and free security contexts and check for
    permission to access a PKey.
    
    Allocate and free a security context when creating and destroying a QP.
    This context is used for controlling access to PKeys.
    
    When a request is made to modify a QP that changes the port, PKey index,
    or alternate path, check that the QP has permission for the PKey in the
    PKey table index on the subnet prefix of the port. If the QP is shared
    make sure all handles to the QP also have access.
    
    Store which port and PKey index a QP is using. After the reset to init
    transition the user can modify the port, PKey index and alternate path
    independently. So port and PKey settings changes can be a merge of the
    previous settings and the new ones.
    
    In order to maintain access control if there are PKey table or subnet
    prefix change keep a list of all QPs are using each PKey index on
    each port. If a change occurs all QPs using that device and port must
    have access enforced for the new cache settings.
    
    These changes add a transaction to the QP modify process. Association
    with the old port and PKey index must be maintained if the modify fails,
    and must be removed if it succeeds. Association with the new port and
    PKey index must be established prior to the modify and removed if the
    modify fails.
    
    1. When a QP is modified to a particular Port, PKey index or alternate
       path insert that QP into the appropriate lists.
    
    2. Check permission to access the new settings.
    
    3. If step 2 grants access attempt to modify the QP.
    
    4a. If steps 2 and 3 succeed remove any prior associations.
    
    4b. If ether fails remove the new setting associations.
    
    If a PKey table or subnet prefix changes walk the list of QPs and
    check that they have permission. If not send the QP to the error state
    and raise a fatal error event. If it's a shared QP make sure all the
    QPs that share the real_qp have permission as well. If the QP that
    owns a security structure is denied access the security structure is
    marked as such and the QP is added to an error_list. Once the moving
    the QP to error is complete the security structure mark is cleared.
    
    Maintaining the lists correctly turns QP destroy into a transaction.
    The hardware driver for the device frees the ib_qp structure, so while
    the destroy is in progress the ib_qp pointer in the ib_qp_security
    struct is undefined. When the destroy process begins the ib_qp_security
    structure is marked as destroying. This prevents any action from being
    taken on the QP pointer. After the QP is destroyed successfully it
    could still listed on an error_list wait for it to be processed by that
    flow before cleaning up the structure.
    
    If the destroy fails the QPs port and PKey settings are reinserted into
    the appropriate lists, the destroying flag is cleared, and access control
    is enforced, in case there were any cache changes during the destroy
    flow.
    
    To keep the security changes isolated a new file is used to hold security
    related functionality.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Acked-by: Doug Ledford <dledford@redhat.com>
    [PM: merge fixup in ib_verbs.h and uverbs_cmd.c]
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9dc4e7e0aba4..0e480a5630d4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1614,6 +1614,45 @@ struct ib_rwq_ind_table_init_attr {
 	struct ib_wq	**ind_tbl;
 };
 
+enum port_pkey_state {
+	IB_PORT_PKEY_NOT_VALID = 0,
+	IB_PORT_PKEY_VALID = 1,
+	IB_PORT_PKEY_LISTED = 2,
+};
+
+struct ib_qp_security;
+
+struct ib_port_pkey {
+	enum port_pkey_state	state;
+	u16			pkey_index;
+	u8			port_num;
+	struct list_head	qp_list;
+	struct list_head	to_error_list;
+	struct ib_qp_security  *sec;
+};
+
+struct ib_ports_pkeys {
+	struct ib_port_pkey	main;
+	struct ib_port_pkey	alt;
+};
+
+struct ib_qp_security {
+	struct ib_qp	       *qp;
+	struct ib_device       *dev;
+	/* Hold this mutex when changing port and pkey settings. */
+	struct mutex		mutex;
+	struct ib_ports_pkeys  *ports_pkeys;
+	/* A list of all open shared QP handles.  Required to enforce security
+	 * properly for all users of a shared QP.
+	 */
+	struct list_head        shared_qp_list;
+	void                   *security;
+	bool			destroying;
+	atomic_t		error_list_count;
+	struct completion	error_complete;
+	int			error_comps_pending;
+};
+
 /*
  * @max_write_sge: Maximum SGE elements per RDMA WRITE request.
  * @max_read_sge:  Maximum SGE elements per RDMA READ request.
@@ -1643,6 +1682,7 @@ struct ib_qp {
 	u32			max_read_sge;
 	enum ib_qp_type		qp_type;
 	struct ib_rwq_ind_table *rwq_ind_tbl;
+	struct ib_qp_security  *qp_sec;
 };
 
 struct ib_mr {
@@ -1941,6 +1981,12 @@ struct rdma_netdev {
 			    union ib_gid *gid, u16 mlid);
 };
 
+struct ib_port_pkey_list {
+	/* Lock to hold while modifying the list. */
+	spinlock_t                    list_lock;
+	struct list_head              pkey_list;
+};
+
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -1964,6 +2010,8 @@ struct ib_device {
 
 	int			      num_comp_vectors;
 
+	struct ib_port_pkey_list     *port_pkey_list;
+
 	struct iw_cm_verbs	     *iwcm;
 
 	/**

commit 883c71feaf2e810e0331cf780c738cbb09e93b58
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Fri May 19 15:48:51 2017 +0300

    IB/core: IB cache enhancements to support Infiniband security
    
    Cache the subnet prefix and add a function to access it. Enforcing
    security requires frequent queries of the subnet prefix and the pkeys in
    the pkey table.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: James Morris <james.l.morris@oracle.com>
    Acked-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ba8314ec5768..9dc4e7e0aba4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1891,6 +1891,7 @@ enum ib_mad_result {
 };
 
 struct ib_port_cache {
+	u64		      subnet_prefix;
 	struct ib_pkey_cache  *pkey;
 	struct ib_gid_table   *gid;
 	u8                     lmc;

commit ea6819e1f2d6c30624ea067f4b3a50a3cca79d8a
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Mon Mar 27 14:20:14 2017 +0200

    smc_diag.h: fix include from userland
    
    This patch prepares the uapi export by fixing the following error:
    
    .../linux/smc_diag.h:6:27: fatal error: rdma/ib_verbs.h: No such file or directory
     #include <rdma/ib_verbs.h>
    
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f0cb4906478a..ba8314ec5768 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -62,6 +62,7 @@
 #include <linux/mmu_notifier.h>
 #include <linux/uaccess.h>
 #include <linux/cgroup_rdma.h>
+#include <uapi/rdma/ib_user_verbs.h>
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
@@ -1889,8 +1890,6 @@ enum ib_mad_result {
 	IB_MAD_RESULT_CONSUMED = 1 << 2  /* Packet consumed: stop processing */
 };
 
-#define IB_DEVICE_NAME_MAX 64
-
 struct ib_port_cache {
 	struct ib_pkey_cache  *pkey;
 	struct ib_gid_table   *gid;

commit 64b4646eaf3dab4cc7b3040d10d565a83654446f
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:30 2017 -0400

    IB/core: Define 'opa' rdma_ah_attr type
    
    OPA ah_attr types allows core components to specify
    attributes that may be specific to opa devices.
    For instance, opa type ah_attr provides 32 bit lids
    enabling larger OPA fabric sizes.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 803927b31742..f0cb4906478a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -843,6 +843,7 @@ __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 enum rdma_ah_attr_type {
 	RDMA_AH_ATTR_TYPE_IB,
 	RDMA_AH_ATTR_TYPE_ROCE,
+	RDMA_AH_ATTR_TYPE_OPA,
 };
 
 struct ib_ah_attr {
@@ -854,6 +855,11 @@ struct roce_ah_attr {
 	u8			dmac[ETH_ALEN];
 };
 
+struct opa_ah_attr {
+	u32			dlid;
+	u8			src_path_bits;
+};
+
 struct rdma_ah_attr {
 	struct ib_global_route	grh;
 	u8			sl;
@@ -864,6 +870,7 @@ struct rdma_ah_attr {
 	union {
 		struct ib_ah_attr ib;
 		struct roce_ah_attr roce;
+		struct opa_ah_attr opa;
 	};
 };
 
@@ -3490,16 +3497,20 @@ static inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)
 	return NULL;
 }
 
-static inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u16 dlid)
+static inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u32 dlid)
 {
 	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
-		attr->ib.dlid = dlid;
+		attr->ib.dlid = (u16)dlid;
+	else if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		attr->opa.dlid = dlid;
 }
 
-static inline u16 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)
+static inline u32 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)
 {
 	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
 		return attr->ib.dlid;
+	else if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		return attr->opa.dlid;
 	return 0;
 }
 
@@ -3518,12 +3529,16 @@ static inline void rdma_ah_set_path_bits(struct rdma_ah_attr *attr,
 {
 	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
 		attr->ib.src_path_bits = src_path_bits;
+	else if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		attr->opa.src_path_bits = src_path_bits;
 }
 
 static inline u8 rdma_ah_get_path_bits(const struct rdma_ah_attr *attr)
 {
 	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
 		return attr->ib.src_path_bits;
+	else if (attr->type == RDMA_AH_ATTR_TYPE_OPA)
+		return attr->opa.src_path_bits;
 	return 0;
 }
 
@@ -3619,6 +3634,9 @@ static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
 	if ((rdma_protocol_roce(dev, port_num)) ||
 	    (rdma_protocol_iwarp(dev, port_num)))
 		return RDMA_AH_ATTR_TYPE_ROCE;
+	else if ((rdma_protocol_ib(dev, port_num)) &&
+		 (rdma_cap_opa_ah(dev, port_num)))
+		return RDMA_AH_ATTR_TYPE_OPA;
 	else
 		return RDMA_AH_ATTR_TYPE_IB;
 }

commit 44c58487d51a0dc43d96f1dc864f0461ec6a346a
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:29 2017 -0400

    IB/core: Define 'ib' and 'roce' rdma_ah_attr types
    
    rdma_ah_attr can now be either ib or roce allowing
    core components to use one type or the other and also
    to define attributes unique to a specific type. struct
    ib_ah is also initialized with the type when its first
    created. This ensures that calls such as modify_ah
    dont modify the type of the address handle attribute.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 46a5be62c052..803927b31742 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -840,15 +840,31 @@ struct ib_mr_status {
  */
 __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 
+enum rdma_ah_attr_type {
+	RDMA_AH_ATTR_TYPE_IB,
+	RDMA_AH_ATTR_TYPE_ROCE,
+};
+
+struct ib_ah_attr {
+	u16			dlid;
+	u8			src_path_bits;
+};
+
+struct roce_ah_attr {
+	u8			dmac[ETH_ALEN];
+};
+
 struct rdma_ah_attr {
 	struct ib_global_route	grh;
-	u16			dlid;
 	u8			sl;
-	u8			src_path_bits;
 	u8			static_rate;
-	u8			ah_flags;
 	u8			port_num;
-	u8			dmac[ETH_ALEN];
+	u8			ah_flags;
+	enum rdma_ah_attr_type type;
+	union {
+		struct ib_ah_attr ib;
+		struct roce_ah_attr roce;
+	};
 };
 
 enum ib_wc_status {
@@ -1467,6 +1483,7 @@ struct ib_ah {
 	struct ib_device	*device;
 	struct ib_pd		*pd;
 	struct ib_uobject	*uobject;
+	enum rdma_ah_attr_type	type;
 };
 
 typedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);
@@ -3468,17 +3485,22 @@ int ib_resolve_eth_dmac(struct ib_device *device,
 
 static inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)
 {
-	return attr->dmac;
+	if (attr->type == RDMA_AH_ATTR_TYPE_ROCE)
+		return attr->roce.dmac;
+	return NULL;
 }
 
-static inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u32 dlid)
+static inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u16 dlid)
 {
-	attr->dlid = (u16)dlid;
+	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
+		attr->ib.dlid = dlid;
 }
 
-static inline u32 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)
+static inline u16 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)
 {
-	return attr->dlid;
+	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
+		return attr->ib.dlid;
+	return 0;
 }
 
 static inline void rdma_ah_set_sl(struct rdma_ah_attr *attr, u8 sl)
@@ -3494,12 +3516,15 @@ static inline u8 rdma_ah_get_sl(const struct rdma_ah_attr *attr)
 static inline void rdma_ah_set_path_bits(struct rdma_ah_attr *attr,
 					 u8 src_path_bits)
 {
-	attr->src_path_bits = src_path_bits;
+	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
+		attr->ib.src_path_bits = src_path_bits;
 }
 
 static inline u8 rdma_ah_get_path_bits(const struct rdma_ah_attr *attr)
 {
-	return attr->src_path_bits;
+	if (attr->type == RDMA_AH_ATTR_TYPE_IB)
+		return attr->ib.src_path_bits;
+	return 0;
 }
 
 static inline void rdma_ah_set_port_num(struct rdma_ah_attr *attr, u8 port_num)
@@ -3586,4 +3611,15 @@ static inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,
 	grh->hop_limit = hop_limit;
 	grh->traffic_class = traffic_class;
 }
+
+/*Get AH type */
+static inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,
+						       u32 port_num)
+{
+	if ((rdma_protocol_roce(dev, port_num)) ||
+	    (rdma_protocol_iwarp(dev, port_num)))
+		return RDMA_AH_ATTR_TYPE_ROCE;
+	else
+		return RDMA_AH_ATTR_TYPE_IB;
+}
 #endif /* IB_VERBS_H */

commit 2224c47ace2387610f8cff0c562db2c6e63df026
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:27 2017 -0400

    IB/core: Add accessor functions for rdma_ah_attr fields
    
    These accessor functions are supposed to be used to get
    and set individual fields of struct rdma_ah_attr
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ecaf9d50876c..46a5be62c052 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3465,4 +3465,125 @@ void ib_drain_qp(struct ib_qp *qp);
 
 int ib_resolve_eth_dmac(struct ib_device *device,
 			struct rdma_ah_attr *ah_attr);
+
+static inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)
+{
+	return attr->dmac;
+}
+
+static inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u32 dlid)
+{
+	attr->dlid = (u16)dlid;
+}
+
+static inline u32 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)
+{
+	return attr->dlid;
+}
+
+static inline void rdma_ah_set_sl(struct rdma_ah_attr *attr, u8 sl)
+{
+	attr->sl = sl;
+}
+
+static inline u8 rdma_ah_get_sl(const struct rdma_ah_attr *attr)
+{
+	return attr->sl;
+}
+
+static inline void rdma_ah_set_path_bits(struct rdma_ah_attr *attr,
+					 u8 src_path_bits)
+{
+	attr->src_path_bits = src_path_bits;
+}
+
+static inline u8 rdma_ah_get_path_bits(const struct rdma_ah_attr *attr)
+{
+	return attr->src_path_bits;
+}
+
+static inline void rdma_ah_set_port_num(struct rdma_ah_attr *attr, u8 port_num)
+{
+	attr->port_num = port_num;
+}
+
+static inline u8 rdma_ah_get_port_num(const struct rdma_ah_attr *attr)
+{
+	return attr->port_num;
+}
+
+static inline void rdma_ah_set_static_rate(struct rdma_ah_attr *attr,
+					   u8 static_rate)
+{
+	attr->static_rate = static_rate;
+}
+
+static inline u8 rdma_ah_get_static_rate(const struct rdma_ah_attr *attr)
+{
+	return attr->static_rate;
+}
+
+static inline void rdma_ah_set_ah_flags(struct rdma_ah_attr *attr,
+					enum ib_ah_flags flag)
+{
+	attr->ah_flags = flag;
+}
+
+static inline enum ib_ah_flags
+		rdma_ah_get_ah_flags(const struct rdma_ah_attr *attr)
+{
+	return attr->ah_flags;
+}
+
+static inline const struct ib_global_route
+		*rdma_ah_read_grh(const struct rdma_ah_attr *attr)
+{
+	return &attr->grh;
+}
+
+/*To retrieve and modify the grh */
+static inline struct ib_global_route
+		*rdma_ah_retrieve_grh(struct rdma_ah_attr *attr)
+{
+	return &attr->grh;
+}
+
+static inline void rdma_ah_set_dgid_raw(struct rdma_ah_attr *attr, void *dgid)
+{
+	struct ib_global_route *grh = rdma_ah_retrieve_grh(attr);
+
+	memcpy(grh->dgid.raw, dgid, sizeof(grh->dgid));
+}
+
+static inline void rdma_ah_set_subnet_prefix(struct rdma_ah_attr *attr,
+					     __be64 prefix)
+{
+	struct ib_global_route *grh = rdma_ah_retrieve_grh(attr);
+
+	grh->dgid.global.subnet_prefix = prefix;
+}
+
+static inline void rdma_ah_set_interface_id(struct rdma_ah_attr *attr,
+					    __be64 if_id)
+{
+	struct ib_global_route *grh = rdma_ah_retrieve_grh(attr);
+
+	grh->dgid.global.interface_id = if_id;
+}
+
+static inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,
+				   union ib_gid *dgid, u32 flow_label,
+				   u8 sgid_index, u8 hop_limit,
+				   u8 traffic_class)
+{
+	struct ib_global_route *grh = rdma_ah_retrieve_grh(attr);
+
+	attr->ah_flags = IB_AH_GRH;
+	if (dgid)
+		grh->dgid = *dgid;
+	grh->flow_label = flow_label;
+	grh->sgid_index = sgid_index;
+	grh->hop_limit = hop_limit;
+	grh->traffic_class = traffic_class;
+}
 #endif /* IB_VERBS_H */

commit 365231593409fb79b11dd9bfcc27a29090bf9de6
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:22 2017 -0400

    IB/core: Rename ib_destroy_ah to rdma_destroy_ah
    
    Rename ib_destroy_ah to rdma_destroy_ah so its in sync with the
    rename of the ib address handle attribute
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 16fd94026d49..ecaf9d50876c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2796,10 +2796,10 @@ int rdma_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 int rdma_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
- * ib_destroy_ah - Destroys an address handle.
+ * rdma_destroy_ah - Destroys an address handle.
  * @ah: The address handle to destroy.
  */
-int ib_destroy_ah(struct ib_ah *ah);
+int rdma_destroy_ah(struct ib_ah *ah);
 
 /**
  * ib_create_srq - Creates a SRQ associated with the specified protection

commit bfbfd661c9ea2cceb5bb4de8b280ac8a37cf68c2
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:21 2017 -0400

    IB/core: Rename ib_query_ah to rdma_query_ah
    
    Rename ib_query_ah to rdma_query_ah so its in sync with the
    rename of the ib address handle attribute
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73fb465e88fc..16fd94026d49 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2787,13 +2787,13 @@ struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, const struct ib_wc *wc,
 int rdma_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
- * ib_query_ah - Queries the address vector associated with an address
+ * rdma_query_ah - Queries the address vector associated with an address
  *   handle.
  * @ah: The address handle to query.
  * @ah_attr: The address vector attributes associated with the address
  *   handle.
  */
-int ib_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
+int rdma_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_destroy_ah - Destroys an address handle.

commit 67b985b6c75530bd3dccd55d61d2c9027ab2ca38
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:20 2017 -0400

    IB/core: Rename ib_modify_ah to rdma_modify_ah
    
    Rename ib_modify_ah to rdma_modify_ah so its in sync with the
    rename of the ib address handle attribute
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d65812147602..73fb465e88fc 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2778,13 +2778,13 @@ struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, const struct ib_wc *wc,
 				   const struct ib_grh *grh, u8 port_num);
 
 /**
- * ib_modify_ah - Modifies the address vector associated with an address
+ * rdma_modify_ah - Modifies the address vector associated with an address
  *   handle.
  * @ah: The address handle to modify.
  * @ah_attr: The new address vector attributes to associate with the
  *   address handle.
  */
-int ib_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
+int rdma_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_query_ah - Queries the address vector associated with an address

commit 0a18cfe4f6d7dba135a04dc18633006ba5b51646
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:19 2017 -0400

    IB/core: Rename ib_create_ah to rdma_create_ah
    
    Rename ib_create_ah to rdma_create_ah so its in sync with the
    rename of the ib address handle attribute
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ed9f19817db8..d65812147602 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2720,14 +2720,14 @@ struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,
 void ib_dealloc_pd(struct ib_pd *pd);
 
 /**
- * ib_create_ah - Creates an address handle for the given address vector.
+ * rdma_create_ah - Creates an address handle for the given address vector.
  * @pd: The protection domain associated with the address handle.
  * @ah_attr: The attributes of the address vector.
  *
  * The address handle is used to reference a local or global destination
  * in all UD QP post sends.
  */
-struct ib_ah *ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr);
+struct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_get_gids_from_rdma_hdr - Get sgid and dgid from GRH or IPv4 header

commit 90898850ec4e7b3ba0f9a35cc7169ff19ff367a6
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:18 2017 -0400

    IB/core: Rename struct ib_ah_attr to rdma_ah_attr
    
    This patch simply renames struct ib_ah_attr to
    rdma_ah_attr as these fields specify attributes that are
    not necessarily specific to IB.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b6a0b5fa7f15..ed9f19817db8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -840,7 +840,7 @@ struct ib_mr_status {
  */
 __attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 
-struct ib_ah_attr {
+struct rdma_ah_attr {
 	struct ib_global_route	grh;
 	u16			dlid;
 	u8			sl;
@@ -1167,8 +1167,8 @@ struct ib_qp_attr {
 	u32			dest_qp_num;
 	int			qp_access_flags;
 	struct ib_qp_cap	cap;
-	struct ib_ah_attr	ah_attr;
-	struct ib_ah_attr	alt_ah_attr;
+	struct rdma_ah_attr	ah_attr;
+	struct rdma_ah_attr	alt_ah_attr;
 	u16			pkey_index;
 	u16			alt_pkey_index;
 	u8			en_sqd_async_notify;
@@ -2032,12 +2032,12 @@ struct ib_device {
 					       struct ib_udata *udata);
 	int                        (*dealloc_pd)(struct ib_pd *pd);
 	struct ib_ah *             (*create_ah)(struct ib_pd *pd,
-						struct ib_ah_attr *ah_attr,
+						struct rdma_ah_attr *ah_attr,
 						struct ib_udata *udata);
 	int                        (*modify_ah)(struct ib_ah *ah,
-						struct ib_ah_attr *ah_attr);
+						struct rdma_ah_attr *ah_attr);
 	int                        (*query_ah)(struct ib_ah *ah,
-					       struct ib_ah_attr *ah_attr);
+					       struct rdma_ah_attr *ah_attr);
 	int                        (*destroy_ah)(struct ib_ah *ah);
 	struct ib_srq *            (*create_srq)(struct ib_pd *pd,
 						 struct ib_srq_init_attr *srq_init_attr,
@@ -2727,7 +2727,7 @@ void ib_dealloc_pd(struct ib_pd *pd);
  * The address handle is used to reference a local or global destination
  * in all UD QP post sends.
  */
-struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
+struct ib_ah *ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_get_gids_from_rdma_hdr - Get sgid and dgid from GRH or IPv4 header
@@ -2760,7 +2760,7 @@ int ib_get_rdma_header_version(const union rdma_network_hdr *hdr);
  */
 int ib_init_ah_from_wc(struct ib_device *device, u8 port_num,
 		       const struct ib_wc *wc, const struct ib_grh *grh,
-		       struct ib_ah_attr *ah_attr);
+		       struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_create_ah_from_wc - Creates an address handle associated with the
@@ -2784,7 +2784,7 @@ struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, const struct ib_wc *wc,
  * @ah_attr: The new address vector attributes to associate with the
  *   address handle.
  */
-int ib_modify_ah(struct ib_ah *ah, struct ib_ah_attr *ah_attr);
+int ib_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_query_ah - Queries the address vector associated with an address
@@ -2793,7 +2793,7 @@ int ib_modify_ah(struct ib_ah *ah, struct ib_ah_attr *ah_attr);
  * @ah_attr: The address vector attributes associated with the address
  *   handle.
  */
-int ib_query_ah(struct ib_ah *ah, struct ib_ah_attr *ah_attr);
+int ib_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);
 
 /**
  * ib_destroy_ah - Destroys an address handle.
@@ -3464,5 +3464,5 @@ void ib_drain_sq(struct ib_qp *qp);
 void ib_drain_qp(struct ib_qp *qp);
 
 int ib_resolve_eth_dmac(struct ib_device *device,
-			struct ib_ah_attr *ah_attr);
+			struct rdma_ah_attr *ah_attr);
 #endif /* IB_VERBS_H */

commit 94d595c56077fd8b0f61701e03fd4b3dc8c62038
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Mon Mar 20 19:38:09 2017 -0400

    IB/core: Add rdma_cap_opa_ah to expose opa address handles
    
    rdma_cap_opa_ah(..) enables core components to check if the
    corresponding port supports OPA extended addressing.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ab2dc5284e8c..b6a0b5fa7f15 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -501,6 +501,7 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 /* Address format                       0x000FF000 */
 #define RDMA_CORE_CAP_AF_IB             0x00001000
 #define RDMA_CORE_CAP_ETH_AH            0x00002000
+#define RDMA_CORE_CAP_OPA_AH            0x00004000
 
 /* Protocol                             0xFFF00000 */
 #define RDMA_CORE_CAP_PROT_IB           0x00100000
@@ -2604,6 +2605,21 @@ static inline bool rdma_cap_eth_ah(const struct ib_device *device, u8 port_num)
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_ETH_AH;
 }
 
+/**
+ * rdma_cap_opa_ah - Check if the port of device supports
+ * OPA Address handles
+ * @device: Device to check
+ * @port_num: Port number to check
+ *
+ * Return: true if we are running on an OPA device which supports
+ * the extended OPA addressing.
+ */
+static inline bool rdma_cap_opa_ah(struct ib_device *device, u8 port_num)
+{
+	return (device->port_immutable[port_num].core_cap_flags &
+		RDMA_CORE_CAP_OPA_AH) == RDMA_CORE_CAP_OPA_AH;
+}
+
 /**
  * rdma_max_mad_size - Return the max MAD size required by this RDMA Port.
  *

commit 0008b84ea9afe6ec255c09044e8090cb76babc80
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Apr 5 09:23:57 2017 +0300

    IB/umem: Add support to huge ODP
    
    Add IB_ACCESS_HUGETLB ib_reg_mr flag.
    Hugetlb region registered with this flag
    will use single translation entry per huge page.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 91686d2e94dd..ab2dc5284e8c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1339,6 +1339,7 @@ enum ib_access_flags {
 	IB_ACCESS_MW_BIND	= (1<<4),
 	IB_ZERO_BASED		= (1<<5),
 	IB_ACCESS_ON_DEMAND     = (1<<6),
+	IB_ACCESS_HUGETLB	= (1<<7),
 };
 
 /*

commit 12113a35ada6bba074836d3d26671213e12069bf
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Thu Apr 20 20:53:31 2017 +0300

    IB/core: Add HDR speed enum
    
    Add high data rate speed to the ib_port_speed enumeration.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 56fa31e1948a..91686d2e94dd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -433,7 +433,8 @@ enum ib_port_speed {
 	IB_SPEED_QDR	= 4,
 	IB_SPEED_FDR10	= 8,
 	IB_SPEED_FDR	= 16,
-	IB_SPEED_EDR	= 32
+	IB_SPEED_EDR	= 32,
+	IB_SPEED_HDR	= 64
 };
 
 /**

commit 483a3966b570529a910dc2a02deac0036e642309
Author: Slava Shwartsman <slavash@mellanox.com>
Date:   Mon Apr 3 13:13:51 2017 +0300

    IB/core: Introduce drop flow specification
    
    This flow steering specification identifies flow for drop by the HW.
    If user create a flow only with the drop specification,
    then all the packets that hit this flow will be dropped, otherwise the HW
    will drop only the packets that match the other L2/L3/L4 specifications.
    
    Signed-off-by: Slava Shwartsman <slavash@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 16f15ea8606e..56fa31e1948a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1679,6 +1679,7 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_INNER		= 0x100,
 	/* Actions */
 	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
+	IB_FLOW_SPEC_ACTION_DROP        = 0x1001,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
 #define IB_FLOW_SPEC_SUPPORT_LAYERS 8
@@ -1807,6 +1808,11 @@ struct ib_flow_spec_action_tag {
 	u32                           tag_id;
 };
 
+struct ib_flow_spec_action_drop {
+	enum ib_flow_spec_type	      type;
+	u16			      size;
+};
+
 union ib_flow_spec {
 	struct {
 		u32			type;
@@ -1819,6 +1825,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_ipv6        ipv6;
 	struct ib_flow_spec_tunnel      tunnel;
 	struct ib_flow_spec_action_tag  flow_tag;
+	struct ib_flow_spec_action_drop drop;
 };
 
 struct ib_flow_attr {

commit f0ad83ac58468e3807e72e929317519b69bcde1c
Author: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
Date:   Mon Apr 10 11:22:25 2017 +0300

    IB/IPoIB: Introduce RDMA netdev interface and IPoIB structs
    
    Add RDMA netdev interface to ib device structure allowing RDMA
    netdev devices to be allocated by ib clients.
    
    The idea is to allow to providers to optimize IPoIB data path.
    New struct that includes functions and data member is exposed.
    It exposes set of callback functions for handling data path flows
    in IPoIB driver.
    
    Each provider can support these set of functions in order
    to optimize its specific data path, and let IPoIB to leverage
    its data path.
    
    There is an assumption, that providers should give the full set
    of functions and not only part of them, in order to work properly.
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5d5c0a918798..16f15ea8606e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1881,7 +1881,8 @@ struct ib_port_immutable {
 
 /* rdma netdev type - specifies protocol type */
 enum rdma_netdev_t {
-	RDMA_NETDEV_OPA_VNIC
+	RDMA_NETDEV_OPA_VNIC,
+	RDMA_NETDEV_IPOIB,
 };
 
 /**
@@ -1895,6 +1896,15 @@ struct rdma_netdev {
 
 	/* control functions */
 	void (*set_id)(struct net_device *netdev, int id);
+	/* send packet */
+	int (*send)(struct net_device *dev, struct sk_buff *skb,
+		    struct ib_ah *address, u32 dqpn);
+	/* multicast */
+	int (*attach_mcast)(struct net_device *dev, struct ib_device *hca,
+			    union ib_gid *gid, u16 mlid,
+			    int set_qkey, u32 qkey);
+	int (*detach_mcast)(struct net_device *dev, struct ib_device *hca,
+			    union ib_gid *gid, u16 mlid);
 };
 
 struct ib_device {

commit 62e4594940da086b74cc47d7031b38a455483d07
Author: Vishwanathapura, Niranjana <niranjana.vishwanathapura@intel.com>
Date:   Wed Apr 12 20:29:21 2017 -0700

    IB/opa-vnic: Virtual Network Interface Controller (VNIC) interface
    
    Define OPA VNIC interface between hardware independent VNIC
    functionality and the hardware dependent VNIC functionality.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dd02ba53fd1f..5d5c0a918798 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -225,6 +225,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_VIRTUAL_FUNCTION		= (1ULL << 33),
 	/* Deprecated. Please use IB_RAW_PACKET_CAP_SCATTER_FCS. */
 	IB_DEVICE_RAW_SCATTER_FCS		= (1ULL << 34),
+	IB_DEVICE_RDMA_NETDEV_OPA_VNIC		= (1ULL << 35),
 };
 
 enum ib_signature_prot_cap {

commit 2fc77572649163f8d669389e87217cc99942847a
Author: Vishwanathapura, Niranjana <niranjana.vishwanathapura@intel.com>
Date:   Wed Apr 12 20:29:20 2017 -0700

    IB/opa-vnic: RDMA NETDEV interface
    
    Add rdma netdev interface to ib device structure allowing rdma netdev
    devices to be allocated by ib clients.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4ce7c20fe219..dd02ba53fd1f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -55,6 +55,7 @@
 #include <net/ip.h>
 #include <linux/string.h>
 #include <linux/slab.h>
+#include <linux/netdevice.h>
 
 #include <linux/if_link.h>
 #include <linux/atomic.h>
@@ -1877,6 +1878,24 @@ struct ib_port_immutable {
 	u32                           max_mad_size;
 };
 
+/* rdma netdev type - specifies protocol type */
+enum rdma_netdev_t {
+	RDMA_NETDEV_OPA_VNIC
+};
+
+/**
+ * struct rdma_netdev - rdma netdev
+ * For cases where netstack interfacing is required.
+ */
+struct rdma_netdev {
+	void              *clnt_priv;
+	struct ib_device  *hca;
+	u8                 port_num;
+
+	/* control functions */
+	void (*set_id)(struct net_device *netdev, int id);
+};
+
 struct ib_device {
 	/* Do not access @dma_device directly from ULP nor from HW drivers. */
 	struct device                *dma_device;
@@ -2130,6 +2149,20 @@ struct ib_device {
 							   struct ib_rwq_ind_table_init_attr *init_attr,
 							   struct ib_udata *udata);
 	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
+	/**
+	 * rdma netdev operations
+	 *
+	 * Driver implementing alloc_rdma_netdev must return -EOPNOTSUPP if it
+	 * doesn't support the specified rdma netdev type.
+	 */
+	struct net_device *(*alloc_rdma_netdev)(
+					struct ib_device *device,
+					u8 port_num,
+					enum rdma_netdev_t type,
+					const char *name,
+					unsigned char name_assign_type,
+					void (*setup)(struct net_device *));
+	void (*free_rdma_netdev)(struct net_device *netdev);
 
 	struct module               *owner;
 	struct device                dev;

commit 23790ba2d775698ed44ef37b8c72b94c73ae5a6c
Merge: 70d40b366d2f db1b5ddd5336
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Apr 20 12:00:41 2017 -0400

    Merge branch 'k.o/for-4.12' into k.o/for-4.12-rdma-netdevice

commit cf8966b3477d5e6545393bb4499f2051ea554c62
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Apr 4 13:31:46 2017 +0300

    IB/core: Add support for fd objects
    
    The completion channel we use in verbs infrastructure is FD based.
    Previously, we had a separate way to manage this object. Since we
    strive for a single way to manage any kind of object in this
    infrastructure, we conceptually treat all objects as subclasses
    of ib_uobject.
    
    This commit adds the necessary mechanism to support FD based objects
    like their IDR counterparts. FD objects release need to be synchronized
    with context release. We use the cleanup_mutex on the uverbs_file for
    that.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2e8f661c900a..3a8e05894e9b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1421,6 +1421,12 @@ struct ib_uobject {
 	const struct uverbs_obj_type *type;
 };
 
+struct ib_uobject_file {
+	struct ib_uobject	uobj;
+	/* ufile contains the lock between context release and file close */
+	struct ib_uverbs_file	*ufile;
+};
+
 struct ib_udata {
 	const void __user *inbuf;
 	void __user *outbuf;

commit fd3c7904db6e05043398aee5c1448682acfb025b
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Apr 4 13:31:44 2017 +0300

    IB/core: Change idr objects to use the new schema
    
    This changes only the handlers which deals with idr based objects to
    use the new idr allocation, fetching and destruction schema.
    This patch consists of the following changes:
    (1) Allocation, fetching and destruction is done via idr ops.
    (2) Context initializing and release is done through
        uverbs_initialize_ucontext and uverbs_cleanup_ucontext.
    (3) Ditching the live flag. Mostly, this is pretty straight
        forward. The only place that is a bit trickier is in
        ib_uverbs_open_qp. Commit [1] added code to check whether
        the uobject is already live and initialized. This mostly
        happens because of a race between open_qp and events.
        We delayed assigning the uobject's pointer in order to
        eliminate this race without using the live variable.
    
    [1] commit a040f95dc819
            ("IB/core: Fix XRC race condition in ib_uverbs_open_qp")
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d3efd22943e9..2e8f661c900a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1377,17 +1377,6 @@ struct ib_rdmacg_object {
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct ib_uverbs_file  *ufile;
-	struct list_head	pd_list;
-	struct list_head	mr_list;
-	struct list_head	mw_list;
-	struct list_head	cq_list;
-	struct list_head	qp_list;
-	struct list_head	srq_list;
-	struct list_head	ah_list;
-	struct list_head	xrcd_list;
-	struct list_head	rule_list;
-	struct list_head	wq_list;
-	struct list_head	rwq_ind_tbl_list;
 	int			closing;
 
 	/* locking the uobjects_list */
@@ -1426,10 +1415,8 @@ struct ib_uobject {
 	struct ib_rdmacg_object	cg_obj;		/* rdmacg object */
 	int			id;		/* index into kernel idr */
 	struct kref		ref;
-	struct rw_semaphore	mutex;		/* protects .live */
 	atomic_t		usecnt;		/* protects exclusive access */
 	struct rcu_head		rcu;		/* kfree_rcu() overhead */
-	int			live;
 
 	const struct uverbs_obj_type *type;
 };

commit 3832125624b75b54567be906e9aa67e1343be569
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Apr 4 13:31:42 2017 +0300

    IB/core: Add support for idr types
    
    The new ioctl infrastructure supports driver specific objects.
    Each such object type has a hot unplug function, allocation size and
    an order of destruction.
    
    When a ucontext is created, a new list is created in this ib_ucontext.
    This list contains all objects created under this ib_ucontext.
    When a ib_ucontext is destroyed, we traverse this list several time
    destroying the various objects by the order mentioned in the object
    type description. If few object types have the same destruction order,
    they are destroyed in an order opposite to their creation.
    
    Adding an object is done in two parts.
    First, an object is allocated and added to idr tree. Then, the
    command's handlers (in downstream patches) could work on this object
    and fill in its required details.
    After a successful command, the commit part is called and the user
    objects become ucontext visible. If the handler failed, alloc_abort
    should be called.
    
    Removing an uboject is done by calling lookup_get with the write flag
    and finalizing it with destroy_commit. A major change from the previous
    code is that we actually destroy the kernel object itself in
    destroy_commit (rather than just the uobject).
    
    We should make sure idr (per-uverbs-file) and list (per-ucontext) could
    be accessed concurrently without corrupting them.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 319e69106a26..d3efd22943e9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1357,6 +1357,17 @@ struct ib_fmr_attr {
 
 struct ib_umem;
 
+enum rdma_remove_reason {
+	/* Userspace requested uobject deletion. Call could fail */
+	RDMA_REMOVE_DESTROY,
+	/* Context deletion. This call should delete the actual object itself */
+	RDMA_REMOVE_CLOSE,
+	/* Driver is being hot-unplugged. This call should delete the actual object itself */
+	RDMA_REMOVE_DRIVER_REMOVE,
+	/* Context is being cleaned-up, but commit was just completed */
+	RDMA_REMOVE_DURING_CLEANUP,
+};
+
 struct ib_rdmacg_object {
 #ifdef CONFIG_CGROUP_RDMA
 	struct rdma_cgroup	*cg;		/* owner rdma cgroup */
@@ -1379,6 +1390,13 @@ struct ib_ucontext {
 	struct list_head	rwq_ind_tbl_list;
 	int			closing;
 
+	/* locking the uobjects_list */
+	struct mutex		uobjects_lock;
+	struct list_head	uobjects;
+	/* protects cleanup process from other actions */
+	struct rw_semaphore	cleanup_rwsem;
+	enum rdma_remove_reason cleanup_reason;
+
 	struct pid             *tgid;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct rb_root      umem_tree;
@@ -1409,8 +1427,11 @@ struct ib_uobject {
 	int			id;		/* index into kernel idr */
 	struct kref		ref;
 	struct rw_semaphore	mutex;		/* protects .live */
+	atomic_t		usecnt;		/* protects exclusive access */
 	struct rcu_head		rcu;		/* kfree_rcu() overhead */
 	int			live;
+
+	const struct uverbs_obj_type *type;
 };
 
 struct ib_udata {

commit 771addf60ac0a266a023c3e7fcae9a629658b455
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Apr 4 13:31:41 2017 +0300

    IB/core: Refactor idr to be per uverbs_file
    
    The current code creates an idr per type. Since types are currently
    common for all drivers and known in advance, this was good enough.
    However, the proposed ioctl based infrastructure allows each driver
    to declare only some of the common types and declare its own specific
    types.
    
    Thus, we decided to implement idr to be per uverbs_file.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0f1813c13687..319e69106a26 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1365,6 +1365,7 @@ struct ib_rdmacg_object {
 
 struct ib_ucontext {
 	struct ib_device       *device;
+	struct ib_uverbs_file  *ufile;
 	struct list_head	pd_list;
 	struct list_head	mr_list;
 	struct list_head	mw_list;

commit 0957c29f78af7d890c4ac506eda8f76bfc5a137a
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Mar 7 22:56:53 2017 +0000

    IB/core: Restore I/O MMU, s390 and powerpc support
    
    Avoid that the following error message is reported on the console
    while loading an RDMA driver with I/O MMU support enabled:
    
    DMAR: Allocating domain for mlx5_0 failed
    
    Ensure that DMA mapping operations that use to_pci_dev() to
    access to struct pci_dev see the correct PCI device. E.g. the s390
    and powerpc DMA mapping operations use to_pci_dev() even with I/O
    MMU support disabled.
    
    This patch preserves the following changes of the DMA mapping updates
    patch series:
    - Introduction of dma_virt_ops.
    - Removal of ib_device.dma_ops.
    - Removal of struct ib_dma_mapping_ops.
    - Removal of an if-statement from each ib_dma_*() operation.
    - IB HW drivers no longer set dma_device directly.
    
    Reported-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Reported-by: Parav Pandit <parav@mellanox.com>
    Fixes: commit 99db9494035f ("IB/core: Remove ib_device.dma_device")
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: parav@mellanox.com
    Tested-by: parav@mellanox.com
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0f1813c13687..99e4423eb2b8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1863,6 +1863,9 @@ struct ib_port_immutable {
 };
 
 struct ib_device {
+	/* Do not access @dma_device directly from ULP nor from HW drivers. */
+	struct device                *dma_device;
+
 	char                          name[IB_DEVICE_NAME_MAX];
 
 	struct list_head              event_handler_list;
@@ -3007,7 +3010,7 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
  */
 static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
-	return dma_mapping_error(&dev->dev, dma_addr);
+	return dma_mapping_error(dev->dma_device, dma_addr);
 }
 
 /**
@@ -3021,7 +3024,7 @@ static inline u64 ib_dma_map_single(struct ib_device *dev,
 				    void *cpu_addr, size_t size,
 				    enum dma_data_direction direction)
 {
-	return dma_map_single(&dev->dev, cpu_addr, size, direction);
+	return dma_map_single(dev->dma_device, cpu_addr, size, direction);
 }
 
 /**
@@ -3035,7 +3038,7 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 				       u64 addr, size_t size,
 				       enum dma_data_direction direction)
 {
-	dma_unmap_single(&dev->dev, addr, size, direction);
+	dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
 /**
@@ -3052,7 +3055,7 @@ static inline u64 ib_dma_map_page(struct ib_device *dev,
 				  size_t size,
 					 enum dma_data_direction direction)
 {
-	return dma_map_page(&dev->dev, page, offset, size, direction);
+	return dma_map_page(dev->dma_device, page, offset, size, direction);
 }
 
 /**
@@ -3066,7 +3069,7 @@ static inline void ib_dma_unmap_page(struct ib_device *dev,
 				     u64 addr, size_t size,
 				     enum dma_data_direction direction)
 {
-	dma_unmap_page(&dev->dev, addr, size, direction);
+	dma_unmap_page(dev->dma_device, addr, size, direction);
 }
 
 /**
@@ -3080,7 +3083,7 @@ static inline int ib_dma_map_sg(struct ib_device *dev,
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction direction)
 {
-	return dma_map_sg(&dev->dev, sg, nents, direction);
+	return dma_map_sg(dev->dma_device, sg, nents, direction);
 }
 
 /**
@@ -3094,7 +3097,7 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction direction)
 {
-	dma_unmap_sg(&dev->dev, sg, nents, direction);
+	dma_unmap_sg(dev->dma_device, sg, nents, direction);
 }
 
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
@@ -3102,7 +3105,8 @@ static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      enum dma_data_direction direction,
 				      unsigned long dma_attrs)
 {
-	return dma_map_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
+	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
+				dma_attrs);
 }
 
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
@@ -3110,7 +3114,7 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 enum dma_data_direction direction,
 					 unsigned long dma_attrs)
 {
-	dma_unmap_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
+	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
 }
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
@@ -3152,7 +3156,7 @@ static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
 					      size_t size,
 					      enum dma_data_direction dir)
 {
-	dma_sync_single_for_cpu(&dev->dev, addr, size, dir);
+	dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
 }
 
 /**
@@ -3167,7 +3171,7 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
 						 size_t size,
 						 enum dma_data_direction dir)
 {
-	dma_sync_single_for_device(&dev->dev, addr, size, dir);
+	dma_sync_single_for_device(dev->dma_device, addr, size, dir);
 }
 
 /**
@@ -3182,7 +3186,7 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 					   dma_addr_t *dma_handle,
 					   gfp_t flag)
 {
-	return dma_alloc_coherent(&dev->dev, size, dma_handle, flag);
+	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
 }
 
 /**
@@ -3196,7 +3200,7 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 					size_t size, void *cpu_addr,
 					dma_addr_t dma_handle)
 {
-	dma_free_coherent(&dev->dev, size, cpu_addr, dma_handle);
+	dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 
 /**

commit f7878dc3a9d3d900c86a66d9742f7e06681b06cd
Merge: fb15a78210f1 f83f3c515654
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 27 21:41:08 2017 -0800

    Merge branch 'for-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Several noteworthy changes.
    
       - Parav's rdma controller is finally merged. It is very straight
         forward and can limit the abosolute numbers of common rdma
         constructs used by different cgroups.
    
       - kernel/cgroup.c got too chubby and disorganized. Created
         kernel/cgroup/ subdirectory and moved all cgroup related files
         under kernel/ there and reorganized the core code. This hurts for
         backporting patches but was long overdue.
    
       - cgroup v2 process listing reimplemented so that it no longer
         depends on allocating a buffer large enough to cache the entire
         result to sort and uniq the output. v2 has always mangled the sort
         order to ensure that users don't depend on the sorted output, so
         this shouldn't surprise anybody. This makes the pid listing
         functions use the same iterators that are used internally, which
         have to have the same iterating capabilities anyway.
    
       - perf cgroup filtering now works automatically on cgroup v2. This
         patch was posted a long time ago but somehow fell through the
         cracks.
    
       - misc fixes asnd documentation updates"
    
    * 'for-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (27 commits)
      kernfs: fix locking around kernfs_ops->release() callback
      cgroup: drop the matching uid requirement on migration for cgroup v2
      cgroup, perf_event: make perf_event controller work on cgroup2 hierarchy
      cgroup: misc cleanups
      cgroup: call subsys->*attach() only for subsystems which are actually affected by migration
      cgroup: track migration context in cgroup_mgctx
      cgroup: cosmetic update to cgroup_taskset_add()
      rdmacg: Fixed uninitialized current resource usage
      cgroup: Add missing cgroup-v2 PID controller documentation.
      rdmacg: Added documentation for rdmacg
      IB/core: added support to use rdma cgroup controller
      rdmacg: Added rdma cgroup controller
      cgroup: fix a comment typo
      cgroup: fix RCU related sparse warnings
      cgroup: move namespace code to kernel/cgroup/namespace.c
      cgroup: rename functions for consistency
      cgroup: move v1 mount functions to kernel/cgroup/cgroup-v1.c
      cgroup: separate out cgroup1_kf_syscall_ops
      cgroup: refactor mount path and clearly distinguish v1 and v2 paths
      cgroup: move cgroup v1 specific code to kernel/cgroup/cgroup-v1.c
      ...

commit ac1820fb286b552b6885d40ab34f1e59b815f1f1
Merge: edccb5942965 0bbb3b7496ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 25 13:45:43 2017 -0800

    Merge tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma DMA mapping updates from Doug Ledford:
     "Drop IB DMA mapping code and use core DMA code instead.
    
      Bart Van Assche noted that the ib DMA mapping code was significantly
      similar enough to the core DMA mapping code that with a few changes it
      was possible to remove the IB DMA mapping code entirely and switch the
      RDMA stack to use the core DMA mapping code.
    
      This resulted in a nice set of cleanups, but touched the entire tree
      and has been kept separate for that reason."
    
    * tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (37 commits)
      IB/rxe, IB/rdmavt: Use dma_virt_ops instead of duplicating it
      IB/core: Remove ib_device.dma_device
      nvme-rdma: Switch from dma_device to dev.parent
      RDS: net: Switch from dma_device to dev.parent
      IB/srpt: Modify a debug statement
      IB/srp: Switch from dma_device to dev.parent
      IB/iser: Switch from dma_device to dev.parent
      IB/IPoIB: Switch from dma_device to dev.parent
      IB/rxe: Switch from dma_device to dev.parent
      IB/vmw_pvrdma: Switch from dma_device to dev.parent
      IB/usnic: Switch from dma_device to dev.parent
      IB/qib: Switch from dma_device to dev.parent
      IB/qedr: Switch from dma_device to dev.parent
      IB/ocrdma: Switch from dma_device to dev.parent
      IB/nes: Remove a superfluous assignment statement
      IB/mthca: Switch from dma_device to dev.parent
      IB/mlx5: Switch from dma_device to dev.parent
      IB/mlx4: Switch from dma_device to dev.parent
      IB/i40iw: Remove a superfluous assignment statement
      IB/hns: Switch from dma_device to dev.parent
      ...

commit af17fe7a63db7e11d65f1296f0cbf156a89a2735
Merge: f14cc3b13d8f cdbe33d0f82d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 23 11:27:49 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull Mellanox rdma updates from Doug Ledford:
     "Mellanox specific updates for 4.11 merge window
    
      Because the Mellanox code required being based on a net-next tree, I
      keept it separate from the remainder of the RDMA stack submission that
      is based on 4.10-rc3.
    
      This branch contains:
    
       - Various mlx4 and mlx5 fixes and minor changes
    
       - Support for adding a tag match rule to flow specs
    
       - Support for cvlan offload operation for raw ethernet QPs
    
       - A change to the core IB code to recognize raw eth capabilities and
         enumerate them (touches non-Mellanox code)
    
       - Implicit On-Demand Paging memory registration support"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (40 commits)
      IB/mlx5: Fix configuration of port capabilities
      IB/mlx4: Take source GID by index from HW GID table
      IB/mlx5: Fix blue flame buffer size calculation
      IB/mlx4: Remove unused variable from function declaration
      IB: Query ports via the core instead of direct into the driver
      IB: Add protocol for USNIC
      IB/mlx4: Support raw packet protocol
      IB/mlx5: Support raw packet protocol
      IB/core: Add raw packet protocol
      IB/mlx5: Add implicit MR support
      IB/mlx5: Expose MR cache for mlx5_ib
      IB/mlx5: Add null_mkey access
      IB/umem: Indicate that process is being terminated
      IB/umem: Update on demand page (ODP) support
      IB/core: Add implicit MR flag
      IB/mlx5: Support creation of a WQ with scatter FCS offload
      IB/mlx5: Enable QP creation with cvlan offload
      IB/mlx5: Enable WQ creation and modification with cvlan offload
      IB/mlx5: Expose vlan offloads capabilities
      IB/uverbs: Enable QP creation with cvlan offload
      ...

commit 6dd7abae7110da6fa01f048baf5b679b5a4a56dd
Merge: 6df6b4a9ce43 646ebd4166ca
Author: Doug Ledford <dledford@redhat.com>
Date:   Sun Feb 19 09:18:21 2017 -0500

    Merge branch 'k.o/for-4.10-rc' into HEAD

commit ce1e055fb9aba56c80d84f63edfa102691f63d41
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Jan 24 13:02:38 2017 +0200

    IB: Add protocol for USNIC
    
    Add protocol definition for the proprietary the USNIC driver.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Christian Benvenuti <benve@cisco.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f311c2593a85..07399023352b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -490,6 +490,7 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 #define RDMA_CORE_CAP_PROT_IWARP        0x00400000
 #define RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP 0x00800000
 #define RDMA_CORE_CAP_PROT_RAW_PACKET   0x01000000
+#define RDMA_CORE_CAP_PROT_USNIC        0x02000000
 
 #define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \
 					| RDMA_CORE_CAP_IB_MAD \
@@ -515,6 +516,8 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 
 #define RDMA_CORE_PORT_RAW_PACKET	(RDMA_CORE_CAP_PROT_RAW_PACKET)
 
+#define RDMA_CORE_PORT_USNIC		(RDMA_CORE_CAP_PROT_USNIC)
+
 struct ib_port_attr {
 	u64			subnet_prefix;
 	enum ib_port_state	state;
@@ -2350,6 +2353,11 @@ static inline bool rdma_protocol_raw_packet(const struct ib_device *device, u8 p
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_RAW_PACKET;
 }
 
+static inline bool rdma_protocol_usnic(const struct ib_device *device, u8 port_num)
+{
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_USNIC;
+}
+
 /**
  * rdma_cap_ib_mad - Check if the port of a device supports Infiniband
  * Management Datagrams.

commit aa773bd49541b62274c3624f9b28b2f813ba6c90
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Jan 24 13:02:35 2017 +0200

    IB/core: Add raw packet protocol
    
    Define raw packet protocol which comes to denote this port supports
    working with raw ethernet frames, e.g as done with RAW_PACKET QPs.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 22a71397db01..f311c2593a85 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -489,6 +489,7 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 #define RDMA_CORE_CAP_PROT_ROCE         0x00200000
 #define RDMA_CORE_CAP_PROT_IWARP        0x00400000
 #define RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP 0x00800000
+#define RDMA_CORE_CAP_PROT_RAW_PACKET   0x01000000
 
 #define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \
 					| RDMA_CORE_CAP_IB_MAD \
@@ -512,6 +513,8 @@ static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
 #define RDMA_CORE_PORT_INTEL_OPA       (RDMA_CORE_PORT_IBA_IB  \
 					| RDMA_CORE_CAP_OPA_MAD)
 
+#define RDMA_CORE_PORT_RAW_PACKET	(RDMA_CORE_CAP_PROT_RAW_PACKET)
+
 struct ib_port_attr {
 	u64			subnet_prefix;
 	enum ib_port_state	state;
@@ -2342,6 +2345,11 @@ static inline bool rdma_ib_or_roce(const struct ib_device *device, u8 port_num)
 		rdma_protocol_roce(device, port_num);
 }
 
+static inline bool rdma_protocol_raw_packet(const struct ib_device *device, u8 port_num)
+{
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_RAW_PACKET;
+}
+
 /**
  * rdma_cap_ib_mad - Check if the port of a device supports Infiniband
  * Management Datagrams.

commit 25bf14d6f5898a59325f3ecabda7695565776594
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Jan 18 16:58:06 2017 +0200

    IB/core: Add implicit MR flag
    
    Add flag IB_ODP_SUPPORT_IMPLICIT indicating implicit MR supported.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 62fb9c61b354..22a71397db01 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -243,7 +243,8 @@ enum ib_atomic_cap {
 };
 
 enum ib_odp_general_cap_bits {
-	IB_ODP_SUPPORT = 1 << 0,
+	IB_ODP_SUPPORT		= 1 << 0,
+	IB_ODP_SUPPORT_IMPLICIT = 1 << 1,
 };
 
 enum ib_odp_transport_cap_bits {

commit 27b0df1175d543c46f63de6dccba98d0e5cf2276
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Jan 18 15:39:57 2017 +0200

    IB/core: Add scatter FCS flag to use in WQ creation
    
    Add a new creation flag to set the scatter FCS capability of a WQ.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 44d58510c020..62fb9c61b354 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1497,6 +1497,7 @@ struct ib_wq {
 
 enum ib_wq_flags {
 	IB_WQ_FLAGS_CVLAN_STRIPPING	= 1 << 0,
+	IB_WQ_FLAGS_SCATTER_FCS		= 1 << 1,
 };
 
 struct ib_wq_init_attr {

commit 9c2b270e69da3ea3ef76f1c14ba4ee7ca60863fd
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Jan 18 15:39:56 2017 +0200

    IB/core: Enable QP creation with cvlan offload
    
    Add a QP creation flag to support cvlan stripping, it's applicable
    for RAW Ethernet QP.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7028b0ec8447..44d58510c020 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1008,6 +1008,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
 	IB_QP_CREATE_USE_GFP_NOIO		= 1 << 7,
 	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
+	IB_QP_CREATE_CVLAN_STRIPPING		= 1 << 9,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,

commit 10bac72be10542a29169582e6f74f88bcdfdf19f
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Jan 18 15:39:55 2017 +0200

    IB/core: Enable WQ creation and modification with cvlan offload
    
    Enable WQ creation and modification with cvlan stripping offload.
    This includes:
    - Adding WQ creation flags.
    - Extending modify WQ to get flags and flags mask to enable turning
      it on and off.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 903ed76604f5..7028b0ec8447 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1494,6 +1494,10 @@ struct ib_wq {
 	atomic_t		usecnt;
 };
 
+enum ib_wq_flags {
+	IB_WQ_FLAGS_CVLAN_STRIPPING	= 1 << 0,
+};
+
 struct ib_wq_init_attr {
 	void		       *wq_context;
 	enum ib_wq_type	wq_type;
@@ -1501,16 +1505,20 @@ struct ib_wq_init_attr {
 	u32		max_sge;
 	struct	ib_cq	       *cq;
 	void		    (*event_handler)(struct ib_event *, void *);
+	u32		create_flags; /* Use enum ib_wq_flags */
 };
 
 enum ib_wq_attr_mask {
-	IB_WQ_STATE	= 1 << 0,
-	IB_WQ_CUR_STATE	= 1 << 1,
+	IB_WQ_STATE		= 1 << 0,
+	IB_WQ_CUR_STATE		= 1 << 1,
+	IB_WQ_FLAGS		= 1 << 2,
 };
 
 struct ib_wq_attr {
 	enum	ib_wq_state	wq_state;
 	enum	ib_wq_state	curr_wq_state;
+	u32			flags; /* Use enum ib_wq_flags */
+	u32			flags_mask; /* Use enum ib_wq_flags */
 };
 
 struct ib_rwq_ind_table {

commit ebaaee253ad3a3c573ab7d3d77e849056bdfa9ea
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Jan 18 15:39:54 2017 +0200

    IB/core: Expose vlan offloads capabilities
    
    Expose raw packet capabilities in the core layer to enable a device
    to report it.
    Two existing capabilities, scatter FCS and IP CSUM were added to this
    field for a better user experience by exposing the raw packet caps
    from one location.
    This field will serve also for future capabilities for raw packet QP.
    
    A new capability was introduced - cvlan stripping, which is the
    device's ability to remove cvlan tag from an incoming packet and
    report it in the matching work completion.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index da08a0440adb..903ed76604f5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -207,6 +207,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MEM_WINDOW_TYPE_2A		= (1 << 23),
 	IB_DEVICE_MEM_WINDOW_TYPE_2B		= (1 << 24),
 	IB_DEVICE_RC_IP_CSUM			= (1 << 25),
+	/* Deprecated. Please use IB_RAW_PACKET_CAP_IP_CSUM. */
 	IB_DEVICE_RAW_IP_CSUM			= (1 << 26),
 	/*
 	 * Devices should set IB_DEVICE_CROSS_CHANNEL if they
@@ -220,6 +221,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_ON_DEMAND_PAGING		= (1ULL << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 	IB_DEVICE_VIRTUAL_FUNCTION		= (1ULL << 33),
+	/* Deprecated. Please use IB_RAW_PACKET_CAP_SCATTER_FCS. */
 	IB_DEVICE_RAW_SCATTER_FCS		= (1ULL << 34),
 };
 
@@ -330,6 +332,7 @@ struct ib_device_attr {
 	uint64_t		hca_core_clock; /* in KHZ */
 	struct ib_rss_caps	rss_caps;
 	u32			max_wq_type_rq;
+	u32			raw_packet_caps; /* Use ib_raw_packet_caps enum */
 };
 
 enum ib_mtu {
@@ -1456,6 +1459,18 @@ struct ib_srq {
 	} ext;
 };
 
+enum ib_raw_packet_caps {
+	/* Strip cvlan from incoming packet and report it in the matching work
+	 * completion is supported.
+	 */
+	IB_RAW_PACKET_CAP_CVLAN_STRIPPING	= (1 << 0),
+	/* Scatter FCS field of an incoming packet to host memory is supported.
+	 */
+	IB_RAW_PACKET_CAP_SCATTER_FCS		= (1 << 1),
+	/* Checksum offloads are supported (for both send and receive). */
+	IB_RAW_PACKET_CAP_IP_CSUM		= (1 << 2),
+};
+
 enum ib_wq_type {
 	IB_WQT_RQ
 };

commit 460d019860ccb8735cf577394108270a5bc178b8
Author: Moses Reuben <mosesr@mellanox.com>
Date:   Wed Jan 18 14:59:48 2017 +0200

    IB/core: Introduce flow tag specification
    
    This specification identifies flow with a specific tag-id.
    This tag-id will be reported in the CQE.
    
    Signed-off-by: Moses Reuben <mosesr@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 958a24d8fae7..da08a0440adb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1604,6 +1604,8 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_UDP		= 0x41,
 	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
 	IB_FLOW_SPEC_INNER		= 0x100,
+	/* Actions */
+	IB_FLOW_SPEC_ACTION_TAG         = 0x1000,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
 #define IB_FLOW_SPEC_SUPPORT_LAYERS 8
@@ -1726,6 +1728,12 @@ struct ib_flow_spec_tunnel {
 	struct ib_flow_tunnel_filter  mask;
 };
 
+struct ib_flow_spec_action_tag {
+	enum ib_flow_spec_type	      type;
+	u16			      size;
+	u32                           tag_id;
+};
+
 union ib_flow_spec {
 	struct {
 		u32			type;
@@ -1737,6 +1745,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_tcp_udp	tcp_udp;
 	struct ib_flow_spec_ipv6        ipv6;
 	struct ib_flow_spec_tunnel      tunnel;
+	struct ib_flow_spec_action_tag  flow_tag;
 };
 
 struct ib_flow_attr {

commit 24dc831b77eca9361cf835be59fa69ea0e471afc
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Wed Jan 25 18:41:37 2017 +0200

    IB/core: Add inline function to validate port
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e55afec6bb84..b1ac9735fbbe 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2280,6 +2280,13 @@ static inline u8 rdma_end_port(const struct ib_device *device)
 	return rdma_cap_ib_switch(device) ? 0 : device->phys_port_cnt;
 }
 
+static inline int rdma_is_port_valid(const struct ib_device *device,
+				     unsigned int port)
+{
+	return (port >= rdma_start_port(device) &&
+		port <= rdma_end_port(device));
+}
+
 static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;

commit 21d6454a392d552c7e845f39884f7cf86f9426b9
Author: Jack Wang <jinpu.wang@profitbricks.com>
Date:   Tue Jan 17 10:11:12 2017 +0100

    RDMA/core: create struct ib_port_cache
    
    As Jason suggested, we have 4 elements for per port arrays,
    it's better to have a separate structure to represent them.
    
    It simplifies code a bit, ~ 30 lines of code less :)
    
    Signed-off-by: Jack Wang <jinpu.wang@profitbricks.com>
    Reviewed-by: Michael Wang <yun.wang@profitbricks.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fafa988e0e9a..e55afec6bb84 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1775,13 +1775,17 @@ enum ib_mad_result {
 
 #define IB_DEVICE_NAME_MAX 64
 
+struct ib_port_cache {
+	struct ib_pkey_cache  *pkey;
+	struct ib_gid_table   *gid;
+	u8                     lmc;
+	enum ib_port_state     port_state;
+};
+
 struct ib_cache {
 	rwlock_t                lock;
 	struct ib_event_handler event_handler;
-	struct ib_pkey_cache  **pkey_cache;
-	struct ib_gid_table   **gid_cache;
-	u8                     *lmc_cache;
-	enum ib_port_state     *port_state_cache;
+	struct ib_port_cache   *ports;
 };
 
 struct ib_dma_mapping_ops {

commit d3f4aadd614c4627244452ad64eaf351179f2c31
Author: Amrani, Ram <Ram.Amrani@cavium.com>
Date:   Mon Dec 26 08:40:57 2016 +0200

    RDMA/core: Add the function ib_mtu_int_to_enum
    
    As the functionality to convert the MTU from a number to enum_ib_mtu
    is ubiquitous, define a dedicated function and remove the duplicated
    code.
    
    Signed-off-by: Ram Amrani <Ram.Amrani@cavium.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 958a24d8fae7..b567e4452a47 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -352,6 +352,20 @@ static inline int ib_mtu_enum_to_int(enum ib_mtu mtu)
 	}
 }
 
+static inline enum ib_mtu ib_mtu_int_to_enum(int mtu)
+{
+	if (mtu >= 4096)
+		return IB_MTU_4096;
+	else if (mtu >= 2048)
+		return IB_MTU_2048;
+	else if (mtu >= 1024)
+		return IB_MTU_1024;
+	else if (mtu >= 512)
+		return IB_MTU_512;
+	else
+		return IB_MTU_256;
+}
+
 enum ib_port_state {
 	IB_PORT_NOP		= 0,
 	IB_PORT_DOWN		= 1,

commit 0bbb3b7496eabb6779962a998a8a91f4a8e589ff
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:37 2017 -0800

    IB/rxe, IB/rdmavt: Use dma_virt_ops instead of duplicating it
    
    Make the rxe and rdmavt drivers use dma_virt_ops. Update the
    comments that refer to the source files removed by this patch.
    Remove struct ib_dma_mapping_ops. Remove ib_device.dma_ops.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Andrew Boyer <andrew.boyer@dell.com>
    Cc: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Cc: Jonathan Toppins <jtoppins@redhat.com>
    Cc: Alex Estrin <alex.estrin@intel.com>
    Cc: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a20a15f81936..f199c42b9a86 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1784,53 +1784,6 @@ struct ib_cache {
 	enum ib_port_state     *port_state_cache;
 };
 
-struct ib_dma_mapping_ops {
-	int		(*mapping_error)(struct ib_device *dev,
-					 u64 dma_addr);
-	u64		(*map_single)(struct ib_device *dev,
-				      void *ptr, size_t size,
-				      enum dma_data_direction direction);
-	void		(*unmap_single)(struct ib_device *dev,
-					u64 addr, size_t size,
-					enum dma_data_direction direction);
-	u64		(*map_page)(struct ib_device *dev,
-				    struct page *page, unsigned long offset,
-				    size_t size,
-				    enum dma_data_direction direction);
-	void		(*unmap_page)(struct ib_device *dev,
-				      u64 addr, size_t size,
-				      enum dma_data_direction direction);
-	int		(*map_sg)(struct ib_device *dev,
-				  struct scatterlist *sg, int nents,
-				  enum dma_data_direction direction);
-	void		(*unmap_sg)(struct ib_device *dev,
-				    struct scatterlist *sg, int nents,
-				    enum dma_data_direction direction);
-	int		(*map_sg_attrs)(struct ib_device *dev,
-					struct scatterlist *sg, int nents,
-					enum dma_data_direction direction,
-					unsigned long attrs);
-	void		(*unmap_sg_attrs)(struct ib_device *dev,
-					  struct scatterlist *sg, int nents,
-					  enum dma_data_direction direction,
-					  unsigned long attrs);
-	void		(*sync_single_for_cpu)(struct ib_device *dev,
-					       u64 dma_handle,
-					       size_t size,
-					       enum dma_data_direction dir);
-	void		(*sync_single_for_device)(struct ib_device *dev,
-						  u64 dma_handle,
-						  size_t size,
-						  enum dma_data_direction dir);
-	void		*(*alloc_coherent)(struct ib_device *dev,
-					   size_t size,
-					   u64 *dma_handle,
-					   gfp_t flag);
-	void		(*free_coherent)(struct ib_device *dev,
-					 size_t size, void *cpu_addr,
-					 u64 dma_handle);
-};
-
 struct iw_cm_verbs;
 
 struct ib_port_immutable {
@@ -2090,7 +2043,6 @@ struct ib_device {
 							   struct ib_rwq_ind_table_init_attr *init_attr,
 							   struct ib_udata *udata);
 	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
-	struct ib_dma_mapping_ops   *dma_ops;
 
 	struct module               *owner;
 	struct device                dev;
@@ -2965,8 +2917,6 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
  */
 static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->mapping_error(dev, dma_addr);
 	return dma_mapping_error(&dev->dev, dma_addr);
 }
 
@@ -2981,8 +2931,6 @@ static inline u64 ib_dma_map_single(struct ib_device *dev,
 				    void *cpu_addr, size_t size,
 				    enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->map_single(dev, cpu_addr, size, direction);
 	return dma_map_single(&dev->dev, cpu_addr, size, direction);
 }
 
@@ -2997,10 +2945,7 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 				       u64 addr, size_t size,
 				       enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->unmap_single(dev, addr, size, direction);
-	else
-		dma_unmap_single(&dev->dev, addr, size, direction);
+	dma_unmap_single(&dev->dev, addr, size, direction);
 }
 
 /**
@@ -3017,8 +2962,6 @@ static inline u64 ib_dma_map_page(struct ib_device *dev,
 				  size_t size,
 					 enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->map_page(dev, page, offset, size, direction);
 	return dma_map_page(&dev->dev, page, offset, size, direction);
 }
 
@@ -3033,10 +2976,7 @@ static inline void ib_dma_unmap_page(struct ib_device *dev,
 				     u64 addr, size_t size,
 				     enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->unmap_page(dev, addr, size, direction);
-	else
-		dma_unmap_page(&dev->dev, addr, size, direction);
+	dma_unmap_page(&dev->dev, addr, size, direction);
 }
 
 /**
@@ -3050,8 +2990,6 @@ static inline int ib_dma_map_sg(struct ib_device *dev,
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->map_sg(dev, sg, nents, direction);
 	return dma_map_sg(&dev->dev, sg, nents, direction);
 }
 
@@ -3066,10 +3004,7 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction direction)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->unmap_sg(dev, sg, nents, direction);
-	else
-		dma_unmap_sg(&dev->dev, sg, nents, direction);
+	dma_unmap_sg(&dev->dev, sg, nents, direction);
 }
 
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
@@ -3077,9 +3012,6 @@ static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      enum dma_data_direction direction,
 				      unsigned long dma_attrs)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->map_sg_attrs(dev, sg, nents, direction,
-						  dma_attrs);
 	return dma_map_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
 }
 
@@ -3088,11 +3020,7 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 enum dma_data_direction direction,
 					 unsigned long dma_attrs)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->unmap_sg_attrs(dev, sg, nents, direction,
-						  dma_attrs);
-	else
-		dma_unmap_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
+	dma_unmap_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
 }
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
@@ -3134,10 +3062,7 @@ static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
 					      size_t size,
 					      enum dma_data_direction dir)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir);
-	else
-		dma_sync_single_for_cpu(&dev->dev, addr, size, dir);
+	dma_sync_single_for_cpu(&dev->dev, addr, size, dir);
 }
 
 /**
@@ -3152,10 +3077,7 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
 						 size_t size,
 						 enum dma_data_direction dir)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->sync_single_for_device(dev, addr, size, dir);
-	else
-		dma_sync_single_for_device(&dev->dev, addr, size, dir);
+	dma_sync_single_for_device(&dev->dev, addr, size, dir);
 }
 
 /**
@@ -3170,14 +3092,6 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 					   dma_addr_t *dma_handle,
 					   gfp_t flag)
 {
-	if (dev->dma_ops) {
-		u64 handle;
-		void *ret;
-
-		ret = dev->dma_ops->alloc_coherent(dev, size, &handle, flag);
-		*dma_handle = handle;
-		return ret;
-	}
 	return dma_alloc_coherent(&dev->dev, size, dma_handle, flag);
 }
 
@@ -3192,10 +3106,7 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 					size_t size, void *cpu_addr,
 					dma_addr_t dma_handle)
 {
-	if (dev->dma_ops)
-		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle);
-	else
-		dma_free_coherent(&dev->dev, size, cpu_addr, dma_handle);
+	dma_free_coherent(&dev->dev, size, cpu_addr, dma_handle);
 }
 
 /**

commit 99db9494035f5b9fbb1d579f89c6fa1beba6dbb7
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:36 2017 -0800

    IB/core: Remove ib_device.dma_device
    
    Add code in ib_register_device() for copying the DMA masks. Use
    &ib_device.dev in DMA mapping operations instead of dma_device.
    Remove ib_device.dma_device because due to this and previous patches
    it is no longer used.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 694e39e4f1ff..a20a15f81936 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1841,8 +1841,6 @@ struct ib_port_immutable {
 };
 
 struct ib_device {
-	struct device                *dma_device;
-
 	char                          name[IB_DEVICE_NAME_MAX];
 
 	struct list_head              event_handler_list;
@@ -2969,7 +2967,7 @@ static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->mapping_error(dev, dma_addr);
-	return dma_mapping_error(dev->dma_device, dma_addr);
+	return dma_mapping_error(&dev->dev, dma_addr);
 }
 
 /**
@@ -2985,7 +2983,7 @@ static inline u64 ib_dma_map_single(struct ib_device *dev,
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->map_single(dev, cpu_addr, size, direction);
-	return dma_map_single(dev->dma_device, cpu_addr, size, direction);
+	return dma_map_single(&dev->dev, cpu_addr, size, direction);
 }
 
 /**
@@ -3002,7 +3000,7 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->unmap_single(dev, addr, size, direction);
 	else
-		dma_unmap_single(dev->dma_device, addr, size, direction);
+		dma_unmap_single(&dev->dev, addr, size, direction);
 }
 
 /**
@@ -3021,7 +3019,7 @@ static inline u64 ib_dma_map_page(struct ib_device *dev,
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->map_page(dev, page, offset, size, direction);
-	return dma_map_page(dev->dma_device, page, offset, size, direction);
+	return dma_map_page(&dev->dev, page, offset, size, direction);
 }
 
 /**
@@ -3038,7 +3036,7 @@ static inline void ib_dma_unmap_page(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->unmap_page(dev, addr, size, direction);
 	else
-		dma_unmap_page(dev->dma_device, addr, size, direction);
+		dma_unmap_page(&dev->dev, addr, size, direction);
 }
 
 /**
@@ -3054,7 +3052,7 @@ static inline int ib_dma_map_sg(struct ib_device *dev,
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->map_sg(dev, sg, nents, direction);
-	return dma_map_sg(dev->dma_device, sg, nents, direction);
+	return dma_map_sg(&dev->dev, sg, nents, direction);
 }
 
 /**
@@ -3071,7 +3069,7 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->unmap_sg(dev, sg, nents, direction);
 	else
-		dma_unmap_sg(dev->dma_device, sg, nents, direction);
+		dma_unmap_sg(&dev->dev, sg, nents, direction);
 }
 
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
@@ -3082,9 +3080,7 @@ static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 	if (dev->dma_ops)
 		return dev->dma_ops->map_sg_attrs(dev, sg, nents, direction,
 						  dma_attrs);
-	else
-		return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
-					dma_attrs);
+	return dma_map_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
 }
 
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
@@ -3096,8 +3092,7 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 		return dev->dma_ops->unmap_sg_attrs(dev, sg, nents, direction,
 						  dma_attrs);
 	else
-		dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction,
-				   dma_attrs);
+		dma_unmap_sg_attrs(&dev->dev, sg, nents, direction, dma_attrs);
 }
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
@@ -3142,7 +3137,7 @@ static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir);
 	else
-		dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
+		dma_sync_single_for_cpu(&dev->dev, addr, size, dir);
 }
 
 /**
@@ -3160,7 +3155,7 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->sync_single_for_device(dev, addr, size, dir);
 	else
-		dma_sync_single_for_device(dev->dma_device, addr, size, dir);
+		dma_sync_single_for_device(&dev->dev, addr, size, dir);
 }
 
 /**
@@ -3183,7 +3178,7 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 		*dma_handle = handle;
 		return ret;
 	}
-	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
+	return dma_alloc_coherent(&dev->dev, size, dma_handle, flag);
 }
 
 /**
@@ -3200,7 +3195,7 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 	if (dev->dma_ops)
 		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle);
 	else
-		dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
+		dma_free_coherent(&dev->dev, size, cpu_addr, dma_handle);
 }
 
 /**

commit d43dbacfc06349309a99e50c575d86bc36ca4178
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:10 2017 -0800

    IB/core: Change the type of an ib_dma_alloc_coherent() argument
    
    Change the type of the dma_handle argument from u64 * to dma_addr_t *.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c2f2f54dbd49..694e39e4f1ff 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3172,19 +3172,18 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
  */
 static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 					   size_t size,
-					   u64 *dma_handle,
+					   dma_addr_t *dma_handle,
 					   gfp_t flag)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->alloc_coherent(dev, size, dma_handle, flag);
-	else {
-		dma_addr_t handle;
+	if (dev->dma_ops) {
+		u64 handle;
 		void *ret;
 
-		ret = dma_alloc_coherent(dev->dma_device, size, &handle, flag);
+		ret = dev->dma_ops->alloc_coherent(dev, size, &handle, flag);
 		*dma_handle = handle;
 		return ret;
 	}
+	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
 }
 
 /**
@@ -3196,7 +3195,7 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
  */
 static inline void ib_dma_free_coherent(struct ib_device *dev,
 					size_t size, void *cpu_addr,
-					u64 dma_handle)
+					dma_addr_t dma_handle)
 {
 	if (dev->dma_ops)
 		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle);

commit 6532c380bf4099e81e55e0c8c14a13232ead08c0
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:08 2017 -0800

    IB/core: Remove ib_dma_*map_single_attrs()
    
    Remove these functions because these are not used. Additionally, the
    implementation of these functions is not correct for the hfi1, qib and
    rxe drivers because dma_device is used instead of dma_ops.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fafa988e0e9a..c2f2f54dbd49 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3005,24 +3005,6 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 		dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
-static inline u64 ib_dma_map_single_attrs(struct ib_device *dev,
-					  void *cpu_addr, size_t size,
-					  enum dma_data_direction direction,
-					  unsigned long dma_attrs)
-{
-	return dma_map_single_attrs(dev->dma_device, cpu_addr, size,
-				    direction, dma_attrs);
-}
-
-static inline void ib_dma_unmap_single_attrs(struct ib_device *dev,
-					     u64 addr, size_t size,
-					     enum dma_data_direction direction,
-					     unsigned long dma_attrs)
-{
-	return dma_unmap_single_attrs(dev->dma_device, addr, size,
-				      direction, dma_attrs);
-}
-
 /**
  * ib_dma_map_page - Map a physical page to DMA address
  * @dev: The device for which the dma_addr is to be created

commit aaaca121c7cf9217ab81d9db0a04835d52aabebe
Author: Jack Wang <jinpu.wang@profitbricks.com>
Date:   Mon Jan 2 13:17:36 2017 +0100

    RDMA/core: add port state cache
    
    We need a port state cache in ib_core, later we will use in rdma_cm.
    
    Signed-off-by: Jack Wang <jinpu.wang@profitbricks.com>
    Reviewed-by: Michael Wang <yun.wang@profitbricks.com>
    Acked-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 958a24d8fae7..fafa988e0e9a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1781,6 +1781,7 @@ struct ib_cache {
 	struct ib_pkey_cache  **pkey_cache;
 	struct ib_gid_table   **gid_cache;
 	u8                     *lmc_cache;
+	enum ib_port_state     *port_state_cache;
 };
 
 struct ib_dma_mapping_ops {

commit 43579b5f2c79d747d8294bd233db41c954e2dc4a
Author: Parav Pandit <pandit.parav@gmail.com>
Date:   Tue Jan 10 00:02:14 2017 +0000

    IB/core: added support to use rdma cgroup controller
    
    Added support APIs for IB core to register/unregister every IB/RDMA
    device with rdma cgroup for tracking rdma resources.
    IB core registers with rdma cgroup controller.
    Added support APIs for uverbs layer to make use of rdma controller.
    Added uverbs layer to perform resource charge/uncharge functionality.
    Added support during query_device uverb operation to ensure it
    returns resource limits by honoring rdma cgroup configured limits.
    
    Signed-off-by: Parav Pandit <pandit.parav@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 958a24d8fae7..63896a477896 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -60,6 +60,7 @@
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
 #include <linux/uaccess.h>
+#include <linux/cgroup_rdma.h>
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;
@@ -1331,6 +1332,12 @@ struct ib_fmr_attr {
 
 struct ib_umem;
 
+struct ib_rdmacg_object {
+#ifdef CONFIG_CGROUP_RDMA
+	struct rdma_cgroup	*cg;		/* owner rdma cgroup */
+#endif
+};
+
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct list_head	pd_list;
@@ -1363,6 +1370,8 @@ struct ib_ucontext {
 	struct list_head	no_private_counters;
 	int                     odp_mrs_count;
 #endif
+
+	struct ib_rdmacg_object	cg_obj;
 };
 
 struct ib_uobject {
@@ -1370,6 +1379,7 @@ struct ib_uobject {
 	struct ib_ucontext     *context;	/* associated user context */
 	void		       *object;		/* containing object */
 	struct list_head	list;		/* link to context's list */
+	struct ib_rdmacg_object	cg_obj;		/* rdmacg object */
 	int			id;		/* index into kernel idr */
 	struct kref		ref;
 	struct rw_semaphore	mutex;		/* protects .live */
@@ -2118,6 +2128,10 @@ struct ib_device {
 	struct attribute_group	     *hw_stats_ag;
 	struct rdma_hw_stats         *hw_stats;
 
+#ifdef CONFIG_CGROUP_RDMA
+	struct rdmacg_device         cg_device;
+#endif
+
 	/**
 	 * The following mandatory functions are used only at device
 	 * registration.  Keep functions such as these at the end of this

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8029d2a51f14..958a24d8fae7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -59,7 +59,7 @@
 #include <linux/if_link.h>
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 
 extern struct workqueue_struct *ib_wq;
 extern struct workqueue_struct *ib_comp_wq;

commit 528e5a1bd3f0e9b760cb3a1062fce7513712a15d
Author: Bodong Wang <bodong@mellanox.com>
Date:   Thu Dec 1 13:43:14 2016 +0200

    IB/core: Support rate limit for packet pacing
    
    Add new member rate_limit to ib_qp_attr which holds the packet pacing rate
    in kbps, 0 means unlimited.
    
    IB_QP_RATE_LIMIT is added to ib_attr_mask and could be used by RAW
    QPs when changing QP state from RTR to RTS, RTS to RTS.
    
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73417a22ee4d..8029d2a51f14 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1102,6 +1102,7 @@ enum ib_qp_attr_mask {
 	IB_QP_RESERVED2			= (1<<22),
 	IB_QP_RESERVED3			= (1<<23),
 	IB_QP_RESERVED4			= (1<<24),
+	IB_QP_RATE_LIMIT		= (1<<25),
 };
 
 enum ib_qp_state {
@@ -1151,6 +1152,7 @@ struct ib_qp_attr {
 	u8			rnr_retry;
 	u8			alt_port_num;
 	u8			alt_timeout;
+	u32			rate_limit;
 };
 
 enum ib_wr_opcode {

commit 477864c8fcd953e5a988073ca5be18bb7fd93410
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Nov 23 08:23:24 2016 +0200

    IB/core: Let create_ah return extended response to user
    
    Add struct ib_udata to the signature of create_ah callback that is
    implemented by IB device drivers. This allows HW drivers to return extra
    data to the userspace library.
    This patch prepares the ground for mlx5 driver to resolve destination
    mac address for a given GID and return it to userspace.
    This patch was previously submitted by Knut Omang as a part of the
    patch set to support Oracle's Infiniband HCA (SIF).
    
    Signed-off-by: Knut Omang <knut.omang@oracle.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0c6f973e407c..73417a22ee4d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1951,7 +1951,8 @@ struct ib_device {
 					       struct ib_udata *udata);
 	int                        (*dealloc_pd)(struct ib_pd *pd);
 	struct ib_ah *             (*create_ah)(struct ib_pd *pd,
-						struct ib_ah_attr *ah_attr);
+						struct ib_ah_attr *ah_attr,
+						struct ib_udata *udata);
 	int                        (*modify_ah)(struct ib_ah *ah,
 						struct ib_ah_attr *ah_attr);
 	int                        (*query_ah)(struct ib_ah *ah,

commit c90ea9d8e51196d9c528e57d9ab09ee7d41f0ba0
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Nov 23 08:23:22 2016 +0200

    IB/core: Change ib_resolve_eth_dmac to use it in create AH
    
    The function ib_resolve_eth_dmac() requires struct qp_attr * and
    qp_attr_mask as parameters while the function might be useful to resolve
    dmac for address handles. This patch changes the signature of the
    function so it can be used in the flow of creating an address handle.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6d0dd6525e14..0c6f973e407c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3394,4 +3394,6 @@ void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);
 void ib_drain_qp(struct ib_qp *qp);
 
+int ib_resolve_eth_dmac(struct ib_device *device,
+			struct ib_ah_attr *ah_attr);
 #endif /* IB_VERBS_H */

commit fbf46860b19ddb485f00bef1ad1a43aabc9f71ad
Author: Moses Reuben <mosesr@mellanox.com>
Date:   Mon Nov 14 19:04:51 2016 +0200

    IB/core: Introduce inner flow steering
    
    For a tunneled packet which contains external and internal headers,
    we refer to the external headers as "outer fields" and the internal
    headers as "inner fields".
    
    Example of a tunneled packet:
    
    { L2 | L3 | L4 | tunnel header | L2 | L3 | l4 | data }
      |     |    |         |         |    |    |
    {       outer fields           }{ inner fields }
    
    This patch introduces a new flag for flow steering rules
    - IB_FLOW_SPEC_INNER - which specifies that the rule applies
    to the inner fields, rather than to the outer fields of the packet.
    
    Signed-off-by: Moses Reuben <mosesr@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4bc748fddcac..6d0dd6525e14 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1601,9 +1601,10 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_TCP		= 0x40,
 	IB_FLOW_SPEC_UDP		= 0x41,
 	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
+	IB_FLOW_SPEC_INNER		= 0x100,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
-#define IB_FLOW_SPEC_SUPPORT_LAYERS 4
+#define IB_FLOW_SPEC_SUPPORT_LAYERS 8
 
 /* Flow steering rule priority is set according to it's domain.
  * Lower domain value means higher priority.
@@ -1631,7 +1632,7 @@ struct ib_flow_eth_filter {
 };
 
 struct ib_flow_spec_eth {
-	enum ib_flow_spec_type	  type;
+	u32			  type;
 	u16			  size;
 	struct ib_flow_eth_filter val;
 	struct ib_flow_eth_filter mask;
@@ -1645,7 +1646,7 @@ struct ib_flow_ib_filter {
 };
 
 struct ib_flow_spec_ib {
-	enum ib_flow_spec_type	 type;
+	u32			 type;
 	u16			 size;
 	struct ib_flow_ib_filter val;
 	struct ib_flow_ib_filter mask;
@@ -1670,7 +1671,7 @@ struct ib_flow_ipv4_filter {
 };
 
 struct ib_flow_spec_ipv4 {
-	enum ib_flow_spec_type	   type;
+	u32			   type;
 	u16			   size;
 	struct ib_flow_ipv4_filter val;
 	struct ib_flow_ipv4_filter mask;
@@ -1688,7 +1689,7 @@ struct ib_flow_ipv6_filter {
 };
 
 struct ib_flow_spec_ipv6 {
-	enum ib_flow_spec_type	   type;
+	u32			   type;
 	u16			   size;
 	struct ib_flow_ipv6_filter val;
 	struct ib_flow_ipv6_filter mask;
@@ -1702,7 +1703,7 @@ struct ib_flow_tcp_udp_filter {
 };
 
 struct ib_flow_spec_tcp_udp {
-	enum ib_flow_spec_type	      type;
+	u32			      type;
 	u16			      size;
 	struct ib_flow_tcp_udp_filter val;
 	struct ib_flow_tcp_udp_filter mask;
@@ -1717,7 +1718,7 @@ struct ib_flow_tunnel_filter {
  * the tunnel_id from val has the vni value
  */
 struct ib_flow_spec_tunnel {
-	enum ib_flow_spec_type	      type;
+	u32			      type;
 	u16			      size;
 	struct ib_flow_tunnel_filter  val;
 	struct ib_flow_tunnel_filter  mask;
@@ -1725,7 +1726,7 @@ struct ib_flow_spec_tunnel {
 
 union ib_flow_spec {
 	struct {
-		enum ib_flow_spec_type	type;
+		u32			type;
 		u16			size;
 	};
 	struct ib_flow_spec_eth		eth;

commit 76bd23b34204cad78f48aec4cef38869a66aa594
Author: Moses Reuben <mosesr@mellanox.com>
Date:   Mon Nov 14 19:04:48 2016 +0200

    IB/core: Align structure ib_flow_spec_type
    
    Aligned the structure ib_flow_spec_type indentation,
    after adding a new definition.
    
    Signed-off-by: Moses Reuben <mosesr@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b8213818de89..4bc748fddcac 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1592,14 +1592,14 @@ enum ib_flow_attr_type {
 /* Supported steering header types */
 enum ib_flow_spec_type {
 	/* L2 headers*/
-	IB_FLOW_SPEC_ETH	= 0x20,
-	IB_FLOW_SPEC_IB		= 0x22,
+	IB_FLOW_SPEC_ETH		= 0x20,
+	IB_FLOW_SPEC_IB			= 0x22,
 	/* L3 header*/
-	IB_FLOW_SPEC_IPV4	= 0x30,
-	IB_FLOW_SPEC_IPV6	= 0x31,
+	IB_FLOW_SPEC_IPV4		= 0x30,
+	IB_FLOW_SPEC_IPV6		= 0x31,
 	/* L4 headers*/
-	IB_FLOW_SPEC_TCP	= 0x40,
-	IB_FLOW_SPEC_UDP	= 0x41,
+	IB_FLOW_SPEC_TCP		= 0x40,
+	IB_FLOW_SPEC_UDP		= 0x41,
 	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0

commit 0dbf3332b7b683db33a385a3ce9baab157e3ff9a
Author: Moses Reuben <mosesr@mellanox.com>
Date:   Mon Nov 14 19:04:47 2016 +0200

    IB/core: Add flow spec tunneling support
    
    In order to support tunneling, that can be used by the QP,
    both struct ib_flow_spec_tunnel and struct ib_flow_tunnel_filter can be
    used to more IP or UDP based tunneling protocols (e.g NVGRE, GRE, etc).
    
    IB_FLOW_SPEC_VXLAN_TUNNEL type flow specification is added to use this
    functionality and match specific Vxlan packets.
    
    In similar to IPv6, we check overflow of the vni value by
    comparing with the maximum size.
    
    Signed-off-by: Moses Reuben <mosesr@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 467a4b476e29..b8213818de89 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1599,7 +1599,8 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_IPV6	= 0x31,
 	/* L4 headers*/
 	IB_FLOW_SPEC_TCP	= 0x40,
-	IB_FLOW_SPEC_UDP	= 0x41
+	IB_FLOW_SPEC_UDP	= 0x41,
+	IB_FLOW_SPEC_VXLAN_TUNNEL	= 0x50,
 };
 #define IB_FLOW_SPEC_LAYER_MASK	0xF0
 #define IB_FLOW_SPEC_SUPPORT_LAYERS 4
@@ -1707,6 +1708,21 @@ struct ib_flow_spec_tcp_udp {
 	struct ib_flow_tcp_udp_filter mask;
 };
 
+struct ib_flow_tunnel_filter {
+	__be32	tunnel_id;
+	u8	real_sz[0];
+};
+
+/* ib_flow_spec_tunnel describes the Vxlan tunnel
+ * the tunnel_id from val has the vni value
+ */
+struct ib_flow_spec_tunnel {
+	enum ib_flow_spec_type	      type;
+	u16			      size;
+	struct ib_flow_tunnel_filter  val;
+	struct ib_flow_tunnel_filter  mask;
+};
+
 union ib_flow_spec {
 	struct {
 		enum ib_flow_spec_type	type;
@@ -1717,6 +1733,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_ipv4        ipv4;
 	struct ib_flow_spec_tcp_udp	tcp_udp;
 	struct ib_flow_spec_ipv6        ipv6;
+	struct ib_flow_spec_tunnel      tunnel;
 };
 
 struct ib_flow_attr {

commit 850d8fd765079d223d47de89f1f03ab95d1036cb
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Nov 10 11:30:56 2016 +0200

    IB/mlx4: Handle IPv4 header when demultiplexing MAD
    
    When MAD arrives to the hypervisor, we need to identify which slave it
    should be sent by destination GID. When L3 protocol is IPv4 the
    GRH is replaced by an IPv4 header. This patch detects when IPv4 header
    needs to be parsed instead of GRH.
    
    Fixes: b6ffaeffaea4 ('mlx4: In RoCE allow guests to have multiple GIDS')
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5ad43a487745..467a4b476e29 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2580,6 +2580,24 @@ void ib_dealloc_pd(struct ib_pd *pd);
  */
 struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
 
+/**
+ * ib_get_gids_from_rdma_hdr - Get sgid and dgid from GRH or IPv4 header
+ *   work completion.
+ * @hdr: the L3 header to parse
+ * @net_type: type of header to parse
+ * @sgid: place to store source gid
+ * @dgid: place to store destination gid
+ */
+int ib_get_gids_from_rdma_hdr(const union rdma_network_hdr *hdr,
+			      enum rdma_network_type net_type,
+			      union ib_gid *sgid, union ib_gid *dgid);
+
+/**
+ * ib_get_rdma_header_version - Get the header version
+ * @hdr: the L3 header to parse
+ */
+int ib_get_rdma_header_version(const union rdma_network_hdr *hdr);
+
 /**
  * ib_init_ah_from_wc - Initializes address handle attributes from a
  *   work completion.
@@ -3357,4 +3375,5 @@ int ib_sg_to_pages(struct ib_mr *mr, struct scatterlist *sgl, int sg_nents,
 void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);
 void ib_drain_qp(struct ib_qp *qp);
+
 #endif /* IB_VERBS_H */

commit b9044ac8292fc94bee33f6f08acaed3ac55f0c75
Merge: 1fde76f173e4 2937f3757519
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:04:33 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull main rdma updates from Doug Ledford:
     "This is the main pull request for the rdma stack this release.  The
      code has been through 0day and I had it tagged for linux-next testing
      for a couple days.
    
      Summary:
    
       - updates to mlx5
    
       - updates to mlx4 (two conflicts, both minor and easily resolved)
    
       - updates to iw_cxgb4 (one conflict, not so obvious to resolve,
         proper resolution is to keep the code in cxgb4_main.c as it is in
         Linus' tree as attach_uld was refactored and moved into
         cxgb4_uld.c)
    
       - improvements to uAPI (moved vendor specific API elements to uAPI
         area)
    
       - add hns-roce driver and hns and hns-roce ACPI reset support
    
       - conversion of all rdma code away from deprecated
         create_singlethread_workqueue
    
       - security improvement: remove unsafe ib_get_dma_mr (breaks lustre in
         staging)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (75 commits)
      staging/lustre: Disable InfiniBand support
      iw_cxgb4: add fast-path for small REG_MR operations
      cxgb4: advertise support for FR_NSMR_TPTE_WR
      IB/core: correctly handle rdma_rw_init_mrs() failure
      IB/srp: Fix infinite loop when FMR sg[0].offset != 0
      IB/srp: Remove an unused argument
      IB/core: Improve ib_map_mr_sg() documentation
      IB/mlx4: Fix possible vl/sl field mismatch in LRH header in QP1 packets
      IB/mthca: Move user vendor structures
      IB/nes: Move user vendor structures
      IB/ocrdma: Move user vendor structures
      IB/mlx4: Move user vendor structures
      IB/cxgb4: Move user vendor structures
      IB/cxgb3: Move user vendor structures
      IB/mlx5: Move and decouple user vendor structures
      IB/{core,hw}: Add constant for node_desc
      ipoib: Make ipoib_warn ratelimited
      IB/mlx4/alias_GUID: Remove deprecated create_singlethread_workqueue
      IB/ipoib_verbs: Remove deprecated create_singlethread_workqueue
      IB/ipoib: Remove deprecated create_singlethread_workqueue
      ...

commit bd99fdea420b00925e9b83a50f2ccc5e1f07ef7d
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Thu Aug 25 10:57:07 2016 -0700

    IB/{core,hw}: Add constant for node_desc
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0cec4da51eb7..d3fba0a56e17 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -537,9 +537,11 @@ enum ib_device_modify_flags {
 	IB_DEVICE_MODIFY_NODE_DESC	= 1 << 1
 };
 
+#define IB_DEVICE_NODE_DESC_MAX 64
+
 struct ib_device_modify {
 	u64	sys_image_guid;
-	char	node_desc[64];
+	char	node_desc[IB_DEVICE_NODE_DESC_MAX];
 };
 
 enum ib_port_modify_flags {
@@ -2077,7 +2079,7 @@ struct ib_device {
 	u64			     uverbs_cmd_mask;
 	u64			     uverbs_ex_cmd_mask;
 
-	char			     node_desc[64];
+	char			     node_desc[IB_DEVICE_NODE_DESC_MAX];
 	__be64			     node_guid;
 	u32			     local_dma_lkey;
 	u16                          is_switch:1;

commit a72c6a2b0e699fcbcf679b881d5af2683228ae98
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue Aug 30 16:58:34 2016 +0300

    IB/core: Add more fields to IPv6 flow specification
    
    Add the following fields to IPv6 flow filter specification:
    1. Traffic Class
    2. Flow Label
    3. Next Header
    4. Hop Limit
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 990220b757f0..0cec4da51eb7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1676,6 +1676,10 @@ struct ib_flow_spec_ipv4 {
 struct ib_flow_ipv6_filter {
 	u8	src_ip[16];
 	u8	dst_ip[16];
+	__be32	flow_label;
+	u8	next_hdr;
+	u8	traffic_class;
+	u8	hop_limit;
 	/* Must be last */
 	u8	real_sz[0];
 };

commit 989a3a8f91ba02f71b84ecde3b948388d8bf2200
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue Aug 30 16:58:33 2016 +0300

    IB/uverbs: Add more fields to IPv4 flow specification
    
    Add the following fields to IPv4 flow filter specification:
    1. Type of Service
    2. Time to Live
    3. Flags
    4. Protocol
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4570d8844f63..990220b757f0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1648,9 +1648,20 @@ struct ib_flow_spec_ib {
 	struct ib_flow_ib_filter mask;
 };
 
+/* IPv4 header flags */
+enum ib_ipv4_flags {
+	IB_IPV4_DONT_FRAG = 0x2, /* Don't enable packet fragmentation */
+	IB_IPV4_MORE_FRAG = 0X4  /* For All fragmented packets except the
+				    last have this flag set */
+};
+
 struct ib_flow_ipv4_filter {
 	__be32	src_ip;
 	__be32	dst_ip;
+	u8	proto;
+	u8	tos;
+	u8	ttl;
+	u8	flags;
 	/* Must be last */
 	u8	real_sz[0];
 };

commit 15dfbd6b4f058571f41be5b7917465dab2ff55da
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue Aug 30 16:58:32 2016 +0300

    IB/uverbs: Add support to extend flow steering specifications
    
    Flow steering specifications structures were implemented as in an
    extensible way that allows one to add new filters and new fields
    to existing filters.
    These specifications have never been extended, therefore the
    kernel flow specifications size and the user flow specifications size
    were must to be equal.
    
    In downstream patch, the IPv4 flow specifications type is extended to
    support TOS and TTL fields.
    
    To support an extension we change the flow specifications size
    condition test to be as following:
    
    * If the user flow specifications is bigger than the kernel
    specifications, we verify that all the bits which not in the kernel
    specifications are zeros and the flow is added only with the kernel
    specifications fields.
    
    * Otherwise, we add flow rule only with the user specifications fields.
    
    User space filters must be aligned with 32bits.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d74c76bf76d3..4570d8844f63 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1623,6 +1623,8 @@ struct ib_flow_eth_filter {
 	u8	src_mac[6];
 	__be16	ether_type;
 	__be16	vlan_tag;
+	/* Must be last */
+	u8	real_sz[0];
 };
 
 struct ib_flow_spec_eth {
@@ -1635,6 +1637,8 @@ struct ib_flow_spec_eth {
 struct ib_flow_ib_filter {
 	__be16 dlid;
 	__u8   sl;
+	/* Must be last */
+	u8	real_sz[0];
 };
 
 struct ib_flow_spec_ib {
@@ -1647,6 +1651,8 @@ struct ib_flow_spec_ib {
 struct ib_flow_ipv4_filter {
 	__be32	src_ip;
 	__be32	dst_ip;
+	/* Must be last */
+	u8	real_sz[0];
 };
 
 struct ib_flow_spec_ipv4 {
@@ -1659,6 +1665,8 @@ struct ib_flow_spec_ipv4 {
 struct ib_flow_ipv6_filter {
 	u8	src_ip[16];
 	u8	dst_ip[16];
+	/* Must be last */
+	u8	real_sz[0];
 };
 
 struct ib_flow_spec_ipv6 {
@@ -1671,6 +1679,8 @@ struct ib_flow_spec_ipv6 {
 struct ib_flow_tcp_udp_filter {
 	__be16	dst_port;
 	__be16	src_port;
+	/* Must be last */
+	u8	real_sz[0];
 };
 
 struct ib_flow_spec_tcp_udp {

commit ccf20562601d2b911787ffa730ad96bb78269a9c
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Aug 28 11:28:43 2016 +0300

    IB/core: Expose RSS related capabilities
    
    Expose RSS related capabilities, it includes both direct ones (i.e.
    struct ib_rss_caps) and max_wq_type_rq which may be used in both
    RSS and non RSS flows.
    
    Specifically,
    supported_qpts:
    - QP types that support RSS on the device.
    
    max_rwq_indirection_tables:
    - Max number of receive work queue indirection tables that
      could be opened on the device.
    
    max_rwq_indirection_table_size:
    - Max size of a receive work queue indirection table.
    
    max_wq_type_rq:
    - Max number of work queues of receive type that
      could be opened on the device.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0a6c70895c8b..d74c76bf76d3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -261,6 +261,16 @@ struct ib_odp_caps {
 	} per_transport_caps;
 };
 
+struct ib_rss_caps {
+	/* Corresponding bit will be set if qp type from
+	 * 'enum ib_qp_type' is supported, e.g.
+	 * supported_qpts |= 1 << IB_QPT_UD
+	 */
+	u32 supported_qpts;
+	u32 max_rwq_indirection_tables;
+	u32 max_rwq_indirection_table_size;
+};
+
 enum ib_cq_creation_flags {
 	IB_CQ_FLAGS_TIMESTAMP_COMPLETION   = 1 << 0,
 	IB_CQ_FLAGS_IGNORE_OVERRUN	   = 1 << 1,
@@ -318,6 +328,8 @@ struct ib_device_attr {
 	struct ib_odp_caps	odp_caps;
 	uint64_t		timestamp_mask;
 	uint64_t		hca_core_clock; /* in KHZ */
+	struct ib_rss_caps	rss_caps;
+	u32			max_wq_type_rq;
 };
 
 enum ib_mtu {

commit d9703650f4aba7555fde92636d8d9a689029e8f8
Author: Parav Pandit <pandit.parav@gmail.com>
Date:   Wed Sep 28 20:25:47 2016 +0000

    IB/{rxe,core,rdmavt}: Fix kernel crash for reg MR
    
    This patch fixes below kernel crash on memory registration for rxe
    and other transport drivers which has dma_ops extension.
    
    IB/core invokes ib_map_sg_attrs() in generic manner with dma attributes
    which is used by mlx5 and mthca adapters.  However in doing so it
    ignored honoring dma_ops extension of software based transports for
    sg map/unmap operation.  This results in calling dma_map_sg_attrs of
    hardware virtual device resulting in crash for null reference.
    
    We extend the core to support sg_map/unmap_attrs and transport drivers
    to implement those dma_ops callback functions.
    
    Verified usign perftest applications.
    
    BUG: unable to handle kernel NULL pointer dereference at           (null)
    IP: [<ffffffff81032a75>] check_addr+0x35/0x60
    ...
    Call Trace:
     [<ffffffff81032b39>] ? nommu_map_sg+0x99/0xd0
     [<ffffffffa02b31c6>] ib_umem_get+0x3d6/0x470 [ib_core]
     [<ffffffffa01cc329>] rxe_mem_init_user+0x49/0x270 [rdma_rxe]
     [<ffffffffa01c793a>] ? rxe_add_index+0xca/0x100 [rdma_rxe]
     [<ffffffffa01c995f>] rxe_reg_user_mr+0x9f/0x130 [rdma_rxe]
     [<ffffffffa00419fe>] ib_uverbs_reg_mr+0x14e/0x2c0 [ib_uverbs]
     [<ffffffffa003d3ab>] ib_uverbs_write+0x15b/0x3b0 [ib_uverbs]
     [<ffffffff811e92a6>] ? mem_cgroup_commit_charge+0x76/0xe0
     [<ffffffff811af0a9>] ? page_add_new_anon_rmap+0x89/0xc0
     [<ffffffff8117e6c9>] ? lru_cache_add_active_or_unevictable+0x39/0xc0
     [<ffffffff811f0da8>] __vfs_write+0x28/0x120
     [<ffffffff811f1239>] ? rw_verify_area+0x49/0xb0
     [<ffffffff811f1492>] vfs_write+0xb2/0x1b0
     [<ffffffff811f27d6>] SyS_write+0x46/0xa0
     [<ffffffff814f7d32>] entry_SYSCALL_64_fastpath+0x1a/0xa4
    
    Signed-off-by: Parav Pandit <pandit.parav@gmail.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e1f96737c2a1..9e935655fccb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1739,6 +1739,14 @@ struct ib_dma_mapping_ops {
 	void		(*unmap_sg)(struct ib_device *dev,
 				    struct scatterlist *sg, int nents,
 				    enum dma_data_direction direction);
+	int		(*map_sg_attrs)(struct ib_device *dev,
+					struct scatterlist *sg, int nents,
+					enum dma_data_direction direction,
+					unsigned long attrs);
+	void		(*unmap_sg_attrs)(struct ib_device *dev,
+					  struct scatterlist *sg, int nents,
+					  enum dma_data_direction direction,
+					  unsigned long attrs);
 	void		(*sync_single_for_cpu)(struct ib_device *dev,
 					       u64 dma_handle,
 					       size_t size,
@@ -3000,8 +3008,12 @@ static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      enum dma_data_direction direction,
 				      unsigned long dma_attrs)
 {
-	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
-				dma_attrs);
+	if (dev->dma_ops)
+		return dev->dma_ops->map_sg_attrs(dev, sg, nents, direction,
+						  dma_attrs);
+	else
+		return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
+					dma_attrs);
 }
 
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
@@ -3009,7 +3021,12 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 enum dma_data_direction direction,
 					 unsigned long dma_attrs)
 {
-	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
+	if (dev->dma_ops)
+		return dev->dma_ops->unmap_sg_attrs(dev, sg, nents, direction,
+						  dma_attrs);
+	else
+		dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction,
+				   dma_attrs);
 }
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry

commit 5ef990f06bd7e3cf521b5705d898d8e43d04ea90
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 5 12:56:21 2016 +0200

    IB/core: remove ib_get_dma_mr
    
    We now only use it from ib_alloc_pd to create a local DMA lkey if the
    device doesn't provide one, or a global rkey if the ULP requests it.
    
    This patch removes ib_get_dma_mr and open codes the functionality in
    ib_alloc_pd so that we can simplify the code and prevent abuse of the
    functionality.  As a side effect we can also simplify things by removing
    the valid access bit check, and the PD refcounting.
    
    In the future I hope to also remove the per-PD global MR entirely by
    shifting this work into the HW drivers, as one step towards avoiding
    the struct ib_mr overload for various different use cases.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4bdd898697cf..0a6c70895c8b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2878,18 +2878,6 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
 		-ENOSYS;
 }
 
-/**
- * ib_get_dma_mr - Returns a memory region for system memory that is
- *   usable for DMA.
- * @pd: The protection domain associated with the memory region.
- * @mr_access_flags: Specifies the memory access rights.
- *
- * Note that the ib_dma_*() functions defined below must be used
- * to create/destroy addresses used with the Lkey or Rkey returned
- * by ib_get_dma_mr().
- */
-struct ib_mr *ib_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
-
 /**
  * ib_dma_mapping_error - check a DMA addr for error
  * @dev: The device for which the dma_addr was created

commit ed082d36a7b2c27d1cda55fdfb28af18040c4a89
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 5 12:56:17 2016 +0200

    IB/core: add support to create a unsafe global rkey to ib_create_pd
    
    Instead of exposing ib_get_dma_mr to ULPs and letting them use it more or
    less unchecked, this moves the capability of creating a global rkey into
    the RDMA core, where it can be easily audited.  It also prints a warning
    everytime this feature is used as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 38a08dae49c4..4bdd898697cf 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1370,10 +1370,13 @@ struct ib_udata {
 
 struct ib_pd {
 	u32			local_dma_lkey;
+	u32			flags;
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;
 	atomic_t          	usecnt; /* count all resources */
 
+	u32			unsafe_global_rkey;
+
 	/*
 	 * Implementation details of the RDMA core, don't use in drivers:
 	 */
@@ -2506,8 +2509,23 @@ int ib_find_gid(struct ib_device *device, union ib_gid *gid,
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);
 
-struct ib_pd *ib_alloc_pd(struct ib_device *device);
+enum ib_pd_flags {
+	/*
+	 * Create a memory registration for all memory in the system and place
+	 * the rkey for it into pd->unsafe_global_rkey.  This can be used by
+	 * ULPs to avoid the overhead of dynamic MRs.
+	 *
+	 * This flag is generally considered unsafe and must only be used in
+	 * extremly trusted environments.  Every use of it will log a warning
+	 * in the kernel log.
+	 */
+	IB_PD_UNSAFE_GLOBAL_RKEY	= 0x01,
+};
 
+struct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,
+		const char *caller);
+#define ib_alloc_pd(device, flags) \
+	__ib_alloc_pd((device), (flags), __func__)
 void ib_dealloc_pd(struct ib_pd *pd);
 
 /**

commit 50d46335b03baa9767e79a6f4757df7bd31eba21
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 5 12:56:16 2016 +0200

    IB/core: rename pd->local_mr to pd->__internal_mr
    
    This has two reasons: a) to clearly mark that drivers don't have any
    business using it, and b) because we're going to use it for the
    (dangerous) global rkey soon, so that drivers don't create on themselves.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8e90dd28bb75..38a08dae49c4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1373,7 +1373,11 @@ struct ib_pd {
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;
 	atomic_t          	usecnt; /* count all resources */
-	struct ib_mr	       *local_mr;
+
+	/*
+	 * Implementation details of the RDMA core, don't use in drivers:
+	 */
+	struct ib_mr	       *__internal_mr;
 };
 
 struct ib_xrcd {

commit 92d27ae6b3bb3491c1685fb3ca7ae1b26d81bdf4
Author: Markus Elfring <elfring@users.sourceforge.net>
Date:   Mon Aug 22 18:23:24 2016 +0200

    IB/core: Use memdup_user() rather than duplicating its implementation
    
    * Reuse existing functionality from memdup_user() instead of keeping
      duplicate source code.
    
      This issue was detected by using the Coccinelle software.
    
    * The local variable "ret" will be set to an appropriate value a bit later.
      Thus omit the explicit initialisation at the beginning.
    
    Signed-off-by: Markus Elfring <elfring@users.sourceforge.net>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8e90dd28bb75..e1f96737c2a1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2115,22 +2115,17 @@ static inline bool ib_is_udata_cleared(struct ib_udata *udata,
 				       size_t len)
 {
 	const void __user *p = udata->inbuf + offset;
-	bool ret = false;
+	bool ret;
 	u8 *buf;
 
 	if (len > USHRT_MAX)
 		return false;
 
-	buf = kmalloc(len, GFP_KERNEL);
-	if (!buf)
+	buf = memdup_user(p, len);
+	if (IS_ERR(buf))
 		return false;
 
-	if (copy_from_user(buf, p, len))
-		goto free;
-
 	ret = !memchr_inv(buf, 0, len);
-
-free:
 	kfree(buf);
 	return ret;
 }

commit 84e39eeb08c0ea7e9ec43ac820bf76a6fe8ecbad
Merge: 0cda611386b2 7c41765d8c30
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 4 20:26:31 2016 -0400

    Merge tag 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull second round of rdma updates from Doug Ledford:
     "This can be split out into just two categories:
    
       - fixes to the RDMA R/W API in regards to SG list length limits
         (about 5 patches)
    
       - fixes/features for the Intel hfi1 driver (everything else)
    
      The hfi1 driver is still being brought to full feature support by
      Intel, and they have a lot of people working on it, so that amounts to
      almost the entirety of this pull request"
    
    * tag 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (84 commits)
      IB/hfi1: Add cache evict LRU list
      IB/hfi1: Fix memory leak during unexpected shutdown
      IB/hfi1: Remove unneeded mm argument in remove function
      IB/hfi1: Consistently call ops->remove outside spinlock
      IB/hfi1: Use evict mmu rb operation
      IB/hfi1: Add evict operation to the mmu rb handler
      IB/hfi1: Fix TID caching actions
      IB/hfi1: Make the cache handler own its rb tree root
      IB/hfi1: Make use of mm consistent
      IB/hfi1: Fix user SDMA racy user request claim
      IB/hfi1: Fix error condition that needs to clean up
      IB/hfi1: Release node on insert failure
      IB/hfi1: Validate SDMA user iovector count
      IB/hfi1: Validate SDMA user request index
      IB/hfi1: Use the same capability state for all shared contexts
      IB/hfi1: Prevent null pointer dereference
      IB/hfi1: Rename TID mmu_rb_* functions
      IB/hfi1: Remove unneeded empty check in hfi1_mmu_rb_unregister()
      IB/hfi1: Restructure hfi1_file_open
      IB/hfi1: Make iovec loop index easy to understand
      ...

commit 0cda611386b2fcbf8bb32e9a5d82bfed4856fc36
Merge: fdf1f7ff1bd7 7f1d25b47d91
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 4 20:10:31 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull base rdma updates from Doug Ledford:
     "Round one of 4.8 code: while this is mostly normal, there is a new
      driver in here (the driver was hosted outside the kernel for several
      years and is actually a fairly mature and well coded driver).  It
      amounts to 13,000 of the 16,000 lines of added code in here.
    
      Summary:
    
       - Updates/fixes for iw_cxgb4 driver
       - Updates/fixes for mlx5 driver
       - Add flow steering and RSS API
       - Add hardware stats to mlx4 and mlx5 drivers
       - Add firmware version API for RDMA driver use
       - Add the rxe driver (this is a software RoCE driver that makes any
         Ethernet device a RoCE device)
       - Fixes for i40iw driver
       - Support for send only multicast joins in the cma layer
       - Other minor fixes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (72 commits)
      Soft RoCE driver
      IB/core: Support for CMA multicast join flags
      IB/sa: Add cached attribute containing SM information to SA port
      IB/uverbs: Fix race between uverbs_close and remove_one
      IB/mthca: Clean up error unwind flow in mthca_reset()
      IB/mthca: NULL arg to pci_dev_put is OK
      IB/hfi1: NULL arg to sc_return_credits is OK
      IB/mlx4: Add diagnostic hardware counters
      net/mlx4: Query performance and diagnostics counters
      net/mlx4: Add diagnostic counters capability bit
      Use smaller 512 byte messages for portmapper messages
      IB/ipoib: Report SG feature regardless of HW UD CSUM capability
      IB/mlx4: Don't use GFP_ATOMIC for CQ resize struct
      IB/hfi1: Disable by default
      IB/rdmavt: Disable by default
      IB/mlx5: Fix port counter ID association to QP offset
      IB/mlx5: Fix iteration overrun in GSI qps
      i40iw: Add NULL check for puda buffer
      i40iw: Change dup_ack_thresh to u8
      i40iw: Remove unnecessary check for moving CQ head
      ...

commit 00085f1efa387a8ce100e3734920f7639c80caa3
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Wed Aug 3 13:46:00 2016 -0700

    dma-mapping: use unsigned long for dma_attrs
    
    The dma-mapping core and the implementations do not change the DMA
    attributes passed by pointer.  Thus the pointer can point to const data.
    However the attributes do not have to be a bitfield.  Instead unsigned
    long will do fine:
    
    1. This is just simpler.  Both in terms of reading the code and setting
       attributes.  Instead of initializing local attributes on the stack
       and passing pointer to it to dma_set_attr(), just set the bits.
    
    2. It brings safeness and checking for const correctness because the
       attributes are passed by value.
    
    Semantic patches for this change (at least most of them):
    
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
    
        @@
        f(...,
        - struct dma_attrs *attrs
        + unsigned long attrs
        , ...)
        {
        ...
        }
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    and
    
        // Options: --all-includes
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
        type t;
    
        @@
        t f(..., struct dma_attrs *attrs);
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    Link: http://lkml.kernel.org/r/1468399300-5399-2-git-send-email-k.kozlowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no>
    Acked-by: Mark Salter <msalter@redhat.com> [c6x]
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com> [cris]
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch> [drm]
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Fabien Dessenne <fabien.dessenne@st.com> [bdisp]
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com> [vb2-core]
    Acked-by: David Vrabel <david.vrabel@citrix.com> [xen]
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen swiotlb]
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Richard Kuo <rkuo@codeaurora.org> [hexagon]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> [s390]
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no> [avr32]
    Acked-by: Vineet Gupta <vgupta@synopsys.com> [arc]
    Acked-by: Robin Murphy <robin.murphy@arm.com> [arm64 and dma-iommu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7e440d41487a..a8137dcf5a00 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2819,19 +2819,19 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 static inline u64 ib_dma_map_single_attrs(struct ib_device *dev,
 					  void *cpu_addr, size_t size,
 					  enum dma_data_direction direction,
-					  struct dma_attrs *attrs)
+					  unsigned long dma_attrs)
 {
 	return dma_map_single_attrs(dev->dma_device, cpu_addr, size,
-				    direction, attrs);
+				    direction, dma_attrs);
 }
 
 static inline void ib_dma_unmap_single_attrs(struct ib_device *dev,
 					     u64 addr, size_t size,
 					     enum dma_data_direction direction,
-					     struct dma_attrs *attrs)
+					     unsigned long dma_attrs)
 {
 	return dma_unmap_single_attrs(dev->dma_device, addr, size,
-				      direction, attrs);
+				      direction, dma_attrs);
 }
 
 /**
@@ -2906,17 +2906,18 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
 				      struct scatterlist *sg, int nents,
 				      enum dma_data_direction direction,
-				      struct dma_attrs *attrs)
+				      unsigned long dma_attrs)
 {
-	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
+	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction,
+				dma_attrs);
 }
 
 static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
 					 struct scatterlist *sg, int nents,
 					 enum dma_data_direction direction,
-					 struct dma_attrs *attrs)
+					 unsigned long dma_attrs)
 {
-	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
+	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, dma_attrs);
 }
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry

commit 632bc3f65081dd1e2e5394a9161580a0f78e8839
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Thu Jul 21 13:03:30 2016 -0700

    IB/core, RDMA RW API: Do not exceed QP SGE send limit
    
    Compute the SGE limit for RDMA READ and WRITE requests in
    ib_create_qp(). Use that limit in the RDMA RW API implementation.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Steve Wise <swise@opengridcomputing.com>
    Cc: Parav Pandit <pandit.parav@gmail.com>
    Cc: Nicholas Bellinger <nab@linux-iscsi.org>
    Cc: Laurence Oberman <loberman@redhat.com>
    Cc: <stable@vger.kernel.org> #v4.7+
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7e440d41487a..e694f02d42e3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1428,6 +1428,10 @@ struct ib_srq {
 	} ext;
 };
 
+/*
+ * @max_write_sge: Maximum SGE elements per RDMA WRITE request.
+ * @max_read_sge:  Maximum SGE elements per RDMA READ request.
+ */
 struct ib_qp {
 	struct ib_device       *device;
 	struct ib_pd	       *pd;
@@ -1449,6 +1453,8 @@ struct ib_qp {
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
 	u32			qp_num;
+	u32			max_write_sge;
+	u32			max_read_sge;
 	enum ib_qp_type		qp_type;
 };
 

commit fb92d8fb1b9c2de7d7d50c199e6d3020544702e8
Merge: 33688abb2802 dd6b0241260d 0ad17a8f7fa0 939b6ca873e7
Author: Doug Ledford <dledford@redhat.com>
Date:   Thu Jun 23 12:29:26 2016 -0400

    Merge branches 'cxgb4-4.8', 'mlx5-4.8' and 'fw-version' into k.o/for-4.8

commit 5fa76c20458518ed6181adddef2e31c5afc0745c
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Wed Jun 15 02:21:56 2016 -0400

    IB/core: Add get FW version string to the core
    
    Allow for a common core function to get firmware version strings
    from the individual devices.
    
    In later patches this format can then then be used to pass a
    properly formated version string through the IPoIB layer.
    
    The problem with the current code in the IPoIB layer is that it is
    specific to certain hardware types.
    
    Furthermore, this gives us a common function through which the core
    can provide a common sysfs entry.  Eventually we may want to
    remove the sysfs export but this provides for user space backwards
    compatibility.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7e440d41487a..1dc3d0d90202 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1956,6 +1956,7 @@ struct ib_device {
 	 * in fast paths.
 	 */
 	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
+	void (*get_dev_fw_str)(struct ib_device *, char *str, size_t str_len);
 };
 
 struct ib_client {
@@ -1991,6 +1992,8 @@ struct ib_client {
 struct ib_device *ib_alloc_device(size_t size);
 void ib_dealloc_device(struct ib_device *device);
 
+void ib_get_device_fw_str(struct ib_device *device, char *str, size_t str_len);
+
 int ib_register_device(struct ib_device *device,
 		       int (*port_callback)(struct ib_device *,
 					    u8, struct kobject *));

commit 4c2aae712cb024f9d30a1fa62e3ba2ff785c6a3e
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Jun 17 15:14:50 2016 +0300

    IB/core: Add IPv6 support to flow steering
    
    Add IPv6 flow specification support.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9b2fafe5eb38..9bbca6887f7f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1569,6 +1569,7 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_IB		= 0x22,
 	/* L3 header*/
 	IB_FLOW_SPEC_IPV4	= 0x30,
+	IB_FLOW_SPEC_IPV6	= 0x31,
 	/* L4 headers*/
 	IB_FLOW_SPEC_TCP	= 0x40,
 	IB_FLOW_SPEC_UDP	= 0x41
@@ -1630,6 +1631,18 @@ struct ib_flow_spec_ipv4 {
 	struct ib_flow_ipv4_filter mask;
 };
 
+struct ib_flow_ipv6_filter {
+	u8	src_ip[16];
+	u8	dst_ip[16];
+};
+
+struct ib_flow_spec_ipv6 {
+	enum ib_flow_spec_type	   type;
+	u16			   size;
+	struct ib_flow_ipv6_filter val;
+	struct ib_flow_ipv6_filter mask;
+};
+
 struct ib_flow_tcp_udp_filter {
 	__be16	dst_port;
 	__be16	src_port;
@@ -1651,6 +1664,7 @@ union ib_flow_spec {
 	struct ib_flow_spec_ib		ib;
 	struct ib_flow_spec_ipv4        ipv4;
 	struct ib_flow_spec_tcp_udp	tcp_udp;
+	struct ib_flow_spec_ipv6        ipv6;
 };
 
 struct ib_flow_attr {

commit a9017e232ff9eaabeb50eb89841d99310cfc98dc
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:54 2016 +0300

    IB/core: Extend create QP to get indirection table
    
    Extend create QP to get Receive Work Queue (WQ) indirection table.
    
    QP can be created with external Receive Work Queue indirection table,
    in that case it is ready to receive immediately.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e305c9a36663..9b2fafe5eb38 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1017,6 +1017,7 @@ struct ib_qp_init_attr {
 	 * Only needed for special QP types, or when using the RW API.
 	 */
 	u8			port_num;
+	struct ib_rwq_ind_table *rwq_ind_tbl;
 };
 
 struct ib_qp_open_attr {
@@ -1511,6 +1512,7 @@ struct ib_qp {
 	void		       *qp_context;
 	u32			qp_num;
 	enum ib_qp_type		qp_type;
+	struct ib_rwq_ind_table *rwq_ind_tbl;
 };
 
 struct ib_mr {

commit de019a94049d579608a5511f8c50652faf125182
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:52 2016 +0300

    IB/uverbs: Introduce RWQ Indirection table
    
    User applications that want to spread traffic on several WQs, need to
    create an indirection table, by using already created WQs.
    
    Adding uverbs API in order to create and destroy this table.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fa2e0184dcc5..e305c9a36663 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1326,6 +1326,7 @@ struct ib_ucontext {
 	struct list_head	xrcd_list;
 	struct list_head	rule_list;
 	struct list_head	wq_list;
+	struct list_head	rwq_ind_tbl_list;
 	int			closing;
 
 	struct pid             *tgid;

commit 6d39786bf116e476d75eca91f7cfa22586a32e5f
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:51 2016 +0300

    IB/core: Introduce Receive Work Queue indirection table
    
    Introduce Receive Work Queue (WQ) indirection table.
    This object can be used to spread incoming traffic to different
    receive Work Queues.
    
    A Receive WQ indirection table points to variable size of WQs.
    This table is given to a QP in downstream patches.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimerg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0c1956a98573..fa2e0184dcc5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1473,6 +1473,21 @@ struct ib_wq_attr {
 	enum	ib_wq_state	curr_wq_state;
 };
 
+struct ib_rwq_ind_table {
+	struct ib_device	*device;
+	struct ib_uobject      *uobject;
+	atomic_t		usecnt;
+	u32		ind_tbl_num;
+	u32		log_ind_tbl_size;
+	struct ib_wq	**ind_tbl;
+};
+
+struct ib_rwq_ind_table_init_attr {
+	u32		log_ind_tbl_size;
+	/* Each entry is a pointer to Receive Work Queue */
+	struct ib_wq	**ind_tbl;
+};
+
 struct ib_qp {
 	struct ib_device       *device;
 	struct ib_pd	       *pd;
@@ -1974,6 +1989,10 @@ struct ib_device {
 						struct ib_wq_attr *attr,
 						u32 wq_attr_mask,
 						struct ib_udata *udata);
+	struct ib_rwq_ind_table *  (*create_rwq_ind_table)(struct ib_device *device,
+							   struct ib_rwq_ind_table_init_attr *init_attr,
+							   struct ib_udata *udata);
+	int                        (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);
 	struct ib_dma_mapping_ops   *dma_ops;
 
 	struct module               *owner;
@@ -3224,6 +3243,10 @@ struct ib_wq *ib_create_wq(struct ib_pd *pd,
 int ib_destroy_wq(struct ib_wq *wq);
 int ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *attr,
 		 u32 wq_attr_mask);
+struct ib_rwq_ind_table *ib_create_rwq_ind_table(struct ib_device *device,
+						 struct ib_rwq_ind_table_init_attr*
+						 wq_ind_table_init_attr);
+int ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
 
 int ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
 		 unsigned int *sg_offset, unsigned int page_size);

commit f213c05272100f385912372fff678d0af4d7f8ad
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:49 2016 +0300

    IB/uverbs: Add WQ support
    
    User space applications which use RSS functionality need to create
    a work queue object (WQ). The lifetime of such an object is:
     * Create a WQ
     * Modify the WQ from reset to init state.
     * Use the WQ (by downstream patches).
     * Destroy the WQ.
    
    These commands are added to the uverbs API.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@rimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f2d954ac42f6..0c1956a98573 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -562,6 +562,7 @@ enum ib_event_type {
 	IB_EVENT_QP_LAST_WQE_REACHED,
 	IB_EVENT_CLIENT_REREGISTER,
 	IB_EVENT_GID_CHANGE,
+	IB_EVENT_WQ_FATAL,
 };
 
 const char *__attribute_const__ ib_event_msg(enum ib_event_type event);
@@ -572,6 +573,7 @@ struct ib_event {
 		struct ib_cq	*cq;
 		struct ib_qp	*qp;
 		struct ib_srq	*srq;
+		struct ib_wq	*wq;
 		u8		port_num;
 	} element;
 	enum ib_event_type	event;
@@ -1323,6 +1325,7 @@ struct ib_ucontext {
 	struct list_head	ah_list;
 	struct list_head	xrcd_list;
 	struct list_head	rule_list;
+	struct list_head	wq_list;
 	int			closing;
 
 	struct pid             *tgid;

commit 5fd251c8b4c52da0d0916470a67fbb77b972125e
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:48 2016 +0300

    IB/core: Introduce Work Queue object and its verbs
    
    Introduce Work Queue object and its create/destroy/modify verbs.
    
    QP can be created without internal WQs "packaged" inside it,
    this QP can be configured to use "external" WQ object as its
    receive/send queue.
    WQ is a necessary component for RSS technology since RSS mechanism
    is supposed to distribute the traffic between multiple
    Receive Work Queues.
    
    WQ associated (many to one) with Completion Queue and it owns WQ
    properties (PD, WQ size, etc.).
    WQ has a type, this patch introduces the IB_WQT_RQ (i.e.receive queue),
    it may be extend to others such as IB_WQT_SQ. (send queue).
    WQ from type IB_WQT_RQ contains receive work requests.
    
    PD is an attribute of a work queue (i.e. send/receive queue), it's used
    by the hardware for security validation before scattering to a memory
    region which is pointed by the WQ. For that, an external WQ object
    needs a PD, letting the hardware makes that validation.
    
    When accessing a memory region that is pointed by the WQ its PD
    is used and not the QP's PD, this behavior is similar
    to a SRQ and a QP.
    
    WQ context is subject to a well-defined state transitions done by
    the modify_wq verb.
    When WQ is created its initial state becomes IB_WQS_RESET.
    >From IB_WQS_RESET it can be modified to itself or to IB_WQS_RDY.
    >From IB_WQS_RDY it can be modified to itself, to IB_WQS_RESET
    or to IB_WQS_ERR.
    >From IB_WQS_ERR it can be modified to IB_WQS_RESET.
    
    Note: transition to IB_WQS_ERR might occur implicitly in case there
    was some HW error.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7e440d41487a..f2d954ac42f6 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1428,6 +1428,48 @@ struct ib_srq {
 	} ext;
 };
 
+enum ib_wq_type {
+	IB_WQT_RQ
+};
+
+enum ib_wq_state {
+	IB_WQS_RESET,
+	IB_WQS_RDY,
+	IB_WQS_ERR
+};
+
+struct ib_wq {
+	struct ib_device       *device;
+	struct ib_uobject      *uobject;
+	void		    *wq_context;
+	void		    (*event_handler)(struct ib_event *, void *);
+	struct ib_pd	       *pd;
+	struct ib_cq	       *cq;
+	u32		wq_num;
+	enum ib_wq_state       state;
+	enum ib_wq_type	wq_type;
+	atomic_t		usecnt;
+};
+
+struct ib_wq_init_attr {
+	void		       *wq_context;
+	enum ib_wq_type	wq_type;
+	u32		max_wr;
+	u32		max_sge;
+	struct	ib_cq	       *cq;
+	void		    (*event_handler)(struct ib_event *, void *);
+};
+
+enum ib_wq_attr_mask {
+	IB_WQ_STATE	= 1 << 0,
+	IB_WQ_CUR_STATE	= 1 << 1,
+};
+
+struct ib_wq_attr {
+	enum	ib_wq_state	wq_state;
+	enum	ib_wq_state	curr_wq_state;
+};
+
 struct ib_qp {
 	struct ib_device       *device;
 	struct ib_pd	       *pd;
@@ -1921,7 +1963,14 @@ struct ib_device {
 						   struct ifla_vf_stats *stats);
 	int			   (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
 						  int type);
-
+	struct ib_wq *		   (*create_wq)(struct ib_pd *pd,
+						struct ib_wq_init_attr *init_attr,
+						struct ib_udata *udata);
+	int			   (*destroy_wq)(struct ib_wq *wq);
+	int			   (*modify_wq)(struct ib_wq *wq,
+						struct ib_wq_attr *attr,
+						u32 wq_attr_mask,
+						struct ib_udata *udata);
 	struct ib_dma_mapping_ops   *dma_ops;
 
 	struct module               *owner;
@@ -3167,6 +3216,11 @@ int ib_check_mr_status(struct ib_mr *mr, u32 check_mask,
 struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);
+struct ib_wq *ib_create_wq(struct ib_pd *pd,
+			   struct ib_wq_init_attr *init_attr);
+int ib_destroy_wq(struct ib_wq *wq);
+int ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *attr,
+		 u32 wq_attr_mask);
 
 int ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
 		 unsigned int *sg_offset, unsigned int page_size);

commit c7e162a417488f3c79eb09f3c4f1d36f1e042463
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Mon Jun 6 19:34:40 2016 +0300

    IB/core: Make all casts in ib_device_cap_flags enum consistent
    
    Replace the few u64 casts with ULL to match the rest of the casts.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c97357b6c276..7e440d41487a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -219,8 +219,8 @@ enum ib_device_cap_flags {
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1ULL << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
-	IB_DEVICE_VIRTUAL_FUNCTION		= ((u64)1 << 33),
-	IB_DEVICE_RAW_SCATTER_FCS		= ((u64)1 << 34),
+	IB_DEVICE_VIRTUAL_FUNCTION		= (1ULL << 33),
+	IB_DEVICE_RAW_SCATTER_FCS		= (1ULL << 34),
 };
 
 enum ib_signature_prot_cap {

commit 47355b3cd7d3c9c5226bff7c449b9d269fb17fa6
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Mon Jun 6 19:34:39 2016 +0300

    IB/core: Fix bit curruption in ib_device_cap_flags structure
    
    ib_device_cap_flags 64-bit expansion caused caps overlapping
    and made consumers read wrong device capabilities. For example
    IB_DEVICE_SG_GAPS_REG was falsely read by the iser driver causing
    it to use a non-existing capability. This happened because signed
    int becomes sign extended when converted it to u64. Fix this by
    casting IB_DEVICE_ON_DEMAND_PAGING enumeration to ULL.
    
    Fixes: f5aa9159a418 ('IB/core: Add arbitrary sg_list support')
    Reported-by: Robert LeBlanc <robert@leblancnet.us>
    Cc: Stable <stable@vger.kernel.org> #[v4.6+]
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 432bed510369..c97357b6c276 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -217,7 +217,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_CROSS_CHANNEL		= (1 << 27),
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
-	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
+	IB_DEVICE_ON_DEMAND_PAGING		= (1ULL << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 	IB_DEVICE_VIRTUAL_FUNCTION		= ((u64)1 << 33),
 	IB_DEVICE_RAW_SCATTER_FCS		= ((u64)1 << 34),

commit b40f4757daa1b28e586fddad76638c98e2edfc34
Author: Christoph Lameter <cl@linux.com>
Date:   Mon May 16 12:49:33 2016 -0500

    IB/core: Make device counter infrastructure dynamic
    
    In practice, each RDMA device has a unique set of counters that the
    hardware implements.  Having a central set of counters that they must
    all adhere to is limiting and causes many useful counters to not be
    available.
    
    Therefore we create a dynamic counter registration infrastructure.
    
    The driver must implement a stats structure allocation routine, in
    which the driver must place the directory name it wants, a list of
    names for all of the counters, an array of u64 counters themselves,
    plus a few generic configuration options.
    
    We then implement a core routine to create a sysfs file for each
    of the named stats elements, and a core routine to retrieve the
    stats when any of the sysfs attribute files are read.
    
    To avoid excessive beating on the stats generation routine in the
    drivers, the core code also caches the stats for a short period of
    time so that someone attempting to read all of the stats in a
    given device's directory will not result in a stats generation
    call per file read.
    
    Future work will attempt to standardize just the shared stats
    elements, and possibly add a method to get the stats via netlink
    in addition to sysfs.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    [ Add caching, make structure names more informative, add i40iw support,
      other significant rewrites from the original patch ]

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fc0320c004a3..432bed510369 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -403,56 +403,55 @@ enum ib_port_speed {
 	IB_SPEED_EDR	= 32
 };
 
-struct ib_protocol_stats {
-	/* TBD... */
-};
-
-struct iw_protocol_stats {
-	u64	ipInReceives;
-	u64	ipInHdrErrors;
-	u64	ipInTooBigErrors;
-	u64	ipInNoRoutes;
-	u64	ipInAddrErrors;
-	u64	ipInUnknownProtos;
-	u64	ipInTruncatedPkts;
-	u64	ipInDiscards;
-	u64	ipInDelivers;
-	u64	ipOutForwDatagrams;
-	u64	ipOutRequests;
-	u64	ipOutDiscards;
-	u64	ipOutNoRoutes;
-	u64	ipReasmTimeout;
-	u64	ipReasmReqds;
-	u64	ipReasmOKs;
-	u64	ipReasmFails;
-	u64	ipFragOKs;
-	u64	ipFragFails;
-	u64	ipFragCreates;
-	u64	ipInMcastPkts;
-	u64	ipOutMcastPkts;
-	u64	ipInBcastPkts;
-	u64	ipOutBcastPkts;
-
-	u64	tcpRtoAlgorithm;
-	u64	tcpRtoMin;
-	u64	tcpRtoMax;
-	u64	tcpMaxConn;
-	u64	tcpActiveOpens;
-	u64	tcpPassiveOpens;
-	u64	tcpAttemptFails;
-	u64	tcpEstabResets;
-	u64	tcpCurrEstab;
-	u64	tcpInSegs;
-	u64	tcpOutSegs;
-	u64	tcpRetransSegs;
-	u64	tcpInErrs;
-	u64	tcpOutRsts;
-};
-
-union rdma_protocol_stats {
-	struct ib_protocol_stats	ib;
-	struct iw_protocol_stats	iw;
-};
+/**
+ * struct rdma_hw_stats
+ * @timestamp - Used by the core code to track when the last update was
+ * @lifespan - Used by the core code to determine how old the counters
+ *   should be before being updated again.  Stored in jiffies, defaults
+ *   to 10 milliseconds, drivers can override the default be specifying
+ *   their own value during their allocation routine.
+ * @name - Array of pointers to static names used for the counters in
+ *   directory.
+ * @num_counters - How many hardware counters there are.  If name is
+ *   shorter than this number, a kernel oops will result.  Driver authors
+ *   are encouraged to leave BUILD_BUG_ON(ARRAY_SIZE(@name) < num_counters)
+ *   in their code to prevent this.
+ * @value - Array of u64 counters that are accessed by the sysfs code and
+ *   filled in by the drivers get_stats routine
+ */
+struct rdma_hw_stats {
+	unsigned long	timestamp;
+	unsigned long	lifespan;
+	const char * const *names;
+	int		num_counters;
+	u64		value[];
+};
+
+#define RDMA_HW_STATS_DEFAULT_LIFESPAN 10
+/**
+ * rdma_alloc_hw_stats_struct - Helper function to allocate dynamic struct
+ *   for drivers.
+ * @names - Array of static const char *
+ * @num_counters - How many elements in array
+ * @lifespan - How many milliseconds between updates
+ */
+static inline struct rdma_hw_stats *rdma_alloc_hw_stats_struct(
+		const char * const *names, int num_counters,
+		unsigned long lifespan)
+{
+	struct rdma_hw_stats *stats;
+
+	stats = kzalloc(sizeof(*stats) + num_counters * sizeof(u64),
+			GFP_KERNEL);
+	if (!stats)
+		return NULL;
+	stats->names = names;
+	stats->num_counters = num_counters;
+	stats->lifespan = msecs_to_jiffies(lifespan);
+
+	return stats;
+}
+
 
 /* Define bits for the various functionality this port needs to be supported by
  * the core.
@@ -1707,8 +1706,29 @@ struct ib_device {
 
 	struct iw_cm_verbs	     *iwcm;
 
-	int		           (*get_protocol_stats)(struct ib_device *device,
-							 union rdma_protocol_stats *stats);
+	/**
+	 * alloc_hw_stats - Allocate a struct rdma_hw_stats and fill in the
+	 *   driver initialized data.  The struct is kfree()'ed by the sysfs
+	 *   core when the device is removed.  A lifespan of -1 in the return
+	 *   struct tells the core to set a default lifespan.
+	 */
+	struct rdma_hw_stats      *(*alloc_hw_stats)(struct ib_device *device,
+						     u8 port_num);
+	/**
+	 * get_hw_stats - Fill in the counter value(s) in the stats struct.
+	 * @index - The index in the value array we wish to have updated, or
+	 *   num_counters if we want all stats updated
+	 * Return codes -
+	 *   < 0 - Error, no counters updated
+	 *   index - Updated the single counter pointed to by index
+	 *   num_counters - Updated all counters (will reset the timestamp
+	 *     and prevent further calls for lifespan milliseconds)
+	 * Drivers are allowed to update all counters in leiu of just the
+	 *   one given in index at their option
+	 */
+	int		           (*get_hw_stats)(struct ib_device *device,
+						   struct rdma_hw_stats *stats,
+						   u8 port, int index);
 	int		           (*query_device)(struct ib_device *device,
 						   struct ib_device_attr *device_attr,
 						   struct ib_udata *udata);
@@ -1926,6 +1946,8 @@ struct ib_device {
 	u8                           node_type;
 	u8                           phys_port_cnt;
 	struct ib_device_attr        attrs;
+	struct attribute_group	     *hw_stats_ag;
+	struct rdma_hw_stats         *hw_stats;
 
 	/**
 	 * The following mandatory functions are used only at device

commit 0651ec932afffce6547efb3e0352e5d229273962
Merge: e9bb8af98a98 ba987e51a637 78c49f83ee28 e3614bc9dc44 37aa5c36aa70 cff5a0f3a3cd
Author: Doug Ledford <dledford@redhat.com>
Date:   Fri May 13 19:40:38 2016 -0400

    Merge branches 'cxgb4-2', 'i40iw-2', 'ipoib', 'misc-4.7' and 'mlx5-fcs' into k.o/for-4.7

commit b531b909481933f78493e4d2fcda25c606acf120
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Sun Apr 17 17:19:36 2016 +0300

    IB/core: Add Scatter FCS create flag
    
    Raw Packet QPs that were created with Scatter FCS flag, will scatter
    the FCS into the receive buffers.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6d6172d66be8..195b233d1161 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -982,6 +982,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
 	IB_QP_CREATE_USE_GFP_NOIO		= 1 << 7,
+	IB_QP_CREATE_SCATTER_FCS		= 1 << 8,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,

commit 45686f2d6535525a9875e4a77a35da013814de82
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Sun Apr 17 17:19:35 2016 +0300

    IB/core: Add Raw Scatter FCS device capability
    
    Raw Scatter FCS device capability is set when the NIC supports
    scattering the FCS to the receive buffers of Raw Packet QPs.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fb2cef4e9747..6d6172d66be8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -220,6 +220,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 	IB_DEVICE_VIRTUAL_FUNCTION		= ((u64)1 << 33),
+	IB_DEVICE_RAW_SCATTER_FCS		= ((u64)1 << 34),
 };
 
 enum ib_signature_prot_cap {

commit 9aa8b3217ed3c13d4e3496020b140da0e6f49a08
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Thu May 12 10:49:15 2016 -0700

    IB/core: Enhance ib_map_mr_sg()
    
    The SRP initiator allows to set max_sectors to a value that exceeds
    the largest amount of data that can be mapped at once with an mlx4
    HCA using fast registration and a page size of 4 KB. Hence modify
    ib_map_mr_sg() such that it can map partial sg-elements. If an
    sg-element has been mapped partially, let the caller know
    which fraction has been mapped by adjusting *sg_offset.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Tested-by: Laurence Oberman <loberman@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 544c55b4c84a..56bb0f39ce79 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1848,7 +1848,7 @@ struct ib_device {
 	int                        (*map_mr_sg)(struct ib_mr *mr,
 						struct scatterlist *sg,
 						int sg_nents,
-						unsigned sg_offset);
+						unsigned int *sg_offset);
 	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
 					       enum ib_mw_type type,
 					       struct ib_udata *udata);
@@ -3145,11 +3145,11 @@ struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    const struct sockaddr *addr);
 
 int ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
-		unsigned int sg_offset, unsigned int page_size);
+		 unsigned int *sg_offset, unsigned int page_size);
 
 static inline int
 ib_map_mr_sg_zbva(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
-		unsigned int sg_offset, unsigned int page_size)
+		  unsigned int *sg_offset, unsigned int page_size)
 {
 	int n;
 
@@ -3160,7 +3160,7 @@ ib_map_mr_sg_zbva(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
 }
 
 int ib_sg_to_pages(struct ib_mr *mr, struct scatterlist *sgl, int sg_nents,
-		unsigned int sg_offset, int (*set_page)(struct ib_mr *, u64));
+		unsigned int *sg_offset, int (*set_page)(struct ib_mr *, u64));
 
 void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);

commit 0e353e34e1e740fe575eb479ca0f2a723a4ef51c
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:12 2016 +0200

    IB/core: add RW API support for signature MRs
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dd8e15dfc1a8..544c55b4c84a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1435,6 +1435,7 @@ struct ib_qp {
 	spinlock_t		mr_lock;
 	int			mrs_used;
 	struct list_head	rdma_mrs;
+	struct list_head	sig_mrs;
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;

commit a060b5629ab066dd1d321430eeb96f70939a1790
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:09 2016 +0200

    IB/core: generic RDMA READ/WRITE API
    
    This supports both manual mapping of lots of SGEs, as well as using MRs
    from the QP's MR pool, for iWarp or other cases where it's more optimal.
    For now, MRs are only used for iWARP transports.  The user of the RDMA-RW
    API must allocate the QP MR pool as well as size the SQ accordingly.
    
    Thanks to Steve Wise for testing, fixing and rewriting the iWarp support,
    and to Sagi Grimberg for ideas, reviews and fixes.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3f66647749ca..dd8e15dfc1a8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -931,6 +931,13 @@ struct ib_qp_cap {
 	u32	max_send_sge;
 	u32	max_recv_sge;
 	u32	max_inline_data;
+
+	/*
+	 * Maximum number of rdma_rw_ctx structures in flight at a time.
+	 * ib_create_qp() will calculate the right amount of neededed WRs
+	 * and MRs based on this.
+	 */
+	u32	max_rdma_ctxs;
 };
 
 enum ib_sig_type {
@@ -1002,7 +1009,11 @@ struct ib_qp_init_attr {
 	enum ib_sig_type	sq_sig_type;
 	enum ib_qp_type		qp_type;
 	enum ib_qp_create_flags	create_flags;
-	u8			port_num; /* special QP types only */
+
+	/*
+	 * Only needed for special QP types, or when using the RW API.
+	 */
+	u8			port_num;
 };
 
 struct ib_qp_open_attr {
@@ -1423,6 +1434,7 @@ struct ib_qp {
 	struct ib_cq	       *recv_cq;
 	spinlock_t		mr_lock;
 	int			mrs_used;
+	struct list_head	rdma_mrs;
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;

commit d4a85c309b33f93cb211f2fa9d26fa77d0bb7b5e
Author: Steve Wise <swise@chelsio.com>
Date:   Tue May 3 18:01:08 2016 +0200

    IB/core: add a need_inval flag to struct ib_mr
    
    This is the first step toward moving MR invalidation decisions
    to the core.  It will be needed by the upcoming RW API.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 400a8a0422a4..3f66647749ca 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1446,6 +1446,7 @@ struct ib_mr {
 	u64		   iova;
 	u32		   length;
 	unsigned int	   page_size;
+	bool		   need_inval;
 	union {
 		struct ib_uobject	*uobject;	/* user */
 		struct list_head	qp_entry;	/* FR */

commit fffb0383cf0b433ad029d19e6e9d6f1f46523ace
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:07 2016 +0200

    IB/core: add a simple MR pool
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9e8616af6899..400a8a0422a4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1421,9 +1421,12 @@ struct ib_qp {
 	struct ib_pd	       *pd;
 	struct ib_cq	       *send_cq;
 	struct ib_cq	       *recv_cq;
+	spinlock_t		mr_lock;
+	int			mrs_used;
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;
+
 	/* count times opened, mcast attaches, flow attaches */
 	atomic_t		usecnt;
 	struct list_head	open_list;
@@ -1438,12 +1441,15 @@ struct ib_qp {
 struct ib_mr {
 	struct ib_device  *device;
 	struct ib_pd	  *pd;
-	struct ib_uobject *uobject;
 	u32		   lkey;
 	u32		   rkey;
 	u64		   iova;
 	u32		   length;
 	unsigned int	   page_size;
+	union {
+		struct ib_uobject	*uobject;	/* user */
+		struct list_head	qp_entry;	/* FR */
+	};
 };
 
 struct ib_mw {

commit 002516edf5176dde2bee54addb7e6e8129a9532b
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:05 2016 +0200

    IB/core: add a helper to check for READ WITH INVALIDATE support
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 24d0d82a07b4..9e8616af6899 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2318,6 +2318,18 @@ static inline bool rdma_cap_roce_gid_table(const struct ib_device *device,
 		device->add_gid && device->del_gid;
 }
 
+/*
+ * Check if the device supports READ W/ INVALIDATE.
+ */
+static inline bool rdma_cap_read_inv(struct ib_device *dev, u32 port_num)
+{
+	/*
+	 * iWarp drivers must support READ W/ INVALIDATE.  No other protocol
+	 * has support for it yet.
+	 */
+	return rdma_protocol_iwarp(dev, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid,
 		 struct ib_gid_attr *attr);

commit ff2ba9936591a1364ae21adf18366dca7608395a
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:04 2016 +0200

    IB/core: Add passing an offset into the SG to ib_map_mr_sg
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fb2cef4e9747..24d0d82a07b4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1827,7 +1827,8 @@ struct ib_device {
 					       u32 max_num_sg);
 	int                        (*map_mr_sg)(struct ib_mr *mr,
 						struct scatterlist *sg,
-						int sg_nents);
+						int sg_nents,
+						unsigned sg_offset);
 	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
 					       enum ib_mw_type type,
 					       struct ib_udata *udata);
@@ -3111,29 +3112,23 @@ struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);
 
-int ib_map_mr_sg(struct ib_mr *mr,
-		 struct scatterlist *sg,
-		 int sg_nents,
-		 unsigned int page_size);
+int ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
+		unsigned int sg_offset, unsigned int page_size);
 
 static inline int
-ib_map_mr_sg_zbva(struct ib_mr *mr,
-		  struct scatterlist *sg,
-		  int sg_nents,
-		  unsigned int page_size)
+ib_map_mr_sg_zbva(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,
+		unsigned int sg_offset, unsigned int page_size)
 {
 	int n;
 
-	n = ib_map_mr_sg(mr, sg, sg_nents, page_size);
+	n = ib_map_mr_sg(mr, sg, sg_nents, sg_offset, page_size);
 	mr->iova = 0;
 
 	return n;
 }
 
-int ib_sg_to_pages(struct ib_mr *mr,
-		   struct scatterlist *sgl,
-		   int sg_nents,
-		   int (*set_page)(struct ib_mr *, u64));
+int ib_sg_to_pages(struct ib_mr *mr, struct scatterlist *sgl, int sg_nents,
+		unsigned int sg_offset, int (*set_page)(struct ib_mr *, u64));
 
 void ib_drain_rq(struct ib_qp *qp);
 void ib_drain_sq(struct ib_qp *qp);

commit 520a07bff6fbb23cac905007d74c67058b189acb
Merge: 34abf9ed73f2 68996a6e760e 5511d7810752
Author: Doug Ledford <dledford@redhat.com>
Date:   Mon Mar 21 17:32:23 2016 -0400

    Merge branches 'i40iw', 'sriov' and 'hfi1' into k.o/for-4.6

commit 50174a7f2c24d13cdeec435ee1ba70b1e0b1318f
Author: Eli Cohen <eli@mellanox.com>
Date:   Fri Mar 11 22:58:38 2016 +0200

    IB/core: Add interfaces to control VF attributes
    
    Following the practice exercised for network devices which allow the PF
    net device to configure attributes of its virtual functions, we
    introduce the following functions to be used by IPoIB which is the
    network driver implementation for IB devices.
    
    ib_set_vf_link_state - set the policy for a VF link. More below.
    ib_get_vf_config - read configuration information of a VF
    ib_get_vf_stats - read VF statistics
    ib_set_vf_guid - set the node or port GUID of a VF
    
    Also add an indication in the device cap flags that indicates that this
    IB devices is based on a virtual function.
    
    A VF shares the physical port with the PF and other VFs. When setting
    the link state we have three options:
    
    1. Auto - in this mode, the virtual port follows the state of the
       physical port and becomes active only if the physical port's state is
       active. In all other cases it remains in a Down state.
    2. Down - sets the state of the virtual port to Down
    3. Up - causes the virtual port to transition into Initialize state if
       it was not already in this state. A virtualization aware subnet manager
       can then bring the state of the port into the Active state.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3a5a66b7a33f..8a245a7f981a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -56,6 +56,7 @@
 #include <linux/string.h>
 #include <linux/slab.h>
 
+#include <linux/if_link.h>
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
 #include <asm/uaccess.h>
@@ -218,6 +219,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
 	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
+	IB_DEVICE_VIRTUAL_FUNCTION		= ((u64)1 << 33),
 };
 
 enum ib_signature_prot_cap {
@@ -1867,6 +1869,14 @@ struct ib_device {
 	void			   (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 	void			   (*drain_rq)(struct ib_qp *qp);
 	void			   (*drain_sq)(struct ib_qp *qp);
+	int			   (*set_vf_link_state)(struct ib_device *device, int vf, u8 port,
+							int state);
+	int			   (*get_vf_config)(struct ib_device *device, int vf, u8 port,
+						   struct ifla_vf_info *ivf);
+	int			   (*get_vf_stats)(struct ib_device *device, int vf, u8 port,
+						   struct ifla_vf_stats *stats);
+	int			   (*set_vf_guid)(struct ib_device *device, int vf, u8 port, u64 guid,
+						  int type);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 
@@ -2310,6 +2320,15 @@ int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid,
 		 struct ib_gid_attr *attr);
 
+int ib_set_vf_link_state(struct ib_device *device, int vf, u8 port,
+			 int state);
+int ib_get_vf_config(struct ib_device *device, int vf, u8 port,
+		     struct ifla_vf_info *info);
+int ib_get_vf_stats(struct ib_device *device, int vf, u8 port,
+		    struct ifla_vf_stats *stats);
+int ib_set_vf_guid(struct ib_device *device, int vf, u8 port, u64 guid,
+		   int type);
+
 int ib_query_pkey(struct ib_device *device,
 		  u8 port_num, u16 index, u16 *pkey);
 

commit a0c1b2a3508714281f604db818fa0cc83c2f9b6a
Author: Eli Cohen <eli@mellanox.com>
Date:   Fri Mar 11 22:58:37 2016 +0200

    IB/core: Support accessing SA in virtualized environment
    
    Per the ongoing standardisation process, when virtual HCAs are present
    in a network, traffic is routed based on a destination GID. In order to
    access the SA we use the well known SA GID.
    
    We also add a GRH required boolean field to the port attributes which is
    used to report to the verbs consumer whether this port is connected to a
    virtual network. We use this field to realize whether we need to create
    an address vector with GRH to access the subnet administrator. We clear
    the port attributes struct before calling the hardware driver to make
    sure the default remains that GRH is not required.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7239b9a4499e..3a5a66b7a33f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -97,6 +97,11 @@ enum rdma_node_type {
 	RDMA_NODE_USNIC_UDP,
 };
 
+enum {
+	/* set the local administered indication */
+	IB_SA_WELL_KNOWN_GUID	= BIT_ULL(57) | 2,
+};
+
 enum rdma_transport_type {
 	RDMA_TRANSPORT_IB,
 	RDMA_TRANSPORT_IWARP,
@@ -510,6 +515,7 @@ struct ib_port_attr {
 	u8			active_width;
 	u8			active_speed;
 	u8                      phys_state;
+	bool			grh_required;
 };
 
 enum ib_device_modify_flags {

commit fad61ad4e755f5dd13c7702a87cd907207392534
Author: Eli Cohen <eli@mellanox.com>
Date:   Fri Mar 11 22:58:36 2016 +0200

    IB/core: Add subnet prefix to port info
    
    The subnet prefix is a part of the port_info MAD returned and should be
    available at the ib_port_attr struct. We define it here and provide a
    default implementation in case the hardware driver does not provide one.
    The subnet prefix is required when creating the address vector to access
    the SA in networks where GRH must be used.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c9b62344d22e..7239b9a4499e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -490,6 +490,7 @@ union rdma_protocol_stats {
 					| RDMA_CORE_CAP_OPA_MAD)
 
 struct ib_port_attr {
+	u64			subnet_prefix;
 	enum ib_port_state	state;
 	enum ib_mtu		max_mtu;
 	enum ib_mtu		active_mtu;

commit fb532d6a79b96a4c8f678024d7ed3549ff0ca916
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Feb 23 10:25:25 2016 +0200

    IB/{core, ulp} Support above 32 possible device capability flags
    
    The old bitwise device_cap_flags variable was limited to u32 which
    has all bits already defined. In order to overcome it, we converted
    device_cap_flags variable to be u64 type.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3a03c1d18afa..c9b62344d22e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -274,7 +274,7 @@ struct ib_device_attr {
 	u32			hw_ver;
 	int			max_qp;
 	int			max_qp_wr;
-	int			device_cap_flags;
+	u64			device_cap_flags;
 	int			max_sge;
 	int			max_sge_rd;
 	int			max_cq;

commit d2ad9cc75963714d04d4596c226a499765950dbf
Merge: 76b06402796c 35d1901134e9 318d311e8f01 95f60bb8118c
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Mar 16 13:38:28 2016 -0400

    Merge branches 'mlx4', 'mlx5' and 'ocrdma' into k.o/for-4.6

commit b4e64397dabc946b83ffb1defa1215ede84c3b97
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Wed Jan 6 10:04:31 2016 -0800

    IB/rdmavt: Break rdma_vt main include header file up
    
    Until all functionality is moved over to rdmavt drivers still need to
    access a number of fields in data structures that are predominantly
    meant to be used by rdmavt. Once these rdmavt_<ibta_object>.h header
    files are no longer being touched by drivers their content should be
    moved to rdmavt/<ibta_object>.h. While here move a couple #defines
    over to more general IB verbs header files because they fit better.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 284b00c8fea4..d7d531cf00b7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -613,6 +613,7 @@ enum {
 };
 
 #define IB_LID_PERMISSIVE	cpu_to_be16(0xFFFF)
+#define IB_MULTICAST_LID_BASE	cpu_to_be16(0xC000)
 
 enum ib_ah_flags {
 	IB_AH_GRH	= 1

commit f5aa9159a418726d74b67c8815ffd2739afb4c7a
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Mon Feb 29 19:07:32 2016 +0200

    IB/core: Add arbitrary sg_list support
    
    Devices that are capable in registering SG lists
    with gaps can now expose it in the core to ULPs
    using a new device capability IB_DEVICE_SG_GAPS_REG
    (in a new field device_cap_flags_ex in the device attributes
    as we ran out of bits), and a new mr_type IB_MR_TYPE_SG_GAPS_REG
    which allocates a memory region which is capable of handling
    SG lists with gaps.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3f79070de547..bcd5b242e6b1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -212,6 +212,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
+	IB_DEVICE_SG_GAPS_REG			= (1ULL << 32),
 };
 
 enum ib_signature_prot_cap {
@@ -662,10 +663,15 @@ __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
  * @IB_MR_TYPE_SIGNATURE:     memory region that is used for
  *                            signature operations (data-integrity
  *                            capable regions)
+ * @IB_MR_TYPE_SG_GAPS:       memory region that is capable to
+ *                            register any arbitrary sg lists (without
+ *                            the normal mr constraints - see
+ *                            ib_map_mr_sg)
  */
 enum ib_mr_type {
 	IB_MR_TYPE_MEM_REG,
 	IB_MR_TYPE_SIGNATURE,
+	IB_MR_TYPE_SG_GAPS,
 };
 
 /**

commit b2a239df4e65fe35240ddf3e5f9f31335c90589b
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Feb 29 18:05:29 2016 +0200

    IB/core: Add vendor's specific data to alloc mw
    
    Passing udata to the vendor's driver in order to pass data from the
    user-space driver to the kernel-space driver. This data will be
    used in downstream patches.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 284b00c8fea4..3f79070de547 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1808,7 +1808,8 @@ struct ib_device {
 						struct scatterlist *sg,
 						int sg_nents);
 	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
-					       enum ib_mw_type type);
+					       enum ib_mw_type type,
+					       struct ib_udata *udata);
 	int                        (*dealloc_mw)(struct ib_mw *mw);
 	struct ib_fmr *	           (*alloc_fmr)(struct ib_pd *pd,
 						int mr_access_flags,

commit a3100a78794175d7f2488a3155d247da3d7390e4
Author: Marina Varshaver <marinav@mellanox.com>
Date:   Thu Feb 18 18:31:05 2016 +0200

    IB/core: Add don't trap flag to flow creation
    
    Don't trap flag (i.e. IB_FLOW_ATTR_FLAGS_DONT_TRAP) indicates that QP
    will receive traffic, but will not steal it.
    
    When a packet matches a flow steering rule that was created with
    the don't trap flag, the QPs assigned to this rule will get this
    packet, but matching will continue to other equal/lower priority
    rules. This will let other QPs assigned to those rules to get the
    packet too.
    
    If both don't trap rule and other rules have the same priority
    and match the same packet, the behavior is undefined.
    
    The don't trap flag can't be set with default rule types
    (i.e. IB_FLOW_ATTR_ALL_DEFAULT, IB_FLOW_ATTR_MC_DEFAULT) as default rules
    don't have rules after them and don't trap has no meaning here.
    
    Signed-off-by: Marina Varshaver <marinav@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 284b00c8fea4..514223f522c8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1487,6 +1487,11 @@ enum ib_flow_domain {
 	IB_FLOW_DOMAIN_NUM /* Must be last */
 };
 
+enum ib_flow_flags {
+	IB_FLOW_ATTR_FLAGS_DONT_TRAP = 1UL << 1, /* Continue match, no steal */
+	IB_FLOW_ATTR_FLAGS_RESERVED  = 1UL << 2  /* Must be last */
+};
+
 struct ib_flow_eth_filter {
 	u8	dst_mac[6];
 	u8	src_mac[6];

commit 765d67748bcf802c4642a49cd0139787d0d80783
Author: Steve Wise <swise@opengridcomputing.com.com>
Date:   Wed Feb 17 08:15:41 2016 -0800

    IB: new common API for draining queues
    
    Add provider-specific drain_sq/drain_rq functions for providers needing
    special drain logic.
    
    Add static functions __ib_drain_sq() and __ib_drain_rq() which post noop
    WRs to the SQ or RQ and block until their completions are processed.
    This ensures the applications completions for work requests posted prior
    to the drain work request have all been processed.
    
    Add API functions ib_drain_sq(), ib_drain_rq(), and ib_drain_qp().
    
    For the drain logic to work, the caller must:
    
    ensure there is room in the CQ(s) and QP for the drain work request
    and completion.
    
    allocate the CQ using ib_alloc_cq() and the CQ poll context cannot be
    IB_POLL_DIRECT.
    
    ensure that there are no other contexts that are posting WRs concurrently.
    Otherwise the drain is not guaranteed.
    
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 284b00c8fea4..68b7e978a27d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1846,6 +1846,8 @@ struct ib_device {
 	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
 						      struct ib_mr_status *mr_status);
 	void			   (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
+	void			   (*drain_rq)(struct ib_qp *qp);
+	void			   (*drain_sq)(struct ib_qp *qp);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 
@@ -3094,4 +3096,7 @@ int ib_sg_to_pages(struct ib_mr *mr,
 		   int sg_nents,
 		   int (*set_page)(struct ib_mr *, u64));
 
+void ib_drain_rq(struct ib_qp *qp);
+void ib_drain_sq(struct ib_qp *qp);
+void ib_drain_qp(struct ib_qp *qp);
 #endif /* IB_VERBS_H */

commit 7ead4bcb1b788732516755ef84ef1272d3e152eb
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Jan 14 17:50:38 2016 +0200

    IB/core: Add definition for the standard RoCE V2 UDP port
    
    This will be used in hardware device driver when building QP or AH
    contexts.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 75fcc97886de..284b00c8fea4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -81,6 +81,7 @@ enum ib_gid_type {
 	IB_GID_TYPE_SIZE
 };
 
+#define ROCE_V2_UDP_DPORT      4791
 struct ib_gid_attr {
 	enum ib_gid_type	gid_type;
 	struct net_device	*ndev;

commit 8a06ce59a4cd034c52c59c44ff6b0785a3969409
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Dec 20 12:16:10 2015 +0200

    IB/core: Add cross-channel support
    
    The cross-channel feature allows to execute WQEs that involve
    synchronization of I/O operations on different QPs.
    
    This capability enables to program complex flows with a single
    function call, hereby significantly reducing overhead associated
    with I/O processing.
    
    Cross-channel operations support is indicated by HCA capability
    information.
    
    The queue pairs can be configured to work as a sync master queue
    or sync slave queues.
    
    The added flags are:
    
    1. Device capability flag IB_DEVICE_CROSS_CHANNEL for the
       devices that can perform cross-channel operations.
    
    2. CQ property flag IB_CQ_FLAGS_IGNORE_OVERRUN to disable CQ overrun
       check. This check is useless in cross-channel scenario.
    
    3. QP property flags to indicate if queues are slave or master:
       * IB_QP_CREATE_MANAGED_SEND indicates that posted send work requests
         will not be executed immediately and requires enabling.
       * IB_QP_CREATE_MANAGED_RECV indicates that posted receive work
         requests will not be executed immediately and requires enabling.
       * IB_QP_CREATE_CROSS_CHANNEL declares the QP to work in cross-channel
         mode. If IB_QP_CREATE_MANAGED_SEND and IB_QP_CREATE_MANAGED_RECV are
         not provided, this QP will be sync master queue, else it will be sync
         slave.
    
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 862b8a07c028..75fcc97886de 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -201,6 +201,13 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MEM_WINDOW_TYPE_2B		= (1 << 24),
 	IB_DEVICE_RC_IP_CSUM			= (1 << 25),
 	IB_DEVICE_RAW_IP_CSUM			= (1 << 26),
+	/*
+	 * Devices should set IB_DEVICE_CROSS_CHANNEL if they
+	 * support execution of WQEs that involve synchronization
+	 * of I/O operations with single completion queue managed
+	 * by hardware.
+	 */
+	IB_DEVICE_CROSS_CHANNEL		= (1 << 27),
 	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
 	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
 	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
@@ -246,6 +253,7 @@ struct ib_odp_caps {
 
 enum ib_cq_creation_flags {
 	IB_CQ_FLAGS_TIMESTAMP_COMPLETION   = 1 << 0,
+	IB_CQ_FLAGS_IGNORE_OVERRUN	   = 1 << 1,
 };
 
 struct ib_cq_init_attr {
@@ -950,6 +958,9 @@ enum ib_qp_type {
 enum ib_qp_create_flags {
 	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
 	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
+	IB_QP_CREATE_CROSS_CHANNEL              = 1 << 2,
+	IB_QP_CREATE_MANAGED_SEND               = 1 << 3,
+	IB_QP_CREATE_MANAGED_RECV               = 1 << 4,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
 	IB_QP_CREATE_USE_GFP_NOIO		= 1 << 7,

commit 7ca0bc53652382529b1cc1c39987cf93084d5828
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Dec 20 12:16:09 2015 +0200

    IB/core: Align coding style of ib_device_cap_flags structure
    
    Modify enum ib_device_cap_flags such that other patches which add new
    enum values pass strict checkpatch.pl checks.
    
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bbb1c349cb3d..862b8a07c028 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -149,21 +149,21 @@ enum rdma_link_layer {
 };
 
 enum ib_device_cap_flags {
-	IB_DEVICE_RESIZE_MAX_WR		= 1,
-	IB_DEVICE_BAD_PKEY_CNTR		= (1<<1),
-	IB_DEVICE_BAD_QKEY_CNTR		= (1<<2),
-	IB_DEVICE_RAW_MULTI		= (1<<3),
-	IB_DEVICE_AUTO_PATH_MIG		= (1<<4),
-	IB_DEVICE_CHANGE_PHY_PORT	= (1<<5),
-	IB_DEVICE_UD_AV_PORT_ENFORCE	= (1<<6),
-	IB_DEVICE_CURR_QP_STATE_MOD	= (1<<7),
-	IB_DEVICE_SHUTDOWN_PORT		= (1<<8),
-	IB_DEVICE_INIT_TYPE		= (1<<9),
-	IB_DEVICE_PORT_ACTIVE_EVENT	= (1<<10),
-	IB_DEVICE_SYS_IMAGE_GUID	= (1<<11),
-	IB_DEVICE_RC_RNR_NAK_GEN	= (1<<12),
-	IB_DEVICE_SRQ_RESIZE		= (1<<13),
-	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
+	IB_DEVICE_RESIZE_MAX_WR			= (1 << 0),
+	IB_DEVICE_BAD_PKEY_CNTR			= (1 << 1),
+	IB_DEVICE_BAD_QKEY_CNTR			= (1 << 2),
+	IB_DEVICE_RAW_MULTI			= (1 << 3),
+	IB_DEVICE_AUTO_PATH_MIG			= (1 << 4),
+	IB_DEVICE_CHANGE_PHY_PORT		= (1 << 5),
+	IB_DEVICE_UD_AV_PORT_ENFORCE		= (1 << 6),
+	IB_DEVICE_CURR_QP_STATE_MOD		= (1 << 7),
+	IB_DEVICE_SHUTDOWN_PORT			= (1 << 8),
+	IB_DEVICE_INIT_TYPE			= (1 << 9),
+	IB_DEVICE_PORT_ACTIVE_EVENT		= (1 << 10),
+	IB_DEVICE_SYS_IMAGE_GUID		= (1 << 11),
+	IB_DEVICE_RC_RNR_NAK_GEN		= (1 << 12),
+	IB_DEVICE_SRQ_RESIZE			= (1 << 13),
+	IB_DEVICE_N_NOTIFY_CQ			= (1 << 14),
 
 	/*
 	 * This device supports a per-device lkey or stag that can be
@@ -172,9 +172,9 @@ enum ib_device_cap_flags {
 	 * instead of use the local_dma_lkey flag in the ib_pd structure,
 	 * which will always contain a usable lkey.
 	 */
-	IB_DEVICE_LOCAL_DMA_LKEY	= (1<<15),
-	IB_DEVICE_RESERVED		= (1<<16), /* old SEND_W_INV */
-	IB_DEVICE_MEM_WINDOW		= (1<<17),
+	IB_DEVICE_LOCAL_DMA_LKEY		= (1 << 15),
+	IB_DEVICE_RESERVED /* old SEND_W_INV */	= (1 << 16),
+	IB_DEVICE_MEM_WINDOW			= (1 << 17),
 	/*
 	 * Devices should set IB_DEVICE_UD_IP_SUM if they support
 	 * insertion of UDP and TCP checksum on outgoing UD IPoIB
@@ -182,9 +182,9 @@ enum ib_device_cap_flags {
 	 * incoming messages.  Setting this flag implies that the
 	 * IPoIB driver may set NETIF_F_IP_CSUM for datagram mode.
 	 */
-	IB_DEVICE_UD_IP_CSUM		= (1<<18),
-	IB_DEVICE_UD_TSO		= (1<<19),
-	IB_DEVICE_XRC			= (1<<20),
+	IB_DEVICE_UD_IP_CSUM			= (1 << 18),
+	IB_DEVICE_UD_TSO			= (1 << 19),
+	IB_DEVICE_XRC				= (1 << 20),
 
 	/*
 	 * This device supports the IB "base memory management extension",
@@ -195,15 +195,15 @@ enum ib_device_cap_flags {
 	 * IB_WR_RDMA_READ_WITH_INV verb for RDMA READs that invalidate the
 	 * stag.
 	 */
-	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
-	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
-	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
-	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24),
-	IB_DEVICE_RC_IP_CSUM		= (1<<25),
-	IB_DEVICE_RAW_IP_CSUM		= (1<<26),
-	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29),
-	IB_DEVICE_SIGNATURE_HANDOVER	= (1<<30),
-	IB_DEVICE_ON_DEMAND_PAGING	= (1<<31),
+	IB_DEVICE_MEM_MGT_EXTENSIONS		= (1 << 21),
+	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK	= (1 << 22),
+	IB_DEVICE_MEM_WINDOW_TYPE_2A		= (1 << 23),
+	IB_DEVICE_MEM_WINDOW_TYPE_2B		= (1 << 24),
+	IB_DEVICE_RC_IP_CSUM			= (1 << 25),
+	IB_DEVICE_RAW_IP_CSUM			= (1 << 26),
+	IB_DEVICE_MANAGED_FLOW_STEERING		= (1 << 29),
+	IB_DEVICE_SIGNATURE_HANDOVER		= (1 << 30),
+	IB_DEVICE_ON_DEMAND_PAGING		= (1 << 31),
 };
 
 enum ib_signature_prot_cap {

commit 301a721e1fcb890afc29997f46de9561686ed391
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Dec 15 20:30:10 2015 +0200

    IB/core: Add ib_is_udata_cleared
    
    Extending core and vendor verb commands require us to check that the
    unknown part of the user's given command is all zeros.
    Adding ib_is_udata_cleared in order to do so.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 36acb30eac85..bbb1c349cb3d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -53,6 +53,8 @@
 #include <uapi/linux/if_ether.h>
 #include <net/ipv6.h>
 #include <net/ip.h>
+#include <linux/string.h>
+#include <linux/slab.h>
 
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
@@ -1922,6 +1924,31 @@ static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len
 	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
 }
 
+static inline bool ib_is_udata_cleared(struct ib_udata *udata,
+				       size_t offset,
+				       size_t len)
+{
+	const void __user *p = udata->inbuf + offset;
+	bool ret = false;
+	u8 *buf;
+
+	if (len > USHRT_MAX)
+		return false;
+
+	buf = kmalloc(len, GFP_KERNEL);
+	if (!buf)
+		return false;
+
+	if (copy_from_user(buf, p, len))
+		goto free;
+
+	ret = !memchr_inv(buf, 0, len);
+
+free:
+	kfree(buf);
+	return ret;
+}
+
 /**
  * ib_modify_qp_is_ok - Check that the supplied attribute mask
  * contains all required attributes and no attributes not allowed for

commit ab67ed8de0250e9ad7956ff4d98c3c98858b6c3c
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:54 2015 +0100

    IB: remove the write-only usecnt field from struct ib_mr
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bvanassche@sandisk.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 197b620eec9f..36acb30eac85 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1414,7 +1414,6 @@ struct ib_mr {
 	u64		   iova;
 	u32		   length;
 	unsigned int	   page_size;
-	atomic_t	   usecnt; /* count number of MWs */
 };
 
 struct ib_mw {

commit 7cf9ff643b7f709173ca6ff6376fdff5b8d16124
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:53 2015 +0100

    IB: remove the struct ib_phys_buf definition
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com> [core]
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 177844265c98..197b620eec9f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1251,11 +1251,6 @@ enum ib_access_flags {
 	IB_ACCESS_ON_DEMAND     = (1<<6),
 };
 
-struct ib_phys_buf {
-	u64      addr;
-	u64      size;
-};
-
 /*
  * XXX: these are apparently used for ->rereg_user_mr, no idea why they
  * are hidden here instead of a uapi header!

commit feb7c1e38bccfd18cc06677cb648ed2340788fe8
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:48 2015 +0100

    IB: remove in-kernel support for memory windows
    
    Remove the unused ib_allow_mw and ib_bind_mw functions, remove the
    unused IB_WR_BIND_MW and IB_WC_BIND_MW opcodes and move ib_dealloc_mw
    into the uverbs module.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com> [core]
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 86970f3e90b4..177844265c98 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -812,7 +812,6 @@ enum ib_wc_opcode {
 	IB_WC_RDMA_READ,
 	IB_WC_COMP_SWAP,
 	IB_WC_FETCH_ADD,
-	IB_WC_BIND_MW,
 	IB_WC_LSO,
 	IB_WC_LOCAL_INV,
 	IB_WC_REG_MR,
@@ -1110,7 +1109,6 @@ enum ib_wr_opcode {
 	IB_WR_REG_MR,
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
-	IB_WR_BIND_MW,
 	IB_WR_REG_SIG_MR,
 	/* reserve values for low level drivers' internal use.
 	 * These values will not be used at all in the ib core layer.
@@ -1145,23 +1143,6 @@ struct ib_sge {
 	u32	lkey;
 };
 
-/**
- * struct ib_mw_bind_info - Parameters for a memory window bind operation.
- * @mr: A memory region to bind the memory window to.
- * @addr: The address where the memory window should begin.
- * @length: The length of the memory window, in bytes.
- * @mw_access_flags: Access flags from enum ib_access_flags for the window.
- *
- * This struct contains the shared parameters for type 1 and type 2
- * memory window bind operations.
- */
-struct ib_mw_bind_info {
-	struct ib_mr   *mr;
-	u64		addr;
-	u64		length;
-	int		mw_access_flags;
-};
-
 struct ib_cqe {
 	void (*done)(struct ib_cq *cq, struct ib_wc *wc);
 };
@@ -1237,19 +1218,6 @@ static inline struct ib_reg_wr *reg_wr(struct ib_send_wr *wr)
 	return container_of(wr, struct ib_reg_wr, wr);
 }
 
-struct ib_bind_mw_wr {
-	struct ib_send_wr	wr;
-	struct ib_mw		*mw;
-	/* The new rkey for the memory window. */
-	u32			rkey;
-	struct ib_mw_bind_info	bind_info;
-};
-
-static inline struct ib_bind_mw_wr *bind_mw_wr(struct ib_send_wr *wr)
-{
-	return container_of(wr, struct ib_bind_mw_wr, wr);
-}
-
 struct ib_sig_handover_wr {
 	struct ib_send_wr	wr;
 	struct ib_sig_attrs    *sig_attrs;
@@ -1299,18 +1267,6 @@ enum ib_mr_rereg_flags {
 	IB_MR_REREG_SUPPORTED	= ((IB_MR_REREG_ACCESS << 1) - 1)
 };
 
-/**
- * struct ib_mw_bind - Parameters for a type 1 memory window bind operation.
- * @wr_id:      Work request id.
- * @send_flags: Flags from ib_send_flags enum.
- * @bind_info:  More parameters of the bind operation.
- */
-struct ib_mw_bind {
-	u64                    wr_id;
-	int                    send_flags;
-	struct ib_mw_bind_info bind_info;
-};
-
 struct ib_fmr_attr {
 	int	max_pages;
 	int	max_maps;
@@ -1845,9 +1801,6 @@ struct ib_device {
 						int sg_nents);
 	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
 					       enum ib_mw_type type);
-	int                        (*bind_mw)(struct ib_qp *qp,
-					      struct ib_mw *mw,
-					      struct ib_mw_bind *mw_bind);
 	int                        (*dealloc_mw)(struct ib_mw *mw);
 	struct ib_fmr *	           (*alloc_fmr)(struct ib_pd *pd,
 						int mr_access_flags,
@@ -2975,42 +2928,6 @@ static inline u32 ib_inc_rkey(u32 rkey)
 	return ((rkey + 1) & mask) | (rkey & ~mask);
 }
 
-/**
- * ib_alloc_mw - Allocates a memory window.
- * @pd: The protection domain associated with the memory window.
- * @type: The type of the memory window (1 or 2).
- */
-struct ib_mw *ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type);
-
-/**
- * ib_bind_mw - Posts a work request to the send queue of the specified
- *   QP, which binds the memory window to the given address range and
- *   remote access attributes.
- * @qp: QP to post the bind work request on.
- * @mw: The memory window to bind.
- * @mw_bind: Specifies information about the memory window, including
- *   its address range, remote access rights, and associated memory region.
- *
- * If there is no immediate error, the function will update the rkey member
- * of the mw parameter to its new value. The bind operation can still fail
- * asynchronously.
- */
-static inline int ib_bind_mw(struct ib_qp *qp,
-			     struct ib_mw *mw,
-			     struct ib_mw_bind *mw_bind)
-{
-	/* XXX reference counting in corresponding MR? */
-	return mw->device->bind_mw ?
-		mw->device->bind_mw(qp, mw, mw_bind) :
-		-ENOSYS;
-}
-
-/**
- * ib_dealloc_mw - Deallocates a memory window.
- * @mw: The memory window to deallocate.
- */
-int ib_dealloc_mw(struct ib_mw *mw);
-
 /**
  * ib_alloc_fmr - Allocates a unmapped fast memory region.
  * @pd: The protection domain associated with the unmapped region.

commit b7d3e0a94fe128912bbebf0ae68551c85fd2d429
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:47 2015 +0100

    IB: remove support for phys MRs
    
    We have stopped using phys MRs in the kernel a while ago, so let's
    remove all the cruft used to implement them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com> [core]
    Reviewed-By: Devesh Sharma<devesh.sharma@avagotech.com> [ocrdma]
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 368d9559472f..86970f3e90b4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1288,6 +1288,10 @@ struct ib_phys_buf {
 	u64      size;
 };
 
+/*
+ * XXX: these are apparently used for ->rereg_user_mr, no idea why they
+ * are hidden here instead of a uapi header!
+ */
 enum ib_mr_rereg_flags {
 	IB_MR_REREG_TRANS	= 1,
 	IB_MR_REREG_PD		= (1<<1),
@@ -1820,11 +1824,6 @@ struct ib_device {
 						      int wc_cnt);
 	struct ib_mr *             (*get_dma_mr)(struct ib_pd *pd,
 						 int mr_access_flags);
-	struct ib_mr *             (*reg_phys_mr)(struct ib_pd *pd,
-						  struct ib_phys_buf *phys_buf_array,
-						  int num_phys_buf,
-						  int mr_access_flags,
-						  u64 *iova_start);
 	struct ib_mr *             (*reg_user_mr)(struct ib_pd *pd,
 						  u64 start, u64 length,
 						  u64 virt_addr,
@@ -1844,13 +1843,6 @@ struct ib_device {
 	int                        (*map_mr_sg)(struct ib_mr *mr,
 						struct scatterlist *sg,
 						int sg_nents);
-	int                        (*rereg_phys_mr)(struct ib_mr *mr,
-						    int mr_rereg_mask,
-						    struct ib_pd *pd,
-						    struct ib_phys_buf *phys_buf_array,
-						    int num_phys_buf,
-						    int mr_access_flags,
-						    u64 *iova_start);
 	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
 					       enum ib_mw_type type);
 	int                        (*bind_mw)(struct ib_qp *qp,

commit a4d825a01e51b9c74d5a64237dd8b901822cf035
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:46 2015 +0100

    IB: remove ib_query_mr
    
    This functionality has no users and was only supported by the staged out
    EHCA driver.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com> [core]
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f06ce80f12e3..368d9559472f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1288,15 +1288,6 @@ struct ib_phys_buf {
 	u64      size;
 };
 
-struct ib_mr_attr {
-	struct ib_pd	*pd;
-	u64		device_virt_addr;
-	u64		size;
-	int		mr_access_flags;
-	u32		lkey;
-	u32		rkey;
-};
-
 enum ib_mr_rereg_flags {
 	IB_MR_REREG_TRANS	= 1,
 	IB_MR_REREG_PD		= (1<<1),
@@ -1846,8 +1837,6 @@ struct ib_device {
 						    int mr_access_flags,
 						    struct ib_pd *pd,
 						    struct ib_udata *udata);
-	int                        (*query_mr)(struct ib_mr *mr,
-					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);
 	struct ib_mr *		   (*alloc_mr)(struct ib_pd *pd,
 					       enum ib_mr_type mr_type,
@@ -2958,13 +2947,6 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 		dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 
-/**
- * ib_query_mr - Retrieves information about a specific memory region.
- * @mr: The memory region to retrieve information about.
- * @mr_attr: The attributes of the specified memory region.
- */
-int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
-
 /**
  * ib_dereg_mr - Deregisters a memory region and removes it from the
  *   HCA translation table.

commit b1adc7146af54487bb2ce1cb3012e66a1bbd8e39
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Dec 23 19:12:45 2015 +0100

    IB: start documenting device capabilities
    
    Just IB_DEVICE_LOCAL_DMA_LKEY and IB_DEVICE_MEM_MGT_EXTENSIONS for now
    as I'm most familar with those.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-By: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 368fc22f30f1..f06ce80f12e3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -162,6 +162,14 @@ enum ib_device_cap_flags {
 	IB_DEVICE_RC_RNR_NAK_GEN	= (1<<12),
 	IB_DEVICE_SRQ_RESIZE		= (1<<13),
 	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
+
+	/*
+	 * This device supports a per-device lkey or stag that can be
+	 * used without performing a memory registration for the local
+	 * memory.  Note that ULPs should never check this flag, but
+	 * instead of use the local_dma_lkey flag in the ib_pd structure,
+	 * which will always contain a usable lkey.
+	 */
 	IB_DEVICE_LOCAL_DMA_LKEY	= (1<<15),
 	IB_DEVICE_RESERVED		= (1<<16), /* old SEND_W_INV */
 	IB_DEVICE_MEM_WINDOW		= (1<<17),
@@ -175,6 +183,16 @@ enum ib_device_cap_flags {
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
 	IB_DEVICE_XRC			= (1<<20),
+
+	/*
+	 * This device supports the IB "base memory management extension",
+	 * which includes support for fast registrations (IB_WR_REG_MR,
+	 * IB_WR_LOCAL_INV and IB_WR_SEND_WITH_INV verbs).  This flag should
+	 * also be set by any iWarp device which must support FRs to comply
+	 * to the iWarp verbs spec.  iWarp devices also support the
+	 * IB_WR_RDMA_READ_WITH_INV verb for RDMA READs that invalidate the
+	 * stag.
+	 */
 	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),

commit c865f24628b9310e1815d59f723a34ea3df4890f
Author: Somnath Kotur <Somnath.Kotur@Avagotech.Com>
Date:   Wed Dec 23 14:56:51 2015 +0200

    IB/core: Add rdma_network_type to wc
    
    Providers should tell IB core the wc's network type.
    This is used in order to search for the proper GID in the
    GID table. When using HCAs that can't provide this info,
    IB core tries to deep examine the packet and extract
    the GID type by itself.
    
    We choose sgid_index and type from all the matching entries in
    RDMA-CM based on hint from the IP stack and we set hop_limit for
    the IP packet based on above hint from IP stack.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Somnath Kotur <Somnath.Kotur@Avagotech.Com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ab05ef695d63..368fc22f30f1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -51,6 +51,8 @@
 #include <linux/socket.h>
 #include <linux/irq_poll.h>
 #include <uapi/linux/if_ether.h>
+#include <net/ipv6.h>
+#include <net/ip.h>
 
 #include <linux/atomic.h>
 #include <linux/mmu_notifier.h>
@@ -109,6 +111,35 @@ enum rdma_protocol_type {
 __attribute_const__ enum rdma_transport_type
 rdma_node_get_transport(enum rdma_node_type node_type);
 
+enum rdma_network_type {
+	RDMA_NETWORK_IB,
+	RDMA_NETWORK_ROCE_V1 = RDMA_NETWORK_IB,
+	RDMA_NETWORK_IPV4,
+	RDMA_NETWORK_IPV6
+};
+
+static inline enum ib_gid_type ib_network_to_gid_type(enum rdma_network_type network_type)
+{
+	if (network_type == RDMA_NETWORK_IPV4 ||
+	    network_type == RDMA_NETWORK_IPV6)
+		return IB_GID_TYPE_ROCE_UDP_ENCAP;
+
+	/* IB_GID_TYPE_IB same as RDMA_NETWORK_ROCE_V1 */
+	return IB_GID_TYPE_IB;
+}
+
+static inline enum rdma_network_type ib_gid_to_network_type(enum ib_gid_type gid_type,
+							    union ib_gid *gid)
+{
+	if (gid_type == IB_GID_TYPE_IB)
+		return RDMA_NETWORK_IB;
+
+	if (ipv6_addr_v4mapped((struct in6_addr *)gid))
+		return RDMA_NETWORK_IPV4;
+	else
+		return RDMA_NETWORK_IPV6;
+}
+
 enum rdma_link_layer {
 	IB_LINK_LAYER_UNSPECIFIED,
 	IB_LINK_LAYER_INFINIBAND,
@@ -537,6 +568,17 @@ struct ib_grh {
 	union ib_gid	dgid;
 };
 
+union rdma_network_hdr {
+	struct ib_grh ibgrh;
+	struct {
+		/* The IB spec states that if it's IPv4, the header
+		 * is located in the last 20 bytes of the header.
+		 */
+		u8		reserved[20];
+		struct iphdr	roce4grh;
+	};
+};
+
 enum {
 	IB_MULTICAST_QPN = 0xffffff
 };
@@ -773,6 +815,7 @@ enum ib_wc_flags {
 	IB_WC_IP_CSUM_OK	= (1<<3),
 	IB_WC_WITH_SMAC		= (1<<4),
 	IB_WC_WITH_VLAN		= (1<<5),
+	IB_WC_WITH_NETWORK_HDR_TYPE	= (1<<6),
 };
 
 struct ib_wc {
@@ -798,6 +841,7 @@ struct ib_wc {
 	u8			port_num;	/* valid only for DR SMPs on switches */
 	u8			smac[ETH_ALEN];
 	u16			vlan_id;
+	u8			network_hdr_type;
 };
 
 enum ib_cq_notify_flags {

commit 7766a99fdcd30c78fc8299db9102e3624232007c
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Dec 23 14:56:50 2015 +0200

    IB/core: Add ROCE_UDP_ENCAP (RoCE V2) type
    
    Adding RoCE v2 GID type and port type. Vendors
    which support this type will get their GID table
    populated with RoCE v2 GIDs automatically.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 00d2006ad816..ab05ef695d63 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -73,6 +73,7 @@ enum ib_gid_type {
 	/* If link layer is Ethernet, this is RoCE V1 */
 	IB_GID_TYPE_IB        = 0,
 	IB_GID_TYPE_ROCE      = 0,
+	IB_GID_TYPE_ROCE_UDP_ENCAP = 1,
 	IB_GID_TYPE_SIZE
 };
 
@@ -403,6 +404,7 @@ union rdma_protocol_stats {
 #define RDMA_CORE_CAP_PROT_IB           0x00100000
 #define RDMA_CORE_CAP_PROT_ROCE         0x00200000
 #define RDMA_CORE_CAP_PROT_IWARP        0x00400000
+#define RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP 0x00800000
 
 #define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \
 					| RDMA_CORE_CAP_IB_MAD \
@@ -415,6 +417,12 @@ union rdma_protocol_stats {
 					| RDMA_CORE_CAP_IB_CM   \
 					| RDMA_CORE_CAP_AF_IB   \
 					| RDMA_CORE_CAP_ETH_AH)
+#define RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP			\
+					(RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP \
+					| RDMA_CORE_CAP_IB_MAD  \
+					| RDMA_CORE_CAP_IB_CM   \
+					| RDMA_CORE_CAP_AF_IB   \
+					| RDMA_CORE_CAP_ETH_AH)
 #define RDMA_CORE_PORT_IWARP           (RDMA_CORE_CAP_PROT_IWARP \
 					| RDMA_CORE_CAP_IW_CM)
 #define RDMA_CORE_PORT_INTEL_OPA       (RDMA_CORE_PORT_IBA_IB  \
@@ -2000,6 +2008,17 @@ static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)
 }
 
 static inline bool rdma_protocol_roce(const struct ib_device *device, u8 port_num)
+{
+	return device->port_immutable[port_num].core_cap_flags &
+		(RDMA_CORE_CAP_PROT_ROCE | RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP);
+}
+
+static inline bool rdma_protocol_roce_udp_encap(const struct ib_device *device, u8 port_num)
+{
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;
+}
+
+static inline bool rdma_protocol_roce_eth_encap(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE;
 }
@@ -2011,8 +2030,8 @@ static inline bool rdma_protocol_iwarp(const struct ib_device *device, u8 port_n
 
 static inline bool rdma_ib_or_roce(const struct ib_device *device, u8 port_num)
 {
-	return device->port_immutable[port_num].core_cap_flags &
-		(RDMA_CORE_CAP_PROT_IB | RDMA_CORE_CAP_PROT_ROCE);
+	return rdma_protocol_ib(device, port_num) ||
+		rdma_protocol_roce(device, port_num);
 }
 
 /**

commit b39ffa1df505378336a85064ad9ec403765bbb0b
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Dec 23 14:56:47 2015 +0200

    IB/core: Add gid_type to gid attribute
    
    In order to support multiple GID types, we need to store the gid_type
    with each GID. This is also aligned with the RoCE v2 annex "RoCEv2 PORT
    GID table entries shall have a "GID type" attribute that denotes the L3
    Address type". The currently supported GID is IB_GID_TYPE_IB which is
    also RoCE v1 GID type.
    
    This implies that gid_type should be added to roce_gid_table meta-data.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b14feaba39f1..00d2006ad816 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -69,7 +69,15 @@ union ib_gid {
 
 extern union ib_gid zgid;
 
+enum ib_gid_type {
+	/* If link layer is Ethernet, this is RoCE V1 */
+	IB_GID_TYPE_IB        = 0,
+	IB_GID_TYPE_ROCE      = 0,
+	IB_GID_TYPE_SIZE
+};
+
 struct ib_gid_attr {
+	enum ib_gid_type	gid_type;
 	struct net_device	*ndev;
 };
 
@@ -2245,7 +2253,8 @@ int ib_modify_port(struct ib_device *device,
 		   struct ib_port_modify *port_modify);
 
 int ib_find_gid(struct ib_device *device, union ib_gid *gid,
-		struct net_device *ndev, u8 *port_num, u16 *index);
+		enum ib_gid_type gid_type, struct net_device *ndev,
+		u8 *port_num, u16 *index);
 
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);

commit 882f3b3b916a01c703bf7516800a0dca741cd3e7
Merge: 182a2da0c768 c6333f9f9f76
Author: Doug Ledford <dledford@redhat.com>
Date:   Tue Dec 22 17:03:15 2015 -0500

    Merge branches '4.5/Or-cleanup' and '4.5/rdma-cq' into k.o/for-4.5
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    
    Conflicts:
            drivers/infiniband/ulp/iser/iser_verbs.c

commit 182a2da0c768a9ec64abb0d6009667057f1c06af
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Fri Dec 18 10:59:50 2015 +0200

    IB/core: Remove ib_query_device
    
    The copy of the attributes present on the device is now used by all consumers
    except for uverbs in case of serving user-space query, where dev->query_device
    is called.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 730dcfb209b9..0567866a05e2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1913,9 +1913,6 @@ int ib_register_event_handler  (struct ib_event_handler *event_handler);
 int ib_unregister_event_handler(struct ib_event_handler *event_handler);
 void ib_dispatch_event(struct ib_event *event);
 
-int ib_query_device(struct ib_device *device,
-		    struct ib_device_attr *device_attr);
-
 int ib_query_port(struct ib_device *device,
 		  u8 port_num, struct ib_port_attr *port_attr);
 

commit 3e153a93a1c12e3354dd38cca414fb51a15136a2
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Fri Dec 18 10:59:44 2015 +0200

    IB/core: Save the device attributes on the device structure
    
    This way both the IB core and upper level drivers can access these cached
    device attributes rather than querying or caching them on their own.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 120da1d7f57e..730dcfb209b9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1823,6 +1823,7 @@ struct ib_device {
 	u16                          is_switch:1;
 	u8                           node_type;
 	u8                           phys_port_cnt;
+	struct ib_device_attr        attrs;
 
 	/**
 	 * The following mandatory functions are used only at device

commit c6333f9f9f7646e311248a09e8ed96126a97aba8
Merge: 9f9499ae8e64 cfeb91b375ad
Author: Doug Ledford <dledford@redhat.com>
Date:   Tue Dec 15 14:10:44 2015 -0500

    Merge branch 'rdma-cq.2' of git://git.infradead.org/users/hch/rdma into 4.5/rdma-cq
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    
    Conflicts:
            drivers/infiniband/ulp/srp/ib_srp.c - Conflicts with changes in
            ib_srp.c introduced during 4.4-rc updates

commit 14d3a3b2498edadec344cb11e60e66091f5daf63
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 11 11:53:03 2015 -0800

    IB: add a proper completion queue abstraction
    
    This adds an abstraction that allows ULPs to simply pass a completion
    object and completion callback with each submitted WR and let the RDMA
    core handle the nitty gritty details of how to handle completion
    interrupts and poll the CQ.
    
    In detail there is a new ib_cqe structure which just contains the
    completion callback, and which can be used to get at the containing
    object using container_of.  It is pointed to by the WR and WC as an
    alternative to the wr_id field, similar to how many ULPs already use
    the field to store a pointer using casts.
    
    A driver using the new completion callbacks allocates it's CQs using
    the new ib_create_cq API, which in addition to the number of CQEs and
    the completion vectors also takes a mode on how we poll for CQEs.
    Three modes are available: direct for drivers that never take CQ
    interrupts and just poll for them, softirq to poll from softirq context
    using the to be renamed blk-iopoll infrastructure which takes care of
    rearming and budgeting, or a workqueue for consumer who want to be
    called from user context.
    
    Thanks a lot to Sagi Grimberg who helped reviewing the API, wrote
    the current version of the workqueue code because my two previous
    attempts sucked too much and converted the iSER initiator to the new
    API.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9a68a19532ba..131dd4b5176c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -49,6 +49,7 @@
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
 #include <linux/socket.h>
+#include <linux/irq_poll.h>
 #include <uapi/linux/if_ether.h>
 
 #include <linux/atomic.h>
@@ -56,6 +57,7 @@
 #include <asm/uaccess.h>
 
 extern struct workqueue_struct *ib_wq;
+extern struct workqueue_struct *ib_comp_wq;
 
 union ib_gid {
 	u8	raw[16];
@@ -758,7 +760,10 @@ enum ib_wc_flags {
 };
 
 struct ib_wc {
-	u64			wr_id;
+	union {
+		u64		wr_id;
+		struct ib_cqe	*wr_cqe;
+	};
 	enum ib_wc_status	status;
 	enum ib_wc_opcode	opcode;
 	u32			vendor_err;
@@ -1079,9 +1084,16 @@ struct ib_mw_bind_info {
 	int		mw_access_flags;
 };
 
+struct ib_cqe {
+	void (*done)(struct ib_cq *cq, struct ib_wc *wc);
+};
+
 struct ib_send_wr {
 	struct ib_send_wr      *next;
-	u64			wr_id;
+	union {
+		u64		wr_id;
+		struct ib_cqe	*wr_cqe;
+	};
 	struct ib_sge	       *sg_list;
 	int			num_sge;
 	enum ib_wr_opcode	opcode;
@@ -1175,7 +1187,10 @@ static inline struct ib_sig_handover_wr *sig_handover_wr(struct ib_send_wr *wr)
 
 struct ib_recv_wr {
 	struct ib_recv_wr      *next;
-	u64			wr_id;
+	union {
+		u64		wr_id;
+		struct ib_cqe	*wr_cqe;
+	};
 	struct ib_sge	       *sg_list;
 	int			num_sge;
 };
@@ -1306,6 +1321,12 @@ struct ib_ah {
 
 typedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);
 
+enum ib_poll_context {
+	IB_POLL_DIRECT,		/* caller context, no hw completions */
+	IB_POLL_SOFTIRQ,	/* poll from softirq context */
+	IB_POLL_WORKQUEUE,	/* poll from workqueue */
+};
+
 struct ib_cq {
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;
@@ -1314,6 +1335,12 @@ struct ib_cq {
 	void                   *cq_context;
 	int               	cqe;
 	atomic_t          	usecnt; /* count number of work queues */
+	enum ib_poll_context	poll_ctx;
+	struct ib_wc		*wc;
+	union {
+		struct irq_poll		iop;
+		struct work_struct	work;
+	};
 };
 
 struct ib_srq {
@@ -2453,6 +2480,11 @@ static inline int ib_post_recv(struct ib_qp *qp,
 	return qp->device->post_recv(qp, recv_wr, bad_recv_wr);
 }
 
+struct ib_cq *ib_alloc_cq(struct ib_device *dev, void *private,
+		int nr_cqe, int comp_vector, enum ib_poll_context poll_ctx);
+void ib_free_cq(struct ib_cq *cq);
+int ib_process_cq_direct(struct ib_cq *cq, int budget);
+
 /**
  * ib_create_cq - Creates a CQ on the specified device.
  * @device: The device on which to create the CQ.

commit d144da8c6f51f48ec39d891ea9dff80169c45f3b
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Mon Nov 2 12:13:25 2015 -0500

    IB/core: use RCU for uverbs id lookup
    
    The current implementation gets a spin_lock, and at any scale with
    qib and hfi1 post send, the lock contention grows exponentially
    with the number of QPs.
    
    idr_find() is RCU compatibile, so read doesn't need the lock.
    
    Change to use rcu_read_lock() and rcu_read_unlock() in
    __idr_get_uobj().
    
    kfree_rcu() is used to insure a grace period between the
    idr removal and actual free.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-By: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 9a68a19532ba..120da1d7f57e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1271,6 +1271,7 @@ struct ib_uobject {
 	int			id;		/* index into kernel idr */
 	struct kref		ref;
 	struct rw_semaphore	mutex;		/* protects .live */
+	struct rcu_head		rcu;		/* kfree_rcu() overhead */
 	int			live;
 };
 

commit db7489e07669073970358b6cacf6a9dd8dc9275e
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Mon Aug 3 10:01:52 2015 -0700

    IB/core, cma: Make __attribute_const__ declarations sparse-friendly
    
    Move the __attribute_const__ declarations such that sparse understands
    that these apply to the function itself and not to the return type.
    This avoids that sparse reports error messages like the following:
    
    drivers/infiniband/core/verbs.c:73:12: error: symbol 'ib_event_msg' redeclared with different type (originally declared at include/rdma/ib_verbs.h:470) - different modifiers
    
    Fixes: 2b1b5b601230 ("IB/core, cma: Nice log-friendly string helpers")
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 324e9bf8e66c..9a68a19532ba 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -476,7 +476,7 @@ enum ib_event_type {
 	IB_EVENT_GID_CHANGE,
 };
 
-__attribute_const__ const char *ib_event_msg(enum ib_event_type event);
+const char *__attribute_const__ ib_event_msg(enum ib_event_type event);
 
 struct ib_event {
 	struct ib_device	*device;
@@ -726,7 +726,7 @@ enum ib_wc_status {
 	IB_WC_GENERAL_ERR
 };
 
-__attribute_const__ const char *ib_wc_status_msg(enum ib_wc_status status);
+const char *__attribute_const__ ib_wc_status_msg(enum ib_wc_status status);
 
 enum ib_wc_opcode {
 	IB_WC_SEND,

commit 39bfc271bd687be2c8e396e976c0fb9a97963400
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:49 2015 +0300

    IB/core: Remove old fast registration API
    
    No callers and no providers left, go ahead and remove it.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 065d37c13aa6..324e9bf8e66c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -737,7 +737,6 @@ enum ib_wc_opcode {
 	IB_WC_BIND_MW,
 	IB_WC_LSO,
 	IB_WC_LOCAL_INV,
-	IB_WC_FAST_REG_MR,
 	IB_WC_REG_MR,
 	IB_WC_MASKED_COMP_SWAP,
 	IB_WC_MASKED_FETCH_ADD,
@@ -1025,7 +1024,6 @@ enum ib_wr_opcode {
 	IB_WR_SEND_WITH_INV,
 	IB_WR_RDMA_READ_WITH_INV,
 	IB_WR_LOCAL_INV,
-	IB_WR_FAST_REG_MR,
 	IB_WR_REG_MR,
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
@@ -1064,12 +1062,6 @@ struct ib_sge {
 	u32	lkey;
 };
 
-struct ib_fast_reg_page_list {
-	struct ib_device       *device;
-	u64		       *page_list;
-	unsigned int		max_page_list_len;
-};
-
 /**
  * struct ib_mw_bind_info - Parameters for a memory window bind operation.
  * @mr: A memory region to bind the memory window to.
@@ -1143,22 +1135,6 @@ static inline struct ib_ud_wr *ud_wr(struct ib_send_wr *wr)
 	return container_of(wr, struct ib_ud_wr, wr);
 }
 
-struct ib_fast_reg_wr {
-	struct ib_send_wr	wr;
-	u64			iova_start;
-	struct ib_fast_reg_page_list *page_list;
-	unsigned int		page_shift;
-	unsigned int		page_list_len;
-	u32			length;
-	int			access_flags;
-	u32			rkey;
-};
-
-static inline struct ib_fast_reg_wr *fast_reg_wr(struct ib_send_wr *wr)
-{
-	return container_of(wr, struct ib_fast_reg_wr, wr);
-}
-
 struct ib_reg_wr {
 	struct ib_send_wr	wr;
 	struct ib_mr		*mr;
@@ -1773,9 +1749,6 @@ struct ib_device {
 	int                        (*map_mr_sg)(struct ib_mr *mr,
 						struct scatterlist *sg,
 						int sg_nents);
-	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
-								   int page_list_len);
-	void			   (*free_fast_reg_page_list)(struct ib_fast_reg_page_list *page_list);
 	int                        (*rereg_phys_mr)(struct ib_mr *mr,
 						    int mr_rereg_mask,
 						    struct ib_pd *pd,
@@ -2884,33 +2857,6 @@ struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
 			  enum ib_mr_type mr_type,
 			  u32 max_num_sg);
 
-/**
- * ib_alloc_fast_reg_page_list - Allocates a page list array
- * @device - ib device pointer.
- * @page_list_len - size of the page list array to be allocated.
- *
- * This allocates and returns a struct ib_fast_reg_page_list * and a
- * page_list array that is at least page_list_len in size.  The actual
- * size is returned in max_page_list_len.  The caller is responsible
- * for initializing the contents of the page_list array before posting
- * a send work request with the IB_WC_FAST_REG_MR opcode.
- *
- * The page_list array entries must be translated using one of the
- * ib_dma_*() functions just like the addresses passed to
- * ib_map_phys_fmr().  Once the ib_post_send() is issued, the struct
- * ib_fast_reg_page_list must not be modified by the caller until the
- * IB_WC_FAST_REG_MR work request completes.
- */
-struct ib_fast_reg_page_list *ib_alloc_fast_reg_page_list(
-				struct ib_device *device, int page_list_len);
-
-/**
- * ib_free_fast_reg_page_list - Deallocates a previously allocated
- *   page list array.
- * @page_list - struct ib_fast_reg_page_list pointer to be deallocated.
- */
-void ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);
-
 /**
  * ib_update_fast_reg_key - updates the key portion of the fast_reg MR
  *   R_Key and L_Key.

commit 4c67e2bfc8b7121d51434362fa7c2d012f8bcf1b
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:24 2015 +0300

    IB/core: Introduce new fast registration API
    
    The new fast registration  verb ib_map_mr_sg receives a scatterlist
    and converts it to a page list under the verbs API thus hiding
    the specific HW mapping details away from the consumer.
    
    The provider drivers are provided with a generic helper ib_sg_to_pages
    that converts a scatterlist into a vector of page addresses. The
    drivers can still perform any HW specific page address setting
    by passing a set_page function pointer which will be invoked for
    each page address. This allows drivers to avoid keeping a shadow
    page vectors and convert them to HW specific translations by doing
    extra copies.
    
    This API will allow ULPs to remove the duplicated code of constructing
    a page vector from a given sg list.
    
    The send work request ib_reg_wr also shrinks as it will contain only
    mr, key and access flags in addition.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 85103aff909b..065d37c13aa6 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -738,6 +738,7 @@ enum ib_wc_opcode {
 	IB_WC_LSO,
 	IB_WC_LOCAL_INV,
 	IB_WC_FAST_REG_MR,
+	IB_WC_REG_MR,
 	IB_WC_MASKED_COMP_SWAP,
 	IB_WC_MASKED_FETCH_ADD,
 /*
@@ -1025,6 +1026,7 @@ enum ib_wr_opcode {
 	IB_WR_RDMA_READ_WITH_INV,
 	IB_WR_LOCAL_INV,
 	IB_WR_FAST_REG_MR,
+	IB_WR_REG_MR,
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
 	IB_WR_BIND_MW,
@@ -1157,6 +1159,18 @@ static inline struct ib_fast_reg_wr *fast_reg_wr(struct ib_send_wr *wr)
 	return container_of(wr, struct ib_fast_reg_wr, wr);
 }
 
+struct ib_reg_wr {
+	struct ib_send_wr	wr;
+	struct ib_mr		*mr;
+	u32			key;
+	int			access;
+};
+
+static inline struct ib_reg_wr *reg_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_reg_wr, wr);
+}
+
 struct ib_bind_mw_wr {
 	struct ib_send_wr	wr;
 	struct ib_mw		*mw;
@@ -1369,6 +1383,9 @@ struct ib_mr {
 	struct ib_uobject *uobject;
 	u32		   lkey;
 	u32		   rkey;
+	u64		   iova;
+	u32		   length;
+	unsigned int	   page_size;
 	atomic_t	   usecnt; /* count number of MWs */
 };
 
@@ -1753,6 +1770,9 @@ struct ib_device {
 	struct ib_mr *		   (*alloc_mr)(struct ib_pd *pd,
 					       enum ib_mr_type mr_type,
 					       u32 max_num_sg);
+	int                        (*map_mr_sg)(struct ib_mr *mr,
+						struct scatterlist *sg,
+						int sg_nents);
 	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
 								   int page_list_len);
 	void			   (*free_fast_reg_page_list)(struct ib_fast_reg_page_list *page_list);
@@ -3059,4 +3079,28 @@ struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
 					    u16 pkey, const union ib_gid *gid,
 					    const struct sockaddr *addr);
 
+int ib_map_mr_sg(struct ib_mr *mr,
+		 struct scatterlist *sg,
+		 int sg_nents,
+		 unsigned int page_size);
+
+static inline int
+ib_map_mr_sg_zbva(struct ib_mr *mr,
+		  struct scatterlist *sg,
+		  int sg_nents,
+		  unsigned int page_size)
+{
+	int n;
+
+	n = ib_map_mr_sg(mr, sg, sg_nents, page_size);
+	mr->iova = 0;
+
+	return n;
+}
+
+int ib_sg_to_pages(struct ib_mr *mr,
+		   struct scatterlist *sgl,
+		   int sg_nents,
+		   int (*set_page)(struct ib_mr *, u64));
+
 #endif /* IB_VERBS_H */

commit 63e8790d39a2d7c9a0ebeab987a6033d184bc6ba
Merge: 95893dde99d9 eb14ab3ba140
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Oct 28 22:23:34 2015 -0400

    Merge branch 'wr-cleanup' into k.o/for-4.4

commit aa744cc01fe0f21dfbe2744d3fd5f2fb3244c9b3
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Oct 15 18:38:53 2015 +0300

    IB/core: Remove smac and vlan id from qp_attr and ah_attr
    
    Smac and vlan id could be resolved from the GID attribute, and thus
    these attributes aren't needed anymore. Removing them.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 98ded0b749cd..e4cc389c43cb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -699,7 +699,6 @@ struct ib_ah_attr {
 	u8			ah_flags;
 	u8			port_num;
 	u8			dmac[ETH_ALEN];
-	u16			vlan_id;
 };
 
 enum ib_wc_status {
@@ -958,10 +957,10 @@ enum ib_qp_attr_mask {
 	IB_QP_PATH_MIG_STATE		= (1<<18),
 	IB_QP_CAP			= (1<<19),
 	IB_QP_DEST_QPN			= (1<<20),
-	IB_QP_SMAC			= (1<<21),
-	IB_QP_ALT_SMAC			= (1<<22),
-	IB_QP_VID			= (1<<23),
-	IB_QP_ALT_VID			= (1<<24),
+	IB_QP_RESERVED1			= (1<<21),
+	IB_QP_RESERVED2			= (1<<22),
+	IB_QP_RESERVED3			= (1<<23),
+	IB_QP_RESERVED4			= (1<<24),
 };
 
 enum ib_qp_state {
@@ -1011,10 +1010,6 @@ struct ib_qp_attr {
 	u8			rnr_retry;
 	u8			alt_port_num;
 	u8			alt_timeout;
-	u8			smac[ETH_ALEN];
-	u8			alt_smac[ETH_ALEN];
-	u16			vlan_id;
-	u16			alt_vlan_id;
 };
 
 enum ib_wr_opcode {

commit 55ee3ab2e49a9ead850722ef47698243dd226d16
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Oct 15 18:38:45 2015 +0300

    IB/core: Add netdev and gid attributes paramteres to cache
    
    Adding an ability to query the IB cache by a netdev and get the
    attributes of a GID. These parameters are necessary in order to
    successfully resolve the required GID (when the netdevice is known)
    and get the Ethernet L2 attributes from a GID.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e1f65e204d37..98ded0b749cd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2177,7 +2177,8 @@ static inline bool rdma_cap_roce_gid_table(const struct ib_device *device,
 }
 
 int ib_query_gid(struct ib_device *device,
-		 u8 port_num, int index, union ib_gid *gid);
+		 u8 port_num, int index, union ib_gid *gid,
+		 struct ib_gid_attr *attr);
 
 int ib_query_pkey(struct ib_device *device,
 		  u8 port_num, u16 index, u16 *pkey);
@@ -2191,7 +2192,7 @@ int ib_modify_port(struct ib_device *device,
 		   struct ib_port_modify *port_modify);
 
 int ib_find_gid(struct ib_device *device, union ib_gid *gid,
-		u8 *port_num, u16 *index);
+		struct net_device *ndev, u8 *port_num, u16 *index);
 
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);

commit 25556ae6b965321c7e7469faa06ddbeae50dac91
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 19 14:58:43 2015 +0200

    IB: remove xrc_remote_srq_num from struct ib_send_wr
    
    The field is only initialized in mlx, but never used.
    
    If we want to add proper XRC support it should be done with a new
    struct ib_xrc_wr.
    
    This shrinks the various WR structures by another 4 bytes.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Haggai Eran <haggaie@mellanox.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 25f022c9aaac..edf02908a0fd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1100,7 +1100,6 @@ struct ib_send_wr {
 		__be32		imm_data;
 		u32		invalidate_rkey;
 	} ex;
-	u32			xrc_remote_srq_num;	/* XRC TGT QPs only */
 };
 
 struct ib_rdma_wr {

commit e622f2f4ad2142d2a613a57fb85f8cf737935ef5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 8 09:16:33 2015 +0100

    IB: split struct ib_send_wr
    
    This patch split up struct ib_send_wr so that all non-trivial verbs
    use their own structure which embedds struct ib_send_wr.  This dramaticly
    shrinks the size of a WR for most common operations:
    
    sizeof(struct ib_send_wr) (old):        96
    
    sizeof(struct ib_send_wr):              48
    sizeof(struct ib_rdma_wr):              64
    sizeof(struct ib_atomic_wr):            96
    sizeof(struct ib_ud_wr):                88
    sizeof(struct ib_fast_reg_wr):          88
    sizeof(struct ib_bind_mw_wr):           96
    sizeof(struct ib_sig_handover_wr):      80
    
    And with Sagi's pending MR rework the fast registration WR will also be
    down to a reasonable size:
    
    sizeof(struct ib_fastreg_wr):           64
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com> [srp, srpt]
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com> [sunrpc]
    Tested-by: Haggai Eran <haggaie@mellanox.com>
    Tested-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7845fae6f2df..25f022c9aaac 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1100,54 +1100,94 @@ struct ib_send_wr {
 		__be32		imm_data;
 		u32		invalidate_rkey;
 	} ex;
-	union {
-		struct {
-			u64	remote_addr;
-			u32	rkey;
-		} rdma;
-		struct {
-			u64	remote_addr;
-			u64	compare_add;
-			u64	swap;
-			u64	compare_add_mask;
-			u64	swap_mask;
-			u32	rkey;
-		} atomic;
-		struct {
-			struct ib_ah *ah;
-			void   *header;
-			int     hlen;
-			int     mss;
-			u32	remote_qpn;
-			u32	remote_qkey;
-			u16	pkey_index; /* valid for GSI only */
-			u8	port_num;   /* valid for DR SMPs on switch only */
-		} ud;
-		struct {
-			u64				iova_start;
-			struct ib_fast_reg_page_list   *page_list;
-			unsigned int			page_shift;
-			unsigned int			page_list_len;
-			u32				length;
-			int				access_flags;
-			u32				rkey;
-		} fast_reg;
-		struct {
-			struct ib_mw            *mw;
-			/* The new rkey for the memory window. */
-			u32                      rkey;
-			struct ib_mw_bind_info   bind_info;
-		} bind_mw;
-		struct {
-			struct ib_sig_attrs    *sig_attrs;
-			struct ib_mr	       *sig_mr;
-			int			access_flags;
-			struct ib_sge	       *prot;
-		} sig_handover;
-	} wr;
 	u32			xrc_remote_srq_num;	/* XRC TGT QPs only */
 };
 
+struct ib_rdma_wr {
+	struct ib_send_wr	wr;
+	u64			remote_addr;
+	u32			rkey;
+};
+
+static inline struct ib_rdma_wr *rdma_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_rdma_wr, wr);
+}
+
+struct ib_atomic_wr {
+	struct ib_send_wr	wr;
+	u64			remote_addr;
+	u64			compare_add;
+	u64			swap;
+	u64			compare_add_mask;
+	u64			swap_mask;
+	u32			rkey;
+};
+
+static inline struct ib_atomic_wr *atomic_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_atomic_wr, wr);
+}
+
+struct ib_ud_wr {
+	struct ib_send_wr	wr;
+	struct ib_ah		*ah;
+	void			*header;
+	int			hlen;
+	int			mss;
+	u32			remote_qpn;
+	u32			remote_qkey;
+	u16			pkey_index; /* valid for GSI only */
+	u8			port_num;   /* valid for DR SMPs on switch only */
+};
+
+static inline struct ib_ud_wr *ud_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_ud_wr, wr);
+}
+
+struct ib_fast_reg_wr {
+	struct ib_send_wr	wr;
+	u64			iova_start;
+	struct ib_fast_reg_page_list *page_list;
+	unsigned int		page_shift;
+	unsigned int		page_list_len;
+	u32			length;
+	int			access_flags;
+	u32			rkey;
+};
+
+static inline struct ib_fast_reg_wr *fast_reg_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_fast_reg_wr, wr);
+}
+
+struct ib_bind_mw_wr {
+	struct ib_send_wr	wr;
+	struct ib_mw		*mw;
+	/* The new rkey for the memory window. */
+	u32			rkey;
+	struct ib_mw_bind_info	bind_info;
+};
+
+static inline struct ib_bind_mw_wr *bind_mw_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_bind_mw_wr, wr);
+}
+
+struct ib_sig_handover_wr {
+	struct ib_send_wr	wr;
+	struct ib_sig_attrs    *sig_attrs;
+	struct ib_mr	       *sig_mr;
+	int			access_flags;
+	struct ib_sge	       *prot;
+};
+
+static inline struct ib_sig_handover_wr *sig_handover_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct ib_sig_handover_wr, wr);
+}
+
 struct ib_recv_wr {
 	struct ib_recv_wr      *next;
 	u64			wr_id;

commit 470a55358186d0bb93558a87d13159dfbc989351
Author: Bodong Wang <bodong@mellanox.com>
Date:   Tue Sep 22 23:18:10 2015 +0300

    IB/core: Add support of checksum capability reporting for RC and RAW
    
    Two enum members IB_DEVICE_RC_IP_CSUM and IB_DEVICE_RAW_IP_CSUM are
    added to ib_device_cap_flags. Device should set these two flags if they support
    insertion of UDP and TCP checksum on outgoing IPv4 messages and can verify the
    validity of checksum for incoming IPv4 messages, for RC IPoIB and RAW over
    Ethernet respectively. They are similar to IB_DEVICE_UD_IP_CSUM.
    
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7845fae6f2df..e1f65e204d37 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -137,6 +137,8 @@ enum ib_device_cap_flags {
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
 	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24),
+	IB_DEVICE_RC_IP_CSUM		= (1<<25),
+	IB_DEVICE_RAW_IP_CSUM		= (1<<26),
 	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29),
 	IB_DEVICE_SIGNATURE_HANDOVER	= (1<<30),
 	IB_DEVICE_ON_DEMAND_PAGING	= (1<<31),
@@ -873,7 +875,6 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
 };
 
-
 /*
  * Note: users may not call ib_close_qp or ib_destroy_qp from the event_handler
  * callback to destroy the passed in QP.

commit 26d2177e977c912863ac04f6c1a967e793ca3a56
Merge: a794b4f32921 d1178cbcdcf9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 9 08:33:31 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull inifiniband/rdma updates from Doug Ledford:
     "This is a fairly sizeable set of changes.  I've put them through a
      decent amount of testing prior to sending the pull request due to
      that.
    
      There are still a few fixups that I know are coming, but I wanted to
      go ahead and get the big, sizable chunk into your hands sooner rather
      than waiting for those last few fixups.
    
      Of note is the fact that this creates what is intended to be a
      temporary area in the drivers/staging tree specifically for some
      cleanups and additions that are coming for the RDMA stack.  We
      deprecated two drivers (ipath and amso1100) and are waiting to hear
      back if we can deprecate another one (ehca).  We also put Intel's new
      hfi1 driver into this area because it needs to be refactored and a
      transfer library created out of the factored out code, and then it and
      the qib driver and the soft-roce driver should all be modified to use
      that library.
    
      I expect drivers/staging/rdma to be around for three or four kernel
      releases and then to go away as all of the work is completed and final
      deletions of deprecated drivers are done.
    
      Summary of changes for 4.3:
    
       - Create drivers/staging/rdma
       - Move amso1100 driver to staging/rdma and schedule for deletion
       - Move ipath driver to staging/rdma and schedule for deletion
       - Add hfi1 driver to staging/rdma and set TODO for move to regular
         tree
       - Initial support for namespaces to be used on RDMA devices
       - Add RoCE GID table handling to the RDMA core caching code
       - Infrastructure to support handling of devices with differing read
         and write scatter gather capabilities
       - Various iSER updates
       - Kill off unsafe usage of global mr registrations
       - Update SRP driver
       - Misc  mlx4 driver updates
       - Support for the mr_alloc verb
       - Support for a netlink interface between kernel and user space cache
         daemon to speed path record queries and route resolution
       - Ininitial support for safe hot removal of verbs devices"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (136 commits)
      IB/ipoib: Suppress warning for send only join failures
      IB/ipoib: Clean up send-only multicast joins
      IB/srp: Fix possible protection fault
      IB/core: Move SM class defines from ib_mad.h to ib_smi.h
      IB/core: Remove unnecessary defines from ib_mad.h
      IB/hfi1: Add PSM2 user space header to header_install
      IB/hfi1: Add CSRs for CONFIG_SDMA_VERBOSITY
      mlx5: Fix incorrect wc pkey_index assignment for GSI messages
      IB/mlx5: avoid destroying a NULL mr in reg_user_mr error flow
      IB/uverbs: reject invalid or unknown opcodes
      IB/cxgb4: Fix if statement in pick_local_ip6adddrs
      IB/sa: Fix rdma netlink message flags
      IB/ucma: HW Device hot-removal support
      IB/mlx4_ib: Disassociate support
      IB/uverbs: Enable device removal when there are active user space applications
      IB/uverbs: Explicitly pass ib_dev to uverbs commands
      IB/uverbs: Fix race between ib_uverbs_open and remove_one
      IB/uverbs: Fix reference counting usage of event files
      IB/core: Make ib_dealloc_pd return void
      IB/srp: Create an insecure all physical rkey only if needed
      ...

commit 036b10635739ffd030246eedde3d67f724800177
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Aug 13 18:32:05 2015 +0300

    IB/uverbs: Enable device removal when there are active user space applications
    
    Enables the uverbs_remove_one to succeed despite the fact that there are
    running IB applications working with the given ib device.  This
    functionality enables a HW device to be unbind/reset despite the fact that
    there are running user space applications using it.
    
    It exposes a new IB kernel API named 'disassociate_ucontext' which lets
    a driver detaching its HW resources from a given user context without
    crashing/terminating the application. In case a driver implemented the
    above API and registered with ib_uverb there will be no dependency between its
    device to its uverbs_device. Upon calling remove_one of ib_uverbs the call
    should return after disassociating the open HW resources without waiting to
    clients disconnecting. In case driver didn't implement this API there will be no
    change to current behaviour and uverbs_remove_one will return only when last
    client has disconnected and reference count on uverbs device became 0.
    
    In case the lower driver device was removed any application will
    continue working over some zombie HCA, further calls will ended with an
    immediate error.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 128abf2888ab..40b83f412d24 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1769,6 +1769,7 @@ struct ib_device {
 	int			   (*destroy_flow)(struct ib_flow *flow_id);
 	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
 						      struct ib_mr_status *mr_status);
+	void			   (*disassociate_ucontext)(struct ib_ucontext *ibcontext);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 

commit 7dd78647a2c2c224e376fc72797d411a3a0bb047
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Wed Aug 5 14:34:31 2015 -0600

    IB/core: Make ib_dealloc_pd return void
    
    The majority of callers never check the return value, and even if they
    did, they can't do anything about a failure.
    
    All possible failure cases represent a bug in the caller, so just
    WARN_ON inside the function instead.
    
    This fixes a few random errors:
     net/rd/iw.c infinite loops while it fails. (racing with EBUSY?)
    
    This also lays the ground work to get rid of error return from the
    drivers. Most drivers do not error, the few that do are broken since
    it cannot be handled.
    
    Since uverbs can legitimately make use of EBUSY, open code the
    check.
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 09400512d579..128abf2888ab 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2196,11 +2196,7 @@ int ib_find_pkey(struct ib_device *device,
 
 struct ib_pd *ib_alloc_pd(struct ib_device *device);
 
-/**
- * ib_dealloc_pd - Deallocates a protection domain.
- * @pd: The protection domain to deallocate.
- */
-int ib_dealloc_pd(struct ib_pd *pd);
+void ib_dealloc_pd(struct ib_pd *pd);
 
 /**
  * ib_create_ah - Creates an address handle for the given address vector.

commit 96249d70dd70496084c7ec1465ec449cd032955a
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Wed Aug 5 14:14:45 2015 -0600

    IB/core: Guarantee that a local_dma_lkey is available
    
    Every single ULP requires a local_dma_lkey to do anything with
    a QP, so let us ensure one exists for every PD created.
    
    If the driver can supply a global local_dma_lkey then use that, otherwise
    ask the driver to create a local use all physical memory MR associated
    with the new PD.
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Sagi Grimberg <sagig@dev.mellanox.co.il>
    Acked-by: Christoph Hellwig <hch@infradead.org>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5eff55c8b39d..09400512d579 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1257,9 +1257,11 @@ struct ib_udata {
 };
 
 struct ib_pd {
+	u32			local_dma_lkey;
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;
 	atomic_t          	usecnt; /* count all resources */
+	struct ib_mr	       *local_mr;
 };
 
 struct ib_xrcd {
@@ -2192,13 +2194,6 @@ int ib_find_gid(struct ib_device *device, union ib_gid *gid,
 int ib_find_pkey(struct ib_device *device,
 		 u8 port_num, u16 pkey, u16 *index);
 
-/**
- * ib_alloc_pd - Allocates an unused protection domain.
- * @device: The device on which to allocate the protection domain.
- *
- * A protection domain object provides an association between QPs, shared
- * receive queues, address handles, memory regions, and memory windows.
- */
 struct ib_pd *ib_alloc_pd(struct ib_device *device);
 
 /**

commit e26be1bfef81a2314a075f54dd8930cf5e8656df
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Jul 30 18:33:29 2015 +0300

    IB/mlx4: Implement ib_device callbacks
    
    get_netdev: get the net_device on the physical port of the IB transport port. In
    port aggregation mode it is required to return the netdev of the active port.
    
    modify_gid: note for a change in the RoCE gid cache. Handle this by writing to
    the harsware GID table. It is possible that indexes in cahce and hardware tables
    won't match so a translation is required when modifying a QP or creating an
    address handle.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2de56834a6be..5eff55c8b39d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -65,6 +65,8 @@ union ib_gid {
 	} global;
 };
 
+extern union ib_gid zgid;
+
 struct ib_gid_attr {
 	struct net_device	*ndev;
 };

commit 03db3a2d81e6e84f3ed3cb9e087cae17d762642b
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jul 30 18:33:26 2015 +0300

    IB/core: Add RoCE GID table management
    
    RoCE GIDs are based on IP addresses configured on Ethernet net-devices
    which relate to the RDMA (RoCE) device port.
    
    Currently, each of the low-level drivers that support RoCE (ocrdma,
    mlx4) manages its own RoCE port GID table. As there's nothing which is
    essentially vendor specific, we generalize that, and enhance the RDMA
    core GID cache to do this job.
    
    In order to populate the GID table, we listen for events:
    
    (a) netdev up/down/change_addr events - if a netdev is built onto
        our RoCE device, we need to add/delete its IPs. This involves
        adding all GIDs related to this ndev, add default GIDs, etc.
    
    (b) inet events - add new GIDs (according to the IP addresses)
        to the table.
    
    For programming the port RoCE GID table, providers must implement
    the add_gid and del_gid callbacks.
    
    RoCE GID management requires us to state the associated net_device
    alongside the GID. This information is necessary in order to manage
    the GID table. For example, when a net_device is removed, its
    associated GIDs need to be removed as well.
    
    RoCE mandates generating a default GID for each port, based on the
    related net-device's IPv6 link local. In contrast to the GID based on
    the regular IPv6 link-local (as we generate GID per IP address),
    the default GID is also available when the net device is down (in
    order to support loopback).
    
    Locking is done as follows:
    The patch modify the GID table code both for new RoCE drivers
    implementing the add_gid/del_gid callbacks and for current RoCE and
    IB drivers that do not. The flows for updating the table are
    different, so the locking requirements are too.
    
    While updating RoCE GID table, protection against multiple writers is
    achieved via mutex_lock(&table->lock). Since writing to a table
    requires us to find an entry (possible a free entry) in the table and
    then modify it, this mutex protects both the find_gid and write_gid
    ensuring the atomicity of the action.
    Each entry in the GID cache is protected by rwlock. In RoCE, writing
    (usually results from netdev notifier) involves invoking the vendor's
    add_gid and del_gid callbacks, which could sleep.
    Therefore, an invalid flag is added for each entry. Updates for RoCE are
    done via a workqueue, thus sleeping is permitted.
    
    In IB, updates are done in write_lock_irq(&device->cache.lock), thus
    write_gid isn't allowed to sleep and add_gid/del_gid are not called.
    
    When passing net-device into/out-of the GID cache, the device
    is always passed held (dev_hold).
    
    The code uses a single work item for updating all RDMA devices,
    following a netdev or inet notifier.
    
    The patch moves the cache from being a client (which was incorrect,
    as the cache is part of the IB infrastructure) to being explicitly
    initialized/freed when a device is registered/removed.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3540da2731f..2de56834a6be 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -65,6 +65,10 @@ union ib_gid {
 	} global;
 };
 
+struct ib_gid_attr {
+	struct net_device	*ndev;
+};
+
 enum rdma_node_type {
 	/* IB values map to NodeInfo:NodeType. */
 	RDMA_NODE_IB_CA 	= 1,
@@ -285,7 +289,7 @@ enum ib_port_cap_flags {
 	IB_PORT_BOOT_MGMT_SUP			= 1 << 23,
 	IB_PORT_LINK_LATENCY_SUP		= 1 << 24,
 	IB_PORT_CLIENT_REG_SUP			= 1 << 25,
-	IB_PORT_IP_BASED_GIDS			= 1 << 26
+	IB_PORT_IP_BASED_GIDS			= 1 << 26,
 };
 
 enum ib_port_width {
@@ -1487,7 +1491,7 @@ struct ib_cache {
 	rwlock_t                lock;
 	struct ib_event_handler event_handler;
 	struct ib_pkey_cache  **pkey_cache;
-	struct ib_gid_cache   **gid_cache;
+	struct ib_gid_table   **gid_cache;
 	u8                     *lmc_cache;
 };
 
@@ -1573,9 +1577,47 @@ struct ib_device {
 						 struct ib_port_attr *port_attr);
 	enum rdma_link_layer	   (*get_link_layer)(struct ib_device *device,
 						     u8 port_num);
+	/* When calling get_netdev, the HW vendor's driver should return the
+	 * net device of device @device at port @port_num or NULL if such
+	 * a net device doesn't exist. The vendor driver should call dev_hold
+	 * on this net device. The HW vendor's device driver must guarantee
+	 * that this function returns NULL before the net device reaches
+	 * NETDEV_UNREGISTER_FINAL state.
+	 */
+	struct net_device	  *(*get_netdev)(struct ib_device *device,
+						 u8 port_num);
 	int		           (*query_gid)(struct ib_device *device,
 						u8 port_num, int index,
 						union ib_gid *gid);
+	/* When calling add_gid, the HW vendor's driver should
+	 * add the gid of device @device at gid index @index of
+	 * port @port_num to be @gid. Meta-info of that gid (for example,
+	 * the network device related to this gid is available
+	 * at @attr. @context allows the HW vendor driver to store extra
+	 * information together with a GID entry. The HW vendor may allocate
+	 * memory to contain this information and store it in @context when a
+	 * new GID entry is written to. Params are consistent until the next
+	 * call of add_gid or delete_gid. The function should return 0 on
+	 * success or error otherwise. The function could be called
+	 * concurrently for different ports. This function is only called
+	 * when roce_gid_table is used.
+	 */
+	int		           (*add_gid)(struct ib_device *device,
+					      u8 port_num,
+					      unsigned int index,
+					      const union ib_gid *gid,
+					      const struct ib_gid_attr *attr,
+					      void **context);
+	/* When calling del_gid, the HW vendor's driver should delete the
+	 * gid of device @device at gid index @index of port @port_num.
+	 * Upon the deletion of a GID entry, the HW vendor must free any
+	 * allocated memory. The caller will clear @context afterwards.
+	 * This function is only called when roce_gid_table is used.
+	 */
+	int		           (*del_gid)(struct ib_device *device,
+					      u8 port_num,
+					      unsigned int index,
+					      void **context);
 	int		           (*query_pkey)(struct ib_device *device,
 						 u8 port_num, u16 index, u16 *pkey);
 	int		           (*modify_device)(struct ib_device *device,
@@ -2108,6 +2150,26 @@ static inline size_t rdma_max_mad_size(const struct ib_device *device, u8 port_n
 	return device->port_immutable[port_num].max_mad_size;
 }
 
+/**
+ * rdma_cap_roce_gid_table - Check if the port of device uses roce_gid_table
+ * @device: Device to check
+ * @port_num: Port number to check
+ *
+ * RoCE GID table mechanism manages the various GIDs for a device.
+ *
+ * NOTE: if allocating the port's GID table has failed, this call will still
+ * return true, but any RoCE GID table API will fail.
+ *
+ * Return: true if the port uses RoCE GID table mechanism in order to manage
+ * its GIDs.
+ */
+static inline bool rdma_cap_roce_gid_table(const struct ib_device *device,
+					   u8 port_num)
+{
+	return rdma_protocol_roce(device, port_num) &&
+		device->add_gid && device->del_gid;
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit d9f272c523db47a56a64942eb6f25361c400de66
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:48 2015 +0300

    IB/core: Drop ib_alloc_fast_reg_mr
    
    Fully replaced by a more generic and suitable
    ib_alloc_mr.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index aba51431c986..c3540da2731f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1672,8 +1672,6 @@ struct ib_device {
 	struct ib_mr *		   (*alloc_mr)(struct ib_pd *pd,
 					       enum ib_mr_type mr_type,
 					       u32 max_num_sg);
-	struct ib_mr *		   (*alloc_fast_reg_mr)(struct ib_pd *pd,
-					       int max_page_list_len);
 	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
 								   int page_list_len);
 	void			   (*free_fast_reg_page_list)(struct ib_fast_reg_page_list *page_list);
@@ -2820,15 +2818,6 @@ struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
 			  enum ib_mr_type mr_type,
 			  u32 max_num_sg);
 
-/**
- * ib_alloc_fast_reg_mr - Allocates memory region usable with the
- *   IB_WR_FAST_REG_MR send work request.
- * @pd: The protection domain associated with the region.
- * @max_page_list_len: requested max physical buffer list length to be
- *   used with fast register work requests for this MR.
- */
-struct ib_mr *ib_alloc_fast_reg_mr(struct ib_pd *pd, int max_page_list_len);
-
 /**
  * ib_alloc_fast_reg_page_list - Allocates a page list array
  * @device - ib device pointer.

commit 9bee178b4f6b3e122ed8eda990450a638706e271
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:35 2015 +0300

    IB: Modify ib_create_mr API
    
    Use ib_alloc_mr with specific parameters.
    Change the existing callers.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 521c2059ede5..aba51431c986 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -557,20 +557,18 @@ __attribute_const__ int ib_rate_to_mult(enum ib_rate rate);
  */
 __attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
 
-enum ib_mr_create_flags {
-	IB_MR_SIGNATURE_EN = 1,
-};
 
 /**
- * ib_mr_init_attr - Memory region init attributes passed to routine
- *     ib_create_mr.
- * @max_reg_descriptors: max number of registration descriptors that
- *     may be used with registration work requests.
- * @flags: MR creation flags bit mask.
+ * enum ib_mr_type - memory region type
+ * @IB_MR_TYPE_MEM_REG:       memory region that is used for
+ *                            normal registration
+ * @IB_MR_TYPE_SIGNATURE:     memory region that is used for
+ *                            signature operations (data-integrity
+ *                            capable regions)
  */
-struct ib_mr_init_attr {
-	int	    max_reg_descriptors;
-	u32	    flags;
+enum ib_mr_type {
+	IB_MR_TYPE_MEM_REG,
+	IB_MR_TYPE_SIGNATURE,
 };
 
 /**
@@ -1671,8 +1669,9 @@ struct ib_device {
 	int                        (*query_mr)(struct ib_mr *mr,
 					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);
-	struct ib_mr *		   (*create_mr)(struct ib_pd *pd,
-						struct ib_mr_init_attr *mr_init_attr);
+	struct ib_mr *		   (*alloc_mr)(struct ib_pd *pd,
+					       enum ib_mr_type mr_type,
+					       u32 max_num_sg);
 	struct ib_mr *		   (*alloc_fast_reg_mr)(struct ib_pd *pd,
 					       int max_page_list_len);
 	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
@@ -2817,15 +2816,9 @@ int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
  */
 int ib_dereg_mr(struct ib_mr *mr);
 
-
-/**
- * ib_create_mr - Allocates a memory region that may be used for
- *     signature handover operations.
- * @pd: The protection domain associated with the region.
- * @mr_init_attr: memory region init attributes.
- */
-struct ib_mr *ib_create_mr(struct ib_pd *pd,
-			   struct ib_mr_init_attr *mr_init_attr);
+struct ib_mr *ib_alloc_mr(struct ib_pd *pd,
+			  enum ib_mr_type mr_type,
+			  u32 max_num_sg);
 
 /**
  * ib_alloc_fast_reg_mr - Allocates memory region usable with the

commit 8b91ffc1cf67d3f0834197c80c5182890c8d508d
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:34 2015 +0300

    IB/core: Get rid of redundant verb ib_destroy_mr
    
    This was added in a thought of uniting all mr allocation
    and deallocation routines but the fact is we have a single
    deallocation routine already, ib_dereg_mr.
    
    And, move mlx5_ib_destroy_mr specific logic into mlx5_ib_dereg_mr
    (includes only signature stuff for now).
    
    And, fixup the only callers (iser/isert) accordingly.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fd7a6950df75..521c2059ede5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1671,7 +1671,6 @@ struct ib_device {
 	int                        (*query_mr)(struct ib_mr *mr,
 					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);
-	int                        (*destroy_mr)(struct ib_mr *mr);
 	struct ib_mr *		   (*create_mr)(struct ib_pd *pd,
 						struct ib_mr_init_attr *mr_init_attr);
 	struct ib_mr *		   (*alloc_fast_reg_mr)(struct ib_pd *pd,
@@ -2828,15 +2827,6 @@ int ib_dereg_mr(struct ib_mr *mr);
 struct ib_mr *ib_create_mr(struct ib_pd *pd,
 			   struct ib_mr_init_attr *mr_init_attr);
 
-/**
- * ib_destroy_mr - Destroys a memory region that was created using
- *     ib_create_mr and removes it from HW translation tables.
- * @mr: The memory region to destroy.
- *
- * This function can fail, if the memory region has memory windows bound to it.
- */
-int ib_destroy_mr(struct ib_mr *mr);
-
 /**
  * ib_alloc_fast_reg_mr - Allocates memory region usable with the
  *   IB_WR_FAST_REG_MR send work request.

commit 9268f72dcb24348c8b4cf9bcf8afeb24035157a5
Author: Yotam Kenneth <yotamke@mellanox.com>
Date:   Thu Jul 30 17:50:15 2015 +0300

    IB/core: Find the network device matching connection parameters
    
    In the case of IPoIB, and maybe in other cases, the network device is
    managed by an upper-layer protocol (ULP). In order to expose this
    network device to other users of the IB device, let ULPs implement
    a callback that returns network device according to connection parameters.
    
    The IB device and port, together with the P_Key and the GID should
    be enough to uniquely identify the ULP net device. However, in current
    kernels there can be multiple IPoIB interfaces created with the same GID.
    Furthermore, such configuration may be desireable to support ipvlan-like
    configurations for RDMA CM with IPoIB.  To resolve the device in these
    cases the code will also take the IP address as an additional input.
    
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Yotam Kenneth <yotamke@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Guy Shapiro <guysh@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 449609b70928..fd7a6950df75 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -48,6 +48,7 @@
 #include <linux/rwsem.h>
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
+#include <linux/socket.h>
 #include <uapi/linux/if_ether.h>
 
 #include <linux/atomic.h>
@@ -1765,6 +1766,28 @@ struct ib_client {
 	void (*add)   (struct ib_device *);
 	void (*remove)(struct ib_device *, void *client_data);
 
+	/* Returns the net_dev belonging to this ib_client and matching the
+	 * given parameters.
+	 * @dev:	 An RDMA device that the net_dev use for communication.
+	 * @port:	 A physical port number on the RDMA device.
+	 * @pkey:	 P_Key that the net_dev uses if applicable.
+	 * @gid:	 A GID that the net_dev uses to communicate.
+	 * @addr:	 An IP address the net_dev is configured with.
+	 * @client_data: The device's client data set by ib_set_client_data().
+	 *
+	 * An ib_client that implements a net_dev on top of RDMA devices
+	 * (such as IP over IB) should implement this callback, allowing the
+	 * rdma_cm module to find the right net_dev for a given request.
+	 *
+	 * The caller is responsible for calling dev_put on the returned
+	 * netdev. */
+	struct net_device *(*get_net_dev_by_params)(
+			struct ib_device *dev,
+			u8 port,
+			u16 pkey,
+			const union ib_gid *gid,
+			const struct sockaddr *addr,
+			void *client_data);
 	struct list_head list;
 };
 
@@ -3014,4 +3037,8 @@ static inline int ib_check_mr_access(int flags)
 int ib_check_mr_status(struct ib_mr *mr, u32 check_mask,
 		       struct ib_mr_status *mr_status);
 
+struct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u8 port,
+					    u16 pkey, const union ib_gid *gid,
+					    const struct sockaddr *addr);
+
 #endif /* IB_VERBS_H */

commit 7c1eb45a22d76bb99236e7485958f87ef7c449cf
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Jul 30 17:50:14 2015 +0300

    IB/core: lock client data with lists_rwsem
    
    An ib_client callback that is called with the lists_rwsem locked only for
    read is protected from changes to the IB client lists, but not from
    ib_unregister_device() freeing its client data. This is because
    ib_unregister_device() will remove the device from the device list with
    lists_rwsem locked for write, but perform the rest of the cleanup,
    including the call to remove() without that lock.
    
    Mark client data that is undergoing de-registration with a new going_down
    flag in the client data context. Lock the client data list with lists_rwsem
    for write in addition to using the spinlock, so that functions calling the
    callback would be able to lock only lists_rwsem for read and let callbacks
    sleep.
    
    Since ib_unregister_client() now marks the client data context, no need for
    remove() to search the context again, so pass the client data directly to
    remove() callbacks.
    
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7448a2740287..449609b70928 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1550,6 +1550,8 @@ struct ib_device {
 
 	spinlock_t                    client_data_lock;
 	struct list_head              core_list;
+	/* Access to the client_data_list is protected by the client_data_lock
+	 * spinlock and the lists_rwsem read-write semaphore */
 	struct list_head              client_data_list;
 
 	struct ib_cache               cache;
@@ -1761,7 +1763,7 @@ struct ib_device {
 struct ib_client {
 	char  *name;
 	void (*add)   (struct ib_device *);
-	void (*remove)(struct ib_device *);
+	void (*remove)(struct ib_device *, void *client_data);
 
 	struct list_head list;
 };

commit 3403051ebbd486a342272a404f16e7f1aca8758e
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jul 27 18:10:18 2015 -0500

    RDMA/Core: remove rdma_cap_read_multi_sge() helper
    
    This functionality already exists via the max_sge_rd
    device capability.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b0f898e3b2e7..7448a2740287 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2070,34 +2070,6 @@ static inline bool rdma_cap_eth_ah(const struct ib_device *device, u8 port_num)
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_ETH_AH;
 }
 
-/**
- * rdma_cap_read_multi_sge - Check if the port of device has the capability
- * RDMA Read Multiple Scatter-Gather Entries.
- * @device: Device to check
- * @port_num: Port number to check
- *
- * iWARP has a restriction that RDMA READ requests may only have a single
- * Scatter/Gather Entry (SGE) in the work request.
- *
- * NOTE: although the linux kernel currently assumes all devices are either
- * single SGE RDMA READ devices or identical SGE maximums for RDMA READs and
- * WRITEs, according to Tom Talpey, this is not accurate.  There are some
- * devices out there that support more than a single SGE on RDMA READ
- * requests, but do not support the same number of SGEs as they do on
- * RDMA WRITE requests.  The linux kernel would need rearchitecting to
- * support these imbalanced READ/WRITE SGEs allowed devices.  So, for now,
- * suffice with either the device supports the same READ/WRITE SGEs, or
- * it only gets one READ sge.
- *
- * Return: true for any device that allows more than one SGE in RDMA READ
- * requests.
- */
-static inline bool rdma_cap_read_multi_sge(struct ib_device *device,
-					   u8 port_num)
-{
-	return !(device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP);
-}
-
 /**
  * rdma_max_mad_size - Return the max MAD size required by this RDMA Port.
  *

commit 1241d7bf2ac8838d0d2d0b54a6173ac3eb3747a4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:04:54 2015 -0400

    core: Remove the ib_reg_phys_mr() and ib_rereg_phys_mr() verbs
    
    The verbs are obsolete. The ib_rereg_phys_mr() verb is not used by
    kernel ULPs, and the last ib_reg_phys_mr() call site in the kernel
    tree has now been removed.
    
    Two staging tree call sites remain in the Lustre client. The Lustre
    team has been notified of the deprecation of reg_phys_mr.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b0f898e3b2e7..43c1cf01c84b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2759,52 +2759,6 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 		dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 
-/**
- * ib_reg_phys_mr - Prepares a virtually addressed memory region for use
- *   by an HCA.
- * @pd: The protection domain associated assigned to the registered region.
- * @phys_buf_array: Specifies a list of physical buffers to use in the
- *   memory region.
- * @num_phys_buf: Specifies the size of the phys_buf_array.
- * @mr_access_flags: Specifies the memory access rights.
- * @iova_start: The offset of the region's starting I/O virtual address.
- */
-struct ib_mr *ib_reg_phys_mr(struct ib_pd *pd,
-			     struct ib_phys_buf *phys_buf_array,
-			     int num_phys_buf,
-			     int mr_access_flags,
-			     u64 *iova_start);
-
-/**
- * ib_rereg_phys_mr - Modifies the attributes of an existing memory region.
- *   Conceptually, this call performs the functions deregister memory region
- *   followed by register physical memory region.  Where possible,
- *   resources are reused instead of deallocated and reallocated.
- * @mr: The memory region to modify.
- * @mr_rereg_mask: A bit-mask used to indicate which of the following
- *   properties of the memory region are being modified.
- * @pd: If %IB_MR_REREG_PD is set in mr_rereg_mask, this field specifies
- *   the new protection domain to associated with the memory region,
- *   otherwise, this parameter is ignored.
- * @phys_buf_array: If %IB_MR_REREG_TRANS is set in mr_rereg_mask, this
- *   field specifies a list of physical buffers to use in the new
- *   translation, otherwise, this parameter is ignored.
- * @num_phys_buf: If %IB_MR_REREG_TRANS is set in mr_rereg_mask, this
- *   field specifies the size of the phys_buf_array, otherwise, this
- *   parameter is ignored.
- * @mr_access_flags: If %IB_MR_REREG_ACCESS is set in mr_rereg_mask, this
- *   field specifies the new memory access rights, otherwise, this
- *   parameter is ignored.
- * @iova_start: The offset of the region's starting I/O virtual address.
- */
-int ib_rereg_phys_mr(struct ib_mr *mr,
-		     int mr_rereg_mask,
-		     struct ib_pd *pd,
-		     struct ib_phys_buf *phys_buf_array,
-		     int num_phys_buf,
-		     int mr_access_flags,
-		     u64 *iova_start);
-
 /**
  * ib_query_mr - Retrieves information about a specific memory region.
  * @mr: The memory region to retrieve information about.

commit 4139032b4860c06ff3a7687041f06535fed901ed
Author: Hal Rosenstock <hal@dev.mellanox.co.il>
Date:   Mon Jun 29 09:57:00 2015 -0400

    IB: Add rdma_cap_ib_switch helper and use where appropriate
    
    Persuant to Liran's comments on node_type on linux-rdma
    mailing list:
    
    In an effort to reform the RDMA core and ULPs to minimize use of
    node_type in struct ib_device, an additional bit is added to
    struct ib_device for is_switch (IB switch). This is needed
    to be initialized by any IB switch device driver. This is a
    NEW requirement on such device drivers which are all
    "out of tree".
    
    In addition, an ib_switch helper was added to ib_verbs.h
    based on the is_switch device bit rather than node_type
    (although those should be consistent).
    
    The RDMA core (MAD, SMI, agent, sa_query, multicast, sysfs)
    as well as (IPoIB and SRP) ULPs are updated where
    appropriate to use this new helper. In some cases,
    the helper is now used under the covers of using
    rdma_[start end]_port rather than the open coding
    previously used.
    
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-By: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Hal Rosenstock <hal@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 986fddb08579..b0f898e3b2e7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1745,6 +1745,7 @@ struct ib_device {
 	char			     node_desc[64];
 	__be64			     node_guid;
 	u32			     local_dma_lkey;
+	u16                          is_switch:1;
 	u8                           node_type;
 	u8                           phys_port_cnt;
 
@@ -1823,6 +1824,20 @@ int ib_query_port(struct ib_device *device,
 enum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,
 					       u8 port_num);
 
+/**
+ * rdma_cap_ib_switch - Check if the device is IB switch
+ * @device: Device to check
+ *
+ * Device driver is responsible for setting is_switch bit on
+ * in ib_device structure at init time.
+ *
+ * Return: true if the device is IB switch.
+ */
+static inline bool rdma_cap_ib_switch(const struct ib_device *device)
+{
+	return device->is_switch;
+}
+
 /**
  * rdma_start_port - Return the first valid port number for the device
  * specified
@@ -1833,7 +1848,7 @@ enum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,
  */
 static inline u8 rdma_start_port(const struct ib_device *device)
 {
-	return (device->node_type == RDMA_NODE_IB_SWITCH) ? 0 : 1;
+	return rdma_cap_ib_switch(device) ? 0 : 1;
 }
 
 /**
@@ -1846,8 +1861,7 @@ static inline u8 rdma_start_port(const struct ib_device *device)
  */
 static inline u8 rdma_end_port(const struct ib_device *device)
 {
-	return (device->node_type == RDMA_NODE_IB_SWITCH) ?
-		0 : device->phys_port_cnt;
+	return rdma_cap_ib_switch(device) ? 0 : device->phys_port_cnt;
 }
 
 static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)

commit 65995fee842fd9f1427b62be24053846d9c32102
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sat Jun 6 14:38:32 2015 -0400

    IB/core: Add OPA MAD core capability flag
    
    Add OPA MAD support flags to the core capability immutable flags.  In addition
    add the rdma_cap_opa_mad helper function for core functions to use to detect
    OPA MAD support.
    
    OPA MADs share a common header with IBTA MADs but with some differences for
    increased performance.
    
    Sharing a common header with IBTA MADs allows us to share most of the MAD
    processing code when dealing with OPA MADs in addition to supporting some IBTA
    MADs on OPA devices.
    
    OPA MADs differ in the following ways:
    
            1) MADs are variable size up to 2K
               IBTA defined MADs remain fixed at 256 bytes
            2) OPA SMPs must carry valid PKeys
            3) OPA SMP packets are a different format
    
    The MAD stack will use this new functionality to determine if OPA MAD
    processing should occur on individual device ports.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 46c8eb8f1bc7..986fddb08579 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -374,6 +374,7 @@ union rdma_protocol_stats {
 #define RDMA_CORE_CAP_IB_CM             0x00000004
 #define RDMA_CORE_CAP_IW_CM             0x00000008
 #define RDMA_CORE_CAP_IB_SA             0x00000010
+#define RDMA_CORE_CAP_OPA_MAD           0x00000020
 
 /* Address format                       0x000FF000 */
 #define RDMA_CORE_CAP_AF_IB             0x00001000
@@ -397,6 +398,8 @@ union rdma_protocol_stats {
 					| RDMA_CORE_CAP_ETH_AH)
 #define RDMA_CORE_PORT_IWARP           (RDMA_CORE_CAP_PROT_IWARP \
 					| RDMA_CORE_CAP_IW_CM)
+#define RDMA_CORE_PORT_INTEL_OPA       (RDMA_CORE_PORT_IBA_IB  \
+					| RDMA_CORE_CAP_OPA_MAD)
 
 struct ib_port_attr {
 	enum ib_port_state	state;
@@ -1885,6 +1888,31 @@ static inline bool rdma_cap_ib_mad(const struct ib_device *device, u8 port_num)
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_MAD;
 }
 
+/**
+ * rdma_cap_opa_mad - Check if the port of device provides support for OPA
+ * Management Datagrams.
+ * @device: Device to check
+ * @port_num: Port number to check
+ *
+ * Intel OmniPath devices extend and/or replace the InfiniBand Management
+ * datagrams with their own versions.  These OPA MADs share many but not all of
+ * the characteristics of InfiniBand MADs.
+ *
+ * OPA MADs differ in the following ways:
+ *
+ *    1) MADs are variable size up to 2K
+ *       IBTA defined MADs remain fixed at 256 bytes
+ *    2) OPA SMPs must carry valid PKeys
+ *    3) OPA SMP packets are a different format
+ *
+ * Return: true if the port supports OPA MAD packet formats.
+ */
+static inline bool rdma_cap_opa_mad(struct ib_device *device, u8 port_num)
+{
+	return (device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_OPA_MAD)
+		== RDMA_CORE_CAP_OPA_MAD;
+}
+
 /**
  * rdma_cap_ib_smi - Check if the port of a device provides an Infiniband
  * Subnet Management Agent (SMA) on the Subnet Management Interface (SMI).

commit 4cd7c9479aff33746af490fa4a5a7dee8654891a
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sat Jun 6 14:38:31 2015 -0400

    IB/mad: Add support for additional MAD info to/from drivers
    
    In order to support alternate sized MADs (and variable sized MADs on OPA
    devices) add in/out MAD size parameters to the process_mad core call.
    
    In addition, add an out_mad_pkey_index to communicate the pkey index the driver
    wishes the MAD stack to use when sending OPA MAD responses.
    
    The out MAD size and the out MAD PKey index are required by the MAD
    stack to generate responses on OPA devices.
    
    Furthermore, the in and out MAD parameters are made generic by specifying them
    as ib_mad_hdr rather than ib_mad.
    
    Drivers are modified as needed and are protected by BUG_ON flags if the MAD
    sizes passed to them is incorrect.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 75c349969b6e..46c8eb8f1bc7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1463,7 +1463,7 @@ struct ib_flow {
 	struct ib_uobject	*uobject;
 };
 
-struct ib_mad;
+struct ib_mad_hdr;
 struct ib_grh;
 
 enum ib_process_mad_flags {
@@ -1705,8 +1705,11 @@ struct ib_device {
 						  u8 port_num,
 						  const struct ib_wc *in_wc,
 						  const struct ib_grh *in_grh,
-						  const struct ib_mad *in_mad,
-						  struct ib_mad *out_mad);
+						  const struct ib_mad_hdr *in_mad,
+						  size_t in_mad_size,
+						  struct ib_mad_hdr *out_mad,
+						  size_t *out_mad_size,
+						  u16 *out_mad_pkey_index);
 	struct ib_xrcd *	   (*alloc_xrcd)(struct ib_device *device,
 						 struct ib_ucontext *ucontext,
 						 struct ib_udata *udata);

commit 337877a466bb8b0c51f4fa727eeef7d734665632
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sat Jun 6 14:38:29 2015 -0400

    IB/core: Add ability for drivers to report an alternate MAD size.
    
    Add max MAD size to the device immutable data set and have all drivers that
    support MADs report the current IB MAD size (IB_MGMT_MAD_SIZE) to the core.
    
    Verify MAD size data in both the MAD core and when reading the immutable data.
    
    OPA drivers will report alternate MAD sizes in subsequent patches.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b02778812729..75c349969b6e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1534,6 +1534,7 @@ struct ib_port_immutable {
 	int                           pkey_tbl_len;
 	int                           gid_tbl_len;
 	u32                           core_cap_flags;
+	u32                           max_mad_size;
 };
 
 struct ib_device {
@@ -2052,6 +2053,23 @@ static inline bool rdma_cap_read_multi_sge(struct ib_device *device,
 	return !(device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP);
 }
 
+/**
+ * rdma_max_mad_size - Return the max MAD size required by this RDMA Port.
+ *
+ * @device: Device
+ * @port_num: Port number
+ *
+ * This MAD size includes the MAD headers and MAD payload.  No other headers
+ * are included.
+ *
+ * Return the max MAD size required by the Port.  Will return 0 if the port
+ * does not support MADs
+ */
+static inline size_t rdma_max_mad_size(const struct ib_device *device, u8 port_num)
+{
+	return device->port_immutable[port_num].max_mad_size;
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit 2528e33e680921d95092f83c4a64046744f111b3
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:25 2015 +0300

    IB/core: Pass hardware specific data in query_device
    
    Vendors should be able to pass vendor specific data to/from
    user-space via query_device uverb. In order to do this,
    we need to pass the vendors' specific udata.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c0929245ddee..b02778812729 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1561,7 +1561,8 @@ struct ib_device {
 	int		           (*get_protocol_stats)(struct ib_device *device,
 							 union rdma_protocol_stats *stats);
 	int		           (*query_device)(struct ib_device *device,
-						   struct ib_device_attr *device_attr);
+						   struct ib_device_attr *device_attr,
+						   struct ib_udata *udata);
 	int		           (*query_port)(struct ib_device *device,
 						 u8 port_num,
 						 struct ib_port_attr *port_attr);

commit 24306dc66149020c59a07b64e2a325af59ee7d10
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:24 2015 +0300

    IB/core: Add timestamp_mask and hca_core_clock to query_device
    
    In order to expose timestamp we need to expose two new attributes in
    query_device to be used for CQ completion time-stamping:
    
    timestamp_mask - how many bits are valid in the timestamp, where timestamp
    values could be 64bits the most.
    
    hca_core_clock - timestamp is given in HW cycles, the frequency in KHZ units
    of the HCA, necessary in order to convert cycles to seconds.
    
    This is added both to ib_query_device and its respective uverbs counterpart.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2b4bf0632c64..c0929245ddee 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -227,6 +227,8 @@ struct ib_device_attr {
 	int			sig_prot_cap;
 	int			sig_guard_cap;
 	struct ib_odp_caps	odp_caps;
+	uint64_t		timestamp_mask;
+	uint64_t		hca_core_clock; /* in KHZ */
 };
 
 enum ib_mtu {

commit b9926b92a4d44c94b0d99bfa802ac7b37dabc9e8
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:22 2015 +0300

    IB/core: Add CQ creation time-stamping flag
    
    Add CQ creation flag which dictates that the created CQ will report
    completion time-stamp value in the WC.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ea01e9953ec7..2b4bf0632c64 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -173,6 +173,10 @@ struct ib_odp_caps {
 	} per_transport_caps;
 };
 
+enum ib_cq_creation_flags {
+	IB_CQ_FLAGS_TIMESTAMP_COMPLETION   = 1 << 0,
+};
+
 struct ib_cq_init_attr {
 	unsigned int	cqe;
 	int		comp_vector;

commit 8e37210b38fb7d6aa06aebde763316ee955d44c0
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:21 2015 +0300

    IB/core: Change ib_create_cq to use struct ib_cq_init_attr
    
    Currently, ib_create_cq uses cqe and comp_vecotr instead
    of the extendible ib_cq_init_attr struct.
    
    Earlier patches already changed the vendors to work with
    ib_cq_init_attr. This patch changes the consumers too.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b25ffa05e338..ea01e9953ec7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2314,16 +2314,15 @@ static inline int ib_post_recv(struct ib_qp *qp,
  *   asynchronous event not associated with a completion occurs on the CQ.
  * @cq_context: Context associated with the CQ returned to the user via
  *   the associated completion and event handlers.
- * @cqe: The minimum size of the CQ.
- * @comp_vector - Completion vector used to signal completion events.
- *     Must be >= 0 and < context->num_comp_vectors.
+ * @cq_attr: The attributes the CQ should be created upon.
  *
  * Users can examine the cq structure to determine the actual CQ size.
  */
 struct ib_cq *ib_create_cq(struct ib_device *device,
 			   ib_comp_handler comp_handler,
 			   void (*event_handler)(struct ib_event *, void *),
-			   void *cq_context, int cqe, int comp_vector);
+			   void *cq_context,
+			   const struct ib_cq_init_attr *cq_attr);
 
 /**
  * ib_resize_cq - Modifies the capacity of the CQ.

commit bcf4c1ea583cd213f0bafdbeb11d80f83c5f10e6
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:20 2015 +0300

    IB/core: Change provider's API of create_cq to be extendible
    
    Add a new ib_cq_init_attr structure which contains the
    previous cqe (minimum number of CQ entries) and comp_vector
    (completion vector) in addition to a new flags field.
    All vendors' create_cq callbacks are changed in order
    to work with the new API.
    
    This commit does not change any functionality.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com> to patch #2
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7725ceea7864..b25ffa05e338 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -173,6 +173,12 @@ struct ib_odp_caps {
 	} per_transport_caps;
 };
 
+struct ib_cq_init_attr {
+	unsigned int	cqe;
+	int		comp_vector;
+	u32		flags;
+};
+
 struct ib_device_attr {
 	u64			fw_ver;
 	__be64			sys_image_guid;
@@ -1613,8 +1619,8 @@ struct ib_device {
 	int                        (*post_recv)(struct ib_qp *qp,
 						struct ib_recv_wr *recv_wr,
 						struct ib_recv_wr **bad_recv_wr);
-	struct ib_cq *             (*create_cq)(struct ib_device *device, int cqe,
-						int comp_vector,
+	struct ib_cq *             (*create_cq)(struct ib_device *device,
+						const struct ib_cq_init_attr *attr,
 						struct ib_ucontext *context,
 						struct ib_udata *udata);
 	int                        (*modify_cq)(struct ib_cq *cq, u16 cq_count,

commit db75d0547cf230a433fab4d68630849532f2c8a0
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Jun 10 11:43:35 2015 +0300

    IB/core: Don't advertise SA in RoCE port capabilities
    
    The Subnet Administrator (SA) is not a component of the RoCE spec.
    Therefore, it should not be a capability of a RoCE port.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7d78794ed189..7725ceea7864 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -381,7 +381,6 @@ union rdma_protocol_stats {
 #define RDMA_CORE_PORT_IBA_ROCE        (RDMA_CORE_CAP_PROT_ROCE \
 					| RDMA_CORE_CAP_IB_MAD  \
 					| RDMA_CORE_CAP_IB_CM   \
-					| RDMA_CORE_CAP_IB_SA   \
 					| RDMA_CORE_CAP_AF_IB   \
 					| RDMA_CORE_CAP_ETH_AH)
 #define RDMA_CORE_PORT_IWARP           (RDMA_CORE_CAP_PROT_IWARP \

commit 73cdaaeed10d91441cb946200b5dbbbeb143bace
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sun May 31 17:15:31 2015 -0400

    IB/core cleanup: Add const to args - agent_send_response
    
    In order to support constant callers of agent_send_response we add const
    specifiers to the its pointer arguments.
    
    Adjust the call tree accordingly.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Hal Rosenstock <hal@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 48c9acbea53e..7d78794ed189 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2096,8 +2096,9 @@ struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
  * @ah_attr: Returned attributes that can be used when creating an address
  *   handle for replying to the message.
  */
-int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
-		       struct ib_grh *grh, struct ib_ah_attr *ah_attr);
+int ib_init_ah_from_wc(struct ib_device *device, u8 port_num,
+		       const struct ib_wc *wc, const struct ib_grh *grh,
+		       struct ib_ah_attr *ah_attr);
 
 /**
  * ib_create_ah_from_wc - Creates an address handle associated with the
@@ -2111,8 +2112,8 @@ int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
  * The address handle is used to reference a local or global destination
  * in all UD QP post sends.
  */
-struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, struct ib_wc *wc,
-				   struct ib_grh *grh, u8 port_num);
+struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, const struct ib_wc *wc,
+				   const struct ib_grh *grh, u8 port_num);
 
 /**
  * ib_modify_ah - Modifies the address vector associated with an address

commit a97e2d86a9b88ea9e9a280b594b80f0eec2c955b
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sun May 31 17:15:30 2015 -0400

    IB/core cleanup: Add const on args - device->process_mad
    
    The process_mad device function declares some parameters as "in".  Make those
    parameters const and adjust the call tree under process_mad in the various
    drivers accordingly.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Hal Rosenstock <hal@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f19739adf80d..48c9acbea53e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1690,9 +1690,9 @@ struct ib_device {
 	int                        (*process_mad)(struct ib_device *device,
 						  int process_mad_flags,
 						  u8 port_num,
-						  struct ib_wc *in_wc,
-						  struct ib_grh *in_grh,
-						  struct ib_mad *in_mad,
+						  const struct ib_wc *in_wc,
+						  const struct ib_grh *in_grh,
+						  const struct ib_mad *in_mad,
 						  struct ib_mad *out_mad);
 	struct ib_xrcd *	   (*alloc_xrcd)(struct ib_device *device,
 						 struct ib_ucontext *ucontext,

commit 5ede9289859d94746d1ce55ad5ba038b42b9406c
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sun May 31 17:15:29 2015 -0400

    IB/core cleanup: Add const to RDMA helpers
    
    The ib_device passed to the new RDMA helpers is constant.  Declare the
    ib_device as const in the following functions.
    
    rdma_protocol_ib
    rdma_protocol_roce
    rdma_protocol_iwarp
    rdma_ib_or_roce
    rdma_cap_ib_mad
    rdma_cap_ib_smi
    rdma_cap_ib_cm
    rdma_cap_iw_cm
    rdma_cap_ib_sa
    rdma_cap_ib_mcast
    rdma_cap_af_ib
    rdma_cap_eth_ah
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Hal Rosenstock <hal@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ad499bda62a4..f19739adf80d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1831,22 +1831,22 @@ static inline u8 rdma_end_port(const struct ib_device *device)
 		0 : device->phys_port_cnt;
 }
 
-static inline bool rdma_protocol_ib(struct ib_device *device, u8 port_num)
+static inline bool rdma_protocol_ib(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;
 }
 
-static inline bool rdma_protocol_roce(struct ib_device *device, u8 port_num)
+static inline bool rdma_protocol_roce(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE;
 }
 
-static inline bool rdma_protocol_iwarp(struct ib_device *device, u8 port_num)
+static inline bool rdma_protocol_iwarp(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP;
 }
 
-static inline bool rdma_ib_or_roce(struct ib_device *device, u8 port_num)
+static inline bool rdma_ib_or_roce(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags &
 		(RDMA_CORE_CAP_PROT_IB | RDMA_CORE_CAP_PROT_ROCE);
@@ -1864,7 +1864,7 @@ static inline bool rdma_ib_or_roce(struct ib_device *device, u8 port_num)
  *
  * Return: true if the port supports sending/receiving of MAD packets.
  */
-static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_ib_mad(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_MAD;
 }
@@ -1889,7 +1889,7 @@ static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
  *
  * Return: true if the port provides an SMI.
  */
-static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_ib_smi(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SMI;
 }
@@ -1909,7 +1909,7 @@ static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
  * Return: true if the port supports an IB CM (this does not guarantee that
  * a CM is actually running however).
  */
-static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_ib_cm(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_CM;
 }
@@ -1926,7 +1926,7 @@ static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
  * Return: true if the port supports an iWARP CM (this does not guarantee that
  * a CM is actually running however).
  */
-static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_iw_cm(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IW_CM;
 }
@@ -1946,7 +1946,7 @@ static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
  * Administration interface.  This does not imply that the SA service is
  * running locally.
  */
-static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_ib_sa(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SA;
 }
@@ -1968,7 +1968,7 @@ static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
  * overhead of registering/unregistering with the SM and tracking of the
  * total number of queue pairs attached to the multicast group.
  */
-static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_ib_mcast(const struct ib_device *device, u8 port_num)
 {
 	return rdma_cap_ib_sa(device, port_num);
 }
@@ -1986,7 +1986,7 @@ static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
  * Return: true if the port uses a GID address to identify devices on the
  * network.
  */
-static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_af_ib(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_AF_IB;
 }
@@ -2007,7 +2007,7 @@ static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
  * addition of a Global Route Header built from our Ethernet Address
  * Handle into our header list for connectionless packets.
  */
-static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
+static inline bool rdma_cap_eth_ah(const struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_ETH_AH;
 }

commit 175e8efe69974e240a89fb3a4bbeeb0b892de10d
Merge: 985aa49556a5 3c88f3dcff7b f766c58fa3ea 5d9fb0440698
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed May 20 16:12:40 2015 -0400

    Merge branches 'bart-srp', 'generic-errors', 'ira-cleanups' and 'mwang-v8' into k.o/for-4.2

commit 5d9fb0440698a8b9e8595353d60cfac7ab30efae
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Thu May 14 15:01:46 2015 -0400

    IB/core: Change rdma_protocol_iboe to roce
    
    After discussion upstream, it was agreed to transition the usage of iboe
    in the kernel to roce.  This keeps our terminology consistent with what
    was finalized in the IBTA Annex 16 and IBTA Annex 17 publications.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73d1b1000785..3ebf0c019a66 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1832,7 +1832,7 @@ static inline bool rdma_protocol_ib(struct ib_device *device, u8 port_num)
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;
 }
 
-static inline bool rdma_protocol_iboe(struct ib_device *device, u8 port_num)
+static inline bool rdma_protocol_roce(struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE;
 }
@@ -1842,7 +1842,7 @@ static inline bool rdma_protocol_iwarp(struct ib_device *device, u8 port_num)
 	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP;
 }
 
-static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
+static inline bool rdma_ib_or_roce(struct ib_device *device, u8 port_num)
 {
 	return device->port_immutable[port_num].core_cap_flags &
 		(RDMA_CORE_CAP_PROT_IB | RDMA_CORE_CAP_PROT_ROCE);

commit f9b22e355d38c8dbfa19a2d9d5ef9bf07e7c17e6
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Wed May 13 20:02:59 2015 -0400

    IB/core: Convert core to use bitfield for caps
    
    Remove query_protocol callback
    
    Use the new Core Capability bits for:
    
    rdma_protocol_*
    rdma_cap_ib_mad
    rdma_cap_ib_smi
    rdma_cap_ib_cm
    rdma_cap_iw_cm
    rdma_cap_ib_sa
    rdma_cap_ib_mcast
    rdma_cap_af_ib
    rdma_cap_eth_ah
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2d3515edc3fa..73d1b1000785 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -353,6 +353,40 @@ union rdma_protocol_stats {
 	struct iw_protocol_stats	iw;
 };
 
+/* Define bits for the various functionality this port needs to be supported by
+ * the core.
+ */
+/* Management                           0x00000FFF */
+#define RDMA_CORE_CAP_IB_MAD            0x00000001
+#define RDMA_CORE_CAP_IB_SMI            0x00000002
+#define RDMA_CORE_CAP_IB_CM             0x00000004
+#define RDMA_CORE_CAP_IW_CM             0x00000008
+#define RDMA_CORE_CAP_IB_SA             0x00000010
+
+/* Address format                       0x000FF000 */
+#define RDMA_CORE_CAP_AF_IB             0x00001000
+#define RDMA_CORE_CAP_ETH_AH            0x00002000
+
+/* Protocol                             0xFFF00000 */
+#define RDMA_CORE_CAP_PROT_IB           0x00100000
+#define RDMA_CORE_CAP_PROT_ROCE         0x00200000
+#define RDMA_CORE_CAP_PROT_IWARP        0x00400000
+
+#define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \
+					| RDMA_CORE_CAP_IB_MAD \
+					| RDMA_CORE_CAP_IB_SMI \
+					| RDMA_CORE_CAP_IB_CM  \
+					| RDMA_CORE_CAP_IB_SA  \
+					| RDMA_CORE_CAP_AF_IB)
+#define RDMA_CORE_PORT_IBA_ROCE        (RDMA_CORE_CAP_PROT_ROCE \
+					| RDMA_CORE_CAP_IB_MAD  \
+					| RDMA_CORE_CAP_IB_CM   \
+					| RDMA_CORE_CAP_IB_SA   \
+					| RDMA_CORE_CAP_AF_IB   \
+					| RDMA_CORE_CAP_ETH_AH)
+#define RDMA_CORE_PORT_IWARP           (RDMA_CORE_CAP_PROT_IWARP \
+					| RDMA_CORE_CAP_IW_CM)
+
 struct ib_port_attr {
 	enum ib_port_state	state;
 	enum ib_mtu		max_mtu;
@@ -1484,6 +1518,7 @@ struct iw_cm_verbs;
 struct ib_port_immutable {
 	int                           pkey_tbl_len;
 	int                           gid_tbl_len;
+	u32                           core_cap_flags;
 };
 
 struct ib_device {
@@ -1515,8 +1550,6 @@ struct ib_device {
 	int		           (*query_port)(struct ib_device *device,
 						 u8 port_num,
 						 struct ib_port_attr *port_attr);
-	enum rdma_protocol_type    (*query_protocol)(struct ib_device *device,
-						     u8 port_num);
 	enum rdma_link_layer	   (*get_link_layer)(struct ib_device *device,
 						     u8 port_num);
 	int		           (*query_gid)(struct ib_device *device,
@@ -1796,24 +1829,23 @@ static inline u8 rdma_end_port(const struct ib_device *device)
 
 static inline bool rdma_protocol_ib(struct ib_device *device, u8 port_num)
 {
-	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IB;
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IB;
 }
 
 static inline bool rdma_protocol_iboe(struct ib_device *device, u8 port_num)
 {
-	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IBOE;
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_ROCE;
 }
 
 static inline bool rdma_protocol_iwarp(struct ib_device *device, u8 port_num)
 {
-	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IWARP;
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP;
 }
 
 static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
 {
-	enum rdma_protocol_type pt = device->query_protocol(device, port_num);
-
-	return (pt == RDMA_PROTOCOL_IB || pt == RDMA_PROTOCOL_IBOE);
+	return device->port_immutable[port_num].core_cap_flags &
+		(RDMA_CORE_CAP_PROT_IB | RDMA_CORE_CAP_PROT_ROCE);
 }
 
 /**
@@ -1830,7 +1862,7 @@ static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
 {
-	return rdma_ib_or_iboe(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_MAD;
 }
 
 /**
@@ -1855,7 +1887,7 @@ static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
 {
-	return rdma_protocol_ib(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SMI;
 }
 
 /**
@@ -1875,7 +1907,7 @@ static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
 {
-	return rdma_ib_or_iboe(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_CM;
 }
 
 /**
@@ -1892,7 +1924,7 @@ static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
 {
-	return rdma_protocol_iwarp(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IW_CM;
 }
 
 /**
@@ -1912,7 +1944,7 @@ static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
 {
-	return rdma_protocol_ib(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_IB_SA;
 }
 
 /**
@@ -1952,7 +1984,7 @@ static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
 {
-	return rdma_ib_or_iboe(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_AF_IB;
 }
 
 /**
@@ -1973,7 +2005,7 @@ static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
  */
 static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
 {
-	return rdma_protocol_iboe(device, port_num);
+	return device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_ETH_AH;
 }
 
 /**
@@ -2001,7 +2033,7 @@ static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
 static inline bool rdma_cap_read_multi_sge(struct ib_device *device,
 					   u8 port_num)
 {
-	return !rdma_protocol_iwarp(device, port_num);
+	return !(device->port_immutable[port_num].core_cap_flags & RDMA_CORE_CAP_PROT_IWARP);
 }
 
 int ib_query_gid(struct ib_device *device,

commit 7738613e7cb419179545910744b1777d87edac5c
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Wed May 13 20:02:58 2015 -0400

    IB/core: Add per port immutable struct to ib_device
    
    As of commit 5eb620c81ce3 "IB/core: Add helpers for uncached GID and P_Key
    searches"; pkey_tbl_len and gid_tbl_len are immutable data which are stored in
    the ib_device.
    
    The per port core capability flags to be added later are also immutable data to
    be stored in the ib_device object.
    
    In preparation for this create a structure for per port immutable data and
    place the pkey and gid table lengths within this structure.
    
    "get_port_immutable" is added as a mandatory device function to allow the
    drivers to fill in this data.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index be4465b5df7b..2d3515edc3fa 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1481,6 +1481,11 @@ struct ib_dma_mapping_ops {
 
 struct iw_cm_verbs;
 
+struct ib_port_immutable {
+	int                           pkey_tbl_len;
+	int                           gid_tbl_len;
+};
+
 struct ib_device {
 	struct device                *dma_device;
 
@@ -1494,8 +1499,10 @@ struct ib_device {
 	struct list_head              client_data_list;
 
 	struct ib_cache               cache;
-	int                          *pkey_tbl_len;
-	int                          *gid_tbl_len;
+	/**
+	 * port_immutable is indexed by port number
+	 */
+	struct ib_port_immutable     *port_immutable;
 
 	int			      num_comp_vectors;
 
@@ -1684,6 +1691,14 @@ struct ib_device {
 	u32			     local_dma_lkey;
 	u8                           node_type;
 	u8                           phys_port_cnt;
+
+	/**
+	 * The following mandatory functions are used only at device
+	 * registration.  Keep functions such as these at the end of this
+	 * structure to avoid cache line misses when accessing struct ib_device
+	 * in fast paths.
+	 */
+	int (*get_port_immutable)(struct ib_device *, u8, struct ib_port_immutable *);
 };
 
 struct ib_client {

commit 2b1b5b601230ae4356be4724ea7a058ed7203c63
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Mon May 18 13:40:28 2015 +0300

    IB/core, cma: Nice log-friendly string helpers
    
    Some of us keep revisiting the code to decode enumerations that
    appear in out logs. Let's borrow the nice logging helpers that
    exists in xprtrdma and rds for CMA events, IB events and WC statuses.
    
    Reviewd-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 65994a19e840..672fc8f20409 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -412,6 +412,8 @@ enum ib_event_type {
 	IB_EVENT_GID_CHANGE,
 };
 
+__attribute_const__ const char *ib_event_msg(enum ib_event_type event);
+
 struct ib_event {
 	struct ib_device	*device;
 	union {
@@ -663,6 +665,8 @@ enum ib_wc_status {
 	IB_WC_GENERAL_ERR
 };
 
+__attribute_const__ const char *ib_wc_status_msg(enum ib_wc_status status);
+
 enum ib_wc_opcode {
 	IB_WC_SEND,
 	IB_WC_RDMA_WRITE,

commit 0cf18d7723055709faf51b50f5a33253b480637f
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Wed May 13 20:02:55 2015 -0400

    IB/core: Create common start/end port functions
    
    Previously start_port and end_port were defined in 2 places, cache.c and
    device.c and this prevented their use in other modules.
    
    Make these common functions, change the name to reflect the rdma
    name space, and update existing users.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 81740c14fdb1..be4465b5df7b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1752,6 +1752,33 @@ int ib_query_port(struct ib_device *device,
 enum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,
 					       u8 port_num);
 
+/**
+ * rdma_start_port - Return the first valid port number for the device
+ * specified
+ *
+ * @device: Device to be checked
+ *
+ * Return start port number
+ */
+static inline u8 rdma_start_port(const struct ib_device *device)
+{
+	return (device->node_type == RDMA_NODE_IB_SWITCH) ? 0 : 1;
+}
+
+/**
+ * rdma_end_port - Return the last valid port number for the device
+ * specified
+ *
+ * @device: Device to be checked
+ *
+ * Return last port number
+ */
+static inline u8 rdma_end_port(const struct ib_device *device)
+{
+	return (device->node_type == RDMA_NODE_IB_SWITCH) ?
+		0 : device->phys_port_cnt;
+}
+
 static inline bool rdma_protocol_ib(struct ib_device *device, u8 port_num)
 {
 	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IB;

commit 296ec00995fb28c4e34b41f80b5a876f3a25c134
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Mon May 18 10:41:45 2015 +0200

    IB/Verbs: Improve docs for rdma-helpers
    
    Increase the level of documentation for the rdma_cap_* helpers
    introduced by Michael Wang <yun.wang@profitbricks.com>.
    
    This patch is loosely based on a patch Michael wrote to enhance the
    documentation of these functions, but has been significantly modified
    in terms of verbiage.  In addition, the comments were moved from a kernel
    Documentation/infiniband/ file to being inline in the header file itself
    for the functions in question.  Finally, the documentation was formated
    in proper kdoc format.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8d59479eea4d..81740c14fdb1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1775,14 +1775,16 @@ static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
 }
 
 /**
- * rdma_cap_ib_mad - Check if the port of device has the capability Infiniband
+ * rdma_cap_ib_mad - Check if the port of a device supports Infiniband
  * Management Datagrams.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * Management Datagrams (MAD) are a required part of the InfiniBand
+ * specification and are supported on all InfiniBand devices.  A slightly
+ * extended version are also supported on OPA interfaces.
  *
- * Return false when port of the device don't support Infiniband
- * Management Datagrams.
+ * Return: true if the port supports sending/receiving of MAD packets.
  */
 static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
 {
@@ -1790,14 +1792,24 @@ static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
 }
 
 /**
- * rdma_cap_ib_smi - Check if the port of device has the capability Infiniband
- * Subnet Management Interface.
+ * rdma_cap_ib_smi - Check if the port of a device provides an Infiniband
+ * Subnet Management Agent (SMA) on the Subnet Management Interface (SMI).
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * Each InfiniBand node is required to provide a Subnet Management Agent
+ * that the subnet manager can access.  Prior to the fabric being fully
+ * configured by the subnet manager, the SMA is accessed via a well known
+ * interface called the Subnet Management Interface (SMI).  This interface
+ * uses directed route packets to communicate with the SM to get around the
+ * chicken and egg problem of the SM needing to know what's on the fabric
+ * in order to configure the fabric, and needing to configure the fabric in
+ * order to send packets to the devices on the fabric.  These directed
+ * route packets do not need the fabric fully configured in order to reach
+ * their destination.  The SMI is the only method allowed to send
+ * directed route packets on an InfiniBand fabric.
  *
- * Return false when port of the device don't support Infiniband
- * Subnet Management Interface.
+ * Return: true if the port provides an SMI.
  */
 static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
 {
@@ -1807,12 +1819,17 @@ static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_ib_cm - Check if the port of device has the capability Infiniband
  * Communication Manager.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * The InfiniBand Communication Manager is one of many pre-defined General
+ * Service Agents (GSA) that are accessed via the General Service
+ * Interface (GSI).  It's role is to facilitate establishment of connections
+ * between nodes as well as other management related tasks for established
+ * connections.
  *
- * Return false when port of the device don't support Infiniband
- * Communication Manager.
+ * Return: true if the port supports an IB CM (this does not guarantee that
+ * a CM is actually running however).
  */
 static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
 {
@@ -1822,12 +1839,14 @@ static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_iw_cm - Check if the port of device has the capability IWARP
  * Communication Manager.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * Similar to above, but specific to iWARP connections which have a different
+ * managment protocol than InfiniBand.
  *
- * Return false when port of the device don't support IWARP
- * Communication Manager.
+ * Return: true if the port supports an iWARP CM (this does not guarantee that
+ * a CM is actually running however).
  */
 static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
 {
@@ -1837,12 +1856,17 @@ static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_ib_sa - Check if the port of device has the capability Infiniband
  * Subnet Administration.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * An InfiniBand Subnet Administration (SA) service is a pre-defined General
+ * Service Agent (GSA) provided by the Subnet Manager (SM).  On InfiniBand
+ * fabrics, devices should resolve routes to other hosts by contacting the
+ * SA to query the proper route.
  *
- * Return false when port of the device don't support Infiniband
- * Subnet Administration.
+ * Return: true if the port should act as a client to the fabric Subnet
+ * Administration interface.  This does not imply that the SA service is
+ * running locally.
  */
 static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
 {
@@ -1852,12 +1876,19 @@ static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_ib_mcast - Check if the port of device has the capability Infiniband
  * Multicast.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * InfiniBand multicast registration is more complex than normal IPv4 or
+ * IPv6 multicast registration.  Each Host Channel Adapter must register
+ * with the Subnet Manager when it wishes to join a multicast group.  It
+ * should do so only once regardless of how many queue pairs it subscribes
+ * to this group.  And it should leave the group only after all queue pairs
+ * attached to the group have been detached.
  *
- * Return false when port of the device don't support Infiniband
- * Multicast.
+ * Return: true if the port must undertake the additional adminstrative
+ * overhead of registering/unregistering with the SM and tracking of the
+ * total number of queue pairs attached to the multicast group.
  */
 static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
 {
@@ -1867,12 +1898,15 @@ static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_af_ib - Check if the port of device has the capability
  * Native Infiniband Address.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * InfiniBand addressing uses a port's GUID + Subnet Prefix to make a default
+ * GID.  RoCE uses a different mechanism, but still generates a GID via
+ * a prescribed mechanism and port specific data.
  *
- * Return false when port of the device don't support
- * Native Infiniband Address.
+ * Return: true if the port uses a GID address to identify devices on the
+ * network.
  */
 static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
 {
@@ -1881,13 +1915,19 @@ static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
 
 /**
  * rdma_cap_eth_ah - Check if the port of device has the capability
- * Ethernet Address Handler.
+ * Ethernet Address Handle.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * RoCE is InfiniBand over Ethernet, and it uses a well defined technique
+ * to fabricate GIDs over Ethernet/IP specific addresses native to the
+ * port.  Normally, packet headers are generated by the sending host
+ * adapter, but when sending connectionless datagrams, we must manually
+ * inject the proper headers for the fabric we are communicating over.
  *
- * Return false when port of the device don't support
- * Ethernet Address Handler.
+ * Return: true if we are running as a RoCE port and must force the
+ * addition of a Global Route Header built from our Ethernet Address
+ * Handle into our header list for connectionless packets.
  */
 static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
 {
@@ -1897,12 +1937,24 @@ static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
 /**
  * rdma_cap_read_multi_sge - Check if the port of device has the capability
  * RDMA Read Multiple Scatter-Gather Entries.
+ * @device: Device to check
+ * @port_num: Port number to check
  *
- * @device: Device to be checked
- * @port_num: Port number of the device
+ * iWARP has a restriction that RDMA READ requests may only have a single
+ * Scatter/Gather Entry (SGE) in the work request.
  *
- * Return false when port of the device don't support
- * RDMA Read Multiple Scatter-Gather Entries.
+ * NOTE: although the linux kernel currently assumes all devices are either
+ * single SGE RDMA READ devices or identical SGE maximums for RDMA READs and
+ * WRITEs, according to Tom Talpey, this is not accurate.  There are some
+ * devices out there that support more than a single SGE on RDMA READ
+ * requests, but do not support the same number of SGEs as they do on
+ * RDMA WRITE requests.  The linux kernel would need rearchitecting to
+ * support these imbalanced READ/WRITE SGEs allowed devices.  So, for now,
+ * suffice with either the device supports the same READ/WRITE SGEs, or
+ * it only gets one READ sge.
+ *
+ * Return: true for any device that allows more than one SGE in RDMA READ
+ * requests.
  */
 static inline bool rdma_cap_read_multi_sge(struct ib_device *device,
 					   u8 port_num)

commit 227128fc68401d8e36b660ffeef4320c5fb492d7
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:40 2015 +0200

    IB/Verbs: Use management helper rdma_cap_eth_ah()
    
    Introduce helper rdma_cap_eth_ah() to help us check if the port of an
    IB device support Ethernet Address Handler.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 349d1216564c..8d59479eea4d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1879,6 +1879,21 @@ static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
 	return rdma_ib_or_iboe(device, port_num);
 }
 
+/**
+ * rdma_cap_eth_ah - Check if the port of device has the capability
+ * Ethernet Address Handler.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support
+ * Ethernet Address Handler.
+ */
+static inline bool rdma_cap_eth_ah(struct ib_device *device, u8 port_num)
+{
+	return rdma_protocol_iboe(device, port_num);
+}
+
 /**
  * rdma_cap_read_multi_sge - Check if the port of device has the capability
  * RDMA Read Multiple Scatter-Gather Entries.

commit 30a74ef41d5293cb2f85fcce120fe869a672ade4
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:39 2015 +0200

    IB/Verbs: Use management helper rdma_cap_af_ib()
    
    Introduce helper rdma_cap_af_ib() to help us check if the port of an
    IB device support Native Infiniband Address.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2cf23b130f9f..349d1216564c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1864,6 +1864,21 @@ static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
 	return rdma_cap_ib_sa(device, port_num);
 }
 
+/**
+ * rdma_cap_af_ib - Check if the port of device has the capability
+ * Native Infiniband Address.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support
+ * Native Infiniband Address.
+ */
+static inline bool rdma_cap_af_ib(struct ib_device *device, u8 port_num)
+{
+	return rdma_ib_or_iboe(device, port_num);
+}
+
 /**
  * rdma_cap_read_multi_sge - Check if the port of device has the capability
  * RDMA Read Multiple Scatter-Gather Entries.

commit bc0f1d71536063f8b2df966625e0136bca03b3e6
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:38 2015 +0200

    IB/Verbs: Use management helper rdma_cap_read_multi_sge()
    
    Introduce helper rdma_cap_read_multi_sge() to help us check if the port of an
    IB device support RDMA Read Multiple Scatter-Gather Entries.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6bbbc86d39d9..2cf23b130f9f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1864,6 +1864,22 @@ static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
 	return rdma_cap_ib_sa(device, port_num);
 }
 
+/**
+ * rdma_cap_read_multi_sge - Check if the port of device has the capability
+ * RDMA Read Multiple Scatter-Gather Entries.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support
+ * RDMA Read Multiple Scatter-Gather Entries.
+ */
+static inline bool rdma_cap_read_multi_sge(struct ib_device *device,
+					   u8 port_num)
+{
+	return !rdma_protocol_iwarp(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit a31ad3b0e35f7e340c1ab6668080cff91d48c90a
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:37 2015 +0200

    IB/Verbs: Use management helper rdma_cap_ib_mcast()
    
    Introduce helper rdma_cap_ib_mcast() to help us check if the port of an
    IB device support Infiniband Multicast.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3a561e891b1..6bbbc86d39d9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1849,6 +1849,21 @@ static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
 	return rdma_protocol_ib(device, port_num);
 }
 
+/**
+ * rdma_cap_ib_mcast - Check if the port of device has the capability Infiniband
+ * Multicast.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support Infiniband
+ * Multicast.
+ */
+static inline bool rdma_cap_ib_mcast(struct ib_device *device, u8 port_num)
+{
+	return rdma_cap_ib_sa(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit fe53ba2f0c3de0416422407bab2c1982a2e85b6a
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:36 2015 +0200

    IB/Verbs: Use management helper rdma_cap_ib_sa()
    
    Introduce helper rdma_cap_ib_sa() to help us check if the port of an
    IB device support Infiniband Subnet Administration.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cc92a6489136..c3a561e891b1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1834,6 +1834,21 @@ static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
 	return rdma_protocol_iwarp(device, port_num);
 }
 
+/**
+ * rdma_cap_ib_sa - Check if the port of device has the capability Infiniband
+ * Subnet Administration.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support Infiniband
+ * Subnet Administration.
+ */
+static inline bool rdma_cap_ib_sa(struct ib_device *device, u8 port_num)
+{
+	return rdma_protocol_ib(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit 042153306d9d08da67459f187d63a68aefd97388
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:35 2015 +0200

    IB/Verbs: Use management helper rdma_cap_iw_cm()
    
    Introduce helper rdma_cap_iw_cm() to help us check if the port of an
    IB device support IWARP Communication Manager.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e349596fd500..cc92a6489136 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1819,6 +1819,21 @@ static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
 	return rdma_ib_or_iboe(device, port_num);
 }
 
+/**
+ * rdma_cap_iw_cm - Check if the port of device has the capability IWARP
+ * Communication Manager.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support IWARP
+ * Communication Manager.
+ */
+static inline bool rdma_cap_iw_cm(struct ib_device *device, u8 port_num)
+{
+	return rdma_protocol_iwarp(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit 72219cea8e246a55bff92e5ff6ec21f331a8791e
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:34 2015 +0200

    IB/Verbs: Use management helper rdma_cap_ib_cm()
    
    Introduce helper rdma_cap_ib_cm() to help us check if the port of an
    IB device support Infiniband Communication Manager.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e983e335af21..e349596fd500 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1804,6 +1804,21 @@ static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
 	return rdma_protocol_ib(device, port_num);
 }
 
+/**
+ * rdma_cap_ib_cm - Check if the port of device has the capability Infiniband
+ * Communication Manager.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support Infiniband
+ * Communication Manager.
+ */
+static inline bool rdma_cap_ib_cm(struct ib_device *device, u8 port_num)
+{
+	return rdma_ib_or_iboe(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit 29541e3add4d03682b78311823a5426e82019cda
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:33 2015 +0200

    IB/Verbs: Use management helper rdma_cap_ib_smi()
    
    Introduce helper rdma_cap_ib_smi() to help us check if the port of an
    IB device support Infiniband Subnet Management Interface.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 23ba66e25b7f..e983e335af21 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1789,6 +1789,21 @@ static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
 	return rdma_ib_or_iboe(device, port_num);
 }
 
+/**
+ * rdma_cap_ib_smi - Check if the port of device has the capability Infiniband
+ * Subnet Management Interface.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support Infiniband
+ * Subnet Management Interface.
+ */
+static inline bool rdma_cap_ib_smi(struct ib_device *device, u8 port_num)
+{
+	return rdma_protocol_ib(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit c757dea816407dc472452091e3ea941cef6638a2
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:32 2015 +0200

    IB/Verbs: Use management helper rdma_cap_ib_mad()
    
    Introduce helper rdma_cap_ib_mad() to help us check if the port of an
    IB device support Infiniband Management Datagrams.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e6dd9846b6c2..23ba66e25b7f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1774,6 +1774,21 @@ static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
 	return (pt == RDMA_PROTOCOL_IB || pt == RDMA_PROTOCOL_IBOE);
 }
 
+/**
+ * rdma_cap_ib_mad - Check if the port of device has the capability Infiniband
+ * Management Datagrams.
+ *
+ * @device: Device to be checked
+ * @port_num: Port number of the device
+ *
+ * Return false when port of the device don't support Infiniband
+ * Management Datagrams.
+ */
+static inline bool rdma_cap_ib_mad(struct ib_device *device, u8 port_num)
+{
+	return rdma_ib_or_iboe(device, port_num);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit de66be94749d75c2f3578f3d01f91d31a8eb85ef
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:19 2015 +0200

    IB/Verbs: Implement raw management helpers
    
    Add raw helpers:
            rdma_protocol_ib
            rdma_protocol_iboe
            rdma_protocol_iwarp
            rdma_ib_or_iboe (transition, clean up later)
    To help us detect which technology the port supported.
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 080f204273e4..e6dd9846b6c2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1752,6 +1752,28 @@ int ib_query_port(struct ib_device *device,
 enum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,
 					       u8 port_num);
 
+static inline bool rdma_protocol_ib(struct ib_device *device, u8 port_num)
+{
+	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IB;
+}
+
+static inline bool rdma_protocol_iboe(struct ib_device *device, u8 port_num)
+{
+	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IBOE;
+}
+
+static inline bool rdma_protocol_iwarp(struct ib_device *device, u8 port_num)
+{
+	return device->query_protocol(device, port_num) == RDMA_PROTOCOL_IWARP;
+}
+
+static inline bool rdma_ib_or_iboe(struct ib_device *device, u8 port_num)
+{
+	enum rdma_protocol_type pt = device->query_protocol(device, port_num);
+
+	return (pt == RDMA_PROTOCOL_IB || pt == RDMA_PROTOCOL_IBOE);
+}
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit 6b90a6d66b17bfe09351e18c705cb4a2ed147300
Author: Michael Wang <yun.wang@profitbricks.com>
Date:   Tue May 5 14:50:18 2015 +0200

    IB/Verbs: Implement new callback query_protocol()
    
    Add new callback query_protocol() and implement for each HW.
    
    Mapping List:
                    node-type       link-layer      transport       protocol
    nes             RNIC            ETH             IWARP           IWARP
    amso1100        RNIC            ETH             IWARP           IWARP
    cxgb3           RNIC            ETH             IWARP           IWARP
    cxgb4           RNIC            ETH             IWARP           IWARP
    usnic           USNIC_UDP       ETH             USNIC_UDP       USNIC_UDP
    ocrdma          IB_CA           ETH             IB              IBOE
    mlx4            IB_CA           IB/ETH          IB              IB/IBOE
    mlx5            IB_CA           IB              IB              IB
    ehca            IB_CA           IB              IB              IB
    ipath           IB_CA           IB              IB              IB
    mthca           IB_CA           IB              IB              IB
    qib             IB_CA           IB              IB              IB
    
    Signed-off-by: Michael Wang <yun.wang@profitbricks.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Tested-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 65994a19e840..080f204273e4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -81,6 +81,13 @@ enum rdma_transport_type {
 	RDMA_TRANSPORT_USNIC_UDP
 };
 
+enum rdma_protocol_type {
+	RDMA_PROTOCOL_IB,
+	RDMA_PROTOCOL_IBOE,
+	RDMA_PROTOCOL_IWARP,
+	RDMA_PROTOCOL_USNIC_UDP
+};
+
 __attribute_const__ enum rdma_transport_type
 rdma_node_get_transport(enum rdma_node_type node_type);
 
@@ -1501,6 +1508,8 @@ struct ib_device {
 	int		           (*query_port)(struct ib_device *device,
 						 u8 port_num,
 						 struct ib_port_attr *port_attr);
+	enum rdma_protocol_type    (*query_protocol)(struct ib_device *device,
+						     u8 port_num);
 	enum rdma_link_layer	   (*get_link_layer)(struct ib_device *device,
 						     u8 port_num);
 	int		           (*query_gid)(struct ib_device *device,

commit 43c6116573ca0f9fc907e6b46861f2f142acb33b
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Thu Feb 5 22:10:18 2015 +0100

    Revert "IB/core: Add support for extended query device caps"
    
    While commit 7e36ef8205ff ("IB/core: Temporarily disable
    ex_query_device uverb") is correct as it makes the extended
    QUERY_DEVICE uverb (which came as part of commit 5a77abf9a97a
    ("IB/core: Add support for extended query device caps") and commit
    860f10a799c8 ("IB/core: Add flags for on demand paging support")) not
    available to userspace, it doesn't address the initial issue regarding
    ib_copy_to_udata() [1][2].
    
    Additionally, further discussions around this new uverb seems to
    conclude it would require a different data structure than the one
    currently described in <rdma/ib_user_verbs.h> [3].
    
    Both of these issues require a revert of the changes, so this patch
    partially reverts commit 8cdd312cfed7 ("IB/mlx5: Implement the ODP
    capability query verb") and commit 860f10a799c8 ("IB/core: Add flags
    for on demand paging support") and fully reverts commit 5a77abf9a97a
    ("IB/core: Add support for extended query device caps").
    
    [1] "Re: [PATCH v3 06/17] IB/core: Add support for extended query device caps"
        http://mid.gmane.org/1418733236.2779.26.camel@opteya.com
    
    [2] "Re: [PATCH] IB/core: Temporarily disable ex_query_device uverb"
        http://mid.gmane.org/1423067503.3030.83.camel@opteya.com
    
    [3] "RE: [PATCH v1 1/5] IB/uverbs: ex_query_device: answer must not depend on request's comp_mask"
        http://mid.gmane.org/2807E5FD2F6FDA4886F6618EAC48510E0CC12C30@CRSMSX101.amr.corp.intel.com
    
    Cc: Eli Cohen <eli@mellanox.com>
    Cc: Haggai Eran <haggaie@mellanox.com>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Cc: Sagi Grimberg <sagig@mellanox.com>
    Cc: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0d74f1de99aa..65994a19e840 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1707,10 +1707,7 @@ static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t
 
 static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len)
 {
-	size_t copy_sz;
-
-	copy_sz = min_t(size_t, len, udata->outlen);
-	return copy_to_user(udata->outbuf, src, copy_sz) ? -EFAULT : 0;
+	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
 }
 
 /**

commit 882214e2b12860bff1ccff15a3ec2bbb29d58c02
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:18 2014 +0200

    IB/core: Implement support for MMU notifiers regarding on demand paging regions
    
    * Add an interval tree implementation for ODP umems. Create an
      interval tree for each ucontext (including a count of the number of
      ODP MRs in this context, semaphore, etc.), and register ODP umems in
      the interval tree.
    * Add MMU notifiers handling functions, using the interval tree to
      notify only the relevant umems and underlying MRs.
    * Register to receive MMU notifier events from the MM subsystem upon
      ODP MR registration (and unregister accordingly).
    * Add a completion object to synchronize the destruction of ODP umems.
    * Add mechanism to abort page faults when there's a concurrent invalidation.
    
    The way we synchronize between concurrent invalidations and page
    faults is by keeping a counter of currently running invalidations, and
    a sequence number that is incremented whenever an invalidation is
    caught. The page fault code checks the counter and also verifies that
    the sequence number hasn't progressed before it updates the umem's
    page tables. This is similar to what the kvm module does.
    
    In order to prevent the case where we register a umem in the middle of
    an ongoing notifier, we also keep a per ucontext counter of the total
    number of active mmu notifiers. We only enable new umems when all the
    running notifiers complete.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Yuval Dagan <yuvalda@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3af5dcad1b69..0d74f1de99aa 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -51,6 +51,7 @@
 #include <uapi/linux/if_ether.h>
 
 #include <linux/atomic.h>
+#include <linux/mmu_notifier.h>
 #include <asm/uaccess.h>
 
 extern struct workqueue_struct *ib_wq;
@@ -1139,6 +1140,8 @@ struct ib_fmr_attr {
 	u8	page_shift;
 };
 
+struct ib_umem;
+
 struct ib_ucontext {
 	struct ib_device       *device;
 	struct list_head	pd_list;
@@ -1153,6 +1156,22 @@ struct ib_ucontext {
 	int			closing;
 
 	struct pid             *tgid;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+	struct rb_root      umem_tree;
+	/*
+	 * Protects .umem_rbroot and tree, as well as odp_mrs_count and
+	 * mmu notifiers registration.
+	 */
+	struct rw_semaphore	umem_rwsem;
+	void (*invalidate_range)(struct ib_umem *umem,
+				 unsigned long start, unsigned long end);
+
+	struct mmu_notifier	mn;
+	atomic_t		notifier_count;
+	/* A list of umems that don't have private mmu notifier counters yet. */
+	struct list_head	no_private_counters;
+	int                     odp_mrs_count;
+#endif
 };
 
 struct ib_uobject {

commit 8ada2c1c0c1d75a60723cd2ca7d49c594a146af6
Author: Shachar Raindel <raindel@mellanox.com>
Date:   Thu Dec 11 17:04:17 2014 +0200

    IB/core: Add support for on demand paging regions
    
    * Extend the umem struct to keep the ODP related data.
    * Allocate and initialize the ODP related information in the umem
      (page_list, dma_list) and freeing as needed in the end of the run.
    * Store a reference to the process PID struct in the ucontext.  Used to
      safely obtain the task_struct and the mm during fault handling,
      without preventing the task destruction if needed.
    * Add 2 helper functions: ib_umem_odp_map_dma_pages and
      ib_umem_odp_unmap_dma_pages. These functions get the DMA addresses
      of specific pages of the umem (and, currently, pin them).
    * Support for page faults only - IB core will keep the reference on
      the pages used and call put_page when freeing an ODP umem
      area. Invalidations support will be added in a later patch.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a41bc5a39ebf..3af5dcad1b69 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1151,6 +1151,8 @@ struct ib_ucontext {
 	struct list_head	xrcd_list;
 	struct list_head	rule_list;
 	int			closing;
+
+	struct pid             *tgid;
 };
 
 struct ib_uobject {

commit 860f10a799c83e38a69d5a69d80da5312a4c4106
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Dec 11 17:04:16 2014 +0200

    IB/core: Add flags for on demand paging support
    
    * Add a configuration option for enable on-demand paging support in
      the infiniband subsystem (CONFIG_INFINIBAND_ON_DEMAND_PAGING). In a
      later patch, this configuration option will select the MMU_NOTIFIER
      configuration option to enable mmu notifiers.
    * Add a flag for on demand paging (ODP) support in the IB device capabilities.
    * Add a flag to request ODP MR in the access flags to reg_mr.
    * Fail registrations done with the ODP flag when the low-level driver
      doesn't support this.
    * Change the conditions in which an MR will be writable to explicitly
      specify the access flags.  This is to avoid making an MR writable just
      because it is an ODP MR.
    * Add a ODP capabilities to the extended query device verb.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 97a999f9e4d8..a41bc5a39ebf 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -123,7 +123,8 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
 	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24),
 	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29),
-	IB_DEVICE_SIGNATURE_HANDOVER	= (1<<30)
+	IB_DEVICE_SIGNATURE_HANDOVER	= (1<<30),
+	IB_DEVICE_ON_DEMAND_PAGING	= (1<<31),
 };
 
 enum ib_signature_prot_cap {
@@ -143,6 +144,27 @@ enum ib_atomic_cap {
 	IB_ATOMIC_GLOB
 };
 
+enum ib_odp_general_cap_bits {
+	IB_ODP_SUPPORT = 1 << 0,
+};
+
+enum ib_odp_transport_cap_bits {
+	IB_ODP_SUPPORT_SEND	= 1 << 0,
+	IB_ODP_SUPPORT_RECV	= 1 << 1,
+	IB_ODP_SUPPORT_WRITE	= 1 << 2,
+	IB_ODP_SUPPORT_READ	= 1 << 3,
+	IB_ODP_SUPPORT_ATOMIC	= 1 << 4,
+};
+
+struct ib_odp_caps {
+	uint64_t general_caps;
+	struct {
+		uint32_t  rc_odp_caps;
+		uint32_t  uc_odp_caps;
+		uint32_t  ud_odp_caps;
+	} per_transport_caps;
+};
+
 struct ib_device_attr {
 	u64			fw_ver;
 	__be64			sys_image_guid;
@@ -186,6 +208,7 @@ struct ib_device_attr {
 	u8			local_ca_ack_delay;
 	int			sig_prot_cap;
 	int			sig_guard_cap;
+	struct ib_odp_caps	odp_caps;
 };
 
 enum ib_mtu {
@@ -1073,7 +1096,8 @@ enum ib_access_flags {
 	IB_ACCESS_REMOTE_READ	= (1<<2),
 	IB_ACCESS_REMOTE_ATOMIC	= (1<<3),
 	IB_ACCESS_MW_BIND	= (1<<4),
-	IB_ZERO_BASED		= (1<<5)
+	IB_ZERO_BASED		= (1<<5),
+	IB_ACCESS_ON_DEMAND     = (1<<6),
 };
 
 struct ib_phys_buf {

commit 5a77abf9a97a7ecc8fb0f6bf4ad411fb12b02f31
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Thu Dec 11 17:04:15 2014 +0200

    IB/core: Add support for extended query device caps
    
    Add extensible query device capabilities verb to allow adding new features.
    ib_uverbs_ex_query_device is added and copy_query_dev_fields is used to
    copy capability fields to be used by both ib_uverbs_query_device and
    ib_uverbs_ex_query_device.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 470a011d6fa4..97a999f9e4d8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1662,7 +1662,10 @@ static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t
 
 static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len)
 {
-	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
+	size_t copy_sz;
+
+	copy_sz = min_t(size_t, len, udata->outlen);
+	return copy_to_user(udata->outbuf, src, copy_sz) ? -EFAULT : 0;
 }
 
 /**

commit 78eda2bb6542057b214af3bc1cae09c63e65d1d1
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Wed Aug 13 19:54:35 2014 +0300

    IB/mlx5, iser, isert: Add Signature API additions
    
    Expose more signature setting parameters. We modify the signature API
    to allow usage of some new execution parameters relevant to data
    integrity feature.
    
    This patch modifies ib_sig_domain structure by:
    
    - Deprecate DIF type in signature API (operation will
      be determined by the parameters alone, no DIF type awareness)
    - Add APPTAG check bitmask (for input domain)
    - Add REFTAG remap (increment) flag for each domain
    - Add APPTAG/REFTAG escape options for each domain
    
    The mlx5 driver is modified to follow the new parameters in HW
    signature setup.
    
    At the moment the callers (iser/isert) hard-code new parameters (by
    DIF type). In the future, callers will retrieve them from the scsi
    command structure.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ed44cc07a7b3..470a011d6fa4 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -491,20 +491,14 @@ struct ib_mr_init_attr {
 	u32	    flags;
 };
 
-enum ib_signature_type {
-	IB_SIG_TYPE_T10_DIF,
-};
-
 /**
- * T10-DIF Signature types
- * T10-DIF types are defined by SCSI
- * specifications.
+ * Signature types
+ * IB_SIG_TYPE_NONE: Unprotected.
+ * IB_SIG_TYPE_T10_DIF: Type T10-DIF
  */
-enum ib_t10_dif_type {
-	IB_T10DIF_NONE,
-	IB_T10DIF_TYPE1,
-	IB_T10DIF_TYPE2,
-	IB_T10DIF_TYPE3
+enum ib_signature_type {
+	IB_SIG_TYPE_NONE,
+	IB_SIG_TYPE_T10_DIF,
 };
 
 /**
@@ -520,24 +514,26 @@ enum ib_t10_dif_bg_type {
 /**
  * struct ib_t10_dif_domain - Parameters specific for T10-DIF
  *     domain.
- * @type: T10-DIF type (0|1|2|3)
  * @bg_type: T10-DIF block guard type (CRC|CSUM)
  * @pi_interval: protection information interval.
  * @bg: seed of guard computation.
  * @app_tag: application tag of guard block
  * @ref_tag: initial guard block reference tag.
- * @type3_inc_reftag: T10-DIF type 3 does not state
- *     about the reference tag, it is the user
- *     choice to increment it or not.
+ * @ref_remap: Indicate wethear the reftag increments each block
+ * @app_escape: Indicate to skip block check if apptag=0xffff
+ * @ref_escape: Indicate to skip block check if reftag=0xffffffff
+ * @apptag_check_mask: check bitmask of application tag.
  */
 struct ib_t10_dif_domain {
-	enum ib_t10_dif_type	type;
 	enum ib_t10_dif_bg_type bg_type;
 	u16			pi_interval;
 	u16			bg;
 	u16			app_tag;
 	u32			ref_tag;
-	bool			type3_inc_reftag;
+	bool			ref_remap;
+	bool			app_escape;
+	bool			ref_escape;
+	u16			apptag_check_mask;
 };
 
 /**

commit 7e6edb9b2e0bcfb2a588db390c44d120213c57ae
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jul 31 11:01:28 2014 +0300

    IB/core: Add user MR re-registration support
    
    Memory re-registration is a feature that enables changing the
    attributes of a memory region registered by user-space, including PD,
    translation (address and length) and access flags.
    
    Add the required support in uverbs and the kernel verbs API.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7ccef342f724..ed44cc07a7b3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1097,7 +1097,8 @@ struct ib_mr_attr {
 enum ib_mr_rereg_flags {
 	IB_MR_REREG_TRANS	= 1,
 	IB_MR_REREG_PD		= (1<<1),
-	IB_MR_REREG_ACCESS	= (1<<2)
+	IB_MR_REREG_ACCESS	= (1<<2),
+	IB_MR_REREG_SUPPORTED	= ((IB_MR_REREG_ACCESS << 1) - 1)
 };
 
 /**
@@ -1547,6 +1548,13 @@ struct ib_device {
 						  u64 virt_addr,
 						  int mr_access_flags,
 						  struct ib_udata *udata);
+	int			   (*rereg_user_mr)(struct ib_mr *mr,
+						    int flags,
+						    u64 start, u64 length,
+						    u64 virt_addr,
+						    int mr_access_flags,
+						    struct ib_pd *pd,
+						    struct ib_udata *udata);
 	int                        (*query_mr)(struct ib_mr *mr,
 					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);

commit eeaddf3670d4974e17268ec78a576ad397e2dcd9
Merge: 60e1751cb52c 3c735d481b9a b7dfa8895f64 c7ca4b69d9ac 9eccfe109b27 8385fd841468 165cb465f73c 6c9b5d9b00ed 6fcd8d0d93fb 0cc65dd6918f 0a66d2bd300c d236cd0e209c ed477c4c83b3
Author: Roland Dreier <roland@purestorage.com>
Date:   Tue Jun 10 10:12:14 2014 -0700

    Merge branches 'core', 'cxgb3', 'cxgb4', 'iser', 'iwpm', 'misc', 'mlx4', 'mlx5', 'noio', 'ocrdma', 'qib', 'srp' and 'usnic' into for-next

commit 8385fd841468868e0b37a722530d75b0e8bfc5a8
Author: Roland Dreier <roland@purestorage.com>
Date:   Wed Jun 4 10:00:16 2014 -0700

    IB/core: Fix sparse warnings about redeclared functions
    
    Fix a few functions that are declared with __attribute_const__ in the
    ib_verbs.h header file but defined without it in verbs.c.  This gets rid
    of the following sparse warnings:
    
        drivers/infiniband/core/verbs.c:51:5: error: symbol 'ib_rate_to_mult' redeclared with different type (originally declared at include/rdma/ib_verbs.h:469) - different modifiers
        drivers/infiniband/core/verbs.c:68:14: error: symbol 'mult_to_ib_rate' redeclared with different type (originally declared at include/rdma/ib_verbs.h:607) - different modifiers
        drivers/infiniband/core/verbs.c:85:5: error: symbol 'ib_rate_to_mbps' redeclared with different type (originally declared at include/rdma/ib_verbs.h:476) - different modifiers
        drivers/infiniband/core/verbs.c:111:1: error: symbol 'rdma_node_get_transport' redeclared with different type (originally declared at include/rdma/ib_verbs.h:84) - different modifiers
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index acd825182977..e9da1bc89d25 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -80,8 +80,8 @@ enum rdma_transport_type {
 	RDMA_TRANSPORT_USNIC_UDP
 };
 
-enum rdma_transport_type
-rdma_node_get_transport(enum rdma_node_type node_type) __attribute_const__;
+__attribute_const__ enum rdma_transport_type
+rdma_node_get_transport(enum rdma_node_type node_type);
 
 enum rdma_link_layer {
 	IB_LINK_LAYER_UNSPECIFIED,
@@ -466,14 +466,14 @@ enum ib_rate {
  * converted to 2, since 5 Gbit/sec is 2 * 2.5 Gbit/sec.
  * @rate: rate to convert.
  */
-int ib_rate_to_mult(enum ib_rate rate) __attribute_const__;
+__attribute_const__ int ib_rate_to_mult(enum ib_rate rate);
 
 /**
  * ib_rate_to_mbps - Convert the IB rate enum to Mbps.
  * For example, IB_RATE_2_5_GBPS will be converted to 2500.
  * @rate: rate to convert.
  */
-int ib_rate_to_mbps(enum ib_rate rate) __attribute_const__;
+__attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);
 
 enum ib_mr_create_flags {
 	IB_MR_SIGNATURE_EN = 1,
@@ -604,7 +604,7 @@ struct ib_mr_status {
  * enum.
  * @mult: multiple to convert.
  */
-enum ib_rate mult_to_ib_rate(int mult) __attribute_const__;
+__attribute_const__ enum ib_rate mult_to_ib_rate(int mult);
 
 struct ib_ah_attr {
 	struct ib_global_route	grh;

commit 09b93088d75009807b72293f26e2634430ce5ba9
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Sun May 11 15:15:11 2014 +0300

    IB: Add a QP creation flag to use GFP_NOIO allocations
    
    This addresses a problem where NFS client writes over IPoIB connected
    mode may deadlock on memory allocation/writeback.
    
    The problem is not directly memory reclamation.  There is an indirect
    dependency between network filesystems writing back pages and
    ipoib_cm_tx_init() due to how a kworker is used.  Page reclaim cannot
    make forward progress until ipoib_cm_tx_init() succeeds and it is
    stuck in page reclaim itself waiting for network transmission.
    Ordinarily this situation may be avoided by having the caller use
    GFP_NOFS but ipoib_cm_tx_init() does not have that information.
    
    To address this, take a general approach and add a new QP creation
    flag that tells the low-level hardware driver to use GFP_NOIO for the
    memory allocations related to the new QP.
    
    Use the new flag in the ipoib connected mode path, and if the driver
    doesn't support it, re-issue the QP creation without the flag.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index acd825182977..d75b02f9c80d 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -783,6 +783,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
+	IB_QP_CREATE_USE_GFP_NOIO		= 1 << 7,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,

commit f7eaa7ed8fd46542275cf249cd934a366f6556bb
Merge: 2dea909444c2 96bb2706c883 b2853fd6c2d0 5de2ad986d7e 4661bd798f1b 0e9855dbf43a 349850f0a918 2d8f57d56f58 186f8ba062f7 ea58a595657d b3fe628da289 970918b32b03
Author: Roland Dreier <roland@purestorage.com>
Date:   Thu Apr 3 08:30:17 2014 -0700

    Merge branches 'core', 'cxgb4', 'ip-roce', 'iser', 'misc', 'mlx4', 'nes', 'ocrdma', 'qib', 'sgwrapper', 'srp' and 'usnic' into for-next

commit ea58a595657db88f55b5159442fdf0e34e1b4d95
Author: Mike Marciniszyn <mike.marciniszyn@intel.com>
Date:   Fri Mar 28 13:26:59 2014 -0400

    IB/core: Remove overload in ib_sg_dma*
    
    The code is replaced by driver specific changes and avoids the pointer
    NULL test for drivers that don't overload these operations.
    
    Suggested-by: <Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Tested-by: Vinod Kumar <vinod.kumar@intel.com>
    Signed-off-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6793f32ccb58..57777167dea7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1266,10 +1266,6 @@ struct ib_dma_mapping_ops {
 	void		(*unmap_sg)(struct ib_device *dev,
 				    struct scatterlist *sg, int nents,
 				    enum dma_data_direction direction);
-	u64		(*dma_address)(struct ib_device *dev,
-				       struct scatterlist *sg);
-	unsigned int	(*dma_len)(struct ib_device *dev,
-				   struct scatterlist *sg);
 	void		(*sync_single_for_cpu)(struct ib_device *dev,
 					       u64 dma_handle,
 					       size_t size,
@@ -2089,12 +2085,13 @@ static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
  * @dev: The device for which the DMA addresses were created
  * @sg: The scatter/gather entry
+ *
+ * Note: this function is obsolete. To do: change all occurrences of
+ * ib_sg_dma_address() into sg_dma_address().
  */
 static inline u64 ib_sg_dma_address(struct ib_device *dev,
 				    struct scatterlist *sg)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->dma_address(dev, sg);
 	return sg_dma_address(sg);
 }
 
@@ -2102,12 +2099,13 @@ static inline u64 ib_sg_dma_address(struct ib_device *dev,
  * ib_sg_dma_len - Return the DMA length from a scatter/gather entry
  * @dev: The device for which the DMA addresses were created
  * @sg: The scatter/gather entry
+ *
+ * Note: this function is obsolete. To do: change all occurrences of
+ * ib_sg_dma_len() into sg_dma_len().
  */
 static inline unsigned int ib_sg_dma_len(struct ib_device *dev,
 					 struct scatterlist *sg)
 {
-	if (dev->dma_ops)
-		return dev->dma_ops->dma_len(dev, sg);
 	return sg_dma_len(sg);
 }
 

commit 1b01d33560e78417334c2dc673bbfac6c644424c
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Sun Feb 23 14:19:05 2014 +0200

    IB/core: Introduce signature verbs API
    
    Introduce a verbs interface for signature-related operations.  A
    signature handover operation configures the layouts of data and
    protection attributes both in memory and wire domains.
    
    Signature operations are:
    
    - INSERT:
      Generate and insert protection information when handing over
      data from input space to output space.
    - validate and STRIP:
      Validate protection information and remove it when handing over
      data from input space to output space.
    - validate and PASS:
      Validate protection information and pass it when handing over
      data from input space to output space.
    
    Once the signature handover opration is done, the HCA will offload
    data integrity generation/validation while performing the actual data
    transfer.
    
    Additions:
    
    1. HCA signature capabilities in device attributes
        Verbs provider supporting signature handover operations fills
        relevant fields in device attributes structure returned by
        ib_query_device.
    
    2. QP creation flag IB_QP_CREATE_SIGNATURE_EN
        Creating a QP that will carry signature handover operations may
        require some special preparations from the verbs provider.  So we
        add QP creation flag IB_QP_CREATE_SIGNATURE_EN to declare that the
        created QP may carry out signature handover operations.  Expose
        signature support to verbs layer (no support for now).
    
    3. New send work request IB_WR_REG_SIG_MR
        Signature handover work request. This WR will define the signature
        handover properties of the memory/wire domains as well as the
        domains layout. The purpose of this work request is to bind all
        the needed information for the signature operation:
    
        - data to be transferred:  wr->sg_list (ib_sge).
          * The raw data, pre-registered to a single MR (normally, before
            signature, this MR would have been used directly for the data
            transfer)
        - data protection guards: sig_handover.prot (ib_sge).
          * The data protection buffer, pre-registered to a single MR, which
            contains the data integrity guards of the raw data blocks.
            Note that it may not always exist, only in cases where the user is
            interested in storing protection guards in memory.
        - signature operation attributes: sig_handover.sig_attrs.
          * Tells the HCA how to validate/generate the protection information.
    
        Once the work request is executed, the memory region that will
        describe the signature transaction will be the sig_mr.  The
        application can now go ahead and send the sig_mr.rkey or use the
        sig_mr.lkey for data transfer.
    
    4. New Verb ib_check_mr_status
        check_mr_status verb checks the status of the memory region post
        transaction.  The first check that may be used is
        IB_MR_CHECK_SIG_STATUS, which will indicate if any signature
        errors are pending for a specific signature-enabled ib_mr.  This
        verb is a lightwight check and is allowed to be taken from
        interrupt context.  An application must call this verb after it is
        known that the actual data transfer has finished.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cb12e6a4e553..82ab5c1e7605 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -122,7 +122,19 @@ enum ib_device_cap_flags {
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
 	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24),
-	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29)
+	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29),
+	IB_DEVICE_SIGNATURE_HANDOVER	= (1<<30)
+};
+
+enum ib_signature_prot_cap {
+	IB_PROT_T10DIF_TYPE_1 = 1,
+	IB_PROT_T10DIF_TYPE_2 = 1 << 1,
+	IB_PROT_T10DIF_TYPE_3 = 1 << 2,
+};
+
+enum ib_signature_guard_cap {
+	IB_GUARD_T10DIF_CRC	= 1,
+	IB_GUARD_T10DIF_CSUM	= 1 << 1,
 };
 
 enum ib_atomic_cap {
@@ -172,6 +184,8 @@ struct ib_device_attr {
 	unsigned int		max_fast_reg_page_list_len;
 	u16			max_pkeys;
 	u8			local_ca_ack_delay;
+	int			sig_prot_cap;
+	int			sig_guard_cap;
 };
 
 enum ib_mtu {
@@ -477,6 +491,114 @@ struct ib_mr_init_attr {
 	u32	    flags;
 };
 
+enum ib_signature_type {
+	IB_SIG_TYPE_T10_DIF,
+};
+
+/**
+ * T10-DIF Signature types
+ * T10-DIF types are defined by SCSI
+ * specifications.
+ */
+enum ib_t10_dif_type {
+	IB_T10DIF_NONE,
+	IB_T10DIF_TYPE1,
+	IB_T10DIF_TYPE2,
+	IB_T10DIF_TYPE3
+};
+
+/**
+ * Signature T10-DIF block-guard types
+ * IB_T10DIF_CRC: Corresponds to T10-PI mandated CRC checksum rules.
+ * IB_T10DIF_CSUM: Corresponds to IP checksum rules.
+ */
+enum ib_t10_dif_bg_type {
+	IB_T10DIF_CRC,
+	IB_T10DIF_CSUM
+};
+
+/**
+ * struct ib_t10_dif_domain - Parameters specific for T10-DIF
+ *     domain.
+ * @type: T10-DIF type (0|1|2|3)
+ * @bg_type: T10-DIF block guard type (CRC|CSUM)
+ * @pi_interval: protection information interval.
+ * @bg: seed of guard computation.
+ * @app_tag: application tag of guard block
+ * @ref_tag: initial guard block reference tag.
+ * @type3_inc_reftag: T10-DIF type 3 does not state
+ *     about the reference tag, it is the user
+ *     choice to increment it or not.
+ */
+struct ib_t10_dif_domain {
+	enum ib_t10_dif_type	type;
+	enum ib_t10_dif_bg_type bg_type;
+	u16			pi_interval;
+	u16			bg;
+	u16			app_tag;
+	u32			ref_tag;
+	bool			type3_inc_reftag;
+};
+
+/**
+ * struct ib_sig_domain - Parameters for signature domain
+ * @sig_type: specific signauture type
+ * @sig: union of all signature domain attributes that may
+ *     be used to set domain layout.
+ */
+struct ib_sig_domain {
+	enum ib_signature_type sig_type;
+	union {
+		struct ib_t10_dif_domain dif;
+	} sig;
+};
+
+/**
+ * struct ib_sig_attrs - Parameters for signature handover operation
+ * @check_mask: bitmask for signature byte check (8 bytes)
+ * @mem: memory domain layout desciptor.
+ * @wire: wire domain layout desciptor.
+ */
+struct ib_sig_attrs {
+	u8			check_mask;
+	struct ib_sig_domain	mem;
+	struct ib_sig_domain	wire;
+};
+
+enum ib_sig_err_type {
+	IB_SIG_BAD_GUARD,
+	IB_SIG_BAD_REFTAG,
+	IB_SIG_BAD_APPTAG,
+};
+
+/**
+ * struct ib_sig_err - signature error descriptor
+ */
+struct ib_sig_err {
+	enum ib_sig_err_type	err_type;
+	u32			expected;
+	u32			actual;
+	u64			sig_err_offset;
+	u32			key;
+};
+
+enum ib_mr_status_check {
+	IB_MR_CHECK_SIG_STATUS = 1,
+};
+
+/**
+ * struct ib_mr_status - Memory region status container
+ *
+ * @fail_status: Bitmask of MR checks status. For each
+ *     failed check a corresponding status bit is set.
+ * @sig_err: Additional info for IB_MR_CEHCK_SIG_STATUS
+ *     failure.
+ */
+struct ib_mr_status {
+	u32		    fail_status;
+	struct ib_sig_err   sig_err;
+};
+
 /**
  * mult_to_ib_rate - Convert a multiple of 2.5 Gbit/sec to an IB rate
  * enum.
@@ -660,6 +782,7 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
 	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
 	IB_QP_CREATE_NETIF_QP			= 1 << 5,
+	IB_QP_CREATE_SIGNATURE_EN		= 1 << 6,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
@@ -824,6 +947,7 @@ enum ib_wr_opcode {
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
 	IB_WR_BIND_MW,
+	IB_WR_REG_SIG_MR,
 	/* reserve values for low level drivers' internal use.
 	 * These values will not be used at all in the ib core layer.
 	 */
@@ -929,6 +1053,12 @@ struct ib_send_wr {
 			u32                      rkey;
 			struct ib_mw_bind_info   bind_info;
 		} bind_mw;
+		struct {
+			struct ib_sig_attrs    *sig_attrs;
+			struct ib_mr	       *sig_mr;
+			int			access_flags;
+			struct ib_sge	       *prot;
+		} sig_handover;
 	} wr;
 	u32			xrc_remote_srq_num;	/* XRC TGT QPs only */
 };
@@ -1474,6 +1604,8 @@ struct ib_device {
 						  *flow_attr,
 						  int domain);
 	int			   (*destroy_flow)(struct ib_flow *flow_id);
+	int			   (*check_mr_status)(struct ib_mr *mr, u32 check_mask,
+						      struct ib_mr_status *mr_status);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 
@@ -2473,4 +2605,19 @@ static inline int ib_check_mr_access(int flags)
 	return 0;
 }
 
+/**
+ * ib_check_mr_status: lightweight check of MR status.
+ *     This routine may provide status checks on a selected
+ *     ib_mr. first use is for signature status check.
+ *
+ * @mr: A memory region.
+ * @check_mask: Bitmask of which checks to perform from
+ *     ib_mr_status_check enumeration.
+ * @mr_status: The container of relevant status checks.
+ *     failed checks will be indicated in the status bitmask
+ *     and the relevant info shall be in the error item.
+ */
+int ib_check_mr_status(struct ib_mr *mr, u32 check_mask,
+		       struct ib_mr_status *mr_status);
+
 #endif /* IB_VERBS_H */

commit 17cd3a2db825506c3e3bb9548ad20f67e2f8d0e7
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Sun Feb 23 14:19:04 2014 +0200

    IB/core: Introduce protected memory regions
    
    This commit introduces verbs for creating/destoying memory
    regions which will allow new types of memory key operations such
    as protected memory registration.
    
    Indirect memory registration is registering several (one
    of more) pre-registered memory regions in a specific layout.
    The Indirect region may potentialy describe several regions
    and some repitition format between them.
    
    Protected Memory registration is registering a memory region
    with various data integrity attributes which will describe protection
    schemes that will be handled by the HCA in an offloaded manner.
    These memory regions will be applicable for a new REG_SIG_MR
    work request introduced later in this patchset.
    
    In the future these routines may replace or implement current memory
    regions creation routines existing today:
    - ib_reg_user_mr
    - ib_alloc_fast_reg_mr
    - ib_get_dma_mr
    - ib_dereg_mr
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6793f32ccb58..cb12e6a4e553 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -461,6 +461,22 @@ int ib_rate_to_mult(enum ib_rate rate) __attribute_const__;
  */
 int ib_rate_to_mbps(enum ib_rate rate) __attribute_const__;
 
+enum ib_mr_create_flags {
+	IB_MR_SIGNATURE_EN = 1,
+};
+
+/**
+ * ib_mr_init_attr - Memory region init attributes passed to routine
+ *     ib_create_mr.
+ * @max_reg_descriptors: max number of registration descriptors that
+ *     may be used with registration work requests.
+ * @flags: MR creation flags bit mask.
+ */
+struct ib_mr_init_attr {
+	int	    max_reg_descriptors;
+	u32	    flags;
+};
+
 /**
  * mult_to_ib_rate - Convert a multiple of 2.5 Gbit/sec to an IB rate
  * enum.
@@ -1407,6 +1423,9 @@ struct ib_device {
 	int                        (*query_mr)(struct ib_mr *mr,
 					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);
+	int                        (*destroy_mr)(struct ib_mr *mr);
+	struct ib_mr *		   (*create_mr)(struct ib_pd *pd,
+						struct ib_mr_init_attr *mr_init_attr);
 	struct ib_mr *		   (*alloc_fast_reg_mr)(struct ib_pd *pd,
 					       int max_page_list_len);
 	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
@@ -2250,6 +2269,25 @@ int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
  */
 int ib_dereg_mr(struct ib_mr *mr);
 
+
+/**
+ * ib_create_mr - Allocates a memory region that may be used for
+ *     signature handover operations.
+ * @pd: The protection domain associated with the region.
+ * @mr_init_attr: memory region init attributes.
+ */
+struct ib_mr *ib_create_mr(struct ib_pd *pd,
+			   struct ib_mr_init_attr *mr_init_attr);
+
+/**
+ * ib_destroy_mr - Destroys a memory region that was created using
+ *     ib_create_mr and removes it from HW translation tables.
+ * @mr: The memory region to destroy.
+ *
+ * This function can fail, if the memory region has memory windows bound to it.
+ */
+int ib_destroy_mr(struct ib_mr *mr);
+
 /**
  * ib_alloc_fast_reg_mr - Allocates memory region usable with the
  *   IB_WR_FAST_REG_MR send work request.

commit b4a26a27287a7f81933ba016aeed6c69dd155323
Author: Moni Shoua <monis@mellanox.co.il>
Date:   Sun Feb 9 11:54:34 2014 +0200

    IB: Report using RoCE IP based gids in port caps
    
    For userspace RoCE UD QPs we need to know the GID format that the
    kernel uses, e.g when working over older kernels. For that end, add a
    new port capability IB_PORT_IP_BASED_GIDS and report it when query
    port is issued.
    
    Signed-off-by: Moni Shoua <monis@mellanox.co.il>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8d4a1c06f7e4..6793f32ccb58 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -226,7 +226,8 @@ enum ib_port_cap_flags {
 	IB_PORT_CAP_MASK_NOTICE_SUP		= 1 << 22,
 	IB_PORT_BOOT_MGMT_SUP			= 1 << 23,
 	IB_PORT_LINK_LATENCY_SUP		= 1 << 24,
-	IB_PORT_CLIENT_REG_SUP			= 1 << 25
+	IB_PORT_CLIENT_REG_SUP			= 1 << 25,
+	IB_PORT_IP_BASED_GIDS			= 1 << 26
 };
 
 enum ib_port_width {

commit fb1b5034e4987b158179a62732fb6dfb8f7ec88e
Merge: 8f399921ea9a 27cdef637c25
Author: Roland Dreier <roland@purestorage.com>
Date:   Wed Jan 22 23:24:21 2014 -0800

    Merge branch 'ip-roce' into for-next
    
    Conflicts:
            drivers/infiniband/hw/mlx4/main.c

commit 8f399921ea9a562bc8221258c4b8a7bd69577939
Merge: 5462eddd7a78 298589b1cb62 c1c98501121e 437708c44395 79adc5321e4d af2e2e35a23e 57761d8df8ef be8348df6efa 6e0ea9e6cbce 0c7f82189d22 a384b20e417a
Author: Roland Dreier <roland@purestorage.com>
Date:   Wed Jan 22 23:24:13 2014 -0800

    Merge branches 'cma', 'cxgb4', 'flowsteer', 'ipoib', 'misc', 'mlx4', 'mlx5', 'ocrdma', 'qib', 'srp' and 'usnic' into for-next

commit 5db5765e255de4072eb0e35facfeafce53af001b
Author: Upinder Malhi <umalhi@cisco.com>
Date:   Wed Jan 15 17:02:36 2014 -0800

    IB/core: Add support for RDMA_NODE_USNIC_UDP
    
    Add the complementary RDMA_NODE_USNIC_UDP for RDMA_TRANSPORT_USNIC_UDP.
    
    Signed-off-by: Upinder Malhi <umalhi@cisco.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index b19aa7285ea3..688511ed52f5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -69,6 +69,7 @@ enum rdma_node_type {
 	RDMA_NODE_IB_ROUTER,
 	RDMA_NODE_RNIC,
 	RDMA_NODE_USNIC,
+	RDMA_NODE_USNIC_UDP,
 };
 
 enum rdma_transport_type {

commit dd5f03beb4f76ae65d76d8c22a8815e424fc607c
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Dec 12 18:03:11 2013 +0200

    IB/core: Ethernet L2 attributes in verbs/cm structures
    
    This patch add the support for Ethernet L2 attributes in the
    verbs/cm/cma structures.
    
    When dealing with L2 Ethernet, we should use smac, dmac, vlan ID and priority
    in a similar manner that the IB L2 (and the L4 PKEY) attributes are used.
    
    Thus, those attributes were added to the following structures:
    
    * ib_ah_attr - added dmac
    * ib_qp_attr - added smac and vlan_id, (sl remains vlan priority)
    * ib_wc - added smac, vlan_id
    * ib_sa_path_rec - added smac, dmac, vlan_id
    * cm_av - added smac and vlan_id
    
    For the path record structure, extra care was taken to avoid the new
    fields when packing it into wire format, so we don't break the IB CM
    and SA wire protocol.
    
    On the active side, the CM fills. its internal structures from the
    path provided by the ULP.  We add there taking the ETH L2 attributes
    and placing them into the CM Address Handle (struct cm_av).
    
    On the passive side, the CM fills its internal structures from the WC
    associated with the REQ message.  We add there taking the ETH L2
    attributes from the WC.
    
    When the HW driver provides the required ETH L2 attributes in the WC,
    they set the IB_WC_WITH_SMAC and IB_WC_WITH_VLAN flags. The IB core
    code checks for the presence of these flags, and in their absence does
    address resolution from the ib_init_ah_from_wc() helper function.
    
    ib_modify_qp_is_ok is also updated to consider the link layer. Some
    parameters are mandatory for Ethernet link layer, while they are
    irrelevant for IB.  Vendor drivers are modified to support the new
    function signature.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 61e1935c91b1..ea0f6eed7863 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -48,6 +48,7 @@
 #include <linux/rwsem.h>
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
+#include <uapi/linux/if_ether.h>
 
 #include <linux/atomic.h>
 #include <asm/uaccess.h>
@@ -472,6 +473,8 @@ struct ib_ah_attr {
 	u8			static_rate;
 	u8			ah_flags;
 	u8			port_num;
+	u8			dmac[ETH_ALEN];
+	u16			vlan_id;
 };
 
 enum ib_wc_status {
@@ -524,6 +527,8 @@ enum ib_wc_flags {
 	IB_WC_WITH_IMM		= (1<<1),
 	IB_WC_WITH_INVALIDATE	= (1<<2),
 	IB_WC_IP_CSUM_OK	= (1<<3),
+	IB_WC_WITH_SMAC		= (1<<4),
+	IB_WC_WITH_VLAN		= (1<<5),
 };
 
 struct ib_wc {
@@ -544,6 +549,8 @@ struct ib_wc {
 	u8			sl;
 	u8			dlid_path_bits;
 	u8			port_num;	/* valid only for DR SMPs on switches */
+	u8			smac[ETH_ALEN];
+	u16			vlan_id;
 };
 
 enum ib_cq_notify_flags {
@@ -721,7 +728,11 @@ enum ib_qp_attr_mask {
 	IB_QP_MAX_DEST_RD_ATOMIC	= (1<<17),
 	IB_QP_PATH_MIG_STATE		= (1<<18),
 	IB_QP_CAP			= (1<<19),
-	IB_QP_DEST_QPN			= (1<<20)
+	IB_QP_DEST_QPN			= (1<<20),
+	IB_QP_SMAC			= (1<<21),
+	IB_QP_ALT_SMAC			= (1<<22),
+	IB_QP_VID			= (1<<23),
+	IB_QP_ALT_VID			= (1<<24),
 };
 
 enum ib_qp_state {
@@ -771,6 +782,10 @@ struct ib_qp_attr {
 	u8			rnr_retry;
 	u8			alt_port_num;
 	u8			alt_timeout;
+	u8			smac[ETH_ALEN];
+	u8			alt_smac[ETH_ALEN];
+	u16			vlan_id;
+	u16			alt_vlan_id;
 };
 
 enum ib_wr_opcode {
@@ -1488,6 +1503,7 @@ static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len
  * @next_state: Next QP state
  * @type: QP type
  * @mask: Mask of supplied QP attributes
+ * @ll : link layer of port
  *
  * This function is a helper function that a low-level driver's
  * modify_qp method can use to validate the consumer's input.  It
@@ -1496,7 +1512,8 @@ static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len
  * and that the attribute mask supplied is allowed for the transition.
  */
 int ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
-		       enum ib_qp_type type, enum ib_qp_attr_mask mask);
+		       enum ib_qp_type type, enum ib_qp_attr_mask mask,
+		       enum rdma_link_layer ll);
 
 int ib_register_event_handler  (struct ib_event_handler *event_handler);
 int ib_unregister_event_handler(struct ib_event_handler *event_handler);

commit 240ae00e4d834e387f4f09e236130f520e357a70
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Nov 7 15:25:13 2013 +0200

    IB/core: Add support for IB L2 device-managed steering
    
    This patch adds preliminary support for IB L2 device-managed steering,
    currently exposed only in the kernel.
    
    This flow spec can be used by low-level drivers that need to indicate
    the link layer type when creating device-managed flow rules.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d4d16394455f..aa24760f4bc2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1100,13 +1100,14 @@ enum ib_flow_attr_type {
 enum ib_flow_spec_type {
 	/* L2 headers*/
 	IB_FLOW_SPEC_ETH	= 0x20,
+	IB_FLOW_SPEC_IB		= 0x22,
 	/* L3 header*/
 	IB_FLOW_SPEC_IPV4	= 0x30,
 	/* L4 headers*/
 	IB_FLOW_SPEC_TCP	= 0x40,
 	IB_FLOW_SPEC_UDP	= 0x41
 };
-
+#define IB_FLOW_SPEC_LAYER_MASK	0xF0
 #define IB_FLOW_SPEC_SUPPORT_LAYERS 4
 
 /* Flow steering rule priority is set according to it's domain.
@@ -1134,6 +1135,18 @@ struct ib_flow_spec_eth {
 	struct ib_flow_eth_filter mask;
 };
 
+struct ib_flow_ib_filter {
+	__be16 dlid;
+	__u8   sl;
+};
+
+struct ib_flow_spec_ib {
+	enum ib_flow_spec_type	 type;
+	u16			 size;
+	struct ib_flow_ib_filter val;
+	struct ib_flow_ib_filter mask;
+};
+
 struct ib_flow_ipv4_filter {
 	__be32	src_ip;
 	__be32	dst_ip;
@@ -1164,6 +1177,7 @@ union ib_flow_spec {
 		u16			size;
 	};
 	struct ib_flow_spec_eth		eth;
+	struct ib_flow_spec_ib		ib;
 	struct ib_flow_spec_ipv4        ipv4;
 	struct ib_flow_spec_tcp_udp	tcp_udp;
 };

commit 90f1d1b41b70474bf73d07d4300196901cd81718
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Nov 7 15:25:12 2013 +0200

    IB/core: Add flow steering support for IPoIB UD traffic
    
    When creating an IPoIB UD QP, provide a hint to the low level driver
    that the QP should support flow-steering.  This means that privileged
    user space applications can steer TCP/IP IPoIB traffic from the
    network stack, in a similar manner done with Ethernet RAW_PACKET QPs.
    
    The hint is provided through new QP creation flag called NETIF_QP.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 61e1935c91b1..d4d16394455f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -633,6 +633,7 @@ enum ib_qp_type {
 enum ib_qp_create_flags {
 	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
 	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
+	IB_QP_CREATE_NETIF_QP			= 1 << 5,
 	/* reserve bits 26-31 for low level drivers' internal use */
 	IB_QP_CREATE_RESERVED_START		= 1 << 26,
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,

commit 248567f79304b953ea492fb92ade097b62ed09b2
Author: Upinder Malhi <umalhi@cisco.com>
Date:   Thu Jan 9 14:48:19 2014 -0800

    IB/core: Add RDMA_TRANSPORT_USNIC_UDP
    
    Add RDMA_TRANSPORT_USNIC_UDP which will be used by usNIC.
    
    Signed-off-by: Upinder Malhi <umalhi@cisco.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 61e1935c91b1..b19aa7285ea3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -74,7 +74,8 @@ enum rdma_node_type {
 enum rdma_transport_type {
 	RDMA_TRANSPORT_IB,
 	RDMA_TRANSPORT_IWARP,
-	RDMA_TRANSPORT_USNIC
+	RDMA_TRANSPORT_USNIC,
+	RDMA_TRANSPORT_USNIC_UDP
 };
 
 enum rdma_transport_type

commit 309243ec14fde1149e1c66f19746e239e86caf39
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Wed Dec 11 23:01:44 2013 +0100

    IB/core: const'ify inbuf in struct ib_udata
    
    Userspace input buffer is not modified by kernel, so it can be 'const'.
    
    This is also a prerequisite to remove the implicit cast
    from INIT_UDATA().
    
    Link: http://marc.info/?i=cover.1386798254.git.ydroneaud@opteya.com>
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 979874c627ee..61e1935c91b1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -978,7 +978,7 @@ struct ib_uobject {
 };
 
 struct ib_udata {
-	void __user *inbuf;
+	const void __user *inbuf;
 	void __user *outbuf;
 	size_t       inlen;
 	size_t       outlen;

commit b4fdf52b3fc8dd3ce13ece334f5fdff869705429
Merge: 352b9056352f 649fb5ec0e52 69ad5da41b4e 7f1a38671c55 f3a5e3e37e22 93b80ac297b1 cf1c5e1f1c96 4127c365c9db d5e3f3783375 2fadd83184d5 cd4e38542a5c
Author: Roland Dreier <roland@purestorage.com>
Date:   Sun Nov 17 08:22:19 2013 -0800

    Merge branches 'cma', 'cxgb4', 'flowsteer', 'ipoib', 'misc', 'mlx4', 'mlx5', 'nes', 'ocrdma', 'qib' and 'srp' into for-next

commit f21519b23c1b6fa25366be4114ccf7fcf1c190f9
Author: Yann Droneaud <ydroneaud@opteya.com>
Date:   Wed Nov 6 23:21:49 2013 +0100

    IB/core: extended command: an improved infrastructure for uverbs commands
    
    Commit 400dbc96583f ("IB/core: Infrastructure for extensible uverbs
    commands") added an infrastructure for extensible uverbs commands
    while later commit 436f2ad05a0b ("IB/core: Export ib_create/destroy_flow
    through uverbs") exported ib_create_flow()/ib_destroy_flow() functions
    using this new infrastructure.
    
    According to the commit 400dbc96583f, the purpose of this
    infrastructure is to support passing around provider (eg. hardware)
    specific buffers when userspace issue commands to the kernel, so that
    it would be possible to extend uverbs (eg. core) buffers independently
    from the provider buffers.
    
    But the new kernel command function prototypes were not modified to
    take advantage of this extension. This issue was exposed by Roland
    Dreier in a previous review[1].
    
    So the following patch is an attempt to a revised extensible command
    infrastructure.
    
    This improved extensible command infrastructure distinguish between
    core (eg. legacy)'s command/response buffers from provider
    (eg. hardware)'s command/response buffers: each extended command
    implementing function is given a struct ib_udata to hold core
    (eg. uverbs) input and output buffers, and another struct ib_udata to
    hold the hw (eg. provider) input and output buffers.
    
    Having those buffers identified separately make it easier to increase
    one buffer to support extension without having to add some code to
    guess the exact size of each command/response parts: This should make
    the extended functions more reliable.
    
    Additionally, instead of relying on command identifier being greater
    than IB_USER_VERBS_CMD_THRESHOLD, the proposed infrastructure rely on
    unused bits in command field: on the 32 bits provided by command
    field, only 6 bits are really needed to encode the identifier of
    commands currently supported by the kernel. (Even using only 6 bits
    leaves room for about 23 new commands).
    
    So this patch makes use of some high order bits in command field to
    store flags, leaving enough room for more command identifiers than one
    will ever need (eg. 256).
    
    The new flags are used to specify if the command should be processed
    as an extended one or a legacy one. While designing the new command
    format, care was taken to make usage of flags itself extensible.
    
    Using high order bits of the commands field ensure that newer
    libibverbs on older kernel will properly fail when trying to call
    extended commands. On the other hand, older libibverbs on newer kernel
    will never be able to issue calls to extended commands.
    
    The extended command header includes the optional response pointer so
    that output buffer length and output buffer pointer are located
    together in the command, allowing proper parameters checking. This
    should make implementing functions easier and safer.
    
    Additionally the extended header ensure 64bits alignment, while making
    all sizes multiple of 8 bytes, extending the maximum buffer size:
    
                                 legacy      extended
    
       Maximum command buffer:  256KBytes   1024KBytes (512KBytes + 512KBytes)
      Maximum response buffer:  256KBytes   1024KBytes (512KBytes + 512KBytes)
    
    For the purpose of doing proper buffer size accounting, the headers
    size are no more taken in account in "in_words".
    
    One of the odds of the current extensible infrastructure, reading
    twice the "legacy" command header, is fixed by removing the "legacy"
    command header from the extended command header: they are processed as
    two different parts of the command: memory is read once and
    information are not duplicated: it's making clear that's an extended
    command scheme and not a different command scheme.
    
    The proposed scheme will format input (command) and output (response)
    buffers this way:
    
    - command:
    
      legacy header +
      extended header +
      command data (core + hw):
    
        +----------------------------------------+
        | flags     |   00      00    |  command |
        |        in_words    |   out_words       |
        +----------------------------------------+
        |                 response               |
        |                 response               |
        | provider_in_words | provider_out_words |
        |                 padding                |
        +----------------------------------------+
        |                                        |
        .              <uverbs input>            .
        .              (in_words * 8)            .
        |                                        |
        +----------------------------------------+
        |                                        |
        .             <provider input>           .
        .          (provider_in_words * 8)       .
        |                                        |
        +----------------------------------------+
    
    - response, if present:
    
        +----------------------------------------+
        |                                        |
        .          <uverbs output space>         .
        .             (out_words * 8)            .
        |                                        |
        +----------------------------------------+
        |                                        |
        .         <provider output space>        .
        .         (provider_out_words * 8)       .
        |                                        |
        +----------------------------------------+
    
    The overall design is to ensure that the extensible infrastructure is
    itself extensible while begin more reliable with more input and bound
    checking.
    
    Note:
    
    The unused field in the extended header would be perfect candidate to
    hold the command "comp_mask" (eg. bit field used to handle
    compatibility).  This was suggested by Roland Dreier in a previous
    review[2].  But "comp_mask" field is likely to be present in the uverb
    input and/or provider input, likewise for the response, as noted by
    Matan Barak[3], so it doesn't make sense to put "comp_mask" in the
    header.
    
    [1]:
    http://marc.info/?i=CAL1RGDWxmM17W2o_era24A-TTDeKyoL6u3NRu_=t_dhV_ZA9MA@mail.gmail.com
    
    [2]:
    http://marc.info/?i=CAL1RGDXJtrc849M6_XNZT5xO1+ybKtLWGq6yg6LhoSsKpsmkYA@mail.gmail.com
    
    [3]:
    http://marc.info/?i=525C1149.6000701@mellanox.com
    
    Signed-off-by: Yann Droneaud <ydroneaud@opteya.com>
    Link: http://marc.info/?i=cover.1383773832.git.ydroneaud@opteya.com
    
    [ Convert "ret ? ret : 0" to the equivalent "ret".  - Roland ]
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e393171e2fac..a06fc122f803 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1436,6 +1436,7 @@ struct ib_device {
 
 	int			     uverbs_abi_ver;
 	u64			     uverbs_cmd_mask;
+	u64			     uverbs_ex_cmd_mask;
 
 	char			     node_desc[64];
 	__be64			     node_guid;

commit 1c636f801615bdfc9b1d46904e8258c7a025670b
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Thu Oct 31 15:26:32 2013 +0200

    IB/core: Encorce MR access rights rules on kernel consumers
    
    Enforce the rule that when requesting remote write or atomic permissions, local
    write must be indicated as well. See IB spec 11.2.8.2.
    
    Spotted by: Hagay Abramovsky <hagaya@mellanox.com>
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 60354d53948e..68c053d0d629 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2386,4 +2386,17 @@ struct ib_flow *ib_create_flow(struct ib_qp *qp,
 			       struct ib_flow_attr *flow_attr, int domain);
 int ib_destroy_flow(struct ib_flow *flow_id);
 
+static inline int ib_check_mr_access(int flags)
+{
+	/*
+	 * Local write permission is required if remote write or
+	 * remote atomic permission is also requested.
+	 */
+	if (flags & (IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_REMOTE_WRITE) &&
+	    !(flags & IB_ACCESS_LOCAL_WRITE))
+		return -EINVAL;
+
+	return 0;
+}
+
 #endif /* IB_VERBS_H */

commit 180771a3707a4c0577cbf4f830c754dbabfdfccb
Author: Upinder Malhi \(umalhi\) <umalhi@cisco.com>
Date:   Tue Sep 10 03:36:59 2013 +0000

    IB/core: Add Cisco usNIC rdma node and transport types
    
    This patch adds new rdma node and new rdma transport, and supporting
    code used by Cisco's low latency driver called usNIC.  usNIC uses its
    own transport, distinct from IB and iWARP.
    
    Signed-off-by: Upinder Malhi <umalhi@cisco.com>
    Signed-off-by: Jeff Squyres <jsquyres@cisco.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e393171e2fac..60354d53948e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -67,12 +67,14 @@ enum rdma_node_type {
 	RDMA_NODE_IB_CA 	= 1,
 	RDMA_NODE_IB_SWITCH,
 	RDMA_NODE_IB_ROUTER,
-	RDMA_NODE_RNIC
+	RDMA_NODE_RNIC,
+	RDMA_NODE_USNIC,
 };
 
 enum rdma_transport_type {
 	RDMA_TRANSPORT_IB,
-	RDMA_TRANSPORT_IWARP
+	RDMA_TRANSPORT_IWARP,
+	RDMA_TRANSPORT_USNIC
 };
 
 enum rdma_transport_type

commit 82af24ac6fc3f4910218419a0ca4f05d42b45c67
Merge: 09992579bc8c 22878dbc9173 49b8e7443806 2e02d653febf 846be90d810c 33ccbd858f3a 0318f685213e
Author: Roland Dreier <roland@purestorage.com>
Date:   Tue Sep 3 09:01:08 2013 -0700

    Merge branches 'cxgb4', 'flowsteer', 'ipoib', 'iser', 'mlx4', 'ocrdma' and 'qib' into for-next

commit 22878dbc9173a7f0322dd697b1b5b49a83a1d4d5
Author: Matan Barak <matanb@mellanox.com>
Date:   Sun Sep 1 18:39:52 2013 +0300

    IB/core: Better checking of userspace values for receive flow steering
    
      - Don't allow unsupported comp_mask values, user should check
        ibv_query_device to know which features are supported.
      - Add a check in ib_uverbs_create_flow() to verify the size passed
        from the user space.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 274205d4df97..bd151eac1e61 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1098,6 +1098,8 @@ enum ib_flow_spec_type {
 	IB_FLOW_SPEC_UDP	= 0x41
 };
 
+#define IB_FLOW_SPEC_SUPPORT_LAYERS 4
+
 /* Flow steering rule priority is set according to it's domain.
  * Lower domain value means higher priority.
  */

commit 436f2ad05a0b65b1467ddf51bc68171c381bf844
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Wed Aug 14 13:58:30 2013 +0300

    IB/core: Export ib_create/destroy_flow through uverbs
    
    Implement ib_uverbs_create_flow() and ib_uverbs_destroy_flow() to
    support flow steering for user space applications.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6f874b00491a..274205d4df97 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -954,6 +954,7 @@ struct ib_ucontext {
 	struct list_head	srq_list;
 	struct list_head	ah_list;
 	struct list_head	xrcd_list;
+	struct list_head	rule_list;
 	int			closing;
 };
 

commit 319a441d1361ea703b091caf92418f8121eadfc5
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Wed Aug 7 14:01:59 2013 +0300

    IB/core: Add receive flow steering support
    
    The RDMA stack allows for applications to create IB_QPT_RAW_PACKET
    QPs, which receive plain Ethernet packets, specifically packets that
    don't carry any QPN to be matched by the receiving side.  Applications
    using these QPs must be provided with a method to program some
    steering rule with the HW so packets arriving at the local port can be
    routed to them.
    
    This patch adds ib_create_flow(), which allow providing a flow
    specification for a QP.  When there's a match between the
    specification and a received packet, the packet is forwarded to that
    QP, in a the same way one uses ib_attach_multicast() for IB UD
    multicast handling.
    
    Flow specifications are provided as instances of struct ib_flow_spec_yyy,
    which describe L2, L3 and L4 headers.  Currently specs for Ethernet, IPv4,
    TCP and UDP are defined.  Flow specs are made of values and masks.
    
    The input to ib_create_flow() is a struct ib_flow_attr, which contains
    a few mandatory control elements and optional flow specs.
    
        struct ib_flow_attr {
                enum ib_flow_attr_type type;
                u16      size;
                u16      priority;
                u32      flags;
                u8       num_of_specs;
                u8       port;
                /* Following are the optional layers according to user request
                 * struct ib_flow_spec_yyy
                 * struct ib_flow_spec_zzz
                 */
        };
    
    As these specs are eventually coming from user space, they are defined and
    used in a way which allows adding new spec types without kernel/user ABI
    change, just with a little API enhancement which defines the newly added spec.
    
    The flow spec structures are defined with TLV (Type-Length-Value)
    entries, which allows calling ib_create_flow() with a list of variable
    length of optional specs.
    
    For the actual processing of ib_flow_attr the driver uses the number
    of specs and the size mandatory fields along with the TLV nature of
    the specs.
    
    Steering rules processing order is according to the domain over which
    the rule is set and the rule priority.  All rules set by user space
    applicatations fall into the IB_FLOW_DOMAIN_USER domain, other domains
    could be used by future IPoIB RFS and Ethetool flow-steering interface
    implementation.  Lower numerical value for the priority field means
    higher priority.
    
    The returned value from ib_create_flow() is a struct ib_flow, which
    contains a database pointer (handle) provided by the HW driver to be
    used when calling ib_destroy_flow().
    
    Applications that offload TCP/IP traffic can also be written over IB
    UD QPs.  The ib_create_flow() / ib_destroy_flow() API is designed to
    support UD QPs too.  A HW driver can set IB_DEVICE_MANAGED_FLOW_STEERING
    to denote support for flow steering.
    
    The ib_flow_attr enum type supports usage of flow steering for promiscuous
    and sniffer purposes:
    
        IB_FLOW_ATTR_NORMAL - "regular" rule, steering according to rule specification
    
        IB_FLOW_ATTR_ALL_DEFAULT - default unicast and multicast rule, receive
            all Ethernet traffic which isn't steered to any QP
    
        IB_FLOW_ATTR_MC_DEFAULT - same as IB_FLOW_ATTR_ALL_DEFAULT but only for multicast
    
        IB_FLOW_ATTR_SNIFFER - sniffer rule, receive all port traffic
    
    ALL_DEFAULT and MC_DEFAULT rules options are valid only for Ethernet link type.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 645c3cedce9c..6f874b00491a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -116,7 +116,8 @@ enum ib_device_cap_flags {
 	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
-	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24)
+	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24),
+	IB_DEVICE_MANAGED_FLOW_STEERING = (1<<29)
 };
 
 enum ib_atomic_cap {
@@ -1033,7 +1034,8 @@ struct ib_qp {
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;
-	atomic_t		usecnt; /* count times opened, mcast attaches */
+	/* count times opened, mcast attaches, flow attaches */
+	atomic_t		usecnt;
 	struct list_head	open_list;
 	struct ib_qp           *real_qp;
 	struct ib_uobject      *uobject;
@@ -1068,6 +1070,110 @@ struct ib_fmr {
 	u32			rkey;
 };
 
+/* Supported steering options */
+enum ib_flow_attr_type {
+	/* steering according to rule specifications */
+	IB_FLOW_ATTR_NORMAL		= 0x0,
+	/* default unicast and multicast rule -
+	 * receive all Eth traffic which isn't steered to any QP
+	 */
+	IB_FLOW_ATTR_ALL_DEFAULT	= 0x1,
+	/* default multicast rule -
+	 * receive all Eth multicast traffic which isn't steered to any QP
+	 */
+	IB_FLOW_ATTR_MC_DEFAULT		= 0x2,
+	/* sniffer rule - receive all port traffic */
+	IB_FLOW_ATTR_SNIFFER		= 0x3
+};
+
+/* Supported steering header types */
+enum ib_flow_spec_type {
+	/* L2 headers*/
+	IB_FLOW_SPEC_ETH	= 0x20,
+	/* L3 header*/
+	IB_FLOW_SPEC_IPV4	= 0x30,
+	/* L4 headers*/
+	IB_FLOW_SPEC_TCP	= 0x40,
+	IB_FLOW_SPEC_UDP	= 0x41
+};
+
+/* Flow steering rule priority is set according to it's domain.
+ * Lower domain value means higher priority.
+ */
+enum ib_flow_domain {
+	IB_FLOW_DOMAIN_USER,
+	IB_FLOW_DOMAIN_ETHTOOL,
+	IB_FLOW_DOMAIN_RFS,
+	IB_FLOW_DOMAIN_NIC,
+	IB_FLOW_DOMAIN_NUM /* Must be last */
+};
+
+struct ib_flow_eth_filter {
+	u8	dst_mac[6];
+	u8	src_mac[6];
+	__be16	ether_type;
+	__be16	vlan_tag;
+};
+
+struct ib_flow_spec_eth {
+	enum ib_flow_spec_type	  type;
+	u16			  size;
+	struct ib_flow_eth_filter val;
+	struct ib_flow_eth_filter mask;
+};
+
+struct ib_flow_ipv4_filter {
+	__be32	src_ip;
+	__be32	dst_ip;
+};
+
+struct ib_flow_spec_ipv4 {
+	enum ib_flow_spec_type	   type;
+	u16			   size;
+	struct ib_flow_ipv4_filter val;
+	struct ib_flow_ipv4_filter mask;
+};
+
+struct ib_flow_tcp_udp_filter {
+	__be16	dst_port;
+	__be16	src_port;
+};
+
+struct ib_flow_spec_tcp_udp {
+	enum ib_flow_spec_type	      type;
+	u16			      size;
+	struct ib_flow_tcp_udp_filter val;
+	struct ib_flow_tcp_udp_filter mask;
+};
+
+union ib_flow_spec {
+	struct {
+		enum ib_flow_spec_type	type;
+		u16			size;
+	};
+	struct ib_flow_spec_eth		eth;
+	struct ib_flow_spec_ipv4        ipv4;
+	struct ib_flow_spec_tcp_udp	tcp_udp;
+};
+
+struct ib_flow_attr {
+	enum ib_flow_attr_type type;
+	u16	     size;
+	u16	     priority;
+	u32	     flags;
+	u8	     num_of_specs;
+	u8	     port;
+	/* Following are the optional layers according to user request
+	 * struct ib_flow_spec_xxx
+	 * struct ib_flow_spec_yyy
+	 */
+};
+
+struct ib_flow {
+	struct ib_qp		*qp;
+	struct ib_uobject	*uobject;
+};
+
 struct ib_mad;
 struct ib_grh;
 
@@ -1300,6 +1406,11 @@ struct ib_device {
 						 struct ib_ucontext *ucontext,
 						 struct ib_udata *udata);
 	int			   (*dealloc_xrcd)(struct ib_xrcd *xrcd);
+	struct ib_flow *	   (*create_flow)(struct ib_qp *qp,
+						  struct ib_flow_attr
+						  *flow_attr,
+						  int domain);
+	int			   (*destroy_flow)(struct ib_flow *flow_id);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 
@@ -2260,4 +2371,8 @@ struct ib_xrcd *ib_alloc_xrcd(struct ib_device *device);
  */
 int ib_dealloc_xrcd(struct ib_xrcd *xrcd);
 
+struct ib_flow *ib_create_flow(struct ib_qp *qp,
+			       struct ib_flow_attr *flow_attr, int domain);
+int ib_destroy_flow(struct ib_flow *flow_id);
+
 #endif /* IB_VERBS_H */

commit 73c40c616a33fcb7961b3c90a91b550813129b3e
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Aug 1 18:49:53 2013 +0300

    IB/core: Add locking around event dispatching on XRC target QPs
    
    Fix a potential race when event occurrs on a target XRC QP and in the
    middle of reporting that on its shared qps, one of them is destroyed
    by user space application.  Also add note for kernel consumers in
    ib_verbs.h that they must not destroy the QP from within the handler.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 645c3cedce9c..a84d3dfc4076 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -635,6 +635,12 @@ enum ib_qp_create_flags {
 	IB_QP_CREATE_RESERVED_END		= 1 << 31,
 };
 
+
+/*
+ * Note: users may not call ib_close_qp or ib_destroy_qp from the event_handler
+ * callback to destroy the passed in QP.
+ */
+
 struct ib_qp_init_attr {
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;

commit 0134f16bc91cc15a38c867b81568b791c9b626aa
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Sun Jul 7 17:25:52 2013 +0300

    IB/core: Add reserved values to enums for low-level driver use
    
    Continue the approach taken by commit d2b57063e4a ("IB/core: Reserve
    bits in enum ib_qp_create_flags for low-level driver use") and add
    reserved entries to the ib_qp_type and ib_wr_opcode enums.  Low-level
    drivers can then define macros to use these reserved values, giving
    proper names to the macros for readability.  Also add a range of
    reserved flags to enum ib_send_flags.
    
    The mlx5 IB driver uses the new additions.
    
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 98cc4b29fc5b..645c3cedce9c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -610,7 +610,21 @@ enum ib_qp_type {
 	IB_QPT_RAW_PACKET = 8,
 	IB_QPT_XRC_INI = 9,
 	IB_QPT_XRC_TGT,
-	IB_QPT_MAX
+	IB_QPT_MAX,
+	/* Reserve a range for qp types internal to the low level driver.
+	 * These qp types will not be visible at the IB core layer, so the
+	 * IB_QPT_MAX usages should not be affected in the core layer
+	 */
+	IB_QPT_RESERVED1 = 0x1000,
+	IB_QPT_RESERVED2,
+	IB_QPT_RESERVED3,
+	IB_QPT_RESERVED4,
+	IB_QPT_RESERVED5,
+	IB_QPT_RESERVED6,
+	IB_QPT_RESERVED7,
+	IB_QPT_RESERVED8,
+	IB_QPT_RESERVED9,
+	IB_QPT_RESERVED10,
 };
 
 enum ib_qp_create_flags {
@@ -766,6 +780,19 @@ enum ib_wr_opcode {
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
 	IB_WR_BIND_MW,
+	/* reserve values for low level drivers' internal use.
+	 * These values will not be used at all in the ib core layer.
+	 */
+	IB_WR_RESERVED1 = 0xf0,
+	IB_WR_RESERVED2,
+	IB_WR_RESERVED3,
+	IB_WR_RESERVED4,
+	IB_WR_RESERVED5,
+	IB_WR_RESERVED6,
+	IB_WR_RESERVED7,
+	IB_WR_RESERVED8,
+	IB_WR_RESERVED9,
+	IB_WR_RESERVED10,
 };
 
 enum ib_send_flags {
@@ -773,7 +800,11 @@ enum ib_send_flags {
 	IB_SEND_SIGNALED	= (1<<1),
 	IB_SEND_SOLICITED	= (1<<2),
 	IB_SEND_INLINE		= (1<<3),
-	IB_SEND_IP_CSUM		= (1<<4)
+	IB_SEND_IP_CSUM		= (1<<4),
+
+	/* reserve bits 26-31 for low level drivers' internal use */
+	IB_SEND_RESERVED_START	= (1 << 26),
+	IB_SEND_RESERVED_END	= (1 << 31),
 };
 
 struct ib_sge {

commit 7083e42ee2ff43a11481e0e7211ec4f9ac68cb79
Author: Shani Michaeli <shanim@mellanox.com>
Date:   Wed Feb 6 16:19:12 2013 +0000

    IB/core: Add "type 2" memory windows support
    
    This patch enhances the IB core support for Memory Windows (MWs).
    
    MWs allow an application to have better/flexible control over remote
    access to memory.
    
    Two types of MWs are supported, with the second type having two flavors:
    
        Type 1  - associated with PD only
        Type 2A - associated with QPN only
        Type 2B - associated with PD and QPN
    
    Applications can allocate a MW once, and then repeatedly bind the MW
    to different ranges in MRs that are associated to the same PD. Type 1
    windows are bound through a verb, while type 2 windows are bound by
    posting a work request.
    
    The 32-bit memory key is composed of a 24-bit index and an 8-bit
    key. The key is changed with each bind, thus allowing more control
    over the peer's use of the memory key.
    
    The changes introduced are the following:
    
    * add memory window type enum and a corresponding parameter to ib_alloc_mw.
    * type 2 memory window bind work request support.
    * create a struct that contains the common part of the bind verb struct
      ibv_mw_bind and the bind work request into a single struct.
    * add the ib_inc_rkey helper function to advance the tag part of an rkey.
    
    Consumer interface details:
    
    * new device capability flags IB_DEVICE_MEM_WINDOW_TYPE_2A and
      IB_DEVICE_MEM_WINDOW_TYPE_2B are added to indicate device support
      for these features.
    
      Devices can set either IB_DEVICE_MEM_WINDOW_TYPE_2A or
      IB_DEVICE_MEM_WINDOW_TYPE_2B if it supports type 2A or type 2B
      memory windows. It can set neither to indicate it doesn't support
      type 2 windows at all.
    
    * modify existing provides and consumers code to the new param of
      ib_alloc_mw and the ib_mw_bind_info structure
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Shani Michaeli <shanim@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 46bc045bbe15..98cc4b29fc5b 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -115,6 +115,8 @@ enum ib_device_cap_flags {
 	IB_DEVICE_XRC			= (1<<20),
 	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
+	IB_DEVICE_MEM_WINDOW_TYPE_2A	= (1<<23),
+	IB_DEVICE_MEM_WINDOW_TYPE_2B	= (1<<24)
 };
 
 enum ib_atomic_cap {
@@ -715,6 +717,11 @@ enum ib_mig_state {
 	IB_MIG_ARMED
 };
 
+enum ib_mw_type {
+	IB_MW_TYPE_1 = 1,
+	IB_MW_TYPE_2 = 2
+};
+
 struct ib_qp_attr {
 	enum ib_qp_state	qp_state;
 	enum ib_qp_state	cur_qp_state;
@@ -758,6 +765,7 @@ enum ib_wr_opcode {
 	IB_WR_FAST_REG_MR,
 	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
 	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
+	IB_WR_BIND_MW,
 };
 
 enum ib_send_flags {
@@ -780,6 +788,23 @@ struct ib_fast_reg_page_list {
 	unsigned int		max_page_list_len;
 };
 
+/**
+ * struct ib_mw_bind_info - Parameters for a memory window bind operation.
+ * @mr: A memory region to bind the memory window to.
+ * @addr: The address where the memory window should begin.
+ * @length: The length of the memory window, in bytes.
+ * @mw_access_flags: Access flags from enum ib_access_flags for the window.
+ *
+ * This struct contains the shared parameters for type 1 and type 2
+ * memory window bind operations.
+ */
+struct ib_mw_bind_info {
+	struct ib_mr   *mr;
+	u64		addr;
+	u64		length;
+	int		mw_access_flags;
+};
+
 struct ib_send_wr {
 	struct ib_send_wr      *next;
 	u64			wr_id;
@@ -823,6 +848,12 @@ struct ib_send_wr {
 			int				access_flags;
 			u32				rkey;
 		} fast_reg;
+		struct {
+			struct ib_mw            *mw;
+			/* The new rkey for the memory window. */
+			u32                      rkey;
+			struct ib_mw_bind_info   bind_info;
+		} bind_mw;
 	} wr;
 	u32			xrc_remote_srq_num;	/* XRC TGT QPs only */
 };
@@ -839,7 +870,8 @@ enum ib_access_flags {
 	IB_ACCESS_REMOTE_WRITE	= (1<<1),
 	IB_ACCESS_REMOTE_READ	= (1<<2),
 	IB_ACCESS_REMOTE_ATOMIC	= (1<<3),
-	IB_ACCESS_MW_BIND	= (1<<4)
+	IB_ACCESS_MW_BIND	= (1<<4),
+	IB_ZERO_BASED		= (1<<5)
 };
 
 struct ib_phys_buf {
@@ -862,13 +894,16 @@ enum ib_mr_rereg_flags {
 	IB_MR_REREG_ACCESS	= (1<<2)
 };
 
+/**
+ * struct ib_mw_bind - Parameters for a type 1 memory window bind operation.
+ * @wr_id:      Work request id.
+ * @send_flags: Flags from ib_send_flags enum.
+ * @bind_info:  More parameters of the bind operation.
+ */
 struct ib_mw_bind {
-	struct ib_mr   *mr;
-	u64		wr_id;
-	u64		addr;
-	u32		length;
-	int		send_flags;
-	int		mw_access_flags;
+	u64                    wr_id;
+	int                    send_flags;
+	struct ib_mw_bind_info bind_info;
 };
 
 struct ib_fmr_attr {
@@ -991,6 +1026,7 @@ struct ib_mw {
 	struct ib_pd		*pd;
 	struct ib_uobject	*uobject;
 	u32			rkey;
+	enum ib_mw_type         type;
 };
 
 struct ib_fmr {
@@ -1202,7 +1238,8 @@ struct ib_device {
 						    int num_phys_buf,
 						    int mr_access_flags,
 						    u64 *iova_start);
-	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd);
+	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd,
+					       enum ib_mw_type type);
 	int                        (*bind_mw)(struct ib_qp *qp,
 					      struct ib_mw *mw,
 					      struct ib_mw_bind *mw_bind);
@@ -2019,6 +2056,8 @@ int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
  * ib_dereg_mr - Deregisters a memory region and removes it from the
  *   HCA translation table.
  * @mr: The memory region to deregister.
+ *
+ * This function can fail, if the memory region has memory windows bound to it.
  */
 int ib_dereg_mr(struct ib_mr *mr);
 
@@ -2070,11 +2109,23 @@ static inline void ib_update_fast_reg_key(struct ib_mr *mr, u8 newkey)
 	mr->rkey = (mr->rkey & 0xffffff00) | newkey;
 }
 
+/**
+ * ib_inc_rkey - increments the key portion of the given rkey. Can be used
+ * for calculating a new rkey for type 2 memory windows.
+ * @rkey - the rkey to increment.
+ */
+static inline u32 ib_inc_rkey(u32 rkey)
+{
+	const u32 mask = 0x000000ff;
+	return ((rkey + 1) & mask) | (rkey & ~mask);
+}
+
 /**
  * ib_alloc_mw - Allocates a memory window.
  * @pd: The protection domain associated with the memory window.
+ * @type: The type of the memory window (1 or 2).
  */
-struct ib_mw *ib_alloc_mw(struct ib_pd *pd);
+struct ib_mw *ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type);
 
 /**
  * ib_bind_mw - Posts a work request to the send queue of the specified
@@ -2084,6 +2135,10 @@ struct ib_mw *ib_alloc_mw(struct ib_pd *pd);
  * @mw: The memory window to bind.
  * @mw_bind: Specifies information about the memory window, including
  *   its address range, remote access rights, and associated memory region.
+ *
+ * If there is no immediate error, the function will update the rkey member
+ * of the mw parameter to its new value. The bind operation can still fail
+ * asynchronously.
  */
 static inline int ib_bind_mw(struct ib_qp *qp,
 			     struct ib_mw *mw,

commit d2b57063e4aba51d3c49ec957607d2e4c9d5f29a
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Fri Aug 3 08:40:37 2012 +0000

    IB/core: Reserve bits in enum ib_qp_create_flags for low-level driver use
    
    Reserve bits 26-31 for internal use by low-level drivers. Two such
    bits are used in the mlx4_b driver SR-IOV implementation.
    
    These enum additions guarantee that the core layer will never use
    these bits, so that low level drivers may safely make use of them.
    
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 07996af8265a..46bc045bbe15 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -614,6 +614,9 @@ enum ib_qp_type {
 enum ib_qp_create_flags {
 	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
 	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
+	/* reserve bits 26-31 for low level drivers' internal use */
+	IB_QP_CREATE_RESERVED_START		= 1 << 26,
+	IB_QP_CREATE_RESERVED_END		= 1 << 31,
 };
 
 struct ib_qp_init_attr {

commit cc169165c82e14ea43e313f937a0a475ca97e588
Merge: 02daaf2741e7 e572568fbcc6 464357a75913 7d9c0de4ab4e b6cec8aa4a79 035b1032b576 784d135f9678 349556692df9 1c94283ddbe8 3987a2d3193c
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon May 21 09:00:47 2012 -0700

    Merge branches 'core', 'cxgb4', 'ipath', 'iser', 'lockdep', 'mlx4', 'nes', 'ocrdma', 'qib' and 'raw-qp' into for-linus

commit c938a616aadb621b8e26b0ac09ac13d053c7ed1c
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Thu Mar 1 12:17:51 2012 +0200

    IB/core: Add raw packet QP type
    
    IB_QPT_RAW_PACKET allows applications to build a complete packet,
    including L2 headers, when sending; on the receive side, the HW will
    not strip any headers.
    
    This QP type is designed for userspace direct access to Ethernet; for
    example by applications that do TCP/IP themselves.  Only processes
    with the NET_RAW capability are allowed to create raw packet QPs (the
    name "raw packet QP" is supposed to suggest an analogy to AF_PACKET /
    SOL_RAW sockets).
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3cca5a4dacd..a3fa3232b8a0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -605,7 +605,7 @@ enum ib_qp_type {
 	IB_QPT_UD,
 	IB_QPT_RAW_IPV6,
 	IB_QPT_RAW_ETHERTYPE,
-	/* Save 8 for RAW_PACKET */
+	IB_QPT_RAW_PACKET = 8,
 	IB_QPT_XRC_INI = 9,
 	IB_QPT_XRC_TGT,
 	IB_QPT_MAX

commit c3bccbfbb71f5e8a77129c7a069f5c5648cc9cf1
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Sun Apr 29 17:04:22 2012 +0300

    IB/core: Use qp->usecnt to track multicast attach/detach
    
    Just as we don't allow PDs, CQs, etc. to be destroyed if there are QPs
    that are attached to them, don't let a QP be destroyed if there are
    multicast group(s) attached to it.  Use the existing usecnt field of
    struct ib_qp which was added by commit 0e0ec7e ("RDMA/core: Export
    ib_open_qp() to share XRC TGT QPs") to track this.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3cca5a4dacd..c84d1016618e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -964,7 +964,7 @@ struct ib_qp {
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;
-	atomic_t		usecnt; /* count times opened */
+	atomic_t		usecnt; /* count times opened, mcast attaches */
 	struct list_head	open_list;
 	struct ib_qp           *real_qp;
 	struct ib_uobject      *uobject;

commit d927d505c59a0c7353343174e6225c43c61fba6d
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Jan 11 19:03:51 2012 +0200

    IB: Change CQE "csum_ok" field to a bit flag
    
    Use a bit in wc_flags rather then a whole integer to hold the
    "checksum OK" flag.  By itself, this change doesn't reduce the size of
    struct ib_wc on 64bit machines -- it stays on 56 bytes because of
    padding.  However, it will allow to add more fields in the future
    without enlarging the struct.  Also, it will let us have a unified
    approach with future libibverbs checksum offload reporting, because a
    bit flag doesn't break the library ABI.
    
    This patch was suggested during conversation with Liran Liss
    <liranl@mellanox.com>.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ff22a73e0937..c3cca5a4dacd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -518,6 +518,7 @@ enum ib_wc_flags {
 	IB_WC_GRH		= 1,
 	IB_WC_WITH_IMM		= (1<<1),
 	IB_WC_WITH_INVALIDATE	= (1<<2),
+	IB_WC_IP_CSUM_OK	= (1<<3),
 };
 
 struct ib_wc {
@@ -538,7 +539,6 @@ struct ib_wc {
 	u8			sl;
 	u8			dlid_path_bits;
 	u8			port_num;	/* valid only for DR SMPs on switches */
-	int			csum_ok;
 };
 
 enum ib_cq_notify_flags {

commit 2e96691c31ecf749f48aa94ea837b95dd656f5c2
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Feb 28 18:49:50 2012 +0200

    IB: Use central enum for speed instead of hard-coded values
    
    The kernel IB stack uses one enumeration for IB speed, which wasn't
    explicitly specified in the verbs header file.  Add that enum, and use
    it all over the code.
    
    The IB speed/width notation is also used by iWARP and IBoE HW drivers,
    which use the convention of rate = speed * width to advertise their
    port link rate.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bf5daafe8ecc..ff22a73e0937 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -239,6 +239,15 @@ static inline int ib_width_enum_to_int(enum ib_port_width width)
 	}
 }
 
+enum ib_port_speed {
+	IB_SPEED_SDR	= 1,
+	IB_SPEED_DDR	= 2,
+	IB_SPEED_QDR	= 4,
+	IB_SPEED_FDR10	= 8,
+	IB_SPEED_FDR	= 16,
+	IB_SPEED_EDR	= 32
+};
+
 struct ib_protocol_stats {
 	/* TBD... */
 };

commit 504255f8d0480cf293962adf4bc3aecac645ae71
Merge: 2be6053318aa 615eb715ae10 f7cc25d018f1 d32ae393dbf0 a5e12dff757b 3e60a77ea218 787adb9d6ad9 caf6e3f221dd cb29688aaa4c 0f0bee8bbc2b 16d99812d58b 42849b2697c3
Author: Roland Dreier <roland@purestorage.com>
Date:   Tue Nov 1 09:37:08 2011 -0700

    Merge branches 'amso1100', 'cma', 'cxgb3', 'cxgb4', 'fdr', 'ipath', 'ipoib', 'misc', 'mlx4', 'misc', 'nes', 'qib' and 'xrc' into for-next

commit 0e0ec7e0638ef48e0c661873dfcc8caccab984c6
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Mon Aug 8 15:31:51 2011 -0700

    RDMA/core: Export ib_open_qp() to share XRC TGT QPs
    
    XRC TGT QPs are shared resources among multiple processes.  Since the
    creating process may exit, allow other processes which share the same
    XRC domain to open an existing QP.  This allows us to transfer
    ownership of an XRC TGT QP to another process.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index dfd9b87b7ffd..8705539bce75 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -605,6 +605,13 @@ struct ib_qp_init_attr {
 	u8			port_num; /* special QP types only */
 };
 
+struct ib_qp_open_attr {
+	void                  (*event_handler)(struct ib_event *, void *);
+	void		       *qp_context;
+	u32			qp_num;
+	enum ib_qp_type		qp_type;
+};
+
 enum ib_rnr_timeout {
 	IB_RNR_TIMER_655_36 =  0,
 	IB_RNR_TIMER_000_01 =  1,
@@ -932,6 +939,9 @@ struct ib_qp {
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct list_head	xrcd_list;
+	atomic_t		usecnt; /* count times opened */
+	struct list_head	open_list;
+	struct ib_qp           *real_qp;
 	struct ib_uobject      *uobject;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
@@ -1488,15 +1498,23 @@ int ib_query_qp(struct ib_qp *qp,
 int ib_destroy_qp(struct ib_qp *qp);
 
 /**
- * ib_release_qp - Release an external reference to a QP.
+ * ib_open_qp - Obtain a reference to an existing sharable QP.
+ * @xrcd - XRC domain
+ * @qp_open_attr: Attributes identifying the QP to open.
+ *
+ * Returns a reference to a sharable QP.
+ */
+struct ib_qp *ib_open_qp(struct ib_xrcd *xrcd,
+			 struct ib_qp_open_attr *qp_open_attr);
+
+/**
+ * ib_close_qp - Release an external reference to a QP.
  * @qp: The QP handle to release
  *
- * The specified QP handle is released by the caller.  If the QP is
- * referenced internally, it is not destroyed until all internal
- * references are released.  After releasing the qp, the caller
- * can no longer access it and all events on the QP are discarded.
+ * The opened QP handle is released by the caller.  The underlying
+ * shared QP is not destroyed until all internal references are released.
  */
-int ib_release_qp(struct ib_qp *qp);
+int ib_close_qp(struct ib_qp *qp);
 
 /**
  * ib_post_send - Posts a list of work requests to the send queue of

commit 53d0bd1e7ff2fc626321f337c609fb76ae5d12c9
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Tue May 24 08:33:46 2011 -0700

    RDMA/uverbs: Export XRC domains to user space
    
    Allow user space to create XRC domains.  Because XRCDs are expected to
    be shared among multiple processes, we use inodes to identify an XRCD.
    
    Based on patches by Jack Morgenstein <jackm@dev.mellanox.co.il>
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ac46dcf04358..dfd9b87b7ffd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -851,6 +851,7 @@ struct ib_ucontext {
 	struct list_head	qp_list;
 	struct list_head	srq_list;
 	struct list_head	ah_list;
+	struct list_head	xrcd_list;
 	int			closing;
 };
 
@@ -881,6 +882,7 @@ struct ib_pd {
 struct ib_xrcd {
 	struct ib_device       *device;
 	atomic_t		usecnt; /* count all exposed resources */
+	struct inode	       *inode;
 
 	struct mutex		tgt_qp_mutex;
 	struct list_head	tgt_qp_list;

commit d3d72d909e783d048ee39046aa7b4fa798a4dda8
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Thu May 26 23:06:44 2011 -0700

    RDMA/verbs: Cleanup XRC TGT QPs when destroying XRCD
    
    XRC TGT QPs are intended to be shared among multiple users and
    processes.  Allow the destruction of an XRC TGT QP to be done explicitly
    through ib_destroy_qp() or when the XRCD is destroyed.
    
    To support destroying an XRC TGT QP, we need to track TGT QPs with the
    XRCD.  When the XRCD is destroyed, all tracked XRC TGT QPs are also
    cleaned up.
    
    To avoid stale reference issues, if a user is holding a reference on a
    TGT QP, we increment a reference count on the QP.  The user releases the
    reference by calling ib_release_qp.  This releases any access to the QP
    from a user above verbs, but allows the QP to continue to exist until
    destroyed by the XRCD.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3d7602b5e90..ac46dcf04358 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -880,7 +880,10 @@ struct ib_pd {
 
 struct ib_xrcd {
 	struct ib_device       *device;
-	atomic_t          	usecnt; /* count all resources */
+	atomic_t		usecnt; /* count all exposed resources */
+
+	struct mutex		tgt_qp_mutex;
+	struct list_head	tgt_qp_list;
 };
 
 struct ib_ah {
@@ -926,6 +929,7 @@ struct ib_qp {
 	struct ib_cq	       *recv_cq;
 	struct ib_srq	       *srq;
 	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
+	struct list_head	xrcd_list;
 	struct ib_uobject      *uobject;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
@@ -1481,6 +1485,17 @@ int ib_query_qp(struct ib_qp *qp,
  */
 int ib_destroy_qp(struct ib_qp *qp);
 
+/**
+ * ib_release_qp - Release an external reference to a QP.
+ * @qp: The QP handle to release
+ *
+ * The specified QP handle is released by the caller.  If the QP is
+ * referenced internally, it is not destroyed until all internal
+ * references are released.  After releasing the qp, the caller
+ * can no longer access it and all events on the QP are discarded.
+ */
+int ib_release_qp(struct ib_qp *qp);
+
 /**
  * ib_post_send - Posts a list of work requests to the send queue of
  *   the specified QP.

commit b42b63cf0dde2af6eec462b2d6cca7d938702a28
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Mon May 23 19:59:25 2011 -0700

    RDMA/core: Add XRC QPs
    
    XRC ("eXtended reliable connected") is an IB transport that provides
    better scalability by allowing senders to specify which shared receive
    queue (SRQ) should be used to receive a message, which essentially
    allows one transport context (QP connection) to serve multiple
    destinations (as long as they share an adapter, of course).
    
    XRC communication is between an initiator (INI) QP and a target (TGT)
    QP.  Target QPs are associated with SRQs through an XRCD.  An XRC TGT QP
    behaves like a receive-only RD QP.  XRC INI QPs behave similarly to RC
    QPs, except that work requests posted to an XRC INI QP must specify the
    remote SRQ that is the target of the work request.
    
    We define two new QP types for XRC, to distinguish between INI and TGT
    QPs, and update the core layer to support XRC QPs.
    
    This patch is derived from work by Jack Morgenstein
    <jackm@dev.mellanox.co.il>
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 516647a22135..c3d7602b5e90 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -579,7 +579,11 @@ enum ib_qp_type {
 	IB_QPT_UC,
 	IB_QPT_UD,
 	IB_QPT_RAW_IPV6,
-	IB_QPT_RAW_ETHERTYPE
+	IB_QPT_RAW_ETHERTYPE,
+	/* Save 8 for RAW_PACKET */
+	IB_QPT_XRC_INI = 9,
+	IB_QPT_XRC_TGT,
+	IB_QPT_MAX
 };
 
 enum ib_qp_create_flags {
@@ -593,6 +597,7 @@ struct ib_qp_init_attr {
 	struct ib_cq	       *send_cq;
 	struct ib_cq	       *recv_cq;
 	struct ib_srq	       *srq;
+	struct ib_xrcd	       *xrcd;     /* XRC TGT QPs only */
 	struct ib_qp_cap	cap;
 	enum ib_sig_type	sq_sig_type;
 	enum ib_qp_type		qp_type;
@@ -784,6 +789,7 @@ struct ib_send_wr {
 			u32				rkey;
 		} fast_reg;
 	} wr;
+	u32			xrc_remote_srq_num;	/* XRC TGT QPs only */
 };
 
 struct ib_recv_wr {
@@ -919,6 +925,7 @@ struct ib_qp {
 	struct ib_cq	       *send_cq;
 	struct ib_cq	       *recv_cq;
 	struct ib_srq	       *srq;
+	struct ib_xrcd	       *xrcd; /* XRC TGT QPs only */
 	struct ib_uobject      *uobject;
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;

commit 418d51307d102e72e745031adb4f5ba0ddb646e2
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Mon May 23 19:42:29 2011 -0700

    RDMA/core: Add XRC SRQ type
    
    XRC ("eXtended reliable connected") is an IB transport that provides
    better scalability by allowing senders to specify which shared receive
    queue (SRQ) should be used to receive a message, which essentially
    allows one transport context (QP connection) to serve multiple
    destinations (as long as they share an adapter, of course).
    
    XRC defines SRQs that are specifically used by XRC connections.  Expand
    the SRQ code to support XRC SRQs.  An XRC SRQ is currently restricted to
    only XRC use according to the IB XRC Annex.
    
    Portions of this patch were derived from work by
    Jack Morgenstein <jackm@dev.mellanox.co.il>.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d0c2dc034054..516647a22135 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -524,7 +524,8 @@ enum ib_cq_notify_flags {
 };
 
 enum ib_srq_type {
-	IB_SRQT_BASIC
+	IB_SRQT_BASIC,
+	IB_SRQT_XRC
 };
 
 enum ib_srq_attr_mask {
@@ -543,6 +544,13 @@ struct ib_srq_init_attr {
 	void		       *srq_context;
 	struct ib_srq_attr	attr;
 	enum ib_srq_type	srq_type;
+
+	union {
+		struct {
+			struct ib_xrcd *xrcd;
+			struct ib_cq   *cq;
+		} xrc;
+	} ext;
 };
 
 struct ib_qp_cap {
@@ -895,6 +903,14 @@ struct ib_srq {
 	void		       *srq_context;
 	enum ib_srq_type	srq_type;
 	atomic_t		usecnt;
+
+	union {
+		struct {
+			struct ib_xrcd *xrcd;
+			struct ib_cq   *cq;
+			u32		srq_num;
+		} xrc;
+	} ext;
 };
 
 struct ib_qp {

commit 96104eda01695a26da2c8f7423ec0ba3509c8c97
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Mon May 23 16:31:36 2011 -0700

    RDMA/core: Add SRQ type field
    
    Currently, there is only a single ("basic") type of SRQ, but with XRC
    support we will add a second.  Prepare for this by defining an SRQ type
    and setting all current users to IB_SRQT_BASIC.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d2a5c9b991d1..d0c2dc034054 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -523,6 +523,10 @@ enum ib_cq_notify_flags {
 	IB_CQ_REPORT_MISSED_EVENTS	= 1 << 2,
 };
 
+enum ib_srq_type {
+	IB_SRQT_BASIC
+};
+
 enum ib_srq_attr_mask {
 	IB_SRQ_MAX_WR	= 1 << 0,
 	IB_SRQ_LIMIT	= 1 << 1,
@@ -538,6 +542,7 @@ struct ib_srq_init_attr {
 	void		      (*event_handler)(struct ib_event *, void *);
 	void		       *srq_context;
 	struct ib_srq_attr	attr;
+	enum ib_srq_type	srq_type;
 };
 
 struct ib_qp_cap {
@@ -888,6 +893,7 @@ struct ib_srq {
 	struct ib_uobject      *uobject;
 	void		      (*event_handler)(struct ib_event *, void *);
 	void		       *srq_context;
+	enum ib_srq_type	srq_type;
 	atomic_t		usecnt;
 };
 

commit 59991f94eb32e954aa767f659eb642461e9e8b37
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Mon May 23 17:52:46 2011 -0700

    RDMA/core: Add XRC domain support
    
    XRC ("eXtended reliable connected") is an IB transport that provides
    better scalability by allowing senders to specify which shared receive
    queue (SRQ) should be used to receive a message, which essentially
    allows one transport context (QP connection) to serve multiple
    destinations (as long as they share an adapter, of course).
    
    A few new concepts are introduced to support this.  This patch adds:
    
     - A new device capability flag, IB_DEVICE_XRC, which low-level
       drivers set to indicate that a device supports XRC.
     - A new object type, XRC domains (struct ib_xrcd), and new verbs
       ib_alloc_xrcd()/ib_dealloc_xrcd().  XRCDs are used to limit which
       XRC SRQs an incoming message can target.
    
    This patch is derived from work by Jack Morgenstein <jackm@dev.mellanox.co.il>.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 228be3e220d9..d2a5c9b991d1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -112,6 +112,7 @@ enum ib_device_cap_flags {
 	 */
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
+	IB_DEVICE_XRC			= (1<<20),
 	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
 	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 };
@@ -858,6 +859,11 @@ struct ib_pd {
 	atomic_t          	usecnt; /* count all resources */
 };
 
+struct ib_xrcd {
+	struct ib_device       *device;
+	atomic_t          	usecnt; /* count all resources */
+};
+
 struct ib_ah {
 	struct ib_device	*device;
 	struct ib_pd		*pd;
@@ -1149,6 +1155,10 @@ struct ib_device {
 						  struct ib_grh *in_grh,
 						  struct ib_mad *in_mad,
 						  struct ib_mad *out_mad);
+	struct ib_xrcd *	   (*alloc_xrcd)(struct ib_device *device,
+						 struct ib_ucontext *ucontext,
+						 struct ib_udata *udata);
+	int			   (*dealloc_xrcd)(struct ib_xrcd *xrcd);
 
 	struct ib_dma_mapping_ops   *dma_ops;
 
@@ -2060,4 +2070,16 @@ int ib_attach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);
  */
 int ib_detach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);
 
+/**
+ * ib_alloc_xrcd - Allocates an XRC domain.
+ * @device: The device on which to allocate the XRC domain.
+ */
+struct ib_xrcd *ib_alloc_xrcd(struct ib_device *device);
+
+/**
+ * ib_dealloc_xrcd - Deallocates an XRC domain.
+ * @xrcd: The XRC domain to deallocate.
+ */
+int ib_dealloc_xrcd(struct ib_xrcd *xrcd);
+
 #endif /* IB_VERBS_H */

commit 71eeba161d7611238ecb6f525a82325aa35339f0
Author: Marcel Apfelbaum <marcela@dev.mellanox.co.il>
Date:   Wed Oct 5 14:21:47 2011 +0300

    IB: Add new InfiniBand link speeds
    
    Introduce support for the following extended speeds:
    
    FDR-10: a Mellanox proprietary link speed which is 10.3125 Gbps with
            64b/66b encoding rather than 8b/10b encoding.
    FDR:    IBA extended speed 14.0625 Gbps.
    EDR:    IBA extended speed 25.78125 Gbps.
    
    Signed-off-by: Marcel Apfelbaum <marcela@dev.mellanox.co.il>
    Reviewed-by: Hal Rosenstock <hal@mellanox.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 228be3e220d9..97c98c0d89b1 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -207,6 +207,7 @@ enum ib_port_cap_flags {
 	IB_PORT_SM_DISABLED			= 1 << 10,
 	IB_PORT_SYS_IMAGE_GUID_SUP		= 1 << 11,
 	IB_PORT_PKEY_SW_EXT_PORT_TRAP_SUP	= 1 << 12,
+	IB_PORT_EXTENDED_SPEEDS_SUP             = 1 << 14,
 	IB_PORT_CM_SUP				= 1 << 16,
 	IB_PORT_SNMP_TUNNEL_SUP			= 1 << 17,
 	IB_PORT_REINIT_SUP			= 1 << 18,
@@ -415,7 +416,15 @@ enum ib_rate {
 	IB_RATE_40_GBPS  = 7,
 	IB_RATE_60_GBPS  = 8,
 	IB_RATE_80_GBPS  = 9,
-	IB_RATE_120_GBPS = 10
+	IB_RATE_120_GBPS = 10,
+	IB_RATE_14_GBPS  = 11,
+	IB_RATE_56_GBPS  = 12,
+	IB_RATE_112_GBPS = 13,
+	IB_RATE_168_GBPS = 14,
+	IB_RATE_25_GBPS  = 15,
+	IB_RATE_100_GBPS = 16,
+	IB_RATE_200_GBPS = 17,
+	IB_RATE_300_GBPS = 18
 };
 
 /**
@@ -426,6 +435,13 @@ enum ib_rate {
  */
 int ib_rate_to_mult(enum ib_rate rate) __attribute_const__;
 
+/**
+ * ib_rate_to_mbps - Convert the IB rate enum to Mbps.
+ * For example, IB_RATE_2_5_GBPS will be converted to 2500.
+ * @rate: rate to convert.
+ */
+int ib_rate_to_mbps(enum ib_rate rate) __attribute_const__;
+
 /**
  * mult_to_ib_rate - Convert a multiple of 2.5 Gbit/sec to an IB rate
  * enum.

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index bf4306aea169..228be3e220d9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -49,7 +49,7 @@
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
 
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/uaccess.h>
 
 extern struct workqueue_struct *ib_wq;

commit 761d90ed4c36798a2f5fcb161d4ee2119bab478b
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Jun 15 14:39:29 2011 +0000

    IB/core: Add GID change event
    
    Add IB GID change event type.  This is needed for IBoE when the HW
    driver updates the GID (e.g when new VLANs are added/deleted) table
    and the change should be reflected to the IB core cache.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.co.il>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 55cd0a0bc977..bf4306aea169 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -350,7 +350,8 @@ enum ib_event_type {
 	IB_EVENT_SRQ_ERR,
 	IB_EVENT_SRQ_LIMIT_REACHED,
 	IB_EVENT_QP_LAST_WQE_REACHED,
-	IB_EVENT_CLIENT_REREGISTER
+	IB_EVENT_CLIENT_REREGISTER,
+	IB_EVENT_GID_CHANGE,
 };
 
 struct ib_event {

commit f06267104dd9112f11586830d22501d0e26245ea
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Oct 19 15:24:36 2010 +0000

    RDMA: Update workqueue usage
    
    * ib_wq is added, which is used as the common workqueue for infiniband
      instead of the system workqueue.  All system workqueue usages
      including flush_scheduled_work() callers are converted to use and
      flush ib_wq.
    
    * cancel_delayed_work() + flush_scheduled_work() converted to
      cancel_delayed_work_sync().
    
    * qib_wq is removed and ib_wq is used instead.
    
    This is to prepare for deprecation of flush_scheduled_work().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e04c4888d1fd..55cd0a0bc977 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -47,10 +47,13 @@
 #include <linux/list.h>
 #include <linux/rwsem.h>
 #include <linux/scatterlist.h>
+#include <linux/workqueue.h>
 
 #include <asm/atomic.h>
 #include <asm/uaccess.h>
 
+extern struct workqueue_struct *ib_wq;
+
 union ib_gid {
 	u8	raw[16];
 	struct {

commit a3f5adaf491490089215f863a61b9422fae902f8
Author: Eli Cohen <eli@mellanox.co.il>
Date:   Mon Sep 27 17:51:10 2010 -0700

    IB/core: Add link layer property to ports
    
    This patch allows ports to have different link layers:
    IB_LINK_LAYER_INFINIBAND or IB_LINK_LAYER_ETHERNET.  This is required
    for adding IBoE (InfiniBand-over-Ethernet, aka RoCE) support.  For
    devices that do not provide an implementation for querying the link
    layer property of a port, we return a default value based on the
    transport: RMA_TRANSPORT_IB nodes will return IB_LINK_LAYER_INFINIBAND
    and RDMA_TRANSPORT_IWARP nodes will return IB_LINK_LAYER_ETHERNET.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 857b3b9cf120..e04c4888d1fd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -75,6 +75,12 @@ enum rdma_transport_type {
 enum rdma_transport_type
 rdma_node_get_transport(enum rdma_node_type node_type) __attribute_const__;
 
+enum rdma_link_layer {
+	IB_LINK_LAYER_UNSPECIFIED,
+	IB_LINK_LAYER_INFINIBAND,
+	IB_LINK_LAYER_ETHERNET,
+};
+
 enum ib_device_cap_flags {
 	IB_DEVICE_RESIZE_MAX_WR		= 1,
 	IB_DEVICE_BAD_PKEY_CNTR		= (1<<1),
@@ -1010,6 +1016,8 @@ struct ib_device {
 	int		           (*query_port)(struct ib_device *device,
 						 u8 port_num,
 						 struct ib_port_attr *port_attr);
+	enum rdma_link_layer	   (*get_link_layer)(struct ib_device *device,
+						     u8 port_num);
 	int		           (*query_gid)(struct ib_device *device,
 						u8 port_num, int index,
 						union ib_gid *gid);
@@ -1222,6 +1230,9 @@ int ib_query_device(struct ib_device *device,
 int ib_query_port(struct ib_device *device,
 		  u8 port_num, struct ib_port_attr *port_attr);
 
+enum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,
+					       u8 port_num);
+
 int ib_query_gid(struct ib_device *device,
 		 u8 port_num, int index, union ib_gid *gid);
 

commit a2ebf07ae53e65bd073f96877e4818f2e89271ae
Author: Aleksey Senin <alex@senin.name>
Date:   Sun Jul 4 13:55:57 2010 +0000

    IB: Rename RAW_ETY to RAW_ETHERTYPE
    
    Change abbreviated IB_QPT_RAW_ETY to IB_QPT_RAW_ETHERTYPE to make
    the special QP type easier to understand.
    
    cf http://www.mail-archive.com/linux-rdma@vger.kernel.org/msg04530.html
    
    Signed-off-by: Aleksey Senin <alekseys@voltaire.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f3e8f3c07725..857b3b9cf120 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -555,7 +555,7 @@ enum ib_qp_type {
 	IB_QPT_UC,
 	IB_QPT_UD,
 	IB_QPT_RAW_IPV6,
-	IB_QPT_RAW_ETY
+	IB_QPT_RAW_ETHERTYPE
 };
 
 enum ib_qp_create_flags {

commit 9a6edb60ec10d86b1025a0cdad68fd89f1ddaf02
Author: Ralph Campbell <ralph.campbell@qlogic.com>
Date:   Thu May 6 17:03:25 2010 -0700

    IB/core: Allow device-specific per-port sysfs files
    
    Add a new parameter to ib_register_device() so that low-level device
    drivers can pass in a pointer to a callback function that will be
    called for each port that is registered in sysfs.  This allows
    low-level device drivers to create files in
    
        /sys/class/infiniband/<hca>/ports/<N>/
    
    without having to poke through the internals of the RDMA sysfs handling.
    
    There is no need for an unregister function since the kobject
    reference will go to zero when ib_unregister_device() is called.
    
    Signed-off-by: Ralph Campbell <ralph.campbell@qlogic.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 310d31474034..f3e8f3c07725 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1172,7 +1172,9 @@ struct ib_client {
 struct ib_device *ib_alloc_device(size_t size);
 void ib_dealloc_device(struct ib_device *device);
 
-int ib_register_device   (struct ib_device *device);
+int ib_register_device(struct ib_device *device,
+		       int (*port_callback)(struct ib_device *,
+					    u8, struct kobject *));
 void ib_unregister_device(struct ib_device *device);
 
 int ib_register_client   (struct ib_client *client);

commit 5e80ba8ff0bd33ff4af2365969a231cbdb98cafb
Author: Vladimir Sokolovsky <vlad@mellanox.co.il>
Date:   Wed Apr 14 17:23:01 2010 +0300

    IB/core: Add support for masked atomic operations
    
     - Add new IB_WR_MASKED_ATOMIC_CMP_AND_SWP and IB_WR_MASKED_ATOMIC_FETCH_AND_ADD
       send opcodes that can be used to post "masked atomic compare and
       swap" and "masked atomic fetch and add" work request respectively.
     - Add masked_atomic_cap capability.
     - Add mask fields to atomic struct of ib_send_wr
     - Add new opcodes to ib_wc_opcode
    
    The new operations are described more precisely below:
    
    * Masked Compare and Swap (MskCmpSwap)
    
    The MskCmpSwap atomic operation is an extension to the CmpSwap
    operation defined in the IB spec.  MskCmpSwap allows the user to
    select a portion of the 64 bit target data for the compare check as
    well as to restrict the swap to a (possibly different) portion.  The
    pseudo code below describes the operation:
    
    | atomic_response = *va
    | if (!((compare_add ^ *va) & compare_add_mask)) then
    |     *va = (*va & ~(swap_mask)) | (swap & swap_mask)
    |
    | return atomic_response
    
    The additional operands are carried in the Extended Transport Header.
    Atomic response generation and packet format for MskCmpSwap is as for
    standard IB Atomic operations.
    
    * Masked Fetch and Add (MFetchAdd)
    
    The MFetchAdd Atomic operation extends the functionality of the
    standard IB FetchAdd by allowing the user to split the target into
    multiple fields of selectable length. The atomic add is done
    independently on each one of this fields. A bit set in the
    field_boundary parameter specifies the field boundaries. The pseudo
    code below describes the operation:
    
    | bit_adder(ci, b1, b2, *co)
    | {
    |       value = ci + b1 + b2
    |       *co = !!(value & 2)
    |
    |       return value & 1
    | }
    |
    | #define MASK_IS_SET(mask, attr)      (!!((mask)&(attr)))
    | bit_position = 1
    | carry = 0
    | atomic_response = 0
    |
    | for i = 0 to 63
    | {
    |         if ( i != 0 )
    |                 bit_position =  bit_position << 1
    |
    |         bit_add_res = bit_adder(carry, MASK_IS_SET(*va, bit_position),
    |                                 MASK_IS_SET(compare_add, bit_position), &new_carry)
    |         if (bit_add_res)
    |                 atomic_response |= bit_position
    |
    |         carry = ((new_carry) && (!MASK_IS_SET(compare_add_mask, bit_position)))
    | }
    |
    | return atomic_response
    
    Signed-off-by: Vladimir Sokolovsky <vlad@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a585e0f92bc3..310d31474034 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -136,6 +136,7 @@ struct ib_device_attr {
 	int			max_qp_init_rd_atom;
 	int			max_ee_init_rd_atom;
 	enum ib_atomic_cap	atomic_cap;
+	enum ib_atomic_cap	masked_atomic_cap;
 	int			max_ee;
 	int			max_rdd;
 	int			max_mw;
@@ -467,6 +468,8 @@ enum ib_wc_opcode {
 	IB_WC_LSO,
 	IB_WC_LOCAL_INV,
 	IB_WC_FAST_REG_MR,
+	IB_WC_MASKED_COMP_SWAP,
+	IB_WC_MASKED_FETCH_ADD,
 /*
  * Set value of IB_WC_RECV so consumers can test if a completion is a
  * receive by testing (opcode & IB_WC_RECV).
@@ -689,6 +692,8 @@ enum ib_wr_opcode {
 	IB_WR_RDMA_READ_WITH_INV,
 	IB_WR_LOCAL_INV,
 	IB_WR_FAST_REG_MR,
+	IB_WR_MASKED_ATOMIC_CMP_AND_SWP,
+	IB_WR_MASKED_ATOMIC_FETCH_AND_ADD,
 };
 
 enum ib_send_flags {
@@ -731,6 +736,8 @@ struct ib_send_wr {
 			u64	remote_addr;
 			u64	compare_add;
 			u64	swap;
+			u64	compare_add_mask;
+			u64	swap_mask;
 			u32	rkey;
 		} atomic;
 		struct {

commit 17a55f79fd8051a6a8a6e84176c83af71877a98b
Author: Alexander Chiang <achiang@hp.com>
Date:   Tue Feb 2 19:09:16 2010 +0000

    IB/core: Pack struct ib_device a little tighter
    
    A small change to reduce the size of ib_device to 1112 bytes
    (from 1128).
    
    Signed-off-by: Alex Chiang <achiang@hp.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 09509edb1c5f..a585e0f92bc3 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -984,9 +984,9 @@ struct ib_device {
 	struct list_head              event_handler_list;
 	spinlock_t                    event_handler_lock;
 
+	spinlock_t                    client_data_lock;
 	struct list_head              core_list;
 	struct list_head              client_data_list;
-	spinlock_t                    client_data_lock;
 
 	struct ib_cache               cache;
 	int                          *pkey_tbl_len;
@@ -1144,8 +1144,8 @@ struct ib_device {
 		IB_DEV_UNREGISTERED
 	}                            reg_state;
 
-	u64			     uverbs_cmd_mask;
 	int			     uverbs_abi_ver;
+	u64			     uverbs_cmd_mask;
 
 	char			     node_desc[64];
 	__be64			     node_guid;

commit 55464d461bdcffc4422aebfb750eacf99e3c0f27
Author: Bart Van Assche <bart.vanassche@gmail.com>
Date:   Wed Dec 9 14:20:04 2009 -0800

    IB: Clarify the documentation of ib_post_send()
    
    Clarify the behavior of ib_post_send() when a list of work requests is
    passed in and an immediate error is returned.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@gmail.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c179318edd92..09509edb1c5f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1425,6 +1425,11 @@ int ib_destroy_qp(struct ib_qp *qp);
  * @send_wr: A list of work requests to post on the send queue.
  * @bad_send_wr: On an immediate failure, this parameter will reference
  *   the work request that failed to be posted on the QP.
+ *
+ * While IBA Vol. 1 section 11.4.1.1 specifies that if an immediate
+ * error is returned, the QP state shall not be affected,
+ * ib_post_send() will return an immediate error after queueing any
+ * earlier work requests in the list.
  */
 static inline int ib_post_send(struct ib_qp *qp,
 			       struct ib_send_wr *send_wr,

commit f3a7c66b5ce0b75a9774a50b5dcce93e5ba28370
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Sat Feb 14 22:58:35 2009 -0800

    net: replace __constant_{endian} uses in net headers
    
    Base versions handle constant folding now.  For headers exposed to
    userspace, we must only expose the __ prefixed versions.
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 936e333e7ce5..c179318edd92 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -388,7 +388,7 @@ enum {
 	IB_MULTICAST_QPN = 0xffffff
 };
 
-#define IB_LID_PERMISSIVE	__constant_htons(0xFFFF)
+#define IB_LID_PERMISSIVE	cpu_to_be16(0xFFFF)
 
 enum ib_ah_flags {
 	IB_AH_GRH	= 1

commit 8d8bb39b9eba32dd70e87fd5ad5c5dd4ba118e06
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Jul 25 19:44:49 2008 -0700

    dma-mapping: add the device argument to dma_mapping_error()
    
    Add per-device dma_mapping_ops support for CONFIG_X86_64 as POWER
    architecture does:
    
    This enables us to cleanly fix the Calgary IOMMU issue that some devices
    are not behind the IOMMU (http://lkml.org/lkml/2008/5/8/423).
    
    I think that per-device dma_mapping_ops support would be also helpful for
    KVM people to support PCI passthrough but Andi thinks that this makes it
    difficult to support the PCI passthrough (see the above thread).  So I
    CC'ed this to KVM camp.  Comments are appreciated.
    
    A pointer to dma_mapping_ops to struct dev_archdata is added.  If the
    pointer is non NULL, DMA operations in asm/dma-mapping.h use it.  If it's
    NULL, the system-wide dma_ops pointer is used as before.
    
    If it's useful for KVM people, I plan to implement a mechanism to register
    a hook called when a new pci (or dma capable) device is created (it works
    with hot plugging).  It enables IOMMUs to set up an appropriate
    dma_mapping_ops per device.
    
    The major obstacle is that dma_mapping_error doesn't take a pointer to the
    device unlike other DMA operations.  So x86 can't have dma_mapping_ops per
    device.  Note all the POWER IOMMUs use the same dma_mapping_error function
    so this is not a problem for POWER but x86 IOMMUs use different
    dma_mapping_error functions.
    
    The first patch adds the device argument to dma_mapping_error.  The patch
    is trivial but large since it touches lots of drivers and dma-mapping.h in
    all the architecture.
    
    This patch:
    
    dma_mapping_error() doesn't take a pointer to the device unlike other DMA
    operations.  So we can't have dma_mapping_ops per device.
    
    Note that POWER already has dma_mapping_ops per device but all the POWER
    IOMMUs use the same dma_mapping_error function.  x86 IOMMUs use device
    argument.
    
    [akpm@linux-foundation.org: fix sge]
    [akpm@linux-foundation.org: fix svc_rdma]
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: fix bnx2x]
    [akpm@linux-foundation.org: fix s2io]
    [akpm@linux-foundation.org: fix pasemi_mac]
    [akpm@linux-foundation.org: fix sdhci]
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: fix sparc]
    [akpm@linux-foundation.org: fix ibmvscsi]
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Muli Ben-Yehuda <muli@il.ibm.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 90b529f7a154..936e333e7ce5 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1590,7 +1590,7 @@ static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->mapping_error(dev, dma_addr);
-	return dma_mapping_error(dma_addr);
+	return dma_mapping_error(dev->dma_device, dma_addr);
 }
 
 /**

commit 96f15c03532282366364ecfd20f04e49b5d96f3a
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jul 14 23:48:53 2008 -0700

    RDMA/core: Add local DMA L_Key support
    
    - Change the IB_DEVICE_ZERO_STAG flag to the transport-neutral name
      IB_DEVICE_LOCAL_DMA_LKEY, which is used by iWARP RNICs to indicate 0
      STag support and IB HCAs to indicate reserved L_Key support.
    
    - Add a u32 local_dma_lkey member to struct ib_device.  Drivers fill
      this in with the appropriate local DMA L_Key (if they support it).
    
    - Fix up the drivers using this flag.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 07b41e05565a..90b529f7a154 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -91,7 +91,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_RC_RNR_NAK_GEN	= (1<<12),
 	IB_DEVICE_SRQ_RESIZE		= (1<<13),
 	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
-	IB_DEVICE_ZERO_STAG		= (1<<15),
+	IB_DEVICE_LOCAL_DMA_LKEY	= (1<<15),
 	IB_DEVICE_RESERVED		= (1<<16), /* old SEND_W_INV */
 	IB_DEVICE_MEM_WINDOW		= (1<<17),
 	/*
@@ -1149,6 +1149,7 @@ struct ib_device {
 
 	char			     node_desc[64];
 	__be64			     node_guid;
+	u32			     local_dma_lkey;
 	u8                           node_type;
 	u8                           phys_port_cnt;
 };

commit 47ee1b9f2e7bf73950602efe0b74fa1a8481f222
Author: Ron Livne <ronli@voltaire.com>
Date:   Mon Jul 14 23:48:48 2008 -0700

    IB/core: Add support for multicast loopback blocking
    
    This patch also adds a creation flag for QPs,
    IB_QP_CREATE_MULTICAST_BLOCK_LOOPBACK, which when set means that
    multicast sends from the QP to a group that the QP is attached to will
    not be looped back to the QP's receive queue.  This can be used to
    save receive resources when a consumer does not want a local copy of
    multicast traffic; for example IPoIB must waste CPU time throwing away
    such local copies of multicast traffic.
    
    This patch also adds a device capability flag that shows whether a
    device supports this feature or not.
    
    Signed-off-by: Ron Livne <ronli@voltaire.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 10ebaaae0161..07b41e05565a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -104,6 +104,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
 	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
+	IB_DEVICE_BLOCK_MULTICAST_LOOPBACK = (1<<22),
 };
 
 enum ib_atomic_cap {
@@ -555,7 +556,8 @@ enum ib_qp_type {
 };
 
 enum ib_qp_create_flags {
-	IB_QP_CREATE_IPOIB_UD_LSO	= 1 << 0,
+	IB_QP_CREATE_IPOIB_UD_LSO		= 1 << 0,
+	IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK	= 1 << 1,
 };
 
 struct ib_qp_init_attr {

commit 7f624d023b5fb150831e02c1e4c0f2619ade72c2
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jul 14 23:48:48 2008 -0700

    RDMA/core: Add iWARP protocol statistics attributes in sysfs
    
    This patch adds a sysfs attribute group called "proto_stats" under
    /sys/class/infiniband/$device/ and populates this group with protocol
    statistics if they exist for a given device.  Currently, only iWARP
    stats are defined, but the code is designed to allow InfiniBand
    protocol stats if they become available.  These stats are per-device
    and more importantly -not- per port.
    
    Details:
    
    - Add union rdma_protocol_stats in ib_verbs.h.  This union allows
      defining transport-specific stats.  Currently only iwarp stats are
      defined.
    
    - Add struct iw_protocol_stats to define the current set of iwarp
      protocol stats.
    
    - Add new ib_device method called get_proto_stats() to return protocol
      statistics.
    
    - Add logic in core/sysfs.c to create iwarp protocol stats attributes
      if the device is an RNIC and has a get_proto_stats() method.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 74c24b908908..10ebaaae0161 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -226,6 +226,57 @@ static inline int ib_width_enum_to_int(enum ib_port_width width)
 	}
 }
 
+struct ib_protocol_stats {
+	/* TBD... */
+};
+
+struct iw_protocol_stats {
+	u64	ipInReceives;
+	u64	ipInHdrErrors;
+	u64	ipInTooBigErrors;
+	u64	ipInNoRoutes;
+	u64	ipInAddrErrors;
+	u64	ipInUnknownProtos;
+	u64	ipInTruncatedPkts;
+	u64	ipInDiscards;
+	u64	ipInDelivers;
+	u64	ipOutForwDatagrams;
+	u64	ipOutRequests;
+	u64	ipOutDiscards;
+	u64	ipOutNoRoutes;
+	u64	ipReasmTimeout;
+	u64	ipReasmReqds;
+	u64	ipReasmOKs;
+	u64	ipReasmFails;
+	u64	ipFragOKs;
+	u64	ipFragFails;
+	u64	ipFragCreates;
+	u64	ipInMcastPkts;
+	u64	ipOutMcastPkts;
+	u64	ipInBcastPkts;
+	u64	ipOutBcastPkts;
+
+	u64	tcpRtoAlgorithm;
+	u64	tcpRtoMin;
+	u64	tcpRtoMax;
+	u64	tcpMaxConn;
+	u64	tcpActiveOpens;
+	u64	tcpPassiveOpens;
+	u64	tcpAttemptFails;
+	u64	tcpEstabResets;
+	u64	tcpCurrEstab;
+	u64	tcpInSegs;
+	u64	tcpOutSegs;
+	u64	tcpRetransSegs;
+	u64	tcpInErrs;
+	u64	tcpOutRsts;
+};
+
+union rdma_protocol_stats {
+	struct ib_protocol_stats	ib;
+	struct iw_protocol_stats	iw;
+};
+
 struct ib_port_attr {
 	enum ib_port_state	state;
 	enum ib_mtu		max_mtu;
@@ -943,6 +994,8 @@ struct ib_device {
 
 	struct iw_cm_verbs	     *iwcm;
 
+	int		           (*get_protocol_stats)(struct ib_device *device,
+							 union rdma_protocol_stats *stats);
 	int		           (*query_device)(struct ib_device *device,
 						   struct ib_device_attr *device_attr);
 	int		           (*query_port)(struct ib_device *device,

commit 00f7ec36c9324928e4cd23f02e6d8550f30c32ca
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Jul 14 23:48:45 2008 -0700

    RDMA/core: Add memory management extensions support
    
    This patch adds support for the IB "base memory management extension"
    (BMME) and the equivalent iWARP operations (which the iWARP verbs
    mandates all devices must implement).  The new operations are:
    
     - Allocate an ib_mr for use in fast register work requests.
    
     - Allocate/free a physical buffer lists for use in fast register work
       requests.  This allows device drivers to allocate this memory as
       needed for use in posting send requests (eg via dma_alloc_coherent).
    
     - New send queue work requests:
       * send with remote invalidate
       * fast register memory region
       * local invalidate memory region
       * RDMA read with invalidate local memory region (iWARP only)
    
    Consumer interface details:
    
     - A new device capability flag IB_DEVICE_MEM_MGT_EXTENSIONS is added
       to indicate device support for these features.
    
     - New send work request opcodes IB_WR_FAST_REG_MR, IB_WR_LOCAL_INV,
       IB_WR_RDMA_READ_WITH_INV are added.
    
     - A new consumer API function, ib_alloc_mr() is added to allocate
       fast register memory regions.
    
     - New consumer API functions, ib_alloc_fast_reg_page_list() and
       ib_free_fast_reg_page_list() are added to allocate and free
       device-specific memory for fast registration page lists.
    
     - A new consumer API function, ib_update_fast_reg_key(), is added to
       allow the key portion of the R_Key and L_Key of a fast registration
       MR to be updated.  Consumers call this if desired before posting
       a IB_WR_FAST_REG_MR work request.
    
    Consumers can use this as follows:
    
     - MR is allocated with ib_alloc_mr().
    
     - Page list memory is allocated with ib_alloc_fast_reg_page_list().
    
     - MR R_Key/L_Key "key" field is updated with ib_update_fast_reg_key().
    
     - MR made VALID and bound to a specific page list via
       ib_post_send(IB_WR_FAST_REG_MR)
    
     - MR made INVALID via ib_post_send(IB_WR_LOCAL_INV),
       ib_post_send(IB_WR_RDMA_READ_WITH_INV) or an incoming send with
       invalidate operation.
    
     - MR is deallocated with ib_dereg_mr()
    
     - page lists dealloced via ib_free_fast_reg_page_list().
    
    Applications can allocate a fast register MR once, and then can
    repeatedly bind the MR to different physical block lists (PBLs) via
    posting work requests to a send queue (SQ).  For each outstanding
    MR-to-PBL binding in the SQ pipe, a fast_reg_page_list needs to be
    allocated (the fast_reg_page_list is owned by the low-level driver
    from the consumer posting a work request until the request completes).
    Thus pipelining can be achieved while still allowing device-specific
    page_list processing.
    
    The 32-bit fast register memory key/STag is composed of a 24-bit index
    and an 8-bit key.  The application can change the key each time it
    fast registers thus allowing more control over the peer's use of the
    key/STag (ie it can effectively be changed each time the rkey is
    rebound to a page list).
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5f5621bf70bd..74c24b908908 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -103,6 +103,7 @@ enum ib_device_cap_flags {
 	 */
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
+	IB_DEVICE_MEM_MGT_EXTENSIONS	= (1<<21),
 };
 
 enum ib_atomic_cap {
@@ -148,6 +149,7 @@ struct ib_device_attr {
 	int			max_srq;
 	int			max_srq_wr;
 	int			max_srq_sge;
+	unsigned int		max_fast_reg_page_list_len;
 	u16			max_pkeys;
 	u8			local_ca_ack_delay;
 };
@@ -411,6 +413,8 @@ enum ib_wc_opcode {
 	IB_WC_FETCH_ADD,
 	IB_WC_BIND_MW,
 	IB_WC_LSO,
+	IB_WC_LOCAL_INV,
+	IB_WC_FAST_REG_MR,
 /*
  * Set value of IB_WC_RECV so consumers can test if a completion is a
  * receive by testing (opcode & IB_WC_RECV).
@@ -421,7 +425,8 @@ enum ib_wc_opcode {
 
 enum ib_wc_flags {
 	IB_WC_GRH		= 1,
-	IB_WC_WITH_IMM		= (1<<1)
+	IB_WC_WITH_IMM		= (1<<1),
+	IB_WC_WITH_INVALIDATE	= (1<<2),
 };
 
 struct ib_wc {
@@ -431,7 +436,10 @@ struct ib_wc {
 	u32			vendor_err;
 	u32			byte_len;
 	struct ib_qp	       *qp;
-	__be32			imm_data;
+	union {
+		__be32		imm_data;
+		u32		invalidate_rkey;
+	} ex;
 	u32			src_qp;
 	int			wc_flags;
 	u16			pkey_index;
@@ -625,6 +633,9 @@ enum ib_wr_opcode {
 	IB_WR_ATOMIC_FETCH_AND_ADD,
 	IB_WR_LSO,
 	IB_WR_SEND_WITH_INV,
+	IB_WR_RDMA_READ_WITH_INV,
+	IB_WR_LOCAL_INV,
+	IB_WR_FAST_REG_MR,
 };
 
 enum ib_send_flags {
@@ -641,6 +652,12 @@ struct ib_sge {
 	u32	lkey;
 };
 
+struct ib_fast_reg_page_list {
+	struct ib_device       *device;
+	u64		       *page_list;
+	unsigned int		max_page_list_len;
+};
+
 struct ib_send_wr {
 	struct ib_send_wr      *next;
 	u64			wr_id;
@@ -673,6 +690,15 @@ struct ib_send_wr {
 			u16	pkey_index; /* valid for GSI only */
 			u8	port_num;   /* valid for DR SMPs on switch only */
 		} ud;
+		struct {
+			u64				iova_start;
+			struct ib_fast_reg_page_list   *page_list;
+			unsigned int			page_shift;
+			unsigned int			page_list_len;
+			u32				length;
+			int				access_flags;
+			u32				rkey;
+		} fast_reg;
 	} wr;
 };
 
@@ -1011,6 +1037,11 @@ struct ib_device {
 	int                        (*query_mr)(struct ib_mr *mr,
 					       struct ib_mr_attr *mr_attr);
 	int                        (*dereg_mr)(struct ib_mr *mr);
+	struct ib_mr *		   (*alloc_fast_reg_mr)(struct ib_pd *pd,
+					       int max_page_list_len);
+	struct ib_fast_reg_page_list * (*alloc_fast_reg_page_list)(struct ib_device *device,
+								   int page_list_len);
+	void			   (*free_fast_reg_page_list)(struct ib_fast_reg_page_list *page_list);
 	int                        (*rereg_phys_mr)(struct ib_mr *mr,
 						    int mr_rereg_mask,
 						    struct ib_pd *pd,
@@ -1804,6 +1835,54 @@ int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
  */
 int ib_dereg_mr(struct ib_mr *mr);
 
+/**
+ * ib_alloc_fast_reg_mr - Allocates memory region usable with the
+ *   IB_WR_FAST_REG_MR send work request.
+ * @pd: The protection domain associated with the region.
+ * @max_page_list_len: requested max physical buffer list length to be
+ *   used with fast register work requests for this MR.
+ */
+struct ib_mr *ib_alloc_fast_reg_mr(struct ib_pd *pd, int max_page_list_len);
+
+/**
+ * ib_alloc_fast_reg_page_list - Allocates a page list array
+ * @device - ib device pointer.
+ * @page_list_len - size of the page list array to be allocated.
+ *
+ * This allocates and returns a struct ib_fast_reg_page_list * and a
+ * page_list array that is at least page_list_len in size.  The actual
+ * size is returned in max_page_list_len.  The caller is responsible
+ * for initializing the contents of the page_list array before posting
+ * a send work request with the IB_WC_FAST_REG_MR opcode.
+ *
+ * The page_list array entries must be translated using one of the
+ * ib_dma_*() functions just like the addresses passed to
+ * ib_map_phys_fmr().  Once the ib_post_send() is issued, the struct
+ * ib_fast_reg_page_list must not be modified by the caller until the
+ * IB_WC_FAST_REG_MR work request completes.
+ */
+struct ib_fast_reg_page_list *ib_alloc_fast_reg_page_list(
+				struct ib_device *device, int page_list_len);
+
+/**
+ * ib_free_fast_reg_page_list - Deallocates a previously allocated
+ *   page list array.
+ * @page_list - struct ib_fast_reg_page_list pointer to be deallocated.
+ */
+void ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);
+
+/**
+ * ib_update_fast_reg_key - updates the key portion of the fast_reg MR
+ *   R_Key and L_Key.
+ * @mr - struct ib_mr pointer to be updated.
+ * @newkey - new key to be used.
+ */
+static inline void ib_update_fast_reg_key(struct ib_mr *mr, u8 newkey)
+{
+	mr->lkey = (mr->lkey & 0xffffff00) | newkey;
+	mr->rkey = (mr->rkey & 0xffffff00) | newkey;
+}
+
 /**
  * ib_alloc_mw - Allocates a memory window.
  * @pd: The protection domain associated with the memory window.

commit 4deccd6d95f1f1536dad3c842e39c1ace577329d
Author: Dotan Barak <dotanba@gmail.com>
Date:   Mon Jul 14 23:48:44 2008 -0700

    RDMA: Improve include file coding style
    
    Remove subversion $Id lines and improve readability by fixing other
    coding style problems pointed out by checkpatch.pl.
    
    Signed-off-by: Dotan Barak <dotanba@gmail.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 31d30b1852e8..5f5621bf70bd 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -34,8 +34,6 @@
  * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
  * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  * SOFTWARE.
- *
- * $Id: ib_verbs.h 1349 2004-12-16 21:09:43Z roland $
  */
 
 #if !defined(IB_VERBS_H)
@@ -777,7 +775,7 @@ struct ib_cq {
 	struct ib_uobject      *uobject;
 	ib_comp_handler   	comp_handler;
 	void                  (*event_handler)(struct ib_event *, void *);
-	void *            	cq_context;
+	void                   *cq_context;
 	int               	cqe;
 	atomic_t          	usecnt; /* count number of work queues */
 };
@@ -883,7 +881,7 @@ struct ib_dma_mapping_ops {
 	void		(*sync_single_for_cpu)(struct ib_device *dev,
 					       u64 dma_handle,
 					       size_t size,
-				               enum dma_data_direction dir);
+					       enum dma_data_direction dir);
 	void		(*sync_single_for_device)(struct ib_device *dev,
 						  u64 dma_handle,
 						  size_t size,

commit 4c0283fc561d79a4f94ab48ec37282e15273d1f8
Author: Roland Dreier <rolandd@cisco.com>
Date:   Mon Jun 9 09:58:42 2008 -0700

    IB/core: Remove IB_DEVICE_SEND_W_INV capability flag
    
    In 2.6.26, we added some support for send with invalidate work
    requests, including a device capability flag to indicate whether a
    device supports such requests.  However, the support was incomplete:
    the completion structure was not extended with a field for the key
    contained in incoming send with invalidate requests.
    
    Full support for memory management extensions (send with invalidate,
    local invalidate, fast register through a send queue, etc) is planned
    for 2.6.27.  Since send with invalidate is not very useful by itself,
    just remove the IB_DEVICE_SEND_W_INV bit before the 2.6.26 final
    release; we will add an IB_DEVICE_MEM_MGT_EXTENSIONS bit in 2.6.27,
    which makes things simpler for applications, since they will not have
    quite as confusing an array of fine-grained bits to check.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 911a661b7278..31d30b1852e8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -105,7 +105,6 @@ enum ib_device_cap_flags {
 	 */
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
-	IB_DEVICE_SEND_W_INV		= (1<<21),
 };
 
 enum ib_atomic_cap {

commit cb9fbc5c37b69ac584e61d449cfd590f5ae1f90d
Author: Arthur Kepner <akepner@sgi.com>
Date:   Tue Apr 29 01:00:34 2008 -0700

    IB: expand ib_umem_get() prototype
    
    Add a new parameter, dmasync, to the ib_umem_get() prototype.  Use dmasync = 1
    when mapping user-allocated CQs with ib_umem_get().
    
    Signed-off-by: Arthur Kepner <akepner@sgi.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Grundler <grundler@parisc-linux.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2dcbecce3f61..911a661b7278 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1542,6 +1542,24 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 		dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
+static inline u64 ib_dma_map_single_attrs(struct ib_device *dev,
+					  void *cpu_addr, size_t size,
+					  enum dma_data_direction direction,
+					  struct dma_attrs *attrs)
+{
+	return dma_map_single_attrs(dev->dma_device, cpu_addr, size,
+				    direction, attrs);
+}
+
+static inline void ib_dma_unmap_single_attrs(struct ib_device *dev,
+					     u64 addr, size_t size,
+					     enum dma_data_direction direction,
+					     struct dma_attrs *attrs)
+{
+	return dma_unmap_single_attrs(dev->dma_device, addr, size,
+				      direction, attrs);
+}
+
 /**
  * ib_dma_map_page - Map a physical page to DMA address
  * @dev: The device for which the dma_addr is to be created
@@ -1611,6 +1629,21 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 		dma_unmap_sg(dev->dma_device, sg, nents, direction);
 }
 
+static inline int ib_dma_map_sg_attrs(struct ib_device *dev,
+				      struct scatterlist *sg, int nents,
+				      enum dma_data_direction direction,
+				      struct dma_attrs *attrs)
+{
+	return dma_map_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
+}
+
+static inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,
+					 struct scatterlist *sg, int nents,
+					 enum dma_data_direction direction,
+					 struct dma_attrs *attrs)
+{
+	dma_unmap_sg_attrs(dev->dma_device, sg, nents, direction, attrs);
+}
 /**
  * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
  * @dev: The device for which the DMA addresses were created

commit f4e91eb4a81559da87a3843758a641b5cc590b65
Author: Tony Jones <tonyj@suse.de>
Date:   Fri Feb 22 00:13:36 2008 +0100

    IB: convert struct class_device to struct device
    
    This converts the main ib_device to use struct device instead of struct
    class_device as class_device is going away.
    
    Signed-off-by: Tony Jones <tonyj@suse.de>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Roland Dreier <rolandd@cisco.com>
    Cc: Sean Hefty <sean.hefty@intel.com>
    Cc: Hal Rosenstock <hal.rosenstock@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 95bf4bac44cb..2dcbecce3f61 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1051,7 +1051,7 @@ struct ib_device {
 	struct ib_dma_mapping_ops   *dma_ops;
 
 	struct module               *owner;
-	struct class_device          class_dev;
+	struct device                dev;
 	struct kobject               *ports_parent;
 	struct list_head             port_list;
 

commit 2dd5716227878d5950988514a2cbabf72f7fc888
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Wed Apr 16 21:09:33 2008 -0700

    IB/core: Add support for modify CQ
    
    Add support for modifying CQ parameters for controlling event
    generation moderation.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c48f6af5ef9a..95bf4bac44cb 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -987,6 +987,8 @@ struct ib_device {
 						int comp_vector,
 						struct ib_ucontext *context,
 						struct ib_udata *udata);
+	int                        (*modify_cq)(struct ib_cq *cq, u16 cq_count,
+						u16 cq_period);
 	int                        (*destroy_cq)(struct ib_cq *cq);
 	int                        (*resize_cq)(struct ib_cq *cq, int cqe,
 						struct ib_udata *udata);
@@ -1391,6 +1393,15 @@ struct ib_cq *ib_create_cq(struct ib_device *device,
  */
 int ib_resize_cq(struct ib_cq *cq, int cqe);
 
+/**
+ * ib_modify_cq - Modifies moderation params of the CQ
+ * @cq: The CQ to modify.
+ * @cq_count: number of CQEs that will trigger an event
+ * @cq_period: max period of time in usec before triggering an event
+ *
+ */
+int ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
+
 /**
  * ib_destroy_cq - Destroys the specified CQ.
  * @cq: The CQ to destroy.

commit 0f39cf3d54e67a705773fd0ec56ca3dcd3e9272f
Author: Roland Dreier <rolandd@cisco.com>
Date:   Wed Apr 16 21:09:32 2008 -0700

    IB/core: Add support for "send with invalidate" work requests
    
    Add a new IB_WR_SEND_WITH_INV send opcode that can be used to mark a
    "send with invalidate" work request as defined in the iWARP verbs and
    the InfiniBand base memory management extensions.  Also put "imm_data"
    and a new "invalidate_rkey" member in a new "ex" union in struct
    ib_send_wr. The invalidate_rkey member can be used to pass in an
    R_Key/STag to be invalidated.  Add this new union to struct
    ib_uverbs_send_wr.  Add code to copy the invalidate_rkey field in
    ib_uverbs_post_send().
    
    Fix up low-level drivers to deal with the change to struct ib_send_wr,
    and just remove the imm_data initialization from net/sunrpc/xprtrdma/,
    since that code never does any send with immediate operations.
    
    Also, move the existing IB_DEVICE_SEND_W_INV flag to a new bit, since
    the iWARP drivers currently in the tree set the bit.  The amso1100
    driver at least will silently fail to honor the IB_SEND_INVALIDATE bit
    if passed in as part of userspace send requests (since it does not
    implement kernel bypass work request queueing).  Remove the flag from
    all existing drivers that set it until we know which ones are OK.
    
    The values chosen for the new flag is not consecutive to avoid clashing
    with flags defined in the XRC patches, which are not merged yet but
    which are already in use and are likely to be merged soon.
    
    This resurrects a patch sent long ago by Mikkel Hagen <mhagen@iol.unh.edu>.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 66928e9cab19..c48f6af5ef9a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -94,7 +94,7 @@ enum ib_device_cap_flags {
 	IB_DEVICE_SRQ_RESIZE		= (1<<13),
 	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
 	IB_DEVICE_ZERO_STAG		= (1<<15),
-	IB_DEVICE_SEND_W_INV		= (1<<16),
+	IB_DEVICE_RESERVED		= (1<<16), /* old SEND_W_INV */
 	IB_DEVICE_MEM_WINDOW		= (1<<17),
 	/*
 	 * Devices should set IB_DEVICE_UD_IP_SUM if they support
@@ -105,6 +105,7 @@ enum ib_device_cap_flags {
 	 */
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 	IB_DEVICE_UD_TSO		= (1<<19),
+	IB_DEVICE_SEND_W_INV		= (1<<21),
 };
 
 enum ib_atomic_cap {
@@ -625,7 +626,8 @@ enum ib_wr_opcode {
 	IB_WR_RDMA_READ,
 	IB_WR_ATOMIC_CMP_AND_SWP,
 	IB_WR_ATOMIC_FETCH_AND_ADD,
-	IB_WR_LSO
+	IB_WR_LSO,
+	IB_WR_SEND_WITH_INV,
 };
 
 enum ib_send_flags {
@@ -649,7 +651,10 @@ struct ib_send_wr {
 	int			num_sge;
 	enum ib_wr_opcode	opcode;
 	int			send_flags;
-	__be32			imm_data;
+	union {
+		__be32		imm_data;
+		u32		invalidate_rkey;
+	} ex;
 	union {
 		struct {
 			u64	remote_addr;

commit c93570f23a98c633570397aedc6d1808f5d5846a
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Wed Apr 16 21:09:27 2008 -0700

    IB/core: Add IPoIB UD LSO support
    
    LSO (large send offload) allows the networking stack to pass SKBs with
    data size larger than the MTU to the IPoIB driver and have the HCA HW
    fragment the data to multiple MSS-sized packets.  Add a device
    capability flag IB_DEVICE_UD_TSO for devices that can perform TCP
    segmentation offload, a new send work request opcode IB_WR_LSO,
    header, hlen and mss fields for the work request structure, and a new
    IB_WC_LSO completion type.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c3299be41c6f..66928e9cab19 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -104,6 +104,7 @@ enum ib_device_cap_flags {
 	 * IPoIB driver may set NETIF_F_IP_CSUM for datagram mode.
 	 */
 	IB_DEVICE_UD_IP_CSUM		= (1<<18),
+	IB_DEVICE_UD_TSO		= (1<<19),
 };
 
 enum ib_atomic_cap {
@@ -411,6 +412,7 @@ enum ib_wc_opcode {
 	IB_WC_COMP_SWAP,
 	IB_WC_FETCH_ADD,
 	IB_WC_BIND_MW,
+	IB_WC_LSO,
 /*
  * Set value of IB_WC_RECV so consumers can test if a completion is a
  * receive by testing (opcode & IB_WC_RECV).
@@ -622,7 +624,8 @@ enum ib_wr_opcode {
 	IB_WR_SEND_WITH_IMM,
 	IB_WR_RDMA_READ,
 	IB_WR_ATOMIC_CMP_AND_SWP,
-	IB_WR_ATOMIC_FETCH_AND_ADD
+	IB_WR_ATOMIC_FETCH_AND_ADD,
+	IB_WR_LSO
 };
 
 enum ib_send_flags {
@@ -660,6 +663,9 @@ struct ib_send_wr {
 		} atomic;
 		struct {
 			struct ib_ah *ah;
+			void   *header;
+			int     hlen;
+			int     mss;
 			u32	remote_qpn;
 			u32	remote_qkey;
 			u16	pkey_index; /* valid for GSI only */

commit b846f25aa2a353355aec5202fe4dbdc6674dfc64
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Wed Apr 16 21:09:27 2008 -0700

    IB/core: Add creation flags to struct ib_qp_init_attr
    
    Add a create_flags member to struct ib_qp_init_attr that will allow a
    kernel verbs consumer to create a pass special flags when creating a QP.
    Add a flag value for telling low-level drivers that a QP will be used
    for IPoIB UD LSO.  The create_flags member will also be useful for XRC
    and ehca low-latency QP support.
    
    Since no create_flags handling is implemented yet, add code to all
    low-level drivers to return -EINVAL if create_flags is non-zero.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 40ff51244d19..c3299be41c6f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -495,6 +495,10 @@ enum ib_qp_type {
 	IB_QPT_RAW_ETY
 };
 
+enum ib_qp_create_flags {
+	IB_QP_CREATE_IPOIB_UD_LSO	= 1 << 0,
+};
+
 struct ib_qp_init_attr {
 	void                  (*event_handler)(struct ib_event *, void *);
 	void		       *qp_context;
@@ -504,6 +508,7 @@ struct ib_qp_init_attr {
 	struct ib_qp_cap	cap;
 	enum ib_sig_type	sq_sig_type;
 	enum ib_qp_type		qp_type;
+	enum ib_qp_create_flags	create_flags;
 	u8			port_num; /* special QP types only */
 };
 

commit b3d636b0d1b2eb870a55ae196b8f3838e1399554
Author: Roland Dreier <rolandd@cisco.com>
Date:   Wed Apr 16 21:01:06 2008 -0700

    IB: Make struct ib_uobject.id a signed int
    
    IDR IDs are signed, so struct ib_uobject.id should be signed.  This
    avoids some sparse pointer signedness warnings.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 701e7b40560a..40ff51244d19 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -730,7 +730,7 @@ struct ib_uobject {
 	struct ib_ucontext     *context;	/* associated user context */
 	void		       *object;		/* containing object */
 	struct list_head	list;		/* link to context's list */
-	u32			id;		/* index into kernel idr */
+	int			id;		/* index into kernel idr */
 	struct kref		ref;
 	struct rw_semaphore	mutex;		/* protects .live */
 	int			live;

commit 5128bdc97a1018aacac2550cf73bda61041cc3b8
Author: Roland Dreier <rolandd@cisco.com>
Date:   Fri Feb 8 14:47:26 2008 -0800

    IB/core: Remove unused struct ib_device.flags member
    
    Avoid confusion about what it might mean, since it's never initialized.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a5a7f9678ab8..701e7b40560a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -900,8 +900,6 @@ struct ib_device {
 	int                          *pkey_tbl_len;
 	int                          *gid_tbl_len;
 
-	u32                           flags;
-
 	int			      num_comp_vectors;
 
 	struct iw_cm_verbs	     *iwcm;

commit e0605d9199b462454f2f2e5ca01810255a6d5cfa
Author: Eli Cohen <eli@mellanox.co.il>
Date:   Wed Jan 30 18:30:57 2008 +0200

    IB/core: Add IP checksum offload support
    
    Add a device capability to show when it can handle checksum offload.
    Also add a send flag for inserting checksums and a csum_ok field to
    the completion record.
    
    Signed-off-by: Eli Cohen <eli@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index cfbd38fe2998..a5a7f9678ab8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -95,7 +95,15 @@ enum ib_device_cap_flags {
 	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
 	IB_DEVICE_ZERO_STAG		= (1<<15),
 	IB_DEVICE_SEND_W_INV		= (1<<16),
-	IB_DEVICE_MEM_WINDOW		= (1<<17)
+	IB_DEVICE_MEM_WINDOW		= (1<<17),
+	/*
+	 * Devices should set IB_DEVICE_UD_IP_SUM if they support
+	 * insertion of UDP and TCP checksum on outgoing UD IPoIB
+	 * messages and can verify the validity of checksum for
+	 * incoming messages.  Setting this flag implies that the
+	 * IPoIB driver may set NETIF_F_IP_CSUM for datagram mode.
+	 */
+	IB_DEVICE_UD_IP_CSUM		= (1<<18),
 };
 
 enum ib_atomic_cap {
@@ -431,6 +439,7 @@ struct ib_wc {
 	u8			sl;
 	u8			dlid_path_bits;
 	u8			port_num;	/* valid only for DR SMPs on switches */
+	int			csum_ok;
 };
 
 enum ib_cq_notify_flags {
@@ -615,7 +624,8 @@ enum ib_send_flags {
 	IB_SEND_FENCE		= 1,
 	IB_SEND_SIGNALED	= (1<<1),
 	IB_SEND_SOLICITED	= (1<<2),
-	IB_SEND_INLINE		= (1<<3)
+	IB_SEND_INLINE		= (1<<3),
+	IB_SEND_IP_CSUM		= (1<<4)
 };
 
 struct ib_sge {

commit 35be0681983752116a8161ad3b30e830963108a4
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Dec 17 15:54:39 2007 -0400

    Kobject: change drivers/infiniband to use kobject_init_and_add
    
    Stop using kobject_register, as this way we can control the sending of
    the uevent properly, after everything is properly initialized.
    
    Cc: Roland Dreier <rolandd@cisco.com>
    Cc: Sean Hefty <mshefty@ichips.intel.com>
    Cc: Hal Rosenstock <hal.rosenstock@gmail.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 11f39606e7d9..cfbd38fe2998 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1026,7 +1026,7 @@ struct ib_device {
 
 	struct module               *owner;
 	struct class_device          class_dev;
-	struct kobject               ports_parent;
+	struct kobject               *ports_parent;
 	struct list_head             port_list;
 
 	enum {

commit 87ae9afdcada236d0a1b38ce2c465a65916961dc
Author: Adrian Bunk <bunk@kernel.org>
Date:   Tue Oct 30 10:35:04 2007 +0100

    cleanup asm/scatterlist.h includes
    
    Not architecture specific code should not #include <asm/scatterlist.h>.
    
    This patch therefore either replaces them with
    #include <linux/scatterlist.h> or simply removes them if they were
    unused.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 4bea182d7116..11f39606e7d9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -48,9 +48,9 @@
 #include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/rwsem.h>
+#include <linux/scatterlist.h>
 
 #include <asm/atomic.h>
-#include <asm/scatterlist.h>
 #include <asm/uaccess.h>
 
 union ib_gid {

commit 92ddc447ce7382e36b72a240697c00bf4beb8d75
Author: Dotan Barak <dotanb@dev.mellanox.co.il>
Date:   Wed Aug 1 13:33:56 2007 +0300

    IB: Move the macro IB_UMEM_MAX_PAGE_CHUNK() to umem.c
    
    After moving the definition of struct ib_umem_chunk from ib_verbs.h to
    ib_umem.h there isn't any reason for the macro IB_UMEM_MAX_PAGE_CHUNK
    to stay in ib_verbs.h.  Move the macro to umem.c, the only place where
    it is used.
    
    Signed-off-by: Dotan Barak <dotanb@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7a99f1125d24..4bea182d7116 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -733,11 +733,6 @@ struct ib_udata {
 	size_t       outlen;
 };
 
-#define IB_UMEM_MAX_PAGE_CHUNK						\
-	((PAGE_SIZE - offsetof(struct ib_umem_chunk, page_list)) /	\
-	 ((void *) &((struct ib_umem_chunk *) 0)->page_list[1] -	\
-	  (void *) &((struct ib_umem_chunk *) 0)->page_list[0]))
-
 struct ib_pd {
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;

commit bfb3ea125174813cdf87b1120caf0c9bd580283e
Author: Dotan Barak <dotanb@dev.mellanox.co.il>
Date:   Tue Jul 31 16:49:15 2007 +0300

    IB: Include <linux/list.h> and <linux/rwsem.h> from <rdma/ib_verbs.h>
    
    ib_verbs.h uses struct list_head and rw_semaphore, so while the files
    <linux/list.h> and <linux/rwsem.h> seem to be pulled in indirectly by
    the other header files it includes, the right thing is to include
    those files directly.
    
    Signed-off-by: Dotan Barak <dotanb@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0627a6aa282a..7a99f1125d24 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -46,6 +46,8 @@
 #include <linux/mm.h>
 #include <linux/dma-mapping.h>
 #include <linux/kref.h>
+#include <linux/list.h>
+#include <linux/rwsem.h>
 
 #include <asm/atomic.h>
 #include <asm/scatterlist.h>

commit 5eb620c81ce35aa0c533131bf4d06c4c8c2bfadf
Author: Yosef Etigin <yosefe@voltaire.com>
Date:   Mon May 14 07:26:51 2007 +0300

    IB/core: Add helpers for uncached GID and P_Key searches
    
    Add ib_find_gid() and ib_find_pkey() functions that use uncached device
    queries.  The calls might block but the returns are always up-to-date.
    Cache P_Key and GID table lengths in core to avoid extra port info queries.
    
    Signed-off-by: Yosef Etigin <yosefe@voltaire.com>
    Acked-by: Michael S. Tsirkin <mst@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 47cefca59c89..0627a6aa282a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -890,6 +890,8 @@ struct ib_device {
 	spinlock_t                    client_data_lock;
 
 	struct ib_cache               cache;
+	int                          *pkey_tbl_len;
+	int                          *gid_tbl_len;
 
 	u32                           flags;
 
@@ -1118,6 +1120,12 @@ int ib_modify_port(struct ib_device *device,
 		   u8 port_num, int port_modify_mask,
 		   struct ib_port_modify *port_modify);
 
+int ib_find_gid(struct ib_device *device, union ib_gid *gid,
+		u8 *port_num, u16 *index);
+
+int ib_find_pkey(struct ib_device *device,
+		 u8 port_num, u16 pkey, u16 *index);
+
 /**
  * ib_alloc_pd - Allocates an unused protection domain.
  * @device: The device on which to allocate the protection domain.

commit f7c6a7b5d59980b076abbf2ceeb8735591290285
Author: Roland Dreier <rolandd@cisco.com>
Date:   Sun Mar 4 16:15:11 2007 -0800

    IB/uverbs: Export ib_umem_get()/ib_umem_release() to modules
    
    Export ib_umem_get()/ib_umem_release() and put low-level drivers in
    control of when to call ib_umem_get() to pin and DMA map userspace,
    rather than always calling it in ib_uverbs_reg_mr() before calling the
    low-level driver's reg_user_mr method.
    
    Also move these functions to be in the ib_core module instead of
    ib_uverbs, so that driver modules using them do not depend on
    ib_uverbs.
    
    This has a number of advantages:
     - It is better design from the standpoint of making generic code a
       library that can be used or overridden by device-specific code as
       the details of specific devices dictate.
     - Drivers that do not need to pin userspace memory regions do not
       need to take the performance hit of calling ib_mem_get().  For
       example, although I have not tried to implement it in this patch,
       the ipath driver should be able to avoid pinning memory and just
       use copy_{to,from}_user() to access userspace memory regions.
     - Buffers that need special mapping treatment can be identified by
       the low-level driver.  For example, it may be possible to solve
       some Altix-specific memory ordering issues with mthca CQs in
       userspace by mapping CQ buffers with extra flags.
     - Drivers that need to pin and DMA map userspace memory for things
       other than memory regions can use ib_umem_get() directly, instead
       of hacks using extra parameters to their reg_phys_mr method.  For
       example, the mlx4 driver that is pending being merged needs to pin
       and DMA map QP and CQ buffers, but it does not need to create a
       memory key for these buffers.  So the cleanest solution is for mlx4
       to call ib_umem_get() in the create_qp and create_cq methods.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5342ac64ed1a..47cefca59c89 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -5,7 +5,7 @@
  * Copyright (c) 2004 Topspin Corporation.  All rights reserved.
  * Copyright (c) 2004 Voltaire Corporation.  All rights reserved.
  * Copyright (c) 2005 Sun Microsystems, Inc. All rights reserved.
- * Copyright (c) 2005, 2006 Cisco Systems.  All rights reserved.
+ * Copyright (c) 2005, 2006, 2007 Cisco Systems.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -710,6 +710,7 @@ struct ib_ucontext {
 	struct list_head	qp_list;
 	struct list_head	srq_list;
 	struct list_head	ah_list;
+	int			closing;
 };
 
 struct ib_uobject {
@@ -723,23 +724,6 @@ struct ib_uobject {
 	int			live;
 };
 
-struct ib_umem {
-	unsigned long		user_base;
-	unsigned long		virt_base;
-	size_t			length;
-	int			offset;
-	int			page_size;
-	int                     writable;
-	struct list_head	chunk_list;
-};
-
-struct ib_umem_chunk {
-	struct list_head	list;
-	int                     nents;
-	int                     nmap;
-	struct scatterlist      page_list[0];
-};
-
 struct ib_udata {
 	void __user *inbuf;
 	void __user *outbuf;
@@ -752,11 +736,6 @@ struct ib_udata {
 	 ((void *) &((struct ib_umem_chunk *) 0)->page_list[1] -	\
 	  (void *) &((struct ib_umem_chunk *) 0)->page_list[0]))
 
-struct ib_umem_object {
-	struct ib_uobject	uobject;
-	struct ib_umem		umem;
-};
-
 struct ib_pd {
 	struct ib_device       *device;
 	struct ib_uobject      *uobject;
@@ -1003,7 +982,8 @@ struct ib_device {
 						  int mr_access_flags,
 						  u64 *iova_start);
 	struct ib_mr *             (*reg_user_mr)(struct ib_pd *pd,
-						  struct ib_umem *region,
+						  u64 start, u64 length,
+						  u64 virt_addr,
 						  int mr_access_flags,
 						  struct ib_udata *udata);
 	int                        (*query_mr)(struct ib_mr *mr,

commit ed23a72778f3dbd465e55b06fe31629e7e1dd2f3
Author: Roland Dreier <rolandd@cisco.com>
Date:   Sun May 6 21:02:48 2007 -0700

    IB: Return "maybe missed event" hint from ib_req_notify_cq()
    
    The semantics defined by the InfiniBand specification say that
    completion events are only generated when a completions is added to a
    completion queue (CQ) after completion notification is requested.  In
    other words, this means that the following race is possible:
    
            while (CQ is not empty)
                    ib_poll_cq(CQ);
            // new completion is added after while loop is exited
            ib_req_notify_cq(CQ);
            // no event is generated for the existing completion
    
    To close this race, the IB spec recommends doing another poll of the
    CQ after requesting notification.
    
    However, it is not always possible to arrange code this way (for
    example, we have found that NAPI for IPoIB cannot poll after
    requesting notification).  Also, some hardware (eg Mellanox HCAs)
    actually will generate an event for completions added before the call
    to ib_req_notify_cq() -- which is allowed by the spec, since there's
    no way for any upper-layer consumer to know exactly when a completion
    was really added -- so the extra poll of the CQ is just a waste.
    
    Motivated by this, we add a new flag "IB_CQ_REPORT_MISSED_EVENTS" for
    ib_req_notify_cq() so that it can return a hint about whether the a
    completion may have been added before the request for notification.
    The return value of ib_req_notify_cq() is extended so:
    
             < 0    means an error occurred while requesting notification
            == 0    means notification was requested successfully, and if
                    IB_CQ_REPORT_MISSED_EVENTS was passed in, then no
                    events were missed and it is safe to wait for another
                    event.
             > 0    is only returned if IB_CQ_REPORT_MISSED_EVENTS was
                    passed in.  It means that the consumer must poll the
                    CQ again to make sure it is empty to avoid the race
                    described above.
    
    We add a flag to enable this behavior rather than turning it on
    unconditionally, because checking for missed events may incur
    significant overhead for some low-level drivers, and consumers that
    don't care about the results of this test shouldn't be forced to pay
    for the test.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 17cc309d03ef..5342ac64ed1a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -431,9 +431,11 @@ struct ib_wc {
 	u8			port_num;	/* valid only for DR SMPs on switches */
 };
 
-enum ib_cq_notify {
-	IB_CQ_SOLICITED,
-	IB_CQ_NEXT_COMP
+enum ib_cq_notify_flags {
+	IB_CQ_SOLICITED			= 1 << 0,
+	IB_CQ_NEXT_COMP			= 1 << 1,
+	IB_CQ_SOLICITED_MASK		= IB_CQ_SOLICITED | IB_CQ_NEXT_COMP,
+	IB_CQ_REPORT_MISSED_EVENTS	= 1 << 2,
 };
 
 enum ib_srq_attr_mask {
@@ -990,7 +992,7 @@ struct ib_device {
 					      struct ib_wc *wc);
 	int                        (*peek_cq)(struct ib_cq *cq, int wc_cnt);
 	int                        (*req_notify_cq)(struct ib_cq *cq,
-						    enum ib_cq_notify cq_notify);
+						    enum ib_cq_notify_flags flags);
 	int                        (*req_ncomp_notif)(struct ib_cq *cq,
 						      int wc_cnt);
 	struct ib_mr *             (*get_dma_mr)(struct ib_pd *pd,
@@ -1419,14 +1421,34 @@ int ib_peek_cq(struct ib_cq *cq, int wc_cnt);
 /**
  * ib_req_notify_cq - Request completion notification on a CQ.
  * @cq: The CQ to generate an event for.
- * @cq_notify: If set to %IB_CQ_SOLICITED, completion notification will
- *   occur on the next solicited event. If set to %IB_CQ_NEXT_COMP,
- *   notification will occur on the next completion.
+ * @flags:
+ *   Must contain exactly one of %IB_CQ_SOLICITED or %IB_CQ_NEXT_COMP
+ *   to request an event on the next solicited event or next work
+ *   completion at any type, respectively. %IB_CQ_REPORT_MISSED_EVENTS
+ *   may also be |ed in to request a hint about missed events, as
+ *   described below.
+ *
+ * Return Value:
+ *    < 0 means an error occurred while requesting notification
+ *   == 0 means notification was requested successfully, and if
+ *        IB_CQ_REPORT_MISSED_EVENTS was passed in, then no events
+ *        were missed and it is safe to wait for another event.  In
+ *        this case is it guaranteed that any work completions added
+ *        to the CQ since the last CQ poll will trigger a completion
+ *        notification event.
+ *    > 0 is only returned if IB_CQ_REPORT_MISSED_EVENTS was passed
+ *        in.  It means that the consumer must poll the CQ again to
+ *        make sure it is empty to avoid missing an event because of a
+ *        race between requesting notification and an entry being
+ *        added to the CQ.  This return value means it is possible
+ *        (but not guaranteed) that a work completion has been added
+ *        to the CQ since the last poll without triggering a
+ *        completion notification event.
  */
 static inline int ib_req_notify_cq(struct ib_cq *cq,
-				   enum ib_cq_notify cq_notify)
+				   enum ib_cq_notify_flags flags)
 {
-	return cq->device->req_notify_cq(cq, cq_notify);
+	return cq->device->req_notify_cq(cq, flags);
 }
 
 /**

commit f4fd0b224d60044d2da5ca02f8f2b5150c1d8731
Author: Michael S. Tsirkin <mst@dev.mellanox.co.il>
Date:   Thu May 3 13:48:47 2007 +0300

    IB: Add CQ comp_vector support
    
    Add a num_comp_vectors member to struct ib_device and extend
    ib_create_cq() to pass in a comp_vector parameter -- this parallels
    the userspace libibverbs API.  Update all hardware drivers to set
    num_comp_vectors to 1 and have all ULPs pass 0 for the comp_vector
    value.  Pass the value of num_comp_vectors to userspace rather than
    hard-coding a value of 1.
    
    We want multiple CQ event vector support (via MSI-X or similar for
    adapters that can generate multiple interrupts), but it's not clear
    how many vectors we want, or how we want to deal with policy issues
    such as how to decide which vector to use or how to set up interrupt
    affinity.  This patch is useful for experimenting, since no core
    changes will be necessary when updating a driver to support multiple
    vectors, and we know that we want to make at least these changes
    anyway.
    
    Signed-off-by: Michael S. Tsirkin <mst@dev.mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 765589f4d166..17cc309d03ef 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -912,6 +912,8 @@ struct ib_device {
 
 	u32                           flags;
 
+	int			      num_comp_vectors;
+
 	struct iw_cm_verbs	     *iwcm;
 
 	int		           (*query_device)(struct ib_device *device,
@@ -978,6 +980,7 @@ struct ib_device {
 						struct ib_recv_wr *recv_wr,
 						struct ib_recv_wr **bad_recv_wr);
 	struct ib_cq *             (*create_cq)(struct ib_device *device, int cqe,
+						int comp_vector,
 						struct ib_ucontext *context,
 						struct ib_udata *udata);
 	int                        (*destroy_cq)(struct ib_cq *cq);
@@ -1358,13 +1361,15 @@ static inline int ib_post_recv(struct ib_qp *qp,
  * @cq_context: Context associated with the CQ returned to the user via
  *   the associated completion and event handlers.
  * @cqe: The minimum size of the CQ.
+ * @comp_vector - Completion vector used to signal completion events.
+ *     Must be >= 0 and < context->num_comp_vectors.
  *
  * Users can examine the cq structure to determine the actual CQ size.
  */
 struct ib_cq *ib_create_cq(struct ib_device *device,
 			   ib_comp_handler comp_handler,
 			   void (*event_handler)(struct ib_event *, void *),
-			   void *cq_context, int cqe);
+			   void *cq_context, int cqe, int comp_vector);
 
 /**
  * ib_resize_cq - Modifies the capacity of the CQ.

commit 062dbb69f32b9ccea701b30f8cc0049482e6211f
Author: Michael S. Tsirkin <mst@mellanox.co.il>
Date:   Sun Dec 31 21:09:42 2006 +0200

    IB: Return qp pointer as part of ib_wc
    
    struct ib_wc currently only includes the local QP number: this matches
    the IB spec, but seems mostly useless. The following patch replaces
    this with the pointer to qp itself, and updates all low level drivers
    and all users.
    
    This has the following advantages:
    - Ability to get a per-qp context through wc->qp->qp_context
    - Existing drivers already have the qp pointer ready in poll cq, so
      this change actually saves a tiny bit (extra memory read) on data path
      (for ehca it would actually be expensive to find the QP pointer when
      polling a CQ, but ehca does not support SRQ so we can leave wc->qp as
      NULL for ehca)
    - Users that need the QP number can still get it through wc->qp->qp_num
    
    Use case:
    
    In IPoIB connected mode code, I have a common CQ shared by multiple
    QPs.  To track connection usage, I need a way to get at some per-QP
    context upon the completion, and I would like to avoid allocating
    context object per work request just to stick a QP pointer into it.
    With this code, I can just use wc->qp->qp_context.
    
    Signed-off-by: Michael S. Tsirkin <mst@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 73aafd7bb503..765589f4d166 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -420,8 +420,8 @@ struct ib_wc {
 	enum ib_wc_opcode	opcode;
 	u32			vendor_err;
 	u32			byte_len;
+	struct ib_qp	       *qp;
 	__be32			imm_data;
-	u32			qp_num;
 	u32			src_qp;
 	int			wc_flags;
 	u16			pkey_index;

commit 459d6e2a541a5226825db998e627e0aa046aa257
Author: Michael S. Tsirkin <mst@mellanox.co.il>
Date:   Sun Feb 4 14:11:55 2007 -0800

    IB: Include <linux/kref.h> explicitly in <rdma/ib_verbs.h>
    
    <rdma/ib_verbs.h> uses struct kref, so it should include <linux/kref.h>
    explicitly to avoid hidden include dependencies.
    
    Signed-off-by: Michael S. Tsirkin <mst@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 0bfa3328d686..73aafd7bb503 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -45,6 +45,7 @@
 #include <linux/device.h>
 #include <linux/mm.h>
 #include <linux/dma-mapping.h>
+#include <linux/kref.h>
 
 #include <asm/atomic.h>
 #include <asm/scatterlist.h>

commit c59a3da1342ff456e5123361739bc331446cda21
Author: Roland Dreier <rolandd@cisco.com>
Date:   Fri Dec 15 13:57:26 2006 -0800

    IB: Fix ib_dma_alloc_coherent() wrapper
    
    The ib_dma_alloc_coherent() wrapper uses a u64* for the dma_handle
    parameter, unlike dma_alloc_coherent, which uses dma_addr_t*.  This
    means that we need a temporary variable to handle the case when
    ib_dma_alloc_coherent() just falls through directly to
    dma_alloc_coherent() on architectures where sizeof u64 != sizeof
    dma_addr_t.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 3c2e10574b23..0bfa3328d686 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1639,7 +1639,14 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 {
 	if (dev->dma_ops)
 		return dev->dma_ops->alloc_coherent(dev, size, dma_handle, flag);
-	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
+	else {
+		dma_addr_t handle;
+		void *ret;
+
+		ret = dma_alloc_coherent(dev->dma_device, size, &handle, flag);
+		*dma_handle = handle;
+		return ret;
+	}
 }
 
 /**

commit d1998ef38a13c4e74c69df55ccd38b0440c429b2
Author: Ben Collins <ben.collins@ubuntu.com>
Date:   Wed Dec 13 22:10:05 2006 -0500

    [PATCH] ib_verbs: Use explicit if-else statements to avoid errors with do-while macros
    
    At least on PPC, the "op ? op : dma" construct causes a compile failure
    because the dma_* is a do{}while(0) macro.
    
    This turns all of them into proper if/else to avoid this problem.
    
    Signed-off-by: Ben Collins <bcollins@ubuntu.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index fd2353fa7e12..3c2e10574b23 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1456,9 +1456,9 @@ struct ib_mr *ib_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
  */
 static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->mapping_error(dev, dma_addr) :
-		dma_mapping_error(dma_addr);
+	if (dev->dma_ops)
+		return dev->dma_ops->mapping_error(dev, dma_addr);
+	return dma_mapping_error(dma_addr);
 }
 
 /**
@@ -1472,9 +1472,9 @@ static inline u64 ib_dma_map_single(struct ib_device *dev,
 				    void *cpu_addr, size_t size,
 				    enum dma_data_direction direction)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->map_single(dev, cpu_addr, size, direction) :
-		dma_map_single(dev->dma_device, cpu_addr, size, direction);
+	if (dev->dma_ops)
+		return dev->dma_ops->map_single(dev, cpu_addr, size, direction);
+	return dma_map_single(dev->dma_device, cpu_addr, size, direction);
 }
 
 /**
@@ -1488,8 +1488,9 @@ static inline void ib_dma_unmap_single(struct ib_device *dev,
 				       u64 addr, size_t size,
 				       enum dma_data_direction direction)
 {
-	dev->dma_ops ?
-		dev->dma_ops->unmap_single(dev, addr, size, direction) :
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_single(dev, addr, size, direction);
+	else
 		dma_unmap_single(dev->dma_device, addr, size, direction);
 }
 
@@ -1507,9 +1508,9 @@ static inline u64 ib_dma_map_page(struct ib_device *dev,
 				  size_t size,
 					 enum dma_data_direction direction)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->map_page(dev, page, offset, size, direction) :
-		dma_map_page(dev->dma_device, page, offset, size, direction);
+	if (dev->dma_ops)
+		return dev->dma_ops->map_page(dev, page, offset, size, direction);
+	return dma_map_page(dev->dma_device, page, offset, size, direction);
 }
 
 /**
@@ -1523,8 +1524,9 @@ static inline void ib_dma_unmap_page(struct ib_device *dev,
 				     u64 addr, size_t size,
 				     enum dma_data_direction direction)
 {
-	dev->dma_ops ?
-		dev->dma_ops->unmap_page(dev, addr, size, direction) :
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_page(dev, addr, size, direction);
+	else
 		dma_unmap_page(dev->dma_device, addr, size, direction);
 }
 
@@ -1539,9 +1541,9 @@ static inline int ib_dma_map_sg(struct ib_device *dev,
 				struct scatterlist *sg, int nents,
 				enum dma_data_direction direction)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->map_sg(dev, sg, nents, direction) :
-		dma_map_sg(dev->dma_device, sg, nents, direction);
+	if (dev->dma_ops)
+		return dev->dma_ops->map_sg(dev, sg, nents, direction);
+	return dma_map_sg(dev->dma_device, sg, nents, direction);
 }
 
 /**
@@ -1555,8 +1557,9 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 				   struct scatterlist *sg, int nents,
 				   enum dma_data_direction direction)
 {
-	dev->dma_ops ?
-		dev->dma_ops->unmap_sg(dev, sg, nents, direction) :
+	if (dev->dma_ops)
+		dev->dma_ops->unmap_sg(dev, sg, nents, direction);
+	else
 		dma_unmap_sg(dev->dma_device, sg, nents, direction);
 }
 
@@ -1568,8 +1571,9 @@ static inline void ib_dma_unmap_sg(struct ib_device *dev,
 static inline u64 ib_sg_dma_address(struct ib_device *dev,
 				    struct scatterlist *sg)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->dma_address(dev, sg) : sg_dma_address(sg);
+	if (dev->dma_ops)
+		return dev->dma_ops->dma_address(dev, sg);
+	return sg_dma_address(sg);
 }
 
 /**
@@ -1580,8 +1584,9 @@ static inline u64 ib_sg_dma_address(struct ib_device *dev,
 static inline unsigned int ib_sg_dma_len(struct ib_device *dev,
 					 struct scatterlist *sg)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->dma_len(dev, sg) : sg_dma_len(sg);
+	if (dev->dma_ops)
+		return dev->dma_ops->dma_len(dev, sg);
+	return sg_dma_len(sg);
 }
 
 /**
@@ -1596,8 +1601,9 @@ static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
 					      size_t size,
 					      enum dma_data_direction dir)
 {
-	dev->dma_ops ?
-		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir) :
+	if (dev->dma_ops)
+		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir);
+	else
 		dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
 }
 
@@ -1613,8 +1619,9 @@ static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
 						 size_t size,
 						 enum dma_data_direction dir)
 {
-	dev->dma_ops ?
-		dev->dma_ops->sync_single_for_device(dev, addr, size, dir) :
+	if (dev->dma_ops)
+		dev->dma_ops->sync_single_for_device(dev, addr, size, dir);
+	else
 		dma_sync_single_for_device(dev->dma_device, addr, size, dir);
 }
 
@@ -1630,9 +1637,9 @@ static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
 					   u64 *dma_handle,
 					   gfp_t flag)
 {
-	return dev->dma_ops ?
-		dev->dma_ops->alloc_coherent(dev, size, dma_handle, flag) :
-		dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
+	if (dev->dma_ops)
+		return dev->dma_ops->alloc_coherent(dev, size, dma_handle, flag);
+	return dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
 }
 
 /**
@@ -1646,8 +1653,9 @@ static inline void ib_dma_free_coherent(struct ib_device *dev,
 					size_t size, void *cpu_addr,
 					u64 dma_handle)
 {
-	dev->dma_ops ?
-		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle) :
+	if (dev->dma_ops)
+		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle);
+	else
 		dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
 }
 

commit 9b513090a3c5e4964f9ac09016c1586988abb3d5
Author: Ralph Campbell <ralph.campbell@qlogic.com>
Date:   Tue Dec 12 14:27:41 2006 -0800

    IB: Add DMA mapping functions to allow device drivers to interpose
    
    The QLogic InfiniPath HCAs use programmed I/O instead of HW DMA.
    This patch allows a verbs device driver to interpose on DMA mapping
    function calls in order to avoid relying on bus_to_virt() and
    phys_to_virt() to undo the mappings created by dma_map_single(),
    dma_map_sg(), etc.
    
    Signed-off-by: Ralph Campbell <ralph.campbell@qlogic.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 8eacc3510993..fd2353fa7e12 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -43,6 +43,8 @@
 
 #include <linux/types.h>
 #include <linux/device.h>
+#include <linux/mm.h>
+#include <linux/dma-mapping.h>
 
 #include <asm/atomic.h>
 #include <asm/scatterlist.h>
@@ -848,6 +850,49 @@ struct ib_cache {
 	u8                     *lmc_cache;
 };
 
+struct ib_dma_mapping_ops {
+	int		(*mapping_error)(struct ib_device *dev,
+					 u64 dma_addr);
+	u64		(*map_single)(struct ib_device *dev,
+				      void *ptr, size_t size,
+				      enum dma_data_direction direction);
+	void		(*unmap_single)(struct ib_device *dev,
+					u64 addr, size_t size,
+					enum dma_data_direction direction);
+	u64		(*map_page)(struct ib_device *dev,
+				    struct page *page, unsigned long offset,
+				    size_t size,
+				    enum dma_data_direction direction);
+	void		(*unmap_page)(struct ib_device *dev,
+				      u64 addr, size_t size,
+				      enum dma_data_direction direction);
+	int		(*map_sg)(struct ib_device *dev,
+				  struct scatterlist *sg, int nents,
+				  enum dma_data_direction direction);
+	void		(*unmap_sg)(struct ib_device *dev,
+				    struct scatterlist *sg, int nents,
+				    enum dma_data_direction direction);
+	u64		(*dma_address)(struct ib_device *dev,
+				       struct scatterlist *sg);
+	unsigned int	(*dma_len)(struct ib_device *dev,
+				   struct scatterlist *sg);
+	void		(*sync_single_for_cpu)(struct ib_device *dev,
+					       u64 dma_handle,
+					       size_t size,
+				               enum dma_data_direction dir);
+	void		(*sync_single_for_device)(struct ib_device *dev,
+						  u64 dma_handle,
+						  size_t size,
+						  enum dma_data_direction dir);
+	void		*(*alloc_coherent)(struct ib_device *dev,
+					   size_t size,
+					   u64 *dma_handle,
+					   gfp_t flag);
+	void		(*free_coherent)(struct ib_device *dev,
+					 size_t size, void *cpu_addr,
+					 u64 dma_handle);
+};
+
 struct iw_cm_verbs;
 
 struct ib_device {
@@ -992,6 +1037,8 @@ struct ib_device {
 						  struct ib_mad *in_mad,
 						  struct ib_mad *out_mad);
 
+	struct ib_dma_mapping_ops   *dma_ops;
+
 	struct module               *owner;
 	struct class_device          class_dev;
 	struct kobject               ports_parent;
@@ -1395,9 +1442,215 @@ static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
  *   usable for DMA.
  * @pd: The protection domain associated with the memory region.
  * @mr_access_flags: Specifies the memory access rights.
+ *
+ * Note that the ib_dma_*() functions defined below must be used
+ * to create/destroy addresses used with the Lkey or Rkey returned
+ * by ib_get_dma_mr().
  */
 struct ib_mr *ib_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
 
+/**
+ * ib_dma_mapping_error - check a DMA addr for error
+ * @dev: The device for which the dma_addr was created
+ * @dma_addr: The DMA address to check
+ */
+static inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->mapping_error(dev, dma_addr) :
+		dma_mapping_error(dma_addr);
+}
+
+/**
+ * ib_dma_map_single - Map a kernel virtual address to DMA address
+ * @dev: The device for which the dma_addr is to be created
+ * @cpu_addr: The kernel virtual address
+ * @size: The size of the region in bytes
+ * @direction: The direction of the DMA
+ */
+static inline u64 ib_dma_map_single(struct ib_device *dev,
+				    void *cpu_addr, size_t size,
+				    enum dma_data_direction direction)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->map_single(dev, cpu_addr, size, direction) :
+		dma_map_single(dev->dma_device, cpu_addr, size, direction);
+}
+
+/**
+ * ib_dma_unmap_single - Destroy a mapping created by ib_dma_map_single()
+ * @dev: The device for which the DMA address was created
+ * @addr: The DMA address
+ * @size: The size of the region in bytes
+ * @direction: The direction of the DMA
+ */
+static inline void ib_dma_unmap_single(struct ib_device *dev,
+				       u64 addr, size_t size,
+				       enum dma_data_direction direction)
+{
+	dev->dma_ops ?
+		dev->dma_ops->unmap_single(dev, addr, size, direction) :
+		dma_unmap_single(dev->dma_device, addr, size, direction);
+}
+
+/**
+ * ib_dma_map_page - Map a physical page to DMA address
+ * @dev: The device for which the dma_addr is to be created
+ * @page: The page to be mapped
+ * @offset: The offset within the page
+ * @size: The size of the region in bytes
+ * @direction: The direction of the DMA
+ */
+static inline u64 ib_dma_map_page(struct ib_device *dev,
+				  struct page *page,
+				  unsigned long offset,
+				  size_t size,
+					 enum dma_data_direction direction)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->map_page(dev, page, offset, size, direction) :
+		dma_map_page(dev->dma_device, page, offset, size, direction);
+}
+
+/**
+ * ib_dma_unmap_page - Destroy a mapping created by ib_dma_map_page()
+ * @dev: The device for which the DMA address was created
+ * @addr: The DMA address
+ * @size: The size of the region in bytes
+ * @direction: The direction of the DMA
+ */
+static inline void ib_dma_unmap_page(struct ib_device *dev,
+				     u64 addr, size_t size,
+				     enum dma_data_direction direction)
+{
+	dev->dma_ops ?
+		dev->dma_ops->unmap_page(dev, addr, size, direction) :
+		dma_unmap_page(dev->dma_device, addr, size, direction);
+}
+
+/**
+ * ib_dma_map_sg - Map a scatter/gather list to DMA addresses
+ * @dev: The device for which the DMA addresses are to be created
+ * @sg: The array of scatter/gather entries
+ * @nents: The number of scatter/gather entries
+ * @direction: The direction of the DMA
+ */
+static inline int ib_dma_map_sg(struct ib_device *dev,
+				struct scatterlist *sg, int nents,
+				enum dma_data_direction direction)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->map_sg(dev, sg, nents, direction) :
+		dma_map_sg(dev->dma_device, sg, nents, direction);
+}
+
+/**
+ * ib_dma_unmap_sg - Unmap a scatter/gather list of DMA addresses
+ * @dev: The device for which the DMA addresses were created
+ * @sg: The array of scatter/gather entries
+ * @nents: The number of scatter/gather entries
+ * @direction: The direction of the DMA
+ */
+static inline void ib_dma_unmap_sg(struct ib_device *dev,
+				   struct scatterlist *sg, int nents,
+				   enum dma_data_direction direction)
+{
+	dev->dma_ops ?
+		dev->dma_ops->unmap_sg(dev, sg, nents, direction) :
+		dma_unmap_sg(dev->dma_device, sg, nents, direction);
+}
+
+/**
+ * ib_sg_dma_address - Return the DMA address from a scatter/gather entry
+ * @dev: The device for which the DMA addresses were created
+ * @sg: The scatter/gather entry
+ */
+static inline u64 ib_sg_dma_address(struct ib_device *dev,
+				    struct scatterlist *sg)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->dma_address(dev, sg) : sg_dma_address(sg);
+}
+
+/**
+ * ib_sg_dma_len - Return the DMA length from a scatter/gather entry
+ * @dev: The device for which the DMA addresses were created
+ * @sg: The scatter/gather entry
+ */
+static inline unsigned int ib_sg_dma_len(struct ib_device *dev,
+					 struct scatterlist *sg)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->dma_len(dev, sg) : sg_dma_len(sg);
+}
+
+/**
+ * ib_dma_sync_single_for_cpu - Prepare DMA region to be accessed by CPU
+ * @dev: The device for which the DMA address was created
+ * @addr: The DMA address
+ * @size: The size of the region in bytes
+ * @dir: The direction of the DMA
+ */
+static inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,
+					      u64 addr,
+					      size_t size,
+					      enum dma_data_direction dir)
+{
+	dev->dma_ops ?
+		dev->dma_ops->sync_single_for_cpu(dev, addr, size, dir) :
+		dma_sync_single_for_cpu(dev->dma_device, addr, size, dir);
+}
+
+/**
+ * ib_dma_sync_single_for_device - Prepare DMA region to be accessed by device
+ * @dev: The device for which the DMA address was created
+ * @addr: The DMA address
+ * @size: The size of the region in bytes
+ * @dir: The direction of the DMA
+ */
+static inline void ib_dma_sync_single_for_device(struct ib_device *dev,
+						 u64 addr,
+						 size_t size,
+						 enum dma_data_direction dir)
+{
+	dev->dma_ops ?
+		dev->dma_ops->sync_single_for_device(dev, addr, size, dir) :
+		dma_sync_single_for_device(dev->dma_device, addr, size, dir);
+}
+
+/**
+ * ib_dma_alloc_coherent - Allocate memory and map it for DMA
+ * @dev: The device for which the DMA address is requested
+ * @size: The size of the region to allocate in bytes
+ * @dma_handle: A pointer for returning the DMA address of the region
+ * @flag: memory allocator flags
+ */
+static inline void *ib_dma_alloc_coherent(struct ib_device *dev,
+					   size_t size,
+					   u64 *dma_handle,
+					   gfp_t flag)
+{
+	return dev->dma_ops ?
+		dev->dma_ops->alloc_coherent(dev, size, dma_handle, flag) :
+		dma_alloc_coherent(dev->dma_device, size, dma_handle, flag);
+}
+
+/**
+ * ib_dma_free_coherent - Free memory allocated by ib_dma_alloc_coherent()
+ * @dev: The device for which the DMA addresses were allocated
+ * @size: The size of the region
+ * @cpu_addr: the address returned by ib_dma_alloc_coherent()
+ * @dma_handle: the DMA address returned by ib_dma_alloc_coherent()
+ */
+static inline void ib_dma_free_coherent(struct ib_device *dev,
+					size_t size, void *cpu_addr,
+					u64 dma_handle)
+{
+	dev->dma_ops ?
+		dev->dma_ops->free_coherent(dev, size, cpu_addr, dma_handle) :
+		dma_free_coherent(dev->dma_device, size, cpu_addr, dma_handle);
+}
+
 /**
  * ib_reg_phys_mr - Prepares a virtually addressed memory region for use
  *   by an HCA.

commit 07ebafbaaa72aa6a35472879008f5a1d1d469a0c
Author: Tom Tucker <tom@opengridcomputing.com>
Date:   Thu Aug 3 16:02:42 2006 -0500

    RDMA: iWARP Core Changes.
    
    Modifications to the existing rdma header files, core files, drivers,
    and ulp files to support iWARP, including:
     - Hook iWARP CM into the build system and use it in rdma_cm.
     - Convert enum ib_node_type to enum rdma_node_type, which includes
       the possibility of RDMA_NODE_RNIC, and update everything for this.
    
    Signed-off-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 61eed3996117..8eacc3510993 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -56,12 +56,22 @@ union ib_gid {
 	} global;
 };
 
-enum ib_node_type {
-	IB_NODE_CA 	= 1,
-	IB_NODE_SWITCH,
-	IB_NODE_ROUTER
+enum rdma_node_type {
+	/* IB values map to NodeInfo:NodeType. */
+	RDMA_NODE_IB_CA 	= 1,
+	RDMA_NODE_IB_SWITCH,
+	RDMA_NODE_IB_ROUTER,
+	RDMA_NODE_RNIC
 };
 
+enum rdma_transport_type {
+	RDMA_TRANSPORT_IB,
+	RDMA_TRANSPORT_IWARP
+};
+
+enum rdma_transport_type
+rdma_node_get_transport(enum rdma_node_type node_type) __attribute_const__;
+
 enum ib_device_cap_flags {
 	IB_DEVICE_RESIZE_MAX_WR		= 1,
 	IB_DEVICE_BAD_PKEY_CNTR		= (1<<1),
@@ -78,6 +88,9 @@ enum ib_device_cap_flags {
 	IB_DEVICE_RC_RNR_NAK_GEN	= (1<<12),
 	IB_DEVICE_SRQ_RESIZE		= (1<<13),
 	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
+	IB_DEVICE_ZERO_STAG		= (1<<15),
+	IB_DEVICE_SEND_W_INV		= (1<<16),
+	IB_DEVICE_MEM_WINDOW		= (1<<17)
 };
 
 enum ib_atomic_cap {
@@ -835,6 +848,8 @@ struct ib_cache {
 	u8                     *lmc_cache;
 };
 
+struct iw_cm_verbs;
+
 struct ib_device {
 	struct device                *dma_device;
 
@@ -851,6 +866,8 @@ struct ib_device {
 
 	u32                           flags;
 
+	struct iw_cm_verbs	     *iwcm;
+
 	int		           (*query_device)(struct ib_device *device,
 						   struct ib_device_attr *device_attr);
 	int		           (*query_port)(struct ib_device *device,

commit 9bc57e2d19db4da81c1150120658cc3658a99ed4
Author: Ralph Campbell <ralphc@pathscale.com>
Date:   Fri Aug 11 14:58:09 2006 -0700

    IB/uverbs: Pass userspace data to modify_srq and modify_qp methods
    
    Pass a struct ib_udata to the low-level driver's ->modify_srq() and
    ->modify_qp() methods, so that it can get to the device-specific data
    passed in by the userspace driver.
    
    Signed-off-by: Ralph Campbell <ralph.campbell@qlogic.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index ee1f3a355666..61eed3996117 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -888,7 +888,8 @@ struct ib_device {
 						 struct ib_udata *udata);
 	int                        (*modify_srq)(struct ib_srq *srq,
 						 struct ib_srq_attr *srq_attr,
-						 enum ib_srq_attr_mask srq_attr_mask);
+						 enum ib_srq_attr_mask srq_attr_mask,
+						 struct ib_udata *udata);
 	int                        (*query_srq)(struct ib_srq *srq,
 						struct ib_srq_attr *srq_attr);
 	int                        (*destroy_srq)(struct ib_srq *srq);
@@ -900,7 +901,8 @@ struct ib_device {
 						struct ib_udata *udata);
 	int                        (*modify_qp)(struct ib_qp *qp,
 						struct ib_qp_attr *qp_attr,
-						int qp_attr_mask);
+						int qp_attr_mask,
+						struct ib_udata *udata);
 	int                        (*query_qp)(struct ib_qp *qp,
 					       struct ib_qp_attr *qp_attr,
 					       int qp_attr_mask,

commit 9ead190bfde2a434c74ea604382d08acb2eceef5
Author: Roland Dreier <rolandd@cisco.com>
Date:   Sat Jun 17 20:44:49 2006 -0700

    IB/uverbs: Don't serialize with ib_uverbs_idr_mutex
    
    Currently, all userspace verbs operations that call into the kernel
    are serialized by ib_uverbs_idr_mutex.  This can be a scalability
    issue for some workloads, especially for devices driven by the ipath
    driver, which needs to call into the kernel even for datapath
    operations.
    
    Fix this by adding reference counts to the userspace objects, and then
    converting ib_uverbs_idr_mutex into a spinlock that only protects the
    idrs long enough to take a reference on the object being looked up.
    Because remove operations may fail, we have to do a slightly funky
    two-step deletion, which is described in the comments at the top of
    uverbs_cmd.c.
    
    This also still leaves ib_uverbs_idr_lock as a single lock that is
    possibly subject to contention.  However, the lock hold time will only
    be a single idr operation, so multiple threads should still be able to
    make progress, even if ib_uverbs_idr_lock is being ping-ponged.
    
    Surprisingly, these changes even shrink the object code:
    
    add/remove: 23/5 grow/shrink: 4/21 up/down: 633/-693 (-60)
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 7ced208edacf..ee1f3a355666 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -697,8 +697,12 @@ struct ib_ucontext {
 struct ib_uobject {
 	u64			user_handle;	/* handle given to us by userspace */
 	struct ib_ucontext     *context;	/* associated user context */
+	void		       *object;		/* containing object */
 	struct list_head	list;		/* link to context's list */
 	u32			id;		/* index into kernel idr */
+	struct kref		ref;
+	struct rw_semaphore	mutex;		/* protects .live */
+	int			live;
 };
 
 struct ib_umem {

commit 4e00d69454a8747798de11dc4eeef1edeee5ce98
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Sat Jun 17 20:37:39 2006 -0700

    IB: Add ib_init_ah_from_wc()
    
    Add a function to initialize address handle attributes from a work
    completion.  This functionality is duplicated by both verbs and the CM.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 10a6268c2cc2..7ced208edacf 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1087,6 +1087,20 @@ int ib_dealloc_pd(struct ib_pd *pd);
  */
 struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
 
+/**
+ * ib_init_ah_from_wc - Initializes address handle attributes from a
+ *   work completion.
+ * @device: Device on which the received message arrived.
+ * @port_num: Port on which the received message arrived.
+ * @wc: Work completion associated with the received message.
+ * @grh: References the received global route header.  This parameter is
+ *   ignored unless the work completion indicates that the GRH is valid.
+ * @ah_attr: Returned attributes that can be used when creating an address
+ *   handle for replying to the message.
+ */
+int ib_init_ah_from_wc(struct ib_device *device, u8 port_num, struct ib_wc *wc,
+		       struct ib_grh *grh, struct ib_ah_attr *ah_attr);
+
 /**
  * ib_create_ah_from_wc - Creates an address handle associated with the
  *   sender of the specified work completion.

commit 63942c9a981ecfa2aabfb27ff1a87b88f2ee9f5b
Author: Leonid Arsh <leonida@voltaire.com>
Date:   Sat Jun 17 20:37:35 2006 -0700

    IB: Add client reregister event type
    
    Add IB_EVENT_CLIENT_REREGISTER to enum so low-level drivers can
    generate "client reregister" events.
    
    Signed-off-by: Leonid Arsh <leonida@voltaire.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index aeb4fcd86a9e..10a6268c2cc2 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -260,7 +260,8 @@ enum ib_event_type {
 	IB_EVENT_SM_CHANGE,
 	IB_EVENT_SRQ_ERR,
 	IB_EVENT_SRQ_LIMIT_REACHED,
-	IB_EVENT_QP_LAST_WQE_REACHED
+	IB_EVENT_QP_LAST_WQE_REACHED,
+	IB_EVENT_CLIENT_REREGISTER
 };
 
 struct ib_event {

commit 6fb9cdbf2cdb2ea187e57ec2e16cc59df2adf86a
Author: Jack Morgenstein <jackm@mellanox.co.il>
Date:   Sat Jun 17 20:37:34 2006 -0700

    IB: Add caching of ports' LMC
    
    Add an LMC cache to struct ib_device, and add a function
    ib_get_cached_lmc() to query the cache.
    
    Signed-off-by: Jack Morgenstein <jackm@mellanox.co.il>
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 6bbf1b364400..aeb4fcd86a9e 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -827,6 +827,7 @@ struct ib_cache {
 	struct ib_event_handler event_handler;
 	struct ib_pkey_cache  **pkey_cache;
 	struct ib_gid_cache   **gid_cache;
+	u8                     *lmc_cache;
 };
 
 struct ib_device {

commit bf6a9e31cfa768ce0a8e18474b3ca808641d9243
Author: Jack Morgenstein <jackm@mellanox.co.il>
Date:   Mon Apr 10 09:43:47 2006 -0700

    IB: simplify static rate encoding
    
    Push translation of static rate to HCA format into low-level drivers,
    where it belongs.  For static rate encoding, use encoding of rate
    field from IB standard PathRecord, with addition of value 0, for
    backwards compatibility with current usage.  The changes are:
    
     - Add enum ib_rate to midlayer includes.
     - Get rid of static rate translation in IPoIB; just use static rate
       directly from Path and MulticastGroup records.
     - Update mthca driver to translate absolute static rate into the
       format used by hardware.  This also fixes mthca's static rate
       handling for HCAs that are capable of 4X DDR.
    
    Signed-off-by: Jack Morgenstein <jackm@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index c1ad6273ac6c..6bbf1b364400 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -314,6 +314,34 @@ enum ib_ah_flags {
 	IB_AH_GRH	= 1
 };
 
+enum ib_rate {
+	IB_RATE_PORT_CURRENT = 0,
+	IB_RATE_2_5_GBPS = 2,
+	IB_RATE_5_GBPS   = 5,
+	IB_RATE_10_GBPS  = 3,
+	IB_RATE_20_GBPS  = 6,
+	IB_RATE_30_GBPS  = 4,
+	IB_RATE_40_GBPS  = 7,
+	IB_RATE_60_GBPS  = 8,
+	IB_RATE_80_GBPS  = 9,
+	IB_RATE_120_GBPS = 10
+};
+
+/**
+ * ib_rate_to_mult - Convert the IB rate enum to a multiple of the
+ * base rate of 2.5 Gbit/sec.  For example, IB_RATE_5_GBPS will be
+ * converted to 2, since 5 Gbit/sec is 2 * 2.5 Gbit/sec.
+ * @rate: rate to convert.
+ */
+int ib_rate_to_mult(enum ib_rate rate) __attribute_const__;
+
+/**
+ * mult_to_ib_rate - Convert a multiple of 2.5 Gbit/sec to an IB rate
+ * enum.
+ * @mult: multiple to convert.
+ */
+enum ib_rate mult_to_ib_rate(int mult) __attribute_const__;
+
 struct ib_ah_attr {
 	struct ib_global_route	grh;
 	u16			dlid;

commit abb6e9ba17eb133ab385d0f9017fa8afa809d52a
Author: Dotan Barak <dotanb@mellanox.co.il>
Date:   Thu Feb 23 12:13:51 2006 -0800

    IB/mthca: Return actual capacity from create_srq
    
    Have mthca's create_srq method return the actual capacity of the SRQ
    that gets created.  Also update comments in <rdma/ib_verbs.h> to
    clarify that this is what is expected from ib_create_srq().
    
    Signed-off-by: Dotan Barak <dotanb@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 010287c844e7..c1ad6273ac6c 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -1100,7 +1100,9 @@ int ib_destroy_ah(struct ib_ah *ah);
  * ib_create_srq - Creates a SRQ associated with the specified protection
  *   domain.
  * @pd: The protection domain associated with the SRQ.
- * @srq_init_attr: A list of initial attributes required to create the SRQ.
+ * @srq_init_attr: A list of initial attributes required to create the
+ *   SRQ.  If SRQ creation succeeds, then the attributes are updated to
+ *   the actual capabilities of the created SRQ.
  *
  * srq_attr->max_wr and srq_attr->max_sge are read the determine the
  * requested size of the SRQ, and set to the actual values allocated
@@ -1159,7 +1161,9 @@ static inline int ib_post_srq_recv(struct ib_srq *srq,
  * ib_create_qp - Creates a QP associated with the specified protection
  *   domain.
  * @pd: The protection domain associated with the QP.
- * @qp_init_attr: A list of initial attributes required to create the QP.
+ * @qp_init_attr: A list of initial attributes required to create the
+ *   QP.  If QP creation succeeds, then the attributes are updated to
+ *   the actual capabilities of the created QP.
  */
 struct ib_qp *ib_create_qp(struct ib_pd *pd,
 			   struct ib_qp_init_attr *qp_init_attr);

commit 8a51866f08103ba04894ce0f65eef567ddc3ed40
Author: Roland Dreier <rolandd@cisco.com>
Date:   Mon Feb 13 12:48:12 2006 -0800

    IB: Add ib_modify_qp_is_ok() library function
    
    The in-kernel mthca driver contains a table of which attributes are
    valid for each queue pair state transition.  It turns out that both
    other IB drivers -- ipath and ehca -- which are being prepared for
    merging have copied this table, errors and all.
    
    To forestall this code duplication, move this table and the code to
    check parameters against it into a midlayer library function,
    ib_modify_qp_is_ok().
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 61a4390ae9d8..010287c844e7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -990,6 +990,24 @@ static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len
 	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
 }
 
+/**
+ * ib_modify_qp_is_ok - Check that the supplied attribute mask
+ * contains all required attributes and no attributes not allowed for
+ * the given QP state transition.
+ * @cur_state: Current QP state
+ * @next_state: Next QP state
+ * @type: QP type
+ * @mask: Mask of supplied QP attributes
+ *
+ * This function is a helper function that a low-level driver's
+ * modify_qp method can use to validate the consumer's input.  It
+ * checks that cur_state and next_state are valid QP states, that a
+ * transition from cur_state to next_state is allowed by the IB spec,
+ * and that the attribute mask supplied is allowed for the transition.
+ */
+int ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,
+		       enum ib_qp_type type, enum ib_qp_attr_mask mask);
+
 int ib_register_event_handler  (struct ib_event_handler *event_handler);
 int ib_unregister_event_handler(struct ib_event_handler *event_handler);
 void ib_dispatch_event(struct ib_event *event);

commit d36f34aadf184d8cc4c240de2b6319ccea8334bb
Author: Or Gerlitz <ogerlitz@voltaire.com>
Date:   Thu Feb 2 10:43:45 2006 -0800

    IB: Enable FMR pool user to set page size
    
    This patch allows the consumer to set the page size of "pages" mapped
    by the pool FMRs, which is a feature already existing in the base
    verbs API.  On the cosmetic side it changes ib_fmr_attr.page_size field
    to be named page_shift.
    
    Signed-off-by: Or Gerlitz <ogerlitz@voltaire.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 1d31c8cd5ce0..61a4390ae9d8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -651,7 +651,7 @@ struct ib_mw_bind {
 struct ib_fmr_attr {
 	int	max_pages;
 	int	max_maps;
-	u8	page_size;
+	u8	page_shift;
 };
 
 struct ib_ucontext {

commit c5bcbbb9fe00128d500c2f473d5ddc8d8c2c53a7
Author: Roland Dreier <rolandd@cisco.com>
Date:   Thu Feb 2 09:47:14 2006 -0800

    IB: Allow userspace to set node description
    
    Expose a writable "node_desc" sysfs attribute for InfiniBand devices.
    This allows userspace to update the node description with information
    such as the node's hostname, so that IB network management software
    can tie its view to the real world.
    
    Signed-off-by: Michael S. Tsirkin <mst@mellanox.co.il>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 38fa6c082eae..1d31c8cd5ce0 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -222,11 +222,13 @@ struct ib_port_attr {
 };
 
 enum ib_device_modify_flags {
-	IB_DEVICE_MODIFY_SYS_IMAGE_GUID	= 1
+	IB_DEVICE_MODIFY_SYS_IMAGE_GUID	= 1 << 0,
+	IB_DEVICE_MODIFY_NODE_DESC	= 1 << 1
 };
 
 struct ib_device_modify {
 	u64	sys_image_guid;
+	char	node_desc[64];
 };
 
 enum ib_port_modify_flags {
@@ -951,6 +953,7 @@ struct ib_device {
 	u64			     uverbs_cmd_mask;
 	int			     uverbs_abi_ver;
 
+	char			     node_desc[64];
 	__be64			     node_guid;
 	u8                           node_type;
 	u8                           phys_port_cnt;

commit 33b9b3ee9709b19c4f02ab91571d53540d05c3d1
Author: Roland Dreier <rolandd@cisco.com>
Date:   Mon Jan 30 14:29:21 2006 -0800

    IB: Add userspace support for resizing CQs
    
    Add support to uverbs to handle resizing userspace CQs (completion
    queues), including adding an ABI for marshalling requests and
    responses.  The kernel midlayer already has ib_resize_cq().
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 22fc886b9695..38fa6c082eae 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -5,7 +5,7 @@
  * Copyright (c) 2004 Topspin Corporation.  All rights reserved.
  * Copyright (c) 2004 Voltaire Corporation.  All rights reserved.
  * Copyright (c) 2005 Sun Microsystems, Inc. All rights reserved.
- * Copyright (c) 2005 Cisco Systems.  All rights reserved.
+ * Copyright (c) 2005, 2006 Cisco Systems.  All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -880,7 +880,8 @@ struct ib_device {
 						struct ib_ucontext *context,
 						struct ib_udata *udata);
 	int                        (*destroy_cq)(struct ib_cq *cq);
-	int                        (*resize_cq)(struct ib_cq *cq, int cqe);
+	int                        (*resize_cq)(struct ib_cq *cq, int cqe,
+						struct ib_udata *udata);
 	int                        (*poll_cq)(struct ib_cq *cq, int num_entries,
 					      struct ib_wc *wc);
 	int                        (*peek_cq)(struct ib_cq *cq, int wc_cnt);

commit cf311cd49a78f1e431787068cc31d29d06a415e6
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Tue Jan 10 07:39:34 2006 -0800

    IB: Add node_guid to struct ib_device
    
    Add a node_guid field to struct ib_device.  It is the responsibility
    of the low-level driver to initialize this field before registering a
    device with the midlayer.  Convert everyone to looking at this field
    instead of calling ib_query_device() when all they want is the node
    GUID, and remove the node_guid field from struct ib_device_attr.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a7f4c355a91f..22fc886b9695 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -88,7 +88,6 @@ enum ib_atomic_cap {
 
 struct ib_device_attr {
 	u64			fw_ver;
-	__be64			node_guid;
 	__be64			sys_image_guid;
 	u64			max_mr_size;
 	u64			page_size_cap;
@@ -951,6 +950,7 @@ struct ib_device {
 	u64			     uverbs_cmd_mask;
 	int			     uverbs_abi_ver;
 
+	__be64			     node_guid;
 	u8                           node_type;
 	u8                           phys_port_cnt;
 };

commit 40de2e548c225e3ef859e3c60de9785e37e1b5b1
Author: Roland Dreier <rolandd@cisco.com>
Date:   Tue Nov 8 11:10:25 2005 -0800

    [IB] Have cq_resize() method take an int, not int*
    
    Change the struct ib_device.resize_cq() method to take a plain integer
    that holds the new CQ size, rather than a pointer to an integer that
    it uses to return the new size.  This makes the interface match the
    exported ib_resize_cq() signature, and allows the low-level driver to
    update the CQ size with proper locking if necessary.
    
    No in-tree drivers are exporting this method yet.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index f72d46d54e0a..a7f4c355a91f 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -881,7 +881,7 @@ struct ib_device {
 						struct ib_ucontext *context,
 						struct ib_udata *udata);
 	int                        (*destroy_cq)(struct ib_cq *cq);
-	int                        (*resize_cq)(struct ib_cq *cq, int *cqe);
+	int                        (*resize_cq)(struct ib_cq *cq, int cqe);
 	int                        (*poll_cq)(struct ib_cq *cq, int num_entries,
 					      struct ib_wc *wc);
 	int                        (*peek_cq)(struct ib_cq *cq, int wc_cnt);

commit 34816ad98efe4d47ffd858a0345321f9d85d9420
Author: Sean Hefty <sean.hefty@intel.com>
Date:   Tue Oct 25 10:51:39 2005 -0700

    [IB] Fix MAD layer DMA mappings to avoid touching data buffer once mapped
    
    The MAD layer was violating the DMA API by touching data buffers used
    for sends after the DMA mapping was done.  This causes problems on
    non-cache-coherent architectures, because the device doing DMA won't
    see updates to the payload buffers that exist only in the CPU cache.
    
    Fix this by having all MAD consumers use ib_create_send_mad() to
    allocate their send buffers, and moving the DMA mapping into the MAD
    layer so it can be done just before calling send (and after any
    modifications of the send buffer by the MAD layer).
    
    Tested on a non-cache-coherent PowerPC 440SPe system.
    
    Signed-off-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index a5a963cb5676..f72d46d54e0a 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -595,11 +595,8 @@ struct ib_send_wr {
 		} atomic;
 		struct {
 			struct ib_ah *ah;
-			struct ib_mad_hdr *mad_hdr;
 			u32	remote_qpn;
 			u32	remote_qkey;
-			int	timeout_ms; /* valid for MADs only */
-			int	retries;    /* valid for MADs only */
 			u16	pkey_index; /* valid for GSI only */
 			u8	port_num;   /* valid for DR SMPs on switch only */
 		} ud;

commit 883a99c7024c5763d6d4f22d9239c133893e8d74
Author: Roland Dreier <rolandd@cisco.com>
Date:   Fri Oct 14 14:00:58 2005 -0700

    [IB] uverbs: Add a mask of device methods allowed for userspace
    
    Give each device a uverbs_cmd_mask, so that a low-level driver can
    control which methods may be called on behalf of userspace.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index d13f25a78192..a5a963cb5676 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -951,6 +951,7 @@ struct ib_device {
 		IB_DEV_UNREGISTERED
 	}                            reg_state;
 
+	u64			     uverbs_cmd_mask;
 	int			     uverbs_abi_ver;
 
 	u8                           node_type;

commit 274c0891637c44ac71f3ac40be91b43c2318883a
Author: Roland Dreier <rolandd@cisco.com>
Date:   Thu Sep 29 14:17:48 2005 -0700

    [IB] uverbs: Add device-specific ABI version attribute
    
    Add abi_version attribute to uverbs class devices to allow for
    ABI versioning of device-specific interfaces.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e6f4c9e55df7..d13f25a78192 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -951,6 +951,8 @@ struct ib_device {
 		IB_DEV_UNREGISTERED
 	}                            reg_state;
 
+	int			     uverbs_abi_ver;
+
 	u8                           node_type;
 	u8                           phys_port_cnt;
 };

commit 63c47c286d062d93e0501d60797274c84a587e97
Author: Roland Dreier <rolandd@cisco.com>
Date:   Mon Sep 26 13:01:03 2005 -0700

    [IB] uverbs: Close some exploitable races
    
    Al Viro pointed out that the current IB userspace verbs interface
    allows userspace to cause mischief by closing file descriptors before
    we're ready, or issuing the same command twice at the same time.  This
    patch closes those races, and fixes other obvious problems such as a
    module reference leak.
    
    Some other interface bogosities will require an ABI change to fix
    properly, so I'm deferring those fixes until 2.6.15.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index e16cf94870f2..e6f4c9e55df7 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -665,7 +665,6 @@ struct ib_ucontext {
 	struct list_head	qp_list;
 	struct list_head	srq_list;
 	struct list_head	ah_list;
-	spinlock_t              lock;
 };
 
 struct ib_uobject {

commit a4d61e84804f3b14cc35c5e2af768a07c0f64ef6
Author: Roland Dreier <roland@eddore.topspincom.com>
Date:   Thu Aug 25 13:40:04 2005 -0700

    [PATCH] IB: move include files to include/rdma
    
    Move the InfiniBand headers from drivers/infiniband/include to include/rdma.
    This allows InfiniBand-using code to live elsewhere, and lets us remove the
    ugly EXTRA_CFLAGS include path from the InfiniBand Makefiles.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
new file mode 100644
index 000000000000..e16cf94870f2
--- /dev/null
+++ b/include/rdma/ib_verbs.h
@@ -0,0 +1,1461 @@
+/*
+ * Copyright (c) 2004 Mellanox Technologies Ltd.  All rights reserved.
+ * Copyright (c) 2004 Infinicon Corporation.  All rights reserved.
+ * Copyright (c) 2004 Intel Corporation.  All rights reserved.
+ * Copyright (c) 2004 Topspin Corporation.  All rights reserved.
+ * Copyright (c) 2004 Voltaire Corporation.  All rights reserved.
+ * Copyright (c) 2005 Sun Microsystems, Inc. All rights reserved.
+ * Copyright (c) 2005 Cisco Systems.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * $Id: ib_verbs.h 1349 2004-12-16 21:09:43Z roland $
+ */
+
+#if !defined(IB_VERBS_H)
+#define IB_VERBS_H
+
+#include <linux/types.h>
+#include <linux/device.h>
+
+#include <asm/atomic.h>
+#include <asm/scatterlist.h>
+#include <asm/uaccess.h>
+
+union ib_gid {
+	u8	raw[16];
+	struct {
+		__be64	subnet_prefix;
+		__be64	interface_id;
+	} global;
+};
+
+enum ib_node_type {
+	IB_NODE_CA 	= 1,
+	IB_NODE_SWITCH,
+	IB_NODE_ROUTER
+};
+
+enum ib_device_cap_flags {
+	IB_DEVICE_RESIZE_MAX_WR		= 1,
+	IB_DEVICE_BAD_PKEY_CNTR		= (1<<1),
+	IB_DEVICE_BAD_QKEY_CNTR		= (1<<2),
+	IB_DEVICE_RAW_MULTI		= (1<<3),
+	IB_DEVICE_AUTO_PATH_MIG		= (1<<4),
+	IB_DEVICE_CHANGE_PHY_PORT	= (1<<5),
+	IB_DEVICE_UD_AV_PORT_ENFORCE	= (1<<6),
+	IB_DEVICE_CURR_QP_STATE_MOD	= (1<<7),
+	IB_DEVICE_SHUTDOWN_PORT		= (1<<8),
+	IB_DEVICE_INIT_TYPE		= (1<<9),
+	IB_DEVICE_PORT_ACTIVE_EVENT	= (1<<10),
+	IB_DEVICE_SYS_IMAGE_GUID	= (1<<11),
+	IB_DEVICE_RC_RNR_NAK_GEN	= (1<<12),
+	IB_DEVICE_SRQ_RESIZE		= (1<<13),
+	IB_DEVICE_N_NOTIFY_CQ		= (1<<14),
+};
+
+enum ib_atomic_cap {
+	IB_ATOMIC_NONE,
+	IB_ATOMIC_HCA,
+	IB_ATOMIC_GLOB
+};
+
+struct ib_device_attr {
+	u64			fw_ver;
+	__be64			node_guid;
+	__be64			sys_image_guid;
+	u64			max_mr_size;
+	u64			page_size_cap;
+	u32			vendor_id;
+	u32			vendor_part_id;
+	u32			hw_ver;
+	int			max_qp;
+	int			max_qp_wr;
+	int			device_cap_flags;
+	int			max_sge;
+	int			max_sge_rd;
+	int			max_cq;
+	int			max_cqe;
+	int			max_mr;
+	int			max_pd;
+	int			max_qp_rd_atom;
+	int			max_ee_rd_atom;
+	int			max_res_rd_atom;
+	int			max_qp_init_rd_atom;
+	int			max_ee_init_rd_atom;
+	enum ib_atomic_cap	atomic_cap;
+	int			max_ee;
+	int			max_rdd;
+	int			max_mw;
+	int			max_raw_ipv6_qp;
+	int			max_raw_ethy_qp;
+	int			max_mcast_grp;
+	int			max_mcast_qp_attach;
+	int			max_total_mcast_qp_attach;
+	int			max_ah;
+	int			max_fmr;
+	int			max_map_per_fmr;
+	int			max_srq;
+	int			max_srq_wr;
+	int			max_srq_sge;
+	u16			max_pkeys;
+	u8			local_ca_ack_delay;
+};
+
+enum ib_mtu {
+	IB_MTU_256  = 1,
+	IB_MTU_512  = 2,
+	IB_MTU_1024 = 3,
+	IB_MTU_2048 = 4,
+	IB_MTU_4096 = 5
+};
+
+static inline int ib_mtu_enum_to_int(enum ib_mtu mtu)
+{
+	switch (mtu) {
+	case IB_MTU_256:  return  256;
+	case IB_MTU_512:  return  512;
+	case IB_MTU_1024: return 1024;
+	case IB_MTU_2048: return 2048;
+	case IB_MTU_4096: return 4096;
+	default: 	  return -1;
+	}
+}
+
+enum ib_port_state {
+	IB_PORT_NOP		= 0,
+	IB_PORT_DOWN		= 1,
+	IB_PORT_INIT		= 2,
+	IB_PORT_ARMED		= 3,
+	IB_PORT_ACTIVE		= 4,
+	IB_PORT_ACTIVE_DEFER	= 5
+};
+
+enum ib_port_cap_flags {
+	IB_PORT_SM				= 1 <<  1,
+	IB_PORT_NOTICE_SUP			= 1 <<  2,
+	IB_PORT_TRAP_SUP			= 1 <<  3,
+	IB_PORT_OPT_IPD_SUP                     = 1 <<  4,
+	IB_PORT_AUTO_MIGR_SUP			= 1 <<  5,
+	IB_PORT_SL_MAP_SUP			= 1 <<  6,
+	IB_PORT_MKEY_NVRAM			= 1 <<  7,
+	IB_PORT_PKEY_NVRAM			= 1 <<  8,
+	IB_PORT_LED_INFO_SUP			= 1 <<  9,
+	IB_PORT_SM_DISABLED			= 1 << 10,
+	IB_PORT_SYS_IMAGE_GUID_SUP		= 1 << 11,
+	IB_PORT_PKEY_SW_EXT_PORT_TRAP_SUP	= 1 << 12,
+	IB_PORT_CM_SUP				= 1 << 16,
+	IB_PORT_SNMP_TUNNEL_SUP			= 1 << 17,
+	IB_PORT_REINIT_SUP			= 1 << 18,
+	IB_PORT_DEVICE_MGMT_SUP			= 1 << 19,
+	IB_PORT_VENDOR_CLASS_SUP		= 1 << 20,
+	IB_PORT_DR_NOTICE_SUP			= 1 << 21,
+	IB_PORT_CAP_MASK_NOTICE_SUP		= 1 << 22,
+	IB_PORT_BOOT_MGMT_SUP			= 1 << 23,
+	IB_PORT_LINK_LATENCY_SUP		= 1 << 24,
+	IB_PORT_CLIENT_REG_SUP			= 1 << 25
+};
+
+enum ib_port_width {
+	IB_WIDTH_1X	= 1,
+	IB_WIDTH_4X	= 2,
+	IB_WIDTH_8X	= 4,
+	IB_WIDTH_12X	= 8
+};
+
+static inline int ib_width_enum_to_int(enum ib_port_width width)
+{
+	switch (width) {
+	case IB_WIDTH_1X:  return  1;
+	case IB_WIDTH_4X:  return  4;
+	case IB_WIDTH_8X:  return  8;
+	case IB_WIDTH_12X: return 12;
+	default: 	  return -1;
+	}
+}
+
+struct ib_port_attr {
+	enum ib_port_state	state;
+	enum ib_mtu		max_mtu;
+	enum ib_mtu		active_mtu;
+	int			gid_tbl_len;
+	u32			port_cap_flags;
+	u32			max_msg_sz;
+	u32			bad_pkey_cntr;
+	u32			qkey_viol_cntr;
+	u16			pkey_tbl_len;
+	u16			lid;
+	u16			sm_lid;
+	u8			lmc;
+	u8			max_vl_num;
+	u8			sm_sl;
+	u8			subnet_timeout;
+	u8			init_type_reply;
+	u8			active_width;
+	u8			active_speed;
+	u8                      phys_state;
+};
+
+enum ib_device_modify_flags {
+	IB_DEVICE_MODIFY_SYS_IMAGE_GUID	= 1
+};
+
+struct ib_device_modify {
+	u64	sys_image_guid;
+};
+
+enum ib_port_modify_flags {
+	IB_PORT_SHUTDOWN		= 1,
+	IB_PORT_INIT_TYPE		= (1<<2),
+	IB_PORT_RESET_QKEY_CNTR		= (1<<3)
+};
+
+struct ib_port_modify {
+	u32	set_port_cap_mask;
+	u32	clr_port_cap_mask;
+	u8	init_type;
+};
+
+enum ib_event_type {
+	IB_EVENT_CQ_ERR,
+	IB_EVENT_QP_FATAL,
+	IB_EVENT_QP_REQ_ERR,
+	IB_EVENT_QP_ACCESS_ERR,
+	IB_EVENT_COMM_EST,
+	IB_EVENT_SQ_DRAINED,
+	IB_EVENT_PATH_MIG,
+	IB_EVENT_PATH_MIG_ERR,
+	IB_EVENT_DEVICE_FATAL,
+	IB_EVENT_PORT_ACTIVE,
+	IB_EVENT_PORT_ERR,
+	IB_EVENT_LID_CHANGE,
+	IB_EVENT_PKEY_CHANGE,
+	IB_EVENT_SM_CHANGE,
+	IB_EVENT_SRQ_ERR,
+	IB_EVENT_SRQ_LIMIT_REACHED,
+	IB_EVENT_QP_LAST_WQE_REACHED
+};
+
+struct ib_event {
+	struct ib_device	*device;
+	union {
+		struct ib_cq	*cq;
+		struct ib_qp	*qp;
+		struct ib_srq	*srq;
+		u8		port_num;
+	} element;
+	enum ib_event_type	event;
+};
+
+struct ib_event_handler {
+	struct ib_device *device;
+	void            (*handler)(struct ib_event_handler *, struct ib_event *);
+	struct list_head  list;
+};
+
+#define INIT_IB_EVENT_HANDLER(_ptr, _device, _handler)		\
+	do {							\
+		(_ptr)->device  = _device;			\
+		(_ptr)->handler = _handler;			\
+		INIT_LIST_HEAD(&(_ptr)->list);			\
+	} while (0)
+
+struct ib_global_route {
+	union ib_gid	dgid;
+	u32		flow_label;
+	u8		sgid_index;
+	u8		hop_limit;
+	u8		traffic_class;
+};
+
+struct ib_grh {
+	__be32		version_tclass_flow;
+	__be16		paylen;
+	u8		next_hdr;
+	u8		hop_limit;
+	union ib_gid	sgid;
+	union ib_gid	dgid;
+};
+
+enum {
+	IB_MULTICAST_QPN = 0xffffff
+};
+
+#define IB_LID_PERMISSIVE	__constant_htons(0xFFFF)
+
+enum ib_ah_flags {
+	IB_AH_GRH	= 1
+};
+
+struct ib_ah_attr {
+	struct ib_global_route	grh;
+	u16			dlid;
+	u8			sl;
+	u8			src_path_bits;
+	u8			static_rate;
+	u8			ah_flags;
+	u8			port_num;
+};
+
+enum ib_wc_status {
+	IB_WC_SUCCESS,
+	IB_WC_LOC_LEN_ERR,
+	IB_WC_LOC_QP_OP_ERR,
+	IB_WC_LOC_EEC_OP_ERR,
+	IB_WC_LOC_PROT_ERR,
+	IB_WC_WR_FLUSH_ERR,
+	IB_WC_MW_BIND_ERR,
+	IB_WC_BAD_RESP_ERR,
+	IB_WC_LOC_ACCESS_ERR,
+	IB_WC_REM_INV_REQ_ERR,
+	IB_WC_REM_ACCESS_ERR,
+	IB_WC_REM_OP_ERR,
+	IB_WC_RETRY_EXC_ERR,
+	IB_WC_RNR_RETRY_EXC_ERR,
+	IB_WC_LOC_RDD_VIOL_ERR,
+	IB_WC_REM_INV_RD_REQ_ERR,
+	IB_WC_REM_ABORT_ERR,
+	IB_WC_INV_EECN_ERR,
+	IB_WC_INV_EEC_STATE_ERR,
+	IB_WC_FATAL_ERR,
+	IB_WC_RESP_TIMEOUT_ERR,
+	IB_WC_GENERAL_ERR
+};
+
+enum ib_wc_opcode {
+	IB_WC_SEND,
+	IB_WC_RDMA_WRITE,
+	IB_WC_RDMA_READ,
+	IB_WC_COMP_SWAP,
+	IB_WC_FETCH_ADD,
+	IB_WC_BIND_MW,
+/*
+ * Set value of IB_WC_RECV so consumers can test if a completion is a
+ * receive by testing (opcode & IB_WC_RECV).
+ */
+	IB_WC_RECV			= 1 << 7,
+	IB_WC_RECV_RDMA_WITH_IMM
+};
+
+enum ib_wc_flags {
+	IB_WC_GRH		= 1,
+	IB_WC_WITH_IMM		= (1<<1)
+};
+
+struct ib_wc {
+	u64			wr_id;
+	enum ib_wc_status	status;
+	enum ib_wc_opcode	opcode;
+	u32			vendor_err;
+	u32			byte_len;
+	__be32			imm_data;
+	u32			qp_num;
+	u32			src_qp;
+	int			wc_flags;
+	u16			pkey_index;
+	u16			slid;
+	u8			sl;
+	u8			dlid_path_bits;
+	u8			port_num;	/* valid only for DR SMPs on switches */
+};
+
+enum ib_cq_notify {
+	IB_CQ_SOLICITED,
+	IB_CQ_NEXT_COMP
+};
+
+enum ib_srq_attr_mask {
+	IB_SRQ_MAX_WR	= 1 << 0,
+	IB_SRQ_LIMIT	= 1 << 1,
+};
+
+struct ib_srq_attr {
+	u32	max_wr;
+	u32	max_sge;
+	u32	srq_limit;
+};
+
+struct ib_srq_init_attr {
+	void		      (*event_handler)(struct ib_event *, void *);
+	void		       *srq_context;
+	struct ib_srq_attr	attr;
+};
+
+struct ib_qp_cap {
+	u32	max_send_wr;
+	u32	max_recv_wr;
+	u32	max_send_sge;
+	u32	max_recv_sge;
+	u32	max_inline_data;
+};
+
+enum ib_sig_type {
+	IB_SIGNAL_ALL_WR,
+	IB_SIGNAL_REQ_WR
+};
+
+enum ib_qp_type {
+	/*
+	 * IB_QPT_SMI and IB_QPT_GSI have to be the first two entries
+	 * here (and in that order) since the MAD layer uses them as
+	 * indices into a 2-entry table.
+	 */
+	IB_QPT_SMI,
+	IB_QPT_GSI,
+
+	IB_QPT_RC,
+	IB_QPT_UC,
+	IB_QPT_UD,
+	IB_QPT_RAW_IPV6,
+	IB_QPT_RAW_ETY
+};
+
+struct ib_qp_init_attr {
+	void                  (*event_handler)(struct ib_event *, void *);
+	void		       *qp_context;
+	struct ib_cq	       *send_cq;
+	struct ib_cq	       *recv_cq;
+	struct ib_srq	       *srq;
+	struct ib_qp_cap	cap;
+	enum ib_sig_type	sq_sig_type;
+	enum ib_qp_type		qp_type;
+	u8			port_num; /* special QP types only */
+};
+
+enum ib_rnr_timeout {
+	IB_RNR_TIMER_655_36 =  0,
+	IB_RNR_TIMER_000_01 =  1,
+	IB_RNR_TIMER_000_02 =  2,
+	IB_RNR_TIMER_000_03 =  3,
+	IB_RNR_TIMER_000_04 =  4,
+	IB_RNR_TIMER_000_06 =  5,
+	IB_RNR_TIMER_000_08 =  6,
+	IB_RNR_TIMER_000_12 =  7,
+	IB_RNR_TIMER_000_16 =  8,
+	IB_RNR_TIMER_000_24 =  9,
+	IB_RNR_TIMER_000_32 = 10,
+	IB_RNR_TIMER_000_48 = 11,
+	IB_RNR_TIMER_000_64 = 12,
+	IB_RNR_TIMER_000_96 = 13,
+	IB_RNR_TIMER_001_28 = 14,
+	IB_RNR_TIMER_001_92 = 15,
+	IB_RNR_TIMER_002_56 = 16,
+	IB_RNR_TIMER_003_84 = 17,
+	IB_RNR_TIMER_005_12 = 18,
+	IB_RNR_TIMER_007_68 = 19,
+	IB_RNR_TIMER_010_24 = 20,
+	IB_RNR_TIMER_015_36 = 21,
+	IB_RNR_TIMER_020_48 = 22,
+	IB_RNR_TIMER_030_72 = 23,
+	IB_RNR_TIMER_040_96 = 24,
+	IB_RNR_TIMER_061_44 = 25,
+	IB_RNR_TIMER_081_92 = 26,
+	IB_RNR_TIMER_122_88 = 27,
+	IB_RNR_TIMER_163_84 = 28,
+	IB_RNR_TIMER_245_76 = 29,
+	IB_RNR_TIMER_327_68 = 30,
+	IB_RNR_TIMER_491_52 = 31
+};
+
+enum ib_qp_attr_mask {
+	IB_QP_STATE			= 1,
+	IB_QP_CUR_STATE			= (1<<1),
+	IB_QP_EN_SQD_ASYNC_NOTIFY	= (1<<2),
+	IB_QP_ACCESS_FLAGS		= (1<<3),
+	IB_QP_PKEY_INDEX		= (1<<4),
+	IB_QP_PORT			= (1<<5),
+	IB_QP_QKEY			= (1<<6),
+	IB_QP_AV			= (1<<7),
+	IB_QP_PATH_MTU			= (1<<8),
+	IB_QP_TIMEOUT			= (1<<9),
+	IB_QP_RETRY_CNT			= (1<<10),
+	IB_QP_RNR_RETRY			= (1<<11),
+	IB_QP_RQ_PSN			= (1<<12),
+	IB_QP_MAX_QP_RD_ATOMIC		= (1<<13),
+	IB_QP_ALT_PATH			= (1<<14),
+	IB_QP_MIN_RNR_TIMER		= (1<<15),
+	IB_QP_SQ_PSN			= (1<<16),
+	IB_QP_MAX_DEST_RD_ATOMIC	= (1<<17),
+	IB_QP_PATH_MIG_STATE		= (1<<18),
+	IB_QP_CAP			= (1<<19),
+	IB_QP_DEST_QPN			= (1<<20)
+};
+
+enum ib_qp_state {
+	IB_QPS_RESET,
+	IB_QPS_INIT,
+	IB_QPS_RTR,
+	IB_QPS_RTS,
+	IB_QPS_SQD,
+	IB_QPS_SQE,
+	IB_QPS_ERR
+};
+
+enum ib_mig_state {
+	IB_MIG_MIGRATED,
+	IB_MIG_REARM,
+	IB_MIG_ARMED
+};
+
+struct ib_qp_attr {
+	enum ib_qp_state	qp_state;
+	enum ib_qp_state	cur_qp_state;
+	enum ib_mtu		path_mtu;
+	enum ib_mig_state	path_mig_state;
+	u32			qkey;
+	u32			rq_psn;
+	u32			sq_psn;
+	u32			dest_qp_num;
+	int			qp_access_flags;
+	struct ib_qp_cap	cap;
+	struct ib_ah_attr	ah_attr;
+	struct ib_ah_attr	alt_ah_attr;
+	u16			pkey_index;
+	u16			alt_pkey_index;
+	u8			en_sqd_async_notify;
+	u8			sq_draining;
+	u8			max_rd_atomic;
+	u8			max_dest_rd_atomic;
+	u8			min_rnr_timer;
+	u8			port_num;
+	u8			timeout;
+	u8			retry_cnt;
+	u8			rnr_retry;
+	u8			alt_port_num;
+	u8			alt_timeout;
+};
+
+enum ib_wr_opcode {
+	IB_WR_RDMA_WRITE,
+	IB_WR_RDMA_WRITE_WITH_IMM,
+	IB_WR_SEND,
+	IB_WR_SEND_WITH_IMM,
+	IB_WR_RDMA_READ,
+	IB_WR_ATOMIC_CMP_AND_SWP,
+	IB_WR_ATOMIC_FETCH_AND_ADD
+};
+
+enum ib_send_flags {
+	IB_SEND_FENCE		= 1,
+	IB_SEND_SIGNALED	= (1<<1),
+	IB_SEND_SOLICITED	= (1<<2),
+	IB_SEND_INLINE		= (1<<3)
+};
+
+struct ib_sge {
+	u64	addr;
+	u32	length;
+	u32	lkey;
+};
+
+struct ib_send_wr {
+	struct ib_send_wr      *next;
+	u64			wr_id;
+	struct ib_sge	       *sg_list;
+	int			num_sge;
+	enum ib_wr_opcode	opcode;
+	int			send_flags;
+	__be32			imm_data;
+	union {
+		struct {
+			u64	remote_addr;
+			u32	rkey;
+		} rdma;
+		struct {
+			u64	remote_addr;
+			u64	compare_add;
+			u64	swap;
+			u32	rkey;
+		} atomic;
+		struct {
+			struct ib_ah *ah;
+			struct ib_mad_hdr *mad_hdr;
+			u32	remote_qpn;
+			u32	remote_qkey;
+			int	timeout_ms; /* valid for MADs only */
+			int	retries;    /* valid for MADs only */
+			u16	pkey_index; /* valid for GSI only */
+			u8	port_num;   /* valid for DR SMPs on switch only */
+		} ud;
+	} wr;
+};
+
+struct ib_recv_wr {
+	struct ib_recv_wr      *next;
+	u64			wr_id;
+	struct ib_sge	       *sg_list;
+	int			num_sge;
+};
+
+enum ib_access_flags {
+	IB_ACCESS_LOCAL_WRITE	= 1,
+	IB_ACCESS_REMOTE_WRITE	= (1<<1),
+	IB_ACCESS_REMOTE_READ	= (1<<2),
+	IB_ACCESS_REMOTE_ATOMIC	= (1<<3),
+	IB_ACCESS_MW_BIND	= (1<<4)
+};
+
+struct ib_phys_buf {
+	u64      addr;
+	u64      size;
+};
+
+struct ib_mr_attr {
+	struct ib_pd	*pd;
+	u64		device_virt_addr;
+	u64		size;
+	int		mr_access_flags;
+	u32		lkey;
+	u32		rkey;
+};
+
+enum ib_mr_rereg_flags {
+	IB_MR_REREG_TRANS	= 1,
+	IB_MR_REREG_PD		= (1<<1),
+	IB_MR_REREG_ACCESS	= (1<<2)
+};
+
+struct ib_mw_bind {
+	struct ib_mr   *mr;
+	u64		wr_id;
+	u64		addr;
+	u32		length;
+	int		send_flags;
+	int		mw_access_flags;
+};
+
+struct ib_fmr_attr {
+	int	max_pages;
+	int	max_maps;
+	u8	page_size;
+};
+
+struct ib_ucontext {
+	struct ib_device       *device;
+	struct list_head	pd_list;
+	struct list_head	mr_list;
+	struct list_head	mw_list;
+	struct list_head	cq_list;
+	struct list_head	qp_list;
+	struct list_head	srq_list;
+	struct list_head	ah_list;
+	spinlock_t              lock;
+};
+
+struct ib_uobject {
+	u64			user_handle;	/* handle given to us by userspace */
+	struct ib_ucontext     *context;	/* associated user context */
+	struct list_head	list;		/* link to context's list */
+	u32			id;		/* index into kernel idr */
+};
+
+struct ib_umem {
+	unsigned long		user_base;
+	unsigned long		virt_base;
+	size_t			length;
+	int			offset;
+	int			page_size;
+	int                     writable;
+	struct list_head	chunk_list;
+};
+
+struct ib_umem_chunk {
+	struct list_head	list;
+	int                     nents;
+	int                     nmap;
+	struct scatterlist      page_list[0];
+};
+
+struct ib_udata {
+	void __user *inbuf;
+	void __user *outbuf;
+	size_t       inlen;
+	size_t       outlen;
+};
+
+#define IB_UMEM_MAX_PAGE_CHUNK						\
+	((PAGE_SIZE - offsetof(struct ib_umem_chunk, page_list)) /	\
+	 ((void *) &((struct ib_umem_chunk *) 0)->page_list[1] -	\
+	  (void *) &((struct ib_umem_chunk *) 0)->page_list[0]))
+
+struct ib_umem_object {
+	struct ib_uobject	uobject;
+	struct ib_umem		umem;
+};
+
+struct ib_pd {
+	struct ib_device       *device;
+	struct ib_uobject      *uobject;
+	atomic_t          	usecnt; /* count all resources */
+};
+
+struct ib_ah {
+	struct ib_device	*device;
+	struct ib_pd		*pd;
+	struct ib_uobject	*uobject;
+};
+
+typedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);
+
+struct ib_cq {
+	struct ib_device       *device;
+	struct ib_uobject      *uobject;
+	ib_comp_handler   	comp_handler;
+	void                  (*event_handler)(struct ib_event *, void *);
+	void *            	cq_context;
+	int               	cqe;
+	atomic_t          	usecnt; /* count number of work queues */
+};
+
+struct ib_srq {
+	struct ib_device       *device;
+	struct ib_pd	       *pd;
+	struct ib_uobject      *uobject;
+	void		      (*event_handler)(struct ib_event *, void *);
+	void		       *srq_context;
+	atomic_t		usecnt;
+};
+
+struct ib_qp {
+	struct ib_device       *device;
+	struct ib_pd	       *pd;
+	struct ib_cq	       *send_cq;
+	struct ib_cq	       *recv_cq;
+	struct ib_srq	       *srq;
+	struct ib_uobject      *uobject;
+	void                  (*event_handler)(struct ib_event *, void *);
+	void		       *qp_context;
+	u32			qp_num;
+	enum ib_qp_type		qp_type;
+};
+
+struct ib_mr {
+	struct ib_device  *device;
+	struct ib_pd	  *pd;
+	struct ib_uobject *uobject;
+	u32		   lkey;
+	u32		   rkey;
+	atomic_t	   usecnt; /* count number of MWs */
+};
+
+struct ib_mw {
+	struct ib_device	*device;
+	struct ib_pd		*pd;
+	struct ib_uobject	*uobject;
+	u32			rkey;
+};
+
+struct ib_fmr {
+	struct ib_device	*device;
+	struct ib_pd		*pd;
+	struct list_head	list;
+	u32			lkey;
+	u32			rkey;
+};
+
+struct ib_mad;
+struct ib_grh;
+
+enum ib_process_mad_flags {
+	IB_MAD_IGNORE_MKEY	= 1,
+	IB_MAD_IGNORE_BKEY	= 2,
+	IB_MAD_IGNORE_ALL	= IB_MAD_IGNORE_MKEY | IB_MAD_IGNORE_BKEY
+};
+
+enum ib_mad_result {
+	IB_MAD_RESULT_FAILURE  = 0,      /* (!SUCCESS is the important flag) */
+	IB_MAD_RESULT_SUCCESS  = 1 << 0, /* MAD was successfully processed   */
+	IB_MAD_RESULT_REPLY    = 1 << 1, /* Reply packet needs to be sent    */
+	IB_MAD_RESULT_CONSUMED = 1 << 2  /* Packet consumed: stop processing */
+};
+
+#define IB_DEVICE_NAME_MAX 64
+
+struct ib_cache {
+	rwlock_t                lock;
+	struct ib_event_handler event_handler;
+	struct ib_pkey_cache  **pkey_cache;
+	struct ib_gid_cache   **gid_cache;
+};
+
+struct ib_device {
+	struct device                *dma_device;
+
+	char                          name[IB_DEVICE_NAME_MAX];
+
+	struct list_head              event_handler_list;
+	spinlock_t                    event_handler_lock;
+
+	struct list_head              core_list;
+	struct list_head              client_data_list;
+	spinlock_t                    client_data_lock;
+
+	struct ib_cache               cache;
+
+	u32                           flags;
+
+	int		           (*query_device)(struct ib_device *device,
+						   struct ib_device_attr *device_attr);
+	int		           (*query_port)(struct ib_device *device,
+						 u8 port_num,
+						 struct ib_port_attr *port_attr);
+	int		           (*query_gid)(struct ib_device *device,
+						u8 port_num, int index,
+						union ib_gid *gid);
+	int		           (*query_pkey)(struct ib_device *device,
+						 u8 port_num, u16 index, u16 *pkey);
+	int		           (*modify_device)(struct ib_device *device,
+						    int device_modify_mask,
+						    struct ib_device_modify *device_modify);
+	int		           (*modify_port)(struct ib_device *device,
+						  u8 port_num, int port_modify_mask,
+						  struct ib_port_modify *port_modify);
+	struct ib_ucontext *       (*alloc_ucontext)(struct ib_device *device,
+						     struct ib_udata *udata);
+	int                        (*dealloc_ucontext)(struct ib_ucontext *context);
+	int                        (*mmap)(struct ib_ucontext *context,
+					   struct vm_area_struct *vma);
+	struct ib_pd *             (*alloc_pd)(struct ib_device *device,
+					       struct ib_ucontext *context,
+					       struct ib_udata *udata);
+	int                        (*dealloc_pd)(struct ib_pd *pd);
+	struct ib_ah *             (*create_ah)(struct ib_pd *pd,
+						struct ib_ah_attr *ah_attr);
+	int                        (*modify_ah)(struct ib_ah *ah,
+						struct ib_ah_attr *ah_attr);
+	int                        (*query_ah)(struct ib_ah *ah,
+					       struct ib_ah_attr *ah_attr);
+	int                        (*destroy_ah)(struct ib_ah *ah);
+	struct ib_srq *            (*create_srq)(struct ib_pd *pd,
+						 struct ib_srq_init_attr *srq_init_attr,
+						 struct ib_udata *udata);
+	int                        (*modify_srq)(struct ib_srq *srq,
+						 struct ib_srq_attr *srq_attr,
+						 enum ib_srq_attr_mask srq_attr_mask);
+	int                        (*query_srq)(struct ib_srq *srq,
+						struct ib_srq_attr *srq_attr);
+	int                        (*destroy_srq)(struct ib_srq *srq);
+	int                        (*post_srq_recv)(struct ib_srq *srq,
+						    struct ib_recv_wr *recv_wr,
+						    struct ib_recv_wr **bad_recv_wr);
+	struct ib_qp *             (*create_qp)(struct ib_pd *pd,
+						struct ib_qp_init_attr *qp_init_attr,
+						struct ib_udata *udata);
+	int                        (*modify_qp)(struct ib_qp *qp,
+						struct ib_qp_attr *qp_attr,
+						int qp_attr_mask);
+	int                        (*query_qp)(struct ib_qp *qp,
+					       struct ib_qp_attr *qp_attr,
+					       int qp_attr_mask,
+					       struct ib_qp_init_attr *qp_init_attr);
+	int                        (*destroy_qp)(struct ib_qp *qp);
+	int                        (*post_send)(struct ib_qp *qp,
+						struct ib_send_wr *send_wr,
+						struct ib_send_wr **bad_send_wr);
+	int                        (*post_recv)(struct ib_qp *qp,
+						struct ib_recv_wr *recv_wr,
+						struct ib_recv_wr **bad_recv_wr);
+	struct ib_cq *             (*create_cq)(struct ib_device *device, int cqe,
+						struct ib_ucontext *context,
+						struct ib_udata *udata);
+	int                        (*destroy_cq)(struct ib_cq *cq);
+	int                        (*resize_cq)(struct ib_cq *cq, int *cqe);
+	int                        (*poll_cq)(struct ib_cq *cq, int num_entries,
+					      struct ib_wc *wc);
+	int                        (*peek_cq)(struct ib_cq *cq, int wc_cnt);
+	int                        (*req_notify_cq)(struct ib_cq *cq,
+						    enum ib_cq_notify cq_notify);
+	int                        (*req_ncomp_notif)(struct ib_cq *cq,
+						      int wc_cnt);
+	struct ib_mr *             (*get_dma_mr)(struct ib_pd *pd,
+						 int mr_access_flags);
+	struct ib_mr *             (*reg_phys_mr)(struct ib_pd *pd,
+						  struct ib_phys_buf *phys_buf_array,
+						  int num_phys_buf,
+						  int mr_access_flags,
+						  u64 *iova_start);
+	struct ib_mr *             (*reg_user_mr)(struct ib_pd *pd,
+						  struct ib_umem *region,
+						  int mr_access_flags,
+						  struct ib_udata *udata);
+	int                        (*query_mr)(struct ib_mr *mr,
+					       struct ib_mr_attr *mr_attr);
+	int                        (*dereg_mr)(struct ib_mr *mr);
+	int                        (*rereg_phys_mr)(struct ib_mr *mr,
+						    int mr_rereg_mask,
+						    struct ib_pd *pd,
+						    struct ib_phys_buf *phys_buf_array,
+						    int num_phys_buf,
+						    int mr_access_flags,
+						    u64 *iova_start);
+	struct ib_mw *             (*alloc_mw)(struct ib_pd *pd);
+	int                        (*bind_mw)(struct ib_qp *qp,
+					      struct ib_mw *mw,
+					      struct ib_mw_bind *mw_bind);
+	int                        (*dealloc_mw)(struct ib_mw *mw);
+	struct ib_fmr *	           (*alloc_fmr)(struct ib_pd *pd,
+						int mr_access_flags,
+						struct ib_fmr_attr *fmr_attr);
+	int		           (*map_phys_fmr)(struct ib_fmr *fmr,
+						   u64 *page_list, int list_len,
+						   u64 iova);
+	int		           (*unmap_fmr)(struct list_head *fmr_list);
+	int		           (*dealloc_fmr)(struct ib_fmr *fmr);
+	int                        (*attach_mcast)(struct ib_qp *qp,
+						   union ib_gid *gid,
+						   u16 lid);
+	int                        (*detach_mcast)(struct ib_qp *qp,
+						   union ib_gid *gid,
+						   u16 lid);
+	int                        (*process_mad)(struct ib_device *device,
+						  int process_mad_flags,
+						  u8 port_num,
+						  struct ib_wc *in_wc,
+						  struct ib_grh *in_grh,
+						  struct ib_mad *in_mad,
+						  struct ib_mad *out_mad);
+
+	struct module               *owner;
+	struct class_device          class_dev;
+	struct kobject               ports_parent;
+	struct list_head             port_list;
+
+	enum {
+		IB_DEV_UNINITIALIZED,
+		IB_DEV_REGISTERED,
+		IB_DEV_UNREGISTERED
+	}                            reg_state;
+
+	u8                           node_type;
+	u8                           phys_port_cnt;
+};
+
+struct ib_client {
+	char  *name;
+	void (*add)   (struct ib_device *);
+	void (*remove)(struct ib_device *);
+
+	struct list_head list;
+};
+
+struct ib_device *ib_alloc_device(size_t size);
+void ib_dealloc_device(struct ib_device *device);
+
+int ib_register_device   (struct ib_device *device);
+void ib_unregister_device(struct ib_device *device);
+
+int ib_register_client   (struct ib_client *client);
+void ib_unregister_client(struct ib_client *client);
+
+void *ib_get_client_data(struct ib_device *device, struct ib_client *client);
+void  ib_set_client_data(struct ib_device *device, struct ib_client *client,
+			 void *data);
+
+static inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)
+{
+	return copy_from_user(dest, udata->inbuf, len) ? -EFAULT : 0;
+}
+
+static inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len)
+{
+	return copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;
+}
+
+int ib_register_event_handler  (struct ib_event_handler *event_handler);
+int ib_unregister_event_handler(struct ib_event_handler *event_handler);
+void ib_dispatch_event(struct ib_event *event);
+
+int ib_query_device(struct ib_device *device,
+		    struct ib_device_attr *device_attr);
+
+int ib_query_port(struct ib_device *device,
+		  u8 port_num, struct ib_port_attr *port_attr);
+
+int ib_query_gid(struct ib_device *device,
+		 u8 port_num, int index, union ib_gid *gid);
+
+int ib_query_pkey(struct ib_device *device,
+		  u8 port_num, u16 index, u16 *pkey);
+
+int ib_modify_device(struct ib_device *device,
+		     int device_modify_mask,
+		     struct ib_device_modify *device_modify);
+
+int ib_modify_port(struct ib_device *device,
+		   u8 port_num, int port_modify_mask,
+		   struct ib_port_modify *port_modify);
+
+/**
+ * ib_alloc_pd - Allocates an unused protection domain.
+ * @device: The device on which to allocate the protection domain.
+ *
+ * A protection domain object provides an association between QPs, shared
+ * receive queues, address handles, memory regions, and memory windows.
+ */
+struct ib_pd *ib_alloc_pd(struct ib_device *device);
+
+/**
+ * ib_dealloc_pd - Deallocates a protection domain.
+ * @pd: The protection domain to deallocate.
+ */
+int ib_dealloc_pd(struct ib_pd *pd);
+
+/**
+ * ib_create_ah - Creates an address handle for the given address vector.
+ * @pd: The protection domain associated with the address handle.
+ * @ah_attr: The attributes of the address vector.
+ *
+ * The address handle is used to reference a local or global destination
+ * in all UD QP post sends.
+ */
+struct ib_ah *ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
+
+/**
+ * ib_create_ah_from_wc - Creates an address handle associated with the
+ *   sender of the specified work completion.
+ * @pd: The protection domain associated with the address handle.
+ * @wc: Work completion information associated with a received message.
+ * @grh: References the received global route header.  This parameter is
+ *   ignored unless the work completion indicates that the GRH is valid.
+ * @port_num: The outbound port number to associate with the address.
+ *
+ * The address handle is used to reference a local or global destination
+ * in all UD QP post sends.
+ */
+struct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, struct ib_wc *wc,
+				   struct ib_grh *grh, u8 port_num);
+
+/**
+ * ib_modify_ah - Modifies the address vector associated with an address
+ *   handle.
+ * @ah: The address handle to modify.
+ * @ah_attr: The new address vector attributes to associate with the
+ *   address handle.
+ */
+int ib_modify_ah(struct ib_ah *ah, struct ib_ah_attr *ah_attr);
+
+/**
+ * ib_query_ah - Queries the address vector associated with an address
+ *   handle.
+ * @ah: The address handle to query.
+ * @ah_attr: The address vector attributes associated with the address
+ *   handle.
+ */
+int ib_query_ah(struct ib_ah *ah, struct ib_ah_attr *ah_attr);
+
+/**
+ * ib_destroy_ah - Destroys an address handle.
+ * @ah: The address handle to destroy.
+ */
+int ib_destroy_ah(struct ib_ah *ah);
+
+/**
+ * ib_create_srq - Creates a SRQ associated with the specified protection
+ *   domain.
+ * @pd: The protection domain associated with the SRQ.
+ * @srq_init_attr: A list of initial attributes required to create the SRQ.
+ *
+ * srq_attr->max_wr and srq_attr->max_sge are read the determine the
+ * requested size of the SRQ, and set to the actual values allocated
+ * on return.  If ib_create_srq() succeeds, then max_wr and max_sge
+ * will always be at least as large as the requested values.
+ */
+struct ib_srq *ib_create_srq(struct ib_pd *pd,
+			     struct ib_srq_init_attr *srq_init_attr);
+
+/**
+ * ib_modify_srq - Modifies the attributes for the specified SRQ.
+ * @srq: The SRQ to modify.
+ * @srq_attr: On input, specifies the SRQ attributes to modify.  On output,
+ *   the current values of selected SRQ attributes are returned.
+ * @srq_attr_mask: A bit-mask used to specify which attributes of the SRQ
+ *   are being modified.
+ *
+ * The mask may contain IB_SRQ_MAX_WR to resize the SRQ and/or
+ * IB_SRQ_LIMIT to set the SRQ's limit and request notification when
+ * the number of receives queued drops below the limit.
+ */
+int ib_modify_srq(struct ib_srq *srq,
+		  struct ib_srq_attr *srq_attr,
+		  enum ib_srq_attr_mask srq_attr_mask);
+
+/**
+ * ib_query_srq - Returns the attribute list and current values for the
+ *   specified SRQ.
+ * @srq: The SRQ to query.
+ * @srq_attr: The attributes of the specified SRQ.
+ */
+int ib_query_srq(struct ib_srq *srq,
+		 struct ib_srq_attr *srq_attr);
+
+/**
+ * ib_destroy_srq - Destroys the specified SRQ.
+ * @srq: The SRQ to destroy.
+ */
+int ib_destroy_srq(struct ib_srq *srq);
+
+/**
+ * ib_post_srq_recv - Posts a list of work requests to the specified SRQ.
+ * @srq: The SRQ to post the work request on.
+ * @recv_wr: A list of work requests to post on the receive queue.
+ * @bad_recv_wr: On an immediate failure, this parameter will reference
+ *   the work request that failed to be posted on the QP.
+ */
+static inline int ib_post_srq_recv(struct ib_srq *srq,
+				   struct ib_recv_wr *recv_wr,
+				   struct ib_recv_wr **bad_recv_wr)
+{
+	return srq->device->post_srq_recv(srq, recv_wr, bad_recv_wr);
+}
+
+/**
+ * ib_create_qp - Creates a QP associated with the specified protection
+ *   domain.
+ * @pd: The protection domain associated with the QP.
+ * @qp_init_attr: A list of initial attributes required to create the QP.
+ */
+struct ib_qp *ib_create_qp(struct ib_pd *pd,
+			   struct ib_qp_init_attr *qp_init_attr);
+
+/**
+ * ib_modify_qp - Modifies the attributes for the specified QP and then
+ *   transitions the QP to the given state.
+ * @qp: The QP to modify.
+ * @qp_attr: On input, specifies the QP attributes to modify.  On output,
+ *   the current values of selected QP attributes are returned.
+ * @qp_attr_mask: A bit-mask used to specify which attributes of the QP
+ *   are being modified.
+ */
+int ib_modify_qp(struct ib_qp *qp,
+		 struct ib_qp_attr *qp_attr,
+		 int qp_attr_mask);
+
+/**
+ * ib_query_qp - Returns the attribute list and current values for the
+ *   specified QP.
+ * @qp: The QP to query.
+ * @qp_attr: The attributes of the specified QP.
+ * @qp_attr_mask: A bit-mask used to select specific attributes to query.
+ * @qp_init_attr: Additional attributes of the selected QP.
+ *
+ * The qp_attr_mask may be used to limit the query to gathering only the
+ * selected attributes.
+ */
+int ib_query_qp(struct ib_qp *qp,
+		struct ib_qp_attr *qp_attr,
+		int qp_attr_mask,
+		struct ib_qp_init_attr *qp_init_attr);
+
+/**
+ * ib_destroy_qp - Destroys the specified QP.
+ * @qp: The QP to destroy.
+ */
+int ib_destroy_qp(struct ib_qp *qp);
+
+/**
+ * ib_post_send - Posts a list of work requests to the send queue of
+ *   the specified QP.
+ * @qp: The QP to post the work request on.
+ * @send_wr: A list of work requests to post on the send queue.
+ * @bad_send_wr: On an immediate failure, this parameter will reference
+ *   the work request that failed to be posted on the QP.
+ */
+static inline int ib_post_send(struct ib_qp *qp,
+			       struct ib_send_wr *send_wr,
+			       struct ib_send_wr **bad_send_wr)
+{
+	return qp->device->post_send(qp, send_wr, bad_send_wr);
+}
+
+/**
+ * ib_post_recv - Posts a list of work requests to the receive queue of
+ *   the specified QP.
+ * @qp: The QP to post the work request on.
+ * @recv_wr: A list of work requests to post on the receive queue.
+ * @bad_recv_wr: On an immediate failure, this parameter will reference
+ *   the work request that failed to be posted on the QP.
+ */
+static inline int ib_post_recv(struct ib_qp *qp,
+			       struct ib_recv_wr *recv_wr,
+			       struct ib_recv_wr **bad_recv_wr)
+{
+	return qp->device->post_recv(qp, recv_wr, bad_recv_wr);
+}
+
+/**
+ * ib_create_cq - Creates a CQ on the specified device.
+ * @device: The device on which to create the CQ.
+ * @comp_handler: A user-specified callback that is invoked when a
+ *   completion event occurs on the CQ.
+ * @event_handler: A user-specified callback that is invoked when an
+ *   asynchronous event not associated with a completion occurs on the CQ.
+ * @cq_context: Context associated with the CQ returned to the user via
+ *   the associated completion and event handlers.
+ * @cqe: The minimum size of the CQ.
+ *
+ * Users can examine the cq structure to determine the actual CQ size.
+ */
+struct ib_cq *ib_create_cq(struct ib_device *device,
+			   ib_comp_handler comp_handler,
+			   void (*event_handler)(struct ib_event *, void *),
+			   void *cq_context, int cqe);
+
+/**
+ * ib_resize_cq - Modifies the capacity of the CQ.
+ * @cq: The CQ to resize.
+ * @cqe: The minimum size of the CQ.
+ *
+ * Users can examine the cq structure to determine the actual CQ size.
+ */
+int ib_resize_cq(struct ib_cq *cq, int cqe);
+
+/**
+ * ib_destroy_cq - Destroys the specified CQ.
+ * @cq: The CQ to destroy.
+ */
+int ib_destroy_cq(struct ib_cq *cq);
+
+/**
+ * ib_poll_cq - poll a CQ for completion(s)
+ * @cq:the CQ being polled
+ * @num_entries:maximum number of completions to return
+ * @wc:array of at least @num_entries &struct ib_wc where completions
+ *   will be returned
+ *
+ * Poll a CQ for (possibly multiple) completions.  If the return value
+ * is < 0, an error occurred.  If the return value is >= 0, it is the
+ * number of completions returned.  If the return value is
+ * non-negative and < num_entries, then the CQ was emptied.
+ */
+static inline int ib_poll_cq(struct ib_cq *cq, int num_entries,
+			     struct ib_wc *wc)
+{
+	return cq->device->poll_cq(cq, num_entries, wc);
+}
+
+/**
+ * ib_peek_cq - Returns the number of unreaped completions currently
+ *   on the specified CQ.
+ * @cq: The CQ to peek.
+ * @wc_cnt: A minimum number of unreaped completions to check for.
+ *
+ * If the number of unreaped completions is greater than or equal to wc_cnt,
+ * this function returns wc_cnt, otherwise, it returns the actual number of
+ * unreaped completions.
+ */
+int ib_peek_cq(struct ib_cq *cq, int wc_cnt);
+
+/**
+ * ib_req_notify_cq - Request completion notification on a CQ.
+ * @cq: The CQ to generate an event for.
+ * @cq_notify: If set to %IB_CQ_SOLICITED, completion notification will
+ *   occur on the next solicited event. If set to %IB_CQ_NEXT_COMP,
+ *   notification will occur on the next completion.
+ */
+static inline int ib_req_notify_cq(struct ib_cq *cq,
+				   enum ib_cq_notify cq_notify)
+{
+	return cq->device->req_notify_cq(cq, cq_notify);
+}
+
+/**
+ * ib_req_ncomp_notif - Request completion notification when there are
+ *   at least the specified number of unreaped completions on the CQ.
+ * @cq: The CQ to generate an event for.
+ * @wc_cnt: The number of unreaped completions that should be on the
+ *   CQ before an event is generated.
+ */
+static inline int ib_req_ncomp_notif(struct ib_cq *cq, int wc_cnt)
+{
+	return cq->device->req_ncomp_notif ?
+		cq->device->req_ncomp_notif(cq, wc_cnt) :
+		-ENOSYS;
+}
+
+/**
+ * ib_get_dma_mr - Returns a memory region for system memory that is
+ *   usable for DMA.
+ * @pd: The protection domain associated with the memory region.
+ * @mr_access_flags: Specifies the memory access rights.
+ */
+struct ib_mr *ib_get_dma_mr(struct ib_pd *pd, int mr_access_flags);
+
+/**
+ * ib_reg_phys_mr - Prepares a virtually addressed memory region for use
+ *   by an HCA.
+ * @pd: The protection domain associated assigned to the registered region.
+ * @phys_buf_array: Specifies a list of physical buffers to use in the
+ *   memory region.
+ * @num_phys_buf: Specifies the size of the phys_buf_array.
+ * @mr_access_flags: Specifies the memory access rights.
+ * @iova_start: The offset of the region's starting I/O virtual address.
+ */
+struct ib_mr *ib_reg_phys_mr(struct ib_pd *pd,
+			     struct ib_phys_buf *phys_buf_array,
+			     int num_phys_buf,
+			     int mr_access_flags,
+			     u64 *iova_start);
+
+/**
+ * ib_rereg_phys_mr - Modifies the attributes of an existing memory region.
+ *   Conceptually, this call performs the functions deregister memory region
+ *   followed by register physical memory region.  Where possible,
+ *   resources are reused instead of deallocated and reallocated.
+ * @mr: The memory region to modify.
+ * @mr_rereg_mask: A bit-mask used to indicate which of the following
+ *   properties of the memory region are being modified.
+ * @pd: If %IB_MR_REREG_PD is set in mr_rereg_mask, this field specifies
+ *   the new protection domain to associated with the memory region,
+ *   otherwise, this parameter is ignored.
+ * @phys_buf_array: If %IB_MR_REREG_TRANS is set in mr_rereg_mask, this
+ *   field specifies a list of physical buffers to use in the new
+ *   translation, otherwise, this parameter is ignored.
+ * @num_phys_buf: If %IB_MR_REREG_TRANS is set in mr_rereg_mask, this
+ *   field specifies the size of the phys_buf_array, otherwise, this
+ *   parameter is ignored.
+ * @mr_access_flags: If %IB_MR_REREG_ACCESS is set in mr_rereg_mask, this
+ *   field specifies the new memory access rights, otherwise, this
+ *   parameter is ignored.
+ * @iova_start: The offset of the region's starting I/O virtual address.
+ */
+int ib_rereg_phys_mr(struct ib_mr *mr,
+		     int mr_rereg_mask,
+		     struct ib_pd *pd,
+		     struct ib_phys_buf *phys_buf_array,
+		     int num_phys_buf,
+		     int mr_access_flags,
+		     u64 *iova_start);
+
+/**
+ * ib_query_mr - Retrieves information about a specific memory region.
+ * @mr: The memory region to retrieve information about.
+ * @mr_attr: The attributes of the specified memory region.
+ */
+int ib_query_mr(struct ib_mr *mr, struct ib_mr_attr *mr_attr);
+
+/**
+ * ib_dereg_mr - Deregisters a memory region and removes it from the
+ *   HCA translation table.
+ * @mr: The memory region to deregister.
+ */
+int ib_dereg_mr(struct ib_mr *mr);
+
+/**
+ * ib_alloc_mw - Allocates a memory window.
+ * @pd: The protection domain associated with the memory window.
+ */
+struct ib_mw *ib_alloc_mw(struct ib_pd *pd);
+
+/**
+ * ib_bind_mw - Posts a work request to the send queue of the specified
+ *   QP, which binds the memory window to the given address range and
+ *   remote access attributes.
+ * @qp: QP to post the bind work request on.
+ * @mw: The memory window to bind.
+ * @mw_bind: Specifies information about the memory window, including
+ *   its address range, remote access rights, and associated memory region.
+ */
+static inline int ib_bind_mw(struct ib_qp *qp,
+			     struct ib_mw *mw,
+			     struct ib_mw_bind *mw_bind)
+{
+	/* XXX reference counting in corresponding MR? */
+	return mw->device->bind_mw ?
+		mw->device->bind_mw(qp, mw, mw_bind) :
+		-ENOSYS;
+}
+
+/**
+ * ib_dealloc_mw - Deallocates a memory window.
+ * @mw: The memory window to deallocate.
+ */
+int ib_dealloc_mw(struct ib_mw *mw);
+
+/**
+ * ib_alloc_fmr - Allocates a unmapped fast memory region.
+ * @pd: The protection domain associated with the unmapped region.
+ * @mr_access_flags: Specifies the memory access rights.
+ * @fmr_attr: Attributes of the unmapped region.
+ *
+ * A fast memory region must be mapped before it can be used as part of
+ * a work request.
+ */
+struct ib_fmr *ib_alloc_fmr(struct ib_pd *pd,
+			    int mr_access_flags,
+			    struct ib_fmr_attr *fmr_attr);
+
+/**
+ * ib_map_phys_fmr - Maps a list of physical pages to a fast memory region.
+ * @fmr: The fast memory region to associate with the pages.
+ * @page_list: An array of physical pages to map to the fast memory region.
+ * @list_len: The number of pages in page_list.
+ * @iova: The I/O virtual address to use with the mapped region.
+ */
+static inline int ib_map_phys_fmr(struct ib_fmr *fmr,
+				  u64 *page_list, int list_len,
+				  u64 iova)
+{
+	return fmr->device->map_phys_fmr(fmr, page_list, list_len, iova);
+}
+
+/**
+ * ib_unmap_fmr - Removes the mapping from a list of fast memory regions.
+ * @fmr_list: A linked list of fast memory regions to unmap.
+ */
+int ib_unmap_fmr(struct list_head *fmr_list);
+
+/**
+ * ib_dealloc_fmr - Deallocates a fast memory region.
+ * @fmr: The fast memory region to deallocate.
+ */
+int ib_dealloc_fmr(struct ib_fmr *fmr);
+
+/**
+ * ib_attach_mcast - Attaches the specified QP to a multicast group.
+ * @qp: QP to attach to the multicast group.  The QP must be type
+ *   IB_QPT_UD.
+ * @gid: Multicast group GID.
+ * @lid: Multicast group LID in host byte order.
+ *
+ * In order to send and receive multicast packets, subnet
+ * administration must have created the multicast group and configured
+ * the fabric appropriately.  The port associated with the specified
+ * QP must also be a member of the multicast group.
+ */
+int ib_attach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);
+
+/**
+ * ib_detach_mcast - Detaches the specified QP from a multicast group.
+ * @qp: QP to detach from the multicast group.
+ * @gid: Multicast group GID.
+ * @lid: Multicast group LID in host byte order.
+ */
+int ib_detach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);
+
+#endif /* IB_VERBS_H */
