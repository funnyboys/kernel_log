commit c320e527e1548305f31d95ec405140b04aed25f5
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Jan 15 14:43:31 2020 +0200

    IB: Allow calls to ib_umem_get from kernel ULPs
    
    So far the assumption was that ib_umem_get() and ib_umem_odp_get()
    are called from flows that start in UVERBS and therefore has a user
    context. This assumption restricts flows that are initiated by ULPs
    and need the service that ib_umem_get() provides.
    
    This patch changes ib_umem_get() and ib_umem_odp_get() to get IB device
    directly by relying on the fact that both UVERBS and ULPs sets that
    field correctly.
    
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 753f54e17e0a..e3518fd6b95b 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -69,7 +69,7 @@ static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
-struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
+struct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,
 			    size_t size, int access);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
@@ -83,7 +83,7 @@ unsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,
 
 #include <linux/err.h>
 
-static inline struct ib_umem *ib_umem_get(struct ib_udata *udata,
+static inline struct ib_umem *ib_umem_get(struct ib_device *device,
 					  unsigned long addr, size_t size,
 					  int access)
 {

commit 72b894b09a96b741c92562709f6629310f2b34a1
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 13 08:32:14 2019 +0100

    IB/umem: remove the dmasync argument to ib_umem_get
    
    The argument is always ignored, so remove it.
    
    Link: https://lore.kernel.org/r/20191113073214.9514-3-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Acked-by: Michal Kalderon <michal.kalderon@marvell.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index a91b2af64ec4..753f54e17e0a 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -70,7 +70,7 @@ static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
 struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
-			    size_t size, int access, int dmasync);
+			    size_t size, int access);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
 int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
@@ -85,7 +85,7 @@ unsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,
 
 static inline struct ib_umem *ib_umem_get(struct ib_udata *udata,
 					  unsigned long addr, size_t size,
-					  int access, int dmasync)
+					  int access)
 {
 	return ERR_PTR(-EINVAL);
 }

commit 47f725ee7b5f5cae1f83512961bcf8b41a7a5794
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Aug 6 20:15:44 2019 -0300

    RDMA/odp: remove ib_ucontext from ib_umem
    
    At this point the ucontext is only being stored to access the ib_device,
    so just store the ib_device directly instead. This is more natural and
    logical as the umem has nothing to do with the ucontext.
    
    Link: https://lore.kernel.org/r/20190806231548.25242-8-jgg@ziepe.ca
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 1052d0d62be7..a91b2af64ec4 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -42,7 +42,7 @@ struct ib_ucontext;
 struct ib_umem_odp;
 
 struct ib_umem {
-	struct ib_ucontext     *context;
+	struct ib_device       *ibdev;
 	struct mm_struct       *owning_mm;
 	size_t			length;
 	unsigned long		address;

commit d2183c6f1958e6b6dfdde279f4cee04280710e34
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon May 20 09:05:25 2019 +0300

    RDMA/umem: Move page_shift from ib_umem to ib_odp_umem
    
    This value has always been set to PAGE_SHIFT in the core code, the only
    thing that does differently was the ODP path. Move the value into the ODP
    struct and still use it for ODP, but change all the non-ODP things to just
    use PAGE_SHIFT/PAGE_SIZE/PAGE_MASK directly.
    
    Reviewed-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 040d853077c6..1052d0d62be7 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -46,7 +46,6 @@ struct ib_umem {
 	struct mm_struct       *owning_mm;
 	size_t			length;
 	unsigned long		address;
-	int			page_shift;
 	u32 writable : 1;
 	u32 is_odp : 1;
 	struct work_struct	work;
@@ -58,24 +57,14 @@ struct ib_umem {
 /* Returns the offset of the umem start relative to the first page. */
 static inline int ib_umem_offset(struct ib_umem *umem)
 {
-	return umem->address & (BIT(umem->page_shift) - 1);
-}
-
-/* Returns the first page of an ODP umem. */
-static inline unsigned long ib_umem_start(struct ib_umem *umem)
-{
-	return umem->address - ib_umem_offset(umem);
-}
-
-/* Returns the address of the page after the last one of an ODP umem. */
-static inline unsigned long ib_umem_end(struct ib_umem *umem)
-{
-	return ALIGN(umem->address + umem->length, BIT(umem->page_shift));
+	return umem->address & ~PAGE_MASK;
 }
 
 static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 {
-	return (ib_umem_end(umem) - ib_umem_start(umem)) >> umem->page_shift;
+	return (ALIGN(umem->address + umem->length, PAGE_SIZE) -
+		ALIGN_DOWN(umem->address, PAGE_SIZE)) >>
+	       PAGE_SHIFT;
 }
 
 #ifdef CONFIG_INFINIBAND_USER_MEM

commit db6c6774af0d4861a7c5181ecc3c9ac320de46d9
Author: Shiraz Saleem <shiraz.saleem@intel.com>
Date:   Mon May 6 08:53:36 2019 -0500

    RDMA/umem: Remove hugetlb flag
    
    The drivers i40iw and bnxt_re no longer dependent on the hugetlb flag. So
    remove this flag from ib_umem structure.
    
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 917b687010f0..040d853077c6 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -48,7 +48,6 @@ struct ib_umem {
 	unsigned long		address;
 	int			page_shift;
 	u32 writable : 1;
-	u32 hugetlb : 1;
 	u32 is_odp : 1;
 	struct work_struct	work;
 	struct sg_table sg_head;

commit 4a35339958f16d42a4ca06a8da9d4b5ab39ee8ea
Author: Shiraz Saleem <shiraz.saleem@intel.com>
Date:   Mon May 6 08:53:32 2019 -0500

    RDMA/umem: Add API to find best driver supported page size in an MR
    
    This helper iterates through the SG list to find the best page size to use
    from a bitmap of HW supported page sizes. Drivers that support multiple
    page sizes, but not mixed sizes in an MR can use this API.
    
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index b13a2e9a50d4..917b687010f0 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -87,6 +87,9 @@ void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
 int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
 		      size_t length);
+unsigned long ib_umem_find_best_pgsz(struct ib_umem *umem,
+				     unsigned long pgsz_bitmap,
+				     unsigned long virt);
 
 #else /* CONFIG_INFINIBAND_USER_MEM */
 
@@ -104,6 +107,12 @@ static inline int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offs
 		      		    size_t length) {
 	return -EINVAL;
 }
+static inline int ib_umem_find_best_pgsz(struct ib_umem *umem,
+					 unsigned long pgsz_bitmap,
+					 unsigned long virt) {
+	return -EINVAL;
+}
+
 #endif /* CONFIG_INFINIBAND_USER_MEM */
 
 #endif /* IB_UMEM_H */

commit d10bcf947a3ea240351a8182d71e4aa9c8ddba56
Author: Shiraz Saleem <shiraz.saleem@intel.com>
Date:   Tue Apr 2 14:52:52 2019 -0500

    RDMA/umem: Combine contiguous PAGE_SIZE regions in SGEs
    
    Combine contiguous regions of PAGE_SIZE pages into single scatter list
    entry while building the scatter table for a umem. This minimizes the
    number of the entries in the scatter list and reduces the DMA mapping
    overhead, particularly with the IOMMU.
    
    Set default max_seg_size in core for IB devices to 2G and do not combine
    if we exceed this limit.
    
    Also, purge npages in struct ib_umem as we now DMA map the umem SGL with
    sg_nents and npage computation is not needed. Drivers should now be using
    ib_umem_num_pages(), so fix the last stragglers.
    
    Move npages tracking to ib_umem_odp as ODP drivers still need it.
    
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Acked-by: Adit Ranadive <aditr@vmware.com>
    Signed-off-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Tested-by: Gal Pressman <galpress@amazon.com>
    Tested-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 73af05db04c7..b13a2e9a50d4 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -53,7 +53,7 @@ struct ib_umem {
 	struct work_struct	work;
 	struct sg_table sg_head;
 	int             nmap;
-	int             npages;
+	unsigned int    sg_nents;
 };
 
 /* Returns the offset of the umem start relative to the first page. */

commit b0ea0fa5435f9df7213a9af098558f7dd584d8e8
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 9 11:15:16 2019 +0200

    IB/{core,hw}: Have ib_umem_get extract the ib_ucontext from ib_udata
    
    ib_umem_get() can only be called in a method callback, which always has a
    udata parameter. This allows ib_umem_get() to derive the ucontext pointer
    directly from the udata without requiring the drivers to find it in some
    way or another.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 5d3755ec5afa..73af05db04c7 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -36,6 +36,7 @@
 #include <linux/list.h>
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
+#include <rdma/ib_verbs.h>
 
 struct ib_ucontext;
 struct ib_umem_odp;
@@ -80,7 +81,7 @@ static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
-struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
+struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
 			    size_t size, int access, int dmasync);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
@@ -91,9 +92,10 @@ int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
 
 #include <linux/err.h>
 
-static inline struct ib_umem *ib_umem_get(struct ib_ucontext *context,
+static inline struct ib_umem *ib_umem_get(struct ib_udata *udata,
 					  unsigned long addr, size_t size,
-					  int access, int dmasync) {
+					  int access, int dmasync)
+{
 	return ERR_PTR(-EINVAL);
 }
 static inline void ib_umem_release(struct ib_umem *umem) { }

commit 597ecc5a095406a668e53ab330495ddb65327f77
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:06 2018 +0300

    RDMA/umem: Get rid of struct ib_umem.odp_data
    
    This no longer has any use, we can use container_of to get to the
    umem_odp, and a simple flag to indicate if this is an odp MR. Remove the
    few remaining references to it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index e1c00b2ead19..5d3755ec5afa 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -46,10 +46,10 @@ struct ib_umem {
 	size_t			length;
 	unsigned long		address;
 	int			page_shift;
-	int                     writable;
-	int                     hugetlb;
+	u32 writable : 1;
+	u32 hugetlb : 1;
+	u32 is_odp : 1;
 	struct work_struct	work;
-	struct ib_umem_odp     *odp_data;
 	struct sg_table sg_head;
 	int             nmap;
 	int             npages;

commit d4b4dd1b9706e48c370f88d3adfe713e43423cc9
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:44:45 2018 +0300

    RDMA/umem: Do not use current->tgid to track the mm_struct
    
    This is just wrong, the process that calls into the reg_mr is the process
    associated with the umem, and that does not have to be the same process
    that created the context.
    
    When this code was first written mmgrab() didn't exist, however these days
    we can just directly hold the mm_struct pointer in the umem and have no
    ambiguity when it comes to releasing the umem as to which mm it was
    associated with.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index a1fd63871d17..e1c00b2ead19 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -42,14 +42,13 @@ struct ib_umem_odp;
 
 struct ib_umem {
 	struct ib_ucontext     *context;
+	struct mm_struct       *owning_mm;
 	size_t			length;
 	unsigned long		address;
 	int			page_shift;
 	int                     writable;
 	int                     hugetlb;
 	struct work_struct	work;
-	struct mm_struct       *mm;
-	unsigned long		diff;
 	struct ib_umem_odp     *odp_data;
 	struct sg_table sg_head;
 	int             nmap;

commit 8e907ed4882714fd13cfe670681fc6cb5284c780
Author: Lidong Chen <jemmy858585@gmail.com>
Date:   Tue May 8 16:50:16 2018 +0800

    IB/umem: Use the correct mm during ib_umem_release
    
    User-space may invoke ibv_reg_mr and ibv_dereg_mr in different threads.
    
    If ibv_dereg_mr is called after the thread which invoked ibv_reg_mr has
    exited, get_pid_task will return NULL and ib_umem_release will not
    decrease mm->pinned_vm.
    
    Instead of using threads to locate the mm, use the overall tgid from the
    ib_ucontext struct instead. This matches the behavior of ODP and
    disassociate in handling the mm of the process that called ibv_reg_mr.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 87773dd56d54 ("IB: ib_umem_release() should decrement mm->pinned_vm from ib_umem_get")
    Signed-off-by: Lidong Chen <lidongchen@tencent.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 23159dd5be18..a1fd63871d17 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -48,7 +48,6 @@ struct ib_umem {
 	int                     writable;
 	int                     hugetlb;
 	struct work_struct	work;
-	struct pid             *pid;
 	struct mm_struct       *mm;
 	unsigned long		diff;
 	struct ib_umem_odp     *odp_data;

commit 403cd12e2cf759ead5cbdcb62bf9872b9618d400
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Apr 5 09:23:55 2017 +0300

    IB/umem: Add contiguous ODP support
    
    Currenlty ODP supports only regular MMU pages.
    Add ODP support for regions consisting of physically contiguous chunks
    of arbitrary order (huge pages for instance) to improve performance.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 7f4af1e1ae64..23159dd5be18 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -72,12 +72,12 @@ static inline unsigned long ib_umem_start(struct ib_umem *umem)
 /* Returns the address of the page after the last one of an ODP umem. */
 static inline unsigned long ib_umem_end(struct ib_umem *umem)
 {
-	return PAGE_ALIGN(umem->address + umem->length);
+	return ALIGN(umem->address + umem->length, BIT(umem->page_shift));
 }
 
 static inline size_t ib_umem_num_pages(struct ib_umem *umem)
 {
-	return (ib_umem_end(umem) - ib_umem_start(umem)) >> PAGE_SHIFT;
+	return (ib_umem_end(umem) - ib_umem_start(umem)) >> umem->page_shift;
 }
 
 #ifdef CONFIG_INFINIBAND_USER_MEM

commit 3e7e1193e28a1428e857f3f44870ec2dbd615e6a
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Apr 5 09:23:50 2017 +0300

    IB: Replace ib_umem page_size by page_shift
    
    Size of pages are held by struct ib_umem in page_size field.
    
    It is better to store it as an exponent, because page size by nature
    is always power-of-two and used as a factor, divisor or ilog2's argument.
    
    The conversion of page_size to be page_shift allows to have portable
    code and avoid following error while compiling on ARM:
    
      ERROR: "__aeabi_uldivmod" [drivers/infiniband/core/ib_core.ko] undefined!
    
    CC: Selvin Xavier <selvin.xavier@broadcom.com>
    CC: Steve Wise <swise@chelsio.com>
    CC: Lijun Ou <oulijun@huawei.com>
    CC: Shiraz Saleem <shiraz.saleem@intel.com>
    CC: Adit Ranadive <aditr@vmware.com>
    CC: Dennis Dalessandro <dennis.dalessandro@intel.com>
    CC: Ram Amrani <Ram.Amrani@Cavium.com>
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Acked-by: Ram Amrani <Ram.Amrani@cavium.com>
    Acked-by: Shiraz Saleem <shiraz.saleem@intel.com>
    Acked-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Acked-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Acked-by: Adit Ranadive <aditr@vmware.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 2d83cfd7e6ce..7f4af1e1ae64 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -44,7 +44,7 @@ struct ib_umem {
 	struct ib_ucontext     *context;
 	size_t			length;
 	unsigned long		address;
-	int			page_size;
+	int			page_shift;
 	int                     writable;
 	int                     hugetlb;
 	struct work_struct	work;
@@ -60,7 +60,7 @@ struct ib_umem {
 /* Returns the offset of the umem start relative to the first page. */
 static inline int ib_umem_offset(struct ib_umem *umem)
 {
-	return umem->address & ((unsigned long)umem->page_size - 1);
+	return umem->address & (BIT(umem->page_shift) - 1);
 }
 
 /* Returns the first page of an ODP umem. */

commit 8ada2c1c0c1d75a60723cd2ca7d49c594a146af6
Author: Shachar Raindel <raindel@mellanox.com>
Date:   Thu Dec 11 17:04:17 2014 +0200

    IB/core: Add support for on demand paging regions
    
    * Extend the umem struct to keep the ODP related data.
    * Allocate and initialize the ODP related information in the umem
      (page_list, dma_list) and freeing as needed in the end of the run.
    * Store a reference to the process PID struct in the ucontext.  Used to
      safely obtain the task_struct and the mm during fault handling,
      without preventing the task destruction if needed.
    * Add 2 helper functions: ib_umem_odp_map_dma_pages and
      ib_umem_odp_unmap_dma_pages. These functions get the DMA addresses
      of specific pages of the umem (and, currently, pin them).
    * Support for page faults only - IB core will keep the reference on
      the pages used and call put_page when freeing an ODP umem
      area. Invalidations support will be added in a later patch.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index a51f4091489a..2d83cfd7e6ce 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -38,6 +38,7 @@
 #include <linux/workqueue.h>
 
 struct ib_ucontext;
+struct ib_umem_odp;
 
 struct ib_umem {
 	struct ib_ucontext     *context;
@@ -50,6 +51,7 @@ struct ib_umem {
 	struct pid             *pid;
 	struct mm_struct       *mm;
 	unsigned long		diff;
+	struct ib_umem_odp     *odp_data;
 	struct sg_table sg_head;
 	int             nmap;
 	int             npages;

commit c1395a2a8c01e8a919e47d64eb3d23d00e824b8b
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:14 2014 +0200

    IB/mlx5: Add function to read WQE from user-space
    
    Add a helper function mlx5_ib_read_user_wqe to read information from
    user-space owned work queues.  The function will be used in a later
    patch by the page-fault handling code in mlx5_ib.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    
    [ Add stub for ib_umem_copy_from() for CONFIG_INFINIBAND_USER_MEM=n
      - Roland ]
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 45bb04bc88cd..a51f4091489a 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -98,7 +98,10 @@ static inline struct ib_umem *ib_umem_get(struct ib_ucontext *context,
 }
 static inline void ib_umem_release(struct ib_umem *umem) { }
 static inline int ib_umem_page_count(struct ib_umem *umem) { return 0; }
-
+static inline int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
+		      		    size_t length) {
+	return -EINVAL;
+}
 #endif /* CONFIG_INFINIBAND_USER_MEM */
 
 #endif /* IB_UMEM_H */

commit c5d76f130b286682b64c659eaf6af701e3d79a7b
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:13 2014 +0200

    IB/core: Add umem function to read data from user-space
    
    In some drivers there's a need to read data from a user space area
    that was pinned using ib_umem when running from a different process
    context.
    
    The ib_umem_copy_from function allows reading data from the physical
    pages pinned in the ib_umem struct.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 7ed6d4ff58dc..45bb04bc88cd 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -84,6 +84,8 @@ struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
 			    size_t size, int access, int dmasync);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
+int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
+		      size_t length);
 
 #else /* CONFIG_INFINIBAND_USER_MEM */
 

commit 406f9e5fa9a7a60b42e676841e39f2d752266814
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:12 2014 +0200

    IB/core: Replace ib_umem's offset field with a full address
    
    In order to allow umems that do not pin memory, we need the umem to
    keep track of its region's address.
    
    This makes the offset field redundant, and so this patch removes it.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index a2bf41e0bde9..7ed6d4ff58dc 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -42,7 +42,7 @@ struct ib_ucontext;
 struct ib_umem {
 	struct ib_ucontext     *context;
 	size_t			length;
-	int			offset;
+	unsigned long		address;
 	int			page_size;
 	int                     writable;
 	int                     hugetlb;
@@ -55,6 +55,29 @@ struct ib_umem {
 	int             npages;
 };
 
+/* Returns the offset of the umem start relative to the first page. */
+static inline int ib_umem_offset(struct ib_umem *umem)
+{
+	return umem->address & ((unsigned long)umem->page_size - 1);
+}
+
+/* Returns the first page of an ODP umem. */
+static inline unsigned long ib_umem_start(struct ib_umem *umem)
+{
+	return umem->address - ib_umem_offset(umem);
+}
+
+/* Returns the address of the page after the last one of an ODP umem. */
+static inline unsigned long ib_umem_end(struct ib_umem *umem)
+{
+	return PAGE_ALIGN(umem->address + umem->length);
+}
+
+static inline size_t ib_umem_num_pages(struct ib_umem *umem)
+{
+	return (ib_umem_end(umem) - ib_umem_start(umem)) >> PAGE_SHIFT;
+}
+
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
 struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,

commit 87773dd56d5405ac28119fcfadacefd35877c18f
Author: Shawn Bohrer <sbohrer@rgmadvisors.com>
Date:   Wed Sep 3 12:13:57 2014 -0500

    IB: ib_umem_release() should decrement mm->pinned_vm from ib_umem_get
    
    In debugging an application that receives -ENOMEM from ib_reg_mr(), I
    found that ib_umem_get() can fail because the pinned_vm count has
    wrapped causing it to always be larger than the lock limit even with
    RLIMIT_MEMLOCK set to RLIM_INFINITY.
    
    The wrapping of pinned_vm occurs because the process that calls
    ib_reg_mr() will have its mm->pinned_vm count incremented.  Later a
    different process with a different mm_struct than the one that
    allocated the ib_umem struct ends up releasing it which results in
    decrementing the new processes mm->pinned_vm count past zero and
    wrapping.
    
    I'm not entirely sure what circumstances cause a different process to
    release the ib_umem than the one that allocated it but the kernel
    stack trace of the freeing process from my situation looks like the
    following:
    
        Call Trace:
         [<ffffffff814d64b1>] dump_stack+0x19/0x1b
         [<ffffffffa0b522a5>] ib_umem_release+0x1f5/0x200 [ib_core]
         [<ffffffffa0b90681>] mlx4_ib_destroy_qp+0x241/0x440 [mlx4_ib]
         [<ffffffffa0b4d93c>] ib_destroy_qp+0x12c/0x170 [ib_core]
         [<ffffffffa0cc7129>] ib_uverbs_close+0x259/0x4e0 [ib_uverbs]
         [<ffffffff81141cba>] __fput+0xba/0x240
         [<ffffffff81141e4e>] ____fput+0xe/0x10
         [<ffffffff81060894>] task_work_run+0xc4/0xe0
         [<ffffffff810029e5>] do_notify_resume+0x95/0xa0
         [<ffffffff814e3dd0>] int_signal+0x12/0x17
    
    The following patch fixes the issue by storing the pid struct of the
    process that calls ib_umem_get() so that ib_umem_release and/or
    ib_umem_account() can properly decrement the pinned_vm count of the
    correct mm_struct.
    
    Signed-off-by: Shawn Bohrer <sbohrer@rgmadvisors.com>
    Reviewed-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 1ea0b65c4cfb..a2bf41e0bde9 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -47,6 +47,7 @@ struct ib_umem {
 	int                     writable;
 	int                     hugetlb;
 	struct work_struct	work;
+	struct pid             *pid;
 	struct mm_struct       *mm;
 	unsigned long		diff;
 	struct sg_table sg_head;

commit eeb8461e36c99fdf2d058751be924a2aab215005
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Jan 28 13:40:15 2014 +0200

    IB: Refactor umem to use linear SG table
    
    This patch refactors the IB core umem code and vendor drivers to use a
    linear (chained) SG table instead of chunk list.  With this change the
    relevant code becomes clearer—no need for nested loops to build and
    use umem.
    
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 9ee0d2e51b16..1ea0b65c4cfb 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -46,17 +46,12 @@ struct ib_umem {
 	int			page_size;
 	int                     writable;
 	int                     hugetlb;
-	struct list_head	chunk_list;
 	struct work_struct	work;
 	struct mm_struct       *mm;
 	unsigned long		diff;
-};
-
-struct ib_umem_chunk {
-	struct list_head	list;
-	int                     nents;
-	int                     nmap;
-	struct scatterlist      page_list[0];
+	struct sg_table sg_head;
+	int             nmap;
+	int             npages;
 };
 
 #ifdef CONFIG_INFINIBAND_USER_MEM

commit cb9fbc5c37b69ac584e61d449cfd590f5ae1f90d
Author: Arthur Kepner <akepner@sgi.com>
Date:   Tue Apr 29 01:00:34 2008 -0700

    IB: expand ib_umem_get() prototype
    
    Add a new parameter, dmasync, to the ib_umem_get() prototype.  Use dmasync = 1
    when mapping user-allocated CQs with ib_umem_get().
    
    Signed-off-by: Arthur Kepner <akepner@sgi.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Grant Grundler <grundler@parisc-linux.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 22298423cf0b..9ee0d2e51b16 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -62,7 +62,7 @@ struct ib_umem_chunk {
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
 struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
-			    size_t size, int access);
+			    size_t size, int access, int dmasync);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_page_count(struct ib_umem *umem);
 
@@ -72,7 +72,7 @@ int ib_umem_page_count(struct ib_umem *umem);
 
 static inline struct ib_umem *ib_umem_get(struct ib_ucontext *context,
 					  unsigned long addr, size_t size,
-					  int access) {
+					  int access, int dmasync) {
 	return ERR_PTR(-EINVAL);
 }
 static inline void ib_umem_release(struct ib_umem *umem) { }

commit c8d8beea0383e47c9d65d45f0ca95626ec435fcd
Author: Joachim Fenkes <fenkes@de.ibm.com>
Date:   Thu Sep 13 18:15:28 2007 +0200

    IB/umem: Add hugetlb flag to struct ib_umem
    
    During ib_umem_get(), determine whether all pages from the memory
    region are hugetlb pages and report this in the "hugetlb" member.
    Low-level drivers can use this information if they need it.
    
    Signed-off-by: Joachim Fenkes <fenkes@de.ibm.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index c533d6c7903f..22298423cf0b 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -45,6 +45,7 @@ struct ib_umem {
 	int			offset;
 	int			page_size;
 	int                     writable;
+	int                     hugetlb;
 	struct list_head	chunk_list;
 	struct work_struct	work;
 	struct mm_struct       *mm;

commit e8edc6e03a5c8562dc70a6d969f732bdb355a7e7
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 21 01:22:52 2007 +0400

    Detach sched.h from mm.h
    
    First thing mm.h does is including sched.h solely for can_do_mlock() inline
    function which has "current" dereference inside. By dealing with can_do_mlock()
    mm.h can be detached from sched.h which is good. See below, why.
    
    This patch
    a) removes unconditional inclusion of sched.h from mm.h
    b) makes can_do_mlock() normal function in mm/mlock.c
    c) exports can_do_mlock() to not break compilation
    d) adds sched.h inclusions back to files that were getting it indirectly.
    e) adds less bloated headers to some files (asm/signal.h, jiffies.h) that were
       getting them indirectly
    
    Net result is:
    a) mm.h users would get less code to open, read, preprocess, parse, ... if
       they don't need sched.h
    b) sched.h stops being dependency for significant number of files:
       on x86_64 allmodconfig touching sched.h results in recompile of 4083 files,
       after patch it's only 3744 (-8.3%).
    
    Cross-compile tested on
    
            all arm defconfigs, all mips defconfigs, all powerpc defconfigs,
            alpha alpha-up
            arm
            i386 i386-up i386-defconfig i386-allnoconfig
            ia64 ia64-up
            m68k
            mips
            parisc parisc-up
            powerpc powerpc-up
            s390 s390-up
            sparc sparc-up
            sparc64 sparc64-up
            um-x86_64
            x86_64 x86_64-up x86_64-defconfig x86_64-allnoconfig
    
    as well as my two usual configs.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index b3a36f7d79e5..c533d6c7903f 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -35,6 +35,7 @@
 
 #include <linux/list.h>
 #include <linux/scatterlist.h>
+#include <linux/workqueue.h>
 
 struct ib_ucontext;
 

commit 1bf66a30421ca772820f489d88c16d0c430d6a67
Author: Roland Dreier <rolandd@cisco.com>
Date:   Wed Apr 18 20:20:28 2007 -0700

    IB: Put rlimit accounting struct in struct ib_umem
    
    When memory pinned with ib_umem_get() is released, ib_umem_release()
    needs to subtract the amount of memory being unpinned from
    mm->locked_vm.  However, ib_umem_release() may be called with
    mm->mmap_sem already held for writing if the memory is being released
    as part of an munmap() call, so it is sometimes necessary to defer
    this accounting into a workqueue.
    
    However, the work struct used to defer this accounting is dynamically
    allocated before it is queued, so there is the possibility of failing
    that allocation.  If the allocation fails, then ib_umem_release has no
    choice except to bail out and leave the process with a permanently
    elevated locked_vm.
    
    Fix this by allocating the structure to defer accounting as part of
    the original struct ib_umem, so there's no possibility of failing a
    later allocation if creating the struct ib_umem and pinning memory
    succeeds.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
index 06307f7e43e0..b3a36f7d79e5 100644
--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -45,6 +45,9 @@ struct ib_umem {
 	int			page_size;
 	int                     writable;
 	struct list_head	chunk_list;
+	struct work_struct	work;
+	struct mm_struct       *mm;
+	unsigned long		diff;
 };
 
 struct ib_umem_chunk {

commit f7c6a7b5d59980b076abbf2ceeb8735591290285
Author: Roland Dreier <rolandd@cisco.com>
Date:   Sun Mar 4 16:15:11 2007 -0800

    IB/uverbs: Export ib_umem_get()/ib_umem_release() to modules
    
    Export ib_umem_get()/ib_umem_release() and put low-level drivers in
    control of when to call ib_umem_get() to pin and DMA map userspace,
    rather than always calling it in ib_uverbs_reg_mr() before calling the
    low-level driver's reg_user_mr method.
    
    Also move these functions to be in the ib_core module instead of
    ib_uverbs, so that driver modules using them do not depend on
    ib_uverbs.
    
    This has a number of advantages:
     - It is better design from the standpoint of making generic code a
       library that can be used or overridden by device-specific code as
       the details of specific devices dictate.
     - Drivers that do not need to pin userspace memory regions do not
       need to take the performance hit of calling ib_mem_get().  For
       example, although I have not tried to implement it in this patch,
       the ipath driver should be able to avoid pinning memory and just
       use copy_{to,from}_user() to access userspace memory regions.
     - Buffers that need special mapping treatment can be identified by
       the low-level driver.  For example, it may be possible to solve
       some Altix-specific memory ordering issues with mthca CQs in
       userspace by mapping CQ buffers with extra flags.
     - Drivers that need to pin and DMA map userspace memory for things
       other than memory regions can use ib_umem_get() directly, instead
       of hacks using extra parameters to their reg_phys_mr method.  For
       example, the mlx4 driver that is pending being merged needs to pin
       and DMA map QP and CQ buffers, but it does not need to create a
       memory key for these buffers.  So the cleanest solution is for mlx4
       to call ib_umem_get() in the create_qp and create_cq methods.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

diff --git a/include/rdma/ib_umem.h b/include/rdma/ib_umem.h
new file mode 100644
index 000000000000..06307f7e43e0
--- /dev/null
+++ b/include/rdma/ib_umem.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (c) 2007 Cisco Systems.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef IB_UMEM_H
+#define IB_UMEM_H
+
+#include <linux/list.h>
+#include <linux/scatterlist.h>
+
+struct ib_ucontext;
+
+struct ib_umem {
+	struct ib_ucontext     *context;
+	size_t			length;
+	int			offset;
+	int			page_size;
+	int                     writable;
+	struct list_head	chunk_list;
+};
+
+struct ib_umem_chunk {
+	struct list_head	list;
+	int                     nents;
+	int                     nmap;
+	struct scatterlist      page_list[0];
+};
+
+#ifdef CONFIG_INFINIBAND_USER_MEM
+
+struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,
+			    size_t size, int access);
+void ib_umem_release(struct ib_umem *umem);
+int ib_umem_page_count(struct ib_umem *umem);
+
+#else /* CONFIG_INFINIBAND_USER_MEM */
+
+#include <linux/err.h>
+
+static inline struct ib_umem *ib_umem_get(struct ib_ucontext *context,
+					  unsigned long addr, size_t size,
+					  int access) {
+	return ERR_PTR(-EINVAL);
+}
+static inline void ib_umem_release(struct ib_umem *umem) { }
+static inline int ib_umem_page_count(struct ib_umem *umem) { return 0; }
+
+#endif /* CONFIG_INFINIBAND_USER_MEM */
+
+#endif /* IB_UMEM_H */
