commit d3bd37f587b4438d47751d0f1d5aaae3d39bd416
Merge: 60347451ddb0 2c523b344dfa
Author: Dave Airlie <airlied@redhat.com>
Date:   Wed Mar 11 07:27:21 2020 +1000

    Merge v5.6-rc5 into drm-next
    
    Requested my mripard for some misc patches that need this as a base.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit 1b245ec5b685ebf8e6e5d1e6b5bcc03b6608e8b0
Merge: 11a48a5a18c6 06f749af622c
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Feb 20 15:21:02 2020 +1000

    Merge tag 'drm-misc-next-2020-02-10' of git://anongit.freedesktop.org/drm/drm-misc into drm-next
    
    drm-misc-next for 5.7:
    
    UAPI Changes:
      - lima: Add support for heap buffers
    
    Cross-subsystem Changes:
    
    Core Changes:
      - Implement mode_config mode_valid for memory constrained drivers
      - Bus format negociation between bridges
      - Consolidate fake vblank events for drivers without vblank interrupts
      - drm/bufs: dma_alloc related cleanups
      - drm/dp_mst: Various fixes
      - drm/print: New drm_device based print helpers
      - Thomas is a drm-misc maintainer now!
    
    Driver Changes:
      - DPMS cleanups for atomic drivers
      - Removal of owner field in SPI tinydrm drivers
      - Removal of explicit dependency on DT for tinydrm drivers
      - Conversion to YAML schemas for DT bindings
      - tidss: New driver
      - virtio: various reworks and fixes
      - Our usual dozen or so new panels or bridges
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Maxime Ripard <maxime@cerno.tech>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200210093421.xu4sofldm6wm6xq6@gilmour.lan

commit 984f0103fcd153363f77eb6dc68ab6caf8a5a779
Merge: bb6d3fb354c5 7e0cf7e9936c
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Feb 14 12:52:49 2020 +1000

    Merge tag 'drm-misc-fixes-2020-02-07' of git://anongit.freedesktop.org/drm/drm-misc into drm-fixes
    
    Fixes for v5.6:
    - Revert allow_fb_modifiers in sun4i, as it causes a regression for DE2 and DE3.
    - Fix null pointer deref in drm_dp_mst_process_up_req().
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/672810c3-4212-0a46-337b-2cb855573fd2@linux.intel.com

commit fe154a2422333c4cd87bb0d8a19fb2df066cc6ee
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Feb 3 15:27:24 2020 +0000

    drm/panfrost: Remove set but not used variable 'bo'
    
    Fixes gcc '-Wunused-but-set-variable' warning:
    
    drivers/gpu/drm/panfrost/panfrost_job.c: In function 'panfrost_job_cleanup':
    drivers/gpu/drm/panfrost/panfrost_job.c:278:31: warning:
     variable 'bo' set but not used [-Wunused-but-set-variable]
    
    commit bdefca2d8dc0 ("drm/panfrost: Add the panfrost_gem_mapping concept")
    involved this unused variable.
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Reviewed-by: Alyssas Rosenzweig <alyssa.rosenzweig@collabora.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200203152724.42611-1-yuehaibing@huawei.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 7157dfd7dea3..9a1a72a748e7 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -280,12 +280,8 @@ static void panfrost_job_cleanup(struct kref *ref)
 	}
 
 	if (job->bos) {
-		struct panfrost_gem_object *bo;
-
-		for (i = 0; i < job->bo_count; i++) {
-			bo = to_panfrost_bo(job->bos[i]);
+		for (i = 0; i < job->bo_count; i++)
 			drm_gem_object_put_unlocked(job->bos[i]);
-		}
 
 		kvfree(job->bos);
 	}

commit 74c12ee02af109adcde36ec184fa59c0afb0edaa
Merge: 48bc281e4bf0 bb6d3fb354c5
Author: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
Date:   Wed Feb 12 14:08:59 2020 +0100

    Merge v5.6-rc1 into drm-misc-fixes
    
    We're based on v5.6, need v5.6-rc1 at least. :)
    
    Signed-off-by: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>

commit 7e0cf7e9936c4358b0863357b90aa12afe6489da
Author: Boris Brezillon <boris.brezillon@collabora.com>
Date:   Fri Nov 29 14:59:08 2019 +0100

    drm/panfrost: Make sure the shrinker does not reclaim referenced BOs
    
    Userspace might tag a BO purgeable while it's still referenced by GPU
    jobs. We need to make sure the shrinker does not purge such BOs until
    all jobs referencing it are finished.
    
    Fixes: 013b65101315 ("drm/panfrost: Add madvise and shrinker support")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191129135908.2439529-9-boris.brezillon@collabora.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index e364ee00f3d0..4d383831c1fc 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -269,8 +269,13 @@ static void panfrost_job_cleanup(struct kref *ref)
 	dma_fence_put(job->render_done_fence);
 
 	if (job->mappings) {
-		for (i = 0; i < job->bo_count; i++)
+		for (i = 0; i < job->bo_count; i++) {
+			if (!job->mappings[i])
+				break;
+
+			atomic_dec(&job->mappings[i]->obj->gpu_usecount);
 			panfrost_gem_mapping_put(job->mappings[i]);
+		}
 		kvfree(job->mappings);
 	}
 

commit 9f68e3655aae6d49d6ba05dd263f99f33c2567af
Merge: 4cadc60d6bcf d47c7f062680
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 30 08:04:01 2020 -0800

    Merge tag 'drm-next-2020-01-30' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm updates from Davbe Airlie:
     "This is the main pull request for graphics for 5.6. Usual selection of
      changes all over.
    
      I've got one outstanding vmwgfx pull that touches mm so kept it
      separate until after all of this lands. I'll try and get it to you
      soon after this, but it might be early next week (nothing wrong with
      code, just my schedule is messy)
    
      This also hits a lot of fbdev drivers with some cleanups.
    
      Other notables:
       - vulkan timeline semaphore support added to syncobjs
       - nouveau turing secureboot/graphics support
       - Displayport MST display stream compression support
    
      Detailed summary:
    
      uapi:
       - dma-buf heaps added (and fixed)
       - command line add support for panel oreientation
       - command line allow overriding penguin count
    
      drm:
       - mipi dsi definition updates
       - lockdep annotations for dma_resv
       - remove dma-buf kmap/kunmap support
       - constify fb_ops in all fbdev drivers
       - MST fix for daisy chained hotplug-
       - CTA-861-G modes with VIC >= 193 added
       - fix drm_panel_of_backlight export
       - LVDS decoder support
       - more device based logging support
       - scanline alighment for dumb buffers
       - MST DSC helpers
    
      scheduler:
       - documentation fixes
       - job distribution improvements
    
      panel:
       - Logic PD type 28 panel support
       - Jimax8729d MIPI-DSI
       - igenic JZ4770
       - generic DSI devicetree bindings
       - sony acx424AKP panel
       - Leadtek LTK500HD1829
       - xinpeng XPP055C272
       - AUO B116XAK01
       - GiantPlus GPM940B0
       - BOE NV140FHM-N49
       - Satoz SAT050AT40H12R2
       - Sharp LS020B1DD01D panels.
    
      ttm:
       - use blocking WW lock
    
      i915:
       - hw/uapi state separation
       - Lock annotation improvements
       - selftest improvements
       - ICL/TGL DSI VDSC support
       - VBT parsing improvments
       - Display refactoring
       - DSI updates + fixes
       - HDCP 2.2 for CFL
       - CML PCI ID fixes
       - GLK+ fbc fix
       - PSR fixes
       - GEN/GT refactor improvments
       - DP MST fixes
       - switch context id alloc to xarray
       - workaround updates
       - LMEM debugfs support
       - tiled monitor fixes
       - ICL+ clock gating programming removed
       - DP MST disable sequence fixed
       - LMEM discontiguous object maps
       - prefaulting for discontiguous objects
       - use LMEM for dumb buffers if possible
       - add LMEM mmap support
    
      amdgpu:
       - enable sync object timelines for vulkan
       - MST atomic routines
       - enable MST DSC support
       - add DMCUB display microengine support
       - DC OEM i2c support
       - Renoir DC fixes
       - Initial HDCP 2.x support
       - BACO support for Arcturus
       - Use BACO for runtime PM power save
       - gfxoff on navi10
       - gfx10 golden updates and fixes
       - DCN support on POWER
       - GFXOFF for raven1 refresh
       - MM engine idle handlers cleanup
       - 10bpc EDP panel fixes
       - renoir watermark fixes
       - SR-IOV fixes
       - Arcturus VCN fixes
       - GDDR6 training fixes
       - freesync fixes
       - Pollock support
    
      amdkfd:
       - unify more codepath with amdgpu
       - use KIQ to setup HIQ rather than MMIO
    
      radeon:
       - fix vma fault handler race
       - PPC DMA fix
       - register check fixes for r100/r200
    
      nouveau:
       - mmap_sem vs dma_resv fix
       - rewrite the ACR secure boot code for Turing
       - TU10x graphics engine support (TU11x pending)
       - Page kind mapping for turing
       - 10-bit LUT support
       - GP10B Tegra fixes
       - HD audio regression fix
    
      hisilicon/hibmc:
       - use generic fbdev code and helpers
    
      rockchip:
       - dsi/px30 support
    
      virtio:
       - fb damage support
       - static some functions
    
      vc4:
       - use dma_resv lock wrappers
    
      msm:
       - use dma_resv lock wrappers
       - sc7180 display + DSI support
       - a618 support
       - UBWC support improvements
    
      vmwgfx:
       - updates + new logging uapi
    
      exynos:
       - enable/disable callback cleanups
    
      etnaviv:
       - use dma_resv lock wrappers
    
      atmel-hlcdc:
       - clock fixes
    
      mediatek:
       - cmdq support
       - non-smooth cursor fixes
       - ctm property support
    
      sun4i:
       - suspend support
       - A64 mipi dsi support
    
      rcar-du:
       - Color management module support
       - LVDS encoder dual-link support
       - R8A77980 support
    
      analogic:
       - add support for an6345
    
      ast:
       - atomic modeset support
       - primary plane garbage fix
    
      arcgpu:
       - fixes for fourcc handling
    
      tegra:
       - minor fixes and improvments
    
      mcde:
       - vblank support
    
      meson:
       - OSD1 plane AFBC commit
    
      gma500:
       - add pageflip support
       - reomve global drm_dev
    
      komeda:
       - tweak debugfs output
       - d32 support
       - runtime PM suppotr
    
      udl:
       - use generic shmem helpers
       - cleanup and fixes"
    
    * tag 'drm-next-2020-01-30' of git://anongit.freedesktop.org/drm/drm: (1998 commits)
      drm/nouveau/fb/gp102-: allow module to load even when scrubber binary is missing
      drm/nouveau/acr: return error when registering LSF if ACR not supported
      drm/nouveau/disp/gv100-: not all channel types support reporting error codes
      drm/nouveau/disp/nv50-: prevent oops when no channel method map provided
      drm/nouveau: support synchronous pushbuf submission
      drm/nouveau: signal pending fences when channel has been killed
      drm/nouveau: reject attempts to submit to dead channels
      drm/nouveau: zero vma pointer even if we only unreference it rather than free
      drm/nouveau: Add HD-audio component notifier support
      drm/nouveau: fix build error without CONFIG_IOMMU_API
      drm/nouveau/kms/nv04: remove set but not used variable 'width'
      drm/nouveau/kms/nv50: remove set but not unused variable 'nv_connector'
      drm/nouveau/mmu: fix comptag memory leak
      drm/nouveau/gr/gp10b: Use gp100_grctx and gp100_gr_zbc
      drm/nouveau/pmu/gm20b,gp10b: Fix Falcon bootstrapping
      drm/exynos: Rename Exynos to lowercase
      drm/exynos: change callback names
      drm/mst: Don't do atomic checks over disabled managers
      drm/amdgpu: add the lost mutex_init back
      drm/amd/display: skip opp blank or unblank if test pattern enabled
      ...

commit bdefca2d8dc0f80bbe49e08bf52a717146490706
Author: Boris Brezillon <boris.brezillon@collabora.com>
Date:   Wed Jan 15 20:15:54 2020 -0600

    drm/panfrost: Add the panfrost_gem_mapping concept
    
    With the introduction of per-FD address space, the same BO can be mapped
    in different address space if the BO is globally visible (GEM_FLINK)
    and opened in different context or if the dmabuf is self-imported. The
    current implementation does not take case into account, and attaches the
    mapping directly to the panfrost_gem_object.
    
    Let's create a panfrost_gem_mapping struct and allow multiple mappings
    per BO.
    
    The mappings are refcounted which helps solve another problem where
    mappings were torn down (GEM handle closed by userspace) while GPU
    jobs accessing those BOs were still in-flight. Jobs now keep a
    reference on the mappings they use.
    
    v2 (robh):
    - Minor review comment clean-ups from Steven
    - Use list_is_singular helper
    - Just WARN if we add a mapping when madvise state is not WILLNEED.
      With that, drop the use of object_name_lock.
    
    v3 (robh):
    - Revert returning list iterator in panfrost_gem_mapping_get()
    
    Fixes: a5efb4c9a562 ("drm/panfrost: Restructure the GEM object creation")
    Fixes: 7282f7645d06 ("drm/panfrost: Implement per FD address spaces")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Boris Brezillon <boris.brezillon@collabora.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Acked-by: Boris Brezillon <boris.brezillon@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200116021554.15090-1-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index d411eb6c8eb9..e364ee00f3d0 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -268,9 +268,20 @@ static void panfrost_job_cleanup(struct kref *ref)
 	dma_fence_put(job->done_fence);
 	dma_fence_put(job->render_done_fence);
 
-	if (job->bos) {
+	if (job->mappings) {
 		for (i = 0; i < job->bo_count; i++)
+			panfrost_gem_mapping_put(job->mappings[i]);
+		kvfree(job->mappings);
+	}
+
+	if (job->bos) {
+		struct panfrost_gem_object *bo;
+
+		for (i = 0; i < job->bo_count; i++) {
+			bo = to_panfrost_bo(job->bos[i]);
 			drm_gem_object_put_unlocked(job->bos[i]);
+		}
+
 		kvfree(job->bos);
 	}
 

commit 73896f60d4865657740c64821a7b18825a9bf96c
Author: Ezequiel Garcia <ezequiel@collabora.com>
Date:   Sat Dec 14 01:59:52 2019 -0300

    drm/panfrost: Prefix interrupt handlers' names
    
    Currently, the interrupt lines requested by Panfrost
    use unmeaningful names, which adds some obscurity
    to interrupt introspection (i.e. any tool based
    on procfs' interrupts file).
    
    In order to improve this, prefix each requested
    interrupt with the module name: panfrost-{gpu,job,mmu}.
    
    Signed-off-by: Ezequiel Garcia <ezequiel@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Reviewed-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191214045952.9452-1-ezequiel@collabora.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index d411eb6c8eb9..f68ad9df701a 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -496,7 +496,7 @@ int panfrost_job_init(struct panfrost_device *pfdev)
 		return -ENODEV;
 
 	ret = devm_request_irq(pfdev->dev, irq, panfrost_job_irq_handler,
-			       IRQF_SHARED, "job", pfdev);
+			       IRQF_SHARED, KBUILD_MODNAME "-job", pfdev);
 	if (ret) {
 		dev_err(pfdev->dev, "failed to request job irq");
 		return ret;

commit b3ac17667f115e64c67ea6101fc814f47134b530
Author: Nirmoy Das <nirmoy.das@amd.com>
Date:   Thu Dec 5 11:38:00 2019 +0100

    drm/scheduler: rework entity creation
    
    Entity currently keeps a copy of run_queue list and modify it in
    drm_sched_entity_set_priority(). Entities shouldn't modify run_queue
    list. Use drm_gpu_scheduler list instead of drm_sched_rq list
    in drm_sched_entity struct. In this way we can select a runqueue based
    on entity/ctx's priority for a  drm scheduler.
    
    Signed-off-by: Nirmoy Das <nirmoy.das@amd.com>
    Reviewed-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index d411eb6c8eb9..a9ed088ebf08 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -542,12 +542,14 @@ int panfrost_job_open(struct panfrost_file_priv *panfrost_priv)
 {
 	struct panfrost_device *pfdev = panfrost_priv->pfdev;
 	struct panfrost_job_slot *js = pfdev->js;
-	struct drm_sched_rq *rq;
+	struct drm_gpu_scheduler *sched;
 	int ret, i;
 
 	for (i = 0; i < NUM_JOB_SLOTS; i++) {
-		rq = &js->queue[i].sched.sched_rq[DRM_SCHED_PRIORITY_NORMAL];
-		ret = drm_sched_entity_init(&panfrost_priv->sched_entity[i], &rq, 1, NULL);
+		sched = &js->queue[i].sched;
+		ret = drm_sched_entity_init(&panfrost_priv->sched_entity[i],
+					    DRM_SCHED_PRIORITY_NORMAL, &sched,
+					    1, NULL);
 		if (WARN_ON(ret))
 			return ret;
 	}

commit 9e62b885f71568d166c97b040b4b2e218c2a5584
Author: Steven Price <steven.price@arm.com>
Date:   Fri Oct 25 14:41:43 2019 +0100

    drm/panfrost: Simplify devfreq utilisation tracking
    
    Instead of tracking per-slot utilisation track a single value for the
    entire GPU. Ultimately it doesn't matter if the GPU is busy with only
    vertex or a combination of vertex and fragment processing - if it's busy
    then it's busy and devfreq should be scaling appropriately.
    
    This also makes way for being able to submit multiple jobs per slot
    which requires more values than the original boolean per slot.
    
    Reviewed-by: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Reviewed-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191025134143.14324-3-steven.price@arm.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 33bf25ba506e..d411eb6c8eb9 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -155,8 +155,7 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 	}
 
 	cfg = panfrost_mmu_as_get(pfdev, &job->file_priv->mmu);
-
-	panfrost_devfreq_record_transition(pfdev, js);
+	panfrost_devfreq_record_busy(pfdev);
 
 	job_write(pfdev, JS_HEAD_NEXT_LO(js), jc_head & 0xFFFFFFFF);
 	job_write(pfdev, JS_HEAD_NEXT_HI(js), jc_head >> 32);
@@ -404,7 +403,7 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	}
 	spin_unlock_irqrestore(&pfdev->js->job_lock, flags);
 
-	panfrost_devfreq_record_transition(pfdev, js);
+	panfrost_devfreq_record_idle(pfdev);
 	panfrost_device_reset(pfdev);
 
 	for (i = 0; i < NUM_JOB_SLOTS; i++)
@@ -467,7 +466,7 @@ static irqreturn_t panfrost_job_irq_handler(int irq, void *data)
 				pfdev->jobs[j] = NULL;
 
 				panfrost_mmu_as_put(pfdev, &job->file_priv->mmu);
-				panfrost_devfreq_record_transition(pfdev, j);
+				panfrost_devfreq_record_idle(pfdev);
 
 				dma_fence_signal_locked(job->done_fence);
 				pm_runtime_put_autosuspend(pfdev->dev);
@@ -568,14 +567,14 @@ int panfrost_job_is_idle(struct panfrost_device *pfdev)
 	struct panfrost_job_slot *js = pfdev->js;
 	int i;
 
+	/* Check whether the hardware is idle */
+	if (atomic_read(&pfdev->devfreq.busy_count))
+		return false;
+
 	for (i = 0; i < NUM_JOB_SLOTS; i++) {
 		/* If there are any jobs in the HW queue, we're not idle */
 		if (atomic_read(&js->queue[i].sched.hw_rq_count))
 			return false;
-
-		/* Check whether the hardware is idle */
-		if (pfdev->devfreq.slot[i].busy)
-			return false;
 	}
 
 	return true;

commit cfbb2e38dfc60b04f0b226f5f926ce31f353fc26
Author: Steven Price <steven.price@arm.com>
Date:   Wed Oct 9 10:44:56 2019 +0100

    drm/panfrost: Remove commented out call to panfrost_core_dump
    
    panfrost_core_dump() has never existed in mainline, so remove it and add
    a TODO entry that core dump support is currently lacking.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191009094456.9704-2-steven.price@arm.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 21f34d44aac2..33bf25ba506e 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -404,8 +404,6 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	}
 	spin_unlock_irqrestore(&pfdev->js->job_lock, flags);
 
-	/* panfrost_core_dump(pfdev); */
-
 	panfrost_devfreq_record_transition(pfdev, js);
 	panfrost_device_reset(pfdev);
 

commit de89212ddb3dc87152dd6f2b88a20d5176156777
Author: Steven Price <steven.price@arm.com>
Date:   Wed Oct 9 10:44:55 2019 +0100

    drm/panfrost: Handle resetting on timeout better
    
    Panfrost uses multiple schedulers (one for each slot, so 2 in reality),
    and on a timeout has to stop all the schedulers to safely perform a
    reset. However more than one scheduler can trigger a timeout at the same
    time. This race condition results in jobs being freed while they are
    still in use.
    
    When stopping other slots use cancel_delayed_work_sync() to ensure that
    any timeout started for that slot has completed. Also use
    mutex_trylock() to obtain reset_lock. This means that only one thread
    attempts the reset, the other threads will simply complete without doing
    anything (the first thread will wait for this in the call to
    cancel_delayed_work_sync()).
    
    While we're here and since the function is already dependent on
    sched_job not being NULL, let's remove the unnecessary checks.
    
    Fixes: aa20236784ab ("drm/panfrost: Prevent concurrent resets")
    Tested-by: Neil Armstrong <narmstrong@baylibre.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191009094456.9704-1-steven.price@arm.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index a58551668d9a..21f34d44aac2 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -381,13 +381,19 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 		job_read(pfdev, JS_TAIL_LO(js)),
 		sched_job);
 
-	mutex_lock(&pfdev->reset_lock);
+	if (!mutex_trylock(&pfdev->reset_lock))
+		return;
 
-	for (i = 0; i < NUM_JOB_SLOTS; i++)
-		drm_sched_stop(&pfdev->js->queue[i].sched, sched_job);
+	for (i = 0; i < NUM_JOB_SLOTS; i++) {
+		struct drm_gpu_scheduler *sched = &pfdev->js->queue[i].sched;
+
+		drm_sched_stop(sched, sched_job);
+		if (js != i)
+			/* Ensure any timeouts on other slots have finished */
+			cancel_delayed_work_sync(&sched->work_tdr);
+	}
 
-	if (sched_job)
-		drm_sched_increase_karma(sched_job);
+	drm_sched_increase_karma(sched_job);
 
 	spin_lock_irqsave(&pfdev->js->job_lock, flags);
 	for (i = 0; i < NUM_JOB_SLOTS; i++) {

commit 45d0dbd15a3b89ced53e058bc71b751e6473b664
Author: Rob Herring <robh@kernel.org>
Date:   Mon Aug 26 17:33:17 2019 -0500

    drm/panfrost: Remove unnecessary hwaccess_lock spin_lock
    
    With the introduction of the as_lock to serialize address space registers,
    the hwaccess_lock is only used within the job code and is not protecting
    anything. panfrost_job_hw_submit() only accesses registers for 1 job slot
    and it's already serialized by drm_sched.
    
    Fixes: 7282f7645d06 ("drm/panfrost: Implement per FD address spaces")
    Cc: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Acked-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190826223317.28509-9-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 18bcc9bac6d2..a58551668d9a 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -141,7 +141,6 @@ static void panfrost_job_write_affinity(struct panfrost_device *pfdev,
 static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 {
 	struct panfrost_device *pfdev = job->pfdev;
-	unsigned long flags;
 	u32 cfg;
 	u64 jc_head = job->jc;
 	int ret;
@@ -158,7 +157,6 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 	cfg = panfrost_mmu_as_get(pfdev, &job->file_priv->mmu);
 
 	panfrost_devfreq_record_transition(pfdev, js);
-	spin_lock_irqsave(&pfdev->hwaccess_lock, flags);
 
 	job_write(pfdev, JS_HEAD_NEXT_LO(js), jc_head & 0xFFFFFFFF);
 	job_write(pfdev, JS_HEAD_NEXT_HI(js), jc_head >> 32);
@@ -187,8 +185,6 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 				job, js, jc_head);
 
 	job_write(pfdev, JS_COMMAND_NEXT(js), JS_COMMAND_START);
-
-	spin_unlock_irqrestore(&pfdev->hwaccess_lock, flags);
 }
 
 static void panfrost_acquire_object_fences(struct drm_gem_object **bos,

commit 330bec4b7ccf4a556469cfec2ad695a180e35954
Author: Rob Herring <robh@kernel.org>
Date:   Mon Aug 26 17:33:11 2019 -0500

    drm/panfrost: Hold runtime PM reference until jobs complete
    
    Doing a pm_runtime_put as soon as a job is submitted is wrong as it should
    not happen until the job completes. It works currently because we are
    relying on the autosuspend timeout to keep the h/w enabled.
    
    Fixes: f3ba91228e8e ("drm/panfrost: Add initial panfrost driver")
    Cc: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Cc: Steven Price <steven.price@arm.com>
    Cc: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Acked-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190826223317.28509-3-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 05c85f45a0de..18bcc9bac6d2 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -150,8 +150,10 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 	if (ret < 0)
 		return;
 
-	if (WARN_ON(job_read(pfdev, JS_COMMAND_NEXT(js))))
-		goto end;
+	if (WARN_ON(job_read(pfdev, JS_COMMAND_NEXT(js)))) {
+		pm_runtime_put_sync_autosuspend(pfdev->dev);
+		return;
+	}
 
 	cfg = panfrost_mmu_as_get(pfdev, &job->file_priv->mmu);
 
@@ -187,10 +189,6 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 	job_write(pfdev, JS_COMMAND_NEXT(js), JS_COMMAND_START);
 
 	spin_unlock_irqrestore(&pfdev->hwaccess_lock, flags);
-
-end:
-	pm_runtime_mark_last_busy(pfdev->dev);
-	pm_runtime_put_autosuspend(pfdev->dev);
 }
 
 static void panfrost_acquire_object_fences(struct drm_gem_object **bos,
@@ -369,6 +367,7 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	struct panfrost_job *job = to_panfrost_job(sched_job);
 	struct panfrost_device *pfdev = job->pfdev;
 	int js = panfrost_job_get_slot(job);
+	unsigned long flags;
 	int i;
 
 	/*
@@ -394,6 +393,15 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	if (sched_job)
 		drm_sched_increase_karma(sched_job);
 
+	spin_lock_irqsave(&pfdev->js->job_lock, flags);
+	for (i = 0; i < NUM_JOB_SLOTS; i++) {
+		if (pfdev->jobs[i]) {
+			pm_runtime_put_noidle(pfdev->dev);
+			pfdev->jobs[i] = NULL;
+		}
+	}
+	spin_unlock_irqrestore(&pfdev->js->job_lock, flags);
+
 	/* panfrost_core_dump(pfdev); */
 
 	panfrost_devfreq_record_transition(pfdev, js);
@@ -450,12 +458,21 @@ static irqreturn_t panfrost_job_irq_handler(int irq, void *data)
 		}
 
 		if (status & JOB_INT_MASK_DONE(j)) {
-			struct panfrost_job *job = pfdev->jobs[j];
+			struct panfrost_job *job;
+
+			spin_lock(&pfdev->js->job_lock);
+			job = pfdev->jobs[j];
+			/* Only NULL if job timeout occurred */
+			if (job) {
+				pfdev->jobs[j] = NULL;
+
+				panfrost_mmu_as_put(pfdev, &job->file_priv->mmu);
+				panfrost_devfreq_record_transition(pfdev, j);
 
-			pfdev->jobs[j] = NULL;
-			panfrost_mmu_as_put(pfdev, &job->file_priv->mmu);
-			panfrost_devfreq_record_transition(pfdev, j);
-			dma_fence_signal(job->done_fence);
+				dma_fence_signal_locked(job->done_fence);
+				pm_runtime_put_autosuspend(pfdev->dev);
+			}
+			spin_unlock(&pfdev->js->job_lock);
 		}
 
 		status &= ~mask;

commit 7282f7645d06bf0afe0a3c11ab92d9392528b819
Author: Rob Herring <robh@kernel.org>
Date:   Tue Aug 13 09:01:15 2019 -0600

    drm/panfrost: Implement per FD address spaces
    
    Up until now, a single shared GPU address space was used. This is not
    ideal as there's no protection between processes and doesn't work for
    supporting the same GPU/CPU VA feature. Most importantly, this will
    hopefully mitigate Alyssa's fear of WebGL, whatever that is.
    
    Most of the changes here are moving struct drm_mm and struct
    panfrost_mmu objects from the per device struct to the per FD struct.
    The critical function is panfrost_mmu_as_get() which handles allocating
    and switching the h/w address spaces.
    
    There's 3 states an AS can be in: free, allocated, and in use. When a
    job runs, it requests an address space and then marks it not in use when
    job is complete(but stays assigned). The first time thru, we find a free
    AS in the alloc_mask and assign the AS to the FD. Then the next time
    thru, we most likely already have our AS and we just mark it in use with
    a ref count. We need a ref count because we have multiple job slots. If
    the job/FD doesn't have an AS assigned and there are no free ones, then
    we pick an allocated one not in use from our LRU list and switch the AS
    from the old FD to the new one.
    
    Cc: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Acked-by: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190813150115.30338-1-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 0fc4539fd08d..05c85f45a0de 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -153,6 +153,8 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 	if (WARN_ON(job_read(pfdev, JS_COMMAND_NEXT(js))))
 		goto end;
 
+	cfg = panfrost_mmu_as_get(pfdev, &job->file_priv->mmu);
+
 	panfrost_devfreq_record_transition(pfdev, js);
 	spin_lock_irqsave(&pfdev->hwaccess_lock, flags);
 
@@ -163,8 +165,7 @@ static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
 
 	/* start MMU, medium priority, cache clean/flush on end, clean/flush on
 	 * start */
-	/* TODO: different address spaces */
-	cfg = JS_CONFIG_THREAD_PRI(8) |
+	cfg |= JS_CONFIG_THREAD_PRI(8) |
 		JS_CONFIG_START_FLUSH_CLEAN_INVALIDATE |
 		JS_CONFIG_END_FLUSH_CLEAN_INVALIDATE;
 
@@ -377,8 +378,9 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	if (dma_fence_is_signaled(job->done_fence))
 		return;
 
-	dev_err(pfdev->dev, "gpu sched timeout, js=%d, status=0x%x, head=0x%x, tail=0x%x, sched_job=%p",
+	dev_err(pfdev->dev, "gpu sched timeout, js=%d, config=0x%x, status=0x%x, head=0x%x, tail=0x%x, sched_job=%p",
 		js,
+		job_read(pfdev, JS_CONFIG(js)),
 		job_read(pfdev, JS_STATUS(js)),
 		job_read(pfdev, JS_HEAD_LO(js)),
 		job_read(pfdev, JS_TAIL_LO(js)),
@@ -448,8 +450,12 @@ static irqreturn_t panfrost_job_irq_handler(int irq, void *data)
 		}
 
 		if (status & JOB_INT_MASK_DONE(j)) {
+			struct panfrost_job *job = pfdev->jobs[j];
+
+			pfdev->jobs[j] = NULL;
+			panfrost_mmu_as_put(pfdev, &job->file_priv->mmu);
 			panfrost_devfreq_record_transition(pfdev, j);
-			dma_fence_signal(pfdev->jobs[j]->done_fence);
+			dma_fence_signal(job->done_fence);
 		}
 
 		status &= ~mask;

commit 52791eeec1d9f4a7e7fe08aaba0b1553149d93bc
Author: Christian König <christian.koenig@amd.com>
Date:   Sun Aug 11 10:06:32 2019 +0200

    dma-buf: rename reservation_object to dma_resv
    
    Be more consistent with the naming of the other DMA-buf objects.
    
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/323401/

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index d567ce98494c..0fc4539fd08d 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -6,7 +6,7 @@
 #include <linux/io.h>
 #include <linux/platform_device.h>
 #include <linux/pm_runtime.h>
-#include <linux/reservation.h>
+#include <linux/dma-resv.h>
 #include <drm/gpu_scheduler.h>
 #include <drm/panfrost_drm.h>
 
@@ -199,7 +199,7 @@ static void panfrost_acquire_object_fences(struct drm_gem_object **bos,
 	int i;
 
 	for (i = 0; i < bo_count; i++)
-		implicit_fences[i] = reservation_object_get_excl_rcu(bos[i]->resv);
+		implicit_fences[i] = dma_resv_get_excl_rcu(bos[i]->resv);
 }
 
 static void panfrost_attach_object_fences(struct drm_gem_object **bos,
@@ -209,7 +209,7 @@ static void panfrost_attach_object_fences(struct drm_gem_object **bos,
 	int i;
 
 	for (i = 0; i < bo_count; i++)
-		reservation_object_add_excl_fence(bos[i]->resv, fence);
+		dma_resv_add_excl_fence(bos[i]->resv, fence);
 }
 
 int panfrost_job_push(struct panfrost_job *job)

commit 73e467f60acdabd480d1b377a623ba13db0e5dd2
Author: Rob Herring <robh@kernel.org>
Date:   Thu Aug 8 14:30:39 2019 -0600

    drm/panfrost: Consolidate reset handling
    
    Runtime PM resume and job timeouts both call the same sequence of
    functions, so consolidate them to a common function. This will make
    changing the reset related code easier. The MMU also needs some
    re-initialization on reset, so rework its call. In the process, we
    hide the address space details within the MMU code in preparation to
    support multiple address spaces.
    
    Cc: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Alyssa Rosenzweig <alyssa.rosenzweig@collabora.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190808222200.13176-7-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 9bb9260d9181..d567ce98494c 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -395,12 +395,7 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	/* panfrost_core_dump(pfdev); */
 
 	panfrost_devfreq_record_transition(pfdev, js);
-	panfrost_gpu_soft_reset(pfdev);
-
-	/* TODO: Re-enable all other address spaces */
-	panfrost_mmu_enable(pfdev, 0);
-	panfrost_gpu_power_on(pfdev);
-	panfrost_job_enable_interrupts(pfdev);
+	panfrost_device_reset(pfdev);
 
 	for (i = 0; i < NUM_JOB_SLOTS; i++)
 		drm_sched_resubmit_jobs(&pfdev->js->queue[i].sched);

commit 5918045c4ed492fb5813f980dcf89a90fefd0a4e
Author: Christian König <christian.koenig@amd.com>
Date:   Thu Apr 18 11:00:21 2019 -0400

    drm/scheduler: rework job destruction
    
    We now destroy finished jobs from the worker thread to make sure that
    we never destroy a job currently in timeout processing.
    By this we avoid holding lock around ring mirror list in drm_sched_stop
    which should solve a deadlock reported by a user.
    
    v2: Remove unused variable.
    v4: Move guilty job free into sched code.
    v5:
    Move sched->hw_rq_count to drm_sched_start to account for counter
    decrement in drm_sched_stop even when we don't call resubmit jobs
    if guily job did signal.
    v6: remove unused variable
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=109692
    
    Acked-by: Chunming Zhou <david1.zhou@amd.com>
    Signed-off-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/1555599624-12285-3-git-send-email-andrey.grodzovsky@amd.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index a5716c8fe8b3..9bb9260d9181 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -387,7 +387,7 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	mutex_lock(&pfdev->reset_lock);
 
 	for (i = 0; i < NUM_JOB_SLOTS; i++)
-		drm_sched_stop(&pfdev->js->queue[i].sched);
+		drm_sched_stop(&pfdev->js->queue[i].sched, sched_job);
 
 	if (sched_job)
 		drm_sched_increase_karma(sched_job);

commit aa20236784ab3d0372591d6eca692956bca4ebfb
Author: Tomeu Vizoso <tomeu.vizoso@collabora.com>
Date:   Thu Apr 18 10:41:48 2019 +0200

    drm/panfrost: Prevent concurrent resets
    
    If a job times out in slot 0 while a reset is performed because a job
    timed out in slot 1, the drm-sched core can get into a deadlock.
    
    Signed-off-by: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190418084305.45021-1-tomeu.vizoso@collabora.com

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
index 0a7ed04f7d52..a5716c8fe8b3 100644
--- a/drivers/gpu/drm/panfrost/panfrost_job.c
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -384,6 +384,8 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 		job_read(pfdev, JS_TAIL_LO(js)),
 		sched_job);
 
+	mutex_lock(&pfdev->reset_lock);
+
 	for (i = 0; i < NUM_JOB_SLOTS; i++)
 		drm_sched_stop(&pfdev->js->queue[i].sched);
 
@@ -406,6 +408,8 @@ static void panfrost_job_timedout(struct drm_sched_job *sched_job)
 	/* restart scheduler after GPU is usable again */
 	for (i = 0; i < NUM_JOB_SLOTS; i++)
 		drm_sched_start(&pfdev->js->queue[i].sched, true);
+
+	mutex_unlock(&pfdev->reset_lock);
 }
 
 static const struct drm_sched_backend_ops panfrost_sched_ops = {

commit f3ba91228e8e917e5bd6c4b72bfe846933d17370
Author: Rob Herring <robh@kernel.org>
Date:   Mon Sep 10 14:27:58 2018 -0500

    drm/panfrost: Add initial panfrost driver
    
    This adds the initial driver for panfrost which supports Arm Mali
    Midgard and Bifrost family of GPUs. Currently, only the T860 and
    T760 Midgard GPUs have been tested.
    
    v2:
    - Add GPU reset on job hangs (Tomeu)
    - Add RuntimePM and devfreq support (Tomeu)
    - Fix T760 support (Tomeu)
    - Add a TODO file (Rob, Tomeu)
    - Support multiple in fences (Tomeu)
    - Drop support for shared fences (Tomeu)
    - Fill in MMU de-init (Rob)
    - Move register definitions back to single header (Rob)
    - Clean-up hardcoded job submit todos (Rob)
    - Implement feature setup based on features/issues (Rob)
    - Add remaining Midgard DT compatible strings (Rob)
    
    v3:
    - Add support for reset lines (Neil)
    - Add a MAINTAINERS entry (Rob)
    - Call dma_set_mask_and_coherent (Rob)
    - Do MMU invalidate on map and unmap. Restructure to do a single
      operation per map/unmap call. (Rob)
    - Add a missing explicit padding to struct drm_panfrost_create_bo (Rob)
    - Fix 0-day error: "panfrost_devfreq.c:151:9-16: ERROR: PTR_ERR applied after initialization to constant on line 150"
    - Drop HW_FEATURE_AARCH64_MMU conditional (Rob)
    - s/DRM_PANFROST_PARAM_GPU_ID/DRM_PANFROST_PARAM_GPU_PROD_ID/ (Rob)
    - Check drm_gem_shmem_prime_import_sg_table() error code (Rob)
    - Re-order power on sequence (Rob)
    - Move panfrost_acquire_object_fences() before scheduling job (Rob)
    - Add NULL checks on array pointers in job clean-up (Rob)
    - Rework devfreq (Tomeu)
    - Fix devfreq init with no regulator (Rob)
    - Various WS and comments clean-up (Rob)
    
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: Maxime Ripard <maxime.ripard@bootlin.com>
    Cc: Sean Paul <sean@poorly.run>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: Lyude Paul <lyude@redhat.com>
    Reviewed-by: Alyssa Rosenzweig <alyssa@rosenzweig.io>
    Reviewed-by: Eric Anholt <eric@anholt.net>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marty E. Plummer <hanetzer@startmail.com>
    Signed-off-by: Tomeu Vizoso <tomeu.vizoso@collabora.com>
    Signed-off-by: Neil Armstrong <narmstrong@baylibre.com>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190409205427.6943-4-robh@kernel.org

diff --git a/drivers/gpu/drm/panfrost/panfrost_job.c b/drivers/gpu/drm/panfrost/panfrost_job.c
new file mode 100644
index 000000000000..0a7ed04f7d52
--- /dev/null
+++ b/drivers/gpu/drm/panfrost/panfrost_job.c
@@ -0,0 +1,560 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright 2019 Linaro, Ltd, Rob Herring <robh@kernel.org> */
+/* Copyright 2019 Collabora ltd. */
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/reservation.h>
+#include <drm/gpu_scheduler.h>
+#include <drm/panfrost_drm.h>
+
+#include "panfrost_device.h"
+#include "panfrost_devfreq.h"
+#include "panfrost_job.h"
+#include "panfrost_features.h"
+#include "panfrost_issues.h"
+#include "panfrost_gem.h"
+#include "panfrost_regs.h"
+#include "panfrost_gpu.h"
+#include "panfrost_mmu.h"
+
+#define job_write(dev, reg, data) writel(data, dev->iomem + (reg))
+#define job_read(dev, reg) readl(dev->iomem + (reg))
+
+struct panfrost_queue_state {
+	struct drm_gpu_scheduler sched;
+
+	u64 fence_context;
+	u64 emit_seqno;
+};
+
+struct panfrost_job_slot {
+	struct panfrost_queue_state queue[NUM_JOB_SLOTS];
+	spinlock_t job_lock;
+};
+
+static struct panfrost_job *
+to_panfrost_job(struct drm_sched_job *sched_job)
+{
+	return container_of(sched_job, struct panfrost_job, base);
+}
+
+struct panfrost_fence {
+	struct dma_fence base;
+	struct drm_device *dev;
+	/* panfrost seqno for signaled() test */
+	u64 seqno;
+	int queue;
+};
+
+static inline struct panfrost_fence *
+to_panfrost_fence(struct dma_fence *fence)
+{
+	return (struct panfrost_fence *)fence;
+}
+
+static const char *panfrost_fence_get_driver_name(struct dma_fence *fence)
+{
+	return "panfrost";
+}
+
+static const char *panfrost_fence_get_timeline_name(struct dma_fence *fence)
+{
+	struct panfrost_fence *f = to_panfrost_fence(fence);
+
+	switch (f->queue) {
+	case 0:
+		return "panfrost-js-0";
+	case 1:
+		return "panfrost-js-1";
+	case 2:
+		return "panfrost-js-2";
+	default:
+		return NULL;
+	}
+}
+
+static const struct dma_fence_ops panfrost_fence_ops = {
+	.get_driver_name = panfrost_fence_get_driver_name,
+	.get_timeline_name = panfrost_fence_get_timeline_name,
+};
+
+static struct dma_fence *panfrost_fence_create(struct panfrost_device *pfdev, int js_num)
+{
+	struct panfrost_fence *fence;
+	struct panfrost_job_slot *js = pfdev->js;
+
+	fence = kzalloc(sizeof(*fence), GFP_KERNEL);
+	if (!fence)
+		return ERR_PTR(-ENOMEM);
+
+	fence->dev = pfdev->ddev;
+	fence->queue = js_num;
+	fence->seqno = ++js->queue[js_num].emit_seqno;
+	dma_fence_init(&fence->base, &panfrost_fence_ops, &js->job_lock,
+		       js->queue[js_num].fence_context, fence->seqno);
+
+	return &fence->base;
+}
+
+static int panfrost_job_get_slot(struct panfrost_job *job)
+{
+	/* JS0: fragment jobs.
+	 * JS1: vertex/tiler jobs
+	 * JS2: compute jobs
+	 */
+	if (job->requirements & PANFROST_JD_REQ_FS)
+		return 0;
+
+/* Not exposed to userspace yet */
+#if 0
+	if (job->requirements & PANFROST_JD_REQ_ONLY_COMPUTE) {
+		if ((job->requirements & PANFROST_JD_REQ_CORE_GRP_MASK) &&
+		    (job->pfdev->features.nr_core_groups == 2))
+			return 2;
+		if (panfrost_has_hw_issue(job->pfdev, HW_ISSUE_8987))
+			return 2;
+	}
+#endif
+	return 1;
+}
+
+static void panfrost_job_write_affinity(struct panfrost_device *pfdev,
+					u32 requirements,
+					int js)
+{
+	u64 affinity;
+
+	/*
+	 * Use all cores for now.
+	 * Eventually we may need to support tiler only jobs and h/w with
+	 * multiple (2) coherent core groups
+	 */
+	affinity = pfdev->features.shader_present;
+
+	job_write(pfdev, JS_AFFINITY_NEXT_LO(js), affinity & 0xFFFFFFFF);
+	job_write(pfdev, JS_AFFINITY_NEXT_HI(js), affinity >> 32);
+}
+
+static void panfrost_job_hw_submit(struct panfrost_job *job, int js)
+{
+	struct panfrost_device *pfdev = job->pfdev;
+	unsigned long flags;
+	u32 cfg;
+	u64 jc_head = job->jc;
+	int ret;
+
+	ret = pm_runtime_get_sync(pfdev->dev);
+	if (ret < 0)
+		return;
+
+	if (WARN_ON(job_read(pfdev, JS_COMMAND_NEXT(js))))
+		goto end;
+
+	panfrost_devfreq_record_transition(pfdev, js);
+	spin_lock_irqsave(&pfdev->hwaccess_lock, flags);
+
+	job_write(pfdev, JS_HEAD_NEXT_LO(js), jc_head & 0xFFFFFFFF);
+	job_write(pfdev, JS_HEAD_NEXT_HI(js), jc_head >> 32);
+
+	panfrost_job_write_affinity(pfdev, job->requirements, js);
+
+	/* start MMU, medium priority, cache clean/flush on end, clean/flush on
+	 * start */
+	/* TODO: different address spaces */
+	cfg = JS_CONFIG_THREAD_PRI(8) |
+		JS_CONFIG_START_FLUSH_CLEAN_INVALIDATE |
+		JS_CONFIG_END_FLUSH_CLEAN_INVALIDATE;
+
+	if (panfrost_has_hw_feature(pfdev, HW_FEATURE_FLUSH_REDUCTION))
+		cfg |= JS_CONFIG_ENABLE_FLUSH_REDUCTION;
+
+	if (panfrost_has_hw_issue(pfdev, HW_ISSUE_10649))
+		cfg |= JS_CONFIG_START_MMU;
+
+	job_write(pfdev, JS_CONFIG_NEXT(js), cfg);
+
+	if (panfrost_has_hw_feature(pfdev, HW_FEATURE_FLUSH_REDUCTION))
+		job_write(pfdev, JS_FLUSH_ID_NEXT(js), job->flush_id);
+
+	/* GO ! */
+	dev_dbg(pfdev->dev, "JS: Submitting atom %p to js[%d] with head=0x%llx",
+				job, js, jc_head);
+
+	job_write(pfdev, JS_COMMAND_NEXT(js), JS_COMMAND_START);
+
+	spin_unlock_irqrestore(&pfdev->hwaccess_lock, flags);
+
+end:
+	pm_runtime_mark_last_busy(pfdev->dev);
+	pm_runtime_put_autosuspend(pfdev->dev);
+}
+
+static void panfrost_acquire_object_fences(struct drm_gem_object **bos,
+					   int bo_count,
+					   struct dma_fence **implicit_fences)
+{
+	int i;
+
+	for (i = 0; i < bo_count; i++)
+		implicit_fences[i] = reservation_object_get_excl_rcu(bos[i]->resv);
+}
+
+static void panfrost_attach_object_fences(struct drm_gem_object **bos,
+					  int bo_count,
+					  struct dma_fence *fence)
+{
+	int i;
+
+	for (i = 0; i < bo_count; i++)
+		reservation_object_add_excl_fence(bos[i]->resv, fence);
+}
+
+int panfrost_job_push(struct panfrost_job *job)
+{
+	struct panfrost_device *pfdev = job->pfdev;
+	int slot = panfrost_job_get_slot(job);
+	struct drm_sched_entity *entity = &job->file_priv->sched_entity[slot];
+	struct ww_acquire_ctx acquire_ctx;
+	int ret = 0;
+
+	mutex_lock(&pfdev->sched_lock);
+
+	ret = drm_gem_lock_reservations(job->bos, job->bo_count,
+					    &acquire_ctx);
+	if (ret) {
+		mutex_unlock(&pfdev->sched_lock);
+		return ret;
+	}
+
+	ret = drm_sched_job_init(&job->base, entity, NULL);
+	if (ret) {
+		mutex_unlock(&pfdev->sched_lock);
+		goto unlock;
+	}
+
+	job->render_done_fence = dma_fence_get(&job->base.s_fence->finished);
+
+	kref_get(&job->refcount); /* put by scheduler job completion */
+
+	panfrost_acquire_object_fences(job->bos, job->bo_count,
+				       job->implicit_fences);
+
+	drm_sched_entity_push_job(&job->base, entity);
+
+	mutex_unlock(&pfdev->sched_lock);
+
+	panfrost_attach_object_fences(job->bos, job->bo_count,
+				      job->render_done_fence);
+
+unlock:
+	drm_gem_unlock_reservations(job->bos, job->bo_count, &acquire_ctx);
+
+	return ret;
+}
+
+static void panfrost_job_cleanup(struct kref *ref)
+{
+	struct panfrost_job *job = container_of(ref, struct panfrost_job,
+						refcount);
+	unsigned int i;
+
+	if (job->in_fences) {
+		for (i = 0; i < job->in_fence_count; i++)
+			dma_fence_put(job->in_fences[i]);
+		kvfree(job->in_fences);
+	}
+	if (job->implicit_fences) {
+		for (i = 0; i < job->bo_count; i++)
+			dma_fence_put(job->implicit_fences[i]);
+		kvfree(job->implicit_fences);
+	}
+	dma_fence_put(job->done_fence);
+	dma_fence_put(job->render_done_fence);
+
+	if (job->bos) {
+		for (i = 0; i < job->bo_count; i++)
+			drm_gem_object_put_unlocked(job->bos[i]);
+		kvfree(job->bos);
+	}
+
+	kfree(job);
+}
+
+void panfrost_job_put(struct panfrost_job *job)
+{
+	kref_put(&job->refcount, panfrost_job_cleanup);
+}
+
+static void panfrost_job_free(struct drm_sched_job *sched_job)
+{
+	struct panfrost_job *job = to_panfrost_job(sched_job);
+
+	drm_sched_job_cleanup(sched_job);
+
+	panfrost_job_put(job);
+}
+
+static struct dma_fence *panfrost_job_dependency(struct drm_sched_job *sched_job,
+						 struct drm_sched_entity *s_entity)
+{
+	struct panfrost_job *job = to_panfrost_job(sched_job);
+	struct dma_fence *fence;
+	unsigned int i;
+
+	/* Explicit fences */
+	for (i = 0; i < job->in_fence_count; i++) {
+		if (job->in_fences[i]) {
+			fence = job->in_fences[i];
+			job->in_fences[i] = NULL;
+			return fence;
+		}
+	}
+
+	/* Implicit fences, max. one per BO */
+	for (i = 0; i < job->bo_count; i++) {
+		if (job->implicit_fences[i]) {
+			fence = job->implicit_fences[i];
+			job->implicit_fences[i] = NULL;
+			return fence;
+		}
+	}
+
+	return NULL;
+}
+
+static struct dma_fence *panfrost_job_run(struct drm_sched_job *sched_job)
+{
+	struct panfrost_job *job = to_panfrost_job(sched_job);
+	struct panfrost_device *pfdev = job->pfdev;
+	int slot = panfrost_job_get_slot(job);
+	struct dma_fence *fence = NULL;
+
+	if (unlikely(job->base.s_fence->finished.error))
+		return NULL;
+
+	pfdev->jobs[slot] = job;
+
+	fence = panfrost_fence_create(pfdev, slot);
+	if (IS_ERR(fence))
+		return NULL;
+
+	if (job->done_fence)
+		dma_fence_put(job->done_fence);
+	job->done_fence = dma_fence_get(fence);
+
+	panfrost_job_hw_submit(job, slot);
+
+	return fence;
+}
+
+void panfrost_job_enable_interrupts(struct panfrost_device *pfdev)
+{
+	int j;
+	u32 irq_mask = 0;
+
+	for (j = 0; j < NUM_JOB_SLOTS; j++) {
+		irq_mask |= MK_JS_MASK(j);
+	}
+
+	job_write(pfdev, JOB_INT_CLEAR, irq_mask);
+	job_write(pfdev, JOB_INT_MASK, irq_mask);
+}
+
+static void panfrost_job_timedout(struct drm_sched_job *sched_job)
+{
+	struct panfrost_job *job = to_panfrost_job(sched_job);
+	struct panfrost_device *pfdev = job->pfdev;
+	int js = panfrost_job_get_slot(job);
+	int i;
+
+	/*
+	 * If the GPU managed to complete this jobs fence, the timeout is
+	 * spurious. Bail out.
+	 */
+	if (dma_fence_is_signaled(job->done_fence))
+		return;
+
+	dev_err(pfdev->dev, "gpu sched timeout, js=%d, status=0x%x, head=0x%x, tail=0x%x, sched_job=%p",
+		js,
+		job_read(pfdev, JS_STATUS(js)),
+		job_read(pfdev, JS_HEAD_LO(js)),
+		job_read(pfdev, JS_TAIL_LO(js)),
+		sched_job);
+
+	for (i = 0; i < NUM_JOB_SLOTS; i++)
+		drm_sched_stop(&pfdev->js->queue[i].sched);
+
+	if (sched_job)
+		drm_sched_increase_karma(sched_job);
+
+	/* panfrost_core_dump(pfdev); */
+
+	panfrost_devfreq_record_transition(pfdev, js);
+	panfrost_gpu_soft_reset(pfdev);
+
+	/* TODO: Re-enable all other address spaces */
+	panfrost_mmu_enable(pfdev, 0);
+	panfrost_gpu_power_on(pfdev);
+	panfrost_job_enable_interrupts(pfdev);
+
+	for (i = 0; i < NUM_JOB_SLOTS; i++)
+		drm_sched_resubmit_jobs(&pfdev->js->queue[i].sched);
+
+	/* restart scheduler after GPU is usable again */
+	for (i = 0; i < NUM_JOB_SLOTS; i++)
+		drm_sched_start(&pfdev->js->queue[i].sched, true);
+}
+
+static const struct drm_sched_backend_ops panfrost_sched_ops = {
+	.dependency = panfrost_job_dependency,
+	.run_job = panfrost_job_run,
+	.timedout_job = panfrost_job_timedout,
+	.free_job = panfrost_job_free
+};
+
+static irqreturn_t panfrost_job_irq_handler(int irq, void *data)
+{
+	struct panfrost_device *pfdev = data;
+	u32 status = job_read(pfdev, JOB_INT_STAT);
+	int j;
+
+	dev_dbg(pfdev->dev, "jobslot irq status=%x\n", status);
+
+	if (!status)
+		return IRQ_NONE;
+
+	pm_runtime_mark_last_busy(pfdev->dev);
+
+	for (j = 0; status; j++) {
+		u32 mask = MK_JS_MASK(j);
+
+		if (!(status & mask))
+			continue;
+
+		job_write(pfdev, JOB_INT_CLEAR, mask);
+
+		if (status & JOB_INT_MASK_ERR(j)) {
+			job_write(pfdev, JS_COMMAND_NEXT(j), JS_COMMAND_NOP);
+
+			dev_err(pfdev->dev, "js fault, js=%d, status=%s, head=0x%x, tail=0x%x",
+				j,
+				panfrost_exception_name(pfdev, job_read(pfdev, JS_STATUS(j))),
+				job_read(pfdev, JS_HEAD_LO(j)),
+				job_read(pfdev, JS_TAIL_LO(j)));
+
+			drm_sched_fault(&pfdev->js->queue[j].sched);
+		}
+
+		if (status & JOB_INT_MASK_DONE(j)) {
+			panfrost_devfreq_record_transition(pfdev, j);
+			dma_fence_signal(pfdev->jobs[j]->done_fence);
+		}
+
+		status &= ~mask;
+	}
+
+	return IRQ_HANDLED;
+}
+
+int panfrost_job_init(struct panfrost_device *pfdev)
+{
+	struct panfrost_job_slot *js;
+	int ret, j, irq;
+
+	pfdev->js = js = devm_kzalloc(pfdev->dev, sizeof(*js), GFP_KERNEL);
+	if (!js)
+		return -ENOMEM;
+
+	spin_lock_init(&js->job_lock);
+
+	irq = platform_get_irq_byname(to_platform_device(pfdev->dev), "job");
+	if (irq <= 0)
+		return -ENODEV;
+
+	ret = devm_request_irq(pfdev->dev, irq, panfrost_job_irq_handler,
+			       IRQF_SHARED, "job", pfdev);
+	if (ret) {
+		dev_err(pfdev->dev, "failed to request job irq");
+		return ret;
+	}
+
+	for (j = 0; j < NUM_JOB_SLOTS; j++) {
+		js->queue[j].fence_context = dma_fence_context_alloc(1);
+
+		ret = drm_sched_init(&js->queue[j].sched,
+				     &panfrost_sched_ops,
+				     1, 0, msecs_to_jiffies(500),
+				     "pan_js");
+		if (ret) {
+			dev_err(pfdev->dev, "Failed to create scheduler: %d.", ret);
+			goto err_sched;
+		}
+	}
+
+	panfrost_job_enable_interrupts(pfdev);
+
+	return 0;
+
+err_sched:
+	for (j--; j >= 0; j--)
+		drm_sched_fini(&js->queue[j].sched);
+
+	return ret;
+}
+
+void panfrost_job_fini(struct panfrost_device *pfdev)
+{
+	struct panfrost_job_slot *js = pfdev->js;
+	int j;
+
+	job_write(pfdev, JOB_INT_MASK, 0);
+
+	for (j = 0; j < NUM_JOB_SLOTS; j++)
+		drm_sched_fini(&js->queue[j].sched);
+
+}
+
+int panfrost_job_open(struct panfrost_file_priv *panfrost_priv)
+{
+	struct panfrost_device *pfdev = panfrost_priv->pfdev;
+	struct panfrost_job_slot *js = pfdev->js;
+	struct drm_sched_rq *rq;
+	int ret, i;
+
+	for (i = 0; i < NUM_JOB_SLOTS; i++) {
+		rq = &js->queue[i].sched.sched_rq[DRM_SCHED_PRIORITY_NORMAL];
+		ret = drm_sched_entity_init(&panfrost_priv->sched_entity[i], &rq, 1, NULL);
+		if (WARN_ON(ret))
+			return ret;
+	}
+	return 0;
+}
+
+void panfrost_job_close(struct panfrost_file_priv *panfrost_priv)
+{
+	int i;
+
+	for (i = 0; i < NUM_JOB_SLOTS; i++)
+		drm_sched_entity_destroy(&panfrost_priv->sched_entity[i]);
+}
+
+int panfrost_job_is_idle(struct panfrost_device *pfdev)
+{
+	struct panfrost_job_slot *js = pfdev->js;
+	int i;
+
+	for (i = 0; i < NUM_JOB_SLOTS; i++) {
+		/* If there are any jobs in the HW queue, we're not idle */
+		if (atomic_read(&js->queue[i].sched.hw_rq_count))
+			return false;
+
+		/* Check whether the hardware is idle */
+		if (pfdev->devfreq.slot[i].busy)
+			return false;
+	}
+
+	return true;
+}
