commit 322b38ce3dd6458b92ee25acee68c973281dbbde
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Wed Oct 17 16:28:51 2018 +0200

    drm/nouveau/nvkm: mark expected switch fall-throughs
    
    In preparation to enabling -Wimplicit-fallthrough, mark switch cases
    where we are expecting to fall through.
    
    This patch aims to suppress 29 missing-break-in-switch false positives.
    
    Addresses-Coverity-ID: 1456891 ("Missing break in switch")
    Addresses-Coverity-ID: 1324063 ("Missing break in switch")
    Addresses-Coverity-ID: 1324063 ("Missing break in switch")
    Addresses-Coverity-ID: 141432 ("Missing break in switch")
    Addresses-Coverity-ID: 141433 ("Missing break in switch")
    Addresses-Coverity-ID: 141434 ("Missing break in switch")
    Addresses-Coverity-ID: 141435 ("Missing break in switch")
    Addresses-Coverity-ID: 141436 ("Missing break in switch")
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
index 49ef7e57aad4..7f1adab21a5f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -122,6 +122,7 @@ nv04_dmaobj_new(struct nvkm_dma *dma, const struct nvkm_oclass *oclass,
 		break;
 	case NV_MEM_ACCESS_WO:
 		dmaobj->flags0 |= 0x00008000;
+		/* fall through */
 	case NV_MEM_ACCESS_RW:
 		dmaobj->flags2 |= 0x00000002;
 		break;

commit 5b17f3624efa5473e78ff9b09177e3db0806bb27
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/nv04: implement vmm on top of new base
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
index acc45ce5f93e..49ef7e57aad4 100644
--- a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -26,7 +26,7 @@
 
 #include <core/gpuobj.h>
 #include <subdev/fb.h>
-#include <subdev/mmu/nv04.h>
+#include <subdev/mmu/vmm.h>
 
 #include <nvif/class.h>
 
@@ -49,7 +49,8 @@ nv04_dmaobj_bind(struct nvkm_dmaobj *base, struct nvkm_gpuobj *parent,
 	int ret;
 
 	if (dmaobj->clone) {
-		struct nvkm_memory *pgt = device->mmu->vmm->pgt[0].mem[0];
+		struct nvkm_memory *pgt =
+			device->mmu->vmm->pd->pt[0]->memory;
 		if (!dmaobj->base.start)
 			return nvkm_gpuobj_wrap(pgt, pgpuobj);
 		nvkm_kmap(pgt);

commit 0b11b30de9d2960d87373e50223800c8f9f6a89f
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/nv04-nv4x: move global vmm to nvkm_mmu
    
    In a future commit, this will be constructed by common code.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
index c95942ef8216..acc45ce5f93e 100644
--- a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -49,8 +49,7 @@ nv04_dmaobj_bind(struct nvkm_dmaobj *base, struct nvkm_gpuobj *parent,
 	int ret;
 
 	if (dmaobj->clone) {
-		struct nv04_mmu *mmu = nv04_mmu(device->mmu);
-		struct nvkm_memory *pgt = mmu->vm->pgt[0].mem[0];
+		struct nvkm_memory *pgt = device->mmu->vmm->pgt[0].mem[0];
 		if (!dmaobj->base.start)
 			return nvkm_gpuobj_wrap(pgt, pgpuobj);
 		nvkm_kmap(pgt);

commit c9582455ab74246ec9f5986db3821b33058de585
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:21 2015 +1000

    drm/nouveau/mmu: convert to new-style nvkm_subdev
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
index 2c2fb0988ed4..c95942ef8216 100644
--- a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -81,7 +81,7 @@ int
 nv04_dmaobj_new(struct nvkm_dma *dma, const struct nvkm_oclass *oclass,
 		void *data, u32 size, struct nvkm_dmaobj **pdmaobj)
 {
-	struct nv04_mmu *mmu = nv04_mmu(dma);
+	struct nvkm_device *device = dma->engine.subdev.device;
 	struct nv04_dmaobj *dmaobj;
 	int ret;
 
@@ -95,7 +95,7 @@ nv04_dmaobj_new(struct nvkm_dma *dma, const struct nvkm_oclass *oclass,
 		return ret;
 
 	if (dmaobj->base.target == NV_MEM_TARGET_VM) {
-		if (nv_object(mmu)->oclass == &nv04_mmu_oclass)
+		if (device->mmu->func == &nv04_mmu)
 			dmaobj->clone = true;
 		dmaobj->base.target = NV_MEM_TARGET_PCI;
 		dmaobj->base.access = NV_MEM_ACCESS_RW;

commit 0710cc31482ae3711367c42e61580126c50c8ec0
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:18 2015 +1000

    drm/nouveau/dma: convert user classes to new-style nvkm_object
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
index bd8ca8ec09b6..2c2fb0988ed4 100644
--- a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -21,6 +21,7 @@
  *
  * Authors: Ben Skeggs
  */
+#define nv04_dmaobj(p) container_of((p), struct nv04_dmaobj, base)
 #include "user.h"
 
 #include <core/gpuobj.h>
@@ -36,19 +37,19 @@ struct nv04_dmaobj {
 	u32 flags2;
 };
 
-int
-nv04_dmaobj_bind(struct nvkm_dmaobj *obj, struct nvkm_gpuobj *parent,
-		 struct nvkm_gpuobj **pgpuobj)
+static int
+nv04_dmaobj_bind(struct nvkm_dmaobj *base, struct nvkm_gpuobj *parent,
+		 int align, struct nvkm_gpuobj **pgpuobj)
 {
-	struct nv04_dmaobj *dmaobj = container_of(obj, typeof(*dmaobj), base);
-	struct nvkm_device *device = dmaobj->base.base.engine->subdev.device;
+	struct nv04_dmaobj *dmaobj = nv04_dmaobj(base);
+	struct nvkm_device *device = dmaobj->base.dma->engine.subdev.device;
 	u64 offset = dmaobj->base.start & 0xfffff000;
 	u64 adjust = dmaobj->base.start & 0x00000fff;
 	u32 length = dmaobj->base.limit - dmaobj->base.start;
 	int ret;
 
 	if (dmaobj->clone) {
-		struct nv04_mmu *mmu = nv04_mmu(dmaobj);
+		struct nv04_mmu *mmu = nv04_mmu(device->mmu);
 		struct nvkm_memory *pgt = mmu->vm->pgt[0].mem[0];
 		if (!dmaobj->base.start)
 			return nvkm_gpuobj_wrap(pgt, pgpuobj);
@@ -58,7 +59,7 @@ nv04_dmaobj_bind(struct nvkm_dmaobj *obj, struct nvkm_gpuobj *parent,
 		nvkm_done(pgt);
 	}
 
-	ret = nvkm_gpuobj_new(device, 16, 16, false, parent, pgpuobj);
+	ret = nvkm_gpuobj_new(device, 16, align, false, parent, pgpuobj);
 	if (ret == 0) {
 		nvkm_kmap(*pgpuobj);
 		nvkm_wo32(*pgpuobj, 0x00, dmaobj->flags0 | (adjust << 20));
@@ -71,19 +72,26 @@ nv04_dmaobj_bind(struct nvkm_dmaobj *obj, struct nvkm_gpuobj *parent,
 	return ret;
 }
 
-static int
-nv04_dmaobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
-		 struct nvkm_oclass *oclass, void *data, u32 size,
-		 struct nvkm_object **pobject)
+static const struct nvkm_dmaobj_func
+nv04_dmaobj_func = {
+	.bind = nv04_dmaobj_bind,
+};
+
+int
+nv04_dmaobj_new(struct nvkm_dma *dma, const struct nvkm_oclass *oclass,
+		void *data, u32 size, struct nvkm_dmaobj **pdmaobj)
 {
-	struct nvkm_dma *dmaeng = (void *)engine;
-	struct nv04_mmu *mmu = nv04_mmu(engine);
+	struct nv04_mmu *mmu = nv04_mmu(dma);
 	struct nv04_dmaobj *dmaobj;
 	int ret;
 
-	ret = nvkm_dmaobj_create(parent, engine, oclass, &data, &size, &dmaobj);
-	*pobject = nv_object(dmaobj);
-	if (ret || (ret = -ENOSYS, size))
+	if (!(dmaobj = kzalloc(sizeof(*dmaobj), GFP_KERNEL)))
+		return -ENOMEM;
+	*pdmaobj = &dmaobj->base;
+
+	ret = nvkm_dmaobj_ctor(&nv04_dmaobj_func, dma, oclass,
+			       &data, &size, &dmaobj->base);
+	if (ret)
 		return ret;
 
 	if (dmaobj->base.target == NV_MEM_TARGET_VM) {
@@ -93,7 +101,7 @@ nv04_dmaobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 		dmaobj->base.access = NV_MEM_ACCESS_RW;
 	}
 
-	dmaobj->flags0 = nv_mclass(dmaobj);
+	dmaobj->flags0 = oclass->base.oclass;
 	switch (dmaobj->base.target) {
 	case NV_MEM_TARGET_VRAM:
 		dmaobj->flags0 |= 0x00003000;
@@ -121,21 +129,5 @@ nv04_dmaobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 		return -EINVAL;
 	}
 
-	return dmaeng->bind(&dmaobj->base, (void *)dmaobj, (void *)pobject);
+	return 0;
 }
-
-static struct nvkm_ofuncs
-nv04_dmaobj_ofuncs = {
-	.ctor =  nv04_dmaobj_ctor,
-	.dtor = _nvkm_dmaobj_dtor,
-	.init = _nvkm_dmaobj_init,
-	.fini = _nvkm_dmaobj_fini,
-};
-
-struct nvkm_oclass
-nv04_dmaeng_sclass[] = {
-	{ NV_DMA_FROM_MEMORY, &nv04_dmaobj_ofuncs },
-	{ NV_DMA_TO_MEMORY, &nv04_dmaobj_ofuncs },
-	{ NV_DMA_IN_MEMORY, &nv04_dmaobj_ofuncs },
-	{}
-};

commit 19fef52d93518cc01fd284b55ee93c0a9967634f
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:18 2015 +1000

    drm/nouveau/dma: split user classes out from engine implementations
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
new file mode 100644
index 000000000000..bd8ca8ec09b6
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/nvkm/engine/dma/usernv04.c
@@ -0,0 +1,141 @@
+/*
+ * Copyright 2012 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Ben Skeggs
+ */
+#include "user.h"
+
+#include <core/gpuobj.h>
+#include <subdev/fb.h>
+#include <subdev/mmu/nv04.h>
+
+#include <nvif/class.h>
+
+struct nv04_dmaobj {
+	struct nvkm_dmaobj base;
+	bool clone;
+	u32 flags0;
+	u32 flags2;
+};
+
+int
+nv04_dmaobj_bind(struct nvkm_dmaobj *obj, struct nvkm_gpuobj *parent,
+		 struct nvkm_gpuobj **pgpuobj)
+{
+	struct nv04_dmaobj *dmaobj = container_of(obj, typeof(*dmaobj), base);
+	struct nvkm_device *device = dmaobj->base.base.engine->subdev.device;
+	u64 offset = dmaobj->base.start & 0xfffff000;
+	u64 adjust = dmaobj->base.start & 0x00000fff;
+	u32 length = dmaobj->base.limit - dmaobj->base.start;
+	int ret;
+
+	if (dmaobj->clone) {
+		struct nv04_mmu *mmu = nv04_mmu(dmaobj);
+		struct nvkm_memory *pgt = mmu->vm->pgt[0].mem[0];
+		if (!dmaobj->base.start)
+			return nvkm_gpuobj_wrap(pgt, pgpuobj);
+		nvkm_kmap(pgt);
+		offset  = nvkm_ro32(pgt, 8 + (offset >> 10));
+		offset &= 0xfffff000;
+		nvkm_done(pgt);
+	}
+
+	ret = nvkm_gpuobj_new(device, 16, 16, false, parent, pgpuobj);
+	if (ret == 0) {
+		nvkm_kmap(*pgpuobj);
+		nvkm_wo32(*pgpuobj, 0x00, dmaobj->flags0 | (adjust << 20));
+		nvkm_wo32(*pgpuobj, 0x04, length);
+		nvkm_wo32(*pgpuobj, 0x08, dmaobj->flags2 | offset);
+		nvkm_wo32(*pgpuobj, 0x0c, dmaobj->flags2 | offset);
+		nvkm_done(*pgpuobj);
+	}
+
+	return ret;
+}
+
+static int
+nv04_dmaobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
+		 struct nvkm_oclass *oclass, void *data, u32 size,
+		 struct nvkm_object **pobject)
+{
+	struct nvkm_dma *dmaeng = (void *)engine;
+	struct nv04_mmu *mmu = nv04_mmu(engine);
+	struct nv04_dmaobj *dmaobj;
+	int ret;
+
+	ret = nvkm_dmaobj_create(parent, engine, oclass, &data, &size, &dmaobj);
+	*pobject = nv_object(dmaobj);
+	if (ret || (ret = -ENOSYS, size))
+		return ret;
+
+	if (dmaobj->base.target == NV_MEM_TARGET_VM) {
+		if (nv_object(mmu)->oclass == &nv04_mmu_oclass)
+			dmaobj->clone = true;
+		dmaobj->base.target = NV_MEM_TARGET_PCI;
+		dmaobj->base.access = NV_MEM_ACCESS_RW;
+	}
+
+	dmaobj->flags0 = nv_mclass(dmaobj);
+	switch (dmaobj->base.target) {
+	case NV_MEM_TARGET_VRAM:
+		dmaobj->flags0 |= 0x00003000;
+		break;
+	case NV_MEM_TARGET_PCI:
+		dmaobj->flags0 |= 0x00023000;
+		break;
+	case NV_MEM_TARGET_PCI_NOSNOOP:
+		dmaobj->flags0 |= 0x00033000;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (dmaobj->base.access) {
+	case NV_MEM_ACCESS_RO:
+		dmaobj->flags0 |= 0x00004000;
+		break;
+	case NV_MEM_ACCESS_WO:
+		dmaobj->flags0 |= 0x00008000;
+	case NV_MEM_ACCESS_RW:
+		dmaobj->flags2 |= 0x00000002;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return dmaeng->bind(&dmaobj->base, (void *)dmaobj, (void *)pobject);
+}
+
+static struct nvkm_ofuncs
+nv04_dmaobj_ofuncs = {
+	.ctor =  nv04_dmaobj_ctor,
+	.dtor = _nvkm_dmaobj_dtor,
+	.init = _nvkm_dmaobj_init,
+	.fini = _nvkm_dmaobj_fini,
+};
+
+struct nvkm_oclass
+nv04_dmaeng_sclass[] = {
+	{ NV_DMA_FROM_MEMORY, &nv04_dmaobj_ofuncs },
+	{ NV_DMA_TO_MEMORY, &nv04_dmaobj_ofuncs },
+	{ NV_DMA_IN_MEMORY, &nv04_dmaobj_ofuncs },
+	{}
+};
