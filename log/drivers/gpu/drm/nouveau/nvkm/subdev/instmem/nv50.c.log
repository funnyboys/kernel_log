commit 1786bf56e4180dfd6a51929230e60d11b899032e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Tue Dec 11 14:50:02 2018 +1000

    drm/nouveau/imem/nv50: support pinning objects in BAR2 and returning address
    
    Various structures are accessed by the GPU through BAR2 for some reason
    on newer GPUs.  This commit makes it more convenient to handle.
    
    Will be used for GP100- fault buffers, and GV100- fault method buffers.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index db48a1daca0c..02c4eb28cef4 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -288,6 +288,19 @@ nv50_instobj_addr(struct nvkm_memory *memory)
 	return nvkm_memory_addr(nv50_instobj(memory)->ram);
 }
 
+static u64
+nv50_instobj_bar2(struct nvkm_memory *memory)
+{
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	u64 addr = ~0ULL;
+	if (nv50_instobj_acquire(&iobj->base.memory)) {
+		iobj->lru.next = NULL; /* Exclude from eviction. */
+		addr = iobj->bar->addr;
+	}
+	nv50_instobj_release(&iobj->base.memory);
+	return addr;
+}
+
 static enum nvkm_memory_target
 nv50_instobj_target(struct nvkm_memory *memory)
 {
@@ -325,8 +338,9 @@ static const struct nvkm_memory_func
 nv50_instobj_func = {
 	.dtor = nv50_instobj_dtor,
 	.target = nv50_instobj_target,
-	.size = nv50_instobj_size,
+	.bar2 = nv50_instobj_bar2,
 	.addr = nv50_instobj_addr,
+	.size = nv50_instobj_size,
 	.boot = nv50_instobj_boot,
 	.acquire = nv50_instobj_acquire,
 	.release = nv50_instobj_release,

commit 81a24b9ae8eea95b74337c253059da761043ed06
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Dec 7 11:08:52 2017 +1000

    drm/nouveau/imem/nv50: fix refcount_t warning
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 1ba7289684aa..db48a1daca0c 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -249,7 +249,7 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 			iobj->base.memory.ptrs = &nv50_instobj_fast;
 		else
 			iobj->base.memory.ptrs = &nv50_instobj_slow;
-		refcount_inc(&iobj->maps);
+		refcount_set(&iobj->maps, 1);
 	}
 
 	mutex_unlock(&imem->subdev.mutex);

commit 9202d732e6bc3b46566db3ed25f7a5a3eeaee3c1
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50-: use new interfaces for vmm operations
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 64e2b6e0e8b1..1ba7289684aa 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -46,7 +46,7 @@ struct nv50_instobj {
 	struct nvkm_instobj base;
 	struct nv50_instmem *imem;
 	struct nvkm_memory *ram;
-	struct nvkm_vma bar;
+	struct nvkm_vma *bar;
 	refcount_t maps;
 	void *map;
 	struct list_head lru;
@@ -124,7 +124,7 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	struct nvkm_memory *memory = &iobj->base.memory;
 	struct nvkm_subdev *subdev = &imem->base.subdev;
 	struct nvkm_device *device = subdev->device;
-	struct nvkm_vma bar = {}, ebar;
+	struct nvkm_vma *bar = NULL, *ebar;
 	u64 size = nvkm_memory_size(memory);
 	void *emap;
 	int ret;
@@ -134,7 +134,7 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	 * to the possibility of recursion for page table allocation.
 	 */
 	mutex_unlock(&subdev->mutex);
-	while ((ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &bar))) {
+	while ((ret = nvkm_vmm_get(vmm, 12, size, &bar))) {
 		/* Evict unused mappings, and keep retrying until we either
 		 * succeed,or there's no more objects left on the LRU.
 		 */
@@ -144,10 +144,10 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 			nvkm_debug(subdev, "evict %016llx %016llx @ %016llx\n",
 				   nvkm_memory_addr(&eobj->base.memory),
 				   nvkm_memory_size(&eobj->base.memory),
-				   eobj->bar.offset);
+				   eobj->bar->addr);
 			list_del_init(&eobj->lru);
 			ebar = eobj->bar;
-			eobj->bar.node = NULL;
+			eobj->bar = NULL;
 			emap = eobj->map;
 			eobj->map = NULL;
 		}
@@ -155,16 +155,16 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 		if (!eobj)
 			break;
 		iounmap(emap);
-		nvkm_vm_put(&ebar);
+		nvkm_vmm_put(vmm, &ebar);
 	}
 
 	if (ret == 0)
-		ret = nvkm_memory_map(memory, 0, vmm, &bar, NULL, 0);
+		ret = nvkm_memory_map(memory, 0, vmm, bar, NULL, 0);
 	mutex_lock(&subdev->mutex);
-	if (ret || iobj->bar.node) {
+	if (ret || iobj->bar) {
 		/* We either failed, or another thread beat us. */
 		mutex_unlock(&subdev->mutex);
-		nvkm_vm_put(&bar);
+		nvkm_vmm_put(vmm, &bar);
 		mutex_lock(&subdev->mutex);
 		return;
 	}
@@ -172,10 +172,10 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	/* Make the mapping visible to the host. */
 	iobj->bar = bar;
 	iobj->map = ioremap_wc(device->func->resource_addr(device, 3) +
-			       (u32)iobj->bar.offset, size);
+			       (u32)iobj->bar->addr, size);
 	if (!iobj->map) {
 		nvkm_warn(subdev, "PRAMIN ioremap failed\n");
-		nvkm_vm_put(&iobj->bar);
+		nvkm_vmm_put(vmm, &iobj->bar);
 	}
 }
 
@@ -299,7 +299,7 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nvkm_instmem *imem = &iobj->imem->base;
-	struct nvkm_vma bar;
+	struct nvkm_vma *bar;
 	void *map = map;
 
 	mutex_lock(&imem->subdev.mutex);
@@ -310,8 +310,10 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 	mutex_unlock(&imem->subdev.mutex);
 
 	if (map) {
+		struct nvkm_vmm *vmm = nvkm_bar_bar2_vmm(imem->subdev.device);
 		iounmap(map);
-		nvkm_vm_put(&bar);
+		if (likely(vmm)) /* Can be NULL during BAR destructor. */
+			nvkm_vmm_put(vmm, &bar);
 	}
 
 	nvkm_memory_unref(&iobj->ram);

commit 7f4f82af6e48c22b3fa5e41aab4dc0fdf0a7717e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: allocate memory with nvkm_ram_get()
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 657257daa74d..64e2b6e0e8b1 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -45,7 +45,7 @@ struct nv50_instmem {
 struct nv50_instobj {
 	struct nvkm_instobj base;
 	struct nv50_instmem *imem;
-	struct nvkm_mem *mem;
+	struct nvkm_memory *ram;
 	struct nvkm_vma bar;
 	refcount_t maps;
 	void *map;
@@ -58,8 +58,8 @@ nv50_instobj_wr32_slow(struct nvkm_memory *memory, u64 offset, u32 data)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_device *device = imem->base.subdev.device;
-	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
+	u64 base = (nvkm_memory_addr(iobj->ram) + offset) & 0xffffff00000ULL;
+	u64 addr = (nvkm_memory_addr(iobj->ram) + offset) & 0x000000fffffULL;
 	unsigned long flags;
 
 	spin_lock_irqsave(&imem->base.lock, flags);
@@ -77,8 +77,8 @@ nv50_instobj_rd32_slow(struct nvkm_memory *memory, u64 offset)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_device *device = imem->base.subdev.device;
-	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
+	u64 base = (nvkm_memory_addr(iobj->ram) + offset) & 0xffffff00000ULL;
+	u64 addr = (nvkm_memory_addr(iobj->ram) + offset) & 0x000000fffffULL;
 	u32 data;
 	unsigned long flags;
 
@@ -183,9 +183,8 @@ static int
 nv50_instobj_map(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,
 		 struct nvkm_vma *vma, void *argv, u32 argc)
 {
-	struct nv50_instobj *iobj = nv50_instobj(memory);
-	nvkm_vm_map_at(vma, offset, iobj->mem);
-	return 0;
+	memory = nv50_instobj(memory)->ram;
+	return nvkm_memory_map(memory, offset, vmm, vma, argv, argc);
 }
 
 static void
@@ -280,19 +279,19 @@ nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)
 static u64
 nv50_instobj_size(struct nvkm_memory *memory)
 {
-	return (u64)nv50_instobj(memory)->mem->size << NVKM_RAM_MM_SHIFT;
+	return nvkm_memory_size(nv50_instobj(memory)->ram);
 }
 
 static u64
 nv50_instobj_addr(struct nvkm_memory *memory)
 {
-	return nv50_instobj(memory)->mem->offset;
+	return nvkm_memory_addr(nv50_instobj(memory)->ram);
 }
 
 static enum nvkm_memory_target
 nv50_instobj_target(struct nvkm_memory *memory)
 {
-	return NVKM_MEM_TARGET_VRAM;
+	return nvkm_memory_target(nv50_instobj(memory)->ram);
 }
 
 static void *
@@ -300,7 +299,6 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nvkm_instmem *imem = &iobj->imem->base;
-	struct nvkm_ram *ram = imem->subdev.device->fb->ram;
 	struct nvkm_vma bar;
 	void *map = map;
 
@@ -316,7 +314,7 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 		nvkm_vm_put(&bar);
 	}
 
-	ram->func->put(ram, &iobj->mem);
+	nvkm_memory_unref(&iobj->ram);
 	nvkm_instobj_dtor(imem, &iobj->base);
 	return iobj;
 }
@@ -339,8 +337,8 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 {
 	struct nv50_instmem *imem = nv50_instmem(base);
 	struct nv50_instobj *iobj;
-	struct nvkm_ram *ram = imem->base.subdev.device->fb->ram;
-	int ret;
+	struct nvkm_device *device = imem->base.subdev.device;
+	u8 page = max(order_base_2(align), 12);
 
 	if (!(iobj = kzalloc(sizeof(*iobj), GFP_KERNEL)))
 		return -ENOMEM;
@@ -351,14 +349,7 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 	refcount_set(&iobj->maps, 0);
 	INIT_LIST_HEAD(&iobj->lru);
 
-	size  = max((size  + 4095) & ~4095, (u32)4096);
-	align = max((align + 4095) & ~4095, (u32)4096);
-
-	ret = ram->func->get(ram, size, align, 0, 0x800, &iobj->mem);
-	if (ret)
-		return ret;
-
-	return 0;
+	return nvkm_ram_get(device, 0, 1, page, size, true, true, &iobj->ram);
 }
 
 /******************************************************************************

commit 19a82e492c3d71efe8763d50496a1701dfcf3f15
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/core/memory: change map interface to support upcoming mmu changes
    
    Map flags (access, kind, etc) are currently defined in either the VMA,
    or the memory object, which turns out to not be ideal for things like
    suballocated buffers, etc.
    
    These will become per-map flags instead, so we need to support passing
    these arguments in nvkm_memory_map().
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index bb524e3aa5d9..657257daa74d 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -159,7 +159,7 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	}
 
 	if (ret == 0)
-		nvkm_memory_map(memory, &bar, 0);
+		ret = nvkm_memory_map(memory, 0, vmm, &bar, NULL, 0);
 	mutex_lock(&subdev->mutex);
 	if (ret || iobj->bar.node) {
 		/* We either failed, or another thread beat us. */
@@ -179,11 +179,13 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	}
 }
 
-static void
-nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
+static int
+nv50_instobj_map(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,
+		 struct nvkm_vma *vma, void *argv, u32 argc)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	nvkm_vm_map_at(vma, offset, iobj->mem);
+	return 0;
 }
 
 static void

commit 9ce523cc3bf2ac19922e0a5d4b491221da01d1bc
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau: separate buffer object backing memory from nvkm structures
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 4ccb8cdc7dbc..bb524e3aa5d9 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -356,7 +356,6 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 	if (ret)
 		return ret;
 
-	iobj->mem->page_shift = 12;
 	return 0;
 }
 

commit ffd937bbd219331e4b67344c104dea09b9ed4a6a
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem: use fast-path for resume restore
    
    Before: "imem: init completed in 299277us"
     After: "imem: init completed in  11574us"
    
    Suspend from Fedora 26 gnome desktop on GP102.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index be5670f9fefa..4ccb8cdc7dbc 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -206,7 +206,7 @@ nv50_instobj_release(struct nvkm_memory *memory)
 		}
 
 		/* Switch back to NULL accessors when last map is gone. */
-		iobj->base.memory.ptrs = &nv50_instobj_slow;
+		iobj->base.memory.ptrs = NULL;
 		mutex_unlock(&subdev->mutex);
 	}
 }
@@ -345,7 +345,6 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 	*pmemory = &iobj->base.memory;
 
 	nvkm_instobj_ctor(&nv50_instobj_func, &imem->base, &iobj->base);
-	iobj->base.memory.ptrs = &nv50_instobj_slow;
 	iobj->imem = imem;
 	refcount_set(&iobj->maps, 0);
 	INIT_LIST_HEAD(&iobj->lru);

commit b00b8430468d2922c5ea9a0557c7a36136df98c2
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem: separate pre-BAR2-bootstrap objects from the rest
    
    These will require slow-path access during suspend/resume.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 374df1ebe2e8..be5670f9fefa 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -271,6 +271,7 @@ nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)
 	}
 
 	nv50_instobj_kmap(iobj, vmm);
+	nvkm_instmem_boot(imem);
 	mutex_unlock(&imem->subdev.mutex);
 }
 

commit 71370e620a97fe98daebea86c6ae3775cf0f4fc8
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem: remove now-unused wrapper for backend objects
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index a054e8d155ee..374df1ebe2e8 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -374,7 +374,6 @@ static const struct nvkm_instmem_func
 nv50_instmem = {
 	.fini = nv50_instmem_fini,
 	.memory_new = nv50_instobj_new,
-	.persistent = true,
 	.zero = false,
 };
 

commit 03edf1b31a091254e95793a688abf02b96cfdd85
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: support eviction of BAR2 mappings
    
    A good deal of the structures we map into here aren't accessed very often
    at all, and Fedora 26 has exposed an issue where after creating a heap of
    channels, BAR2 space would run out, and we'd need to make use of the slow
    path while accessing important structures like page tables.
    
    This implements an LRU on BAR2 space, which allows eviction of mappings
    that aren't currently needed, to make space for other objects.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 1a254e69fece..a054e8d155ee 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -32,6 +32,9 @@
 struct nv50_instmem {
 	struct nvkm_instmem base;
 	u64 addr;
+
+	/* Mappings that can be evicted when BAR2 space has been exhausted. */
+	struct list_head lru;
 };
 
 /******************************************************************************
@@ -46,6 +49,7 @@ struct nv50_instobj {
 	struct nvkm_vma bar;
 	refcount_t maps;
 	void *map;
+	struct list_head lru;
 };
 
 static void
@@ -116,11 +120,13 @@ static void
 nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 {
 	struct nv50_instmem *imem = iobj->imem;
+	struct nv50_instobj *eobj;
 	struct nvkm_memory *memory = &iobj->base.memory;
 	struct nvkm_subdev *subdev = &imem->base.subdev;
 	struct nvkm_device *device = subdev->device;
-	struct nvkm_vma bar = {};
+	struct nvkm_vma bar = {}, ebar;
 	u64 size = nvkm_memory_size(memory);
+	void *emap;
 	int ret;
 
 	/* Attempt to allocate BAR2 address-space and map the object
@@ -128,7 +134,30 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 	 * to the possibility of recursion for page table allocation.
 	 */
 	mutex_unlock(&subdev->mutex);
-	ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &bar);
+	while ((ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &bar))) {
+		/* Evict unused mappings, and keep retrying until we either
+		 * succeed,or there's no more objects left on the LRU.
+		 */
+		mutex_lock(&subdev->mutex);
+		eobj = list_first_entry_or_null(&imem->lru, typeof(*eobj), lru);
+		if (eobj) {
+			nvkm_debug(subdev, "evict %016llx %016llx @ %016llx\n",
+				   nvkm_memory_addr(&eobj->base.memory),
+				   nvkm_memory_size(&eobj->base.memory),
+				   eobj->bar.offset);
+			list_del_init(&eobj->lru);
+			ebar = eobj->bar;
+			eobj->bar.node = NULL;
+			emap = eobj->map;
+			eobj->map = NULL;
+		}
+		mutex_unlock(&subdev->mutex);
+		if (!eobj)
+			break;
+		iounmap(emap);
+		nvkm_vm_put(&ebar);
+	}
+
 	if (ret == 0)
 		nvkm_memory_map(memory, &bar, 0);
 	mutex_lock(&subdev->mutex);
@@ -168,6 +197,14 @@ nv50_instobj_release(struct nvkm_memory *memory)
 	nvkm_bar_flush(subdev->device->bar);
 
 	if (refcount_dec_and_mutex_lock(&iobj->maps, &subdev->mutex)) {
+		/* Add the now-unused mapping to the LRU instead of directly
+		 * unmapping it here, in case we need to map it again later.
+		 */
+		if (likely(iobj->lru.next) && iobj->map) {
+			BUG_ON(!list_empty(&iobj->lru));
+			list_add_tail(&iobj->lru, &imem->lru);
+		}
+
 		/* Switch back to NULL accessors when last map is gone. */
 		iobj->base.memory.ptrs = &nv50_instobj_slow;
 		mutex_unlock(&subdev->mutex);
@@ -203,6 +240,10 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	}
 
 	if (!refcount_inc_not_zero(&iobj->maps)) {
+		/* Exclude object from eviction while it's being accessed. */
+		if (likely(iobj->lru.next))
+			list_del_init(&iobj->lru);
+
 		if (map)
 			iobj->base.memory.ptrs = &nv50_instobj_fast;
 		else
@@ -220,7 +261,15 @@ nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nvkm_instmem *imem = &iobj->imem->base;
 
+	/* Exclude bootstrapped objects (ie. the page tables for the
+	 * instmem BAR itself) from eviction.
+	 */
 	mutex_lock(&imem->subdev.mutex);
+	if (likely(iobj->lru.next)) {
+		list_del_init(&iobj->lru);
+		iobj->lru.next = NULL;
+	}
+
 	nv50_instobj_kmap(iobj, vmm);
 	mutex_unlock(&imem->subdev.mutex);
 }
@@ -249,10 +298,21 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nvkm_instmem *imem = &iobj->imem->base;
 	struct nvkm_ram *ram = imem->subdev.device->fb->ram;
-	if (iobj->map) {
-		iounmap(iobj->map);
-		nvkm_vm_put(&iobj->bar);
+	struct nvkm_vma bar;
+	void *map = map;
+
+	mutex_lock(&imem->subdev.mutex);
+	if (likely(iobj->lru.next))
+		list_del(&iobj->lru);
+	map = iobj->map;
+	bar = iobj->bar;
+	mutex_unlock(&imem->subdev.mutex);
+
+	if (map) {
+		iounmap(map);
+		nvkm_vm_put(&bar);
 	}
+
 	ram->func->put(ram, &iobj->mem);
 	nvkm_instobj_dtor(imem, &iobj->base);
 	return iobj;
@@ -287,6 +347,7 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 	iobj->base.memory.ptrs = &nv50_instobj_slow;
 	iobj->imem = imem;
 	refcount_set(&iobj->maps, 0);
+	INIT_LIST_HEAD(&iobj->lru);
 
 	size  = max((size  + 4095) & ~4095, (u32)4096);
 	align = max((align + 4095) & ~4095, (u32)4096);
@@ -326,6 +387,7 @@ nv50_instmem_new(struct nvkm_device *device, int index,
 	if (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))
 		return -ENOMEM;
 	nvkm_instmem_ctor(&nv50_instmem, device, index, &imem->base);
+	INIT_LIST_HEAD(&imem->lru);
 	*pimem = &imem->base;
 	return 0;
 }

commit 69b136f200006ee37b039195eaeb08942c419ecc
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: prevent fast-path for mapped objects when BAR isn't ready
    
    Another piece of solving the "GP100 BAR2 VMM bootstrap" puzzle.
    
    Without doing this, we'd attempt to write PDEs for the lower page table
    levels through BAR2 before BAR2 access has been fully initialised.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index ec2904a0e9fc..1a254e69fece 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -196,9 +196,11 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	}
 
 	/* Attempt to get a direct CPU mapping of the object. */
-	if (!iobj->map && (vmm = nvkm_bar_bar2_vmm(imem->subdev.device)))
-		nv50_instobj_kmap(iobj, vmm);
-	map = iobj->map;
+	if ((vmm = nvkm_bar_bar2_vmm(imem->subdev.device))) {
+		if (!iobj->map)
+			nv50_instobj_kmap(iobj, vmm);
+		map = iobj->map;
+	}
 
 	if (!refcount_inc_not_zero(&iobj->maps)) {
 		if (map)

commit dfcbd5506817c7bfba67c7c2232610f7693b1938
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: map bar2 write-combined
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 9b5606cb2f4e..ec2904a0e9fc 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -142,8 +142,8 @@ nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 
 	/* Make the mapping visible to the host. */
 	iobj->bar = bar;
-	iobj->map = ioremap(device->func->resource_addr(device, 3) +
-			    (u32)iobj->bar.offset, size);
+	iobj->map = ioremap_wc(device->func->resource_addr(device, 3) +
+			       (u32)iobj->bar.offset, size);
 	if (!iobj->map) {
 		nvkm_warn(subdev, "PRAMIN ioremap failed\n");
 		nvkm_vm_put(&iobj->bar);
@@ -164,6 +164,7 @@ nv50_instobj_release(struct nvkm_memory *memory)
 	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_subdev *subdev = &imem->base.subdev;
 
+	wmb();
 	nvkm_bar_flush(subdev->device->bar);
 
 	if (refcount_dec_and_mutex_lock(&iobj->maps, &subdev->mutex)) {

commit be55287aa5ba6895e9d4d3ed2f08a1be7a065957
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: embed nvkm_instobj directly into nv04_instobj
    
    This is not as simple as it was for earlier GPUs, due to the need to swap
    accessor functions depending on whether BAR2 is usable or not.
    
    We were previously protected by nvkm_instobj's accessor functions keeping
    an object mapped permanently, with some unclear magic that managed to hit
    the slow-path where needed even if an object was marked as mapped.
    
    That's been replaced here by reference counting maps (some objects, like
    page tables can be accessed concurrently), and swapping the functions as
    necessary.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index d0159d5876f3..9b5606cb2f4e 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -37,13 +37,14 @@ struct nv50_instmem {
 /******************************************************************************
  * instmem object implementation
  *****************************************************************************/
-#define nv50_instobj(p) container_of((p), struct nv50_instobj, memory)
+#define nv50_instobj(p) container_of((p), struct nv50_instobj, base.memory)
 
 struct nv50_instobj {
-	struct nvkm_memory memory;
+	struct nvkm_instobj base;
 	struct nv50_instmem *imem;
 	struct nvkm_mem *mem;
 	struct nvkm_vma bar;
+	refcount_t maps;
 	void *map;
 };
 
@@ -93,31 +94,59 @@ nv50_instobj_slow = {
 	.wr32 = nv50_instobj_wr32_slow,
 };
 
+static void
+nv50_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)
+{
+	iowrite32_native(data, nv50_instobj(memory)->map + offset);
+}
+
+static u32
+nv50_instobj_rd32(struct nvkm_memory *memory, u64 offset)
+{
+	return ioread32_native(nv50_instobj(memory)->map + offset);
+}
+
+static const struct nvkm_memory_ptrs
+nv50_instobj_fast = {
+	.rd32 = nv50_instobj_rd32,
+	.wr32 = nv50_instobj_wr32,
+};
+
 static void
 nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
 {
-	struct nvkm_memory *memory = &iobj->memory;
-	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
+	struct nv50_instmem *imem = iobj->imem;
+	struct nvkm_memory *memory = &iobj->base.memory;
+	struct nvkm_subdev *subdev = &imem->base.subdev;
 	struct nvkm_device *device = subdev->device;
+	struct nvkm_vma bar = {};
 	u64 size = nvkm_memory_size(memory);
-	void __iomem *map;
 	int ret;
 
-	iobj->map = ERR_PTR(-ENOMEM);
-
-	ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
-	if (ret == 0) {
-		map = ioremap(device->func->resource_addr(device, 3) +
-			      (u32)iobj->bar.offset, size);
-		if (map) {
-			nvkm_memory_map(memory, &iobj->bar, 0);
-			iobj->map = map;
-		} else {
-			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
-			nvkm_vm_put(&iobj->bar);
-		}
-	} else {
-		nvkm_warn(subdev, "PRAMIN exhausted\n");
+	/* Attempt to allocate BAR2 address-space and map the object
+	 * into it.  The lock has to be dropped while doing this due
+	 * to the possibility of recursion for page table allocation.
+	 */
+	mutex_unlock(&subdev->mutex);
+	ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &bar);
+	if (ret == 0)
+		nvkm_memory_map(memory, &bar, 0);
+	mutex_lock(&subdev->mutex);
+	if (ret || iobj->bar.node) {
+		/* We either failed, or another thread beat us. */
+		mutex_unlock(&subdev->mutex);
+		nvkm_vm_put(&bar);
+		mutex_lock(&subdev->mutex);
+		return;
+	}
+
+	/* Make the mapping visible to the host. */
+	iobj->bar = bar;
+	iobj->map = ioremap(device->func->resource_addr(device, 3) +
+			    (u32)iobj->bar.offset, size);
+	if (!iobj->map) {
+		nvkm_warn(subdev, "PRAMIN ioremap failed\n");
+		nvkm_vm_put(&iobj->bar);
 	}
 }
 
@@ -131,28 +160,66 @@ nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
 static void
 nv50_instobj_release(struct nvkm_memory *memory)
 {
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
+	struct nvkm_subdev *subdev = &imem->base.subdev;
+
+	nvkm_bar_flush(subdev->device->bar);
+
+	if (refcount_dec_and_mutex_lock(&iobj->maps, &subdev->mutex)) {
+		/* Switch back to NULL accessors when last map is gone. */
+		iobj->base.memory.ptrs = &nv50_instobj_slow;
+		mutex_unlock(&subdev->mutex);
+	}
 }
 
 static void __iomem *
 nv50_instobj_acquire(struct nvkm_memory *memory)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nv50_instmem *imem = iobj->imem;
-	struct nvkm_vm *vm;
+	struct nvkm_instmem *imem = &iobj->imem->base;
+	struct nvkm_vmm *vmm;
+	void __iomem *map = NULL;
 
-	if (!iobj->map && (vm = nvkm_bar_bar2_vmm(imem->base.subdev.device)))
-		nv50_instobj_kmap(iobj, vm);
-	if (!IS_ERR_OR_NULL(iobj->map))
+	/* Already mapped? */
+	if (refcount_inc_not_zero(&iobj->maps))
 		return iobj->map;
 
-	return NULL;
+	/* Take the lock, and re-check that another thread hasn't
+	 * already mapped the object in the meantime.
+	 */
+	mutex_lock(&imem->subdev.mutex);
+	if (refcount_inc_not_zero(&iobj->maps)) {
+		mutex_unlock(&imem->subdev.mutex);
+		return iobj->map;
+	}
+
+	/* Attempt to get a direct CPU mapping of the object. */
+	if (!iobj->map && (vmm = nvkm_bar_bar2_vmm(imem->subdev.device)))
+		nv50_instobj_kmap(iobj, vmm);
+	map = iobj->map;
+
+	if (!refcount_inc_not_zero(&iobj->maps)) {
+		if (map)
+			iobj->base.memory.ptrs = &nv50_instobj_fast;
+		else
+			iobj->base.memory.ptrs = &nv50_instobj_slow;
+		refcount_inc(&iobj->maps);
+	}
+
+	mutex_unlock(&imem->subdev.mutex);
+	return map;
 }
 
 static void
 nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nvkm_instmem *imem = &iobj->imem->base;
+
+	mutex_lock(&imem->subdev.mutex);
 	nv50_instobj_kmap(iobj, vmm);
+	mutex_unlock(&imem->subdev.mutex);
 }
 
 static u64
@@ -177,12 +244,14 @@ static void *
 nv50_instobj_dtor(struct nvkm_memory *memory)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nvkm_ram *ram = iobj->imem->base.subdev.device->fb->ram;
-	if (!IS_ERR_OR_NULL(iobj->map)) {
+	struct nvkm_instmem *imem = &iobj->imem->base;
+	struct nvkm_ram *ram = imem->subdev.device->fb->ram;
+	if (iobj->map) {
 		iounmap(iobj->map);
 		nvkm_vm_put(&iobj->bar);
 	}
 	ram->func->put(ram, &iobj->mem);
+	nvkm_instobj_dtor(imem, &iobj->base);
 	return iobj;
 }
 
@@ -209,11 +278,12 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 
 	if (!(iobj = kzalloc(sizeof(*iobj), GFP_KERNEL)))
 		return -ENOMEM;
-	*pmemory = &iobj->memory;
+	*pmemory = &iobj->base.memory;
 
-	nvkm_memory_ctor(&nv50_instobj_func, &iobj->memory);
-	iobj->memory.ptrs = &nv50_instobj_slow;
+	nvkm_instobj_ctor(&nv50_instobj_func, &imem->base, &iobj->base);
+	iobj->base.memory.ptrs = &nv50_instobj_slow;
 	iobj->imem = imem;
+	refcount_set(&iobj->maps, 0);
 
 	size  = max((size  + 4095) & ~4095, (u32)4096);
 	align = max((align + 4095) & ~4095, (u32)4096);
@@ -240,7 +310,7 @@ static const struct nvkm_instmem_func
 nv50_instmem = {
 	.fini = nv50_instmem_fini,
 	.memory_new = nv50_instobj_new,
-	.persistent = false,
+	.persistent = true,
 	.zero = false,
 };
 

commit af515ec8d3fbd8376513eee9648a52d5ab92bbac
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: move slow-path locking into rd/wr functions
    
    This is to simplify upcoming changes.  The slow-path is something that
    currently occurs during bootstrap of the BAR2 VMM, while backing up an
    object during suspend/resume, or when BAR2 address space runs out.
    
    The latter is a real problem that can happen at runtime, and occurs in
    Fedora 26 already (due to some change that causes a lot of channels to
    be created at login), so ideally we'd prefer not to make it any slower.
    
    We'd also like suspend/resume speed to not suffer.
    
    Upcoming commits will solve those problems in a better way, making the
    extra overhead of moving the locking here a non-issue.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 95b2c560fe4b..d0159d5876f3 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -31,8 +31,6 @@
 
 struct nv50_instmem {
 	struct nvkm_instmem base;
-	unsigned long lock_flags;
-	spinlock_t lock;
 	u64 addr;
 };
 
@@ -57,12 +55,15 @@ nv50_instobj_wr32_slow(struct nvkm_memory *memory, u64 offset, u32 data)
 	struct nvkm_device *device = imem->base.subdev.device;
 	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
+	unsigned long flags;
 
+	spin_lock_irqsave(&imem->base.lock, flags);
 	if (unlikely(imem->addr != base)) {
 		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
 	nvkm_wr32(device, 0x700000 + addr, data);
+	spin_unlock_irqrestore(&imem->base.lock, flags);
 }
 
 static u32
@@ -74,12 +75,15 @@ nv50_instobj_rd32_slow(struct nvkm_memory *memory, u64 offset)
 	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
 	u32 data;
+	unsigned long flags;
 
+	spin_lock_irqsave(&imem->base.lock, flags);
 	if (unlikely(imem->addr != base)) {
 		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
 	data = nvkm_rd32(device, 0x700000 + addr);
+	spin_unlock_irqrestore(&imem->base.lock, flags);
 	return data;
 }
 
@@ -127,8 +131,6 @@ nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
 static void
 nv50_instobj_release(struct nvkm_memory *memory)
 {
-	struct nv50_instmem *imem = nv50_instobj(memory)->imem;
-	spin_unlock_irqrestore(&imem->lock, imem->lock_flags);
 }
 
 static void __iomem *
@@ -137,15 +139,12 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_vm *vm;
-	unsigned long flags;
 
 	if (!iobj->map && (vm = nvkm_bar_bar2_vmm(imem->base.subdev.device)))
 		nv50_instobj_kmap(iobj, vm);
 	if (!IS_ERR_OR_NULL(iobj->map))
 		return iobj->map;
 
-	spin_lock_irqsave(&imem->lock, flags);
-	imem->lock_flags = flags;
 	return NULL;
 }
 
@@ -254,7 +253,6 @@ nv50_instmem_new(struct nvkm_device *device, int index,
 	if (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))
 		return -ENOMEM;
 	nvkm_instmem_ctor(&nv50_instmem, device, index, &imem->base);
-	spin_lock_init(&imem->lock);
 	*pimem = &imem->base;
 	return 0;
 }

commit f584bde6095af4d91e917be54c487258856ace89
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/imem/nv50: split object map out from api functions
    
    acquire()/boot() will need different logic in addition to performing
    the actual mapping.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index e3273aed3381..95b2c560fe4b 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -89,6 +89,34 @@ nv50_instobj_slow = {
 	.wr32 = nv50_instobj_wr32_slow,
 };
 
+static void
+nv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)
+{
+	struct nvkm_memory *memory = &iobj->memory;
+	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
+	struct nvkm_device *device = subdev->device;
+	u64 size = nvkm_memory_size(memory);
+	void __iomem *map;
+	int ret;
+
+	iobj->map = ERR_PTR(-ENOMEM);
+
+	ret = nvkm_vm_get(vmm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
+	if (ret == 0) {
+		map = ioremap(device->func->resource_addr(device, 3) +
+			      (u32)iobj->bar.offset, size);
+		if (map) {
+			nvkm_memory_map(memory, &iobj->bar, 0);
+			iobj->map = map;
+		} else {
+			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
+			nvkm_vm_put(&iobj->bar);
+		}
+	} else {
+		nvkm_warn(subdev, "PRAMIN exhausted\n");
+	}
+}
+
 static void
 nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
 {
@@ -112,7 +140,7 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	unsigned long flags;
 
 	if (!iobj->map && (vm = nvkm_bar_bar2_vmm(imem->base.subdev.device)))
-		nvkm_memory_boot(memory, vm);
+		nv50_instobj_kmap(iobj, vm);
 	if (!IS_ERR_OR_NULL(iobj->map))
 		return iobj->map;
 
@@ -122,31 +150,10 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 }
 
 static void
-nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vm *vm)
+nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
-	struct nvkm_device *device = subdev->device;
-	u64 size = nvkm_memory_size(memory);
-	void __iomem *map;
-	int ret;
-
-	iobj->map = ERR_PTR(-ENOMEM);
-
-	ret = nvkm_vm_get(vm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
-	if (ret == 0) {
-		map = ioremap(device->func->resource_addr(device, 3) +
-			      (u32)iobj->bar.offset, size);
-		if (map) {
-			nvkm_memory_map(memory, &iobj->bar, 0);
-			iobj->map = map;
-		} else {
-			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
-			nvkm_vm_put(&iobj->bar);
-		}
-	} else {
-		nvkm_warn(subdev, "PRAMIN exhausted\n");
-	}
+	nv50_instobj_kmap(iobj, vmm);
 }
 
 static u64
@@ -173,8 +180,8 @@ nv50_instobj_dtor(struct nvkm_memory *memory)
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nvkm_ram *ram = iobj->imem->base.subdev.device->fb->ram;
 	if (!IS_ERR_OR_NULL(iobj->map)) {
-		nvkm_vm_put(&iobj->bar);
 		iounmap(iobj->map);
+		nvkm_vm_put(&iobj->bar);
 	}
 	ram->func->put(ram, &iobj->mem);
 	return iobj;

commit 07bbc1c5f49b64323d9e5c1e0d5d7d201e1f2627
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/core/memory: split info pointers from accessor pointers
    
    The accessor functions can change as a result of acquire()/release() calls,
    and are protected by any refcounting done there.
    
    Other functions must remain constant, as they can be called any time.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index a3cd3e193d03..e3273aed3381 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -49,50 +49,51 @@ struct nv50_instobj {
 	void *map;
 };
 
-static enum nvkm_memory_target
-nv50_instobj_target(struct nvkm_memory *memory)
+static void
+nv50_instobj_wr32_slow(struct nvkm_memory *memory, u64 offset, u32 data)
 {
-	return NVKM_MEM_TARGET_VRAM;
-}
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
+	struct nvkm_device *device = imem->base.subdev.device;
+	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
 
-static u64
-nv50_instobj_addr(struct nvkm_memory *memory)
-{
-	return nv50_instobj(memory)->mem->offset;
+	if (unlikely(imem->addr != base)) {
+		nvkm_wr32(device, 0x001700, base >> 16);
+		imem->addr = base;
+	}
+	nvkm_wr32(device, 0x700000 + addr, data);
 }
 
-static u64
-nv50_instobj_size(struct nvkm_memory *memory)
+static u32
+nv50_instobj_rd32_slow(struct nvkm_memory *memory, u64 offset)
 {
-	return (u64)nv50_instobj(memory)->mem->size << NVKM_RAM_MM_SHIFT;
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
+	struct nvkm_device *device = imem->base.subdev.device;
+	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
+	u32 data;
+
+	if (unlikely(imem->addr != base)) {
+		nvkm_wr32(device, 0x001700, base >> 16);
+		imem->addr = base;
+	}
+	data = nvkm_rd32(device, 0x700000 + addr);
+	return data;
 }
 
+static const struct nvkm_memory_ptrs
+nv50_instobj_slow = {
+	.rd32 = nv50_instobj_rd32_slow,
+	.wr32 = nv50_instobj_wr32_slow,
+};
+
 static void
-nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vm *vm)
+nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
-	struct nvkm_device *device = subdev->device;
-	u64 size = nvkm_memory_size(memory);
-	void __iomem *map;
-	int ret;
-
-	iobj->map = ERR_PTR(-ENOMEM);
-
-	ret = nvkm_vm_get(vm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
-	if (ret == 0) {
-		map = ioremap(device->func->resource_addr(device, 3) +
-			      (u32)iobj->bar.offset, size);
-		if (map) {
-			nvkm_memory_map(memory, &iobj->bar, 0);
-			iobj->map = map;
-		} else {
-			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
-			nvkm_vm_put(&iobj->bar);
-		}
-	} else {
-		nvkm_warn(subdev, "PRAMIN exhausted\n");
-	}
+	nvkm_vm_map_at(vma, offset, iobj->mem);
 }
 
 static void
@@ -120,45 +121,50 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	return NULL;
 }
 
-static u32
-nv50_instobj_rd32(struct nvkm_memory *memory, u64 offset)
+static void
+nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vm *vm)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nv50_instmem *imem = iobj->imem;
-	struct nvkm_device *device = imem->base.subdev.device;
-	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
-	u32 data;
+	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
+	struct nvkm_device *device = subdev->device;
+	u64 size = nvkm_memory_size(memory);
+	void __iomem *map;
+	int ret;
 
-	if (unlikely(imem->addr != base)) {
-		nvkm_wr32(device, 0x001700, base >> 16);
-		imem->addr = base;
+	iobj->map = ERR_PTR(-ENOMEM);
+
+	ret = nvkm_vm_get(vm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
+	if (ret == 0) {
+		map = ioremap(device->func->resource_addr(device, 3) +
+			      (u32)iobj->bar.offset, size);
+		if (map) {
+			nvkm_memory_map(memory, &iobj->bar, 0);
+			iobj->map = map;
+		} else {
+			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
+			nvkm_vm_put(&iobj->bar);
+		}
+	} else {
+		nvkm_warn(subdev, "PRAMIN exhausted\n");
 	}
-	data = nvkm_rd32(device, 0x700000 + addr);
-	return data;
 }
 
-static void
-nv50_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)
+static u64
+nv50_instobj_size(struct nvkm_memory *memory)
 {
-	struct nv50_instobj *iobj = nv50_instobj(memory);
-	struct nv50_instmem *imem = iobj->imem;
-	struct nvkm_device *device = imem->base.subdev.device;
-	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
+	return (u64)nv50_instobj(memory)->mem->size << NVKM_RAM_MM_SHIFT;
+}
 
-	if (unlikely(imem->addr != base)) {
-		nvkm_wr32(device, 0x001700, base >> 16);
-		imem->addr = base;
-	}
-	nvkm_wr32(device, 0x700000 + addr, data);
+static u64
+nv50_instobj_addr(struct nvkm_memory *memory)
+{
+	return nv50_instobj(memory)->mem->offset;
 }
 
-static void
-nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
+static enum nvkm_memory_target
+nv50_instobj_target(struct nvkm_memory *memory)
 {
-	struct nv50_instobj *iobj = nv50_instobj(memory);
-	nvkm_vm_map_at(vma, offset, iobj->mem);
+	return NVKM_MEM_TARGET_VRAM;
 }
 
 static void *
@@ -183,8 +189,6 @@ nv50_instobj_func = {
 	.boot = nv50_instobj_boot,
 	.acquire = nv50_instobj_acquire,
 	.release = nv50_instobj_release,
-	.rd32 = nv50_instobj_rd32,
-	.wr32 = nv50_instobj_wr32,
 	.map = nv50_instobj_map,
 };
 
@@ -202,6 +206,7 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
 	*pmemory = &iobj->memory;
 
 	nvkm_memory_ctor(&nv50_instobj_func, &iobj->memory);
+	iobj->memory.ptrs = &nv50_instobj_slow;
 	iobj->imem = imem;
 
 	size  = max((size  + 4095) & ~4095, (u32)4096);

commit a78dbce9a161a3a985b837bd07afd8651d42cabd
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/bar: modify interface to bar2 vmm mapping
    
    Match API with the BAR1 version.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 6d512c062ae3..a3cd3e193d03 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -107,11 +107,10 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 {
 	struct nv50_instobj *iobj = nv50_instobj(memory);
 	struct nv50_instmem *imem = iobj->imem;
-	struct nvkm_bar *bar = imem->base.subdev.device->bar;
 	struct nvkm_vm *vm;
 	unsigned long flags;
 
-	if (!iobj->map && (vm = nvkm_bar_kmap(bar)))
+	if (!iobj->map && (vm = nvkm_bar_bar2_vmm(imem->base.subdev.device)))
 		nvkm_memory_boot(memory, vm);
 	if (!IS_ERR_OR_NULL(iobj->map))
 		return iobj->map;

commit 7e8820fed712c6de1933dcc91edbf08dcec74925
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:23 2015 +1000

    drm/nouveau/device: cleaner abstraction for device resource functions
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index ec5020e3fc42..6d512c062ae3 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -81,7 +81,7 @@ nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vm *vm)
 
 	ret = nvkm_vm_get(vm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
 	if (ret == 0) {
-		map = ioremap(nv_device_resource_start(device, 3) +
+		map = ioremap(device->func->resource_addr(device, 3) +
 			      (u32)iobj->bar.offset, size);
 		if (map) {
 			nvkm_memory_map(memory, &iobj->bar, 0);

commit b7a2bc1886d00f5f1358079e1e6f4979006a4ed6
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:20 2015 +1000

    drm/nouveau/imem: convert to new-style nvkm_subdev
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index af236f8e4ddc..ec5020e3fc42 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -220,41 +220,30 @@ nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
  * instmem subdev implementation
  *****************************************************************************/
 
-static int
-nv50_instmem_fini(struct nvkm_object *object, bool suspend)
+static void
+nv50_instmem_fini(struct nvkm_instmem *base)
 {
-	struct nv50_instmem *imem = (void *)object;
-	imem->addr = ~0ULL;
-	return nvkm_instmem_fini(&imem->base, suspend);
+	nv50_instmem(base)->addr = ~0ULL;
 }
 
-static int
-nv50_instmem_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
-		  struct nvkm_oclass *oclass, void *data, u32 size,
-		  struct nvkm_object **pobject)
+static const struct nvkm_instmem_func
+nv50_instmem = {
+	.fini = nv50_instmem_fini,
+	.memory_new = nv50_instobj_new,
+	.persistent = false,
+	.zero = false,
+};
+
+int
+nv50_instmem_new(struct nvkm_device *device, int index,
+		 struct nvkm_instmem **pimem)
 {
 	struct nv50_instmem *imem;
-	int ret;
-
-	ret = nvkm_instmem_create(parent, engine, oclass, &imem);
-	*pobject = nv_object(imem);
-	if (ret)
-		return ret;
 
+	if (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))
+		return -ENOMEM;
+	nvkm_instmem_ctor(&nv50_instmem, device, index, &imem->base);
 	spin_lock_init(&imem->lock);
+	*pimem = &imem->base;
 	return 0;
 }
-
-struct nvkm_oclass *
-nv50_instmem_oclass = &(struct nvkm_instmem_impl) {
-	.base.handle = NV_SUBDEV(INSTMEM, 0x50),
-	.base.ofuncs = &(struct nvkm_ofuncs) {
-		.ctor = nv50_instmem_ctor,
-		.dtor = _nvkm_instmem_dtor,
-		.init = _nvkm_instmem_init,
-		.fini = nv50_instmem_fini,
-	},
-	.memory_new = nv50_instobj_new,
-	.persistent = false,
-	.zero = false,
-}.base;

commit 3293228174e4d44cca56d809cc8409c3f88f8b90
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:20 2015 +1000

    drm/nouveau/bar: convert to new-style nvkm_subdev
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 535a8f9c23ce..af236f8e4ddc 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -111,7 +111,7 @@ nv50_instobj_acquire(struct nvkm_memory *memory)
 	struct nvkm_vm *vm;
 	unsigned long flags;
 
-	if (!iobj->map && bar && bar->kmap && (vm = bar->kmap(bar)))
+	if (!iobj->map && (vm = nvkm_bar_kmap(bar)))
 		nvkm_memory_boot(memory, vm);
 	if (!IS_ERR_OR_NULL(iobj->map))
 		return iobj->map;

commit d8e83994aaf6749b7124a219f5b46bd1329e2a08
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/imem: improve management of instance memory
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 4f6354df538a..535a8f9c23ce 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -21,115 +21,201 @@
  *
  * Authors: Ben Skeggs
  */
+#define nv50_instmem(p) container_of((p), struct nv50_instmem, base)
 #include "priv.h"
 
+#include <core/memory.h>
+#include <subdev/bar.h>
 #include <subdev/fb.h>
+#include <subdev/mmu.h>
 
 struct nv50_instmem {
 	struct nvkm_instmem base;
+	unsigned long lock_flags;
 	spinlock_t lock;
 	u64 addr;
 };
 
+/******************************************************************************
+ * instmem object implementation
+ *****************************************************************************/
+#define nv50_instobj(p) container_of((p), struct nv50_instobj, memory)
+
 struct nv50_instobj {
-	struct nvkm_instobj base;
+	struct nvkm_memory memory;
+	struct nv50_instmem *imem;
 	struct nvkm_mem *mem;
+	struct nvkm_vma bar;
+	void *map;
 };
 
-/******************************************************************************
- * instmem object implementation
- *****************************************************************************/
+static enum nvkm_memory_target
+nv50_instobj_target(struct nvkm_memory *memory)
+{
+	return NVKM_MEM_TARGET_VRAM;
+}
+
+static u64
+nv50_instobj_addr(struct nvkm_memory *memory)
+{
+	return nv50_instobj(memory)->mem->offset;
+}
+
+static u64
+nv50_instobj_size(struct nvkm_memory *memory)
+{
+	return (u64)nv50_instobj(memory)->mem->size << NVKM_RAM_MM_SHIFT;
+}
+
+static void
+nv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vm *vm)
+{
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nvkm_subdev *subdev = &iobj->imem->base.subdev;
+	struct nvkm_device *device = subdev->device;
+	u64 size = nvkm_memory_size(memory);
+	void __iomem *map;
+	int ret;
+
+	iobj->map = ERR_PTR(-ENOMEM);
+
+	ret = nvkm_vm_get(vm, size, 12, NV_MEM_ACCESS_RW, &iobj->bar);
+	if (ret == 0) {
+		map = ioremap(nv_device_resource_start(device, 3) +
+			      (u32)iobj->bar.offset, size);
+		if (map) {
+			nvkm_memory_map(memory, &iobj->bar, 0);
+			iobj->map = map;
+		} else {
+			nvkm_warn(subdev, "PRAMIN ioremap failed\n");
+			nvkm_vm_put(&iobj->bar);
+		}
+	} else {
+		nvkm_warn(subdev, "PRAMIN exhausted\n");
+	}
+}
+
+static void
+nv50_instobj_release(struct nvkm_memory *memory)
+{
+	struct nv50_instmem *imem = nv50_instobj(memory)->imem;
+	spin_unlock_irqrestore(&imem->lock, imem->lock_flags);
+}
+
+static void __iomem *
+nv50_instobj_acquire(struct nvkm_memory *memory)
+{
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
+	struct nvkm_bar *bar = imem->base.subdev.device->bar;
+	struct nvkm_vm *vm;
+	unsigned long flags;
+
+	if (!iobj->map && bar && bar->kmap && (vm = bar->kmap(bar)))
+		nvkm_memory_boot(memory, vm);
+	if (!IS_ERR_OR_NULL(iobj->map))
+		return iobj->map;
+
+	spin_lock_irqsave(&imem->lock, flags);
+	imem->lock_flags = flags;
+	return NULL;
+}
 
 static u32
-nv50_instobj_rd32(struct nvkm_object *object, u64 offset)
+nv50_instobj_rd32(struct nvkm_memory *memory, u64 offset)
 {
-	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
-	struct nv50_instobj *node = (void *)object;
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_device *device = imem->base.subdev.device;
-	unsigned long flags;
-	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
+	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
 	u32 data;
 
-	spin_lock_irqsave(&imem->lock, flags);
 	if (unlikely(imem->addr != base)) {
 		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
 	data = nvkm_rd32(device, 0x700000 + addr);
-	spin_unlock_irqrestore(&imem->lock, flags);
 	return data;
 }
 
 static void
-nv50_instobj_wr32(struct nvkm_object *object, u64 offset, u32 data)
+nv50_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)
 {
-	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
-	struct nv50_instobj *node = (void *)object;
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nv50_instmem *imem = iobj->imem;
 	struct nvkm_device *device = imem->base.subdev.device;
-	unsigned long flags;
-	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
-	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
+	u64 base = (iobj->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (iobj->mem->offset + offset) & 0x000000fffffULL;
 
-	spin_lock_irqsave(&imem->lock, flags);
 	if (unlikely(imem->addr != base)) {
 		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
 	nvkm_wr32(device, 0x700000 + addr, data);
-	spin_unlock_irqrestore(&imem->lock, flags);
 }
 
 static void
-nv50_instobj_dtor(struct nvkm_object *object)
+nv50_instobj_map(struct nvkm_memory *memory, struct nvkm_vma *vma, u64 offset)
 {
-	struct nv50_instobj *node = (void *)object;
-	struct nvkm_ram *ram = nvkm_fb(object)->ram;
-	ram->func->put(ram, &node->mem);
-	nvkm_instobj_destroy(&node->base);
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	nvkm_vm_map_at(vma, offset, iobj->mem);
 }
 
+static void *
+nv50_instobj_dtor(struct nvkm_memory *memory)
+{
+	struct nv50_instobj *iobj = nv50_instobj(memory);
+	struct nvkm_ram *ram = iobj->imem->base.subdev.device->fb->ram;
+	if (!IS_ERR_OR_NULL(iobj->map)) {
+		nvkm_vm_put(&iobj->bar);
+		iounmap(iobj->map);
+	}
+	ram->func->put(ram, &iobj->mem);
+	return iobj;
+}
+
+static const struct nvkm_memory_func
+nv50_instobj_func = {
+	.dtor = nv50_instobj_dtor,
+	.target = nv50_instobj_target,
+	.size = nv50_instobj_size,
+	.addr = nv50_instobj_addr,
+	.boot = nv50_instobj_boot,
+	.acquire = nv50_instobj_acquire,
+	.release = nv50_instobj_release,
+	.rd32 = nv50_instobj_rd32,
+	.wr32 = nv50_instobj_wr32,
+	.map = nv50_instobj_map,
+};
+
 static int
-nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
-		  struct nvkm_oclass *oclass, void *data, u32 size,
-		  struct nvkm_object **pobject)
+nv50_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,
+		 struct nvkm_memory **pmemory)
 {
-	struct nvkm_ram *ram = nvkm_fb(parent)->ram;
-	struct nvkm_instobj_args *args = data;
-	struct nv50_instobj *node;
+	struct nv50_instmem *imem = nv50_instmem(base);
+	struct nv50_instobj *iobj;
+	struct nvkm_ram *ram = imem->base.subdev.device->fb->ram;
 	int ret;
 
-	args->size  = max((args->size  + 4095) & ~4095, (u32)4096);
-	args->align = max((args->align + 4095) & ~4095, (u32)4096);
+	if (!(iobj = kzalloc(sizeof(*iobj), GFP_KERNEL)))
+		return -ENOMEM;
+	*pmemory = &iobj->memory;
 
-	ret = nvkm_instobj_create(parent, engine, oclass, &node);
-	*pobject = nv_object(node);
-	if (ret)
-		return ret;
+	nvkm_memory_ctor(&nv50_instobj_func, &iobj->memory);
+	iobj->imem = imem;
+
+	size  = max((size  + 4095) & ~4095, (u32)4096);
+	align = max((align + 4095) & ~4095, (u32)4096);
 
-	ret = ram->func->get(ram, args->size, args->align, 0, 0x800,
-			     &node->mem);
+	ret = ram->func->get(ram, size, align, 0, 0x800, &iobj->mem);
 	if (ret)
 		return ret;
 
-	node->base.addr = node->mem->offset;
-	node->base.size = node->mem->size << 12;
-	node->mem->page_shift = 12;
+	iobj->mem->page_shift = 12;
 	return 0;
 }
 
-static struct nvkm_instobj_impl
-nv50_instobj_oclass = {
-	.base.ofuncs = &(struct nvkm_ofuncs) {
-		.ctor = nv50_instobj_ctor,
-		.dtor = nv50_instobj_dtor,
-		.init = _nvkm_instobj_init,
-		.fini = _nvkm_instobj_fini,
-		.rd32 = nv50_instobj_rd32,
-		.wr32 = nv50_instobj_wr32,
-	},
-};
-
 /******************************************************************************
  * instmem subdev implementation
  *****************************************************************************/
@@ -168,5 +254,7 @@ nv50_instmem_oclass = &(struct nvkm_instmem_impl) {
 		.init = _nvkm_instmem_init,
 		.fini = nv50_instmem_fini,
 	},
-	.instobj = &nv50_instobj_oclass.base,
+	.memory_new = nv50_instobj_new,
+	.persistent = false,
+	.zero = false,
 }.base;

commit d36a99d2da22bdffebf644e4a5f811e8eff82360
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:14 2015 +1000

    drm/nouveau/fb: transition nvkm_ram away from being based on nvkm_object
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 339dd19e0e45..4f6354df538a 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -84,8 +84,8 @@ static void
 nv50_instobj_dtor(struct nvkm_object *object)
 {
 	struct nv50_instobj *node = (void *)object;
-	struct nvkm_fb *fb = nvkm_fb(object);
-	fb->ram->put(fb, &node->mem);
+	struct nvkm_ram *ram = nvkm_fb(object)->ram;
+	ram->func->put(ram, &node->mem);
 	nvkm_instobj_destroy(&node->base);
 }
 
@@ -94,7 +94,7 @@ nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 		  struct nvkm_oclass *oclass, void *data, u32 size,
 		  struct nvkm_object **pobject)
 {
-	struct nvkm_fb *fb = nvkm_fb(parent);
+	struct nvkm_ram *ram = nvkm_fb(parent)->ram;
 	struct nvkm_instobj_args *args = data;
 	struct nv50_instobj *node;
 	int ret;
@@ -107,7 +107,8 @@ nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 	if (ret)
 		return ret;
 
-	ret = fb->ram->get(fb, args->size, args->align, 0, 0x800, &node->mem);
+	ret = ram->func->get(ram, args->size, args->align, 0, 0x800,
+			     &node->mem);
 	if (ret)
 		return ret;
 

commit d5c5bcf693e7c72f2f853066858f3d40a42ba942
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:09 2015 +1000

    drm/nouveau/imem: switch to device pri macros
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 6c83c5797e32..339dd19e0e45 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -45,6 +45,7 @@ nv50_instobj_rd32(struct nvkm_object *object, u64 offset)
 {
 	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
 	struct nv50_instobj *node = (void *)object;
+	struct nvkm_device *device = imem->base.subdev.device;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
@@ -52,10 +53,10 @@ nv50_instobj_rd32(struct nvkm_object *object, u64 offset)
 
 	spin_lock_irqsave(&imem->lock, flags);
 	if (unlikely(imem->addr != base)) {
-		nv_wr32(imem, 0x001700, base >> 16);
+		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
-	data = nv_rd32(imem, 0x700000 + addr);
+	data = nvkm_rd32(device, 0x700000 + addr);
 	spin_unlock_irqrestore(&imem->lock, flags);
 	return data;
 }
@@ -65,16 +66,17 @@ nv50_instobj_wr32(struct nvkm_object *object, u64 offset, u32 data)
 {
 	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
 	struct nv50_instobj *node = (void *)object;
+	struct nvkm_device *device = imem->base.subdev.device;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
 
 	spin_lock_irqsave(&imem->lock, flags);
 	if (unlikely(imem->addr != base)) {
-		nv_wr32(imem, 0x001700, base >> 16);
+		nvkm_wr32(device, 0x001700, base >> 16);
 		imem->addr = base;
 	}
-	nv_wr32(imem, 0x700000 + addr, data);
+	nvkm_wr32(device, 0x700000 + addr, data);
 	spin_unlock_irqrestore(&imem->lock, flags);
 }
 

commit c44c06aeebf481fb69c665a21090f2f0aac878c5
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:06 2015 +1000

    drm/nouveau/imem: cosmetic changes
    
    This is purely preparation for upcoming commits, there should be no
    code changes here.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index b6c5e2d12f20..6c83c5797e32 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -25,13 +25,13 @@
 
 #include <subdev/fb.h>
 
-struct nv50_instmem_priv {
+struct nv50_instmem {
 	struct nvkm_instmem base;
 	spinlock_t lock;
 	u64 addr;
 };
 
-struct nv50_instobj_priv {
+struct nv50_instobj {
 	struct nvkm_instobj base;
 	struct nvkm_mem *mem;
 };
@@ -43,45 +43,45 @@ struct nv50_instobj_priv {
 static u32
 nv50_instobj_rd32(struct nvkm_object *object, u64 offset)
 {
-	struct nv50_instmem_priv *priv = (void *)nvkm_instmem(object);
-	struct nv50_instobj_priv *node = (void *)object;
+	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
+	struct nv50_instobj *node = (void *)object;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
 	u32 data;
 
-	spin_lock_irqsave(&priv->lock, flags);
-	if (unlikely(priv->addr != base)) {
-		nv_wr32(priv, 0x001700, base >> 16);
-		priv->addr = base;
+	spin_lock_irqsave(&imem->lock, flags);
+	if (unlikely(imem->addr != base)) {
+		nv_wr32(imem, 0x001700, base >> 16);
+		imem->addr = base;
 	}
-	data = nv_rd32(priv, 0x700000 + addr);
-	spin_unlock_irqrestore(&priv->lock, flags);
+	data = nv_rd32(imem, 0x700000 + addr);
+	spin_unlock_irqrestore(&imem->lock, flags);
 	return data;
 }
 
 static void
 nv50_instobj_wr32(struct nvkm_object *object, u64 offset, u32 data)
 {
-	struct nv50_instmem_priv *priv = (void *)nvkm_instmem(object);
-	struct nv50_instobj_priv *node = (void *)object;
+	struct nv50_instmem *imem = (void *)nvkm_instmem(object);
+	struct nv50_instobj *node = (void *)object;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
 	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
 
-	spin_lock_irqsave(&priv->lock, flags);
-	if (unlikely(priv->addr != base)) {
-		nv_wr32(priv, 0x001700, base >> 16);
-		priv->addr = base;
+	spin_lock_irqsave(&imem->lock, flags);
+	if (unlikely(imem->addr != base)) {
+		nv_wr32(imem, 0x001700, base >> 16);
+		imem->addr = base;
 	}
-	nv_wr32(priv, 0x700000 + addr, data);
-	spin_unlock_irqrestore(&priv->lock, flags);
+	nv_wr32(imem, 0x700000 + addr, data);
+	spin_unlock_irqrestore(&imem->lock, flags);
 }
 
 static void
 nv50_instobj_dtor(struct nvkm_object *object)
 {
-	struct nv50_instobj_priv *node = (void *)object;
+	struct nv50_instobj *node = (void *)object;
 	struct nvkm_fb *fb = nvkm_fb(object);
 	fb->ram->put(fb, &node->mem);
 	nvkm_instobj_destroy(&node->base);
@@ -94,7 +94,7 @@ nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 {
 	struct nvkm_fb *fb = nvkm_fb(parent);
 	struct nvkm_instobj_args *args = data;
-	struct nv50_instobj_priv *node;
+	struct nv50_instobj *node;
 	int ret;
 
 	args->size  = max((args->size  + 4095) & ~4095, (u32)4096);
@@ -134,9 +134,9 @@ nv50_instobj_oclass = {
 static int
 nv50_instmem_fini(struct nvkm_object *object, bool suspend)
 {
-	struct nv50_instmem_priv *priv = (void *)object;
-	priv->addr = ~0ULL;
-	return nvkm_instmem_fini(&priv->base, suspend);
+	struct nv50_instmem *imem = (void *)object;
+	imem->addr = ~0ULL;
+	return nvkm_instmem_fini(&imem->base, suspend);
 }
 
 static int
@@ -144,15 +144,15 @@ nv50_instmem_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 		  struct nvkm_oclass *oclass, void *data, u32 size,
 		  struct nvkm_object **pobject)
 {
-	struct nv50_instmem_priv *priv;
+	struct nv50_instmem *imem;
 	int ret;
 
-	ret = nvkm_instmem_create(parent, engine, oclass, &priv);
-	*pobject = nv_object(priv);
+	ret = nvkm_instmem_create(parent, engine, oclass, &imem);
+	*pobject = nv_object(imem);
 	if (ret)
 		return ret;
 
-	spin_lock_init(&priv->lock);
+	spin_lock_init(&imem->lock);
 	return 0;
 }
 

commit b1e4553cb1f9deddbd8c13d95e9cef81967a3f41
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:06 2015 +1000

    drm/nouveau/fb: cosmetic changes
    
    This is purely preparation for upcoming commits, there should be no
    code changes here.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 8404143f93ee..b6c5e2d12f20 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -82,8 +82,8 @@ static void
 nv50_instobj_dtor(struct nvkm_object *object)
 {
 	struct nv50_instobj_priv *node = (void *)object;
-	struct nvkm_fb *pfb = nvkm_fb(object);
-	pfb->ram->put(pfb, &node->mem);
+	struct nvkm_fb *fb = nvkm_fb(object);
+	fb->ram->put(fb, &node->mem);
 	nvkm_instobj_destroy(&node->base);
 }
 
@@ -92,7 +92,7 @@ nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 		  struct nvkm_oclass *oclass, void *data, u32 size,
 		  struct nvkm_object **pobject)
 {
-	struct nvkm_fb *pfb = nvkm_fb(parent);
+	struct nvkm_fb *fb = nvkm_fb(parent);
 	struct nvkm_instobj_args *args = data;
 	struct nv50_instobj_priv *node;
 	int ret;
@@ -105,7 +105,7 @@ nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
 	if (ret)
 		return ret;
 
-	ret = pfb->ram->get(pfb, args->size, args->align, 0, 0x800, &node->mem);
+	ret = fb->ram->get(fb, args->size, args->align, 0, 0x800, &node->mem);
 	if (ret)
 		return ret;
 

commit 78b2b4e76be2100637a92b9721aaa2ce73d0fa22
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 14 15:05:26 2015 +1000

    drm/nouveau/instmem: namespace + nvidia gpu names (no binary change)
    
    The namespace of NVKM is being changed to nvkm_ instead of nouveau_,
    which will be used for the DRM part of the driver.  This is being
    done in order to make it very clear as to what part of the driver a
    given symbol belongs to, and as a minor step towards splitting the
    DRM driver out to be able to stand on its own (for virt).
    
    Because there's already a large amount of churn here anyway, this is
    as good a time as any to also switch to NVIDIA's device and chipset
    naming to ease collaboration with them.
    
    A comparison of objdump disassemblies proves no code changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
index 64ee680232e2..8404143f93ee 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -21,21 +21,19 @@
  *
  * Authors: Ben Skeggs
  */
+#include "priv.h"
 
 #include <subdev/fb.h>
-#include <core/mm.h>
-
-#include "priv.h"
 
 struct nv50_instmem_priv {
-	struct nouveau_instmem base;
+	struct nvkm_instmem base;
 	spinlock_t lock;
 	u64 addr;
 };
 
 struct nv50_instobj_priv {
-	struct nouveau_instobj base;
-	struct nouveau_mem *mem;
+	struct nvkm_instobj base;
+	struct nvkm_mem *mem;
 };
 
 /******************************************************************************
@@ -43,9 +41,9 @@ struct nv50_instobj_priv {
  *****************************************************************************/
 
 static u32
-nv50_instobj_rd32(struct nouveau_object *object, u64 offset)
+nv50_instobj_rd32(struct nvkm_object *object, u64 offset)
 {
-	struct nv50_instmem_priv *priv = (void *)nouveau_instmem(object);
+	struct nv50_instmem_priv *priv = (void *)nvkm_instmem(object);
 	struct nv50_instobj_priv *node = (void *)object;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
@@ -63,9 +61,9 @@ nv50_instobj_rd32(struct nouveau_object *object, u64 offset)
 }
 
 static void
-nv50_instobj_wr32(struct nouveau_object *object, u64 offset, u32 data)
+nv50_instobj_wr32(struct nvkm_object *object, u64 offset, u32 data)
 {
-	struct nv50_instmem_priv *priv = (void *)nouveau_instmem(object);
+	struct nv50_instmem_priv *priv = (void *)nvkm_instmem(object);
 	struct nv50_instobj_priv *node = (void *)object;
 	unsigned long flags;
 	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
@@ -81,28 +79,28 @@ nv50_instobj_wr32(struct nouveau_object *object, u64 offset, u32 data)
 }
 
 static void
-nv50_instobj_dtor(struct nouveau_object *object)
+nv50_instobj_dtor(struct nvkm_object *object)
 {
 	struct nv50_instobj_priv *node = (void *)object;
-	struct nouveau_fb *pfb = nouveau_fb(object);
+	struct nvkm_fb *pfb = nvkm_fb(object);
 	pfb->ram->put(pfb, &node->mem);
-	nouveau_instobj_destroy(&node->base);
+	nvkm_instobj_destroy(&node->base);
 }
 
 static int
-nv50_instobj_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
-		  struct nouveau_oclass *oclass, void *data, u32 size,
-		  struct nouveau_object **pobject)
+nv50_instobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
+		  struct nvkm_oclass *oclass, void *data, u32 size,
+		  struct nvkm_object **pobject)
 {
-	struct nouveau_fb *pfb = nouveau_fb(parent);
-	struct nouveau_instobj_args *args = data;
+	struct nvkm_fb *pfb = nvkm_fb(parent);
+	struct nvkm_instobj_args *args = data;
 	struct nv50_instobj_priv *node;
 	int ret;
 
 	args->size  = max((args->size  + 4095) & ~4095, (u32)4096);
 	args->align = max((args->align + 4095) & ~4095, (u32)4096);
 
-	ret = nouveau_instobj_create(parent, engine, oclass, &node);
+	ret = nvkm_instobj_create(parent, engine, oclass, &node);
 	*pobject = nv_object(node);
 	if (ret)
 		return ret;
@@ -117,13 +115,13 @@ nv50_instobj_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
 	return 0;
 }
 
-static struct nouveau_instobj_impl
+static struct nvkm_instobj_impl
 nv50_instobj_oclass = {
-	.base.ofuncs = &(struct nouveau_ofuncs) {
+	.base.ofuncs = &(struct nvkm_ofuncs) {
 		.ctor = nv50_instobj_ctor,
 		.dtor = nv50_instobj_dtor,
-		.init = _nouveau_instobj_init,
-		.fini = _nouveau_instobj_fini,
+		.init = _nvkm_instobj_init,
+		.fini = _nvkm_instobj_fini,
 		.rd32 = nv50_instobj_rd32,
 		.wr32 = nv50_instobj_wr32,
 	},
@@ -134,22 +132,22 @@ nv50_instobj_oclass = {
  *****************************************************************************/
 
 static int
-nv50_instmem_fini(struct nouveau_object *object, bool suspend)
+nv50_instmem_fini(struct nvkm_object *object, bool suspend)
 {
 	struct nv50_instmem_priv *priv = (void *)object;
 	priv->addr = ~0ULL;
-	return nouveau_instmem_fini(&priv->base, suspend);
+	return nvkm_instmem_fini(&priv->base, suspend);
 }
 
 static int
-nv50_instmem_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
-		  struct nouveau_oclass *oclass, void *data, u32 size,
-		  struct nouveau_object **pobject)
+nv50_instmem_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
+		  struct nvkm_oclass *oclass, void *data, u32 size,
+		  struct nvkm_object **pobject)
 {
 	struct nv50_instmem_priv *priv;
 	int ret;
 
-	ret = nouveau_instmem_create(parent, engine, oclass, &priv);
+	ret = nvkm_instmem_create(parent, engine, oclass, &priv);
 	*pobject = nv_object(priv);
 	if (ret)
 		return ret;
@@ -158,13 +156,13 @@ nv50_instmem_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
 	return 0;
 }
 
-struct nouveau_oclass *
-nv50_instmem_oclass = &(struct nouveau_instmem_impl) {
+struct nvkm_oclass *
+nv50_instmem_oclass = &(struct nvkm_instmem_impl) {
 	.base.handle = NV_SUBDEV(INSTMEM, 0x50),
-	.base.ofuncs = &(struct nouveau_ofuncs) {
+	.base.ofuncs = &(struct nvkm_ofuncs) {
 		.ctor = nv50_instmem_ctor,
-		.dtor = _nouveau_instmem_dtor,
-		.init = _nouveau_instmem_init,
+		.dtor = _nvkm_instmem_dtor,
+		.init = _nvkm_instmem_init,
 		.fini = nv50_instmem_fini,
 	},
 	.instobj = &nv50_instobj_oclass.base,

commit c39f472e9f14e49a9bc091977ced0ec45fc00c57
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Tue Jan 13 22:13:14 2015 +1000

    drm/nouveau: remove symlinks, move core/ to nvkm/ (no code changes)
    
    The symlinks were annoying some people, and they're not used anywhere
    else in the kernel tree.  The include directory structure has been
    changed so that symlinks aren't needed anymore.
    
    NVKM has been moved from core/ to nvkm/ to make it more obvious as to
    what the directory is for, and as some minor prep for when NVKM gets
    split out into its own module (virt) at a later date.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
new file mode 100644
index 000000000000..64ee680232e2
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c
@@ -0,0 +1,171 @@
+/*
+ * Copyright 2012 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Ben Skeggs
+ */
+
+#include <subdev/fb.h>
+#include <core/mm.h>
+
+#include "priv.h"
+
+struct nv50_instmem_priv {
+	struct nouveau_instmem base;
+	spinlock_t lock;
+	u64 addr;
+};
+
+struct nv50_instobj_priv {
+	struct nouveau_instobj base;
+	struct nouveau_mem *mem;
+};
+
+/******************************************************************************
+ * instmem object implementation
+ *****************************************************************************/
+
+static u32
+nv50_instobj_rd32(struct nouveau_object *object, u64 offset)
+{
+	struct nv50_instmem_priv *priv = (void *)nouveau_instmem(object);
+	struct nv50_instobj_priv *node = (void *)object;
+	unsigned long flags;
+	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
+	u32 data;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	if (unlikely(priv->addr != base)) {
+		nv_wr32(priv, 0x001700, base >> 16);
+		priv->addr = base;
+	}
+	data = nv_rd32(priv, 0x700000 + addr);
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return data;
+}
+
+static void
+nv50_instobj_wr32(struct nouveau_object *object, u64 offset, u32 data)
+{
+	struct nv50_instmem_priv *priv = (void *)nouveau_instmem(object);
+	struct nv50_instobj_priv *node = (void *)object;
+	unsigned long flags;
+	u64 base = (node->mem->offset + offset) & 0xffffff00000ULL;
+	u64 addr = (node->mem->offset + offset) & 0x000000fffffULL;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	if (unlikely(priv->addr != base)) {
+		nv_wr32(priv, 0x001700, base >> 16);
+		priv->addr = base;
+	}
+	nv_wr32(priv, 0x700000 + addr, data);
+	spin_unlock_irqrestore(&priv->lock, flags);
+}
+
+static void
+nv50_instobj_dtor(struct nouveau_object *object)
+{
+	struct nv50_instobj_priv *node = (void *)object;
+	struct nouveau_fb *pfb = nouveau_fb(object);
+	pfb->ram->put(pfb, &node->mem);
+	nouveau_instobj_destroy(&node->base);
+}
+
+static int
+nv50_instobj_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
+		  struct nouveau_oclass *oclass, void *data, u32 size,
+		  struct nouveau_object **pobject)
+{
+	struct nouveau_fb *pfb = nouveau_fb(parent);
+	struct nouveau_instobj_args *args = data;
+	struct nv50_instobj_priv *node;
+	int ret;
+
+	args->size  = max((args->size  + 4095) & ~4095, (u32)4096);
+	args->align = max((args->align + 4095) & ~4095, (u32)4096);
+
+	ret = nouveau_instobj_create(parent, engine, oclass, &node);
+	*pobject = nv_object(node);
+	if (ret)
+		return ret;
+
+	ret = pfb->ram->get(pfb, args->size, args->align, 0, 0x800, &node->mem);
+	if (ret)
+		return ret;
+
+	node->base.addr = node->mem->offset;
+	node->base.size = node->mem->size << 12;
+	node->mem->page_shift = 12;
+	return 0;
+}
+
+static struct nouveau_instobj_impl
+nv50_instobj_oclass = {
+	.base.ofuncs = &(struct nouveau_ofuncs) {
+		.ctor = nv50_instobj_ctor,
+		.dtor = nv50_instobj_dtor,
+		.init = _nouveau_instobj_init,
+		.fini = _nouveau_instobj_fini,
+		.rd32 = nv50_instobj_rd32,
+		.wr32 = nv50_instobj_wr32,
+	},
+};
+
+/******************************************************************************
+ * instmem subdev implementation
+ *****************************************************************************/
+
+static int
+nv50_instmem_fini(struct nouveau_object *object, bool suspend)
+{
+	struct nv50_instmem_priv *priv = (void *)object;
+	priv->addr = ~0ULL;
+	return nouveau_instmem_fini(&priv->base, suspend);
+}
+
+static int
+nv50_instmem_ctor(struct nouveau_object *parent, struct nouveau_object *engine,
+		  struct nouveau_oclass *oclass, void *data, u32 size,
+		  struct nouveau_object **pobject)
+{
+	struct nv50_instmem_priv *priv;
+	int ret;
+
+	ret = nouveau_instmem_create(parent, engine, oclass, &priv);
+	*pobject = nv_object(priv);
+	if (ret)
+		return ret;
+
+	spin_lock_init(&priv->lock);
+	return 0;
+}
+
+struct nouveau_oclass *
+nv50_instmem_oclass = &(struct nouveau_instmem_impl) {
+	.base.handle = NV_SUBDEV(INSTMEM, 0x50),
+	.base.ofuncs = &(struct nouveau_ofuncs) {
+		.ctor = nv50_instmem_ctor,
+		.dtor = _nouveau_instmem_dtor,
+		.init = _nouveau_instmem_init,
+		.fini = nv50_instmem_fini,
+	},
+	.instobj = &nv50_instobj_oclass.base,
+}.base;
