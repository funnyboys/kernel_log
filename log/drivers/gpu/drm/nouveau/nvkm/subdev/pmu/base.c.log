commit 7a4dde711b38dd10df71bd71151cb1f59dfbfdac
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/secboot: move code to boot LS falcons to subdevs
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 706bbb782844..a0fe607c9c07 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -24,7 +24,6 @@
 #include "priv.h"
 
 #include <core/firmware.h>
-#include <core/msgqueue.h>
 #include <subdev/timer.h>
 
 bool
@@ -144,7 +143,6 @@ static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
-	nvkm_msgqueue_del(&pmu->queue);
 	nvkm_falcon_msgq_del(&pmu->msgq);
 	nvkm_falcon_cmdq_del(&pmu->lpq);
 	nvkm_falcon_cmdq_del(&pmu->hpq);

commit d114a1393fa01c4034d895072905578319a903f9
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/flcn/msgq: move handling of init message to subdevs
    
    When the PMU/SEC2 LS FWs have booted, they'll send a message to the host
    with various information, including the configuration of message/command
    queues that are available.
    
    Move the handling for this to the relevant subdevs.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index bc0eb84c2c90..706bbb782844 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -91,6 +91,7 @@ nvkm_pmu_fini(struct nvkm_subdev *subdev, bool suspend)
 
 	nvkm_falcon_cmdq_fini(pmu->lpq);
 	nvkm_falcon_cmdq_fini(pmu->hpq);
+	pmu->initmsg_received = false;
 	return 0;
 }
 

commit 2e8a65973b9afeebbff5e8a8e51e7cdd14f745a7
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/flcn/cmdq: split the condition for queue readiness vs pmu acr readiness
    
    This is to allow for proper separation of the LS interface code from the
    queue handling code.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 86af2ddb3e78..bc0eb84c2c90 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -87,6 +87,8 @@ nvkm_pmu_fini(struct nvkm_subdev *subdev, bool suspend)
 
 	flush_work(&pmu->recv.work);
 
+	reinit_completion(&pmu->wpr_ready);
+
 	nvkm_falcon_cmdq_fini(pmu->lpq);
 	nvkm_falcon_cmdq_fini(pmu->hpq);
 	return 0;
@@ -188,6 +190,7 @@ nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 	    (ret = nvkm_falcon_msgq_new(pmu->qmgr, "msgq", &pmu->msgq)))
 		return ret;
 
+	init_completion(&pmu->wpr_ready);
 	return 0;
 }
 

commit 22431189d6690071db01079606feb1daa2784afe
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/flcn/msgq: explicitly create message queue from subdevs
    
    Code to interface with LS firmwares is being moved to the subdevs where it
    belongs, rather than living in the common falcon code.
    
    This is an incremental step towards that goal.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 9dc0a002f530..86af2ddb3e78 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -142,6 +142,7 @@ nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	nvkm_msgqueue_del(&pmu->queue);
+	nvkm_falcon_msgq_del(&pmu->msgq);
 	nvkm_falcon_cmdq_del(&pmu->lpq);
 	nvkm_falcon_cmdq_del(&pmu->hpq);
 	nvkm_falcon_qmgr_del(&pmu->qmgr);
@@ -183,7 +184,8 @@ nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 
 	if ((ret = nvkm_falcon_qmgr_new(&pmu->falcon, &pmu->qmgr)) ||
 	    (ret = nvkm_falcon_cmdq_new(pmu->qmgr, "hpq", &pmu->hpq)) ||
-	    (ret = nvkm_falcon_cmdq_new(pmu->qmgr, "lpq", &pmu->lpq)))
+	    (ret = nvkm_falcon_cmdq_new(pmu->qmgr, "lpq", &pmu->lpq)) ||
+	    (ret = nvkm_falcon_msgq_new(pmu->qmgr, "msgq", &pmu->msgq)))
 		return ret;
 
 	return 0;

commit acc466ab46574f0d9de65606f0796cff07c9a7d5
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/flcn/cmdq: explicitly create command queue(s) from subdevs
    
    Code to interface with LS firmwares is being moved to the subdevs where it
    belongs, rather than living in the common falcon code.
    
    This is an incremental step towards that goal.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 850939f7e287..9dc0a002f530 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -86,6 +86,9 @@ nvkm_pmu_fini(struct nvkm_subdev *subdev, bool suspend)
 		pmu->func->fini(pmu);
 
 	flush_work(&pmu->recv.work);
+
+	nvkm_falcon_cmdq_fini(pmu->lpq);
+	nvkm_falcon_cmdq_fini(pmu->hpq);
 	return 0;
 }
 
@@ -139,6 +142,8 @@ nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	nvkm_msgqueue_del(&pmu->queue);
+	nvkm_falcon_cmdq_del(&pmu->lpq);
+	nvkm_falcon_cmdq_del(&pmu->hpq);
 	nvkm_falcon_qmgr_del(&pmu->qmgr);
 	nvkm_falcon_dtor(&pmu->falcon);
 	return nvkm_pmu(subdev);
@@ -176,7 +181,9 @@ nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 	if (ret)
 		return ret;
 
-	if ((ret = nvkm_falcon_qmgr_new(&pmu->falcon, &pmu->qmgr)))
+	if ((ret = nvkm_falcon_qmgr_new(&pmu->falcon, &pmu->qmgr)) ||
+	    (ret = nvkm_falcon_cmdq_new(pmu->qmgr, "hpq", &pmu->hpq)) ||
+	    (ret = nvkm_falcon_cmdq_new(pmu->qmgr, "lpq", &pmu->lpq)))
 		return ret;
 
 	return 0;

commit 8763955ba73807c6f5d38364074c1802d8415172
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:22 2020 +1000

    drm/nouveau/flcn/qmgr: explicitly create queue manager from subdevs
    
    Code to interface with LS firmwares is being moved to the subdevs where it
    belongs, rather than living in the common falcon code.
    
    This is an incremental step towards that goal.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 60e14f9a5415..850939f7e287 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -139,6 +139,7 @@ nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	nvkm_msgqueue_del(&pmu->queue);
+	nvkm_falcon_qmgr_del(&pmu->qmgr);
 	nvkm_falcon_dtor(&pmu->falcon);
 	return nvkm_pmu(subdev);
 }
@@ -156,6 +157,8 @@ int
 nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 	      int index, struct nvkm_pmu *pmu)
 {
+	int ret;
+
 	nvkm_subdev_ctor(&nvkm_pmu, device, index, &pmu->subdev);
 
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
@@ -167,9 +170,16 @@ nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 
 	pmu->func = fwif->func;
 
-	return nvkm_falcon_ctor(pmu->func->flcn, &pmu->subdev,
-				nvkm_subdev_name[pmu->subdev.index], 0x10a000,
-				&pmu->falcon);
+	ret = nvkm_falcon_ctor(pmu->func->flcn, &pmu->subdev,
+			       nvkm_subdev_name[pmu->subdev.index], 0x10a000,
+			       &pmu->falcon);
+	if (ret)
+		return ret;
+
+	if ((ret = nvkm_falcon_qmgr_new(&pmu->falcon, &pmu->qmgr)))
+		return ret;
+
+	return 0;
 }
 
 int

commit 2952a2b42e17ea0f72ee52de061975dddb9c62ec
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:21 2020 +1000

    drm/nouveau/pmu: initialise SW state for falcon from constructor
    
    This will allow us to register the falcon with ACR, and further customise
    its behaviour by providing the nvkm_falcon_func structure directly.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 1c308227ad6b..60e14f9a5415 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -134,19 +134,12 @@ nvkm_pmu_init(struct nvkm_subdev *subdev)
 	return ret;
 }
 
-static int
-nvkm_pmu_oneinit(struct nvkm_subdev *subdev)
-{
-	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
-	return nvkm_falcon_v1_new(&pmu->subdev, "PMU", 0x10a000, &pmu->falcon);
-}
-
 static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	nvkm_msgqueue_del(&pmu->queue);
-	nvkm_falcon_del(&pmu->falcon);
+	nvkm_falcon_dtor(&pmu->falcon);
 	return nvkm_pmu(subdev);
 }
 
@@ -154,7 +147,6 @@ static const struct nvkm_subdev_func
 nvkm_pmu = {
 	.dtor = nvkm_pmu_dtor,
 	.preinit = nvkm_pmu_preinit,
-	.oneinit = nvkm_pmu_oneinit,
 	.init = nvkm_pmu_init,
 	.fini = nvkm_pmu_fini,
 	.intr = nvkm_pmu_intr,
@@ -174,7 +166,10 @@ nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 		return PTR_ERR(fwif);
 
 	pmu->func = fwif->func;
-	return 0;
+
+	return nvkm_falcon_ctor(pmu->func->flcn, &pmu->subdev,
+				nvkm_subdev_name[pmu->subdev.index], 0x10a000,
+				&pmu->falcon);
 }
 
 int

commit 989863d7cbe58180cf0e69fd5ed72279c7fac901
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 15 06:34:21 2020 +1000

    drm/nouveau/pmu: select implementation based on available firmware
    
    This will allow for further customisation of the subdev depending on what
    firmware is available.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index ea2e11771bca..1c308227ad6b 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -23,6 +23,7 @@
  */
 #include "priv.h"
 
+#include <core/firmware.h>
 #include <core/msgqueue.h>
 #include <subdev/timer.h>
 
@@ -160,22 +161,28 @@ nvkm_pmu = {
 };
 
 int
-nvkm_pmu_ctor(const struct nvkm_pmu_func *func, struct nvkm_device *device,
+nvkm_pmu_ctor(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 	      int index, struct nvkm_pmu *pmu)
 {
 	nvkm_subdev_ctor(&nvkm_pmu, device, index, &pmu->subdev);
-	pmu->func = func;
+
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);
+
+	fwif = nvkm_firmware_load(&pmu->subdev, fwif, "Pmu", pmu);
+	if (IS_ERR(fwif))
+		return PTR_ERR(fwif);
+
+	pmu->func = fwif->func;
 	return 0;
 }
 
 int
-nvkm_pmu_new_(const struct nvkm_pmu_func *func, struct nvkm_device *device,
+nvkm_pmu_new_(const struct nvkm_pmu_fwif *fwif, struct nvkm_device *device,
 	      int index, struct nvkm_pmu **ppmu)
 {
 	struct nvkm_pmu *pmu;
 	if (!(pmu = *ppmu = kzalloc(sizeof(*pmu), GFP_KERNEL)))
 		return -ENOMEM;
-	return nvkm_pmu_ctor(func, device, index, *ppmu);
+	return nvkm_pmu_ctor(fwif, device, index, *ppmu);
 }

commit 69cbbb7b04ff57c17018b27a86e9c2d758d4366e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Tue Jul 2 14:19:12 2019 +1000

    drm/nouveau/therm: don't attempt fan control where PMU is already managing it
    
    There's already a condition in place which attempts to detect this, but
    since we've begun to require a PMU subdev even on boards where we don't
    load a custom FW, it's become inaccurate.
    
    This will prevent unnecessarily running a periodic fan update thread on
    GP100 and newer, where we don't yet override the default PMU FW.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index ce70a193caa7..ea2e11771bca 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -26,6 +26,24 @@
 #include <core/msgqueue.h>
 #include <subdev/timer.h>
 
+bool
+nvkm_pmu_fan_controlled(struct nvkm_device *device)
+{
+	struct nvkm_pmu *pmu = device->pmu;
+
+	/* Internal PMU FW does not currently control fans in any way,
+	 * allow SW control of fans instead.
+	 */
+	if (pmu && pmu->func->code.size)
+		return false;
+
+	/* Default (board-loaded, or VBIOS PMU/PREOS) PMU FW on Fermi
+	 * and newer automatically control the fan speed, which would
+	 * interfere with SW control.
+	 */
+	return (device->chipset >= 0xc0);
+}
+
 void
 nvkm_pmu_pgob(struct nvkm_pmu *pmu, bool enable)
 {

commit 6b1277c837630749b55f743dd62023d9dd7a510a
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Fri Jul 21 10:45:32 2017 +1000

    drm/nouveau/pmu/gt215-: abstract detection of whether reset is needed
    
    GT215, GF100-GP100, and GP10x are all different.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 3306f9fe7140..ce70a193caa7 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -75,7 +75,7 @@ nvkm_pmu_reset(struct nvkm_pmu *pmu)
 {
 	struct nvkm_device *device = pmu->subdev.device;
 
-	if (!(nvkm_rd32(device, 0x000200) & 0x00002000))
+	if (!pmu->func->enabled(pmu))
 		return 0;
 
 	/* Inhibit interrupts, and wait for idle. */

commit 9e4397579fa4cd5cc411d47815eb805e337d0203
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Wed Feb 22 20:48:30 2017 +0900

    drm/nouveau/falcon: delay construction of falcons to oneinit()
    
    Reading registers at device construction time can be harmful, as there
    is no guarantee the underlying engine will be up, or in its runtime
    configuration. Defer register reading to the oneinit() hook and update
    users accordingly.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 1871a92e8f2f..3306f9fe7140 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -115,6 +115,13 @@ nvkm_pmu_init(struct nvkm_subdev *subdev)
 	return ret;
 }
 
+static int
+nvkm_pmu_oneinit(struct nvkm_subdev *subdev)
+{
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
+	return nvkm_falcon_v1_new(&pmu->subdev, "PMU", 0x10a000, &pmu->falcon);
+}
+
 static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
@@ -128,6 +135,7 @@ static const struct nvkm_subdev_func
 nvkm_pmu = {
 	.dtor = nvkm_pmu_dtor,
 	.preinit = nvkm_pmu_preinit,
+	.oneinit = nvkm_pmu_oneinit,
 	.init = nvkm_pmu_init,
 	.fini = nvkm_pmu_fini,
 	.intr = nvkm_pmu_intr,
@@ -141,7 +149,7 @@ nvkm_pmu_ctor(const struct nvkm_pmu_func *func, struct nvkm_device *device,
 	pmu->func = func;
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);
-	return nvkm_falcon_v1_new(&pmu->subdev, "PMU", 0x10a000, &pmu->falcon);
+	return 0;
 }
 
 int

commit 9ce480fead6cf57640a2bde8024578e61acb77a3
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Thu Jan 19 13:16:40 2017 +0900

    drm/nouveau/pmu: add msgqueue member
    
    NVIDIA-provided PMU firmware is controlled by a msgqueue. Add a member
    to the PMU structure as well as the required cleanup code if this
    feature is used.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index ad4ac4e8b29e..1871a92e8f2f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -23,6 +23,7 @@
  */
 #include "priv.h"
 
+#include <core/msgqueue.h>
 #include <subdev/timer.h>
 
 void
@@ -118,6 +119,7 @@ static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
+	nvkm_msgqueue_del(&pmu->queue);
 	nvkm_falcon_del(&pmu->falcon);
 	return nvkm_pmu(subdev);
 }

commit 485a20eff20a2cbc55c9061724356cae3df01781
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Thu Nov 17 16:37:29 2016 +0900

    drm/nouveau/pmu: make sure the reset hook exists before running it
    
    Some PMU implementations (in particular the ones managed by secure
    boot) may not have a reset() hook. Make sure we don't crash in that
    case.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index a73f690eb4b5..ad4ac4e8b29e 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -85,7 +85,8 @@ nvkm_pmu_reset(struct nvkm_pmu *pmu)
 	);
 
 	/* Reset. */
-	pmu->func->reset(pmu);
+	if (pmu->func->reset)
+		pmu->func->reset(pmu);
 
 	/* Wait for IMEM/DMEM scrubbing to be complete. */
 	nvkm_msec(device, 2000,

commit 1e2115d8c0c0da62405400316f5499d909e479bc
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Tue Dec 13 17:11:20 2016 +0900

    drm/nouveau/pmu: instanciate the falcon in PMU device
    
    Have an instance of nvkm_falcon in the PMU structure, ready to be used
    by other subdevs (i.e. secboot).
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index b7edde42789c..a73f690eb4b5 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -116,6 +116,8 @@ nvkm_pmu_init(struct nvkm_subdev *subdev)
 static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
+	nvkm_falcon_del(&pmu->falcon);
 	return nvkm_pmu(subdev);
 }
 
@@ -136,7 +138,7 @@ nvkm_pmu_ctor(const struct nvkm_pmu_func *func, struct nvkm_device *device,
 	pmu->func = func;
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);
-	return 0;
+	return nvkm_falcon_v1_new(&pmu->subdev, "PMU", 0x10a000, &pmu->falcon);
 }
 
 int

commit e72da6e04f739253b175cd87aab47337c4646a66
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Tue Dec 13 17:11:21 2016 +0900

    drm/nouveau/pmu: add nvkm_pmu_ctor() function
    
    Add a PMU constructor so implementations that extend the nvkm_pmu
    structure can have all base members properly initialized.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index e611ce80f8ef..b7edde42789c 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -129,15 +129,22 @@ nvkm_pmu = {
 };
 
 int
-nvkm_pmu_new_(const struct nvkm_pmu_func *func, struct nvkm_device *device,
-	      int index, struct nvkm_pmu **ppmu)
+nvkm_pmu_ctor(const struct nvkm_pmu_func *func, struct nvkm_device *device,
+	      int index, struct nvkm_pmu *pmu)
 {
-	struct nvkm_pmu *pmu;
-	if (!(pmu = *ppmu = kzalloc(sizeof(*pmu), GFP_KERNEL)))
-		return -ENOMEM;
 	nvkm_subdev_ctor(&nvkm_pmu, device, index, &pmu->subdev);
 	pmu->func = func;
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);
 	return 0;
 }
+
+int
+nvkm_pmu_new_(const struct nvkm_pmu_func *func, struct nvkm_device *device,
+	      int index, struct nvkm_pmu **ppmu)
+{
+	struct nvkm_pmu *pmu;
+	if (!(pmu = *ppmu = kzalloc(sizeof(*pmu), GFP_KERNEL)))
+		return -ENOMEM;
+	return nvkm_pmu_ctor(func, device, index, *ppmu);
+}

commit 2f524aa0b72965b28eb7f648d6faaeb2719c7582
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 9 10:39:08 2016 +1000

    drm/nouveau/pmu: execute reset before running devinit
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index a843cef2475d..e611ce80f8ef 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -96,6 +96,13 @@ nvkm_pmu_reset(struct nvkm_pmu *pmu)
 	return 0;
 }
 
+static int
+nvkm_pmu_preinit(struct nvkm_subdev *subdev)
+{
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
+	return nvkm_pmu_reset(pmu);
+}
+
 static int
 nvkm_pmu_init(struct nvkm_subdev *subdev)
 {
@@ -115,6 +122,7 @@ nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 static const struct nvkm_subdev_func
 nvkm_pmu = {
 	.dtor = nvkm_pmu_dtor,
+	.preinit = nvkm_pmu_preinit,
 	.init = nvkm_pmu_init,
 	.fini = nvkm_pmu_fini,
 	.intr = nvkm_pmu_intr,

commit da7d2062fc541e307a290427b0dc7276ed2beb0e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 9 10:23:55 2016 +1000

    drm/nouveau/pmu: move ucode handling into gt215 implementation
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 8dd164d13043..a843cef2475d 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -32,227 +32,80 @@ nvkm_pmu_pgob(struct nvkm_pmu *pmu, bool enable)
 		pmu->func->pgob(pmu, enable);
 }
 
-int
-nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
-	      u32 process, u32 message, u32 data0, u32 data1)
-{
-	struct nvkm_subdev *subdev = &pmu->subdev;
-	struct nvkm_device *device = subdev->device;
-	u32 addr;
-
-	mutex_lock(&subdev->mutex);
-	/* wait for a free slot in the fifo */
-	addr  = nvkm_rd32(device, 0x10a4a0);
-	if (nvkm_msec(device, 2000,
-		u32 tmp = nvkm_rd32(device, 0x10a4b0);
-		if (tmp != (addr ^ 8))
-			break;
-	) < 0) {
-		mutex_unlock(&subdev->mutex);
-		return -EBUSY;
-	}
-
-	/* we currently only support a single process at a time waiting
-	 * on a synchronous reply, take the PMU mutex and tell the
-	 * receive handler what we're waiting for
-	 */
-	if (reply) {
-		pmu->recv.message = message;
-		pmu->recv.process = process;
-	}
-
-	/* acquire data segment access */
-	do {
-		nvkm_wr32(device, 0x10a580, 0x00000001);
-	} while (nvkm_rd32(device, 0x10a580) != 0x00000001);
-
-	/* write the packet */
-	nvkm_wr32(device, 0x10a1c0, 0x01000000 | (((addr & 0x07) << 4) +
-				pmu->send.base));
-	nvkm_wr32(device, 0x10a1c4, process);
-	nvkm_wr32(device, 0x10a1c4, message);
-	nvkm_wr32(device, 0x10a1c4, data0);
-	nvkm_wr32(device, 0x10a1c4, data1);
-	nvkm_wr32(device, 0x10a4a0, (addr + 1) & 0x0f);
-
-	/* release data segment access */
-	nvkm_wr32(device, 0x10a580, 0x00000000);
-
-	/* wait for reply, if requested */
-	if (reply) {
-		wait_event(pmu->recv.wait, (pmu->recv.process == 0));
-		reply[0] = pmu->recv.data[0];
-		reply[1] = pmu->recv.data[1];
-	}
-
-	mutex_unlock(&subdev->mutex);
-	return 0;
-}
-
 static void
 nvkm_pmu_recv(struct work_struct *work)
 {
-	struct nvkm_pmu *pmu = container_of(work, struct nvkm_pmu, recv.work);
-	struct nvkm_subdev *subdev = &pmu->subdev;
-	struct nvkm_device *device = subdev->device;
-	u32 process, message, data0, data1;
-
-	/* nothing to do if GET == PUT */
-	u32 addr =  nvkm_rd32(device, 0x10a4cc);
-	if (addr == nvkm_rd32(device, 0x10a4c8))
-		return;
-
-	/* acquire data segment access */
-	do {
-		nvkm_wr32(device, 0x10a580, 0x00000002);
-	} while (nvkm_rd32(device, 0x10a580) != 0x00000002);
-
-	/* read the packet */
-	nvkm_wr32(device, 0x10a1c0, 0x02000000 | (((addr & 0x07) << 4) +
-				pmu->recv.base));
-	process = nvkm_rd32(device, 0x10a1c4);
-	message = nvkm_rd32(device, 0x10a1c4);
-	data0   = nvkm_rd32(device, 0x10a1c4);
-	data1   = nvkm_rd32(device, 0x10a1c4);
-	nvkm_wr32(device, 0x10a4cc, (addr + 1) & 0x0f);
-
-	/* release data segment access */
-	nvkm_wr32(device, 0x10a580, 0x00000000);
-
-	/* wake process if it's waiting on a synchronous reply */
-	if (pmu->recv.process) {
-		if (process == pmu->recv.process &&
-		    message == pmu->recv.message) {
-			pmu->recv.data[0] = data0;
-			pmu->recv.data[1] = data1;
-			pmu->recv.process = 0;
-			wake_up(&pmu->recv.wait);
-			return;
-		}
-	}
+	struct nvkm_pmu *pmu = container_of(work, typeof(*pmu), recv.work);
+	return pmu->func->recv(pmu);
+}
 
-	/* right now there's no other expected responses from the engine,
-	 * so assume that any unexpected message is an error.
-	 */
-	nvkm_warn(subdev, "%c%c%c%c %08x %08x %08x %08x\n",
-		  (char)((process & 0x000000ff) >>  0),
-		  (char)((process & 0x0000ff00) >>  8),
-		  (char)((process & 0x00ff0000) >> 16),
-		  (char)((process & 0xff000000) >> 24),
-		  process, message, data0, data1);
+int
+nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
+	      u32 process, u32 message, u32 data0, u32 data1)
+{
+	if (!pmu || !pmu->func->send)
+		return -ENODEV;
+	return pmu->func->send(pmu, reply, process, message, data0, data1);
 }
 
 static void
 nvkm_pmu_intr(struct nvkm_subdev *subdev)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
-	struct nvkm_device *device = pmu->subdev.device;
-	u32 disp = nvkm_rd32(device, 0x10a01c);
-	u32 intr = nvkm_rd32(device, 0x10a008) & disp & ~(disp >> 16);
-
-	if (intr & 0x00000020) {
-		u32 stat = nvkm_rd32(device, 0x10a16c);
-		if (stat & 0x80000000) {
-			nvkm_error(subdev, "UAS fault at %06x addr %08x\n",
-				   stat & 0x00ffffff,
-				   nvkm_rd32(device, 0x10a168));
-			nvkm_wr32(device, 0x10a16c, 0x00000000);
-			intr &= ~0x00000020;
-		}
-	}
-
-	if (intr & 0x00000040) {
-		schedule_work(&pmu->recv.work);
-		nvkm_wr32(device, 0x10a004, 0x00000040);
-		intr &= ~0x00000040;
-	}
-
-	if (intr & 0x00000080) {
-		nvkm_info(subdev, "wr32 %06x %08x\n",
-			  nvkm_rd32(device, 0x10a7a0),
-			  nvkm_rd32(device, 0x10a7a4));
-		nvkm_wr32(device, 0x10a004, 0x00000080);
-		intr &= ~0x00000080;
-	}
-
-	if (intr) {
-		nvkm_error(subdev, "intr %08x\n", intr);
-		nvkm_wr32(device, 0x10a004, intr);
-	}
+	if (!pmu->func->intr)
+		return;
+	pmu->func->intr(pmu);
 }
 
 static int
 nvkm_pmu_fini(struct nvkm_subdev *subdev, bool suspend)
 {
 	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
-	struct nvkm_device *device = pmu->subdev.device;
 
-	nvkm_wr32(device, 0x10a014, 0x00000060);
+	if (pmu->func->fini)
+		pmu->func->fini(pmu);
+
 	flush_work(&pmu->recv.work);
 	return 0;
 }
 
 static int
-nvkm_pmu_init(struct nvkm_subdev *subdev)
+nvkm_pmu_reset(struct nvkm_pmu *pmu)
 {
-	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	struct nvkm_device *device = pmu->subdev.device;
-	int i;
 
-	/* prevent previous ucode from running, wait for idle, reset */
-	nvkm_wr32(device, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
+	if (!(nvkm_rd32(device, 0x000200) & 0x00002000))
+		return 0;
+
+	/* Inhibit interrupts, and wait for idle. */
+	nvkm_wr32(device, 0x10a014, 0x0000ffff);
 	nvkm_msec(device, 2000,
 		if (!nvkm_rd32(device, 0x10a04c))
 			break;
 	);
-	nvkm_mask(device, 0x000200, 0x00002000, 0x00000000);
-	nvkm_mask(device, 0x000200, 0x00002000, 0x00002000);
-	nvkm_rd32(device, 0x000200);
+
+	/* Reset. */
+	pmu->func->reset(pmu);
+
+	/* Wait for IMEM/DMEM scrubbing to be complete. */
 	nvkm_msec(device, 2000,
 		if (!(nvkm_rd32(device, 0x10a10c) & 0x00000006))
 			break;
 	);
 
-	/* upload data segment */
-	nvkm_wr32(device, 0x10a1c0, 0x01000000);
-	for (i = 0; i < pmu->func->data.size / 4; i++)
-		nvkm_wr32(device, 0x10a1c4, pmu->func->data.data[i]);
-
-	/* upload code segment */
-	nvkm_wr32(device, 0x10a180, 0x01000000);
-	for (i = 0; i < pmu->func->code.size / 4; i++) {
-		if ((i & 0x3f) == 0)
-			nvkm_wr32(device, 0x10a188, i >> 6);
-		nvkm_wr32(device, 0x10a184, pmu->func->code.data[i]);
-	}
-
-	/* start it running */
-	nvkm_wr32(device, 0x10a10c, 0x00000000);
-	nvkm_wr32(device, 0x10a104, 0x00000000);
-	nvkm_wr32(device, 0x10a100, 0x00000002);
-
-	/* wait for valid host->pmu ring configuration */
-	if (nvkm_msec(device, 2000,
-		if (nvkm_rd32(device, 0x10a4d0))
-			break;
-	) < 0)
-		return -EBUSY;
-	pmu->send.base = nvkm_rd32(device, 0x10a4d0) & 0x0000ffff;
-	pmu->send.size = nvkm_rd32(device, 0x10a4d0) >> 16;
-
-	/* wait for valid pmu->host ring configuration */
-	if (nvkm_msec(device, 2000,
-		if (nvkm_rd32(device, 0x10a4dc))
-			break;
-	) < 0)
-		return -EBUSY;
-	pmu->recv.base = nvkm_rd32(device, 0x10a4dc) & 0x0000ffff;
-	pmu->recv.size = nvkm_rd32(device, 0x10a4dc) >> 16;
-
-	nvkm_wr32(device, 0x10a010, 0x000000e0);
 	return 0;
 }
 
+static int
+nvkm_pmu_init(struct nvkm_subdev *subdev)
+{
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
+	int ret = nvkm_pmu_reset(pmu);
+	if (ret == 0 && pmu->func->init)
+		ret = pmu->func->init(pmu);
+	return ret;
+}
+
 static void *
 nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {

commit 56d06fa29edd58c448766014afd833b7ff51247b
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Fri Apr 8 17:24:40 2016 +1000

    drm/nouveau/core: remove pmc_enable argument from subdev ctor
    
    These are now specified directly in the MC subdev.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 6e6d2ef598c8..8dd164d13043 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -274,7 +274,7 @@ nvkm_pmu_new_(const struct nvkm_pmu_func *func, struct nvkm_device *device,
 	struct nvkm_pmu *pmu;
 	if (!(pmu = *ppmu = kzalloc(sizeof(*pmu), GFP_KERNEL)))
 		return -ENOMEM;
-	nvkm_subdev_ctor(&nvkm_pmu, device, index, 0, &pmu->subdev);
+	nvkm_subdev_ctor(&nvkm_pmu, device, index, &pmu->subdev);
 	pmu->func = func;
 	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);

commit 7d28dbae228d16b28b6af98bf020effabfb7e0b0
Author: Karol Herbst <nouveau@karolherbst.de>
Date:   Mon Jan 11 02:58:03 2016 +0100

    drm/nouveau/pmu: be more strict about locking
    
    When we start communicating with the pmu a bit more, the current code is
    a real issue. I encountered a dead lock here, while testing my dynamic
    reclocking code
    
    Signed-off-by: Karol Herbst <nouveau@karolherbst.de>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index d95eb8659d1b..6e6d2ef598c8 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -40,21 +40,23 @@ nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 	struct nvkm_device *device = subdev->device;
 	u32 addr;
 
+	mutex_lock(&subdev->mutex);
 	/* wait for a free slot in the fifo */
 	addr  = nvkm_rd32(device, 0x10a4a0);
 	if (nvkm_msec(device, 2000,
 		u32 tmp = nvkm_rd32(device, 0x10a4b0);
 		if (tmp != (addr ^ 8))
 			break;
-	) < 0)
+	) < 0) {
+		mutex_unlock(&subdev->mutex);
 		return -EBUSY;
+	}
 
 	/* we currently only support a single process at a time waiting
 	 * on a synchronous reply, take the PMU mutex and tell the
 	 * receive handler what we're waiting for
 	 */
 	if (reply) {
-		mutex_lock(&subdev->mutex);
 		pmu->recv.message = message;
 		pmu->recv.process = process;
 	}
@@ -81,9 +83,9 @@ nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 		wait_event(pmu->recv.wait, (pmu->recv.process == 0));
 		reply[0] = pmu->recv.data[0];
 		reply[1] = pmu->recv.data[1];
-		mutex_unlock(&subdev->mutex);
 	}
 
+	mutex_unlock(&subdev->mutex);
 	return 0;
 }
 

commit 579b7c58215329803ce184704463de09f0f310ac
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Thu Sep 3 17:39:52 2015 +0900

    drm/nouveau/pmu: do not assume a PMU is present
    
    Some devices may not have a PMU. Avoid a NULL pointer dereference in
    such cases by checking whether the pointer given to nvkm_pmu_pgob() is
    valid.
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 27a79c0c3888..d95eb8659d1b 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -28,7 +28,7 @@
 void
 nvkm_pmu_pgob(struct nvkm_pmu *pmu, bool enable)
 {
-	if (pmu->func->pgob)
+	if (pmu && pmu->func->pgob)
 		pmu->func->pgob(pmu, enable);
 }
 

commit e2ca4e7d6e56cb73a068708f0b0c9bd62ab9e02c
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:21 2015 +1000

    drm/nouveau/pmu: convert to new-style nvkm_subdev
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index c700d3d956e8..27a79c0c3888 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -28,12 +28,11 @@
 void
 nvkm_pmu_pgob(struct nvkm_pmu *pmu, bool enable)
 {
-	const struct nvkm_pmu_impl *impl = (void *)nv_oclass(pmu);
-	if (impl->pgob)
-		impl->pgob(pmu, enable);
+	if (pmu->func->pgob)
+		pmu->func->pgob(pmu, enable);
 }
 
-static int
+int
 nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 	      u32 process, u32 message, u32 data0, u32 data1)
 {
@@ -144,7 +143,7 @@ nvkm_pmu_recv(struct work_struct *work)
 static void
 nvkm_pmu_intr(struct nvkm_subdev *subdev)
 {
-	struct nvkm_pmu *pmu = container_of(subdev, typeof(*pmu), subdev);
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	struct nvkm_device *device = pmu->subdev.device;
 	u32 disp = nvkm_rd32(device, 0x10a01c);
 	u32 intr = nvkm_rd32(device, 0x10a008) & disp & ~(disp >> 16);
@@ -180,33 +179,23 @@ nvkm_pmu_intr(struct nvkm_subdev *subdev)
 	}
 }
 
-int
-_nvkm_pmu_fini(struct nvkm_object *object, bool suspend)
+static int
+nvkm_pmu_fini(struct nvkm_subdev *subdev, bool suspend)
 {
-	struct nvkm_pmu *pmu = (void *)object;
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	struct nvkm_device *device = pmu->subdev.device;
 
 	nvkm_wr32(device, 0x10a014, 0x00000060);
 	flush_work(&pmu->recv.work);
-
-	return nvkm_subdev_fini_old(&pmu->subdev, suspend);
+	return 0;
 }
 
-int
-_nvkm_pmu_init(struct nvkm_object *object)
+static int
+nvkm_pmu_init(struct nvkm_subdev *subdev)
 {
-	const struct nvkm_pmu_impl *impl = (void *)object->oclass;
-	struct nvkm_pmu *pmu = (void *)object;
+	struct nvkm_pmu *pmu = nvkm_pmu(subdev);
 	struct nvkm_device *device = pmu->subdev.device;
-	int ret, i;
-
-	ret = nvkm_subdev_init_old(&pmu->subdev);
-	if (ret)
-		return ret;
-
-	nv_subdev(pmu)->intr = nvkm_pmu_intr;
-	pmu->message = nvkm_pmu_send;
-	pmu->pgob = nvkm_pmu_pgob;
+	int i;
 
 	/* prevent previous ucode from running, wait for idle, reset */
 	nvkm_wr32(device, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
@@ -224,15 +213,15 @@ _nvkm_pmu_init(struct nvkm_object *object)
 
 	/* upload data segment */
 	nvkm_wr32(device, 0x10a1c0, 0x01000000);
-	for (i = 0; i < impl->data.size / 4; i++)
-		nvkm_wr32(device, 0x10a1c4, impl->data.data[i]);
+	for (i = 0; i < pmu->func->data.size / 4; i++)
+		nvkm_wr32(device, 0x10a1c4, pmu->func->data.data[i]);
 
 	/* upload code segment */
 	nvkm_wr32(device, 0x10a180, 0x01000000);
-	for (i = 0; i < impl->code.size / 4; i++) {
+	for (i = 0; i < pmu->func->code.size / 4; i++) {
 		if ((i & 0x3f) == 0)
 			nvkm_wr32(device, 0x10a188, i >> 6);
-		nvkm_wr32(device, 0x10a184, impl->code.data[i]);
+		nvkm_wr32(device, 0x10a184, pmu->func->code.data[i]);
 	}
 
 	/* start it running */
@@ -262,31 +251,30 @@ _nvkm_pmu_init(struct nvkm_object *object)
 	return 0;
 }
 
-int
-nvkm_pmu_create_(struct nvkm_object *parent, struct nvkm_object *engine,
-		 struct nvkm_oclass *oclass, int length, void **pobject)
+static void *
+nvkm_pmu_dtor(struct nvkm_subdev *subdev)
 {
-	struct nvkm_pmu *pmu;
-	int ret;
-
-	ret = nvkm_subdev_create_(parent, engine, oclass, 0, "PMU",
-				  "pmu", length, pobject);
-	pmu = *pobject;
-	if (ret)
-		return ret;
-
-	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
-	init_waitqueue_head(&pmu->recv.wait);
-	return 0;
+	return nvkm_pmu(subdev);
 }
 
+static const struct nvkm_subdev_func
+nvkm_pmu = {
+	.dtor = nvkm_pmu_dtor,
+	.init = nvkm_pmu_init,
+	.fini = nvkm_pmu_fini,
+	.intr = nvkm_pmu_intr,
+};
+
 int
-_nvkm_pmu_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
-	       struct nvkm_oclass *oclass, void *data, u32 size,
-	       struct nvkm_object **pobject)
+nvkm_pmu_new_(const struct nvkm_pmu_func *func, struct nvkm_device *device,
+	      int index, struct nvkm_pmu **ppmu)
 {
 	struct nvkm_pmu *pmu;
-	int ret = nvkm_pmu_create(parent, engine, oclass, &pmu);
-	*pobject = nv_object(pmu);
-	return ret;
+	if (!(pmu = *ppmu = kzalloc(sizeof(*pmu), GFP_KERNEL)))
+		return -ENOMEM;
+	nvkm_subdev_ctor(&nvkm_pmu, device, index, 0, &pmu->subdev);
+	pmu->func = func;
+	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
+	init_waitqueue_head(&pmu->recv.wait);
+	return 0;
 }

commit 3a8c3400f3e74638bedd0d2410416aa8b794c0fd
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:16 2015 +1000

    drm/nouveau/subdev: rename some functions to avoid upcoming conflicts
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 247994017965..c700d3d956e8 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -189,7 +189,7 @@ _nvkm_pmu_fini(struct nvkm_object *object, bool suspend)
 	nvkm_wr32(device, 0x10a014, 0x00000060);
 	flush_work(&pmu->recv.work);
 
-	return nvkm_subdev_fini(&pmu->subdev, suspend);
+	return nvkm_subdev_fini_old(&pmu->subdev, suspend);
 }
 
 int
@@ -200,7 +200,7 @@ _nvkm_pmu_init(struct nvkm_object *object)
 	struct nvkm_device *device = pmu->subdev.device;
 	int ret, i;
 
-	ret = nvkm_subdev_init(&pmu->subdev);
+	ret = nvkm_subdev_init_old(&pmu->subdev);
 	if (ret)
 		return ret;
 

commit c19e329d663715014b367c4fedb217e0378342bf
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:12 2015 +1000

    drm/nouveau/pmu: switch to subdev printk macros
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 29c692c661da..247994017965 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -92,7 +92,8 @@ static void
 nvkm_pmu_recv(struct work_struct *work)
 {
 	struct nvkm_pmu *pmu = container_of(work, struct nvkm_pmu, recv.work);
-	struct nvkm_device *device = pmu->subdev.device;
+	struct nvkm_subdev *subdev = &pmu->subdev;
+	struct nvkm_device *device = subdev->device;
 	u32 process, message, data0, data1;
 
 	/* nothing to do if GET == PUT */
@@ -132,12 +133,12 @@ nvkm_pmu_recv(struct work_struct *work)
 	/* right now there's no other expected responses from the engine,
 	 * so assume that any unexpected message is an error.
 	 */
-	nv_warn(pmu, "%c%c%c%c 0x%08x 0x%08x 0x%08x 0x%08x\n",
-		(char)((process & 0x000000ff) >>  0),
-		(char)((process & 0x0000ff00) >>  8),
-		(char)((process & 0x00ff0000) >> 16),
-		(char)((process & 0xff000000) >> 24),
-		process, message, data0, data1);
+	nvkm_warn(subdev, "%c%c%c%c %08x %08x %08x %08x\n",
+		  (char)((process & 0x000000ff) >>  0),
+		  (char)((process & 0x0000ff00) >>  8),
+		  (char)((process & 0x00ff0000) >> 16),
+		  (char)((process & 0xff000000) >> 24),
+		  process, message, data0, data1);
 }
 
 static void
@@ -151,8 +152,9 @@ nvkm_pmu_intr(struct nvkm_subdev *subdev)
 	if (intr & 0x00000020) {
 		u32 stat = nvkm_rd32(device, 0x10a16c);
 		if (stat & 0x80000000) {
-			nv_error(pmu, "UAS fault at 0x%06x addr 0x%08x\n",
-				 stat & 0x00ffffff, nvkm_rd32(device, 0x10a168));
+			nvkm_error(subdev, "UAS fault at %06x addr %08x\n",
+				   stat & 0x00ffffff,
+				   nvkm_rd32(device, 0x10a168));
 			nvkm_wr32(device, 0x10a16c, 0x00000000);
 			intr &= ~0x00000020;
 		}
@@ -165,14 +167,15 @@ nvkm_pmu_intr(struct nvkm_subdev *subdev)
 	}
 
 	if (intr & 0x00000080) {
-		nv_info(pmu, "wr32 0x%06x 0x%08x\n", nvkm_rd32(device, 0x10a7a0),
-						     nvkm_rd32(device, 0x10a7a4));
+		nvkm_info(subdev, "wr32 %06x %08x\n",
+			  nvkm_rd32(device, 0x10a7a0),
+			  nvkm_rd32(device, 0x10a7a4));
 		nvkm_wr32(device, 0x10a004, 0x00000080);
 		intr &= ~0x00000080;
 	}
 
 	if (intr) {
-		nv_error(pmu, "intr 0x%08x\n", intr);
+		nvkm_error(subdev, "intr %08x\n", intr);
 		nvkm_wr32(device, 0x10a004, intr);
 	}
 }

commit dd4bb3eccc806e78baee3009aa466466daae0f29
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:11 2015 +1000

    drm/nouveau/pmu: switch to new-style timer macros
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index b754b65f2c36..29c692c661da 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -43,7 +43,11 @@ nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 
 	/* wait for a free slot in the fifo */
 	addr  = nvkm_rd32(device, 0x10a4a0);
-	if (!nv_wait_ne(pmu, 0x10a4b0, 0xffffffff, addr ^ 8))
+	if (nvkm_msec(device, 2000,
+		u32 tmp = nvkm_rd32(device, 0x10a4b0);
+		if (tmp != (addr ^ 8))
+			break;
+	) < 0)
 		return -EBUSY;
 
 	/* we currently only support a single process at a time waiting
@@ -203,11 +207,17 @@ _nvkm_pmu_init(struct nvkm_object *object)
 
 	/* prevent previous ucode from running, wait for idle, reset */
 	nvkm_wr32(device, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
-	nv_wait(pmu, 0x10a04c, 0xffffffff, 0x00000000);
+	nvkm_msec(device, 2000,
+		if (!nvkm_rd32(device, 0x10a04c))
+			break;
+	);
 	nvkm_mask(device, 0x000200, 0x00002000, 0x00000000);
 	nvkm_mask(device, 0x000200, 0x00002000, 0x00002000);
 	nvkm_rd32(device, 0x000200);
-	nv_wait(pmu, 0x10a10c, 0x00000006, 0x00000000);
+	nvkm_msec(device, 2000,
+		if (!(nvkm_rd32(device, 0x10a10c) & 0x00000006))
+			break;
+	);
 
 	/* upload data segment */
 	nvkm_wr32(device, 0x10a1c0, 0x01000000);
@@ -228,13 +238,19 @@ _nvkm_pmu_init(struct nvkm_object *object)
 	nvkm_wr32(device, 0x10a100, 0x00000002);
 
 	/* wait for valid host->pmu ring configuration */
-	if (!nv_wait_ne(pmu, 0x10a4d0, 0xffffffff, 0x00000000))
+	if (nvkm_msec(device, 2000,
+		if (nvkm_rd32(device, 0x10a4d0))
+			break;
+	) < 0)
 		return -EBUSY;
 	pmu->send.base = nvkm_rd32(device, 0x10a4d0) & 0x0000ffff;
 	pmu->send.size = nvkm_rd32(device, 0x10a4d0) >> 16;
 
 	/* wait for valid pmu->host ring configuration */
-	if (!nv_wait_ne(pmu, 0x10a4dc, 0xffffffff, 0x00000000))
+	if (nvkm_msec(device, 2000,
+		if (nvkm_rd32(device, 0x10a4dc))
+			break;
+	) < 0)
 		return -EBUSY;
 	pmu->recv.base = nvkm_rd32(device, 0x10a4dc) & 0x0000ffff;
 	pmu->recv.size = nvkm_rd32(device, 0x10a4dc) >> 16;

commit bef002e87f1accc12a7fc7c98b80354c136c199e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:09 2015 +1000

    drm/nouveau/pmu: switch to device pri macros
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index e0fbf5aaeeeb..b754b65f2c36 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -37,11 +37,12 @@ static int
 nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 	      u32 process, u32 message, u32 data0, u32 data1)
 {
-	struct nvkm_subdev *subdev = nv_subdev(pmu);
+	struct nvkm_subdev *subdev = &pmu->subdev;
+	struct nvkm_device *device = subdev->device;
 	u32 addr;
 
 	/* wait for a free slot in the fifo */
-	addr  = nv_rd32(pmu, 0x10a4a0);
+	addr  = nvkm_rd32(device, 0x10a4a0);
 	if (!nv_wait_ne(pmu, 0x10a4b0, 0xffffffff, addr ^ 8))
 		return -EBUSY;
 
@@ -57,20 +58,20 @@ nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
 
 	/* acquire data segment access */
 	do {
-		nv_wr32(pmu, 0x10a580, 0x00000001);
-	} while (nv_rd32(pmu, 0x10a580) != 0x00000001);
+		nvkm_wr32(device, 0x10a580, 0x00000001);
+	} while (nvkm_rd32(device, 0x10a580) != 0x00000001);
 
 	/* write the packet */
-	nv_wr32(pmu, 0x10a1c0, 0x01000000 | (((addr & 0x07) << 4) +
+	nvkm_wr32(device, 0x10a1c0, 0x01000000 | (((addr & 0x07) << 4) +
 				pmu->send.base));
-	nv_wr32(pmu, 0x10a1c4, process);
-	nv_wr32(pmu, 0x10a1c4, message);
-	nv_wr32(pmu, 0x10a1c4, data0);
-	nv_wr32(pmu, 0x10a1c4, data1);
-	nv_wr32(pmu, 0x10a4a0, (addr + 1) & 0x0f);
+	nvkm_wr32(device, 0x10a1c4, process);
+	nvkm_wr32(device, 0x10a1c4, message);
+	nvkm_wr32(device, 0x10a1c4, data0);
+	nvkm_wr32(device, 0x10a1c4, data1);
+	nvkm_wr32(device, 0x10a4a0, (addr + 1) & 0x0f);
 
 	/* release data segment access */
-	nv_wr32(pmu, 0x10a580, 0x00000000);
+	nvkm_wr32(device, 0x10a580, 0x00000000);
 
 	/* wait for reply, if requested */
 	if (reply) {
@@ -87,29 +88,30 @@ static void
 nvkm_pmu_recv(struct work_struct *work)
 {
 	struct nvkm_pmu *pmu = container_of(work, struct nvkm_pmu, recv.work);
+	struct nvkm_device *device = pmu->subdev.device;
 	u32 process, message, data0, data1;
 
 	/* nothing to do if GET == PUT */
-	u32 addr =  nv_rd32(pmu, 0x10a4cc);
-	if (addr == nv_rd32(pmu, 0x10a4c8))
+	u32 addr =  nvkm_rd32(device, 0x10a4cc);
+	if (addr == nvkm_rd32(device, 0x10a4c8))
 		return;
 
 	/* acquire data segment access */
 	do {
-		nv_wr32(pmu, 0x10a580, 0x00000002);
-	} while (nv_rd32(pmu, 0x10a580) != 0x00000002);
+		nvkm_wr32(device, 0x10a580, 0x00000002);
+	} while (nvkm_rd32(device, 0x10a580) != 0x00000002);
 
 	/* read the packet */
-	nv_wr32(pmu, 0x10a1c0, 0x02000000 | (((addr & 0x07) << 4) +
+	nvkm_wr32(device, 0x10a1c0, 0x02000000 | (((addr & 0x07) << 4) +
 				pmu->recv.base));
-	process = nv_rd32(pmu, 0x10a1c4);
-	message = nv_rd32(pmu, 0x10a1c4);
-	data0   = nv_rd32(pmu, 0x10a1c4);
-	data1   = nv_rd32(pmu, 0x10a1c4);
-	nv_wr32(pmu, 0x10a4cc, (addr + 1) & 0x0f);
+	process = nvkm_rd32(device, 0x10a1c4);
+	message = nvkm_rd32(device, 0x10a1c4);
+	data0   = nvkm_rd32(device, 0x10a1c4);
+	data1   = nvkm_rd32(device, 0x10a1c4);
+	nvkm_wr32(device, 0x10a4cc, (addr + 1) & 0x0f);
 
 	/* release data segment access */
-	nv_wr32(pmu, 0x10a580, 0x00000000);
+	nvkm_wr32(device, 0x10a580, 0x00000000);
 
 	/* wake process if it's waiting on a synchronous reply */
 	if (pmu->recv.process) {
@@ -137,36 +139,37 @@ nvkm_pmu_recv(struct work_struct *work)
 static void
 nvkm_pmu_intr(struct nvkm_subdev *subdev)
 {
-	struct nvkm_pmu *pmu = (void *)subdev;
-	u32 disp = nv_rd32(pmu, 0x10a01c);
-	u32 intr = nv_rd32(pmu, 0x10a008) & disp & ~(disp >> 16);
+	struct nvkm_pmu *pmu = container_of(subdev, typeof(*pmu), subdev);
+	struct nvkm_device *device = pmu->subdev.device;
+	u32 disp = nvkm_rd32(device, 0x10a01c);
+	u32 intr = nvkm_rd32(device, 0x10a008) & disp & ~(disp >> 16);
 
 	if (intr & 0x00000020) {
-		u32 stat = nv_rd32(pmu, 0x10a16c);
+		u32 stat = nvkm_rd32(device, 0x10a16c);
 		if (stat & 0x80000000) {
 			nv_error(pmu, "UAS fault at 0x%06x addr 0x%08x\n",
-				 stat & 0x00ffffff, nv_rd32(pmu, 0x10a168));
-			nv_wr32(pmu, 0x10a16c, 0x00000000);
+				 stat & 0x00ffffff, nvkm_rd32(device, 0x10a168));
+			nvkm_wr32(device, 0x10a16c, 0x00000000);
 			intr &= ~0x00000020;
 		}
 	}
 
 	if (intr & 0x00000040) {
 		schedule_work(&pmu->recv.work);
-		nv_wr32(pmu, 0x10a004, 0x00000040);
+		nvkm_wr32(device, 0x10a004, 0x00000040);
 		intr &= ~0x00000040;
 	}
 
 	if (intr & 0x00000080) {
-		nv_info(pmu, "wr32 0x%06x 0x%08x\n", nv_rd32(pmu, 0x10a7a0),
-						     nv_rd32(pmu, 0x10a7a4));
-		nv_wr32(pmu, 0x10a004, 0x00000080);
+		nv_info(pmu, "wr32 0x%06x 0x%08x\n", nvkm_rd32(device, 0x10a7a0),
+						     nvkm_rd32(device, 0x10a7a4));
+		nvkm_wr32(device, 0x10a004, 0x00000080);
 		intr &= ~0x00000080;
 	}
 
 	if (intr) {
 		nv_error(pmu, "intr 0x%08x\n", intr);
-		nv_wr32(pmu, 0x10a004, intr);
+		nvkm_wr32(device, 0x10a004, intr);
 	}
 }
 
@@ -174,8 +177,9 @@ int
 _nvkm_pmu_fini(struct nvkm_object *object, bool suspend)
 {
 	struct nvkm_pmu *pmu = (void *)object;
+	struct nvkm_device *device = pmu->subdev.device;
 
-	nv_wr32(pmu, 0x10a014, 0x00000060);
+	nvkm_wr32(device, 0x10a014, 0x00000060);
 	flush_work(&pmu->recv.work);
 
 	return nvkm_subdev_fini(&pmu->subdev, suspend);
@@ -186,6 +190,7 @@ _nvkm_pmu_init(struct nvkm_object *object)
 {
 	const struct nvkm_pmu_impl *impl = (void *)object->oclass;
 	struct nvkm_pmu *pmu = (void *)object;
+	struct nvkm_device *device = pmu->subdev.device;
 	int ret, i;
 
 	ret = nvkm_subdev_init(&pmu->subdev);
@@ -197,44 +202,44 @@ _nvkm_pmu_init(struct nvkm_object *object)
 	pmu->pgob = nvkm_pmu_pgob;
 
 	/* prevent previous ucode from running, wait for idle, reset */
-	nv_wr32(pmu, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
+	nvkm_wr32(device, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
 	nv_wait(pmu, 0x10a04c, 0xffffffff, 0x00000000);
-	nv_mask(pmu, 0x000200, 0x00002000, 0x00000000);
-	nv_mask(pmu, 0x000200, 0x00002000, 0x00002000);
-	nv_rd32(pmu, 0x000200);
+	nvkm_mask(device, 0x000200, 0x00002000, 0x00000000);
+	nvkm_mask(device, 0x000200, 0x00002000, 0x00002000);
+	nvkm_rd32(device, 0x000200);
 	nv_wait(pmu, 0x10a10c, 0x00000006, 0x00000000);
 
 	/* upload data segment */
-	nv_wr32(pmu, 0x10a1c0, 0x01000000);
+	nvkm_wr32(device, 0x10a1c0, 0x01000000);
 	for (i = 0; i < impl->data.size / 4; i++)
-		nv_wr32(pmu, 0x10a1c4, impl->data.data[i]);
+		nvkm_wr32(device, 0x10a1c4, impl->data.data[i]);
 
 	/* upload code segment */
-	nv_wr32(pmu, 0x10a180, 0x01000000);
+	nvkm_wr32(device, 0x10a180, 0x01000000);
 	for (i = 0; i < impl->code.size / 4; i++) {
 		if ((i & 0x3f) == 0)
-			nv_wr32(pmu, 0x10a188, i >> 6);
-		nv_wr32(pmu, 0x10a184, impl->code.data[i]);
+			nvkm_wr32(device, 0x10a188, i >> 6);
+		nvkm_wr32(device, 0x10a184, impl->code.data[i]);
 	}
 
 	/* start it running */
-	nv_wr32(pmu, 0x10a10c, 0x00000000);
-	nv_wr32(pmu, 0x10a104, 0x00000000);
-	nv_wr32(pmu, 0x10a100, 0x00000002);
+	nvkm_wr32(device, 0x10a10c, 0x00000000);
+	nvkm_wr32(device, 0x10a104, 0x00000000);
+	nvkm_wr32(device, 0x10a100, 0x00000002);
 
 	/* wait for valid host->pmu ring configuration */
 	if (!nv_wait_ne(pmu, 0x10a4d0, 0xffffffff, 0x00000000))
 		return -EBUSY;
-	pmu->send.base = nv_rd32(pmu, 0x10a4d0) & 0x0000ffff;
-	pmu->send.size = nv_rd32(pmu, 0x10a4d0) >> 16;
+	pmu->send.base = nvkm_rd32(device, 0x10a4d0) & 0x0000ffff;
+	pmu->send.size = nvkm_rd32(device, 0x10a4d0) >> 16;
 
 	/* wait for valid pmu->host ring configuration */
 	if (!nv_wait_ne(pmu, 0x10a4dc, 0xffffffff, 0x00000000))
 		return -EBUSY;
-	pmu->recv.base = nv_rd32(pmu, 0x10a4dc) & 0x0000ffff;
-	pmu->recv.size = nv_rd32(pmu, 0x10a4dc) >> 16;
+	pmu->recv.base = nvkm_rd32(device, 0x10a4dc) & 0x0000ffff;
+	pmu->recv.size = nvkm_rd32(device, 0x10a4dc) >> 16;
 
-	nv_wr32(pmu, 0x10a010, 0x000000e0);
+	nvkm_wr32(device, 0x10a010, 0x000000e0);
 	return 0;
 }
 

commit 5a7d1e22feedd3cfab5a94bba5f26ab61610bc62
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:07 2015 +1000

    drm/nouveau/pmu: cosmetic changes
    
    This is purely preparation for upcoming commits, there should be no
    code changes here.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 054b2d2eec35..e0fbf5aaeeeb 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -178,7 +178,7 @@ _nvkm_pmu_fini(struct nvkm_object *object, bool suspend)
 	nv_wr32(pmu, 0x10a014, 0x00000060);
 	flush_work(&pmu->recv.work);
 
-	return nvkm_subdev_fini(&pmu->base, suspend);
+	return nvkm_subdev_fini(&pmu->subdev, suspend);
 }
 
 int
@@ -188,7 +188,7 @@ _nvkm_pmu_init(struct nvkm_object *object)
 	struct nvkm_pmu *pmu = (void *)object;
 	int ret, i;
 
-	ret = nvkm_subdev_init(&pmu->base);
+	ret = nvkm_subdev_init(&pmu->subdev);
 	if (ret)
 		return ret;
 

commit 21b137916ec25a507dbf7b6fe8b353fe9dc723c0
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 14 15:10:40 2015 +1000

    drm/nouveau/pmu: namespace + nvidia gpu names (no binary change)
    
    The namespace of NVKM is being changed to nvkm_ instead of nouveau_,
    which will be used for the DRM part of the driver.  This is being
    done in order to make it very clear as to what part of the driver a
    given symbol belongs to, and as a minor step towards splitting the
    DRM driver out to be able to stand on its own (for virt).
    
    Because there's already a large amount of churn here anyway, this is
    as good a time as any to also switch to NVIDIA's device and chipset
    naming to ease collaboration with them.
    
    A comparison of objdump disassemblies proves no code changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
index 562ea6e16819..054b2d2eec35 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -21,13 +21,12 @@
  *
  * Authors: Ben Skeggs
  */
+#include "priv.h"
 
 #include <subdev/timer.h>
 
-#include "priv.h"
-
 void
-nouveau_pmu_pgob(struct nouveau_pmu *pmu, bool enable)
+nvkm_pmu_pgob(struct nvkm_pmu *pmu, bool enable)
 {
 	const struct nvkm_pmu_impl *impl = (void *)nv_oclass(pmu);
 	if (impl->pgob)
@@ -35,10 +34,10 @@ nouveau_pmu_pgob(struct nouveau_pmu *pmu, bool enable)
 }
 
 static int
-nouveau_pmu_send(struct nouveau_pmu *pmu, u32 reply[2],
-		 u32 process, u32 message, u32 data0, u32 data1)
+nvkm_pmu_send(struct nvkm_pmu *pmu, u32 reply[2],
+	      u32 process, u32 message, u32 data0, u32 data1)
 {
-	struct nouveau_subdev *subdev = nv_subdev(pmu);
+	struct nvkm_subdev *subdev = nv_subdev(pmu);
 	u32 addr;
 
 	/* wait for a free slot in the fifo */
@@ -85,10 +84,9 @@ nouveau_pmu_send(struct nouveau_pmu *pmu, u32 reply[2],
 }
 
 static void
-nouveau_pmu_recv(struct work_struct *work)
+nvkm_pmu_recv(struct work_struct *work)
 {
-	struct nouveau_pmu *pmu =
-		container_of(work, struct nouveau_pmu, recv.work);
+	struct nvkm_pmu *pmu = container_of(work, struct nvkm_pmu, recv.work);
 	u32 process, message, data0, data1;
 
 	/* nothing to do if GET == PUT */
@@ -137,9 +135,9 @@ nouveau_pmu_recv(struct work_struct *work)
 }
 
 static void
-nouveau_pmu_intr(struct nouveau_subdev *subdev)
+nvkm_pmu_intr(struct nvkm_subdev *subdev)
 {
-	struct nouveau_pmu *pmu = (void *)subdev;
+	struct nvkm_pmu *pmu = (void *)subdev;
 	u32 disp = nv_rd32(pmu, 0x10a01c);
 	u32 intr = nv_rd32(pmu, 0x10a008) & disp & ~(disp >> 16);
 
@@ -161,7 +159,7 @@ nouveau_pmu_intr(struct nouveau_subdev *subdev)
 
 	if (intr & 0x00000080) {
 		nv_info(pmu, "wr32 0x%06x 0x%08x\n", nv_rd32(pmu, 0x10a7a0),
-						      nv_rd32(pmu, 0x10a7a4));
+						     nv_rd32(pmu, 0x10a7a4));
 		nv_wr32(pmu, 0x10a004, 0x00000080);
 		intr &= ~0x00000080;
 	}
@@ -173,30 +171,30 @@ nouveau_pmu_intr(struct nouveau_subdev *subdev)
 }
 
 int
-_nouveau_pmu_fini(struct nouveau_object *object, bool suspend)
+_nvkm_pmu_fini(struct nvkm_object *object, bool suspend)
 {
-	struct nouveau_pmu *pmu = (void *)object;
+	struct nvkm_pmu *pmu = (void *)object;
 
 	nv_wr32(pmu, 0x10a014, 0x00000060);
 	flush_work(&pmu->recv.work);
 
-	return nouveau_subdev_fini(&pmu->base, suspend);
+	return nvkm_subdev_fini(&pmu->base, suspend);
 }
 
 int
-_nouveau_pmu_init(struct nouveau_object *object)
+_nvkm_pmu_init(struct nvkm_object *object)
 {
 	const struct nvkm_pmu_impl *impl = (void *)object->oclass;
-	struct nouveau_pmu *pmu = (void *)object;
+	struct nvkm_pmu *pmu = (void *)object;
 	int ret, i;
 
-	ret = nouveau_subdev_init(&pmu->base);
+	ret = nvkm_subdev_init(&pmu->base);
 	if (ret)
 		return ret;
 
-	nv_subdev(pmu)->intr = nouveau_pmu_intr;
-	pmu->message = nouveau_pmu_send;
-	pmu->pgob = nouveau_pmu_pgob;
+	nv_subdev(pmu)->intr = nvkm_pmu_intr;
+	pmu->message = nvkm_pmu_send;
+	pmu->pgob = nvkm_pmu_pgob;
 
 	/* prevent previous ucode from running, wait for idle, reset */
 	nv_wr32(pmu, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
@@ -241,32 +239,30 @@ _nouveau_pmu_init(struct nouveau_object *object)
 }
 
 int
-nouveau_pmu_create_(struct nouveau_object *parent,
-		    struct nouveau_object *engine,
-		    struct nouveau_oclass *oclass, int length, void **pobject)
+nvkm_pmu_create_(struct nvkm_object *parent, struct nvkm_object *engine,
+		 struct nvkm_oclass *oclass, int length, void **pobject)
 {
-	struct nouveau_pmu *pmu;
+	struct nvkm_pmu *pmu;
 	int ret;
 
-	ret = nouveau_subdev_create_(parent, engine, oclass, 0, "PMU",
-				     "pmu", length, pobject);
+	ret = nvkm_subdev_create_(parent, engine, oclass, 0, "PMU",
+				  "pmu", length, pobject);
 	pmu = *pobject;
 	if (ret)
 		return ret;
 
-	INIT_WORK(&pmu->recv.work, nouveau_pmu_recv);
+	INIT_WORK(&pmu->recv.work, nvkm_pmu_recv);
 	init_waitqueue_head(&pmu->recv.wait);
 	return 0;
 }
 
 int
-_nouveau_pmu_ctor(struct nouveau_object *parent,
-		  struct nouveau_object *engine,
-		  struct nouveau_oclass *oclass, void *data, u32 size,
-		  struct nouveau_object **pobject)
+_nvkm_pmu_ctor(struct nvkm_object *parent, struct nvkm_object *engine,
+	       struct nvkm_oclass *oclass, void *data, u32 size,
+	       struct nvkm_object **pobject)
 {
-	struct nouveau_pmu *pmu;
-	int ret = nouveau_pmu_create(parent, engine, oclass, &pmu);
+	struct nvkm_pmu *pmu;
+	int ret = nvkm_pmu_create(parent, engine, oclass, &pmu);
 	*pobject = nv_object(pmu);
 	return ret;
 }

commit ebb58dc2ef8c62d1affa28160f57faa7b0e1dc02
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 14 00:04:21 2015 +1000

    drm/nouveau/pmu: rename from pwr (no binary change)
    
    Switch to NVIDIA's name for the device.
    
    The namespace of NVKM is being changed to nvkm_ instead of nouveau_,
    which will be used for the DRM part of the driver.  This is being
    done in order to make it very clear as to what part of the driver a
    given symbol belongs to, and as a minor step towards splitting the
    DRM driver out to be able to stand on its own (for virt).
    
    Because there's already a large amount of churn here anyway, this is
    as good a time as any to also switch to NVIDIA's device and chipset
    naming to ease collaboration with them.
    
    A comparison of objdump disassemblies proves no code changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
new file mode 100644
index 000000000000..562ea6e16819
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/pmu/base.c
@@ -0,0 +1,272 @@
+/*
+ * Copyright 2013 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Ben Skeggs
+ */
+
+#include <subdev/timer.h>
+
+#include "priv.h"
+
+void
+nouveau_pmu_pgob(struct nouveau_pmu *pmu, bool enable)
+{
+	const struct nvkm_pmu_impl *impl = (void *)nv_oclass(pmu);
+	if (impl->pgob)
+		impl->pgob(pmu, enable);
+}
+
+static int
+nouveau_pmu_send(struct nouveau_pmu *pmu, u32 reply[2],
+		 u32 process, u32 message, u32 data0, u32 data1)
+{
+	struct nouveau_subdev *subdev = nv_subdev(pmu);
+	u32 addr;
+
+	/* wait for a free slot in the fifo */
+	addr  = nv_rd32(pmu, 0x10a4a0);
+	if (!nv_wait_ne(pmu, 0x10a4b0, 0xffffffff, addr ^ 8))
+		return -EBUSY;
+
+	/* we currently only support a single process at a time waiting
+	 * on a synchronous reply, take the PMU mutex and tell the
+	 * receive handler what we're waiting for
+	 */
+	if (reply) {
+		mutex_lock(&subdev->mutex);
+		pmu->recv.message = message;
+		pmu->recv.process = process;
+	}
+
+	/* acquire data segment access */
+	do {
+		nv_wr32(pmu, 0x10a580, 0x00000001);
+	} while (nv_rd32(pmu, 0x10a580) != 0x00000001);
+
+	/* write the packet */
+	nv_wr32(pmu, 0x10a1c0, 0x01000000 | (((addr & 0x07) << 4) +
+				pmu->send.base));
+	nv_wr32(pmu, 0x10a1c4, process);
+	nv_wr32(pmu, 0x10a1c4, message);
+	nv_wr32(pmu, 0x10a1c4, data0);
+	nv_wr32(pmu, 0x10a1c4, data1);
+	nv_wr32(pmu, 0x10a4a0, (addr + 1) & 0x0f);
+
+	/* release data segment access */
+	nv_wr32(pmu, 0x10a580, 0x00000000);
+
+	/* wait for reply, if requested */
+	if (reply) {
+		wait_event(pmu->recv.wait, (pmu->recv.process == 0));
+		reply[0] = pmu->recv.data[0];
+		reply[1] = pmu->recv.data[1];
+		mutex_unlock(&subdev->mutex);
+	}
+
+	return 0;
+}
+
+static void
+nouveau_pmu_recv(struct work_struct *work)
+{
+	struct nouveau_pmu *pmu =
+		container_of(work, struct nouveau_pmu, recv.work);
+	u32 process, message, data0, data1;
+
+	/* nothing to do if GET == PUT */
+	u32 addr =  nv_rd32(pmu, 0x10a4cc);
+	if (addr == nv_rd32(pmu, 0x10a4c8))
+		return;
+
+	/* acquire data segment access */
+	do {
+		nv_wr32(pmu, 0x10a580, 0x00000002);
+	} while (nv_rd32(pmu, 0x10a580) != 0x00000002);
+
+	/* read the packet */
+	nv_wr32(pmu, 0x10a1c0, 0x02000000 | (((addr & 0x07) << 4) +
+				pmu->recv.base));
+	process = nv_rd32(pmu, 0x10a1c4);
+	message = nv_rd32(pmu, 0x10a1c4);
+	data0   = nv_rd32(pmu, 0x10a1c4);
+	data1   = nv_rd32(pmu, 0x10a1c4);
+	nv_wr32(pmu, 0x10a4cc, (addr + 1) & 0x0f);
+
+	/* release data segment access */
+	nv_wr32(pmu, 0x10a580, 0x00000000);
+
+	/* wake process if it's waiting on a synchronous reply */
+	if (pmu->recv.process) {
+		if (process == pmu->recv.process &&
+		    message == pmu->recv.message) {
+			pmu->recv.data[0] = data0;
+			pmu->recv.data[1] = data1;
+			pmu->recv.process = 0;
+			wake_up(&pmu->recv.wait);
+			return;
+		}
+	}
+
+	/* right now there's no other expected responses from the engine,
+	 * so assume that any unexpected message is an error.
+	 */
+	nv_warn(pmu, "%c%c%c%c 0x%08x 0x%08x 0x%08x 0x%08x\n",
+		(char)((process & 0x000000ff) >>  0),
+		(char)((process & 0x0000ff00) >>  8),
+		(char)((process & 0x00ff0000) >> 16),
+		(char)((process & 0xff000000) >> 24),
+		process, message, data0, data1);
+}
+
+static void
+nouveau_pmu_intr(struct nouveau_subdev *subdev)
+{
+	struct nouveau_pmu *pmu = (void *)subdev;
+	u32 disp = nv_rd32(pmu, 0x10a01c);
+	u32 intr = nv_rd32(pmu, 0x10a008) & disp & ~(disp >> 16);
+
+	if (intr & 0x00000020) {
+		u32 stat = nv_rd32(pmu, 0x10a16c);
+		if (stat & 0x80000000) {
+			nv_error(pmu, "UAS fault at 0x%06x addr 0x%08x\n",
+				 stat & 0x00ffffff, nv_rd32(pmu, 0x10a168));
+			nv_wr32(pmu, 0x10a16c, 0x00000000);
+			intr &= ~0x00000020;
+		}
+	}
+
+	if (intr & 0x00000040) {
+		schedule_work(&pmu->recv.work);
+		nv_wr32(pmu, 0x10a004, 0x00000040);
+		intr &= ~0x00000040;
+	}
+
+	if (intr & 0x00000080) {
+		nv_info(pmu, "wr32 0x%06x 0x%08x\n", nv_rd32(pmu, 0x10a7a0),
+						      nv_rd32(pmu, 0x10a7a4));
+		nv_wr32(pmu, 0x10a004, 0x00000080);
+		intr &= ~0x00000080;
+	}
+
+	if (intr) {
+		nv_error(pmu, "intr 0x%08x\n", intr);
+		nv_wr32(pmu, 0x10a004, intr);
+	}
+}
+
+int
+_nouveau_pmu_fini(struct nouveau_object *object, bool suspend)
+{
+	struct nouveau_pmu *pmu = (void *)object;
+
+	nv_wr32(pmu, 0x10a014, 0x00000060);
+	flush_work(&pmu->recv.work);
+
+	return nouveau_subdev_fini(&pmu->base, suspend);
+}
+
+int
+_nouveau_pmu_init(struct nouveau_object *object)
+{
+	const struct nvkm_pmu_impl *impl = (void *)object->oclass;
+	struct nouveau_pmu *pmu = (void *)object;
+	int ret, i;
+
+	ret = nouveau_subdev_init(&pmu->base);
+	if (ret)
+		return ret;
+
+	nv_subdev(pmu)->intr = nouveau_pmu_intr;
+	pmu->message = nouveau_pmu_send;
+	pmu->pgob = nouveau_pmu_pgob;
+
+	/* prevent previous ucode from running, wait for idle, reset */
+	nv_wr32(pmu, 0x10a014, 0x0000ffff); /* INTR_EN_CLR = ALL */
+	nv_wait(pmu, 0x10a04c, 0xffffffff, 0x00000000);
+	nv_mask(pmu, 0x000200, 0x00002000, 0x00000000);
+	nv_mask(pmu, 0x000200, 0x00002000, 0x00002000);
+	nv_rd32(pmu, 0x000200);
+	nv_wait(pmu, 0x10a10c, 0x00000006, 0x00000000);
+
+	/* upload data segment */
+	nv_wr32(pmu, 0x10a1c0, 0x01000000);
+	for (i = 0; i < impl->data.size / 4; i++)
+		nv_wr32(pmu, 0x10a1c4, impl->data.data[i]);
+
+	/* upload code segment */
+	nv_wr32(pmu, 0x10a180, 0x01000000);
+	for (i = 0; i < impl->code.size / 4; i++) {
+		if ((i & 0x3f) == 0)
+			nv_wr32(pmu, 0x10a188, i >> 6);
+		nv_wr32(pmu, 0x10a184, impl->code.data[i]);
+	}
+
+	/* start it running */
+	nv_wr32(pmu, 0x10a10c, 0x00000000);
+	nv_wr32(pmu, 0x10a104, 0x00000000);
+	nv_wr32(pmu, 0x10a100, 0x00000002);
+
+	/* wait for valid host->pmu ring configuration */
+	if (!nv_wait_ne(pmu, 0x10a4d0, 0xffffffff, 0x00000000))
+		return -EBUSY;
+	pmu->send.base = nv_rd32(pmu, 0x10a4d0) & 0x0000ffff;
+	pmu->send.size = nv_rd32(pmu, 0x10a4d0) >> 16;
+
+	/* wait for valid pmu->host ring configuration */
+	if (!nv_wait_ne(pmu, 0x10a4dc, 0xffffffff, 0x00000000))
+		return -EBUSY;
+	pmu->recv.base = nv_rd32(pmu, 0x10a4dc) & 0x0000ffff;
+	pmu->recv.size = nv_rd32(pmu, 0x10a4dc) >> 16;
+
+	nv_wr32(pmu, 0x10a010, 0x000000e0);
+	return 0;
+}
+
+int
+nouveau_pmu_create_(struct nouveau_object *parent,
+		    struct nouveau_object *engine,
+		    struct nouveau_oclass *oclass, int length, void **pobject)
+{
+	struct nouveau_pmu *pmu;
+	int ret;
+
+	ret = nouveau_subdev_create_(parent, engine, oclass, 0, "PMU",
+				     "pmu", length, pobject);
+	pmu = *pobject;
+	if (ret)
+		return ret;
+
+	INIT_WORK(&pmu->recv.work, nouveau_pmu_recv);
+	init_waitqueue_head(&pmu->recv.wait);
+	return 0;
+}
+
+int
+_nouveau_pmu_ctor(struct nouveau_object *parent,
+		  struct nouveau_object *engine,
+		  struct nouveau_oclass *oclass, void *data, u32 size,
+		  struct nouveau_object **pobject)
+{
+	struct nouveau_pmu *pmu;
+	int ret = nouveau_pmu_create(parent, engine, oclass, &pmu);
+	*pobject = nv_object(pmu);
+	return ret;
+}
