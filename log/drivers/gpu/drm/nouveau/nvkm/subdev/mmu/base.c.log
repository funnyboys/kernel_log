commit 632b740c5481988152a3a60319aaa49c99577b77
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:20 2017 +1000

    drm/nouveau/mmu: remove old vmm frontend
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index e0fa0cae7960..ee11ccaf0563 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -216,67 +216,6 @@ nvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)
 	return pt;
 }
 
-static void
-nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
-	     struct nvkm_mem *mem, nvkm_vmm_pte_func fn,
-	     struct nvkm_vmm_map *map)
-{
-	union {
-		struct nv50_vmm_map_v0 nv50;
-		struct gf100_vmm_map_v0 gf100;
-	} args;
-	struct nvkm_vmm *vmm = vma->vm;
-	void *argv = NULL;
-	u32 argc = 0;
-	int ret;
-
-	map->memory = mem->memory;
-	map->page = page;
-
-	if (vmm->func->valid) {
-		switch (vmm->mmu->subdev.device->card_type) {
-		case NV_50:
-			args.nv50.version = 0;
-			args.nv50.ro = !(vma->access & NV_MEM_ACCESS_WO);
-			args.nv50.priv = !!(vma->access & NV_MEM_ACCESS_SYS);
-			args.nv50.kind = (mem->memtype & 0x07f);
-			args.nv50.comp = (mem->memtype & 0x180) >> 7;
-			argv = &args.nv50;
-			argc = sizeof(args.nv50);
-			break;
-		case NV_C0:
-		case NV_E0:
-		case GM100:
-		case GP100: {
-			args.gf100.version = 0;
-			args.gf100.vol = (nvkm_memory_target(map->memory) != NVKM_MEM_TARGET_VRAM);
-			args.gf100.ro = !(vma->access & NV_MEM_ACCESS_WO);
-			args.gf100.priv = !!(vma->access & NV_MEM_ACCESS_SYS);
-			args.gf100.kind = (mem->memtype & 0x0ff);
-			argv = &args.gf100;
-			argc = sizeof(args.gf100);
-		}
-			break;
-		default:
-			break;
-		}
-
-		ret = vmm->func->valid(vmm, argv, argc, map);
-		if (WARN_ON(ret))
-			return;
-	}
-
-	mutex_lock(&vmm->mutex);
-	nvkm_vmm_ptes_map(vmm, page, vma->node->addr + delta,
-				     vma->node->size, map, fn);
-	mutex_unlock(&vmm->mutex);
-
-	nvkm_memory_tags_put(vma->node->memory, vmm->mmu->subdev.device, &vma->node->tags);
-	nvkm_memory_unref(&vma->node->memory);
-	vma->node->memory = nvkm_memory_ref(map->memory);
-	vma->node->tags = map->tags;
-}
-
 void
 nvkm_mmu_ptc_dump(struct nvkm_mmu *mmu)
 {
@@ -312,138 +251,6 @@ nvkm_mmu_ptc_init(struct nvkm_mmu *mmu)
 	INIT_LIST_HEAD(&mmu->ptp.list);
 }
 
-void
-nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
-{
-	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
-	if (page->desc->func->unmap) {
-		struct nvkm_vmm_map map = { .mem = node->mem };
-		nvkm_vm_map_(page, vma, delta, node, page->desc->func->mem, &map);
-		return;
-	}
-}
-
-static void
-nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
-		     struct nvkm_mem *mem)
-{
-	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
-	if (page->desc->func->unmap) {
-		struct nvkm_vmm_map map = { .sgl = mem->sg->sgl };
-		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->sgl, &map);
-		return;
-	}
-}
-
-static void
-nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
-	       struct nvkm_mem *mem)
-{
-	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
-	if (page->desc->func->unmap) {
-		struct nvkm_vmm_map map = { .dma = mem->pages };
-		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->dma, &map);
-		return;
-	}
-}
-
-void
-nvkm_vm_map(struct nvkm_vma *vma, struct nvkm_mem *node)
-{
-	if (node->sg)
-		nvkm_vm_map_sg_table(vma, 0, node->size << 12, node);
-	else
-	if (node->pages)
-		nvkm_vm_map_sg(vma, 0, node->size << 12, node);
-	else
-		nvkm_vm_map_at(vma, 0, node);
-}
-
-void
-nvkm_vm_unmap(struct nvkm_vma *vma)
-{
-	nvkm_vmm_unmap(vma->vm, vma->node);
-}
-
-int
-nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
-	    struct nvkm_vma *vma)
-{
-	int ret;
-
-	mutex_lock(&vm->mutex);
-	ret = nvkm_vmm_get_locked(vm, true, false, false, page_shift, 0,
-				  size, &vma->node);
-	mutex_unlock(&vm->mutex);
-	if (ret)
-		return ret;
-
-	vma->memory = NULL;
-	vma->tags = NULL;
-	vma->vm = NULL;
-	nvkm_vm_ref(vm, &vma->vm, NULL);
-	vma->offset = vma->addr = vma->node->addr;
-	vma->access = access;
-	return 0;
-}
-
-void
-nvkm_vm_put(struct nvkm_vma *vma)
-{
-	nvkm_vmm_put(vma->vm, &vma->node);
-	nvkm_vm_ref(NULL, &vma->vm, NULL);
-}
-
-int
-nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
-{
-	return nvkm_vmm_boot(vm);
-}
-
-int
-nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
-	    struct lock_class_key *key, struct nvkm_vm **pvm)
-{
-	struct nvkm_mmu *mmu = device->mmu;
-
-	*pvm = NULL;
-	if (mmu->func->vmm.ctor) {
-		int ret = mmu->func->vmm.ctor(mmu, mm_offset,
-					      offset + length - mm_offset,
-					      NULL, 0, key, "legacy", pvm);
-		if (ret) {
-			nvkm_vm_ref(NULL, pvm, NULL);
-			return ret;
-		}
-
-		return ret;
-	}
-
-	return -EINVAL;
-}
-
-int
-nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
-{
-	if (ref) {
-		if (inst) {
-			int ret = nvkm_vmm_join(ref, inst);
-			if (ret)
-				return ret;
-		}
-
-		nvkm_vmm_ref(ref);
-	}
-
-	if (*ptr) {
-		nvkm_vmm_part(*ptr, inst);
-		nvkm_vmm_unref(ptr);
-	}
-
-	*ptr = ref;
-	return 0;
-}
-
 static void
 nvkm_mmu_type(struct nvkm_mmu *mmu, int heap, u8 type)
 {
@@ -611,9 +418,7 @@ nvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,
 {
 	nvkm_subdev_ctor(&nvkm_mmu, device, index, &mmu->subdev);
 	mmu->func = func;
-	mmu->limit = func->limit;
 	mmu->dma_bits = func->dma_bits;
-	mmu->lpg_shift = func->lpg_shift;
 	nvkm_mmu_ptc_init(mmu);
 	mmu->user.ctor = nvkm_ummu_new;
 	mmu->user.base = func->mmu.user;

commit eea5cf0f0170fbc54fbb3c501b0ec7cce7f68369
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: define user interfaces to mmu
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index be600049f221..e0fa0cae7960 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -21,7 +21,7 @@
  *
  * Authors: Ben Skeggs
  */
-#include "priv.h"
+#include "ummu.h"
 #include "vmm.h"
 
 #include <subdev/bar.h>
@@ -615,6 +615,8 @@ nvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,
 	mmu->dma_bits = func->dma_bits;
 	mmu->lpg_shift = func->lpg_shift;
 	nvkm_mmu_ptc_init(mmu);
+	mmu->user.ctor = nvkm_ummu_new;
+	mmu->user.base = func->mmu.user;
 }
 
 int

commit 51645eb71485dd3d72d9fe2acd0298057afdf437
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: build up information on available memory types
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 9bf688df24f0..be600049f221 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -24,6 +24,7 @@
 #include "priv.h"
 #include "vmm.h"
 
+#include <subdev/bar.h>
 #include <subdev/fb.h>
 
 #include <nvif/if500d.h>
@@ -443,11 +444,130 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 	return 0;
 }
 
+static void
+nvkm_mmu_type(struct nvkm_mmu *mmu, int heap, u8 type)
+{
+	if (heap >= 0 && !WARN_ON(mmu->type_nr == ARRAY_SIZE(mmu->type))) {
+		mmu->type[mmu->type_nr].type = type | mmu->heap[heap].type;
+		mmu->type[mmu->type_nr].heap = heap;
+		mmu->type_nr++;
+	}
+}
+
+static int
+nvkm_mmu_heap(struct nvkm_mmu *mmu, u8 type, u64 size)
+{
+	if (size) {
+		if (!WARN_ON(mmu->heap_nr == ARRAY_SIZE(mmu->heap))) {
+			mmu->heap[mmu->heap_nr].type = type;
+			mmu->heap[mmu->heap_nr].size = size;
+			return mmu->heap_nr++;
+		}
+	}
+	return -EINVAL;
+}
+
+static void
+nvkm_mmu_host(struct nvkm_mmu *mmu)
+{
+	struct nvkm_device *device = mmu->subdev.device;
+	u8 type = NVKM_MEM_KIND * !!mmu->func->kind_sys;
+	int heap;
+
+	/* Non-mappable system memory. */
+	heap = nvkm_mmu_heap(mmu, NVKM_MEM_HOST, ~0ULL);
+	nvkm_mmu_type(mmu, heap, type);
+
+	/* Non-coherent, cached, system memory.
+	 *
+	 * Block-linear mappings of system memory must be done through
+	 * BAR1, and cannot be supported on systems where we're unable
+	 * to map BAR1 with write-combining.
+	 */
+	type |= NVKM_MEM_MAPPABLE;
+	if (!device->bar || device->bar->iomap_uncached)
+		nvkm_mmu_type(mmu, heap, type & ~NVKM_MEM_KIND);
+	else
+		nvkm_mmu_type(mmu, heap, type);
+
+	/* Coherent, cached, system memory.
+	 *
+	 * Unsupported on systems that aren't able to support snooped
+	 * mappings, and also for block-linear mappings which must be
+	 * done through BAR1.
+	 */
+	type |= NVKM_MEM_COHERENT;
+	if (device->func->cpu_coherent)
+		nvkm_mmu_type(mmu, heap, type & ~NVKM_MEM_KIND);
+
+	/* Uncached system memory. */
+	nvkm_mmu_type(mmu, heap, type |= NVKM_MEM_UNCACHED);
+}
+
+static void
+nvkm_mmu_vram(struct nvkm_mmu *mmu)
+{
+	struct nvkm_device *device = mmu->subdev.device;
+	struct nvkm_mm *mm = &device->fb->ram->vram;
+	const u32 sizeN = nvkm_mm_heap_size(mm, NVKM_RAM_MM_NORMAL);
+	const u32 sizeU = nvkm_mm_heap_size(mm, NVKM_RAM_MM_NOMAP);
+	const u32 sizeM = nvkm_mm_heap_size(mm, NVKM_RAM_MM_MIXED);
+	u8 type = NVKM_MEM_KIND * !!mmu->func->kind;
+	u8 heap = NVKM_MEM_VRAM;
+	int heapM, heapN, heapU;
+
+	/* Mixed-memory doesn't support compression or display. */
+	heapM = nvkm_mmu_heap(mmu, heap, sizeM << NVKM_RAM_MM_SHIFT);
+
+	heap |= NVKM_MEM_COMP;
+	heap |= NVKM_MEM_DISP;
+	heapN = nvkm_mmu_heap(mmu, heap, sizeN << NVKM_RAM_MM_SHIFT);
+	heapU = nvkm_mmu_heap(mmu, heap, sizeU << NVKM_RAM_MM_SHIFT);
+
+	/* Add non-mappable VRAM types first so that they're preferred
+	 * over anything else.  Mixed-memory will be slower than other
+	 * heaps, it's prioritised last.
+	 */
+	nvkm_mmu_type(mmu, heapU, type);
+	nvkm_mmu_type(mmu, heapN, type);
+	nvkm_mmu_type(mmu, heapM, type);
+
+	/* Add host memory types next, under the assumption that users
+	 * wanting mappable memory want to use them as staging buffers
+	 * or the like.
+	 */
+	nvkm_mmu_host(mmu);
+
+	/* Mappable VRAM types go last, as they're basically the worst
+	 * possible type to ask for unless there's no other choice.
+	 */
+	if (device->bar) {
+		/* Write-combined BAR1 access. */
+		type |= NVKM_MEM_MAPPABLE;
+		if (!device->bar->iomap_uncached) {
+			nvkm_mmu_type(mmu, heapN, type);
+			nvkm_mmu_type(mmu, heapM, type);
+		}
+
+		/* Uncached BAR1 access. */
+		type |= NVKM_MEM_COHERENT;
+		type |= NVKM_MEM_UNCACHED;
+		nvkm_mmu_type(mmu, heapN, type);
+		nvkm_mmu_type(mmu, heapM, type);
+	}
+}
+
 static int
 nvkm_mmu_oneinit(struct nvkm_subdev *subdev)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
 
+	/* Determine available memory types. */
+	if (mmu->subdev.device->fb && mmu->subdev.device->fb->ram)
+		nvkm_mmu_vram(mmu);
+	else
+		nvkm_mmu_host(mmu);
+
 	if (mmu->func->vmm.global) {
 		int ret = nvkm_vmm_new(subdev->device, 0, 0, NULL, 0, NULL,
 				       "gart", &mmu->vmm);

commit f9463a4bc8ea2df5ea25c4d6e0be72011e559b95
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: implement new vmm frontend
    
    These are the new priviledged interfaces to the VMM backends, and expose
    some functionality that wasn't previously available.
    
    It's now possible to allocate a chunk of address-space (even all of it),
    without causing page tables to be allocated up-front, and then map into
    it at arbitrary locations.  This is the basic primitive used to support
    features such as sparse mapping, or to allow userspace control over its
    own address-space, or HMM (where the GPU driver isn't in control of the
    address-space layout).
    
    Rather than being tied to a subtle combination of memory object and VMA
    properties, arguments that control map flags (ro, kind, etc) are passed
    explicitly at map time.
    
    The compatibility hacks to implement the old frontend on top of the new
    driver backends have been replaced with something similar to implement
    the old frontend's interfaces on top of the new frontend.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 31832398f1e9..9bf688df24f0 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -266,14 +266,14 @@ nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
 	}
 
 	mutex_lock(&vmm->mutex);
-	nvkm_vmm_ptes_map(vmm, page, ((u64)vma->node->offset << 12) + delta,
-				      (u64)vma->node->length << 12, map, fn);
+	nvkm_vmm_ptes_map(vmm, page, vma->node->addr + delta,
+				     vma->node->size, map, fn);
 	mutex_unlock(&vmm->mutex);
 
-	nvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);
-	nvkm_memory_unref(&vma->memory);
-	vma->memory = nvkm_memory_ref(map->memory);
-	vma->tags = map->tags;
+	nvkm_memory_tags_put(vma->node->memory, vmm->mmu->subdev.device, &vma->node->tags);
+	nvkm_memory_unref(&vma->node->memory);
+	vma->node->memory = nvkm_memory_ref(map->memory);
+	vma->node->tags = map->tags;
 }
 
 void
@@ -314,11 +314,9 @@ nvkm_mmu_ptc_init(struct nvkm_mmu *mmu)
 void
 nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
-	const struct nvkm_vmm_page *page = vma->vm->func->page;
+	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .mem = node->mem };
-		while (page->shift != vma->node->type)
-			page++;
 		nvkm_vm_map_(page, vma, delta, node, page->desc->func->mem, &map);
 		return;
 	}
@@ -328,11 +326,9 @@ static void
 nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 		     struct nvkm_mem *mem)
 {
-	const struct nvkm_vmm_page *page = vma->vm->func->page;
+	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .sgl = mem->sg->sgl };
-		while (page->shift != vma->node->type)
-			page++;
 		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->sgl, &map);
 		return;
 	}
@@ -342,11 +338,9 @@ static void
 nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	       struct nvkm_mem *mem)
 {
-	const struct nvkm_vmm_page *page = vma->vm->func->page;
+	const struct nvkm_vmm_page *page = &vma->vm->func->page[vma->node->page];
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .dma = mem->pages };
-		while (page->shift != vma->node->type)
-			page++;
 		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->dma, &map);
 		return;
 	}
@@ -364,67 +358,30 @@ nvkm_vm_map(struct nvkm_vma *vma, struct nvkm_mem *node)
 		nvkm_vm_map_at(vma, 0, node);
 }
 
-void
-nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
-{
-	struct nvkm_vm *vm = vma->vm;
-	if (vm->func->page->desc->func->unmap) {
-		const struct nvkm_vmm_page *page = vm->func->page;
-		while (page->shift != vma->node->type)
-			page++;
-		mutex_lock(&vm->mutex);
-		nvkm_vmm_ptes_unmap(vm, page, (vma->node->offset << 12) + delta,
-					       vma->node->length << 12, false);
-		mutex_unlock(&vm->mutex);
-		return;
-	}
-}
-
 void
 nvkm_vm_unmap(struct nvkm_vma *vma)
 {
-	nvkm_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);
-
-	nvkm_memory_tags_put(vma->memory, vma->vm->mmu->subdev.device, &vma->tags);
-	nvkm_memory_unref(&vma->memory);
+	nvkm_vmm_unmap(vma->vm, vma->node);
 }
 
 int
 nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 	    struct nvkm_vma *vma)
 {
-	u32 align = (1 << page_shift) >> 12;
-	u32 msize = size >> 12;
 	int ret;
 
 	mutex_lock(&vm->mutex);
-	ret = nvkm_mm_head(&vm->mm, 0, page_shift, msize, msize, align,
-			   &vma->node);
-	if (unlikely(ret != 0)) {
-		mutex_unlock(&vm->mutex);
-		return ret;
-	}
-
-	if (vm->func->page->desc->func->unmap) {
-		const struct nvkm_vmm_page *page = vm->func->page;
-		while (page->shift != page_shift)
-			page++;
-
-		ret = nvkm_vmm_ptes_get(vm, page, vma->node->offset << 12,
-						  vma->node->length << 12);
-		if (ret) {
-			nvkm_mm_free(&vm->mm, &vma->node);
-			mutex_unlock(&vm->mutex);
-			return ret;
-		}
-	}
+	ret = nvkm_vmm_get_locked(vm, true, false, false, page_shift, 0,
+				  size, &vma->node);
 	mutex_unlock(&vm->mutex);
+	if (ret)
+		return ret;
 
 	vma->memory = NULL;
 	vma->tags = NULL;
 	vma->vm = NULL;
 	nvkm_vm_ref(vm, &vma->vm, NULL);
-	vma->offset = (u64)vma->node->offset << 12;
+	vma->offset = vma->addr = vma->node->addr;
 	vma->access = access;
 	return 0;
 }
@@ -432,30 +389,7 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 void
 nvkm_vm_put(struct nvkm_vma *vma)
 {
-	struct nvkm_mmu *mmu;
-	struct nvkm_vm *vm;
-
-	if (unlikely(vma->node == NULL))
-		return;
-	vm = vma->vm;
-	mmu = vm->mmu;
-
-	nvkm_memory_tags_put(vma->memory, mmu->subdev.device, &vma->tags);
-	nvkm_memory_unref(&vma->memory);
-
-	mutex_lock(&vm->mutex);
-	if (vm->func->page->desc->func->unmap) {
-		const struct nvkm_vmm_page *page = vm->func->page;
-		while (page->shift != vma->node->type)
-			page++;
-
-		nvkm_vmm_ptes_put(vm, page, vma->node->offset << 12,
-					    vma->node->length << 12);
-	}
-
-	nvkm_mm_free(&vm->mm, &vma->node);
-	mutex_unlock(&vm->mutex);
-
+	nvkm_vmm_put(vma->vm, &vma->node);
 	nvkm_vm_ref(NULL, &vma->vm, NULL);
 }
 
@@ -465,26 +399,6 @@ nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 	return nvkm_vmm_boot(vm);
 }
 
-static int
-nvkm_vm_legacy(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
-	       u32 block, struct nvkm_vm *vm)
-{
-	u64 mm_length = (offset + length) - mm_offset;
-	int ret;
-
-	kref_init(&vm->refcount);
-
-	if (block > length)
-		block = length;
-
-	ret = nvkm_mm_init(&vm->mm, 0, mm_offset >> 12, mm_length >> 12,
-			   block >> 12);
-	if (ret)
-		return ret;
-
-	return 0;
-}
-
 int
 nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 	    struct lock_class_key *key, struct nvkm_vm **pvm)
@@ -501,46 +415,28 @@ nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 			return ret;
 		}
 
-		ret = nvkm_vm_legacy(mmu, offset, length, mm_offset,
-				     (*pvm)->func->page_block ?
-				     (*pvm)->func->page_block : 4096, *pvm);
-		if (ret)
-			nvkm_vm_ref(NULL, pvm, NULL);
-
 		return ret;
 	}
 
 	return -EINVAL;
 }
 
-static void
-nvkm_vm_del(struct kref *kref)
-{
-	struct nvkm_vm *vm = container_of(kref, typeof(*vm), refcount);
-
-	nvkm_mm_fini(&vm->mm);
-	if (vm->func)
-		nvkm_vmm_dtor(vm);
-	kfree(vm);
-}
-
 int
 nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 {
 	if (ref) {
-		if (ref->func->join && inst) {
-			int ret = ref->func->join(ref, inst);
+		if (inst) {
+			int ret = nvkm_vmm_join(ref, inst);
 			if (ret)
 				return ret;
 		}
 
-		kref_get(&ref->refcount);
+		nvkm_vmm_ref(ref);
 	}
 
 	if (*ptr) {
-		if ((*ptr)->func->part && inst)
-			(*ptr)->func->part(*ptr, inst);
-		kref_put(&(*ptr)->refcount, nvkm_vm_del);
+		nvkm_vmm_part(*ptr, inst);
+		nvkm_vmm_unref(ptr);
 	}
 
 	*ptr = ref;
@@ -553,8 +449,8 @@ nvkm_mmu_oneinit(struct nvkm_subdev *subdev)
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
 
 	if (mmu->func->vmm.global) {
-		int ret = nvkm_vm_new(subdev->device, 0, mmu->limit, 0,
-				      NULL, &mmu->vmm);
+		int ret = nvkm_vmm_new(subdev->device, 0, 0, NULL, 0, NULL,
+				       "gart", &mmu->vmm);
 		if (ret)
 			return ret;
 	}
@@ -576,7 +472,7 @@ nvkm_mmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
 
-	nvkm_vm_ref(NULL, &mmu->vmm, NULL);
+	nvkm_vmm_unref(&mmu->vmm);
 
 	nvkm_mmu_ptc_fini(mmu);
 	return mmu;

commit 26880e76863ace2dd34c14fcadaedf97a2ace417
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: remove support for old backends
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 741021ff8c27..31832398f1e9 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -24,7 +24,6 @@
 #include "priv.h"
 #include "vmm.h"
 
-#include <core/gpuobj.h>
 #include <subdev/fb.h>
 
 #include <nvif/if500d.h>
@@ -316,17 +315,6 @@ void
 nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
 	const struct nvkm_vmm_page *page = vma->vm->func->page;
-	struct nvkm_vm *vm = vma->vm;
-	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_mm_node *r = node->mem;
-	int big = vma->node->type != mmu->func->spg_shift;
-	u32 offset = vma->node->offset + (delta >> 12);
-	u32 bits = vma->node->type - 12;
-	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->func->pgt_bits - bits);
-	u32 end, len;
-
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .mem = node->mem };
 		while (page->shift != vma->node->type)
@@ -334,36 +322,6 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 		nvkm_vm_map_(page, vma, delta, node, page->desc->func->mem, &map);
 		return;
 	}
-
-	delta = 0;
-	while (r) {
-		u64 phys = (u64)r->offset << 12;
-		u32 num  = r->length >> bits;
-
-		while (num) {
-			struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
-
-			end = (pte + num);
-			if (unlikely(end >= max))
-				end = max;
-			len = end - pte;
-
-			mmu->func->map(vma, pgt, node, pte, len, phys, delta);
-
-			num -= len;
-			pte += len;
-			if (unlikely(end >= max)) {
-				phys += len << (bits + 12);
-				pde++;
-				pte = 0;
-			}
-
-			delta += (u64)len << vma->node->type;
-		}
-		r = r->next;
-	}
-
-	mmu->func->flush(vm);
 }
 
 static void
@@ -371,20 +329,6 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 		     struct nvkm_mem *mem)
 {
 	const struct nvkm_vmm_page *page = vma->vm->func->page;
-	struct nvkm_vm *vm = vma->vm;
-	struct nvkm_mmu *mmu = vm->mmu;
-	int big = vma->node->type != mmu->func->spg_shift;
-	u32 offset = vma->node->offset + (delta >> 12);
-	u32 bits = vma->node->type - 12;
-	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->func->pgt_bits - bits);
-	unsigned m, sglen;
-	u32 end, len;
-	int i;
-	struct scatterlist *sg;
-
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .sgl = mem->sg->sgl };
 		while (page->shift != vma->node->type)
@@ -392,45 +336,6 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->sgl, &map);
 		return;
 	}
-
-	for_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {
-		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
-		sglen = sg_dma_len(sg) >> PAGE_SHIFT;
-
-		end = pte + sglen;
-		if (unlikely(end >= max))
-			end = max;
-		len = end - pte;
-
-		for (m = 0; m < len; m++) {
-			dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
-
-			mmu->func->map_sg(vma, pgt, mem, pte, 1, &addr);
-			num--;
-			pte++;
-
-			if (num == 0)
-				goto finish;
-		}
-		if (unlikely(end >= max)) {
-			pde++;
-			pte = 0;
-		}
-		if (m < sglen) {
-			for (; m < sglen; m++) {
-				dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
-
-				mmu->func->map_sg(vma, pgt, mem, pte, 1, &addr);
-				num--;
-				pte++;
-				if (num == 0)
-					goto finish;
-			}
-		}
-
-	}
-finish:
-	mmu->func->flush(vm);
 }
 
 static void
@@ -438,18 +343,6 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	       struct nvkm_mem *mem)
 {
 	const struct nvkm_vmm_page *page = vma->vm->func->page;
-	struct nvkm_vm *vm = vma->vm;
-	struct nvkm_mmu *mmu = vm->mmu;
-	dma_addr_t *list = mem->pages;
-	int big = vma->node->type != mmu->func->spg_shift;
-	u32 offset = vma->node->offset + (delta >> 12);
-	u32 bits = vma->node->type - 12;
-	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->func->pgt_bits - bits);
-	u32 end, len;
-
 	if (page->desc->func->unmap) {
 		struct nvkm_vmm_map map = { .dma = mem->pages };
 		while (page->shift != vma->node->type)
@@ -457,27 +350,6 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->dma, &map);
 		return;
 	}
-
-	while (num) {
-		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
-
-		end = (pte + num);
-		if (unlikely(end >= max))
-			end = max;
-		len = end - pte;
-
-		mmu->func->map_sg(vma, pgt, mem, pte, len, list);
-
-		num  -= len;
-		pte  += len;
-		list += len;
-		if (unlikely(end >= max)) {
-			pde++;
-			pte = 0;
-		}
-	}
-
-	mmu->func->flush(vm);
 }
 
 void
@@ -496,16 +368,6 @@ void
 nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 {
 	struct nvkm_vm *vm = vma->vm;
-	struct nvkm_mmu *mmu = vm->mmu;
-	int big = vma->node->type != mmu->func->spg_shift;
-	u32 offset = vma->node->offset + (delta >> 12);
-	u32 bits = vma->node->type - 12;
-	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->func->pgt_bits - bits);
-	u32 end, len;
-
 	if (vm->func->page->desc->func->unmap) {
 		const struct nvkm_vmm_page *page = vm->func->page;
 		while (page->shift != vma->node->type)
@@ -516,26 +378,6 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 		mutex_unlock(&vm->mutex);
 		return;
 	}
-
-	while (num) {
-		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
-
-		end = (pte + num);
-		if (unlikely(end >= max))
-			end = max;
-		len = end - pte;
-
-		mmu->func->unmap(vma, pgt, pte, len);
-
-		num -= len;
-		pte += len;
-		if (unlikely(end >= max)) {
-			pde++;
-			pte = 0;
-		}
-	}
-
-	mmu->func->flush(vm);
 }
 
 void
@@ -547,63 +389,12 @@ nvkm_vm_unmap(struct nvkm_vma *vma)
 	nvkm_memory_unref(&vma->memory);
 }
 
-static void
-nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
-{
-	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_vm_pgt *vpgt;
-	struct nvkm_memory *pgt;
-	u32 pde;
-
-	for (pde = fpde; pde <= lpde; pde++) {
-		vpgt = &vm->pgt[pde - vm->fpde];
-		if (--vpgt->refcount[big])
-			continue;
-
-		pgt = vpgt->mem[big];
-		vpgt->mem[big] = NULL;
-
-		if (mmu->func->map_pgt)
-			mmu->func->map_pgt(vm, pde, vpgt->mem);
-
-		mmu->func->flush(vm);
-
-		nvkm_memory_unref(&pgt);
-	}
-}
-
-static int
-nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
-{
-	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
-	int big = (type != mmu->func->spg_shift);
-	u32 pgt_size;
-	int ret;
-
-	pgt_size  = (1 << (mmu->func->pgt_bits + 12)) >> type;
-	pgt_size *= 8;
-
-	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
-			      pgt_size, 0x1000, true, &vpgt->mem[big]);
-	if (unlikely(ret))
-		return ret;
-
-	if (mmu->func->map_pgt)
-		mmu->func->map_pgt(vm, pde, vpgt->mem);
-
-	vpgt->refcount[big]++;
-	return 0;
-}
-
 int
 nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 	    struct nvkm_vma *vma)
 {
-	struct nvkm_mmu *mmu = vm->mmu;
 	u32 align = (1 << page_shift) >> 12;
 	u32 msize = size >> 12;
-	u32 fpde, lpde, pde;
 	int ret;
 
 	mutex_lock(&vm->mutex);
@@ -626,32 +417,7 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 			mutex_unlock(&vm->mutex);
 			return ret;
 		}
-
-		goto done;
-	}
-
-	fpde = (vma->node->offset >> mmu->func->pgt_bits);
-	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
-
-	for (pde = fpde; pde <= lpde; pde++) {
-		struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
-		int big = (vma->node->type != mmu->func->spg_shift);
-
-		if (likely(vpgt->refcount[big])) {
-			vpgt->refcount[big]++;
-			continue;
-		}
-
-		ret = nvkm_vm_map_pgt(vm, pde, vma->node->type);
-		if (ret) {
-			if (pde != fpde)
-				nvkm_vm_unmap_pgt(vm, big, fpde, pde - 1);
-			nvkm_mm_free(&vm->mm, &vma->node);
-			mutex_unlock(&vm->mutex);
-			return ret;
-		}
 	}
-done:
 	mutex_unlock(&vm->mutex);
 
 	vma->memory = NULL;
@@ -668,7 +434,6 @@ nvkm_vm_put(struct nvkm_vma *vma)
 {
 	struct nvkm_mmu *mmu;
 	struct nvkm_vm *vm;
-	u32 fpde, lpde;
 
 	if (unlikely(vma->node == NULL))
 		return;
@@ -678,9 +443,6 @@ nvkm_vm_put(struct nvkm_vma *vma)
 	nvkm_memory_tags_put(vma->memory, mmu->subdev.device, &vma->tags);
 	nvkm_memory_unref(&vma->memory);
 
-	fpde = (vma->node->offset >> mmu->func->pgt_bits);
-	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
-
 	mutex_lock(&vm->mutex);
 	if (vm->func->page->desc->func->unmap) {
 		const struct nvkm_vmm_page *page = vm->func->page;
@@ -689,11 +451,8 @@ nvkm_vm_put(struct nvkm_vma *vma)
 
 		nvkm_vmm_ptes_put(vm, page, vma->node->offset << 12,
 					    vma->node->length << 12);
-		goto done;
 	}
 
-	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->func->spg_shift, fpde, lpde);
-done:
 	nvkm_mm_free(&vm->mm, &vma->node);
 	mutex_unlock(&vm->mutex);
 
@@ -703,23 +462,7 @@ nvkm_vm_put(struct nvkm_vma *vma)
 int
 nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 {
-	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_memory *pgt;
-	int ret;
-
-	if (vm->func->page->desc->func->unmap)
-		return nvkm_vmm_boot(vm);
-
-	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
-			      (size >> mmu->func->spg_shift) * 8, 0x1000, true, &pgt);
-	if (ret == 0) {
-		vm->pgt[0].refcount[0] = 1;
-		vm->pgt[0].mem[0] = pgt;
-		nvkm_memory_boot(pgt, vm);
-		vm->bootstrapped = true;
-	}
-
-	return ret;
+	return nvkm_vmm_boot(vm);
 }
 
 static int
@@ -730,24 +473,14 @@ nvkm_vm_legacy(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	int ret;
 
 	kref_init(&vm->refcount);
-	vm->fpde = offset >> (mmu->func->pgt_bits + 12);
-	vm->lpde = (offset + length - 1) >> (mmu->func->pgt_bits + 12);
-
-	vm->pgt  = vzalloc((vm->lpde - vm->fpde + 1) * sizeof(*vm->pgt));
-	if (!vm->pgt) {
-		kfree(vm);
-		return -ENOMEM;
-	}
 
 	if (block > length)
 		block = length;
 
 	ret = nvkm_mm_init(&vm->mm, 0, mm_offset >> 12, mm_length >> 12,
 			   block >> 12);
-	if (ret) {
-		vfree(vm->pgt);
+	if (ret)
 		return ret;
-	}
 
 	return 0;
 }
@@ -786,7 +519,6 @@ nvkm_vm_del(struct kref *kref)
 	struct nvkm_vm *vm = container_of(kref, typeof(*vm), refcount);
 
 	nvkm_mm_fini(&vm->mm);
-	vfree(vm->pgt);
 	if (vm->func)
 		nvkm_vmm_dtor(vm);
 	kfree(vm);
@@ -797,14 +529,9 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 {
 	if (ref) {
 		if (ref->func->join && inst) {
-			int ret = ref->func->join(ref, inst), i;
+			int ret = ref->func->join(ref, inst);
 			if (ret)
 				return ret;
-
-			if (!ref->func->page->desc->func->unmap && ref->mmu->func->map_pgt) {
-				for (i = ref->fpde; i <= ref->lpde; i++)
-					ref->mmu->func->map_pgt(ref, i, ref->pgt[i - ref->fpde].mem);
-			}
 		}
 
 		kref_get(&ref->refcount);
@@ -813,12 +540,6 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 	if (*ptr) {
 		if ((*ptr)->func->part && inst)
 			(*ptr)->func->part(*ptr, inst);
-		if ((*ptr)->bootstrapped && inst) {
-			if (!(*ptr)->func->page->desc->func->unmap) {
-				nvkm_memory_unref(&(*ptr)->pgt[0].mem[0]);
-				(*ptr)->bootstrapped = false;
-			}
-		}
 		kref_put(&(*ptr)->refcount, nvkm_vm_del);
 	}
 
@@ -838,9 +559,6 @@ nvkm_mmu_oneinit(struct nvkm_subdev *subdev)
 			return ret;
 	}
 
-	if (mmu->func->oneinit)
-		return mmu->func->oneinit(mmu);
-
 	return 0;
 }
 

commit b77791da0ee009dcb1813d8b00919962b6a5f851
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/gf100: implement new vmm backend
    
    Adds support for:
    - 64KiB big page size.
    - System-memory PTs.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 4d75dcb41468..741021ff8c27 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -28,6 +28,7 @@
 #include <subdev/fb.h>
 
 #include <nvif/if500d.h>
+#include <nvif/if900d.h>
 
 struct nvkm_mmu_ptp {
 	struct nvkm_mmu_pt *pt;
@@ -222,6 +223,7 @@ nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
 {
 	union {
 		struct nv50_vmm_map_v0 nv50;
+		struct gf100_vmm_map_v0 gf100;
 	} args;
 	struct nvkm_vmm *vmm = vma->vm;
 	void *argv = NULL;
@@ -242,6 +244,19 @@ nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
 			argv = &args.nv50;
 			argc = sizeof(args.nv50);
 			break;
+		case NV_C0:
+		case NV_E0:
+		case GM100:
+		case GP100: {
+			args.gf100.version = 0;
+			args.gf100.vol = (nvkm_memory_target(map->memory) != NVKM_MEM_TARGET_VRAM);
+			args.gf100.ro = !(vma->access & NV_MEM_ACCESS_WO);
+			args.gf100.priv = !!(vma->access & NV_MEM_ACCESS_SYS);
+			args.gf100.kind = (mem->memtype & 0x0ff);
+			argv = &args.gf100;
+			argc = sizeof(args.gf100);
+		}
+			break;
 		default:
 			break;
 		}

commit fd542a3e525c9f7a7de186cb24208c035bcea2d1
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/nv50,g84: implement new vmm backend
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 536187952372..4d75dcb41468 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -27,6 +27,8 @@
 #include <core/gpuobj.h>
 #include <subdev/fb.h>
 
+#include <nvif/if500d.h>
+
 struct nvkm_mmu_ptp {
 	struct nvkm_mmu_pt *pt;
 	struct list_head head;
@@ -218,6 +220,9 @@ nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
 	     struct nvkm_mem *mem, nvkm_vmm_pte_func fn,
 	     struct nvkm_vmm_map *map)
 {
+	union {
+		struct nv50_vmm_map_v0 nv50;
+	} args;
 	struct nvkm_vmm *vmm = vma->vm;
 	void *argv = NULL;
 	u32 argc = 0;
@@ -227,6 +232,20 @@ nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
 	map->page = page;
 
 	if (vmm->func->valid) {
+		switch (vmm->mmu->subdev.device->card_type) {
+		case NV_50:
+			args.nv50.version = 0;
+			args.nv50.ro = !(vma->access & NV_MEM_ACCESS_WO);
+			args.nv50.priv = !!(vma->access & NV_MEM_ACCESS_SYS);
+			args.nv50.kind = (mem->memtype & 0x07f);
+			args.nv50.comp = (mem->memtype & 0x180) >> 7;
+			argv = &args.nv50;
+			argc = sizeof(args.nv50);
+			break;
+		default:
+			break;
+		}
+
 		ret = vmm->func->valid(vmm, argv, argc, map);
 		if (WARN_ON(ret))
 			return;

commit eb813999f20097d24310836dfa07a97e2eb0c936
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: implement new vmm backend
    
    This is the common code to support a rework of the VMM backends.
    
    It adds support for more than 2 levels of page table nesting, which
    is required to be able to support GP100's MMU layout.
    
    Sparse mappings (that don't cause MMU faults when accessed) are now
    supported, where the backend provides it.
    
    Dual-PT handling had to become more sophisticated to support sparse,
    but this also allows us to support an optimisation the MMU provides
    on GK104 and newer.
    
    Certain operations can now be combined into a single page tree walk
    to avoid some overhead, but also enables optimsations like skipping
    PTE unmap writes when the PT will be destroyed anyway.
    
    The old backend has been hacked up to forward requests onto the new
    backend, if present, so that it's possible to bisect between issues
    in the backend changes vs the upcoming frontend changes.
    
    Until the new frontend has been merged, new backends will leak BAR2
    page tables on module unload.  This is expected, and it's not worth
    the effort of hacking around this as it doesn't effect runtime.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 22264d3db22f..536187952372 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -213,6 +213,36 @@ nvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)
 	return pt;
 }
 
+static void
+nvkm_vm_map_(const struct nvkm_vmm_page *page, struct nvkm_vma *vma, u64 delta,
+	     struct nvkm_mem *mem, nvkm_vmm_pte_func fn,
+	     struct nvkm_vmm_map *map)
+{
+	struct nvkm_vmm *vmm = vma->vm;
+	void *argv = NULL;
+	u32 argc = 0;
+	int ret;
+
+	map->memory = mem->memory;
+	map->page = page;
+
+	if (vmm->func->valid) {
+		ret = vmm->func->valid(vmm, argv, argc, map);
+		if (WARN_ON(ret))
+			return;
+	}
+
+	mutex_lock(&vmm->mutex);
+	nvkm_vmm_ptes_map(vmm, page, ((u64)vma->node->offset << 12) + delta,
+				      (u64)vma->node->length << 12, map, fn);
+	mutex_unlock(&vmm->mutex);
+
+	nvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);
+	nvkm_memory_unref(&vma->memory);
+	vma->memory = nvkm_memory_ref(map->memory);
+	vma->tags = map->tags;
+}
+
 void
 nvkm_mmu_ptc_dump(struct nvkm_mmu *mmu)
 {
@@ -251,6 +281,7 @@ nvkm_mmu_ptc_init(struct nvkm_mmu *mmu)
 void
 nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
+	const struct nvkm_vmm_page *page = vma->vm->func->page;
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_mm_node *r = node->mem;
@@ -262,6 +293,14 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
+	if (page->desc->func->unmap) {
+		struct nvkm_vmm_map map = { .mem = node->mem };
+		while (page->shift != vma->node->type)
+			page++;
+		nvkm_vm_map_(page, vma, delta, node, page->desc->func->mem, &map);
+		return;
+	}
+
 	delta = 0;
 	while (r) {
 		u64 phys = (u64)r->offset << 12;
@@ -297,6 +336,7 @@ static void
 nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 		     struct nvkm_mem *mem)
 {
+	const struct nvkm_vmm_page *page = vma->vm->func->page;
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
 	int big = vma->node->type != mmu->func->spg_shift;
@@ -311,6 +351,14 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 	int i;
 	struct scatterlist *sg;
 
+	if (page->desc->func->unmap) {
+		struct nvkm_vmm_map map = { .sgl = mem->sg->sgl };
+		while (page->shift != vma->node->type)
+			page++;
+		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->sgl, &map);
+		return;
+	}
+
 	for_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {
 		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 		sglen = sg_dma_len(sg) >> PAGE_SHIFT;
@@ -355,6 +403,7 @@ static void
 nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	       struct nvkm_mem *mem)
 {
+	const struct nvkm_vmm_page *page = vma->vm->func->page;
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
 	dma_addr_t *list = mem->pages;
@@ -367,6 +416,14 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
+	if (page->desc->func->unmap) {
+		struct nvkm_vmm_map map = { .dma = mem->pages };
+		while (page->shift != vma->node->type)
+			page++;
+		nvkm_vm_map_(page, vma, delta, mem, page->desc->func->dma, &map);
+		return;
+	}
+
 	while (num) {
 		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 
@@ -415,6 +472,17 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
+	if (vm->func->page->desc->func->unmap) {
+		const struct nvkm_vmm_page *page = vm->func->page;
+		while (page->shift != vma->node->type)
+			page++;
+		mutex_lock(&vm->mutex);
+		nvkm_vmm_ptes_unmap(vm, page, (vma->node->offset << 12) + delta,
+					       vma->node->length << 12, false);
+		mutex_unlock(&vm->mutex);
+		return;
+	}
+
 	while (num) {
 		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 
@@ -440,6 +508,9 @@ void
 nvkm_vm_unmap(struct nvkm_vma *vma)
 {
 	nvkm_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);
+
+	nvkm_memory_tags_put(vma->memory, vma->vm->mmu->subdev.device, &vma->tags);
+	nvkm_memory_unref(&vma->memory);
 }
 
 static void
@@ -509,6 +580,22 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 		return ret;
 	}
 
+	if (vm->func->page->desc->func->unmap) {
+		const struct nvkm_vmm_page *page = vm->func->page;
+		while (page->shift != page_shift)
+			page++;
+
+		ret = nvkm_vmm_ptes_get(vm, page, vma->node->offset << 12,
+						  vma->node->length << 12);
+		if (ret) {
+			nvkm_mm_free(&vm->mm, &vma->node);
+			mutex_unlock(&vm->mutex);
+			return ret;
+		}
+
+		goto done;
+	}
+
 	fpde = (vma->node->offset >> mmu->func->pgt_bits);
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
 
@@ -530,8 +617,11 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 			return ret;
 		}
 	}
+done:
 	mutex_unlock(&vm->mutex);
 
+	vma->memory = NULL;
+	vma->tags = NULL;
 	vma->vm = NULL;
 	nvkm_vm_ref(vm, &vma->vm, NULL);
 	vma->offset = (u64)vma->node->offset << 12;
@@ -551,11 +641,25 @@ nvkm_vm_put(struct nvkm_vma *vma)
 	vm = vma->vm;
 	mmu = vm->mmu;
 
+	nvkm_memory_tags_put(vma->memory, mmu->subdev.device, &vma->tags);
+	nvkm_memory_unref(&vma->memory);
+
 	fpde = (vma->node->offset >> mmu->func->pgt_bits);
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
 
 	mutex_lock(&vm->mutex);
+	if (vm->func->page->desc->func->unmap) {
+		const struct nvkm_vmm_page *page = vm->func->page;
+		while (page->shift != vma->node->type)
+			page++;
+
+		nvkm_vmm_ptes_put(vm, page, vma->node->offset << 12,
+					    vma->node->length << 12);
+		goto done;
+	}
+
 	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->func->spg_shift, fpde, lpde);
+done:
 	nvkm_mm_free(&vm->mm, &vma->node);
 	mutex_unlock(&vm->mutex);
 
@@ -569,6 +673,9 @@ nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 	struct nvkm_memory *pgt;
 	int ret;
 
+	if (vm->func->page->desc->func->unmap)
+		return nvkm_vmm_boot(vm);
+
 	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
 			      (size >> mmu->func->spg_shift) * 8, 0x1000, true, &pgt);
 	if (ret == 0) {
@@ -660,7 +767,7 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 			if (ret)
 				return ret;
 
-			if (ref->mmu->func->map_pgt) {
+			if (!ref->func->page->desc->func->unmap && ref->mmu->func->map_pgt) {
 				for (i = ref->fpde; i <= ref->lpde; i++)
 					ref->mmu->func->map_pgt(ref, i, ref->pgt[i - ref->fpde].mem);
 			}
@@ -672,8 +779,12 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 	if (*ptr) {
 		if ((*ptr)->func->part && inst)
 			(*ptr)->func->part(*ptr, inst);
-		if ((*ptr)->bootstrapped && inst)
-			nvkm_memory_unref(&(*ptr)->pgt[0].mem[0]);
+		if ((*ptr)->bootstrapped && inst) {
+			if (!(*ptr)->func->page->desc->func->unmap) {
+				nvkm_memory_unref(&(*ptr)->pgt[0].mem[0]);
+				(*ptr)->bootstrapped = false;
+			}
+		}
 		kref_put(&(*ptr)->refcount, nvkm_vm_del);
 	}
 

commit d30af7ce2c96e57b503da1d70454818331f0a6d5
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: handle instance block setup
    
    We previously required each VMM user to allocate their own page directory
    and fill in the instance block themselves.
    
    It makes more sense to handle this in a common location.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 9fdd1446da5f..22264d3db22f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -446,7 +446,6 @@ static void
 nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 {
 	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_vm_pgd *vpgd;
 	struct nvkm_vm_pgt *vpgt;
 	struct nvkm_memory *pgt;
 	u32 pde;
@@ -459,9 +458,8 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 		pgt = vpgt->mem[big];
 		vpgt->mem[big] = NULL;
 
-		list_for_each_entry(vpgd, &vm->pgd_list, head) {
-			mmu->func->map_pgt(vpgd->obj, pde, vpgt->mem);
-		}
+		if (mmu->func->map_pgt)
+			mmu->func->map_pgt(vm, pde, vpgt->mem);
 
 		mmu->func->flush(vm);
 
@@ -474,7 +472,6 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 {
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
-	struct nvkm_vm_pgd *vpgd;
 	int big = (type != mmu->func->spg_shift);
 	u32 pgt_size;
 	int ret;
@@ -487,9 +484,8 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 	if (unlikely(ret))
 		return ret;
 
-	list_for_each_entry(vpgd, &vm->pgd_list, head) {
-		mmu->func->map_pgt(vpgd->obj, pde, vpgt->mem);
-	}
+	if (mmu->func->map_pgt)
+		mmu->func->map_pgt(vm, pde, vpgt->mem);
 
 	vpgt->refcount[big]++;
 	return 0;
@@ -592,7 +588,6 @@ nvkm_vm_legacy(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	u64 mm_length = (offset + length) - mm_offset;
 	int ret;
 
-	INIT_LIST_HEAD(&vm->pgd_list);
 	kref_init(&vm->refcount);
 	vm->fpde = offset >> (mmu->func->pgt_bits + 12);
 	vm->lpde = (offset + length - 1) >> (mmu->func->pgt_bits + 12);
@@ -644,58 +639,10 @@ nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 	return -EINVAL;
 }
 
-static int
-nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
-{
-	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_vm_pgd *vpgd;
-	int i;
-
-	if (!pgd)
-		return 0;
-
-	vpgd = kzalloc(sizeof(*vpgd), GFP_KERNEL);
-	if (!vpgd)
-		return -ENOMEM;
-
-	vpgd->obj = pgd;
-
-	mutex_lock(&vm->mutex);
-	for (i = vm->fpde; i <= vm->lpde; i++)
-		mmu->func->map_pgt(pgd, i, vm->pgt[i - vm->fpde].mem);
-	list_add(&vpgd->head, &vm->pgd_list);
-	mutex_unlock(&vm->mutex);
-	return 0;
-}
-
-static void
-nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
-{
-	struct nvkm_vm_pgd *vpgd, *tmp;
-
-	if (!mpgd)
-		return;
-
-	mutex_lock(&vm->mutex);
-	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
-		if (vpgd->obj == mpgd) {
-			list_del(&vpgd->head);
-			kfree(vpgd);
-			break;
-		}
-	}
-	mutex_unlock(&vm->mutex);
-}
-
 static void
 nvkm_vm_del(struct kref *kref)
 {
 	struct nvkm_vm *vm = container_of(kref, typeof(*vm), refcount);
-	struct nvkm_vm_pgd *vpgd, *tmp;
-
-	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
-		nvkm_vm_unlink(vm, vpgd->obj);
-	}
 
 	nvkm_mm_fini(&vm->mm);
 	vfree(vm->pgt);
@@ -705,20 +652,28 @@ nvkm_vm_del(struct kref *kref)
 }
 
 int
-nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_gpuobj *pgd)
+nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_memory *inst)
 {
 	if (ref) {
-		int ret = nvkm_vm_link(ref, pgd);
-		if (ret)
-			return ret;
+		if (ref->func->join && inst) {
+			int ret = ref->func->join(ref, inst), i;
+			if (ret)
+				return ret;
+
+			if (ref->mmu->func->map_pgt) {
+				for (i = ref->fpde; i <= ref->lpde; i++)
+					ref->mmu->func->map_pgt(ref, i, ref->pgt[i - ref->fpde].mem);
+			}
+		}
 
 		kref_get(&ref->refcount);
 	}
 
 	if (*ptr) {
-		if ((*ptr)->bootstrapped && pgd)
+		if ((*ptr)->func->part && inst)
+			(*ptr)->func->part(*ptr, inst);
+		if ((*ptr)->bootstrapped && inst)
 			nvkm_memory_unref(&(*ptr)->pgt[0].mem[0]);
-		nvkm_vm_unlink(*ptr, pgd);
 		kref_put(&(*ptr)->refcount, nvkm_vm_del);
 	}
 

commit af3b8d53869c175fce424b6bfd1f49c1b53baef1
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: remove old vm creation hooks
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 1965a52ebe60..9fdd1446da5f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -616,31 +616,6 @@ nvkm_vm_legacy(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	return 0;
 }
 
-int
-nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
-	       u32 block, struct lock_class_key *key, struct nvkm_vm **pvm)
-{
-	static struct lock_class_key _key;
-	struct nvkm_vm *vm;
-	int ret;
-
-	vm = kzalloc(sizeof(*vm), GFP_KERNEL);
-	if (!vm)
-		return -ENOMEM;
-
-	__mutex_init(&vm->mutex, "&vm->mutex", key ? key : &_key);
-	vm->mmu = mmu;
-
-	ret = nvkm_vm_legacy(mmu, offset, length, mm_offset, block, vm);
-	if (ret) {
-		kfree(vm);
-		return ret;
-	}
-
-	*pvm = vm;
-	return 0;
-}
-
 int
 nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 	    struct lock_class_key *key, struct nvkm_vm **pvm)
@@ -666,10 +641,7 @@ nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 		return ret;
 	}
 
-	if (!mmu->func->create)
-		return -EINVAL;
-
-	return mmu->func->create(mmu, offset, length, mm_offset, key, pvm);
+	return -EINVAL;
 }
 
 static int

commit 9f6219fde7457df7a982174d496a012f4f42e776
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/nv50,g84: implement vmm on top of new base
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 0b4cb7b6a81f..1965a52ebe60 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -727,7 +727,6 @@ nvkm_vm_del(struct kref *kref)
 
 	nvkm_mm_fini(&vm->mm);
 	vfree(vm->pgt);
-
 	if (vm->func)
 		nvkm_vmm_dtor(vm);
 	kfree(vm);

commit 03b0ba7b545ba0c5b19fedb14a771a3517a1328e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu/nv44: implement vmm on top of new base
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index d55ec0e85dca..0b4cb7b6a81f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -786,14 +786,11 @@ static void *
 nvkm_mmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
-	void *data = mmu;
 
-	if (mmu->func->dtor)
-		data = mmu->func->dtor(mmu);
 	nvkm_vm_ref(NULL, &mmu->vmm, NULL);
 
 	nvkm_mmu_ptc_fini(mmu);
-	return data;
+	return mmu;
 }
 
 static const struct nvkm_subdev_func

commit 806a7335653743a33f476a3705d55bada95b7dfe
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: implement base for new vm management
    
    This is the first chunk of the new VMM code that provides the structures
    needed to describe a GPU virtual address-space layout, as well as common
    interfaces to handle VMM creation, and connecting instances to a VMM.
    
    The constructor now allocates the PD itself, rather than having the user
    handle that manually.  This won't/can't be used until after all backends
    have been ported to these interfaces, so a little bit of memory will be
    wasted on Fermi and newer for a couple of commits in the series.
    
    Compatibility has been hacked into the old code to allow each GPU backend
    to be ported individually.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 1bdae020057e..d55ec0e85dca 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -22,6 +22,7 @@
  * Authors: Ben Skeggs
  */
 #include "priv.h"
+#include "vmm.h"
 
 #include <core/gpuobj.h>
 #include <subdev/fb.h>
@@ -584,22 +585,14 @@ nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 	return ret;
 }
 
-int
-nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
-	       u32 block, struct lock_class_key *key, struct nvkm_vm **pvm)
+static int
+nvkm_vm_legacy(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
+	       u32 block, struct nvkm_vm *vm)
 {
-	static struct lock_class_key _key;
-	struct nvkm_vm *vm;
 	u64 mm_length = (offset + length) - mm_offset;
 	int ret;
 
-	vm = kzalloc(sizeof(*vm), GFP_KERNEL);
-	if (!vm)
-		return -ENOMEM;
-
-	__mutex_init(&vm->mutex, "&vm->mutex", key ? key : &_key);
 	INIT_LIST_HEAD(&vm->pgd_list);
-	vm->mmu = mmu;
 	kref_init(&vm->refcount);
 	vm->fpde = offset >> (mmu->func->pgt_bits + 12);
 	vm->lpde = (offset + length - 1) >> (mmu->func->pgt_bits + 12);
@@ -610,16 +603,41 @@ nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 		return -ENOMEM;
 	}
 
+	if (block > length)
+		block = length;
+
 	ret = nvkm_mm_init(&vm->mm, 0, mm_offset >> 12, mm_length >> 12,
 			   block >> 12);
 	if (ret) {
 		vfree(vm->pgt);
+		return ret;
+	}
+
+	return 0;
+}
+
+int
+nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
+	       u32 block, struct lock_class_key *key, struct nvkm_vm **pvm)
+{
+	static struct lock_class_key _key;
+	struct nvkm_vm *vm;
+	int ret;
+
+	vm = kzalloc(sizeof(*vm), GFP_KERNEL);
+	if (!vm)
+		return -ENOMEM;
+
+	__mutex_init(&vm->mutex, "&vm->mutex", key ? key : &_key);
+	vm->mmu = mmu;
+
+	ret = nvkm_vm_legacy(mmu, offset, length, mm_offset, block, vm);
+	if (ret) {
 		kfree(vm);
 		return ret;
 	}
 
 	*pvm = vm;
-
 	return 0;
 }
 
@@ -628,8 +646,29 @@ nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 	    struct lock_class_key *key, struct nvkm_vm **pvm)
 {
 	struct nvkm_mmu *mmu = device->mmu;
+
+	*pvm = NULL;
+	if (mmu->func->vmm.ctor) {
+		int ret = mmu->func->vmm.ctor(mmu, mm_offset,
+					      offset + length - mm_offset,
+					      NULL, 0, key, "legacy", pvm);
+		if (ret) {
+			nvkm_vm_ref(NULL, pvm, NULL);
+			return ret;
+		}
+
+		ret = nvkm_vm_legacy(mmu, offset, length, mm_offset,
+				     (*pvm)->func->page_block ?
+				     (*pvm)->func->page_block : 4096, *pvm);
+		if (ret)
+			nvkm_vm_ref(NULL, pvm, NULL);
+
+		return ret;
+	}
+
 	if (!mmu->func->create)
 		return -EINVAL;
+
 	return mmu->func->create(mmu, offset, length, mm_offset, key, pvm);
 }
 
@@ -688,6 +727,9 @@ nvkm_vm_del(struct kref *kref)
 
 	nvkm_mm_fini(&vm->mm);
 	vfree(vm->pgt);
+
+	if (vm->func)
+		nvkm_vmm_dtor(vm);
 	kfree(vm);
 }
 
@@ -717,8 +759,17 @@ static int
 nvkm_mmu_oneinit(struct nvkm_subdev *subdev)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
+
+	if (mmu->func->vmm.global) {
+		int ret = nvkm_vm_new(subdev->device, 0, mmu->limit, 0,
+				      NULL, &mmu->vmm);
+		if (ret)
+			return ret;
+	}
+
 	if (mmu->func->oneinit)
 		return mmu->func->oneinit(mmu);
+
 	return 0;
 }
 
@@ -739,6 +790,7 @@ nvkm_mmu_dtor(struct nvkm_subdev *subdev)
 
 	if (mmu->func->dtor)
 		data = mmu->func->dtor(mmu);
+	nvkm_vm_ref(NULL, &mmu->vmm, NULL);
 
 	nvkm_mmu_ptc_fini(mmu);
 	return data;

commit f1280394109a3a3a7ef1c37950d4356b12e75cb9
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: implement page table sub-allocation
    
    GP100 "big" (which is a funny name, when it supports "even bigger") page
    tables are small enough that we want to be able to suballocate them from
    a larger block of memory.
    
    This builds on the previous page table cache interfaces so that the VMM
    code doesn't need to know the difference.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 1e4a92e0068e..1bdae020057e 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -26,6 +26,85 @@
 #include <core/gpuobj.h>
 #include <subdev/fb.h>
 
+struct nvkm_mmu_ptp {
+	struct nvkm_mmu_pt *pt;
+	struct list_head head;
+	u8  shift;
+	u16 mask;
+	u16 free;
+};
+
+static void
+nvkm_mmu_ptp_put(struct nvkm_mmu *mmu, bool force, struct nvkm_mmu_pt *pt)
+{
+	const int slot = pt->base >> pt->ptp->shift;
+	struct nvkm_mmu_ptp *ptp = pt->ptp;
+
+	/* If there were no free slots in the parent allocation before,
+	 * there will be now, so return PTP to the cache.
+	 */
+	if (!ptp->free)
+		list_add(&ptp->head, &mmu->ptp.list);
+	ptp->free |= BIT(slot);
+
+	/* If there's no more sub-allocations, destroy PTP. */
+	if (ptp->free == ptp->mask) {
+		nvkm_mmu_ptc_put(mmu, force, &ptp->pt);
+		list_del(&ptp->head);
+		kfree(ptp);
+	}
+
+	kfree(pt);
+}
+
+struct nvkm_mmu_pt *
+nvkm_mmu_ptp_get(struct nvkm_mmu *mmu, u32 size, bool zero)
+{
+	struct nvkm_mmu_pt *pt;
+	struct nvkm_mmu_ptp *ptp;
+	int slot;
+
+	if (!(pt = kzalloc(sizeof(*pt), GFP_KERNEL)))
+		return NULL;
+
+	ptp = list_first_entry_or_null(&mmu->ptp.list, typeof(*ptp), head);
+	if (!ptp) {
+		/* Need to allocate a new parent to sub-allocate from. */
+		if (!(ptp = kmalloc(sizeof(*ptp), GFP_KERNEL))) {
+			kfree(pt);
+			return NULL;
+		}
+
+		ptp->pt = nvkm_mmu_ptc_get(mmu, 0x1000, 0x1000, false);
+		if (!ptp->pt) {
+			kfree(ptp);
+			kfree(pt);
+			return NULL;
+		}
+
+		ptp->shift = order_base_2(size);
+		slot = nvkm_memory_size(ptp->pt->memory) >> ptp->shift;
+		ptp->mask = (1 << slot) - 1;
+		ptp->free = ptp->mask;
+		list_add(&ptp->head, &mmu->ptp.list);
+	}
+	pt->ptp = ptp;
+	pt->sub = true;
+
+	/* Sub-allocate from parent object, removing PTP from cache
+	 * if there's no more free slots left.
+	 */
+	slot = __ffs(ptp->free);
+	ptp->free &= ~BIT(slot);
+	if (!ptp->free)
+		list_del(&ptp->head);
+
+	pt->memory = pt->ptp->pt->memory;
+	pt->base = slot << ptp->shift;
+	pt->addr = pt->ptp->pt->addr + pt->base;
+	return pt;
+}
+
 struct nvkm_mmu_ptc {
 	struct list_head head;
 	struct list_head item;
@@ -59,6 +138,14 @@ nvkm_mmu_ptc_put(struct nvkm_mmu *mmu, bool force, struct nvkm_mmu_pt **ppt)
 {
 	struct nvkm_mmu_pt *pt = *ppt;
 	if (pt) {
+		/* Handle sub-allocated page tables. */
+		if (pt->sub) {
+			mutex_lock(&mmu->ptp.mutex);
+			nvkm_mmu_ptp_put(mmu, force, pt);
+			mutex_unlock(&mmu->ptp.mutex);
+			return;
+		}
+
 		/* Either cache or free the object. */
 		mutex_lock(&mmu->ptc.mutex);
 		if (pt->ptc->refs < 8 /* Heuristic. */ && !force) {
@@ -79,6 +166,14 @@ nvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)
 	struct nvkm_mmu_pt *pt;
 	int ret;
 
+	/* Sub-allocated page table (ie. GP100 LPT). */
+	if (align < 0x1000) {
+		mutex_lock(&mmu->ptp.mutex);
+		pt = nvkm_mmu_ptp_get(mmu, align, zero);
+		mutex_unlock(&mmu->ptp.mutex);
+		return pt;
+	}
+
 	/* Lookup cache for this page table size. */
 	mutex_lock(&mmu->ptc.mutex);
 	ptc = nvkm_mmu_ptc_find(mmu, size);
@@ -103,6 +198,7 @@ nvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)
 	if (!(pt = kmalloc(sizeof(*pt), GFP_KERNEL)))
 		return NULL;
 	pt->ptc = ptc;
+	pt->sub = false;
 
 	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
 			      size, align, zero, &pt->memory);
@@ -147,6 +243,8 @@ nvkm_mmu_ptc_init(struct nvkm_mmu *mmu)
 {
 	mutex_init(&mmu->ptc.mutex);
 	INIT_LIST_HEAD(&mmu->ptc.list);
+	mutex_init(&mmu->ptp.mutex);
+	INIT_LIST_HEAD(&mmu->ptp.list);
 }
 
 void

commit 9a45ddaaa674aa103cd74a0df9a3f9c2c8fb3124
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: implement page table cache
    
    Builds up and maintains a small cache of each page table size in order
    to reduce the frequency of expensive allocations, particularly in the
    pathological case where an address range ping-pongs between allocated
    and free.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index d9f572db5c29..1e4a92e0068e 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -26,6 +26,129 @@
 #include <core/gpuobj.h>
 #include <subdev/fb.h>
 
+struct nvkm_mmu_ptc {
+	struct list_head head;
+	struct list_head item;
+	u32 size;
+	u32 refs;
+};
+
+static inline struct nvkm_mmu_ptc *
+nvkm_mmu_ptc_find(struct nvkm_mmu *mmu, u32 size)
+{
+	struct nvkm_mmu_ptc *ptc;
+
+	list_for_each_entry(ptc, &mmu->ptc.list, head) {
+		if (ptc->size == size)
+			return ptc;
+	}
+
+	ptc = kmalloc(sizeof(*ptc), GFP_KERNEL);
+	if (ptc) {
+		INIT_LIST_HEAD(&ptc->item);
+		ptc->size = size;
+		ptc->refs = 0;
+		list_add(&ptc->head, &mmu->ptc.list);
+	}
+
+	return ptc;
+}
+
+void
+nvkm_mmu_ptc_put(struct nvkm_mmu *mmu, bool force, struct nvkm_mmu_pt **ppt)
+{
+	struct nvkm_mmu_pt *pt = *ppt;
+	if (pt) {
+		/* Either cache or free the object. */
+		mutex_lock(&mmu->ptc.mutex);
+		if (pt->ptc->refs < 8 /* Heuristic. */ && !force) {
+			list_add_tail(&pt->head, &pt->ptc->item);
+			pt->ptc->refs++;
+		} else {
+			nvkm_memory_unref(&pt->memory);
+			kfree(pt);
+		}
+		mutex_unlock(&mmu->ptc.mutex);
+	}
+}
+
+struct nvkm_mmu_pt *
+nvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)
+{
+	struct nvkm_mmu_ptc *ptc;
+	struct nvkm_mmu_pt *pt;
+	int ret;
+
+	/* Lookup cache for this page table size. */
+	mutex_lock(&mmu->ptc.mutex);
+	ptc = nvkm_mmu_ptc_find(mmu, size);
+	if (!ptc) {
+		mutex_unlock(&mmu->ptc.mutex);
+		return NULL;
+	}
+
+	/* If there's a free PT in the cache, reuse it. */
+	pt = list_first_entry_or_null(&ptc->item, typeof(*pt), head);
+	if (pt) {
+		if (zero)
+			nvkm_fo64(pt->memory, 0, 0, size >> 3);
+		list_del(&pt->head);
+		ptc->refs--;
+		mutex_unlock(&mmu->ptc.mutex);
+		return pt;
+	}
+	mutex_unlock(&mmu->ptc.mutex);
+
+	/* No such luck, we need to allocate. */
+	if (!(pt = kmalloc(sizeof(*pt), GFP_KERNEL)))
+		return NULL;
+	pt->ptc = ptc;
+
+	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
+			      size, align, zero, &pt->memory);
+	if (ret) {
+		kfree(pt);
+		return NULL;
+	}
+
+	pt->base = 0;
+	pt->addr = nvkm_memory_addr(pt->memory);
+	return pt;
+}
+
+void
+nvkm_mmu_ptc_dump(struct nvkm_mmu *mmu)
+{
+	struct nvkm_mmu_ptc *ptc;
+	list_for_each_entry(ptc, &mmu->ptc.list, head) {
+		struct nvkm_mmu_pt *pt, *tt;
+		list_for_each_entry_safe(pt, tt, &ptc->item, head) {
+			nvkm_memory_unref(&pt->memory);
+			list_del(&pt->head);
+			kfree(pt);
+		}
+	}
+}
+
+static void
+nvkm_mmu_ptc_fini(struct nvkm_mmu *mmu)
+{
+	struct nvkm_mmu_ptc *ptc, *ptct;
+
+	list_for_each_entry_safe(ptc, ptct, &mmu->ptc.list, head) {
+		WARN_ON(!list_empty(&ptc->item));
+		list_del(&ptc->head);
+		kfree(ptc);
+	}
+}
+
+static void
+nvkm_mmu_ptc_init(struct nvkm_mmu *mmu)
+{
+	mutex_init(&mmu->ptc.mutex);
+	INIT_LIST_HEAD(&mmu->ptc.list);
+}
+
 void
 nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
@@ -514,9 +637,13 @@ static void *
 nvkm_mmu_dtor(struct nvkm_subdev *subdev)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
+	void *data = mmu;
+
 	if (mmu->func->dtor)
-		return mmu->func->dtor(mmu);
-	return mmu;
+		data = mmu->func->dtor(mmu);
+
+	nvkm_mmu_ptc_fini(mmu);
+	return data;
 }
 
 static const struct nvkm_subdev_func
@@ -535,6 +662,7 @@ nvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,
 	mmu->limit = func->limit;
 	mmu->dma_bits = func->dma_bits;
 	mmu->lpg_shift = func->lpg_shift;
+	nvkm_mmu_ptc_init(mmu);
 }
 
 int

commit 5e075fdeb166098a3dc493026534c7631e845782
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/mmu: automatically handle "un-bootstrapping" of vmm
    
    Removes the need to expose internals outside of MMU, and GP100 is both
    different, and a lot harder to deal with.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index ad11db458fcc..d9f572db5c29 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -357,6 +357,7 @@ nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 		vm->pgt[0].refcount[0] = 1;
 		vm->pgt[0].mem[0] = pgt;
 		nvkm_memory_boot(pgt, vm);
+		vm->bootstrapped = true;
 	}
 
 	return ret;
@@ -481,6 +482,8 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_gpuobj *pgd)
 	}
 
 	if (*ptr) {
+		if ((*ptr)->bootstrapped && pgd)
+			nvkm_memory_unref(&(*ptr)->pgt[0].mem[0]);
 		nvkm_vm_unlink(*ptr, pgd);
 		kref_put(&(*ptr)->refcount, nvkm_vm_del);
 	}

commit 997a89003c2d950466bc289147ffb823c0c51fb0
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/core/memory: add reference counting
    
    We need to be able to prevent memory from being freed while it's still
    mapped in a GPU's address-space.
    
    Will be used by upcoming MMU changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 44c2403e88e6..ad11db458fcc 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -243,7 +243,7 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 
 		mmu->func->flush(vm);
 
-		nvkm_memory_del(&pgt);
+		nvkm_memory_unref(&pgt);
 	}
 }
 

commit 4d058fab63f79e5cf13d21edd9db1a63748da0a1
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    drm/nouveau/core/mm: have users explicitly define heap identifiers
    
    Different sections of VRAM may have different properties (ie. can't be used
    for compression/display, can't be mapped, etc).
    
    We currently already support this, but it's a bit magic.  This change makes
    it more obvious where we're allocating from.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index f2b1a3e75f17..44c2403e88e6 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -388,7 +388,7 @@ nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 		return -ENOMEM;
 	}
 
-	ret = nvkm_mm_init(&vm->mm, mm_offset >> 12, mm_length >> 12,
+	ret = nvkm_mm_init(&vm->mm, 0, mm_offset >> 12, mm_length >> 12,
 			   block >> 12);
 	if (ret) {
 		vfree(vm->pgt);

commit f5a5b5232b899c06626960b601d1943cc2fb21d9
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Nov 1 03:56:19 2017 +1000

    remove some useless semicolons
    
    Reported-by: Dave Airlie <airlied@redhat.com>
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 455da298227f..f2b1a3e75f17 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -66,7 +66,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 			delta += (u64)len << vma->node->type;
 		}
 		r = r->next;
-	};
+	}
 
 	mmu->func->flush(vm);
 }

commit 77913bbcb43ac9a07a6fe849c2fd3bf85fc8bdd8
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Mon Sep 25 15:05:38 2017 +1000

    drm/nouveau/mmu: flush tlbs before deleting page tables
    
    Even though we've zeroed the PDE, the GPU may have cached the PD, so we
    need to flush when deleting them.
    
    Noticed while working on replacement MMU code, but a backport might be a
    good idea, so let's fix it in the current code too.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>
    Cc: stable@vger.kernel.org

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index d06ad2c372bf..455da298227f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -241,6 +241,8 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 			mmu->func->map_pgt(vpgd->obj, pde, vpgt->mem);
 		}
 
+		mmu->func->flush(vm);
+
 		nvkm_memory_del(&pgt);
 	}
 }

commit 134fdc1a704f0042465ea993c33540aaec7e1d2e
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Sat Oct 3 17:34:25 2015 +1000

    drm/nouveau/core/mm: replace region list with next pointer
    
    We never have any need for a double-linked list here, and as there's
    generally a large number of these objects, replace it with a single-
    linked list in order to save some memory.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 5df9669ea39c..d06ad2c372bf 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -31,7 +31,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_mm_node *r;
+	struct nvkm_mm_node *r = node->mem;
 	int big = vma->node->type != mmu->func->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
@@ -41,7 +41,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 	u32 end, len;
 
 	delta = 0;
-	list_for_each_entry(r, &node->regions, rl_entry) {
+	while (r) {
 		u64 phys = (u64)r->offset << 12;
 		u32 num  = r->length >> bits;
 
@@ -65,7 +65,8 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 
 			delta += (u64)len << vma->node->type;
 		}
-	}
+		r = r->next;
+	};
 
 	mmu->func->flush(vm);
 }

commit 56d06fa29edd58c448766014afd833b7ff51247b
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Fri Apr 8 17:24:40 2016 +1000

    drm/nouveau/core: remove pmc_enable argument from subdev ctor
    
    These are now specified directly in the MC subdev.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index e04a2296ecd0..5df9669ea39c 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -524,7 +524,7 @@ void
 nvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,
 	      int index, struct nvkm_mmu *mmu)
 {
-	nvkm_subdev_ctor(&nvkm_mmu, device, index, 0, &mmu->subdev);
+	nvkm_subdev_ctor(&nvkm_mmu, device, index, &mmu->subdev);
 	mmu->func = func;
 	mmu->limit = func->limit;
 	mmu->dma_bits = func->dma_bits;

commit c9582455ab74246ec9f5986db3821b33058de585
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:21 2015 +1000

    drm/nouveau/mmu: convert to new-style nvkm_subdev
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 6fa1bdb02dfd..e04a2296ecd0 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -21,10 +21,10 @@
  *
  * Authors: Ben Skeggs
  */
-#include <subdev/mmu.h>
-#include <subdev/fb.h>
+#include "priv.h"
 
 #include <core/gpuobj.h>
+#include <subdev/fb.h>
 
 void
 nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
@@ -32,12 +32,12 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_mm_node *r;
-	int big = vma->node->type != mmu->spg_shift;
+	int big = vma->node->type != mmu->func->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
-	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
 	delta = 0;
@@ -53,7 +53,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 				end = max;
 			len = end - pte;
 
-			mmu->map(vma, pgt, node, pte, len, phys, delta);
+			mmu->func->map(vma, pgt, node, pte, len, phys, delta);
 
 			num -= len;
 			pte += len;
@@ -67,7 +67,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 		}
 	}
 
-	mmu->flush(vm);
+	mmu->func->flush(vm);
 }
 
 static void
@@ -76,13 +76,13 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 {
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
-	int big = vma->node->type != mmu->spg_shift;
+	int big = vma->node->type != mmu->func->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
 	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	unsigned m, sglen;
 	u32 end, len;
 	int i;
@@ -100,7 +100,7 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 		for (m = 0; m < len; m++) {
 			dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
 
-			mmu->map_sg(vma, pgt, mem, pte, 1, &addr);
+			mmu->func->map_sg(vma, pgt, mem, pte, 1, &addr);
 			num--;
 			pte++;
 
@@ -115,7 +115,7 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 			for (; m < sglen; m++) {
 				dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
 
-				mmu->map_sg(vma, pgt, mem, pte, 1, &addr);
+				mmu->func->map_sg(vma, pgt, mem, pte, 1, &addr);
 				num--;
 				pte++;
 				if (num == 0)
@@ -125,7 +125,7 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 
 	}
 finish:
-	mmu->flush(vm);
+	mmu->func->flush(vm);
 }
 
 static void
@@ -135,13 +135,13 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
 	dma_addr_t *list = mem->pages;
-	int big = vma->node->type != mmu->spg_shift;
+	int big = vma->node->type != mmu->func->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
 	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
 	while (num) {
@@ -152,7 +152,7 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 			end = max;
 		len = end - pte;
 
-		mmu->map_sg(vma, pgt, mem, pte, len, list);
+		mmu->func->map_sg(vma, pgt, mem, pte, len, list);
 
 		num  -= len;
 		pte  += len;
@@ -163,7 +163,7 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 		}
 	}
 
-	mmu->flush(vm);
+	mmu->func->flush(vm);
 }
 
 void
@@ -183,13 +183,13 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 {
 	struct nvkm_vm *vm = vma->vm;
 	struct nvkm_mmu *mmu = vm->mmu;
-	int big = vma->node->type != mmu->spg_shift;
+	int big = vma->node->type != mmu->func->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
 	u32 num  = length >> vma->node->type;
-	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
-	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
-	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 pde  = (offset >> mmu->func->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->func->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->func->pgt_bits - bits);
 	u32 end, len;
 
 	while (num) {
@@ -200,7 +200,7 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 			end = max;
 		len = end - pte;
 
-		mmu->unmap(vma, pgt, pte, len);
+		mmu->func->unmap(vma, pgt, pte, len);
 
 		num -= len;
 		pte += len;
@@ -210,7 +210,7 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 		}
 	}
 
-	mmu->flush(vm);
+	mmu->func->flush(vm);
 }
 
 void
@@ -237,7 +237,7 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 		vpgt->mem[big] = NULL;
 
 		list_for_each_entry(vpgd, &vm->pgd_list, head) {
-			mmu->map_pgt(vpgd->obj, pde, vpgt->mem);
+			mmu->func->map_pgt(vpgd->obj, pde, vpgt->mem);
 		}
 
 		nvkm_memory_del(&pgt);
@@ -250,11 +250,11 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
 	struct nvkm_vm_pgd *vpgd;
-	int big = (type != mmu->spg_shift);
+	int big = (type != mmu->func->spg_shift);
 	u32 pgt_size;
 	int ret;
 
-	pgt_size  = (1 << (mmu->pgt_bits + 12)) >> type;
+	pgt_size  = (1 << (mmu->func->pgt_bits + 12)) >> type;
 	pgt_size *= 8;
 
 	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
@@ -263,7 +263,7 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 		return ret;
 
 	list_for_each_entry(vpgd, &vm->pgd_list, head) {
-		mmu->map_pgt(vpgd->obj, pde, vpgt->mem);
+		mmu->func->map_pgt(vpgd->obj, pde, vpgt->mem);
 	}
 
 	vpgt->refcount[big]++;
@@ -288,12 +288,12 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 		return ret;
 	}
 
-	fpde = (vma->node->offset >> mmu->pgt_bits);
-	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
+	fpde = (vma->node->offset >> mmu->func->pgt_bits);
+	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
 
 	for (pde = fpde; pde <= lpde; pde++) {
 		struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
-		int big = (vma->node->type != mmu->spg_shift);
+		int big = (vma->node->type != mmu->func->spg_shift);
 
 		if (likely(vpgt->refcount[big])) {
 			vpgt->refcount[big]++;
@@ -330,11 +330,11 @@ nvkm_vm_put(struct nvkm_vma *vma)
 	vm = vma->vm;
 	mmu = vm->mmu;
 
-	fpde = (vma->node->offset >> mmu->pgt_bits);
-	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
+	fpde = (vma->node->offset >> mmu->func->pgt_bits);
+	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->func->pgt_bits;
 
 	mutex_lock(&vm->mutex);
-	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->spg_shift, fpde, lpde);
+	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->func->spg_shift, fpde, lpde);
 	nvkm_mm_free(&vm->mm, &vma->node);
 	mutex_unlock(&vm->mutex);
 
@@ -349,7 +349,7 @@ nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 	int ret;
 
 	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
-			      (size >> mmu->spg_shift) * 8, 0x1000, true, &pgt);
+			      (size >> mmu->func->spg_shift) * 8, 0x1000, true, &pgt);
 	if (ret == 0) {
 		vm->pgt[0].refcount[0] = 1;
 		vm->pgt[0].mem[0] = pgt;
@@ -376,8 +376,8 @@ nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	INIT_LIST_HEAD(&vm->pgd_list);
 	vm->mmu = mmu;
 	kref_init(&vm->refcount);
-	vm->fpde = offset >> (mmu->pgt_bits + 12);
-	vm->lpde = (offset + length - 1) >> (mmu->pgt_bits + 12);
+	vm->fpde = offset >> (mmu->func->pgt_bits + 12);
+	vm->lpde = (offset + length - 1) >> (mmu->func->pgt_bits + 12);
 
 	vm->pgt  = vzalloc((vm->lpde - vm->fpde + 1) * sizeof(*vm->pgt));
 	if (!vm->pgt) {
@@ -402,8 +402,10 @@ int
 nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
 	    struct lock_class_key *key, struct nvkm_vm **pvm)
 {
-	struct nvkm_mmu *mmu = nvkm_mmu(device);
-	return mmu->create(mmu, offset, length, mm_offset, key, pvm);
+	struct nvkm_mmu *mmu = device->mmu;
+	if (!mmu->func->create)
+		return -EINVAL;
+	return mmu->func->create(mmu, offset, length, mm_offset, key, pvm);
 }
 
 static int
@@ -424,7 +426,7 @@ nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
 
 	mutex_lock(&vm->mutex);
 	for (i = vm->fpde; i <= vm->lpde; i++)
-		mmu->map_pgt(pgd, i, vm->pgt[i - vm->fpde].mem);
+		mmu->func->map_pgt(pgd, i, vm->pgt[i - vm->fpde].mem);
 	list_add(&vpgd->head, &vm->pgd_list);
 	mutex_unlock(&vm->mutex);
 	return 0;
@@ -483,3 +485,58 @@ nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_gpuobj *pgd)
 	*ptr = ref;
 	return 0;
 }
+
+static int
+nvkm_mmu_oneinit(struct nvkm_subdev *subdev)
+{
+	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
+	if (mmu->func->oneinit)
+		return mmu->func->oneinit(mmu);
+	return 0;
+}
+
+static int
+nvkm_mmu_init(struct nvkm_subdev *subdev)
+{
+	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
+	if (mmu->func->init)
+		mmu->func->init(mmu);
+	return 0;
+}
+
+static void *
+nvkm_mmu_dtor(struct nvkm_subdev *subdev)
+{
+	struct nvkm_mmu *mmu = nvkm_mmu(subdev);
+	if (mmu->func->dtor)
+		return mmu->func->dtor(mmu);
+	return mmu;
+}
+
+static const struct nvkm_subdev_func
+nvkm_mmu = {
+	.dtor = nvkm_mmu_dtor,
+	.oneinit = nvkm_mmu_oneinit,
+	.init = nvkm_mmu_init,
+};
+
+void
+nvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,
+	      int index, struct nvkm_mmu *mmu)
+{
+	nvkm_subdev_ctor(&nvkm_mmu, device, index, 0, &mmu->subdev);
+	mmu->func = func;
+	mmu->limit = func->limit;
+	mmu->dma_bits = func->dma_bits;
+	mmu->lpg_shift = func->lpg_shift;
+}
+
+int
+nvkm_mmu_new_(const struct nvkm_mmu_func *func, struct nvkm_device *device,
+	      int index, struct nvkm_mmu **pmmu)
+{
+	if (!(*pmmu = kzalloc(sizeof(**pmmu), GFP_KERNEL)))
+		return -ENOMEM;
+	nvkm_mmu_ctor(func, device, index, *pmmu);
+	return 0;
+}

commit f027f49166171c98d5945af12ac3ee9bc9f9bf4c
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/gpuobj: separate allocation from nvkm_object
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 9c712818528b..6fa1bdb02dfd 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -420,7 +420,7 @@ nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
 	if (!vpgd)
 		return -ENOMEM;
 
-	nvkm_gpuobj_ref(pgd, &vpgd->obj);
+	vpgd->obj = pgd;
 
 	mutex_lock(&vm->mutex);
 	for (i = vm->fpde; i <= vm->lpde; i++)
@@ -434,7 +434,6 @@ static void
 nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
 {
 	struct nvkm_vm_pgd *vpgd, *tmp;
-	struct nvkm_gpuobj *pgd = NULL;
 
 	if (!mpgd)
 		return;
@@ -442,15 +441,12 @@ nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
 	mutex_lock(&vm->mutex);
 	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
 		if (vpgd->obj == mpgd) {
-			pgd = vpgd->obj;
 			list_del(&vpgd->head);
 			kfree(vpgd);
 			break;
 		}
 	}
 	mutex_unlock(&vm->mutex);
-
-	nvkm_gpuobj_ref(NULL, &pgd);
 }
 
 static void

commit 227c95d90a3c50defbc7b4f98605e13af4e6214c
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/gr: directly use instmem where currently possible
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 6f284e5dc2c8..9c712818528b 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -321,12 +321,15 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 void
 nvkm_vm_put(struct nvkm_vma *vma)
 {
-	struct nvkm_vm *vm = vma->vm;
-	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_mmu *mmu;
+	struct nvkm_vm *vm;
 	u32 fpde, lpde;
 
 	if (unlikely(vma->node == NULL))
 		return;
+	vm = vma->vm;
+	mmu = vm->mmu;
+
 	fpde = (vma->node->offset >> mmu->pgt_bits);
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
 

commit d0659d3277cd7bf50e45d48f4692a7fbb11e5957
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/mmu: directly use instmem for page tables
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 35b6d33f6669..6f284e5dc2c8 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -46,7 +46,7 @@ nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 		u32 num  = r->length >> bits;
 
 		while (num) {
-			struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
+			struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 
 			end = (pte + num);
 			if (unlikely(end >= max))
@@ -89,7 +89,7 @@ nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
 	struct scatterlist *sg;
 
 	for_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {
-		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 		sglen = sg_dma_len(sg) >> PAGE_SHIFT;
 
 		end = pte + sglen;
@@ -145,7 +145,7 @@ nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
 	u32 end, len;
 
 	while (num) {
-		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 
 		end = (pte + num);
 		if (unlikely(end >= max))
@@ -193,14 +193,14 @@ nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 	u32 end, len;
 
 	while (num) {
-		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_memory *pgt = vm->pgt[pde].mem[big];
 
 		end = (pte + num);
 		if (unlikely(end >= max))
 			end = max;
 		len = end - pte;
 
-		mmu->unmap(pgt, pte, len);
+		mmu->unmap(vma, pgt, pte, len);
 
 		num -= len;
 		pte += len;
@@ -225,7 +225,7 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_vm_pgd *vpgd;
 	struct nvkm_vm_pgt *vpgt;
-	struct nvkm_gpuobj *pgt;
+	struct nvkm_memory *pgt;
 	u32 pde;
 
 	for (pde = fpde; pde <= lpde; pde++) {
@@ -233,14 +233,14 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 		if (--vpgt->refcount[big])
 			continue;
 
-		pgt = vpgt->obj[big];
-		vpgt->obj[big] = NULL;
+		pgt = vpgt->mem[big];
+		vpgt->mem[big] = NULL;
 
 		list_for_each_entry(vpgd, &vm->pgd_list, head) {
-			mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
+			mmu->map_pgt(vpgd->obj, pde, vpgt->mem);
 		}
 
-		nvkm_gpuobj_ref(NULL, &pgt);
+		nvkm_memory_del(&pgt);
 	}
 }
 
@@ -257,13 +257,13 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 	pgt_size  = (1 << (mmu->pgt_bits + 12)) >> type;
 	pgt_size *= 8;
 
-	ret = nvkm_gpuobj_new(nv_object(vm->mmu), NULL, pgt_size, 0x1000,
-			      NVOBJ_FLAG_ZERO_ALLOC, &vpgt->obj[big]);
+	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
+			      pgt_size, 0x1000, true, &vpgt->mem[big]);
 	if (unlikely(ret))
 		return ret;
 
 	list_for_each_entry(vpgd, &vm->pgd_list, head) {
-		mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
+		mmu->map_pgt(vpgd->obj, pde, vpgt->mem);
 	}
 
 	vpgt->refcount[big]++;
@@ -342,16 +342,15 @@ int
 nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
 {
 	struct nvkm_mmu *mmu = vm->mmu;
-	struct nvkm_gpuobj *pgt;
+	struct nvkm_memory *pgt;
 	int ret;
 
-	ret = nvkm_gpuobj_new(nv_object(mmu), NULL,
-			      (size >> mmu->spg_shift) * 8, 0x1000,
-			      NVOBJ_FLAG_ZERO_ALLOC, &pgt);
+	ret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,
+			      (size >> mmu->spg_shift) * 8, 0x1000, true, &pgt);
 	if (ret == 0) {
 		vm->pgt[0].refcount[0] = 1;
-		vm->pgt[0].obj[0] = pgt;
-		nvkm_memory_boot(pgt->memory, vm);
+		vm->pgt[0].mem[0] = pgt;
+		nvkm_memory_boot(pgt, vm);
 	}
 
 	return ret;
@@ -422,7 +421,7 @@ nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
 
 	mutex_lock(&vm->mutex);
 	for (i = vm->fpde; i <= vm->lpde; i++)
-		mmu->map_pgt(pgd, i, vm->pgt[i - vm->fpde].obj);
+		mmu->map_pgt(pgd, i, vm->pgt[i - vm->fpde].mem);
 	list_add(&vpgd->head, &vm->pgd_list);
 	mutex_unlock(&vm->mutex);
 	return 0;

commit d8e83994aaf6749b7124a219f5b46bd1329e2a08
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/imem: improve management of instance memory
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index e81d3170325f..35b6d33f6669 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -338,6 +338,25 @@ nvkm_vm_put(struct nvkm_vma *vma)
 	nvkm_vm_ref(NULL, &vma->vm, NULL);
 }
 
+int
+nvkm_vm_boot(struct nvkm_vm *vm, u64 size)
+{
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_gpuobj *pgt;
+	int ret;
+
+	ret = nvkm_gpuobj_new(nv_object(mmu), NULL,
+			      (size >> mmu->spg_shift) * 8, 0x1000,
+			      NVOBJ_FLAG_ZERO_ALLOC, &pgt);
+	if (ret == 0) {
+		vm->pgt[0].refcount[0] = 1;
+		vm->pgt[0].obj[0] = pgt;
+		nvkm_memory_boot(pgt->memory, vm);
+	}
+
+	return ret;
+}
+
 int
 nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	       u32 block, struct lock_class_key *key, struct nvkm_vm **pvm)

commit 1de68568d69ac518db076cc6118af91e930b5f90
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Thu Aug 20 14:54:17 2015 +1000

    drm/nouveau/mmu: protect each vm with its own mutex
    
    An upcoming commit requires being able to modify the PRAMIN BAR page
    tables while already holding the MMU subdev mutex.
    
    To solve this issue, each VM has been given its own mutex.  As a nice
    side-effect, this also allows separate VMs to be updated concurrently.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index 277b6ec04e24..e81d3170325f 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -240,9 +240,7 @@ nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 			mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
 		}
 
-		mutex_unlock(&nv_subdev(mmu)->mutex);
 		nvkm_gpuobj_ref(NULL, &pgt);
-		mutex_lock(&nv_subdev(mmu)->mutex);
 	}
 }
 
@@ -252,7 +250,6 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
 	struct nvkm_vm_pgd *vpgd;
-	struct nvkm_gpuobj *pgt;
 	int big = (type != mmu->spg_shift);
 	u32 pgt_size;
 	int ret;
@@ -260,26 +257,16 @@ nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 	pgt_size  = (1 << (mmu->pgt_bits + 12)) >> type;
 	pgt_size *= 8;
 
-	mutex_unlock(&nv_subdev(mmu)->mutex);
 	ret = nvkm_gpuobj_new(nv_object(vm->mmu), NULL, pgt_size, 0x1000,
-			      NVOBJ_FLAG_ZERO_ALLOC, &pgt);
-	mutex_lock(&nv_subdev(mmu)->mutex);
+			      NVOBJ_FLAG_ZERO_ALLOC, &vpgt->obj[big]);
 	if (unlikely(ret))
 		return ret;
 
-	/* someone beat us to filling the PDE while we didn't have the lock */
-	if (unlikely(vpgt->refcount[big]++)) {
-		mutex_unlock(&nv_subdev(mmu)->mutex);
-		nvkm_gpuobj_ref(NULL, &pgt);
-		mutex_lock(&nv_subdev(mmu)->mutex);
-		return 0;
-	}
-
-	vpgt->obj[big] = pgt;
 	list_for_each_entry(vpgd, &vm->pgd_list, head) {
 		mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
 	}
 
+	vpgt->refcount[big]++;
 	return 0;
 }
 
@@ -293,11 +280,11 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 	u32 fpde, lpde, pde;
 	int ret;
 
-	mutex_lock(&nv_subdev(mmu)->mutex);
+	mutex_lock(&vm->mutex);
 	ret = nvkm_mm_head(&vm->mm, 0, page_shift, msize, msize, align,
 			   &vma->node);
 	if (unlikely(ret != 0)) {
-		mutex_unlock(&nv_subdev(mmu)->mutex);
+		mutex_unlock(&vm->mutex);
 		return ret;
 	}
 
@@ -318,11 +305,11 @@ nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
 			if (pde != fpde)
 				nvkm_vm_unmap_pgt(vm, big, fpde, pde - 1);
 			nvkm_mm_free(&vm->mm, &vma->node);
-			mutex_unlock(&nv_subdev(mmu)->mutex);
+			mutex_unlock(&vm->mutex);
 			return ret;
 		}
 	}
-	mutex_unlock(&nv_subdev(mmu)->mutex);
+	mutex_unlock(&vm->mutex);
 
 	vma->vm = NULL;
 	nvkm_vm_ref(vm, &vma->vm, NULL);
@@ -343,18 +330,19 @@ nvkm_vm_put(struct nvkm_vma *vma)
 	fpde = (vma->node->offset >> mmu->pgt_bits);
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
 
-	mutex_lock(&nv_subdev(mmu)->mutex);
+	mutex_lock(&vm->mutex);
 	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->spg_shift, fpde, lpde);
 	nvkm_mm_free(&vm->mm, &vma->node);
-	mutex_unlock(&nv_subdev(mmu)->mutex);
+	mutex_unlock(&vm->mutex);
 
 	nvkm_vm_ref(NULL, &vma->vm, NULL);
 }
 
 int
 nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
-	       u32 block, struct nvkm_vm **pvm)
+	       u32 block, struct lock_class_key *key, struct nvkm_vm **pvm)
 {
+	static struct lock_class_key _key;
 	struct nvkm_vm *vm;
 	u64 mm_length = (offset + length) - mm_offset;
 	int ret;
@@ -363,6 +351,7 @@ nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 	if (!vm)
 		return -ENOMEM;
 
+	__mutex_init(&vm->mutex, "&vm->mutex", key ? key : &_key);
 	INIT_LIST_HEAD(&vm->pgd_list);
 	vm->mmu = mmu;
 	kref_init(&vm->refcount);
@@ -390,10 +379,10 @@ nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
 
 int
 nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
-	    struct nvkm_vm **pvm)
+	    struct lock_class_key *key, struct nvkm_vm **pvm)
 {
 	struct nvkm_mmu *mmu = nvkm_mmu(device);
-	return mmu->create(mmu, offset, length, mm_offset, pvm);
+	return mmu->create(mmu, offset, length, mm_offset, key, pvm);
 }
 
 static int
@@ -412,25 +401,24 @@ nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
 
 	nvkm_gpuobj_ref(pgd, &vpgd->obj);
 
-	mutex_lock(&nv_subdev(mmu)->mutex);
+	mutex_lock(&vm->mutex);
 	for (i = vm->fpde; i <= vm->lpde; i++)
 		mmu->map_pgt(pgd, i, vm->pgt[i - vm->fpde].obj);
 	list_add(&vpgd->head, &vm->pgd_list);
-	mutex_unlock(&nv_subdev(mmu)->mutex);
+	mutex_unlock(&vm->mutex);
 	return 0;
 }
 
 static void
 nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
 {
-	struct nvkm_mmu *mmu = vm->mmu;
 	struct nvkm_vm_pgd *vpgd, *tmp;
 	struct nvkm_gpuobj *pgd = NULL;
 
 	if (!mpgd)
 		return;
 
-	mutex_lock(&nv_subdev(mmu)->mutex);
+	mutex_lock(&vm->mutex);
 	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
 		if (vpgd->obj == mpgd) {
 			pgd = vpgd->obj;
@@ -439,7 +427,7 @@ nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
 			break;
 		}
 	}
-	mutex_unlock(&nv_subdev(mmu)->mutex);
+	mutex_unlock(&vm->mutex);
 
 	nvkm_gpuobj_ref(NULL, &pgd);
 }

commit 42594600095f03244a674fecdd2b5f6da2441180
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 14 15:09:19 2015 +1000

    drm/nouveau/mmu: namespace + nvidia gpu names (no binary change)
    
    The namespace of NVKM is being changed to nvkm_ instead of nouveau_,
    which will be used for the DRM part of the driver.  This is being
    done in order to make it very clear as to what part of the driver a
    given symbol belongs to, and as a minor step towards splitting the
    DRM driver out to be able to stand on its own (for virt).
    
    Because there's already a large amount of churn here anyway, this is
    as good a time as any to also switch to NVIDIA's device and chipset
    naming to ease collaboration with them.
    
    A comparison of objdump disassemblies proves no code changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
index e3cb186c440b..277b6ec04e24 100644
--- a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -21,19 +21,17 @@
  *
  * Authors: Ben Skeggs
  */
+#include <subdev/mmu.h>
+#include <subdev/fb.h>
 
 #include <core/gpuobj.h>
-#include <core/mm.h>
-
-#include <subdev/fb.h>
-#include <subdev/mmu.h>
 
 void
-nouveau_vm_map_at(struct nouveau_vma *vma, u64 delta, struct nouveau_mem *node)
+nvkm_vm_map_at(struct nvkm_vma *vma, u64 delta, struct nvkm_mem *node)
 {
-	struct nouveau_vm *vm = vma->vm;
-	struct nouveau_mmu *mmu = vm->mmu;
-	struct nouveau_mm_node *r;
+	struct nvkm_vm *vm = vma->vm;
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_mm_node *r;
 	int big = vma->node->type != mmu->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
@@ -48,7 +46,7 @@ nouveau_vm_map_at(struct nouveau_vma *vma, u64 delta, struct nouveau_mem *node)
 		u32 num  = r->length >> bits;
 
 		while (num) {
-			struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+			struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
 
 			end = (pte + num);
 			if (unlikely(end >= max))
@@ -73,11 +71,11 @@ nouveau_vm_map_at(struct nouveau_vma *vma, u64 delta, struct nouveau_mem *node)
 }
 
 static void
-nouveau_vm_map_sg_table(struct nouveau_vma *vma, u64 delta, u64 length,
-			struct nouveau_mem *mem)
+nvkm_vm_map_sg_table(struct nvkm_vma *vma, u64 delta, u64 length,
+		     struct nvkm_mem *mem)
 {
-	struct nouveau_vm *vm = vma->vm;
-	struct nouveau_mmu *mmu = vm->mmu;
+	struct nvkm_vm *vm = vma->vm;
+	struct nvkm_mmu *mmu = vm->mmu;
 	int big = vma->node->type != mmu->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
@@ -91,7 +89,7 @@ nouveau_vm_map_sg_table(struct nouveau_vma *vma, u64 delta, u64 length,
 	struct scatterlist *sg;
 
 	for_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {
-		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
 		sglen = sg_dma_len(sg) >> PAGE_SHIFT;
 
 		end = pte + sglen;
@@ -131,11 +129,11 @@ nouveau_vm_map_sg_table(struct nouveau_vma *vma, u64 delta, u64 length,
 }
 
 static void
-nouveau_vm_map_sg(struct nouveau_vma *vma, u64 delta, u64 length,
-		  struct nouveau_mem *mem)
+nvkm_vm_map_sg(struct nvkm_vma *vma, u64 delta, u64 length,
+	       struct nvkm_mem *mem)
 {
-	struct nouveau_vm *vm = vma->vm;
-	struct nouveau_mmu *mmu = vm->mmu;
+	struct nvkm_vm *vm = vma->vm;
+	struct nvkm_mmu *mmu = vm->mmu;
 	dma_addr_t *list = mem->pages;
 	int big = vma->node->type != mmu->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
@@ -147,7 +145,7 @@ nouveau_vm_map_sg(struct nouveau_vma *vma, u64 delta, u64 length,
 	u32 end, len;
 
 	while (num) {
-		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
 
 		end = (pte + num);
 		if (unlikely(end >= max))
@@ -169,22 +167,22 @@ nouveau_vm_map_sg(struct nouveau_vma *vma, u64 delta, u64 length,
 }
 
 void
-nouveau_vm_map(struct nouveau_vma *vma, struct nouveau_mem *node)
+nvkm_vm_map(struct nvkm_vma *vma, struct nvkm_mem *node)
 {
 	if (node->sg)
-		nouveau_vm_map_sg_table(vma, 0, node->size << 12, node);
+		nvkm_vm_map_sg_table(vma, 0, node->size << 12, node);
 	else
 	if (node->pages)
-		nouveau_vm_map_sg(vma, 0, node->size << 12, node);
+		nvkm_vm_map_sg(vma, 0, node->size << 12, node);
 	else
-		nouveau_vm_map_at(vma, 0, node);
+		nvkm_vm_map_at(vma, 0, node);
 }
 
 void
-nouveau_vm_unmap_at(struct nouveau_vma *vma, u64 delta, u64 length)
+nvkm_vm_unmap_at(struct nvkm_vma *vma, u64 delta, u64 length)
 {
-	struct nouveau_vm *vm = vma->vm;
-	struct nouveau_mmu *mmu = vm->mmu;
+	struct nvkm_vm *vm = vma->vm;
+	struct nvkm_mmu *mmu = vm->mmu;
 	int big = vma->node->type != mmu->spg_shift;
 	u32 offset = vma->node->offset + (delta >> 12);
 	u32 bits = vma->node->type - 12;
@@ -195,7 +193,7 @@ nouveau_vm_unmap_at(struct nouveau_vma *vma, u64 delta, u64 length)
 	u32 end, len;
 
 	while (num) {
-		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+		struct nvkm_gpuobj *pgt = vm->pgt[pde].obj[big];
 
 		end = (pte + num);
 		if (unlikely(end >= max))
@@ -216,18 +214,18 @@ nouveau_vm_unmap_at(struct nouveau_vma *vma, u64 delta, u64 length)
 }
 
 void
-nouveau_vm_unmap(struct nouveau_vma *vma)
+nvkm_vm_unmap(struct nvkm_vma *vma)
 {
-	nouveau_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);
+	nvkm_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);
 }
 
 static void
-nouveau_vm_unmap_pgt(struct nouveau_vm *vm, int big, u32 fpde, u32 lpde)
+nvkm_vm_unmap_pgt(struct nvkm_vm *vm, int big, u32 fpde, u32 lpde)
 {
-	struct nouveau_mmu *mmu = vm->mmu;
-	struct nouveau_vm_pgd *vpgd;
-	struct nouveau_vm_pgt *vpgt;
-	struct nouveau_gpuobj *pgt;
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_vm_pgd *vpgd;
+	struct nvkm_vm_pgt *vpgt;
+	struct nvkm_gpuobj *pgt;
 	u32 pde;
 
 	for (pde = fpde; pde <= lpde; pde++) {
@@ -243,18 +241,18 @@ nouveau_vm_unmap_pgt(struct nouveau_vm *vm, int big, u32 fpde, u32 lpde)
 		}
 
 		mutex_unlock(&nv_subdev(mmu)->mutex);
-		nouveau_gpuobj_ref(NULL, &pgt);
+		nvkm_gpuobj_ref(NULL, &pgt);
 		mutex_lock(&nv_subdev(mmu)->mutex);
 	}
 }
 
 static int
-nouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)
+nvkm_vm_map_pgt(struct nvkm_vm *vm, u32 pde, u32 type)
 {
-	struct nouveau_mmu *mmu = vm->mmu;
-	struct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
-	struct nouveau_vm_pgd *vpgd;
-	struct nouveau_gpuobj *pgt;
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
+	struct nvkm_vm_pgd *vpgd;
+	struct nvkm_gpuobj *pgt;
 	int big = (type != mmu->spg_shift);
 	u32 pgt_size;
 	int ret;
@@ -263,8 +261,8 @@ nouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)
 	pgt_size *= 8;
 
 	mutex_unlock(&nv_subdev(mmu)->mutex);
-	ret = nouveau_gpuobj_new(nv_object(vm->mmu), NULL, pgt_size, 0x1000,
-				 NVOBJ_FLAG_ZERO_ALLOC, &pgt);
+	ret = nvkm_gpuobj_new(nv_object(vm->mmu), NULL, pgt_size, 0x1000,
+			      NVOBJ_FLAG_ZERO_ALLOC, &pgt);
 	mutex_lock(&nv_subdev(mmu)->mutex);
 	if (unlikely(ret))
 		return ret;
@@ -272,7 +270,7 @@ nouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)
 	/* someone beat us to filling the PDE while we didn't have the lock */
 	if (unlikely(vpgt->refcount[big]++)) {
 		mutex_unlock(&nv_subdev(mmu)->mutex);
-		nouveau_gpuobj_ref(NULL, &pgt);
+		nvkm_gpuobj_ref(NULL, &pgt);
 		mutex_lock(&nv_subdev(mmu)->mutex);
 		return 0;
 	}
@@ -286,18 +284,18 @@ nouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)
 }
 
 int
-nouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,
-	       u32 access, struct nouveau_vma *vma)
+nvkm_vm_get(struct nvkm_vm *vm, u64 size, u32 page_shift, u32 access,
+	    struct nvkm_vma *vma)
 {
-	struct nouveau_mmu *mmu = vm->mmu;
+	struct nvkm_mmu *mmu = vm->mmu;
 	u32 align = (1 << page_shift) >> 12;
 	u32 msize = size >> 12;
 	u32 fpde, lpde, pde;
 	int ret;
 
 	mutex_lock(&nv_subdev(mmu)->mutex);
-	ret = nouveau_mm_head(&vm->mm, 0, page_shift, msize, msize, align,
-			     &vma->node);
+	ret = nvkm_mm_head(&vm->mm, 0, page_shift, msize, msize, align,
+			   &vma->node);
 	if (unlikely(ret != 0)) {
 		mutex_unlock(&nv_subdev(mmu)->mutex);
 		return ret;
@@ -307,7 +305,7 @@ nouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
 
 	for (pde = fpde; pde <= lpde; pde++) {
-		struct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
+		struct nvkm_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
 		int big = (vma->node->type != mmu->spg_shift);
 
 		if (likely(vpgt->refcount[big])) {
@@ -315,11 +313,11 @@ nouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,
 			continue;
 		}
 
-		ret = nouveau_vm_map_pgt(vm, pde, vma->node->type);
+		ret = nvkm_vm_map_pgt(vm, pde, vma->node->type);
 		if (ret) {
 			if (pde != fpde)
-				nouveau_vm_unmap_pgt(vm, big, fpde, pde - 1);
-			nouveau_mm_free(&vm->mm, &vma->node);
+				nvkm_vm_unmap_pgt(vm, big, fpde, pde - 1);
+			nvkm_mm_free(&vm->mm, &vma->node);
 			mutex_unlock(&nv_subdev(mmu)->mutex);
 			return ret;
 		}
@@ -327,17 +325,17 @@ nouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,
 	mutex_unlock(&nv_subdev(mmu)->mutex);
 
 	vma->vm = NULL;
-	nouveau_vm_ref(vm, &vma->vm, NULL);
+	nvkm_vm_ref(vm, &vma->vm, NULL);
 	vma->offset = (u64)vma->node->offset << 12;
 	vma->access = access;
 	return 0;
 }
 
 void
-nouveau_vm_put(struct nouveau_vma *vma)
+nvkm_vm_put(struct nvkm_vma *vma)
 {
-	struct nouveau_vm *vm = vma->vm;
-	struct nouveau_mmu *mmu = vm->mmu;
+	struct nvkm_vm *vm = vma->vm;
+	struct nvkm_mmu *mmu = vm->mmu;
 	u32 fpde, lpde;
 
 	if (unlikely(vma->node == NULL))
@@ -346,18 +344,18 @@ nouveau_vm_put(struct nouveau_vma *vma)
 	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
 
 	mutex_lock(&nv_subdev(mmu)->mutex);
-	nouveau_vm_unmap_pgt(vm, vma->node->type != mmu->spg_shift, fpde, lpde);
-	nouveau_mm_free(&vm->mm, &vma->node);
+	nvkm_vm_unmap_pgt(vm, vma->node->type != mmu->spg_shift, fpde, lpde);
+	nvkm_mm_free(&vm->mm, &vma->node);
 	mutex_unlock(&nv_subdev(mmu)->mutex);
 
-	nouveau_vm_ref(NULL, &vma->vm, NULL);
+	nvkm_vm_ref(NULL, &vma->vm, NULL);
 }
 
 int
-nouveau_vm_create(struct nouveau_mmu *mmu, u64 offset, u64 length,
-		  u64 mm_offset, u32 block, struct nouveau_vm **pvm)
+nvkm_vm_create(struct nvkm_mmu *mmu, u64 offset, u64 length, u64 mm_offset,
+	       u32 block, struct nvkm_vm **pvm)
 {
-	struct nouveau_vm *vm;
+	struct nvkm_vm *vm;
 	u64 mm_length = (offset + length) - mm_offset;
 	int ret;
 
@@ -377,8 +375,8 @@ nouveau_vm_create(struct nouveau_mmu *mmu, u64 offset, u64 length,
 		return -ENOMEM;
 	}
 
-	ret = nouveau_mm_init(&vm->mm, mm_offset >> 12, mm_length >> 12,
-			      block >> 12);
+	ret = nvkm_mm_init(&vm->mm, mm_offset >> 12, mm_length >> 12,
+			   block >> 12);
 	if (ret) {
 		vfree(vm->pgt);
 		kfree(vm);
@@ -391,18 +389,18 @@ nouveau_vm_create(struct nouveau_mmu *mmu, u64 offset, u64 length,
 }
 
 int
-nouveau_vm_new(struct nouveau_device *device, u64 offset, u64 length,
-	       u64 mm_offset, struct nouveau_vm **pvm)
+nvkm_vm_new(struct nvkm_device *device, u64 offset, u64 length, u64 mm_offset,
+	    struct nvkm_vm **pvm)
 {
-	struct nouveau_mmu *mmu = nouveau_mmu(device);
+	struct nvkm_mmu *mmu = nvkm_mmu(device);
 	return mmu->create(mmu, offset, length, mm_offset, pvm);
 }
 
 static int
-nouveau_vm_link(struct nouveau_vm *vm, struct nouveau_gpuobj *pgd)
+nvkm_vm_link(struct nvkm_vm *vm, struct nvkm_gpuobj *pgd)
 {
-	struct nouveau_mmu *mmu = vm->mmu;
-	struct nouveau_vm_pgd *vpgd;
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_vm_pgd *vpgd;
 	int i;
 
 	if (!pgd)
@@ -412,7 +410,7 @@ nouveau_vm_link(struct nouveau_vm *vm, struct nouveau_gpuobj *pgd)
 	if (!vpgd)
 		return -ENOMEM;
 
-	nouveau_gpuobj_ref(pgd, &vpgd->obj);
+	nvkm_gpuobj_ref(pgd, &vpgd->obj);
 
 	mutex_lock(&nv_subdev(mmu)->mutex);
 	for (i = vm->fpde; i <= vm->lpde; i++)
@@ -423,11 +421,11 @@ nouveau_vm_link(struct nouveau_vm *vm, struct nouveau_gpuobj *pgd)
 }
 
 static void
-nouveau_vm_unlink(struct nouveau_vm *vm, struct nouveau_gpuobj *mpgd)
+nvkm_vm_unlink(struct nvkm_vm *vm, struct nvkm_gpuobj *mpgd)
 {
-	struct nouveau_mmu *mmu = vm->mmu;
-	struct nouveau_vm_pgd *vpgd, *tmp;
-	struct nouveau_gpuobj *pgd = NULL;
+	struct nvkm_mmu *mmu = vm->mmu;
+	struct nvkm_vm_pgd *vpgd, *tmp;
+	struct nvkm_gpuobj *pgd = NULL;
 
 	if (!mpgd)
 		return;
@@ -443,30 +441,29 @@ nouveau_vm_unlink(struct nouveau_vm *vm, struct nouveau_gpuobj *mpgd)
 	}
 	mutex_unlock(&nv_subdev(mmu)->mutex);
 
-	nouveau_gpuobj_ref(NULL, &pgd);
+	nvkm_gpuobj_ref(NULL, &pgd);
 }
 
 static void
-nouveau_vm_del(struct kref *kref)
+nvkm_vm_del(struct kref *kref)
 {
-	struct nouveau_vm *vm = container_of(kref, typeof(*vm), refcount);
-	struct nouveau_vm_pgd *vpgd, *tmp;
+	struct nvkm_vm *vm = container_of(kref, typeof(*vm), refcount);
+	struct nvkm_vm_pgd *vpgd, *tmp;
 
 	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
-		nouveau_vm_unlink(vm, vpgd->obj);
+		nvkm_vm_unlink(vm, vpgd->obj);
 	}
 
-	nouveau_mm_fini(&vm->mm);
+	nvkm_mm_fini(&vm->mm);
 	vfree(vm->pgt);
 	kfree(vm);
 }
 
 int
-nouveau_vm_ref(struct nouveau_vm *ref, struct nouveau_vm **ptr,
-	       struct nouveau_gpuobj *pgd)
+nvkm_vm_ref(struct nvkm_vm *ref, struct nvkm_vm **ptr, struct nvkm_gpuobj *pgd)
 {
 	if (ref) {
-		int ret = nouveau_vm_link(ref, pgd);
+		int ret = nvkm_vm_link(ref, pgd);
 		if (ret)
 			return ret;
 
@@ -474,8 +471,8 @@ nouveau_vm_ref(struct nouveau_vm *ref, struct nouveau_vm **ptr,
 	}
 
 	if (*ptr) {
-		nouveau_vm_unlink(*ptr, pgd);
-		kref_put(&(*ptr)->refcount, nouveau_vm_del);
+		nvkm_vm_unlink(*ptr, pgd);
+		kref_put(&(*ptr)->refcount, nvkm_vm_del);
 	}
 
 	*ptr = ref;

commit 5ce3bf3c72436c49fbd9a5b71d7d278665f4bf55
Author: Ben Skeggs <bskeggs@redhat.com>
Date:   Wed Jan 14 09:57:36 2015 +1000

    drm/nouveau/mmu: rename from vmmgr (no binary change)
    
    Switch to NVIDIA's name for the device.
    
    The namespace of NVKM is being changed to nvkm_ instead of nouveau_,
    which will be used for the DRM part of the driver.  This is being
    done in order to make it very clear as to what part of the driver a
    given symbol belongs to, and as a minor step towards splitting the
    DRM driver out to be able to stand on its own (for virt).
    
    Because there's already a large amount of churn here anyway, this is
    as good a time as any to also switch to NVIDIA's device and chipset
    naming to ease collaboration with them.
    
    A comparison of objdump disassemblies proves no code changes.
    
    Signed-off-by: Ben Skeggs <bskeggs@redhat.com>

diff --git a/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
new file mode 100644
index 000000000000..e3cb186c440b
--- /dev/null
+++ b/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c
@@ -0,0 +1,483 @@
+/*
+ * Copyright 2010 Red Hat Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: Ben Skeggs
+ */
+
+#include <core/gpuobj.h>
+#include <core/mm.h>
+
+#include <subdev/fb.h>
+#include <subdev/mmu.h>
+
+void
+nouveau_vm_map_at(struct nouveau_vma *vma, u64 delta, struct nouveau_mem *node)
+{
+	struct nouveau_vm *vm = vma->vm;
+	struct nouveau_mmu *mmu = vm->mmu;
+	struct nouveau_mm_node *r;
+	int big = vma->node->type != mmu->spg_shift;
+	u32 offset = vma->node->offset + (delta >> 12);
+	u32 bits = vma->node->type - 12;
+	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 end, len;
+
+	delta = 0;
+	list_for_each_entry(r, &node->regions, rl_entry) {
+		u64 phys = (u64)r->offset << 12;
+		u32 num  = r->length >> bits;
+
+		while (num) {
+			struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+
+			end = (pte + num);
+			if (unlikely(end >= max))
+				end = max;
+			len = end - pte;
+
+			mmu->map(vma, pgt, node, pte, len, phys, delta);
+
+			num -= len;
+			pte += len;
+			if (unlikely(end >= max)) {
+				phys += len << (bits + 12);
+				pde++;
+				pte = 0;
+			}
+
+			delta += (u64)len << vma->node->type;
+		}
+	}
+
+	mmu->flush(vm);
+}
+
+static void
+nouveau_vm_map_sg_table(struct nouveau_vma *vma, u64 delta, u64 length,
+			struct nouveau_mem *mem)
+{
+	struct nouveau_vm *vm = vma->vm;
+	struct nouveau_mmu *mmu = vm->mmu;
+	int big = vma->node->type != mmu->spg_shift;
+	u32 offset = vma->node->offset + (delta >> 12);
+	u32 bits = vma->node->type - 12;
+	u32 num  = length >> vma->node->type;
+	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->pgt_bits - bits);
+	unsigned m, sglen;
+	u32 end, len;
+	int i;
+	struct scatterlist *sg;
+
+	for_each_sg(mem->sg->sgl, sg, mem->sg->nents, i) {
+		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+		sglen = sg_dma_len(sg) >> PAGE_SHIFT;
+
+		end = pte + sglen;
+		if (unlikely(end >= max))
+			end = max;
+		len = end - pte;
+
+		for (m = 0; m < len; m++) {
+			dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
+
+			mmu->map_sg(vma, pgt, mem, pte, 1, &addr);
+			num--;
+			pte++;
+
+			if (num == 0)
+				goto finish;
+		}
+		if (unlikely(end >= max)) {
+			pde++;
+			pte = 0;
+		}
+		if (m < sglen) {
+			for (; m < sglen; m++) {
+				dma_addr_t addr = sg_dma_address(sg) + (m << PAGE_SHIFT);
+
+				mmu->map_sg(vma, pgt, mem, pte, 1, &addr);
+				num--;
+				pte++;
+				if (num == 0)
+					goto finish;
+			}
+		}
+
+	}
+finish:
+	mmu->flush(vm);
+}
+
+static void
+nouveau_vm_map_sg(struct nouveau_vma *vma, u64 delta, u64 length,
+		  struct nouveau_mem *mem)
+{
+	struct nouveau_vm *vm = vma->vm;
+	struct nouveau_mmu *mmu = vm->mmu;
+	dma_addr_t *list = mem->pages;
+	int big = vma->node->type != mmu->spg_shift;
+	u32 offset = vma->node->offset + (delta >> 12);
+	u32 bits = vma->node->type - 12;
+	u32 num  = length >> vma->node->type;
+	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 end, len;
+
+	while (num) {
+		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+
+		end = (pte + num);
+		if (unlikely(end >= max))
+			end = max;
+		len = end - pte;
+
+		mmu->map_sg(vma, pgt, mem, pte, len, list);
+
+		num  -= len;
+		pte  += len;
+		list += len;
+		if (unlikely(end >= max)) {
+			pde++;
+			pte = 0;
+		}
+	}
+
+	mmu->flush(vm);
+}
+
+void
+nouveau_vm_map(struct nouveau_vma *vma, struct nouveau_mem *node)
+{
+	if (node->sg)
+		nouveau_vm_map_sg_table(vma, 0, node->size << 12, node);
+	else
+	if (node->pages)
+		nouveau_vm_map_sg(vma, 0, node->size << 12, node);
+	else
+		nouveau_vm_map_at(vma, 0, node);
+}
+
+void
+nouveau_vm_unmap_at(struct nouveau_vma *vma, u64 delta, u64 length)
+{
+	struct nouveau_vm *vm = vma->vm;
+	struct nouveau_mmu *mmu = vm->mmu;
+	int big = vma->node->type != mmu->spg_shift;
+	u32 offset = vma->node->offset + (delta >> 12);
+	u32 bits = vma->node->type - 12;
+	u32 num  = length >> vma->node->type;
+	u32 pde  = (offset >> mmu->pgt_bits) - vm->fpde;
+	u32 pte  = (offset & ((1 << mmu->pgt_bits) - 1)) >> bits;
+	u32 max  = 1 << (mmu->pgt_bits - bits);
+	u32 end, len;
+
+	while (num) {
+		struct nouveau_gpuobj *pgt = vm->pgt[pde].obj[big];
+
+		end = (pte + num);
+		if (unlikely(end >= max))
+			end = max;
+		len = end - pte;
+
+		mmu->unmap(pgt, pte, len);
+
+		num -= len;
+		pte += len;
+		if (unlikely(end >= max)) {
+			pde++;
+			pte = 0;
+		}
+	}
+
+	mmu->flush(vm);
+}
+
+void
+nouveau_vm_unmap(struct nouveau_vma *vma)
+{
+	nouveau_vm_unmap_at(vma, 0, (u64)vma->node->length << 12);
+}
+
+static void
+nouveau_vm_unmap_pgt(struct nouveau_vm *vm, int big, u32 fpde, u32 lpde)
+{
+	struct nouveau_mmu *mmu = vm->mmu;
+	struct nouveau_vm_pgd *vpgd;
+	struct nouveau_vm_pgt *vpgt;
+	struct nouveau_gpuobj *pgt;
+	u32 pde;
+
+	for (pde = fpde; pde <= lpde; pde++) {
+		vpgt = &vm->pgt[pde - vm->fpde];
+		if (--vpgt->refcount[big])
+			continue;
+
+		pgt = vpgt->obj[big];
+		vpgt->obj[big] = NULL;
+
+		list_for_each_entry(vpgd, &vm->pgd_list, head) {
+			mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
+		}
+
+		mutex_unlock(&nv_subdev(mmu)->mutex);
+		nouveau_gpuobj_ref(NULL, &pgt);
+		mutex_lock(&nv_subdev(mmu)->mutex);
+	}
+}
+
+static int
+nouveau_vm_map_pgt(struct nouveau_vm *vm, u32 pde, u32 type)
+{
+	struct nouveau_mmu *mmu = vm->mmu;
+	struct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
+	struct nouveau_vm_pgd *vpgd;
+	struct nouveau_gpuobj *pgt;
+	int big = (type != mmu->spg_shift);
+	u32 pgt_size;
+	int ret;
+
+	pgt_size  = (1 << (mmu->pgt_bits + 12)) >> type;
+	pgt_size *= 8;
+
+	mutex_unlock(&nv_subdev(mmu)->mutex);
+	ret = nouveau_gpuobj_new(nv_object(vm->mmu), NULL, pgt_size, 0x1000,
+				 NVOBJ_FLAG_ZERO_ALLOC, &pgt);
+	mutex_lock(&nv_subdev(mmu)->mutex);
+	if (unlikely(ret))
+		return ret;
+
+	/* someone beat us to filling the PDE while we didn't have the lock */
+	if (unlikely(vpgt->refcount[big]++)) {
+		mutex_unlock(&nv_subdev(mmu)->mutex);
+		nouveau_gpuobj_ref(NULL, &pgt);
+		mutex_lock(&nv_subdev(mmu)->mutex);
+		return 0;
+	}
+
+	vpgt->obj[big] = pgt;
+	list_for_each_entry(vpgd, &vm->pgd_list, head) {
+		mmu->map_pgt(vpgd->obj, pde, vpgt->obj);
+	}
+
+	return 0;
+}
+
+int
+nouveau_vm_get(struct nouveau_vm *vm, u64 size, u32 page_shift,
+	       u32 access, struct nouveau_vma *vma)
+{
+	struct nouveau_mmu *mmu = vm->mmu;
+	u32 align = (1 << page_shift) >> 12;
+	u32 msize = size >> 12;
+	u32 fpde, lpde, pde;
+	int ret;
+
+	mutex_lock(&nv_subdev(mmu)->mutex);
+	ret = nouveau_mm_head(&vm->mm, 0, page_shift, msize, msize, align,
+			     &vma->node);
+	if (unlikely(ret != 0)) {
+		mutex_unlock(&nv_subdev(mmu)->mutex);
+		return ret;
+	}
+
+	fpde = (vma->node->offset >> mmu->pgt_bits);
+	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
+
+	for (pde = fpde; pde <= lpde; pde++) {
+		struct nouveau_vm_pgt *vpgt = &vm->pgt[pde - vm->fpde];
+		int big = (vma->node->type != mmu->spg_shift);
+
+		if (likely(vpgt->refcount[big])) {
+			vpgt->refcount[big]++;
+			continue;
+		}
+
+		ret = nouveau_vm_map_pgt(vm, pde, vma->node->type);
+		if (ret) {
+			if (pde != fpde)
+				nouveau_vm_unmap_pgt(vm, big, fpde, pde - 1);
+			nouveau_mm_free(&vm->mm, &vma->node);
+			mutex_unlock(&nv_subdev(mmu)->mutex);
+			return ret;
+		}
+	}
+	mutex_unlock(&nv_subdev(mmu)->mutex);
+
+	vma->vm = NULL;
+	nouveau_vm_ref(vm, &vma->vm, NULL);
+	vma->offset = (u64)vma->node->offset << 12;
+	vma->access = access;
+	return 0;
+}
+
+void
+nouveau_vm_put(struct nouveau_vma *vma)
+{
+	struct nouveau_vm *vm = vma->vm;
+	struct nouveau_mmu *mmu = vm->mmu;
+	u32 fpde, lpde;
+
+	if (unlikely(vma->node == NULL))
+		return;
+	fpde = (vma->node->offset >> mmu->pgt_bits);
+	lpde = (vma->node->offset + vma->node->length - 1) >> mmu->pgt_bits;
+
+	mutex_lock(&nv_subdev(mmu)->mutex);
+	nouveau_vm_unmap_pgt(vm, vma->node->type != mmu->spg_shift, fpde, lpde);
+	nouveau_mm_free(&vm->mm, &vma->node);
+	mutex_unlock(&nv_subdev(mmu)->mutex);
+
+	nouveau_vm_ref(NULL, &vma->vm, NULL);
+}
+
+int
+nouveau_vm_create(struct nouveau_mmu *mmu, u64 offset, u64 length,
+		  u64 mm_offset, u32 block, struct nouveau_vm **pvm)
+{
+	struct nouveau_vm *vm;
+	u64 mm_length = (offset + length) - mm_offset;
+	int ret;
+
+	vm = kzalloc(sizeof(*vm), GFP_KERNEL);
+	if (!vm)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&vm->pgd_list);
+	vm->mmu = mmu;
+	kref_init(&vm->refcount);
+	vm->fpde = offset >> (mmu->pgt_bits + 12);
+	vm->lpde = (offset + length - 1) >> (mmu->pgt_bits + 12);
+
+	vm->pgt  = vzalloc((vm->lpde - vm->fpde + 1) * sizeof(*vm->pgt));
+	if (!vm->pgt) {
+		kfree(vm);
+		return -ENOMEM;
+	}
+
+	ret = nouveau_mm_init(&vm->mm, mm_offset >> 12, mm_length >> 12,
+			      block >> 12);
+	if (ret) {
+		vfree(vm->pgt);
+		kfree(vm);
+		return ret;
+	}
+
+	*pvm = vm;
+
+	return 0;
+}
+
+int
+nouveau_vm_new(struct nouveau_device *device, u64 offset, u64 length,
+	       u64 mm_offset, struct nouveau_vm **pvm)
+{
+	struct nouveau_mmu *mmu = nouveau_mmu(device);
+	return mmu->create(mmu, offset, length, mm_offset, pvm);
+}
+
+static int
+nouveau_vm_link(struct nouveau_vm *vm, struct nouveau_gpuobj *pgd)
+{
+	struct nouveau_mmu *mmu = vm->mmu;
+	struct nouveau_vm_pgd *vpgd;
+	int i;
+
+	if (!pgd)
+		return 0;
+
+	vpgd = kzalloc(sizeof(*vpgd), GFP_KERNEL);
+	if (!vpgd)
+		return -ENOMEM;
+
+	nouveau_gpuobj_ref(pgd, &vpgd->obj);
+
+	mutex_lock(&nv_subdev(mmu)->mutex);
+	for (i = vm->fpde; i <= vm->lpde; i++)
+		mmu->map_pgt(pgd, i, vm->pgt[i - vm->fpde].obj);
+	list_add(&vpgd->head, &vm->pgd_list);
+	mutex_unlock(&nv_subdev(mmu)->mutex);
+	return 0;
+}
+
+static void
+nouveau_vm_unlink(struct nouveau_vm *vm, struct nouveau_gpuobj *mpgd)
+{
+	struct nouveau_mmu *mmu = vm->mmu;
+	struct nouveau_vm_pgd *vpgd, *tmp;
+	struct nouveau_gpuobj *pgd = NULL;
+
+	if (!mpgd)
+		return;
+
+	mutex_lock(&nv_subdev(mmu)->mutex);
+	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
+		if (vpgd->obj == mpgd) {
+			pgd = vpgd->obj;
+			list_del(&vpgd->head);
+			kfree(vpgd);
+			break;
+		}
+	}
+	mutex_unlock(&nv_subdev(mmu)->mutex);
+
+	nouveau_gpuobj_ref(NULL, &pgd);
+}
+
+static void
+nouveau_vm_del(struct kref *kref)
+{
+	struct nouveau_vm *vm = container_of(kref, typeof(*vm), refcount);
+	struct nouveau_vm_pgd *vpgd, *tmp;
+
+	list_for_each_entry_safe(vpgd, tmp, &vm->pgd_list, head) {
+		nouveau_vm_unlink(vm, vpgd->obj);
+	}
+
+	nouveau_mm_fini(&vm->mm);
+	vfree(vm->pgt);
+	kfree(vm);
+}
+
+int
+nouveau_vm_ref(struct nouveau_vm *ref, struct nouveau_vm **ptr,
+	       struct nouveau_gpuobj *pgd)
+{
+	if (ref) {
+		int ret = nouveau_vm_link(ref, pgd);
+		if (ret)
+			return ret;
+
+		kref_get(&ref->refcount);
+	}
+
+	if (*ptr) {
+		nouveau_vm_unlink(*ptr, pgd);
+		kref_put(&(*ptr)->refcount, nouveau_vm_del);
+	}
+
+	*ptr = ref;
+	return 0;
+}
