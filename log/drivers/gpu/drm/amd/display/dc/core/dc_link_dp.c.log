commit 7d1ee78f033a1b478cfa78eb4e11dd40e060f977
Author: Vladimir Stempen <vladimir.stempen@amd.com>
Date:   Tue Apr 28 13:04:35 2020 -0400

    drm/amd/display: DP training to set properly SCRAMBLING_DISABLE
    
    [Why]
    DP training sequence to set SCRAMBLING_DISABLE bit properly based on
    training pattern - per DP Spec.
    
    [How]
    Update dpcd_pattern.v1_4.SCRAMBLING_DISABLE with 1 for TPS1, TPS2, TPS3,
    but not for TPS4.
    
    Signed-off-by: Vladimir Stempen <vladimir.stempen@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1db592372435..91cd884d6f25 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -219,6 +219,30 @@ static enum dpcd_training_patterns
 	return dpcd_tr_pattern;
 }
 
+static uint8_t dc_dp_initialize_scrambling_data_symbols(
+	struct dc_link *link,
+	enum dc_dp_training_pattern pattern)
+{
+	uint8_t disable_scrabled_data_symbols = 0;
+
+	switch (pattern) {
+	case DP_TRAINING_PATTERN_SEQUENCE_1:
+	case DP_TRAINING_PATTERN_SEQUENCE_2:
+	case DP_TRAINING_PATTERN_SEQUENCE_3:
+		disable_scrabled_data_symbols = 1;
+		break;
+	case DP_TRAINING_PATTERN_SEQUENCE_4:
+		disable_scrabled_data_symbols = 0;
+		break;
+	default:
+		ASSERT(0);
+		DC_LOG_HW_LINK_TRAINING("%s: Invalid HW Training pattern: %d\n",
+			__func__, pattern);
+		break;
+	}
+	return disable_scrabled_data_symbols;
+}
+
 static inline bool is_repeater(struct dc_link *link, uint32_t offset)
 {
 	return (!link->is_lttpr_mode_transparent && offset != 0);
@@ -251,6 +275,9 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_pattern.v1_4.TRAINING_PATTERN_SET =
 		dc_dp_training_pattern_to_dpcd_training_pattern(link, pattern);
 
+	dpcd_pattern.v1_4.SCRAMBLING_DISABLE =
+		dc_dp_initialize_scrambling_data_symbols(link, pattern);
+
 	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - DP_TRAINING_PATTERN_SET]
 		= dpcd_pattern.raw;
 

commit 49eea1c6573a6e31d79783c716b9001c968e5662
Merge: a1fb54896239 37e4f052cc97
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu May 14 13:21:30 2020 +1000

    Merge tag 'amd-drm-next-5.8-2020-05-12' of git://people.freedesktop.org/~agd5f/linux into drm-next
    
    amd-drm-next-5.8-2020-05-12:
    
    amdgpu:
    - Misc cleanups
    - RAS fixes
    - Expose FP16 for modesetting
    - DP 1.4 compliance test fixes
    - Clockgating fixes
    - MAINTAINERS update
    - Soft recovery for gfx10
    - Runtime PM cleanups
    - PSP code cleanups
    
    amdkfd:
    - Track GPU memory utilization per process
    - Report PCI domain in topology
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    From: Alex Deucher <alexdeucher@gmail.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200512213703.4039-1-alexander.deucher@amd.com

commit 370fb6b0aaf07c66a3317d5b35fba4345b31035c
Merge: 937eea297e26 b8020b0304c8
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri May 8 13:31:06 2020 +1000

    Merge tag 'amd-drm-next-5.8-2020-04-30' of git://people.freedesktop.org/~agd5f/linux into drm-next
    
    amd-drm-next-5.8-2020-04-30:
    
    amdgpu:
    - SR-IOV fixes
    - SDMA fix for Navi
    - VCN 2.5 DPG fixes
    - Display fixes
    - Display stuttering fixes for pageflip and cursor
    - Add support for handling encrypted GPU memory
    - Add UAPI for encrypted GPU memory
    - Rework IB pool handling
    
    amdkfd:
    - Expose asic revision in topology
    - Add UAPI for GWS (Global Wave Sync) resource management
    
    UAPI:
    - Add amdgpu UAPI for encrypted GPU memory
      Used by: https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/4401
    - Add amdkfd UAPI for GWS (Global Wave Sync) resource management
      Thunk usage of KFD ioctl: https://github.com/RadeonOpenCompute/ROCT-Thunk-Interface/blob/roc-2.8.0/src/queues.c#L840
      ROCr usage of Thunk API: https://github.com/RadeonOpenCompute/ROCR-Runtime/blob/roc-3.1.0/src/core/runtime/amd_gpu_agent.cpp#L597
      HCC code using ROCr API: https://github.com/RadeonOpenCompute/hcc/blob/98ee9f34945d3b5f572d7a4c15cbffa506487734/lib/hsa/mcwamp_hsa.cpp#L2161
      HIP code using HCC API: https://github.com/ROCm-Developer-Tools/HIP/blob/cf8589b8c8a40ddcc55fa3a51e23390a49824130/src/hip_module.cpp#L567
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    From: Alex Deucher <alexdeucher@gmail.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200430212951.3902-1-alexander.deucher@amd.com

commit 3852ee795324ba7d9bc06acbc9b50eb1da2a9c7d
Author: Chen Zhou <chenzhou10@huawei.com>
Date:   Thu May 7 21:50:23 2020 +0800

    drm/amd/display: remove duplicate headers
    
    Remove duplicate headers which are included twice.
    
    Signed-off-by: Chen Zhou <chenzhou10@huawei.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index f9fa0f7712b3..ebad1787f5cb 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -13,7 +13,6 @@
 #include "core_status.h"
 #include "dpcd_defs.h"
 
-#include "resource.h"
 #define DC_LOGGER \
 	link->ctx->logger
 

commit 937eea297e26effac6809a0bf8c20e6ca9d90b9a
Merge: 126a34061eec e748f07d00c1
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Apr 30 11:08:54 2020 +1000

    Merge tag 'amd-drm-next-5.8-2020-04-24' of git://people.freedesktop.org/~agd5f/linux into drm-next
    
    amd-drm-next-5.8-2020-04-24:
    
    amdgpu:
    - Documentation improvements
    - Enable FRU chip access on boards that support it
    - RAS updates
    - SR-IOV updates
    - Powerplay locking fixes for older SMU versions
    - VCN DPG (dynamic powergating) cleanup
    - VCN 2.5 DPG enablement
    - Rework GPU scheduler handling
    - Improve scheduler priority handling
    - Add SPM (streaming performance monitor) golden settings for navi
    - GFX10 clockgating fixes
    - DC ABM (automatic backlight modulation) fixes
    - DC cursor and plane fixes
    - DC watermark fixes
    - DC clock handling fixes
    - DC color management fixes
    - GPU reset fixes
    - Clean up MMIO access macros
    - EEPROM access fixes
    - Misc code cleanups
    
    amdkfd:
    - Misc code cleanups
    
    radeon:
    - Clean up safe reg list generation
    - Misc code cleanups
    
    From: Alex Deucher <alexdeucher@gmail.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200424190827.4542-1-alexander.deucher@amd.com

commit a96f661a471596f3f2945b335e8eac849e9b8a15
Author: Colin Ian King <colin.king@canonical.com>
Date:   Thu Apr 23 15:17:28 2020 +0100

    drm/amd/display: remove redundant assignment to variable ret
    
    The variable ret is being initialized with a value that is never read
    and it is being updated later with a new value. The initialization is
    redundant and can be removed.
    
    Addresses-Coverity: ("Unused value")
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a87302f729c7..f9fa0f7712b3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4231,7 +4231,7 @@ void dpcd_set_source_specific_data(struct dc_link *link)
 {
 	const uint32_t post_oui_delay = 30; // 30ms
 	uint8_t dspc = 0;
-	enum dc_status ret = DC_ERROR_UNEXPECTED;
+	enum dc_status ret;
 
 	ret = core_link_read_dpcd(link, DP_DOWN_STREAM_PORT_COUNT, &dspc,
 				  sizeof(dspc));

commit 422d9091f7be46b0d38c562a5750195af832370a
Author: Xiaodong Yan <Xiaodong.Yan@amd.com>
Date:   Thu Apr 9 17:37:40 2020 +0800

    drm/amd/display: blank dp stream before re-train the link
    
    [Why]
    When link loss happened, monitor can not light up if only re-train the
    link.
    
    [How]
    Blank all the DP streams on this link before re-train the link, and then
    unblank the stream
    
    Signed-off-by: Xiaodong Yan <Xiaodong.Yan@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5d2ae2fb7e45..a87302f729c7 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2899,6 +2899,12 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 					sizeof(hpd_irq_dpcd_data),
 					"Status: ");
 
+		for (i = 0; i < MAX_PIPES; i++) {
+			pipe_ctx = &link->dc->current_state->res_ctx.pipe_ctx[i];
+			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link)
+				link->dc->hwss.blank_stream(pipe_ctx);
+		}
+
 		for (i = 0; i < MAX_PIPES; i++) {
 			pipe_ctx = &link->dc->current_state->res_ctx.pipe_ctx[i];
 			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link)
@@ -2918,6 +2924,12 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 		if (pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST)
 			dc_link_reallocate_mst_payload(link);
 
+		for (i = 0; i < MAX_PIPES; i++) {
+			pipe_ctx = &link->dc->current_state->res_ctx.pipe_ctx[i];
+			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link)
+				link->dc->hwss.unblank_stream(pipe_ctx, &previous_link_settings);
+		}
+
 		status = false;
 		if (out_link_loss)
 			*out_link_loss = true;

commit 967727021e75f6c1d9972d5a87f759155f935d75
Author: Aurabindo Pillai <aurabindo.pillai@amd.com>
Date:   Tue Apr 7 10:22:27 2020 -0400

    drm/amd/display: DispalyPort: Write OUI only if panel supports it
    
    [why]
    Organizational Unit Identifier register is optional, and its
    presence is published via Down Stream Port Count register.
    Writing this register when not available will result in errors
    
    [how]
    Read this register and continue writing OUI only if the panel
    has the support advertised.
    
    Signed-off-by: Aurabindo Pillai <aurabindo.pillai@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3408c36ace48..5d2ae2fb7e45 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4218,6 +4218,21 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 void dpcd_set_source_specific_data(struct dc_link *link)
 {
 	const uint32_t post_oui_delay = 30; // 30ms
+	uint8_t dspc = 0;
+	enum dc_status ret = DC_ERROR_UNEXPECTED;
+
+	ret = core_link_read_dpcd(link, DP_DOWN_STREAM_PORT_COUNT, &dspc,
+				  sizeof(dspc));
+
+	if (ret != DC_OK) {
+		DC_LOG_ERROR("Error in DP aux read transaction,"
+			     " not writing source specific data\n");
+		return;
+	}
+
+	/* Return if OUI unsupported */
+	if (!(dspc & DP_OUI_SUPPORT))
+		return;
 
 	if (!link->dc->vendor_signature.is_valid) {
 		struct dpcd_amd_signature amd_signature;

commit d1ebfdd8d0fc30ff65b85a4bf3fa9e5e35006466
Author: Wyatt Wood <wyatt.wood@amd.com>
Date:   Tue Mar 31 09:31:16 2020 -0400

    drm/amd/display: Unify psr feature flags
    
    [Why]
    As it stands, psr has feature flags in dm, stream, and link. Most are
    not defined well enough, and different dm layers have different uses for
    these same flags.
    
    [How]
    We define a new structure called psr_settings in dc_link that will hold
    the following psr feature flags:
    
    psr_feature_enable - psr is supported
    psr_allow_active - psr is currently active
    psr_version - internal psr version supported
    psr_frame_capture_indication_req
    psr_sdp_transmit_line_num_deadline
    The last two flags were moved out of the power module
    for the purposes of consolidating psr flags.
    Their use is already well-defined.
    
    Psr caps reported by sink will also be stored in dc_link,
    in dpcd_caps.psr_caps.
    
    Signed-off-by: Wyatt Wood <wyatt.wood@amd.com>
    Reviewed-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 74cf06aacd6c..3408c36ace48 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2417,7 +2417,7 @@ static bool handle_hpd_irq_psr_sink(struct dc_link *link)
 {
 	union dpcd_psr_configuration psr_configuration;
 
-	if (!link->psr_feature_enabled)
+	if (!link->psr_settings.psr_feature_enabled)
 		return false;
 
 	dm_helpers_dp_read_dpcd(

commit 8ccf0e20769d96b8d0ccbfcb56bc7ca7874154a4
Author: Wenjing Liu <wenjing.liu@amd.com>
Date:   Sun Apr 5 16:41:13 2020 -0400

    drm/amd/display: determine USB C DP2 mode only when USB DP Alt is enabled
    
    [why]
    When display is connected with a native DP port, DP2 mode register value
    is a don't care. Driver mistakenly reduce max supported lane count to 2
    lane based on the don't care value.
    
    [how]
    Add additional check only if USB C DP alt mode is enabled, we will
    determine max lane count supported based on current mode.
    
    Signed-off-by: Wenjing Liu <wenjing.liu@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7cbb1efb4f68..74cf06aacd6c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1710,19 +1710,10 @@ bool dc_link_dp_sync_lt_end(struct dc_link *link, bool link_down)
 
 static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 {
-	/* Set Default link settings */
-	struct dc_link_settings max_link_cap = {LANE_COUNT_FOUR, LINK_RATE_HIGH,
-			LINK_SPREAD_05_DOWNSPREAD_30KHZ, false, 0};
-
-	/* Higher link settings based on feature supported */
-	if (link->link_enc->features.flags.bits.IS_HBR2_CAPABLE)
-		max_link_cap.link_rate = LINK_RATE_HIGH2;
-
-	if (link->link_enc->features.flags.bits.IS_HBR3_CAPABLE)
-		max_link_cap.link_rate = LINK_RATE_HIGH3;
+	struct dc_link_settings max_link_cap = {0};
 
-	if (link->link_enc->funcs->get_max_link_cap)
-		link->link_enc->funcs->get_max_link_cap(link->link_enc, &max_link_cap);
+	/* get max link encoder capability */
+	link->link_enc->funcs->get_max_link_cap(link->link_enc, &max_link_cap);
 
 	/* Lower link settings based on sink's link cap */
 	if (link->reported_link_cap.lane_count < max_link_cap.lane_count)

commit 8811d9eb4dfa6ce6fbbb8dabcec1e049f3e03329
Author: Animesh Manna <animesh.manna@intel.com>
Date:   Mon Mar 16 16:07:53 2020 +0530

    drm/amd/display: Align macro name as per DP spec
    
    [Why]:
    Aligh with DP spec wanted to follow same naming convention.
    
    [How]:
    Changed the macro name of the dpcd address used for getting requested
    test-pattern.
    
    Cc: Harry Wentland <harry.wentland@amd.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Manasi Navare <manasi.d.navare@intel.com>
    Signed-off-by: Animesh Manna <animesh.manna@intel.com>
    Signed-off-by: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200316103759.12867-2-animesh.manna@intel.com

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7cbb1efb4f68..aa3c45a69b5e 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2530,7 +2530,7 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 	/* get phy test pattern and pattern parameters from DP receiver */
 	core_link_read_dpcd(
 			link,
-			DP_TEST_PHY_PATTERN,
+			DP_PHY_TEST_PATTERN,
 			&dpcd_test_pattern.raw,
 			sizeof(dpcd_test_pattern));
 	core_link_read_dpcd(

commit ef65c702d40637ed9ee25edc8e8a994168a32377
Author: Jerry (Fangzhi) Zuo <Jerry.Zuo@amd.com>
Date:   Mon Mar 2 09:55:10 2020 -0500

    drm/amd/display: Fix test pattern color space inconsistency for Linux
    
    [why]
    When reprogram MSA with updated color space, the test color space shows
    inconsistency. Linux has separate routine to set up test pattern color
    space, but it fails to configure RGB.
    
    [How]
    Add RGB to test pattern.
    
    Fixes: 43563bc2e6a769 ("drm/amd/display: update MSA and VSC SDP on video test pattern request")
    Signed-off-by: Jerry (Fangzhi) Zuo <Jerry.Zuo@amd.com>
    Reviewed-by: Hersen Wu <hersenxs.wu@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index f822049cc590..7cbb1efb4f68 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2674,9 +2674,12 @@ static void dp_test_send_link_test_pattern(struct dc_link *link)
 	break;
 	}
 
-	test_pattern_color_space = dpcd_test_params.bits.YCBCR_COEFS ?
-			DP_TEST_PATTERN_COLOR_SPACE_YCBCR709 :
-			DP_TEST_PATTERN_COLOR_SPACE_YCBCR601;
+	if (dpcd_test_params.bits.CLR_FORMAT == 0)
+		test_pattern_color_space = DP_TEST_PATTERN_COLOR_SPACE_RGB;
+	else
+		test_pattern_color_space = dpcd_test_params.bits.YCBCR_COEFS ?
+				DP_TEST_PATTERN_COLOR_SPACE_YCBCR709 :
+				DP_TEST_PATTERN_COLOR_SPACE_YCBCR601;
 
 	dc_link_dp_set_test_pattern(
 			link,

commit 473e3f7720f652716eb7a5b8f49628d6f66b1e87
Author: Mario Kleiner <mario.kleiner.de@gmail.com>
Date:   Fri Feb 28 22:36:07 2020 +0100

    drm/amd/display: Add link_rate quirk for Apple 15" MBP 2017
    
    This fixes a problem found on the MacBookPro 2017 Retina panel:
    
    The panel reports 10 bpc color depth in its EDID, and the
    firmware chooses link settings at boot which support enough
    bandwidth for 10 bpc (324000 kbit/sec aka LINK_RATE_RBR2
    aka 0xc), but the DP_MAX_LINK_RATE dpcd register only reports
    2.7 Gbps (multiplier value 0xa) as possible, in direct
    contradiction of what the firmware successfully set up.
    
    This restricts the panel to 8 bpc, not providing the full
    color depth of the panel on Linux <= 5.5. Additionally, commit
    '4a8ca46bae8a ("drm/amd/display: Default max bpc to 16 for eDP")'
    introduced into Linux 5.6-rc1 will unclamp panel depth to
    its full 10 bpc, thereby requiring a eDP bandwidth for all
    modes that exceeds the bandwidth available and causes all modes
    to fail validation -> No modes for the laptop panel -> failure
    to set any mode -> Panel goes dark.
    
    This patch adds a quirk specific to the MBP 2017 15" Retina
    panel to override reported max link rate to the correct maximum
    of 0xc = LINK_RATE_RBR2 to fix the darkness and reduced display
    precision.
    
    Please apply for Linux 5.6+ to avoid regressing Apple MBP panel
    support.
    
    Signed-off-by: Mario Kleiner <mario.kleiner.de@gmail.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 9553755be286..f822049cc590 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3438,6 +3438,17 @@ static bool retrieve_link_cap(struct dc_link *link)
 		sink_id.ieee_device_id,
 		sizeof(sink_id.ieee_device_id));
 
+	/* Quirk Apple MBP 2017 15" Retina panel: Wrong DP_MAX_LINK_RATE */
+	{
+		uint8_t str_mbp_2017[] = { 101, 68, 21, 101, 98, 97 };
+
+		if ((link->dpcd_caps.sink_dev_id == 0x0010fa) &&
+		    !memcmp(link->dpcd_caps.sink_dev_id_str, str_mbp_2017,
+			    sizeof(str_mbp_2017))) {
+			link->reported_link_cap.link_rate = 0x0c;
+		}
+	}
+
 	core_link_read_dpcd(
 		link,
 		DP_SINK_HW_REVISION_START,

commit e6b11b43cd565cbf69dd67b85ff3dbed18b4d491
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Mon Feb 24 17:19:02 2020 -0500

    drm/amd/display: separate FEC capability from fec debug flag
    
    [why]
    FEC capability query should not be affected by debugging decision on
    whether to disable FEC. We should not determine if display supports FEC
    by checking debug option.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Ashley Thomas <Ashley.Thomas2@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index fc3664dd5e88..9553755be286 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4152,7 +4152,7 @@ void dp_set_fec_ready(struct dc_link *link, bool ready)
 	struct link_encoder *link_enc = link->link_enc;
 	uint8_t fec_config = 0;
 
-	if (!dc_link_is_fec_supported(link))
+	if (!dc_link_is_fec_supported(link) || link->dc->debug.disable_fec)
 		return;
 
 	if (link_enc->funcs->fec_set_ready &&
@@ -4187,7 +4187,7 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 {
 	struct link_encoder *link_enc = link->link_enc;
 
-	if (!dc_link_is_fec_supported(link))
+	if (!dc_link_is_fec_supported(link) || link->dc->debug.disable_fec)
 		return;
 
 	if (link_enc->funcs->fec_set_enable &&

commit bcc5042a2209037da2784ac88d9e3dad6c6ed915
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Tue Feb 18 17:05:05 2020 -0500

    drm/amd/display: set lttpr mode before link settings
    
    [Why]
    Some lttpr devices do not work properly when lttpr mode is configured
    after link settings.
    
    [How]
    Move lttpr configuration before lane settings.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index bf5406eaf7b7..fc3664dd5e88 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1446,6 +1446,10 @@ enum link_training_result dc_link_dp_perform_link_training(
 			&link->preferred_training_settings,
 			&lt_settings);
 
+	/* Configure lttpr mode */
+	if (!link->is_lttpr_mode_transparent)
+		configure_lttpr_mode(link);
+
 	if (link->ctx->dc->work_arounds.lt_early_cr_pattern)
 		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
 
@@ -1460,8 +1464,6 @@ enum link_training_result dc_link_dp_perform_link_training(
 	dp_set_fec_ready(link, fec_enable);
 
 	if (!link->is_lttpr_mode_transparent) {
-		/* Configure lttpr mode */
-		configure_lttpr_mode(link);
 
 		/* 2. perform link training (set link training done
 		 *  to false is done as well)

commit 10b4e64e58b4a9d7669bc6682b3fd12ae62744b2
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Feb 11 10:27:21 2020 -0500

    drm/amd/display: program DPG_OFFSET_SEGMENT for odm_pipe
    
    [why]
    When test pattern is enabled with ODM combine, test pattern is generated
    by piecing multiple DPGs image together.  The current code will program
    all DPGs with horizontal offset of 0. This will cause all DPGs to output
    the beginning of the pattern. Instead each DPG should program a
    horizontal offset of its x position to form a continous pattern when
    pieced together.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c81f55b28497..bf5406eaf7b7 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3720,6 +3720,8 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			struct pipe_ctx *odm_pipe;
 			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
+			int offset = 0;
+			int dpg_width = width;
 
 			switch (test_pattern_color_space) {
 			case DP_TEST_PATTERN_COLOR_SPACE_RGB:
@@ -3741,28 +3743,31 @@ static void set_crtc_test_pattern(struct dc_link *link,
 
 			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe)
 				opp_cnt++;
+			dpg_width = width / opp_cnt;
+			offset = dpg_width;
 
-			width /= opp_cnt;
+			opp->funcs->opp_set_disp_pattern_generator(opp,
+				controller_test_pattern,
+				controller_color_space,
+				color_depth,
+				NULL,
+				dpg_width,
+				height,
+				0);
 
 			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) {
 				struct output_pixel_processor *odm_opp = odm_pipe->stream_res.opp;
-
 				odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params);
 				odm_opp->funcs->opp_set_disp_pattern_generator(odm_opp,
 					controller_test_pattern,
 					controller_color_space,
 					color_depth,
 					NULL,
-					width,
-					height);
+					dpg_width,
+					height,
+					offset);
+				offset += offset;
 			}
-			opp->funcs->opp_set_disp_pattern_generator(opp,
-				controller_test_pattern,
-				controller_color_space,
-				color_depth,
-				NULL,
-				width,
-				height);
 		}
 	}
 	break;
@@ -3779,11 +3784,12 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
 			struct pipe_ctx *odm_pipe;
 			int opp_cnt = 1;
+			int dpg_width = width;
 
 			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe)
 				opp_cnt++;
 
-			width /= opp_cnt;
+			dpg_width = width / opp_cnt;
 			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) {
 				struct output_pixel_processor *odm_opp = odm_pipe->stream_res.opp;
 
@@ -3793,16 +3799,18 @@ static void set_crtc_test_pattern(struct dc_link *link,
 					CONTROLLER_DP_COLOR_SPACE_UDEFINED,
 					color_depth,
 					NULL,
-					width,
-					height);
+					dpg_width,
+					height,
+					0);
 			}
 			opp->funcs->opp_set_disp_pattern_generator(opp,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 				CONTROLLER_DP_COLOR_SPACE_UDEFINED,
 				color_depth,
 				NULL,
-				width,
-				height);
+				dpg_width,
+				height,
+				0);
 		}
 	}
 	break;

commit e8f9ecf261fe1de0213710ce1fc53d5ca75d96ce
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Jan 22 11:55:48 2020 -0500

    drm/amd/display: add vsc update support for test pattern request
    
    [how]
    Allow vsc info packet if vsc is supported.  Update vsc based on test
    pattern request.  Remove dpg_is_blanked polling, apply hardware global
    lock instead to ensure double buffered dpg is updated with vsc in one
    frame
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c805bec18044..c81f55b28497 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3720,7 +3720,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			struct pipe_ctx *odm_pipe;
 			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
-			uint16_t count = 0;
 
 			switch (test_pattern_color_space) {
 			case DP_TEST_PATTERN_COLOR_SPACE_RGB:
@@ -3764,12 +3763,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				NULL,
 				width,
 				height);
-			/* wait for dpg to blank pixel data with test pattern */
-			for (count = 0; count < 1000; count++) {
-				if (opp->funcs->dpg_is_blanked(opp))
-					break;
-				udelay(100);
-			}
 		}
 	}
 	break;
@@ -3987,6 +3980,11 @@ bool dc_link_dp_set_test_pattern(
 		default:
 			break;
 		}
+
+		if (pipe_ctx->stream_res.tg->funcs->lock_doublebuffer_enable)
+			pipe_ctx->stream_res.tg->funcs->lock_doublebuffer_enable(
+					pipe_ctx->stream_res.tg);
+		pipe_ctx->stream_res.tg->funcs->lock(pipe_ctx->stream_res.tg);
 		/* update MSA to requested color space */
 		pipe_ctx->stream_res.stream_enc->funcs->dp_set_stream_attribute(pipe_ctx->stream_res.stream_enc,
 				&pipe_ctx->stream->timing,
@@ -3994,9 +3992,27 @@ bool dc_link_dp_set_test_pattern(
 				pipe_ctx->stream->use_vsc_sdp_for_colorimetry,
 				link->dpcd_caps.dprx_feature.bits.SST_SPLIT_SDP_CAP);
 
+		if (pipe_ctx->stream->use_vsc_sdp_for_colorimetry) {
+			if (test_pattern == DP_TEST_PATTERN_COLOR_SQUARES_CEA)
+				pipe_ctx->stream->vsc_infopacket.sb[17] |= (1 << 7); // sb17 bit 7 Dynamic Range: 0 = VESA range, 1 = CTA range
+			else
+				pipe_ctx->stream->vsc_infopacket.sb[17] &= ~(1 << 7);
+			resource_build_info_frame(pipe_ctx);
+			link->dc->hwss.update_info_frame(pipe_ctx);
+		}
+
 		/* CRTC Patterns */
 		set_crtc_test_pattern(link, pipe_ctx, test_pattern, test_pattern_color_space);
-
+		pipe_ctx->stream_res.tg->funcs->unlock(pipe_ctx->stream_res.tg);
+		pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg,
+				CRTC_STATE_VACTIVE);
+		pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg,
+				CRTC_STATE_VBLANK);
+		pipe_ctx->stream_res.tg->funcs->wait_for_state(pipe_ctx->stream_res.tg,
+				CRTC_STATE_VACTIVE);
+		if (pipe_ctx->stream_res.tg->funcs->lock_doublebuffer_disable)
+			pipe_ctx->stream_res.tg->funcs->lock_doublebuffer_disable(
+					pipe_ctx->stream_res.tg);
 		/* Set Test Pattern state */
 		link->test_pattern_enabled = true;
 	}

commit c14b726ee0ca387931b9605405de9dddd4e76ee5
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Feb 11 15:33:38 2020 -0500

    drm/amd/display: only include FEC overhead if both asic and display support FEC
    
    [why]
    Some asics don't support FEC but FEC overhead is added into link
    bandwidth calculation by mistake. This causes certain timing cannot be
    validated.
    
    [how]
    Only include FEC overhead if both asic and display support FEC.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Ashley Thomas <Ashley.Thomas2@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5aa3de9644ea..c805bec18044 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4126,8 +4126,7 @@ void dp_set_fec_ready(struct dc_link *link, bool ready)
 	struct link_encoder *link_enc = link->link_enc;
 	uint8_t fec_config = 0;
 
-	if (link->dc->debug.disable_fec ||
-			IS_FPGA_MAXIMUS_DC(link->ctx->dce_environment))
+	if (!dc_link_is_fec_supported(link))
 		return;
 
 	if (link_enc->funcs->fec_set_ready &&
@@ -4162,8 +4161,7 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 {
 	struct link_encoder *link_enc = link->link_enc;
 
-	if (link->dc->debug.disable_fec ||
-			IS_FPGA_MAXIMUS_DC(link->ctx->dce_environment))
+	if (!dc_link_is_fec_supported(link))
 		return;
 
 	if (link_enc->funcs->fec_set_enable &&

commit 834a9a9f04c7088dce82812b9c34b36faa043e41
Author: Martin Leung <martin.leung@amd.com>
Date:   Thu Feb 13 15:40:06 2020 -0500

    drm/amd/display: Link training TPS1 workaround add back in dpcd
    
    [Why]
    Previously implemented early_cr_pattern we mistook dp_hw_link_settings
    for a redundant call of dpcd_set_link_settings
    
    [How]
    revert the changes to dpcd_set_link_settings calls for this workaround.
    Do not need to revert the entire change since it only affects patched
    case
    
    Signed-off-by: Martin Leung <martin.leung@amd.com>
    Reviewed-by: David Galiffi <David.Galiffi@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 93127bc90f3c..5aa3de9644ea 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1446,11 +1446,11 @@ enum link_training_result dc_link_dp_perform_link_training(
 			&link->preferred_training_settings,
 			&lt_settings);
 
-	/* 1. set link rate, lane count and spread. */
 	if (link->ctx->dc->work_arounds.lt_early_cr_pattern)
 		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
-	else
-		dpcd_set_link_settings(link, &lt_settings);
+
+	/* 1. set link rate, lane count and spread. */
+	dpcd_set_link_settings(link, &lt_settings);
 
 	if (link->preferred_training_settings.fec_enable != NULL)
 		fec_enable = *link->preferred_training_settings.fec_enable;
@@ -1669,11 +1669,11 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	dp_set_panel_mode(link, panel_mode);
 
 	/* Attempt to train with given link training settings */
-	/* Set link rate, lane count and spread. */
 	if (link->ctx->dc->work_arounds.lt_early_cr_pattern)
 		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
-	else
-		dpcd_set_link_settings(link, &lt_settings);
+
+	/* Set link rate, lane count and spread. */
+	dpcd_set_link_settings(link, &lt_settings);
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)

commit 82054678aeb66907acd63df7d1d5f9556e29a5cc
Author: Martin Leung <martin.leung@amd.com>
Date:   Wed Feb 12 15:38:51 2020 -0500

    drm/amd/display: Link training TPS1 workaround
    
    [Why]
    Previously implemented early_cr_pattern was link level but the whole
    asic should be affected.
    
    [How]
     - change old link flag to dc level
     - new bit in dc->work_arounds set by DM
    
    Signed-off-by: Martin Leung <martin.leung@amd.com>
    Reviewed-by: Joshua Aberback <Joshua.Aberback@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8de9d6f9a477..93127bc90f3c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -973,7 +973,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 	retries_cr = 0;
 	retry_count = 0;
 
-	if (!link->wa_flags.dp_early_cr_pattern)
+	if (!link->ctx->dc->work_arounds.lt_early_cr_pattern)
 		dp_set_hw_training_pattern(link, tr_pattern, offset);
 
 	/* najeeb - The synaptics MST hub can put the LT in
@@ -1446,11 +1446,11 @@ enum link_training_result dc_link_dp_perform_link_training(
 			&link->preferred_training_settings,
 			&lt_settings);
 
-	if (link->wa_flags.dp_early_cr_pattern)
-		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
-
 	/* 1. set link rate, lane count and spread. */
-	dpcd_set_link_settings(link, &lt_settings);
+	if (link->ctx->dc->work_arounds.lt_early_cr_pattern)
+		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
+	else
+		dpcd_set_link_settings(link, &lt_settings);
 
 	if (link->preferred_training_settings.fec_enable != NULL)
 		fec_enable = *link->preferred_training_settings.fec_enable;
@@ -1669,11 +1669,11 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	dp_set_panel_mode(link, panel_mode);
 
 	/* Attempt to train with given link training settings */
-	if (link->wa_flags.dp_early_cr_pattern)
-		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
-
 	/* Set link rate, lane count and spread. */
-	dpcd_set_link_settings(link, &lt_settings);
+	if (link->ctx->dc->work_arounds.lt_early_cr_pattern)
+		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
+	else
+		dpcd_set_link_settings(link, &lt_settings);
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)

commit b01f22ec88103d781f27aadb29277a35302db083
Author: David Galiffi <David.Galiffi@amd.com>
Date:   Wed Jan 29 17:02:32 2020 -0500

    drm/amd/display: Workaround required for link training reliability
    
    [Why]
    A software workaround is required for all vendor-built cards on platform.
    
    [How]
    When performing DP link training, we must send TPS1 before DPCD:100h is
    written with the proper bit rate value. This change must be applies in
    ALL cases when LT happens.
    
    Signed-off-by: David Galiffi <David.Galiffi@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c0fcee4b5b69..8de9d6f9a477 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -945,6 +945,17 @@ static enum link_training_result perform_channel_equalization_sequence(
 }
 #define TRAINING_AUX_RD_INTERVAL 100 //us
 
+static void start_clock_recovery_pattern_early(struct dc_link *link,
+		struct link_training_settings *lt_settings,
+		uint32_t offset)
+{
+	DC_LOG_HW_LINK_TRAINING("%s\n GPU sends TPS1. Wait 400us.\n",
+			__func__);
+	dp_set_hw_training_pattern(link, DP_TRAINING_PATTERN_SEQUENCE_1, offset);
+	dp_set_hw_lane_settings(link, lt_settings, offset);
+	udelay(400);
+}
+
 static enum link_training_result perform_clock_recovery_sequence(
 	struct dc_link *link,
 	struct link_training_settings *lt_settings,
@@ -962,7 +973,8 @@ static enum link_training_result perform_clock_recovery_sequence(
 	retries_cr = 0;
 	retry_count = 0;
 
-	dp_set_hw_training_pattern(link, tr_pattern, offset);
+	if (!link->wa_flags.dp_early_cr_pattern)
+		dp_set_hw_training_pattern(link, tr_pattern, offset);
 
 	/* najeeb - The synaptics MST hub can put the LT in
 	* infinite loop by switching the VS
@@ -1434,6 +1446,9 @@ enum link_training_result dc_link_dp_perform_link_training(
 			&link->preferred_training_settings,
 			&lt_settings);
 
+	if (link->wa_flags.dp_early_cr_pattern)
+		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
+
 	/* 1. set link rate, lane count and spread. */
 	dpcd_set_link_settings(link, &lt_settings);
 
@@ -1654,6 +1669,8 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	dp_set_panel_mode(link, panel_mode);
 
 	/* Attempt to train with given link training settings */
+	if (link->wa_flags.dp_early_cr_pattern)
+		start_clock_recovery_pattern_early(link, &lt_settings, DPRX);
 
 	/* Set link rate, lane count and spread. */
 	dpcd_set_link_settings(link, &lt_settings);

commit 8a683eb6311a03b655ab8ff368cd7b203074275c
Author: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
Date:   Tue Jan 28 18:01:48 2020 -0500

    drm/amd/display: Add AUX backlight register
    
    Introduce vendor-specific registers for handling backlight via AUX.
    
    Signed-off-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Reviewed-by: Nicholas Kazlauskas <Nicholas.Kazlauskas@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3bc05fa93ed5..c0fcee4b5b69 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -20,14 +20,6 @@
 
 #define DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE   0x50
 
-#define DP_SOURCE_TABLE_REVISION	    0x310
-#define DP_SOURCE_PAYLOAD_SIZE		    0x311
-#define DP_SOURCE_SINK_CAP		    0x317
-#define DP_SOURCE_BACKLIGHT_LEVEL	    0x320
-#define DP_SOURCE_BACKLIGHT_CURRENT_PEAK    0x326
-#define DP_SOURCE_BACKLIGHT_CONTROL	    0x32E
-#define DP_SOURCE_BACKLIGHT_ENABLE	    0x32F
-
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
 		PRE_EMPHASIS_LEVEL3,

commit 0136684f9b58b78fb2b8671e8eaa7bfc41330e0f
Author: Calvin Hou <Calvin.Hou@amd.com>
Date:   Mon Jan 27 14:40:41 2020 -0500

    drm/amd/display: Pass override OUI in to dc_init_data
    
    [WHY]
    Vendor dongle requires propietary OUI and handshake sequence.
    
    [HOW]
    Add a new structure to dc_init_data, to allow creator to pass
    an override vendor_oui. This value will be written to DP_SOURCE_OUI
    instead of AMD signature, when dpcd_set_source_specific_data is
    called.
    
    Signed-off-by: Calvin Hou <Calvin.Hou@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c5b45d17e8cd..3bc05fa93ed5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4179,25 +4179,32 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 
 void dpcd_set_source_specific_data(struct dc_link *link)
 {
-	struct dpcd_amd_signature amd_signature;
 	const uint32_t post_oui_delay = 30; // 30ms
 
-	amd_signature.AMD_IEEE_TxSignature_byte1 = 0x0;
-	amd_signature.AMD_IEEE_TxSignature_byte2 = 0x0;
-	amd_signature.AMD_IEEE_TxSignature_byte3 = 0x1A;
-	amd_signature.device_id_byte1 =
-			(uint8_t)(link->ctx->asic_id.chip_id);
-	amd_signature.device_id_byte2 =
-			(uint8_t)(link->ctx->asic_id.chip_id >> 8);
-	memset(&amd_signature.zero, 0, 4);
-	amd_signature.dce_version =
-			(uint8_t)(link->ctx->dce_version);
-	amd_signature.dal_version_byte1 = 0x0; // needed? where to get?
-	amd_signature.dal_version_byte2 = 0x0; // needed? where to get?
-
-	core_link_write_dpcd(link, DP_SOURCE_OUI,
-			(uint8_t *)(&amd_signature),
-			sizeof(amd_signature));
+	if (!link->dc->vendor_signature.is_valid) {
+		struct dpcd_amd_signature amd_signature;
+		amd_signature.AMD_IEEE_TxSignature_byte1 = 0x0;
+		amd_signature.AMD_IEEE_TxSignature_byte2 = 0x0;
+		amd_signature.AMD_IEEE_TxSignature_byte3 = 0x1A;
+		amd_signature.device_id_byte1 =
+				(uint8_t)(link->ctx->asic_id.chip_id);
+		amd_signature.device_id_byte2 =
+				(uint8_t)(link->ctx->asic_id.chip_id >> 8);
+		memset(&amd_signature.zero, 0, 4);
+		amd_signature.dce_version =
+				(uint8_t)(link->ctx->dce_version);
+		amd_signature.dal_version_byte1 = 0x0; // needed? where to get?
+		amd_signature.dal_version_byte2 = 0x0; // needed? where to get?
+
+		core_link_write_dpcd(link, DP_SOURCE_OUI,
+				(uint8_t *)(&amd_signature),
+				sizeof(amd_signature));
+
+	} else {
+		core_link_write_dpcd(link, DP_SOURCE_OUI,
+				link->dc->vendor_signature.data.raw,
+				sizeof(link->dc->vendor_signature.data.raw));
+	}
 
 	// Sink may need to configure internals based on vendor, so allow some
 	// time before proceeding with possibly vendor specific transactions

commit 96577cf82a1331732a71199522398120c649f1cf
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Tue Jan 14 15:39:07 2020 -0500

    drm/amd/display: linux enable oled panel support dc part
    
    [Why] old panel has been enabled for window driver but not linux.
    
    [How] enable oled panel support for linux. this patch is dc part.
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Hersen Wu <hersenxs.wu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 07b9aa1d01af..c5b45d17e8cd 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -20,6 +20,14 @@
 
 #define DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE   0x50
 
+#define DP_SOURCE_TABLE_REVISION	    0x310
+#define DP_SOURCE_PAYLOAD_SIZE		    0x311
+#define DP_SOURCE_SINK_CAP		    0x317
+#define DP_SOURCE_BACKLIGHT_LEVEL	    0x320
+#define DP_SOURCE_BACKLIGHT_CURRENT_PEAK    0x326
+#define DP_SOURCE_BACKLIGHT_CONTROL	    0x32E
+#define DP_SOURCE_BACKLIGHT_ENABLE	    0x32F
+
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
 		PRE_EMPHASIS_LEVEL3,
@@ -3166,6 +3174,23 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		link->wa_flags.dp_keep_receiver_powered = false;
 }
 
+/* Read additional sink caps defined in source specific DPCD area
+ * This function currently only reads from SinkCapability address (DP_SOURCE_SINK_CAP)
+ */
+static bool dpcd_read_sink_ext_caps(struct dc_link *link)
+{
+	uint8_t dpcd_data;
+
+	if (!link)
+		return false;
+
+	if (core_link_read_dpcd(link, DP_SOURCE_SINK_CAP, &dpcd_data, 1) != DC_OK)
+		return false;
+
+	link->dpcd_sink_ext_caps.raw = dpcd_data;
+	return true;
+}
+
 static bool retrieve_link_cap(struct dc_link *link)
 {
 	/* DP_ADAPTER_CAP - DP_DPCD_REV + 1 == 16 and also DP_DSC_BITS_PER_PIXEL_INC - DP_DSC_SUPPORT + 1 == 16,
@@ -3438,6 +3463,9 @@ static bool retrieve_link_cap(struct dc_link *link)
 				sizeof(link->dpcd_caps.dsc_caps.dsc_ext_caps.raw));
 	}
 
+	if (!dpcd_read_sink_ext_caps(link))
+		link->dpcd_sink_ext_caps.raw = 0;
+
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 
@@ -3590,6 +3618,8 @@ void detect_edp_sink_caps(struct dc_link *link)
 		}
 	}
 	link->verified_link_cap = link->reported_link_cap;
+
+	dc_link_set_default_brightness_aux(link);
 }
 
 void dc_link_dp_enable_hpd(const struct dc_link *link)
@@ -4147,3 +4177,141 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 	}
 }
 
+void dpcd_set_source_specific_data(struct dc_link *link)
+{
+	struct dpcd_amd_signature amd_signature;
+	const uint32_t post_oui_delay = 30; // 30ms
+
+	amd_signature.AMD_IEEE_TxSignature_byte1 = 0x0;
+	amd_signature.AMD_IEEE_TxSignature_byte2 = 0x0;
+	amd_signature.AMD_IEEE_TxSignature_byte3 = 0x1A;
+	amd_signature.device_id_byte1 =
+			(uint8_t)(link->ctx->asic_id.chip_id);
+	amd_signature.device_id_byte2 =
+			(uint8_t)(link->ctx->asic_id.chip_id >> 8);
+	memset(&amd_signature.zero, 0, 4);
+	amd_signature.dce_version =
+			(uint8_t)(link->ctx->dce_version);
+	amd_signature.dal_version_byte1 = 0x0; // needed? where to get?
+	amd_signature.dal_version_byte2 = 0x0; // needed? where to get?
+
+	core_link_write_dpcd(link, DP_SOURCE_OUI,
+			(uint8_t *)(&amd_signature),
+			sizeof(amd_signature));
+
+	// Sink may need to configure internals based on vendor, so allow some
+	// time before proceeding with possibly vendor specific transactions
+	msleep(post_oui_delay);
+}
+
+bool dc_link_set_backlight_level_nits(struct dc_link *link,
+		bool isHDR,
+		uint32_t backlight_millinits,
+		uint32_t transition_time_in_ms)
+{
+	struct dpcd_source_backlight_set dpcd_backlight_set;
+	uint8_t backlight_control = isHDR ? 1 : 0;
+
+	if (!link || (link->connector_signal != SIGNAL_TYPE_EDP &&
+			link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT))
+		return false;
+
+	// OLEDs have no PWM, they can only use AUX
+	if (link->dpcd_sink_ext_caps.bits.oled == 1)
+		backlight_control = 1;
+
+	*(uint32_t *)&dpcd_backlight_set.backlight_level_millinits = backlight_millinits;
+	*(uint16_t *)&dpcd_backlight_set.backlight_transition_time_ms = (uint16_t)transition_time_in_ms;
+
+
+	if (core_link_write_dpcd(link, DP_SOURCE_BACKLIGHT_LEVEL,
+			(uint8_t *)(&dpcd_backlight_set),
+			sizeof(dpcd_backlight_set)) != DC_OK)
+		return false;
+
+	if (core_link_write_dpcd(link, DP_SOURCE_BACKLIGHT_CONTROL,
+			&backlight_control, 1) != DC_OK)
+		return false;
+
+	return true;
+}
+
+bool dc_link_get_backlight_level_nits(struct dc_link *link,
+		uint32_t *backlight_millinits_avg,
+		uint32_t *backlight_millinits_peak)
+{
+	union dpcd_source_backlight_get dpcd_backlight_get;
+
+	memset(&dpcd_backlight_get, 0, sizeof(union dpcd_source_backlight_get));
+
+	if (!link || (link->connector_signal != SIGNAL_TYPE_EDP &&
+			link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT))
+		return false;
+
+	if (!core_link_read_dpcd(link, DP_SOURCE_BACKLIGHT_CURRENT_PEAK,
+			dpcd_backlight_get.raw,
+			sizeof(union dpcd_source_backlight_get)))
+		return false;
+
+	*backlight_millinits_avg =
+		dpcd_backlight_get.bytes.backlight_millinits_avg;
+	*backlight_millinits_peak =
+		dpcd_backlight_get.bytes.backlight_millinits_peak;
+
+	/* On non-supported panels dpcd_read usually succeeds with 0 returned */
+	if (*backlight_millinits_avg == 0 ||
+			*backlight_millinits_avg > *backlight_millinits_peak)
+		return false;
+
+	return true;
+}
+
+bool dc_link_backlight_enable_aux(struct dc_link *link, bool enable)
+{
+	uint8_t backlight_enable = enable ? 1 : 0;
+
+	if (!link || (link->connector_signal != SIGNAL_TYPE_EDP &&
+		link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT))
+		return false;
+
+	if (core_link_write_dpcd(link, DP_SOURCE_BACKLIGHT_ENABLE,
+		&backlight_enable, 1) != DC_OK)
+		return false;
+
+	return true;
+}
+
+// we read default from 0x320 because we expect BIOS wrote it there
+// regular get_backlight_nit reads from panel set at 0x326
+bool dc_link_read_default_bl_aux(struct dc_link *link, uint32_t *backlight_millinits)
+{
+	if (!link || (link->connector_signal != SIGNAL_TYPE_EDP &&
+		link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT))
+		return false;
+
+	if (!core_link_read_dpcd(link, DP_SOURCE_BACKLIGHT_LEVEL,
+		(uint8_t *) backlight_millinits,
+		sizeof(uint32_t)))
+		return false;
+
+	return true;
+}
+
+bool dc_link_set_default_brightness_aux(struct dc_link *link)
+{
+	uint32_t default_backlight;
+
+	if (link &&
+		(link->dpcd_sink_ext_caps.bits.hdr_aux_backlight_control == 1 ||
+		link->dpcd_sink_ext_caps.bits.sdr_aux_backlight_control == 1)) {
+		if (!dc_link_read_default_bl_aux(link, &default_backlight))
+			default_backlight = 150000;
+		// if < 5 nits or > 5000, it might be wrong readback
+		if (default_backlight < 5000 || default_backlight > 5000000)
+			default_backlight = 150000; //
+
+		return dc_link_set_backlight_level_nits(link, true,
+				default_backlight, 0);
+	}
+	return false;
+}

commit 47b0c91f29f85282f1ad777d4059ce4e0c6aeb12
Author: Eric Bernstein <eric.bernstein@amd.com>
Date:   Wed Jan 15 14:41:23 2020 -0500

    drm/amd/display: Fix various issues found by compiler warning as errors
    
    [Why]
    Diagnostics team reported various issues found when enabling warnings as errors
    
    [How]
    Fix implicit conversions
    
    Signed-off-by: Eric Bernstein <eric.bernstein@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8ff0f5b7104b..07b9aa1d01af 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3681,7 +3681,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			struct pipe_ctx *odm_pipe;
 			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
-			int count;
+			uint16_t count = 0;
 
 			switch (test_pattern_color_space) {
 			case DP_TEST_PATTERN_COLOR_SPACE_RGB:

commit b32827384cc5fd1151da6193d596ac1036828da5
Author: George Shen <george.shen@amd.com>
Date:   Tue Jan 14 17:11:15 2020 -0500

    drm/amd/display: Move USB-C workaround to after parameter variables are set
    
    [Why]
    The call to dp_enable_link_phy are using default/invalid values for clock id
    and link settings.
    
    [How]
    Move workaround code to after its parameter variables are determined.
    
    Signed-off-by: George Shen <george.shen@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index cb731c1d30b1..8ff0f5b7104b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1892,6 +1892,16 @@ bool dp_verify_link_cap(
 	/* disable PHY done possible by BIOS, will be done by driver itself */
 	dp_disable_link_phy(link, link->connector_signal);
 
+	dp_cs_id = get_clock_source_id(link);
+
+	/* link training starts with the maximum common settings
+	 * supported by both sink and ASIC.
+	 */
+	initial_link_settings = get_common_supported_link_settings(
+			*known_limit_link_setting,
+			max_link_cap);
+	cur_link_setting = initial_link_settings;
+
 	/* Temporary Renoir-specific workaround for SWDEV-215184;
 	 * PHY will sometimes be in bad state on hotplugging display from certain USB-C dongle,
 	 * so add extra cycle of enabling and disabling the PHY before first link training.
@@ -1902,15 +1912,6 @@ bool dp_verify_link_cap(
 		dp_disable_link_phy(link, link->connector_signal);
 	}
 
-	dp_cs_id = get_clock_source_id(link);
-
-	/* link training starts with the maximum common settings
-	 * supported by both sink and ASIC.
-	 */
-	initial_link_settings = get_common_supported_link_settings(
-			*known_limit_link_setting,
-			max_link_cap);
-	cur_link_setting = initial_link_settings;
 	do {
 		skip_video_pattern = true;
 

commit 54f73df4cab18734b71a4325feb4749d334dc877
Author: Colin Ian King <colin.king@canonical.com>
Date:   Fri Jan 17 13:33:05 2020 +0000

    drm/amd/display: fix for-loop with incorrectly sized loop counter (v2)
    
    A for-loop is iterating from 0 up to 1000 however the loop variable count
    is a u8 and hence not large enough.  Fix this by making count an int.
    Also remove the redundant initialization of count since this is never used
    and add { } on the loop statement make the loop block clearer.
    
    v2: drop useless else (Walter Harms)
    
    Addresses-Coverity: ("Operands don't affect result")
    Fixes: ed581a0ace44 ("drm/amd/display: wait for update when setting dpg test pattern")
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a53e8fed56f3..cb731c1d30b1 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3680,7 +3680,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			struct pipe_ctx *odm_pipe;
 			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
-			uint8_t count = 0;
+			int count;
 
 			switch (test_pattern_color_space) {
 			case DP_TEST_PATTERN_COLOR_SPACE_RGB:
@@ -3725,11 +3725,11 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				width,
 				height);
 			/* wait for dpg to blank pixel data with test pattern */
-			for (count = 0; count < 1000; count++)
+			for (count = 0; count < 1000; count++) {
 				if (opp->funcs->dpg_is_blanked(opp))
 					break;
-				else
-					udelay(100);
+				udelay(100);
+			}
 		}
 	}
 	break;

commit 23bc5f3404de2e8b3adaedf33507409fda6f5528
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Jan 8 16:59:23 2020 -0500

    drm/amd/display: support VSC SDP update on video test pattern request
    
    [why]
    MSA will be deprecated in the future.
    Need to support VSC during DP test automation.
    
    [how]
    Do not disable VSC during DP test automation.
    TODO - need to add VSC update on DM side on test request.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3bb1b481451b..a53e8fed56f3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3926,7 +3926,6 @@ bool dc_link_dp_set_test_pattern(
 		}
 	} else {
 		enum dc_color_space color_space = COLOR_SPACE_UNKNOWN;
-		struct encoder_info_frame info_frame = pipe_ctx->stream_res.encoder_info_frame;
 
 		switch (test_pattern_color_space) {
 		case DP_TEST_PATTERN_COLOR_SPACE_RGB:
@@ -3951,13 +3950,9 @@ bool dc_link_dp_set_test_pattern(
 		/* update MSA to requested color space */
 		pipe_ctx->stream_res.stream_enc->funcs->dp_set_stream_attribute(pipe_ctx->stream_res.stream_enc,
 				&pipe_ctx->stream->timing,
-				color_space, false, link->dpcd_caps.dprx_feature.bits.SST_SPLIT_SDP_CAP);
-
-		/* disable vsc so no need to update it based on request */
-		info_frame.vsc.valid = false;
-		pipe_ctx->stream_res.stream_enc->funcs->update_dp_info_packets(
-				pipe_ctx->stream_res.stream_enc,
-				&info_frame);
+				color_space,
+				pipe_ctx->stream->use_vsc_sdp_for_colorimetry,
+				link->dpcd_caps.dprx_feature.bits.SST_SPLIT_SDP_CAP);
 
 		/* CRTC Patterns */
 		set_crtc_test_pattern(link, pipe_ctx, test_pattern, test_pattern_color_space);

commit 50d2c6027ef4d09d7c2a9d12b2a7957576c0fa3c
Author: Sung Lee <sung.lee@amd.com>
Date:   Fri Jan 3 14:03:52 2020 -0500

    drm/amd/display: Do not send training pattern if VS Different
    
    [Why]
    The DP 1.4a Spec requires that training pattern only under certain
    specific conditions. Currently driver will re-send
    training pattern every time voltage swing value changes,
    but that should not be the case.
    
    [How]
    Do not re-send training pattern every time VS values
    are different. Only send it on the first iteration.
    
    Signed-off-by: Sung Lee <sung.lee@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Abdoulaye Berthe <Abdoulaye.Berthe@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1bd0946829e3..3bb1b481451b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -983,7 +983,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 				offset);
 
 		/* 2. update DPCD of the receiver*/
-		if (!retries_cr)
+		if (!retry_count)
 			/* EPR #361076 - write as a 5-byte burst,
 			 * but only for the 1-st iteration.*/
 			dpcd_set_lt_pattern_and_lane_settings(

commit 43563bc2e6a769502d23f4ec9cd590e4636cf0ea
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Mon Dec 23 16:02:13 2019 -0500

    drm/amd/display: update MSA and VSC SDP on video test pattern request
    
    [why]
    On video test pattern request we need to update MSA and VSC so
    it will match the requested test pattern dynamic range field.
    
    [how]
    Update dynamic range field in MSA and disable VSC as updating VSC
    info packet is complicated and not required for test pattern purpose.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 6ab298c65247..1bd0946829e3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3925,8 +3925,43 @@ bool dc_link_dp_set_test_pattern(
 					     sizeof(training_pattern));
 		}
 	} else {
-	/* CRTC Patterns */
+		enum dc_color_space color_space = COLOR_SPACE_UNKNOWN;
+		struct encoder_info_frame info_frame = pipe_ctx->stream_res.encoder_info_frame;
+
+		switch (test_pattern_color_space) {
+		case DP_TEST_PATTERN_COLOR_SPACE_RGB:
+			color_space = COLOR_SPACE_SRGB;
+			if (test_pattern == DP_TEST_PATTERN_COLOR_SQUARES_CEA)
+				color_space = COLOR_SPACE_SRGB_LIMITED;
+			break;
+
+		case DP_TEST_PATTERN_COLOR_SPACE_YCBCR601:
+			color_space = COLOR_SPACE_YCBCR601;
+			if (test_pattern == DP_TEST_PATTERN_COLOR_SQUARES_CEA)
+				color_space = COLOR_SPACE_YCBCR601_LIMITED;
+			break;
+		case DP_TEST_PATTERN_COLOR_SPACE_YCBCR709:
+			color_space = COLOR_SPACE_YCBCR709;
+			if (test_pattern == DP_TEST_PATTERN_COLOR_SQUARES_CEA)
+				color_space = COLOR_SPACE_YCBCR709_LIMITED;
+			break;
+		default:
+			break;
+		}
+		/* update MSA to requested color space */
+		pipe_ctx->stream_res.stream_enc->funcs->dp_set_stream_attribute(pipe_ctx->stream_res.stream_enc,
+				&pipe_ctx->stream->timing,
+				color_space, false, link->dpcd_caps.dprx_feature.bits.SST_SPLIT_SDP_CAP);
+
+		/* disable vsc so no need to update it based on request */
+		info_frame.vsc.valid = false;
+		pipe_ctx->stream_res.stream_enc->funcs->update_dp_info_packets(
+				pipe_ctx->stream_res.stream_enc,
+				&info_frame);
+
+		/* CRTC Patterns */
 		set_crtc_test_pattern(link, pipe_ctx, test_pattern, test_pattern_color_space);
+
 		/* Set Test Pattern state */
 		link->test_pattern_enabled = true;
 	}

commit ffdaeb1f45ee4414e7ecc2b841bea18bec35d1c0
Author: Paul Hsieh <paul.hsieh@amd.com>
Date:   Fri Dec 27 11:35:33 2019 +0800

    drm/amd/display: reallocate MST payload when link loss
    
    [Why]
    Try to allocate MST payload but receive HPD short pulse with link loss
    casue driver allocate payload twice. It cause monitor can't light up
    successfully.
    
    [How]
    When driver receive HPD short pulse with link loss, we need to
    deallocate payload then allocate payload.
    Then we will not allocate payload twice with same sink.
    
    Signed-off-by: Paul Hsieh <paul.hsieh@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2124bc10fc53..6ab298c65247 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2901,11 +2901,8 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 			pipe_ctx,
 			pipe_ctx->stream->signal);
 
-		if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link &&
-				pipe_ctx->stream->dpms_off == false &&
-				pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
-			dc_link_allocate_mst_payload(pipe_ctx);
-		}
+		if (pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST)
+			dc_link_reallocate_mst_payload(link);
 
 		status = false;
 		if (out_link_loss)

commit 8547058b17f1f4fba10f389191dd7e08bf95b791
Author: Lewis Huang <Lewis.Huang@amd.com>
Date:   Tue Dec 24 09:50:21 2019 +0800

    drm/amd/display: Add monitor patch for AUO dpcd issue
    
    [Why]
    dpcd cap mismatch in 2200 vs base
    
    [How]
    Add monitor patch which using based caps to overwrite 2200
    
    Signed-off-by: Lewis Huang <Lewis.Huang@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8c257a9f4c7f..2124bc10fc53 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3446,6 +3446,68 @@ static bool retrieve_link_cap(struct dc_link *link)
 	return true;
 }
 
+bool dp_overwrite_extended_receiver_cap(struct dc_link *link)
+{
+	uint8_t dpcd_data[16];
+	uint32_t read_dpcd_retry_cnt = 3;
+	enum dc_status status = DC_ERROR_UNEXPECTED;
+	union dp_downstream_port_present ds_port = { 0 };
+	union down_stream_port_count down_strm_port_count;
+	union edp_configuration_cap edp_config_cap;
+
+	int i;
+
+	for (i = 0; i < read_dpcd_retry_cnt; i++) {
+		status = core_link_read_dpcd(
+				link,
+				DP_DPCD_REV,
+				dpcd_data,
+				sizeof(dpcd_data));
+		if (status == DC_OK)
+			break;
+	}
+
+	link->dpcd_caps.dpcd_rev.raw =
+		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
+
+	if (dpcd_data[DP_MAX_LANE_COUNT - DP_DPCD_REV] == 0)
+		return false;
+
+	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
+			DP_DPCD_REV];
+
+	get_active_converter_info(ds_port.byte, link);
+
+	down_strm_port_count.raw = dpcd_data[DP_DOWN_STREAM_PORT_COUNT -
+			DP_DPCD_REV];
+
+	link->dpcd_caps.allow_invalid_MSA_timing_param =
+		down_strm_port_count.bits.IGNORE_MSA_TIMING_PARAM;
+
+	link->dpcd_caps.max_ln_count.raw = dpcd_data[
+		DP_MAX_LANE_COUNT - DP_DPCD_REV];
+
+	link->dpcd_caps.max_down_spread.raw = dpcd_data[
+		DP_MAX_DOWNSPREAD - DP_DPCD_REV];
+
+	link->reported_link_cap.lane_count =
+		link->dpcd_caps.max_ln_count.bits.MAX_LANE_COUNT;
+	link->reported_link_cap.link_rate = dpcd_data[
+		DP_MAX_LINK_RATE - DP_DPCD_REV];
+	link->reported_link_cap.link_spread =
+		link->dpcd_caps.max_down_spread.bits.MAX_DOWN_SPREAD ?
+		LINK_SPREAD_05_DOWNSPREAD_30KHZ : LINK_SPREAD_DISABLED;
+
+	edp_config_cap.raw = dpcd_data[
+		DP_EDP_CONFIGURATION_CAP - DP_DPCD_REV];
+	link->dpcd_caps.panel_mode_edp =
+		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
+	link->dpcd_caps.dpcd_display_control_capable =
+		edp_config_cap.bits.DPCD_DISPLAY_CONTROL_CAPABLE;
+
+	return true;
+}
+
 bool detect_dp_sink_caps(struct dc_link *link)
 {
 	return retrieve_link_cap(link);

commit 4134aaa11bd9c3e65ec07a1fcd59f57d4c58c434
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Dec 17 12:38:54 2019 -0500

    drm/amd/display: wait for test pattern after when all pipes are programmed
    
    [why]
    Currently we wait for test pattern after each pipe is programmed.  For
    ODM combined scenario it will cause test pattern is shown on only half
    screen for 1 frame. This is not desirable.
    
    [how]
    No wait between odm pipe programming, only wait after all pipes are
    programmed.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 75dc387bbb2b..8c257a9f4c7f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3657,12 +3657,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 					NULL,
 					width,
 					height);
-				/* wait for dpg to blank pixel data with test pattern */
-				for (count = 0; count < 1000; count++)
-					if (odm_opp->funcs->dpg_is_blanked(odm_opp))
-						break;
-					else
-						udelay(100);
 			}
 			opp->funcs->opp_set_disp_pattern_generator(opp,
 				controller_test_pattern,

commit ee76592482d34c56c17dd8b75bf8e30deeb1e6af
Author: George Shen <george.shen@amd.com>
Date:   Tue Dec 17 14:34:33 2019 -0500

    drm/amd/display: Add w/a to reset PHY before link training in verify_link_cap
    
    [Why]
    PHY will sometimes be in bad state on hotplugging display from USB-C
    dongle.
    
    [How]
    Add additional calls to disable and then enable PHY before link training
    starts during verify_link_cap.
    
    Signed-off-by: George Shen <george.shen@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e415f7730f43..75dc387bbb2b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1892,6 +1892,16 @@ bool dp_verify_link_cap(
 	/* disable PHY done possible by BIOS, will be done by driver itself */
 	dp_disable_link_phy(link, link->connector_signal);
 
+	/* Temporary Renoir-specific workaround for SWDEV-215184;
+	 * PHY will sometimes be in bad state on hotplugging display from certain USB-C dongle,
+	 * so add extra cycle of enabling and disabling the PHY before first link training.
+	 */
+	if (link->link_enc->features.flags.bits.DP_IS_USB_C &&
+			link->dc->debug.usbc_combo_phy_reset_wa) {
+		dp_enable_link_phy(link, link->connector_signal, dp_cs_id, cur);
+		dp_disable_link_phy(link, link->connector_signal);
+	}
+
 	dp_cs_id = get_clock_source_id(link);
 
 	/* link training starts with the maximum common settings

commit ed581a0ace44fc4f454e7765a1625a46258080c7
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Fri Dec 13 11:54:29 2019 -0500

    drm/amd/display: wait for update when setting dpg test pattern
    
    Test pattern should be applied to hardware when exiting set test pattern
    function.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index f703b3998644..e415f7730f43 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3611,6 +3611,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			struct pipe_ctx *odm_pipe;
 			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
+			uint8_t count = 0;
 
 			switch (test_pattern_color_space) {
 			case DP_TEST_PATTERN_COLOR_SPACE_RGB:
@@ -3646,6 +3647,12 @@ static void set_crtc_test_pattern(struct dc_link *link,
 					NULL,
 					width,
 					height);
+				/* wait for dpg to blank pixel data with test pattern */
+				for (count = 0; count < 1000; count++)
+					if (odm_opp->funcs->dpg_is_blanked(odm_opp))
+						break;
+					else
+						udelay(100);
 			}
 			opp->funcs->opp_set_disp_pattern_generator(opp,
 				controller_test_pattern,
@@ -3654,6 +3661,12 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				NULL,
 				width,
 				height);
+			/* wait for dpg to blank pixel data with test pattern */
+			for (count = 0; count < 1000; count++)
+				if (opp->funcs->dpg_is_blanked(opp))
+					break;
+				else
+					udelay(100);
 		}
 	}
 	break;

commit a166f86e8a3c91adb303fc511acbfa9d9f2899a1
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Wed Dec 11 15:18:08 2019 -0500

    drm/amd/display: store lttpr mode with dpcd
    
    Make sure that lttpr_caps has the mode set to repeater.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5d0e7abb2b98..f703b3998644 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1217,24 +1217,33 @@ static void configure_lttpr_mode(struct dc_link *link)
 	uint8_t repeater_cnt;
 	uint32_t aux_interval_address;
 	uint8_t repeater_id;
+	enum dc_status result = DC_ERROR_UNEXPECTED;
 	uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT;
 
 	DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Transparent Mode\n", __func__);
-	core_link_write_dpcd(link,
+	result = core_link_write_dpcd(link,
 			DP_PHY_REPEATER_MODE,
 			(uint8_t *)&repeater_mode,
 			sizeof(repeater_mode));
 
+	if (result == DC_OK) {
+		link->dpcd_caps.lttpr_caps.mode = repeater_mode;
+	}
+
 	if (!link->is_lttpr_mode_transparent) {
 
 		DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Non Transparent Mode\n", __func__);
 
 		repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT;
-		core_link_write_dpcd(link,
+		result = core_link_write_dpcd(link,
 				DP_PHY_REPEATER_MODE,
 				(uint8_t *)&repeater_mode,
 				sizeof(repeater_mode));
 
+		if (result == DC_OK) {
+			link->dpcd_caps.lttpr_caps.mode = repeater_mode;
+		}
+
 		repeater_cnt = convert_to_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt);
 		for (repeater_id = repeater_cnt; repeater_id > 0; repeater_id--) {
 			aux_interval_address = DP_TRAINING_AUX_RD_INTERVAL_PHY_REPEATER1 +

commit b239b59bf4345da904b3d96006d8d994c5b7b996
Author: Chen Zhou <chenzhou10@huawei.com>
Date:   Fri Jan 10 15:16:16 2020 +0800

    drm/amd/display: remove unnecessary conversion to bool
    
    The conversion to bool is not needed, remove it.
    
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Chen Zhou <chenzhou10@huawei.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 49f48d432923..5d0e7abb2b98 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3268,7 +3268,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 			dpcd_data[DP_TRAINING_AUX_RD_INTERVAL];
 
 		link->dpcd_caps.ext_receiver_cap_field_present =
-				aux_rd_interval.bits.EXT_RECEIVER_CAP_FIELD_PRESENT == 1 ? true:false;
+				aux_rd_interval.bits.EXT_RECEIVER_CAP_FIELD_PRESENT == 1;
 
 		if (aux_rd_interval.bits.EXT_RECEIVER_CAP_FIELD_PRESENT == 1) {
 			uint8_t ext_cap_data[16];

commit 2d5ef0b42c0bca33aaaff2f1b98855e4e3225a0f
Author: Zhan Liu <zhan.liu@amd.com>
Date:   Mon Dec 23 15:37:24 2019 -0500

    drm/amd/display: Don't disable DP PHY when link loss happens
    
    [Why]
    There is a use case that link loss happens accidentally,
    and we need to recover that link loss as soon as possible.
    Under this circumstance, we will perform link training,
    and try to recover the link that's just lost.
    
    However, if link PHY is disabled before link training
    happens, then DP display will never come back again.
    
    Also, please note that dropping this disable_phy function
    call won't break USB-C hotplug functionality.
    (This line of code was firstly introduced associated with
    a patch to fix USB-C hotplug issue)
    
    [How]
    Don't disable DP transmitter and its encoder before link
    training happens, even if link loss is detected.
    
    Signed-off-by: Zhan Liu <zhan.liu@amd.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 38b0f4347383..49f48d432923 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2876,7 +2876,6 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 			return false;
 
 		previous_link_settings = link->cur_link_settings;
-		dp_disable_link_phy(link, pipe_ctx->stream->signal);
 
 		perform_link_training_with_retries(&previous_link_settings,
 			true, LINK_TRAINING_ATTEMPTS,

commit e97ed49690ea13f73437bc06905215159de6fab6
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Thu Dec 5 14:55:24 2019 -0500

    drm/amd/display: Do not handle linkloss for eDP
    
    [Why]
    eDP is internal link and link loss is unexpected.
    It is typically going to be PSR related errors, which is
    handled separately.
    
    [How]
    Check for eDP and skip check for link loss
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 42aa889fd0f5..38b0f4347383 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2854,10 +2854,12 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	/* For now we only handle 'Downstream port status' case.
 	 * If we got sink count changed it means
 	 * Downstream port status changed,
-	 * then DM should call DC to do the detection. */
-	if (hpd_rx_irq_check_link_loss_status(
-		link,
-		&hpd_irq_dpcd_data)) {
+	 * then DM should call DC to do the detection.
+	 * NOTE: Do not handle link loss on eDP since it is internal link*/
+	if ((link->connector_signal != SIGNAL_TYPE_EDP) &&
+		hpd_rx_irq_check_link_loss_status(
+			link,
+			&hpd_irq_dpcd_data)) {
 		/* Connectivity log: link loss */
 		CONN_DATA_LINK_LOSS(link,
 					hpd_irq_dpcd_data.raw,

commit c14f2507be0bd81e2fd8a14abb9dc7196af60a78
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Mon Nov 18 12:22:06 2019 -0500

    drm/amd/display: correct log message for lttpr
    
    [Why]
    When setting lttpr mode, the new mode to bet is not logged properly.
    
    [How]
    Update log message to show the right mode.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: George Shen <George.Shen@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index dfcd6421ee01..42aa889fd0f5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1219,7 +1219,7 @@ static void configure_lttpr_mode(struct dc_link *link)
 	uint8_t repeater_id;
 	uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT;
 
-	DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Non Transparent Mode\n", __func__);
+	DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Transparent Mode\n", __func__);
 	core_link_write_dpcd(link,
 			DP_PHY_REPEATER_MODE,
 			(uint8_t *)&repeater_mode,
@@ -1227,7 +1227,7 @@ static void configure_lttpr_mode(struct dc_link *link)
 
 	if (!link->is_lttpr_mode_transparent) {
 
-		DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Transparent Mode\n", __func__);
+		DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Non Transparent Mode\n", __func__);
 
 		repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT;
 		core_link_write_dpcd(link,

commit 5fd21b394cfec3611a4ddf49ec61c8920c001899
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Tue Nov 12 11:07:24 2019 -0500

    drm/amd/display: check for repeater when setting aux_rd_interval.
    
    [Why]
    When training with repeater the aux read interval must be set to
    repeater specific aux_red_interval. This value is always 100us for CR.
    
    [How]
    Check for repeater when setting the aux_rd_interval in channel
    equalization.
    Use the right offset in the aux_rd_interval array
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: George Shen <George.Shen@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 015fa0c52746..dfcd6421ee01 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -906,10 +906,10 @@ static enum link_training_result perform_channel_equalization_sequence(
 		/* 3. wait for receiver to lock-on*/
 		wait_time_microsec = lt_settings->eq_pattern_time;
 
-		if (!link->is_lttpr_mode_transparent)
+		if (is_repeater(link, offset))
 			wait_time_microsec =
 					translate_training_aux_read_interval(
-						link->dpcd_caps.lttpr_caps.aux_rd_interval[offset]);
+						link->dpcd_caps.lttpr_caps.aux_rd_interval[offset - 1]);
 
 		wait_for_training_aux_rd_interval(
 				link,

commit 99218d122a2b329c179cc6917836068da3c3e1ad
Author: Hugo Hu <hugo.hu@amd.com>
Date:   Wed Nov 13 16:18:09 2019 -0500

    drm/amd/display: Save/restore link setting for disable phy when link retraining
    
    [Why]
    The link setting will be modify after disable phy
    and due to DP Compliance Fails.
    
    [How]
    Save and resotre link setting for disable link phy when link retraining.
    
    Signed-off-by: Hugo Hu <hugo.hu@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 486c14e0cd41..015fa0c52746 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2788,9 +2788,9 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	union hpd_irq_data hpd_irq_dpcd_data = { { { {0} } } };
 	union device_service_irq device_service_clear = { { 0 } };
 	enum dc_status result;
-
 	bool status = false;
 	struct pipe_ctx *pipe_ctx;
+	struct dc_link_settings previous_link_settings;
 	int i;
 
 	if (out_link_loss)
@@ -2873,9 +2873,10 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 		if (pipe_ctx == NULL || pipe_ctx->stream == NULL)
 			return false;
 
+		previous_link_settings = link->cur_link_settings;
 		dp_disable_link_phy(link, pipe_ctx->stream->signal);
 
-		perform_link_training_with_retries(&link->cur_link_settings,
+		perform_link_training_with_retries(&previous_link_settings,
 			true, LINK_TRAINING_ATTEMPTS,
 			pipe_ctx,
 			pipe_ctx->stream->signal);

commit 460adc6b699672f3a15c8d6175b8f0bcb2b3c50f
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Wed Oct 23 17:16:51 2019 -0400

    drm/amd/display: add log for lttpr
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b10019106030..486c14e0cd41 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -255,11 +255,18 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - DP_TRAINING_PATTERN_SET]
 		= dpcd_pattern.raw;
 
-	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X pattern = %x\n",
-		__func__,
-		dpcd_base_lt_offset,
-		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
-
+	if (is_repeater(link, offset)) {
+		DC_LOG_HW_LINK_TRAINING("%s\n LTTPR Repeater ID: %d\n 0x%X pattern = %x\n",
+			__func__,
+			offset,
+			dpcd_base_lt_offset,
+			dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s\n 0x%X pattern = %x\n",
+			__func__,
+			dpcd_base_lt_offset,
+			dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
+	}
 	/*****************************************************************
 	* DpcdAddress_Lane0Set -> DpcdAddress_Lane3Set
 	*****************************************************************/
@@ -289,14 +296,25 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		dpcd_lane,
 		size_in_bytes);
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
-		__func__,
-		dpcd_base_lt_offset,
-		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
-		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
-		dpcd_lane[0].bits.MAX_SWING_REACHED,
-		dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
-
+	if (is_repeater(link, offset)) {
+		DC_LOG_HW_LINK_TRAINING("%s:\n LTTPR Repeater ID: %d\n"
+				" 0x%X VS set = %x PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+			__func__,
+			offset,
+			dpcd_base_lt_offset,
+			dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+			dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+			dpcd_lane[0].bits.MAX_SWING_REACHED,
+			dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+			__func__,
+			dpcd_base_lt_offset,
+			dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+			dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+			dpcd_lane[0].bits.MAX_SWING_REACHED,
+			dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+	}
 	if (edp_workaround) {
 		/* for eDP write in 2 parts because the 5-byte burst is
 		* causing issues on some eDP panels (EPR#366724)
@@ -544,23 +562,42 @@ static void get_lane_status_and_drive_settings(
 
 	ln_status_updated->raw = dpcd_buf[2];
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01Status = %x\n 0x%X Lane23Status = %x\n ",
-		__func__,
-		lane01_status_address, dpcd_buf[0],
-		lane01_status_address + 1, dpcd_buf[1]);
-
+	if (is_repeater(link, offset)) {
+		DC_LOG_HW_LINK_TRAINING("%s:\n LTTPR Repeater ID: %d\n"
+				" 0x%X Lane01Status = %x\n 0x%X Lane23Status = %x\n ",
+			__func__,
+			offset,
+			lane01_status_address, dpcd_buf[0],
+			lane01_status_address + 1, dpcd_buf[1]);
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01Status = %x\n 0x%X Lane23Status = %x\n ",
+			__func__,
+			lane01_status_address, dpcd_buf[0],
+			lane01_status_address + 1, dpcd_buf[1]);
+	}
 	lane01_adjust_address = DP_ADJUST_REQUEST_LANE0_1;
 
 	if (is_repeater(link, offset))
 		lane01_adjust_address = DP_ADJUST_REQUEST_LANE0_1_PHY_REPEATER1 +
 				((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01AdjustRequest = %x\n 0x%X Lane23AdjustRequest = %x\n",
-		__func__,
-		lane01_adjust_address,
-		dpcd_buf[lane_adjust_offset],
-		lane01_adjust_address + 1,
-		dpcd_buf[lane_adjust_offset + 1]);
+	if (is_repeater(link, offset)) {
+		DC_LOG_HW_LINK_TRAINING("%s:\n LTTPR Repeater ID: %d\n"
+				" 0x%X Lane01AdjustRequest = %x\n 0x%X Lane23AdjustRequest = %x\n",
+					__func__,
+					offset,
+					lane01_adjust_address,
+					dpcd_buf[lane_adjust_offset],
+					lane01_adjust_address + 1,
+					dpcd_buf[lane_adjust_offset + 1]);
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01AdjustRequest = %x\n 0x%X Lane23AdjustRequest = %x\n",
+			__func__,
+			lane01_adjust_address,
+			dpcd_buf[lane_adjust_offset],
+			lane01_adjust_address + 1,
+			dpcd_buf[lane_adjust_offset + 1]);
+	}
 
 	/*copy to req_settings*/
 	request_settings.link_settings.lane_count =
@@ -656,14 +693,26 @@ static void dpcd_set_lane_settings(
 	}
 	*/
 
-	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
-		__func__,
-		lane0_set_address,
-		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
-		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
-		dpcd_lane[0].bits.MAX_SWING_REACHED,
-		dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+	if (is_repeater(link, offset)) {
+		DC_LOG_HW_LINK_TRAINING("%s\n LTTPR Repeater ID: %d\n"
+				" 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+			__func__,
+			offset,
+			lane0_set_address,
+			dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+			dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+			dpcd_lane[0].bits.MAX_SWING_REACHED,
+			dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
 
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+			__func__,
+			lane0_set_address,
+			dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+			dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+			dpcd_lane[0].bits.MAX_SWING_REACHED,
+			dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+	}
 	link->cur_lane_setting = link_training_setting->lane_settings[0];
 
 }
@@ -1170,12 +1219,16 @@ static void configure_lttpr_mode(struct dc_link *link)
 	uint8_t repeater_id;
 	uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT;
 
+	DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Non Transparent Mode\n", __func__);
 	core_link_write_dpcd(link,
 			DP_PHY_REPEATER_MODE,
 			(uint8_t *)&repeater_mode,
 			sizeof(repeater_mode));
 
 	if (!link->is_lttpr_mode_transparent) {
+
+		DC_LOG_HW_LINK_TRAINING("%s\n Set LTTPR to Transparent Mode\n", __func__);
+
 		repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT;
 		core_link_write_dpcd(link,
 				DP_PHY_REPEATER_MODE,
@@ -1212,8 +1265,9 @@ static void repeater_training_done(struct dc_link *link, uint32_t offset)
 		&dpcd_pattern.raw,
 		1);
 
-	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X pattern = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n LTTPR Id: %d 0x%X pattern = %x\n",
 		__func__,
+		offset,
 		dpcd_base_lt_offset,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 }
@@ -1663,6 +1717,11 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 
 		if (link->dpcd_caps.lttpr_caps.max_link_rate < max_link_cap.link_rate)
 			max_link_cap.link_rate = link->dpcd_caps.lttpr_caps.max_link_rate;
+
+		DC_LOG_HW_LINK_TRAINING("%s\n Training with LTTPR,  max_lane count %d max_link rate %d \n",
+						__func__,
+						max_link_cap.lane_count,
+						max_link_cap.link_rate);
 	}
 	return max_link_cap;
 }
@@ -3196,6 +3255,8 @@ static bool retrieve_link_cap(struct dc_link *link)
 			link->is_lttpr_mode_transparent = true;
 			dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_TIMEOUT_PERIOD);
 		}
+
+		CONN_DATA_DETECT(link, lttpr_dpcd_data, sizeof(lttpr_dpcd_data), "LTTPR Caps: ");
 	}
 
 	{

commit fa11d3c9425354c4b47e40f34d29c5b0949fe4ce
Author: Leo (Hanghong) Ma <hanghong.ma@amd.com>
Date:   Thu Nov 7 16:30:04 2019 -0500

    drm/amd/display: Change the delay time before enabling FEC
    
    [why]
    DP spec requires 1000 symbols delay between the end of link training
    and enabling FEC in the stream. Currently we are using 1 miliseconds
    delay which is not accurate.
    
    [how]
    One lane RBR should have the maximum time for transmitting 1000 LL
    codes which is 6.173 us. So using 7 microseconds delay instead of
    1 miliseconds.
    
    Signed-off-by: Leo (Hanghong) Ma <hanghong.ma@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 537b4dee8f22..b10019106030 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3951,7 +3951,14 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 	if (link_enc->funcs->fec_set_enable &&
 			link->dpcd_caps.fec_cap.bits.FEC_CAPABLE) {
 		if (link->fec_state == dc_link_fec_ready && enable) {
-			msleep(1);
+			/* Accord to DP spec, FEC enable sequence can first
+			 * be transmitted anytime after 1000 LL codes have
+			 * been transmitted on the link after link training
+			 * completion. Using 1 lane RBR should have the maximum
+			 * time for transmitting 1000 LL codes which is 6.173 us.
+			 * So use 7 microseconds delay instead.
+			 */
+			udelay(7);
 			link_enc->funcs->fec_set_enable(link_enc, true);
 			link->fec_state = dc_link_fec_enabled;
 		} else if (link->fec_state == dc_link_fec_enabled && !enable) {

commit 832aa63bef346fc6a58bf46412036d368142fddf
Author: Paul Hsieh <paul.hsieh@amd.com>
Date:   Fri Nov 1 14:41:37 2019 +0800

    drm/amd/display: Reset PHY in link re-training
    
    [Why]
    Link training failed randomly when plugging USB-C display in/out.
    
    [How]
    If link training failed, reset PHY in link re-training.
    
    Signed-off-by: Paul Hsieh <paul.hsieh@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 272261192e82..537b4dee8f22 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1433,23 +1433,58 @@ enum link_training_result dc_link_dp_perform_link_training(
 }
 
 bool perform_link_training_with_retries(
-	struct dc_link *link,
 	const struct dc_link_settings *link_setting,
 	bool skip_video_pattern,
-	int attempts)
+	int attempts,
+	struct pipe_ctx *pipe_ctx,
+	enum signal_type signal)
 {
 	uint8_t j;
 	uint8_t delay_between_attempts = LINK_TRAINING_RETRY_DELAY;
+	struct dc_stream_state *stream = pipe_ctx->stream;
+	struct dc_link *link = stream->link;
+	enum dp_panel_mode panel_mode = dp_get_panel_mode(link);
 
 	for (j = 0; j < attempts; ++j) {
 
-		if (dc_link_dp_perform_link_training(
+		dp_enable_link_phy(
+			link,
+			signal,
+			pipe_ctx->clock_source->id,
+			link_setting);
+
+		if (stream->sink_patches.dppowerup_delay > 0) {
+			int delay_dp_power_up_in_ms = stream->sink_patches.dppowerup_delay;
+
+			msleep(delay_dp_power_up_in_ms);
+		}
+
+		dp_set_panel_mode(link, panel_mode);
+
+		/* We need to do this before the link training to ensure the idle pattern in SST
+		 * mode will be sent right after the link training
+		 */
+		link->link_enc->funcs->connect_dig_be_to_fe(link->link_enc,
+								pipe_ctx->stream_res.stream_enc->id, true);
+
+		if (link->aux_access_disabled) {
+			dc_link_dp_perform_link_training_skip_aux(link, link_setting);
+			return true;
+		} else if (dc_link_dp_perform_link_training(
 				link,
 				link_setting,
 				skip_video_pattern) == LINK_TRAINING_SUCCESS)
 			return true;
 
+		/* latest link training still fail, skip delay and keep PHY on
+		 */
+		if (j == (attempts - 1))
+			break;
+
+		dp_disable_link_phy(link, signal);
+
 		msleep(delay_between_attempts);
+
 		delay_between_attempts += LINK_TRAINING_RETRY_DELAY;
 	}
 
@@ -2770,17 +2805,26 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 					sizeof(hpd_irq_dpcd_data),
 					"Status: ");
 
-		perform_link_training_with_retries(link,
-			&link->cur_link_settings,
-			true, LINK_TRAINING_ATTEMPTS);
-
 		for (i = 0; i < MAX_PIPES; i++) {
 			pipe_ctx = &link->dc->current_state->res_ctx.pipe_ctx[i];
-			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link &&
-					pipe_ctx->stream->dpms_off == false &&
-					pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
-				dc_link_allocate_mst_payload(pipe_ctx);
-			}
+			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link)
+				break;
+		}
+
+		if (pipe_ctx == NULL || pipe_ctx->stream == NULL)
+			return false;
+
+		dp_disable_link_phy(link, pipe_ctx->stream->signal);
+
+		perform_link_training_with_retries(&link->cur_link_settings,
+			true, LINK_TRAINING_ATTEMPTS,
+			pipe_ctx,
+			pipe_ctx->stream->signal);
+
+		if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link &&
+				pipe_ctx->stream->dpms_off == false &&
+				pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
+			dc_link_allocate_mst_payload(pipe_ctx);
 		}
 
 		status = false;

commit 2057b7e1cf77bdf090a3571a8d2ca00a76f34a9e
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Oct 15 15:12:57 2019 -0400

    drm/amd/display: add color space option when sending link test pattern
    
    [why]
    In the TEST_MSIC dpcd register field definition, the test equipment
    has the option to choose between YCbCr601 or YCbCr709.
    We will apply corresponding YCbCr coefficient based on this test
    request.
    
    [how]
    Add a new input parameter in dc_link_dp_set_test_pattern to allow the
    selection between different color space.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b72db01afeed..272261192e82 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2493,6 +2493,7 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 	dc_link_dp_set_test_pattern(
 		link,
 		test_pattern,
+		DP_TEST_PATTERN_COLOR_SPACE_UNDEFINED,
 		&link_training_settings,
 		test_80_bit_pattern,
 		(DP_TEST_80BIT_CUSTOM_PATTERN_79_72 -
@@ -2504,6 +2505,8 @@ static void dp_test_send_link_test_pattern(struct dc_link *link)
 	union link_test_pattern dpcd_test_pattern;
 	union test_misc dpcd_test_params;
 	enum dp_test_pattern test_pattern;
+	enum dp_test_pattern_color_space test_pattern_color_space =
+			DP_TEST_PATTERN_COLOR_SPACE_UNDEFINED;
 
 	memset(&dpcd_test_pattern, 0, sizeof(dpcd_test_pattern));
 	memset(&dpcd_test_params, 0, sizeof(dpcd_test_params));
@@ -2538,9 +2541,14 @@ static void dp_test_send_link_test_pattern(struct dc_link *link)
 	break;
 	}
 
+	test_pattern_color_space = dpcd_test_params.bits.YCBCR_COEFS ?
+			DP_TEST_PATTERN_COLOR_SPACE_YCBCR709 :
+			DP_TEST_PATTERN_COLOR_SPACE_YCBCR601;
+
 	dc_link_dp_set_test_pattern(
 			link,
 			test_pattern,
+			test_pattern_color_space,
 			NULL,
 			NULL,
 			0);
@@ -3426,7 +3434,8 @@ static bool is_dp_phy_pattern(enum dp_test_pattern test_pattern)
 
 static void set_crtc_test_pattern(struct dc_link *link,
 				struct pipe_ctx *pipe_ctx,
-				enum dp_test_pattern test_pattern)
+				enum dp_test_pattern test_pattern,
+				enum dp_test_pattern_color_space test_pattern_color_space)
 {
 	enum controller_dp_test_pattern controller_test_pattern;
 	enum dc_color_depth color_depth = pipe_ctx->
@@ -3484,8 +3493,27 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				controller_test_pattern, color_depth);
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
 			struct pipe_ctx *odm_pipe;
+			enum controller_dp_color_space controller_color_space;
 			int opp_cnt = 1;
 
+			switch (test_pattern_color_space) {
+			case DP_TEST_PATTERN_COLOR_SPACE_RGB:
+				controller_color_space = CONTROLLER_DP_COLOR_SPACE_RGB;
+				break;
+			case DP_TEST_PATTERN_COLOR_SPACE_YCBCR601:
+				controller_color_space = CONTROLLER_DP_COLOR_SPACE_YCBCR601;
+				break;
+			case DP_TEST_PATTERN_COLOR_SPACE_YCBCR709:
+				controller_color_space = CONTROLLER_DP_COLOR_SPACE_YCBCR709;
+				break;
+			case DP_TEST_PATTERN_COLOR_SPACE_UNDEFINED:
+			default:
+				controller_color_space = CONTROLLER_DP_COLOR_SPACE_UDEFINED;
+				DC_LOG_ERROR("%s: Color space must be defined for test pattern", __func__);
+				ASSERT(0);
+				break;
+			}
+
 			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe)
 				opp_cnt++;
 
@@ -3497,6 +3525,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params);
 				odm_opp->funcs->opp_set_disp_pattern_generator(odm_opp,
 					controller_test_pattern,
+					controller_color_space,
 					color_depth,
 					NULL,
 					width,
@@ -3504,6 +3533,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			}
 			opp->funcs->opp_set_disp_pattern_generator(opp,
 				controller_test_pattern,
+				controller_color_space,
 				color_depth,
 				NULL,
 				width,
@@ -3535,6 +3565,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params);
 				odm_opp->funcs->opp_set_disp_pattern_generator(odm_opp,
 					CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
+					CONTROLLER_DP_COLOR_SPACE_UDEFINED,
 					color_depth,
 					NULL,
 					width,
@@ -3542,6 +3573,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			}
 			opp->funcs->opp_set_disp_pattern_generator(opp,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
+				CONTROLLER_DP_COLOR_SPACE_UDEFINED,
 				color_depth,
 				NULL,
 				width,
@@ -3558,6 +3590,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 bool dc_link_dp_set_test_pattern(
 	struct dc_link *link,
 	enum dp_test_pattern test_pattern,
+	enum dp_test_pattern_color_space test_pattern_color_space,
 	const struct link_training_settings *p_link_settings,
 	const unsigned char *p_custom_pattern,
 	unsigned int cust_pattern_size)
@@ -3586,7 +3619,7 @@ bool dc_link_dp_set_test_pattern(
 	if (link->test_pattern_enabled && test_pattern ==
 			DP_TEST_PATTERN_VIDEO_MODE) {
 		/* Set CRTC Test Pattern */
-		set_crtc_test_pattern(link, pipe_ctx, test_pattern);
+		set_crtc_test_pattern(link, pipe_ctx, test_pattern, test_pattern_color_space);
 		dp_set_hw_test_pattern(link, test_pattern,
 				(uint8_t *)p_custom_pattern,
 				(uint32_t)cust_pattern_size);
@@ -3701,7 +3734,7 @@ bool dc_link_dp_set_test_pattern(
 		}
 	} else {
 	/* CRTC Patterns */
-		set_crtc_test_pattern(link, pipe_ctx, test_pattern);
+		set_crtc_test_pattern(link, pipe_ctx, test_pattern, test_pattern_color_space);
 		/* Set Test Pattern state */
 		link->test_pattern_enabled = true;
 	}

commit 8c8048f207e785707270bc4985d8d4e1673eefb8
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Fri Jul 26 11:25:43 2019 -0400

    drm/amd/display: add automated audio test support
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b814b749724b..b72db01afeed 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2546,6 +2546,92 @@ static void dp_test_send_link_test_pattern(struct dc_link *link)
 			0);
 }
 
+static void dp_test_get_audio_test_data(struct dc_link *link, bool disable_video)
+{
+	union audio_test_mode            dpcd_test_mode = {0};
+	struct audio_test_pattern_type   dpcd_pattern_type = {0};
+	union audio_test_pattern_period  dpcd_pattern_period[AUDIO_CHANNELS_COUNT] = {0};
+	enum dp_test_pattern test_pattern = DP_TEST_PATTERN_AUDIO_OPERATOR_DEFINED;
+
+	struct pipe_ctx *pipes = link->dc->current_state->res_ctx.pipe_ctx;
+	struct pipe_ctx *pipe_ctx = &pipes[0];
+	unsigned int channel_count;
+	unsigned int channel = 0;
+	unsigned int modes = 0;
+	unsigned int sampling_rate_in_hz = 0;
+
+	// get audio test mode and test pattern parameters
+	core_link_read_dpcd(
+		link,
+		DP_TEST_AUDIO_MODE,
+		&dpcd_test_mode.raw,
+		sizeof(dpcd_test_mode));
+
+	core_link_read_dpcd(
+		link,
+		DP_TEST_AUDIO_PATTERN_TYPE,
+		&dpcd_pattern_type.value,
+		sizeof(dpcd_pattern_type));
+
+	channel_count = dpcd_test_mode.bits.channel_count + 1;
+
+	// read pattern periods for requested channels when sawTooth pattern is requested
+	if (dpcd_pattern_type.value == AUDIO_TEST_PATTERN_SAWTOOTH ||
+			dpcd_pattern_type.value == AUDIO_TEST_PATTERN_OPERATOR_DEFINED) {
+
+		test_pattern = (dpcd_pattern_type.value == AUDIO_TEST_PATTERN_SAWTOOTH) ?
+				DP_TEST_PATTERN_AUDIO_SAWTOOTH : DP_TEST_PATTERN_AUDIO_OPERATOR_DEFINED;
+		// read period for each channel
+		for (channel = 0; channel < channel_count; channel++) {
+			core_link_read_dpcd(
+							link,
+							DP_TEST_AUDIO_PERIOD_CH1 + channel,
+							&dpcd_pattern_period[channel].raw,
+							sizeof(dpcd_pattern_period[channel]));
+		}
+	}
+
+	// translate sampling rate
+	switch (dpcd_test_mode.bits.sampling_rate) {
+	case AUDIO_SAMPLING_RATE_32KHZ:
+		sampling_rate_in_hz = 32000;
+		break;
+	case AUDIO_SAMPLING_RATE_44_1KHZ:
+		sampling_rate_in_hz = 44100;
+		break;
+	case AUDIO_SAMPLING_RATE_48KHZ:
+		sampling_rate_in_hz = 48000;
+		break;
+	case AUDIO_SAMPLING_RATE_88_2KHZ:
+		sampling_rate_in_hz = 88200;
+		break;
+	case AUDIO_SAMPLING_RATE_96KHZ:
+		sampling_rate_in_hz = 96000;
+		break;
+	case AUDIO_SAMPLING_RATE_176_4KHZ:
+		sampling_rate_in_hz = 176400;
+		break;
+	case AUDIO_SAMPLING_RATE_192KHZ:
+		sampling_rate_in_hz = 192000;
+		break;
+	default:
+		sampling_rate_in_hz = 0;
+		break;
+	}
+
+	link->audio_test_data.flags.test_requested = 1;
+	link->audio_test_data.flags.disable_video = disable_video;
+	link->audio_test_data.sampling_rate = sampling_rate_in_hz;
+	link->audio_test_data.channel_count = channel_count;
+	link->audio_test_data.pattern_type = test_pattern;
+
+	if (test_pattern == DP_TEST_PATTERN_AUDIO_SAWTOOTH) {
+		for (modes = 0; modes < pipe_ctx->stream->audio_info.mode_count; modes++) {
+			link->audio_test_data.pattern_period[modes] = dpcd_pattern_period[modes].bits.pattern_period;
+		}
+	}
+}
+
 static void handle_automated_test(struct dc_link *link)
 {
 	union test_request test_request;
@@ -2575,6 +2661,12 @@ static void handle_automated_test(struct dc_link *link)
 		dp_test_send_link_test_pattern(link);
 		test_response.bits.ACK = 1;
 	}
+
+	if (test_request.bits.AUDIO_TEST_PATTERN) {
+		dp_test_get_audio_test_data(link, test_request.bits.TEST_AUDIO_DISABLED_VIDEO);
+		test_response.bits.ACK = 1;
+	}
+
 	if (test_request.bits.PHY_TEST_PATTERN) {
 		dp_test_send_phy_test_pattern(link);
 		test_response.bits.ACK = 1;

commit 026674cf055f15da0fa92fbef293d5852346380d
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Sat Nov 9 17:37:25 2019 +0800

    drm/amd/display: remove set but not used variable 'ds_port'
    
    Fixes gcc '-Wunused-but-set-variable' warning:
    
    drivers/gpu/drm/amd/amdgpu/../display/dc/core/dc_link_dp.c: In function dp_wa_power_up_0010FA:
    drivers/gpu/drm/amd/amdgpu/../display/dc/core/dc_link_dp.c:2320:35: warning:
     variable ds_port set but not used [-Wunused-but-set-variable]
    
    It is never used, so can be removed.
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 65de32fbcc83..b814b749724b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2910,7 +2910,6 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		int length)
 {
 	int retry = 0;
-	union dp_downstream_port_present ds_port = { 0 };
 
 	if (!link->dpcd_caps.dpcd_rev.raw) {
 		do {
@@ -2923,9 +2922,6 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		} while (retry++ < 4 && !link->dpcd_caps.dpcd_rev.raw);
 	}
 
-	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
-				 DP_DPCD_REV];
-
 	if (link->dpcd_caps.dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) {
 		switch (link->dpcd_caps.branch_dev_id) {
 		/* 0010FA active dongles (DP-VGA, DP-DLDVI converters) power down

commit 1da37801a8b0fffb024fea594c7f1d7867ed8aa0
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Wed Nov 6 14:38:55 2019 -0500

    drm/amd/display: Drop CONFIG_DRM_AMD_DC_DCN2_0 and DSC_SUPPORTED
    
    [Why]
    
    DCN2 and DSC are stable enough to be build by default. So drop the flags.
    
    [How]
    
    Remove them using the unifdef tool. The following commands were executed
    in sequence:
    
    $ find -name '*.c' -exec unifdef -m -DCONFIG_DRM_AMD_DC_DSC_SUPPORT -DCONFIG_DRM_AMD_DC_DCN2_0 -UCONFIG_TRIM_DRM_AMD_DC_DCN2_0 '{}' ';'
    $ find -name '*.h' -exec unifdef -m -DCONFIG_DRM_AMD_DC_DSC_SUPPORT -DCONFIG_DRM_AMD_DC_DCN2_0 -UCONFIG_TRIM_DRM_AMD_DC_DCN2_0 '{}' ';'
    
    In addition:
    
    * Remove from kconfig, and replace any dependencies with DCN1_0.
    * Remove from any makefiles.
    * Fix and cleanup NV defninitions in dal_asic_id.h
    * Expand DCN1 ifdef to include DCN2 code in the following files:
        * clk_mgr/clk_mgr.c: dc_clk_mgr_create()
        * core/dc_resources.c: dc_create_resource_pool()
        * dce/dce_dmcu.c: dcn20_*lock_phy()
        * dce/dce_dmcu.c: dcn20_funcs
        * dce/dce_dmcu.c: dcn20_dmcu_create()
        * gpio/hw_factory.c: dal_hw_factory_init()
        * gpio/hw_translate.c: dal_hw_translate_init()
    
    Signed-off-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7d18fc1e68c6..65de32fbcc83 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4,12 +4,8 @@
 #include "dc_link_dp.h"
 #include "dm_helpers.h"
 #include "opp.h"
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 #include "dsc.h"
-#endif
-#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 #include "resource.h"
-#endif
 
 #include "inc/core_types.h"
 #include "link_hwss.h"
@@ -1365,9 +1361,7 @@ enum link_training_result dc_link_dp_perform_link_training(
 	enum link_training_result status = LINK_TRAINING_SUCCESS;
 	struct link_training_settings lt_settings;
 
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	bool fec_enable;
-#endif
 	uint8_t repeater_cnt;
 	uint8_t repeater_id;
 
@@ -1380,14 +1374,12 @@ enum link_training_result dc_link_dp_perform_link_training(
 	/* 1. set link rate, lane count and spread. */
 	dpcd_set_link_settings(link, &lt_settings);
 
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	if (link->preferred_training_settings.fec_enable != NULL)
 		fec_enable = *link->preferred_training_settings.fec_enable;
 	else
 		fec_enable = true;
 
 	dp_set_fec_ready(link, fec_enable);
-#endif
 
 	if (!link->is_lttpr_mode_transparent) {
 		/* Configure lttpr mode */
@@ -1529,9 +1521,7 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	enum link_training_result lt_status = LINK_TRAINING_SUCCESS;
 	enum dp_panel_mode panel_mode = DP_PANEL_MODE_DEFAULT;
 	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	bool fec_enable = false;
-#endif
 
 	initialize_training_settings(
 		link,
@@ -1551,11 +1541,9 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	dp_enable_link_phy(link, link->connector_signal,
 		dp_cs_id, link_settings);
 
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	/* Set FEC enable */
 	fec_enable = lt_overrides->fec_enable && *lt_overrides->fec_enable;
 	dp_set_fec_ready(link, fec_enable);
-#endif
 
 	if (lt_overrides->alternate_scrambler_reset) {
 		if (*lt_overrides->alternate_scrambler_reset)
@@ -1596,9 +1584,7 @@ bool dc_link_dp_sync_lt_end(struct dc_link *link, bool link_down)
 	 */
 	if (link_down == true) {
 		dp_disable_link_phy(link, link->connector_signal);
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 		dp_set_fec_ready(link, false);
-#endif
 	}
 
 	link->sync_lt_in_progress = false;
@@ -3210,7 +3196,6 @@ static bool retrieve_link_cap(struct dc_link *link)
 		dp_hw_fw_revision.ieee_fw_rev,
 		sizeof(dp_hw_fw_revision.ieee_fw_rev));
 
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	memset(&link->dpcd_caps.dsc_caps, '\0',
 			sizeof(link->dpcd_caps.dsc_caps));
 	memset(&link->dpcd_caps.fec_cap, '\0', sizeof(link->dpcd_caps.fec_cap));
@@ -3232,7 +3217,6 @@ static bool retrieve_link_cap(struct dc_link *link)
 				link->dpcd_caps.dsc_caps.dsc_ext_caps.raw,
 				sizeof(link->dpcd_caps.dsc_caps.dsc_ext_caps.raw));
 	}
-#endif
 
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
@@ -3361,14 +3345,12 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		stream->timing.display_color_depth;
 	struct bit_depth_reduction_params params;
 	struct output_pixel_processor *opp = pipe_ctx->stream_res.opp;
-#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 	int width = pipe_ctx->stream->timing.h_addressable +
 		pipe_ctx->stream->timing.h_border_left +
 		pipe_ctx->stream->timing.h_border_right;
 	int height = pipe_ctx->stream->timing.v_addressable +
 		pipe_ctx->stream->timing.v_border_bottom +
 		pipe_ctx->stream->timing.v_border_top;
-#endif
 
 	memset(&params, 0, sizeof(params));
 
@@ -3412,7 +3394,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				controller_test_pattern, color_depth);
-#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
 			struct pipe_ctx *odm_pipe;
 			int opp_cnt = 1;
@@ -3440,7 +3421,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				width,
 				height);
 		}
-#endif
 	}
 	break;
 	case DP_TEST_PATTERN_VIDEO_MODE:
@@ -3453,7 +3433,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 				color_depth);
-#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
 			struct pipe_ctx *odm_pipe;
 			int opp_cnt = 1;
@@ -3480,7 +3459,6 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				width,
 				height);
 		}
-#endif
 	}
 	break;
 
@@ -3755,7 +3733,6 @@ enum dp_panel_mode dp_get_panel_mode(struct dc_link *link)
 	return DP_PANEL_MODE_DEFAULT;
 }
 
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 void dp_set_fec_ready(struct dc_link *link, bool ready)
 {
 	/* FEC has to be "set ready" before the link training.
@@ -3818,5 +3795,4 @@ void dp_set_fec_enable(struct dc_link *link, bool enable)
 		}
 	}
 }
-#endif
 

commit 61aa7a6f760e78e50fad708029fc3aa201ec7a89
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Thu Oct 10 16:41:52 2019 -0400

    drm/amd/display: disable lttpr for invalid lttpr caps.
    
    1-Read lttpr caps in 5-bytes
    2-Parse caps
    3-Validate caps and set lttpr_mode
    4-Use hw default timeout when lttpr is disabled.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 6e1f00ab6646..7d18fc1e68c6 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1172,7 +1172,7 @@ static void configure_lttpr_mode(struct dc_link *link)
 	uint8_t repeater_cnt;
 	uint32_t aux_interval_address;
 	uint8_t repeater_id;
-	enum lttpr_mode repeater_mode = phy_repeater_mode_transparent;
+	uint8_t repeater_mode = DP_PHY_REPEATER_MODE_TRANSPARENT;
 
 	core_link_write_dpcd(link,
 			DP_PHY_REPEATER_MODE,
@@ -1180,7 +1180,7 @@ static void configure_lttpr_mode(struct dc_link *link)
 			sizeof(repeater_mode));
 
 	if (!link->is_lttpr_mode_transparent) {
-		repeater_mode = phy_repeater_mode_non_transparent;
+		repeater_mode = DP_PHY_REPEATER_MODE_NON_TRANSPARENT;
 		core_link_write_dpcd(link,
 				DP_PHY_REPEATER_MODE,
 				(uint8_t *)&repeater_mode,
@@ -2964,7 +2964,11 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 
 static bool retrieve_link_cap(struct dc_link *link)
 {
-	uint8_t dpcd_data[DP_ADAPTER_CAP - DP_DPCD_REV + 1];
+	/* DP_ADAPTER_CAP - DP_DPCD_REV + 1 == 16 and also DP_DSC_BITS_PER_PIXEL_INC - DP_DSC_SUPPORT + 1 == 16,
+	 * which means size 16 will be good for both of those DPCD register block reads
+	 */
+	uint8_t dpcd_data[16];
+	uint8_t lttpr_dpcd_data[6];
 
 	/*Only need to read 1 byte starting from DP_DPRX_FEATURE_ENUMERATION_LIST.
 	 */
@@ -2977,7 +2981,6 @@ static bool retrieve_link_cap(struct dc_link *link)
 	union dp_downstream_port_present ds_port = { 0 };
 	enum dc_status status = DC_ERROR_UNEXPECTED;
 	uint32_t read_dpcd_retry_cnt = 3;
-	uint32_t prev_timeout_val;
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 
@@ -2988,12 +2991,12 @@ static bool retrieve_link_cap(struct dc_link *link)
 	link->is_lttpr_mode_transparent = true;
 
 	if (ext_timeout_support) {
-		prev_timeout_val =
-				dc_link_aux_configure_timeout(link->ddc,
-						LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
+		dc_link_aux_configure_timeout(link->ddc,
+					LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
 	}
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
+	memset(lttpr_dpcd_data, '\0', sizeof(lttpr_dpcd_data));
 	memset(&down_strm_port_count,
 		'\0', sizeof(union down_stream_port_count));
 	memset(&edp_config_cap, '\0',
@@ -3026,47 +3029,46 @@ static bool retrieve_link_cap(struct dc_link *link)
 	}
 
 	if (ext_timeout_support) {
+
 		status = core_link_read_dpcd(
 				link,
-				DP_PHY_REPEATER_CNT,
-				&link->dpcd_caps.lttpr_caps.phy_repeater_cnt,
-				sizeof(link->dpcd_caps.lttpr_caps.phy_repeater_cnt));
-
-		if (link->dpcd_caps.lttpr_caps.phy_repeater_cnt > 0) {
-
+				DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV,
+				lttpr_dpcd_data,
+				sizeof(lttpr_dpcd_data));
+
+		link->dpcd_caps.lttpr_caps.revision.raw =
+				lttpr_dpcd_data[DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		link->dpcd_caps.lttpr_caps.max_link_rate =
+				lttpr_dpcd_data[DP_MAX_LINK_RATE_PHY_REPEATER -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		link->dpcd_caps.lttpr_caps.phy_repeater_cnt =
+				lttpr_dpcd_data[DP_PHY_REPEATER_CNT -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		link->dpcd_caps.lttpr_caps.max_lane_count =
+				lttpr_dpcd_data[DP_MAX_LANE_COUNT_PHY_REPEATER -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		link->dpcd_caps.lttpr_caps.mode =
+				lttpr_dpcd_data[DP_PHY_REPEATER_MODE -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		link->dpcd_caps.lttpr_caps.max_ext_timeout =
+				lttpr_dpcd_data[DP_PHY_REPEATER_EXTENDED_WAIT_TIMEOUT -
+								DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV];
+
+		if (link->dpcd_caps.lttpr_caps.phy_repeater_cnt > 0 &&
+				link->dpcd_caps.lttpr_caps.max_lane_count > 0 &&
+				link->dpcd_caps.lttpr_caps.max_lane_count <= 4 &&
+				link->dpcd_caps.lttpr_caps.revision.raw >= 0x14) {
 			link->is_lttpr_mode_transparent = false;
-
-			status = core_link_read_dpcd(
-					link,
-					DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV,
-					(uint8_t *)&link->dpcd_caps.lttpr_caps.revision,
-					sizeof(link->dpcd_caps.lttpr_caps.revision));
-
-			status = core_link_read_dpcd(
-					link,
-					DP_MAX_LINK_RATE_PHY_REPEATER,
-					&link->dpcd_caps.lttpr_caps.max_link_rate,
-					sizeof(link->dpcd_caps.lttpr_caps.max_link_rate));
-
-			status = core_link_read_dpcd(
-					link,
-					DP_PHY_REPEATER_MODE,
-					(uint8_t *)&link->dpcd_caps.lttpr_caps.mode,
-					sizeof(link->dpcd_caps.lttpr_caps.mode));
-
-			status = core_link_read_dpcd(
-					link,
-					DP_MAX_LANE_COUNT_PHY_REPEATER,
-					&link->dpcd_caps.lttpr_caps.max_lane_count,
-					sizeof(link->dpcd_caps.lttpr_caps.max_lane_count));
-
-			status = core_link_read_dpcd(
-					link,
-					DP_PHY_REPEATER_EXTENDED_WAIT_TIMEOUT,
-					&link->dpcd_caps.lttpr_caps.max_ext_timeout,
-					sizeof(link->dpcd_caps.lttpr_caps.max_ext_timeout));
 		} else {
-			dc_link_aux_configure_timeout(link->ddc, prev_timeout_val);
+			/*No lttpr reset timeout to its default value*/
+			link->is_lttpr_mode_transparent = true;
+			dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_TIMEOUT_PERIOD);
 		}
 	}
 

commit 9bffd0806d80de9c189b8cf69b3022783e5c8f2a
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Thu Sep 19 15:51:00 2019 -0400

    drm/amd/display: use previous aux timeout val if no repeater.
    
    [Why]
    The aux timeout value is not default before reading link cap.
    Setting it to default when lttpr is not enabled causes some monitor
    not to light up.
    
    [How]
    Read the aux engine timeout value before setting it to extended.
    Set the aux engine timeout to its previous value if no lttpr.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 11b6e14b345e..6e1f00ab6646 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2977,6 +2977,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 	union dp_downstream_port_present ds_port = { 0 };
 	enum dc_status status = DC_ERROR_UNEXPECTED;
 	uint32_t read_dpcd_retry_cnt = 3;
+	uint32_t prev_timeout_val;
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 
@@ -2987,7 +2988,9 @@ static bool retrieve_link_cap(struct dc_link *link)
 	link->is_lttpr_mode_transparent = true;
 
 	if (ext_timeout_support) {
-		status = dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
+		prev_timeout_val =
+				dc_link_aux_configure_timeout(link->ddc,
+						LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
 	}
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
@@ -3022,7 +3025,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 		return false;
 	}
 
-	if (ext_timeout_support && link->dpcd_caps.dpcd_rev.raw >= 0x14) {
+	if (ext_timeout_support) {
 		status = core_link_read_dpcd(
 				link,
 				DP_PHY_REPEATER_CNT,
@@ -3063,7 +3066,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 					&link->dpcd_caps.lttpr_caps.max_ext_timeout,
 					sizeof(link->dpcd_caps.lttpr_caps.max_ext_timeout));
 		} else {
-			dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_TIMEOUT_PERIOD);
+			dc_link_aux_configure_timeout(link->ddc, prev_timeout_val);
 		}
 	}
 

commit 64c12b733fe7eaffa9207e6f30b313595c6e6597
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Wed Jul 24 11:01:44 2019 -0400

    drm/amd/display: implement lttpr logic
    
    1-If at least one repeater is present in the link and we are in non
    transparent mode, perform clock recovery then channel equalization
    with all repeaters one by one before training DPRX.
    
    2-Mark the end of LT with a repeater by setting training pattern 0
    at the end of channel equalization with each repeater.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 94d5a0ac308f..11b6e14b345e 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -22,7 +22,7 @@
 	link->ctx->logger
 
 
-#define DP_REPEATER_CONFIGURATION_AND_STATUS_OFFSET   0x50
+#define DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE   0x50
 
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
@@ -224,19 +224,31 @@ static enum dpcd_training_patterns
 	return dpcd_tr_pattern;
 }
 
+static inline bool is_repeater(struct dc_link *link, uint32_t offset)
+{
+	return (!link->is_lttpr_mode_transparent && offset != 0);
+}
+
 static void dpcd_set_lt_pattern_and_lane_settings(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings,
-	enum dc_dp_training_pattern pattern)
+	enum dc_dp_training_pattern pattern,
+	uint32_t offset)
 {
 	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = { { {0} } };
-	const uint32_t dpcd_base_lt_offset =
-	DP_TRAINING_PATTERN_SET;
+
+	uint32_t dpcd_base_lt_offset;
+
 	uint8_t dpcd_lt_buffer[5] = {0};
 	union dpcd_training_pattern dpcd_pattern = { {0} };
 	uint32_t lane;
 	uint32_t size_in_bytes;
 	bool edp_workaround = false; /* TODO link_prop.INTERNAL */
+	dpcd_base_lt_offset = DP_TRAINING_PATTERN_SET;
+
+	if (is_repeater(link, offset))
+		dpcd_base_lt_offset = DP_TRAINING_PATTERN_SET_PHY_REPEATER1 +
+			((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
 
 	/*****************************************************************
 	* DpcdAddress_TrainingPatternSet
@@ -244,12 +256,12 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_pattern.v1_4.TRAINING_PATTERN_SET =
 		dc_dp_training_pattern_to_dpcd_training_pattern(link, pattern);
 
-	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
+	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - DP_TRAINING_PATTERN_SET]
 		= dpcd_pattern.raw;
 
-	DC_LOG_HW_LINK_TRAINING("%s\n %x pattern = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X pattern = %x\n",
 		__func__,
-		DP_TRAINING_PATTERN_SET,
+		dpcd_base_lt_offset,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 
 	/*****************************************************************
@@ -271,19 +283,19 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		PRE_EMPHASIS_MAX_LEVEL ? 1 : 0);
 	}
 
-	/* concatinate everything into one buffer*/
+	/* concatenate everything into one buffer*/
 
 	size_in_bytes = lt_settings->link_settings.lane_count * sizeof(dpcd_lane[0]);
 
 	 // 0x00103 - 0x00102
 	memmove(
-		&dpcd_lt_buffer[DP_TRAINING_LANE0_SET - dpcd_base_lt_offset],
+		&dpcd_lt_buffer[DP_TRAINING_LANE0_SET - DP_TRAINING_PATTERN_SET],
 		dpcd_lane,
 		size_in_bytes);
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
-		DP_TRAINING_LANE0_SET,
+		dpcd_base_lt_offset,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
 		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
 		dpcd_lane[0].bits.MAX_SWING_REACHED,
@@ -498,8 +510,12 @@ static void get_lane_status_and_drive_settings(
 	const struct link_training_settings *link_training_setting,
 	union lane_status *ln_status,
 	union lane_align_status_updated *ln_status_updated,
-	struct link_training_settings *req_settings)
+	struct link_training_settings *req_settings,
+	uint32_t offset)
 {
+	unsigned int lane01_status_address = DP_LANE0_1_STATUS;
+	uint8_t lane_adjust_offset = 4;
+	unsigned int lane01_adjust_address;
 	uint8_t dpcd_buf[6] = {0};
 	union lane_adjust dpcd_lane_adjust[LANE_COUNT_DP_MAX] = { { {0} } };
 	struct link_training_settings request_settings = { {0} };
@@ -507,9 +523,16 @@ static void get_lane_status_and_drive_settings(
 
 	memset(req_settings, '\0', sizeof(struct link_training_settings));
 
+	if (is_repeater(link, offset)) {
+		lane01_status_address =
+				DP_LANE0_1_STATUS_PHY_REPEATER1 +
+				((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
+		lane_adjust_offset = 3;
+	}
+
 	core_link_read_dpcd(
 		link,
-		DP_LANE0_1_STATUS,
+		lane01_status_address,
 		(uint8_t *)(dpcd_buf),
 		sizeof(dpcd_buf));
 
@@ -520,22 +543,28 @@ static void get_lane_status_and_drive_settings(
 		ln_status[lane].raw =
 			get_nibble_at_index(&dpcd_buf[0], lane);
 		dpcd_lane_adjust[lane].raw =
-			get_nibble_at_index(&dpcd_buf[4], lane);
+			get_nibble_at_index(&dpcd_buf[lane_adjust_offset], lane);
 	}
 
 	ln_status_updated->raw = dpcd_buf[2];
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
+	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01Status = %x\n 0x%X Lane23Status = %x\n ",
 		__func__,
-		DP_LANE0_1_STATUS, dpcd_buf[0],
-		DP_LANE2_3_STATUS, dpcd_buf[1]);
+		lane01_status_address, dpcd_buf[0],
+		lane01_status_address + 1, dpcd_buf[1]);
+
+	lane01_adjust_address = DP_ADJUST_REQUEST_LANE0_1;
+
+	if (is_repeater(link, offset))
+		lane01_adjust_address = DP_ADJUST_REQUEST_LANE0_1_PHY_REPEATER1 +
+				((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
 
-	DC_LOG_HW_LINK_TRAINING("%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s:\n 0x%X Lane01AdjustRequest = %x\n 0x%X Lane23AdjustRequest = %x\n",
 		__func__,
-		DP_ADJUST_REQUEST_LANE0_1,
-		dpcd_buf[4],
-		DP_ADJUST_REQUEST_LANE2_3,
-		dpcd_buf[5]);
+		lane01_adjust_address,
+		dpcd_buf[lane_adjust_offset],
+		lane01_adjust_address + 1,
+		dpcd_buf[lane_adjust_offset + 1]);
 
 	/*copy to req_settings*/
 	request_settings.link_settings.lane_count =
@@ -574,10 +603,18 @@ static void get_lane_status_and_drive_settings(
 
 static void dpcd_set_lane_settings(
 	struct dc_link *link,
-	const struct link_training_settings *link_training_setting)
+	const struct link_training_settings *link_training_setting,
+	uint32_t offset)
 {
 	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
 	uint32_t lane;
+	unsigned int lane0_set_address;
+
+	lane0_set_address = DP_TRAINING_LANE0_SET;
+
+	if (is_repeater(link, offset))
+		lane0_set_address = DP_TRAINING_LANE0_SET_PHY_REPEATER1 +
+		((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
 
 	for (lane = 0; lane <
 		(uint32_t)(link_training_setting->
@@ -600,7 +637,7 @@ static void dpcd_set_lane_settings(
 	}
 
 	core_link_write_dpcd(link,
-		DP_TRAINING_LANE0_SET,
+		lane0_set_address,
 		(uint8_t *)(dpcd_lane),
 		link_training_setting->link_settings.lane_count);
 
@@ -623,9 +660,9 @@ static void dpcd_set_lane_settings(
 	}
 	*/
 
-	DC_LOG_HW_LINK_TRAINING("%s\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
-		DP_TRAINING_LANE0_SET,
+		lane0_set_address,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
 		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
 		dpcd_lane[0].bits.MAX_SWING_REACHED,
@@ -650,17 +687,6 @@ static bool is_max_vs_reached(
 
 }
 
-void dc_link_dp_set_drive_settings(
-	struct dc_link *link,
-	struct link_training_settings *lt_settings)
-{
-	/* program ASIC PHY settings*/
-	dp_set_hw_lane_settings(link, lt_settings);
-
-	/* Notify DP sink the PHY settings from source */
-	dpcd_set_lane_settings(link, lt_settings);
-}
-
 static bool perform_post_lt_adj_req_sequence(
 	struct dc_link *link,
 	struct link_training_settings *lt_settings)
@@ -693,7 +719,8 @@ static bool perform_post_lt_adj_req_sequence(
 			lt_settings,
 			dpcd_lane_status,
 			&dpcd_lane_status_updated,
-			&req_settings);
+			&req_settings,
+			DPRX);
 
 			if (dpcd_lane_status_updated.bits.
 					POST_LT_ADJ_REQ_IN_PROGRESS == 0)
@@ -750,6 +777,31 @@ static bool perform_post_lt_adj_req_sequence(
 
 }
 
+/* Only used for channel equalization */
+static uint32_t translate_training_aux_read_interval(uint32_t dpcd_aux_read_interval)
+{
+	unsigned int aux_rd_interval_us = 400;
+
+	switch (dpcd_aux_read_interval) {
+	case 0x01:
+		aux_rd_interval_us = 400;
+		break;
+	case 0x02:
+		aux_rd_interval_us = 4000;
+		break;
+	case 0x03:
+		aux_rd_interval_us = 8000;
+		break;
+	case 0x04:
+		aux_rd_interval_us = 16000;
+		break;
+	default:
+		break;
+	}
+
+	return aux_rd_interval_us;
+}
+
 static enum link_training_result get_cr_failure(enum dc_lane_count ln_count,
 					union lane_status *dpcd_lane_status)
 {
@@ -768,37 +820,55 @@ static enum link_training_result get_cr_failure(enum dc_lane_count ln_count,
 
 static enum link_training_result perform_channel_equalization_sequence(
 	struct dc_link *link,
-	struct link_training_settings *lt_settings)
+	struct link_training_settings *lt_settings,
+	uint32_t offset)
 {
 	struct link_training_settings req_settings;
 	enum dc_dp_training_pattern tr_pattern;
 	uint32_t retries_ch_eq;
+	uint32_t wait_time_microsec;
 	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
 	union lane_align_status_updated dpcd_lane_status_updated = { {0} };
 	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = { { {0} } };
 
+	/* Note: also check that TPS4 is a supported feature*/
+
 	tr_pattern = lt_settings->pattern_for_eq;
 
-	dp_set_hw_training_pattern(link, tr_pattern);
+	if (is_repeater(link, offset))
+		tr_pattern = DP_TRAINING_PATTERN_SEQUENCE_4;
+
+	dp_set_hw_training_pattern(link, tr_pattern, offset);
 
 	for (retries_ch_eq = 0; retries_ch_eq <= LINK_TRAINING_MAX_RETRY_COUNT;
 		retries_ch_eq++) {
 
-		dp_set_hw_lane_settings(link, lt_settings);
+		dp_set_hw_lane_settings(link, lt_settings, offset);
 
 		/* 2. update DPCD*/
 		if (!retries_ch_eq)
 			/* EPR #361076 - write as a 5-byte burst,
-			 * but only for the 1-st iteration*/
+			 * but only for the 1-st iteration
+			 */
+
 			dpcd_set_lt_pattern_and_lane_settings(
 				link,
 				lt_settings,
-				tr_pattern);
+				tr_pattern, offset);
 		else
-			dpcd_set_lane_settings(link, lt_settings);
+			dpcd_set_lane_settings(link, lt_settings, offset);
 
 		/* 3. wait for receiver to lock-on*/
-		wait_for_training_aux_rd_interval(link, lt_settings->eq_pattern_time);
+		wait_time_microsec = lt_settings->eq_pattern_time;
+
+		if (!link->is_lttpr_mode_transparent)
+			wait_time_microsec =
+					translate_training_aux_read_interval(
+						link->dpcd_caps.lttpr_caps.aux_rd_interval[offset]);
+
+		wait_for_training_aux_rd_interval(
+				link,
+				wait_time_microsec);
 
 		/* 4. Read lane status and requested
 		 * drive settings as set by the sink*/
@@ -808,7 +878,8 @@ static enum link_training_result perform_channel_equalization_sequence(
 			lt_settings,
 			dpcd_lane_status,
 			&dpcd_lane_status_updated,
-			&req_settings);
+			&req_settings,
+			offset);
 
 		/* 5. check CR done*/
 		if (!is_cr_done(lane_count, dpcd_lane_status))
@@ -827,13 +898,16 @@ static enum link_training_result perform_channel_equalization_sequence(
 	return LINK_TRAINING_EQ_FAIL_EQ;
 
 }
+#define TRAINING_AUX_RD_INTERVAL 100 //us
 
 static enum link_training_result perform_clock_recovery_sequence(
 	struct dc_link *link,
-	struct link_training_settings *lt_settings)
+	struct link_training_settings *lt_settings,
+	uint32_t offset)
 {
 	uint32_t retries_cr;
 	uint32_t retry_count;
+	uint32_t wait_time_microsec;
 	struct link_training_settings req_settings;
 	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
 	enum dc_dp_training_pattern tr_pattern = DP_TRAINING_PATTERN_SEQUENCE_1;
@@ -843,7 +917,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 	retries_cr = 0;
 	retry_count = 0;
 
-	dp_set_hw_training_pattern(link, tr_pattern);
+	dp_set_hw_training_pattern(link, tr_pattern, offset);
 
 	/* najeeb - The synaptics MST hub can put the LT in
 	* infinite loop by switching the VS
@@ -860,7 +934,8 @@ static enum link_training_result perform_clock_recovery_sequence(
 		/* 1. call HWSS to set lane settings*/
 		dp_set_hw_lane_settings(
 				link,
-				lt_settings);
+				lt_settings,
+				offset);
 
 		/* 2. update DPCD of the receiver*/
 		if (!retries_cr)
@@ -869,16 +944,23 @@ static enum link_training_result perform_clock_recovery_sequence(
 			dpcd_set_lt_pattern_and_lane_settings(
 					link,
 					lt_settings,
-					tr_pattern);
+					tr_pattern,
+					offset);
 		else
 			dpcd_set_lane_settings(
 					link,
-					lt_settings);
+					lt_settings,
+					offset);
 
 		/* 3. wait receiver to lock-on*/
+		wait_time_microsec = lt_settings->cr_pattern_time;
+
+		if (!link->is_lttpr_mode_transparent)
+			wait_time_microsec = TRAINING_AUX_RD_INTERVAL;
+
 		wait_for_training_aux_rd_interval(
 				link,
-				lt_settings->cr_pattern_time);
+				wait_time_microsec);
 
 		/* 4. Read lane status and requested drive
 		* settings as set by the sink
@@ -888,7 +970,8 @@ static enum link_training_result perform_clock_recovery_sequence(
 				lt_settings,
 				dpcd_lane_status,
 				&dpcd_lane_status_updated,
-				&req_settings);
+				&req_settings,
+				offset);
 
 		/* 5. check CR done*/
 		if (is_cr_done(lane_count, dpcd_lane_status))
@@ -1057,10 +1140,38 @@ static void initialize_training_settings(
 		lt_settings->enhanced_framing = 1;
 }
 
+static uint8_t convert_to_count(uint8_t lttpr_repeater_count)
+{
+	switch (lttpr_repeater_count) {
+	case 0x80: // 1 lttpr repeater
+		return 1;
+	case 0x40: // 2 lttpr repeaters
+		return 2;
+	case 0x20: // 3 lttpr repeaters
+		return 3;
+	case 0x10: // 4 lttpr repeaters
+		return 4;
+	case 0x08: // 5 lttpr repeaters
+		return 5;
+	case 0x04: // 6 lttpr repeaters
+		return 6;
+	case 0x02: // 7 lttpr repeaters
+		return 7;
+	case 0x01: // 8 lttpr repeaters
+		return 8;
+	default:
+		break;
+	}
+	return 0; // invalid value
+}
+
 static void configure_lttpr_mode(struct dc_link *link)
 {
 	/* aux timeout is already set to extended */
 	/* RESET/SET lttpr mode to enable non transparent mode */
+	uint8_t repeater_cnt;
+	uint32_t aux_interval_address;
+	uint8_t repeater_id;
 	enum lttpr_mode repeater_mode = phy_repeater_mode_transparent;
 
 	core_link_write_dpcd(link,
@@ -1074,9 +1185,43 @@ static void configure_lttpr_mode(struct dc_link *link)
 				DP_PHY_REPEATER_MODE,
 				(uint8_t *)&repeater_mode,
 				sizeof(repeater_mode));
+
+		repeater_cnt = convert_to_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt);
+		for (repeater_id = repeater_cnt; repeater_id > 0; repeater_id--) {
+			aux_interval_address = DP_TRAINING_AUX_RD_INTERVAL_PHY_REPEATER1 +
+						((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (repeater_id - 1));
+			core_link_read_dpcd(
+				link,
+				aux_interval_address,
+				(uint8_t *)&link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1],
+				sizeof(link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1]));
+			link->dpcd_caps.lttpr_caps.aux_rd_interval[repeater_id - 1] &= 0x7F;
+		}
 	}
 }
 
+static void repeater_training_done(struct dc_link *link, uint32_t offset)
+{
+	union dpcd_training_pattern dpcd_pattern = { {0} };
+
+	const uint32_t dpcd_base_lt_offset =
+			DP_TRAINING_PATTERN_SET_PHY_REPEATER1 +
+				((DP_REPEATER_CONFIGURATION_AND_STATUS_SIZE) * (offset - 1));
+	/* Set training not in progress*/
+	dpcd_pattern.v1_4.TRAINING_PATTERN_SET = DPCD_TRAINING_PATTERN_VIDEOIDLE;
+
+	core_link_write_dpcd(
+		link,
+		dpcd_base_lt_offset,
+		&dpcd_pattern.raw,
+		1);
+
+	DC_LOG_HW_LINK_TRAINING("%s\n 0x%X pattern = %x\n",
+		__func__,
+		dpcd_base_lt_offset,
+		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
+}
+
 static void print_status_message(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings,
@@ -1156,6 +1301,17 @@ static void print_status_message(
 				lt_spread);
 }
 
+void dc_link_dp_set_drive_settings(
+	struct dc_link *link,
+	struct link_training_settings *lt_settings)
+{
+	/* program ASIC PHY settings*/
+	dp_set_hw_lane_settings(link, lt_settings, DPRX);
+
+	/* Notify DP sink the PHY settings from source */
+	dpcd_set_lane_settings(link, lt_settings, DPRX);
+}
+
 bool dc_link_dp_perform_link_training_skip_aux(
 	struct dc_link *link,
 	const struct dc_link_settings *link_setting)
@@ -1172,10 +1328,10 @@ bool dc_link_dp_perform_link_training_skip_aux(
 	/* 1. Perform_clock_recovery_sequence. */
 
 	/* transmit training pattern for clock recovery */
-	dp_set_hw_training_pattern(link, pattern_for_cr);
+	dp_set_hw_training_pattern(link, pattern_for_cr, DPRX);
 
 	/* call HWSS to set lane settings*/
-	dp_set_hw_lane_settings(link, &lt_settings);
+	dp_set_hw_lane_settings(link, &lt_settings, DPRX);
 
 	/* wait receiver to lock-on*/
 	wait_for_training_aux_rd_interval(link, lt_settings.cr_pattern_time);
@@ -1183,10 +1339,10 @@ bool dc_link_dp_perform_link_training_skip_aux(
 	/* 2. Perform_channel_equalization_sequence. */
 
 	/* transmit training pattern for channel equalization. */
-	dp_set_hw_training_pattern(link, lt_settings.pattern_for_eq);
+	dp_set_hw_training_pattern(link, lt_settings.pattern_for_eq, DPRX);
 
 	/* call HWSS to set lane settings*/
-	dp_set_hw_lane_settings(link, &lt_settings);
+	dp_set_hw_lane_settings(link, &lt_settings, DPRX);
 
 	/* wait receiver to lock-on. */
 	wait_for_training_aux_rd_interval(link, lt_settings.eq_pattern_time);
@@ -1208,9 +1364,12 @@ enum link_training_result dc_link_dp_perform_link_training(
 {
 	enum link_training_result status = LINK_TRAINING_SUCCESS;
 	struct link_training_settings lt_settings;
+
 #ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 	bool fec_enable;
 #endif
+	uint8_t repeater_cnt;
+	uint8_t repeater_id;
 
 	initialize_training_settings(
 			link,
@@ -1230,17 +1389,40 @@ enum link_training_result dc_link_dp_perform_link_training(
 	dp_set_fec_ready(link, fec_enable);
 #endif
 
-	/* Configure lttpr mode */
-	if (!link->is_lttpr_mode_transparent)
+	if (!link->is_lttpr_mode_transparent) {
+		/* Configure lttpr mode */
 		configure_lttpr_mode(link);
 
-	/* 2. perform link training (set link training done
-	 *  to false is done as well)
-	 */
-	status = perform_clock_recovery_sequence(link, &lt_settings);
+		/* 2. perform link training (set link training done
+		 *  to false is done as well)
+		 */
+		repeater_cnt = convert_to_count(link->dpcd_caps.lttpr_caps.phy_repeater_cnt);
+
+		for (repeater_id = repeater_cnt; (repeater_id > 0 && status == LINK_TRAINING_SUCCESS);
+				repeater_id--) {
+			status = perform_clock_recovery_sequence(link, &lt_settings, repeater_id);
+
+			if (status != LINK_TRAINING_SUCCESS)
+				break;
+
+			status = perform_channel_equalization_sequence(link,
+					&lt_settings,
+					repeater_id);
+
+			if (status != LINK_TRAINING_SUCCESS)
+				break;
+
+			repeater_training_done(link, repeater_id);
+		}
+	}
+
+	if (status == LINK_TRAINING_SUCCESS) {
+		status = perform_clock_recovery_sequence(link, &lt_settings, DPRX);
 	if (status == LINK_TRAINING_SUCCESS) {
 		status = perform_channel_equalization_sequence(link,
-				&lt_settings);
+					&lt_settings,
+					DPRX);
+		}
 	}
 
 	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
@@ -1393,10 +1575,11 @@ enum link_training_result dc_link_dp_sync_lt_attempt(
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)
 	 */
-	lt_status = perform_clock_recovery_sequence(link, &lt_settings);
+	lt_status = perform_clock_recovery_sequence(link, &lt_settings, DPRX);
 	if (lt_status == LINK_TRAINING_SUCCESS) {
 		lt_status = perform_channel_equalization_sequence(link,
-						&lt_settings);
+						&lt_settings,
+						DPRX);
 	}
 
 	/* 3. Sync LT must skip TRAINING_PATTERN_SET:0 (video pattern)*/
@@ -3355,8 +3538,8 @@ bool dc_link_dp_set_test_pattern(
 	if (is_dp_phy_pattern(test_pattern)) {
 		/* Set DPCD Lane Settings before running test pattern */
 		if (p_link_settings != NULL) {
-			dp_set_hw_lane_settings(link, p_link_settings);
-			dpcd_set_lane_settings(link, p_link_settings);
+			dp_set_hw_lane_settings(link, p_link_settings, DPRX);
+			dpcd_set_lane_settings(link, p_link_settings, DPRX);
 		}
 
 		/* Blank stream if running test pattern */

commit bad7ab0be9bea2a4128158751bc29ac4b1c3bce2
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Fri Jul 19 10:43:42 2019 -0400

    drm/amd/display: configure lttpr mode
    
    [Description]
    1-Grant extended timeout request. Done once after detection
    2-Configure lttpr mode based on lttpr support before LT
    3-Account for lttpr cap when determining max link settings
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1e4480f3bd3c..94d5a0ac308f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1057,6 +1057,26 @@ static void initialize_training_settings(
 		lt_settings->enhanced_framing = 1;
 }
 
+static void configure_lttpr_mode(struct dc_link *link)
+{
+	/* aux timeout is already set to extended */
+	/* RESET/SET lttpr mode to enable non transparent mode */
+	enum lttpr_mode repeater_mode = phy_repeater_mode_transparent;
+
+	core_link_write_dpcd(link,
+			DP_PHY_REPEATER_MODE,
+			(uint8_t *)&repeater_mode,
+			sizeof(repeater_mode));
+
+	if (!link->is_lttpr_mode_transparent) {
+		repeater_mode = phy_repeater_mode_non_transparent;
+		core_link_write_dpcd(link,
+				DP_PHY_REPEATER_MODE,
+				(uint8_t *)&repeater_mode,
+				sizeof(repeater_mode));
+	}
+}
+
 static void print_status_message(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings,
@@ -1210,6 +1230,9 @@ enum link_training_result dc_link_dp_perform_link_training(
 	dp_set_fec_ready(link, fec_enable);
 #endif
 
+	/* Configure lttpr mode */
+	if (!link->is_lttpr_mode_transparent)
+		configure_lttpr_mode(link);
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)
@@ -1426,6 +1449,17 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 			max_link_cap.link_spread)
 		max_link_cap.link_spread =
 				link->reported_link_cap.link_spread;
+	/*
+	 * account for lttpr repeaters cap
+	 * notes: repeaters do not snoop in the DPRX Capabilities addresses (3.6.3).
+	 */
+	if (!link->is_lttpr_mode_transparent) {
+		if (link->dpcd_caps.lttpr_caps.max_lane_count < max_link_cap.lane_count)
+			max_link_cap.lane_count = link->dpcd_caps.lttpr_caps.max_lane_count;
+
+		if (link->dpcd_caps.lttpr_caps.max_link_rate < max_link_cap.link_rate)
+			max_link_cap.link_rate = link->dpcd_caps.lttpr_caps.max_link_rate;
+	}
 	return max_link_cap;
 }
 
@@ -1571,6 +1605,13 @@ bool dp_verify_link_cap(
 
 	max_link_cap = get_max_link_cap(link);
 
+	/* Grant extended timeout request */
+	if (!link->is_lttpr_mode_transparent && link->dpcd_caps.lttpr_caps.max_ext_timeout > 0) {
+		uint8_t grant = link->dpcd_caps.lttpr_caps.max_ext_timeout & 0x80;
+
+		core_link_write_dpcd(link, DP_PHY_REPEATER_EXTENDED_WAIT_TIMEOUT, &grant, sizeof(grant));
+	}
+
 	/* TODO implement override and monitor patch later */
 
 	/* try to train the link from high to low to
@@ -2759,6 +2800,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 	/* Set default timeout to 3.2ms and read LTTPR capabilities */
 	bool ext_timeout_support = link->dc->caps.extended_aux_timeout_support &&
 			!link->dc->config.disable_extended_timeout_support;
+
 	link->is_lttpr_mode_transparent = true;
 
 	if (ext_timeout_support) {

commit 903e859b72957985c60de593f364e33639964829
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Wed Sep 18 11:57:47 2019 -0400

    drm/amd/display: check for dp rev before reading lttpr regs
    
    [Why]
    LTTPR was introduced after DP1.2. Reading LTTPR registers 0xFXXXX
    on some DP 1.2 display is causing an unexpected behavior.
    
    [How]
    Make sure that we don't read any lttpr registers on 1.2 displays.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2a89f90ef7a7..1e4480f3bd3c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2759,9 +2759,10 @@ static bool retrieve_link_cap(struct dc_link *link)
 	/* Set default timeout to 3.2ms and read LTTPR capabilities */
 	bool ext_timeout_support = link->dc->caps.extended_aux_timeout_support &&
 			!link->dc->config.disable_extended_timeout_support;
+	link->is_lttpr_mode_transparent = true;
+
 	if (ext_timeout_support) {
 		status = dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
-		link->is_lttpr_mode_transparent = true;
 	}
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
@@ -2796,7 +2797,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 		return false;
 	}
 
-	if (ext_timeout_support) {
+	if (ext_timeout_support && link->dpcd_caps.dpcd_rev.raw >= 0x14) {
 		status = core_link_read_dpcd(
 				link,
 				DP_PHY_REPEATER_CNT,

commit 8e5100a575433cc185a2e224280fbd873b6692dd
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Fri Jul 19 10:25:39 2019 -0400

    drm/amd/display: initialize lttpr
    
    [Description]
    When reading link, update the procedure as follows:
    1-Set aux timeout to extended: 3.2ms
    2-Start with reading lttpr caps
    3-Determine if lttpr support should be enabled. Reset aux timeout to
    400us if no repeater is found.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0f59b68aa4c2..2a89f90ef7a7 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -21,6 +21,9 @@
 #define DC_LOGGER \
 	link->ctx->logger
 
+
+#define DP_REPEATER_CONFIGURATION_AND_STATUS_OFFSET   0x50
+
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
 		PRE_EMPHASIS_LEVEL3,
@@ -2753,6 +2756,14 @@ static bool retrieve_link_cap(struct dc_link *link)
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 
+	/* Set default timeout to 3.2ms and read LTTPR capabilities */
+	bool ext_timeout_support = link->dc->caps.extended_aux_timeout_support &&
+			!link->dc->config.disable_extended_timeout_support;
+	if (ext_timeout_support) {
+		status = dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_EXTENDED_TIMEOUT_PERIOD);
+		link->is_lttpr_mode_transparent = true;
+	}
+
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
 		'\0', sizeof(union down_stream_port_count));
@@ -2785,6 +2796,51 @@ static bool retrieve_link_cap(struct dc_link *link)
 		return false;
 	}
 
+	if (ext_timeout_support) {
+		status = core_link_read_dpcd(
+				link,
+				DP_PHY_REPEATER_CNT,
+				&link->dpcd_caps.lttpr_caps.phy_repeater_cnt,
+				sizeof(link->dpcd_caps.lttpr_caps.phy_repeater_cnt));
+
+		if (link->dpcd_caps.lttpr_caps.phy_repeater_cnt > 0) {
+
+			link->is_lttpr_mode_transparent = false;
+
+			status = core_link_read_dpcd(
+					link,
+					DP_LT_TUNABLE_PHY_REPEATER_FIELD_DATA_STRUCTURE_REV,
+					(uint8_t *)&link->dpcd_caps.lttpr_caps.revision,
+					sizeof(link->dpcd_caps.lttpr_caps.revision));
+
+			status = core_link_read_dpcd(
+					link,
+					DP_MAX_LINK_RATE_PHY_REPEATER,
+					&link->dpcd_caps.lttpr_caps.max_link_rate,
+					sizeof(link->dpcd_caps.lttpr_caps.max_link_rate));
+
+			status = core_link_read_dpcd(
+					link,
+					DP_PHY_REPEATER_MODE,
+					(uint8_t *)&link->dpcd_caps.lttpr_caps.mode,
+					sizeof(link->dpcd_caps.lttpr_caps.mode));
+
+			status = core_link_read_dpcd(
+					link,
+					DP_MAX_LANE_COUNT_PHY_REPEATER,
+					&link->dpcd_caps.lttpr_caps.max_lane_count,
+					sizeof(link->dpcd_caps.lttpr_caps.max_lane_count));
+
+			status = core_link_read_dpcd(
+					link,
+					DP_PHY_REPEATER_EXTENDED_WAIT_TIMEOUT,
+					&link->dpcd_caps.lttpr_caps.max_ext_timeout,
+					sizeof(link->dpcd_caps.lttpr_caps.max_ext_timeout));
+		} else {
+			dc_link_aux_configure_timeout(link->ddc, LINK_AUX_DEFAULT_TIMEOUT_PERIOD);
+		}
+	}
+
 	{
 		union training_aux_rd_interval aux_rd_interval;
 

commit dd998291dbe92106d8c4a7581c409b356928d711
Author: David Galiffi <david.galiffi@amd.com>
Date:   Fri Sep 20 20:20:23 2019 -0400

    drm/amd/display: Fix dongle_caps containing stale information.
    
    [WHY]
    
    During detection:
    function: get_active_converter_info populates link->dpcd_caps.dongle_caps
    only when dpcd_rev >= DPCD_REV_11 and DWN_STRM_PORTX_TYPE is
    DOWN_STREAM_DETAILED_HDMI or DOWN_STREAM_DETAILED_DP_PLUS_PLUS.
    Otherwise, it is not cleared, and stale information remains.
    
    During mode validation:
    function: dp_active_dongle_validate_timing reads
    link->dpcd_caps.dongle_caps->dongle_type to determine the maximum
    pixel clock to support. This information is now stale and no longer
    valid.
    
    [HOW]
    dp_active_dongle_validate_timing should be using
    link->dpcd_caps->dongle_type instead.
    
    Signed-off-by: David Galiffi <david.galiffi@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index aae204141c60..0f59b68aa4c2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2575,6 +2575,7 @@ static void get_active_converter_info(
 	uint8_t data, struct dc_link *link)
 {
 	union dp_downstream_port_present ds_port = { .byte = data };
+	memset(&link->dpcd_caps.dongle_caps, 0, sizeof(link->dpcd_caps.dongle_caps));
 
 	/* decode converter info*/
 	if (!ds_port.fields.PORT_PRESENT) {

commit 566b4252fe9da9582dde008c5e9c3eb7c136e348
Author: Vitaly Prosyak <vitaly.prosyak@amd.com>
Date:   Mon Sep 16 17:04:33 2019 -0500

    drm/amd/display: add new active dongle to existent w/a
    
    [Why & How]
    Dongle 0x00E04C power down all internal circuits including
    AUX communication preventing reading DPCD table.
    Encoder will skip DP RX power down on disable output
    to keep receiver powered all the time.
    
    Signed-off-by: Vitaly Prosyak <vitaly.prosyak@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Acked-by: Vitaly Prosyak <Vitaly.Prosyak@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 649ed31ccfe5..aae204141c60 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2721,6 +2721,7 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		 * keep receiver powered all the time.*/
 		case DP_BRANCH_DEVICE_ID_0010FA:
 		case DP_BRANCH_DEVICE_ID_0080E1:
+		case DP_BRANCH_DEVICE_ID_00E04C:
 			link->wa_flags.dp_keep_receiver_powered = true;
 			break;
 

commit ab4a4072f260162284c15789329522a6773023ed
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Fri Sep 6 18:26:23 2019 -0400

    drm/amd/display: exit PSR during detection
    
    [Why]
    If 48mhz refclk is turned off during PSR, we will have issue doing
    link training during detection.
    
    [How]
    Get out of PSR before detection
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5a0c3384c16b..649ed31ccfe5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2076,11 +2076,11 @@ static bool allow_hpd_rx_irq(const struct dc_link *link)
 	return false;
 }
 
-static bool handle_hpd_irq_psr_sink(const struct dc_link *link)
+static bool handle_hpd_irq_psr_sink(struct dc_link *link)
 {
 	union dpcd_psr_configuration psr_configuration;
 
-	if (!link->psr_enabled)
+	if (!link->psr_feature_enabled)
 		return false;
 
 	dm_helpers_dp_read_dpcd(
@@ -2119,8 +2119,8 @@ static bool handle_hpd_irq_psr_sink(const struct dc_link *link)
 				sizeof(psr_error_status.raw));
 
 			/* PSR error, disable and re-enable PSR */
-			dc_link_set_psr_enable(link, false, true);
-			dc_link_set_psr_enable(link, true, true);
+			dc_link_set_psr_allow_active(link, false, true);
+			dc_link_set_psr_allow_active(link, true, true);
 
 			return true;
 		} else if (psr_sink_psr_status.bits.SINK_SELF_REFRESH_STATUS ==

commit 82db2e3c39dbe9dcbfbd78aa6341647554faaaf9
Author: Sivapiriyan Kumarasamy <sivapiriyan.kumarasamy@amd.com>
Date:   Thu Sep 12 15:55:44 2019 -0400

    drm/amd/display: fix bug with check for HPD Low in verify link cap
    
    [Why]
    There is a bug when determining if link training should be retried when
    HPD is low in dp_verify_link_cap_with_retries.
    
    [How]
    Correctly, fail dp_verify_link_cap_with_retries without retry when
    HPD is low.
    
    Signed-off-by: Sivapiriyan Kumarasamy <sivapiriyan.kumarasamy@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Abdoulaye Berthe <Abdoulaye.Berthe@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 701b73926616..5a0c3384c16b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1656,11 +1656,14 @@ bool dp_verify_link_cap_with_retries(
 
 	for (i = 0; i < attempts; i++) {
 		int fail_count = 0;
-		enum dc_connection_type type;
+		enum dc_connection_type type = dc_connection_none;
 
 		memset(&link->verified_link_cap, 0,
 				sizeof(struct dc_link_settings));
-		if (!dc_link_detect_sink(link, &type)) {
+		if (!dc_link_detect_sink(link, &type) || type == dc_connection_none) {
+			link->verified_link_cap.lane_count = LANE_COUNT_ONE;
+			link->verified_link_cap.link_rate = LINK_RATE_LOW;
+			link->verified_link_cap.link_spread = LINK_SPREAD_DISABLED;
 			break;
 		} else if (dp_verify_link_cap(link,
 				&link->reported_link_cap,

commit f537d474df15393ad25721f5203ce16ed3596d66
Author: Lewis Huang <Lewis.Huang@amd.com>
Date:   Thu Sep 5 15:33:58 2019 +0800

    drm/amd/display: check phy dpalt lane count config
    
    [Why]
    Type-c PHY config is not align with dpcd lane count.
    When those values didn't match, it cause driver do
    link training with 4 lane but phy only can output 2 lane.
    The link trainig always fail.
    
    [How]
    1. Modify get_max_link_cap function. According DPALT_DP4
    to update max lane count.
    2. Add dp_mst_verify_link_cap to handle MST case because
    we didn't call dp_mst_verify_link_cap for MST case.
    
    Signed-off-by: Lewis Huang <Lewis.Huang@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7c78caf7a602..701b73926616 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1409,6 +1409,9 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 	if (link->link_enc->features.flags.bits.IS_HBR3_CAPABLE)
 		max_link_cap.link_rate = LINK_RATE_HIGH3;
 
+	if (link->link_enc->funcs->get_max_link_cap)
+		link->link_enc->funcs->get_max_link_cap(link->link_enc, &max_link_cap);
+
 	/* Lower link settings based on sink's link cap */
 	if (link->reported_link_cap.lane_count < max_link_cap.lane_count)
 		max_link_cap.lane_count =
@@ -1670,6 +1673,19 @@ bool dp_verify_link_cap_with_retries(
 	return success;
 }
 
+bool dp_verify_mst_link_cap(
+	struct dc_link *link)
+{
+	struct dc_link_settings max_link_cap = {0};
+
+	max_link_cap = get_max_link_cap(link);
+	link->verified_link_cap = get_common_supported_link_settings(
+		link->reported_link_cap,
+		max_link_cap);
+
+	return true;
+}
+
 static struct dc_link_settings get_common_supported_link_settings(
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b)

commit 48af9b91b129f1d93221cd9f76032610a8cc6514
Author: Alvin Lee <alvin.lee2@amd.com>
Date:   Fri Aug 2 13:42:49 2019 -0400

    drm/amd/display: Don't allocate payloads if link lost
    
    We should not allocate payloads if the link is lost until the link is retrained.
    Some displays require this.
    
    Signed-off-by: Alvin Lee <alvin.lee2@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index f5742719b5d9..7c78caf7a602 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2364,6 +2364,8 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	enum dc_status result;
 
 	bool status = false;
+	struct pipe_ctx *pipe_ctx;
+	int i;
 
 	if (out_link_loss)
 		*out_link_loss = false;
@@ -2440,6 +2442,15 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 			&link->cur_link_settings,
 			true, LINK_TRAINING_ATTEMPTS);
 
+		for (i = 0; i < MAX_PIPES; i++) {
+			pipe_ctx = &link->dc->current_state->res_ctx.pipe_ctx[i];
+			if (pipe_ctx && pipe_ctx->stream && pipe_ctx->stream->link == link &&
+					pipe_ctx->stream->dpms_off == false &&
+					pipe_ctx->stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
+				dc_link_allocate_mst_payload(pipe_ctx);
+			}
+		}
+
 		status = false;
 		if (out_link_loss)
 			*out_link_loss = true;

commit 24d01c9b32547acf7261585aefb5c953930ed0fd
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Aug 2 16:32:13 2019 -0400

    drm/amd/display: fix odm pipe copy
    
    ODM next and prev pipe were missing from dc_copy_state
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 40067403b043..f5742719b5d9 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3188,7 +3188,7 @@ bool dc_link_dp_set_test_pattern(
 	memset(&training_pattern, 0, sizeof(training_pattern));
 
 	for (i = 0; i < MAX_PIPES; i++) {
-		if (pipes[i].stream->link == link) {
+		if (pipes[i].stream->link == link && !pipes[i].top_pipe && !pipes[i].prev_odm_pipe) {
 			pipe_ctx = &pipes[i];
 			break;
 		}

commit df3b7e32ed459a5348f7b408a9b8142b7358fde8
Author: Qingqing Zhuo <qingqing.zhuo@amd.com>
Date:   Wed Jul 31 18:11:16 2019 -0400

    drm/amd/display: refactor Device ID for external chips
    
    IEEE OUI will now be used while referring to certain vendors.
    instead of normal index
    
    Signed-off-by: Qingqing Zhuo <qingqing.zhuo@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2aa44b28b673..40067403b043 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2684,13 +2684,13 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 
 	if (link->dpcd_caps.dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) {
 		switch (link->dpcd_caps.branch_dev_id) {
-		/* Some active dongles (DP-VGA, DP-DLDVI converters) power down
+		/* 0010FA active dongles (DP-VGA, DP-DLDVI converters) power down
 		 * all internal circuits including AUX communication preventing
 		 * reading DPCD table and EDID (spec violation).
 		 * Encoder will skip DP RX power down on disable_output to
 		 * keep receiver powered all the time.*/
-		case DP_BRANCH_DEVICE_ID_1:
-		case DP_BRANCH_DEVICE_ID_4:
+		case DP_BRANCH_DEVICE_ID_0010FA:
+		case DP_BRANCH_DEVICE_ID_0080E1:
 			link->wa_flags.dp_keep_receiver_powered = true;
 			break;
 
@@ -3394,7 +3394,13 @@ enum dp_panel_mode dp_get_panel_mode(struct dc_link *link)
 	if (link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT) {
 
 		switch (link->dpcd_caps.branch_dev_id) {
-		case DP_BRANCH_DEVICE_ID_2:
+		case DP_BRANCH_DEVICE_ID_0022B9:
+			/* alternate scrambler reset is required for Travis
+			 * for the case when external chip does not
+			 * provide sink device id, alternate scrambler
+			 * scheme will  be overriden later by querying
+			 * Encoder features
+			 */
 			if (strncmp(
 				link->dpcd_caps.branch_dev_name,
 				DP_VGA_LVDS_CONVERTER_ID_2,
@@ -3404,7 +3410,12 @@ enum dp_panel_mode dp_get_panel_mode(struct dc_link *link)
 					return DP_PANEL_MODE_SPECIAL;
 			}
 			break;
-		case DP_BRANCH_DEVICE_ID_3:
+		case DP_BRANCH_DEVICE_ID_00001A:
+			/* alternate scrambler reset is required for Travis
+			 * for the case when external chip does not provide
+			 * sink device id, alternate scrambler scheme will
+			 * be overriden later by querying Encoder feature
+			 */
 			if (strncmp(link->dpcd_caps.branch_dev_name,
 				DP_VGA_LVDS_CONVERTER_ID_3,
 				sizeof(

commit b1f6d01c4a3b7eeb2eb035e79d425cd6a696fa45
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Aug 6 17:17:28 2019 -0400

    drm/amd/display: re structure odm to allow 4 to 1 support
    
    Currently odm is handled using top_bottom pipe by special casing
    the differing opps to differentiate from mpc combine.
    
    Since top/bottom pipe list was made to track mpc muxing this creates
    difficulties in adding a 4 pipe odm case support.
    
    Rather than continue using mpc combine list, this change reworks odm
    to use it's own linked list to keep track of odm combine pipes. This
    also opens up options for using mpo with odm, if a practical use case
    is ever found.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2e87942b3e9c..2aa44b28b673 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3095,14 +3095,19 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				controller_test_pattern, color_depth);
 #if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
-			struct pipe_ctx *bot_odm_pipe = dc_res_get_odm_bottom_pipe(pipe_ctx);
+			struct pipe_ctx *odm_pipe;
+			int opp_cnt = 1;
 
-			if (bot_odm_pipe) {
-				struct output_pixel_processor *bot_opp = bot_odm_pipe->stream_res.opp;
+			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe)
+				opp_cnt++;
 
-				bot_opp->funcs->opp_program_bit_depth_reduction(bot_opp, &params);
-				width /= 2;
-				bot_opp->funcs->opp_set_disp_pattern_generator(bot_opp,
+			width /= opp_cnt;
+
+			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) {
+				struct output_pixel_processor *odm_opp = odm_pipe->stream_res.opp;
+
+				odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params);
+				odm_opp->funcs->opp_set_disp_pattern_generator(odm_opp,
 					controller_test_pattern,
 					color_depth,
 					NULL,
@@ -3131,14 +3136,18 @@ static void set_crtc_test_pattern(struct dc_link *link,
 				color_depth);
 #if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 		else if (opp->funcs->opp_set_disp_pattern_generator) {
-			struct pipe_ctx *bot_odm_pipe = dc_res_get_odm_bottom_pipe(pipe_ctx);
+			struct pipe_ctx *odm_pipe;
+			int opp_cnt = 1;
+
+			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe)
+				opp_cnt++;
 
-			if (bot_odm_pipe) {
-				struct output_pixel_processor *bot_opp = bot_odm_pipe->stream_res.opp;
+			width /= opp_cnt;
+			for (odm_pipe = pipe_ctx->next_odm_pipe; odm_pipe; odm_pipe = odm_pipe->next_odm_pipe) {
+				struct output_pixel_processor *odm_opp = odm_pipe->stream_res.opp;
 
-				bot_opp->funcs->opp_program_bit_depth_reduction(bot_opp, &params);
-				width /= 2;
-				bot_opp->funcs->opp_set_disp_pattern_generator(bot_opp,
+				odm_opp->funcs->opp_program_bit_depth_reduction(odm_opp, &params);
+				odm_opp->funcs->opp_set_disp_pattern_generator(odm_opp,
 					CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 					color_depth,
 					NULL,

commit e7f2c80cbaabf62966d02ba752d19c9a63f422ab
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Fri Jul 26 14:53:20 2019 -0400

    drm/amd/display: check hpd before retry verify link cap
    
    [why]
    During detection link training if a display is disconnected,
    the current code will retry 3 times of link training
    on disconnected link before giving up.
    
    [how]
    Before each retry check for HPD status, only retry
    verify link cap when HPD is still high.
    Also put a 10ms delay between each retry to improve
    the chance of success.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Abdoulaye Berthe <Abdoulaye.Berthe@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8e66b2e9d6af..2e87942b3e9c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1643,6 +1643,33 @@ bool dp_verify_link_cap(
 	return success;
 }
 
+bool dp_verify_link_cap_with_retries(
+	struct dc_link *link,
+	struct dc_link_settings *known_limit_link_setting,
+	int attempts)
+{
+	uint8_t i = 0;
+	bool success = false;
+
+	for (i = 0; i < attempts; i++) {
+		int fail_count = 0;
+		enum dc_connection_type type;
+
+		memset(&link->verified_link_cap, 0,
+				sizeof(struct dc_link_settings));
+		if (!dc_link_detect_sink(link, &type)) {
+			break;
+		} else if (dp_verify_link_cap(link,
+				&link->reported_link_cap,
+				&fail_count) && fail_count == 0) {
+			success = true;
+			break;
+		}
+		msleep(10);
+	}
+	return success;
+}
+
 static struct dc_link_settings get_common_supported_link_settings(
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b)

commit 0b226322434c7786381d7a594efca9cc46f85211
Author: David Galiffi <david.galiffi@amd.com>
Date:   Mon Jun 24 10:34:13 2019 -0400

    drm/amd/display: Synchronous DisplayPort Link Training
    
    [WHY]
    We require a method to perform synchronous link training.
    
    [HOW]
    Sync LT is broken into 3 basic steps.
    "Begin" starts the state machine, and resets "preferred" link settings.
    "Attempt" will attempt to train the link with a given set of training
    parameters.
    "End" stops the state machine, and will optionally disable the link phy.
    Between "Begin" and "End" DPCD:600h must not be set to "2"
    (D3:Powered Down).
    Between "Begin" and "End", there may be multiple "Attempts" with different
    training parameters.
    
    Signed-off-by: David Galiffi <david.galiffi@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 08bd9c96b9b0..8e66b2e9d6af 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -965,6 +965,7 @@ static inline enum link_training_result perform_link_training_int(
 static void initialize_training_settings(
 	 struct dc_link *link,
 	const struct dc_link_settings *link_setting,
+	const struct dc_link_training_overrides *overrides,
 	struct link_training_settings *lt_settings)
 {
 	uint32_t lane;
@@ -997,23 +998,23 @@ static void initialize_training_settings(
 	/* Initialize link spread */
 	if (link->dp_ss_off)
 		lt_settings->link_settings.link_spread = LINK_SPREAD_DISABLED;
-	else if (link->preferred_training_settings.downspread != NULL)
+	else if (overrides->downspread != NULL)
 		lt_settings->link_settings.link_spread
-			= *link->preferred_training_settings.downspread
+			= *overrides->downspread
 			? LINK_SPREAD_05_DOWNSPREAD_30KHZ
 			: LINK_SPREAD_DISABLED;
 	else
 		lt_settings->link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
 
 	/* Initialize lane settings overrides */
-	if (link->preferred_training_settings.voltage_swing != NULL)
-		lt_settings->voltage_swing = link->preferred_training_settings.voltage_swing;
+	if (overrides->voltage_swing != NULL)
+		lt_settings->voltage_swing = overrides->voltage_swing;
 
-	if (link->preferred_training_settings.pre_emphasis != NULL)
-		lt_settings->pre_emphasis = link->preferred_training_settings.pre_emphasis;
+	if (overrides->pre_emphasis != NULL)
+		lt_settings->pre_emphasis = overrides->pre_emphasis;
 
-	if (link->preferred_training_settings.post_cursor2 != NULL)
-		lt_settings->post_cursor2 = link->preferred_training_settings.post_cursor2;
+	if (overrides->post_cursor2 != NULL)
+		lt_settings->post_cursor2 = overrides->post_cursor2;
 
 	/* Initialize lane settings (VS/PE/PC2) */
 	for (lane = 0; lane < LANE_COUNT_DP_MAX; lane++) {
@@ -1032,23 +1033,23 @@ static void initialize_training_settings(
 	}
 
 	/* Initialize training timings */
-	if (link->preferred_training_settings.cr_pattern_time != NULL)
-		lt_settings->cr_pattern_time = *link->preferred_training_settings.cr_pattern_time;
+	if (overrides->cr_pattern_time != NULL)
+		lt_settings->cr_pattern_time = *overrides->cr_pattern_time;
 	else
-		lt_settings->cr_pattern_time = 100;
+		lt_settings->cr_pattern_time = get_training_aux_rd_interval(link, 100);
 
-	if (link->preferred_training_settings.eq_pattern_time != NULL)
-		lt_settings->eq_pattern_time = *link->preferred_training_settings.eq_pattern_time;
+	if (overrides->eq_pattern_time != NULL)
+		lt_settings->eq_pattern_time = *overrides->eq_pattern_time;
 	else
 		lt_settings->eq_pattern_time = get_training_aux_rd_interval(link, 400);
 
-	if (link->preferred_training_settings.pattern_for_eq != NULL)
-		lt_settings->pattern_for_eq = *link->preferred_training_settings.pattern_for_eq;
+	if (overrides->pattern_for_eq != NULL)
+		lt_settings->pattern_for_eq = *overrides->pattern_for_eq;
 	else
 		lt_settings->pattern_for_eq = get_supported_tp(link);
 
-	if (link->preferred_training_settings.enhanced_framing != NULL)
-		lt_settings->enhanced_framing = *link->preferred_training_settings.enhanced_framing;
+	if (overrides->enhanced_framing != NULL)
+		lt_settings->enhanced_framing = *overrides->enhanced_framing;
 	else
 		lt_settings->enhanced_framing = 1;
 }
@@ -1139,7 +1140,11 @@ bool dc_link_dp_perform_link_training_skip_aux(
 	struct link_training_settings lt_settings;
 	enum dc_dp_training_pattern pattern_for_cr = DP_TRAINING_PATTERN_SEQUENCE_1;
 
-	initialize_training_settings(link, link_setting, &lt_settings);
+	initialize_training_settings(
+			link,
+			link_setting,
+			&link->preferred_training_settings,
+			&lt_settings);
 
 	/* 1. Perform_clock_recovery_sequence. */
 
@@ -1184,7 +1189,11 @@ enum link_training_result dc_link_dp_perform_link_training(
 	bool fec_enable;
 #endif
 
-	initialize_training_settings(link, link_setting, &lt_settings);
+	initialize_training_settings(
+			link,
+			link_setting,
+			&link->preferred_training_settings,
+			&lt_settings);
 
 	/* 1. set link rate, lane count and spread. */
 	dpcd_set_link_settings(link, &lt_settings);
@@ -1247,6 +1256,146 @@ bool perform_link_training_with_retries(
 	return false;
 }
 
+static enum clock_source_id get_clock_source_id(struct dc_link *link)
+{
+	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_UNDEFINED;
+	struct clock_source *dp_cs = link->dc->res_pool->dp_clock_source;
+
+	if (dp_cs != NULL) {
+		dp_cs_id = dp_cs->id;
+	} else {
+		/*
+		 * dp clock source is not initialized for some reason.
+		 * Should not happen, CLOCK_SOURCE_ID_EXTERNAL will be used
+		 */
+		ASSERT(dp_cs);
+	}
+
+	return dp_cs_id;
+}
+
+static void set_dp_mst_mode(struct dc_link *link, bool mst_enable)
+{
+	if (mst_enable == false &&
+		link->type == dc_connection_mst_branch) {
+		/* Disable MST on link. Use only local sink. */
+		dp_disable_link_phy_mst(link, link->connector_signal);
+
+		link->type = dc_connection_single;
+		link->local_sink = link->remote_sinks[0];
+		link->local_sink->sink_signal = SIGNAL_TYPE_DISPLAY_PORT;
+	} else if (mst_enable == true &&
+			link->type == dc_connection_single &&
+			link->remote_sinks[0] != NULL) {
+		/* Re-enable MST on link. */
+		dp_disable_link_phy(link, link->connector_signal);
+		dp_enable_mst_on_sink(link, true);
+
+		link->type = dc_connection_mst_branch;
+		link->local_sink->sink_signal = SIGNAL_TYPE_DISPLAY_PORT_MST;
+	}
+}
+
+bool dc_link_dp_sync_lt_begin(struct dc_link *link)
+{
+	/* Begin Sync LT. During this time,
+	 * DPCD:600h must not be powered down.
+	 */
+	link->sync_lt_in_progress = true;
+
+	/*Clear any existing preferred settings.*/
+	memset(&link->preferred_training_settings, 0,
+		sizeof(struct dc_link_training_overrides));
+	memset(&link->preferred_link_setting, 0,
+		sizeof(struct dc_link_settings));
+
+	return true;
+}
+
+enum link_training_result dc_link_dp_sync_lt_attempt(
+    struct dc_link *link,
+    struct dc_link_settings *link_settings,
+    struct dc_link_training_overrides *lt_overrides)
+{
+	struct link_training_settings lt_settings;
+	enum link_training_result lt_status = LINK_TRAINING_SUCCESS;
+	enum dp_panel_mode panel_mode = DP_PANEL_MODE_DEFAULT;
+	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	bool fec_enable = false;
+#endif
+
+	initialize_training_settings(
+		link,
+		link_settings,
+		lt_overrides,
+		&lt_settings);
+
+	/* Setup MST Mode */
+	if (lt_overrides->mst_enable)
+		set_dp_mst_mode(link, *lt_overrides->mst_enable);
+
+	/* Disable link */
+	dp_disable_link_phy(link, link->connector_signal);
+
+	/* Enable link */
+	dp_cs_id = get_clock_source_id(link);
+	dp_enable_link_phy(link, link->connector_signal,
+		dp_cs_id, link_settings);
+
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	/* Set FEC enable */
+	fec_enable = lt_overrides->fec_enable && *lt_overrides->fec_enable;
+	dp_set_fec_ready(link, fec_enable);
+#endif
+
+	if (lt_overrides->alternate_scrambler_reset) {
+		if (*lt_overrides->alternate_scrambler_reset)
+			panel_mode = DP_PANEL_MODE_EDP;
+		else
+			panel_mode = DP_PANEL_MODE_DEFAULT;
+	} else
+		panel_mode = dp_get_panel_mode(link);
+
+	dp_set_panel_mode(link, panel_mode);
+
+	/* Attempt to train with given link training settings */
+
+	/* Set link rate, lane count and spread. */
+	dpcd_set_link_settings(link, &lt_settings);
+
+	/* 2. perform link training (set link training done
+	 *  to false is done as well)
+	 */
+	lt_status = perform_clock_recovery_sequence(link, &lt_settings);
+	if (lt_status == LINK_TRAINING_SUCCESS) {
+		lt_status = perform_channel_equalization_sequence(link,
+						&lt_settings);
+	}
+
+	/* 3. Sync LT must skip TRAINING_PATTERN_SET:0 (video pattern)*/
+	/* 4. print status message*/
+	print_status_message(link, &lt_settings, lt_status);
+
+	return lt_status;
+}
+
+bool dc_link_dp_sync_lt_end(struct dc_link *link, bool link_down)
+{
+	/* If input parameter is set, shut down phy.
+	 * Still shouldn't turn off dp_receiver (DPCD:600h)
+	 */
+	if (link_down == true) {
+		dp_disable_link_phy(link, link->connector_signal);
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+		dp_set_fec_ready(link, false);
+#endif
+	}
+
+	link->sync_lt_in_progress = false;
+	return true;
+}
+
 static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 {
 	/* Set Default link settings */
@@ -1401,7 +1550,6 @@ bool dp_verify_link_cap(
 	bool success;
 	bool skip_link_training;
 	bool skip_video_pattern;
-	struct clock_source *dp_cs;
 	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
 	enum link_training_result status;
 	union hpd_irq_data irq_data;
@@ -1425,17 +1573,7 @@ bool dp_verify_link_cap(
 	/* disable PHY done possible by BIOS, will be done by driver itself */
 	dp_disable_link_phy(link, link->connector_signal);
 
-	dp_cs = link->dc->res_pool->dp_clock_source;
-
-	if (dp_cs)
-		dp_cs_id = dp_cs->id;
-	else {
-		/*
-		 * dp clock source is not initialized for some reason.
-		 * Should not happen, CLOCK_SOURCE_ID_EXTERNAL will be used
-		 */
-		ASSERT(dp_cs);
-	}
+	dp_cs_id = get_clock_source_id(link);
 
 	/* link training starts with the maximum common settings
 	 * supported by both sink and ASIC.
@@ -2307,6 +2445,11 @@ bool is_mst_supported(struct dc_link *link)
 	union dpcd_rev rev;
 	union mstm_cap cap;
 
+	if (link->preferred_training_settings.mst_enable &&
+		*link->preferred_training_settings.mst_enable == false) {
+		return false;
+	}
+
 	rev.raw  = 0;
 	cap.raw  = 0;
 
@@ -3158,6 +3301,94 @@ void dp_enable_mst_on_sink(struct dc_link *link, bool enable)
 	core_link_write_dpcd(link, DP_MSTM_CTRL, &mstmCntl, 1);
 }
 
+void dp_set_panel_mode(struct dc_link *link, enum dp_panel_mode panel_mode)
+{
+	union dpcd_edp_config edp_config_set;
+	bool panel_mode_edp = false;
+
+	memset(&edp_config_set, '\0', sizeof(union dpcd_edp_config));
+
+	if (panel_mode != DP_PANEL_MODE_DEFAULT) {
+
+		switch (panel_mode) {
+		case DP_PANEL_MODE_EDP:
+		case DP_PANEL_MODE_SPECIAL:
+			panel_mode_edp = true;
+			break;
+
+		default:
+				break;
+		}
+
+		/*set edp panel mode in receiver*/
+		core_link_read_dpcd(
+			link,
+			DP_EDP_CONFIGURATION_SET,
+			&edp_config_set.raw,
+			sizeof(edp_config_set.raw));
+
+		if (edp_config_set.bits.PANEL_MODE_EDP
+			!= panel_mode_edp) {
+			enum ddc_result result = DDC_RESULT_UNKNOWN;
+
+			edp_config_set.bits.PANEL_MODE_EDP =
+			panel_mode_edp;
+			result = core_link_write_dpcd(
+				link,
+				DP_EDP_CONFIGURATION_SET,
+				&edp_config_set.raw,
+				sizeof(edp_config_set.raw));
+
+			ASSERT(result == DDC_RESULT_SUCESSFULL);
+		}
+	}
+	DC_LOG_DETECTION_DP_CAPS("Link: %d eDP panel mode supported: %d "
+		 "eDP panel mode enabled: %d \n",
+		 link->link_index,
+		 link->dpcd_caps.panel_mode_edp,
+		 panel_mode_edp);
+}
+
+enum dp_panel_mode dp_get_panel_mode(struct dc_link *link)
+{
+	/* We need to explicitly check that connector
+	 * is not DP. Some Travis_VGA get reported
+	 * by video bios as DP.
+	 */
+	if (link->connector_signal != SIGNAL_TYPE_DISPLAY_PORT) {
+
+		switch (link->dpcd_caps.branch_dev_id) {
+		case DP_BRANCH_DEVICE_ID_2:
+			if (strncmp(
+				link->dpcd_caps.branch_dev_name,
+				DP_VGA_LVDS_CONVERTER_ID_2,
+				sizeof(
+				link->dpcd_caps.
+				branch_dev_name)) == 0) {
+					return DP_PANEL_MODE_SPECIAL;
+			}
+			break;
+		case DP_BRANCH_DEVICE_ID_3:
+			if (strncmp(link->dpcd_caps.branch_dev_name,
+				DP_VGA_LVDS_CONVERTER_ID_3,
+				sizeof(
+				link->dpcd_caps.
+				branch_dev_name)) == 0) {
+					return DP_PANEL_MODE_SPECIAL;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	if (link->dpcd_caps.panel_mode_edp) {
+		return DP_PANEL_MODE_EDP;
+	}
+
+	return DP_PANEL_MODE_DEFAULT;
+}
+
 #ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
 void dp_set_fec_ready(struct dc_link *link, bool ready)
 {

commit 7a83645ac0cc2258de101a6a491d31499eb27cc5
Author: Dale Zhao <dale.zhao@amd.com>
Date:   Wed Jul 10 17:36:53 2019 +0800

    drm/amd/display: handle active dongle port type is DP++ or DP case
    
    [Why]:
    Some active dongles have DP++ port and DP port at the same time. Current
    code doesn't cover DP++ case and processes as default DVI case, in which
    audio is disabled. Because of dual mode, DP case is also treat as DVI case
    for the other port.
    
    [How]:
    According DP 1.4 spec, add DP++ procedure similar with HDMI case. Also
    add None dongle type for DP case.
    
    Signed-off-by: Dale Zhao <dale.zhao@amd.com>
    Reviewed-by: Wenjing Liu <wenjing.liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b512fecae061..08bd9c96b9b0 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2398,8 +2398,8 @@ static void get_active_converter_info(
 	case DOWNSTREAM_VGA:
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_DP_VGA_CONVERTER;
 		break;
-	case DOWNSTREAM_DVI_HDMI:
-		/* At this point we don't know is it DVI or HDMI,
+	case DOWNSTREAM_DVI_HDMI_DP_PLUS_PLUS:
+		/* At this point we don't know is it DVI or HDMI or DP++,
 		 * assume DVI.*/
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_DP_DVI_CONVERTER;
 		break;
@@ -2416,6 +2416,10 @@ static void get_active_converter_info(
 				det_caps, sizeof(det_caps));
 
 		switch (port_caps->bits.DWN_STRM_PORTX_TYPE) {
+		/*Handle DP case as DONGLE_NONE*/
+		case DOWN_STREAM_DETAILED_DP:
+			link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
+			break;
 		case DOWN_STREAM_DETAILED_VGA:
 			link->dpcd_caps.dongle_type =
 				DISPLAY_DONGLE_DP_VGA_CONVERTER;
@@ -2425,6 +2429,8 @@ static void get_active_converter_info(
 				DISPLAY_DONGLE_DP_DVI_CONVERTER;
 			break;
 		case DOWN_STREAM_DETAILED_HDMI:
+		case DOWN_STREAM_DETAILED_DP_PLUS_PLUS:
+			/*Handle DP++ active converter case, process DP++ case as HDMI case according DP1.4 spec*/
 			link->dpcd_caps.dongle_type =
 				DISPLAY_DONGLE_DP_HDMI_CONVERTER;
 
@@ -2440,14 +2446,18 @@ static void get_active_converter_info(
 
 				link->dpcd_caps.dongle_caps.is_dp_hdmi_s3d_converter =
 					hdmi_caps.bits.FRAME_SEQ_TO_FRAME_PACK;
-				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_pass_through =
-					hdmi_caps.bits.YCrCr422_PASS_THROUGH;
-				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_pass_through =
-					hdmi_caps.bits.YCrCr420_PASS_THROUGH;
-				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_converter =
-					hdmi_caps.bits.YCrCr422_CONVERSION;
-				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_converter =
-					hdmi_caps.bits.YCrCr420_CONVERSION;
+				/*YCBCR capability only for HDMI case*/
+				if (port_caps->bits.DWN_STRM_PORTX_TYPE
+						== DOWN_STREAM_DETAILED_HDMI) {
+					link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_pass_through =
+							hdmi_caps.bits.YCrCr422_PASS_THROUGH;
+					link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_pass_through =
+							hdmi_caps.bits.YCrCr420_PASS_THROUGH;
+					link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_converter =
+							hdmi_caps.bits.YCrCr422_CONVERSION;
+					link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_converter =
+							hdmi_caps.bits.YCrCr420_CONVERSION;
+				}
 
 				link->dpcd_caps.dongle_caps.dp_hdmi_max_bpc =
 					translate_dpcd_max_bpc(

commit d68a74541735e030dea56f72746cd26d19986f41
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Tue Jun 25 17:19:25 2019 -0400

    drm/amd/display: Clear FEC_READY shadow register if DPCD write fails
    
    [why]
    As a fail-safe, in case 'set FEC_READY' DPCD write fails, a HW shadow
    register should be cleared and the internal FEC stat should be set to
    'not ready'. This is to make sure HW settings will be consistent with
    FEC_READY state on the RX.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Joshua Aberback <Joshua.Aberback@amd.com>
    Acked-by: Chris Park <Chris.Park@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5c8e3318239c..b512fecae061 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3174,6 +3174,8 @@ void dp_set_fec_ready(struct dc_link *link, bool ready)
 				link_enc->funcs->fec_set_ready(link_enc, true);
 				link->fec_state = dc_link_fec_ready;
 			} else {
+				link->link_enc->funcs->fec_set_ready(link->link_enc, false);
+				link->fec_state = dc_link_fec_not_ready;
 				dm_error("dpcd write failed to set fec_ready");
 			}
 		} else if (link->fec_state == dc_link_fec_ready) {

commit 008a4016c5cf922d33456916ec3fad9ac4c98962
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Mon Jun 24 15:44:42 2019 -0400

    drm/amd/display: Set FEC_READY always before link training
    
    [why]
    Right now we FEC_READY is set only before the final link training,
    i.e. at mode set time. This means FEC_READY won't be set when doing
    link training as a response to HPD. It also fails UCD400 FEC test in
    DP compliance.
    
    [how]
    Move FEC_READY setup to link training.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Abdoulaye Berthe <Abdoulaye.Berthe@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4442e7b1e5b5..5c8e3318239c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1179,14 +1179,26 @@ enum link_training_result dc_link_dp_perform_link_training(
 	bool skip_video_pattern)
 {
 	enum link_training_result status = LINK_TRAINING_SUCCESS;
-
 	struct link_training_settings lt_settings;
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	bool fec_enable;
+#endif
 
 	initialize_training_settings(link, link_setting, &lt_settings);
 
 	/* 1. set link rate, lane count and spread. */
 	dpcd_set_link_settings(link, &lt_settings);
 
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	if (link->preferred_training_settings.fec_enable != NULL)
+		fec_enable = *link->preferred_training_settings.fec_enable;
+	else
+		fec_enable = true;
+
+	dp_set_fec_ready(link, fec_enable);
+#endif
+
+
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)
 	 */
@@ -3153,7 +3165,7 @@ void dp_set_fec_ready(struct dc_link *link, bool ready)
 
 	if (link_enc->funcs->fec_set_ready &&
 			link->dpcd_caps.fec_cap.bits.FEC_CAPABLE) {
-		if (link->fec_state == dc_link_fec_not_ready && ready) {
+		if (ready) {
 			fec_config = 1;
 			if (core_link_write_dpcd(link,
 					DP_FEC_CONFIGURATION,
@@ -3164,7 +3176,7 @@ void dp_set_fec_ready(struct dc_link *link, bool ready)
 			} else {
 				dm_error("dpcd write failed to set fec_ready");
 			}
-		} else if (link->fec_state == dc_link_fec_ready && !ready) {
+		} else if (link->fec_state == dc_link_fec_ready) {
 			fec_config = 0;
 			core_link_write_dpcd(link,
 					DP_FEC_CONFIGURATION,

commit 0430017149c53f20493ebeee856315c669d18f4d
Author: David Galiffi <david.galiffi@amd.com>
Date:   Fri Jun 7 21:32:34 2019 -0400

    drm/amd/display: Incorrect Read Interval Time For CR Sequence
    
    [WHY]
    TRAINING_AUX_RD_INTERVAL (DPCD 000Eh) modifies the read interval
    for the EQ training sequence. CR read interval should remain 100 us.
    Currently, the CR interval is also being modified.
    
    [HOW]
    lt_settings->cr_pattern_time should always be 100 us.
    
    Signed-off-by: David Galiffi <david.galiffi@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index fca1bfc901b6..4442e7b1e5b5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1035,7 +1035,7 @@ static void initialize_training_settings(
 	if (link->preferred_training_settings.cr_pattern_time != NULL)
 		lt_settings->cr_pattern_time = *link->preferred_training_settings.cr_pattern_time;
 	else
-		lt_settings->cr_pattern_time = get_training_aux_rd_interval(link, 100);
+		lt_settings->cr_pattern_time = 100;
 
 	if (link->preferred_training_settings.eq_pattern_time != NULL)
 		lt_settings->eq_pattern_time = *link->preferred_training_settings.eq_pattern_time;

commit ac3d76e0665cd66b06ee13f5ed61cdbeb8180229
Author: Harmanprit Tatla <harmanprit.tatla@amd.com>
Date:   Tue Jun 4 14:12:21 2019 -0400

    drm/amd/display: No audio endpoint for Dell MST display
    
    [Why]
    There are certain MST displays (i.e. Dell P2715Q)
    that although have the MST feature set to off may still
    report it is a branch device and a non-zero
    value for downstream port present.
    This can lead to us incorrectly classifying a
    dp dongle connection as being active and
    disabling the audio endpoint for the display.
    
    [How]
    Modified the placement and
    condition used to assign
    the is_branch_dev bit.
    
    Signed-off-by: Harmanprit Tatla <harmanprit.tatla@amd.com>
    Reviewed-by: Aric Cyr <aric.cyr@amd.com>
    Acked-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3f8a8f61cd76..fca1bfc901b6 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2369,11 +2369,18 @@ static void get_active_converter_info(
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
 		ddc_service_set_dongle_type(link->ddc,
 				link->dpcd_caps.dongle_type);
+		link->dpcd_caps.is_branch_dev = false;
 		return;
 	}
 
 	/* DPCD 0x5 bit 0 = 1, it indicate it's branch device */
-	link->dpcd_caps.is_branch_dev = ds_port.fields.PORT_PRESENT;
+	if (ds_port.fields.PORT_TYPE == DOWNSTREAM_DP) {
+		link->dpcd_caps.is_branch_dev = false;
+	}
+
+	else {
+		link->dpcd_caps.is_branch_dev = ds_port.fields.PORT_PRESENT;
+	}
 
 	switch (ds_port.fields.PORT_TYPE) {
 	case DOWNSTREAM_VGA:

commit e0a6440a2961b1da3ea895b0bef082fc1a78e190
Author: David Galiffi <David.Galiffi@amd.com>
Date:   Thu May 30 11:56:39 2019 -0400

    drm/amd/display: Add ability to set preferred link training parameters.
    
    [WHY]
    To add support for OS requirement to set preferred link training
    parameters.
    
    [HOW]
    Create new structure of dp link training overrides. During link training
    processes, these values should be used instead of the default training
    parameters.
    
    Signed-off-by: David Galiffi <David.Galiffi@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 056be4c34a98..3f8a8f61cd76 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -49,7 +49,7 @@ static struct dc_link_settings get_common_supported_link_settings(
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b);
 
-static void wait_for_training_aux_rd_interval(
+static uint32_t get_training_aux_rd_interval(
 	struct dc_link *link,
 	uint32_t default_wait_in_micro_secs)
 {
@@ -68,15 +68,21 @@ static void wait_for_training_aux_rd_interval(
 			sizeof(training_rd_interval));
 
 		if (training_rd_interval.bits.TRAINIG_AUX_RD_INTERVAL)
-			default_wait_in_micro_secs =
-				training_rd_interval.bits.TRAINIG_AUX_RD_INTERVAL * 4000;
+			default_wait_in_micro_secs = training_rd_interval.bits.TRAINIG_AUX_RD_INTERVAL * 4000;
 	}
 
-	udelay(default_wait_in_micro_secs);
+	return default_wait_in_micro_secs;
+}
+
+static void wait_for_training_aux_rd_interval(
+	struct dc_link *link,
+	uint32_t wait_in_micro_secs)
+{
+	udelay(wait_in_micro_secs);
 
 	DC_LOG_HW_LINK_TRAINING("%s:\n wait = %d\n",
 		__func__,
-		default_wait_in_micro_secs);
+		wait_in_micro_secs);
 }
 
 static void dpcd_set_training_pattern(
@@ -95,27 +101,27 @@ static void dpcd_set_training_pattern(
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 }
 
-static enum hw_dp_training_pattern get_supported_tp(struct dc_link *link)
+static enum dc_dp_training_pattern get_supported_tp(struct dc_link *link)
 {
-	enum hw_dp_training_pattern highest_tp = HW_DP_TRAINING_PATTERN_2;
+	enum dc_dp_training_pattern highest_tp = DP_TRAINING_PATTERN_SEQUENCE_2;
 	struct encoder_feature_support *features = &link->link_enc->features;
 	struct dpcd_caps *dpcd_caps = &link->dpcd_caps;
 
 	if (features->flags.bits.IS_TPS3_CAPABLE)
-		highest_tp = HW_DP_TRAINING_PATTERN_3;
+		highest_tp = DP_TRAINING_PATTERN_SEQUENCE_3;
 
 	if (features->flags.bits.IS_TPS4_CAPABLE)
-		highest_tp = HW_DP_TRAINING_PATTERN_4;
+		highest_tp = DP_TRAINING_PATTERN_SEQUENCE_4;
 
 	if (dpcd_caps->max_down_spread.bits.TPS4_SUPPORTED &&
-		highest_tp >= HW_DP_TRAINING_PATTERN_4)
-		return HW_DP_TRAINING_PATTERN_4;
+		highest_tp >= DP_TRAINING_PATTERN_SEQUENCE_4)
+		return DP_TRAINING_PATTERN_SEQUENCE_4;
 
 	if (dpcd_caps->max_ln_count.bits.TPS3_SUPPORTED &&
-		highest_tp >= HW_DP_TRAINING_PATTERN_3)
-		return HW_DP_TRAINING_PATTERN_3;
+		highest_tp >= DP_TRAINING_PATTERN_SEQUENCE_3)
+		return DP_TRAINING_PATTERN_SEQUENCE_3;
 
-	return HW_DP_TRAINING_PATTERN_2;
+	return DP_TRAINING_PATTERN_SEQUENCE_2;
 }
 
 static void dpcd_set_link_settings(
@@ -126,7 +132,7 @@ static void dpcd_set_link_settings(
 
 	union down_spread_ctrl downspread = { {0} };
 	union lane_count_set lane_count_set = { {0} };
-	enum hw_dp_training_pattern hw_tr_pattern;
+	enum dc_dp_training_pattern dp_tr_pattern;
 
 	downspread.raw = (uint8_t)
 	(lt_settings->link_settings.link_spread);
@@ -134,21 +140,21 @@ static void dpcd_set_link_settings(
 	lane_count_set.bits.LANE_COUNT_SET =
 	lt_settings->link_settings.lane_count;
 
-	lane_count_set.bits.ENHANCED_FRAMING = 1;
-
+	lane_count_set.bits.ENHANCED_FRAMING = lt_settings->enhanced_framing;
 	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED = 0;
 
-	hw_tr_pattern = get_supported_tp(link);
-	if (hw_tr_pattern != HW_DP_TRAINING_PATTERN_4) {
+	dp_tr_pattern = get_supported_tp(link);
+
+	if (dp_tr_pattern != DP_TRAINING_PATTERN_SEQUENCE_4) {
 		lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED =
 				link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED;
 	}
 
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
-	&downspread.raw, sizeof(downspread));
+		&downspread.raw, sizeof(downspread));
 
 	core_link_write_dpcd(link, DP_LANE_COUNT_SET,
-	&lane_count_set.raw, 1);
+		&lane_count_set.raw, 1);
 
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14 &&
 			lt_settings->link_settings.use_link_rate_set == true) {
@@ -162,46 +168,47 @@ static void dpcd_set_link_settings(
 	}
 
 	if (rate) {
-		DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
+		DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x framing = %x\n %x spread = %x\n",
 			__func__,
 			DP_LINK_BW_SET,
 			lt_settings->link_settings.link_rate,
 			DP_LANE_COUNT_SET,
 			lt_settings->link_settings.lane_count,
+			lt_settings->enhanced_framing,
 			DP_DOWNSPREAD_CTRL,
 			lt_settings->link_settings.link_spread);
 	} else {
-		DC_LOG_HW_LINK_TRAINING("%s\n %x rate set = %x\n %x lane = %x\n %x spread = %x\n",
+		DC_LOG_HW_LINK_TRAINING("%s\n %x rate set = %x\n %x lane = %x framing = %x\n %x spread = %x\n",
 			__func__,
 			DP_LINK_RATE_SET,
 			lt_settings->link_settings.link_rate_set,
 			DP_LANE_COUNT_SET,
 			lt_settings->link_settings.lane_count,
+			lt_settings->enhanced_framing,
 			DP_DOWNSPREAD_CTRL,
 			lt_settings->link_settings.link_spread);
 	}
-
 }
 
 static enum dpcd_training_patterns
-	hw_training_pattern_to_dpcd_training_pattern(
+	dc_dp_training_pattern_to_dpcd_training_pattern(
 	struct dc_link *link,
-	enum hw_dp_training_pattern pattern)
+	enum dc_dp_training_pattern pattern)
 {
 	enum dpcd_training_patterns dpcd_tr_pattern =
 	DPCD_TRAINING_PATTERN_VIDEOIDLE;
 
 	switch (pattern) {
-	case HW_DP_TRAINING_PATTERN_1:
+	case DP_TRAINING_PATTERN_SEQUENCE_1:
 		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_1;
 		break;
-	case HW_DP_TRAINING_PATTERN_2:
+	case DP_TRAINING_PATTERN_SEQUENCE_2:
 		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_2;
 		break;
-	case HW_DP_TRAINING_PATTERN_3:
+	case DP_TRAINING_PATTERN_SEQUENCE_3:
 		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_3;
 		break;
-	case HW_DP_TRAINING_PATTERN_4:
+	case DP_TRAINING_PATTERN_SEQUENCE_4:
 		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_4;
 		break;
 	default:
@@ -212,13 +219,12 @@ static enum dpcd_training_patterns
 	}
 
 	return dpcd_tr_pattern;
-
 }
 
 static void dpcd_set_lt_pattern_and_lane_settings(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings,
-	enum hw_dp_training_pattern pattern)
+	enum dc_dp_training_pattern pattern)
 {
 	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = { { {0} } };
 	const uint32_t dpcd_base_lt_offset =
@@ -233,7 +239,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	* DpcdAddress_TrainingPatternSet
 	*****************************************************************/
 	dpcd_pattern.v1_4.TRAINING_PATTERN_SET =
-		hw_training_pattern_to_dpcd_training_pattern(link, pattern);
+		dc_dp_training_pattern_to_dpcd_training_pattern(link, pattern);
 
 	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
 		= dpcd_pattern.raw;
@@ -346,12 +352,20 @@ static void update_drive_settings(
 {
 	uint32_t lane;
 	for (lane = 0; lane < src.link_settings.lane_count; lane++) {
-		dest->lane_settings[lane].VOLTAGE_SWING =
-			src.lane_settings[lane].VOLTAGE_SWING;
-		dest->lane_settings[lane].PRE_EMPHASIS =
-			src.lane_settings[lane].PRE_EMPHASIS;
-		dest->lane_settings[lane].POST_CURSOR2 =
-			src.lane_settings[lane].POST_CURSOR2;
+		if (dest->voltage_swing == NULL)
+			dest->lane_settings[lane].VOLTAGE_SWING = src.lane_settings[lane].VOLTAGE_SWING;
+		else
+			dest->lane_settings[lane].VOLTAGE_SWING = *dest->voltage_swing;
+
+		if (dest->pre_emphasis == NULL)
+			dest->lane_settings[lane].PRE_EMPHASIS = src.lane_settings[lane].PRE_EMPHASIS;
+		else
+			dest->lane_settings[lane].PRE_EMPHASIS = *dest->pre_emphasis;
+
+		if (dest->post_cursor2 == NULL)
+			dest->lane_settings[lane].POST_CURSOR2 = src.lane_settings[lane].POST_CURSOR2;
+		else
+			dest->lane_settings[lane].POST_CURSOR2 = *dest->post_cursor2;
 	}
 }
 
@@ -754,15 +768,15 @@ static enum link_training_result perform_channel_equalization_sequence(
 	struct link_training_settings *lt_settings)
 {
 	struct link_training_settings req_settings;
-	enum hw_dp_training_pattern hw_tr_pattern;
+	enum dc_dp_training_pattern tr_pattern;
 	uint32_t retries_ch_eq;
 	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
 	union lane_align_status_updated dpcd_lane_status_updated = { {0} };
 	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = { { {0} } };
 
-	hw_tr_pattern = get_supported_tp(link);
+	tr_pattern = lt_settings->pattern_for_eq;
 
-	dp_set_hw_training_pattern(link, hw_tr_pattern);
+	dp_set_hw_training_pattern(link, tr_pattern);
 
 	for (retries_ch_eq = 0; retries_ch_eq <= LINK_TRAINING_MAX_RETRY_COUNT;
 		retries_ch_eq++) {
@@ -776,12 +790,12 @@ static enum link_training_result perform_channel_equalization_sequence(
 			dpcd_set_lt_pattern_and_lane_settings(
 				link,
 				lt_settings,
-				hw_tr_pattern);
+				tr_pattern);
 		else
 			dpcd_set_lane_settings(link, lt_settings);
 
 		/* 3. wait for receiver to lock-on*/
-		wait_for_training_aux_rd_interval(link, 400);
+		wait_for_training_aux_rd_interval(link, lt_settings->eq_pattern_time);
 
 		/* 4. Read lane status and requested
 		 * drive settings as set by the sink*/
@@ -817,27 +831,16 @@ static enum link_training_result perform_clock_recovery_sequence(
 {
 	uint32_t retries_cr;
 	uint32_t retry_count;
-	uint32_t lane;
 	struct link_training_settings req_settings;
-	enum dc_lane_count lane_count =
-	lt_settings->link_settings.lane_count;
-	enum hw_dp_training_pattern hw_tr_pattern = HW_DP_TRAINING_PATTERN_1;
+	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
+	enum dc_dp_training_pattern tr_pattern = DP_TRAINING_PATTERN_SEQUENCE_1;
 	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX];
 	union lane_align_status_updated dpcd_lane_status_updated;
 
 	retries_cr = 0;
 	retry_count = 0;
-	/* initial drive setting (VS/PE/PC2)*/
-	for (lane = 0; lane < LANE_COUNT_DP_MAX; lane++) {
-		lt_settings->lane_settings[lane].VOLTAGE_SWING =
-		VOLTAGE_SWING_LEVEL0;
-		lt_settings->lane_settings[lane].PRE_EMPHASIS =
-		PRE_EMPHASIS_DISABLED;
-		lt_settings->lane_settings[lane].POST_CURSOR2 =
-		POST_CURSOR2_DISABLED;
-	}
 
-	dp_set_hw_training_pattern(link, hw_tr_pattern);
+	dp_set_hw_training_pattern(link, tr_pattern);
 
 	/* najeeb - The synaptics MST hub can put the LT in
 	* infinite loop by switching the VS
@@ -845,7 +848,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 	/* between level 0 and level 1 continuously, here
 	* we try for CR lock for LinkTrainingMaxCRRetry count*/
 	while ((retries_cr < LINK_TRAINING_MAX_RETRY_COUNT) &&
-	(retry_count < LINK_TRAINING_MAX_CR_RETRY)) {
+		(retry_count < LINK_TRAINING_MAX_CR_RETRY)) {
 
 		memset(&dpcd_lane_status, '\0', sizeof(dpcd_lane_status));
 		memset(&dpcd_lane_status_updated, '\0',
@@ -863,7 +866,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 			dpcd_set_lt_pattern_and_lane_settings(
 					link,
 					lt_settings,
-					hw_tr_pattern);
+					tr_pattern);
 		else
 			dpcd_set_lane_settings(
 					link,
@@ -872,7 +875,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 		/* 3. wait receiver to lock-on*/
 		wait_for_training_aux_rd_interval(
 				link,
-				100);
+				lt_settings->cr_pattern_time);
 
 		/* 4. Read lane status and requested drive
 		* settings as set by the sink
@@ -939,7 +942,7 @@ static inline enum link_training_result perform_link_training_int(
 	 * TPS4 must be used instead of POST_LT_ADJ_REQ.
 	 */
 	if (link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED != 1 ||
-			get_supported_tp(link) == HW_DP_TRAINING_PATTERN_4)
+			get_supported_tp(link) == DP_TRAINING_PATTERN_SEQUENCE_4)
 		return status;
 
 	if (status == LINK_TRAINING_SUCCESS &&
@@ -947,7 +950,7 @@ static inline enum link_training_result perform_link_training_int(
 		status = LINK_TRAINING_LQA_FAIL;
 
 	lane_count_set.bits.LANE_COUNT_SET = lt_settings->link_settings.lane_count;
-	lane_count_set.bits.ENHANCED_FRAMING = 1;
+	lane_count_set.bits.ENHANCED_FRAMING = lt_settings->enhanced_framing;
 	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED = 0;
 
 	core_link_write_dpcd(
@@ -959,24 +962,28 @@ static inline enum link_training_result perform_link_training_int(
 	return status;
 }
 
-enum link_training_result dc_link_dp_perform_link_training(
-	struct dc_link *link,
+static void initialize_training_settings(
+	 struct dc_link *link,
 	const struct dc_link_settings *link_setting,
-	bool skip_video_pattern)
+	struct link_training_settings *lt_settings)
 {
-	enum link_training_result status = LINK_TRAINING_SUCCESS;
+	uint32_t lane;
 
-	char *link_rate = "Unknown";
-	char *lt_result = "Unknown";
+	memset(lt_settings, '\0', sizeof(struct link_training_settings));
 
-	struct link_training_settings lt_settings;
+	/* Initialize link settings */
+	lt_settings->link_settings.use_link_rate_set = link_setting->use_link_rate_set;
+	lt_settings->link_settings.link_rate_set = link_setting->link_rate_set;
 
-	memset(&lt_settings, '\0', sizeof(lt_settings));
+	if (link->preferred_link_setting.link_rate != LINK_RATE_UNKNOWN)
+		lt_settings->link_settings.link_rate = link->preferred_link_setting.link_rate;
+	else
+		lt_settings->link_settings.link_rate = link_setting->link_rate;
 
-	lt_settings.link_settings.link_rate = link_setting->link_rate;
-	lt_settings.link_settings.lane_count = link_setting->lane_count;
-	lt_settings.link_settings.use_link_rate_set = link_setting->use_link_rate_set;
-	lt_settings.link_settings.link_rate_set = link_setting->link_rate_set;
+	if (link->preferred_link_setting.lane_count != LANE_COUNT_UNKNOWN)
+		lt_settings->link_settings.lane_count = link->preferred_link_setting.lane_count;
+	else
+		lt_settings->link_settings.lane_count = link_setting->lane_count;
 
 	/*@todo[vdevulap] move SS to LS, should not be handled by displaypath*/
 
@@ -987,31 +994,75 @@ enum link_training_result dc_link_dp_perform_link_training(
 	 * LINK_SPREAD_05_DOWNSPREAD_30KHZ :
 	 * LINK_SPREAD_DISABLED;
 	 */
+	/* Initialize link spread */
 	if (link->dp_ss_off)
-		lt_settings.link_settings.link_spread = LINK_SPREAD_DISABLED;
+		lt_settings->link_settings.link_spread = LINK_SPREAD_DISABLED;
+	else if (link->preferred_training_settings.downspread != NULL)
+		lt_settings->link_settings.link_spread
+			= *link->preferred_training_settings.downspread
+			? LINK_SPREAD_05_DOWNSPREAD_30KHZ
+			: LINK_SPREAD_DISABLED;
 	else
-		lt_settings.link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
+		lt_settings->link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
 
-	/* 1. set link rate, lane count and spread*/
-	dpcd_set_link_settings(link, &lt_settings);
+	/* Initialize lane settings overrides */
+	if (link->preferred_training_settings.voltage_swing != NULL)
+		lt_settings->voltage_swing = link->preferred_training_settings.voltage_swing;
 
-	/* 2. perform link training (set link training done
-	 *  to false is done as well)*/
-	status = perform_clock_recovery_sequence(link, &lt_settings);
-	if (status == LINK_TRAINING_SUCCESS) {
-		status = perform_channel_equalization_sequence(link,
-				&lt_settings);
-	}
+	if (link->preferred_training_settings.pre_emphasis != NULL)
+		lt_settings->pre_emphasis = link->preferred_training_settings.pre_emphasis;
 
-	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
-		status = perform_link_training_int(link,
-				&lt_settings,
-				status);
+	if (link->preferred_training_settings.post_cursor2 != NULL)
+		lt_settings->post_cursor2 = link->preferred_training_settings.post_cursor2;
+
+	/* Initialize lane settings (VS/PE/PC2) */
+	for (lane = 0; lane < LANE_COUNT_DP_MAX; lane++) {
+		lt_settings->lane_settings[lane].VOLTAGE_SWING =
+			lt_settings->voltage_swing != NULL ?
+			*lt_settings->voltage_swing :
+			VOLTAGE_SWING_LEVEL0;
+		lt_settings->lane_settings[lane].PRE_EMPHASIS =
+			lt_settings->pre_emphasis != NULL ?
+			*lt_settings->pre_emphasis
+			: PRE_EMPHASIS_DISABLED;
+		lt_settings->lane_settings[lane].POST_CURSOR2 =
+			lt_settings->post_cursor2 != NULL ?
+			*lt_settings->post_cursor2
+			: POST_CURSOR2_DISABLED;
 	}
 
-	/* 6. print status message*/
-	switch (lt_settings.link_settings.link_rate) {
+	/* Initialize training timings */
+	if (link->preferred_training_settings.cr_pattern_time != NULL)
+		lt_settings->cr_pattern_time = *link->preferred_training_settings.cr_pattern_time;
+	else
+		lt_settings->cr_pattern_time = get_training_aux_rd_interval(link, 100);
+
+	if (link->preferred_training_settings.eq_pattern_time != NULL)
+		lt_settings->eq_pattern_time = *link->preferred_training_settings.eq_pattern_time;
+	else
+		lt_settings->eq_pattern_time = get_training_aux_rd_interval(link, 400);
+
+	if (link->preferred_training_settings.pattern_for_eq != NULL)
+		lt_settings->pattern_for_eq = *link->preferred_training_settings.pattern_for_eq;
+	else
+		lt_settings->pattern_for_eq = get_supported_tp(link);
+
+	if (link->preferred_training_settings.enhanced_framing != NULL)
+		lt_settings->enhanced_framing = *link->preferred_training_settings.enhanced_framing;
+	else
+		lt_settings->enhanced_framing = 1;
+}
+
+static void print_status_message(
+	struct dc_link *link,
+	const struct link_training_settings *lt_settings,
+	enum link_training_result status)
+{
+	char *link_rate = "Unknown";
+	char *lt_result = "Unknown";
+	char *lt_spread = "Disabled";
 
+	switch (lt_settings->link_settings.link_rate) {
 	case LINK_RATE_LOW:
 		link_rate = "RBR";
 		break;
@@ -1057,13 +1108,102 @@ enum link_training_result dc_link_dp_perform_link_training(
 		break;
 	}
 
+	switch (lt_settings->link_settings.link_spread) {
+	case LINK_SPREAD_DISABLED:
+		lt_spread = "Disabled";
+		break;
+	case LINK_SPREAD_05_DOWNSPREAD_30KHZ:
+		lt_spread = "0.5% 30KHz";
+		break;
+	case LINK_SPREAD_05_DOWNSPREAD_33KHZ:
+		lt_spread = "0.5% 33KHz";
+		break;
+	default:
+		break;
+	}
+
 	/* Connectivity log: link training */
-	CONN_MSG_LT(link, "%sx%d %s VS=%d, PE=%d",
-			link_rate,
-			lt_settings.link_settings.lane_count,
-			lt_result,
-			lt_settings.lane_settings[0].VOLTAGE_SWING,
-			lt_settings.lane_settings[0].PRE_EMPHASIS);
+	CONN_MSG_LT(link, "%sx%d %s VS=%d, PE=%d, DS=%s",
+				link_rate,
+				lt_settings->link_settings.lane_count,
+				lt_result,
+				lt_settings->lane_settings[0].VOLTAGE_SWING,
+				lt_settings->lane_settings[0].PRE_EMPHASIS,
+				lt_spread);
+}
+
+bool dc_link_dp_perform_link_training_skip_aux(
+	struct dc_link *link,
+	const struct dc_link_settings *link_setting)
+{
+	struct link_training_settings lt_settings;
+	enum dc_dp_training_pattern pattern_for_cr = DP_TRAINING_PATTERN_SEQUENCE_1;
+
+	initialize_training_settings(link, link_setting, &lt_settings);
+
+	/* 1. Perform_clock_recovery_sequence. */
+
+	/* transmit training pattern for clock recovery */
+	dp_set_hw_training_pattern(link, pattern_for_cr);
+
+	/* call HWSS to set lane settings*/
+	dp_set_hw_lane_settings(link, &lt_settings);
+
+	/* wait receiver to lock-on*/
+	wait_for_training_aux_rd_interval(link, lt_settings.cr_pattern_time);
+
+	/* 2. Perform_channel_equalization_sequence. */
+
+	/* transmit training pattern for channel equalization. */
+	dp_set_hw_training_pattern(link, lt_settings.pattern_for_eq);
+
+	/* call HWSS to set lane settings*/
+	dp_set_hw_lane_settings(link, &lt_settings);
+
+	/* wait receiver to lock-on. */
+	wait_for_training_aux_rd_interval(link, lt_settings.eq_pattern_time);
+
+	/* 3. Perform_link_training_int. */
+
+	/* Mainlink output idle pattern. */
+	dp_set_hw_test_pattern(link, DP_TEST_PATTERN_VIDEO_MODE, NULL, 0);
+
+	print_status_message(link, &lt_settings, LINK_TRAINING_SUCCESS);
+
+	return true;
+}
+
+enum link_training_result dc_link_dp_perform_link_training(
+	struct dc_link *link,
+	const struct dc_link_settings *link_setting,
+	bool skip_video_pattern)
+{
+	enum link_training_result status = LINK_TRAINING_SUCCESS;
+
+	struct link_training_settings lt_settings;
+
+	initialize_training_settings(link, link_setting, &lt_settings);
+
+	/* 1. set link rate, lane count and spread. */
+	dpcd_set_link_settings(link, &lt_settings);
+
+	/* 2. perform link training (set link training done
+	 *  to false is done as well)
+	 */
+	status = perform_clock_recovery_sequence(link, &lt_settings);
+	if (status == LINK_TRAINING_SUCCESS) {
+		status = perform_channel_equalization_sequence(link,
+				&lt_settings);
+	}
+
+	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
+		status = perform_link_training_int(link,
+				&lt_settings,
+				status);
+	}
+
+	/* 6. print status message*/
+	print_status_message(link, &lt_settings, status);
 
 	if (status != LINK_TRAINING_SUCCESS)
 		link->ctx->dc->debug_data.ltFailCount++;
@@ -1071,7 +1211,6 @@ enum link_training_result dc_link_dp_perform_link_training(
 	return status;
 }
 
-
 bool perform_link_training_with_retries(
 	struct dc_link *link,
 	const struct dc_link_settings *link_setting,

commit 39a4eb853f9ac85e9b042874ef5fa12c8e20e440
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Thu May 16 13:01:51 2019 -0400

    drm/amd/display: update DSC MST DP virtual DPCD peer device enumeration policy
    
    [why]
    Current policy assumes virtual DPCD peer device as
    an individual MST branch device with 1 input and 1 output.
    However this is only true for virtual DP-to-DP peer device.
    In general there are three types of virtual DP peer devices.
    1. Sink peer device with virtual DPCD.
    2. Virtual DP-to-DP Peer device with virtual DPCD.
    3. Virtual DP-to-HDMI Protocol Converter Peer Device with
    Virtual DPCD.
    So we should break the assumption and handle all three types.
    
    [how]
    DP-to-DP peer device will have virtual DPCD cap upstream.
    Sink peer device will have virtual DPCD on the logical port.
    Dp to HDMI protocol converter peer device will have virtual DPCD
    on its converter port.
    For DSC capable Synaptics non VGA port we workaround by enumerating
    a virutal DPCD peer device on its upstream
    even if it doesn't have one.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 017f88c9f2e4..056be4c34a98 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2382,10 +2382,6 @@ static bool retrieve_link_cap(struct dc_link *link)
 	uint32_t read_dpcd_retry_cnt = 3;
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
-#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
-	uint8_t dsc_data[16]; /* DP_DSC_BITS_PER_PIXEL_INC - DP_DSC_SUPPORT + 1 == 16 */
-	struct dsc_dec_dpcd_caps *dsc_dec_caps;
-#endif
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
@@ -2558,93 +2554,26 @@ static bool retrieve_link_cap(struct dc_link *link)
 		sizeof(dp_hw_fw_revision.ieee_fw_rev));
 
 #ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
-	dsc_dec_caps = &link->dpcd_caps.dsc_sink_caps;
-	memset(dsc_dec_caps, '\0', sizeof(*dsc_dec_caps));
-	memset(&link->dpcd_caps.dsc_sink_caps, '\0',
-			sizeof(link->dpcd_caps.dsc_sink_caps));
+	memset(&link->dpcd_caps.dsc_caps, '\0',
+			sizeof(link->dpcd_caps.dsc_caps));
 	memset(&link->dpcd_caps.fec_cap, '\0', sizeof(link->dpcd_caps.fec_cap));
 	/* Read DSC and FEC sink capabilities if DP revision is 1.4 and up */
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14) {
-		status = core_link_read_dpcd(
-				link,
-				DP_DSC_SUPPORT,
-				dsc_data,
-				sizeof(dsc_data));
-		if (status == DC_OK) {
-			DC_LOG_DSC("DSC DPCD capability read at link %d:",
-					link->link_index);
-			DC_LOG_DSC("\t%02x %02x %02x %02x",
-					dsc_data[0], dsc_data[1],
-					dsc_data[2], dsc_data[3]);
-			DC_LOG_DSC("\t%02x %02x %02x %02x",
-					dsc_data[4], dsc_data[5],
-					dsc_data[6], dsc_data[7]);
-			DC_LOG_DSC("\t%02x %02x %02x %02x",
-					dsc_data[8], dsc_data[9],
-					dsc_data[10], dsc_data[11]);
-			DC_LOG_DSC("\t%02x %02x %02x %02x",
-					dsc_data[12], dsc_data[13],
-					dsc_data[14], dsc_data[15]);
-		} else {
-			dm_error("%s: Read DSC dpcd data failed.\n", __func__);
-			return false;
-		}
-
-		if (dc_dsc_parse_dsc_dpcd(dsc_data, NULL,
-				dsc_dec_caps)) {
-			DC_LOG_DSC("DSC DPCD capabilities parsed at link %d:",
-					link->link_index);
-			DC_LOG_DSC("\tis_dsc_supported:\t%d",
-					dsc_dec_caps->is_dsc_supported);
-			DC_LOG_DSC("\tdsc_version:\t%d", dsc_dec_caps->dsc_version);
-			DC_LOG_DSC("\trc_buffer_size:\t%d",
-					dsc_dec_caps->rc_buffer_size);
-			DC_LOG_DSC("\tslice_caps1:\t0x%x20",
-					dsc_dec_caps->slice_caps1.raw);
-			DC_LOG_DSC("\tslice_caps2:\t0x%x20",
-					dsc_dec_caps->slice_caps2.raw);
-			DC_LOG_DSC("\tlb_bit_depth:\t%d",
-					dsc_dec_caps->lb_bit_depth);
-			DC_LOG_DSC("\tis_block_pred_supported:\t%d",
-					dsc_dec_caps->is_block_pred_supported);
-			DC_LOG_DSC("\tedp_max_bits_per_pixel:\t%d",
-					dsc_dec_caps->edp_max_bits_per_pixel);
-			DC_LOG_DSC("\tcolor_formats:\t%d",
-					dsc_dec_caps->color_formats.raw);
-			DC_LOG_DSC("\tcolor_depth:\t%d",
-					dsc_dec_caps->color_depth.raw);
-			DC_LOG_DSC("\tthroughput_mode_0_mps:\t%d",
-					dsc_dec_caps->throughput_mode_0_mps);
-			DC_LOG_DSC("\tthroughput_mode_1_mps:\t%d",
-					dsc_dec_caps->throughput_mode_1_mps);
-			DC_LOG_DSC("\tmax_slice_width:\t%d",
-					dsc_dec_caps->max_slice_width);
-			DC_LOG_DSC("\tbpp_increment_div:\t%d",
-					dsc_dec_caps->bpp_increment_div);
-			DC_LOG_DSC("\tbranch_overall_throughput_0_mps:\t%d",
-					dsc_dec_caps->branch_overall_throughput_0_mps);
-			DC_LOG_DSC("\tbranch_overall_throughput_1_mps:\t%d",
-					dsc_dec_caps->branch_overall_throughput_1_mps);
-			DC_LOG_DSC("\tbranch_max_line_width:\t%d",
-					dsc_dec_caps->branch_max_line_width);
-		} else {
-			/* Some sinks return bogus DSC DPCD data
-			 * when they don't support DSC.
-			 */
-			dm_error("%s: DSC DPCD data doesn't make sense. "
-					"DSC will be disabled.\n", __func__);
-			memset(&link->dpcd_caps.dsc_sink_caps, '\0',
-					sizeof(link->dpcd_caps.dsc_sink_caps));
-		}
-
 		status = core_link_read_dpcd(
 				link,
 				DP_FEC_CAPABILITY,
 				&link->dpcd_caps.fec_cap.raw,
 				sizeof(link->dpcd_caps.fec_cap.raw));
-		if (status != DC_OK)
-			dm_error("%s: Read FEC dpcd register failed.\n",
-					__func__);
+		status = core_link_read_dpcd(
+				link,
+				DP_DSC_SUPPORT,
+				link->dpcd_caps.dsc_caps.dsc_basic_caps.raw,
+				sizeof(link->dpcd_caps.dsc_caps.dsc_basic_caps.raw));
+		status = core_link_read_dpcd(
+				link,
+				DP_DSC_BRANCH_OVERALL_THROUGHPUT_0,
+				link->dpcd_caps.dsc_caps.dsc_ext_caps.raw,
+				sizeof(link->dpcd_caps.dsc_caps.dsc_ext_caps.raw));
 	}
 #endif
 

commit f446489adcbc9c4833ae724a985166731c577bcd
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Wed Apr 17 19:07:08 2019 -0400

    drm/amd/display: Add support for extended DSC DPCD caps
    
    [why]
    A few of the new DSC DPCD caps were introduced by a DP 1.4a SCR in order
    to give DSC branch decoders a chance to expose their maximum throughput
    and maximum line width limitations.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index effc36745671..017f88c9f2e4 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2383,8 +2383,8 @@ static bool retrieve_link_cap(struct dc_link *link)
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 #ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
-	uint8_t dsc_data[16];
-	struct dsc_dec_dpcd_caps *dsc_caps;
+	uint8_t dsc_data[16]; /* DP_DSC_BITS_PER_PIXEL_INC - DP_DSC_SUPPORT + 1 == 16 */
+	struct dsc_dec_dpcd_caps *dsc_dec_caps;
 #endif
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
@@ -2558,8 +2558,8 @@ static bool retrieve_link_cap(struct dc_link *link)
 		sizeof(dp_hw_fw_revision.ieee_fw_rev));
 
 #ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
-	dsc_caps = &link->dpcd_caps.dsc_sink_caps;
-	memset(dsc_caps, '\0', sizeof(*dsc_caps));
+	dsc_dec_caps = &link->dpcd_caps.dsc_sink_caps;
+	memset(dsc_dec_caps, '\0', sizeof(*dsc_dec_caps));
 	memset(&link->dpcd_caps.dsc_sink_caps, '\0',
 			sizeof(link->dpcd_caps.dsc_sink_caps));
 	memset(&link->dpcd_caps.fec_cap, '\0', sizeof(link->dpcd_caps.fec_cap));
@@ -2571,7 +2571,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 				dsc_data,
 				sizeof(dsc_data));
 		if (status == DC_OK) {
-			DC_LOG_DSC("DSC capability read at link %d:",
+			DC_LOG_DSC("DSC DPCD capability read at link %d:",
 					link->link_index);
 			DC_LOG_DSC("\t%02x %02x %02x %02x",
 					dsc_data[0], dsc_data[1],
@@ -2590,37 +2590,43 @@ static bool retrieve_link_cap(struct dc_link *link)
 			return false;
 		}
 
-		if (dc_dsc_parse_dsc_dpcd(dsc_data,
-				dsc_caps)) {
-			DC_LOG_DSC("DSC capability parsed at link %d:",
+		if (dc_dsc_parse_dsc_dpcd(dsc_data, NULL,
+				dsc_dec_caps)) {
+			DC_LOG_DSC("DSC DPCD capabilities parsed at link %d:",
 					link->link_index);
 			DC_LOG_DSC("\tis_dsc_supported:\t%d",
-					dsc_caps->is_dsc_supported);
-			DC_LOG_DSC("\tdsc_version:\t%d", dsc_caps->dsc_version);
+					dsc_dec_caps->is_dsc_supported);
+			DC_LOG_DSC("\tdsc_version:\t%d", dsc_dec_caps->dsc_version);
 			DC_LOG_DSC("\trc_buffer_size:\t%d",
-					dsc_caps->rc_buffer_size);
+					dsc_dec_caps->rc_buffer_size);
 			DC_LOG_DSC("\tslice_caps1:\t0x%x20",
-					dsc_caps->slice_caps1.raw);
+					dsc_dec_caps->slice_caps1.raw);
 			DC_LOG_DSC("\tslice_caps2:\t0x%x20",
-					dsc_caps->slice_caps2.raw);
+					dsc_dec_caps->slice_caps2.raw);
 			DC_LOG_DSC("\tlb_bit_depth:\t%d",
-					dsc_caps->lb_bit_depth);
+					dsc_dec_caps->lb_bit_depth);
 			DC_LOG_DSC("\tis_block_pred_supported:\t%d",
-					dsc_caps->is_block_pred_supported);
+					dsc_dec_caps->is_block_pred_supported);
 			DC_LOG_DSC("\tedp_max_bits_per_pixel:\t%d",
-					dsc_caps->edp_max_bits_per_pixel);
+					dsc_dec_caps->edp_max_bits_per_pixel);
 			DC_LOG_DSC("\tcolor_formats:\t%d",
-					dsc_caps->color_formats.raw);
+					dsc_dec_caps->color_formats.raw);
 			DC_LOG_DSC("\tcolor_depth:\t%d",
-					dsc_caps->color_depth.raw);
+					dsc_dec_caps->color_depth.raw);
 			DC_LOG_DSC("\tthroughput_mode_0_mps:\t%d",
-					dsc_caps->throughput_mode_0_mps);
+					dsc_dec_caps->throughput_mode_0_mps);
 			DC_LOG_DSC("\tthroughput_mode_1_mps:\t%d",
-					dsc_caps->throughput_mode_1_mps);
+					dsc_dec_caps->throughput_mode_1_mps);
 			DC_LOG_DSC("\tmax_slice_width:\t%d",
-					dsc_caps->max_slice_width);
+					dsc_dec_caps->max_slice_width);
 			DC_LOG_DSC("\tbpp_increment_div:\t%d",
-					dsc_caps->bpp_increment_div);
+					dsc_dec_caps->bpp_increment_div);
+			DC_LOG_DSC("\tbranch_overall_throughput_0_mps:\t%d",
+					dsc_dec_caps->branch_overall_throughput_0_mps);
+			DC_LOG_DSC("\tbranch_overall_throughput_1_mps:\t%d",
+					dsc_dec_caps->branch_overall_throughput_1_mps);
+			DC_LOG_DSC("\tbranch_max_line_width:\t%d",
+					dsc_dec_caps->branch_max_line_width);
 		} else {
 			/* Some sinks return bogus DSC DPCD data
 			 * when they don't support DSC.

commit 97bda0322b8a91aa8d534763e709571b2334e585
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Mon Feb 25 13:26:34 2019 -0500

    drm/amd/display: Add DSC support for Navi (v2)
    
    Add support for DCN2 DSC (Display Stream Compression)
    
    HW Blocks:
    
     +--------++------+       +----------+
     | HUBBUB || HUBP |  <--  | MMHUBBUB |
     +--------++------+       +----------+
            |                     ^
            v                     |
        +--------+            +--------+
        |  DPP   |            |  DWB   |
        +--------+            +--------+
            |
            v                      ^
        +--------+                 |
        |  MPC   |                 |
        +--------+                 |
            |                      |
            v                      |
        +-------+      +-------+   |
        |  OPP  | <--> |  DSC  |   |
        +-------+      +-------+   |
            |                      |
            v                      |
        +--------+                /
        |  OPTC  |  --------------
        +--------+
            |
            v
        +--------+       +--------+
        |  DIO   |       |  DCCG  |
        +--------+       +--------+
    
    v2: rebase (Alex)
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4d0c2bb32dc5..effc36745671 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4,6 +4,9 @@
 #include "dc_link_dp.h"
 #include "dm_helpers.h"
 #include "opp.h"
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+#include "dsc.h"
+#endif
 #if defined(CONFIG_DRM_AMD_DC_DCN2_0)
 #include "resource.h"
 #endif
@@ -2379,6 +2382,10 @@ static bool retrieve_link_cap(struct dc_link *link)
 	uint32_t read_dpcd_retry_cnt = 3;
 	int i;
 	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	uint8_t dsc_data[16];
+	struct dsc_dec_dpcd_caps *dsc_caps;
+#endif
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
@@ -2550,6 +2557,90 @@ static bool retrieve_link_cap(struct dc_link *link)
 		dp_hw_fw_revision.ieee_fw_rev,
 		sizeof(dp_hw_fw_revision.ieee_fw_rev));
 
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+	dsc_caps = &link->dpcd_caps.dsc_sink_caps;
+	memset(dsc_caps, '\0', sizeof(*dsc_caps));
+	memset(&link->dpcd_caps.dsc_sink_caps, '\0',
+			sizeof(link->dpcd_caps.dsc_sink_caps));
+	memset(&link->dpcd_caps.fec_cap, '\0', sizeof(link->dpcd_caps.fec_cap));
+	/* Read DSC and FEC sink capabilities if DP revision is 1.4 and up */
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14) {
+		status = core_link_read_dpcd(
+				link,
+				DP_DSC_SUPPORT,
+				dsc_data,
+				sizeof(dsc_data));
+		if (status == DC_OK) {
+			DC_LOG_DSC("DSC capability read at link %d:",
+					link->link_index);
+			DC_LOG_DSC("\t%02x %02x %02x %02x",
+					dsc_data[0], dsc_data[1],
+					dsc_data[2], dsc_data[3]);
+			DC_LOG_DSC("\t%02x %02x %02x %02x",
+					dsc_data[4], dsc_data[5],
+					dsc_data[6], dsc_data[7]);
+			DC_LOG_DSC("\t%02x %02x %02x %02x",
+					dsc_data[8], dsc_data[9],
+					dsc_data[10], dsc_data[11]);
+			DC_LOG_DSC("\t%02x %02x %02x %02x",
+					dsc_data[12], dsc_data[13],
+					dsc_data[14], dsc_data[15]);
+		} else {
+			dm_error("%s: Read DSC dpcd data failed.\n", __func__);
+			return false;
+		}
+
+		if (dc_dsc_parse_dsc_dpcd(dsc_data,
+				dsc_caps)) {
+			DC_LOG_DSC("DSC capability parsed at link %d:",
+					link->link_index);
+			DC_LOG_DSC("\tis_dsc_supported:\t%d",
+					dsc_caps->is_dsc_supported);
+			DC_LOG_DSC("\tdsc_version:\t%d", dsc_caps->dsc_version);
+			DC_LOG_DSC("\trc_buffer_size:\t%d",
+					dsc_caps->rc_buffer_size);
+			DC_LOG_DSC("\tslice_caps1:\t0x%x20",
+					dsc_caps->slice_caps1.raw);
+			DC_LOG_DSC("\tslice_caps2:\t0x%x20",
+					dsc_caps->slice_caps2.raw);
+			DC_LOG_DSC("\tlb_bit_depth:\t%d",
+					dsc_caps->lb_bit_depth);
+			DC_LOG_DSC("\tis_block_pred_supported:\t%d",
+					dsc_caps->is_block_pred_supported);
+			DC_LOG_DSC("\tedp_max_bits_per_pixel:\t%d",
+					dsc_caps->edp_max_bits_per_pixel);
+			DC_LOG_DSC("\tcolor_formats:\t%d",
+					dsc_caps->color_formats.raw);
+			DC_LOG_DSC("\tcolor_depth:\t%d",
+					dsc_caps->color_depth.raw);
+			DC_LOG_DSC("\tthroughput_mode_0_mps:\t%d",
+					dsc_caps->throughput_mode_0_mps);
+			DC_LOG_DSC("\tthroughput_mode_1_mps:\t%d",
+					dsc_caps->throughput_mode_1_mps);
+			DC_LOG_DSC("\tmax_slice_width:\t%d",
+					dsc_caps->max_slice_width);
+			DC_LOG_DSC("\tbpp_increment_div:\t%d",
+					dsc_caps->bpp_increment_div);
+		} else {
+			/* Some sinks return bogus DSC DPCD data
+			 * when they don't support DSC.
+			 */
+			dm_error("%s: DSC DPCD data doesn't make sense. "
+					"DSC will be disabled.\n", __func__);
+			memset(&link->dpcd_caps.dsc_sink_caps, '\0',
+					sizeof(link->dpcd_caps.dsc_sink_caps));
+		}
+
+		status = core_link_read_dpcd(
+				link,
+				DP_FEC_CAPABILITY,
+				&link->dpcd_caps.fec_cap.raw,
+				sizeof(link->dpcd_caps.fec_cap.raw));
+		if (status != DC_OK)
+			dm_error("%s: Read FEC dpcd register failed.\n",
+					__func__);
+	}
+#endif
 
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
@@ -2964,4 +3055,66 @@ void dp_enable_mst_on_sink(struct dc_link *link, bool enable)
 	core_link_write_dpcd(link, DP_MSTM_CTRL, &mstmCntl, 1);
 }
 
+#ifdef CONFIG_DRM_AMD_DC_DSC_SUPPORT
+void dp_set_fec_ready(struct dc_link *link, bool ready)
+{
+	/* FEC has to be "set ready" before the link training.
+	 * The policy is to always train with FEC
+	 * if the sink supports it and leave it enabled on link.
+	 * If FEC is not supported, disable it.
+	 */
+	struct link_encoder *link_enc = link->link_enc;
+	uint8_t fec_config = 0;
+
+	if (link->dc->debug.disable_fec ||
+			IS_FPGA_MAXIMUS_DC(link->ctx->dce_environment))
+		return;
+
+	if (link_enc->funcs->fec_set_ready &&
+			link->dpcd_caps.fec_cap.bits.FEC_CAPABLE) {
+		if (link->fec_state == dc_link_fec_not_ready && ready) {
+			fec_config = 1;
+			if (core_link_write_dpcd(link,
+					DP_FEC_CONFIGURATION,
+					&fec_config,
+					sizeof(fec_config)) == DC_OK) {
+				link_enc->funcs->fec_set_ready(link_enc, true);
+				link->fec_state = dc_link_fec_ready;
+			} else {
+				dm_error("dpcd write failed to set fec_ready");
+			}
+		} else if (link->fec_state == dc_link_fec_ready && !ready) {
+			fec_config = 0;
+			core_link_write_dpcd(link,
+					DP_FEC_CONFIGURATION,
+					&fec_config,
+					sizeof(fec_config));
+			link->link_enc->funcs->fec_set_ready(
+					link->link_enc, false);
+			link->fec_state = dc_link_fec_not_ready;
+		}
+	}
+}
+
+void dp_set_fec_enable(struct dc_link *link, bool enable)
+{
+	struct link_encoder *link_enc = link->link_enc;
+
+	if (link->dc->debug.disable_fec ||
+			IS_FPGA_MAXIMUS_DC(link->ctx->dce_environment))
+		return;
+
+	if (link_enc->funcs->fec_set_enable &&
+			link->dpcd_caps.fec_cap.bits.FEC_CAPABLE) {
+		if (link->fec_state == dc_link_fec_ready && enable) {
+			msleep(1);
+			link_enc->funcs->fec_set_enable(link_enc, true);
+			link->fec_state = dc_link_fec_enabled;
+		} else if (link->fec_state == dc_link_fec_enabled && !enable) {
+			link_enc->funcs->fec_set_enable(link_enc, false);
+			link->fec_state = dc_link_fec_ready;
+		}
+	}
+}
+#endif
 

commit 6fbefb84a98ecc43cb4035c44fe417e6751ddd83
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Fri Feb 22 16:52:34 2019 -0500

    drm/amd/display: Add DC core changes for DCN2
    
    Core DC changes for DCN2.
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a1187274dbed..4d0c2bb32dc5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -4,6 +4,9 @@
 #include "dc_link_dp.h"
 #include "dm_helpers.h"
 #include "opp.h"
+#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
+#include "resource.h"
+#endif
 
 #include "inc/core_types.h"
 #include "link_hwss.h"
@@ -2547,6 +2550,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 		dp_hw_fw_revision.ieee_fw_rev,
 		sizeof(dp_hw_fw_revision.ieee_fw_rev));
 
+
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 
@@ -2674,6 +2678,14 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		stream->timing.display_color_depth;
 	struct bit_depth_reduction_params params;
 	struct output_pixel_processor *opp = pipe_ctx->stream_res.opp;
+#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
+	int width = pipe_ctx->stream->timing.h_addressable +
+		pipe_ctx->stream->timing.h_border_left +
+		pipe_ctx->stream->timing.h_border_right;
+	int height = pipe_ctx->stream->timing.v_addressable +
+		pipe_ctx->stream->timing.v_border_bottom +
+		pipe_ctx->stream->timing.v_border_top;
+#endif
 
 	memset(&params, 0, sizeof(params));
 
@@ -2717,6 +2729,30 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				controller_test_pattern, color_depth);
+#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
+		else if (opp->funcs->opp_set_disp_pattern_generator) {
+			struct pipe_ctx *bot_odm_pipe = dc_res_get_odm_bottom_pipe(pipe_ctx);
+
+			if (bot_odm_pipe) {
+				struct output_pixel_processor *bot_opp = bot_odm_pipe->stream_res.opp;
+
+				bot_opp->funcs->opp_program_bit_depth_reduction(bot_opp, &params);
+				width /= 2;
+				bot_opp->funcs->opp_set_disp_pattern_generator(bot_opp,
+					controller_test_pattern,
+					color_depth,
+					NULL,
+					width,
+					height);
+			}
+			opp->funcs->opp_set_disp_pattern_generator(opp,
+				controller_test_pattern,
+				color_depth,
+				NULL,
+				width,
+				height);
+		}
+#endif
 	}
 	break;
 	case DP_TEST_PATTERN_VIDEO_MODE:
@@ -2729,6 +2765,30 @@ static void set_crtc_test_pattern(struct dc_link *link,
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 				color_depth);
+#if defined(CONFIG_DRM_AMD_DC_DCN2_0)
+		else if (opp->funcs->opp_set_disp_pattern_generator) {
+			struct pipe_ctx *bot_odm_pipe = dc_res_get_odm_bottom_pipe(pipe_ctx);
+
+			if (bot_odm_pipe) {
+				struct output_pixel_processor *bot_opp = bot_odm_pipe->stream_res.opp;
+
+				bot_opp->funcs->opp_program_bit_depth_reduction(bot_opp, &params);
+				width /= 2;
+				bot_opp->funcs->opp_set_disp_pattern_generator(bot_opp,
+					CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
+					color_depth,
+					NULL,
+					width,
+					height);
+			}
+			opp->funcs->opp_set_disp_pattern_generator(opp,
+				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
+				color_depth,
+				NULL,
+				width,
+				height);
+		}
+#endif
 	}
 	break;
 
@@ -2903,3 +2963,5 @@ void dp_enable_mst_on_sink(struct dc_link *link, bool enable)
 
 	core_link_write_dpcd(link, DP_MSTM_CTRL, &mstmCntl, 1);
 }
+
+

commit 16b6253a083707734bf05b4377095e9c9f2102cc
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Mon May 27 11:38:26 2019 -0400

    drm/amd/display: Do not grant POST_LT_ADJ when TPS4 is used
    
    [Description]
    
    The spec does not allow POST_LT_ADJ_GRANTED to be set when TPS4 is used.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2d519e5fc3ea..a1187274dbed 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -89,6 +89,29 @@ static void dpcd_set_training_pattern(
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 }
 
+static enum hw_dp_training_pattern get_supported_tp(struct dc_link *link)
+{
+	enum hw_dp_training_pattern highest_tp = HW_DP_TRAINING_PATTERN_2;
+	struct encoder_feature_support *features = &link->link_enc->features;
+	struct dpcd_caps *dpcd_caps = &link->dpcd_caps;
+
+	if (features->flags.bits.IS_TPS3_CAPABLE)
+		highest_tp = HW_DP_TRAINING_PATTERN_3;
+
+	if (features->flags.bits.IS_TPS4_CAPABLE)
+		highest_tp = HW_DP_TRAINING_PATTERN_4;
+
+	if (dpcd_caps->max_down_spread.bits.TPS4_SUPPORTED &&
+		highest_tp >= HW_DP_TRAINING_PATTERN_4)
+		return HW_DP_TRAINING_PATTERN_4;
+
+	if (dpcd_caps->max_ln_count.bits.TPS3_SUPPORTED &&
+		highest_tp >= HW_DP_TRAINING_PATTERN_3)
+		return HW_DP_TRAINING_PATTERN_3;
+
+	return HW_DP_TRAINING_PATTERN_2;
+}
+
 static void dpcd_set_link_settings(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings)
@@ -97,6 +120,7 @@ static void dpcd_set_link_settings(
 
 	union down_spread_ctrl downspread = { {0} };
 	union lane_count_set lane_count_set = { {0} };
+	enum hw_dp_training_pattern hw_tr_pattern;
 
 	downspread.raw = (uint8_t)
 	(lt_settings->link_settings.link_spread);
@@ -106,8 +130,13 @@ static void dpcd_set_link_settings(
 
 	lane_count_set.bits.ENHANCED_FRAMING = 1;
 
-	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED =
-		link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED;
+	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED = 0;
+
+	hw_tr_pattern = get_supported_tp(link);
+	if (hw_tr_pattern != HW_DP_TRAINING_PATTERN_4) {
+		lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED =
+				link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED;
+	}
 
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
@@ -698,29 +727,6 @@ static bool perform_post_lt_adj_req_sequence(
 
 }
 
-static enum hw_dp_training_pattern get_supported_tp(struct dc_link *link)
-{
-	enum hw_dp_training_pattern highest_tp = HW_DP_TRAINING_PATTERN_2;
-	struct encoder_feature_support *features = &link->link_enc->features;
-	struct dpcd_caps *dpcd_caps = &link->dpcd_caps;
-
-	if (features->flags.bits.IS_TPS3_CAPABLE)
-		highest_tp = HW_DP_TRAINING_PATTERN_3;
-
-	if (features->flags.bits.IS_TPS4_CAPABLE)
-		highest_tp = HW_DP_TRAINING_PATTERN_4;
-
-	if (dpcd_caps->max_down_spread.bits.TPS4_SUPPORTED &&
-		highest_tp >= HW_DP_TRAINING_PATTERN_4)
-		return HW_DP_TRAINING_PATTERN_4;
-
-	if (dpcd_caps->max_ln_count.bits.TPS3_SUPPORTED &&
-		highest_tp >= HW_DP_TRAINING_PATTERN_3)
-		return HW_DP_TRAINING_PATTERN_3;
-
-	return HW_DP_TRAINING_PATTERN_2;
-}
-
 static enum link_training_result get_cr_failure(enum dc_lane_count ln_count,
 					union lane_status *dpcd_lane_status)
 {

commit 53c81fc7875bc2dca358485dac3999e14ec91a00
Author: Wesley Chalmers <Wesley.Chalmers@amd.com>
Date:   Thu May 16 12:40:25 2019 -0400

    drm/amd/display: Update link rate from DPCD 10
    
    [WHY]
    Some panels return a link rate of 0 (unknown) in DPCD 0. In this case,
    an appropriate mode cannot be set, and certain panels will show
    corruption as they are forced to use a mode they do not support.
    
    [HOW]
    Read DPCD 10 in the case where supported link rate from DPCD 0 is
    unknown, and pass that value on to the reported link rate.
    This re-introduces behaviour present in previous versions that appears
    to have been accidentally removed.
    
    Signed-off-by: Wesley Chalmers <Wesley.Chalmers@amd.com>
    Reviewed-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 65d6caedbd82..2d519e5fc3ea 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1624,8 +1624,7 @@ static bool decide_edp_link_settings(struct dc_link *link, struct dc_link_settin
 	uint32_t link_bw;
 
 	if (link->dpcd_caps.dpcd_rev.raw < DPCD_REV_14 ||
-			link->dpcd_caps.edp_supported_link_rates_count == 0 ||
-			link->dc->config.optimize_edp_link_rate == false) {
+			link->dpcd_caps.edp_supported_link_rates_count == 0) {
 		*link_setting = link->verified_link_cap;
 		return true;
 	}
@@ -2609,7 +2608,8 @@ void detect_edp_sink_caps(struct dc_link *link)
 	memset(supported_link_rates, 0, sizeof(supported_link_rates));
 
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14 &&
-			link->dc->config.optimize_edp_link_rate) {
+			(link->dc->config.optimize_edp_link_rate ||
+			link->reported_link_cap.link_rate == LINK_RATE_UNKNOWN)) {
 		// Read DPCD 00010h - 0001Fh 16 bytes at one shot
 		core_link_read_dpcd(link, DP_SUPPORTED_LINK_RATES,
 							supported_link_rates, sizeof(supported_link_rates));
@@ -2624,6 +2624,9 @@ void detect_edp_sink_caps(struct dc_link *link)
 				link_rate = linkRateInKHzToLinkRateMultiplier(link_rate_in_khz);
 				link->dpcd_caps.edp_supported_link_rates[link->dpcd_caps.edp_supported_link_rates_count] = link_rate;
 				link->dpcd_caps.edp_supported_link_rates_count++;
+
+				if (link->reported_link_cap.link_rate < link_rate)
+					link->reported_link_cap.link_rate = link_rate;
 			}
 		}
 	}

commit 8633d96d3ca14b37dffe303281c1dff320b422fb
Author: Anthony Koo <anthony.koo@amd.com>
Date:   Wed May 15 16:39:23 2019 -0400

    drm/amd/display: fix issues with bad AUX reply on some displays
    
    [Why]
    Some displays take some time to power up AUX CH once they are
    put into D3 state via write to DPCD 600h=2.
    
    Interestingly enough, some display may simply NACK, but some might
    also ACK with a bunch of 0s, which can cause issues with receiver
    cap retrieval. Note that not all DPCD address return 0s, but in
    particular it has been observed on some higher DPCD address such
    as DPCD 2200h, etc.
    
    [How]
    Based on spec, receiver will monitor differential signal while in D3 and
    AUX CH is in low power mode. When detected, it may allow up to
    1 ms to power up AUX CH and reply.
    
    If we read Sink power state D3, we should add 1 ms delay to satisfy
    this spec requirement.
    
    Signed-off-by: Anthony Koo <anthony.koo@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1ee544a32ebb..65d6caedbd82 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2361,6 +2361,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 	/*Only need to read 1 byte starting from DP_DPRX_FEATURE_ENUMERATION_LIST.
 	 */
 	uint8_t dpcd_dprx_data = '\0';
+	uint8_t dpcd_power_state = '\0';
 
 	struct dp_device_vendor_id sink_id;
 	union down_stream_port_count down_strm_port_count;
@@ -2377,6 +2378,17 @@ static bool retrieve_link_cap(struct dc_link *link)
 	memset(&edp_config_cap, '\0',
 		sizeof(union edp_configuration_cap));
 
+	status = core_link_read_dpcd(link, DP_SET_POWER,
+				&dpcd_power_state, sizeof(dpcd_power_state));
+
+	/* Delay 1 ms if AUX CH is in power down state. Based on spec
+	 * section 2.3.1.2, if AUX CH may be powered down due to
+	 * write to DPCD 600h = 2. Sink AUX CH is monitoring differential
+	 * signal and may need up to 1 ms before being able to reply.
+	 */
+	if (status != DC_OK || dpcd_power_state == DP_SET_POWER_D3)
+		udelay(1000);
+
 	for (i = 0; i < read_dpcd_retry_cnt; i++) {
 		status = core_link_read_dpcd(
 				link,

commit ee13cea962b5ab2d70501f51008c05b394cb322e
Author: John Barberiz <John.Barberiz@amd.com>
Date:   Wed Apr 3 19:22:55 2019 -0400

    drm/amd/display: Refactor dp vendor parsing logic to a function
    
    Refactor dp vendor parsing int to a new function, and call it before
    get_active_converter_info().
    
    Also, add a flag to skip parsing of Display ID 2.0. Some devices fail on
    readind DID2, but we shouldn't fail EDID read because of it. Add this
    flag to facilitate the logic.
    
    Signed-off-by: John Barberiz <John.Barberiz@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 64a309a5e1c9..1ee544a32ebb 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2185,6 +2185,30 @@ static int translate_dpcd_max_bpc(enum dpcd_downstream_port_max_bpc bpc)
 	return -1;
 }
 
+static void read_dp_device_vendor_id(struct dc_link *link)
+{
+	struct dp_device_vendor_id dp_id;
+
+	/* read IEEE branch device id */
+	core_link_read_dpcd(
+		link,
+		DP_BRANCH_OUI,
+		(uint8_t *)&dp_id,
+		sizeof(dp_id));
+
+	link->dpcd_caps.branch_dev_id =
+		(dp_id.ieee_oui[0] << 16) +
+		(dp_id.ieee_oui[1] << 8) +
+		dp_id.ieee_oui[2];
+
+	memmove(
+		link->dpcd_caps.branch_dev_name,
+		dp_id.ieee_device_id,
+		sizeof(dp_id.ieee_device_id));
+}
+
+
+
 static void get_active_converter_info(
 	uint8_t data, struct dc_link *link)
 {
@@ -2270,27 +2294,6 @@ static void get_active_converter_info(
 
 	ddc_service_set_dongle_type(link->ddc, link->dpcd_caps.dongle_type);
 
-	{
-		struct dp_device_vendor_id dp_id;
-
-		/* read IEEE branch device id */
-		core_link_read_dpcd(
-			link,
-			DP_BRANCH_OUI,
-			(uint8_t *)&dp_id,
-			sizeof(dp_id));
-
-		link->dpcd_caps.branch_dev_id =
-			(dp_id.ieee_oui[0] << 16) +
-			(dp_id.ieee_oui[1] << 8) +
-			dp_id.ieee_oui[2];
-
-		memmove(
-			link->dpcd_caps.branch_dev_name,
-			dp_id.ieee_device_id,
-			sizeof(dp_id.ieee_device_id));
-	}
-
 	{
 		struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 
@@ -2455,6 +2458,8 @@ static bool retrieve_link_cap(struct dc_link *link)
 	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
 				 DP_DPCD_REV];
 
+	read_dp_device_vendor_id(link);
+
 	get_active_converter_info(ds_port.byte, link);
 
 	dp_wa_power_up_0010FA(link, dpcd_data, sizeof(dpcd_data));

commit 5b7c0d8d2bade0f743009e5c2deb541792e0e963
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Thu Apr 4 14:42:44 2019 -0400

    drm/amd/display: Fix eDP Black screen after S4 resume
    
    [Why]
    Power down of PHY on eDP requires us to call eDP power
    control to power on again
    
    [How]
    1. In the case link rates don't match, disable PHY
    requires calling of eDP power control ON after
    
    2. Link disable case limit to eDP path since
    this is not really applicable to DP since we do
    power down PHY as part of verify link cap
    
    3. Move detection of eDP link settings to be
    done even for S4 resume cases where other
    dpcd cap read and edid read can be skipped
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a6424c70f4c5..64a309a5e1c9 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2586,9 +2586,6 @@ void detect_edp_sink_caps(struct dc_link *link)
 	uint32_t entry;
 	uint32_t link_rate_in_khz;
 	enum dc_link_rate link_rate = LINK_RATE_UNKNOWN;
-	union lane_count_set lane_count_set = { {0} };
-	uint8_t link_bw_set;
-	uint8_t link_rate_set;
 
 	retrieve_link_cap(link);
 	link->dpcd_caps.edp_supported_link_rates_count = 0;
@@ -2614,33 +2611,6 @@ void detect_edp_sink_caps(struct dc_link *link)
 		}
 	}
 	link->verified_link_cap = link->reported_link_cap;
-
-	// Read DPCD 00101h to find out the number of lanes currently set
-	core_link_read_dpcd(link, DP_LANE_COUNT_SET,
-			&lane_count_set.raw, sizeof(lane_count_set));
-	link->cur_link_settings.lane_count = lane_count_set.bits.LANE_COUNT_SET;
-
-	// Read DPCD 00100h to find if standard link rates are set
-	core_link_read_dpcd(link, DP_LINK_BW_SET,
-			&link_bw_set, sizeof(link_bw_set));
-
-	if (link_bw_set == 0) {
-		/* If standard link rates are not being used,
-		 * Read DPCD 00115h to find the link rate set used
-		 */
-		core_link_read_dpcd(link, DP_LINK_RATE_SET,
-				&link_rate_set, sizeof(link_rate_set));
-
-		if (link_rate_set < link->dpcd_caps.edp_supported_link_rates_count) {
-			link->cur_link_settings.link_rate =
-				link->dpcd_caps.edp_supported_link_rates[link_rate_set];
-			link->cur_link_settings.link_rate_set = link_rate_set;
-			link->cur_link_settings.use_link_rate_set = true;
-		}
-	} else {
-		link->cur_link_settings.link_rate = link_bw_set;
-		link->cur_link_settings.use_link_rate_set = false;
-	}
 }
 
 void dc_link_dp_enable_hpd(const struct dc_link *link)

commit 5ac4619b9d2fdbb54ef4b247db774637e347d46e
Author: Samson Tam <Samson.Tam@amd.com>
Date:   Mon Apr 1 17:07:21 2019 -0400

    drm/amd/display: change name from dc_link_get_verified_link_cap to dc_link_get_link_cap
    
    [Why]
    DM doesn't need to know which link cap is being retrieved ( verified
     or preferred ).  Let DC figure it out.
    
    [How]
    Change name.
    
    Signed-off-by: Samson Tam <Samson.Tam@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index ba7502f3d0eb..a6424c70f4c5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1548,8 +1548,7 @@ bool dp_validate_mode_timing(
 		timing->v_addressable == (uint32_t) 480)
 		return true;
 
-	/* We always use verified link settings */
-	link_setting = dc_link_get_verified_link_cap(link);
+	link_setting = dc_link_get_link_cap(link);
 
 	/* TODO: DYNAMIC_VALIDATION needs to be implemented */
 	/*if (flags.DYNAMIC_VALIDATION == 1 &&

commit 32a5b542a6c6c7b175b9a37dd5e155667cbb2e7b
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Mon Mar 25 20:33:35 2019 -0400

    drm/amd/display: Read eDP link settings on detection
    
    [Why]
    Unlike external DP panels, internal eDP does not perform
    verify link caps because the panel connection is fixed.
    
    So if GOP enabled the eDP at boot, we can retain its
    trained link settings to optimize.
    
    [How]
    Read the lane count and link rate by reading this
    information from DPCD 100h, 101h, 115h
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index acb4f829e042..ba7502f3d0eb 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2587,6 +2587,9 @@ void detect_edp_sink_caps(struct dc_link *link)
 	uint32_t entry;
 	uint32_t link_rate_in_khz;
 	enum dc_link_rate link_rate = LINK_RATE_UNKNOWN;
+	union lane_count_set lane_count_set = { {0} };
+	uint8_t link_bw_set;
+	uint8_t link_rate_set;
 
 	retrieve_link_cap(link);
 	link->dpcd_caps.edp_supported_link_rates_count = 0;
@@ -2612,6 +2615,33 @@ void detect_edp_sink_caps(struct dc_link *link)
 		}
 	}
 	link->verified_link_cap = link->reported_link_cap;
+
+	// Read DPCD 00101h to find out the number of lanes currently set
+	core_link_read_dpcd(link, DP_LANE_COUNT_SET,
+			&lane_count_set.raw, sizeof(lane_count_set));
+	link->cur_link_settings.lane_count = lane_count_set.bits.LANE_COUNT_SET;
+
+	// Read DPCD 00100h to find if standard link rates are set
+	core_link_read_dpcd(link, DP_LINK_BW_SET,
+			&link_bw_set, sizeof(link_bw_set));
+
+	if (link_bw_set == 0) {
+		/* If standard link rates are not being used,
+		 * Read DPCD 00115h to find the link rate set used
+		 */
+		core_link_read_dpcd(link, DP_LINK_RATE_SET,
+				&link_rate_set, sizeof(link_rate_set));
+
+		if (link_rate_set < link->dpcd_caps.edp_supported_link_rates_count) {
+			link->cur_link_settings.link_rate =
+				link->dpcd_caps.edp_supported_link_rates[link_rate_set];
+			link->cur_link_settings.link_rate_set = link_rate_set;
+			link->cur_link_settings.use_link_rate_set = true;
+		}
+	} else {
+		link->cur_link_settings.link_rate = link_bw_set;
+		link->cur_link_settings.use_link_rate_set = false;
+	}
 }
 
 void dc_link_dp_enable_hpd(const struct dc_link *link)

commit 7ee3769a37d3f9d7352bb0182b0bf3a2beabe523
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Fri Mar 22 19:33:46 2019 -0400

    drm/amd/display: prefer preferred link cap over verified link settings
    
    [why]
    when preferred link cap is set, we should always use
    preferred in all validation.
    we should not use preferred for some validation but use
    verified for others.
    
    [how]
    create getter function that gets verified link cap.
    if preferred is set, return preferred link settings instead.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0d8ef8f7cb58..acb4f829e042 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1549,7 +1549,7 @@ bool dp_validate_mode_timing(
 		return true;
 
 	/* We always use verified link settings */
-	link_setting = &link->verified_link_cap;
+	link_setting = dc_link_get_verified_link_cap(link);
 
 	/* TODO: DYNAMIC_VALIDATION needs to be implemented */
 	/*if (flags.DYNAMIC_VALIDATION == 1 &&

commit 332c11914a76200dfbafc6b46ae5e728216aac00
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Tue Mar 19 19:47:32 2019 -0400

    drm/amd/display: Calculate link bandwidth in a common function
    
    [why]
    Currently link bandwidth is calculated in two places, using the same
    formula. They should be unified into calling one function.
    
    [how]
    Replace all implementations of link bandwidth calculation with a call
    to a function.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 491d13d27e9e..0d8ef8f7cb58 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1533,22 +1533,6 @@ static bool decide_fallback_link_setting(
 	return true;
 }
 
-static uint32_t bandwidth_in_kbps_from_link_settings(
-	const struct dc_link_settings *link_setting)
-{
-	uint32_t link_rate_in_kbps = link_setting->link_rate *
-		LINK_RATE_REF_FREQ_IN_KHZ;
-
-	uint32_t lane_count  = link_setting->lane_count;
-	uint32_t kbps = link_rate_in_kbps;
-
-	kbps *= lane_count;
-	kbps *= 8;   /* 8 bits per byte*/
-
-	return kbps;
-
-}
-
 bool dp_validate_mode_timing(
 	struct dc_link *link,
 	const struct dc_crtc_timing *timing)
@@ -1574,7 +1558,7 @@ bool dp_validate_mode_timing(
 	*/
 
 	req_bw = dc_bandwidth_in_kbps_from_timing(timing);
-	max_bw = bandwidth_in_kbps_from_link_settings(link_setting);
+	max_bw = dc_link_bandwidth_kbps(link, link_setting);
 
 	if (req_bw <= max_bw) {
 		/* remember the biggest mode here, during
@@ -1609,7 +1593,8 @@ static bool decide_dp_link_settings(struct dc_link *link, struct dc_link_setting
 	 */
 	while (current_link_setting.link_rate <=
 			link->verified_link_cap.link_rate) {
-		link_bw = bandwidth_in_kbps_from_link_settings(
+		link_bw = dc_link_bandwidth_kbps(
+				link,
 				&current_link_setting);
 		if (req_bw <= link_bw) {
 			*link_setting = current_link_setting;
@@ -1660,7 +1645,8 @@ static bool decide_edp_link_settings(struct dc_link *link, struct dc_link_settin
 	 */
 	while (current_link_setting.link_rate <=
 			link->verified_link_cap.link_rate) {
-		link_bw = bandwidth_in_kbps_from_link_settings(
+		link_bw = dc_link_bandwidth_kbps(
+				link,
 				&current_link_setting);
 		if (req_bw <= link_bw) {
 			*link_setting = current_link_setting;

commit e49f69363adf8920883fff7e8ffecb802d897c6b
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Thu Mar 21 13:05:36 2019 -0400

    drm/amd/display: use proper formula to calculate bandwidth from timing
    
    [why]
    The existing calculation uses a wrong formula to
    calculate bandwidth from timing.
    
    [how]
    Expose the existing proper function that calculates the bandwidth,
    so dc_link can use it to calculate timing bandwidth correctly.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 528c9625b5c0..491d13d27e9e 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1533,53 +1533,6 @@ static bool decide_fallback_link_setting(
 	return true;
 }
 
-static uint32_t bandwidth_in_kbps_from_timing(
-	const struct dc_crtc_timing *timing)
-{
-	uint32_t bits_per_channel = 0;
-	uint32_t kbps;
-
-	switch (timing->display_color_depth) {
-	case COLOR_DEPTH_666:
-		bits_per_channel = 6;
-		break;
-	case COLOR_DEPTH_888:
-		bits_per_channel = 8;
-		break;
-	case COLOR_DEPTH_101010:
-		bits_per_channel = 10;
-		break;
-	case COLOR_DEPTH_121212:
-		bits_per_channel = 12;
-		break;
-	case COLOR_DEPTH_141414:
-		bits_per_channel = 14;
-		break;
-	case COLOR_DEPTH_161616:
-		bits_per_channel = 16;
-		break;
-	default:
-		break;
-	}
-
-	ASSERT(bits_per_channel != 0);
-
-	kbps = timing->pix_clk_100hz / 10;
-	kbps *= bits_per_channel;
-
-	if (timing->flags.Y_ONLY != 1) {
-		/*Only YOnly make reduce bandwidth by 1/3 compares to RGB*/
-		kbps *= 3;
-		if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR420)
-			kbps /= 2;
-		else if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR422)
-			kbps = kbps * 2 / 3;
-	}
-
-	return kbps;
-
-}
-
 static uint32_t bandwidth_in_kbps_from_link_settings(
 	const struct dc_link_settings *link_setting)
 {
@@ -1620,7 +1573,7 @@ bool dp_validate_mode_timing(
 		link_setting = &link->verified_link_cap;
 	*/
 
-	req_bw = bandwidth_in_kbps_from_timing(timing);
+	req_bw = dc_bandwidth_in_kbps_from_timing(timing);
 	max_bw = bandwidth_in_kbps_from_link_settings(link_setting);
 
 	if (req_bw <= max_bw) {
@@ -1739,7 +1692,7 @@ void decide_link_settings(struct dc_stream_state *stream,
 	struct dc_link *link;
 	uint32_t req_bw;
 
-	req_bw = bandwidth_in_kbps_from_timing(&stream->timing);
+	req_bw = dc_bandwidth_in_kbps_from_timing(&stream->timing);
 
 	link = stream->link;
 

commit e5490464f45db8b8006bc62d29ec9280debd0ef9
Author: SivapiriyanKumarasamy <sivapiriyan.kumarasamy@amd.com>
Date:   Tue Mar 19 15:21:50 2019 -0400

    drm/amd/display: fix dp_hdmi_max_pixel_clk units
    
    [Why]
    We are incorrectly using dp_hdmi_max_pixel_clk because the units are not clear.
    
    [How]
    Rename to dp_hdmi_max_pixel_clk_in_khz, and change mode timing validation to use
    the value correctly.
    
    Signed-off-by: SivapiriyanKumarasamy <sivapiriyan.kumarasamy@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 063d019a3f6f..528c9625b5c0 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2304,8 +2304,8 @@ static void get_active_converter_info(
 					hdmi_caps = {.raw = det_caps[3] };
 				union dwnstream_port_caps_byte2
 					hdmi_color_caps = {.raw = det_caps[2] };
-				link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk =
-					det_caps[1] * 25000;
+				link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk_in_khz =
+					det_caps[1] * 2500;
 
 				link->dpcd_caps.dongle_caps.is_dp_hdmi_s3d_converter =
 					hdmi_caps.bits.FRAME_SEQ_TO_FRAME_PACK;
@@ -2322,7 +2322,7 @@ static void get_active_converter_info(
 					translate_dpcd_max_bpc(
 						hdmi_color_caps.bits.MAX_BITS_PER_COLOR_COMPONENT);
 
-				if (link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk != 0)
+				if (link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk_in_khz != 0)
 					link->dpcd_caps.dongle_caps.extendedCapValid = true;
 			}
 

commit a504ad265dec380d7314b8e02984c8d294ab0bd5
Author: Hugo Hu <hugo.hu@amd.com>
Date:   Mon Feb 25 19:16:52 2019 +0800

    drm/amd/display: Handle branch device with DFP count = 0 case.
    
    [Why]
    When you have a SST branch device the driver, Even no sink device connected,
    it also send HPD with a valid EDID. Driver will config it to DP sink.
    Therefore, there're two displays in display setting.
    
    DPCD 0x05, DFP_PRESENT = 1 (branch device),
    DFP_TYPE = 00 (Display Port)
    
    [How]
    Driver determine DPCD 0x05 DFP_PRESENT = 1(branch) as an active dongle
    And check DFP count.
    
    Signed-off-by: Hugo Hu <hugo.hu@amd.com>
    Reviewed-by: Hugo Hu <Hugo.Hu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 72a88b1808fe..063d019a3f6f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2226,11 +2226,7 @@ bool is_mst_supported(struct dc_link *link)
 
 bool is_dp_active_dongle(const struct dc_link *link)
 {
-	enum display_dongle_type dongle_type = link->dpcd_caps.dongle_type;
-
-	return (dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) ||
-			(dongle_type == DISPLAY_DONGLE_DP_DVI_CONVERTER) ||
-			(dongle_type == DISPLAY_DONGLE_DP_HDMI_CONVERTER);
+	return link->dpcd_caps.is_branch_dev;
 }
 
 static int translate_dpcd_max_bpc(enum dpcd_downstream_port_max_bpc bpc)
@@ -2264,6 +2260,9 @@ static void get_active_converter_info(
 		return;
 	}
 
+	/* DPCD 0x5 bit 0 = 1, it indicate it's branch device */
+	link->dpcd_caps.is_branch_dev = ds_port.fields.PORT_PRESENT;
+
 	switch (ds_port.fields.PORT_TYPE) {
 	case DOWNSTREAM_VGA:
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_DP_VGA_CONVERTER;

commit 661a8cd9516b182c80fff1b2fdfb1b1e42e212d1
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Mar 8 17:09:46 2019 -0500

    drm/amd/display: add missing opp programming for odm
    
    A number of places opp programming was missing for odm second pipe.
    This change fixes the oversight.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index ef603f24c71c..72a88b1808fe 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2710,6 +2710,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 	enum dc_color_depth color_depth = pipe_ctx->
 		stream->timing.display_color_depth;
 	struct bit_depth_reduction_params params;
+	struct output_pixel_processor *opp = pipe_ctx->stream_res.opp;
 
 	memset(&params, 0, sizeof(params));
 
@@ -2749,8 +2750,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 	{
 		/* disable bit depth reduction */
 		pipe_ctx->stream->bit_depth_params = params;
-		pipe_ctx->stream_res.opp->funcs->
-			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
+		opp->funcs->opp_program_bit_depth_reduction(opp, &params);
 		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				controller_test_pattern, color_depth);
@@ -2759,11 +2759,9 @@ static void set_crtc_test_pattern(struct dc_link *link,
 	case DP_TEST_PATTERN_VIDEO_MODE:
 	{
 		/* restore bitdepth reduction */
-		resource_build_bit_depth_reduction_params(pipe_ctx->stream,
-					&params);
+		resource_build_bit_depth_reduction_params(pipe_ctx->stream, &params);
 		pipe_ctx->stream->bit_depth_params = params;
-		pipe_ctx->stream_res.opp->funcs->
-			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
+		opp->funcs->opp_program_bit_depth_reduction(opp, &params);
 		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
 			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,

commit 3c7dd2cbc8e135fded5844060c815fec262dffd2
Author: Harmanprit Tatla <Harmanprit.Tatla@amd.com>
Date:   Fri Mar 1 11:47:35 2019 -0500

    drm/amd/display: cache additional dpcd caps for HDR capability check
    
    [Why]
    Currently we are missing a few checks to see if HDR10 is allowed.
    In particular we never check for the extended colorimetry bit (whether its
    present or set to 1). Further we don't read in the dpcd block in DC that
    would provide these bits.
    
    [How]
    - Added in DC code to read in the block containing the extended colorimetry
    bit.
    
    Signed-off-by: Harmanprit Tatla <Harmanprit.Tatla@amd.com>
    Reviewed-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e1081e2dffdc..ef603f24c71c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2418,6 +2418,10 @@ static bool retrieve_link_cap(struct dc_link *link)
 {
 	uint8_t dpcd_data[DP_ADAPTER_CAP - DP_DPCD_REV + 1];
 
+	/*Only need to read 1 byte starting from DP_DPRX_FEATURE_ENUMERATION_LIST.
+	 */
+	uint8_t dpcd_dprx_data = '\0';
+
 	struct dp_device_vendor_id sink_id;
 	union down_stream_port_count down_strm_port_count;
 	union edp_configuration_cap edp_config_cap;
@@ -2454,7 +2458,10 @@ static bool retrieve_link_cap(struct dc_link *link)
 		aux_rd_interval.raw =
 			dpcd_data[DP_TRAINING_AUX_RD_INTERVAL];
 
-		if (aux_rd_interval.bits.EXT_RECIEVER_CAP_FIELD_PRESENT == 1) {
+		link->dpcd_caps.ext_receiver_cap_field_present =
+				aux_rd_interval.bits.EXT_RECEIVER_CAP_FIELD_PRESENT == 1 ? true:false;
+
+		if (aux_rd_interval.bits.EXT_RECEIVER_CAP_FIELD_PRESENT == 1) {
 			uint8_t ext_cap_data[16];
 
 			memset(ext_cap_data, '\0', sizeof(ext_cap_data));
@@ -2474,6 +2481,31 @@ static bool retrieve_link_cap(struct dc_link *link)
 		}
 	}
 
+	link->dpcd_caps.dpcd_rev.raw =
+			dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
+
+	if (link->dpcd_caps.dpcd_rev.raw >= 0x14) {
+		for (i = 0; i < read_dpcd_retry_cnt; i++) {
+			status = core_link_read_dpcd(
+					link,
+					DP_DPRX_FEATURE_ENUMERATION_LIST,
+					&dpcd_dprx_data,
+					sizeof(dpcd_dprx_data));
+			if (status == DC_OK)
+				break;
+		}
+
+		link->dpcd_caps.dprx_feature.raw = dpcd_dprx_data;
+
+		if (status != DC_OK)
+			dm_error("%s: Read DPRX caps data failed.\n", __func__);
+	}
+
+	else {
+		link->dpcd_caps.dprx_feature.raw = 0;
+	}
+
+
 	/* Error condition checking...
 	 * It is impossible for Sink to report Max Lane Count = 0.
 	 * It is possible for Sink to report Max Link Rate = 0, if it is
@@ -2483,9 +2515,6 @@ static bool retrieve_link_cap(struct dc_link *link)
 	if (dpcd_data[DP_MAX_LANE_COUNT - DP_DPCD_REV] == 0)
 		return false;
 
-	link->dpcd_caps.dpcd_rev.raw =
-		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
-
 	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
 				 DP_DPCD_REV];
 

commit 07d6a199219562834757ac72c28f3836b4e85694
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Fri Feb 15 14:19:30 2019 -0500

    drm/amd/display: Fix soft hang issue when some DPCD data invalid
    
    [Why]
    AUX transaction returns success, but data has invalid lane count and rate
    which when passed to VBIOS command table causes it to soft hang
    
    [How]
    Do some sanity checking and fail if the DPCD caps are invalid.
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8ad79df56bf8..e1081e2dffdc 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2474,6 +2474,15 @@ static bool retrieve_link_cap(struct dc_link *link)
 		}
 	}
 
+	/* Error condition checking...
+	 * It is impossible for Sink to report Max Lane Count = 0.
+	 * It is possible for Sink to report Max Link Rate = 0, if it is
+	 * an eDP device that is reporting specialized link rates in the
+	 * SUPPORTED_LINK_RATE table.
+	 */
+	if (dpcd_data[DP_MAX_LANE_COUNT - DP_DPCD_REV] == 0)
+		return false;
+
 	link->dpcd_caps.dpcd_rev.raw =
 		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
 

commit 8628d02f60d4a568d02fc12a26273a55f7718ec0
Author: Josip Pavic <Josip.Pavic@amd.com>
Date:   Tue Feb 5 19:27:38 2019 -0500

    drm/amd/display: optionally optimize edp link rate based on timing
    
    [Why]
    eDP v1.4 allows panels to report link rates other than RBR/HBR/HBR2, that
    may be more optimal for the panel's timing. Power can be saved by using
    a link rate closer to the required bandwidth of the panel's timing.
    
    [How]
    Scan the table of reported link rates from the panel, and select the
    minimum link rate that satisfies the bandwidth requirements of the panel's
    timing. Include a flag to make the feature optional.
    
    Signed-off-by: Josip Pavic <Josip.Pavic@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Acked-by: Anthony Koo <Anthony.Koo@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 09d301216076..8ad79df56bf8 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -93,12 +93,10 @@ static void dpcd_set_link_settings(
 	struct dc_link *link,
 	const struct link_training_settings *lt_settings)
 {
-	uint8_t rate = (uint8_t)
-	(lt_settings->link_settings.link_rate);
+	uint8_t rate;
 
 	union down_spread_ctrl downspread = { {0} };
 	union lane_count_set lane_count_set = { {0} };
-	uint8_t link_set_buffer[2];
 
 	downspread.raw = (uint8_t)
 	(lt_settings->link_settings.link_spread);
@@ -111,29 +109,42 @@ static void dpcd_set_link_settings(
 	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED =
 		link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED;
 
-	link_set_buffer[0] = rate;
-	link_set_buffer[1] = lane_count_set.raw;
-
-	core_link_write_dpcd(link, DP_LINK_BW_SET,
-	link_set_buffer, 2);
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
 
+	core_link_write_dpcd(link, DP_LANE_COUNT_SET,
+	&lane_count_set.raw, 1);
+
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14 &&
-		(link->dpcd_caps.link_rate_set >= 1 &&
-		link->dpcd_caps.link_rate_set <= 8)) {
+			lt_settings->link_settings.use_link_rate_set == true) {
+		rate = 0;
+		core_link_write_dpcd(link, DP_LINK_BW_SET, &rate, 1);
 		core_link_write_dpcd(link, DP_LINK_RATE_SET,
-		&link->dpcd_caps.link_rate_set, 1);
+				&lt_settings->link_settings.link_rate_set, 1);
+	} else {
+		rate = (uint8_t) (lt_settings->link_settings.link_rate);
+		core_link_write_dpcd(link, DP_LINK_BW_SET, &rate, 1);
 	}
 
-	DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
-		__func__,
-		DP_LINK_BW_SET,
-		lt_settings->link_settings.link_rate,
-		DP_LANE_COUNT_SET,
-		lt_settings->link_settings.lane_count,
-		DP_DOWNSPREAD_CTRL,
-		lt_settings->link_settings.link_spread);
+	if (rate) {
+		DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
+			__func__,
+			DP_LINK_BW_SET,
+			lt_settings->link_settings.link_rate,
+			DP_LANE_COUNT_SET,
+			lt_settings->link_settings.lane_count,
+			DP_DOWNSPREAD_CTRL,
+			lt_settings->link_settings.link_spread);
+	} else {
+		DC_LOG_HW_LINK_TRAINING("%s\n %x rate set = %x\n %x lane = %x\n %x spread = %x\n",
+			__func__,
+			DP_LINK_RATE_SET,
+			lt_settings->link_settings.link_rate_set,
+			DP_LANE_COUNT_SET,
+			lt_settings->link_settings.lane_count,
+			DP_DOWNSPREAD_CTRL,
+			lt_settings->link_settings.link_spread);
+	}
 
 }
 
@@ -952,6 +963,8 @@ enum link_training_result dc_link_dp_perform_link_training(
 
 	lt_settings.link_settings.link_rate = link_setting->link_rate;
 	lt_settings.link_settings.lane_count = link_setting->lane_count;
+	lt_settings.link_settings.use_link_rate_set = link_setting->use_link_rate_set;
+	lt_settings.link_settings.link_rate_set = link_setting->link_rate_set;
 
 	/*@todo[vdevulap] move SS to LS, should not be handled by displaypath*/
 
@@ -1075,7 +1088,7 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 {
 	/* Set Default link settings */
 	struct dc_link_settings max_link_cap = {LANE_COUNT_FOUR, LINK_RATE_HIGH,
-			LINK_SPREAD_05_DOWNSPREAD_30KHZ};
+			LINK_SPREAD_05_DOWNSPREAD_30KHZ, false, 0};
 
 	/* Higher link settings based on feature supported */
 	if (link->link_enc->features.flags.bits.IS_HBR2_CAPABLE)
@@ -1629,47 +1642,65 @@ bool dp_validate_mode_timing(
 		return false;
 }
 
-void decide_link_settings(struct dc_stream_state *stream,
-	struct dc_link_settings *link_setting)
+static bool decide_dp_link_settings(struct dc_link *link, struct dc_link_settings *link_setting, uint32_t req_bw)
 {
-
 	struct dc_link_settings initial_link_setting = {
-		LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED};
+		LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED, false, 0};
 	struct dc_link_settings current_link_setting =
 			initial_link_setting;
-	struct dc_link *link;
-	uint32_t req_bw;
 	uint32_t link_bw;
 
-	req_bw = bandwidth_in_kbps_from_timing(&stream->timing);
-
-	link = stream->link;
-
-	/* if preferred is specified through AMDDP, use it, if it's enough
-	 * to drive the mode
+	/* search for the minimum link setting that:
+	 * 1. is supported according to the link training result
+	 * 2. could support the b/w requested by the timing
 	 */
-	if (link->preferred_link_setting.lane_count !=
-			LANE_COUNT_UNKNOWN &&
-			link->preferred_link_setting.link_rate !=
-					LINK_RATE_UNKNOWN) {
-		*link_setting =  link->preferred_link_setting;
-		return;
-	}
+	while (current_link_setting.link_rate <=
+			link->verified_link_cap.link_rate) {
+		link_bw = bandwidth_in_kbps_from_link_settings(
+				&current_link_setting);
+		if (req_bw <= link_bw) {
+			*link_setting = current_link_setting;
+			return true;
+		}
 
-	/* MST doesn't perform link training for now
-	 * TODO: add MST specific link training routine
-	 */
-	if (stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
-		*link_setting = link->verified_link_cap;
-		return;
+		if (current_link_setting.lane_count <
+				link->verified_link_cap.lane_count) {
+			current_link_setting.lane_count =
+					increase_lane_count(
+							current_link_setting.lane_count);
+		} else {
+			current_link_setting.link_rate =
+					increase_link_rate(
+							current_link_setting.link_rate);
+			current_link_setting.lane_count =
+					initial_link_setting.lane_count;
+		}
 	}
 
-	/* EDP use the link cap setting */
-	if (link->connector_signal == SIGNAL_TYPE_EDP) {
+	return false;
+}
+
+static bool decide_edp_link_settings(struct dc_link *link, struct dc_link_settings *link_setting, uint32_t req_bw)
+{
+	struct dc_link_settings initial_link_setting;
+	struct dc_link_settings current_link_setting;
+	uint32_t link_bw;
+
+	if (link->dpcd_caps.dpcd_rev.raw < DPCD_REV_14 ||
+			link->dpcd_caps.edp_supported_link_rates_count == 0 ||
+			link->dc->config.optimize_edp_link_rate == false) {
 		*link_setting = link->verified_link_cap;
-		return;
+		return true;
 	}
 
+	memset(&initial_link_setting, 0, sizeof(initial_link_setting));
+	initial_link_setting.lane_count = LANE_COUNT_ONE;
+	initial_link_setting.link_rate = link->dpcd_caps.edp_supported_link_rates[0];
+	initial_link_setting.link_spread = LINK_SPREAD_DISABLED;
+	initial_link_setting.use_link_rate_set = true;
+	initial_link_setting.link_rate_set = 0;
+	current_link_setting = initial_link_setting;
+
 	/* search for the minimum link setting that:
 	 * 1. is supported according to the link training result
 	 * 2. could support the b/w requested by the timing
@@ -1680,7 +1711,7 @@ void decide_link_settings(struct dc_stream_state *stream,
 				&current_link_setting);
 		if (req_bw <= link_bw) {
 			*link_setting = current_link_setting;
-			return;
+			return true;
 		}
 
 		if (current_link_setting.lane_count <
@@ -1689,13 +1720,53 @@ void decide_link_settings(struct dc_stream_state *stream,
 					increase_lane_count(
 							current_link_setting.lane_count);
 		} else {
-			current_link_setting.link_rate =
-					increase_link_rate(
-							current_link_setting.link_rate);
-			current_link_setting.lane_count =
-					initial_link_setting.lane_count;
+			if (current_link_setting.link_rate_set < link->dpcd_caps.edp_supported_link_rates_count) {
+				current_link_setting.link_rate_set++;
+				current_link_setting.link_rate =
+					link->dpcd_caps.edp_supported_link_rates[current_link_setting.link_rate_set];
+				current_link_setting.lane_count =
+									initial_link_setting.lane_count;
+			} else
+				break;
 		}
 	}
+	return false;
+}
+
+void decide_link_settings(struct dc_stream_state *stream,
+	struct dc_link_settings *link_setting)
+{
+	struct dc_link *link;
+	uint32_t req_bw;
+
+	req_bw = bandwidth_in_kbps_from_timing(&stream->timing);
+
+	link = stream->link;
+
+	/* if preferred is specified through AMDDP, use it, if it's enough
+	 * to drive the mode
+	 */
+	if (link->preferred_link_setting.lane_count !=
+			LANE_COUNT_UNKNOWN &&
+			link->preferred_link_setting.link_rate !=
+					LINK_RATE_UNKNOWN) {
+		*link_setting =  link->preferred_link_setting;
+		return;
+	}
+
+	/* MST doesn't perform link training for now
+	 * TODO: add MST specific link training routine
+	 */
+	if (stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
+		*link_setting = link->verified_link_cap;
+		return;
+	}
+
+	if (link->connector_signal == SIGNAL_TYPE_EDP) {
+		if (decide_edp_link_settings(link, link_setting, req_bw))
+			return;
+	} else if (decide_dp_link_settings(link, link_setting, req_bw))
+		return;
 
 	BREAK_TO_DEBUGGER();
 	ASSERT(link->verified_link_cap.lane_count != LANE_COUNT_UNKNOWN);
@@ -2536,31 +2607,31 @@ enum dc_link_rate linkRateInKHzToLinkRateMultiplier(uint32_t link_rate_in_khz)
 
 void detect_edp_sink_caps(struct dc_link *link)
 {
-	uint8_t supported_link_rates[16] = {0};
+	uint8_t supported_link_rates[16];
 	uint32_t entry;
 	uint32_t link_rate_in_khz;
 	enum dc_link_rate link_rate = LINK_RATE_UNKNOWN;
 
 	retrieve_link_cap(link);
+	link->dpcd_caps.edp_supported_link_rates_count = 0;
+	memset(supported_link_rates, 0, sizeof(supported_link_rates));
 
-	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14) {
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14 &&
+			link->dc->config.optimize_edp_link_rate) {
 		// Read DPCD 00010h - 0001Fh 16 bytes at one shot
 		core_link_read_dpcd(link, DP_SUPPORTED_LINK_RATES,
 							supported_link_rates, sizeof(supported_link_rates));
 
-		link->dpcd_caps.link_rate_set = 0;
 		for (entry = 0; entry < 16; entry += 2) {
 			// DPCD register reports per-lane link rate = 16-bit link rate capability
-			// value X 200 kHz. Need multipler to find link rate in kHz.
+			// value X 200 kHz. Need multiplier to find link rate in kHz.
 			link_rate_in_khz = (supported_link_rates[entry+1] * 0x100 +
 										supported_link_rates[entry]) * 200;
 
 			if (link_rate_in_khz != 0) {
 				link_rate = linkRateInKHzToLinkRateMultiplier(link_rate_in_khz);
-				if (link->reported_link_cap.link_rate < link_rate) {
-					link->reported_link_cap.link_rate = link_rate;
-					link->dpcd_caps.link_rate_set = entry;
-				}
+				link->dpcd_caps.edp_supported_link_rates[link->dpcd_caps.edp_supported_link_rates_count] = link_rate;
+				link->dpcd_caps.edp_supported_link_rates_count++;
 			}
 		}
 	}

commit de00d253bc85978c1a7d3be888d675488d18a5dd
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Fri Jan 25 11:50:31 2019 -0500

    drm/amd/display: link_rate_set should index into table
    
    [Why]
    Current implementation that maps link_rate_set value to
    actual link rate is incorrect.
    
    [How]
    Fix this implementation, such that link_rate_set indexes into
    the supported_link_rate table.
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Acked-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5ee36d6e0512..09d301216076 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2540,7 +2540,6 @@ void detect_edp_sink_caps(struct dc_link *link)
 	uint32_t entry;
 	uint32_t link_rate_in_khz;
 	enum dc_link_rate link_rate = LINK_RATE_UNKNOWN;
-	uint8_t link_rate_set = 0;
 
 	retrieve_link_cap(link);
 
@@ -2560,39 +2559,7 @@ void detect_edp_sink_caps(struct dc_link *link)
 				link_rate = linkRateInKHzToLinkRateMultiplier(link_rate_in_khz);
 				if (link->reported_link_cap.link_rate < link_rate) {
 					link->reported_link_cap.link_rate = link_rate;
-
-					switch (link_rate) {
-					case LINK_RATE_LOW:
-						link_rate_set = 1;
-						break;
-					case LINK_RATE_RATE_2:
-						link_rate_set = 2;
-						break;
-					case LINK_RATE_RATE_3:
-						link_rate_set = 3;
-						break;
-					case LINK_RATE_HIGH:
-						link_rate_set = 4;
-						break;
-					case LINK_RATE_RBR2:
-						link_rate_set = 5;
-						break;
-					case LINK_RATE_RATE_6:
-						link_rate_set = 6;
-						break;
-					case LINK_RATE_HIGH2:
-						link_rate_set = 7;
-						break;
-					case LINK_RATE_HIGH3:
-						link_rate_set = 8;
-						break;
-					default:
-						link_rate_set = 0;
-						break;
-					}
-
-					if (link->dpcd_caps.link_rate_set < link_rate_set)
-						link->dpcd_caps.link_rate_set = link_rate_set;
+					link->dpcd_caps.link_rate_set = entry;
 				}
 			}
 		}

commit d6d36b55a0f394a3895349285ed6980278809489
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Fri Feb 1 13:15:41 2019 -0700

    drm/amd/display: Use memset to initialize variable in wait_for_training_aux_rd_interval
    
    Clang warns:
    
    drivers/gpu/drm/amd/amdgpu/../display/dc/core/dc_link_dp.c:50:57:
    warning: suggest braces around initialization of subobject
    [-Wmissing-braces]
            union training_aux_rd_interval training_rd_interval = {0};
                                                                   ^
                                                                   {}
    1 warning generated.
    
    Previous efforts to fix this type of warning by adding or removing
    braces have been met with some pushback in favor of using memset [1][2].
    Do that here, mirroring commit 05794eff1aa6 ("drm/amdgpu/gmc: fix
    compiler errors [-Werror,-Wmissing-braces] (V2)") in this tree.
    
    [1]: https://lore.kernel.org/lkml/022e41c0-8465-dc7a-a45c-64187ecd9684@amd.com/
    [2]: https://lore.kernel.org/lkml/20181128.215241.702406654469517539.davem@davemloft.net/
    
    Fixes: 3cec41769d21 ("drm/amd/display: Fix use of uninitialized union")
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 92f565ca1260..5ee36d6e0512 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -47,7 +47,9 @@ static void wait_for_training_aux_rd_interval(
 	struct dc_link *link,
 	uint32_t default_wait_in_micro_secs)
 {
-	union training_aux_rd_interval training_rd_interval = {0};
+	union training_aux_rd_interval training_rd_interval;
+
+	memset(&training_rd_interval, 0, sizeof(training_rd_interval));
 
 	/* overwrite the delay if rev > 1.1*/
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {

commit 3cec41769d2182e629692a3262cc8b24ec972b04
Author: John Barberiz <John.Barberiz@amd.com>
Date:   Tue Jan 8 17:43:08 2019 -0500

    drm/amd/display: Fix use of uninitialized union
    
    [Why]
    An uninitialized variable would randomly initialize to a large
    value. This caused enough delay to fail DP Compliance Test 400.2.1.
    
    [How]
    Initialize the variable.
    
    Signed-off-by: John Barberiz <John.Barberiz@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 431805c566cf..92f565ca1260 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -47,7 +47,7 @@ static void wait_for_training_aux_rd_interval(
 	struct dc_link *link,
 	uint32_t default_wait_in_micro_secs)
 {
-	union training_aux_rd_interval training_rd_interval;
+	union training_aux_rd_interval training_rd_interval = {0};
 
 	/* overwrite the delay if rev > 1.1*/
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {

commit b03a599b3e1f520e79d3dbf487179bd0d70d4681
Author: Derek Lai <Derek.Lai@amd.com>
Date:   Tue Dec 11 16:27:09 2018 +0800

    drm/amd/display: Set link rate set if eDP ver >= 1.4.
    
    [Why]
    If eDP ver >= 1.4,
    the Source device must use LINK_RATE_SET.
    
    [How]
    Get LINK_RATE_SET by reading DPCD 10h-1fh,
    then write DPCD 00115h before link training.
    
    Signed-off-by: Derek Lai <Derek.Lai@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index cf9362704d12..431805c566cf 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -117,6 +117,13 @@ static void dpcd_set_link_settings(
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
 
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14 &&
+		(link->dpcd_caps.link_rate_set >= 1 &&
+		link->dpcd_caps.link_rate_set <= 8)) {
+		core_link_write_dpcd(link, DP_LINK_RATE_SET,
+		&link->dpcd_caps.link_rate_set, 1);
+	}
+
 	DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
 		__func__,
 		DP_LINK_BW_SET,
@@ -2489,13 +2496,105 @@ bool detect_dp_sink_caps(struct dc_link *link)
 	/* TODO save sink caps in link->sink */
 }
 
+enum dc_link_rate linkRateInKHzToLinkRateMultiplier(uint32_t link_rate_in_khz)
+{
+	enum dc_link_rate link_rate;
+	// LinkRate is normally stored as a multiplier of 0.27 Gbps per lane. Do the translation.
+	switch (link_rate_in_khz) {
+	case 1620000:
+		link_rate = LINK_RATE_LOW;		// Rate_1 (RBR)		- 1.62 Gbps/Lane
+		break;
+	case 2160000:
+		link_rate = LINK_RATE_RATE_2;	// Rate_2			- 2.16 Gbps/Lane
+		break;
+	case 2430000:
+		link_rate = LINK_RATE_RATE_3;	// Rate_3			- 2.43 Gbps/Lane
+		break;
+	case 2700000:
+		link_rate = LINK_RATE_HIGH;		// Rate_4 (HBR)		- 2.70 Gbps/Lane
+		break;
+	case 3240000:
+		link_rate = LINK_RATE_RBR2;		// Rate_5 (RBR2)	- 3.24 Gbps/Lane
+		break;
+	case 4320000:
+		link_rate = LINK_RATE_RATE_6;	// Rate_6			- 4.32 Gbps/Lane
+		break;
+	case 5400000:
+		link_rate = LINK_RATE_HIGH2;	// Rate_7 (HBR2)	- 5.40 Gbps/Lane
+		break;
+	case 8100000:
+		link_rate = LINK_RATE_HIGH3;	// Rate_8 (HBR3)	- 8.10 Gbps/Lane
+		break;
+	default:
+		link_rate = LINK_RATE_UNKNOWN;
+		break;
+	}
+	return link_rate;
+}
+
 void detect_edp_sink_caps(struct dc_link *link)
 {
-	retrieve_link_cap(link);
+	uint8_t supported_link_rates[16] = {0};
+	uint32_t entry;
+	uint32_t link_rate_in_khz;
+	enum dc_link_rate link_rate = LINK_RATE_UNKNOWN;
+	uint8_t link_rate_set = 0;
 
-	if (link->reported_link_cap.link_rate == LINK_RATE_UNKNOWN)
-		link->reported_link_cap.link_rate = LINK_RATE_HIGH2;
+	retrieve_link_cap(link);
 
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_14) {
+		// Read DPCD 00010h - 0001Fh 16 bytes at one shot
+		core_link_read_dpcd(link, DP_SUPPORTED_LINK_RATES,
+							supported_link_rates, sizeof(supported_link_rates));
+
+		link->dpcd_caps.link_rate_set = 0;
+		for (entry = 0; entry < 16; entry += 2) {
+			// DPCD register reports per-lane link rate = 16-bit link rate capability
+			// value X 200 kHz. Need multipler to find link rate in kHz.
+			link_rate_in_khz = (supported_link_rates[entry+1] * 0x100 +
+										supported_link_rates[entry]) * 200;
+
+			if (link_rate_in_khz != 0) {
+				link_rate = linkRateInKHzToLinkRateMultiplier(link_rate_in_khz);
+				if (link->reported_link_cap.link_rate < link_rate) {
+					link->reported_link_cap.link_rate = link_rate;
+
+					switch (link_rate) {
+					case LINK_RATE_LOW:
+						link_rate_set = 1;
+						break;
+					case LINK_RATE_RATE_2:
+						link_rate_set = 2;
+						break;
+					case LINK_RATE_RATE_3:
+						link_rate_set = 3;
+						break;
+					case LINK_RATE_HIGH:
+						link_rate_set = 4;
+						break;
+					case LINK_RATE_RBR2:
+						link_rate_set = 5;
+						break;
+					case LINK_RATE_RATE_6:
+						link_rate_set = 6;
+						break;
+					case LINK_RATE_HIGH2:
+						link_rate_set = 7;
+						break;
+					case LINK_RATE_HIGH3:
+						link_rate_set = 8;
+						break;
+					default:
+						link_rate_set = 0;
+						break;
+					}
+
+					if (link->dpcd_caps.link_rate_set < link_rate_set)
+						link->dpcd_caps.link_rate_set = link_rate_set;
+				}
+			}
+		}
+	}
 	link->verified_link_cap = link->reported_link_cap;
 }
 

commit a6729a5a406a885c749bc05d73c06e6cd9bd6211
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Fri Dec 7 16:12:09 2018 -0500

    drm/amd/display: fix CTS 4.2.2.8
    
    [Why]
    1-Test equipment does not reset test automation flag after completing
    current test causing the next test to fail.
    2.When test request is empty, we shouldn't ack the test.
    
    [How]
    1-Driver should always clear test equipment automation test request
    when request is completed.
    2-Driver should clear test equipement test automation if driver does
    not complete the request.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 568fdc9423e6..cf9362704d12 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2002,11 +2002,7 @@ static void handle_automated_test(struct dc_link *link)
 		dp_test_send_phy_test_pattern(link);
 		test_response.bits.ACK = 1;
 	}
-	if (!test_request.raw)
-		/* no requests, revert all test signals
-		 * TODO: revert all test signals
-		 */
-		test_response.bits.ACK = 1;
+
 	/* send request acknowledgment */
 	if (test_response.bits.ACK)
 		core_link_write_dpcd(

commit 380604e27bc9c26ce64a83044aa1ea76ffd28caf
Author: Ken Chalmers <ken.chalmers@amd.com>
Date:   Tue Nov 6 14:24:12 2018 -0500

    drm/amd/display: Use 100 Hz precision for pipe pixel clocks
    
    [Why]
    Users would like more accurate pixel clocks, especially for fractional
    "TV" frame rates like 59.94 Hz.
    
    [How]
    Store and communicate pixel clocks with 100 Hz accuracy from
    dc_crtc_timing through to BIOS command table setpixelclock call.
    
    Signed-off-by: Ken Chalmers <ken.chalmers@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 39562c93808d..568fdc9423e6 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1542,7 +1542,7 @@ static uint32_t bandwidth_in_kbps_from_timing(
 
 	ASSERT(bits_per_channel != 0);
 
-	kbps = timing->pix_clk_khz;
+	kbps = timing->pix_clk_100hz / 10;
 	kbps *= bits_per_channel;
 
 	if (timing->flags.Y_ONLY != 1) {
@@ -1584,7 +1584,7 @@ bool dp_validate_mode_timing(
 	const struct dc_link_settings *link_setting;
 
 	/*always DP fail safe mode*/
-	if (timing->pix_clk_khz == (uint32_t) 25175 &&
+	if ((timing->pix_clk_100hz / 10) == (uint32_t) 25175 &&
 		timing->h_addressable == (uint32_t) 640 &&
 		timing->v_addressable == (uint32_t) 480)
 		return true;

commit ceb3dbb4690db8377ad127a5666cd4775d9f70f4
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Fri Nov 9 09:21:21 2018 -0500

    drm/amd/display: remove sink reference in dc_stream_state
    
    [why]
    dc_stream_state containing a pointer to sink is poor design.
    Sink describes the display, and the specifications or capabilities
    it has.  That information is irrelevant for dc_stream_state, which describes
    hardware state, and is generally used for hardware programming.  It
    could further be argued that dc_sink itself is just a convenience dc
    provides, and DC should be perfectly capable of programming hardware
    without any dc_sinks (for example, emulated sinks).
    
    [how]
    Phase 1:
    Deprecate use of dc_sink pointer in dc_stream.  Most references are trivial
    to remove, but some call sites are risky (such as is_timing_changed) with
    no obvious logical replacement.  These will be removed in follow up change.
    
    Add dc_link pointer to dc_stream.  This is the typical reason DC really needed
    sink pointer, and most call sites are replaced with this.
    
    DMs also need minor updates, as all 3 DMs leverage stream->sink for
    some functionality.  this is replaced instead by a pointer to private data
    inside dc_stream_state, which is used by DMs as a quality of life improvment
    for some key functionality.  it allows DMs to set pointers have to their own objects
    which associate OS objects to dc_stream_states (such as DisplayTarget
    and amdgpu_dm_connector).  Without the private pointer, DMs would be
    forced to perform a lookup for callbacks.
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: David Francis <David.Francis@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0caacb60b02f..39562c93808d 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1634,7 +1634,7 @@ void decide_link_settings(struct dc_stream_state *stream,
 
 	req_bw = bandwidth_in_kbps_from_timing(&stream->timing);
 
-	link = stream->sink->link;
+	link = stream->link;
 
 	/* if preferred is specified through AMDDP, use it, if it's enough
 	 * to drive the mode
@@ -1656,7 +1656,7 @@ void decide_link_settings(struct dc_stream_state *stream,
 	}
 
 	/* EDP use the link cap setting */
-	if (stream->sink->sink_signal == SIGNAL_TYPE_EDP) {
+	if (link->connector_signal == SIGNAL_TYPE_EDP) {
 		*link_setting = link->verified_link_cap;
 		return;
 	}
@@ -2621,7 +2621,7 @@ bool dc_link_dp_set_test_pattern(
 	memset(&training_pattern, 0, sizeof(training_pattern));
 
 	for (i = 0; i < MAX_PIPES; i++) {
-		if (pipes[i].stream->sink->link == link) {
+		if (pipes[i].stream->link == link) {
 			pipe_ctx = &pipes[i];
 			break;
 		}

commit 99b922f9ed6a6313c0d2247cde8aa1e4a0bd67e4
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Dec 5 12:14:45 2018 -0500

    drm/amd/display: validate extended dongle caps
    
    [why]
    Some dongle doesn't have a valid extended dongle caps,
    but we still set the extended dongle caps to be valid.
    This causes validation fails for all timing.
    
    [how]
    If no dp_hdmi_max_pixel_clk is provided,
    don't use extended dongle caps.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Abdoulaye Berthe <Abdoulaye.Berthe@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0999102e7130..0caacb60b02f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2247,7 +2247,8 @@ static void get_active_converter_info(
 					translate_dpcd_max_bpc(
 						hdmi_color_caps.bits.MAX_BITS_PER_COLOR_COMPONENT);
 
-				link->dpcd_caps.dongle_caps.extendedCapValid = true;
+				if (link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk != 0)
+					link->dpcd_caps.dongle_caps.extendedCapValid = true;
 			}
 
 			break;

commit 1ae62f3114a95982f2f4cbe5c7bd14b81233597a
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Mon Dec 3 17:26:15 2018 -0500

    drm/amd/display: verify lane status before exiting verify link cap
    
    [why]
    DP LL CTS1.4 4.3.2.1 test failure.
    
    [how]
    The failure is caused by not handling DP link loss
    hpd short pusle during set mode. The change is to read link status
    before set mode link training. If link is lost, re-verify link caps.
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 849a3a3032f7..0999102e7130 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1089,6 +1089,121 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 	return max_link_cap;
 }
 
+static enum dc_status read_hpd_rx_irq_data(
+	struct dc_link *link,
+	union hpd_irq_data *irq_data)
+{
+	static enum dc_status retval;
+
+	/* The HW reads 16 bytes from 200h on HPD,
+	 * but if we get an AUX_DEFER, the HW cannot retry
+	 * and this causes the CTS tests 4.3.2.1 - 3.2.4 to
+	 * fail, so we now explicitly read 6 bytes which is
+	 * the req from the above mentioned test cases.
+	 *
+	 * For DP 1.4 we need to read those from 2002h range.
+	 */
+	if (link->dpcd_caps.dpcd_rev.raw < DPCD_REV_14)
+		retval = core_link_read_dpcd(
+			link,
+			DP_SINK_COUNT,
+			irq_data->raw,
+			sizeof(union hpd_irq_data));
+	else {
+		/* Read 14 bytes in a single read and then copy only the required fields.
+		 * This is more efficient than doing it in two separate AUX reads. */
+
+		uint8_t tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI + 1];
+
+		retval = core_link_read_dpcd(
+			link,
+			DP_SINK_COUNT_ESI,
+			tmp,
+			sizeof(tmp));
+
+		if (retval != DC_OK)
+			return retval;
+
+		irq_data->bytes.sink_cnt.raw = tmp[DP_SINK_COUNT_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.device_service_irq.raw = tmp[DP_DEVICE_SERVICE_IRQ_VECTOR_ESI0 - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane01_status.raw = tmp[DP_LANE0_1_STATUS_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane23_status.raw = tmp[DP_LANE2_3_STATUS_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane_status_updated.raw = tmp[DP_LANE_ALIGN_STATUS_UPDATED_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.sink_status.raw = tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI];
+	}
+
+	return retval;
+}
+
+static bool hpd_rx_irq_check_link_loss_status(
+	struct dc_link *link,
+	union hpd_irq_data *hpd_irq_dpcd_data)
+{
+	uint8_t irq_reg_rx_power_state = 0;
+	enum dc_status dpcd_result = DC_ERROR_UNEXPECTED;
+	union lane_status lane_status;
+	uint32_t lane;
+	bool sink_status_changed;
+	bool return_code;
+
+	sink_status_changed = false;
+	return_code = false;
+
+	if (link->cur_link_settings.lane_count == 0)
+		return return_code;
+
+	/*1. Check that Link Status changed, before re-training.*/
+
+	/*parse lane status*/
+	for (lane = 0; lane < link->cur_link_settings.lane_count; lane++) {
+		/* check status of lanes 0,1
+		 * changed DpcdAddress_Lane01Status (0x202)
+		 */
+		lane_status.raw = get_nibble_at_index(
+			&hpd_irq_dpcd_data->bytes.lane01_status.raw,
+			lane);
+
+		if (!lane_status.bits.CHANNEL_EQ_DONE_0 ||
+			!lane_status.bits.CR_DONE_0 ||
+			!lane_status.bits.SYMBOL_LOCKED_0) {
+			/* if one of the channel equalization, clock
+			 * recovery or symbol lock is dropped
+			 * consider it as (link has been
+			 * dropped) dp sink status has changed
+			 */
+			sink_status_changed = true;
+			break;
+		}
+	}
+
+	/* Check interlane align.*/
+	if (sink_status_changed ||
+		!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.INTERLANE_ALIGN_DONE) {
+
+		DC_LOG_HW_HPD_IRQ("%s: Link Status changed.\n", __func__);
+
+		return_code = true;
+
+		/*2. Check that we can handle interrupt: Not in FS DOS,
+		 *  Not in "Display Timeout" state, Link is trained.
+		 */
+		dpcd_result = core_link_read_dpcd(link,
+			DP_SET_POWER,
+			&irq_reg_rx_power_state,
+			sizeof(irq_reg_rx_power_state));
+
+		if (dpcd_result != DC_OK) {
+			DC_LOG_HW_HPD_IRQ("%s: DPCD read failed to obtain power state.\n",
+				__func__);
+		} else {
+			if (irq_reg_rx_power_state != DP_SET_POWER_D0)
+				return_code = false;
+		}
+	}
+
+	return return_code;
+}
+
 bool dp_verify_link_cap(
 	struct dc_link *link,
 	struct dc_link_settings *known_limit_link_setting,
@@ -1104,12 +1219,14 @@ bool dp_verify_link_cap(
 	struct clock_source *dp_cs;
 	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
 	enum link_training_result status;
+	union hpd_irq_data irq_data;
 
 	if (link->dc->debug.skip_detection_link_training) {
 		link->verified_link_cap = *known_limit_link_setting;
 		return true;
 	}
 
+	memset(&irq_data, 0, sizeof(irq_data));
 	success = false;
 	skip_link_training = false;
 
@@ -1168,9 +1285,15 @@ bool dp_verify_link_cap(
 				(*fail_count)++;
 		}
 
-		if (success)
+		if (success) {
 			link->verified_link_cap = *cur;
-
+			udelay(1000);
+			if (read_hpd_rx_irq_data(link, &irq_data) == DC_OK)
+				if (hpd_rx_irq_check_link_loss_status(
+						link,
+						&irq_data))
+					(*fail_count)++;
+		}
 		/* always disable the link before trying another
 		 * setting or before returning we'll enable it later
 		 * based on the actual mode we're driving
@@ -1572,122 +1695,6 @@ void decide_link_settings(struct dc_stream_state *stream,
 }
 
 /*************************Short Pulse IRQ***************************/
-
-static bool hpd_rx_irq_check_link_loss_status(
-	struct dc_link *link,
-	union hpd_irq_data *hpd_irq_dpcd_data)
-{
-	uint8_t irq_reg_rx_power_state = 0;
-	enum dc_status dpcd_result = DC_ERROR_UNEXPECTED;
-	union lane_status lane_status;
-	uint32_t lane;
-	bool sink_status_changed;
-	bool return_code;
-
-	sink_status_changed = false;
-	return_code = false;
-
-	if (link->cur_link_settings.lane_count == 0)
-		return return_code;
-
-	/*1. Check that Link Status changed, before re-training.*/
-
-	/*parse lane status*/
-	for (lane = 0; lane < link->cur_link_settings.lane_count; lane++) {
-		/* check status of lanes 0,1
-		 * changed DpcdAddress_Lane01Status (0x202)
-		 */
-		lane_status.raw = get_nibble_at_index(
-			&hpd_irq_dpcd_data->bytes.lane01_status.raw,
-			lane);
-
-		if (!lane_status.bits.CHANNEL_EQ_DONE_0 ||
-			!lane_status.bits.CR_DONE_0 ||
-			!lane_status.bits.SYMBOL_LOCKED_0) {
-			/* if one of the channel equalization, clock
-			 * recovery or symbol lock is dropped
-			 * consider it as (link has been
-			 * dropped) dp sink status has changed
-			 */
-			sink_status_changed = true;
-			break;
-		}
-	}
-
-	/* Check interlane align.*/
-	if (sink_status_changed ||
-		!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.INTERLANE_ALIGN_DONE) {
-
-		DC_LOG_HW_HPD_IRQ("%s: Link Status changed.\n", __func__);
-
-		return_code = true;
-
-		/*2. Check that we can handle interrupt: Not in FS DOS,
-		 *  Not in "Display Timeout" state, Link is trained.
-		 */
-		dpcd_result = core_link_read_dpcd(link,
-			DP_SET_POWER,
-			&irq_reg_rx_power_state,
-			sizeof(irq_reg_rx_power_state));
-
-		if (dpcd_result != DC_OK) {
-			DC_LOG_HW_HPD_IRQ("%s: DPCD read failed to obtain power state.\n",
-				__func__);
-		} else {
-			if (irq_reg_rx_power_state != DP_SET_POWER_D0)
-				return_code = false;
-		}
-	}
-
-	return return_code;
-}
-
-static enum dc_status read_hpd_rx_irq_data(
-	struct dc_link *link,
-	union hpd_irq_data *irq_data)
-{
-	static enum dc_status retval;
-
-	/* The HW reads 16 bytes from 200h on HPD,
-	 * but if we get an AUX_DEFER, the HW cannot retry
-	 * and this causes the CTS tests 4.3.2.1 - 3.2.4 to
-	 * fail, so we now explicitly read 6 bytes which is
-	 * the req from the above mentioned test cases.
-	 *
-	 * For DP 1.4 we need to read those from 2002h range.
-	 */
-	if (link->dpcd_caps.dpcd_rev.raw < DPCD_REV_14)
-		retval = core_link_read_dpcd(
-			link,
-			DP_SINK_COUNT,
-			irq_data->raw,
-			sizeof(union hpd_irq_data));
-	else {
-		/* Read 14 bytes in a single read and then copy only the required fields.
-		 * This is more efficient than doing it in two separate AUX reads. */
-
-		uint8_t tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI + 1];
-
-		retval = core_link_read_dpcd(
-			link,
-			DP_SINK_COUNT_ESI,
-			tmp,
-			sizeof(tmp));
-
-		if (retval != DC_OK)
-			return retval;
-
-		irq_data->bytes.sink_cnt.raw = tmp[DP_SINK_COUNT_ESI - DP_SINK_COUNT_ESI];
-		irq_data->bytes.device_service_irq.raw = tmp[DP_DEVICE_SERVICE_IRQ_VECTOR_ESI0 - DP_SINK_COUNT_ESI];
-		irq_data->bytes.lane01_status.raw = tmp[DP_LANE0_1_STATUS_ESI - DP_SINK_COUNT_ESI];
-		irq_data->bytes.lane23_status.raw = tmp[DP_LANE2_3_STATUS_ESI - DP_SINK_COUNT_ESI];
-		irq_data->bytes.lane_status_updated.raw = tmp[DP_LANE_ALIGN_STATUS_UPDATED_ESI - DP_SINK_COUNT_ESI];
-		irq_data->bytes.sink_status.raw = tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI];
-	}
-
-	return retval;
-}
-
 static bool allow_hpd_rx_irq(const struct dc_link *link)
 {
 	/*

commit 242b0c8ffa5ea5c089b00a605747a1458bcb9c30
Author: abdoulaye berthe <abdoulaye.berthe@amd.com>
Date:   Fri Nov 2 12:07:46 2018 -0400

    drm/amd/display: CTS 4.2.2.7
    
    [Why]
    Failure to read Detailed Capabilities Info.
    
    [How]
    Read Detailed Capbilities Info 80h-08Fh.
    
    Signed-off-by: abdoulaye berthe <abdoulaye.berthe@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4d1f8ac069c1..849a3a3032f7 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2196,7 +2196,7 @@ static void get_active_converter_info(
 	}
 
 	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_11) {
-		uint8_t det_caps[4];
+		uint8_t det_caps[16]; /* CTS 4.2.2.7 expects source to read Detailed Capabilities Info : 00080h-0008F.*/
 		union dwnstream_port_caps_byte0 *port_caps =
 			(union dwnstream_port_caps_byte0 *)det_caps;
 		core_link_read_dpcd(link, DP_DOWNSTREAM_PORT_0,

commit 818832bf2bc0d86d2a114d3baadfae12a40f7f36
Author: Xiaodong Yan <Xiaodong.Yan@amd.com>
Date:   Wed Oct 24 02:08:53 2018 +0800

    drm/amd/display: retry 3 times before successfully reading
    
    DPCD Extended Receiver Capability Field
    
    [Why]
    1.dpcd extended receiver capability sometimes read fail,
      and corrupted data leads to sink caps is not correct.
    2.sometimes sink reply ack with fewer data
    
    [How]
      check the return value of core_link_read_dpcd,
      try to read again when failure happens
    
    Signed-off-by: Xiaodong Yan <Xiaodong.Yan@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Acked-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index d91df5ef0cb3..4d1f8ac069c1 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2371,11 +2371,22 @@ static bool retrieve_link_cap(struct dc_link *link)
 			dpcd_data[DP_TRAINING_AUX_RD_INTERVAL];
 
 		if (aux_rd_interval.bits.EXT_RECIEVER_CAP_FIELD_PRESENT == 1) {
-			core_link_read_dpcd(
+			uint8_t ext_cap_data[16];
+
+			memset(ext_cap_data, '\0', sizeof(ext_cap_data));
+			for (i = 0; i < read_dpcd_retry_cnt; i++) {
+				status = core_link_read_dpcd(
 				link,
 				DP_DP13_DPCD_REV,
-				dpcd_data,
-				sizeof(dpcd_data));
+				ext_cap_data,
+				sizeof(ext_cap_data));
+				if (status == DC_OK) {
+					memcpy(dpcd_data, ext_cap_data, sizeof(dpcd_data));
+					break;
+				}
+			}
+			if (status != DC_OK)
+				dm_error("%s: Read extend caps data failed, use cap from dpcd 0.\n", __func__);
 		}
 	}
 

commit 98e6436d3af5fef7ca9b59d865dd5807ede36fb9
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Tue Aug 21 14:40:28 2018 -0500

    drm/amd/display: Refactor FreeSync module
    
    Remove dependency on internal sink map and instead
    use existing stream and plane state
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a7553b6d59c2..d91df5ef0cb3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2389,6 +2389,9 @@ static bool retrieve_link_cap(struct dc_link *link)
 
 	dp_wa_power_up_0010FA(link, dpcd_data, sizeof(dpcd_data));
 
+	down_strm_port_count.raw = dpcd_data[DP_DOWN_STREAM_PORT_COUNT -
+				 DP_DPCD_REV];
+
 	link->dpcd_caps.allow_invalid_MSA_timing_param =
 		down_strm_port_count.bits.IGNORE_MSA_TIMING_PARAM;
 

commit ad830e7ab1847f4a014c04496b2581a7497b204f
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jul 18 15:25:34 2018 -0400

    drm/amd/display: add vbios table check for enabling dp ss
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Eric Bernstein <Eric.Bernstein@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 160841da72a7..a7553b6d59c2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -953,7 +953,10 @@ enum link_training_result dc_link_dp_perform_link_training(
 	 * LINK_SPREAD_05_DOWNSPREAD_30KHZ :
 	 * LINK_SPREAD_DISABLED;
 	 */
-	lt_settings.link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
+	if (link->dp_ss_off)
+		lt_settings.link_settings.link_spread = LINK_SPREAD_DISABLED;
+	else
+		lt_settings.link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
 
 	/* 1. set link rate, lane count and spread*/
 	dpcd_set_link_settings(link, &lt_settings);

commit 9315e2399a2cdc236e8d42c1a21fb1071cdad03d
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Mon Jul 16 11:21:12 2018 -0400

    drm/amd/display: Fix DP HBR2 Eye Diagram Pattern on Carrizo
    
    [why] dp hbr2 eye diagram pattern for raven asic is not stabled.
    workaround is to use tp4 pattern. But this should not be
    applied to asic before raven.
    
    [how] add new bool varilable in asic caps. for raven asic,
    use the workaround. for carrizo, vega, do not use workaround.
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 58ee9aad13fb..160841da72a7 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1784,12 +1784,10 @@ static void dp_test_send_link_training(struct dc_link *link)
 	dp_retrain_link_dp_test(link, &link_settings, false);
 }
 
-/* TODO hbr2 compliance eye output is unstable
+/* TODO Raven hbr2 compliance eye output is unstable
  * (toggling on and off) with debugger break
  * This caueses intermittent PHY automation failure
  * Need to look into the root cause */
-static uint8_t force_tps4_for_cp2520 = 1;
-
 static void dp_test_send_phy_test_pattern(struct dc_link *link)
 {
 	union phy_test_pattern dpcd_test_pattern;
@@ -1849,13 +1847,13 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 		break;
 	case PHY_TEST_PATTERN_CP2520_1:
 		/* CP2520 pattern is unstable, temporarily use TPS4 instead */
-		test_pattern = (force_tps4_for_cp2520 == 1) ?
+		test_pattern = (link->dc->caps.force_dp_tps4_for_cp2520 == 1) ?
 				DP_TEST_PATTERN_TRAINING_PATTERN4 :
 				DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
 		break;
 	case PHY_TEST_PATTERN_CP2520_2:
 		/* CP2520 pattern is unstable, temporarily use TPS4 instead */
-		test_pattern = (force_tps4_for_cp2520 == 1) ?
+		test_pattern = (link->dc->caps.force_dp_tps4_for_cp2520 == 1) ?
 				DP_TEST_PATTERN_TRAINING_PATTERN4 :
 				DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
 		break;

commit 824474ba38e27ccacc9d2dd066f780e9b3c2ad78
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Fri Jul 13 18:00:06 2018 -0400

    drm/amd/display: Retry link training again
    
    [Why]
    Some receivers seem to fail the first link training but are good on
    subsequent tries. We want to retry link training again. This fixes
    HTC vive pro not lighting up after being disabled.
    
    [How]
    Check if the link training passed without fall back if this is not
    the case then we retry link training.
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 9d901ca70588..58ee9aad13fb 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1088,7 +1088,8 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 
 bool dp_verify_link_cap(
 	struct dc_link *link,
-	struct dc_link_settings *known_limit_link_setting)
+	struct dc_link_settings *known_limit_link_setting,
+	int *fail_count)
 {
 	struct dc_link_settings max_link_cap = {0};
 	struct dc_link_settings cur_link_setting = {0};
@@ -1160,6 +1161,8 @@ bool dp_verify_link_cap(
 							skip_video_pattern);
 			if (status == LINK_TRAINING_SUCCESS)
 				success = true;
+			else
+				(*fail_count)++;
 		}
 
 		if (success)

commit cfd84fd36531b2f1de01b3530b6953ed34ed2c95
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Thu Jul 12 10:35:01 2018 -0400

    drm/amd/display: separate dc_debug into dc_debug_options and dc_debug data
    
    [why]
    confusing as to which part of debug is informational, and which part causes behavioral change
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b8e6db4382ec..9d901ca70588 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1029,7 +1029,7 @@ enum link_training_result dc_link_dp_perform_link_training(
 			lt_settings.lane_settings[0].PRE_EMPHASIS);
 
 	if (status != LINK_TRAINING_SUCCESS)
-		link->ctx->dc->debug.debug_data.ltFailCount++;
+		link->ctx->dc->debug_data.ltFailCount++;
 
 	return status;
 }

commit aafded888514ba2c6b613548081dedf289a71287
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Wed Jul 11 15:31:24 2018 -0400

    drm/amd/display: allow diags to skip initial link training
    
    [why]
    diag specify what the full config and is only concerned about pass/fail at the end
    
    having inter-op code like verifiying we can actually train at reported link rate
    slows down diag test and add complexity we don't need
    
    [how]
    add dc_debug option to skip capability link trianing
    
    also  remove hbr in function name as verify is not specific to hbr
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Ken Chalmers <ken.chalmers@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 474cd3e01752..b8e6db4382ec 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1086,7 +1086,7 @@ static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 	return max_link_cap;
 }
 
-bool dp_hbr_verify_link_cap(
+bool dp_verify_link_cap(
 	struct dc_link *link,
 	struct dc_link_settings *known_limit_link_setting)
 {
@@ -1101,6 +1101,11 @@ bool dp_hbr_verify_link_cap(
 	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
 	enum link_training_result status;
 
+	if (link->dc->debug.skip_detection_link_training) {
+		link->verified_link_cap = *known_limit_link_setting;
+		return true;
+	}
+
 	success = false;
 	skip_link_training = false;
 

commit 9a6a8075bd439115b41468eaccdcb5e463196fb5
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Fri Jul 6 13:40:33 2018 -0400

    drm/amd/display: Fix some checkpatch.pl errors and warnings in dc_link_dp.c
    
    [Why]
    Any Linux kernel code should pass checkpatch.pl with no errors and
    little, if any, warning.
    
    [How]
    Fixing some spacing errors and warnings.
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 701a882505e9..474cd3e01752 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -39,7 +39,7 @@ static bool decide_fallback_link_setting(
 		struct dc_link_settings initial_link_settings,
 		struct dc_link_settings *current_link_setting,
 		enum link_training_result training_result);
-static struct dc_link_settings get_common_supported_link_settings (
+static struct dc_link_settings get_common_supported_link_settings(
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b);
 
@@ -94,8 +94,8 @@ static void dpcd_set_link_settings(
 	uint8_t rate = (uint8_t)
 	(lt_settings->link_settings.link_rate);
 
-	union down_spread_ctrl downspread = {{0}};
-	union lane_count_set lane_count_set = {{0}};
+	union down_spread_ctrl downspread = { {0} };
+	union lane_count_set lane_count_set = { {0} };
 	uint8_t link_set_buffer[2];
 
 	downspread.raw = (uint8_t)
@@ -165,11 +165,11 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	const struct link_training_settings *lt_settings,
 	enum hw_dp_training_pattern pattern)
 {
-	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
+	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = { { {0} } };
 	const uint32_t dpcd_base_lt_offset =
 	DP_TRAINING_PATTERN_SET;
 	uint8_t dpcd_lt_buffer[5] = {0};
-	union dpcd_training_pattern dpcd_pattern = {{0}};
+	union dpcd_training_pattern dpcd_pattern = { {0} };
 	uint32_t lane;
 	uint32_t size_in_bytes;
 	bool edp_workaround = false; /* TODO link_prop.INTERNAL */
@@ -233,7 +233,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 			link,
 			DP_TRAINING_PATTERN_SET,
 			&dpcd_pattern.raw,
-			sizeof(dpcd_pattern.raw) );
+			sizeof(dpcd_pattern.raw));
 
 		core_link_write_dpcd(
 			link,
@@ -247,7 +247,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 				link,
 				dpcd_base_lt_offset,
 				dpcd_lt_buffer,
-				size_in_bytes + sizeof(dpcd_pattern.raw) );
+				size_in_bytes + sizeof(dpcd_pattern.raw));
 
 	link->cur_lane_setting = lt_settings->lane_settings[0];
 }
@@ -429,8 +429,8 @@ static void get_lane_status_and_drive_settings(
 	struct link_training_settings *req_settings)
 {
 	uint8_t dpcd_buf[6] = {0};
-	union lane_adjust dpcd_lane_adjust[LANE_COUNT_DP_MAX] = {{{0}}};
-	struct link_training_settings request_settings = {{0}};
+	union lane_adjust dpcd_lane_adjust[LANE_COUNT_DP_MAX] = { { {0} } };
+	struct link_training_settings request_settings = { {0} };
 	uint32_t lane;
 
 	memset(req_settings, '\0', sizeof(struct link_training_settings));
@@ -652,7 +652,7 @@ static bool perform_post_lt_adj_req_sequence(
 
 			if (req_drv_setting_changed) {
 				update_drive_settings(
-					lt_settings,req_settings);
+					lt_settings, req_settings);
 
 				dc_link_dp_set_drive_settings(link,
 						lt_settings);
@@ -725,8 +725,8 @@ static enum link_training_result perform_channel_equalization_sequence(
 	enum hw_dp_training_pattern hw_tr_pattern;
 	uint32_t retries_ch_eq;
 	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
-	union lane_align_status_updated dpcd_lane_status_updated = {{0}};
-	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = {{{0}}};
+	union lane_align_status_updated dpcd_lane_status_updated = { {0} };
+	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = { { {0} } };
 
 	hw_tr_pattern = get_supported_tp(link);
 
@@ -1186,7 +1186,7 @@ bool dp_hbr_verify_link_cap(
 	return success;
 }
 
-static struct dc_link_settings get_common_supported_link_settings (
+static struct dc_link_settings get_common_supported_link_settings(
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b)
 {
@@ -1432,6 +1432,7 @@ static uint32_t bandwidth_in_kbps_from_link_settings(
 
 	uint32_t lane_count  = link_setting->lane_count;
 	uint32_t kbps = link_rate_in_kbps;
+
 	kbps *= lane_count;
 	kbps *= 8;   /* 8 bits per byte*/
 
@@ -1449,9 +1450,9 @@ bool dp_validate_mode_timing(
 	const struct dc_link_settings *link_setting;
 
 	/*always DP fail safe mode*/
-	if (timing->pix_clk_khz == (uint32_t)25175 &&
-		timing->h_addressable == (uint32_t)640 &&
-		timing->v_addressable == (uint32_t)480)
+	if (timing->pix_clk_khz == (uint32_t) 25175 &&
+		timing->h_addressable == (uint32_t) 640 &&
+		timing->v_addressable == (uint32_t) 480)
 		return true;
 
 	/* We always use verified link settings */
@@ -2001,7 +2002,7 @@ static void handle_automated_test(struct dc_link *link)
 
 bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd_irq_dpcd_data, bool *out_link_loss)
 {
-	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
+	union hpd_irq_data hpd_irq_dpcd_data = { { { {0} } } };
 	union device_service_irq device_service_clear = { { 0 } };
 	enum dc_status result;
 

commit d6e75df4e5e10362a140c5c6ce8ced3612296ae3
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Fri Jun 22 16:51:47 2018 -0400

    drm/amd/display: add new dc debug structure to track debug data
    
    [why]
    Some DTN tests still failing @ 2%  Need to reduce.
    
    [how]
    add instrumentation code to driver so we can get more information from failed runs.
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 54e396452345..701a882505e9 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1028,6 +1028,9 @@ enum link_training_result dc_link_dp_perform_link_training(
 			lt_settings.lane_settings[0].VOLTAGE_SWING,
 			lt_settings.lane_settings[0].PRE_EMPHASIS);
 
+	if (status != LINK_TRAINING_SUCCESS)
+		link->ctx->dc->debug.debug_data.ltFailCount++;
+
 	return status;
 }
 

commit 4e18814eeec946fa864723423c9a607815ede5cb
Author: Fatemeh Darbehani <fatemeh.darbehani@amd.com>
Date:   Tue Jun 26 16:40:55 2018 -0400

    drm/amd/display: Return out_link_loss from interrupt handler
    
    Signed-off-by: Fatemeh Darbehani <fatemeh.darbehani@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 6d4642bf395d..54e396452345 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1996,12 +1996,16 @@ static void handle_automated_test(struct dc_link *link)
 			sizeof(test_response));
 }
 
-bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd_irq_dpcd_data)
+bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd_irq_dpcd_data, bool *out_link_loss)
 {
 	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
 	union device_service_irq device_service_clear = { { 0 } };
 	enum dc_status result;
+
 	bool status = false;
+
+	if (out_link_loss)
+		*out_link_loss = false;
 	/* For use cases related to down stream connection status change,
 	 * PSR and device auto test, refer to function handle_sst_hpd_irq
 	 * in DAL2.1*/
@@ -2076,6 +2080,8 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 			true, LINK_TRAINING_ATTEMPTS);
 
 		status = false;
+		if (out_link_loss)
+			*out_link_loss = true;
 	}
 
 	if (link->type == dc_connection_active_dongle &&

commit 4b99affbb30027c8414a9d583777ad5c6439ae12
Author: Alvin lee <alvin.lee3@amd.com>
Date:   Tue Jun 19 15:40:09 2018 -0400

    drm/amd/display: read DP sink and DP branch hardware and firmware revision from DPCD
    
    - define new dpcd address in drm
    - implement new members in dpcd_caps to store values read from new dpcd address
    
    Signed-off-by: Alvin lee <alvin.lee3@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 84586b679d73..6d4642bf395d 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2262,6 +2262,11 @@ static void get_active_converter_info(
 
 		link->dpcd_caps.branch_hw_revision =
 			dp_hw_fw_revision.ieee_hw_rev;
+
+		memmove(
+			link->dpcd_caps.branch_fw_revision,
+			dp_hw_fw_revision.ieee_fw_rev,
+			sizeof(dp_hw_fw_revision.ieee_fw_rev));
 	}
 }
 
@@ -2317,6 +2322,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 	enum dc_status status = DC_ERROR_UNEXPECTED;
 	uint32_t read_dpcd_retry_cnt = 3;
 	int i;
+	struct dp_sink_hw_fw_revision dp_hw_fw_revision;
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
@@ -2408,6 +2414,25 @@ static bool retrieve_link_cap(struct dc_link *link)
 			(sink_id.ieee_oui[1] << 8) +
 			(sink_id.ieee_oui[2]);
 
+	memmove(
+		link->dpcd_caps.sink_dev_id_str,
+		sink_id.ieee_device_id,
+		sizeof(sink_id.ieee_device_id));
+
+	core_link_read_dpcd(
+		link,
+		DP_SINK_HW_REVISION_START,
+		(uint8_t *)&dp_hw_fw_revision,
+		sizeof(dp_hw_fw_revision));
+
+	link->dpcd_caps.sink_hw_revision =
+		dp_hw_fw_revision.ieee_hw_rev;
+
+	memmove(
+		link->dpcd_caps.sink_fw_revision,
+		dp_hw_fw_revision.ieee_fw_rev,
+		sizeof(dp_hw_fw_revision.ieee_fw_rev));
+
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 

commit d6258eaa41fc531277d44fbbee670c866025bfdc
Author: Stefan Agner <stefan@agner.ch>
Date:   Sun Jun 17 10:53:38 2018 +0200

    drm/amd/display: don't initialize result
    
    The wrong enum type is used to initialize the result, leading to a
    warning when using clang:
    drivers/gpu/drm/amd/amdgpu/../display/dc/core/dc_link_dp.c:1998:26: warning:
          implicit conversion from enumeration type 'enum ddc_result' to different
          enumeration type 'enum dc_status' [-Wenum-conversion]
            enum dc_status result = DDC_RESULT_UNKNOWN;
                           ~~~~~~   ^~~~~~~~~~~~~~~~~~
    1 warning generated.
    
    Initialization of result is unnecessary anyway, just drop the
    initialization.
    
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 049fc5cce1d2..84586b679d73 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2000,7 +2000,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 {
 	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
 	union device_service_irq device_service_clear = { { 0 } };
-	enum dc_status result = DDC_RESULT_UNKNOWN;
+	enum dc_status result;
 	bool status = false;
 	/* For use cases related to down stream connection status change,
 	 * PSR and device auto test, refer to function handle_sst_hpd_irq

commit 7f93c1de64693dc18afe55559f14cee6b5403c6c
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Sat Jun 9 19:33:14 2018 -0400

    drm/amd/display: add valid regoffset and NULL pointer check
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 509f265663d2..049fc5cce1d2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -3,6 +3,7 @@
 #include "dc.h"
 #include "dc_link_dp.h"
 #include "dm_helpers.h"
+#include "opp.h"
 
 #include "inc/core_types.h"
 #include "link_hwss.h"
@@ -2511,8 +2512,8 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		pipe_ctx->stream->bit_depth_params = params;
 		pipe_ctx->stream_res.opp->funcs->
 			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
-
-		pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
+		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
+			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				controller_test_pattern, color_depth);
 	}
 	break;
@@ -2524,8 +2525,8 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		pipe_ctx->stream->bit_depth_params = params;
 		pipe_ctx->stream_res.opp->funcs->
 			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
-
-		pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
+		if (pipe_ctx->stream_res.tg->funcs->set_test_pattern)
+			pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 				color_depth);
 	}

commit 3c8e4316a0bf6f65e8fbcf777abbc3b0e629e800
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Wed May 9 17:07:36 2018 -0400

    drm/amd/display: Optimize DP_SINK_STATUS_ESI range read on HPD
    
    DP_SINK_STATUS_ESI range data is not continual, but rather than
    getting it in two AUX reads, it's quicker to read more bytes in a
    AUX read and then memcpy the required fields (it's only 8 more
    bytes to read).
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1a42ee5113a9..509f265663d2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1647,22 +1647,26 @@ static enum dc_status read_hpd_rx_irq_data(
 			irq_data->raw,
 			sizeof(union hpd_irq_data));
 	else {
-		/* Read 2 bytes at this location,... */
+		/* Read 14 bytes in a single read and then copy only the required fields.
+		 * This is more efficient than doing it in two separate AUX reads. */
+
+		uint8_t tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI + 1];
+
 		retval = core_link_read_dpcd(
 			link,
 			DP_SINK_COUNT_ESI,
-			irq_data->raw,
-			2);
+			tmp,
+			sizeof(tmp));
 
 		if (retval != DC_OK)
 			return retval;
 
-		/* ... then read remaining 4 at the other location */
-		retval = core_link_read_dpcd(
-			link,
-			DP_LANE0_1_STATUS_ESI,
-			&irq_data->raw[2],
-			4);
+		irq_data->bytes.sink_cnt.raw = tmp[DP_SINK_COUNT_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.device_service_irq.raw = tmp[DP_DEVICE_SERVICE_IRQ_VECTOR_ESI0 - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane01_status.raw = tmp[DP_LANE0_1_STATUS_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane23_status.raw = tmp[DP_LANE2_3_STATUS_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.lane_status_updated.raw = tmp[DP_LANE_ALIGN_STATUS_UPDATED_ESI - DP_SINK_COUNT_ESI];
+		irq_data->bytes.sink_status.raw = tmp[DP_SINK_STATUS_ESI - DP_SINK_COUNT_ESI];
 	}
 
 	return retval;

commit 8ca809008571f87e7d73535175601328175dac81
Author: Anthony Koo <Anthony.Koo@amd.com>
Date:   Tue May 8 17:08:57 2018 -0400

    drm/amd/display: add DPCD read for Sink ieee OUI
    
    Signed-off-by: Anthony Koo <Anthony.Koo@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7857cb42b3e6..1a42ee5113a9 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2305,6 +2305,7 @@ static bool retrieve_link_cap(struct dc_link *link)
 {
 	uint8_t dpcd_data[DP_ADAPTER_CAP - DP_DPCD_REV + 1];
 
+	struct dp_device_vendor_id sink_id;
 	union down_stream_port_count down_strm_port_count;
 	union edp_configuration_cap edp_config_cap;
 	union dp_downstream_port_present ds_port = { 0 };
@@ -2391,6 +2392,17 @@ static bool retrieve_link_cap(struct dc_link *link)
 			&link->dpcd_caps.sink_count.raw,
 			sizeof(link->dpcd_caps.sink_count.raw));
 
+	/* read sink ieee oui */
+	core_link_read_dpcd(link,
+			DP_SINK_OUI,
+			(uint8_t *)(&sink_id),
+			sizeof(sink_id));
+
+	link->dpcd_caps.sink_dev_id =
+			(sink_id.ieee_oui[0] << 16) +
+			(sink_id.ieee_oui[1] << 8) +
+			(sink_id.ieee_oui[2]);
+
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 

commit c733e40c74457ad6aa56cc8b3318e829b8274bef
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Wed May 9 13:11:35 2018 -0400

    drm/amd/display: Read DP_SINK_COUNT_ESI range on HPD for DP 1.4
    
    DP 1.4 compliance now requires that registers at DP_SINK_COUNT_ESI range
    (0x2002-0x2003, 0x200c-0x200f) are read instead of DP_SINK_COUNT range
    (0x200-0x2005.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3fcb67cbc6cc..7857cb42b3e6 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1630,17 +1630,42 @@ static enum dc_status read_hpd_rx_irq_data(
 	struct dc_link *link,
 	union hpd_irq_data *irq_data)
 {
+	static enum dc_status retval;
+
 	/* The HW reads 16 bytes from 200h on HPD,
 	 * but if we get an AUX_DEFER, the HW cannot retry
 	 * and this causes the CTS tests 4.3.2.1 - 3.2.4 to
 	 * fail, so we now explicitly read 6 bytes which is
 	 * the req from the above mentioned test cases.
+	 *
+	 * For DP 1.4 we need to read those from 2002h range.
 	 */
-	return core_link_read_dpcd(
-	link,
-	DP_SINK_COUNT,
-	irq_data->raw,
-	sizeof(union hpd_irq_data));
+	if (link->dpcd_caps.dpcd_rev.raw < DPCD_REV_14)
+		retval = core_link_read_dpcd(
+			link,
+			DP_SINK_COUNT,
+			irq_data->raw,
+			sizeof(union hpd_irq_data));
+	else {
+		/* Read 2 bytes at this location,... */
+		retval = core_link_read_dpcd(
+			link,
+			DP_SINK_COUNT_ESI,
+			irq_data->raw,
+			2);
+
+		if (retval != DC_OK)
+			return retval;
+
+		/* ... then read remaining 4 at the other location */
+		retval = core_link_read_dpcd(
+			link,
+			DP_LANE0_1_STATUS_ESI,
+			&irq_data->raw[2],
+			4);
+	}
+
+	return retval;
 }
 
 static bool allow_hpd_rx_irq(const struct dc_link *link)

commit 794550c6eaf791bfd2e8d70e11aa56fdd6361725
Author: Nikola Cornij <nikola.cornij@amd.com>
Date:   Mon May 7 15:35:15 2018 -0400

    drm/amd/display: Read DPCD link caps up to and including DP_ADAPTER_CAP
    
    DP 1.4 compliance requires 16 bytes to be read when reading link caps,
    i.e. it requires DP_ADAPTER_CAP to be included. Included it for all DP
    versions because reading more than required won't fail.
    
    Signed-off-by: Nikola Cornij <nikola.cornij@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7d609c71394b..3fcb67cbc6cc 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2278,7 +2278,7 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 
 static bool retrieve_link_cap(struct dc_link *link)
 {
-	uint8_t dpcd_data[DP_TRAINING_AUX_RD_INTERVAL - DP_DPCD_REV + 1];
+	uint8_t dpcd_data[DP_ADAPTER_CAP - DP_DPCD_REV + 1];
 
 	union down_stream_port_count down_strm_port_count;
 	union edp_configuration_cap edp_config_cap;

commit 50834eb488a30026de040ab5d209ca9f980ae14b
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Wed Apr 11 15:22:10 2018 -0400

    drm/amd/display: DP link validation bug for YUV422
    
    remove limit YUV422 color depth to 24bits which is
    workaround for old ASIC
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0a190c2b6898..7d609c71394b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1379,34 +1379,29 @@ static uint32_t bandwidth_in_kbps_from_timing(
 	uint32_t bits_per_channel = 0;
 	uint32_t kbps;
 
-	if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR422)
+	switch (timing->display_color_depth) {
+	case COLOR_DEPTH_666:
+		bits_per_channel = 6;
+		break;
+	case COLOR_DEPTH_888:
+		bits_per_channel = 8;
+		break;
+	case COLOR_DEPTH_101010:
+		bits_per_channel = 10;
+		break;
+	case COLOR_DEPTH_121212:
 		bits_per_channel = 12;
-	else{
-
-		switch (timing->display_color_depth) {
-
-		case COLOR_DEPTH_666:
-			bits_per_channel = 6;
-			break;
-		case COLOR_DEPTH_888:
-			bits_per_channel = 8;
-			break;
-		case COLOR_DEPTH_101010:
-			bits_per_channel = 10;
-			break;
-		case COLOR_DEPTH_121212:
-			bits_per_channel = 12;
-			break;
-		case COLOR_DEPTH_141414:
-			bits_per_channel = 14;
-			break;
-		case COLOR_DEPTH_161616:
-			bits_per_channel = 16;
-			break;
-		default:
-			break;
-		}
+		break;
+	case COLOR_DEPTH_141414:
+		bits_per_channel = 14;
+		break;
+	case COLOR_DEPTH_161616:
+		bits_per_channel = 16;
+		break;
+	default:
+		break;
 	}
+
 	ASSERT(bits_per_channel != 0);
 
 	kbps = timing->pix_clk_khz;

commit 48231fd51667a89514d0eaba893ae0743fd0877d
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Tue Nov 21 13:34:48 2017 -0500

    drm/amd/display: Use HBR2 if eDP monitor it doesn't advertise link rate
    
    Some eDP displays use the extra link rate table to advertise link rate
    support. If they do that they don't need to provide link rate through
    the usual registers. Since we don't currently have support for the extra
    link rate table default to HBR2 for the display in this.
    
    Note that this is a HACK. Ultimately we need to teach DC to use the
    extra link rate table.
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 07cc4385a7c1..0a190c2b6898 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2393,6 +2393,10 @@ bool detect_dp_sink_caps(struct dc_link *link)
 void detect_edp_sink_caps(struct dc_link *link)
 {
 	retrieve_link_cap(link);
+
+	if (link->reported_link_cap.link_rate == LINK_RATE_UNKNOWN)
+		link->reported_link_cap.link_rate = LINK_RATE_HIGH2;
+
 	link->verified_link_cap = link->reported_link_cap;
 }
 

commit cf65ebeb687678812eb3ddd5ef253bacf7ef330a
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Fri Mar 23 13:56:16 2018 -0400

    drm/amd/display: fix link bw calculation for 422 and 420 encoding
    
    Link bw required is reduced when we have chroma subsampling.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b86325bb636f..07cc4385a7c1 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1378,37 +1378,48 @@ static uint32_t bandwidth_in_kbps_from_timing(
 {
 	uint32_t bits_per_channel = 0;
 	uint32_t kbps;
-	switch (timing->display_color_depth) {
 
-	case COLOR_DEPTH_666:
-		bits_per_channel = 6;
-		break;
-	case COLOR_DEPTH_888:
-		bits_per_channel = 8;
-		break;
-	case COLOR_DEPTH_101010:
-		bits_per_channel = 10;
-		break;
-	case COLOR_DEPTH_121212:
+	if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR422)
 		bits_per_channel = 12;
-		break;
-	case COLOR_DEPTH_141414:
-		bits_per_channel = 14;
-		break;
-	case COLOR_DEPTH_161616:
-		bits_per_channel = 16;
-		break;
-	default:
-		break;
+	else{
+
+		switch (timing->display_color_depth) {
+
+		case COLOR_DEPTH_666:
+			bits_per_channel = 6;
+			break;
+		case COLOR_DEPTH_888:
+			bits_per_channel = 8;
+			break;
+		case COLOR_DEPTH_101010:
+			bits_per_channel = 10;
+			break;
+		case COLOR_DEPTH_121212:
+			bits_per_channel = 12;
+			break;
+		case COLOR_DEPTH_141414:
+			bits_per_channel = 14;
+			break;
+		case COLOR_DEPTH_161616:
+			bits_per_channel = 16;
+			break;
+		default:
+			break;
+		}
 	}
 	ASSERT(bits_per_channel != 0);
 
 	kbps = timing->pix_clk_khz;
 	kbps *= bits_per_channel;
 
-	if (timing->flags.Y_ONLY != 1)
+	if (timing->flags.Y_ONLY != 1) {
 		/*Only YOnly make reduce bandwidth by 1/3 compares to RGB*/
 		kbps *= 3;
+		if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR420)
+			kbps /= 2;
+		else if (timing->pixel_encoding == PIXEL_ENCODING_YCBCR422)
+			kbps = kbps * 2 / 3;
+	}
 
 	return kbps;
 

commit 3c1a312aa4e4201efa8719e70a6dccd3acd6eba4
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Wed Mar 7 09:12:53 2018 -0500

    drm/amd/display: Retry when read dpcd caps failed.
    
    Some DP panel not detected intermittently due to read dpcd
    caps failed when doing hot plug.
    [root cause] DC_HPD_CONNECT_INT_DELAY is set to 0, not delay
    after HPD toggle and read dpcd data, while some panel need 4ms defer
    to read.
    [solution] Add a retry when read failed.
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3b5053570229..b86325bb636f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2278,6 +2278,8 @@ static bool retrieve_link_cap(struct dc_link *link)
 	union edp_configuration_cap edp_config_cap;
 	union dp_downstream_port_present ds_port = { 0 };
 	enum dc_status status = DC_ERROR_UNEXPECTED;
+	uint32_t read_dpcd_retry_cnt = 3;
+	int i;
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
@@ -2285,11 +2287,15 @@ static bool retrieve_link_cap(struct dc_link *link)
 	memset(&edp_config_cap, '\0',
 		sizeof(union edp_configuration_cap));
 
-	status = core_link_read_dpcd(
-			link,
-			DP_DPCD_REV,
-			dpcd_data,
-			sizeof(dpcd_data));
+	for (i = 0; i < read_dpcd_retry_cnt; i++) {
+		status = core_link_read_dpcd(
+				link,
+				DP_DPCD_REV,
+				dpcd_data,
+				sizeof(dpcd_data));
+		if (status == DC_OK)
+			break;
+	}
 
 	if (status != DC_OK) {
 		dm_error("%s: Read dpcd data failed.\n", __func__);

commit 1296423bf23c7a58133970e223b1f47ec6570308
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Tue Feb 20 17:42:50 2018 -0500

    drm/amd/display: define DC_LOGGER for logger
    
    Created a DC_LOGGER define. This is used to
    pass the logger into the macros.
    
    Anywhere we need to use the logger we need to define
    DC_LOGGER
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 9a041641a539..3b5053570229 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -11,6 +11,8 @@
 #include "dpcd_defs.h"
 
 #include "resource.h"
+#define DC_LOGGER \
+	link->ctx->logger
 
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
@@ -63,8 +65,7 @@ static void wait_for_training_aux_rd_interval(
 
 	udelay(default_wait_in_micro_secs);
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s:\n wait = %d\n",
+	DC_LOG_HW_LINK_TRAINING("%s:\n wait = %d\n",
 		__func__,
 		default_wait_in_micro_secs);
 }
@@ -79,8 +80,7 @@ static void dpcd_set_training_pattern(
 		&dpcd_pattern.raw,
 		1);
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s\n %x pattern = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n %x pattern = %x\n",
 		__func__,
 		DP_TRAINING_PATTERN_SET,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
@@ -116,8 +116,7 @@ static void dpcd_set_link_settings(
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
 		__func__,
 		DP_LINK_BW_SET,
 		lt_settings->link_settings.link_rate,
@@ -151,8 +150,7 @@ static enum dpcd_training_patterns
 		break;
 	default:
 		ASSERT(0);
-		DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-			"%s: Invalid HW Training pattern: %d\n",
+		DC_LOG_HW_LINK_TRAINING("%s: Invalid HW Training pattern: %d\n",
 			__func__, pattern);
 		break;
 	}
@@ -184,8 +182,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
 		= dpcd_pattern.raw;
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s\n %x pattern = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n %x pattern = %x\n",
 		__func__,
 		DP_TRAINING_PATTERN_SET,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
@@ -219,8 +216,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		dpcd_lane,
 		size_in_bytes);
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s:\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s:\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
@@ -456,14 +452,12 @@ static void get_lane_status_and_drive_settings(
 
 	ln_status_updated->raw = dpcd_buf[2];
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
+	DC_LOG_HW_LINK_TRAINING("%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
 		__func__,
 		DP_LANE0_1_STATUS, dpcd_buf[0],
 		DP_LANE2_3_STATUS, dpcd_buf[1]);
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
 		__func__,
 		DP_ADJUST_REQUEST_LANE0_1,
 		dpcd_buf[4],
@@ -556,8 +550,7 @@ static void dpcd_set_lane_settings(
 	}
 	*/
 
-	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
-		"%s\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
+	DC_LOG_HW_LINK_TRAINING("%s\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
@@ -669,16 +662,14 @@ static bool perform_post_lt_adj_req_sequence(
 		}
 
 		if (!req_drv_setting_changed) {
-			DC_LOG_WARNING(link->ctx->logger,
-				"%s: Post Link Training Adjust Request Timed out\n",
+			DC_LOG_WARNING("%s: Post Link Training Adjust Request Timed out\n",
 				__func__);
 
 			ASSERT(0);
 			return true;
 		}
 	}
-	DC_LOG_WARNING(link->ctx->logger,
-		"%s: Post Link Training Adjust Request limit reached\n",
+	DC_LOG_WARNING("%s: Post Link Training Adjust Request limit reached\n",
 		__func__);
 
 	ASSERT(0);
@@ -885,8 +876,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 
 	if (retry_count >= LINK_TRAINING_MAX_CR_RETRY) {
 		ASSERT(0);
-		DC_LOG_ERROR(link->ctx->logger,
-			"%s: Link Training Error, could not get CR after %d tries. Possibly voltage swing issue",
+		DC_LOG_ERROR("%s: Link Training Error, could not get CR after %d tries. Possibly voltage swing issue",
 			__func__,
 			LINK_TRAINING_MAX_CR_RETRY);
 
@@ -1606,8 +1596,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 	if (sink_status_changed ||
 		!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.INTERLANE_ALIGN_DONE) {
 
-		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
-			"%s: Link Status changed.\n", __func__);
+		DC_LOG_HW_HPD_IRQ("%s: Link Status changed.\n", __func__);
 
 		return_code = true;
 
@@ -1620,8 +1609,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 			sizeof(irq_reg_rx_power_state));
 
 		if (dpcd_result != DC_OK) {
-			DC_LOG_HW_HPD_IRQ(link->ctx->logger,
-				"%s: DPCD read failed to obtain power state.\n",
+			DC_LOG_HW_HPD_IRQ("%s: DPCD read failed to obtain power state.\n",
 				__func__);
 		} else {
 			if (irq_reg_rx_power_state != DP_SET_POWER_D0)
@@ -1982,8 +1970,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	 * PSR and device auto test, refer to function handle_sst_hpd_irq
 	 * in DAL2.1*/
 
-	DC_LOG_HW_HPD_IRQ(link->ctx->logger,
-		"%s: Got short pulse HPD on link %d\n",
+	DC_LOG_HW_HPD_IRQ("%s: Got short pulse HPD on link %d\n",
 		__func__, link->link_index);
 
 
@@ -1997,8 +1984,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 		*out_hpd_irq_dpcd_data = hpd_irq_dpcd_data;
 
 	if (result != DC_OK) {
-		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
-			"%s: DPCD read failed to obtain irq data\n",
+		DC_LOG_HW_HPD_IRQ("%s: DPCD read failed to obtain irq data\n",
 			__func__);
 		return false;
 	}
@@ -2016,8 +2002,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	}
 
 	if (!allow_hpd_rx_irq(link)) {
-		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
-			"%s: skipping HPD handling on %d\n",
+		DC_LOG_HW_HPD_IRQ("%s: skipping HPD handling on %d\n",
 			__func__, link->link_index);
 		return false;
 	}

commit 2f3fd67a8af25f5b4d549c3e9cc515dbf1839ffc
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Fri Feb 16 13:57:42 2018 -0500

    drm/amd/display: Use MACROS instead of dm_logger
    
    Created MACROS for all log levels. Also Replaced
    usage of dm_logger_write to the defined MACROS
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4c21da54a9d5..9a041641a539 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -63,7 +63,7 @@ static void wait_for_training_aux_rd_interval(
 
 	udelay(default_wait_in_micro_secs);
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s:\n wait = %d\n",
 		__func__,
 		default_wait_in_micro_secs);
@@ -79,7 +79,7 @@ static void dpcd_set_training_pattern(
 		&dpcd_pattern.raw,
 		1);
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s\n %x pattern = %x\n",
 		__func__,
 		DP_TRAINING_PATTERN_SET,
@@ -116,7 +116,7 @@ static void dpcd_set_link_settings(
 	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
 		__func__,
 		DP_LINK_BW_SET,
@@ -151,7 +151,7 @@ static enum dpcd_training_patterns
 		break;
 	default:
 		ASSERT(0);
-		dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 			"%s: Invalid HW Training pattern: %d\n",
 			__func__, pattern);
 		break;
@@ -184,7 +184,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
 		= dpcd_pattern.raw;
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s\n %x pattern = %x\n",
 		__func__,
 		DP_TRAINING_PATTERN_SET,
@@ -219,7 +219,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		dpcd_lane,
 		size_in_bytes);
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s:\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
@@ -456,13 +456,13 @@ static void get_lane_status_and_drive_settings(
 
 	ln_status_updated->raw = dpcd_buf[2];
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
 		__func__,
 		DP_LANE0_1_STATUS, dpcd_buf[0],
 		DP_LANE2_3_STATUS, dpcd_buf[1]);
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
 		__func__,
 		DP_ADJUST_REQUEST_LANE0_1,
@@ -556,7 +556,7 @@ static void dpcd_set_lane_settings(
 	}
 	*/
 
-	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+	DC_LOG_HW_LINK_TRAINING(link->ctx->logger,
 		"%s\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
@@ -669,7 +669,7 @@ static bool perform_post_lt_adj_req_sequence(
 		}
 
 		if (!req_drv_setting_changed) {
-			dm_logger_write(link->ctx->logger, LOG_WARNING,
+			DC_LOG_WARNING(link->ctx->logger,
 				"%s: Post Link Training Adjust Request Timed out\n",
 				__func__);
 
@@ -677,7 +677,7 @@ static bool perform_post_lt_adj_req_sequence(
 			return true;
 		}
 	}
-	dm_logger_write(link->ctx->logger, LOG_WARNING,
+	DC_LOG_WARNING(link->ctx->logger,
 		"%s: Post Link Training Adjust Request limit reached\n",
 		__func__);
 
@@ -885,7 +885,7 @@ static enum link_training_result perform_clock_recovery_sequence(
 
 	if (retry_count >= LINK_TRAINING_MAX_CR_RETRY) {
 		ASSERT(0);
-		dm_logger_write(link->ctx->logger, LOG_ERROR,
+		DC_LOG_ERROR(link->ctx->logger,
 			"%s: Link Training Error, could not get CR after %d tries. Possibly voltage swing issue",
 			__func__,
 			LINK_TRAINING_MAX_CR_RETRY);
@@ -1606,7 +1606,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 	if (sink_status_changed ||
 		!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.INTERLANE_ALIGN_DONE) {
 
-		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
 			"%s: Link Status changed.\n", __func__);
 
 		return_code = true;
@@ -1620,7 +1620,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 			sizeof(irq_reg_rx_power_state));
 
 		if (dpcd_result != DC_OK) {
-			dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+			DC_LOG_HW_HPD_IRQ(link->ctx->logger,
 				"%s: DPCD read failed to obtain power state.\n",
 				__func__);
 		} else {
@@ -1982,7 +1982,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	 * PSR and device auto test, refer to function handle_sst_hpd_irq
 	 * in DAL2.1*/
 
-	dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+	DC_LOG_HW_HPD_IRQ(link->ctx->logger,
 		"%s: Got short pulse HPD on link %d\n",
 		__func__, link->link_index);
 
@@ -1997,7 +1997,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 		*out_hpd_irq_dpcd_data = hpd_irq_dpcd_data;
 
 	if (result != DC_OK) {
-		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
 			"%s: DPCD read failed to obtain irq data\n",
 			__func__);
 		return false;
@@ -2016,7 +2016,7 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	}
 
 	if (!allow_hpd_rx_irq(link)) {
-		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+		DC_LOG_HW_HPD_IRQ(link->ctx->logger,
 			"%s: skipping HPD handling on %d\n",
 			__func__, link->link_index);
 		return false;

commit 94405cf63884e364ec560d99af47084cb1b47b5f
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Fri Feb 16 14:04:16 2018 -0500

    drm/amd/display: Update Link Training Fallback logic
    
    [Description]
    When CR fails to minimum link rate,
    we should reduce lane count to the number lowest cr_done lanes.
    
    [Code Review]
    Jun Lei
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 604fb0171ee3..4c21da54a9d5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -709,6 +709,22 @@ static enum hw_dp_training_pattern get_supported_tp(struct dc_link *link)
 	return HW_DP_TRAINING_PATTERN_2;
 }
 
+static enum link_training_result get_cr_failure(enum dc_lane_count ln_count,
+					union lane_status *dpcd_lane_status)
+{
+	enum link_training_result result = LINK_TRAINING_SUCCESS;
+
+	if (ln_count >= LANE_COUNT_ONE && !dpcd_lane_status[0].bits.CR_DONE_0)
+		result = LINK_TRAINING_CR_FAIL_LANE0;
+	else if (ln_count >= LANE_COUNT_TWO && !dpcd_lane_status[1].bits.CR_DONE_0)
+		result = LINK_TRAINING_CR_FAIL_LANE1;
+	else if (ln_count >= LANE_COUNT_FOUR && !dpcd_lane_status[2].bits.CR_DONE_0)
+		result = LINK_TRAINING_CR_FAIL_LANE23;
+	else if (ln_count >= LANE_COUNT_FOUR && !dpcd_lane_status[3].bits.CR_DONE_0)
+		result = LINK_TRAINING_CR_FAIL_LANE23;
+	return result;
+}
+
 static enum link_training_result perform_channel_equalization_sequence(
 	struct dc_link *link,
 	struct link_training_settings *lt_settings)
@@ -771,7 +787,7 @@ static enum link_training_result perform_channel_equalization_sequence(
 
 }
 
-static bool perform_clock_recovery_sequence(
+static enum link_training_result perform_clock_recovery_sequence(
 	struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
@@ -846,11 +862,11 @@ static bool perform_clock_recovery_sequence(
 
 		/* 5. check CR done*/
 		if (is_cr_done(lane_count, dpcd_lane_status))
-			return true;
+			return LINK_TRAINING_SUCCESS;
 
 		/* 6. max VS reached*/
 		if (is_max_vs_reached(lt_settings))
-			return false;
+			break;
 
 		/* 7. same voltage*/
 		/* Note: VS same for all lanes,
@@ -876,13 +892,13 @@ static bool perform_clock_recovery_sequence(
 
 	}
 
-	return false;
+	return get_cr_failure(lane_count, dpcd_lane_status);
 }
 
-static inline bool perform_link_training_int(
+static inline enum link_training_result perform_link_training_int(
 	struct dc_link *link,
 	struct link_training_settings *lt_settings,
-	bool status)
+	enum link_training_result status)
 {
 	union lane_count_set lane_count_set = { {0} };
 	union dpcd_training_pattern dpcd_pattern = { {0} };
@@ -903,9 +919,9 @@ static inline bool perform_link_training_int(
 			get_supported_tp(link) == HW_DP_TRAINING_PATTERN_4)
 		return status;
 
-	if (status &&
+	if (status == LINK_TRAINING_SUCCESS &&
 		perform_post_lt_adj_req_sequence(link, lt_settings) == false)
-		status = false;
+		status = LINK_TRAINING_LQA_FAIL;
 
 	lane_count_set.bits.LANE_COUNT_SET = lt_settings->link_settings.lane_count;
 	lane_count_set.bits.ENHANCED_FRAMING = 1;
@@ -928,6 +944,8 @@ enum link_training_result dc_link_dp_perform_link_training(
 	enum link_training_result status = LINK_TRAINING_SUCCESS;
 
 	char *link_rate = "Unknown";
+	char *lt_result = "Unknown";
+
 	struct link_training_settings lt_settings;
 
 	memset(&lt_settings, '\0', sizeof(lt_settings));
@@ -951,22 +969,16 @@ enum link_training_result dc_link_dp_perform_link_training(
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)*/
-	if (!perform_clock_recovery_sequence(link, &lt_settings)) {
-		status = LINK_TRAINING_CR_FAIL;
-	} else {
+	status = perform_clock_recovery_sequence(link, &lt_settings);
+	if (status == LINK_TRAINING_SUCCESS) {
 		status = perform_channel_equalization_sequence(link,
 				&lt_settings);
 	}
 
 	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
-		if (!perform_link_training_int(link,
+		status = perform_link_training_int(link,
 				&lt_settings,
-				status == LINK_TRAINING_SUCCESS)) {
-			/* the next link training setting in this case
-			 * would be the same as CR failure case.
-			 */
-			status = LINK_TRAINING_CR_FAIL;
-		}
+				status);
 	}
 
 	/* 6. print status message*/
@@ -991,13 +1003,37 @@ enum link_training_result dc_link_dp_perform_link_training(
 		break;
 	}
 
+	switch (status) {
+	case LINK_TRAINING_SUCCESS:
+		lt_result = "pass";
+		break;
+	case LINK_TRAINING_CR_FAIL_LANE0:
+		lt_result = "CR failed lane0";
+		break;
+	case LINK_TRAINING_CR_FAIL_LANE1:
+		lt_result = "CR failed lane1";
+		break;
+	case LINK_TRAINING_CR_FAIL_LANE23:
+		lt_result = "CR failed lane23";
+		break;
+	case LINK_TRAINING_EQ_FAIL_CR:
+		lt_result = "CR failed in EQ";
+		break;
+	case LINK_TRAINING_EQ_FAIL_EQ:
+		lt_result = "EQ failed";
+		break;
+	case LINK_TRAINING_LQA_FAIL:
+		lt_result = "LQA failed";
+		break;
+	default:
+		break;
+	}
+
 	/* Connectivity log: link training */
 	CONN_MSG_LT(link, "%sx%d %s VS=%d, PE=%d",
 			link_rate,
 			lt_settings.link_settings.lane_count,
-			(status ==  LINK_TRAINING_SUCCESS) ? "pass" :
-			((status == LINK_TRAINING_CR_FAIL) ? "CR failed" :
-			"EQ failed"),
+			lt_result,
 			lt_settings.lane_settings[0].VOLTAGE_SWING,
 			lt_settings.lane_settings[0].PRE_EMPHASIS);
 
@@ -1115,6 +1151,7 @@ bool dp_hbr_verify_link_cap(
 				dp_cs_id,
 				cur);
 
+
 		if (skip_link_training)
 			success = true;
 		else {
@@ -1279,7 +1316,10 @@ static bool decide_fallback_link_setting(
 		return false;
 
 	switch (training_result) {
-	case LINK_TRAINING_CR_FAIL:
+	case LINK_TRAINING_CR_FAIL_LANE0:
+	case LINK_TRAINING_CR_FAIL_LANE1:
+	case LINK_TRAINING_CR_FAIL_LANE23:
+	case LINK_TRAINING_LQA_FAIL:
 	{
 		if (!reached_minimum_link_rate
 				(current_link_setting->link_rate)) {
@@ -1290,8 +1330,18 @@ static bool decide_fallback_link_setting(
 				(current_link_setting->lane_count)) {
 			current_link_setting->link_rate =
 				initial_link_settings.link_rate;
-			current_link_setting->lane_count =
-				reduce_lane_count(
+			if (training_result == LINK_TRAINING_CR_FAIL_LANE0)
+				return false;
+			else if (training_result == LINK_TRAINING_CR_FAIL_LANE1)
+				current_link_setting->lane_count =
+						LANE_COUNT_ONE;
+			else if (training_result ==
+					LINK_TRAINING_CR_FAIL_LANE23)
+				current_link_setting->lane_count =
+						LANE_COUNT_TWO;
+			else
+				current_link_setting->lane_count =
+					reduce_lane_count(
 					current_link_setting->lane_count);
 		} else {
 			return false;

commit 10dab1934b4b4e56f5b6f07ed021b1ac663c5888
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Tue Jan 30 11:46:16 2018 -0500

    drm/amd/display: VGA black screen from s3 when attached to hook
    
    [Description] For MST, DC already notify MST sink for MST mode, DC stll
    check DP SINK DPCD register to see if MST enabled. DP RX firmware may
    not handle this properly.
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4ee4c03a6724..604fb0171ee3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1465,7 +1465,7 @@ void decide_link_settings(struct dc_stream_state *stream,
 	/* MST doesn't perform link training for now
 	 * TODO: add MST specific link training routine
 	 */
-	if (is_mst_supported(link)) {
+	if (stream->signal == SIGNAL_TYPE_DISPLAY_PORT_MST) {
 		*link_setting = link->verified_link_cap;
 		return;
 	}

commit cdb39798082cde564beeab0ca47a469254122ccd
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Tue Jan 23 11:39:09 2018 -0500

    drm/amd/display: Add return value for detect dp.
    
    System soft hang when hotplug specific 4K DP panel
    due to link caps read error and incorrect link setting
    parmas to enable dp.
    Add status check for DPCD read and add return value
    for detect dp, in case of false, return from caller,
    avoid further false operation.
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 33d91e4474ea..4ee4c03a6724 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2235,13 +2235,14 @@ static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		link->wa_flags.dp_keep_receiver_powered = false;
 }
 
-static void retrieve_link_cap(struct dc_link *link)
+static bool retrieve_link_cap(struct dc_link *link)
 {
 	uint8_t dpcd_data[DP_TRAINING_AUX_RD_INTERVAL - DP_DPCD_REV + 1];
 
 	union down_stream_port_count down_strm_port_count;
 	union edp_configuration_cap edp_config_cap;
 	union dp_downstream_port_present ds_port = { 0 };
+	enum dc_status status = DC_ERROR_UNEXPECTED;
 
 	memset(dpcd_data, '\0', sizeof(dpcd_data));
 	memset(&down_strm_port_count,
@@ -2249,11 +2250,16 @@ static void retrieve_link_cap(struct dc_link *link)
 	memset(&edp_config_cap, '\0',
 		sizeof(union edp_configuration_cap));
 
-	core_link_read_dpcd(
-		link,
-		DP_DPCD_REV,
-		dpcd_data,
-		sizeof(dpcd_data));
+	status = core_link_read_dpcd(
+			link,
+			DP_DPCD_REV,
+			dpcd_data,
+			sizeof(dpcd_data));
+
+	if (status != DC_OK) {
+		dm_error("%s: Read dpcd data failed.\n", __func__);
+		return false;
+	}
 
 	{
 		union training_aux_rd_interval aux_rd_interval;
@@ -2315,11 +2321,13 @@ static void retrieve_link_cap(struct dc_link *link)
 
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
+
+	return true;
 }
 
-void detect_dp_sink_caps(struct dc_link *link)
+bool detect_dp_sink_caps(struct dc_link *link)
 {
-	retrieve_link_cap(link);
+	return retrieve_link_cap(link);
 
 	/* dc init_hw has power encoder using default
 	 * signal for connector. For native DP, no

commit 122fe39da4b43e3891ff3ab82fe70e23e2e29c08
Author: Luis de Bethencourt <luisbg@kernel.org>
Date:   Wed Jan 17 18:50:20 2018 +0000

    drm: amd: Fix trailing semicolons
    
    The trailing semicolon is an empty statement that does no operation.
    Removing the two instances of them since they don't do anything.
    
    Signed-off-by: Luis de Bethencourt <luisbg@kernel.org>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 61e8c3e02d16..33d91e4474ea 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -718,7 +718,7 @@ static enum link_training_result perform_channel_equalization_sequence(
 	uint32_t retries_ch_eq;
 	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
 	union lane_align_status_updated dpcd_lane_status_updated = {{0}};
-	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = {{{0}}};;
+	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = {{{0}}};
 
 	hw_tr_pattern = get_supported_tp(link);
 

commit 4d2f22d141e14c3cbee013406823e1903cd2d42c
Author: Hugo Hu <hugo.hu@amd.com>
Date:   Thu Dec 7 15:33:22 2017 +0800

    drm/amd/display: Use the maximum link setting which EDP reported.
    
    Signed-off-by: Hugo Hu <hugo.hu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 00528b214a9f..61e8c3e02d16 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1470,6 +1470,12 @@ void decide_link_settings(struct dc_stream_state *stream,
 		return;
 	}
 
+	/* EDP use the link cap setting */
+	if (stream->sink->sink_signal == SIGNAL_TYPE_EDP) {
+		*link_setting = link->verified_link_cap;
+		return;
+	}
+
 	/* search for the minimum link setting that:
 	 * 1. is supported according to the link training result
 	 * 2. could support the b/w requested by the timing

commit 4f42a2dd3d7ef106e6bd3e2ad61c55333150d896
Author: Joe Perches <joe@perches.com>
Date:   Thu Nov 16 07:27:27 2017 -0800

    drm: amd: Fix line continuation formats
    
    Line continuations with excess spacing causes unexpected output.
    
    Miscellanea:
    
    o Added missing '\n' to a few of the coalesced pr_<level> formats
    
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b2dcc462afe2..00528b214a9f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -220,8 +220,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		size_in_bytes);
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
-		"%s:\n %x VS set = %x  PE set = %x \
-		max VS Reached = %x  max PE Reached = %x\n",
+		"%s:\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
@@ -558,8 +557,7 @@ static void dpcd_set_lane_settings(
 	*/
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
-		"%s\n %x VS set = %x  PE set = %x \
-		max VS Reached = %x  max PE Reached = %x\n",
+		"%s\n %x VS set = %x  PE set = %x max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
 		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
@@ -872,9 +870,8 @@ static bool perform_clock_recovery_sequence(
 	if (retry_count >= LINK_TRAINING_MAX_CR_RETRY) {
 		ASSERT(0);
 		dm_logger_write(link->ctx->logger, LOG_ERROR,
-			"%s: Link Training Error, could not \
-			 get CR after %d tries. \
-			Possibly voltage swing issue", __func__,
+			"%s: Link Training Error, could not get CR after %d tries. Possibly voltage swing issue",
+			__func__,
 			LINK_TRAINING_MAX_CR_RETRY);
 
 	}

commit 7d8d90d84fe8fd73d09a1efa26c22d4fe902a05e
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Mon Oct 23 12:06:54 2017 -0400

    drm/amd/display: get remote dpcd caps for timing validation
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e6bf05d76a94..b2dcc462afe2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2127,7 +2127,7 @@ static void get_active_converter_info(
 
 				union dwnstream_port_caps_byte3_hdmi
 					hdmi_caps = {.raw = det_caps[3] };
-				union dwnstream_port_caps_byte1
+				union dwnstream_port_caps_byte2
 					hdmi_color_caps = {.raw = det_caps[2] };
 				link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk =
 					det_caps[1] * 25000;

commit 97011249c3b2ba6b038a2af7025063d62d1448ec
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Mon Nov 20 12:45:54 2017 -0500

    drm/amd/display: USB-C / thunderbolt dock specific workaround
    
    reading dpcd 0x600 cause link loss for a particular USB-C dock with
    thurderbolt.  workaround by avoiding dcpd 0x600 read unless it's
    necessary.
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 8e97b42a03a2..e6bf05d76a94 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1512,7 +1512,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 	struct dc_link *link,
 	union hpd_irq_data *hpd_irq_dpcd_data)
 {
-	uint8_t irq_reg_rx_power_state;
+	uint8_t irq_reg_rx_power_state = 0;
 	enum dc_status dpcd_result = DC_ERROR_UNEXPECTED;
 	union lane_status lane_status;
 	uint32_t lane;
@@ -1524,60 +1524,55 @@ static bool hpd_rx_irq_check_link_loss_status(
 
 	if (link->cur_link_settings.lane_count == 0)
 		return return_code;
-	/*1. Check that we can handle interrupt: Not in FS DOS,
-	 *  Not in "Display Timeout" state, Link is trained.
-	 */
 
-	dpcd_result = core_link_read_dpcd(link,
-		DP_SET_POWER,
-		&irq_reg_rx_power_state,
-		sizeof(irq_reg_rx_power_state));
+	/*1. Check that Link Status changed, before re-training.*/
 
-	if (dpcd_result != DC_OK) {
-		irq_reg_rx_power_state = DP_SET_POWER_D0;
-		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
-			"%s: DPCD read failed to obtain power state.\n",
-			__func__);
+	/*parse lane status*/
+	for (lane = 0; lane < link->cur_link_settings.lane_count; lane++) {
+		/* check status of lanes 0,1
+		 * changed DpcdAddress_Lane01Status (0x202)
+		 */
+		lane_status.raw = get_nibble_at_index(
+			&hpd_irq_dpcd_data->bytes.lane01_status.raw,
+			lane);
+
+		if (!lane_status.bits.CHANNEL_EQ_DONE_0 ||
+			!lane_status.bits.CR_DONE_0 ||
+			!lane_status.bits.SYMBOL_LOCKED_0) {
+			/* if one of the channel equalization, clock
+			 * recovery or symbol lock is dropped
+			 * consider it as (link has been
+			 * dropped) dp sink status has changed
+			 */
+			sink_status_changed = true;
+			break;
+		}
 	}
 
-	if (irq_reg_rx_power_state == DP_SET_POWER_D0) {
+	/* Check interlane align.*/
+	if (sink_status_changed ||
+		!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.INTERLANE_ALIGN_DONE) {
 
-		/*2. Check that Link Status changed, before re-training.*/
-
-		/*parse lane status*/
-		for (lane = 0;
-			lane < link->cur_link_settings.lane_count;
-			lane++) {
-
-			/* check status of lanes 0,1
-			 * changed DpcdAddress_Lane01Status (0x202)*/
-			lane_status.raw = get_nibble_at_index(
-				&hpd_irq_dpcd_data->bytes.lane01_status.raw,
-				lane);
-
-			if (!lane_status.bits.CHANNEL_EQ_DONE_0 ||
-				!lane_status.bits.CR_DONE_0 ||
-				!lane_status.bits.SYMBOL_LOCKED_0) {
-				/* if one of the channel equalization, clock
-				 * recovery or symbol lock is dropped
-				 * consider it as (link has been
-				 * dropped) dp sink status has changed*/
-				sink_status_changed = true;
-				break;
-			}
+		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+			"%s: Link Status changed.\n", __func__);
 
-		}
+		return_code = true;
 
-		/* Check interlane align.*/
-		if (sink_status_changed ||
-			!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.
-			INTERLANE_ALIGN_DONE) {
+		/*2. Check that we can handle interrupt: Not in FS DOS,
+		 *  Not in "Display Timeout" state, Link is trained.
+		 */
+		dpcd_result = core_link_read_dpcd(link,
+			DP_SET_POWER,
+			&irq_reg_rx_power_state,
+			sizeof(irq_reg_rx_power_state));
 
+		if (dpcd_result != DC_OK) {
 			dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
-				"%s: Link Status changed.\n",
+				"%s: DPCD read failed to obtain power state.\n",
 				__func__);
-
-			return_code = true;
+		} else {
+			if (irq_reg_rx_power_state != DP_SET_POWER_D0)
+				return_code = false;
 		}
 	}
 

commit 6bffebc90c23e2341a1f8371e7b496ec94136e47
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Wed Oct 18 20:22:40 2017 -0400

    drm/amd/display: Add timing validation against dongle cap
    
    For DP active dongles, the dpcd dongle caps are read but not
    used to validate mode timing. This addresses this.
    
    In particular, this change fixes light up on the HDMI 4k TV
    connected through DP active dongle. Since the 4k TV defaults
    to YCbCr420, which the dongle don't support.
    
    This change does not address MST cases, a more generalized
    approach must be taken for that.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index ced42484dcfc..8e97b42a03a2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2062,6 +2062,24 @@ bool is_dp_active_dongle(const struct dc_link *link)
 			(dongle_type == DISPLAY_DONGLE_DP_HDMI_CONVERTER);
 }
 
+static int translate_dpcd_max_bpc(enum dpcd_downstream_port_max_bpc bpc)
+{
+	switch (bpc) {
+	case DOWN_STREAM_MAX_8BPC:
+		return 8;
+	case DOWN_STREAM_MAX_10BPC:
+		return 10;
+	case DOWN_STREAM_MAX_12BPC:
+		return 12;
+	case DOWN_STREAM_MAX_16BPC:
+		return 16;
+	default:
+		break;
+	}
+
+	return -1;
+}
+
 static void get_active_converter_info(
 	uint8_t data, struct dc_link *link)
 {
@@ -2131,7 +2149,8 @@ static void get_active_converter_info(
 					hdmi_caps.bits.YCrCr420_CONVERSION;
 
 				link->dpcd_caps.dongle_caps.dp_hdmi_max_bpc =
-					hdmi_color_caps.bits.MAX_BITS_PER_COLOR_COMPONENT;
+					translate_dpcd_max_bpc(
+						hdmi_color_caps.bits.MAX_BITS_PER_COLOR_COMPONENT);
 
 				link->dpcd_caps.dongle_caps.extendedCapValid = true;
 			}

commit 25bab0da8f61e7d8f717e4f9be34e97a1aaa0ccc
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Thu Sep 14 18:50:32 2017 -0400

    drm/amd/display: set cp25201 to use TPS4
    
    [Description]
    hbr2 compliance eye output is unstable
    (toggling on and off) with debugger break.
    This caueses intermittent PHY automation failure.
    Need to look into the root cause later
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b735782b8fe0..ced42484dcfc 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1700,6 +1700,12 @@ static void dp_test_send_link_training(struct dc_link *link)
 	dp_retrain_link_dp_test(link, &link_settings, false);
 }
 
+/* TODO hbr2 compliance eye output is unstable
+ * (toggling on and off) with debugger break
+ * This caueses intermittent PHY automation failure
+ * Need to look into the root cause */
+static uint8_t force_tps4_for_cp2520 = 1;
+
 static void dp_test_send_phy_test_pattern(struct dc_link *link)
 {
 	union phy_test_pattern dpcd_test_pattern;
@@ -1758,10 +1764,16 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 		test_pattern = DP_TEST_PATTERN_80BIT_CUSTOM;
 		break;
 	case PHY_TEST_PATTERN_CP2520_1:
-		test_pattern = DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
+		/* CP2520 pattern is unstable, temporarily use TPS4 instead */
+		test_pattern = (force_tps4_for_cp2520 == 1) ?
+				DP_TEST_PATTERN_TRAINING_PATTERN4 :
+				DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
 		break;
 	case PHY_TEST_PATTERN_CP2520_2:
-		test_pattern = DP_TEST_PATTERN_CP2520_2;
+		/* CP2520 pattern is unstable, temporarily use TPS4 instead */
+		test_pattern = (force_tps4_for_cp2520 == 1) ?
+				DP_TEST_PATTERN_TRAINING_PATTERN4 :
+				DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
 		break;
 	case PHY_TEST_PATTERN_CP2520_3:
 		test_pattern = DP_TEST_PATTERN_TRAINING_PATTERN4;

commit 44858055bb28b1ba45dc05acecf9087bc4786701
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Oct 3 15:11:01 2017 +1000

    amdgpu/dc: set a bunch of functions to static.
    
    All of these are unused outside the file they are in.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3c323c1ea366..b735782b8fe0 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1209,7 +1209,7 @@ static inline bool reached_minimum_link_rate(enum dc_link_rate link_rate)
 	return link_rate <= LINK_RATE_LOW;
 }
 
-enum dc_lane_count reduce_lane_count(enum dc_lane_count lane_count)
+static enum dc_lane_count reduce_lane_count(enum dc_lane_count lane_count)
 {
 	switch (lane_count) {
 	case LANE_COUNT_FOUR:

commit 5667ff5c117f2d8735f27a267bc68a0811c55075
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Oct 3 14:27:13 2017 +1000

    amdgpu/dc: fix a bunch of misc whitespace.
    
    This just aligns a few things with kernel style.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 888d268e9622..3c323c1ea366 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1473,10 +1473,10 @@ void decide_link_settings(struct dc_stream_state *stream,
 		return;
 	}
 
-    /* search for the minimum link setting that:
-     * 1. is supported according to the link training result
-     * 2. could support the b/w requested by the timing
-     */
+	/* search for the minimum link setting that:
+	 * 1. is supported according to the link training result
+	 * 2. could support the b/w requested by the timing
+	 */
 	while (current_link_setting.link_rate <=
 			link->verified_link_cap.link_rate) {
 		link_bw = bandwidth_in_kbps_from_link_settings(

commit 04e212926f0d1213bc1ff1f8ab4050878699977d
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Sep 29 17:13:29 2017 +1000

    amdgpu/dc: set some of the link dp code to static.
    
    These aren't currently used outside this file.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 305ca0decfaa..888d268e9622 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -32,6 +32,14 @@ enum {
 	LINK_TRAINING_MAX_CR_RETRY = 100
 };
 
+static bool decide_fallback_link_setting(
+		struct dc_link_settings initial_link_settings,
+		struct dc_link_settings *current_link_setting,
+		enum link_training_result training_result);
+static struct dc_link_settings get_common_supported_link_settings (
+		struct dc_link_settings link_setting_a,
+		struct dc_link_settings link_setting_b);
+
 static void wait_for_training_aux_rd_interval(
 	struct dc_link *link,
 	uint32_t default_wait_in_micro_secs)
@@ -1150,7 +1158,7 @@ bool dp_hbr_verify_link_cap(
 	return success;
 }
 
-struct dc_link_settings get_common_supported_link_settings (
+static struct dc_link_settings get_common_supported_link_settings (
 		struct dc_link_settings link_setting_a,
 		struct dc_link_settings link_setting_b)
 {
@@ -1215,7 +1223,7 @@ enum dc_lane_count reduce_lane_count(enum dc_lane_count lane_count)
 	}
 }
 
-enum dc_link_rate reduce_link_rate(enum dc_link_rate link_rate)
+static enum dc_link_rate reduce_link_rate(enum dc_link_rate link_rate)
 {
 	switch (link_rate) {
 	case LINK_RATE_HIGH3:
@@ -1231,7 +1239,7 @@ enum dc_link_rate reduce_link_rate(enum dc_link_rate link_rate)
 	}
 }
 
-enum dc_lane_count increase_lane_count(enum dc_lane_count lane_count)
+static enum dc_lane_count increase_lane_count(enum dc_lane_count lane_count)
 {
 	switch (lane_count) {
 	case LANE_COUNT_ONE:
@@ -1243,7 +1251,7 @@ enum dc_lane_count increase_lane_count(enum dc_lane_count lane_count)
 	}
 }
 
-enum dc_link_rate increase_link_rate(enum dc_link_rate link_rate)
+static enum dc_link_rate increase_link_rate(enum dc_link_rate link_rate)
 {
 	switch (link_rate) {
 	case LINK_RATE_LOW:
@@ -1265,7 +1273,7 @@ enum dc_link_rate increase_link_rate(enum dc_link_rate link_rate)
  *			false - has reached minimum setting
  *					and no further fallback could be done
  */
-bool decide_fallback_link_setting(
+static bool decide_fallback_link_setting(
 		struct dc_link_settings initial_link_settings,
 		struct dc_link_settings *current_link_setting,
 		enum link_training_result training_result)

commit 450619d328d0824fab649187019cbea3532f879c
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Sep 29 17:13:23 2017 +1000

    amdgpu/dc: move some one line dp functions to inlines.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 38ccc011004d..305ca0decfaa 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1191,12 +1191,12 @@ struct dc_link_settings get_common_supported_link_settings (
 	return link_settings;
 }
 
-bool reached_minimum_lane_count(enum dc_lane_count lane_count)
+static inline bool reached_minimum_lane_count(enum dc_lane_count lane_count)
 {
 	return lane_count <= LANE_COUNT_ONE;
 }
 
-bool reached_minimum_link_rate(enum dc_link_rate link_rate)
+static inline bool reached_minimum_link_rate(enum dc_link_rate link_rate)
 {
 	return link_rate <= LINK_RATE_LOW;
 }

commit acea7183b5d72d0713d2129956e7357ebde28054
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Thu Aug 31 16:49:56 2017 -0400

    drm/amd/display: set CP2520 Test pattern to use DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 1aec586b0367..38ccc011004d 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1750,7 +1750,7 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 		test_pattern = DP_TEST_PATTERN_80BIT_CUSTOM;
 		break;
 	case PHY_TEST_PATTERN_CP2520_1:
-		test_pattern = DP_TEST_PATTERN_CP2520_1;
+		test_pattern = DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
 		break;
 	case PHY_TEST_PATTERN_CP2520_2:
 		test_pattern = DP_TEST_PATTERN_CP2520_2;

commit 78e685f9a385eb984f69720e463878e5c3ae80b7
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Aug 30 17:49:39 2017 -0400

    drm/amd/display: Use TPS4 instead of CP2520_3 for phy pattern 7
    
    [Description]
    We originally use TPS4 phy test pattern for test pattern 7.
    On RV we switched to a new method to use CP2520.
    CP2520 should produce the same result.
    However in reality, it fails DP PHY automation test.
    We use the original method instead.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 527cc0400fd3..1aec586b0367 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1756,7 +1756,7 @@ static void dp_test_send_phy_test_pattern(struct dc_link *link)
 		test_pattern = DP_TEST_PATTERN_CP2520_2;
 		break;
 	case PHY_TEST_PATTERN_CP2520_3:
-		test_pattern = DP_TEST_PATTERN_CP2520_3;
+		test_pattern = DP_TEST_PATTERN_TRAINING_PATTERN4;
 		break;
 	default:
 		test_pattern = DP_TEST_PATTERN_VIDEO_MODE;

commit c7299705e6e76fe499ddc9af7beb0500e945fec9
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Mon Aug 28 16:28:34 2017 -0400

    drm/amd/display: only polling VSync Phase within VSync peroroid
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 5f2b52e41427..527cc0400fd3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1653,8 +1653,8 @@ static bool handle_hpd_irq_psr_sink(const struct dc_link *link)
 				sizeof(psr_error_status.raw));
 
 			/* PSR error, disable and re-enable PSR */
-			dc_link_set_psr_enable(link, false);
-			dc_link_set_psr_enable(link, true);
+			dc_link_set_psr_enable(link, false, true);
+			dc_link_set_psr_enable(link, true, true);
 
 			return true;
 		} else if (psr_sink_psr_status.bits.SINK_SELF_REFRESH_STATUS ==

commit 608ac7bb3924178d7bfa8b88d79d3d9d72b8f485
Author: Jerry Zuo <Jerry.Zuo@amd.com>
Date:   Fri Aug 25 16:16:10 2017 -0400

    drm/amd/display: Rename dc validate_context and current_context
    
    Rename all the dc validate_context to dc_state and
    dc current_context to current_state.
    
    Signed-off-by: Jerry Zuo <Jerry.Zuo@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 446e2933474c..5f2b52e41427 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2406,7 +2406,7 @@ bool dc_link_dp_set_test_pattern(
 	const unsigned char *p_custom_pattern,
 	unsigned int cust_pattern_size)
 {
-	struct pipe_ctx *pipes = link->dc->current_context->res_ctx.pipe_ctx;
+	struct pipe_ctx *pipes = link->dc->current_state->res_ctx.pipe_ctx;
 	struct pipe_ctx *pipe_ctx = &pipes[0];
 	unsigned int lane;
 	unsigned int i;

commit 1cf49dea28dfc76f0816a4bc73c2ab975c72f55d
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Aug 23 17:02:34 2017 -0400

    drm/amd/display: do not reset lane count in EQ fallback
    
    [Description]
    According to DP1.4 specs we should not reset lane count back
    when falling back in failing EQ training.
    This causes PHY test pattern compliance to fail as infinite LT
    when LT fails EQ to 4 RBR and fails CR in a loop.
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e19447d526ea..446e2933474c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1302,8 +1302,6 @@ bool decide_fallback_link_setting(
 					current_link_setting->lane_count);
 		} else if (!reached_minimum_link_rate
 				(current_link_setting->link_rate)) {
-			current_link_setting->lane_count =
-				initial_link_settings.lane_count;
 			current_link_setting->link_rate =
 				reduce_link_rate(
 					current_link_setting->link_rate);

commit aaa15026f273c184454ea8672274a682bc161da1
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Aug 22 15:49:57 2017 -0400

    drm/amd/display: Fix return value from rx irq handler on up request
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e35bdce6a0f3..e19447d526ea 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1964,11 +1964,11 @@ bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd
 	 * so do not handle as a normal sink status change interrupt.
 	 */
 
+	if (hpd_irq_dpcd_data.bytes.device_service_irq.bits.UP_REQ_MSG_RDY)
+		return true;
+
 	/* check if we have MST msg and return since we poll for it */
-	if (hpd_irq_dpcd_data.bytes.device_service_irq.
-			bits.DOWN_REP_MSG_RDY ||
-		hpd_irq_dpcd_data.bytes.device_service_irq.
-			bits.UP_REQ_MSG_RDY)
+	if (hpd_irq_dpcd_data.bytes.device_service_irq.bits.DOWN_REP_MSG_RDY)
 		return false;
 
 	/* For now we only handle 'Downstream port status' case.

commit 3f1f74f436798a54b6ebcacfc026ddc4347eab46
Author: Jerry Zuo <Jerry.Zuo@amd.com>
Date:   Thu Aug 17 16:05:37 2017 -0400

    drm/amd/display: Fix two MST not light up regressions
    
    1. Change 100104: Move verify link cap after read edid causes MST
       link_cap struct not being set. It leads to zero denominator
       pbn_per_slot value, leading to the crash at
       dal_fixed31_32_div(). Skip MST link training for now and will
       need to add MST specific link traning routine later.
    
    2. Change 98822: Adding edp supports changes link setting
       condition from max_link to verified_link. It leads to MST is
       getting wrong link settings.
       e.g. LINK_SPREAD_05_DOWNSPREAD_30KHZ not set in MST case
    
    Signed-off-by: Jerry Zuo <Jerry.Zuo@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index d621237e923e..e35bdce6a0f3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1459,6 +1459,14 @@ void decide_link_settings(struct dc_stream_state *stream,
 		return;
 	}
 
+	/* MST doesn't perform link training for now
+	 * TODO: add MST specific link training routine
+	 */
+	if (is_mst_supported(link)) {
+		*link_setting = link->verified_link_cap;
+		return;
+	}
+
     /* search for the minimum link setting that:
      * 1. is supported according to the link training result
      * 2. could support the b/w requested by the timing

commit 9799624ac236eb238b5a5c885c759b1cbcac6349
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Tue Aug 15 19:10:14 2017 -0400

    drm/amd/display: Cache edp config in dc link
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0144c98fd0d5..d621237e923e 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2258,6 +2258,8 @@ static void retrieve_link_cap(struct dc_link *link)
 		DP_EDP_CONFIGURATION_CAP - DP_DPCD_REV];
 	link->dpcd_caps.panel_mode_edp =
 		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
+	link->dpcd_caps.dpcd_display_control_capable =
+		edp_config_cap.bits.DPCD_DISPLAY_CONTROL_CAPABLE;
 
 	link->test_pattern_enabled = false;
 	link->compliance_test_state.raw = 0;

commit fb3466a450cc4684654367ae2f47fc3fc7846574
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Tue Aug 1 15:00:25 2017 -0400

    drm/amd/display: Flattening core_dc to dc
    
    -Flattening core_dc to dc
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 9d5fe658d14c..0144c98fd0d5 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -10,7 +10,6 @@
 #include "core_status.h"
 #include "dpcd_defs.h"
 
-#include "core_dc.h"
 #include "resource.h"
 
 /* maximum pre emphasis level allowed for each voltage swing level*/

commit f334073ae31eaee742811e6ca282622aad5844ad
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Thu Aug 3 13:57:05 2017 -0400

    drm/amd/display: Move verify link cap after read edid
    
    DP link layer test 400.1.1 fails intermittently.
    The test device will pull hpd low immediately
    after verify link cap.
    Driver reads edid when hpd low that causes the test to fail.
    
    Move read edid before verify link cap, so driver will
    read edid before starting link training
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 3d296b1d118f..9d5fe658d14c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2283,13 +2283,6 @@ void detect_dp_sink_caps(struct dc_link *link)
 	 * DP, hw_init may need check signal or power up
 	 * encoder here.
 	 */
-
-	if (is_mst_supported(link)) {
-		link->verified_link_cap = link->reported_link_cap;
-	} else {
-		dp_hbr_verify_link_cap(link,
-			&link->reported_link_cap);
-	}
 	/* TODO save sink caps in link->sink */
 }
 

commit 8e9c4c8cf35ff23aafc69f9ef4c9cc471dac4094
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 14:36:12 2017 -0400

    drm/amd/display: Move stream_enc to stream_res
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipes->stream_enc/pipes->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/ctx->stream_enc->/ctx->stream_res\.stream_enc->/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe->stream_enc/pipe->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx->stream_enc/pipe_ctx->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_ctx\[pipe_offset\]\.stream_enc/pipe_ctx\[pipe_offset\]\.stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/grouped_pipes\[i\]->stream_enc/grouped_pipes\[i\]->stream_^Cs\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/grouped_pipes\[0\]->stream_enc/grouped_pipes\[0\]->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/grouped_pipes\[1\]->stream_enc/grouped_pipes\[1\]->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_ctx\[i\]->stream_enc/pipe_ctx\[i\]->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_ctx_old->stream_enc/pipe_ctx_old->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_set\[j\]->stream_enc/pipe_set\[j\]->stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_ctx\[i\]\.stream_enc/pipe_ctx\[i\]\.stream_res\.stream_enc/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipes\[i\]\.stream_enc/pipes\[i\]\.stream_res\.stream_enc/g'
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index eda25942aea0..3d296b1d118f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2464,7 +2464,7 @@ bool dc_link_dp_set_test_pattern(
 			 * MuteAudioEndpoint(pPathMode->pDisplayPath, true);
 			 */
 			/* Blank stream */
-			pipes->stream_enc->funcs->dp_blank(pipe_ctx->stream_enc);
+			pipes->stream_res.stream_enc->funcs->dp_blank(pipe_ctx->stream_res.stream_enc);
 		}
 
 		dp_set_hw_test_pattern(link, test_pattern,

commit 6b670fa965b620bf0131a0a339a388a0581bc466
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 13:59:26 2017 -0400

    drm/amd/display: Move TG to stream_res
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipes->tg/pipes->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/ctx->tg->/ctx->stream_res\.tg->/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe->tg/pipe->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx->tg/pipe_ctx->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i \
    's/pipe_ctx\[pipe_offset\]\.tg/pipe_ctx\[pipe_offset\]\.stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i
    's/grouped_pipes\[i\]->tg/grouped_pipes\[i\]->stream_^Cs\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i
    's/grouped_pipes\[0\]->tg/grouped_pipes\[0\]->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i
    's/grouped_pipes\[1\]->tg/grouped_pipes\[1\]->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx\[i\]->tg/pipe_ctx\[i\]->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx_old->tg/pipe_ctx_old->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_set\[j\]->tg/pipe_set\[j\]->stream_res\.tg/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx\[i\]\.tg/pipe_ctx\[i\]\.stream_res\.tg/g'
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 0c90060c0f62..eda25942aea0 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2375,7 +2375,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		pipe_ctx->stream_res.opp->funcs->
 			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
 
-		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
+		pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				controller_test_pattern, color_depth);
 	}
 	break;
@@ -2388,7 +2388,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		pipe_ctx->stream_res.opp->funcs->
 			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
 
-		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
+		pipe_ctx->stream_res.tg->funcs->set_test_pattern(pipe_ctx->stream_res.tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
 				color_depth);
 	}

commit a6a6cb349e39ef23a341a17752eebf69a5c0d7ff
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 13:55:28 2017 -0400

    drm/amd/display: Move OPP to stream_res
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipes->opp/pipes->stream_res\.opp/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/ctx->opp->/ctx->stream_res\.opp->/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe->opp/pipe->stream_res\.opp/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/pipe_ctx->opp/pipe_ctx->stream_res\.opp/g'
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 231cade5e1fd..0c90060c0f62 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2372,8 +2372,8 @@ static void set_crtc_test_pattern(struct dc_link *link,
 	{
 		/* disable bit depth reduction */
 		pipe_ctx->stream->bit_depth_params = params;
-		pipe_ctx->opp->funcs->
-			opp_program_bit_depth_reduction(pipe_ctx->opp, &params);
+		pipe_ctx->stream_res.opp->funcs->
+			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
 
 		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
 				controller_test_pattern, color_depth);
@@ -2385,8 +2385,8 @@ static void set_crtc_test_pattern(struct dc_link *link,
 		resource_build_bit_depth_reduction_params(pipe_ctx->stream,
 					&params);
 		pipe_ctx->stream->bit_depth_params = params;
-		pipe_ctx->opp->funcs->
-			opp_program_bit_depth_reduction(pipe_ctx->opp, &params);
+		pipe_ctx->stream_res.opp->funcs->
+			opp_program_bit_depth_reduction(pipe_ctx->stream_res.opp, &params);
 
 		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
 				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,

commit 0a8f43ff64156525f98220efe0992925995153b6
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 13:19:56 2017 -0400

    drm/amd/display: Remove struct from stack in dp_set_test_pattern
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 50724f9a8e2c..231cade5e1fd 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2407,7 +2407,7 @@ bool dc_link_dp_set_test_pattern(
 	unsigned int cust_pattern_size)
 {
 	struct pipe_ctx *pipes = link->dc->current_context->res_ctx.pipe_ctx;
-	struct pipe_ctx pipe_ctx = pipes[0];
+	struct pipe_ctx *pipe_ctx = &pipes[0];
 	unsigned int lane;
 	unsigned int i;
 	unsigned char link_qual_pattern[LANE_COUNT_DP_MAX] = {0};
@@ -2418,7 +2418,7 @@ bool dc_link_dp_set_test_pattern(
 
 	for (i = 0; i < MAX_PIPES; i++) {
 		if (pipes[i].stream->sink->link == link) {
-			pipe_ctx = pipes[i];
+			pipe_ctx = &pipes[i];
 			break;
 		}
 	}
@@ -2430,14 +2430,14 @@ bool dc_link_dp_set_test_pattern(
 	if (link->test_pattern_enabled && test_pattern ==
 			DP_TEST_PATTERN_VIDEO_MODE) {
 		/* Set CRTC Test Pattern */
-		set_crtc_test_pattern(link, &pipe_ctx, test_pattern);
+		set_crtc_test_pattern(link, pipe_ctx, test_pattern);
 		dp_set_hw_test_pattern(link, test_pattern,
 				(uint8_t *)p_custom_pattern,
 				(uint32_t)cust_pattern_size);
 
 		/* Unblank Stream */
 		link->dc->hwss.unblank_stream(
-			&pipe_ctx,
+			pipe_ctx,
 			&link->verified_link_cap);
 		/* TODO:m_pHwss->MuteAudioEndpoint
 		 * (pPathMode->pDisplayPath, false);
@@ -2464,7 +2464,7 @@ bool dc_link_dp_set_test_pattern(
 			 * MuteAudioEndpoint(pPathMode->pDisplayPath, true);
 			 */
 			/* Blank stream */
-			pipes->stream_enc->funcs->dp_blank(pipe_ctx.stream_enc);
+			pipes->stream_enc->funcs->dp_blank(pipe_ctx->stream_enc);
 		}
 
 		dp_set_hw_test_pattern(link, test_pattern,
@@ -2545,7 +2545,7 @@ bool dc_link_dp_set_test_pattern(
 		}
 	} else {
 	/* CRTC Patterns */
-		set_crtc_test_pattern(link, &pipe_ctx, test_pattern);
+		set_crtc_test_pattern(link, pipe_ctx, test_pattern);
 		/* Set Test Pattern state */
 		link->test_pattern_enabled = true;
 	}

commit 0971c40e180696c3512b9a63ca7ca5161cbfce32
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Thu Jul 27 09:33:33 2017 -0400

    drm/amd/display: Rename dc_stream to dc_stream_state
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/struct dc_stream/struct dc_stream_state/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/struct dc_stream_state_update/struct dc_stream_update/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/struct dc_stream_state_status/struct dc_stream_status/g'
    
    Plus some manual changes
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 17506345f97a..50724f9a8e2c 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1433,7 +1433,7 @@ bool dp_validate_mode_timing(
 		return false;
 }
 
-void decide_link_settings(struct dc_stream *stream,
+void decide_link_settings(struct dc_stream_state *stream,
 	struct dc_link_settings *link_setting)
 {
 

commit 4654a2f7fce0c2fd3dafe2c366ddf8e9e080fa44
Author: Roman Li <Roman.Li@amd.com>
Date:   Wed Jul 26 16:27:37 2017 -0400

    drm/amd/display: add detect caps for edp
    
    1. The caps detect sequence for edp is different from dp.
       Added separate function for edp.
    2. Removed max_link_setting and replaced it with verified
       to avoid confusion.
    3. Reverted sink count guard for edp powerdown as unnecessary
       with the changes above.
    
    Signed-off-by: Roman Li <Roman.Li@amd.com>
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index d9754b5f2543..17506345f97a 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1147,7 +1147,6 @@ bool dp_hbr_verify_link_cap(
 		LINK_SPREAD_DISABLED;
 	}
 
-	link->max_link_setting = link->verified_link_cap;
 
 	return success;
 }
@@ -1466,7 +1465,7 @@ void decide_link_settings(struct dc_stream *stream,
      * 2. could support the b/w requested by the timing
      */
 	while (current_link_setting.link_rate <=
-			link->max_link_setting.link_rate) {
+			link->verified_link_cap.link_rate) {
 		link_bw = bandwidth_in_kbps_from_link_settings(
 				&current_link_setting);
 		if (req_bw <= link_bw) {
@@ -1475,7 +1474,7 @@ void decide_link_settings(struct dc_stream *stream,
 		}
 
 		if (current_link_setting.lane_count <
-				link->max_link_setting.lane_count) {
+				link->verified_link_cap.lane_count) {
 			current_link_setting.lane_count =
 					increase_lane_count(
 							current_link_setting.lane_count);
@@ -2294,6 +2293,12 @@ void detect_dp_sink_caps(struct dc_link *link)
 	/* TODO save sink caps in link->sink */
 }
 
+void detect_edp_sink_caps(struct dc_link *link)
+{
+	retrieve_link_cap(link);
+	link->verified_link_cap = link->reported_link_cap;
+}
+
 void dc_link_dp_enable_hpd(const struct dc_link *link)
 {
 	struct link_encoder *encoder = link->link_enc;

commit 4fa086b9b6640818c053c79d4d7104790ba76cb7
Author: Leo (Sunpeng) Li <sunpeng.li@amd.com>
Date:   Tue Jul 25 20:51:26 2017 -0400

    drm/amd/display: Roll core_stream into dc_stream
    
    Signed-off-by: Leo (Sunpeng) Li <sunpeng.li@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c7b400786121..d9754b5f2543 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1434,7 +1434,7 @@ bool dp_validate_mode_timing(
 		return false;
 }
 
-void decide_link_settings(struct core_stream *stream,
+void decide_link_settings(struct dc_stream *stream,
 	struct dc_link_settings *link_setting)
 {
 
@@ -1446,8 +1446,7 @@ void decide_link_settings(struct core_stream *stream,
 	uint32_t req_bw;
 	uint32_t link_bw;
 
-	req_bw = bandwidth_in_kbps_from_timing(
-			&stream->public.timing);
+	req_bw = bandwidth_in_kbps_from_timing(&stream->timing);
 
 	link = stream->sink->link;
 
@@ -2327,7 +2326,7 @@ static void set_crtc_test_pattern(struct dc_link *link,
 {
 	enum controller_dp_test_pattern controller_test_pattern;
 	enum dc_color_depth color_depth = pipe_ctx->
-		stream->public.timing.display_color_depth;
+		stream->timing.display_color_depth;
 	struct bit_depth_reduction_params params;
 
 	memset(&params, 0, sizeof(params));

commit d0778ebfd58f5650de17531296ee5ecdde39ba68
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sat Jul 22 20:05:20 2017 -0400

    drm/amd/display: Roll core_link into dc_link
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index e90b3ebc8347..c7b400786121 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -34,7 +34,7 @@ enum {
 };
 
 static void wait_for_training_aux_rd_interval(
-	struct core_link* link,
+	struct dc_link *link,
 	uint32_t default_wait_in_micro_secs)
 {
 	union training_aux_rd_interval training_rd_interval;
@@ -63,7 +63,7 @@ static void wait_for_training_aux_rd_interval(
 }
 
 static void dpcd_set_training_pattern(
-	struct core_link* link,
+	struct dc_link *link,
 	union dpcd_training_pattern dpcd_pattern)
 {
 	core_link_write_dpcd(
@@ -80,7 +80,7 @@ static void dpcd_set_training_pattern(
 }
 
 static void dpcd_set_link_settings(
-	struct core_link* link,
+	struct dc_link *link,
 	const struct link_training_settings *lt_settings)
 {
 	uint8_t rate = (uint8_t)
@@ -123,7 +123,7 @@ static void dpcd_set_link_settings(
 
 static enum dpcd_training_patterns
 	hw_training_pattern_to_dpcd_training_pattern(
-	struct core_link* link,
+	struct dc_link *link,
 	enum hw_dp_training_pattern pattern)
 {
 	enum dpcd_training_patterns dpcd_tr_pattern =
@@ -155,7 +155,7 @@ static enum dpcd_training_patterns
 }
 
 static void dpcd_set_lt_pattern_and_lane_settings(
-	struct core_link* link,
+	struct dc_link *link,
 	const struct link_training_settings *lt_settings,
 	enum hw_dp_training_pattern pattern)
 {
@@ -246,7 +246,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 				dpcd_lt_buffer,
 				size_in_bytes + sizeof(dpcd_pattern.raw) );
 
-	link->public.cur_lane_setting = lt_settings->lane_settings[0];
+	link->cur_lane_setting = lt_settings->lane_settings[0];
 }
 
 static bool is_cr_done(enum dc_lane_count ln_count,
@@ -419,7 +419,7 @@ static void find_max_drive_settings(
 }
 
 static void get_lane_status_and_drive_settings(
-	struct core_link* link,
+	struct dc_link *link,
 	const struct link_training_settings *link_training_setting,
 	union lane_status *ln_status,
 	union lane_align_status_updated *ln_status_updated,
@@ -500,7 +500,7 @@ static void get_lane_status_and_drive_settings(
 }
 
 static void dpcd_set_lane_settings(
-	struct core_link* link,
+	struct dc_link *link,
 	const struct link_training_settings *link_training_setting)
 {
 	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
@@ -560,7 +560,7 @@ static void dpcd_set_lane_settings(
 		dpcd_lane[0].bits.MAX_SWING_REACHED,
 		dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
 
-	link->public.cur_lane_setting = link_training_setting->lane_settings[0];
+	link->cur_lane_setting = link_training_setting->lane_settings[0];
 
 }
 
@@ -580,19 +580,18 @@ static bool is_max_vs_reached(
 }
 
 void dc_link_dp_set_drive_settings(
-	const struct dc_link *link,
+	struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
-	struct core_link *core_link = DC_LINK_TO_CORE(link);
 	/* program ASIC PHY settings*/
-	dp_set_hw_lane_settings(core_link, lt_settings);
+	dp_set_hw_lane_settings(link, lt_settings);
 
 	/* Notify DP sink the PHY settings from source */
-	dpcd_set_lane_settings(core_link, lt_settings);
+	dpcd_set_lane_settings(link, lt_settings);
 }
 
 static bool perform_post_lt_adj_req_sequence(
-	struct core_link *link,
+	struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
 	enum dc_lane_count lane_count =
@@ -656,7 +655,7 @@ static bool perform_post_lt_adj_req_sequence(
 				update_drive_settings(
 					lt_settings,req_settings);
 
-				dc_link_dp_set_drive_settings(&link->public,
+				dc_link_dp_set_drive_settings(link,
 						lt_settings);
 				break;
 			}
@@ -682,7 +681,7 @@ static bool perform_post_lt_adj_req_sequence(
 
 }
 
-static enum hw_dp_training_pattern get_supported_tp(struct core_link *link)
+static enum hw_dp_training_pattern get_supported_tp(struct dc_link *link)
 {
 	enum hw_dp_training_pattern highest_tp = HW_DP_TRAINING_PATTERN_2;
 	struct encoder_feature_support *features = &link->link_enc->features;
@@ -706,7 +705,7 @@ static enum hw_dp_training_pattern get_supported_tp(struct core_link *link)
 }
 
 static enum link_training_result perform_channel_equalization_sequence(
-	struct core_link *link,
+	struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
 	struct link_training_settings req_settings;
@@ -768,7 +767,7 @@ static enum link_training_result perform_channel_equalization_sequence(
 }
 
 static bool perform_clock_recovery_sequence(
-	struct core_link *link,
+	struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
 	uint32_t retries_cr;
@@ -877,7 +876,7 @@ static bool perform_clock_recovery_sequence(
 }
 
 static inline bool perform_link_training_int(
-	struct core_link *link,
+	struct dc_link *link,
 	struct link_training_settings *lt_settings,
 	bool status)
 {
@@ -923,7 +922,6 @@ enum link_training_result dc_link_dp_perform_link_training(
 	bool skip_video_pattern)
 {
 	enum link_training_result status = LINK_TRAINING_SUCCESS;
-	struct core_link *core_link = DC_LINK_TO_CORE(link);
 
 	char *link_rate = "Unknown";
 	struct link_training_settings lt_settings;
@@ -945,19 +943,19 @@ enum link_training_result dc_link_dp_perform_link_training(
 	lt_settings.link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
 
 	/* 1. set link rate, lane count and spread*/
-	dpcd_set_link_settings(core_link, &lt_settings);
+	dpcd_set_link_settings(link, &lt_settings);
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)*/
-	if (!perform_clock_recovery_sequence(core_link, &lt_settings)) {
+	if (!perform_clock_recovery_sequence(link, &lt_settings)) {
 		status = LINK_TRAINING_CR_FAIL;
 	} else {
-		status = perform_channel_equalization_sequence(core_link,
+		status = perform_channel_equalization_sequence(link,
 				&lt_settings);
 	}
 
 	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
-		if (!perform_link_training_int(core_link,
+		if (!perform_link_training_int(link,
 				&lt_settings,
 				status == LINK_TRAINING_SUCCESS)) {
 			/* the next link training setting in this case
@@ -990,7 +988,7 @@ enum link_training_result dc_link_dp_perform_link_training(
 	}
 
 	/* Connectivity log: link training */
-	CONN_MSG_LT(core_link, "%sx%d %s VS=%d, PE=%d",
+	CONN_MSG_LT(link, "%sx%d %s VS=%d, PE=%d",
 			link_rate,
 			lt_settings.link_settings.lane_count,
 			(status ==  LINK_TRAINING_SUCCESS) ? "pass" :
@@ -1004,7 +1002,7 @@ enum link_training_result dc_link_dp_perform_link_training(
 
 
 bool perform_link_training_with_retries(
-	struct core_link *link,
+	struct dc_link *link,
 	const struct dc_link_settings *link_setting,
 	bool skip_video_pattern,
 	int attempts)
@@ -1015,7 +1013,7 @@ bool perform_link_training_with_retries(
 	for (j = 0; j < attempts; ++j) {
 
 		if (dc_link_dp_perform_link_training(
-				&link->public,
+				link,
 				link_setting,
 				skip_video_pattern) == LINK_TRAINING_SUCCESS)
 			return true;
@@ -1027,7 +1025,7 @@ bool perform_link_training_with_retries(
 	return false;
 }
 
-static struct dc_link_settings get_max_link_cap(struct core_link *link)
+static struct dc_link_settings get_max_link_cap(struct dc_link *link)
 {
 	/* Set Default link settings */
 	struct dc_link_settings max_link_cap = {LANE_COUNT_FOUR, LINK_RATE_HIGH,
@@ -1041,21 +1039,21 @@ static struct dc_link_settings get_max_link_cap(struct core_link *link)
 		max_link_cap.link_rate = LINK_RATE_HIGH3;
 
 	/* Lower link settings based on sink's link cap */
-	if (link->public.reported_link_cap.lane_count < max_link_cap.lane_count)
+	if (link->reported_link_cap.lane_count < max_link_cap.lane_count)
 		max_link_cap.lane_count =
-				link->public.reported_link_cap.lane_count;
-	if (link->public.reported_link_cap.link_rate < max_link_cap.link_rate)
+				link->reported_link_cap.lane_count;
+	if (link->reported_link_cap.link_rate < max_link_cap.link_rate)
 		max_link_cap.link_rate =
-				link->public.reported_link_cap.link_rate;
-	if (link->public.reported_link_cap.link_spread <
+				link->reported_link_cap.link_rate;
+	if (link->reported_link_cap.link_spread <
 			max_link_cap.link_spread)
 		max_link_cap.link_spread =
-				link->public.reported_link_cap.link_spread;
+				link->reported_link_cap.link_spread;
 	return max_link_cap;
 }
 
 bool dp_hbr_verify_link_cap(
-	struct core_link *link,
+	struct dc_link *link,
 	struct dc_link_settings *known_limit_link_setting)
 {
 	struct dc_link_settings max_link_cap = {0};
@@ -1080,7 +1078,7 @@ bool dp_hbr_verify_link_cap(
 	 * find the physical link capability
 	 */
 	/* disable PHY done possible by BIOS, will be done by driver itself */
-	dp_disable_link_phy(link, link->public.connector_signal);
+	dp_disable_link_phy(link, link->connector_signal);
 
 	dp_cs = link->dc->res_pool->dp_clock_source;
 
@@ -1109,7 +1107,7 @@ bool dp_hbr_verify_link_cap(
 
 		dp_enable_link_phy(
 				link,
-				link->public.connector_signal,
+				link->connector_signal,
 				dp_cs_id,
 				cur);
 
@@ -1117,7 +1115,7 @@ bool dp_hbr_verify_link_cap(
 			success = true;
 		else {
 			status = dc_link_dp_perform_link_training(
-							&link->public,
+							link,
 							cur,
 							skip_video_pattern);
 			if (status == LINK_TRAINING_SUCCESS)
@@ -1125,13 +1123,13 @@ bool dp_hbr_verify_link_cap(
 		}
 
 		if (success)
-			link->public.verified_link_cap = *cur;
+			link->verified_link_cap = *cur;
 
 		/* always disable the link before trying another
 		 * setting or before returning we'll enable it later
 		 * based on the actual mode we're driving
 		 */
-		dp_disable_link_phy(link, link->public.connector_signal);
+		dp_disable_link_phy(link, link->connector_signal);
 	} while (!success && decide_fallback_link_setting(
 			initial_link_settings, cur, status));
 
@@ -1142,14 +1140,14 @@ bool dp_hbr_verify_link_cap(
 		/* If all LT fails for all settings,
 		 * set verified = failed safe (1 lane low)
 		 */
-		link->public.verified_link_cap.lane_count = LANE_COUNT_ONE;
-		link->public.verified_link_cap.link_rate = LINK_RATE_LOW;
+		link->verified_link_cap.lane_count = LANE_COUNT_ONE;
+		link->verified_link_cap.link_rate = LINK_RATE_LOW;
 
-		link->public.verified_link_cap.link_spread =
+		link->verified_link_cap.link_spread =
 		LINK_SPREAD_DISABLED;
 	}
 
-	link->public.max_link_setting = link->public.verified_link_cap;
+	link->max_link_setting = link->verified_link_cap;
 
 	return success;
 }
@@ -1391,7 +1389,7 @@ static uint32_t bandwidth_in_kbps_from_link_settings(
 }
 
 bool dp_validate_mode_timing(
-	struct core_link *link,
+	struct dc_link *link,
 	const struct dc_crtc_timing *timing)
 {
 	uint32_t req_bw;
@@ -1406,12 +1404,12 @@ bool dp_validate_mode_timing(
 		return true;
 
 	/* We always use verified link settings */
-	link_setting = &link->public.verified_link_cap;
+	link_setting = &link->verified_link_cap;
 
 	/* TODO: DYNAMIC_VALIDATION needs to be implemented */
 	/*if (flags.DYNAMIC_VALIDATION == 1 &&
-		link->public.verified_link_cap.lane_count != LANE_COUNT_UNKNOWN)
-		link_setting = &link->public.verified_link_cap;
+		link->verified_link_cap.lane_count != LANE_COUNT_UNKNOWN)
+		link_setting = &link->verified_link_cap;
 	*/
 
 	req_bw = bandwidth_in_kbps_from_timing(timing);
@@ -1444,7 +1442,7 @@ void decide_link_settings(struct core_stream *stream,
 		LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED};
 	struct dc_link_settings current_link_setting =
 			initial_link_setting;
-	struct core_link* link;
+	struct dc_link *link;
 	uint32_t req_bw;
 	uint32_t link_bw;
 
@@ -1456,11 +1454,11 @@ void decide_link_settings(struct core_stream *stream,
 	/* if preferred is specified through AMDDP, use it, if it's enough
 	 * to drive the mode
 	 */
-	if (link->public.preferred_link_setting.lane_count !=
+	if (link->preferred_link_setting.lane_count !=
 			LANE_COUNT_UNKNOWN &&
-			link->public.preferred_link_setting.link_rate !=
+			link->preferred_link_setting.link_rate !=
 					LINK_RATE_UNKNOWN) {
-		*link_setting =  link->public.preferred_link_setting;
+		*link_setting =  link->preferred_link_setting;
 		return;
 	}
 
@@ -1469,7 +1467,7 @@ void decide_link_settings(struct core_stream *stream,
      * 2. could support the b/w requested by the timing
      */
 	while (current_link_setting.link_rate <=
-			link->public.max_link_setting.link_rate) {
+			link->max_link_setting.link_rate) {
 		link_bw = bandwidth_in_kbps_from_link_settings(
 				&current_link_setting);
 		if (req_bw <= link_bw) {
@@ -1478,7 +1476,7 @@ void decide_link_settings(struct core_stream *stream,
 		}
 
 		if (current_link_setting.lane_count <
-				link->public.max_link_setting.lane_count) {
+				link->max_link_setting.lane_count) {
 			current_link_setting.lane_count =
 					increase_lane_count(
 							current_link_setting.lane_count);
@@ -1492,16 +1490,15 @@ void decide_link_settings(struct core_stream *stream,
 	}
 
 	BREAK_TO_DEBUGGER();
-	ASSERT(link->public.verified_link_cap.lane_count !=
-			LANE_COUNT_UNKNOWN);
+	ASSERT(link->verified_link_cap.lane_count != LANE_COUNT_UNKNOWN);
 
-	*link_setting = link->public.verified_link_cap;
+	*link_setting = link->verified_link_cap;
 }
 
 /*************************Short Pulse IRQ***************************/
 
 static bool hpd_rx_irq_check_link_loss_status(
-	struct core_link *link,
+	struct dc_link *link,
 	union hpd_irq_data *hpd_irq_dpcd_data)
 {
 	uint8_t irq_reg_rx_power_state;
@@ -1514,7 +1511,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 	sink_status_changed = false;
 	return_code = false;
 
-	if (link->public.cur_link_settings.lane_count == 0)
+	if (link->cur_link_settings.lane_count == 0)
 		return return_code;
 	/*1. Check that we can handle interrupt: Not in FS DOS,
 	 *  Not in "Display Timeout" state, Link is trained.
@@ -1538,7 +1535,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 
 		/*parse lane status*/
 		for (lane = 0;
-			lane < link->public.cur_link_settings.lane_count;
+			lane < link->cur_link_settings.lane_count;
 			lane++) {
 
 			/* check status of lanes 0,1
@@ -1577,7 +1574,7 @@ static bool hpd_rx_irq_check_link_loss_status(
 }
 
 static enum dc_status read_hpd_rx_irq_data(
-	struct core_link *link,
+	struct dc_link *link,
 	union hpd_irq_data *irq_data)
 {
 	/* The HW reads 16 bytes from 200h on HPD,
@@ -1593,7 +1590,7 @@ static enum dc_status read_hpd_rx_irq_data(
 	sizeof(union hpd_irq_data));
 }
 
-static bool allow_hpd_rx_irq(const struct core_link *link)
+static bool allow_hpd_rx_irq(const struct dc_link *link)
 {
 	/*
 	 * Don't handle RX IRQ unless one of following is met:
@@ -1602,15 +1599,15 @@ static bool allow_hpd_rx_irq(const struct core_link *link)
 	 * 3) We know we're dealing with an active dongle
 	 */
 
-	if ((link->public.cur_link_settings.lane_count != LANE_COUNT_UNKNOWN) ||
-		(link->public.type == dc_connection_mst_branch) ||
+	if ((link->cur_link_settings.lane_count != LANE_COUNT_UNKNOWN) ||
+		(link->type == dc_connection_mst_branch) ||
 		is_dp_active_dongle(link))
 		return true;
 
 	return false;
 }
 
-static bool handle_hpd_irq_psr_sink(const struct core_link *link)
+static bool handle_hpd_irq_psr_sink(const struct dc_link *link)
 {
 	union dpcd_psr_configuration psr_configuration;
 
@@ -1619,7 +1616,7 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 
 	dm_helpers_dp_read_dpcd(
 		link->ctx,
-		&link->public,
+		link,
 		368,/*DpcdAddress_PSR_Enable_Cfg*/
 		&psr_configuration.raw,
 		sizeof(psr_configuration.raw));
@@ -1632,7 +1629,7 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 
 		dm_helpers_dp_read_dpcd(
 			link->ctx,
-			&link->public,
+			link,
 			0x2006, /*DpcdAddress_PSR_Error_Status*/
 			(unsigned char *) dpcdbuf,
 			sizeof(dpcdbuf));
@@ -1647,14 +1644,14 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 			/* Acknowledge and clear error bits */
 			dm_helpers_dp_write_dpcd(
 				link->ctx,
-				&link->public,
+				link,
 				8198,/*DpcdAddress_PSR_Error_Status*/
 				&psr_error_status.raw,
 				sizeof(psr_error_status.raw));
 
 			/* PSR error, disable and re-enable PSR */
-			dc_link_set_psr_enable(&link->public, false);
-			dc_link_set_psr_enable(&link->public, true);
+			dc_link_set_psr_enable(link, false);
+			dc_link_set_psr_enable(link, true);
 
 			return true;
 		} else if (psr_sink_psr_status.bits.SINK_SELF_REFRESH_STATUS ==
@@ -1670,7 +1667,7 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 	return false;
 }
 
-static void dp_test_send_link_training(struct core_link *link)
+static void dp_test_send_link_training(struct dc_link *link)
 {
 	struct dc_link_settings link_settings = {0};
 
@@ -1686,13 +1683,13 @@ static void dp_test_send_link_training(struct core_link *link)
 			1);
 
 	/* Set preferred link settings */
-	link->public.verified_link_cap.lane_count = link_settings.lane_count;
-	link->public.verified_link_cap.link_rate = link_settings.link_rate;
+	link->verified_link_cap.lane_count = link_settings.lane_count;
+	link->verified_link_cap.link_rate = link_settings.link_rate;
 
 	dp_retrain_link_dp_test(link, &link_settings, false);
 }
 
-static void dp_test_send_phy_test_pattern(struct core_link *link)
+static void dp_test_send_phy_test_pattern(struct dc_link *link)
 {
 	union phy_test_pattern dpcd_test_pattern;
 	union lane_adjust dpcd_lane_adjustment[2];
@@ -1771,10 +1768,10 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 				sizeof(test_80_bit_pattern));
 
 	/* prepare link training settings */
-	link_settings.link = link->public.cur_link_settings;
+	link_settings.link = link->cur_link_settings;
 
 	for (lane = 0; lane <
-		(unsigned int)(link->public.cur_link_settings.lane_count);
+		(unsigned int)(link->cur_link_settings.lane_count);
 		lane++) {
 		dpcd_lane_adjust.raw =
 			get_nibble_at_index(&dpcd_lane_adjustment[0].raw, lane);
@@ -1802,7 +1799,7 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	 * forward request to DS
 	 */
 	dc_link_dp_set_test_pattern(
-		&link->public,
+		link,
 		test_pattern,
 		&link_training_settings,
 		test_80_bit_pattern,
@@ -1810,7 +1807,7 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 		DP_TEST_80BIT_CUSTOM_PATTERN_7_0)+1);
 }
 
-static void dp_test_send_link_test_pattern(struct core_link *link)
+static void dp_test_send_link_test_pattern(struct dc_link *link)
 {
 	union link_test_pattern dpcd_test_pattern;
 	union test_misc dpcd_test_params;
@@ -1850,14 +1847,14 @@ static void dp_test_send_link_test_pattern(struct core_link *link)
 	}
 
 	dc_link_dp_set_test_pattern(
-			&link->public,
+			link,
 			test_pattern,
 			NULL,
 			NULL,
 			0);
 }
 
-static void handle_automated_test(struct core_link *link)
+static void handle_automated_test(struct dc_link *link)
 {
 	union test_request test_request;
 	union test_response test_response;
@@ -1904,9 +1901,8 @@ static void handle_automated_test(struct core_link *link)
 			sizeof(test_response));
 }
 
-bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data *out_hpd_irq_dpcd_data)
+bool dc_link_handle_hpd_rx_irq(struct dc_link *link, union hpd_irq_data *out_hpd_irq_dpcd_data)
 {
-	struct core_link *link = DC_LINK_TO_LINK(dc_link);
 	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
 	union device_service_irq device_service_clear = { { 0 } };
 	enum dc_status result = DDC_RESULT_UNKNOWN;
@@ -1917,7 +1913,7 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data
 
 	dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
 		"%s: Got short pulse HPD on link %d\n",
-		__func__, link->public.link_index);
+		__func__, link->link_index);
 
 
 	 /* All the "handle_hpd_irq_xxx()" methods
@@ -1951,7 +1947,7 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data
 	if (!allow_hpd_rx_irq(link)) {
 		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
 			"%s: skipping HPD handling on %d\n",
-			__func__, link->public.link_index);
+			__func__, link->link_index);
 		return false;
 	}
 
@@ -1984,13 +1980,13 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data
 					"Status: ");
 
 		perform_link_training_with_retries(link,
-			&link->public.cur_link_settings,
+			&link->cur_link_settings,
 			true, LINK_TRAINING_ATTEMPTS);
 
 		status = false;
 	}
 
-	if (link->public.type == dc_connection_active_dongle &&
+	if (link->type == dc_connection_active_dongle &&
 		hpd_irq_dpcd_data.bytes.sink_cnt.bits.SINK_COUNT
 			!= link->dpcd_sink_count)
 		status = true;
@@ -2010,7 +2006,7 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data
 }
 
 /*query dpcd for version and mst cap addresses*/
-bool is_mst_supported(struct core_link *link)
+bool is_mst_supported(struct dc_link *link)
 {
 	bool mst          = false;
 	enum dc_status st = DC_OK;
@@ -2034,7 +2030,7 @@ bool is_mst_supported(struct core_link *link)
 
 }
 
-bool is_dp_active_dongle(const struct core_link *link)
+bool is_dp_active_dongle(const struct dc_link *link)
 {
 	enum display_dongle_type dongle_type = link->dpcd_caps.dongle_type;
 
@@ -2044,14 +2040,14 @@ bool is_dp_active_dongle(const struct core_link *link)
 }
 
 static void get_active_converter_info(
-	uint8_t data, struct core_link *link)
+	uint8_t data, struct dc_link *link)
 {
 	union dp_downstream_port_present ds_port = { .byte = data };
 
 	/* decode converter info*/
 	if (!ds_port.fields.PORT_PRESENT) {
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
-		ddc_service_set_dongle_type(link->public.ddc,
+		ddc_service_set_dongle_type(link->ddc,
 				link->dpcd_caps.dongle_type);
 		return;
 	}
@@ -2121,7 +2117,7 @@ static void get_active_converter_info(
 		}
 	}
 
-	ddc_service_set_dongle_type(link->public.ddc, link->dpcd_caps.dongle_type);
+	ddc_service_set_dongle_type(link->ddc, link->dpcd_caps.dongle_type);
 
 	{
 		struct dp_device_vendor_id dp_id;
@@ -2158,7 +2154,7 @@ static void get_active_converter_info(
 	}
 }
 
-static void dp_wa_power_up_0010FA(struct core_link *link, uint8_t *dpcd_data,
+static void dp_wa_power_up_0010FA(struct dc_link *link, uint8_t *dpcd_data,
 		int length)
 {
 	int retry = 0;
@@ -2199,7 +2195,7 @@ static void dp_wa_power_up_0010FA(struct core_link *link, uint8_t *dpcd_data,
 		link->wa_flags.dp_keep_receiver_powered = false;
 }
 
-static void retrieve_link_cap(struct core_link *link)
+static void retrieve_link_cap(struct dc_link *link)
 {
 	uint8_t dpcd_data[DP_TRAINING_AUX_RD_INTERVAL - DP_DPCD_REV + 1];
 
@@ -2253,11 +2249,11 @@ static void retrieve_link_cap(struct core_link *link)
 	link->dpcd_caps.max_down_spread.raw = dpcd_data[
 		DP_MAX_DOWNSPREAD - DP_DPCD_REV];
 
-	link->public.reported_link_cap.lane_count =
+	link->reported_link_cap.lane_count =
 		link->dpcd_caps.max_ln_count.bits.MAX_LANE_COUNT;
-	link->public.reported_link_cap.link_rate = dpcd_data[
+	link->reported_link_cap.link_rate = dpcd_data[
 		DP_MAX_LINK_RATE - DP_DPCD_REV];
-	link->public.reported_link_cap.link_spread =
+	link->reported_link_cap.link_spread =
 		link->dpcd_caps.max_down_spread.bits.MAX_DOWN_SPREAD ?
 		LINK_SPREAD_05_DOWNSPREAD_30KHZ : LINK_SPREAD_DISABLED;
 
@@ -2266,8 +2262,8 @@ static void retrieve_link_cap(struct core_link *link)
 	link->dpcd_caps.panel_mode_edp =
 		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
 
-	link->public.test_pattern_enabled = false;
-	link->public.compliance_test_state.raw = 0;
+	link->test_pattern_enabled = false;
+	link->compliance_test_state.raw = 0;
 
 	/* read sink count */
 	core_link_read_dpcd(link,
@@ -2279,7 +2275,7 @@ static void retrieve_link_cap(struct core_link *link)
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 }
 
-void detect_dp_sink_caps(struct core_link *link)
+void detect_dp_sink_caps(struct dc_link *link)
 {
 	retrieve_link_cap(link);
 
@@ -2291,18 +2287,17 @@ void detect_dp_sink_caps(struct core_link *link)
 	 */
 
 	if (is_mst_supported(link)) {
-		link->public.verified_link_cap = link->public.reported_link_cap;
+		link->verified_link_cap = link->reported_link_cap;
 	} else {
 		dp_hbr_verify_link_cap(link,
-			&link->public.reported_link_cap);
+			&link->reported_link_cap);
 	}
 	/* TODO save sink caps in link->sink */
 }
 
 void dc_link_dp_enable_hpd(const struct dc_link *link)
 {
-	struct core_link *core_link = DC_LINK_TO_CORE(link);
-	struct link_encoder *encoder = core_link->link_enc;
+	struct link_encoder *encoder = link->link_enc;
 
 	if (encoder != NULL && encoder->funcs->enable_hpd != NULL)
 		encoder->funcs->enable_hpd(encoder);
@@ -2310,8 +2305,7 @@ void dc_link_dp_enable_hpd(const struct dc_link *link)
 
 void dc_link_dp_disable_hpd(const struct dc_link *link)
 {
-	struct core_link *core_link = DC_LINK_TO_CORE(link);
-	struct link_encoder *encoder = core_link->link_enc;
+	struct link_encoder *encoder = link->link_enc;
 
 	if (encoder != NULL && encoder->funcs->enable_hpd != NULL)
 		encoder->funcs->disable_hpd(encoder);
@@ -2327,7 +2321,7 @@ static bool is_dp_phy_pattern(enum dp_test_pattern test_pattern)
 		return false;
 }
 
-static void set_crtc_test_pattern(struct core_link *link,
+static void set_crtc_test_pattern(struct dc_link *link,
 				struct pipe_ctx *pipe_ctx,
 				enum dp_test_pattern test_pattern)
 {
@@ -2402,15 +2396,13 @@ static void set_crtc_test_pattern(struct core_link *link,
 }
 
 bool dc_link_dp_set_test_pattern(
-	const struct dc_link *link,
+	struct dc_link *link,
 	enum dp_test_pattern test_pattern,
 	const struct link_training_settings *p_link_settings,
 	const unsigned char *p_custom_pattern,
 	unsigned int cust_pattern_size)
 {
-	struct core_link *core_link = DC_LINK_TO_CORE(link);
-	struct pipe_ctx *pipes =
-			core_link->dc->current_context->res_ctx.pipe_ctx;
+	struct pipe_ctx *pipes = link->dc->current_context->res_ctx.pipe_ctx;
 	struct pipe_ctx pipe_ctx = pipes[0];
 	unsigned int lane;
 	unsigned int i;
@@ -2421,7 +2413,7 @@ bool dc_link_dp_set_test_pattern(
 	memset(&training_pattern, 0, sizeof(training_pattern));
 
 	for (i = 0; i < MAX_PIPES; i++) {
-		if (pipes[i].stream->sink->link == core_link) {
+		if (pipes[i].stream->sink->link == link) {
 			pipe_ctx = pipes[i];
 			break;
 		}
@@ -2431,24 +2423,24 @@ bool dc_link_dp_set_test_pattern(
 	 * is VideoMode Reset DP Phy Test Pattern if it is currently running
 	 * and request is VideoMode
 	 */
-	if (core_link->public.test_pattern_enabled && test_pattern ==
+	if (link->test_pattern_enabled && test_pattern ==
 			DP_TEST_PATTERN_VIDEO_MODE) {
 		/* Set CRTC Test Pattern */
-		set_crtc_test_pattern(core_link, &pipe_ctx, test_pattern);
-		dp_set_hw_test_pattern(core_link, test_pattern,
+		set_crtc_test_pattern(link, &pipe_ctx, test_pattern);
+		dp_set_hw_test_pattern(link, test_pattern,
 				(uint8_t *)p_custom_pattern,
 				(uint32_t)cust_pattern_size);
 
 		/* Unblank Stream */
-		core_link->dc->hwss.unblank_stream(
+		link->dc->hwss.unblank_stream(
 			&pipe_ctx,
-			&core_link->public.verified_link_cap);
+			&link->verified_link_cap);
 		/* TODO:m_pHwss->MuteAudioEndpoint
 		 * (pPathMode->pDisplayPath, false);
 		 */
 
 		/* Reset Test Pattern state */
-		core_link->public.test_pattern_enabled = false;
+		link->test_pattern_enabled = false;
 
 		return true;
 	}
@@ -2457,8 +2449,8 @@ bool dc_link_dp_set_test_pattern(
 	if (is_dp_phy_pattern(test_pattern)) {
 		/* Set DPCD Lane Settings before running test pattern */
 		if (p_link_settings != NULL) {
-			dp_set_hw_lane_settings(core_link, p_link_settings);
-			dpcd_set_lane_settings(core_link, p_link_settings);
+			dp_set_hw_lane_settings(link, p_link_settings);
+			dpcd_set_lane_settings(link, p_link_settings);
 		}
 
 		/* Blank stream if running test pattern */
@@ -2471,15 +2463,15 @@ bool dc_link_dp_set_test_pattern(
 			pipes->stream_enc->funcs->dp_blank(pipe_ctx.stream_enc);
 		}
 
-		dp_set_hw_test_pattern(core_link, test_pattern,
+		dp_set_hw_test_pattern(link, test_pattern,
 				(uint8_t *)p_custom_pattern,
 				(uint32_t)cust_pattern_size);
 
 		if (test_pattern != DP_TEST_PATTERN_VIDEO_MODE) {
 			/* Set Test Pattern state */
-			core_link->public.test_pattern_enabled = true;
+			link->test_pattern_enabled = true;
 			if (p_link_settings != NULL)
-				dpcd_set_link_settings(core_link,
+				dpcd_set_link_settings(link,
 						p_link_settings);
 		}
 
@@ -2516,7 +2508,7 @@ bool dc_link_dp_set_test_pattern(
 		/*TODO:&& !pPathMode->pDisplayPath->IsTargetPoweredOn()*/)
 			return false;
 
-		if (core_link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {
+		if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {
 			/* tell receiver that we are sending qualification
 			 * pattern DP 1.2 or later - DP receiver's link quality
 			 * pattern is set using DPCD LINK_QUAL_LANEx_SET
@@ -2526,12 +2518,12 @@ bool dc_link_dp_set_test_pattern(
 				link_qual_pattern[lane] =
 						(unsigned char)(pattern);
 
-			core_link_write_dpcd(core_link,
+			core_link_write_dpcd(link,
 					DP_LINK_QUAL_LANE0_SET,
 					link_qual_pattern,
 					sizeof(link_qual_pattern));
-		} else if (core_link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_10 ||
-				core_link->dpcd_caps.dpcd_rev.raw == 0) {
+		} else if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_10 ||
+			   link->dpcd_caps.dpcd_rev.raw == 0) {
 			/* tell receiver that we are sending qualification
 			 * pattern DP 1.1a or earlier - DP receiver's link
 			 * quality pattern is set using
@@ -2539,27 +2531,25 @@ bool dc_link_dp_set_test_pattern(
 			 * register (0x102). We will use v_1.3 when we are
 			 * setting test pattern for DP 1.1.
 			 */
-			core_link_read_dpcd(core_link,
-					DP_TRAINING_PATTERN_SET,
-					&training_pattern.raw,
-					sizeof(training_pattern));
+			core_link_read_dpcd(link, DP_TRAINING_PATTERN_SET,
+					    &training_pattern.raw,
+					    sizeof(training_pattern));
 			training_pattern.v1_3.LINK_QUAL_PATTERN_SET = pattern;
-			core_link_write_dpcd(core_link,
-					DP_TRAINING_PATTERN_SET,
-					&training_pattern.raw,
-					sizeof(training_pattern));
+			core_link_write_dpcd(link, DP_TRAINING_PATTERN_SET,
+					     &training_pattern.raw,
+					     sizeof(training_pattern));
 		}
 	} else {
 	/* CRTC Patterns */
-		set_crtc_test_pattern(core_link, &pipe_ctx, test_pattern);
+		set_crtc_test_pattern(link, &pipe_ctx, test_pattern);
 		/* Set Test Pattern state */
-		core_link->public.test_pattern_enabled = true;
+		link->test_pattern_enabled = true;
 	}
 
 	return true;
 }
 
-void dp_enable_mst_on_sink(struct core_link *link, bool enable)
+void dp_enable_mst_on_sink(struct dc_link *link, bool enable)
 {
 	unsigned char mstmCntl;
 

commit 8c4abe0b07a12c402f009abed8217e6c2e33a300
Author: Ding Wang <Ding.Wang@amd.com>
Date:   Tue Jul 18 17:18:11 2017 -0400

    drm/amd/display: fix decide_link_settings
    
    Signed-off-by: Ding Wang <Ding.Wang@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 445cd226d36d..e90b3ebc8347 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -33,32 +33,6 @@ enum {
 	LINK_TRAINING_MAX_CR_RETRY = 100
 };
 
-static const struct dc_link_settings link_training_fallback_table[] = {
-/* 4320 Mbytes/sec*/
-{ LANE_COUNT_FOUR, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
-/* 2160 Mbytes/sec*/
-{ LANE_COUNT_FOUR, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
-/* 1080 Mbytes/sec*/
-{ LANE_COUNT_FOUR, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
-/* 648 Mbytes/sec*/
-{ LANE_COUNT_FOUR, LINK_RATE_LOW, LINK_SPREAD_DISABLED },
-/* 2160 Mbytes/sec*/
-{ LANE_COUNT_TWO, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
-/* 1080 Mbytes/sec*/
-{ LANE_COUNT_TWO, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
-/* 540 Mbytes/sec*/
-{ LANE_COUNT_TWO, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
-/* 324 Mbytes/sec*/
-{ LANE_COUNT_TWO, LINK_RATE_LOW, LINK_SPREAD_DISABLED },
-/* 1080 Mbytes/sec*/
-{ LANE_COUNT_ONE, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
-/* 540 Mbytes/sec*/
-{ LANE_COUNT_ONE, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
-/* 270 Mbytes/sec*/
-{ LANE_COUNT_ONE, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
-/* 162 Mbytes/sec*/
-{ LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED } };
-
 static void wait_for_training_aux_rd_interval(
 	struct core_link* link,
 	uint32_t default_wait_in_micro_secs)
@@ -1053,29 +1027,6 @@ bool perform_link_training_with_retries(
 	return false;
 }
 
-/*TODO add more check to see if link support request link configuration */
-static bool is_link_setting_supported(
-	const struct dc_link_settings *link_setting,
-	const struct dc_link_settings *max_link_setting)
-{
-	if (link_setting->lane_count > max_link_setting->lane_count ||
-		link_setting->link_rate > max_link_setting->link_rate)
-		return false;
-	return true;
-}
-
-static const uint32_t get_link_training_fallback_table_len(
-	struct core_link *link)
-{
-	return ARRAY_SIZE(link_training_fallback_table);
-}
-
-static const struct dc_link_settings *get_link_training_fallback_table(
-	struct core_link *link, uint32_t i)
-{
-	return &link_training_fallback_table[i];
-}
-
 static struct dc_link_settings get_max_link_cap(struct core_link *link)
 {
 	/* Set Default link settings */
@@ -1284,6 +1235,32 @@ enum dc_link_rate reduce_link_rate(enum dc_link_rate link_rate)
 	}
 }
 
+enum dc_lane_count increase_lane_count(enum dc_lane_count lane_count)
+{
+	switch (lane_count) {
+	case LANE_COUNT_ONE:
+		return LANE_COUNT_TWO;
+	case LANE_COUNT_TWO:
+		return LANE_COUNT_FOUR;
+	default:
+		return LANE_COUNT_UNKNOWN;
+	}
+}
+
+enum dc_link_rate increase_link_rate(enum dc_link_rate link_rate)
+{
+	switch (link_rate) {
+	case LINK_RATE_LOW:
+		return LINK_RATE_HIGH;
+	case LINK_RATE_HIGH:
+		return LINK_RATE_HIGH2;
+	case LINK_RATE_HIGH2:
+		return LINK_RATE_HIGH3;
+	default:
+		return LINK_RATE_UNKNOWN;
+	}
+}
+
 /*
  * function: set link rate and lane count fallback based
  * on current link setting and last link training result
@@ -1463,57 +1440,60 @@ void decide_link_settings(struct core_stream *stream,
 	struct dc_link_settings *link_setting)
 {
 
-	const struct dc_link_settings *cur_ls;
+	struct dc_link_settings initial_link_setting = {
+		LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED};
+	struct dc_link_settings current_link_setting =
+			initial_link_setting;
 	struct core_link* link;
 	uint32_t req_bw;
 	uint32_t link_bw;
-	uint32_t i;
 
 	req_bw = bandwidth_in_kbps_from_timing(
 			&stream->public.timing);
 
+	link = stream->sink->link;
+
 	/* if preferred is specified through AMDDP, use it, if it's enough
 	 * to drive the mode
 	 */
-	link = stream->sink->link;
-
-	if ((link->public.reported_link_cap.lane_count != LANE_COUNT_UNKNOWN) &&
-		(link->public.reported_link_cap.link_rate <=
-				link->public.verified_link_cap.link_rate)) {
+	if (link->public.preferred_link_setting.lane_count !=
+			LANE_COUNT_UNKNOWN &&
+			link->public.preferred_link_setting.link_rate !=
+					LINK_RATE_UNKNOWN) {
+		*link_setting =  link->public.preferred_link_setting;
+		return;
+	}
 
+    /* search for the minimum link setting that:
+     * 1. is supported according to the link training result
+     * 2. could support the b/w requested by the timing
+     */
+	while (current_link_setting.link_rate <=
+			link->public.max_link_setting.link_rate) {
 		link_bw = bandwidth_in_kbps_from_link_settings(
-				&link->public.reported_link_cap);
-
-		if (req_bw < link_bw) {
-			*link_setting = link->public.reported_link_cap;
+				&current_link_setting);
+		if (req_bw <= link_bw) {
+			*link_setting = current_link_setting;
 			return;
 		}
-	}
 
-	/* search for first suitable setting for the requested
-	 * bandwidth
-	 */
-	for (i = 0; i < get_link_training_fallback_table_len(link); i++) {
-
-		cur_ls = get_link_training_fallback_table(link, i);
-
-		link_bw =
-				bandwidth_in_kbps_from_link_settings(
-				cur_ls);
-
-		if (req_bw < link_bw) {
-			if (is_link_setting_supported(
-				cur_ls,
-				&link->public.max_link_setting)) {
-				*link_setting = *cur_ls;
-				return;
-			}
+		if (current_link_setting.lane_count <
+				link->public.max_link_setting.lane_count) {
+			current_link_setting.lane_count =
+					increase_lane_count(
+							current_link_setting.lane_count);
+		} else {
+			current_link_setting.link_rate =
+					increase_link_rate(
+							current_link_setting.link_rate);
+			current_link_setting.lane_count =
+					initial_link_setting.lane_count;
 		}
 	}
 
 	BREAK_TO_DEBUGGER();
 	ASSERT(link->public.verified_link_cap.lane_count !=
-		LANE_COUNT_UNKNOWN);
+			LANE_COUNT_UNKNOWN);
 
 	*link_setting = link->public.verified_link_cap;
 }

commit 8ee65d7c93cb082d6c1ca584b7565c1cc08e7861
Author: Wenjing Liu <Wenjing.Liu@amd.com>
Date:   Wed Jul 19 13:18:26 2017 -0400

    drm/amd/display: Return hpd_irq_dpcd from hpd_rx handler
    
    Signed-off-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index dd3f57fce834..445cd226d36d 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1924,7 +1924,7 @@ static void handle_automated_test(struct core_link *link)
 			sizeof(test_response));
 }
 
-bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link)
+bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link, union hpd_irq_data *out_hpd_irq_dpcd_data)
 {
 	struct core_link *link = DC_LINK_TO_LINK(dc_link);
 	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
@@ -1939,12 +1939,15 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link)
 		"%s: Got short pulse HPD on link %d\n",
 		__func__, link->public.link_index);
 
+
 	 /* All the "handle_hpd_irq_xxx()" methods
 		 * should be called only after
 		 * dal_dpsst_ls_read_hpd_irq_data
 		 * Order of calls is important too
 		 */
 	result = read_hpd_rx_irq_data(link, &hpd_irq_dpcd_data);
+	if (out_hpd_irq_dpcd_data)
+		*out_hpd_irq_dpcd_data = hpd_irq_dpcd_data;
 
 	if (result != DC_OK) {
 		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,

commit 820e3935489c85d7dc3024eec709b016ba75b376
Author: Ding Wang <Ding.Wang@amd.com>
Date:   Thu Jul 13 12:09:57 2017 -0400

    drm/amd/display: link training fallback actions
    
    Signed-off-by: Ding Wang <ding.wang@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 98048fe6239e..dd3f57fce834 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -731,7 +731,7 @@ static enum hw_dp_training_pattern get_supported_tp(struct core_link *link)
 	return HW_DP_TRAINING_PATTERN_2;
 }
 
-static bool perform_channel_equalization_sequence(
+static enum link_training_result perform_channel_equalization_sequence(
 	struct core_link *link,
 	struct link_training_settings *lt_settings)
 {
@@ -777,19 +777,19 @@ static bool perform_channel_equalization_sequence(
 
 		/* 5. check CR done*/
 		if (!is_cr_done(lane_count, dpcd_lane_status))
-			return false;
+			return LINK_TRAINING_EQ_FAIL_CR;
 
 		/* 6. check CHEQ done*/
 		if (is_ch_eq_done(lane_count,
 			dpcd_lane_status,
 			&dpcd_lane_status_updated))
-			return true;
+			return LINK_TRAINING_SUCCESS;
 
 		/* 7. update VS/PE/PC2 in lt_settings*/
 		update_drive_settings(lt_settings, req_settings);
 	}
 
-	return false;
+	return LINK_TRAINING_EQ_FAIL_EQ;
 
 }
 
@@ -943,18 +943,17 @@ static inline bool perform_link_training_int(
 	return status;
 }
 
-bool dc_link_dp_perform_link_training(
+enum link_training_result dc_link_dp_perform_link_training(
 	struct dc_link *link,
 	const struct dc_link_settings *link_setting,
 	bool skip_video_pattern)
 {
+	enum link_training_result status = LINK_TRAINING_SUCCESS;
 	struct core_link *core_link = DC_LINK_TO_CORE(link);
-	bool status;
 
 	char *link_rate = "Unknown";
 	struct link_training_settings lt_settings;
 
-	status = false;
 	memset(&lt_settings, '\0', sizeof(lt_settings));
 
 	lt_settings.link_settings.link_rate = link_setting->link_rate;
@@ -976,16 +975,23 @@ bool dc_link_dp_perform_link_training(
 
 	/* 2. perform link training (set link training done
 	 *  to false is done as well)*/
-	if (perform_clock_recovery_sequence(core_link, &lt_settings)) {
-
-		if (perform_channel_equalization_sequence(core_link,
-				&lt_settings))
-			status = true;
+	if (!perform_clock_recovery_sequence(core_link, &lt_settings)) {
+		status = LINK_TRAINING_CR_FAIL;
+	} else {
+		status = perform_channel_equalization_sequence(core_link,
+				&lt_settings);
 	}
 
-	if (status || !skip_video_pattern)
-		status = perform_link_training_int(core_link,
-				&lt_settings, status);
+	if ((status == LINK_TRAINING_SUCCESS) || !skip_video_pattern) {
+		if (!perform_link_training_int(core_link,
+				&lt_settings,
+				status == LINK_TRAINING_SUCCESS)) {
+			/* the next link training setting in this case
+			 * would be the same as CR failure case.
+			 */
+			status = LINK_TRAINING_CR_FAIL;
+		}
+	}
 
 	/* 6. print status message*/
 	switch (lt_settings.link_settings.link_rate) {
@@ -1013,7 +1019,9 @@ bool dc_link_dp_perform_link_training(
 	CONN_MSG_LT(core_link, "%sx%d %s VS=%d, PE=%d",
 			link_rate,
 			lt_settings.link_settings.lane_count,
-			status ? "pass" : "fail",
+			(status ==  LINK_TRAINING_SUCCESS) ? "pass" :
+			((status == LINK_TRAINING_CR_FAIL) ? "CR failed" :
+			"EQ failed"),
 			lt_settings.lane_settings[0].VOLTAGE_SWING,
 			lt_settings.lane_settings[0].PRE_EMPHASIS);
 
@@ -1035,7 +1043,7 @@ bool perform_link_training_with_retries(
 		if (dc_link_dp_perform_link_training(
 				&link->public,
 				link_setting,
-				skip_video_pattern))
+				skip_video_pattern) == LINK_TRAINING_SUCCESS)
 			return true;
 
 		msleep(delay_between_attempts);
@@ -1068,15 +1076,6 @@ static const struct dc_link_settings *get_link_training_fallback_table(
 	return &link_training_fallback_table[i];
 }
 
-static bool exceeded_limit_link_setting(
-	const struct dc_link_settings *link_setting,
-	const struct dc_link_settings *limit_link_setting)
-{
-	return (link_setting->lane_count * link_setting->link_rate
-		 > limit_link_setting->lane_count * limit_link_setting->link_rate ?
-				 true : false);
-}
-
 static struct dc_link_settings get_max_link_cap(struct core_link *link)
 {
 	/* Set Default link settings */
@@ -1109,13 +1108,15 @@ bool dp_hbr_verify_link_cap(
 	struct dc_link_settings *known_limit_link_setting)
 {
 	struct dc_link_settings max_link_cap = {0};
+	struct dc_link_settings cur_link_setting = {0};
+	struct dc_link_settings *cur = &cur_link_setting;
+	struct dc_link_settings initial_link_settings = {0};
 	bool success;
 	bool skip_link_training;
-	const struct dc_link_settings *cur;
 	bool skip_video_pattern;
-	uint32_t i;
 	struct clock_source *dp_cs;
 	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
+	enum link_training_result status;
 
 	success = false;
 	skip_link_training = false;
@@ -1142,19 +1143,16 @@ bool dp_hbr_verify_link_cap(
 		ASSERT(dp_cs);
 	}
 
-	for (i = 0; i < get_link_training_fallback_table_len(link) &&
-		!success; i++) {
-		cur = get_link_training_fallback_table(link, i);
-
-		if (known_limit_link_setting->lane_count != LANE_COUNT_UNKNOWN &&
-			exceeded_limit_link_setting(cur,
-					known_limit_link_setting))
-			continue;
-
-		if (!is_link_setting_supported(cur, &max_link_cap))
-			continue;
-
+	/* link training starts with the maximum common settings
+	 * supported by both sink and ASIC.
+	 */
+	initial_link_settings = get_common_supported_link_settings(
+			*known_limit_link_setting,
+			max_link_cap);
+	cur_link_setting = initial_link_settings;
+	do {
 		skip_video_pattern = true;
+
 		if (cur->link_rate == LINK_RATE_LOW)
 			skip_video_pattern = false;
 
@@ -1167,10 +1165,12 @@ bool dp_hbr_verify_link_cap(
 		if (skip_link_training)
 			success = true;
 		else {
-			success = dc_link_dp_perform_link_training(
+			status = dc_link_dp_perform_link_training(
 							&link->public,
 							cur,
 							skip_video_pattern);
+			if (status == LINK_TRAINING_SUCCESS)
+				success = true;
 		}
 
 		if (success)
@@ -1181,7 +1181,8 @@ bool dp_hbr_verify_link_cap(
 		 * based on the actual mode we're driving
 		 */
 		dp_disable_link_phy(link, link->public.connector_signal);
-	}
+	} while (!success && decide_fallback_link_setting(
+			initial_link_settings, cur, status));
 
 	/* Link Training failed for all Link Settings
 	 *  (Lane Count is still unknown)
@@ -1202,6 +1203,160 @@ bool dp_hbr_verify_link_cap(
 	return success;
 }
 
+struct dc_link_settings get_common_supported_link_settings (
+		struct dc_link_settings link_setting_a,
+		struct dc_link_settings link_setting_b)
+{
+	struct dc_link_settings link_settings = {0};
+
+	link_settings.lane_count =
+		(link_setting_a.lane_count <=
+			link_setting_b.lane_count) ?
+			link_setting_a.lane_count :
+			link_setting_b.lane_count;
+	link_settings.link_rate =
+		(link_setting_a.link_rate <=
+			link_setting_b.link_rate) ?
+			link_setting_a.link_rate :
+			link_setting_b.link_rate;
+	link_settings.link_spread = LINK_SPREAD_DISABLED;
+
+	/* in DP compliance test, DPR-120 may have
+	 * a random value in its MAX_LINK_BW dpcd field.
+	 * We map it to the maximum supported link rate that
+	 * is smaller than MAX_LINK_BW in this case.
+	 */
+	if (link_settings.link_rate > LINK_RATE_HIGH3) {
+		link_settings.link_rate = LINK_RATE_HIGH3;
+	} else if (link_settings.link_rate < LINK_RATE_HIGH3
+			&& link_settings.link_rate > LINK_RATE_HIGH2) {
+		link_settings.link_rate = LINK_RATE_HIGH2;
+	} else if (link_settings.link_rate < LINK_RATE_HIGH2
+			&& link_settings.link_rate > LINK_RATE_HIGH) {
+		link_settings.link_rate = LINK_RATE_HIGH;
+	} else if (link_settings.link_rate < LINK_RATE_HIGH
+			&& link_settings.link_rate > LINK_RATE_LOW) {
+		link_settings.link_rate = LINK_RATE_LOW;
+	} else if (link_settings.link_rate < LINK_RATE_LOW) {
+		link_settings.link_rate = LINK_RATE_UNKNOWN;
+	}
+
+	return link_settings;
+}
+
+bool reached_minimum_lane_count(enum dc_lane_count lane_count)
+{
+	return lane_count <= LANE_COUNT_ONE;
+}
+
+bool reached_minimum_link_rate(enum dc_link_rate link_rate)
+{
+	return link_rate <= LINK_RATE_LOW;
+}
+
+enum dc_lane_count reduce_lane_count(enum dc_lane_count lane_count)
+{
+	switch (lane_count) {
+	case LANE_COUNT_FOUR:
+		return LANE_COUNT_TWO;
+	case LANE_COUNT_TWO:
+		return LANE_COUNT_ONE;
+	case LANE_COUNT_ONE:
+		return LANE_COUNT_UNKNOWN;
+	default:
+		return LANE_COUNT_UNKNOWN;
+	}
+}
+
+enum dc_link_rate reduce_link_rate(enum dc_link_rate link_rate)
+{
+	switch (link_rate) {
+	case LINK_RATE_HIGH3:
+		return LINK_RATE_HIGH2;
+	case LINK_RATE_HIGH2:
+		return LINK_RATE_HIGH;
+	case LINK_RATE_HIGH:
+		return LINK_RATE_LOW;
+	case LINK_RATE_LOW:
+		return LINK_RATE_UNKNOWN;
+	default:
+		return LINK_RATE_UNKNOWN;
+	}
+}
+
+/*
+ * function: set link rate and lane count fallback based
+ * on current link setting and last link training result
+ * return value:
+ *			true - link setting could be set
+ *			false - has reached minimum setting
+ *					and no further fallback could be done
+ */
+bool decide_fallback_link_setting(
+		struct dc_link_settings initial_link_settings,
+		struct dc_link_settings *current_link_setting,
+		enum link_training_result training_result)
+{
+	if (!current_link_setting)
+		return false;
+
+	switch (training_result) {
+	case LINK_TRAINING_CR_FAIL:
+	{
+		if (!reached_minimum_link_rate
+				(current_link_setting->link_rate)) {
+			current_link_setting->link_rate =
+				reduce_link_rate(
+					current_link_setting->link_rate);
+		} else if (!reached_minimum_lane_count
+				(current_link_setting->lane_count)) {
+			current_link_setting->link_rate =
+				initial_link_settings.link_rate;
+			current_link_setting->lane_count =
+				reduce_lane_count(
+					current_link_setting->lane_count);
+		} else {
+			return false;
+		}
+		break;
+	}
+	case LINK_TRAINING_EQ_FAIL_EQ:
+	{
+		if (!reached_minimum_lane_count
+				(current_link_setting->lane_count)) {
+			current_link_setting->lane_count =
+				reduce_lane_count(
+					current_link_setting->lane_count);
+		} else if (!reached_minimum_link_rate
+				(current_link_setting->link_rate)) {
+			current_link_setting->lane_count =
+				initial_link_settings.lane_count;
+			current_link_setting->link_rate =
+				reduce_link_rate(
+					current_link_setting->link_rate);
+		} else {
+			return false;
+		}
+		break;
+	}
+	case LINK_TRAINING_EQ_FAIL_CR:
+	{
+		if (!reached_minimum_link_rate
+				(current_link_setting->link_rate)) {
+			current_link_setting->link_rate =
+				reduce_link_rate(
+					current_link_setting->link_rate);
+		} else {
+			return false;
+		}
+		break;
+	}
+	default:
+		return false;
+	}
+	return true;
+}
+
 static uint32_t bandwidth_in_kbps_from_timing(
 	const struct dc_crtc_timing *timing)
 {

commit 46df790c6b569faffb4cc93889745a7827283749
Author: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
Date:   Sun Apr 30 09:20:55 2017 -0400

    drm/amd/display: i2c/aux Remove link index.
    
    Link index is an unnecessery level of inderection when
    calling from kernel i2c/aux transfer into DAL.
    
    Signed-off-by: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a0d192774a22..98048fe6239e 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1913,7 +1913,7 @@ static void get_active_converter_info(
 	/* decode converter info*/
 	if (!ds_port.fields.PORT_PRESENT) {
 		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
-		ddc_service_set_dongle_type(link->ddc,
+		ddc_service_set_dongle_type(link->public.ddc,
 				link->dpcd_caps.dongle_type);
 		return;
 	}
@@ -1983,7 +1983,7 @@ static void get_active_converter_info(
 		}
 	}
 
-	ddc_service_set_dongle_type(link->ddc, link->dpcd_caps.dongle_type);
+	ddc_service_set_dongle_type(link->public.ddc, link->dpcd_caps.dongle_type);
 
 	{
 		struct dp_device_vendor_id dp_id;

commit 529cad0f945c9e60569e902062d2f2741e4fd71a
Author: Ding Wang <Ding.Wang@amd.com>
Date:   Tue Apr 25 10:03:27 2017 -0400

    drm/amd/display: Add function to set dither option
    
    Signed-off-by: Ding Wang <Ding.Wang@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4b9d3f12406f..a0d192774a22 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -11,6 +11,7 @@
 #include "dpcd_defs.h"
 
 #include "core_dc.h"
+#include "resource.h"
 
 /* maximum pre emphasis level allowed for each voltage swing level*/
 static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
@@ -2245,8 +2246,7 @@ static void set_crtc_test_pattern(struct core_link *link,
 	case DP_TEST_PATTERN_VIDEO_MODE:
 	{
 		/* restore bitdepth reduction */
-		link->dc->res_pool->funcs->
-			build_bit_depth_reduction_params(pipe_ctx->stream,
+		resource_build_bit_depth_reduction_params(pipe_ctx->stream,
 					&params);
 		pipe_ctx->stream->bit_depth_params = params;
 		pipe_ctx->opp->funcs->

commit a2b8659db9b435853cb0dc78d225a492e7ee69d0
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Sat Apr 22 14:17:51 2017 -0400

    drm/amd/display: decouple resource_pool from resource_context
    
    to avoid null access in case res_ctx is used to access res_pool before it's fully constructed
    
    also make it clear which function has dependency on resource_pool
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index f883fdb820c8..4b9d3f12406f 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2245,7 +2245,7 @@ static void set_crtc_test_pattern(struct core_link *link,
 	case DP_TEST_PATTERN_VIDEO_MODE:
 	{
 		/* restore bitdepth reduction */
-		link->dc->current_context->res_ctx.pool->funcs->
+		link->dc->res_pool->funcs->
 			build_bit_depth_reduction_params(pipe_ctx->stream,
 					&params);
 		pipe_ctx->stream->bit_depth_params = params;

commit 94267b3df7ee00f21fa0ff7d618ca7e0574db5ed
Author: Sylvia Tsai <sylvia.tsai@amd.com>
Date:   Fri Apr 21 15:29:55 2017 -0400

    drm/amd/display: PSR Refactor
    
    - Refacotr PSR to follow correct module pattern
    - fix eDP only working on sink index 0.
    
    Signed-off-by: Sylvia Tsai <sylvia.tsai@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 17286465e850..f883fdb820c8 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1478,7 +1478,7 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 {
 	union dpcd_psr_configuration psr_configuration;
 
-	if (link->public.psr_caps.psr_version == 0)
+	if (!link->psr_enabled)
 		return false;
 
 	dm_helpers_dp_read_dpcd(
@@ -2060,36 +2060,6 @@ static void dp_wa_power_up_0010FA(struct core_link *link, uint8_t *dpcd_data,
 		link->wa_flags.dp_keep_receiver_powered = false;
 }
 
-static void retrieve_psr_link_cap(struct core_link *link,
-		enum edp_revision edp_revision)
-{
-	if (edp_revision >= EDP_REVISION_13) {
-		core_link_read_dpcd(link,
-				DP_PSR_SUPPORT,
-				(uint8_t *)(&link->public.psr_caps),
-				sizeof(link->public.psr_caps));
-		if (link->public.psr_caps.psr_version != 0) {
-			unsigned char psr_capability = 0;
-
-			core_link_read_dpcd(link,
-					    DP_PSR_CAPS,
-						&psr_capability,
-						sizeof(psr_capability));
-			/* Bit 0 determines whether fast link training is
-			 * required on PSR exit. If set to 0, link training
-			 * is required. If set to 1, sink must lock within
-			 * five Idle Patterns after Main Link is turned on.
-			 */
-			link->public.psr_caps.psr_exit_link_training_required
-						= !(psr_capability & 0x1);
-
-			psr_capability = (psr_capability >> 1) & 0x7;
-			link->public.psr_caps.psr_rfb_setup_time =
-					55 * (6 - psr_capability);
-		}
-	}
-}
-
 static void retrieve_link_cap(struct core_link *link)
 {
 	uint8_t dpcd_data[DP_TRAINING_AUX_RD_INTERVAL - DP_DPCD_REV + 1];
@@ -2157,38 +2127,17 @@ static void retrieve_link_cap(struct core_link *link)
 	link->dpcd_caps.panel_mode_edp =
 		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
 
-	link->edp_revision = EDP_REVISION_11;
-
 	link->public.test_pattern_enabled = false;
 	link->public.compliance_test_state.raw = 0;
 
-	link->public.psr_caps.psr_exit_link_training_required = false;
-	link->public.psr_caps.psr_frame_capture_indication_req = false;
-	link->public.psr_caps.psr_rfb_setup_time = 0;
-	link->public.psr_caps.psr_sdp_transmit_line_num_deadline = 0;
-	link->public.psr_caps.psr_version = 0;
-
 	/* read sink count */
 	core_link_read_dpcd(link,
 			DP_SINK_COUNT,
 			&link->dpcd_caps.sink_count.raw,
 			sizeof(link->dpcd_caps.sink_count.raw));
 
-	/* Display control registers starting at DPCD 700h are only valid and
-	 * enabled if this eDP config cap bit is set. */
-	if (edp_config_cap.bits.DPCD_DISPLAY_CONTROL_CAPABLE) {
-		/* Read the Panel's eDP revision at DPCD 700h. */
-		core_link_read_dpcd(link,
-			DP_EDP_DPCD_REV,
-			(uint8_t *)(&link->edp_revision),
-			sizeof(link->edp_revision));
-	}
-
 	/* Connectivity log: detection */
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
-
-	/* TODO: Confirm if need retrieve_psr_link_cap */
-	retrieve_psr_link_cap(link, link->edp_revision);
 }
 
 void detect_dp_sink_caps(struct core_link *link)

commit d27383a2b5719be60ac86deae30e89755f868a07
Author: Zeyu Fan <Zeyu.Fan@amd.com>
Date:   Fri Apr 21 10:55:01 2017 -0400

    drm/amd/display: Make dc_link param const in set_drive_settings
    
    Signed-off-by: Zeyu Fan <Zeyu.Fan@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 953e201dc6bc..17286465e850 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -605,7 +605,7 @@ static bool is_max_vs_reached(
 }
 
 void dc_link_dp_set_drive_settings(
-	struct dc_link *link,
+	const struct dc_link *link,
 	struct link_training_settings *lt_settings)
 {
 	struct core_link *core_link = DC_LINK_TO_CORE(link);

commit 03f5c686c3900f74853539cdebe4c25190106402
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Fri Apr 21 17:15:40 2017 -0400

    drm/amd/display: USB-c DP-HDMI dongle shows garbage on Sony TV
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4ef3fec44d2d..953e201dc6bc 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1951,14 +1951,33 @@ static void get_active_converter_info(
 			link->dpcd_caps.dongle_type =
 				DISPLAY_DONGLE_DP_HDMI_CONVERTER;
 
+			link->dpcd_caps.dongle_caps.dongle_type = link->dpcd_caps.dongle_type;
 			if (ds_port.fields.DETAILED_CAPS) {
 
 				union dwnstream_port_caps_byte3_hdmi
 					hdmi_caps = {.raw = det_caps[3] };
+				union dwnstream_port_caps_byte1
+					hdmi_color_caps = {.raw = det_caps[2] };
+				link->dpcd_caps.dongle_caps.dp_hdmi_max_pixel_clk =
+					det_caps[1] * 25000;
 
-				link->dpcd_caps.is_dp_hdmi_s3d_converter =
+				link->dpcd_caps.dongle_caps.is_dp_hdmi_s3d_converter =
 					hdmi_caps.bits.FRAME_SEQ_TO_FRAME_PACK;
+				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_pass_through =
+					hdmi_caps.bits.YCrCr422_PASS_THROUGH;
+				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_pass_through =
+					hdmi_caps.bits.YCrCr420_PASS_THROUGH;
+				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr422_converter =
+					hdmi_caps.bits.YCrCr422_CONVERSION;
+				link->dpcd_caps.dongle_caps.is_dp_hdmi_ycbcr420_converter =
+					hdmi_caps.bits.YCrCr420_CONVERSION;
+
+				link->dpcd_caps.dongle_caps.dp_hdmi_max_bpc =
+					hdmi_color_caps.bits.MAX_BITS_PER_COLOR_COMPONENT;
+
+				link->dpcd_caps.dongle_caps.extendedCapValid = true;
 			}
+
 			break;
 		}
 	}

commit 15350179f2fc3c3b28774b4abcd35c45a5449052
Author: Amy Zhang <Amy.Zhang@amd.com>
Date:   Tue Apr 18 17:43:09 2017 -0400

    drm/amd/display: always retrieve PSR cap
    
    Signed-off-by: Amy Zhang <Amy.Zhang@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 913b01cd7159..4ef3fec44d2d 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2169,8 +2169,7 @@ static void retrieve_link_cap(struct core_link *link)
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 
 	/* TODO: Confirm if need retrieve_psr_link_cap */
-	if (link->public.reported_link_cap.link_rate < LINK_RATE_HIGH2)
-		retrieve_psr_link_cap(link, link->edp_revision);
+	retrieve_psr_link_cap(link, link->edp_revision);
 }
 
 void detect_dp_sink_caps(struct core_link *link)

commit 07c84c7ad388c28ce69303182e06644ef30d3ac3
Author: Ding Wang <Ding.Wang@amd.com>
Date:   Mon Apr 10 14:02:23 2017 -0400

    drm/amd/display: Fix for tile MST
    
    - Set stream signal type to be SST when setting non-tile timing on MST
      tiled display.
      - Disable MST on sink after disabling MST link.
      - Enable MST on sink before enabling MST link.
    
    Signed-off-by: Ding Wang <Ding.Wang@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 9f12ba87827a..913b01cd7159 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2453,3 +2453,16 @@ bool dc_link_dp_set_test_pattern(
 
 	return true;
 }
+
+void dp_enable_mst_on_sink(struct core_link *link, bool enable)
+{
+	unsigned char mstmCntl;
+
+	core_link_read_dpcd(link, DP_MSTM_CTRL, &mstmCntl, 1);
+	if (enable)
+		mstmCntl |= DP_MST_EN;
+	else
+		mstmCntl &= (~DP_MST_EN);
+
+	core_link_write_dpcd(link, DP_MSTM_CTRL, &mstmCntl, 1);
+}

commit 7c7f5b15be6528b33d825ead6acb739d7d061a2e
Author: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
Date:   Tue Mar 28 16:57:52 2017 -0400

    drm/amd/display: Refactor edid read.
    
    Allow Linux to use DRM provided EDID read functioality
    by moving  DAL edid implementation to module hence
    removing this code from DC by this cleaning up DC
    code for upstream.
    
    v2: Removing ddc_service. No more need for it.
    
    Signed-off-by: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index b8c40e8bee5a..9f12ba87827a 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1481,22 +1481,25 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 	if (link->public.psr_caps.psr_version == 0)
 		return false;
 
-	dal_ddc_service_read_dpcd_data(
-					link->ddc,
-					368 /*DpcdAddress_PSR_Enable_Cfg*/,
-					&psr_configuration.raw,
-					sizeof(psr_configuration.raw));
+	dm_helpers_dp_read_dpcd(
+		link->ctx,
+		&link->public,
+		368,/*DpcdAddress_PSR_Enable_Cfg*/
+		&psr_configuration.raw,
+		sizeof(psr_configuration.raw));
+
 
 	if (psr_configuration.bits.ENABLE) {
 		unsigned char dpcdbuf[3] = {0};
 		union psr_error_status psr_error_status;
 		union psr_sink_psr_status psr_sink_psr_status;
 
-		dal_ddc_service_read_dpcd_data(
-					link->ddc,
-					0x2006 /*DpcdAddress_PSR_Error_Status*/,
-					(unsigned char *) dpcdbuf,
-					sizeof(dpcdbuf));
+		dm_helpers_dp_read_dpcd(
+			link->ctx,
+			&link->public,
+			0x2006, /*DpcdAddress_PSR_Error_Status*/
+			(unsigned char *) dpcdbuf,
+			sizeof(dpcdbuf));
 
 		/*DPCD 2006h   ERROR STATUS*/
 		psr_error_status.raw = dpcdbuf[0];
@@ -1506,9 +1509,10 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 		if (psr_error_status.bits.LINK_CRC_ERROR ||
 				psr_error_status.bits.RFB_STORAGE_ERROR) {
 			/* Acknowledge and clear error bits */
-			dal_ddc_service_write_dpcd_data(
-				link->ddc,
-				8198 /*DpcdAddress_PSR_Error_Status*/,
+			dm_helpers_dp_write_dpcd(
+				link->ctx,
+				&link->public,
+				8198,/*DpcdAddress_PSR_Error_Status*/
 				&psr_error_status.raw,
 				sizeof(psr_error_status.raw));
 

commit bddd696ddd437ce15d278b2a5318ef44d3c171cd
Author: Zeyu Fan <Zeyu.Fan@amd.com>
Date:   Wed Mar 29 17:21:56 2017 -0400

    drm/amd/display: Temporary disable PSR for HBR2 & HBR3
    
    Signed-off-by: Zeyu Fan <Zeyu.Fan@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 4f6d1d2d5b9b..b8c40e8bee5a 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2165,7 +2165,8 @@ static void retrieve_link_cap(struct core_link *link)
 	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
 
 	/* TODO: Confirm if need retrieve_psr_link_cap */
-	retrieve_psr_link_cap(link, link->edp_revision);
+	if (link->public.reported_link_cap.link_rate < LINK_RATE_HIGH2)
+		retrieve_psr_link_cap(link, link->edp_revision);
 }
 
 void detect_dp_sink_caps(struct core_link *link)

commit 0e19401f9506e710379396479c3824d80ccdc332
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Tue Mar 14 19:16:36 2017 -0400

    drm/amd/display: support PHY compliance automation for CP2520 pattern 1/2/3
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 282bb59ea2f3..4f6d1d2d5b9b 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1599,19 +1599,25 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	switch (dpcd_test_pattern.bits.PATTERN) {
 	case PHY_TEST_PATTERN_D10_2:
 		test_pattern = DP_TEST_PATTERN_D102;
-	break;
+		break;
 	case PHY_TEST_PATTERN_SYMBOL_ERROR:
 		test_pattern = DP_TEST_PATTERN_SYMBOL_ERROR;
-	break;
+		break;
 	case PHY_TEST_PATTERN_PRBS7:
 		test_pattern = DP_TEST_PATTERN_PRBS7;
-	break;
+		break;
 	case PHY_TEST_PATTERN_80BIT_CUSTOM:
 		test_pattern = DP_TEST_PATTERN_80BIT_CUSTOM;
-	break;
-	case PHY_TEST_PATTERN_HBR2_COMPLIANCE_EYE:
-		test_pattern = DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
-	break;
+		break;
+	case PHY_TEST_PATTERN_CP2520_1:
+		test_pattern = DP_TEST_PATTERN_CP2520_1;
+		break;
+	case PHY_TEST_PATTERN_CP2520_2:
+		test_pattern = DP_TEST_PATTERN_CP2520_2;
+		break;
+	case PHY_TEST_PATTERN_CP2520_3:
+		test_pattern = DP_TEST_PATTERN_CP2520_3;
+		break;
 	default:
 		test_pattern = DP_TEST_PATTERN_VIDEO_MODE;
 	break;
@@ -2202,16 +2208,9 @@ void dc_link_dp_disable_hpd(const struct dc_link *link)
 
 static bool is_dp_phy_pattern(enum dp_test_pattern test_pattern)
 {
-	if (test_pattern == DP_TEST_PATTERN_D102 ||
-	test_pattern == DP_TEST_PATTERN_SYMBOL_ERROR ||
-	test_pattern == DP_TEST_PATTERN_PRBS7 ||
-	test_pattern == DP_TEST_PATTERN_80BIT_CUSTOM ||
-	test_pattern == DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE ||
-	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN1 ||
-	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN2 ||
-	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN3 ||
-	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN4 ||
-	test_pattern == DP_TEST_PATTERN_VIDEO_MODE)
+	if ((DP_TEST_PATTERN_PHY_PATTERN_BEGIN <= test_pattern &&
+			test_pattern <= DP_TEST_PATTERN_PHY_PATTERN_END) ||
+			test_pattern == DP_TEST_PATTERN_VIDEO_MODE)
 		return true;
 	else
 		return false;
@@ -2377,22 +2376,28 @@ bool dc_link_dp_set_test_pattern(
 		switch (test_pattern) {
 		case DP_TEST_PATTERN_VIDEO_MODE:
 			pattern = PHY_TEST_PATTERN_NONE;
-		break;
+			break;
 		case DP_TEST_PATTERN_D102:
 			pattern = PHY_TEST_PATTERN_D10_2;
-		break;
+			break;
 		case DP_TEST_PATTERN_SYMBOL_ERROR:
 			pattern = PHY_TEST_PATTERN_SYMBOL_ERROR;
-		break;
+			break;
 		case DP_TEST_PATTERN_PRBS7:
 			pattern = PHY_TEST_PATTERN_PRBS7;
-		break;
+			break;
 		case DP_TEST_PATTERN_80BIT_CUSTOM:
 			pattern = PHY_TEST_PATTERN_80BIT_CUSTOM;
-		break;
-		case DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE:
-			pattern = PHY_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
-		break;
+			break;
+		case DP_TEST_PATTERN_CP2520_1:
+			pattern = PHY_TEST_PATTERN_CP2520_1;
+			break;
+		case DP_TEST_PATTERN_CP2520_2:
+			pattern = PHY_TEST_PATTERN_CP2520_2;
+			break;
+		case DP_TEST_PATTERN_CP2520_3:
+			pattern = PHY_TEST_PATTERN_CP2520_3;
+			break;
 		default:
 			return false;
 		}

commit cc04bf7e4f87730a35b14056e23852e010f079ff
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Tue Mar 14 01:40:53 2017 -0400

    drm/amd/display: use extended receiver cap for dpcd ver
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Anthony Koo <Anthony.Koo@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 80002795adc2..282bb59ea2f3 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2081,9 +2081,6 @@ static void retrieve_link_cap(struct core_link *link)
 		dpcd_data,
 		sizeof(dpcd_data));
 
-	link->dpcd_caps.dpcd_rev.raw =
-		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
-
 	{
 		union training_aux_rd_interval aux_rd_interval;
 
@@ -2099,6 +2096,9 @@ static void retrieve_link_cap(struct core_link *link)
 		}
 	}
 
+	link->dpcd_caps.dpcd_rev.raw =
+		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
+
 	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
 				 DP_DPCD_REV];
 

commit c30267f5023aef44d4367c631c42f217a2ec37f6
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Fri Mar 3 15:16:03 2017 -0500

    drm/amd/display: TPS4 logic typo fix
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Wenjing Liu <Wenjing.Liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 37efa40a9bca..80002795adc2 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -921,8 +921,8 @@ static inline bool perform_link_training_int(
 	 * If the upstream DPTX and downstream DPRX both support TPS4,
 	 * TPS4 must be used instead of POST_LT_ADJ_REQ.
 	 */
-	if (link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED != 1 &&
-		get_supported_tp(link) == HW_DP_TRAINING_PATTERN_4)
+	if (link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED != 1 ||
+			get_supported_tp(link) == HW_DP_TRAINING_PATTERN_4)
 		return status;
 
 	if (status &&

commit c2e218dda078f4e8d09d2493b01a7e256cfe38aa
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Fri Feb 24 16:25:51 2017 -0500

    drm/amd/display: Some more warning fixes
    
    This doesn't show with gcc6
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 6fe5acbc16ca..37efa40a9bca 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1762,7 +1762,7 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link)
 {
 	struct core_link *link = DC_LINK_TO_LINK(dc_link);
 	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
-	union device_service_irq device_service_clear = {0};
+	union device_service_irq device_service_clear = { { 0 } };
 	enum dc_status result = DDC_RESULT_UNKNOWN;
 	bool status = false;
 	/* For use cases related to down stream connection status change,

commit 75a74755763062b5dd722a19b3fe7a60813ee369
Author: Leon Elazar <leon.elazar@amd.com>
Date:   Thu Jan 26 18:03:16 2017 -0500

    drm/amd/display: DP compliance automation test fixes
    
    Fixes:
    1. Removing pending flag since we are executing teh entire flow without context switches
    2. Adding stream enablment - connection between DIG BE to DIG FE during test link training
    
    Signed-off-by: Leon Elazar <leon.elazar@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index c9c1b48df384..6fe5acbc16ca 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1738,8 +1738,7 @@ static void handle_automated_test(struct core_link *link)
 	}
 	if (test_request.bits.LINK_TEST_PATTRN) {
 		dp_test_send_link_test_pattern(link);
-		link->public.compliance_test_state.bits.
-			SET_TEST_PATTERN_PENDING = 1;
+		test_response.bits.ACK = 1;
 	}
 	if (test_request.bits.PHY_TEST_PATTERN) {
 		dp_test_send_phy_test_pattern(link);
@@ -2308,11 +2307,9 @@ bool dc_link_dp_set_test_pattern(
 	unsigned int i;
 	unsigned char link_qual_pattern[LANE_COUNT_DP_MAX] = {0};
 	union dpcd_training_pattern training_pattern;
-	union test_response test_response;
 	enum dpcd_phy_test_patterns pattern;
 
 	memset(&training_pattern, 0, sizeof(training_pattern));
-	memset(&test_response, 0, sizeof(test_response));
 
 	for (i = 0; i < MAX_PIPES; i++) {
 		if (pipes[i].stream->sink->link == core_link) {
@@ -2442,20 +2439,6 @@ bool dc_link_dp_set_test_pattern(
 		set_crtc_test_pattern(core_link, &pipe_ctx, test_pattern);
 		/* Set Test Pattern state */
 		core_link->public.test_pattern_enabled = true;
-
-		/* If this is called because of compliance test request,
-		 * we respond ack here.
-		 */
-		if (core_link->public.compliance_test_state.bits.
-				SET_TEST_PATTERN_PENDING == 1) {
-			core_link->public.compliance_test_state.bits.
-						SET_TEST_PATTERN_PENDING = 0;
-			test_response.bits.ACK = 1;
-			core_link_write_dpcd(core_link,
-					DP_TEST_RESPONSE,
-					&test_response.raw,
-					sizeof(test_response));
-		}
 	}
 
 	return true;

commit ac0e562c521228215d597fe3ef0c13f02077f700
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Sat Jan 14 11:46:53 2017 -0500

    drm/amd/display: remove un-used defines and dead code
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index a661fc6a69dd..c9c1b48df384 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1922,7 +1922,7 @@ static void get_active_converter_info(
 		break;
 	}
 
-	if (link->dpcd_caps.dpcd_rev.raw >= DCS_DPCD_REV_11) {
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_11) {
 		uint8_t det_caps[4];
 		union dwnstream_port_caps_byte0 *port_caps =
 			(union dwnstream_port_caps_byte0 *)det_caps;

commit 73c7260292db3f506b2562cbb25c06adfe90ca99
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Thu Dec 29 14:58:54 2016 -0500

    drm/amd/display: Fix link retraining hw sequence for auto test
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index d32bf61b66e9..a661fc6a69dd 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -1532,7 +1532,7 @@ static bool handle_hpd_irq_psr_sink(const struct core_link *link)
 
 static void dp_test_send_link_training(struct core_link *link)
 {
-	struct dc_link_settings link_settings;
+	struct dc_link_settings link_settings = {0};
 
 	core_link_read_dpcd(
 			link,
@@ -1549,7 +1549,7 @@ static void dp_test_send_link_training(struct core_link *link)
 	link->public.verified_link_cap.lane_count = link_settings.lane_count;
 	link->public.verified_link_cap.link_rate = link_settings.link_rate;
 
-	dp_retrain_link(link);
+	dp_retrain_link_dp_test(link, &link_settings, false);
 }
 
 static void dp_test_send_phy_test_pattern(struct core_link *link)

commit b39474ef0925a748d0dd43521fc1192777afd329
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Dec 13 16:41:12 2016 +1000

    drm/amd/display: assign correct enum for edp revision
    
    There are 2 edp enum revisions, no idea why, drop one, and just
    assign 1.1 to the default value.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 7238bfe0a068..d32bf61b66e9 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -2129,7 +2129,7 @@ static void retrieve_link_cap(struct core_link *link)
 	link->dpcd_caps.panel_mode_edp =
 		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
 
-	link->edp_revision = DPCD_EDP_REVISION_EDP_UNKNOWN;
+	link->edp_revision = EDP_REVISION_11;
 
 	link->public.test_pattern_enabled = false;
 	link->public.compliance_test_state.raw = 0;

commit 3a340294f7e7a784c83f9cd72f49987cc7daaced
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Dec 13 16:41:11 2016 +1000

    drm/amd/display: port to using drm dpcd defines
    
    We only keep one list of these defines in the kernel, so we should use it.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
index 2585ec332e58..7238bfe0a068 100644
--- a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -70,7 +70,7 @@ static void wait_for_training_aux_rd_interval(
 		 * "DPCD_ADDR_TRAINING_AUX_RD_INTERVAL" register */
 		core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL,
+			DP_TRAINING_AUX_RD_INTERVAL,
 			(uint8_t *)&training_rd_interval,
 			sizeof(training_rd_interval));
 
@@ -93,14 +93,14 @@ static void dpcd_set_training_pattern(
 {
 	core_link_write_dpcd(
 		link,
-		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		DP_TRAINING_PATTERN_SET,
 		&dpcd_pattern.raw,
 		1);
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
 		"%s\n %x pattern = %x\n",
 		__func__,
-		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		DP_TRAINING_PATTERN_SET,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 }
 
@@ -129,19 +129,19 @@ static void dpcd_set_link_settings(
 	link_set_buffer[0] = rate;
 	link_set_buffer[1] = lane_count_set.raw;
 
-	core_link_write_dpcd(link, DPCD_ADDRESS_LINK_BW_SET,
+	core_link_write_dpcd(link, DP_LINK_BW_SET,
 	link_set_buffer, 2);
-	core_link_write_dpcd(link, DPCD_ADDRESS_DOWNSPREAD_CNTL,
+	core_link_write_dpcd(link, DP_DOWNSPREAD_CTRL,
 	&downspread.raw, sizeof(downspread));
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
 		"%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
 		__func__,
-		DPCD_ADDRESS_LINK_BW_SET,
+		DP_LINK_BW_SET,
 		lt_settings->link_settings.link_rate,
-		DPCD_ADDRESS_LANE_COUNT_SET,
+		DP_LANE_COUNT_SET,
 		lt_settings->link_settings.lane_count,
-		DPCD_ADDRESS_DOWNSPREAD_CNTL,
+		DP_DOWNSPREAD_CTRL,
 		lt_settings->link_settings.link_spread);
 
 }
@@ -186,7 +186,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 {
 	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
 	const uint32_t dpcd_base_lt_offset =
-	DPCD_ADDRESS_TRAINING_PATTERN_SET;
+	DP_TRAINING_PATTERN_SET;
 	uint8_t dpcd_lt_buffer[5] = {0};
 	union dpcd_training_pattern dpcd_pattern = {{0}};
 	uint32_t lane;
@@ -199,13 +199,13 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 	dpcd_pattern.v1_4.TRAINING_PATTERN_SET =
 		hw_training_pattern_to_dpcd_training_pattern(link, pattern);
 
-	dpcd_lt_buffer[DPCD_ADDRESS_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
+	dpcd_lt_buffer[DP_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
 		= dpcd_pattern.raw;
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
 		"%s\n %x pattern = %x\n",
 		__func__,
-		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		DP_TRAINING_PATTERN_SET,
 		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
 
 	/*****************************************************************
@@ -233,7 +233,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 
 	 // 0x00103 - 0x00102
 	memmove(
-		&dpcd_lt_buffer[DPCD_ADDRESS_LANE0_SET - dpcd_base_lt_offset],
+		&dpcd_lt_buffer[DP_TRAINING_LANE0_SET - dpcd_base_lt_offset],
 		dpcd_lane,
 		size_in_bytes);
 
@@ -241,7 +241,7 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		"%s:\n %x VS set = %x  PE set = %x \
 		max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
-		DPCD_ADDRESS_LANE0_SET,
+		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
 		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
 		dpcd_lane[0].bits.MAX_SWING_REACHED,
@@ -253,13 +253,13 @@ static void dpcd_set_lt_pattern_and_lane_settings(
 		*/
 		core_link_write_dpcd(
 			link,
-			DPCD_ADDRESS_TRAINING_PATTERN_SET,
+			DP_TRAINING_PATTERN_SET,
 			&dpcd_pattern.raw,
 			sizeof(dpcd_pattern.raw) );
 
 		core_link_write_dpcd(
 			link,
-			DPCD_ADDRESS_LANE0_SET,
+			DP_TRAINING_LANE0_SET,
 			(uint8_t *)(dpcd_lane),
 			size_in_bytes);
 
@@ -459,7 +459,7 @@ static void get_lane_status_and_drive_settings(
 
 	core_link_read_dpcd(
 		link,
-		DPCD_ADDRESS_LANE_01_STATUS,
+		DP_LANE0_1_STATUS,
 		(uint8_t *)(dpcd_buf),
 		sizeof(dpcd_buf));
 
@@ -478,15 +478,15 @@ static void get_lane_status_and_drive_settings(
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
 		"%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
 		__func__,
-		DPCD_ADDRESS_LANE_01_STATUS, dpcd_buf[0],
-		DPCD_ADDRESS_LANE_23_STATUS, dpcd_buf[1]);
+		DP_LANE0_1_STATUS, dpcd_buf[0],
+		DP_LANE2_3_STATUS, dpcd_buf[1]);
 
 	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
 		"%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
 		__func__,
-		DPCD_ADDRESS_ADJUST_REQUEST_LANE0_1,
+		DP_ADJUST_REQUEST_LANE0_1,
 		dpcd_buf[4],
-		DPCD_ADDRESS_ADJUST_REQUEST_LANE2_3,
+		DP_ADJUST_REQUEST_LANE2_3,
 		dpcd_buf[5]);
 
 	/*copy to req_settings*/
@@ -552,7 +552,7 @@ static void dpcd_set_lane_settings(
 	}
 
 	core_link_write_dpcd(link,
-		DPCD_ADDRESS_LANE0_SET,
+		DP_TRAINING_LANE0_SET,
 		(uint8_t *)(dpcd_lane),
 		link_training_setting->link_settings.lane_count);
 
@@ -579,7 +579,7 @@ static void dpcd_set_lane_settings(
 		"%s\n %x VS set = %x  PE set = %x \
 		max VS Reached = %x  max PE Reached = %x\n",
 		__func__,
-		DPCD_ADDRESS_LANE0_SET,
+		DP_TRAINING_LANE0_SET,
 		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
 		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
 		dpcd_lane[0].bits.MAX_SWING_REACHED,
@@ -935,7 +935,7 @@ static inline bool perform_link_training_int(
 
 	core_link_write_dpcd(
 		link,
-		DPCD_ADDRESS_LANE_COUNT_SET,
+		DP_LANE_COUNT_SET,
 		&lane_count_set.raw,
 		sizeof(lane_count_set));
 
@@ -1385,18 +1385,18 @@ static bool hpd_rx_irq_check_link_loss_status(
 	 */
 
 	dpcd_result = core_link_read_dpcd(link,
-		DPCD_ADDRESS_POWER_STATE,
+		DP_SET_POWER,
 		&irq_reg_rx_power_state,
 		sizeof(irq_reg_rx_power_state));
 
 	if (dpcd_result != DC_OK) {
-		irq_reg_rx_power_state = DP_PWR_STATE_D0;
+		irq_reg_rx_power_state = DP_SET_POWER_D0;
 		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
 			"%s: DPCD read failed to obtain power state.\n",
 			__func__);
 	}
 
-	if (irq_reg_rx_power_state == DP_PWR_STATE_D0) {
+	if (irq_reg_rx_power_state == DP_SET_POWER_D0) {
 
 		/*2. Check that Link Status changed, before re-training.*/
 
@@ -1452,7 +1452,7 @@ static enum dc_status read_hpd_rx_irq_data(
 	 */
 	return core_link_read_dpcd(
 	link,
-	DPCD_ADDRESS_SINK_COUNT,
+	DP_SINK_COUNT,
 	irq_data->raw,
 	sizeof(union hpd_irq_data));
 }
@@ -1536,12 +1536,12 @@ static void dp_test_send_link_training(struct core_link *link)
 
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_LANE_COUNT,
+			DP_TEST_LANE_COUNT,
 			(unsigned char *)(&link_settings.lane_count),
 			1);
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_LINK_RATE,
+			DP_TEST_LINK_RATE,
 			(unsigned char *)(&link_settings.link_rate),
 			1);
 
@@ -1558,8 +1558,8 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	union lane_adjust dpcd_lane_adjustment[2];
 	unsigned char dpcd_post_cursor_2_adjustment = 0;
 	unsigned char test_80_bit_pattern[
-			(DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_79_72 -
-			DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0)+1] = {0};
+			(DP_TEST_80BIT_CUSTOM_PATTERN_79_72 -
+			DP_TEST_80BIT_CUSTOM_PATTERN_7_0)+1] = {0};
 	enum dp_test_pattern test_pattern;
 	struct dc_link_training_settings link_settings;
 	union lane_adjust dpcd_lane_adjust;
@@ -1574,12 +1574,12 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	/* get phy test pattern and pattern parameters from DP receiver */
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_PHY_PATTERN,
+			DP_TEST_PHY_PATTERN,
 			&dpcd_test_pattern.raw,
 			sizeof(dpcd_test_pattern));
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_ADJUST_REQUEST_LANE0_1,
+			DP_ADJUST_REQUEST_LANE0_1,
 			&dpcd_lane_adjustment[0].raw,
 			sizeof(dpcd_lane_adjustment));
 
@@ -1591,7 +1591,7 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	 */
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_ADJUST_REQUEST_POST_CURSOR2,
+			DP_ADJUST_REQUEST_POST_CURSOR2,
 			&dpcd_post_cursor_2_adjustment,
 			sizeof(dpcd_post_cursor_2_adjustment));
 
@@ -1620,7 +1620,7 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 	if (test_pattern == DP_TEST_PATTERN_80BIT_CUSTOM)
 		core_link_read_dpcd(
 				link,
-				DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0,
+				DP_TEST_80BIT_CUSTOM_PATTERN_7_0,
 				test_80_bit_pattern,
 				sizeof(test_80_bit_pattern));
 
@@ -1660,8 +1660,8 @@ static void dp_test_send_phy_test_pattern(struct core_link *link)
 		test_pattern,
 		&link_training_settings,
 		test_80_bit_pattern,
-		(DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_79_72 -
-		DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0)+1);
+		(DP_TEST_80BIT_CUSTOM_PATTERN_79_72 -
+		DP_TEST_80BIT_CUSTOM_PATTERN_7_0)+1);
 }
 
 static void dp_test_send_link_test_pattern(struct core_link *link)
@@ -1676,12 +1676,12 @@ static void dp_test_send_link_test_pattern(struct core_link *link)
 	/* get link test pattern and pattern parameters */
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_PATTERN,
+			DP_TEST_PATTERN,
 			&dpcd_test_pattern.raw,
 			sizeof(dpcd_test_pattern));
 	core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_MISC1,
+			DP_TEST_MISC0,
 			&dpcd_test_params.raw,
 			sizeof(dpcd_test_params));
 
@@ -1721,7 +1721,7 @@ static void handle_automated_test(struct core_link *link)
 
 	core_link_read_dpcd(
 		link,
-		DPCD_ADDRESS_TEST_REQUEST,
+		DP_TEST_REQUEST,
 		&test_request.raw,
 		sizeof(union test_request));
 	if (test_request.bits.LINK_TRAINING) {
@@ -1729,7 +1729,7 @@ static void handle_automated_test(struct core_link *link)
 		test_response.bits.ACK = 1;
 		core_link_write_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_RESPONSE,
+			DP_TEST_RESPONSE,
 			&test_response.raw,
 			sizeof(test_response));
 		dp_test_send_link_training(link);
@@ -1754,7 +1754,7 @@ static void handle_automated_test(struct core_link *link)
 	if (test_response.bits.ACK)
 		core_link_write_dpcd(
 			link,
-			DPCD_ADDRESS_TEST_RESPONSE,
+			DP_TEST_RESPONSE,
 			&test_response.raw,
 			sizeof(test_response));
 }
@@ -1792,7 +1792,7 @@ bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link)
 		device_service_clear.bits.AUTOMATED_TEST = 1;
 		core_link_write_dpcd(
 			link,
-			DPCD_ADDRESS_DEVICE_SERVICE_IRQ_VECTOR,
+			DP_DEVICE_SERVICE_IRQ_VECTOR,
 			&device_service_clear.raw,
 			sizeof(device_service_clear.raw));
 		device_service_clear.raw = 0;
@@ -1872,12 +1872,12 @@ bool is_mst_supported(struct core_link *link)
 	rev.raw  = 0;
 	cap.raw  = 0;
 
-	st = core_link_read_dpcd(link, DPCD_ADDRESS_DPCD_REV, &rev.raw,
+	st = core_link_read_dpcd(link, DP_DPCD_REV, &rev.raw,
 			sizeof(rev));
 
 	if (st == DC_OK && rev.raw >= DPCD_REV_12) {
 
-		st = core_link_read_dpcd(link, DPCD_ADDRESS_MSTM_CAP,
+		st = core_link_read_dpcd(link, DP_MSTM_CAP,
 				&cap.raw, sizeof(cap));
 		if (st == DC_OK && cap.bits.MST_CAP == 1)
 			mst = true;
@@ -1926,7 +1926,7 @@ static void get_active_converter_info(
 		uint8_t det_caps[4];
 		union dwnstream_port_caps_byte0 *port_caps =
 			(union dwnstream_port_caps_byte0 *)det_caps;
-		core_link_read_dpcd(link, DPCD_ADDRESS_DWN_STRM_PORT0_CAPS,
+		core_link_read_dpcd(link, DP_DOWNSTREAM_PORT_0,
 				det_caps, sizeof(det_caps));
 
 		switch (port_caps->bits.DWN_STRM_PORTX_TYPE) {
@@ -1962,7 +1962,7 @@ static void get_active_converter_info(
 		/* read IEEE branch device id */
 		core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_BRANCH_DEVICE_ID_START,
+			DP_BRANCH_OUI,
 			(uint8_t *)&dp_id,
 			sizeof(dp_id));
 
@@ -1982,7 +1982,7 @@ static void get_active_converter_info(
 
 		core_link_read_dpcd(
 			link,
-			DPCD_ADDRESS_BRANCH_REVISION_START,
+			DP_BRANCH_REVISION_START,
 			(uint8_t *)&dp_hw_fw_revision,
 			sizeof(dp_hw_fw_revision));
 
@@ -2000,16 +2000,16 @@ static void dp_wa_power_up_0010FA(struct core_link *link, uint8_t *dpcd_data,
 	if (!link->dpcd_caps.dpcd_rev.raw) {
 		do {
 			dp_receiver_power_ctrl(link, true);
-			core_link_read_dpcd(link, DPCD_ADDRESS_DPCD_REV,
+			core_link_read_dpcd(link, DP_DPCD_REV,
 							dpcd_data, length);
 			link->dpcd_caps.dpcd_rev.raw = dpcd_data[
-				DPCD_ADDRESS_DPCD_REV -
-				DPCD_ADDRESS_DPCD_REV];
+				DP_DPCD_REV -
+				DP_DPCD_REV];
 		} while (retry++ < 4 && !link->dpcd_caps.dpcd_rev.raw);
 	}
 
-	ds_port.byte = dpcd_data[DPCD_ADDRESS_DOWNSTREAM_PORT_PRESENT -
-				 DPCD_ADDRESS_DPCD_REV];
+	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
+				 DP_DPCD_REV];
 
 	if (link->dpcd_caps.dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) {
 		switch (link->dpcd_caps.branch_dev_id) {
@@ -2037,14 +2037,14 @@ static void retrieve_psr_link_cap(struct core_link *link,
 {
 	if (edp_revision >= EDP_REVISION_13) {
 		core_link_read_dpcd(link,
-				DPCD_ADDRESS_PSR_SUPPORT_VER,
+				DP_PSR_SUPPORT,
 				(uint8_t *)(&link->public.psr_caps),
 				sizeof(link->public.psr_caps));
 		if (link->public.psr_caps.psr_version != 0) {
 			unsigned char psr_capability = 0;
 
 			core_link_read_dpcd(link,
-					DPCD_ADDRESS_PSR_CAPABILITY,
+					    DP_PSR_CAPS,
 						&psr_capability,
 						sizeof(psr_capability));
 			/* Bit 0 determines whether fast link training is
@@ -2064,7 +2064,7 @@ static void retrieve_psr_link_cap(struct core_link *link,
 
 static void retrieve_link_cap(struct core_link *link)
 {
-	uint8_t dpcd_data[DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL - DPCD_ADDRESS_DPCD_REV + 1];
+	uint8_t dpcd_data[DP_TRAINING_AUX_RD_INTERVAL - DP_DPCD_REV + 1];
 
 	union down_stream_port_count down_strm_port_count;
 	union edp_configuration_cap edp_config_cap;
@@ -2078,30 +2078,30 @@ static void retrieve_link_cap(struct core_link *link)
 
 	core_link_read_dpcd(
 		link,
-		DPCD_ADDRESS_DPCD_REV,
+		DP_DPCD_REV,
 		dpcd_data,
 		sizeof(dpcd_data));
 
 	link->dpcd_caps.dpcd_rev.raw =
-		dpcd_data[DPCD_ADDRESS_DPCD_REV - DPCD_ADDRESS_DPCD_REV];
+		dpcd_data[DP_DPCD_REV - DP_DPCD_REV];
 
 	{
 		union training_aux_rd_interval aux_rd_interval;
 
 		aux_rd_interval.raw =
-			dpcd_data[DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL];
+			dpcd_data[DP_TRAINING_AUX_RD_INTERVAL];
 
 		if (aux_rd_interval.bits.EXT_RECIEVER_CAP_FIELD_PRESENT == 1) {
 			core_link_read_dpcd(
 				link,
-				DPCD_ADDRESS_DP13_DPCD_REV,
+				DP_DP13_DPCD_REV,
 				dpcd_data,
 				sizeof(dpcd_data));
 		}
 	}
 
-	ds_port.byte = dpcd_data[DPCD_ADDRESS_DOWNSTREAM_PORT_PRESENT -
-				 DPCD_ADDRESS_DPCD_REV];
+	ds_port.byte = dpcd_data[DP_DOWNSTREAMPORT_PRESENT -
+				 DP_DPCD_REV];
 
 	get_active_converter_info(ds_port.byte, link);
 
@@ -2111,21 +2111,21 @@ static void retrieve_link_cap(struct core_link *link)
 		down_strm_port_count.bits.IGNORE_MSA_TIMING_PARAM;
 
 	link->dpcd_caps.max_ln_count.raw = dpcd_data[
-		DPCD_ADDRESS_MAX_LANE_COUNT - DPCD_ADDRESS_DPCD_REV];
+		DP_MAX_LANE_COUNT - DP_DPCD_REV];
 
 	link->dpcd_caps.max_down_spread.raw = dpcd_data[
-		DPCD_ADDRESS_MAX_DOWNSPREAD - DPCD_ADDRESS_DPCD_REV];
+		DP_MAX_DOWNSPREAD - DP_DPCD_REV];
 
 	link->public.reported_link_cap.lane_count =
 		link->dpcd_caps.max_ln_count.bits.MAX_LANE_COUNT;
 	link->public.reported_link_cap.link_rate = dpcd_data[
-		DPCD_ADDRESS_MAX_LINK_RATE - DPCD_ADDRESS_DPCD_REV];
+		DP_MAX_LINK_RATE - DP_DPCD_REV];
 	link->public.reported_link_cap.link_spread =
 		link->dpcd_caps.max_down_spread.bits.MAX_DOWN_SPREAD ?
 		LINK_SPREAD_05_DOWNSPREAD_30KHZ : LINK_SPREAD_DISABLED;
 
 	edp_config_cap.raw = dpcd_data[
-		DPCD_ADDRESS_EDP_CONFIG_CAP - DPCD_ADDRESS_DPCD_REV];
+		DP_EDP_CONFIGURATION_CAP - DP_DPCD_REV];
 	link->dpcd_caps.panel_mode_edp =
 		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
 
@@ -2142,7 +2142,7 @@ static void retrieve_link_cap(struct core_link *link)
 
 	/* read sink count */
 	core_link_read_dpcd(link,
-			DPCD_ADDRESS_SINK_COUNT,
+			DP_SINK_COUNT,
 			&link->dpcd_caps.sink_count.raw,
 			sizeof(link->dpcd_caps.sink_count.raw));
 
@@ -2151,7 +2151,7 @@ static void retrieve_link_cap(struct core_link *link)
 	if (edp_config_cap.bits.DPCD_DISPLAY_CONTROL_CAPABLE) {
 		/* Read the Panel's eDP revision at DPCD 700h. */
 		core_link_read_dpcd(link,
-			DPCD_ADDRESS_EDP_REV,
+			DP_EDP_DPCD_REV,
 			(uint8_t *)(&link->edp_revision),
 			sizeof(link->edp_revision));
 	}
@@ -2415,7 +2415,7 @@ bool dc_link_dp_set_test_pattern(
 						(unsigned char)(pattern);
 
 			core_link_write_dpcd(core_link,
-					DPCD_ADDRESS_LINK_QUAL_LANE0_SET,
+					DP_LINK_QUAL_LANE0_SET,
 					link_qual_pattern,
 					sizeof(link_qual_pattern));
 		} else if (core_link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_10 ||
@@ -2428,12 +2428,12 @@ bool dc_link_dp_set_test_pattern(
 			 * setting test pattern for DP 1.1.
 			 */
 			core_link_read_dpcd(core_link,
-					DPCD_ADDRESS_TRAINING_PATTERN_SET,
+					DP_TRAINING_PATTERN_SET,
 					&training_pattern.raw,
 					sizeof(training_pattern));
 			training_pattern.v1_3.LINK_QUAL_PATTERN_SET = pattern;
 			core_link_write_dpcd(core_link,
-					DPCD_ADDRESS_TRAINING_PATTERN_SET,
+					DP_TRAINING_PATTERN_SET,
 					&training_pattern.raw,
 					sizeof(training_pattern));
 		}
@@ -2452,7 +2452,7 @@ bool dc_link_dp_set_test_pattern(
 						SET_TEST_PATTERN_PENDING = 0;
 			test_response.bits.ACK = 1;
 			core_link_write_dpcd(core_link,
-					DPCD_ADDRESS_TEST_RESPONSE,
+					DP_TEST_RESPONSE,
 					&test_response.raw,
 					sizeof(test_response));
 		}

commit 4562236b3bc0a28aeb6ee93b2d8a849a4c4e1c7c
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Tue Sep 12 15:58:20 2017 -0400

    drm/amd/dc: Add dc display driver (v2)
    
    Supported DCE versions: 8.0, 10.0, 11.0, 11.2
    
    v2: rebase against 4.11
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
new file mode 100644
index 000000000000..2585ec332e58
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/core/dc_link_dp.c
@@ -0,0 +1,2462 @@
+/* Copyright 2015 Advanced Micro Devices, Inc. */
+#include "dm_services.h"
+#include "dc.h"
+#include "dc_link_dp.h"
+#include "dm_helpers.h"
+
+#include "inc/core_types.h"
+#include "link_hwss.h"
+#include "dc_link_ddc.h"
+#include "core_status.h"
+#include "dpcd_defs.h"
+
+#include "core_dc.h"
+
+/* maximum pre emphasis level allowed for each voltage swing level*/
+static const enum dc_pre_emphasis voltage_swing_to_pre_emphasis[] = {
+		PRE_EMPHASIS_LEVEL3,
+		PRE_EMPHASIS_LEVEL2,
+		PRE_EMPHASIS_LEVEL1,
+		PRE_EMPHASIS_DISABLED };
+
+enum {
+	POST_LT_ADJ_REQ_LIMIT = 6,
+	POST_LT_ADJ_REQ_TIMEOUT = 200
+};
+
+enum {
+	LINK_TRAINING_MAX_RETRY_COUNT = 5,
+	/* to avoid infinite loop where-in the receiver
+	 * switches between different VS
+	 */
+	LINK_TRAINING_MAX_CR_RETRY = 100
+};
+
+static const struct dc_link_settings link_training_fallback_table[] = {
+/* 4320 Mbytes/sec*/
+{ LANE_COUNT_FOUR, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
+/* 2160 Mbytes/sec*/
+{ LANE_COUNT_FOUR, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
+/* 1080 Mbytes/sec*/
+{ LANE_COUNT_FOUR, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
+/* 648 Mbytes/sec*/
+{ LANE_COUNT_FOUR, LINK_RATE_LOW, LINK_SPREAD_DISABLED },
+/* 2160 Mbytes/sec*/
+{ LANE_COUNT_TWO, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
+/* 1080 Mbytes/sec*/
+{ LANE_COUNT_TWO, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
+/* 540 Mbytes/sec*/
+{ LANE_COUNT_TWO, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
+/* 324 Mbytes/sec*/
+{ LANE_COUNT_TWO, LINK_RATE_LOW, LINK_SPREAD_DISABLED },
+/* 1080 Mbytes/sec*/
+{ LANE_COUNT_ONE, LINK_RATE_HIGH3, LINK_SPREAD_DISABLED },
+/* 540 Mbytes/sec*/
+{ LANE_COUNT_ONE, LINK_RATE_HIGH2, LINK_SPREAD_DISABLED },
+/* 270 Mbytes/sec*/
+{ LANE_COUNT_ONE, LINK_RATE_HIGH, LINK_SPREAD_DISABLED },
+/* 162 Mbytes/sec*/
+{ LANE_COUNT_ONE, LINK_RATE_LOW, LINK_SPREAD_DISABLED } };
+
+static void wait_for_training_aux_rd_interval(
+	struct core_link* link,
+	uint32_t default_wait_in_micro_secs)
+{
+	union training_aux_rd_interval training_rd_interval;
+
+	/* overwrite the delay if rev > 1.1*/
+	if (link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {
+		/* DP 1.2 or later - retrieve delay through
+		 * "DPCD_ADDR_TRAINING_AUX_RD_INTERVAL" register */
+		core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL,
+			(uint8_t *)&training_rd_interval,
+			sizeof(training_rd_interval));
+
+		if (training_rd_interval.bits.TRAINIG_AUX_RD_INTERVAL)
+			default_wait_in_micro_secs =
+				training_rd_interval.bits.TRAINIG_AUX_RD_INTERVAL * 4000;
+	}
+
+	udelay(default_wait_in_micro_secs);
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s:\n wait = %d\n",
+		__func__,
+		default_wait_in_micro_secs);
+}
+
+static void dpcd_set_training_pattern(
+	struct core_link* link,
+	union dpcd_training_pattern dpcd_pattern)
+{
+	core_link_write_dpcd(
+		link,
+		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		&dpcd_pattern.raw,
+		1);
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s\n %x pattern = %x\n",
+		__func__,
+		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
+}
+
+static void dpcd_set_link_settings(
+	struct core_link* link,
+	const struct link_training_settings *lt_settings)
+{
+	uint8_t rate = (uint8_t)
+	(lt_settings->link_settings.link_rate);
+
+	union down_spread_ctrl downspread = {{0}};
+	union lane_count_set lane_count_set = {{0}};
+	uint8_t link_set_buffer[2];
+
+	downspread.raw = (uint8_t)
+	(lt_settings->link_settings.link_spread);
+
+	lane_count_set.bits.LANE_COUNT_SET =
+	lt_settings->link_settings.lane_count;
+
+	lane_count_set.bits.ENHANCED_FRAMING = 1;
+
+	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED =
+		link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED;
+
+	link_set_buffer[0] = rate;
+	link_set_buffer[1] = lane_count_set.raw;
+
+	core_link_write_dpcd(link, DPCD_ADDRESS_LINK_BW_SET,
+	link_set_buffer, 2);
+	core_link_write_dpcd(link, DPCD_ADDRESS_DOWNSPREAD_CNTL,
+	&downspread.raw, sizeof(downspread));
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s\n %x rate = %x\n %x lane = %x\n %x spread = %x\n",
+		__func__,
+		DPCD_ADDRESS_LINK_BW_SET,
+		lt_settings->link_settings.link_rate,
+		DPCD_ADDRESS_LANE_COUNT_SET,
+		lt_settings->link_settings.lane_count,
+		DPCD_ADDRESS_DOWNSPREAD_CNTL,
+		lt_settings->link_settings.link_spread);
+
+}
+
+static enum dpcd_training_patterns
+	hw_training_pattern_to_dpcd_training_pattern(
+	struct core_link* link,
+	enum hw_dp_training_pattern pattern)
+{
+	enum dpcd_training_patterns dpcd_tr_pattern =
+	DPCD_TRAINING_PATTERN_VIDEOIDLE;
+
+	switch (pattern) {
+	case HW_DP_TRAINING_PATTERN_1:
+		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_1;
+		break;
+	case HW_DP_TRAINING_PATTERN_2:
+		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_2;
+		break;
+	case HW_DP_TRAINING_PATTERN_3:
+		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_3;
+		break;
+	case HW_DP_TRAINING_PATTERN_4:
+		dpcd_tr_pattern = DPCD_TRAINING_PATTERN_4;
+		break;
+	default:
+		ASSERT(0);
+		dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+			"%s: Invalid HW Training pattern: %d\n",
+			__func__, pattern);
+		break;
+	}
+
+	return dpcd_tr_pattern;
+
+}
+
+static void dpcd_set_lt_pattern_and_lane_settings(
+	struct core_link* link,
+	const struct link_training_settings *lt_settings,
+	enum hw_dp_training_pattern pattern)
+{
+	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
+	const uint32_t dpcd_base_lt_offset =
+	DPCD_ADDRESS_TRAINING_PATTERN_SET;
+	uint8_t dpcd_lt_buffer[5] = {0};
+	union dpcd_training_pattern dpcd_pattern = {{0}};
+	uint32_t lane;
+	uint32_t size_in_bytes;
+	bool edp_workaround = false; /* TODO link_prop.INTERNAL */
+
+	/*****************************************************************
+	* DpcdAddress_TrainingPatternSet
+	*****************************************************************/
+	dpcd_pattern.v1_4.TRAINING_PATTERN_SET =
+		hw_training_pattern_to_dpcd_training_pattern(link, pattern);
+
+	dpcd_lt_buffer[DPCD_ADDRESS_TRAINING_PATTERN_SET - dpcd_base_lt_offset]
+		= dpcd_pattern.raw;
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s\n %x pattern = %x\n",
+		__func__,
+		DPCD_ADDRESS_TRAINING_PATTERN_SET,
+		dpcd_pattern.v1_4.TRAINING_PATTERN_SET);
+
+	/*****************************************************************
+	* DpcdAddress_Lane0Set -> DpcdAddress_Lane3Set
+	*****************************************************************/
+	for (lane = 0; lane <
+		(uint32_t)(lt_settings->link_settings.lane_count); lane++) {
+
+		dpcd_lane[lane].bits.VOLTAGE_SWING_SET =
+		(uint8_t)(lt_settings->lane_settings[lane].VOLTAGE_SWING);
+		dpcd_lane[lane].bits.PRE_EMPHASIS_SET =
+		(uint8_t)(lt_settings->lane_settings[lane].PRE_EMPHASIS);
+
+		dpcd_lane[lane].bits.MAX_SWING_REACHED =
+		(lt_settings->lane_settings[lane].VOLTAGE_SWING ==
+		VOLTAGE_SWING_MAX_LEVEL ? 1 : 0);
+		dpcd_lane[lane].bits.MAX_PRE_EMPHASIS_REACHED =
+		(lt_settings->lane_settings[lane].PRE_EMPHASIS ==
+		PRE_EMPHASIS_MAX_LEVEL ? 1 : 0);
+	}
+
+	/* concatinate everything into one buffer*/
+
+	size_in_bytes = lt_settings->link_settings.lane_count * sizeof(dpcd_lane[0]);
+
+	 // 0x00103 - 0x00102
+	memmove(
+		&dpcd_lt_buffer[DPCD_ADDRESS_LANE0_SET - dpcd_base_lt_offset],
+		dpcd_lane,
+		size_in_bytes);
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s:\n %x VS set = %x  PE set = %x \
+		max VS Reached = %x  max PE Reached = %x\n",
+		__func__,
+		DPCD_ADDRESS_LANE0_SET,
+		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+		dpcd_lane[0].bits.MAX_SWING_REACHED,
+		dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+
+	if (edp_workaround) {
+		/* for eDP write in 2 parts because the 5-byte burst is
+		* causing issues on some eDP panels (EPR#366724)
+		*/
+		core_link_write_dpcd(
+			link,
+			DPCD_ADDRESS_TRAINING_PATTERN_SET,
+			&dpcd_pattern.raw,
+			sizeof(dpcd_pattern.raw) );
+
+		core_link_write_dpcd(
+			link,
+			DPCD_ADDRESS_LANE0_SET,
+			(uint8_t *)(dpcd_lane),
+			size_in_bytes);
+
+		} else
+		/* write it all in (1 + number-of-lanes)-byte burst*/
+			core_link_write_dpcd(
+				link,
+				dpcd_base_lt_offset,
+				dpcd_lt_buffer,
+				size_in_bytes + sizeof(dpcd_pattern.raw) );
+
+	link->public.cur_lane_setting = lt_settings->lane_settings[0];
+}
+
+static bool is_cr_done(enum dc_lane_count ln_count,
+	union lane_status *dpcd_lane_status)
+{
+	bool done = true;
+	uint32_t lane;
+	/*LANEx_CR_DONE bits All 1's?*/
+	for (lane = 0; lane < (uint32_t)(ln_count); lane++) {
+		if (!dpcd_lane_status[lane].bits.CR_DONE_0)
+			done = false;
+	}
+	return done;
+
+}
+
+static bool is_ch_eq_done(enum dc_lane_count ln_count,
+	union lane_status *dpcd_lane_status,
+	union lane_align_status_updated *lane_status_updated)
+{
+	bool done = true;
+	uint32_t lane;
+	if (!lane_status_updated->bits.INTERLANE_ALIGN_DONE)
+		done = false;
+	else {
+		for (lane = 0; lane < (uint32_t)(ln_count); lane++) {
+			if (!dpcd_lane_status[lane].bits.SYMBOL_LOCKED_0 ||
+				!dpcd_lane_status[lane].bits.CHANNEL_EQ_DONE_0)
+				done = false;
+		}
+	}
+	return done;
+
+}
+
+static void update_drive_settings(
+		struct link_training_settings *dest,
+		struct link_training_settings src)
+{
+	uint32_t lane;
+	for (lane = 0; lane < src.link_settings.lane_count; lane++) {
+		dest->lane_settings[lane].VOLTAGE_SWING =
+			src.lane_settings[lane].VOLTAGE_SWING;
+		dest->lane_settings[lane].PRE_EMPHASIS =
+			src.lane_settings[lane].PRE_EMPHASIS;
+		dest->lane_settings[lane].POST_CURSOR2 =
+			src.lane_settings[lane].POST_CURSOR2;
+	}
+}
+
+static uint8_t get_nibble_at_index(const uint8_t *buf,
+	uint32_t index)
+{
+	uint8_t nibble;
+	nibble = buf[index / 2];
+
+	if (index % 2)
+		nibble >>= 4;
+	else
+		nibble &= 0x0F;
+
+	return nibble;
+}
+
+static enum dc_pre_emphasis get_max_pre_emphasis_for_voltage_swing(
+	enum dc_voltage_swing voltage)
+{
+	enum dc_pre_emphasis pre_emphasis;
+	pre_emphasis = PRE_EMPHASIS_MAX_LEVEL;
+
+	if (voltage <= VOLTAGE_SWING_MAX_LEVEL)
+		pre_emphasis = voltage_swing_to_pre_emphasis[voltage];
+
+	return pre_emphasis;
+
+}
+
+static void find_max_drive_settings(
+	const struct link_training_settings *link_training_setting,
+	struct link_training_settings *max_lt_setting)
+{
+	uint32_t lane;
+	struct dc_lane_settings max_requested;
+
+	max_requested.VOLTAGE_SWING =
+		link_training_setting->
+		lane_settings[0].VOLTAGE_SWING;
+	max_requested.PRE_EMPHASIS =
+		link_training_setting->
+		lane_settings[0].PRE_EMPHASIS;
+	/*max_requested.postCursor2 =
+	 * link_training_setting->laneSettings[0].postCursor2;*/
+
+	/* Determine what the maximum of the requested settings are*/
+	for (lane = 1; lane < link_training_setting->link_settings.lane_count;
+			lane++) {
+		if (link_training_setting->lane_settings[lane].VOLTAGE_SWING >
+			max_requested.VOLTAGE_SWING)
+
+			max_requested.VOLTAGE_SWING =
+			link_training_setting->
+			lane_settings[lane].VOLTAGE_SWING;
+
+		if (link_training_setting->lane_settings[lane].PRE_EMPHASIS >
+				max_requested.PRE_EMPHASIS)
+			max_requested.PRE_EMPHASIS =
+			link_training_setting->
+			lane_settings[lane].PRE_EMPHASIS;
+
+		/*
+		if (link_training_setting->laneSettings[lane].postCursor2 >
+		 max_requested.postCursor2)
+		{
+		max_requested.postCursor2 =
+		link_training_setting->laneSettings[lane].postCursor2;
+		}
+		*/
+	}
+
+	/* make sure the requested settings are
+	 * not higher than maximum settings*/
+	if (max_requested.VOLTAGE_SWING > VOLTAGE_SWING_MAX_LEVEL)
+		max_requested.VOLTAGE_SWING = VOLTAGE_SWING_MAX_LEVEL;
+
+	if (max_requested.PRE_EMPHASIS > PRE_EMPHASIS_MAX_LEVEL)
+		max_requested.PRE_EMPHASIS = PRE_EMPHASIS_MAX_LEVEL;
+	/*
+	if (max_requested.postCursor2 > PostCursor2_MaxLevel)
+	max_requested.postCursor2 = PostCursor2_MaxLevel;
+	*/
+
+	/* make sure the pre-emphasis matches the voltage swing*/
+	if (max_requested.PRE_EMPHASIS >
+		get_max_pre_emphasis_for_voltage_swing(
+			max_requested.VOLTAGE_SWING))
+		max_requested.PRE_EMPHASIS =
+		get_max_pre_emphasis_for_voltage_swing(
+			max_requested.VOLTAGE_SWING);
+
+	/*
+	 * Post Cursor2 levels are completely independent from
+	 * pre-emphasis (Post Cursor1) levels. But Post Cursor2 levels
+	 * can only be applied to each allowable combination of voltage
+	 * swing and pre-emphasis levels */
+	 /* if ( max_requested.postCursor2 >
+	  *  getMaxPostCursor2ForVoltageSwing(max_requested.voltageSwing))
+	  *  max_requested.postCursor2 =
+	  *  getMaxPostCursor2ForVoltageSwing(max_requested.voltageSwing);
+	  */
+
+	max_lt_setting->link_settings.link_rate =
+		link_training_setting->link_settings.link_rate;
+	max_lt_setting->link_settings.lane_count =
+	link_training_setting->link_settings.lane_count;
+	max_lt_setting->link_settings.link_spread =
+		link_training_setting->link_settings.link_spread;
+
+	for (lane = 0; lane <
+		link_training_setting->link_settings.lane_count;
+		lane++) {
+		max_lt_setting->lane_settings[lane].VOLTAGE_SWING =
+			max_requested.VOLTAGE_SWING;
+		max_lt_setting->lane_settings[lane].PRE_EMPHASIS =
+			max_requested.PRE_EMPHASIS;
+		/*max_lt_setting->laneSettings[lane].postCursor2 =
+		 * max_requested.postCursor2;
+		 */
+	}
+
+}
+
+static void get_lane_status_and_drive_settings(
+	struct core_link* link,
+	const struct link_training_settings *link_training_setting,
+	union lane_status *ln_status,
+	union lane_align_status_updated *ln_status_updated,
+	struct link_training_settings *req_settings)
+{
+	uint8_t dpcd_buf[6] = {0};
+	union lane_adjust dpcd_lane_adjust[LANE_COUNT_DP_MAX] = {{{0}}};
+	struct link_training_settings request_settings = {{0}};
+	uint32_t lane;
+
+	memset(req_settings, '\0', sizeof(struct link_training_settings));
+
+	core_link_read_dpcd(
+		link,
+		DPCD_ADDRESS_LANE_01_STATUS,
+		(uint8_t *)(dpcd_buf),
+		sizeof(dpcd_buf));
+
+	for (lane = 0; lane <
+		(uint32_t)(link_training_setting->link_settings.lane_count);
+		lane++) {
+
+		ln_status[lane].raw =
+			get_nibble_at_index(&dpcd_buf[0], lane);
+		dpcd_lane_adjust[lane].raw =
+			get_nibble_at_index(&dpcd_buf[4], lane);
+	}
+
+	ln_status_updated->raw = dpcd_buf[2];
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s:\n%x Lane01Status = %x\n %x Lane23Status = %x\n ",
+		__func__,
+		DPCD_ADDRESS_LANE_01_STATUS, dpcd_buf[0],
+		DPCD_ADDRESS_LANE_23_STATUS, dpcd_buf[1]);
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s:\n %x Lane01AdjustRequest = %x\n %x Lane23AdjustRequest = %x\n",
+		__func__,
+		DPCD_ADDRESS_ADJUST_REQUEST_LANE0_1,
+		dpcd_buf[4],
+		DPCD_ADDRESS_ADJUST_REQUEST_LANE2_3,
+		dpcd_buf[5]);
+
+	/*copy to req_settings*/
+	request_settings.link_settings.lane_count =
+		link_training_setting->link_settings.lane_count;
+	request_settings.link_settings.link_rate =
+		link_training_setting->link_settings.link_rate;
+	request_settings.link_settings.link_spread =
+		link_training_setting->link_settings.link_spread;
+
+	for (lane = 0; lane <
+		(uint32_t)(link_training_setting->link_settings.lane_count);
+		lane++) {
+
+		request_settings.lane_settings[lane].VOLTAGE_SWING =
+			(enum dc_voltage_swing)(dpcd_lane_adjust[lane].bits.
+				VOLTAGE_SWING_LANE);
+		request_settings.lane_settings[lane].PRE_EMPHASIS =
+			(enum dc_pre_emphasis)(dpcd_lane_adjust[lane].bits.
+				PRE_EMPHASIS_LANE);
+	}
+
+	/*Note: for postcursor2, read adjusted
+	 * postcursor2 settings from*/
+	/*DpcdAddress_AdjustRequestPostCursor2 =
+	 *0x020C (not implemented yet)*/
+
+	/* we find the maximum of the requested settings across all lanes*/
+	/* and set this maximum for all lanes*/
+	find_max_drive_settings(&request_settings, req_settings);
+
+	/* if post cursor 2 is needed in the future,
+	 * read DpcdAddress_AdjustRequestPostCursor2 = 0x020C
+	 */
+
+}
+
+static void dpcd_set_lane_settings(
+	struct core_link* link,
+	const struct link_training_settings *link_training_setting)
+{
+	union dpcd_training_lane dpcd_lane[LANE_COUNT_DP_MAX] = {{{0}}};
+	uint32_t lane;
+
+	for (lane = 0; lane <
+		(uint32_t)(link_training_setting->
+		link_settings.lane_count);
+		lane++) {
+		dpcd_lane[lane].bits.VOLTAGE_SWING_SET =
+			(uint8_t)(link_training_setting->
+			lane_settings[lane].VOLTAGE_SWING);
+		dpcd_lane[lane].bits.PRE_EMPHASIS_SET =
+			(uint8_t)(link_training_setting->
+			lane_settings[lane].PRE_EMPHASIS);
+		dpcd_lane[lane].bits.MAX_SWING_REACHED =
+			(link_training_setting->
+			lane_settings[lane].VOLTAGE_SWING ==
+			VOLTAGE_SWING_MAX_LEVEL ? 1 : 0);
+		dpcd_lane[lane].bits.MAX_PRE_EMPHASIS_REACHED =
+			(link_training_setting->
+			lane_settings[lane].PRE_EMPHASIS ==
+			PRE_EMPHASIS_MAX_LEVEL ? 1 : 0);
+	}
+
+	core_link_write_dpcd(link,
+		DPCD_ADDRESS_LANE0_SET,
+		(uint8_t *)(dpcd_lane),
+		link_training_setting->link_settings.lane_count);
+
+	/*
+	if (LTSettings.link.rate == LinkRate_High2)
+	{
+		DpcdTrainingLaneSet2 dpcd_lane2[lane_count_DPMax] = {0};
+		for ( uint32_t lane = 0;
+		lane < lane_count_DPMax; lane++)
+		{
+			dpcd_lane2[lane].bits.post_cursor2_set =
+			static_cast<unsigned char>(
+			LTSettings.laneSettings[lane].postCursor2);
+			dpcd_lane2[lane].bits.max_post_cursor2_reached = 0;
+		}
+		m_pDpcdAccessSrv->WriteDpcdData(
+		DpcdAddress_Lane0Set2,
+		reinterpret_cast<unsigned char*>(dpcd_lane2),
+		LTSettings.link.lanes);
+	}
+	*/
+
+	dm_logger_write(link->ctx->logger, LOG_HW_LINK_TRAINING,
+		"%s\n %x VS set = %x  PE set = %x \
+		max VS Reached = %x  max PE Reached = %x\n",
+		__func__,
+		DPCD_ADDRESS_LANE0_SET,
+		dpcd_lane[0].bits.VOLTAGE_SWING_SET,
+		dpcd_lane[0].bits.PRE_EMPHASIS_SET,
+		dpcd_lane[0].bits.MAX_SWING_REACHED,
+		dpcd_lane[0].bits.MAX_PRE_EMPHASIS_REACHED);
+
+	link->public.cur_lane_setting = link_training_setting->lane_settings[0];
+
+}
+
+static bool is_max_vs_reached(
+	const struct link_training_settings *lt_settings)
+{
+	uint32_t lane;
+	for (lane = 0; lane <
+		(uint32_t)(lt_settings->link_settings.lane_count);
+		lane++) {
+		if (lt_settings->lane_settings[lane].VOLTAGE_SWING
+			== VOLTAGE_SWING_MAX_LEVEL)
+			return true;
+	}
+	return false;
+
+}
+
+void dc_link_dp_set_drive_settings(
+	struct dc_link *link,
+	struct link_training_settings *lt_settings)
+{
+	struct core_link *core_link = DC_LINK_TO_CORE(link);
+	/* program ASIC PHY settings*/
+	dp_set_hw_lane_settings(core_link, lt_settings);
+
+	/* Notify DP sink the PHY settings from source */
+	dpcd_set_lane_settings(core_link, lt_settings);
+}
+
+static bool perform_post_lt_adj_req_sequence(
+	struct core_link *link,
+	struct link_training_settings *lt_settings)
+{
+	enum dc_lane_count lane_count =
+	lt_settings->link_settings.lane_count;
+
+	uint32_t adj_req_count;
+	uint32_t adj_req_timer;
+	bool req_drv_setting_changed;
+	uint32_t lane;
+
+	req_drv_setting_changed = false;
+	for (adj_req_count = 0; adj_req_count < POST_LT_ADJ_REQ_LIMIT;
+	adj_req_count++) {
+
+		req_drv_setting_changed = false;
+
+		for (adj_req_timer = 0;
+			adj_req_timer < POST_LT_ADJ_REQ_TIMEOUT;
+			adj_req_timer++) {
+
+			struct link_training_settings req_settings;
+			union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX];
+			union lane_align_status_updated
+				dpcd_lane_status_updated;
+
+			get_lane_status_and_drive_settings(
+			link,
+			lt_settings,
+			dpcd_lane_status,
+			&dpcd_lane_status_updated,
+			&req_settings);
+
+			if (dpcd_lane_status_updated.bits.
+					POST_LT_ADJ_REQ_IN_PROGRESS == 0)
+				return true;
+
+			if (!is_cr_done(lane_count, dpcd_lane_status))
+				return false;
+
+			if (!is_ch_eq_done(
+				lane_count,
+				dpcd_lane_status,
+				&dpcd_lane_status_updated))
+				return false;
+
+			for (lane = 0; lane < (uint32_t)(lane_count); lane++) {
+
+				if (lt_settings->
+				lane_settings[lane].VOLTAGE_SWING !=
+				req_settings.lane_settings[lane].
+				VOLTAGE_SWING ||
+				lt_settings->lane_settings[lane].PRE_EMPHASIS !=
+				req_settings.lane_settings[lane].PRE_EMPHASIS) {
+
+					req_drv_setting_changed = true;
+					break;
+				}
+			}
+
+			if (req_drv_setting_changed) {
+				update_drive_settings(
+					lt_settings,req_settings);
+
+				dc_link_dp_set_drive_settings(&link->public,
+						lt_settings);
+				break;
+			}
+
+			msleep(1);
+		}
+
+		if (!req_drv_setting_changed) {
+			dm_logger_write(link->ctx->logger, LOG_WARNING,
+				"%s: Post Link Training Adjust Request Timed out\n",
+				__func__);
+
+			ASSERT(0);
+			return true;
+		}
+	}
+	dm_logger_write(link->ctx->logger, LOG_WARNING,
+		"%s: Post Link Training Adjust Request limit reached\n",
+		__func__);
+
+	ASSERT(0);
+	return true;
+
+}
+
+static enum hw_dp_training_pattern get_supported_tp(struct core_link *link)
+{
+	enum hw_dp_training_pattern highest_tp = HW_DP_TRAINING_PATTERN_2;
+	struct encoder_feature_support *features = &link->link_enc->features;
+	struct dpcd_caps *dpcd_caps = &link->dpcd_caps;
+
+	if (features->flags.bits.IS_TPS3_CAPABLE)
+		highest_tp = HW_DP_TRAINING_PATTERN_3;
+
+	if (features->flags.bits.IS_TPS4_CAPABLE)
+		highest_tp = HW_DP_TRAINING_PATTERN_4;
+
+	if (dpcd_caps->max_down_spread.bits.TPS4_SUPPORTED &&
+		highest_tp >= HW_DP_TRAINING_PATTERN_4)
+		return HW_DP_TRAINING_PATTERN_4;
+
+	if (dpcd_caps->max_ln_count.bits.TPS3_SUPPORTED &&
+		highest_tp >= HW_DP_TRAINING_PATTERN_3)
+		return HW_DP_TRAINING_PATTERN_3;
+
+	return HW_DP_TRAINING_PATTERN_2;
+}
+
+static bool perform_channel_equalization_sequence(
+	struct core_link *link,
+	struct link_training_settings *lt_settings)
+{
+	struct link_training_settings req_settings;
+	enum hw_dp_training_pattern hw_tr_pattern;
+	uint32_t retries_ch_eq;
+	enum dc_lane_count lane_count = lt_settings->link_settings.lane_count;
+	union lane_align_status_updated dpcd_lane_status_updated = {{0}};
+	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX] = {{{0}}};;
+
+	hw_tr_pattern = get_supported_tp(link);
+
+	dp_set_hw_training_pattern(link, hw_tr_pattern);
+
+	for (retries_ch_eq = 0; retries_ch_eq <= LINK_TRAINING_MAX_RETRY_COUNT;
+		retries_ch_eq++) {
+
+		dp_set_hw_lane_settings(link, lt_settings);
+
+		/* 2. update DPCD*/
+		if (!retries_ch_eq)
+			/* EPR #361076 - write as a 5-byte burst,
+			 * but only for the 1-st iteration*/
+			dpcd_set_lt_pattern_and_lane_settings(
+				link,
+				lt_settings,
+				hw_tr_pattern);
+		else
+			dpcd_set_lane_settings(link, lt_settings);
+
+		/* 3. wait for receiver to lock-on*/
+		wait_for_training_aux_rd_interval(link, 400);
+
+		/* 4. Read lane status and requested
+		 * drive settings as set by the sink*/
+
+		get_lane_status_and_drive_settings(
+			link,
+			lt_settings,
+			dpcd_lane_status,
+			&dpcd_lane_status_updated,
+			&req_settings);
+
+		/* 5. check CR done*/
+		if (!is_cr_done(lane_count, dpcd_lane_status))
+			return false;
+
+		/* 6. check CHEQ done*/
+		if (is_ch_eq_done(lane_count,
+			dpcd_lane_status,
+			&dpcd_lane_status_updated))
+			return true;
+
+		/* 7. update VS/PE/PC2 in lt_settings*/
+		update_drive_settings(lt_settings, req_settings);
+	}
+
+	return false;
+
+}
+
+static bool perform_clock_recovery_sequence(
+	struct core_link *link,
+	struct link_training_settings *lt_settings)
+{
+	uint32_t retries_cr;
+	uint32_t retry_count;
+	uint32_t lane;
+	struct link_training_settings req_settings;
+	enum dc_lane_count lane_count =
+	lt_settings->link_settings.lane_count;
+	enum hw_dp_training_pattern hw_tr_pattern = HW_DP_TRAINING_PATTERN_1;
+	union lane_status dpcd_lane_status[LANE_COUNT_DP_MAX];
+	union lane_align_status_updated dpcd_lane_status_updated;
+
+	retries_cr = 0;
+	retry_count = 0;
+	/* initial drive setting (VS/PE/PC2)*/
+	for (lane = 0; lane < LANE_COUNT_DP_MAX; lane++) {
+		lt_settings->lane_settings[lane].VOLTAGE_SWING =
+		VOLTAGE_SWING_LEVEL0;
+		lt_settings->lane_settings[lane].PRE_EMPHASIS =
+		PRE_EMPHASIS_DISABLED;
+		lt_settings->lane_settings[lane].POST_CURSOR2 =
+		POST_CURSOR2_DISABLED;
+	}
+
+	dp_set_hw_training_pattern(link, hw_tr_pattern);
+
+	/* najeeb - The synaptics MST hub can put the LT in
+	* infinite loop by switching the VS
+	*/
+	/* between level 0 and level 1 continuously, here
+	* we try for CR lock for LinkTrainingMaxCRRetry count*/
+	while ((retries_cr < LINK_TRAINING_MAX_RETRY_COUNT) &&
+	(retry_count < LINK_TRAINING_MAX_CR_RETRY)) {
+
+		memset(&dpcd_lane_status, '\0', sizeof(dpcd_lane_status));
+		memset(&dpcd_lane_status_updated, '\0',
+		sizeof(dpcd_lane_status_updated));
+
+		/* 1. call HWSS to set lane settings*/
+		dp_set_hw_lane_settings(
+				link,
+				lt_settings);
+
+		/* 2. update DPCD of the receiver*/
+		if (!retries_cr)
+			/* EPR #361076 - write as a 5-byte burst,
+			 * but only for the 1-st iteration.*/
+			dpcd_set_lt_pattern_and_lane_settings(
+					link,
+					lt_settings,
+					hw_tr_pattern);
+		else
+			dpcd_set_lane_settings(
+					link,
+					lt_settings);
+
+		/* 3. wait receiver to lock-on*/
+		wait_for_training_aux_rd_interval(
+				link,
+				100);
+
+		/* 4. Read lane status and requested drive
+		* settings as set by the sink
+		*/
+		get_lane_status_and_drive_settings(
+				link,
+				lt_settings,
+				dpcd_lane_status,
+				&dpcd_lane_status_updated,
+				&req_settings);
+
+		/* 5. check CR done*/
+		if (is_cr_done(lane_count, dpcd_lane_status))
+			return true;
+
+		/* 6. max VS reached*/
+		if (is_max_vs_reached(lt_settings))
+			return false;
+
+		/* 7. same voltage*/
+		/* Note: VS same for all lanes,
+		* so comparing first lane is sufficient*/
+		if (lt_settings->lane_settings[0].VOLTAGE_SWING ==
+			req_settings.lane_settings[0].VOLTAGE_SWING)
+			retries_cr++;
+		else
+			retries_cr = 0;
+
+		/* 8. update VS/PE/PC2 in lt_settings*/
+		update_drive_settings(lt_settings, req_settings);
+
+		retry_count++;
+	}
+
+	if (retry_count >= LINK_TRAINING_MAX_CR_RETRY) {
+		ASSERT(0);
+		dm_logger_write(link->ctx->logger, LOG_ERROR,
+			"%s: Link Training Error, could not \
+			 get CR after %d tries. \
+			Possibly voltage swing issue", __func__,
+			LINK_TRAINING_MAX_CR_RETRY);
+
+	}
+
+	return false;
+}
+
+static inline bool perform_link_training_int(
+	struct core_link *link,
+	struct link_training_settings *lt_settings,
+	bool status)
+{
+	union lane_count_set lane_count_set = { {0} };
+	union dpcd_training_pattern dpcd_pattern = { {0} };
+
+	/* 3. set training not in progress*/
+	dpcd_pattern.v1_4.TRAINING_PATTERN_SET = DPCD_TRAINING_PATTERN_VIDEOIDLE;
+	dpcd_set_training_pattern(link, dpcd_pattern);
+
+	/* 4. mainlink output idle pattern*/
+	dp_set_hw_test_pattern(link, DP_TEST_PATTERN_VIDEO_MODE, NULL, 0);
+
+	/*
+	 * 5. post training adjust if required
+	 * If the upstream DPTX and downstream DPRX both support TPS4,
+	 * TPS4 must be used instead of POST_LT_ADJ_REQ.
+	 */
+	if (link->dpcd_caps.max_ln_count.bits.POST_LT_ADJ_REQ_SUPPORTED != 1 &&
+		get_supported_tp(link) == HW_DP_TRAINING_PATTERN_4)
+		return status;
+
+	if (status &&
+		perform_post_lt_adj_req_sequence(link, lt_settings) == false)
+		status = false;
+
+	lane_count_set.bits.LANE_COUNT_SET = lt_settings->link_settings.lane_count;
+	lane_count_set.bits.ENHANCED_FRAMING = 1;
+	lane_count_set.bits.POST_LT_ADJ_REQ_GRANTED = 0;
+
+	core_link_write_dpcd(
+		link,
+		DPCD_ADDRESS_LANE_COUNT_SET,
+		&lane_count_set.raw,
+		sizeof(lane_count_set));
+
+	return status;
+}
+
+bool dc_link_dp_perform_link_training(
+	struct dc_link *link,
+	const struct dc_link_settings *link_setting,
+	bool skip_video_pattern)
+{
+	struct core_link *core_link = DC_LINK_TO_CORE(link);
+	bool status;
+
+	char *link_rate = "Unknown";
+	struct link_training_settings lt_settings;
+
+	status = false;
+	memset(&lt_settings, '\0', sizeof(lt_settings));
+
+	lt_settings.link_settings.link_rate = link_setting->link_rate;
+	lt_settings.link_settings.lane_count = link_setting->lane_count;
+
+	/*@todo[vdevulap] move SS to LS, should not be handled by displaypath*/
+
+	/* TODO hard coded to SS for now
+	 * lt_settings.link_settings.link_spread =
+	 * dal_display_path_is_ss_supported(
+	 * path_mode->display_path) ?
+	 * LINK_SPREAD_05_DOWNSPREAD_30KHZ :
+	 * LINK_SPREAD_DISABLED;
+	 */
+	lt_settings.link_settings.link_spread = LINK_SPREAD_05_DOWNSPREAD_30KHZ;
+
+	/* 1. set link rate, lane count and spread*/
+	dpcd_set_link_settings(core_link, &lt_settings);
+
+	/* 2. perform link training (set link training done
+	 *  to false is done as well)*/
+	if (perform_clock_recovery_sequence(core_link, &lt_settings)) {
+
+		if (perform_channel_equalization_sequence(core_link,
+				&lt_settings))
+			status = true;
+	}
+
+	if (status || !skip_video_pattern)
+		status = perform_link_training_int(core_link,
+				&lt_settings, status);
+
+	/* 6. print status message*/
+	switch (lt_settings.link_settings.link_rate) {
+
+	case LINK_RATE_LOW:
+		link_rate = "RBR";
+		break;
+	case LINK_RATE_HIGH:
+		link_rate = "HBR";
+		break;
+	case LINK_RATE_HIGH2:
+		link_rate = "HBR2";
+		break;
+	case LINK_RATE_RBR2:
+		link_rate = "RBR2";
+		break;
+	case LINK_RATE_HIGH3:
+		link_rate = "HBR3";
+		break;
+	default:
+		break;
+	}
+
+	/* Connectivity log: link training */
+	CONN_MSG_LT(core_link, "%sx%d %s VS=%d, PE=%d",
+			link_rate,
+			lt_settings.link_settings.lane_count,
+			status ? "pass" : "fail",
+			lt_settings.lane_settings[0].VOLTAGE_SWING,
+			lt_settings.lane_settings[0].PRE_EMPHASIS);
+
+	return status;
+}
+
+
+bool perform_link_training_with_retries(
+	struct core_link *link,
+	const struct dc_link_settings *link_setting,
+	bool skip_video_pattern,
+	int attempts)
+{
+	uint8_t j;
+	uint8_t delay_between_attempts = LINK_TRAINING_RETRY_DELAY;
+
+	for (j = 0; j < attempts; ++j) {
+
+		if (dc_link_dp_perform_link_training(
+				&link->public,
+				link_setting,
+				skip_video_pattern))
+			return true;
+
+		msleep(delay_between_attempts);
+		delay_between_attempts += LINK_TRAINING_RETRY_DELAY;
+	}
+
+	return false;
+}
+
+/*TODO add more check to see if link support request link configuration */
+static bool is_link_setting_supported(
+	const struct dc_link_settings *link_setting,
+	const struct dc_link_settings *max_link_setting)
+{
+	if (link_setting->lane_count > max_link_setting->lane_count ||
+		link_setting->link_rate > max_link_setting->link_rate)
+		return false;
+	return true;
+}
+
+static const uint32_t get_link_training_fallback_table_len(
+	struct core_link *link)
+{
+	return ARRAY_SIZE(link_training_fallback_table);
+}
+
+static const struct dc_link_settings *get_link_training_fallback_table(
+	struct core_link *link, uint32_t i)
+{
+	return &link_training_fallback_table[i];
+}
+
+static bool exceeded_limit_link_setting(
+	const struct dc_link_settings *link_setting,
+	const struct dc_link_settings *limit_link_setting)
+{
+	return (link_setting->lane_count * link_setting->link_rate
+		 > limit_link_setting->lane_count * limit_link_setting->link_rate ?
+				 true : false);
+}
+
+static struct dc_link_settings get_max_link_cap(struct core_link *link)
+{
+	/* Set Default link settings */
+	struct dc_link_settings max_link_cap = {LANE_COUNT_FOUR, LINK_RATE_HIGH,
+			LINK_SPREAD_05_DOWNSPREAD_30KHZ};
+
+	/* Higher link settings based on feature supported */
+	if (link->link_enc->features.flags.bits.IS_HBR2_CAPABLE)
+		max_link_cap.link_rate = LINK_RATE_HIGH2;
+
+	if (link->link_enc->features.flags.bits.IS_HBR3_CAPABLE)
+		max_link_cap.link_rate = LINK_RATE_HIGH3;
+
+	/* Lower link settings based on sink's link cap */
+	if (link->public.reported_link_cap.lane_count < max_link_cap.lane_count)
+		max_link_cap.lane_count =
+				link->public.reported_link_cap.lane_count;
+	if (link->public.reported_link_cap.link_rate < max_link_cap.link_rate)
+		max_link_cap.link_rate =
+				link->public.reported_link_cap.link_rate;
+	if (link->public.reported_link_cap.link_spread <
+			max_link_cap.link_spread)
+		max_link_cap.link_spread =
+				link->public.reported_link_cap.link_spread;
+	return max_link_cap;
+}
+
+bool dp_hbr_verify_link_cap(
+	struct core_link *link,
+	struct dc_link_settings *known_limit_link_setting)
+{
+	struct dc_link_settings max_link_cap = {0};
+	bool success;
+	bool skip_link_training;
+	const struct dc_link_settings *cur;
+	bool skip_video_pattern;
+	uint32_t i;
+	struct clock_source *dp_cs;
+	enum clock_source_id dp_cs_id = CLOCK_SOURCE_ID_EXTERNAL;
+
+	success = false;
+	skip_link_training = false;
+
+	max_link_cap = get_max_link_cap(link);
+
+	/* TODO implement override and monitor patch later */
+
+	/* try to train the link from high to low to
+	 * find the physical link capability
+	 */
+	/* disable PHY done possible by BIOS, will be done by driver itself */
+	dp_disable_link_phy(link, link->public.connector_signal);
+
+	dp_cs = link->dc->res_pool->dp_clock_source;
+
+	if (dp_cs)
+		dp_cs_id = dp_cs->id;
+	else {
+		/*
+		 * dp clock source is not initialized for some reason.
+		 * Should not happen, CLOCK_SOURCE_ID_EXTERNAL will be used
+		 */
+		ASSERT(dp_cs);
+	}
+
+	for (i = 0; i < get_link_training_fallback_table_len(link) &&
+		!success; i++) {
+		cur = get_link_training_fallback_table(link, i);
+
+		if (known_limit_link_setting->lane_count != LANE_COUNT_UNKNOWN &&
+			exceeded_limit_link_setting(cur,
+					known_limit_link_setting))
+			continue;
+
+		if (!is_link_setting_supported(cur, &max_link_cap))
+			continue;
+
+		skip_video_pattern = true;
+		if (cur->link_rate == LINK_RATE_LOW)
+			skip_video_pattern = false;
+
+		dp_enable_link_phy(
+				link,
+				link->public.connector_signal,
+				dp_cs_id,
+				cur);
+
+		if (skip_link_training)
+			success = true;
+		else {
+			success = dc_link_dp_perform_link_training(
+							&link->public,
+							cur,
+							skip_video_pattern);
+		}
+
+		if (success)
+			link->public.verified_link_cap = *cur;
+
+		/* always disable the link before trying another
+		 * setting or before returning we'll enable it later
+		 * based on the actual mode we're driving
+		 */
+		dp_disable_link_phy(link, link->public.connector_signal);
+	}
+
+	/* Link Training failed for all Link Settings
+	 *  (Lane Count is still unknown)
+	 */
+	if (!success) {
+		/* If all LT fails for all settings,
+		 * set verified = failed safe (1 lane low)
+		 */
+		link->public.verified_link_cap.lane_count = LANE_COUNT_ONE;
+		link->public.verified_link_cap.link_rate = LINK_RATE_LOW;
+
+		link->public.verified_link_cap.link_spread =
+		LINK_SPREAD_DISABLED;
+	}
+
+	link->public.max_link_setting = link->public.verified_link_cap;
+
+	return success;
+}
+
+static uint32_t bandwidth_in_kbps_from_timing(
+	const struct dc_crtc_timing *timing)
+{
+	uint32_t bits_per_channel = 0;
+	uint32_t kbps;
+	switch (timing->display_color_depth) {
+
+	case COLOR_DEPTH_666:
+		bits_per_channel = 6;
+		break;
+	case COLOR_DEPTH_888:
+		bits_per_channel = 8;
+		break;
+	case COLOR_DEPTH_101010:
+		bits_per_channel = 10;
+		break;
+	case COLOR_DEPTH_121212:
+		bits_per_channel = 12;
+		break;
+	case COLOR_DEPTH_141414:
+		bits_per_channel = 14;
+		break;
+	case COLOR_DEPTH_161616:
+		bits_per_channel = 16;
+		break;
+	default:
+		break;
+	}
+	ASSERT(bits_per_channel != 0);
+
+	kbps = timing->pix_clk_khz;
+	kbps *= bits_per_channel;
+
+	if (timing->flags.Y_ONLY != 1)
+		/*Only YOnly make reduce bandwidth by 1/3 compares to RGB*/
+		kbps *= 3;
+
+	return kbps;
+
+}
+
+static uint32_t bandwidth_in_kbps_from_link_settings(
+	const struct dc_link_settings *link_setting)
+{
+	uint32_t link_rate_in_kbps = link_setting->link_rate *
+		LINK_RATE_REF_FREQ_IN_KHZ;
+
+	uint32_t lane_count  = link_setting->lane_count;
+	uint32_t kbps = link_rate_in_kbps;
+	kbps *= lane_count;
+	kbps *= 8;   /* 8 bits per byte*/
+
+	return kbps;
+
+}
+
+bool dp_validate_mode_timing(
+	struct core_link *link,
+	const struct dc_crtc_timing *timing)
+{
+	uint32_t req_bw;
+	uint32_t max_bw;
+
+	const struct dc_link_settings *link_setting;
+
+	/*always DP fail safe mode*/
+	if (timing->pix_clk_khz == (uint32_t)25175 &&
+		timing->h_addressable == (uint32_t)640 &&
+		timing->v_addressable == (uint32_t)480)
+		return true;
+
+	/* We always use verified link settings */
+	link_setting = &link->public.verified_link_cap;
+
+	/* TODO: DYNAMIC_VALIDATION needs to be implemented */
+	/*if (flags.DYNAMIC_VALIDATION == 1 &&
+		link->public.verified_link_cap.lane_count != LANE_COUNT_UNKNOWN)
+		link_setting = &link->public.verified_link_cap;
+	*/
+
+	req_bw = bandwidth_in_kbps_from_timing(timing);
+	max_bw = bandwidth_in_kbps_from_link_settings(link_setting);
+
+	if (req_bw <= max_bw) {
+		/* remember the biggest mode here, during
+		 * initial link training (to get
+		 * verified_link_cap), LS sends event about
+		 * cannot train at reported cap to upper
+		 * layer and upper layer will re-enumerate modes.
+		 * this is not necessary if the lower
+		 * verified_link_cap is enough to drive
+		 * all the modes */
+
+		/* TODO: DYNAMIC_VALIDATION needs to be implemented */
+		/* if (flags.DYNAMIC_VALIDATION == 1)
+			dpsst->max_req_bw_for_verified_linkcap = dal_max(
+				dpsst->max_req_bw_for_verified_linkcap, req_bw); */
+		return true;
+	} else
+		return false;
+}
+
+void decide_link_settings(struct core_stream *stream,
+	struct dc_link_settings *link_setting)
+{
+
+	const struct dc_link_settings *cur_ls;
+	struct core_link* link;
+	uint32_t req_bw;
+	uint32_t link_bw;
+	uint32_t i;
+
+	req_bw = bandwidth_in_kbps_from_timing(
+			&stream->public.timing);
+
+	/* if preferred is specified through AMDDP, use it, if it's enough
+	 * to drive the mode
+	 */
+	link = stream->sink->link;
+
+	if ((link->public.reported_link_cap.lane_count != LANE_COUNT_UNKNOWN) &&
+		(link->public.reported_link_cap.link_rate <=
+				link->public.verified_link_cap.link_rate)) {
+
+		link_bw = bandwidth_in_kbps_from_link_settings(
+				&link->public.reported_link_cap);
+
+		if (req_bw < link_bw) {
+			*link_setting = link->public.reported_link_cap;
+			return;
+		}
+	}
+
+	/* search for first suitable setting for the requested
+	 * bandwidth
+	 */
+	for (i = 0; i < get_link_training_fallback_table_len(link); i++) {
+
+		cur_ls = get_link_training_fallback_table(link, i);
+
+		link_bw =
+				bandwidth_in_kbps_from_link_settings(
+				cur_ls);
+
+		if (req_bw < link_bw) {
+			if (is_link_setting_supported(
+				cur_ls,
+				&link->public.max_link_setting)) {
+				*link_setting = *cur_ls;
+				return;
+			}
+		}
+	}
+
+	BREAK_TO_DEBUGGER();
+	ASSERT(link->public.verified_link_cap.lane_count !=
+		LANE_COUNT_UNKNOWN);
+
+	*link_setting = link->public.verified_link_cap;
+}
+
+/*************************Short Pulse IRQ***************************/
+
+static bool hpd_rx_irq_check_link_loss_status(
+	struct core_link *link,
+	union hpd_irq_data *hpd_irq_dpcd_data)
+{
+	uint8_t irq_reg_rx_power_state;
+	enum dc_status dpcd_result = DC_ERROR_UNEXPECTED;
+	union lane_status lane_status;
+	uint32_t lane;
+	bool sink_status_changed;
+	bool return_code;
+
+	sink_status_changed = false;
+	return_code = false;
+
+	if (link->public.cur_link_settings.lane_count == 0)
+		return return_code;
+	/*1. Check that we can handle interrupt: Not in FS DOS,
+	 *  Not in "Display Timeout" state, Link is trained.
+	 */
+
+	dpcd_result = core_link_read_dpcd(link,
+		DPCD_ADDRESS_POWER_STATE,
+		&irq_reg_rx_power_state,
+		sizeof(irq_reg_rx_power_state));
+
+	if (dpcd_result != DC_OK) {
+		irq_reg_rx_power_state = DP_PWR_STATE_D0;
+		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+			"%s: DPCD read failed to obtain power state.\n",
+			__func__);
+	}
+
+	if (irq_reg_rx_power_state == DP_PWR_STATE_D0) {
+
+		/*2. Check that Link Status changed, before re-training.*/
+
+		/*parse lane status*/
+		for (lane = 0;
+			lane < link->public.cur_link_settings.lane_count;
+			lane++) {
+
+			/* check status of lanes 0,1
+			 * changed DpcdAddress_Lane01Status (0x202)*/
+			lane_status.raw = get_nibble_at_index(
+				&hpd_irq_dpcd_data->bytes.lane01_status.raw,
+				lane);
+
+			if (!lane_status.bits.CHANNEL_EQ_DONE_0 ||
+				!lane_status.bits.CR_DONE_0 ||
+				!lane_status.bits.SYMBOL_LOCKED_0) {
+				/* if one of the channel equalization, clock
+				 * recovery or symbol lock is dropped
+				 * consider it as (link has been
+				 * dropped) dp sink status has changed*/
+				sink_status_changed = true;
+				break;
+			}
+
+		}
+
+		/* Check interlane align.*/
+		if (sink_status_changed ||
+			!hpd_irq_dpcd_data->bytes.lane_status_updated.bits.
+			INTERLANE_ALIGN_DONE) {
+
+			dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+				"%s: Link Status changed.\n",
+				__func__);
+
+			return_code = true;
+		}
+	}
+
+	return return_code;
+}
+
+static enum dc_status read_hpd_rx_irq_data(
+	struct core_link *link,
+	union hpd_irq_data *irq_data)
+{
+	/* The HW reads 16 bytes from 200h on HPD,
+	 * but if we get an AUX_DEFER, the HW cannot retry
+	 * and this causes the CTS tests 4.3.2.1 - 3.2.4 to
+	 * fail, so we now explicitly read 6 bytes which is
+	 * the req from the above mentioned test cases.
+	 */
+	return core_link_read_dpcd(
+	link,
+	DPCD_ADDRESS_SINK_COUNT,
+	irq_data->raw,
+	sizeof(union hpd_irq_data));
+}
+
+static bool allow_hpd_rx_irq(const struct core_link *link)
+{
+	/*
+	 * Don't handle RX IRQ unless one of following is met:
+	 * 1) The link is established (cur_link_settings != unknown)
+	 * 2) We kicked off MST detection
+	 * 3) We know we're dealing with an active dongle
+	 */
+
+	if ((link->public.cur_link_settings.lane_count != LANE_COUNT_UNKNOWN) ||
+		(link->public.type == dc_connection_mst_branch) ||
+		is_dp_active_dongle(link))
+		return true;
+
+	return false;
+}
+
+static bool handle_hpd_irq_psr_sink(const struct core_link *link)
+{
+	union dpcd_psr_configuration psr_configuration;
+
+	if (link->public.psr_caps.psr_version == 0)
+		return false;
+
+	dal_ddc_service_read_dpcd_data(
+					link->ddc,
+					368 /*DpcdAddress_PSR_Enable_Cfg*/,
+					&psr_configuration.raw,
+					sizeof(psr_configuration.raw));
+
+	if (psr_configuration.bits.ENABLE) {
+		unsigned char dpcdbuf[3] = {0};
+		union psr_error_status psr_error_status;
+		union psr_sink_psr_status psr_sink_psr_status;
+
+		dal_ddc_service_read_dpcd_data(
+					link->ddc,
+					0x2006 /*DpcdAddress_PSR_Error_Status*/,
+					(unsigned char *) dpcdbuf,
+					sizeof(dpcdbuf));
+
+		/*DPCD 2006h   ERROR STATUS*/
+		psr_error_status.raw = dpcdbuf[0];
+		/*DPCD 2008h   SINK PANEL SELF REFRESH STATUS*/
+		psr_sink_psr_status.raw = dpcdbuf[2];
+
+		if (psr_error_status.bits.LINK_CRC_ERROR ||
+				psr_error_status.bits.RFB_STORAGE_ERROR) {
+			/* Acknowledge and clear error bits */
+			dal_ddc_service_write_dpcd_data(
+				link->ddc,
+				8198 /*DpcdAddress_PSR_Error_Status*/,
+				&psr_error_status.raw,
+				sizeof(psr_error_status.raw));
+
+			/* PSR error, disable and re-enable PSR */
+			dc_link_set_psr_enable(&link->public, false);
+			dc_link_set_psr_enable(&link->public, true);
+
+			return true;
+		} else if (psr_sink_psr_status.bits.SINK_SELF_REFRESH_STATUS ==
+				PSR_SINK_STATE_ACTIVE_DISPLAY_FROM_SINK_RFB){
+			/* No error is detect, PSR is active.
+			 * We should return with IRQ_HPD handled without
+			 * checking for loss of sync since PSR would have
+			 * powered down main link.
+			 */
+			return true;
+		}
+	}
+	return false;
+}
+
+static void dp_test_send_link_training(struct core_link *link)
+{
+	struct dc_link_settings link_settings;
+
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_LANE_COUNT,
+			(unsigned char *)(&link_settings.lane_count),
+			1);
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_LINK_RATE,
+			(unsigned char *)(&link_settings.link_rate),
+			1);
+
+	/* Set preferred link settings */
+	link->public.verified_link_cap.lane_count = link_settings.lane_count;
+	link->public.verified_link_cap.link_rate = link_settings.link_rate;
+
+	dp_retrain_link(link);
+}
+
+static void dp_test_send_phy_test_pattern(struct core_link *link)
+{
+	union phy_test_pattern dpcd_test_pattern;
+	union lane_adjust dpcd_lane_adjustment[2];
+	unsigned char dpcd_post_cursor_2_adjustment = 0;
+	unsigned char test_80_bit_pattern[
+			(DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_79_72 -
+			DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0)+1] = {0};
+	enum dp_test_pattern test_pattern;
+	struct dc_link_training_settings link_settings;
+	union lane_adjust dpcd_lane_adjust;
+	unsigned int lane;
+	struct link_training_settings link_training_settings;
+	int i = 0;
+
+	dpcd_test_pattern.raw = 0;
+	memset(dpcd_lane_adjustment, 0, sizeof(dpcd_lane_adjustment));
+	memset(&link_settings, 0, sizeof(link_settings));
+
+	/* get phy test pattern and pattern parameters from DP receiver */
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_PHY_PATTERN,
+			&dpcd_test_pattern.raw,
+			sizeof(dpcd_test_pattern));
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_ADJUST_REQUEST_LANE0_1,
+			&dpcd_lane_adjustment[0].raw,
+			sizeof(dpcd_lane_adjustment));
+
+	/*get post cursor 2 parameters
+	 * For DP 1.1a or eariler, this DPCD register's value is 0
+	 * For DP 1.2 or later:
+	 * Bits 1:0 = POST_CURSOR2_LANE0; Bits 3:2 = POST_CURSOR2_LANE1
+	 * Bits 5:4 = POST_CURSOR2_LANE2; Bits 7:6 = POST_CURSOR2_LANE3
+	 */
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_ADJUST_REQUEST_POST_CURSOR2,
+			&dpcd_post_cursor_2_adjustment,
+			sizeof(dpcd_post_cursor_2_adjustment));
+
+	/* translate request */
+	switch (dpcd_test_pattern.bits.PATTERN) {
+	case PHY_TEST_PATTERN_D10_2:
+		test_pattern = DP_TEST_PATTERN_D102;
+	break;
+	case PHY_TEST_PATTERN_SYMBOL_ERROR:
+		test_pattern = DP_TEST_PATTERN_SYMBOL_ERROR;
+	break;
+	case PHY_TEST_PATTERN_PRBS7:
+		test_pattern = DP_TEST_PATTERN_PRBS7;
+	break;
+	case PHY_TEST_PATTERN_80BIT_CUSTOM:
+		test_pattern = DP_TEST_PATTERN_80BIT_CUSTOM;
+	break;
+	case PHY_TEST_PATTERN_HBR2_COMPLIANCE_EYE:
+		test_pattern = DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
+	break;
+	default:
+		test_pattern = DP_TEST_PATTERN_VIDEO_MODE;
+	break;
+	}
+
+	if (test_pattern == DP_TEST_PATTERN_80BIT_CUSTOM)
+		core_link_read_dpcd(
+				link,
+				DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0,
+				test_80_bit_pattern,
+				sizeof(test_80_bit_pattern));
+
+	/* prepare link training settings */
+	link_settings.link = link->public.cur_link_settings;
+
+	for (lane = 0; lane <
+		(unsigned int)(link->public.cur_link_settings.lane_count);
+		lane++) {
+		dpcd_lane_adjust.raw =
+			get_nibble_at_index(&dpcd_lane_adjustment[0].raw, lane);
+		link_settings.lane_settings[lane].VOLTAGE_SWING =
+			(enum dc_voltage_swing)
+			(dpcd_lane_adjust.bits.VOLTAGE_SWING_LANE);
+		link_settings.lane_settings[lane].PRE_EMPHASIS =
+			(enum dc_pre_emphasis)
+			(dpcd_lane_adjust.bits.PRE_EMPHASIS_LANE);
+		link_settings.lane_settings[lane].POST_CURSOR2 =
+			(enum dc_post_cursor2)
+			((dpcd_post_cursor_2_adjustment >> (lane * 2)) & 0x03);
+	}
+
+	for (i = 0; i < 4; i++)
+		link_training_settings.lane_settings[i] =
+				link_settings.lane_settings[i];
+	link_training_settings.link_settings = link_settings.link;
+	link_training_settings.allow_invalid_msa_timing_param = false;
+	/*Usage: Measure DP physical lane signal
+	 * by DP SI test equipment automatically.
+	 * PHY test pattern request is generated by equipment via HPD interrupt.
+	 * HPD needs to be active all the time. HPD should be active
+	 * all the time. Do not touch it.
+	 * forward request to DS
+	 */
+	dc_link_dp_set_test_pattern(
+		&link->public,
+		test_pattern,
+		&link_training_settings,
+		test_80_bit_pattern,
+		(DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_79_72 -
+		DPCD_ADDRESS_TEST_80BIT_CUSTOM_PATTERN_7_0)+1);
+}
+
+static void dp_test_send_link_test_pattern(struct core_link *link)
+{
+	union link_test_pattern dpcd_test_pattern;
+	union test_misc dpcd_test_params;
+	enum dp_test_pattern test_pattern;
+
+	memset(&dpcd_test_pattern, 0, sizeof(dpcd_test_pattern));
+	memset(&dpcd_test_params, 0, sizeof(dpcd_test_params));
+
+	/* get link test pattern and pattern parameters */
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_PATTERN,
+			&dpcd_test_pattern.raw,
+			sizeof(dpcd_test_pattern));
+	core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_MISC1,
+			&dpcd_test_params.raw,
+			sizeof(dpcd_test_params));
+
+	switch (dpcd_test_pattern.bits.PATTERN) {
+	case LINK_TEST_PATTERN_COLOR_RAMP:
+		test_pattern = DP_TEST_PATTERN_COLOR_RAMP;
+	break;
+	case LINK_TEST_PATTERN_VERTICAL_BARS:
+		test_pattern = DP_TEST_PATTERN_VERTICAL_BARS;
+	break; /* black and white */
+	case LINK_TEST_PATTERN_COLOR_SQUARES:
+		test_pattern = (dpcd_test_params.bits.DYN_RANGE ==
+				TEST_DYN_RANGE_VESA ?
+				DP_TEST_PATTERN_COLOR_SQUARES :
+				DP_TEST_PATTERN_COLOR_SQUARES_CEA);
+	break;
+	default:
+		test_pattern = DP_TEST_PATTERN_VIDEO_MODE;
+	break;
+	}
+
+	dc_link_dp_set_test_pattern(
+			&link->public,
+			test_pattern,
+			NULL,
+			NULL,
+			0);
+}
+
+static void handle_automated_test(struct core_link *link)
+{
+	union test_request test_request;
+	union test_response test_response;
+
+	memset(&test_request, 0, sizeof(test_request));
+	memset(&test_response, 0, sizeof(test_response));
+
+	core_link_read_dpcd(
+		link,
+		DPCD_ADDRESS_TEST_REQUEST,
+		&test_request.raw,
+		sizeof(union test_request));
+	if (test_request.bits.LINK_TRAINING) {
+		/* ACK first to let DP RX test box monitor LT sequence */
+		test_response.bits.ACK = 1;
+		core_link_write_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_RESPONSE,
+			&test_response.raw,
+			sizeof(test_response));
+		dp_test_send_link_training(link);
+		/* no acknowledge request is needed again */
+		test_response.bits.ACK = 0;
+	}
+	if (test_request.bits.LINK_TEST_PATTRN) {
+		dp_test_send_link_test_pattern(link);
+		link->public.compliance_test_state.bits.
+			SET_TEST_PATTERN_PENDING = 1;
+	}
+	if (test_request.bits.PHY_TEST_PATTERN) {
+		dp_test_send_phy_test_pattern(link);
+		test_response.bits.ACK = 1;
+	}
+	if (!test_request.raw)
+		/* no requests, revert all test signals
+		 * TODO: revert all test signals
+		 */
+		test_response.bits.ACK = 1;
+	/* send request acknowledgment */
+	if (test_response.bits.ACK)
+		core_link_write_dpcd(
+			link,
+			DPCD_ADDRESS_TEST_RESPONSE,
+			&test_response.raw,
+			sizeof(test_response));
+}
+
+bool dc_link_handle_hpd_rx_irq(const struct dc_link *dc_link)
+{
+	struct core_link *link = DC_LINK_TO_LINK(dc_link);
+	union hpd_irq_data hpd_irq_dpcd_data = {{{{0}}}};
+	union device_service_irq device_service_clear = {0};
+	enum dc_status result = DDC_RESULT_UNKNOWN;
+	bool status = false;
+	/* For use cases related to down stream connection status change,
+	 * PSR and device auto test, refer to function handle_sst_hpd_irq
+	 * in DAL2.1*/
+
+	dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+		"%s: Got short pulse HPD on link %d\n",
+		__func__, link->public.link_index);
+
+	 /* All the "handle_hpd_irq_xxx()" methods
+		 * should be called only after
+		 * dal_dpsst_ls_read_hpd_irq_data
+		 * Order of calls is important too
+		 */
+	result = read_hpd_rx_irq_data(link, &hpd_irq_dpcd_data);
+
+	if (result != DC_OK) {
+		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+			"%s: DPCD read failed to obtain irq data\n",
+			__func__);
+		return false;
+	}
+
+	if (hpd_irq_dpcd_data.bytes.device_service_irq.bits.AUTOMATED_TEST) {
+		device_service_clear.bits.AUTOMATED_TEST = 1;
+		core_link_write_dpcd(
+			link,
+			DPCD_ADDRESS_DEVICE_SERVICE_IRQ_VECTOR,
+			&device_service_clear.raw,
+			sizeof(device_service_clear.raw));
+		device_service_clear.raw = 0;
+		handle_automated_test(link);
+		return false;
+	}
+
+	if (!allow_hpd_rx_irq(link)) {
+		dm_logger_write(link->ctx->logger, LOG_HW_HPD_IRQ,
+			"%s: skipping HPD handling on %d\n",
+			__func__, link->public.link_index);
+		return false;
+	}
+
+	if (handle_hpd_irq_psr_sink(link))
+		/* PSR-related error was detected and handled */
+		return true;
+
+	/* If PSR-related error handled, Main link may be off,
+	 * so do not handle as a normal sink status change interrupt.
+	 */
+
+	/* check if we have MST msg and return since we poll for it */
+	if (hpd_irq_dpcd_data.bytes.device_service_irq.
+			bits.DOWN_REP_MSG_RDY ||
+		hpd_irq_dpcd_data.bytes.device_service_irq.
+			bits.UP_REQ_MSG_RDY)
+		return false;
+
+	/* For now we only handle 'Downstream port status' case.
+	 * If we got sink count changed it means
+	 * Downstream port status changed,
+	 * then DM should call DC to do the detection. */
+	if (hpd_rx_irq_check_link_loss_status(
+		link,
+		&hpd_irq_dpcd_data)) {
+		/* Connectivity log: link loss */
+		CONN_DATA_LINK_LOSS(link,
+					hpd_irq_dpcd_data.raw,
+					sizeof(hpd_irq_dpcd_data),
+					"Status: ");
+
+		perform_link_training_with_retries(link,
+			&link->public.cur_link_settings,
+			true, LINK_TRAINING_ATTEMPTS);
+
+		status = false;
+	}
+
+	if (link->public.type == dc_connection_active_dongle &&
+		hpd_irq_dpcd_data.bytes.sink_cnt.bits.SINK_COUNT
+			!= link->dpcd_sink_count)
+		status = true;
+
+	/* reasons for HPD RX:
+	 * 1. Link Loss - ie Re-train the Link
+	 * 2. MST sideband message
+	 * 3. Automated Test - ie. Internal Commit
+	 * 4. CP (copy protection) - (not interesting for DM???)
+	 * 5. DRR
+	 * 6. Downstream Port status changed
+	 * -ie. Detect - this the only one
+	 * which is interesting for DM because
+	 * it must call dc_link_detect.
+	 */
+	return status;
+}
+
+/*query dpcd for version and mst cap addresses*/
+bool is_mst_supported(struct core_link *link)
+{
+	bool mst          = false;
+	enum dc_status st = DC_OK;
+	union dpcd_rev rev;
+	union mstm_cap cap;
+
+	rev.raw  = 0;
+	cap.raw  = 0;
+
+	st = core_link_read_dpcd(link, DPCD_ADDRESS_DPCD_REV, &rev.raw,
+			sizeof(rev));
+
+	if (st == DC_OK && rev.raw >= DPCD_REV_12) {
+
+		st = core_link_read_dpcd(link, DPCD_ADDRESS_MSTM_CAP,
+				&cap.raw, sizeof(cap));
+		if (st == DC_OK && cap.bits.MST_CAP == 1)
+			mst = true;
+	}
+	return mst;
+
+}
+
+bool is_dp_active_dongle(const struct core_link *link)
+{
+	enum display_dongle_type dongle_type = link->dpcd_caps.dongle_type;
+
+	return (dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) ||
+			(dongle_type == DISPLAY_DONGLE_DP_DVI_CONVERTER) ||
+			(dongle_type == DISPLAY_DONGLE_DP_HDMI_CONVERTER);
+}
+
+static void get_active_converter_info(
+	uint8_t data, struct core_link *link)
+{
+	union dp_downstream_port_present ds_port = { .byte = data };
+
+	/* decode converter info*/
+	if (!ds_port.fields.PORT_PRESENT) {
+		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
+		ddc_service_set_dongle_type(link->ddc,
+				link->dpcd_caps.dongle_type);
+		return;
+	}
+
+	switch (ds_port.fields.PORT_TYPE) {
+	case DOWNSTREAM_VGA:
+		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_DP_VGA_CONVERTER;
+		break;
+	case DOWNSTREAM_DVI_HDMI:
+		/* At this point we don't know is it DVI or HDMI,
+		 * assume DVI.*/
+		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_DP_DVI_CONVERTER;
+		break;
+	default:
+		link->dpcd_caps.dongle_type = DISPLAY_DONGLE_NONE;
+		break;
+	}
+
+	if (link->dpcd_caps.dpcd_rev.raw >= DCS_DPCD_REV_11) {
+		uint8_t det_caps[4];
+		union dwnstream_port_caps_byte0 *port_caps =
+			(union dwnstream_port_caps_byte0 *)det_caps;
+		core_link_read_dpcd(link, DPCD_ADDRESS_DWN_STRM_PORT0_CAPS,
+				det_caps, sizeof(det_caps));
+
+		switch (port_caps->bits.DWN_STRM_PORTX_TYPE) {
+		case DOWN_STREAM_DETAILED_VGA:
+			link->dpcd_caps.dongle_type =
+				DISPLAY_DONGLE_DP_VGA_CONVERTER;
+			break;
+		case DOWN_STREAM_DETAILED_DVI:
+			link->dpcd_caps.dongle_type =
+				DISPLAY_DONGLE_DP_DVI_CONVERTER;
+			break;
+		case DOWN_STREAM_DETAILED_HDMI:
+			link->dpcd_caps.dongle_type =
+				DISPLAY_DONGLE_DP_HDMI_CONVERTER;
+
+			if (ds_port.fields.DETAILED_CAPS) {
+
+				union dwnstream_port_caps_byte3_hdmi
+					hdmi_caps = {.raw = det_caps[3] };
+
+				link->dpcd_caps.is_dp_hdmi_s3d_converter =
+					hdmi_caps.bits.FRAME_SEQ_TO_FRAME_PACK;
+			}
+			break;
+		}
+	}
+
+	ddc_service_set_dongle_type(link->ddc, link->dpcd_caps.dongle_type);
+
+	{
+		struct dp_device_vendor_id dp_id;
+
+		/* read IEEE branch device id */
+		core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_BRANCH_DEVICE_ID_START,
+			(uint8_t *)&dp_id,
+			sizeof(dp_id));
+
+		link->dpcd_caps.branch_dev_id =
+			(dp_id.ieee_oui[0] << 16) +
+			(dp_id.ieee_oui[1] << 8) +
+			dp_id.ieee_oui[2];
+
+		memmove(
+			link->dpcd_caps.branch_dev_name,
+			dp_id.ieee_device_id,
+			sizeof(dp_id.ieee_device_id));
+	}
+
+	{
+		struct dp_sink_hw_fw_revision dp_hw_fw_revision;
+
+		core_link_read_dpcd(
+			link,
+			DPCD_ADDRESS_BRANCH_REVISION_START,
+			(uint8_t *)&dp_hw_fw_revision,
+			sizeof(dp_hw_fw_revision));
+
+		link->dpcd_caps.branch_hw_revision =
+			dp_hw_fw_revision.ieee_hw_rev;
+	}
+}
+
+static void dp_wa_power_up_0010FA(struct core_link *link, uint8_t *dpcd_data,
+		int length)
+{
+	int retry = 0;
+	union dp_downstream_port_present ds_port = { 0 };
+
+	if (!link->dpcd_caps.dpcd_rev.raw) {
+		do {
+			dp_receiver_power_ctrl(link, true);
+			core_link_read_dpcd(link, DPCD_ADDRESS_DPCD_REV,
+							dpcd_data, length);
+			link->dpcd_caps.dpcd_rev.raw = dpcd_data[
+				DPCD_ADDRESS_DPCD_REV -
+				DPCD_ADDRESS_DPCD_REV];
+		} while (retry++ < 4 && !link->dpcd_caps.dpcd_rev.raw);
+	}
+
+	ds_port.byte = dpcd_data[DPCD_ADDRESS_DOWNSTREAM_PORT_PRESENT -
+				 DPCD_ADDRESS_DPCD_REV];
+
+	if (link->dpcd_caps.dongle_type == DISPLAY_DONGLE_DP_VGA_CONVERTER) {
+		switch (link->dpcd_caps.branch_dev_id) {
+		/* Some active dongles (DP-VGA, DP-DLDVI converters) power down
+		 * all internal circuits including AUX communication preventing
+		 * reading DPCD table and EDID (spec violation).
+		 * Encoder will skip DP RX power down on disable_output to
+		 * keep receiver powered all the time.*/
+		case DP_BRANCH_DEVICE_ID_1:
+		case DP_BRANCH_DEVICE_ID_4:
+			link->wa_flags.dp_keep_receiver_powered = true;
+			break;
+
+		/* TODO: May need work around for other dongles. */
+		default:
+			link->wa_flags.dp_keep_receiver_powered = false;
+			break;
+		}
+	} else
+		link->wa_flags.dp_keep_receiver_powered = false;
+}
+
+static void retrieve_psr_link_cap(struct core_link *link,
+		enum edp_revision edp_revision)
+{
+	if (edp_revision >= EDP_REVISION_13) {
+		core_link_read_dpcd(link,
+				DPCD_ADDRESS_PSR_SUPPORT_VER,
+				(uint8_t *)(&link->public.psr_caps),
+				sizeof(link->public.psr_caps));
+		if (link->public.psr_caps.psr_version != 0) {
+			unsigned char psr_capability = 0;
+
+			core_link_read_dpcd(link,
+					DPCD_ADDRESS_PSR_CAPABILITY,
+						&psr_capability,
+						sizeof(psr_capability));
+			/* Bit 0 determines whether fast link training is
+			 * required on PSR exit. If set to 0, link training
+			 * is required. If set to 1, sink must lock within
+			 * five Idle Patterns after Main Link is turned on.
+			 */
+			link->public.psr_caps.psr_exit_link_training_required
+						= !(psr_capability & 0x1);
+
+			psr_capability = (psr_capability >> 1) & 0x7;
+			link->public.psr_caps.psr_rfb_setup_time =
+					55 * (6 - psr_capability);
+		}
+	}
+}
+
+static void retrieve_link_cap(struct core_link *link)
+{
+	uint8_t dpcd_data[DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL - DPCD_ADDRESS_DPCD_REV + 1];
+
+	union down_stream_port_count down_strm_port_count;
+	union edp_configuration_cap edp_config_cap;
+	union dp_downstream_port_present ds_port = { 0 };
+
+	memset(dpcd_data, '\0', sizeof(dpcd_data));
+	memset(&down_strm_port_count,
+		'\0', sizeof(union down_stream_port_count));
+	memset(&edp_config_cap, '\0',
+		sizeof(union edp_configuration_cap));
+
+	core_link_read_dpcd(
+		link,
+		DPCD_ADDRESS_DPCD_REV,
+		dpcd_data,
+		sizeof(dpcd_data));
+
+	link->dpcd_caps.dpcd_rev.raw =
+		dpcd_data[DPCD_ADDRESS_DPCD_REV - DPCD_ADDRESS_DPCD_REV];
+
+	{
+		union training_aux_rd_interval aux_rd_interval;
+
+		aux_rd_interval.raw =
+			dpcd_data[DPCD_ADDRESS_TRAINING_AUX_RD_INTERVAL];
+
+		if (aux_rd_interval.bits.EXT_RECIEVER_CAP_FIELD_PRESENT == 1) {
+			core_link_read_dpcd(
+				link,
+				DPCD_ADDRESS_DP13_DPCD_REV,
+				dpcd_data,
+				sizeof(dpcd_data));
+		}
+	}
+
+	ds_port.byte = dpcd_data[DPCD_ADDRESS_DOWNSTREAM_PORT_PRESENT -
+				 DPCD_ADDRESS_DPCD_REV];
+
+	get_active_converter_info(ds_port.byte, link);
+
+	dp_wa_power_up_0010FA(link, dpcd_data, sizeof(dpcd_data));
+
+	link->dpcd_caps.allow_invalid_MSA_timing_param =
+		down_strm_port_count.bits.IGNORE_MSA_TIMING_PARAM;
+
+	link->dpcd_caps.max_ln_count.raw = dpcd_data[
+		DPCD_ADDRESS_MAX_LANE_COUNT - DPCD_ADDRESS_DPCD_REV];
+
+	link->dpcd_caps.max_down_spread.raw = dpcd_data[
+		DPCD_ADDRESS_MAX_DOWNSPREAD - DPCD_ADDRESS_DPCD_REV];
+
+	link->public.reported_link_cap.lane_count =
+		link->dpcd_caps.max_ln_count.bits.MAX_LANE_COUNT;
+	link->public.reported_link_cap.link_rate = dpcd_data[
+		DPCD_ADDRESS_MAX_LINK_RATE - DPCD_ADDRESS_DPCD_REV];
+	link->public.reported_link_cap.link_spread =
+		link->dpcd_caps.max_down_spread.bits.MAX_DOWN_SPREAD ?
+		LINK_SPREAD_05_DOWNSPREAD_30KHZ : LINK_SPREAD_DISABLED;
+
+	edp_config_cap.raw = dpcd_data[
+		DPCD_ADDRESS_EDP_CONFIG_CAP - DPCD_ADDRESS_DPCD_REV];
+	link->dpcd_caps.panel_mode_edp =
+		edp_config_cap.bits.ALT_SCRAMBLER_RESET;
+
+	link->edp_revision = DPCD_EDP_REVISION_EDP_UNKNOWN;
+
+	link->public.test_pattern_enabled = false;
+	link->public.compliance_test_state.raw = 0;
+
+	link->public.psr_caps.psr_exit_link_training_required = false;
+	link->public.psr_caps.psr_frame_capture_indication_req = false;
+	link->public.psr_caps.psr_rfb_setup_time = 0;
+	link->public.psr_caps.psr_sdp_transmit_line_num_deadline = 0;
+	link->public.psr_caps.psr_version = 0;
+
+	/* read sink count */
+	core_link_read_dpcd(link,
+			DPCD_ADDRESS_SINK_COUNT,
+			&link->dpcd_caps.sink_count.raw,
+			sizeof(link->dpcd_caps.sink_count.raw));
+
+	/* Display control registers starting at DPCD 700h are only valid and
+	 * enabled if this eDP config cap bit is set. */
+	if (edp_config_cap.bits.DPCD_DISPLAY_CONTROL_CAPABLE) {
+		/* Read the Panel's eDP revision at DPCD 700h. */
+		core_link_read_dpcd(link,
+			DPCD_ADDRESS_EDP_REV,
+			(uint8_t *)(&link->edp_revision),
+			sizeof(link->edp_revision));
+	}
+
+	/* Connectivity log: detection */
+	CONN_DATA_DETECT(link, dpcd_data, sizeof(dpcd_data), "Rx Caps: ");
+
+	/* TODO: Confirm if need retrieve_psr_link_cap */
+	retrieve_psr_link_cap(link, link->edp_revision);
+}
+
+void detect_dp_sink_caps(struct core_link *link)
+{
+	retrieve_link_cap(link);
+
+	/* dc init_hw has power encoder using default
+	 * signal for connector. For native DP, no
+	 * need to power up encoder again. If not native
+	 * DP, hw_init may need check signal or power up
+	 * encoder here.
+	 */
+
+	if (is_mst_supported(link)) {
+		link->public.verified_link_cap = link->public.reported_link_cap;
+	} else {
+		dp_hbr_verify_link_cap(link,
+			&link->public.reported_link_cap);
+	}
+	/* TODO save sink caps in link->sink */
+}
+
+void dc_link_dp_enable_hpd(const struct dc_link *link)
+{
+	struct core_link *core_link = DC_LINK_TO_CORE(link);
+	struct link_encoder *encoder = core_link->link_enc;
+
+	if (encoder != NULL && encoder->funcs->enable_hpd != NULL)
+		encoder->funcs->enable_hpd(encoder);
+}
+
+void dc_link_dp_disable_hpd(const struct dc_link *link)
+{
+	struct core_link *core_link = DC_LINK_TO_CORE(link);
+	struct link_encoder *encoder = core_link->link_enc;
+
+	if (encoder != NULL && encoder->funcs->enable_hpd != NULL)
+		encoder->funcs->disable_hpd(encoder);
+}
+
+static bool is_dp_phy_pattern(enum dp_test_pattern test_pattern)
+{
+	if (test_pattern == DP_TEST_PATTERN_D102 ||
+	test_pattern == DP_TEST_PATTERN_SYMBOL_ERROR ||
+	test_pattern == DP_TEST_PATTERN_PRBS7 ||
+	test_pattern == DP_TEST_PATTERN_80BIT_CUSTOM ||
+	test_pattern == DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE ||
+	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN1 ||
+	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN2 ||
+	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN3 ||
+	test_pattern == DP_TEST_PATTERN_TRAINING_PATTERN4 ||
+	test_pattern == DP_TEST_PATTERN_VIDEO_MODE)
+		return true;
+	else
+		return false;
+}
+
+static void set_crtc_test_pattern(struct core_link *link,
+				struct pipe_ctx *pipe_ctx,
+				enum dp_test_pattern test_pattern)
+{
+	enum controller_dp_test_pattern controller_test_pattern;
+	enum dc_color_depth color_depth = pipe_ctx->
+		stream->public.timing.display_color_depth;
+	struct bit_depth_reduction_params params;
+
+	memset(&params, 0, sizeof(params));
+
+	switch (test_pattern) {
+	case DP_TEST_PATTERN_COLOR_SQUARES:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_COLORSQUARES;
+	break;
+	case DP_TEST_PATTERN_COLOR_SQUARES_CEA:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_COLORSQUARES_CEA;
+	break;
+	case DP_TEST_PATTERN_VERTICAL_BARS:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_VERTICALBARS;
+	break;
+	case DP_TEST_PATTERN_HORIZONTAL_BARS:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_HORIZONTALBARS;
+	break;
+	case DP_TEST_PATTERN_COLOR_RAMP:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_COLORRAMP;
+	break;
+	default:
+		controller_test_pattern =
+				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE;
+	break;
+	}
+
+	switch (test_pattern) {
+	case DP_TEST_PATTERN_COLOR_SQUARES:
+	case DP_TEST_PATTERN_COLOR_SQUARES_CEA:
+	case DP_TEST_PATTERN_VERTICAL_BARS:
+	case DP_TEST_PATTERN_HORIZONTAL_BARS:
+	case DP_TEST_PATTERN_COLOR_RAMP:
+	{
+		/* disable bit depth reduction */
+		pipe_ctx->stream->bit_depth_params = params;
+		pipe_ctx->opp->funcs->
+			opp_program_bit_depth_reduction(pipe_ctx->opp, &params);
+
+		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
+				controller_test_pattern, color_depth);
+	}
+	break;
+	case DP_TEST_PATTERN_VIDEO_MODE:
+	{
+		/* restore bitdepth reduction */
+		link->dc->current_context->res_ctx.pool->funcs->
+			build_bit_depth_reduction_params(pipe_ctx->stream,
+					&params);
+		pipe_ctx->stream->bit_depth_params = params;
+		pipe_ctx->opp->funcs->
+			opp_program_bit_depth_reduction(pipe_ctx->opp, &params);
+
+		pipe_ctx->tg->funcs->set_test_pattern(pipe_ctx->tg,
+				CONTROLLER_DP_TEST_PATTERN_VIDEOMODE,
+				color_depth);
+	}
+	break;
+
+	default:
+	break;
+	}
+}
+
+bool dc_link_dp_set_test_pattern(
+	const struct dc_link *link,
+	enum dp_test_pattern test_pattern,
+	const struct link_training_settings *p_link_settings,
+	const unsigned char *p_custom_pattern,
+	unsigned int cust_pattern_size)
+{
+	struct core_link *core_link = DC_LINK_TO_CORE(link);
+	struct pipe_ctx *pipes =
+			core_link->dc->current_context->res_ctx.pipe_ctx;
+	struct pipe_ctx pipe_ctx = pipes[0];
+	unsigned int lane;
+	unsigned int i;
+	unsigned char link_qual_pattern[LANE_COUNT_DP_MAX] = {0};
+	union dpcd_training_pattern training_pattern;
+	union test_response test_response;
+	enum dpcd_phy_test_patterns pattern;
+
+	memset(&training_pattern, 0, sizeof(training_pattern));
+	memset(&test_response, 0, sizeof(test_response));
+
+	for (i = 0; i < MAX_PIPES; i++) {
+		if (pipes[i].stream->sink->link == core_link) {
+			pipe_ctx = pipes[i];
+			break;
+		}
+	}
+
+	/* Reset CRTC Test Pattern if it is currently running and request
+	 * is VideoMode Reset DP Phy Test Pattern if it is currently running
+	 * and request is VideoMode
+	 */
+	if (core_link->public.test_pattern_enabled && test_pattern ==
+			DP_TEST_PATTERN_VIDEO_MODE) {
+		/* Set CRTC Test Pattern */
+		set_crtc_test_pattern(core_link, &pipe_ctx, test_pattern);
+		dp_set_hw_test_pattern(core_link, test_pattern,
+				(uint8_t *)p_custom_pattern,
+				(uint32_t)cust_pattern_size);
+
+		/* Unblank Stream */
+		core_link->dc->hwss.unblank_stream(
+			&pipe_ctx,
+			&core_link->public.verified_link_cap);
+		/* TODO:m_pHwss->MuteAudioEndpoint
+		 * (pPathMode->pDisplayPath, false);
+		 */
+
+		/* Reset Test Pattern state */
+		core_link->public.test_pattern_enabled = false;
+
+		return true;
+	}
+
+	/* Check for PHY Test Patterns */
+	if (is_dp_phy_pattern(test_pattern)) {
+		/* Set DPCD Lane Settings before running test pattern */
+		if (p_link_settings != NULL) {
+			dp_set_hw_lane_settings(core_link, p_link_settings);
+			dpcd_set_lane_settings(core_link, p_link_settings);
+		}
+
+		/* Blank stream if running test pattern */
+		if (test_pattern != DP_TEST_PATTERN_VIDEO_MODE) {
+			/*TODO:
+			 * m_pHwss->
+			 * MuteAudioEndpoint(pPathMode->pDisplayPath, true);
+			 */
+			/* Blank stream */
+			pipes->stream_enc->funcs->dp_blank(pipe_ctx.stream_enc);
+		}
+
+		dp_set_hw_test_pattern(core_link, test_pattern,
+				(uint8_t *)p_custom_pattern,
+				(uint32_t)cust_pattern_size);
+
+		if (test_pattern != DP_TEST_PATTERN_VIDEO_MODE) {
+			/* Set Test Pattern state */
+			core_link->public.test_pattern_enabled = true;
+			if (p_link_settings != NULL)
+				dpcd_set_link_settings(core_link,
+						p_link_settings);
+		}
+
+		switch (test_pattern) {
+		case DP_TEST_PATTERN_VIDEO_MODE:
+			pattern = PHY_TEST_PATTERN_NONE;
+		break;
+		case DP_TEST_PATTERN_D102:
+			pattern = PHY_TEST_PATTERN_D10_2;
+		break;
+		case DP_TEST_PATTERN_SYMBOL_ERROR:
+			pattern = PHY_TEST_PATTERN_SYMBOL_ERROR;
+		break;
+		case DP_TEST_PATTERN_PRBS7:
+			pattern = PHY_TEST_PATTERN_PRBS7;
+		break;
+		case DP_TEST_PATTERN_80BIT_CUSTOM:
+			pattern = PHY_TEST_PATTERN_80BIT_CUSTOM;
+		break;
+		case DP_TEST_PATTERN_HBR2_COMPLIANCE_EYE:
+			pattern = PHY_TEST_PATTERN_HBR2_COMPLIANCE_EYE;
+		break;
+		default:
+			return false;
+		}
+
+		if (test_pattern == DP_TEST_PATTERN_VIDEO_MODE
+		/*TODO:&& !pPathMode->pDisplayPath->IsTargetPoweredOn()*/)
+			return false;
+
+		if (core_link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_12) {
+			/* tell receiver that we are sending qualification
+			 * pattern DP 1.2 or later - DP receiver's link quality
+			 * pattern is set using DPCD LINK_QUAL_LANEx_SET
+			 * register (0x10B~0x10E)\
+			 */
+			for (lane = 0; lane < LANE_COUNT_DP_MAX; lane++)
+				link_qual_pattern[lane] =
+						(unsigned char)(pattern);
+
+			core_link_write_dpcd(core_link,
+					DPCD_ADDRESS_LINK_QUAL_LANE0_SET,
+					link_qual_pattern,
+					sizeof(link_qual_pattern));
+		} else if (core_link->dpcd_caps.dpcd_rev.raw >= DPCD_REV_10 ||
+				core_link->dpcd_caps.dpcd_rev.raw == 0) {
+			/* tell receiver that we are sending qualification
+			 * pattern DP 1.1a or earlier - DP receiver's link
+			 * quality pattern is set using
+			 * DPCD TRAINING_PATTERN_SET -> LINK_QUAL_PATTERN_SET
+			 * register (0x102). We will use v_1.3 when we are
+			 * setting test pattern for DP 1.1.
+			 */
+			core_link_read_dpcd(core_link,
+					DPCD_ADDRESS_TRAINING_PATTERN_SET,
+					&training_pattern.raw,
+					sizeof(training_pattern));
+			training_pattern.v1_3.LINK_QUAL_PATTERN_SET = pattern;
+			core_link_write_dpcd(core_link,
+					DPCD_ADDRESS_TRAINING_PATTERN_SET,
+					&training_pattern.raw,
+					sizeof(training_pattern));
+		}
+	} else {
+	/* CRTC Patterns */
+		set_crtc_test_pattern(core_link, &pipe_ctx, test_pattern);
+		/* Set Test Pattern state */
+		core_link->public.test_pattern_enabled = true;
+
+		/* If this is called because of compliance test request,
+		 * we respond ack here.
+		 */
+		if (core_link->public.compliance_test_state.bits.
+				SET_TEST_PATTERN_PENDING == 1) {
+			core_link->public.compliance_test_state.bits.
+						SET_TEST_PATTERN_PENDING = 0;
+			test_response.bits.ACK = 1;
+			core_link_write_dpcd(core_link,
+					DPCD_ADDRESS_TEST_RESPONSE,
+					&test_response.raw,
+					sizeof(test_response));
+		}
+	}
+
+	return true;
+}
