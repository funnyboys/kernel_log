commit b6dbb8ff9dbfdf55ee88e668099d9d7517f109a5
Author: Nicholas Kazlauskas <nicholas.kazlauskas@amd.com>
Date:   Wed May 6 14:21:35 2020 -0400

    drm/amd/display: Avoid pipe split when plane is too small
    
    [Why]
    The minimum plane size we can support in DML is 16x16. If we try to pass
    a 16x16 plane with dynamic pipe split then validation will fail since it
    tries to split it into two pipes, each 8x8.
    
    Some userspace doesn't check that the commit fails and because the
    commit fails the old state is retained, resulting in corruption.
    
    [How]
    Add a workaround to avoid pipe split if any plane is 16x16 or smaller.
    
    Signed-off-by: Nicholas Kazlauskas <nicholas.kazlauskas@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 3960a8db94cb..1e5a92b192a1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -690,6 +690,26 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 		struct dc_debug_options *dbg,
 		struct dc_state *context)
 {
+	int i;
+
+	for (i = 0; i < MAX_PIPES; i++) {
+		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
+
+		/**
+		 * Workaround for avoiding pipe-split in cases where we'd split
+		 * planes that are too small, resulting in splits that aren't
+		 * valid for the scaler.
+		 */
+		if (pipe->plane_state &&
+		    (pipe->plane_state->dst_rect.width <= 16 ||
+		     pipe->plane_state->dst_rect.height <= 16 ||
+		     pipe->plane_state->src_rect.width <= 16 ||
+		     pipe->plane_state->src_rect.height <= 16)) {
+			hack_disable_optional_pipe_split(v);
+			return;
+		}
+	}
+
 	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID)
 		hack_disable_optional_pipe_split(v);
 
@@ -702,7 +722,6 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 		hack_force_pipe_split(v, context->streams[0]->timing.pix_clk_100hz);
 }
 
-
 unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev, uint32_t pci_revision_id)
 {
 	/* for low power RV2 variants, the highest voltage level we want is 0 */

commit 6cc47f3f96345fb21eaff8996d31939ffbda0f58
Author: Aly-Tawfik <altawfik@amd.com>
Date:   Tue Feb 25 15:01:28 2020 -0500

    drm/amdgpu/display: Fix Pollock Variant Detection
    
    Problem Description:
    Currently we are checking internal fused rev id with pci rev id. However, fused
    internal rev id is the same on all raven2 parts (in which Dali and Pollock were
    based on too), thus Pollock detection fails
    
    Fix:
    use the pci rev to preform the detection for bandwidth calculations.
    
    Reviewed-by: Feifei Xu <Feifei.Xu@amd.com>
    Signed-off-by: Aly-Tawfik <altawfik@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index f0f07b160152..3960a8db94cb 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -712,6 +712,11 @@ unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev, uint32_
 		case PRID_DALI_DF:
 		case PRID_DALI_E3:
 		case PRID_DALI_E4:
+		case PRID_POLLOCK_94:
+		case PRID_POLLOCK_95:
+		case PRID_POLLOCK_E9:
+		case PRID_POLLOCK_EA:
+		case PRID_POLLOCK_EB:
 			return 0;
 		default:
 			break;

commit 41ef3dcd86443fab3f0ba50902d13935bc12d24f
Author: Michael Strauss <michael.strauss@amd.com>
Date:   Thu Feb 13 15:08:13 2020 -0500

    drm/amd/display: Fix RV2 Variant Detection
    
    [WHY]
    RV2 and variants are indistinguishable by hw internal rev alone, need to
    be distinguishable in order to correctly set max vlevel.  Previous
    detection change incorrectly checked for hw internal rev.
    
    [HOW]
    Use pci revision to check if RV2 or low power variant Correct a few
    overlapping ASICREV range checks
    
    Signed-off-by: Michael Strauss <michael.strauss@amd.com>
    Reviewed-by: Michael Strauss <Michael.Strauss@amd.com>
    Acked-by: Rodrigo Siqueira <Rodrigo.Siqueira@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1a37550731de..f0f07b160152 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -703,11 +703,19 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 }
 
 
-unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev)
+unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev, uint32_t pci_revision_id)
 {
-	/* for dali & pollock, the highest voltage level we want is 0 */
-	if (ASICREV_IS_POLLOCK(hw_internal_rev) || ASICREV_IS_DALI(hw_internal_rev))
-		return 0;
+	/* for low power RV2 variants, the highest voltage level we want is 0 */
+	if (ASICREV_IS_RAVEN2(hw_internal_rev))
+		switch (pci_revision_id) {
+		case PRID_DALI_DE:
+		case PRID_DALI_DF:
+		case PRID_DALI_E3:
+		case PRID_DALI_E4:
+			return 0;
+		default:
+			break;
+		}
 
 	/* we are ok with all levels */
 	return 4;
@@ -1277,7 +1285,9 @@ bool dcn_validate_bandwidth(
 	PERFORMANCE_TRACE_END();
 	BW_VAL_TRACE_FINISH();
 
-	if (bw_limit_pass && v->voltage_level <= get_highest_allowed_voltage_level(dc->ctx->asic_id.hw_internal_rev))
+	if (bw_limit_pass && v->voltage_level <= get_highest_allowed_voltage_level(
+							dc->ctx->asic_id.hw_internal_rev,
+							dc->ctx->asic_id.pci_revision_id))
 		return true;
 	else
 		return false;

commit c37243579d6c881c575dcfb54cf31c9ded88f946
Author: Alex Deucher <alexander.deucher@amd.com>
Date:   Tue Jan 28 14:39:45 2020 -0500

    drm/amdgpu/display: handle multiple numbers of fclks in dcn_calcs.c (v2)
    
    We might get different numbers of clocks from powerplay depending
    on what the OEM has populated.
    
    v2: add assert for at least one level
    
    Bug: https://gitlab.freedesktop.org/drm/amd/issues/963
    Reviewed-by: Nicholas Kazlauskas <nicholas.kazlauskas@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a27d84ca15a5..1a37550731de 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1435,6 +1435,7 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	struct dc_context *ctx = dc->ctx;
 	struct dm_pp_clock_levels_with_voltage fclks = {0}, dcfclks = {0};
 	bool res;
+	unsigned vmin0p65_idx, vmid0p72_idx, vnom0p8_idx, vmax0p9_idx;
 
 	/* TODO: This is not the proper way to obtain fabric_and_dram_bandwidth, should be min(fclk, memclk) */
 	res = dm_pp_get_clock_levels_by_type_with_voltage(
@@ -1446,17 +1447,28 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 		res = verify_clock_values(&fclks);
 
 	if (res) {
-		ASSERT(fclks.num_levels >= 3);
-		dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (fclks.data[0].clocks_in_khz / 1000.0) / 1000.0;
-		dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
-				(fclks.data[fclks.num_levels - (fclks.num_levels > 2 ? 3 : 2)].clocks_in_khz / 1000.0)
-				* ddr4_dram_factor_single_Channel / 1000.0;
-		dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->number_of_channels *
-				(fclks.data[fclks.num_levels - 2].clocks_in_khz / 1000.0)
-				* ddr4_dram_factor_single_Channel / 1000.0;
-		dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->number_of_channels *
-				(fclks.data[fclks.num_levels - 1].clocks_in_khz / 1000.0)
-				* ddr4_dram_factor_single_Channel / 1000.0;
+		ASSERT(fclks.num_levels);
+
+		vmin0p65_idx = 0;
+		vmid0p72_idx = fclks.num_levels -
+			(fclks.num_levels > 2 ? 3 : (fclks.num_levels > 1 ? 2 : 1));
+		vnom0p8_idx = fclks.num_levels - (fclks.num_levels > 1 ? 2 : 1);
+		vmax0p9_idx = fclks.num_levels - 1;
+
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 =
+			32 * (fclks.data[vmin0p65_idx].clocks_in_khz / 1000.0) / 1000.0;
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 =
+			dc->dcn_soc->number_of_channels *
+			(fclks.data[vmid0p72_idx].clocks_in_khz / 1000.0)
+			* ddr4_dram_factor_single_Channel / 1000.0;
+		dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 =
+			dc->dcn_soc->number_of_channels *
+			(fclks.data[vnom0p8_idx].clocks_in_khz / 1000.0)
+			* ddr4_dram_factor_single_Channel / 1000.0;
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 =
+			dc->dcn_soc->number_of_channels *
+			(fclks.data[vmax0p9_idx].clocks_in_khz / 1000.0)
+			* ddr4_dram_factor_single_Channel / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
 

commit 61e50646f0bbfb24002c4935e1ed9bf04ae4266e
Author: Michael Strauss <michael.strauss@amd.com>
Date:   Mon Nov 4 13:39:20 2019 -0500

    drm/amd/display: add Pollock IDs, fix Pollock & Dali clk mgr construct
    
    [WHY]
    Only a single voltage level should be available to Pollock (min level)
    Pollock & Dali get misidentified as Renoir, use wrong clk mgr constructor
    
    [HOW]
    Add provided Pollock IDs to ASIC Rev. ID list.
    Create new Pollock ASIC RID check, fix RV2 & Dali ASIC checks.
    Check RID and set max voltage level to 0 if Pollock is detected.
    Work around broken ASICREV_IS_RENOIR, IS_RAVEN2, etc. checks by
    performing Dali/Pollock checks before they can be misidentified as RN.
    
    Signed-off-by: Michael Strauss <michael.strauss@amd.com>
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Huang Rui <ray.huang@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index e6c22345f0ea..a27d84ca15a5 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -705,8 +705,8 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 
 unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev)
 {
-	/* for dali, the highest voltage level we want is 0 */
-	if (ASICREV_IS_DALI(hw_internal_rev))
+	/* for dali & pollock, the highest voltage level we want is 0 */
+	if (ASICREV_IS_POLLOCK(hw_internal_rev) || ASICREV_IS_DALI(hw_internal_rev))
 		return 0;
 
 	/* we are ok with all levels */

commit 16a9dea110a67d62401ffeac4828cabdedec7548
Author: Timothy Pearson <tpearson@raptorengineering.com>
Date:   Sat Dec 7 16:47:46 2019 -0600

    amdgpu: Enable initial DCN support on POWER
    
    DCN requires floating point support to operate.  Add the appropriate
    x86/ppc64 guards and FPU / AltiVec / VSX context switches to DCN.
    
    Note that the current DC20 code doesn't contain all required FPU
    wrappers on x86 or POWER, so this patch is insufficient to fully
    enable DC20 on POWER.
    
    v2: s/X86_64/X86/g to retain previous behavior.
    
    Signed-off-by: Timothy Pearson <tpearson@raptorengineering.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 5dc8ffe70862..e6c22345f0ea 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1,5 +1,6 @@
 /*
  * Copyright 2017 Advanced Micro Devices, Inc.
+ * Copyright 2019 Raptor Engineering, LLC
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
  * copy of this software and associated documentation files (the "Software"),

commit 6ca3928da66ea08f87ad200c6e521e421ab5d59b
Author: Timothy Pearson <tpearson@raptorengineering.com>
Date:   Sat Dec 7 16:47:13 2019 -0600

    amdgpu: Prepare DCN floating point macros for generic arch support
    
    Introduce DC_FP_START()/DC_FP_END() macros to help enable floating
    point kernel mode support across various architectures.
    
    v2: move copyright update to commit which adds the changes
    
    Signed-off-by: Timothy Pearson <tpearson@raptorengineering.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a4ddd657598f..5dc8ffe70862 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -622,7 +622,7 @@ static bool dcn_bw_apply_registry_override(struct dc *dc)
 {
 	bool updated = false;
 
-	kernel_fpu_begin();
+	DC_FP_START();
 	if ((int)(dc->dcn_soc->sr_exit_time * 1000) != dc->debug.sr_exit_time_ns
 			&& dc->debug.sr_exit_time_ns) {
 		updated = true;
@@ -658,7 +658,7 @@ static bool dcn_bw_apply_registry_override(struct dc *dc)
 		dc->dcn_soc->dram_clock_change_latency =
 				dc->debug.dram_clock_change_latency_ns / 1000.0;
 	}
-	kernel_fpu_end();
+	DC_FP_END();
 
 	return updated;
 }
@@ -738,7 +738,7 @@ bool dcn_validate_bandwidth(
 		dcn_bw_sync_calcs_and_dml(dc);
 
 	memset(v, 0, sizeof(*v));
-	kernel_fpu_begin();
+	DC_FP_START();
 
 	v->sr_exit_time = dc->dcn_soc->sr_exit_time;
 	v->sr_enter_plus_exit_time = dc->dcn_soc->sr_enter_plus_exit_time;
@@ -1271,7 +1271,7 @@ bool dcn_validate_bandwidth(
 	bw_limit = dc->dcn_soc->percent_disp_bw_limit * v->fabric_and_dram_bandwidth_vmax0p9;
 	bw_limit_pass = (v->total_data_read_bandwidth / 1000.0) < bw_limit;
 
-	kernel_fpu_end();
+	DC_FP_END();
 
 	PERFORMANCE_TRACE_END();
 	BW_VAL_TRACE_FINISH();
@@ -1439,7 +1439,7 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	res = dm_pp_get_clock_levels_by_type_with_voltage(
 			ctx, DM_PP_CLOCK_TYPE_FCLK, &fclks);
 
-	kernel_fpu_begin();
+	DC_FP_START();
 
 	if (res)
 		res = verify_clock_values(&fclks);
@@ -1459,12 +1459,12 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	} else
 		BREAK_TO_DEBUGGER();
 
-	kernel_fpu_end();
+	DC_FP_END();
 
 	res = dm_pp_get_clock_levels_by_type_with_voltage(
 			ctx, DM_PP_CLOCK_TYPE_DCFCLK, &dcfclks);
 
-	kernel_fpu_begin();
+	DC_FP_START();
 
 	if (res)
 		res = verify_clock_values(&dcfclks);
@@ -1477,7 +1477,7 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	} else
 		BREAK_TO_DEBUGGER();
 
-	kernel_fpu_end();
+	DC_FP_END();
 }
 
 void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
@@ -1492,11 +1492,11 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	if (!pp || !pp->set_wm_ranges)
 		return;
 
-	kernel_fpu_begin();
+	DC_FP_START();
 	min_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
 	min_dcfclk_khz = dc->dcn_soc->dcfclkv_min0p65 * 1000;
 	socclk_khz = dc->dcn_soc->socclk * 1000;
-	kernel_fpu_end();
+	DC_FP_END();
 
 	/* Now notify PPLib/SMU about which Watermarks sets they should select
 	 * depending on DPM state they are in. And update BW MGR GFX Engine and
@@ -1547,7 +1547,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 
 void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 {
-	kernel_fpu_begin();
+	DC_FP_START();
 	DC_LOG_BANDWIDTH_CALCS("sr_exit_time: %f ns\n"
 			"sr_enter_plus_exit_time: %f ns\n"
 			"urgent_latency: %f ns\n"
@@ -1736,5 +1736,5 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 	dc->dml.ip.bug_forcing_LC_req_same_size_fixed =
 		dc->dcn_ip->bug_forcing_luma_and_chroma_request_to_same_size_fixed == dcn_bw_yes;
 	dc->dml.ip.dcfclk_cstate_latency = dc->dcn_ip->dcfclk_cstate_latency;
-	kernel_fpu_end();
+	DC_FP_END();
 }

commit 1da37801a8b0fffb024fea594c7f1d7867ed8aa0
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Wed Nov 6 14:38:55 2019 -0500

    drm/amd/display: Drop CONFIG_DRM_AMD_DC_DCN2_0 and DSC_SUPPORTED
    
    [Why]
    
    DCN2 and DSC are stable enough to be build by default. So drop the flags.
    
    [How]
    
    Remove them using the unifdef tool. The following commands were executed
    in sequence:
    
    $ find -name '*.c' -exec unifdef -m -DCONFIG_DRM_AMD_DC_DSC_SUPPORT -DCONFIG_DRM_AMD_DC_DCN2_0 -UCONFIG_TRIM_DRM_AMD_DC_DCN2_0 '{}' ';'
    $ find -name '*.h' -exec unifdef -m -DCONFIG_DRM_AMD_DC_DSC_SUPPORT -DCONFIG_DRM_AMD_DC_DCN2_0 -UCONFIG_TRIM_DRM_AMD_DC_DCN2_0 '{}' ';'
    
    In addition:
    
    * Remove from kconfig, and replace any dependencies with DCN1_0.
    * Remove from any makefiles.
    * Fix and cleanup NV defninitions in dal_asic_id.h
    * Expand DCN1 ifdef to include DCN2 code in the following files:
        * clk_mgr/clk_mgr.c: dc_clk_mgr_create()
        * core/dc_resources.c: dc_create_resource_pool()
        * dce/dce_dmcu.c: dcn20_*lock_phy()
        * dce/dce_dmcu.c: dcn20_funcs
        * dce/dce_dmcu.c: dcn20_dmcu_create()
        * gpio/hw_factory.c: dal_hw_factory_init()
        * gpio/hw_translate.c: dal_hw_translate_init()
    
    Signed-off-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 9b2cb57bf2ba..a4ddd657598f 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -53,13 +53,9 @@
  * remain as-is as it provides us with a guarantee from HW that it is correct.
  */
 
-#ifdef CONFIG_DRM_AMD_DC_DCN2_0
 /* Defaults from spreadsheet rev#247.
  * RV2 delta: dram_clock_change_latency, max_num_dpp
  */
-#else
-/* Defaults from spreadsheet rev#247 */
-#endif
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		/* latencies */
 		.sr_exit_time = 17, /*us*/

commit e42a34dec68950ebad7904f235ed2dfff5bb27b5
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Fri Sep 6 16:32:44 2019 -0400

    drm/amd/display: Implement voltage limitation for dali
    
    [Why]
    we only want the lowest voltage to be available for dali.
    
    [How]
    Use the get_highest_allowed_voltage_level function
    to return 0 for dali
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Huang Rui <ray.huang@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 383f4f8db8f4..9b2cb57bf2ba 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -708,6 +708,10 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 
 unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev)
 {
+	/* for dali, the highest voltage level we want is 0 */
+	if (ASICREV_IS_DALI(hw_internal_rev))
+		return 0;
+
 	/* we are ok with all levels */
 	return 4;
 }

commit 387ad34cb7ffa589464c3ff77e5adb49026c15b4
Author: Joseph Gravenor <joseph.gravenor@amd.com>
Date:   Tue Jul 30 16:37:35 2019 -0400

    drm/amd/display: Implement voltage limitation stub
    
    add new function to get the voltage at the end of
    dcn_validate_bandwidth, to check against the
    highest voltage we allow.
    
    Created a stub to allow for optimizations
    
    Signed-off-by: Joseph Gravenor <joseph.gravenor@amd.com>
    Reviewed-by: Eric Yang <eric.yang2@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Acked-by: Sun peng Li <Sunpeng.Li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 061c6e3a3088..383f4f8db8f4 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -705,6 +705,13 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 		hack_force_pipe_split(v, context->streams[0]->timing.pix_clk_100hz);
 }
 
+
+unsigned int get_highest_allowed_voltage_level(uint32_t hw_internal_rev)
+{
+	/* we are ok with all levels */
+	return 4;
+}
+
 bool dcn_validate_bandwidth(
 		struct dc *dc,
 		struct dc_state *context,
@@ -732,6 +739,7 @@ bool dcn_validate_bandwidth(
 
 	memset(v, 0, sizeof(*v));
 	kernel_fpu_begin();
+
 	v->sr_exit_time = dc->dcn_soc->sr_exit_time;
 	v->sr_enter_plus_exit_time = dc->dcn_soc->sr_enter_plus_exit_time;
 	v->urgent_latency = dc->dcn_soc->urgent_latency;
@@ -1268,7 +1276,7 @@ bool dcn_validate_bandwidth(
 	PERFORMANCE_TRACE_END();
 	BW_VAL_TRACE_FINISH();
 
-	if (bw_limit_pass && v->voltage_level != 5)
+	if (bw_limit_pass && v->voltage_level <= get_highest_allowed_voltage_level(dc->ctx->asic_id.hw_internal_rev))
 		return true;
 	else
 		return false;

commit 12e2b2d4c65f6164830e25fcd9624519a424b182
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jul 3 16:20:42 2019 -0400

    drm/amd/display: add dcc programming for dual plane
    
    Add dual plane dcc programming support for
    surfaces.
    
    Removes unions from plane size and dcc params as they
    serve no practical purpose only making our code
    more convoluted. This results in easy dual plane
    dcc and surface size programming.
    
    Temporary diags_dm code is used to handle the interface
    change without breaking functionality as a diags change
    needs to be applied after this one.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 38365dd911a3..061c6e3a3088 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -329,7 +329,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 			dcc_support_pixel_format(pipe->plane_state->format, &bpe) ? 1 : 0;
 	}
 	input->src.dcc_rate            = 1;
-	input->src.meta_pitch          = pipe->plane_state->dcc.grph.meta_pitch;
+	input->src.meta_pitch          = pipe->plane_state->dcc.meta_pitch;
 	input->src.source_scan         = dm_horz;
 	input->src.sw_mode             = pipe->plane_state->tiling_info.gfx9.swizzle;
 

commit 254eb07cb090374d0e1e90790395ead77f6f6ad5
Author: Joshua Aberback <joshua.aberback@amd.com>
Date:   Mon Apr 1 15:18:29 2019 -0400

    drm/amd/display: Optimize bandwidth validation by adding early return
    
    We can split validation into three parts: getting voltage level, getting
    watermarks, and rq/dlg calculations. The voltage level is enough to answer
    the question "do we support this state", and the rest of it is to determine
    what hardware programming is needed to support the state. Most of the calls
    to validate_bandwidth only care about the first part, so we added an early
    return in that case
    
    Signed-off-by: Joshua Aberback <joshua.aberback@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index ef804948694e..38365dd911a3 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1116,9 +1116,8 @@ bool dcn_validate_bandwidth(
 
 		context->bw_ctx.bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
-		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65) {
+		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65)
 			context->bw_ctx.bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
-		}
 
 		context->bw_ctx.bw.dcn.clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
 		context->bw_ctx.bw.dcn.clk.dcfclk_khz = (int)(v->dcfclk * 1000);
@@ -1133,7 +1132,8 @@ bool dcn_validate_bandwidth(
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw_ctx.bw.dcn.clk.dppclk_khz = context->bw_ctx.bw.dcn.clk.dispclk_khz / v->dispclk_dppclk_ratio;
+		context->bw_ctx.bw.dcn.clk.dppclk_khz = context->bw_ctx.bw.dcn.clk.dispclk_khz /
+				v->dispclk_dppclk_ratio;
 		context->bw_ctx.bw.dcn.clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
 		switch (v->voltage_level) {
 		case 0:
@@ -1220,9 +1220,7 @@ bool dcn_validate_bandwidth(
 						/* pipe not split previously needs split */
 						hsplit_pipe = find_idle_secondary_pipe(&context->res_ctx, pool, pipe);
 						ASSERT(hsplit_pipe);
-						split_stream_across_pipes(
-							&context->res_ctx, pool,
-							pipe, hsplit_pipe);
+						split_stream_across_pipes(&context->res_ctx, pool, pipe, hsplit_pipe);
 					}
 
 					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe, input_idx);
@@ -1253,7 +1251,6 @@ bool dcn_validate_bandwidth(
 	}
 
 	if (v->voltage_level == 0) {
-
 		context->bw_ctx.dml.soc.sr_enter_plus_exit_time_us =
 				dc->dcn_soc->sr_enter_plus_exit_time;
 		context->bw_ctx.dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;

commit 728c06986a4f386c7ec5e5170716e30b610c6d32
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Fri Feb 22 09:59:43 2019 -0500

    drm/amd/display: Add DCN2 changes to DML
    
    Update DML (Display Mode Lib) to support DCN2
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 5c1e0adb142b..ef804948694e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -31,6 +31,7 @@
 #include "resource.h"
 #include "dcn10/dcn10_resource.h"
 #include "dcn10/dcn10_hubbub.h"
+#include "dml/dml1_display_rq_dlg_calc.h"
 
 #include "dcn_calc_math.h"
 
@@ -52,7 +53,13 @@
  * remain as-is as it provides us with a guarantee from HW that it is correct.
  */
 
+#ifdef CONFIG_DRM_AMD_DC_DCN2_0
+/* Defaults from spreadsheet rev#247.
+ * RV2 delta: dram_clock_change_latency, max_num_dpp
+ */
+#else
 /* Defaults from spreadsheet rev#247 */
+#endif
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		/* latencies */
 		.sr_exit_time = 17, /*us*/

commit 4b0ab7dd37a3beec35699f2a821d0474bd269c2d
Author: Su Sung Chung <Su.Chung@amd.com>
Date:   Fri May 10 15:16:45 2019 -0400

    drm/amd/display: fix crash on setmode when mode is close to bw limit
    
    [why]
    during It's possible to call dcn_validate_bandwidth with no plane.
    In that case, as we are only intersted in if output timing is supported or not,
    even if we cannot support native resolution, we still want to support lower
    resolution if it is valid
    
    [how]
    if there exist no surface, make viewport/rec_out size at max 1080p. It is
    already known that 1080p x 6(max # of pipes) is supported, so if we fail
    validation, it is because of the stream
    
    Signed-off-by: Su Sung Chung <Su.Chung@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 0a336c80c7bb..5c1e0adb142b 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -872,8 +872,19 @@ bool dcn_validate_bandwidth(
 			v->lb_bit_per_pixel[input_idx] = 30;
 			v->viewport_width[input_idx] = pipe->stream->timing.h_addressable;
 			v->viewport_height[input_idx] = pipe->stream->timing.v_addressable;
-			v->scaler_rec_out_width[input_idx] = pipe->stream->timing.h_addressable;
-			v->scaler_recout_height[input_idx] = pipe->stream->timing.v_addressable;
+			/*
+			 * for cases where we have no plane, we want to validate up to 1080p
+			 * source size because here we are only interested in if the output
+			 * timing is supported or not. if we cannot support native resolution
+			 * of the high res display, we still want to support lower res up scale
+			 * to native
+			 */
+			if (v->viewport_width[input_idx] > 1920)
+				v->viewport_width[input_idx] = 1920;
+			if (v->viewport_height[input_idx] > 1080)
+				v->viewport_height[input_idx] = 1080;
+			v->scaler_rec_out_width[input_idx] = v->viewport_width[input_idx];
+			v->scaler_recout_height[input_idx] = v->viewport_height[input_idx];
 			v->override_hta_ps[input_idx] = 1;
 			v->override_vta_ps[input_idx] = 1;
 			v->override_hta_pschroma[input_idx] = 1;

commit 88147df0308ede3e72a2edefbb9b705e9b63932b
Author: Su Sung Chung <Su.Chung@amd.com>
Date:   Wed May 1 16:54:56 2019 -0400

    drm/amd/display: fix calculation of total_data_read_bandwidth
    
    [why]
    by adding fast_validate flag, we are skipping some portion of
    dcn_validate_bandwidth code that is not necessary for mode validation.
    However we have a bug where it does not calculate
    v->total_data_read_bandwidth, which is one of the factors determines the
    result of the validation, and therefore report false positive during
    mode validation.
    
    [how]
    add calculation of v->total_data_read_bandwidth outside of the region
    that is guarded by fast_validate flag
    
    Signed-off-by: Su Sung Chung <Su.Chung@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index d53306de2e16..0a336c80c7bb 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -711,7 +711,7 @@ bool dcn_validate_bandwidth(
 
 	const struct resource_pool *pool = dc->res_pool;
 	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;
-	int i, input_idx;
+	int i, input_idx, k;
 	int vesa_sync_start, asic_blank_end, asic_blank_start;
 	bool bw_limit_pass;
 	float bw_limit;
@@ -1022,6 +1022,43 @@ bool dcn_validate_bandwidth(
 		mode_support_and_system_configuration(v);
 	}
 
+	display_pipe_configuration(v);
+
+	for (k = 0; k <= v->number_of_active_planes - 1; k++) {
+		if (v->source_scan[k] == dcn_bw_hor)
+			v->swath_width_y[k] = v->viewport_width[k] / v->dpp_per_plane[k];
+		else
+			v->swath_width_y[k] = v->viewport_height[k] / v->dpp_per_plane[k];
+	}
+	for (k = 0; k <= v->number_of_active_planes - 1; k++) {
+		if (v->source_pixel_format[k] == dcn_bw_rgb_sub_64) {
+			v->byte_per_pixel_dety[k] = 8.0;
+			v->byte_per_pixel_detc[k] = 0.0;
+		} else if (v->source_pixel_format[k] == dcn_bw_rgb_sub_32) {
+			v->byte_per_pixel_dety[k] = 4.0;
+			v->byte_per_pixel_detc[k] = 0.0;
+		} else if (v->source_pixel_format[k] == dcn_bw_rgb_sub_16) {
+			v->byte_per_pixel_dety[k] = 2.0;
+			v->byte_per_pixel_detc[k] = 0.0;
+		} else if (v->source_pixel_format[k] == dcn_bw_yuv420_sub_8) {
+			v->byte_per_pixel_dety[k] = 1.0;
+			v->byte_per_pixel_detc[k] = 2.0;
+		} else {
+			v->byte_per_pixel_dety[k] = 4.0f / 3.0f;
+			v->byte_per_pixel_detc[k] = 8.0f / 3.0f;
+		}
+	}
+
+	v->total_data_read_bandwidth = 0.0;
+	for (k = 0; k <= v->number_of_active_planes - 1; k++) {
+		v->read_bandwidth_plane_luma[k] = v->swath_width_y[k] * v->dpp_per_plane[k] *
+				dcn_bw_ceil2(v->byte_per_pixel_dety[k], 1.0) / (v->htotal[k] / v->pixel_clock[k]) * v->v_ratio[k];
+		v->read_bandwidth_plane_chroma[k] = v->swath_width_y[k] / 2.0 * v->dpp_per_plane[k] *
+				dcn_bw_ceil2(v->byte_per_pixel_detc[k], 2.0) / (v->htotal[k] / v->pixel_clock[k]) * v->v_ratio[k] / 2.0;
+		v->total_data_read_bandwidth = v->total_data_read_bandwidth +
+				v->read_bandwidth_plane_luma[k] + v->read_bandwidth_plane_chroma[k];
+	}
+
 	BW_VAL_TRACE_END_VOLTAGE_LEVEL();
 
 	if (v->voltage_level != number_of_states_plus_one && !fast_validate) {

commit dc88b4a684d284a200c0ecfd8d87179d6f6c89a3
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Mon Apr 22 19:39:35 2019 -0400

    drm/amd/display: make clk mgr soc specific
    
    [Why]
    First step of refactoring clk mgr to better handle different
    ways of handling clock operations. Clock operation policies are
    soc specific and not just DCN vesion specific. It is not a hw resource,
    should not be in the resource pool.
    
    [How]
    Change clock manager creation to be based on HW internal ID, rename
    clock manager members to be more clear. Move clock manager out of
    resource.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1b4b51657f5e..d53306de2e16 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -24,11 +24,10 @@
  */
 
 #include "dm_services.h"
+#include "dc.h"
 #include "dcn_calcs.h"
 #include "dcn_calc_auto.h"
-#include "dc.h"
 #include "dal_asic_id.h"
-
 #include "resource.h"
 #include "dcn10/dcn10_resource.h"
 #include "dcn10/dcn10_hubbub.h"

commit f55be0be5b7296e73f1634e2839a1953dc12d11e
Author: Joshua Aberback <joshua.aberback@amd.com>
Date:   Mon Apr 1 15:21:24 2019 -0400

    drm/amd/display: Add profiling tools for bandwidth validation
    
    [Why]
    We used this change to investigate the performance of bandwidth validation,
    it will be useful to have if we need to investigate further.
    
    [How]
    We use performance counter tick numbers to profile performance, they live
    at dc->debug.bw_val_profile (set .enable in debugger to turn on measuring).
    
    Signed-off-by: Joshua Aberback <joshua.aberback@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index d7aece82e4fe..1b4b51657f5e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -704,6 +704,12 @@ bool dcn_validate_bandwidth(
 		struct dc_state *context,
 		bool fast_validate)
 {
+	/*
+	 * we want a breakdown of the various stages of validation, which the
+	 * perf_trace macro doesn't support
+	 */
+	BW_VAL_TRACE_SETUP();
+
 	const struct resource_pool *pool = dc->res_pool;
 	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;
 	int i, input_idx;
@@ -712,6 +718,9 @@ bool dcn_validate_bandwidth(
 	float bw_limit;
 
 	PERFORMANCE_TRACE_START();
+
+	BW_VAL_TRACE_COUNT();
+
 	if (dcn_bw_apply_registry_override(dc))
 		dcn_bw_sync_calcs_and_dml(dc);
 
@@ -1014,6 +1023,8 @@ bool dcn_validate_bandwidth(
 		mode_support_and_system_configuration(v);
 	}
 
+	BW_VAL_TRACE_END_VOLTAGE_LEVEL();
+
 	if (v->voltage_level != number_of_states_plus_one && !fast_validate) {
 		float bw_consumed = v->total_bandwidth_consumed_gbyte_per_second;
 
@@ -1089,6 +1100,8 @@ bool dcn_validate_bandwidth(
 			break;
 		}
 
+		BW_VAL_TRACE_END_WATERMARKS();
+
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 
@@ -1179,6 +1192,10 @@ bool dcn_validate_bandwidth(
 
 			input_idx++;
 		}
+	} else if (v->voltage_level == number_of_states_plus_one) {
+		BW_VAL_TRACE_SKIP(fail);
+	} else if (fast_validate) {
+		BW_VAL_TRACE_SKIP(fast);
 	}
 
 	if (v->voltage_level == 0) {
@@ -1198,6 +1215,7 @@ bool dcn_validate_bandwidth(
 	kernel_fpu_end();
 
 	PERFORMANCE_TRACE_END();
+	BW_VAL_TRACE_FINISH();
 
 	if (bw_limit_pass && v->voltage_level != 5)
 		return true;

commit afcd526b1ba9dbc5707f9bd8d2d032ae62e337dc
Author: Joshua Aberback <joshua.aberback@amd.com>
Date:   Mon Apr 1 15:18:29 2019 -0400

    drm/amd/display: Add fast_validate parameter
    
    Add a fast_validate parameter in dc_validate_global_state for future use
    
    Signed-off-by: Joshua Aberback <joshua.aberback@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8843361e842d..d7aece82e4fe 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -701,7 +701,8 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 
 bool dcn_validate_bandwidth(
 		struct dc *dc,
-		struct dc_state *context)
+		struct dc_state *context,
+		bool fast_validate)
 {
 	const struct resource_pool *pool = dc->res_pool;
 	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;
@@ -1013,8 +1014,9 @@ bool dcn_validate_bandwidth(
 		mode_support_and_system_configuration(v);
 	}
 
-	if (v->voltage_level != 5) {
+	if (v->voltage_level != number_of_states_plus_one && !fast_validate) {
 		float bw_consumed = v->total_bandwidth_consumed_gbyte_per_second;
+
 		if (bw_consumed < v->fabric_and_dram_bandwidth_vmin0p65)
 			bw_consumed = v->fabric_and_dram_bandwidth_vmin0p65;
 		else if (bw_consumed < v->fabric_and_dram_bandwidth_vmid0p72)

commit 813d20dccf93f84f4c16236f7c037dc34db48f10
Author: Aidan Wood <Aidan.Wood@amd.com>
Date:   Fri Feb 22 13:37:03 2019 -0500

    drm/amd/display: Fix multi-thread writing to 1 state
    
    [Why]
    Multiple threads were writing back to one global VBA in DC resulting
    in multiple threads overwriting eachother's data
    
    [How]
    Add an instance of DML (which contains VBA) to each context and
    change all calls that used dc->dml to use context->dml. Created a
    seperate copy constructor for linux in a case where there is no
    access to DC.
    
    Signed-off-by: Aidan Wood <Aidan.Wood@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1e23ddc7d088..8843361e842d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -544,28 +544,28 @@ static void calc_wm_sets_and_perf_params(
 		v->fabric_and_dram_bandwidth = v->fabric_and_dram_bandwidth_vnom0p8;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->bw.dcn.watermarks.b.cstate_pstate.cstate_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.b.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->bw.dcn.watermarks.b.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.b.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->bw.dcn.watermarks.b.cstate_pstate.pstate_change_ns =
+		context->bw_ctx.bw.dcn.watermarks.b.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->bw.dcn.watermarks.b.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->bw.dcn.watermarks.b.urgent_ns = v->urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.b.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.b.urgent_ns = v->urgent_watermark * 1000;
 
 		v->dcfclk_per_state[1] = v->dcfclkv_nom0p8;
 		v->dcfclk_per_state[0] = v->dcfclkv_nom0p8;
 		v->dcfclk = v->dcfclkv_nom0p8;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->bw.dcn.watermarks.c.cstate_pstate.cstate_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.c.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->bw.dcn.watermarks.c.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.c.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->bw.dcn.watermarks.c.cstate_pstate.pstate_change_ns =
+		context->bw_ctx.bw.dcn.watermarks.c.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->bw.dcn.watermarks.c.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->bw.dcn.watermarks.c.urgent_ns = v->urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.c.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.c.urgent_ns = v->urgent_watermark * 1000;
 	}
 
 	if (v->voltage_level < 3) {
@@ -579,14 +579,14 @@ static void calc_wm_sets_and_perf_params(
 		v->dcfclk = v->dcfclkv_max0p9;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->bw.dcn.watermarks.d.cstate_pstate.cstate_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.d.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->bw.dcn.watermarks.d.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.d.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->bw.dcn.watermarks.d.cstate_pstate.pstate_change_ns =
+		context->bw_ctx.bw.dcn.watermarks.d.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->bw.dcn.watermarks.d.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->bw.dcn.watermarks.d.urgent_ns = v->urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.d.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.d.urgent_ns = v->urgent_watermark * 1000;
 	}
 
 	v->fabric_and_dram_bandwidth_per_state[2] = v->fabric_and_dram_bandwidth_vnom0p8;
@@ -599,20 +599,20 @@ static void calc_wm_sets_and_perf_params(
 	v->dcfclk = v->dcfclk_per_state[v->voltage_level];
 	dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-	context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
+	context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
 		v->stutter_exit_watermark * 1000;
-	context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+	context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
 			v->stutter_enter_plus_exit_watermark * 1000;
-	context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
+	context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
 			v->dram_clock_change_watermark * 1000;
-	context->bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-	context->bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
+	context->bw_ctx.bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+	context->bw_ctx.bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
 	if (v->voltage_level >= 2) {
-		context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
-		context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.b = context->bw_ctx.bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.c = context->bw_ctx.bw.dcn.watermarks.a;
 	}
 	if (v->voltage_level >= 3)
-		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.d = context->bw_ctx.bw.dcn.watermarks.a;
 }
 #endif
 
@@ -1008,8 +1008,8 @@ bool dcn_validate_bandwidth(
 				dc->debug.sr_enter_plus_exit_time_dpm0_ns / 1000.0f;
 		if (dc->debug.sr_exit_time_dpm0_ns)
 			v->sr_exit_time =  dc->debug.sr_exit_time_dpm0_ns / 1000.0f;
-		dc->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
-		dc->dml.soc.sr_exit_time_us = v->sr_exit_time;
+		context->bw_ctx.dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
+		context->bw_ctx.dml.soc.sr_exit_time_us = v->sr_exit_time;
 		mode_support_and_system_configuration(v);
 	}
 
@@ -1035,54 +1035,54 @@ bool dcn_validate_bandwidth(
 		 */
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
+		context->bw_ctx.bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
-		context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
-		context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
-		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
+		context->bw_ctx.bw.dcn.watermarks.b = context->bw_ctx.bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.c = context->bw_ctx.bw.dcn.watermarks.a;
+		context->bw_ctx.bw.dcn.watermarks.d = context->bw_ctx.bw.dcn.watermarks.a;
 
-		context->bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 /
+		context->bw_ctx.bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
 		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65) {
-			context->bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
+			context->bw_ctx.bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
 		}
 
-		context->bw.dcn.clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
-		context->bw.dcn.clk.dcfclk_khz = (int)(v->dcfclk * 1000);
+		context->bw_ctx.bw.dcn.clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
+		context->bw_ctx.bw.dcn.clk.dcfclk_khz = (int)(v->dcfclk * 1000);
 
-		context->bw.dcn.clk.dispclk_khz = (int)(v->dispclk * 1000);
+		context->bw_ctx.bw.dcn.clk.dispclk_khz = (int)(v->dispclk * 1000);
 		if (dc->debug.max_disp_clk == true)
-			context->bw.dcn.clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
+			context->bw_ctx.bw.dcn.clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
 
-		if (context->bw.dcn.clk.dispclk_khz <
+		if (context->bw_ctx.bw.dcn.clk.dispclk_khz <
 				dc->debug.min_disp_clk_khz) {
-			context->bw.dcn.clk.dispclk_khz =
+			context->bw_ctx.bw.dcn.clk.dispclk_khz =
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw.dcn.clk.dppclk_khz = context->bw.dcn.clk.dispclk_khz / v->dispclk_dppclk_ratio;
-		context->bw.dcn.clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
+		context->bw_ctx.bw.dcn.clk.dppclk_khz = context->bw_ctx.bw.dcn.clk.dispclk_khz / v->dispclk_dppclk_ratio;
+		context->bw_ctx.bw.dcn.clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
 		switch (v->voltage_level) {
 		case 0:
-			context->bw.dcn.clk.max_supported_dppclk_khz =
+			context->bw_ctx.bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmin0p65 * 1000);
 			break;
 		case 1:
-			context->bw.dcn.clk.max_supported_dppclk_khz =
+			context->bw_ctx.bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmid0p72 * 1000);
 			break;
 		case 2:
-			context->bw.dcn.clk.max_supported_dppclk_khz =
+			context->bw_ctx.bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vnom0p8 * 1000);
 			break;
 		default:
-			context->bw.dcn.clk.max_supported_dppclk_khz =
+			context->bw_ctx.bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmax0p9 * 1000);
 			break;
 		}
@@ -1181,9 +1181,9 @@ bool dcn_validate_bandwidth(
 
 	if (v->voltage_level == 0) {
 
-		dc->dml.soc.sr_enter_plus_exit_time_us =
+		context->bw_ctx.dml.soc.sr_enter_plus_exit_time_us =
 				dc->dcn_soc->sr_enter_plus_exit_time;
-		dc->dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;
+		context->bw_ctx.dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;
 	}
 
 	/*

commit 457109829f4ee4107e8c7108237afba21fabbb5e
Merge: b4e4538a0ab5 7a65bdc6903d
Author: Dave Airlie <airlied@redhat.com>
Date:   Wed Apr 3 11:36:52 2019 +1000

    Merge branch 'drm-next-5.2' of git://people.freedesktop.org/~agd5f/linux into drm-next
    
    amdgpu:
    - Switch to HMM for userptr (reverted until HMM fixes land)
    - New experimental SMU 11 replacement for powerplay for vega20 (not enabled by default)
    - Initial RAS support for vega20
    - BACO support for vega12
    - BACO fixes for vega20
    - Rework IH handling for page fault and retry interrupts
    - Cleanly split CPU and GPU paths for GPUVM updates
    - Powerplay fixes
    - XGMI fixes
    - Rework how DC interacts with atomic for planes
    - Clean up and simplify DC/Powerplay interfaces
    - Misc cleanups and bug fixes
    
    amdkfd:
    - Switch to HMM for userptr (reverted until HMM fixes land)
    - Add initial RAS support
    - MQD fixes
    
    ttm:
    - Unify DRM_FILE_PAGE_OFFSET handling
    - Account for kernel allocations in kernel zone only
    - Misc cleanups
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Alex Deucher <alexdeucher@gmail.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190402170820.22197-1-alexander.deucher@amd.com

commit 0cbba1638b8ef1272ea9ef9b4b95274596214443
Author: Joshua Aberback <joshua.aberback@amd.com>
Date:   Mon Mar 18 13:40:47 2019 -0400

    drm/amd/display: Populate macro_tile_size field for dml
    
    Create a functions to return swizzle types for dml
    
    Signed-off-by: Joshua Aberback <joshua.aberback@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 5476b7a55236..c9772b85af0c 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -247,6 +247,53 @@ static enum dcn_bw_defs tl_pixel_format_to_bw_defs(enum surface_pixel_format for
 	}
 }
 
+enum source_macro_tile_size swizzle_mode_to_macro_tile_size(enum swizzle_mode_values sw_mode)
+{
+	switch (sw_mode) {
+	/* for 4/8/16 high tiles */
+	case DC_SW_LINEAR:
+		return dm_4k_tile;
+	case DC_SW_4KB_S:
+	case DC_SW_4KB_S_X:
+		return dm_4k_tile;
+	case DC_SW_64KB_S:
+	case DC_SW_64KB_S_X:
+	case DC_SW_64KB_S_T:
+		return dm_64k_tile;
+	case DC_SW_VAR_S:
+	case DC_SW_VAR_S_X:
+		return dm_256k_tile;
+
+	/* For 64bpp 2 high tiles */
+	case DC_SW_4KB_D:
+	case DC_SW_4KB_D_X:
+		return dm_4k_tile;
+	case DC_SW_64KB_D:
+	case DC_SW_64KB_D_X:
+	case DC_SW_64KB_D_T:
+		return dm_64k_tile;
+	case DC_SW_VAR_D:
+	case DC_SW_VAR_D_X:
+		return dm_256k_tile;
+
+	case DC_SW_4KB_R:
+	case DC_SW_4KB_R_X:
+		return dm_4k_tile;
+	case DC_SW_64KB_R:
+	case DC_SW_64KB_R_X:
+		return dm_64k_tile;
+	case DC_SW_VAR_R:
+	case DC_SW_VAR_R_X:
+		return dm_256k_tile;
+
+	/* Unsupported swizzle modes for dcn */
+	case DC_SW_256B_S:
+	default:
+		ASSERT(0); /* Not supported */
+		return 0;
+	}
+}
+
 static void pipe_ctx_to_e2e_pipe_params (
 		const struct pipe_ctx *pipe,
 		struct _vcs_dpi_display_pipe_params_st *input)
@@ -287,46 +334,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	input->src.cur0_src_width      = 128; /* TODO: Cursor calcs, not curently stored */
 	input->src.cur0_bpp            = 32;
 
-	switch (pipe->plane_state->tiling_info.gfx9.swizzle) {
-	/* for 4/8/16 high tiles */
-	case DC_SW_LINEAR:
-		input->src.macro_tile_size = dm_4k_tile;
-		break;
-	case DC_SW_4KB_S:
-	case DC_SW_4KB_S_X:
-		input->src.macro_tile_size = dm_4k_tile;
-		break;
-	case DC_SW_64KB_S:
-	case DC_SW_64KB_S_X:
-	case DC_SW_64KB_S_T:
-		input->src.macro_tile_size = dm_64k_tile;
-		break;
-	case DC_SW_VAR_S:
-	case DC_SW_VAR_S_X:
-		input->src.macro_tile_size = dm_256k_tile;
-		break;
-
-	/* For 64bpp 2 high tiles */
-	case DC_SW_4KB_D:
-	case DC_SW_4KB_D_X:
-		input->src.macro_tile_size = dm_4k_tile;
-		break;
-	case DC_SW_64KB_D:
-	case DC_SW_64KB_D_X:
-	case DC_SW_64KB_D_T:
-		input->src.macro_tile_size = dm_64k_tile;
-		break;
-	case DC_SW_VAR_D:
-	case DC_SW_VAR_D_X:
-		input->src.macro_tile_size = dm_256k_tile;
-		break;
-
-	/* Unsupported swizzle modes for dcn */
-	case DC_SW_256B_S:
-	default:
-		ASSERT(0); /* Not supported */
-		break;
-	}
+	input->src.macro_tile_size = swizzle_mode_to_macro_tile_size(pipe->plane_state->tiling_info.gfx9.swizzle);
 
 	switch (pipe->plane_state->rotation) {
 	case ROTATION_ANGLE_0:

commit 5581192d72337146ae8ca3128359ecd0bad0f8c7
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Tue Mar 12 15:12:41 2019 -0400

    drm/amd/display: add preferred pipe split logic
    
    [why]
    existing logic finds "first free pipe from 5 -> 0" to split
    this will cause certain sequences to require DC to move
    an MPCC from one tree to another, which is unsupported
    this leads to blackscreen
    
    to mitigate this problem, we will always try to acquire the
    "preferred" pipe, and each pipe has a unique preferred pipe
    this means we avoid most of the scenarios where
    pipe splitting leads to moving MPCC from one tree
    to another
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 0090f7491446..5476b7a55236 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1141,7 +1141,7 @@ bool dcn_validate_bandwidth(
 						hsplit_pipe->pipe_dlg_param.vblank_end = pipe->pipe_dlg_param.vblank_end;
 					} else {
 						/* pipe not split previously needs split */
-						hsplit_pipe = find_idle_secondary_pipe(&context->res_ctx, pool);
+						hsplit_pipe = find_idle_secondary_pipe(&context->res_ctx, pool, pipe);
 						ASSERT(hsplit_pipe);
 						split_stream_across_pipes(
 							&context->res_ctx, pool,

commit 33d7598d7022eac064b48e42dd4ae3e1dc9b52cd
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Fri Feb 22 16:50:00 2019 -0500

    drm/amd/display: fix up reference clock abstractions
    
    [why]
    "reference clock" is a very overloaded variable in DC and causes confusion
    as there are multiple sources of reference clock, which may be different values
    incorrect input values to DML will cause DCHUB to be programmed improperly
    and lead to hard to debug underflow issues
    
    [how]
    instead of using ref clock everywhere, specify WHICH ref clock:
    - xtalin
    - dccg refclk
    - dchub refclk
    
    these are all distinct values which may not be equal
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Acked-by: David Francis <David.Francis@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8ee182be394a..0090f7491446 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -466,7 +466,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	input.clks_cfg.dcfclk_mhz = v->dcfclk;
 	input.clks_cfg.dispclk_mhz = v->dispclk;
 	input.clks_cfg.dppclk_mhz = v->dppclk;
-	input.clks_cfg.refclk_mhz = dc->res_pool->ref_clock_inKhz / 1000.0;
+	input.clks_cfg.refclk_mhz = dc->res_pool->ref_clocks.dchub_ref_clock_inKhz / 1000.0;
 	input.clks_cfg.socclk_mhz = v->socclk;
 	input.clks_cfg.voltage = v->voltage_level;
 //	dc->dml.logger = pool->base.logger;

commit a08ac5a62c0f8b29d110004d05b1554a8b8fd5f5
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Tue Feb 19 12:14:19 2019 -0500

    drm/amd/display: Add pp_smu null pointer check
    
    res_pool->pp_smu may be NULL. Check before use
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Krunoslav Kovac <Krunoslav.Kovac@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 2a807b9f77f7..8ee182be394a 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1391,12 +1391,14 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 
 void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
-	struct pp_smu_funcs_rv *pp = &dc->res_pool->pp_smu->rv_funcs;
+	struct pp_smu_funcs_rv *pp = NULL;
 	struct pp_smu_wm_range_sets ranges = {0};
 	int min_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
 
-	if (!pp->set_wm_ranges)
+	if (dc->res_pool->pp_smu)
+		pp = &dc->res_pool->pp_smu->rv_funcs;
+	if (!pp || !pp->set_wm_ranges)
 		return;
 
 	kernel_fpu_begin();

commit 59d3191f14dc18881fec1172c7096b7863622803
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Wed Jan 30 15:45:18 2019 -0500

    drm/amd/display: don't call dm_pp_ function from an fpu block
    
    Powerplay functions called from dm_pp_* functions tend to do a
    mutex_lock which isn't safe to do inside a kernel_fpu_begin/end block as
    those will disable/enable preemption.
    
    Rearrange the dm_pp_get_clock_levels_by_type_with_voltage calls to make
    sure they happen outside of kernel_fpu_begin/end.
    
    Cc: stable@vger.kernel.org
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 12d1842079ae..eb62d10bb65c 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1348,12 +1348,12 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	struct dm_pp_clock_levels_with_voltage fclks = {0}, dcfclks = {0};
 	bool res;
 
-	kernel_fpu_begin();
-
 	/* TODO: This is not the proper way to obtain fabric_and_dram_bandwidth, should be min(fclk, memclk) */
 	res = dm_pp_get_clock_levels_by_type_with_voltage(
 			ctx, DM_PP_CLOCK_TYPE_FCLK, &fclks);
 
+	kernel_fpu_begin();
+
 	if (res)
 		res = verify_clock_values(&fclks);
 
@@ -1372,9 +1372,13 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 	} else
 		BREAK_TO_DEBUGGER();
 
+	kernel_fpu_end();
+
 	res = dm_pp_get_clock_levels_by_type_with_voltage(
 			ctx, DM_PP_CLOCK_TYPE_DCFCLK, &dcfclks);
 
+	kernel_fpu_begin();
+
 	if (res)
 		res = verify_clock_values(&dcfclks);
 

commit 0f1a6ad724cd5270c7c7d1bba98ac1222a0943b6
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Tue Jan 15 10:46:46 2019 -0500

    drm/amd/display: PPLIB Hookup
    
    [Why]
    Make dml and integration with pplib clearer.
    
    [How]
    Change the way the dml formula is initialized to make its values more
    clear. Restructure DC interface with pplib into rv_funcs.
    Cap clocks received from pplib.
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Signed-off-by: Eryk Brol <eryk.brol@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 12d1842079ae..2a807b9f77f7 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1391,7 +1391,7 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 
 void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
-	struct pp_smu_funcs_rv *pp = dc->res_pool->pp_smu;
+	struct pp_smu_funcs_rv *pp = &dc->res_pool->pp_smu->rv_funcs;
 	struct pp_smu_wm_range_sets ranges = {0};
 	int min_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */

commit 380604e27bc9c26ce64a83044aa1ea76ffd28caf
Author: Ken Chalmers <ken.chalmers@amd.com>
Date:   Tue Nov 6 14:24:12 2018 -0500

    drm/amd/display: Use 100 Hz precision for pipe pixel clocks
    
    [Why]
    Users would like more accurate pixel clocks, especially for fractional
    "TV" frame rates like 59.94 Hz.
    
    [How]
    Store and communicate pixel clocks with 100 Hz accuracy from
    dc_crtc_timing through to BIOS command table setpixelclock call.
    
    Signed-off-by: Ken Chalmers <ken.chalmers@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 446ee482fd24..12d1842079ae 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -416,7 +416,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 			- pipe->stream->timing.v_addressable
 			- pipe->stream->timing.v_border_bottom
 			- pipe->stream->timing.v_border_top;
-	input->dest.pixel_rate_mhz = pipe->stream->timing.pix_clk_khz/1000.0;
+	input->dest.pixel_rate_mhz = pipe->stream->timing.pix_clk_100hz/10000.0;
 	input->dest.vstartup_start = pipe->pipe_dlg_param.vstartup_start;
 	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
 	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
@@ -663,9 +663,9 @@ static void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
 }
 
 static void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
-		unsigned int pixel_rate_khz)
+		unsigned int pixel_rate_100hz)
 {
-	float pixel_rate_mhz = pixel_rate_khz / 1000;
+	float pixel_rate_mhz = pixel_rate_100hz / 10000;
 
 	/*
 	 * force enabling pipe split by lower dpp clock for DPM0 to just
@@ -688,7 +688,7 @@ static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 
 	if (context->stream_count == 1 &&
 			dbg->force_single_disp_pipe_split)
-		hack_force_pipe_split(v, context->streams[0]->timing.pix_clk_khz);
+		hack_force_pipe_split(v, context->streams[0]->timing.pix_clk_100hz);
 }
 
 bool dcn_validate_bandwidth(
@@ -845,7 +845,7 @@ bool dcn_validate_bandwidth(
 		v->v_sync_plus_back_porch[input_idx] = pipe->stream->timing.v_total
 				- v->vactive[input_idx]
 				- pipe->stream->timing.v_front_porch;
-		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz/1000.0;
+		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_100hz/10000.0;
 		if (pipe->stream->timing.timing_3d_format == TIMING_3D_FORMAT_HW_FRAME_PACKING)
 			v->pixel_clock[input_idx] *= 2;
 		if (!pipe->plane_state) {

commit fb57452fb4023a8efeb25efd468f53b292ba414e
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Thu Nov 8 14:38:44 2018 -0500

    drm/amd/display: update DCN dml calcs
    
    DV have made updates to DCN dml which we need to pull in
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Eric Bernstein <Eric.Bernstein@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index cd88ceca5c41..446ee482fd24 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -290,41 +290,34 @@ static void pipe_ctx_to_e2e_pipe_params (
 	switch (pipe->plane_state->tiling_info.gfx9.swizzle) {
 	/* for 4/8/16 high tiles */
 	case DC_SW_LINEAR:
-		input->src.is_display_sw = 1;
 		input->src.macro_tile_size = dm_4k_tile;
 		break;
 	case DC_SW_4KB_S:
 	case DC_SW_4KB_S_X:
-		input->src.is_display_sw = 0;
 		input->src.macro_tile_size = dm_4k_tile;
 		break;
 	case DC_SW_64KB_S:
 	case DC_SW_64KB_S_X:
 	case DC_SW_64KB_S_T:
-		input->src.is_display_sw = 0;
 		input->src.macro_tile_size = dm_64k_tile;
 		break;
 	case DC_SW_VAR_S:
 	case DC_SW_VAR_S_X:
-		input->src.is_display_sw = 0;
 		input->src.macro_tile_size = dm_256k_tile;
 		break;
 
 	/* For 64bpp 2 high tiles */
 	case DC_SW_4KB_D:
 	case DC_SW_4KB_D_X:
-		input->src.is_display_sw = 1;
 		input->src.macro_tile_size = dm_4k_tile;
 		break;
 	case DC_SW_64KB_D:
 	case DC_SW_64KB_D_X:
 	case DC_SW_64KB_D_T:
-		input->src.is_display_sw = 1;
 		input->src.macro_tile_size = dm_64k_tile;
 		break;
 	case DC_SW_VAR_D:
 	case DC_SW_VAR_D_X:
-		input->src.is_display_sw = 1;
 		input->src.macro_tile_size = dm_256k_tile;
 		break;
 

commit ceb3dbb4690db8377ad127a5666cd4775d9f70f4
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Fri Nov 9 09:21:21 2018 -0500

    drm/amd/display: remove sink reference in dc_stream_state
    
    [why]
    dc_stream_state containing a pointer to sink is poor design.
    Sink describes the display, and the specifications or capabilities
    it has.  That information is irrelevant for dc_stream_state, which describes
    hardware state, and is generally used for hardware programming.  It
    could further be argued that dc_sink itself is just a convenience dc
    provides, and DC should be perfectly capable of programming hardware
    without any dc_sinks (for example, emulated sinks).
    
    [how]
    Phase 1:
    Deprecate use of dc_sink pointer in dc_stream.  Most references are trivial
    to remove, but some call sites are risky (such as is_timing_changed) with
    no obvious logical replacement.  These will be removed in follow up change.
    
    Add dc_link pointer to dc_stream.  This is the typical reason DC really needed
    sink pointer, and most call sites are replaced with this.
    
    DMs also need minor updates, as all 3 DMs leverage stream->sink for
    some functionality.  this is replaced instead by a pointer to private data
    inside dc_stream_state, which is used by DMs as a quality of life improvment
    for some key functionality.  it allows DMs to set pointers have to their own objects
    which associate OS objects to dc_stream_states (such as DisplayTarget
    and amdgpu_dm_connector).  Without the private pointer, DMs would be
    forced to perform a lookup for callbacks.
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: David Francis <David.Francis@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 43e4a2be0fa6..cd88ceca5c41 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -961,7 +961,7 @@ bool dcn_validate_bandwidth(
 		v->dcc_rate[input_idx] = 1; /*TODO: Worst case? does this change?*/
 		v->output_format[input_idx] = pipe->stream->timing.pixel_encoding ==
 				PIXEL_ENCODING_YCBCR420 ? dcn_bw_420 : dcn_bw_444;
-		v->output[input_idx] = pipe->stream->sink->sink_signal ==
+		v->output[input_idx] = pipe->stream->signal ==
 				SIGNAL_TYPE_HDMI_TYPE_A ? dcn_bw_hdmi : dcn_bw_dp;
 		v->output_deep_color[input_idx] = dcn_bw_encoder_8bpc;
 		if (v->output[input_idx] == dcn_bw_hdmi) {

commit ba7b267a458e133cbd791c818fb4cb41180242c6
Author: Fatemeh Darbehani <fatemeh.darbehani@amd.com>
Date:   Fri Oct 5 17:22:32 2018 -0400

    drm/amd/display: Retiring set_display_requirements in dm_pp_smu.h - part4
    
    [Why]
    In DCN we want direct DC to SMU calls, with minimal interference from
    pplib.
    The reason for each pp_smu interface mapping to 1 SMU message is so we
    can have the sequencing of different SMU message in DC and shared across
    different OS's.
    This will also simplify debugging as DAL owns this interaction and
    there's no confusion about division of ownership.
    
    [How]
    Part 4: Change clock units so they match the values PPLib sends to SMU.
    
    Signed-off-by: Fatemeh Darbehani <fatemeh.darbehani@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 3208188b7ed4..43e4a2be0fa6 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1423,27 +1423,27 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	ranges.num_reader_wm_sets = WM_SET_COUNT;
 	ranges.num_writer_wm_sets = WM_SET_COUNT;
 	ranges.reader_wm_sets[0].wm_inst = WM_A;
-	ranges.reader_wm_sets[0].min_drain_clk_khz = min_dcfclk_khz;
-	ranges.reader_wm_sets[0].max_drain_clk_khz = overdrive;
-	ranges.reader_wm_sets[0].min_fill_clk_khz = min_fclk_khz;
-	ranges.reader_wm_sets[0].max_fill_clk_khz = overdrive;
+	ranges.reader_wm_sets[0].min_drain_clk_mhz = min_dcfclk_khz / 1000;
+	ranges.reader_wm_sets[0].max_drain_clk_mhz = overdrive / 1000;
+	ranges.reader_wm_sets[0].min_fill_clk_mhz = min_fclk_khz / 1000;
+	ranges.reader_wm_sets[0].max_fill_clk_mhz = overdrive / 1000;
 	ranges.writer_wm_sets[0].wm_inst = WM_A;
-	ranges.writer_wm_sets[0].min_fill_clk_khz = socclk_khz;
-	ranges.writer_wm_sets[0].max_fill_clk_khz = overdrive;
-	ranges.writer_wm_sets[0].min_drain_clk_khz = min_fclk_khz;
-	ranges.writer_wm_sets[0].max_drain_clk_khz = overdrive;
+	ranges.writer_wm_sets[0].min_fill_clk_mhz = socclk_khz / 1000;
+	ranges.writer_wm_sets[0].max_fill_clk_mhz = overdrive / 1000;
+	ranges.writer_wm_sets[0].min_drain_clk_mhz = min_fclk_khz / 1000;
+	ranges.writer_wm_sets[0].max_drain_clk_mhz = overdrive / 1000;
 
 	if (dc->debug.pplib_wm_report_mode == WM_REPORT_OVERRIDE) {
 		ranges.reader_wm_sets[0].wm_inst = WM_A;
-		ranges.reader_wm_sets[0].min_drain_clk_khz = 300000;
-		ranges.reader_wm_sets[0].max_drain_clk_khz = 5000000;
-		ranges.reader_wm_sets[0].min_fill_clk_khz = 800000;
-		ranges.reader_wm_sets[0].max_fill_clk_khz = 5000000;
+		ranges.reader_wm_sets[0].min_drain_clk_mhz = 300;
+		ranges.reader_wm_sets[0].max_drain_clk_mhz = 5000;
+		ranges.reader_wm_sets[0].min_fill_clk_mhz = 800;
+		ranges.reader_wm_sets[0].max_fill_clk_mhz = 5000;
 		ranges.writer_wm_sets[0].wm_inst = WM_A;
-		ranges.writer_wm_sets[0].min_fill_clk_khz = 200000;
-		ranges.writer_wm_sets[0].max_fill_clk_khz = 5000000;
-		ranges.writer_wm_sets[0].min_drain_clk_khz = 800000;
-		ranges.writer_wm_sets[0].max_drain_clk_khz = 5000000;
+		ranges.writer_wm_sets[0].min_fill_clk_mhz = 200;
+		ranges.writer_wm_sets[0].max_fill_clk_mhz = 5000;
+		ranges.writer_wm_sets[0].min_drain_clk_mhz = 800;
+		ranges.writer_wm_sets[0].max_drain_clk_mhz = 5000;
 	}
 
 	ranges.reader_wm_sets[1] = ranges.writer_wm_sets[0];

commit fb2b1ea325b411262031435c951d189a75de94b9
Author: Su Sung Chung <su.chung@amd.com>
Date:   Fri Sep 7 16:51:42 2018 -0400

    drm/amd/display: program v_update and v_ready with proper field
    
    [WHY]
    There are two different variables used to calculate v_update and v_ready,
    one for validation and the other for performance parameter calculation.
    Before the variable for validation was used which caused underflow on
    1080edp with vsr enabled
    
    [HOW]
    program v_update and v_ready with the variables for performance parameter
    calculation
    
    Signed-off-by: Su Sung Chung <su.chung@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 80ec09eef44f..3208188b7ed4 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1096,9 +1096,9 @@ bool dcn_validate_bandwidth(
 			if (pipe->top_pipe && pipe->top_pipe->plane_state == pipe->plane_state)
 				continue;
 
-			pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
-			pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
-			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+			pipe->pipe_dlg_param.vupdate_width = v->v_update_width_pix[input_idx];
+			pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset_pix[input_idx];
+			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset_pix[input_idx];
 			pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
 			pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;
@@ -1137,9 +1137,9 @@ bool dcn_validate_bandwidth(
 					 TIMING_3D_FORMAT_SIDE_BY_SIDE))) {
 					if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) {
 						/* update previously split pipe */
-						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
-						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
-						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width_pix[input_idx];
+						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset_pix[input_idx];
+						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset_pix[input_idx];
 						hsplit_pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
 						hsplit_pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;

commit d77f778e59ca858e1fb1e9d4946080d689c04711
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Mon Aug 27 11:31:08 2018 -0400

    drm/amd/display: Fix 3D stereo issues.
    
    We were not providing the correct pixel clocks to DML for marks
    calculation.
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 32b34134c501..80ec09eef44f 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -852,8 +852,9 @@ bool dcn_validate_bandwidth(
 		v->v_sync_plus_back_porch[input_idx] = pipe->stream->timing.v_total
 				- v->vactive[input_idx]
 				- pipe->stream->timing.v_front_porch;
-		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
-
+		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz/1000.0;
+		if (pipe->stream->timing.timing_3d_format == TIMING_3D_FORMAT_HW_FRAME_PACKING)
+			v->pixel_clock[input_idx] *= 2;
 		if (!pipe->plane_state) {
 			v->dcc_enable[input_idx] = dcn_bw_yes;
 			v->source_pixel_format[input_idx] = dcn_bw_rgb_sub_32;

commit 265f5ba6c209875081da7c5f7affe8c2c1913a75
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Mon Jul 16 10:40:31 2018 -0400

    drm/amd/display: Move PME to function pointer call semantics
    
    [why]
    Legacy IRI style is not linux friendly.
    
    [how]
    New function pointer call
    semantics will be used for all future PPLIB/DAL interfaces, and also
    some existing will be refactored.  This change defines how the
    new function pointer structures will look, as well as implements
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index bd039322f697..32b34134c501 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -37,6 +37,13 @@
 
 #define DC_LOGGER \
 	dc->ctx->logger
+
+#define WM_SET_COUNT 4
+#define WM_A 0
+#define WM_B 1
+#define WM_C 2
+#define WM_D 3
+
 /*
  * NOTE:
  *   This file is gcc-parseable HW gospel, coming straight from HW engineers.

commit cfd84fd36531b2f1de01b3530b6953ed34ed2c95
Author: Jun Lei <Jun.Lei@amd.com>
Date:   Thu Jul 12 10:35:01 2018 -0400

    drm/amd/display: separate dc_debug into dc_debug_options and dc_debug data
    
    [why]
    confusing as to which part of debug is informational, and which part causes behavioral change
    
    Signed-off-by: Jun Lei <Jun.Lei@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Leo Li <sunpeng.li@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 080f777d705e..bd039322f697 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -676,7 +676,7 @@ static void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
 }
 
 static void hack_bounding_box(struct dcn_bw_internal_vars *v,
-		struct dc_debug *dbg,
+		struct dc_debug_options *dbg,
 		struct dc_state *context)
 {
 	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID)

commit 30cdbfaa6aa469347db7fcda5949f1ccf7559ecf
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jun 13 13:58:14 2018 -0400

    drm/amd/display: dcc always on for bw calculations on raven
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index e44b8d3d6891..080f777d705e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -250,7 +250,24 @@ static void pipe_ctx_to_e2e_pipe_params (
 	else if (pipe->bottom_pipe != NULL && pipe->bottom_pipe->plane_state == pipe->plane_state)
 		input->src.is_hsplit = true;
 
-	input->src.dcc                 = pipe->plane_state->dcc.enable;
+	if (pipe->plane_res.dpp->ctx->dc->debug.optimized_watermark) {
+		/*
+		 * this method requires us to always re-calculate watermark when dcc change
+		 * between flip.
+		 */
+		input->src.dcc = pipe->plane_state->dcc.enable ? 1 : 0;
+	} else {
+		/*
+		 * allow us to disable dcc on the fly without re-calculating WM
+		 *
+		 * extra overhead for DCC is quite small.  for 1080p WM without
+		 * DCC is only 0.417us lower (urgent goes from 6.979us to 6.562us)
+		 */
+		unsigned int bpe;
+
+		input->src.dcc = pipe->plane_res.dpp->ctx->dc->res_pool->hubbub->funcs->
+			dcc_support_pixel_format(pipe->plane_state->format, &bpe) ? 1 : 0;
+	}
 	input->src.dcc_rate            = 1;
 	input->src.meta_pitch          = pipe->plane_state->dcc.grph.meta_pitch;
 	input->src.source_scan         = dm_horz;

commit f3efec54ed6ac05ba63be1bd93bd741d41b6eb37
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Fri Jun 8 17:36:26 2018 -0400

    drm/amd/display: Allow option to use worst-case watermark
    
    use worse case watermark (consider both DCC and VM)
    to keep golden consistent regardless of DCC
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Aric Cyr <Aric.Cyr@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 12261fbc25e0..e44b8d3d6891 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -31,6 +31,8 @@
 
 #include "resource.h"
 #include "dcn10/dcn10_resource.h"
+#include "dcn10/dcn10_hubbub.h"
+
 #include "dcn_calc_math.h"
 
 #define DC_LOGGER \
@@ -889,7 +891,26 @@ bool dcn_validate_bandwidth(
 				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dc_fixpt_one.value
 					|| v->scaler_rec_out_width[input_idx] == v->viewport_height[input_idx]);
 			}
-			v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;
+
+			if (dc->debug.optimized_watermark) {
+				/*
+				 * this method requires us to always re-calculate watermark when dcc change
+				 * between flip.
+				 */
+				v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;
+			} else {
+				/*
+				 * allow us to disable dcc on the fly without re-calculating WM
+				 *
+				 * extra overhead for DCC is quite small.  for 1080p WM without
+				 * DCC is only 0.417us lower (urgent goes from 6.979us to 6.562us)
+				 */
+				unsigned int bpe;
+
+				v->dcc_enable[input_idx] = dc->res_pool->hubbub->funcs->dcc_support_pixel_format(
+						pipe->plane_state->format, &bpe) ? dcn_bw_yes : dcn_bw_no;
+			}
+
 			v->source_pixel_format[input_idx] = tl_pixel_format_to_bw_defs(
 					pipe->plane_state->format);
 			v->source_surface_mode[input_idx] = tl_sw_mode_to_bw_defs(

commit 69d6bb171fe15f564adfbc485e77dd8c5ca953d5
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Jun 5 07:19:08 2018 -0400

    drm/amd/display: remove dcn1 watermark sets b, c and d
    
    Currently dcn1 will not switch between watermark sets so we can
    save time by not calculating 3 extra sets.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8dc0773b285e..12261fbc25e0 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -505,6 +505,7 @@ static void split_stream_across_pipes(
 	resource_build_scaling_params(secondary_pipe);
 }
 
+#if 0
 static void calc_wm_sets_and_perf_params(
 		struct dc_state *context,
 		struct dcn_bw_internal_vars *v)
@@ -586,6 +587,7 @@ static void calc_wm_sets_and_perf_params(
 	if (v->voltage_level >= 3)
 		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
 }
+#endif
 
 static bool dcn_bw_apply_registry_override(struct dc *dc)
 {
@@ -980,7 +982,24 @@ bool dcn_validate_bandwidth(
 				bw_consumed = v->fabric_and_dram_bandwidth;
 
 		display_pipe_configuration(v);
-		calc_wm_sets_and_perf_params(context, v);
+		/*calc_wm_sets_and_perf_params(context, v);*/
+		/* Only 1 set is used by dcn since no noticeable
+		 * performance improvement was measured and due to hw bug DEGVIDCN10-254
+		 */
+		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
+
+		context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
+			v->stutter_exit_watermark * 1000;
+		context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+				v->stutter_enter_plus_exit_watermark * 1000;
+		context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
+				v->dram_clock_change_watermark * 1000;
+		context->bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
+		context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
+		context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
+		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
+
 		context->bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
 		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65) {

commit 33a6a7eb8014cb7089570534ef4d502efe4372ed
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jun 6 13:19:39 2018 -0400

    drm/amd/display: fix dcn1 watermark range reporting
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index ac4451adeec9..8dc0773b285e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1335,21 +1335,14 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
 	struct pp_smu_funcs_rv *pp = dc->res_pool->pp_smu;
 	struct pp_smu_wm_range_sets ranges = {0};
-	int max_fclk_khz, nom_fclk_khz, mid_fclk_khz, min_fclk_khz;
-	int max_dcfclk_khz, min_dcfclk_khz;
-	int socclk_khz;
+	int min_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
-	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
 	if (!pp->set_wm_ranges)
 		return;
 
 	kernel_fpu_begin();
-	max_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
-	nom_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
-	mid_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 * 1000000 / factor;
 	min_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
-	max_dcfclk_khz = dc->dcn_soc->dcfclkv_max0p9 * 1000;
 	min_dcfclk_khz = dc->dcn_soc->dcfclkv_min0p65 * 1000;
 	socclk_khz = dc->dcn_soc->socclk * 1000;
 	kernel_fpu_end();
@@ -1357,7 +1350,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	/* Now notify PPLib/SMU about which Watermarks sets they should select
 	 * depending on DPM state they are in. And update BW MGR GFX Engine and
 	 * Memory clock member variables for Watermarks calculations for each
-	 * Watermark Set
+	 * Watermark Set. Only one watermark set for dcn1 due to hw bug DEGVIDCN10-254.
 	 */
 	/* SOCCLK does not affect anytihng but writeback for DCN so for now we dont
 	 * care what the value is, hence min to overdrive level
@@ -1366,96 +1359,37 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	ranges.num_writer_wm_sets = WM_SET_COUNT;
 	ranges.reader_wm_sets[0].wm_inst = WM_A;
 	ranges.reader_wm_sets[0].min_drain_clk_khz = min_dcfclk_khz;
-	ranges.reader_wm_sets[0].max_drain_clk_khz = max_dcfclk_khz;
+	ranges.reader_wm_sets[0].max_drain_clk_khz = overdrive;
 	ranges.reader_wm_sets[0].min_fill_clk_khz = min_fclk_khz;
-	ranges.reader_wm_sets[0].max_fill_clk_khz = min_fclk_khz;
+	ranges.reader_wm_sets[0].max_fill_clk_khz = overdrive;
 	ranges.writer_wm_sets[0].wm_inst = WM_A;
 	ranges.writer_wm_sets[0].min_fill_clk_khz = socclk_khz;
 	ranges.writer_wm_sets[0].max_fill_clk_khz = overdrive;
 	ranges.writer_wm_sets[0].min_drain_clk_khz = min_fclk_khz;
-	ranges.writer_wm_sets[0].max_drain_clk_khz = min_fclk_khz;
-
-	ranges.reader_wm_sets[1].wm_inst = WM_B;
-	ranges.reader_wm_sets[1].min_drain_clk_khz = min_fclk_khz;
-	ranges.reader_wm_sets[1].max_drain_clk_khz = max_dcfclk_khz;
-	ranges.reader_wm_sets[1].min_fill_clk_khz = mid_fclk_khz;
-	ranges.reader_wm_sets[1].max_fill_clk_khz = mid_fclk_khz;
-	ranges.writer_wm_sets[1].wm_inst = WM_B;
-	ranges.writer_wm_sets[1].min_fill_clk_khz = socclk_khz;
-	ranges.writer_wm_sets[1].max_fill_clk_khz = overdrive;
-	ranges.writer_wm_sets[1].min_drain_clk_khz = mid_fclk_khz;
-	ranges.writer_wm_sets[1].max_drain_clk_khz = mid_fclk_khz;
-
-
-	ranges.reader_wm_sets[2].wm_inst = WM_C;
-	ranges.reader_wm_sets[2].min_drain_clk_khz = min_fclk_khz;
-	ranges.reader_wm_sets[2].max_drain_clk_khz = max_dcfclk_khz;
-	ranges.reader_wm_sets[2].min_fill_clk_khz = nom_fclk_khz;
-	ranges.reader_wm_sets[2].max_fill_clk_khz = nom_fclk_khz;
-	ranges.writer_wm_sets[2].wm_inst = WM_C;
-	ranges.writer_wm_sets[2].min_fill_clk_khz = socclk_khz;
-	ranges.writer_wm_sets[2].max_fill_clk_khz = overdrive;
-	ranges.writer_wm_sets[2].min_drain_clk_khz = nom_fclk_khz;
-	ranges.writer_wm_sets[2].max_drain_clk_khz = nom_fclk_khz;
-
-	ranges.reader_wm_sets[3].wm_inst = WM_D;
-	ranges.reader_wm_sets[3].min_drain_clk_khz = min_fclk_khz;
-	ranges.reader_wm_sets[3].max_drain_clk_khz = max_dcfclk_khz;
-	ranges.reader_wm_sets[3].min_fill_clk_khz = max_fclk_khz;
-	ranges.reader_wm_sets[3].max_fill_clk_khz = max_fclk_khz;
-	ranges.writer_wm_sets[3].wm_inst = WM_D;
-	ranges.writer_wm_sets[3].min_fill_clk_khz = socclk_khz;
-	ranges.writer_wm_sets[3].max_fill_clk_khz = overdrive;
-	ranges.writer_wm_sets[3].min_drain_clk_khz = max_fclk_khz;
-	ranges.writer_wm_sets[3].max_drain_clk_khz = max_fclk_khz;
+	ranges.writer_wm_sets[0].max_drain_clk_khz = overdrive;
 
 	if (dc->debug.pplib_wm_report_mode == WM_REPORT_OVERRIDE) {
 		ranges.reader_wm_sets[0].wm_inst = WM_A;
 		ranges.reader_wm_sets[0].min_drain_clk_khz = 300000;
-		ranges.reader_wm_sets[0].max_drain_clk_khz = 654000;
+		ranges.reader_wm_sets[0].max_drain_clk_khz = 5000000;
 		ranges.reader_wm_sets[0].min_fill_clk_khz = 800000;
-		ranges.reader_wm_sets[0].max_fill_clk_khz = 800000;
+		ranges.reader_wm_sets[0].max_fill_clk_khz = 5000000;
 		ranges.writer_wm_sets[0].wm_inst = WM_A;
 		ranges.writer_wm_sets[0].min_fill_clk_khz = 200000;
-		ranges.writer_wm_sets[0].max_fill_clk_khz = 757000;
+		ranges.writer_wm_sets[0].max_fill_clk_khz = 5000000;
 		ranges.writer_wm_sets[0].min_drain_clk_khz = 800000;
-		ranges.writer_wm_sets[0].max_drain_clk_khz = 800000;
-
-		ranges.reader_wm_sets[1].wm_inst = WM_B;
-		ranges.reader_wm_sets[1].min_drain_clk_khz = 300000;
-		ranges.reader_wm_sets[1].max_drain_clk_khz = 654000;
-		ranges.reader_wm_sets[1].min_fill_clk_khz = 933000;
-		ranges.reader_wm_sets[1].max_fill_clk_khz = 933000;
-		ranges.writer_wm_sets[1].wm_inst = WM_B;
-		ranges.writer_wm_sets[1].min_fill_clk_khz = 200000;
-		ranges.writer_wm_sets[1].max_fill_clk_khz = 757000;
-		ranges.writer_wm_sets[1].min_drain_clk_khz = 933000;
-		ranges.writer_wm_sets[1].max_drain_clk_khz = 933000;
-
-
-		ranges.reader_wm_sets[2].wm_inst = WM_C;
-		ranges.reader_wm_sets[2].min_drain_clk_khz = 300000;
-		ranges.reader_wm_sets[2].max_drain_clk_khz = 654000;
-		ranges.reader_wm_sets[2].min_fill_clk_khz = 1067000;
-		ranges.reader_wm_sets[2].max_fill_clk_khz = 1067000;
-		ranges.writer_wm_sets[2].wm_inst = WM_C;
-		ranges.writer_wm_sets[2].min_fill_clk_khz = 200000;
-		ranges.writer_wm_sets[2].max_fill_clk_khz = 757000;
-		ranges.writer_wm_sets[2].min_drain_clk_khz = 1067000;
-		ranges.writer_wm_sets[2].max_drain_clk_khz = 1067000;
-
-		ranges.reader_wm_sets[3].wm_inst = WM_D;
-		ranges.reader_wm_sets[3].min_drain_clk_khz = 300000;
-		ranges.reader_wm_sets[3].max_drain_clk_khz = 654000;
-		ranges.reader_wm_sets[3].min_fill_clk_khz = 1200000;
-		ranges.reader_wm_sets[3].max_fill_clk_khz = 1200000;
-		ranges.writer_wm_sets[3].wm_inst = WM_D;
-		ranges.writer_wm_sets[3].min_fill_clk_khz = 200000;
-		ranges.writer_wm_sets[3].max_fill_clk_khz = 757000;
-		ranges.writer_wm_sets[3].min_drain_clk_khz = 1200000;
-		ranges.writer_wm_sets[3].max_drain_clk_khz = 1200000;
+		ranges.writer_wm_sets[0].max_drain_clk_khz = 5000000;
 	}
 
+	ranges.reader_wm_sets[1] = ranges.writer_wm_sets[0];
+	ranges.reader_wm_sets[1].wm_inst = WM_B;
+
+	ranges.reader_wm_sets[2] = ranges.writer_wm_sets[0];
+	ranges.reader_wm_sets[2].wm_inst = WM_C;
+
+	ranges.reader_wm_sets[3] = ranges.writer_wm_sets[0];
+	ranges.reader_wm_sets[3].wm_inst = WM_D;
+
 	/* Notify PP Lib/SMU which Watermarks to use for which clock ranges */
 	pp->set_wm_ranges(&pp->pp_smu, &ranges);
 }

commit b9c1c67aeb8805015f02efb0a6a6ddf68ea8a4b4
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Jun 5 12:54:38 2018 -0400

    drm/amd/display: clean rq/dlg/ttu reg structs before calculations
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index b8195e5a0676..ac4451adeec9 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -423,6 +423,10 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	int total_flip_bytes = 0;
 	int i;
 
+	memset(dlg_regs, 0, sizeof(*dlg_regs));
+	memset(ttu_regs, 0, sizeof(*ttu_regs));
+	memset(rq_regs, 0, sizeof(*rq_regs));
+
 	for (i = 0; i < number_of_planes; i++) {
 		total_active_bw += v->read_bandwidth[i];
 		total_prefetch_bw += v->prefetch_bandwidth[i];

commit d578839ca014e5f4e6d540caadc0b84d50a8977f
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed May 23 18:02:27 2018 -0400

    drm/amd/display: get rid of cur_clks from dcn_bw_output
    
    Cleans up dcn_bw_output to only contain calculated info,
    actual programmed values will now be stored in respective blocks.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Nikola Cornij <Nikola.Cornij@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 9ce329e8f287..b8195e5a0676 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -977,42 +977,42 @@ bool dcn_validate_bandwidth(
 
 		display_pipe_configuration(v);
 		calc_wm_sets_and_perf_params(context, v);
-		context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 /
+		context->bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
 		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65) {
-			context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
+			context->bw.dcn.clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
 		}
 
-		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
-		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
+		context->bw.dcn.clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
+		context->bw.dcn.clk.dcfclk_khz = (int)(v->dcfclk * 1000);
 
-		context->bw.dcn.calc_clk.dispclk_khz = (int)(v->dispclk * 1000);
+		context->bw.dcn.clk.dispclk_khz = (int)(v->dispclk * 1000);
 		if (dc->debug.max_disp_clk == true)
-			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
+			context->bw.dcn.clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
 
-		if (context->bw.dcn.calc_clk.dispclk_khz <
+		if (context->bw.dcn.clk.dispclk_khz <
 				dc->debug.min_disp_clk_khz) {
-			context->bw.dcn.calc_clk.dispclk_khz =
+			context->bw.dcn.clk.dispclk_khz =
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw.dcn.calc_clk.dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
-		context->bw.dcn.calc_clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
+		context->bw.dcn.clk.dppclk_khz = context->bw.dcn.clk.dispclk_khz / v->dispclk_dppclk_ratio;
+		context->bw.dcn.clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
 		switch (v->voltage_level) {
 		case 0:
-			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+			context->bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmin0p65 * 1000);
 			break;
 		case 1:
-			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+			context->bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmid0p72 * 1000);
 			break;
 		case 2:
-			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+			context->bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vnom0p8 * 1000);
 			break;
 		default:
-			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+			context->bw.dcn.clk.max_supported_dppclk_khz =
 					(int)(dc->dcn_soc->max_dppclk_vmax0p9 * 1000);
 			break;
 		}

commit 92276a06f9c3d29183c7bdf46a6dbbc9c00f7acf
Author: Mikita Lipski <mikita.lipski@amd.com>
Date:   Wed Apr 11 14:52:41 2018 -0400

    drm/amd/display: Introduce pp-smu raven functions
    
    DM powerplay calls for DCN10 allowing to bypass PPLib
    and call directly to the SMU functions.
    
    Signed-off-by: Mikita Lipski <mikita.lipski@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 9acdd9da740e..9ce329e8f287 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1358,8 +1358,8 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	/* SOCCLK does not affect anytihng but writeback for DCN so for now we dont
 	 * care what the value is, hence min to overdrive level
 	 */
-	ranges.num_reader_wm_sets = WM_COUNT;
-	ranges.num_writer_wm_sets = WM_COUNT;
+	ranges.num_reader_wm_sets = WM_SET_COUNT;
+	ranges.num_writer_wm_sets = WM_SET_COUNT;
 	ranges.reader_wm_sets[0].wm_inst = WM_A;
 	ranges.reader_wm_sets[0].min_drain_clk_khz = min_dcfclk_khz;
 	ranges.reader_wm_sets[0].max_drain_clk_khz = max_dcfclk_khz;

commit e2e0a1dcd3229eec32ded439f69438a25ec817d6
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed May 23 17:52:04 2018 -0400

    drm/amd/display: move clock programming from set_bandwidth to dccg
    
    This change moves dcn clock programming(with exception of dispclk)
    into dccg. This should have no functional effect.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 2b70ac67e6c2..9acdd9da740e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -997,7 +997,7 @@ bool dcn_validate_bandwidth(
 		}
 
 		context->bw.dcn.calc_clk.dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
-
+		context->bw.dcn.calc_clk.phyclk_khz = v->phyclk_per_state[v->voltage_level];
 		switch (v->voltage_level) {
 		case 0:
 			context->bw.dcn.calc_clk.max_supported_dppclk_khz =

commit fab55d61b9f04693abc6fdbc92e3fdf3872915b1
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed May 23 16:21:54 2018 -0400

    drm/amd/display: redesign dce/dcn clock voltage update request
    
    The goal of this change is to move clock programming and voltage
    requests to a single function. As of this change only dce is affected.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index d8a31650e856..2b70ac67e6c2 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1145,10 +1145,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 
 	switch (clocks_type) {
 	case DM_PP_CLOCK_TYPE_DISPLAY_CLK:
-		/*if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmax0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
-			//BREAK_TO_DEBUGGER();
-		} else*/ if (clocks_in_khz > dc->dcn_soc->max_dispclk_vnom0p8*1000) {
+			BREAK_TO_DEBUGGER();
+		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1158,10 +1158,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 			vdd_level = dcn_bw_v_min0p65;
 		break;
 	case DM_PP_CLOCK_TYPE_DISPLAYPHYCLK:
-		/*if (clocks_in_khz > dc->dcn_soc->phyclkv_max0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->phyclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else*/ if (clocks_in_khz > dc->dcn_soc->phyclkv_nom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1172,10 +1172,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		break;
 
 	case DM_PP_CLOCK_TYPE_DPPCLK:
-		/*if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmax0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else*/ if (clocks_in_khz > dc->dcn_soc->max_dppclk_vnom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1189,10 +1189,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		{
 			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
-			/*if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
+			if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p91;
 				BREAK_TO_DEBUGGER();
-			} else */if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
+			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p9;
 			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72*1000000/factor) {
 				vdd_level = dcn_bw_v_nom0p8;
@@ -1204,10 +1204,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		break;
 
 	case DM_PP_CLOCK_TYPE_DCFCLK:
-		/*if (clocks_in_khz > dc->dcn_soc->dcfclkv_max0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->dcfclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else */if (clocks_in_khz > dc->dcn_soc->dcfclkv_nom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;

commit 765b26836430e9d9ebef95fced42dd167b4ccad6
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed May 23 13:16:50 2018 -0400

    drm/amd/display: replace clocks_value struct with dc_clocks
    
    This will avoid structs with duplicate information. Also
    removes pixel clock voltage request. This has no effect since
    pixel clock does not affect dcn voltage and this function only
    matters for dcn.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 49a4ea45466d..d8a31650e856 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1145,10 +1145,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 
 	switch (clocks_type) {
 	case DM_PP_CLOCK_TYPE_DISPLAY_CLK:
-		if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmax0p9*1000) {
+		/*if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
-			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vnom0p8*1000) {
+			//BREAK_TO_DEBUGGER();
+		} else*/ if (clocks_in_khz > dc->dcn_soc->max_dispclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1158,10 +1158,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 			vdd_level = dcn_bw_v_min0p65;
 		break;
 	case DM_PP_CLOCK_TYPE_DISPLAYPHYCLK:
-		if (clocks_in_khz > dc->dcn_soc->phyclkv_max0p9*1000) {
+		/*if (clocks_in_khz > dc->dcn_soc->phyclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_nom0p8*1000) {
+		} else*/ if (clocks_in_khz > dc->dcn_soc->phyclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1172,10 +1172,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		break;
 
 	case DM_PP_CLOCK_TYPE_DPPCLK:
-		if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmax0p9*1000) {
+		/*if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vnom0p8*1000) {
+		} else*/ if (clocks_in_khz > dc->dcn_soc->max_dppclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1189,10 +1189,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		{
 			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
-			if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
+			/*if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p91;
 				BREAK_TO_DEBUGGER();
-			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
+			} else */if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p9;
 			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72*1000000/factor) {
 				vdd_level = dcn_bw_v_nom0p8;
@@ -1204,10 +1204,10 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 		break;
 
 	case DM_PP_CLOCK_TYPE_DCFCLK:
-		if (clocks_in_khz > dc->dcn_soc->dcfclkv_max0p9*1000) {
+		/*if (clocks_in_khz > dc->dcn_soc->dcfclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_nom0p8*1000) {
+		} else */if (clocks_in_khz > dc->dcn_soc->dcfclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
 		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
@@ -1225,27 +1225,27 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 
 unsigned int dcn_find_dcfclk_suits_all(
 	const struct dc *dc,
-	struct clocks_value *clocks)
+	struct dc_clocks *clocks)
 {
 	unsigned vdd_level, vdd_level_temp;
 	unsigned dcf_clk;
 
 	/*find a common supported voltage level*/
 	vdd_level = dcn_find_normalized_clock_vdd_Level(
-		dc, DM_PP_CLOCK_TYPE_DISPLAY_CLK, clocks->dispclk_in_khz);
+		dc, DM_PP_CLOCK_TYPE_DISPLAY_CLK, clocks->dispclk_khz);
 	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
-		dc, DM_PP_CLOCK_TYPE_DISPLAYPHYCLK, clocks->phyclk_in_khz);
+		dc, DM_PP_CLOCK_TYPE_DISPLAYPHYCLK, clocks->phyclk_khz);
 
 	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
 	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
-		dc, DM_PP_CLOCK_TYPE_DPPCLK, clocks->dppclk_in_khz);
+		dc, DM_PP_CLOCK_TYPE_DPPCLK, clocks->dppclk_khz);
 	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
 
 	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
-		dc, DM_PP_CLOCK_TYPE_MEMORY_CLK, clocks->dcfclock_in_khz);
+		dc, DM_PP_CLOCK_TYPE_MEMORY_CLK, clocks->fclk_khz);
 	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
 	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
-		dc, DM_PP_CLOCK_TYPE_DCFCLK, clocks->dcfclock_in_khz);
+		dc, DM_PP_CLOCK_TYPE_DCFCLK, clocks->dcfclk_khz);
 
 	/*find that level conresponding dcfclk*/
 	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);

commit eb0e515464e4a1be730c7ac7a01c3ba04c98ea97
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Apr 18 11:37:53 2018 -0400

    drm/amd/display: get rid of 32.32 unsigned fixed point
    
    32.32 is redundant, 31.32 does everything we use 32.32 for
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a102c192328d..49a4ea45466d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -873,14 +873,14 @@ bool dcn_validate_bandwidth(
 			}
 
 			if (pipe->plane_state->rotation % 2 == 0) {
-				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dal_fixed31_32_one.value
+				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dc_fixpt_one.value
 					|| v->scaler_rec_out_width[input_idx] == v->viewport_width[input_idx]);
-				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dal_fixed31_32_one.value
+				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dc_fixpt_one.value
 					|| v->scaler_recout_height[input_idx] == v->viewport_height[input_idx]);
 			} else {
-				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dal_fixed31_32_one.value
+				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dc_fixpt_one.value
 					|| v->scaler_recout_height[input_idx] == v->viewport_width[input_idx]);
-				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dal_fixed31_32_one.value
+				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dc_fixpt_one.value
 					|| v->scaler_rec_out_width[input_idx] == v->viewport_height[input_idx]);
 			}
 			v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;

commit 3032deb52a6bf706657c39d6335c81ce3265974d
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Wed Mar 14 11:19:15 2018 -0400

    drm/amd/display: Correct print types in DC_LOGS
    
    Correct the types used for printing in logs. This is needed for adding
    dynamic printing (LINUX), otherwise we get warnings.
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 4bb43a371292..a102c192328d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1459,39 +1459,39 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 {
 	kernel_fpu_begin();
-	DC_LOG_BANDWIDTH_CALCS("sr_exit_time: %d ns\n"
-			"sr_enter_plus_exit_time: %d ns\n"
-			"urgent_latency: %d ns\n"
-			"write_back_latency: %d ns\n"
-			"percent_of_ideal_drambw_received_after_urg_latency: %d %\n"
+	DC_LOG_BANDWIDTH_CALCS("sr_exit_time: %f ns\n"
+			"sr_enter_plus_exit_time: %f ns\n"
+			"urgent_latency: %f ns\n"
+			"write_back_latency: %f ns\n"
+			"percent_of_ideal_drambw_received_after_urg_latency: %f %%\n"
 			"max_request_size: %d bytes\n"
-			"dcfclkv_max0p9: %d kHz\n"
-			"dcfclkv_nom0p8: %d kHz\n"
-			"dcfclkv_mid0p72: %d kHz\n"
-			"dcfclkv_min0p65: %d kHz\n"
-			"max_dispclk_vmax0p9: %d kHz\n"
-			"max_dispclk_vnom0p8: %d kHz\n"
-			"max_dispclk_vmid0p72: %d kHz\n"
-			"max_dispclk_vmin0p65: %d kHz\n"
-			"max_dppclk_vmax0p9: %d kHz\n"
-			"max_dppclk_vnom0p8: %d kHz\n"
-			"max_dppclk_vmid0p72: %d kHz\n"
-			"max_dppclk_vmin0p65: %d kHz\n"
-			"socclk: %d kHz\n"
-			"fabric_and_dram_bandwidth_vmax0p9: %d MB/s\n"
-			"fabric_and_dram_bandwidth_vnom0p8: %d MB/s\n"
-			"fabric_and_dram_bandwidth_vmid0p72: %d MB/s\n"
-			"fabric_and_dram_bandwidth_vmin0p65: %d MB/s\n"
-			"phyclkv_max0p9: %d kHz\n"
-			"phyclkv_nom0p8: %d kHz\n"
-			"phyclkv_mid0p72: %d kHz\n"
-			"phyclkv_min0p65: %d kHz\n"
-			"downspreading: %d %\n"
+			"dcfclkv_max0p9: %f kHz\n"
+			"dcfclkv_nom0p8: %f kHz\n"
+			"dcfclkv_mid0p72: %f kHz\n"
+			"dcfclkv_min0p65: %f kHz\n"
+			"max_dispclk_vmax0p9: %f kHz\n"
+			"max_dispclk_vnom0p8: %f kHz\n"
+			"max_dispclk_vmid0p72: %f kHz\n"
+			"max_dispclk_vmin0p65: %f kHz\n"
+			"max_dppclk_vmax0p9: %f kHz\n"
+			"max_dppclk_vnom0p8: %f kHz\n"
+			"max_dppclk_vmid0p72: %f kHz\n"
+			"max_dppclk_vmin0p65: %f kHz\n"
+			"socclk: %f kHz\n"
+			"fabric_and_dram_bandwidth_vmax0p9: %f MB/s\n"
+			"fabric_and_dram_bandwidth_vnom0p8: %f MB/s\n"
+			"fabric_and_dram_bandwidth_vmid0p72: %f MB/s\n"
+			"fabric_and_dram_bandwidth_vmin0p65: %f MB/s\n"
+			"phyclkv_max0p9: %f kHz\n"
+			"phyclkv_nom0p8: %f kHz\n"
+			"phyclkv_mid0p72: %f kHz\n"
+			"phyclkv_min0p65: %f kHz\n"
+			"downspreading: %f %%\n"
 			"round_trip_ping_latency_cycles: %d DCFCLK Cycles\n"
 			"urgent_out_of_order_return_per_channel: %d Bytes\n"
 			"number_of_channels: %d\n"
 			"vmm_page_size: %d Bytes\n"
-			"dram_clock_change_latency: %d ns\n"
+			"dram_clock_change_latency: %f ns\n"
 			"return_bus_width: %d Bytes\n",
 			dc->dcn_soc->sr_exit_time * 1000,
 			dc->dcn_soc->sr_enter_plus_exit_time * 1000,
@@ -1527,11 +1527,11 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 			dc->dcn_soc->vmm_page_size,
 			dc->dcn_soc->dram_clock_change_latency * 1000,
 			dc->dcn_soc->return_bus_width);
-	DC_LOG_BANDWIDTH_CALCS("rob_buffer_size_in_kbyte: %d\n"
-			"det_buffer_size_in_kbyte: %d\n"
-			"dpp_output_buffer_pixels: %d\n"
-			"opp_output_buffer_lines: %d\n"
-			"pixel_chunk_size_in_kbyte: %d\n"
+	DC_LOG_BANDWIDTH_CALCS("rob_buffer_size_in_kbyte: %f\n"
+			"det_buffer_size_in_kbyte: %f\n"
+			"dpp_output_buffer_pixels: %f\n"
+			"opp_output_buffer_lines: %f\n"
+			"pixel_chunk_size_in_kbyte: %f\n"
 			"pte_enable: %d\n"
 			"pte_chunk_size: %d kbytes\n"
 			"meta_chunk_size: %d kbytes\n"
@@ -1550,13 +1550,13 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 			"max_pscl_tolb_throughput: %d pixels/dppclk\n"
 			"max_lb_tovscl_throughput: %d pixels/dppclk\n"
 			"max_vscl_tohscl_throughput: %d pixels/dppclk\n"
-			"max_hscl_ratio: %d\n"
-			"max_vscl_ratio: %d\n"
+			"max_hscl_ratio: %f\n"
+			"max_vscl_ratio: %f\n"
 			"max_hscl_taps: %d\n"
 			"max_vscl_taps: %d\n"
 			"pte_buffer_size_in_requests: %d\n"
-			"dispclk_ramping_margin: %d %\n"
-			"under_scan_factor: %d %\n"
+			"dispclk_ramping_margin: %f %%\n"
+			"under_scan_factor: %f %%\n"
 			"max_inter_dcn_tile_repeaters: %d\n"
 			"can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one: %d\n"
 			"bug_forcing_luma_and_chroma_request_to_same_size_fixed: %d\n"

commit 17ac50368f83ead67f9e3c9b9704f927f214da9d
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Feb 16 13:18:59 2018 -0500

    drm/amd/display: clean up dcn pplib notification call
    
    We have unused variables being populated when notifying pplib.
    This change amends that.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8bab3fec4d8d..4bb43a371292 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -983,8 +983,6 @@ bool dcn_validate_bandwidth(
 			context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
 		}
 
-		context->bw.dcn.calc_clk.dram_ccm_us = (int)(v->dram_clock_change_margin);
-		context->bw.dcn.calc_clk.min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
 		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
 		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
 
@@ -998,7 +996,7 @@ bool dcn_validate_bandwidth(
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw.dcn.calc_clk.max_dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
+		context->bw.dcn.calc_clk.dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
 
 		switch (v->voltage_level) {
 		case 0:

commit 45bb8dd696eaff9c96afd2b210f0d8cf5025dd65
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Wed Feb 28 17:14:50 2018 -0500

    drm/amd/display: Set disp clk in a safe way to avoid over high dpp clk. (v2)
    
    Increase clock, if current dpp div is 0 and request dpp div is 1, request clk is
    higher than maximum dpp clk as per dpm table.
            set dispclk to the value of maximum supported dpp clk
            set div to 1
            set dispclk to request value.
    Decrease clock, currrent dpp div is 1 and request dpp div is 0, current clk is
    higher than maximum dpp clk as per dpm table.
            set dispclk to the value of maximum supported dpp clk
            set div to 0
            set dispclk to request value.
    
    v2: squash in !DCN build fix
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8020bc7742c1..8bab3fec4d8d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1000,6 +1000,25 @@ bool dcn_validate_bandwidth(
 
 		context->bw.dcn.calc_clk.max_dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
 
+		switch (v->voltage_level) {
+		case 0:
+			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+					(int)(dc->dcn_soc->max_dppclk_vmin0p65 * 1000);
+			break;
+		case 1:
+			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+					(int)(dc->dcn_soc->max_dppclk_vmid0p72 * 1000);
+			break;
+		case 2:
+			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+					(int)(dc->dcn_soc->max_dppclk_vnom0p8 * 1000);
+			break;
+		default:
+			context->bw.dcn.calc_clk.max_supported_dppclk_khz =
+					(int)(dc->dcn_soc->max_dppclk_vmax0p9 * 1000);
+			break;
+		}
+
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 

commit 28d4175413efe4ec4e6db1a197ab66d4b89c0a93
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Feb 21 15:10:02 2018 -0500

    drm/amd/display: fix dcn1 dppclk when min dispclk patch applies
    
    Applying min dispclk patch would result in incorrect dppclk divider
    without this change
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index f1d8db56f406..8020bc7742c1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -998,7 +998,7 @@ bool dcn_validate_bandwidth(
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw.dcn.calc_clk.max_dppclk_khz = (int)(v->dppclk * 1000);
+		context->bw.dcn.calc_clk.max_dppclk_khz = context->bw.dcn.calc_clk.dispclk_khz / v->dispclk_dppclk_ratio;
 
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];

commit 1296423bf23c7a58133970e223b1f47ec6570308
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Tue Feb 20 17:42:50 2018 -0500

    drm/amd/display: define DC_LOGGER for logger
    
    Created a DC_LOGGER define. This is used to
    pass the logger into the macros.
    
    Anywhere we need to use the logger we need to define
    DC_LOGGER
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 7728c85bcb0e..f1d8db56f406 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -33,6 +33,8 @@
 #include "dcn10/dcn10_resource.h"
 #include "dcn_calc_math.h"
 
+#define DC_LOGGER \
+	dc->ctx->logger
 /*
  * NOTE:
  *   This file is gcc-parseable HW gospel, coming straight from HW engineers.
@@ -1242,8 +1244,7 @@ unsigned int dcn_find_dcfclk_suits_all(
 	else
 		dcf_clk =  dc->dcn_soc->dcfclkv_min0p65*1000;
 
-	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
-		"\tdcf_clk for voltage = %d\n", dcf_clk);
+	DC_LOG_BANDWIDTH_CALCS("\tdcf_clk for voltage = %d\n", dcf_clk);
 	return dcf_clk;
 }
 
@@ -1441,8 +1442,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 {
 	kernel_fpu_begin();
-	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
-			"sr_exit_time: %d ns\n"
+	DC_LOG_BANDWIDTH_CALCS("sr_exit_time: %d ns\n"
 			"sr_enter_plus_exit_time: %d ns\n"
 			"urgent_latency: %d ns\n"
 			"write_back_latency: %d ns\n"
@@ -1510,8 +1510,7 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 			dc->dcn_soc->vmm_page_size,
 			dc->dcn_soc->dram_clock_change_latency * 1000,
 			dc->dcn_soc->return_bus_width);
-	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
-			"rob_buffer_size_in_kbyte: %d\n"
+	DC_LOG_BANDWIDTH_CALCS("rob_buffer_size_in_kbyte: %d\n"
 			"det_buffer_size_in_kbyte: %d\n"
 			"dpp_output_buffer_pixels: %d\n"
 			"opp_output_buffer_lines: %d\n"

commit 2f3fd67a8af25f5b4d549c3e9cc515dbf1839ffc
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Fri Feb 16 13:57:42 2018 -0500

    drm/amd/display: Use MACROS instead of dm_logger
    
    Created MACROS for all log levels. Also Replaced
    usage of dm_logger_write to the defined MACROS
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index e4d8a8dbc5ef..7728c85bcb0e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1242,7 +1242,7 @@ unsigned int dcn_find_dcfclk_suits_all(
 	else
 		dcf_clk =  dc->dcn_soc->dcfclkv_min0p65*1000;
 
-	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
+	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
 		"\tdcf_clk for voltage = %d\n", dcf_clk);
 	return dcf_clk;
 }
@@ -1441,7 +1441,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 {
 	kernel_fpu_begin();
-	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
+	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
 			"sr_exit_time: %d ns\n"
 			"sr_enter_plus_exit_time: %d ns\n"
 			"urgent_latency: %d ns\n"
@@ -1510,7 +1510,7 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 			dc->dcn_soc->vmm_page_size,
 			dc->dcn_soc->dram_clock_change_latency * 1000,
 			dc->dcn_soc->return_bus_width);
-	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
+	DC_LOG_BANDWIDTH_CALCS(dc->ctx->logger,
 			"rob_buffer_size_in_kbyte: %d\n"
 			"det_buffer_size_in_kbyte: %d\n"
 			"dpp_output_buffer_pixels: %d\n"

commit f553e6810259d8bd31a4b9ac3001cfcde7f8fb7e
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Mon Feb 12 15:19:20 2018 -0500

    drm/amd/display: add per pipe dppclk
    
    v2: Fix commit title
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index c9aa686d16b9..e4d8a8dbc5ef 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -996,7 +996,7 @@ bool dcn_validate_bandwidth(
 					dc->debug.min_disp_clk_khz;
 		}
 
-		context->bw.dcn.calc_clk.dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
+		context->bw.dcn.calc_clk.max_dppclk_khz = (int)(v->dppclk * 1000);
 
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];

commit c7d76452d254fae8d89861363cfeeb3bf0c77abd
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Jan 9 16:37:03 2018 -0500

    drm/amd/display: revert to hacking bounding box for pipe split
    
    Directly editing pipe config outside of formula is error prone
    and results in higher clocks being used when splitting.
    For this reason we reverted to using bounding box hacking
    to split. Since sometimes this erroneusly results in higher dpm
    being required we unhack the bounding box and recalculate to allow
    dpm0 is possible.
    Side effect is we will lose some stutter efficiency
    in non dpm0 cases. This is not a big concern since increased stutter
    efficiency saves an order of magnitude less power than lower dpm.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index c6a7507d0ee1..c9aa686d16b9 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -626,7 +626,7 @@ static bool dcn_bw_apply_registry_override(struct dc *dc)
 	return updated;
 }
 
-void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
+static void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
 {
 	/*
 	 * disable optional pipe split by lower dispclk bounding box
@@ -635,7 +635,7 @@ void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
 	v->max_dispclk[0] = v->max_dppclk_vmin0p65;
 }
 
-void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
+static void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
 		unsigned int pixel_rate_khz)
 {
 	float pixel_rate_mhz = pixel_rate_khz / 1000;
@@ -648,25 +648,20 @@ void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
 		v->max_dppclk[0] = pixel_rate_mhz;
 }
 
-void hack_bounding_box(struct dcn_bw_internal_vars *v,
+static void hack_bounding_box(struct dcn_bw_internal_vars *v,
 		struct dc_debug *dbg,
 		struct dc_state *context)
 {
-	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID) {
+	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID)
 		hack_disable_optional_pipe_split(v);
-	}
 
 	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID_MULT_DISP &&
-		context->stream_count >= 2) {
+		context->stream_count >= 2)
 		hack_disable_optional_pipe_split(v);
-	}
 
 	if (context->stream_count == 1 &&
-			dbg->force_single_disp_pipe_split) {
-		struct dc_stream_state *stream0 = context->streams[0];
-
-		hack_force_pipe_split(v, stream0->timing.pix_clk_khz);
-	}
+			dbg->force_single_disp_pipe_split)
+		hack_force_pipe_split(v, context->streams[0]->timing.pix_clk_khz);
 }
 
 bool dcn_validate_bandwidth(
@@ -800,23 +795,10 @@ bool dcn_validate_bandwidth(
 	v->phyclk_per_state[2] = v->phyclkv_nom0p8;
 	v->phyclk_per_state[1] = v->phyclkv_mid0p72;
 	v->phyclk_per_state[0] = v->phyclkv_min0p65;
-
-	hack_bounding_box(v, &dc->debug, context);
-
-	if (v->voltage_override == dcn_bw_v_max0p9) {
-		v->voltage_override_level = number_of_states - 1;
-	} else if (v->voltage_override == dcn_bw_v_nom0p8) {
-		v->voltage_override_level = number_of_states - 2;
-	} else if (v->voltage_override == dcn_bw_v_mid0p72) {
-		v->voltage_override_level = number_of_states - 3;
-	} else {
-		v->voltage_override_level = 0;
-	}
 	v->synchronized_vblank = dcn_bw_no;
 	v->ta_pscalculation = dcn_bw_override;
 	v->allow_different_hratio_vratio = dcn_bw_yes;
 
-
 	for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 
@@ -949,8 +931,19 @@ bool dcn_validate_bandwidth(
 	v->number_of_active_planes = input_idx;
 
 	scaler_settings_calculation(v);
+
+	hack_bounding_box(v, &dc->debug, context);
+
 	mode_support_and_system_configuration(v);
 
+	/* Unhack dppclk: dont bother with trying to pipe split if we cannot maintain dpm0 */
+	if (v->voltage_level != 0
+			&& context->stream_count == 1
+			&& dc->debug.force_single_disp_pipe_split) {
+		v->max_dppclk[0] = v->max_dppclk_vmin0p65;
+		mode_support_and_system_configuration(v);
+	}
+
 	if (v->voltage_level == 0 &&
 			(dc->debug.sr_exit_time_dpm0_ns
 				|| dc->debug.sr_enter_plus_exit_time_dpm0_ns)) {

commit e07f541f50a31541f761300aa8bf6e3008ac448b
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Tue Dec 19 16:47:02 2017 -0500

    drm/amd/display: Use real BE and FE index to program regs.
    
    In case of some pipes are fused, pipe_idx should not
    be used to program pipe regs. Instead of that, BE and FE
    inst number should be used for reg index.
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 331891c2c71a..c6a7507d0ee1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -486,6 +486,7 @@ static void split_stream_across_pipes(
 	secondary_pipe->plane_res.ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.xfm = pool->transforms[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.dpp = pool->dpps[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.mpcc_inst = pool->dpps[secondary_pipe->pipe_idx]->inst;
 	if (primary_pipe->bottom_pipe) {
 		ASSERT(primary_pipe->bottom_pipe != secondary_pipe);
 		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;

commit 2961fef7058dcdb470621f0d58f17a6c4c0033e5
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Mon Dec 4 15:48:13 2017 -0500

    drm/amd/display: fix global sync param retrieval when not pipe splitting
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index c3cfd48e0423..331891c2c71a 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1014,9 +1014,9 @@ bool dcn_validate_bandwidth(
 			if (pipe->top_pipe && pipe->top_pipe->plane_state == pipe->plane_state)
 				continue;
 
-			pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
-			pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx];
-			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
+			pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+			pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
 			pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
 			pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;
@@ -1055,9 +1055,9 @@ bool dcn_validate_bandwidth(
 					 TIMING_3D_FORMAT_SIDE_BY_SIDE))) {
 					if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) {
 						/* update previously split pipe */
-						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
-						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx];
-						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
+						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
+						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx][v->dpp_per_plane[input_idx] == 2 ? 1 : 0];
 						hsplit_pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
 						hsplit_pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;

commit 5a095c405e591fabe654d343ce4d9682e29ad3af
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Nov 28 11:22:15 2017 -0500

    drm/amd/display: clean up dcn soc params
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 47dbc953a3a9..c3cfd48e0423 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1585,35 +1585,6 @@ void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 			dc->dcn_ip->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one,
 			dc->dcn_ip->bug_forcing_luma_and_chroma_request_to_same_size_fixed,
 			dc->dcn_ip->dcfclk_cstate_latency);
-	dc->dml.soc.vmin.socclk_mhz = dc->dcn_soc->socclk;
-	dc->dml.soc.vmid.socclk_mhz = dc->dcn_soc->socclk;
-	dc->dml.soc.vnom.socclk_mhz = dc->dcn_soc->socclk;
-	dc->dml.soc.vmax.socclk_mhz = dc->dcn_soc->socclk;
-
-	dc->dml.soc.vmin.dcfclk_mhz = dc->dcn_soc->dcfclkv_min0p65;
-	dc->dml.soc.vmid.dcfclk_mhz = dc->dcn_soc->dcfclkv_mid0p72;
-	dc->dml.soc.vnom.dcfclk_mhz = dc->dcn_soc->dcfclkv_nom0p8;
-	dc->dml.soc.vmax.dcfclk_mhz = dc->dcn_soc->dcfclkv_max0p9;
-
-	dc->dml.soc.vmin.dispclk_mhz = dc->dcn_soc->max_dispclk_vmin0p65;
-	dc->dml.soc.vmid.dispclk_mhz = dc->dcn_soc->max_dispclk_vmid0p72;
-	dc->dml.soc.vnom.dispclk_mhz = dc->dcn_soc->max_dispclk_vnom0p8;
-	dc->dml.soc.vmax.dispclk_mhz = dc->dcn_soc->max_dispclk_vmax0p9;
-
-	dc->dml.soc.vmin.dppclk_mhz = dc->dcn_soc->max_dppclk_vmin0p65;
-	dc->dml.soc.vmid.dppclk_mhz = dc->dcn_soc->max_dppclk_vmid0p72;
-	dc->dml.soc.vnom.dppclk_mhz = dc->dcn_soc->max_dppclk_vnom0p8;
-	dc->dml.soc.vmax.dppclk_mhz = dc->dcn_soc->max_dppclk_vmax0p9;
-
-	dc->dml.soc.vmin.phyclk_mhz = dc->dcn_soc->phyclkv_min0p65;
-	dc->dml.soc.vmid.phyclk_mhz = dc->dcn_soc->phyclkv_mid0p72;
-	dc->dml.soc.vnom.phyclk_mhz = dc->dcn_soc->phyclkv_nom0p8;
-	dc->dml.soc.vmax.phyclk_mhz = dc->dcn_soc->phyclkv_max0p9;
-
-	dc->dml.soc.vmin.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65;
-	dc->dml.soc.vmid.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72;
-	dc->dml.soc.vnom.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8;
-	dc->dml.soc.vmax.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9;
 
 	dc->dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;
 	dc->dml.soc.sr_enter_plus_exit_time_us = dc->dcn_soc->sr_enter_plus_exit_time;

commit 7bc6f1ca9dbe8261d05dd4b63dacc26d4229415f
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Thu Nov 23 12:08:13 2017 -0500

    drm/amd/display: add assert to verify dcn_calc input validity
    
    This reverts commit 978482d0de86 Revert noisy assert messages
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 453ec1c6c181..47dbc953a3a9 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -887,6 +887,17 @@ bool dcn_validate_bandwidth(
 						+ pipe->bottom_pipe->plane_res.scl_data.recout.width;
 			}
 
+			if (pipe->plane_state->rotation % 2 == 0) {
+				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dal_fixed31_32_one.value
+					|| v->scaler_rec_out_width[input_idx] == v->viewport_width[input_idx]);
+				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dal_fixed31_32_one.value
+					|| v->scaler_recout_height[input_idx] == v->viewport_height[input_idx]);
+			} else {
+				ASSERT(pipe->plane_res.scl_data.ratios.horz.value != dal_fixed31_32_one.value
+					|| v->scaler_recout_height[input_idx] == v->viewport_width[input_idx]);
+				ASSERT(pipe->plane_res.scl_data.ratios.vert.value != dal_fixed31_32_one.value
+					|| v->scaler_rec_out_width[input_idx] == v->viewport_height[input_idx]);
+			}
 			v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;
 			v->source_pixel_format[input_idx] = tl_pixel_format_to_bw_defs(
 					pipe->plane_state->format);

commit bce14857bd1b2d6849336f56e7a1926ab937c5ef
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Mon Nov 13 17:03:53 2017 -0500

    drm/amd/display: set chroma taps to 1 when not scaling
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index b5bc9159f48e..453ec1c6c181 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -897,6 +897,15 @@ bool dcn_validate_bandwidth(
 			v->override_vta_ps[input_idx] = pipe->plane_res.scl_data.taps.v_taps;
 			v->override_hta_pschroma[input_idx] = pipe->plane_res.scl_data.taps.h_taps_c;
 			v->override_vta_pschroma[input_idx] = pipe->plane_res.scl_data.taps.v_taps_c;
+			/*
+			 * Spreadsheet doesn't handle taps_c is one properly,
+			 * need to force Chroma to always be scaled to pass
+			 * bandwidth validation.
+			 */
+			if (v->override_hta_pschroma[input_idx] == 1)
+				v->override_hta_pschroma[input_idx] = 2;
+			if (v->override_vta_pschroma[input_idx] == 1)
+				v->override_vta_pschroma[input_idx] = 2;
 			v->source_scan[input_idx] = (pipe->plane_state->rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
 		}
 		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)

commit a018298ff850effd3a8494c839449150aef0934a
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Wed Nov 29 10:28:29 2017 -0500

    drm/amd/display: Add disclaimer to BW and DML code provided by HW
    
    This code can sometimes look troubling but we trust it as it comes from
    HW teams with a guarantee of correctness. Add a note to these files to
    explain this.
    
    v2: thing -> things
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a4fbca34bcdf..b5bc9159f48e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -33,6 +33,15 @@
 #include "dcn10/dcn10_resource.h"
 #include "dcn_calc_math.h"
 
+/*
+ * NOTE:
+ *   This file is gcc-parseable HW gospel, coming straight from HW engineers.
+ *
+ * It doesn't adhere to Linux kernel style and sometimes will do things in odd
+ * ways. Unless there is something clearly wrong with it the code should
+ * remain as-is as it provides us with a guarantee from HW that it is correct.
+ */
+
 /* Defaults from spreadsheet rev#247 */
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		/* latencies */

commit 7260d1187eb5c81b6c0b0d310bf281bed46a4627
Author: Andrew Jiang <Andrew.Jiang@amd.com>
Date:   Mon Nov 13 17:09:12 2017 -0500

    drm/amd/display: Set full update flag in dcn_validate_bandwidth
    
    Doing bandwidth validation implies that this is a full update. Set the
    flag inside the function in case whatever is calling
    dcn_validate_bandwidth doesn't set it.
    
    Signed-off-by: Andrew Jiang <Andrew.Jiang@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index f37fb7c3bf7d..a4fbca34bcdf 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1013,6 +1013,8 @@ bool dcn_validate_bandwidth(
 			if (pipe->plane_state) {
 				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
 
+				pipe->plane_state->update_flags.bits.full_update = 1;
+
 				if (v->dpp_per_plane[input_idx] == 2 ||
 					((pipe->stream->view_format ==
 					  VIEW_3D_FORMAT_SIDE_BY_SIDE ||

commit 69cff5a4e35077d1e6d2527559eab09621a8ead6
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Thu Nov 9 16:02:56 2017 -0500

    drm/amd/display: Rename output_bpc to opp_input_bpc
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index b6b0872f50dd..f37fb7c3bf7d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -439,18 +439,6 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	input.dout.output_format = (v->output_format[in_idx] == dcn_bw_420) ? dm_420 : dm_444;
 	input.dout.output_type  = (v->output[in_idx] == dcn_bw_hdmi) ? dm_hdmi : dm_dp;
 	//input[in_idx].dout.output_standard;
-	switch (v->output_deep_color[in_idx]) {
-	case dcn_bw_encoder_12bpc:
-		input.dout.output_bpc = dm_out_12;
-	break;
-	case dcn_bw_encoder_10bpc:
-		input.dout.output_bpc = dm_out_10;
-	break;
-	case dcn_bw_encoder_8bpc:
-	default:
-		input.dout.output_bpc = dm_out_8;
-	break;
-	}
 
 	/*todo: soc->sr_enter_plus_exit_time??*/
 	dlg_sys_param.t_srx_delay_us = dc->dcn_ip->dcfclk_cstate_latency / v->dcf_clk_deep_sleep;

commit fb7ae8505eb890224112925eed690106e2f3ca24
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Thu Nov 9 15:05:52 2017 -0500

    drm/amd/display: fix refclk conversion from khz int to mhz float
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 88a004cc2690..b6b0872f50dd 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -432,7 +432,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	input.clks_cfg.dcfclk_mhz = v->dcfclk;
 	input.clks_cfg.dispclk_mhz = v->dispclk;
 	input.clks_cfg.dppclk_mhz = v->dppclk;
-	input.clks_cfg.refclk_mhz = dc->res_pool->ref_clock_inKhz/1000;
+	input.clks_cfg.refclk_mhz = dc->res_pool->ref_clock_inKhz / 1000.0;
 	input.clks_cfg.socclk_mhz = v->socclk;
 	input.clks_cfg.voltage = v->voltage_level;
 //	dc->dml.logger = pool->base.logger;

commit e6c258cb4e6fbc7500c493df22f52e1046c575b0
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Mon Oct 30 17:32:14 2017 -0400

    drm/amd/display: Refactor disable front end pipes.
    
    There are different code to disable front end, it is
    difficult to debug and adding new process.
    This refactor makes all disable front end call the same
    functions.
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 6d64a069648e..88a004cc2690 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1064,6 +1064,9 @@ bool dcn_validate_bandwidth(
 					hsplit_pipe->stream = NULL;
 					hsplit_pipe->top_pipe = NULL;
 					hsplit_pipe->bottom_pipe = NULL;
+					/* Clear plane_res and stream_res */
+					memset(&hsplit_pipe->plane_res, 0, sizeof(hsplit_pipe->plane_res));
+					memset(&hsplit_pipe->stream_res, 0, sizeof(hsplit_pipe->stream_res));
 					resource_build_scaling_params(pipe);
 				}
 				/* for now important to do this after pipe split for building e2e params */

commit 00893681a0ff41cacecabc3dafe0987593a3d5c5
Author: Andrew Jiang <Andrew.Jiang@amd.com>
Date:   Thu Oct 19 14:43:36 2017 -0400

    drm/amd/display: Reject PPLib clock values if they are invalid
    
    We should be sticking with the default clock values if the values
    obtained from PPLib are bogus.
    
    Signed-off-by: Andrew Jiang <Andrew.Jiang@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 3dce35e66b09..6d64a069648e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1231,40 +1231,62 @@ unsigned int dcn_find_dcfclk_suits_all(
 	return dcf_clk;
 }
 
+static bool verify_clock_values(struct dm_pp_clock_levels_with_voltage *clks)
+{
+	int i;
+
+	if (clks->num_levels == 0)
+		return false;
+
+	for (i = 0; i < clks->num_levels; i++)
+		/* Ensure that the result is sane */
+		if (clks->data[i].clocks_in_khz == 0)
+			return false;
+
+	return true;
+}
+
 void dcn_bw_update_from_pplib(struct dc *dc)
 {
 	struct dc_context *ctx = dc->ctx;
-	struct dm_pp_clock_levels_with_voltage clks = {0};
+	struct dm_pp_clock_levels_with_voltage fclks = {0}, dcfclks = {0};
+	bool res;
 
 	kernel_fpu_begin();
 
 	/* TODO: This is not the proper way to obtain fabric_and_dram_bandwidth, should be min(fclk, memclk) */
-
-	if (dm_pp_get_clock_levels_by_type_with_voltage(
-			ctx, DM_PP_CLOCK_TYPE_FCLK, &clks) &&
-			clks.num_levels != 0) {
-		ASSERT(clks.num_levels >= 3);
-		dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (clks.data[0].clocks_in_khz / 1000.0) / 1000.0;
-		if (clks.num_levels > 2) {
-			dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
-					(clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
-		} else {
-			dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
-					(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
-		}
+	res = dm_pp_get_clock_levels_by_type_with_voltage(
+			ctx, DM_PP_CLOCK_TYPE_FCLK, &fclks);
+
+	if (res)
+		res = verify_clock_values(&fclks);
+
+	if (res) {
+		ASSERT(fclks.num_levels >= 3);
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (fclks.data[0].clocks_in_khz / 1000.0) / 1000.0;
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
+				(fclks.data[fclks.num_levels - (fclks.num_levels > 2 ? 3 : 2)].clocks_in_khz / 1000.0)
+				* ddr4_dram_factor_single_Channel / 1000.0;
 		dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->number_of_channels *
-				(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+				(fclks.data[fclks.num_levels - 2].clocks_in_khz / 1000.0)
+				* ddr4_dram_factor_single_Channel / 1000.0;
 		dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->number_of_channels *
-				(clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+				(fclks.data[fclks.num_levels - 1].clocks_in_khz / 1000.0)
+				* ddr4_dram_factor_single_Channel / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
-	if (dm_pp_get_clock_levels_by_type_with_voltage(
-				ctx, DM_PP_CLOCK_TYPE_DCFCLK, &clks) &&
-				clks.num_levels >= 3) {
-		dc->dcn_soc->dcfclkv_min0p65 = clks.data[0].clocks_in_khz / 1000.0;
-		dc->dcn_soc->dcfclkv_mid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
-		dc->dcn_soc->dcfclkv_nom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
-		dc->dcn_soc->dcfclkv_max0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
+
+	res = dm_pp_get_clock_levels_by_type_with_voltage(
+			ctx, DM_PP_CLOCK_TYPE_DCFCLK, &dcfclks);
+
+	if (res)
+		res = verify_clock_values(&dcfclks);
+
+	if (res && dcfclks.num_levels >= 3) {
+		dc->dcn_soc->dcfclkv_min0p65 = dcfclks.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_mid0p72 = dcfclks.data[dcfclks.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_nom0p8 = dcfclks.data[dcfclks.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_max0p9 = dcfclks.data[dcfclks.num_levels - 1].clocks_in_khz / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
 

commit bf5563ede9f254fba083c6b56e4ca8b836babb1d
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Nov 7 05:30:47 2017 +1000

    amdgpu/dc: fix indentation warning from smatch.
    
    This fixes all the current smatch:
    warn: inconsistent indenting
    
    Reviewed-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index e1515230c661..3dce35e66b09 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1155,7 +1155,7 @@ static unsigned int dcn_find_normalized_clock_vdd_Level(
 			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
 			if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
-			vdd_level = dcn_bw_v_max0p91;
+				vdd_level = dcn_bw_v_max0p91;
 				BREAK_TO_DEBUGGER();
 			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p9;

commit 215a6f05bcc18ffcd953a8527639ea1f571f4d81
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Oct 6 15:40:07 2017 -0400

    drm/amd/display: add performance trace macro to dc
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index cf0c1459f546..e1515230c661 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -682,6 +682,7 @@ bool dcn_validate_bandwidth(
 	bool bw_limit_pass;
 	float bw_limit;
 
+	PERFORMANCE_TRACE_START();
 	if (dcn_bw_apply_registry_override(dc))
 		dcn_bw_sync_calcs_and_dml(dc);
 
@@ -1089,6 +1090,8 @@ bool dcn_validate_bandwidth(
 
 	kernel_fpu_end();
 
+	PERFORMANCE_TRACE_END();
+
 	if (bw_limit_pass && v->voltage_level != 5)
 		return true;
 	else
@@ -1223,7 +1226,7 @@ unsigned int dcn_find_dcfclk_suits_all(
 	else
 		dcf_clk =  dc->dcn_soc->dcfclkv_min0p65*1000;
 
-	dm_logger_write(dc->ctx->logger, LOG_HW_MARKS,
+	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
 		"\tdcf_clk for voltage = %d\n", dcf_clk);
 	return dcf_clk;
 }

commit d94585a06b8197a723787c6c5502872abcff0e8e
Author: Yue Hin Lau <Yuehin.Lau@amd.com>
Date:   Thu Oct 5 16:47:49 2017 -0400

    drm/amd/display: rename transform to dpp for dcn
    
    Signed-off-by: Yue Hin Lau <Yuehin.Lau@amd.com>
    Reviewed-by: Eric Bernstein <Eric.Bernstein@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index c37f130b0170..cf0c1459f546 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -488,6 +488,7 @@ static void split_stream_across_pipes(
 	secondary_pipe->plane_res.hubp = pool->hubps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.xfm = pool->transforms[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.dpp = pool->dpps[secondary_pipe->pipe_idx];
 	if (primary_pipe->bottom_pipe) {
 		ASSERT(primary_pipe->bottom_pipe != secondary_pipe);
 		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;

commit 8feabd03d34e4555c119e69269dae28f52e0d86c
Author: Yue Hin Lau <Yuehin.Lau@amd.com>
Date:   Mon Oct 2 14:39:42 2017 -0400

    drm/amd/display: rename struct mem_input to hubp for dcn
    
    Signed-off-by: Yue Hin Lau <Yuehin.Lau@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index fd1db8dc2f35..c37f130b0170 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -485,6 +485,7 @@ static void split_stream_across_pipes(
 
 	secondary_pipe->pipe_idx = pipe_idx;
 	secondary_pipe->plane_res.mi = pool->mis[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.hubp = pool->hubps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.xfm = pool->transforms[secondary_pipe->pipe_idx];
 	if (primary_pipe->bottom_pipe) {

commit 6d04ee9dc10149db842d41de66eca201c9d91b60
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Aug 23 16:43:17 2017 -0400

    drm/amd/display: Restructuring and cleaning up DML
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 6318f9f69c92..fd1db8dc2f35 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -386,10 +386,6 @@ static void pipe_ctx_to_e2e_pipe_params (
 			- pipe->stream->timing.v_addressable
 			- pipe->stream->timing.v_border_bottom
 			- pipe->stream->timing.v_border_top;
-
-	input->dest.vsync_plus_back_porch = pipe->stream->timing.v_total
-			- pipe->stream->timing.v_addressable
-			- pipe->stream->timing.v_front_porch;
 	input->dest.pixel_rate_mhz = pipe->stream->timing.pix_clk_khz/1000.0;
 	input->dest.vstartup_start = pipe->pipe_dlg_param.vstartup_start;
 	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
@@ -459,9 +455,9 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	/*todo: soc->sr_enter_plus_exit_time??*/
 	dlg_sys_param.t_srx_delay_us = dc->dcn_ip->dcfclk_cstate_latency / v->dcf_clk_deep_sleep;
 
-	dml_rq_dlg_get_rq_params(dml, &rq_param, input.pipe.src);
-	extract_rq_regs(dml, rq_regs, rq_param);
-	dml_rq_dlg_get_dlg_params(
+	dml1_rq_dlg_get_rq_params(dml, &rq_param, input.pipe.src);
+	dml1_extract_rq_regs(dml, rq_regs, rq_param);
+	dml1_rq_dlg_get_dlg_params(
 			dml,
 			dlg_regs,
 			ttu_regs,
@@ -474,96 +470,6 @@ static void dcn_bw_calc_rq_dlg_ttu(
 			pipe->plane_state->flip_immediate);
 }
 
-static void dcn_dml_wm_override(
-		const struct dcn_bw_internal_vars *v,
-		struct display_mode_lib *dml,
-		struct dc_state *context,
-		const struct resource_pool *pool)
-{
-	int i, in_idx, active_count;
-
-	struct _vcs_dpi_display_e2e_pipe_params_st *input = kzalloc(pool->pipe_count * sizeof(struct _vcs_dpi_display_e2e_pipe_params_st),
-								    GFP_KERNEL);
-	struct wm {
-		double urgent;
-		struct _vcs_dpi_cstate_pstate_watermarks_st cpstate;
-		double pte_meta_urgent;
-	} a;
-
-
-	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
-		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
-
-		if (!pipe->stream || !pipe->plane_state)
-			continue;
-
-		input[in_idx].clks_cfg.dcfclk_mhz = v->dcfclk;
-		input[in_idx].clks_cfg.dispclk_mhz = v->dispclk;
-		input[in_idx].clks_cfg.dppclk_mhz = v->dppclk;
-		input[in_idx].clks_cfg.refclk_mhz = pool->ref_clock_inKhz / 1000;
-		input[in_idx].clks_cfg.socclk_mhz = v->socclk;
-		input[in_idx].clks_cfg.voltage = v->voltage_level;
-		input[in_idx].dout.output_format = (v->output_format[in_idx] == dcn_bw_420) ? dm_420 : dm_444;
-		input[in_idx].dout.output_type  = (v->output[in_idx] == dcn_bw_hdmi) ? dm_hdmi : dm_dp;
-		//input[in_idx].dout.output_standard;
-		switch (v->output_deep_color[in_idx]) {
-		case dcn_bw_encoder_12bpc:
-			input[in_idx].dout.output_bpc = dm_out_12;
-		break;
-		case dcn_bw_encoder_10bpc:
-			input[in_idx].dout.output_bpc = dm_out_10;
-		break;
-		case dcn_bw_encoder_8bpc:
-		default:
-			input[in_idx].dout.output_bpc = dm_out_8;
-		break;
-		}
-		pipe_ctx_to_e2e_pipe_params(pipe, &input[in_idx].pipe);
-		dml_rq_dlg_get_rq_reg(
-			dml,
-			&pipe->rq_regs,
-			input[in_idx].pipe.src);
-		in_idx++;
-	}
-	active_count = in_idx;
-
-	a.urgent = dml_wm_urgent_e2e(dml, input, active_count);
-	a.cpstate = dml_wm_cstate_pstate_e2e(dml, input, active_count);
-	a.pte_meta_urgent = dml_wm_pte_meta_urgent(dml, a.urgent);
-
-	context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
-			a.cpstate.cstate_exit_us * 1000;
-	context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
-			a.cpstate.cstate_enter_plus_exit_us * 1000;
-	context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
-			a.cpstate.pstate_change_us * 1000;
-	context->bw.dcn.watermarks.a.pte_meta_urgent_ns = a.pte_meta_urgent * 1000;
-	context->bw.dcn.watermarks.a.urgent_ns = a.urgent * 1000;
-	context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
-	context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
-	context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
-
-
-	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
-		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
-
-		if (!pipe->stream || !pipe->plane_state)
-			continue;
-
-		dml_rq_dlg_get_dlg_reg(dml,
-			&pipe->dlg_regs,
-			&pipe->ttu_regs,
-			input, active_count,
-			in_idx,
-			true,
-			true,
-			v->pte_enable == dcn_bw_yes,
-			pipe->plane_state->flip_immediate);
-		in_idx++;
-	}
-	kfree(input);
-}
-
 static void split_stream_across_pipes(
 		struct resource_context *res_ctx,
 		const struct resource_pool *pool,
@@ -1163,9 +1069,6 @@ bool dcn_validate_bandwidth(
 
 			input_idx++;
 		}
-		if (dc->debug.use_dml_wm)
-			dcn_dml_wm_override(v, (struct display_mode_lib *)
-					&dc->dml, context, pool);
 	}
 
 	if (v->voltage_level == 0) {

commit 441ad741739e9092f6af231a539781118e23d6df
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Wed Sep 27 11:44:43 2017 -0400

    drm/amd/display: Add override for reporting wm ranges
    
    For verification of watermark select with SMU team, proper
    implementation will follow
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 9337ccadc321..6318f9f69c92 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1441,6 +1441,53 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	ranges.writer_wm_sets[3].min_drain_clk_khz = max_fclk_khz;
 	ranges.writer_wm_sets[3].max_drain_clk_khz = max_fclk_khz;
 
+	if (dc->debug.pplib_wm_report_mode == WM_REPORT_OVERRIDE) {
+		ranges.reader_wm_sets[0].wm_inst = WM_A;
+		ranges.reader_wm_sets[0].min_drain_clk_khz = 300000;
+		ranges.reader_wm_sets[0].max_drain_clk_khz = 654000;
+		ranges.reader_wm_sets[0].min_fill_clk_khz = 800000;
+		ranges.reader_wm_sets[0].max_fill_clk_khz = 800000;
+		ranges.writer_wm_sets[0].wm_inst = WM_A;
+		ranges.writer_wm_sets[0].min_fill_clk_khz = 200000;
+		ranges.writer_wm_sets[0].max_fill_clk_khz = 757000;
+		ranges.writer_wm_sets[0].min_drain_clk_khz = 800000;
+		ranges.writer_wm_sets[0].max_drain_clk_khz = 800000;
+
+		ranges.reader_wm_sets[1].wm_inst = WM_B;
+		ranges.reader_wm_sets[1].min_drain_clk_khz = 300000;
+		ranges.reader_wm_sets[1].max_drain_clk_khz = 654000;
+		ranges.reader_wm_sets[1].min_fill_clk_khz = 933000;
+		ranges.reader_wm_sets[1].max_fill_clk_khz = 933000;
+		ranges.writer_wm_sets[1].wm_inst = WM_B;
+		ranges.writer_wm_sets[1].min_fill_clk_khz = 200000;
+		ranges.writer_wm_sets[1].max_fill_clk_khz = 757000;
+		ranges.writer_wm_sets[1].min_drain_clk_khz = 933000;
+		ranges.writer_wm_sets[1].max_drain_clk_khz = 933000;
+
+
+		ranges.reader_wm_sets[2].wm_inst = WM_C;
+		ranges.reader_wm_sets[2].min_drain_clk_khz = 300000;
+		ranges.reader_wm_sets[2].max_drain_clk_khz = 654000;
+		ranges.reader_wm_sets[2].min_fill_clk_khz = 1067000;
+		ranges.reader_wm_sets[2].max_fill_clk_khz = 1067000;
+		ranges.writer_wm_sets[2].wm_inst = WM_C;
+		ranges.writer_wm_sets[2].min_fill_clk_khz = 200000;
+		ranges.writer_wm_sets[2].max_fill_clk_khz = 757000;
+		ranges.writer_wm_sets[2].min_drain_clk_khz = 1067000;
+		ranges.writer_wm_sets[2].max_drain_clk_khz = 1067000;
+
+		ranges.reader_wm_sets[3].wm_inst = WM_D;
+		ranges.reader_wm_sets[3].min_drain_clk_khz = 300000;
+		ranges.reader_wm_sets[3].max_drain_clk_khz = 654000;
+		ranges.reader_wm_sets[3].min_fill_clk_khz = 1200000;
+		ranges.reader_wm_sets[3].max_fill_clk_khz = 1200000;
+		ranges.writer_wm_sets[3].wm_inst = WM_D;
+		ranges.writer_wm_sets[3].min_fill_clk_khz = 200000;
+		ranges.writer_wm_sets[3].max_fill_clk_khz = 757000;
+		ranges.writer_wm_sets[3].min_drain_clk_khz = 1200000;
+		ranges.writer_wm_sets[3].max_drain_clk_khz = 1200000;
+	}
+
 	/* Notify PP Lib/SMU which Watermarks to use for which clock ranges */
 	pp->set_wm_ranges(&pp->pp_smu, &ranges);
 }

commit 6512387a54357c5d3fbea8995d8879ea817a5ec6
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Wed Sep 27 09:20:51 2017 -0400

    drm/amd/display: align DCLK to voltage level
    
    in past program SMU will use all voltage headroom.  RV does not
    
    if DAL need higher voltage for DCFCLK or DISPCLK, also increase FCLK
    to improve stutter as voltage is already
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index cf474eb1cde8..9337ccadc321 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1049,6 +1049,10 @@ bool dcn_validate_bandwidth(
 		else
 			bw_consumed = v->fabric_and_dram_bandwidth_vmax0p9;
 
+		if (bw_consumed < v->fabric_and_dram_bandwidth)
+			if (dc->debug.voltage_align_fclk)
+				bw_consumed = v->fabric_and_dram_bandwidth;
+
 		display_pipe_configuration(v);
 		calc_wm_sets_and_perf_params(context, v);
 		context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 /

commit 9f945eab797b042930553388d37c12fed38f3396
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Tue Sep 26 10:16:34 2017 -0400

    drm/amd/display: fix bug in force_single_disp_pipe_split
    
    should only lower dpp clock.
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index aa56243f2522..cf474eb1cde8 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -732,11 +732,14 @@ void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
 void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
 		unsigned int pixel_rate_khz)
 {
+	float pixel_rate_mhz = pixel_rate_khz / 1000;
+
 	/*
 	 * force enabling pipe split by lower dpp clock for DPM0 to just
 	 * below the specify pixel_rate, so bw calc would split pipe.
 	 */
-	v->max_dppclk[0] = pixel_rate_khz / 1000;
+	if (pixel_rate_mhz < v->max_dppclk[0])
+		v->max_dppclk[0] = pixel_rate_mhz;
 }
 
 void hack_bounding_box(struct dcn_bw_internal_vars *v,

commit db64fbe73288f0b5f2a36ecdd48c9bae978406e0
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Mon Sep 25 10:52:07 2017 -0400

    drm/amd/display: enable optional pipe split for single display
    
    also refactor debug option.  now pipe_split_policy are
    dynamic = no hack around dcn_calcs.  will split based on HW recommendation
    avoid = avoid split if we can support the config with higher voltage
    avoid_multi_display = allow split with single display output.
    
    force_single_disp_pipe_split
    force single display to pipe split to improve stutter efficiency
    by using DET buffers using 2 HUBP.
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 319450d9cfc1..aa56243f2522 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -720,6 +720,46 @@ static bool dcn_bw_apply_registry_override(struct dc *dc)
 	return updated;
 }
 
+void hack_disable_optional_pipe_split(struct dcn_bw_internal_vars *v)
+{
+	/*
+	 * disable optional pipe split by lower dispclk bounding box
+	 * at DPM0
+	 */
+	v->max_dispclk[0] = v->max_dppclk_vmin0p65;
+}
+
+void hack_force_pipe_split(struct dcn_bw_internal_vars *v,
+		unsigned int pixel_rate_khz)
+{
+	/*
+	 * force enabling pipe split by lower dpp clock for DPM0 to just
+	 * below the specify pixel_rate, so bw calc would split pipe.
+	 */
+	v->max_dppclk[0] = pixel_rate_khz / 1000;
+}
+
+void hack_bounding_box(struct dcn_bw_internal_vars *v,
+		struct dc_debug *dbg,
+		struct dc_state *context)
+{
+	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID) {
+		hack_disable_optional_pipe_split(v);
+	}
+
+	if (dbg->pipe_split_policy == MPC_SPLIT_AVOID_MULT_DISP &&
+		context->stream_count >= 2) {
+		hack_disable_optional_pipe_split(v);
+	}
+
+	if (context->stream_count == 1 &&
+			dbg->force_single_disp_pipe_split) {
+		struct dc_stream_state *stream0 = context->streams[0];
+
+		hack_force_pipe_split(v, stream0->timing.pix_clk_khz);
+	}
+}
+
 bool dcn_validate_bandwidth(
 		struct dc *dc,
 		struct dc_state *context)
@@ -851,9 +891,7 @@ bool dcn_validate_bandwidth(
 	v->phyclk_per_state[1] = v->phyclkv_mid0p72;
 	v->phyclk_per_state[0] = v->phyclkv_min0p65;
 
-	if (dc->debug.disable_pipe_split) {
-		v->max_dispclk[0] = v->max_dppclk_vmin0p65;
-	}
+	hack_bounding_box(v, &dc->debug, context);
 
 	if (v->voltage_override == dcn_bw_v_max0p9) {
 		v->voltage_override_level = number_of_states - 1;

commit fcbbe3da0ab65dc114937857fce81902e3fa2a97
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Thu Sep 21 18:16:01 2017 -0400

    drm/amd/display: Use active + border for bw validation
    
    When doing SLS, KMD gives us clipped v_addressable with
    border. This results in bw validation failure.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index cc99073b7a54..319450d9cfc1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -364,7 +364,8 @@ static void pipe_ctx_to_e2e_pipe_params (
 	}
 
 
-	input->dest.vactive        = pipe->stream->timing.v_addressable;
+	input->dest.vactive        = pipe->stream->timing.v_addressable + pipe->stream->timing.v_border_top
+			+ pipe->stream->timing.v_border_bottom;
 
 	input->dest.recout_width   = pipe->plane_res.scl_data.recout.width;
 	input->dest.recout_height  = pipe->plane_res.scl_data.recout.height;
@@ -882,10 +883,11 @@ bool dcn_validate_bandwidth(
 
 		v->htotal[input_idx] = pipe->stream->timing.h_total;
 		v->vtotal[input_idx] = pipe->stream->timing.v_total;
+		v->vactive[input_idx] = pipe->stream->timing.v_addressable +
+				pipe->stream->timing.v_border_top + pipe->stream->timing.v_border_bottom;
 		v->v_sync_plus_back_porch[input_idx] = pipe->stream->timing.v_total
-				- pipe->stream->timing.v_addressable
+				- v->vactive[input_idx]
 				- pipe->stream->timing.v_front_porch;
-		v->vactive[input_idx] = pipe->stream->timing.v_addressable;
 		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
 
 		if (!pipe->plane_state) {

commit 4f4ee68686c74bf1dfde6fa9fb7124104ff6f283
Author: Hersen Wu <hersenxs.wu@amd.com>
Date:   Wed Sep 20 16:30:44 2017 -0400

    drm/amd/display: screen flickers when connected to ext monitor in clone
    
    Signed-off-by: Hersen Wu <hersenxs.wu@amd.com>
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Hersen Wu <hersenxs.wu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 8ca6c3e4e65a..cc99073b7a54 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1018,9 +1018,17 @@ bool dcn_validate_bandwidth(
 		context->bw.dcn.calc_clk.min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
 		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
 		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
+
 		context->bw.dcn.calc_clk.dispclk_khz = (int)(v->dispclk * 1000);
 		if (dc->debug.max_disp_clk == true)
 			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
+
+		if (context->bw.dcn.calc_clk.dispclk_khz <
+				dc->debug.min_disp_clk_khz) {
+			context->bw.dcn.calc_clk.dispclk_khz =
+					dc->debug.min_disp_clk_khz;
+		}
+
 		context->bw.dcn.calc_clk.dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
 
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {

commit 44858055bb28b1ba45dc05acecf9087bc4786701
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Oct 3 15:11:01 2017 +1000

    amdgpu/dc: set a bunch of functions to static.
    
    All of these are unused outside the file they are in.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Reviewed-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 91f43a1b88ee..8ca6c3e4e65a 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1135,7 +1135,7 @@ bool dcn_validate_bandwidth(
 		return false;
 }
 
-unsigned int dcn_find_normalized_clock_vdd_Level(
+static unsigned int dcn_find_normalized_clock_vdd_Level(
 	const struct dc *dc,
 	enum dm_pp_clock_type clocks_type,
 	int clocks_in_khz)

commit 2004f45ef83f07f43f5da6ede780b08068c7583d
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Wed Sep 27 10:53:50 2017 -0400

    drm/amd/display: Use kernel alloc/free
    
    Abstractions are frowned upon.
    
    cocci script:
    virtual context
    virtual patch
    virtual org
    virtual report
    
    @@
    expression ptr;
    @@
    
    - dm_alloc(ptr)
    + kzalloc(ptr, GFP_KERNEL)
    
    @@
    expression ptr, size;
    @@
    
    - dm_realloc(ptr, size)
    + krealloc(ptr, size, GFP_KERNEL)
    
    @@
    expression ptr;
    @@
    
    - dm_free(ptr)
    + kfree(ptr)
    
    v2: use GFP_KERNEL, not GFP_ATOMIC. add cocci script
    
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1b0f64756be6..91f43a1b88ee 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -481,8 +481,8 @@ static void dcn_dml_wm_override(
 {
 	int i, in_idx, active_count;
 
-	struct _vcs_dpi_display_e2e_pipe_params_st *input = dm_alloc(pool->pipe_count *
-					sizeof(struct _vcs_dpi_display_e2e_pipe_params_st));
+	struct _vcs_dpi_display_e2e_pipe_params_st *input = kzalloc(pool->pipe_count * sizeof(struct _vcs_dpi_display_e2e_pipe_params_st),
+								    GFP_KERNEL);
 	struct wm {
 		double urgent;
 		struct _vcs_dpi_cstate_pstate_watermarks_st cpstate;
@@ -560,7 +560,7 @@ static void dcn_dml_wm_override(
 			pipe->plane_state->flip_immediate);
 		in_idx++;
 	}
-	dm_free(input);
+	kfree(input);
 }
 
 static void split_stream_across_pipes(

commit fd96c1775a75c14bf15465af7040cc00855b1ec0
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Mon Aug 28 09:51:03 2017 -0400

    drm/amd/display: delete dead code
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index ee1b76c074e6..1b0f64756be6 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1312,8 +1312,9 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
 	struct pp_smu_funcs_rv *pp = dc->res_pool->pp_smu;
 	struct pp_smu_wm_range_sets ranges = {0};
-	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
-		nom_dcfclk_khz, mid_fclk_khz, min_dcfclk_khz, socclk_khz;
+	int max_fclk_khz, nom_fclk_khz, mid_fclk_khz, min_fclk_khz;
+	int max_dcfclk_khz, min_dcfclk_khz;
+	int socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
 	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
@@ -1326,7 +1327,6 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	mid_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 * 1000000 / factor;
 	min_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
 	max_dcfclk_khz = dc->dcn_soc->dcfclkv_max0p9 * 1000;
-	nom_dcfclk_khz = dc->dcn_soc->dcfclkv_nom0p8 * 1000;
 	min_dcfclk_khz = dc->dcn_soc->dcfclkv_min0p65 * 1000;
 	socclk_khz = dc->dcn_soc->socclk * 1000;
 	kernel_fpu_end();

commit 608ac7bb3924178d7bfa8b88d79d3d9d72b8f485
Author: Jerry Zuo <Jerry.Zuo@amd.com>
Date:   Fri Aug 25 16:16:10 2017 -0400

    drm/amd/display: Rename dc validate_context and current_context
    
    Rename all the dc validate_context to dc_state and
    dc current_context to current_state.
    
    Signed-off-by: Jerry Zuo <Jerry.Zuo@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 6c85a54e47d6..ee1b76c074e6 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -476,7 +476,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 static void dcn_dml_wm_override(
 		const struct dcn_bw_internal_vars *v,
 		struct display_mode_lib *dml,
-		struct validate_context *context,
+		struct dc_state *context,
 		const struct resource_pool *pool)
 {
 	int i, in_idx, active_count;
@@ -593,7 +593,7 @@ static void split_stream_across_pipes(
 }
 
 static void calc_wm_sets_and_perf_params(
-		struct validate_context *context,
+		struct dc_state *context,
 		struct dcn_bw_internal_vars *v)
 {
 	/* Calculate set A last to keep internal var state consistent for required config */
@@ -721,7 +721,7 @@ static bool dcn_bw_apply_registry_override(struct dc *dc)
 
 bool dcn_validate_bandwidth(
 		struct dc *dc,
-		struct validate_context *context)
+		struct dc_state *context)
 {
 	const struct resource_pool *pool = dc->res_pool;
 	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;

commit e1b522bff39d4834f4cfedc557857bb4d50f3946
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Thu Aug 24 17:29:24 2017 -0400

    drm/amd/display: work around for 8k sleep crash
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 5e5766a63a47..6c85a54e47d6 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -581,6 +581,7 @@ static void split_stream_across_pipes(
 	secondary_pipe->plane_res.ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->plane_res.xfm = pool->transforms[secondary_pipe->pipe_idx];
 	if (primary_pipe->bottom_pipe) {
+		ASSERT(primary_pipe->bottom_pipe != secondary_pipe);
 		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;
 		secondary_pipe->bottom_pipe->top_pipe = secondary_pipe;
 	}

commit 156590454259a19d1709fab2ff7d59870574e822
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Wed Aug 23 15:44:42 2017 -0400

    drm/amd/display: Clean up flattening core_dc to dc
    
    Clean up some code related to flattening core_dc commit
    (Remove redundent dc = dc, which was the result of removing
    DC_TO_CORE() macro)
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index afd403ceb2a7..5e5766a63a47 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -983,15 +983,14 @@ bool dcn_validate_bandwidth(
 	if (v->voltage_level == 0 &&
 			(dc->debug.sr_exit_time_dpm0_ns
 				|| dc->debug.sr_enter_plus_exit_time_dpm0_ns)) {
-		struct dc *dc_core = dc;
 
 		if (dc->debug.sr_enter_plus_exit_time_dpm0_ns)
 			v->sr_enter_plus_exit_time =
 				dc->debug.sr_enter_plus_exit_time_dpm0_ns / 1000.0f;
 		if (dc->debug.sr_exit_time_dpm0_ns)
 			v->sr_exit_time =  dc->debug.sr_exit_time_dpm0_ns / 1000.0f;
-		dc_core->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
-		dc_core->dml.soc.sr_exit_time_us = v->sr_exit_time;
+		dc->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
+		dc->dml.soc.sr_exit_time_us = v->sr_exit_time;
 		mode_support_and_system_configuration(v);
 	}
 
@@ -1114,11 +1113,10 @@ bool dcn_validate_bandwidth(
 	}
 
 	if (v->voltage_level == 0) {
-		struct dc *dc_core = dc;
 
-		dc_core->dml.soc.sr_enter_plus_exit_time_us =
-				dc_core->dcn_soc->sr_enter_plus_exit_time;
-		dc_core->dml.soc.sr_exit_time_us = dc_core->dcn_soc->sr_exit_time;
+		dc->dml.soc.sr_enter_plus_exit_time_us =
+				dc->dcn_soc->sr_enter_plus_exit_time;
+		dc->dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;
 	}
 
 	/*

commit a185048ca88ce143f980f2b819f034cfc09a5377
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Sun Aug 13 13:50:52 2017 -0400

    drm/amd/display: refactor pplib/smu communication
    
    new per SoC interface instead legacy interface with lots of un-used
    field that only cause confusion
    
    model pp_smu like one of our HW objects with func_ptr interface
    to call into it.  struct pp_smu as handle to call pp/smu
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Jun Lei <Jun.Lei@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index f0dfd3c3c12c..afd403ceb2a7 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1311,12 +1311,16 @@ void dcn_bw_update_from_pplib(struct dc *dc)
 
 void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
-	struct dm_pp_wm_sets_with_clock_ranges_soc15 clk_ranges = {0};
+	struct pp_smu_funcs_rv *pp = dc->res_pool->pp_smu;
+	struct pp_smu_wm_range_sets ranges = {0};
 	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
 		nom_dcfclk_khz, mid_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
 	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
+	if (!pp->set_wm_ranges)
+		return;
+
 	kernel_fpu_begin();
 	max_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
 	nom_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
@@ -1336,55 +1340,55 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 	/* SOCCLK does not affect anytihng but writeback for DCN so for now we dont
 	 * care what the value is, hence min to overdrive level
 	 */
-	clk_ranges.num_wm_dmif_sets = 4;
-	clk_ranges.num_wm_mcif_sets = 4;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_set_id = WM_SET_A;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_dcfclk_clk_in_khz = min_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_mem_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_set_id = WM_SET_A;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_socclk_clk_in_khz = socclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_mem_clk_in_khz = min_fclk_khz;
-
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_set_id = WM_SET_B;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_memg_clk_in_khz = mid_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_mem_clk_in_khz = mid_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_set_id = WM_SET_B;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_socclk_clk_in_khz = socclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_memg_clk_in_khz = mid_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_mem_clk_in_khz = mid_fclk_khz;
-
-
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_set_id = WM_SET_C;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_mem_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_set_id = WM_SET_C;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_socclk_clk_in_khz = socclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_mem_clk_in_khz = nom_fclk_khz;
-
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_set_id = WM_SET_D;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_mem_clk_in_khz = max_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_set_id = WM_SET_D;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_socclk_clk_in_khz = socclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_mem_clk_in_khz = max_fclk_khz;
+	ranges.num_reader_wm_sets = WM_COUNT;
+	ranges.num_writer_wm_sets = WM_COUNT;
+	ranges.reader_wm_sets[0].wm_inst = WM_A;
+	ranges.reader_wm_sets[0].min_drain_clk_khz = min_dcfclk_khz;
+	ranges.reader_wm_sets[0].max_drain_clk_khz = max_dcfclk_khz;
+	ranges.reader_wm_sets[0].min_fill_clk_khz = min_fclk_khz;
+	ranges.reader_wm_sets[0].max_fill_clk_khz = min_fclk_khz;
+	ranges.writer_wm_sets[0].wm_inst = WM_A;
+	ranges.writer_wm_sets[0].min_fill_clk_khz = socclk_khz;
+	ranges.writer_wm_sets[0].max_fill_clk_khz = overdrive;
+	ranges.writer_wm_sets[0].min_drain_clk_khz = min_fclk_khz;
+	ranges.writer_wm_sets[0].max_drain_clk_khz = min_fclk_khz;
+
+	ranges.reader_wm_sets[1].wm_inst = WM_B;
+	ranges.reader_wm_sets[1].min_drain_clk_khz = min_fclk_khz;
+	ranges.reader_wm_sets[1].max_drain_clk_khz = max_dcfclk_khz;
+	ranges.reader_wm_sets[1].min_fill_clk_khz = mid_fclk_khz;
+	ranges.reader_wm_sets[1].max_fill_clk_khz = mid_fclk_khz;
+	ranges.writer_wm_sets[1].wm_inst = WM_B;
+	ranges.writer_wm_sets[1].min_fill_clk_khz = socclk_khz;
+	ranges.writer_wm_sets[1].max_fill_clk_khz = overdrive;
+	ranges.writer_wm_sets[1].min_drain_clk_khz = mid_fclk_khz;
+	ranges.writer_wm_sets[1].max_drain_clk_khz = mid_fclk_khz;
+
+
+	ranges.reader_wm_sets[2].wm_inst = WM_C;
+	ranges.reader_wm_sets[2].min_drain_clk_khz = min_fclk_khz;
+	ranges.reader_wm_sets[2].max_drain_clk_khz = max_dcfclk_khz;
+	ranges.reader_wm_sets[2].min_fill_clk_khz = nom_fclk_khz;
+	ranges.reader_wm_sets[2].max_fill_clk_khz = nom_fclk_khz;
+	ranges.writer_wm_sets[2].wm_inst = WM_C;
+	ranges.writer_wm_sets[2].min_fill_clk_khz = socclk_khz;
+	ranges.writer_wm_sets[2].max_fill_clk_khz = overdrive;
+	ranges.writer_wm_sets[2].min_drain_clk_khz = nom_fclk_khz;
+	ranges.writer_wm_sets[2].max_drain_clk_khz = nom_fclk_khz;
+
+	ranges.reader_wm_sets[3].wm_inst = WM_D;
+	ranges.reader_wm_sets[3].min_drain_clk_khz = min_fclk_khz;
+	ranges.reader_wm_sets[3].max_drain_clk_khz = max_dcfclk_khz;
+	ranges.reader_wm_sets[3].min_fill_clk_khz = max_fclk_khz;
+	ranges.reader_wm_sets[3].max_fill_clk_khz = max_fclk_khz;
+	ranges.writer_wm_sets[3].wm_inst = WM_D;
+	ranges.writer_wm_sets[3].min_fill_clk_khz = socclk_khz;
+	ranges.writer_wm_sets[3].max_fill_clk_khz = overdrive;
+	ranges.writer_wm_sets[3].min_drain_clk_khz = max_fclk_khz;
+	ranges.writer_wm_sets[3].max_drain_clk_khz = max_fclk_khz;
 
 	/* Notify PP Lib/SMU which Watermarks to use for which clock ranges */
-	dm_pp_notify_wm_clock_changes_soc15(dc->ctx, &clk_ranges);
+	pp->set_wm_ranges(&pp->pp_smu, &ranges);
 }
 
 void dcn_bw_sync_calcs_and_dml(struct dc *dc)

commit fb3466a450cc4684654367ae2f47fc3fc7846574
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Tue Aug 1 15:00:25 2017 -0400

    drm/amd/display: Flattening core_dc to dc
    
    -Flattening core_dc to dc
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 6fb1b9a91993..f0dfd3c3c12c 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -27,7 +27,6 @@
 #include "dcn_calcs.h"
 #include "dcn_calc_auto.h"
 #include "dc.h"
-#include "core_dc.h"
 #include "dal_asic_id.h"
 
 #include "resource.h"
@@ -399,7 +398,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 }
 
 static void dcn_bw_calc_rq_dlg_ttu(
-		const struct core_dc *dc,
+		const struct dc *dc,
 		const struct dcn_bw_internal_vars *v,
 		struct pipe_ctx *pipe,
 		int in_idx)
@@ -674,45 +673,45 @@ static void calc_wm_sets_and_perf_params(
 		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
 }
 
-static bool dcn_bw_apply_registry_override(struct core_dc *dc)
+static bool dcn_bw_apply_registry_override(struct dc *dc)
 {
 	bool updated = false;
 
 	kernel_fpu_begin();
-	if ((int)(dc->dcn_soc->sr_exit_time * 1000) != dc->public.debug.sr_exit_time_ns
-			&& dc->public.debug.sr_exit_time_ns) {
+	if ((int)(dc->dcn_soc->sr_exit_time * 1000) != dc->debug.sr_exit_time_ns
+			&& dc->debug.sr_exit_time_ns) {
 		updated = true;
-		dc->dcn_soc->sr_exit_time = dc->public.debug.sr_exit_time_ns / 1000.0;
+		dc->dcn_soc->sr_exit_time = dc->debug.sr_exit_time_ns / 1000.0;
 	}
 
 	if ((int)(dc->dcn_soc->sr_enter_plus_exit_time * 1000)
-				!= dc->public.debug.sr_enter_plus_exit_time_ns
-			&& dc->public.debug.sr_enter_plus_exit_time_ns) {
+				!= dc->debug.sr_enter_plus_exit_time_ns
+			&& dc->debug.sr_enter_plus_exit_time_ns) {
 		updated = true;
 		dc->dcn_soc->sr_enter_plus_exit_time =
-				dc->public.debug.sr_enter_plus_exit_time_ns / 1000.0;
+				dc->debug.sr_enter_plus_exit_time_ns / 1000.0;
 	}
 
-	if ((int)(dc->dcn_soc->urgent_latency * 1000) != dc->public.debug.urgent_latency_ns
-			&& dc->public.debug.urgent_latency_ns) {
+	if ((int)(dc->dcn_soc->urgent_latency * 1000) != dc->debug.urgent_latency_ns
+			&& dc->debug.urgent_latency_ns) {
 		updated = true;
-		dc->dcn_soc->urgent_latency = dc->public.debug.urgent_latency_ns / 1000.0;
+		dc->dcn_soc->urgent_latency = dc->debug.urgent_latency_ns / 1000.0;
 	}
 
 	if ((int)(dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency * 1000)
-				!= dc->public.debug.percent_of_ideal_drambw
-			&& dc->public.debug.percent_of_ideal_drambw) {
+				!= dc->debug.percent_of_ideal_drambw
+			&& dc->debug.percent_of_ideal_drambw) {
 		updated = true;
 		dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency =
-				dc->public.debug.percent_of_ideal_drambw;
+				dc->debug.percent_of_ideal_drambw;
 	}
 
 	if ((int)(dc->dcn_soc->dram_clock_change_latency * 1000)
-				!= dc->public.debug.dram_clock_change_latency_ns
-			&& dc->public.debug.dram_clock_change_latency_ns) {
+				!= dc->debug.dram_clock_change_latency_ns
+			&& dc->debug.dram_clock_change_latency_ns) {
 		updated = true;
 		dc->dcn_soc->dram_clock_change_latency =
-				dc->public.debug.dram_clock_change_latency_ns / 1000.0;
+				dc->debug.dram_clock_change_latency_ns / 1000.0;
 	}
 	kernel_fpu_end();
 
@@ -720,7 +719,7 @@ static bool dcn_bw_apply_registry_override(struct core_dc *dc)
 }
 
 bool dcn_validate_bandwidth(
-		const struct core_dc *dc,
+		struct dc *dc,
 		struct validate_context *context)
 {
 	const struct resource_pool *pool = dc->res_pool;
@@ -730,8 +729,8 @@ bool dcn_validate_bandwidth(
 	bool bw_limit_pass;
 	float bw_limit;
 
-	if (dcn_bw_apply_registry_override(DC_TO_CORE(&dc->public)))
-		dcn_bw_sync_calcs_and_dml(DC_TO_CORE(&dc->public));
+	if (dcn_bw_apply_registry_override(dc))
+		dcn_bw_sync_calcs_and_dml(dc);
 
 	memset(v, 0, sizeof(*v));
 	kernel_fpu_begin();
@@ -850,7 +849,7 @@ bool dcn_validate_bandwidth(
 	v->phyclk_per_state[1] = v->phyclkv_mid0p72;
 	v->phyclk_per_state[0] = v->phyclkv_min0p65;
 
-	if (dc->public.debug.disable_pipe_split) {
+	if (dc->debug.disable_pipe_split) {
 		v->max_dispclk[0] = v->max_dppclk_vmin0p65;
 	}
 
@@ -982,15 +981,15 @@ bool dcn_validate_bandwidth(
 	mode_support_and_system_configuration(v);
 
 	if (v->voltage_level == 0 &&
-			(dc->public.debug.sr_exit_time_dpm0_ns
-				|| dc->public.debug.sr_enter_plus_exit_time_dpm0_ns)) {
-		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
+			(dc->debug.sr_exit_time_dpm0_ns
+				|| dc->debug.sr_enter_plus_exit_time_dpm0_ns)) {
+		struct dc *dc_core = dc;
 
-		if (dc->public.debug.sr_enter_plus_exit_time_dpm0_ns)
+		if (dc->debug.sr_enter_plus_exit_time_dpm0_ns)
 			v->sr_enter_plus_exit_time =
-				dc->public.debug.sr_enter_plus_exit_time_dpm0_ns / 1000.0f;
-		if (dc->public.debug.sr_exit_time_dpm0_ns)
-			v->sr_exit_time =  dc->public.debug.sr_exit_time_dpm0_ns / 1000.0f;
+				dc->debug.sr_enter_plus_exit_time_dpm0_ns / 1000.0f;
+		if (dc->debug.sr_exit_time_dpm0_ns)
+			v->sr_exit_time =  dc->debug.sr_exit_time_dpm0_ns / 1000.0f;
 		dc_core->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
 		dc_core->dml.soc.sr_exit_time_us = v->sr_exit_time;
 		mode_support_and_system_configuration(v);
@@ -1020,7 +1019,7 @@ bool dcn_validate_bandwidth(
 		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
 		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
 		context->bw.dcn.calc_clk.dispclk_khz = (int)(v->dispclk * 1000);
-		if (dc->public.debug.max_disp_clk == true)
+		if (dc->debug.max_disp_clk == true)
 			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
 		context->bw.dcn.calc_clk.dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
 
@@ -1109,13 +1108,13 @@ bool dcn_validate_bandwidth(
 
 			input_idx++;
 		}
-		if (dc->public.debug.use_dml_wm)
+		if (dc->debug.use_dml_wm)
 			dcn_dml_wm_override(v, (struct display_mode_lib *)
 					&dc->dml, context, pool);
 	}
 
 	if (v->voltage_level == 0) {
-		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
+		struct dc *dc_core = dc;
 
 		dc_core->dml.soc.sr_enter_plus_exit_time_us =
 				dc_core->dcn_soc->sr_enter_plus_exit_time;
@@ -1138,7 +1137,7 @@ bool dcn_validate_bandwidth(
 }
 
 unsigned int dcn_find_normalized_clock_vdd_Level(
-	const struct core_dc *dc,
+	const struct dc *dc,
 	enum dm_pp_clock_type clocks_type,
 	int clocks_in_khz)
 {
@@ -1228,7 +1227,7 @@ unsigned int dcn_find_normalized_clock_vdd_Level(
 }
 
 unsigned int dcn_find_dcfclk_suits_all(
-	const struct core_dc *dc,
+	const struct dc *dc,
 	struct clocks_value *clocks)
 {
 	unsigned vdd_level, vdd_level_temp;
@@ -1270,7 +1269,7 @@ unsigned int dcn_find_dcfclk_suits_all(
 	return dcf_clk;
 }
 
-void dcn_bw_update_from_pplib(struct core_dc *dc)
+void dcn_bw_update_from_pplib(struct dc *dc)
 {
 	struct dc_context *ctx = dc->ctx;
 	struct dm_pp_clock_levels_with_voltage clks = {0};
@@ -1310,7 +1309,7 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 	kernel_fpu_end();
 }
 
-void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
+void dcn_bw_notify_pplib_of_wm_ranges(struct dc *dc)
 {
 	struct dm_pp_wm_sets_with_clock_ranges_soc15 clk_ranges = {0};
 	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
@@ -1388,7 +1387,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 	dm_pp_notify_wm_clock_changes_soc15(dc->ctx, &clk_ranges);
 }
 
-void dcn_bw_sync_calcs_and_dml(struct core_dc *dc)
+void dcn_bw_sync_calcs_and_dml(struct dc *dc)
 {
 	kernel_fpu_begin();
 	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,

commit 65111f25f1fea751f3b4321a59c993c2898b7dbf
Author: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
Date:   Thu Aug 3 13:02:29 2017 -0400

    drm/amd/display: change dcn_ip and dcn_soc into pointers
    
    -Change dcn_ip into pointer
    -Change dcn_soc into pointer
    
    This is needed for flattening of core_dc into dc, as without
    this the diags build fails
    
    Signed-off-by: Bhawanpreet Lakha <Bhawanpreet.Lakha@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 67da973c898b..6fb1b9a91993 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -457,7 +457,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	}
 
 	/*todo: soc->sr_enter_plus_exit_time??*/
-	dlg_sys_param.t_srx_delay_us = dc->dcn_ip.dcfclk_cstate_latency / v->dcf_clk_deep_sleep;
+	dlg_sys_param.t_srx_delay_us = dc->dcn_ip->dcfclk_cstate_latency / v->dcf_clk_deep_sleep;
 
 	dml_rq_dlg_get_rq_params(dml, &rq_param, input.pipe.src);
 	extract_rq_regs(dml, rq_regs, rq_param);
@@ -679,39 +679,39 @@ static bool dcn_bw_apply_registry_override(struct core_dc *dc)
 	bool updated = false;
 
 	kernel_fpu_begin();
-	if ((int)(dc->dcn_soc.sr_exit_time * 1000) != dc->public.debug.sr_exit_time_ns
+	if ((int)(dc->dcn_soc->sr_exit_time * 1000) != dc->public.debug.sr_exit_time_ns
 			&& dc->public.debug.sr_exit_time_ns) {
 		updated = true;
-		dc->dcn_soc.sr_exit_time = dc->public.debug.sr_exit_time_ns / 1000.0;
+		dc->dcn_soc->sr_exit_time = dc->public.debug.sr_exit_time_ns / 1000.0;
 	}
 
-	if ((int)(dc->dcn_soc.sr_enter_plus_exit_time * 1000)
+	if ((int)(dc->dcn_soc->sr_enter_plus_exit_time * 1000)
 				!= dc->public.debug.sr_enter_plus_exit_time_ns
 			&& dc->public.debug.sr_enter_plus_exit_time_ns) {
 		updated = true;
-		dc->dcn_soc.sr_enter_plus_exit_time =
+		dc->dcn_soc->sr_enter_plus_exit_time =
 				dc->public.debug.sr_enter_plus_exit_time_ns / 1000.0;
 	}
 
-	if ((int)(dc->dcn_soc.urgent_latency * 1000) != dc->public.debug.urgent_latency_ns
+	if ((int)(dc->dcn_soc->urgent_latency * 1000) != dc->public.debug.urgent_latency_ns
 			&& dc->public.debug.urgent_latency_ns) {
 		updated = true;
-		dc->dcn_soc.urgent_latency = dc->public.debug.urgent_latency_ns / 1000.0;
+		dc->dcn_soc->urgent_latency = dc->public.debug.urgent_latency_ns / 1000.0;
 	}
 
-	if ((int)(dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency * 1000)
+	if ((int)(dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency * 1000)
 				!= dc->public.debug.percent_of_ideal_drambw
 			&& dc->public.debug.percent_of_ideal_drambw) {
 		updated = true;
-		dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency =
+		dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency =
 				dc->public.debug.percent_of_ideal_drambw;
 	}
 
-	if ((int)(dc->dcn_soc.dram_clock_change_latency * 1000)
+	if ((int)(dc->dcn_soc->dram_clock_change_latency * 1000)
 				!= dc->public.debug.dram_clock_change_latency_ns
 			&& dc->public.debug.dram_clock_change_latency_ns) {
 		updated = true;
-		dc->dcn_soc.dram_clock_change_latency =
+		dc->dcn_soc->dram_clock_change_latency =
 				dc->public.debug.dram_clock_change_latency_ns / 1000.0;
 	}
 	kernel_fpu_end();
@@ -735,83 +735,83 @@ bool dcn_validate_bandwidth(
 
 	memset(v, 0, sizeof(*v));
 	kernel_fpu_begin();
-	v->sr_exit_time = dc->dcn_soc.sr_exit_time;
-	v->sr_enter_plus_exit_time = dc->dcn_soc.sr_enter_plus_exit_time;
-	v->urgent_latency = dc->dcn_soc.urgent_latency;
-	v->write_back_latency = dc->dcn_soc.write_back_latency;
+	v->sr_exit_time = dc->dcn_soc->sr_exit_time;
+	v->sr_enter_plus_exit_time = dc->dcn_soc->sr_enter_plus_exit_time;
+	v->urgent_latency = dc->dcn_soc->urgent_latency;
+	v->write_back_latency = dc->dcn_soc->write_back_latency;
 	v->percent_of_ideal_drambw_received_after_urg_latency =
-			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency;
-
-	v->dcfclkv_min0p65 = dc->dcn_soc.dcfclkv_min0p65;
-	v->dcfclkv_mid0p72 = dc->dcn_soc.dcfclkv_mid0p72;
-	v->dcfclkv_nom0p8 = dc->dcn_soc.dcfclkv_nom0p8;
-	v->dcfclkv_max0p9 = dc->dcn_soc.dcfclkv_max0p9;
-
-	v->max_dispclk_vmin0p65 = dc->dcn_soc.max_dispclk_vmin0p65;
-	v->max_dispclk_vmid0p72 = dc->dcn_soc.max_dispclk_vmid0p72;
-	v->max_dispclk_vnom0p8 = dc->dcn_soc.max_dispclk_vnom0p8;
-	v->max_dispclk_vmax0p9 = dc->dcn_soc.max_dispclk_vmax0p9;
-
-	v->max_dppclk_vmin0p65 = dc->dcn_soc.max_dppclk_vmin0p65;
-	v->max_dppclk_vmid0p72 = dc->dcn_soc.max_dppclk_vmid0p72;
-	v->max_dppclk_vnom0p8 = dc->dcn_soc.max_dppclk_vnom0p8;
-	v->max_dppclk_vmax0p9 = dc->dcn_soc.max_dppclk_vmax0p9;
-
-	v->socclk = dc->dcn_soc.socclk;
-
-	v->fabric_and_dram_bandwidth_vmin0p65 = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65;
-	v->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72;
-	v->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8;
-	v->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9;
-
-	v->phyclkv_min0p65 = dc->dcn_soc.phyclkv_min0p65;
-	v->phyclkv_mid0p72 = dc->dcn_soc.phyclkv_mid0p72;
-	v->phyclkv_nom0p8 = dc->dcn_soc.phyclkv_nom0p8;
-	v->phyclkv_max0p9 = dc->dcn_soc.phyclkv_max0p9;
-
-	v->downspreading = dc->dcn_soc.downspreading;
-	v->round_trip_ping_latency_cycles = dc->dcn_soc.round_trip_ping_latency_cycles;
-	v->urgent_out_of_order_return_per_channel = dc->dcn_soc.urgent_out_of_order_return_per_channel;
-	v->number_of_channels = dc->dcn_soc.number_of_channels;
-	v->vmm_page_size = dc->dcn_soc.vmm_page_size;
-	v->dram_clock_change_latency = dc->dcn_soc.dram_clock_change_latency;
-	v->return_bus_width = dc->dcn_soc.return_bus_width;
-
-	v->rob_buffer_size_in_kbyte = dc->dcn_ip.rob_buffer_size_in_kbyte;
-	v->det_buffer_size_in_kbyte = dc->dcn_ip.det_buffer_size_in_kbyte;
-	v->dpp_output_buffer_pixels = dc->dcn_ip.dpp_output_buffer_pixels;
-	v->opp_output_buffer_lines = dc->dcn_ip.opp_output_buffer_lines;
-	v->pixel_chunk_size_in_kbyte = dc->dcn_ip.pixel_chunk_size_in_kbyte;
-	v->pte_enable = dc->dcn_ip.pte_enable;
-	v->pte_chunk_size = dc->dcn_ip.pte_chunk_size;
-	v->meta_chunk_size = dc->dcn_ip.meta_chunk_size;
-	v->writeback_chunk_size = dc->dcn_ip.writeback_chunk_size;
-	v->odm_capability = dc->dcn_ip.odm_capability;
-	v->dsc_capability = dc->dcn_ip.dsc_capability;
-	v->line_buffer_size = dc->dcn_ip.line_buffer_size;
-	v->is_line_buffer_bpp_fixed = dc->dcn_ip.is_line_buffer_bpp_fixed;
-	v->line_buffer_fixed_bpp = dc->dcn_ip.line_buffer_fixed_bpp;
-	v->max_line_buffer_lines = dc->dcn_ip.max_line_buffer_lines;
-	v->writeback_luma_buffer_size = dc->dcn_ip.writeback_luma_buffer_size;
-	v->writeback_chroma_buffer_size = dc->dcn_ip.writeback_chroma_buffer_size;
-	v->max_num_dpp = dc->dcn_ip.max_num_dpp;
-	v->max_num_writeback = dc->dcn_ip.max_num_writeback;
-	v->max_dchub_topscl_throughput = dc->dcn_ip.max_dchub_topscl_throughput;
-	v->max_pscl_tolb_throughput = dc->dcn_ip.max_pscl_tolb_throughput;
-	v->max_lb_tovscl_throughput = dc->dcn_ip.max_lb_tovscl_throughput;
-	v->max_vscl_tohscl_throughput = dc->dcn_ip.max_vscl_tohscl_throughput;
-	v->max_hscl_ratio = dc->dcn_ip.max_hscl_ratio;
-	v->max_vscl_ratio = dc->dcn_ip.max_vscl_ratio;
-	v->max_hscl_taps = dc->dcn_ip.max_hscl_taps;
-	v->max_vscl_taps = dc->dcn_ip.max_vscl_taps;
-	v->under_scan_factor = dc->dcn_ip.under_scan_factor;
-	v->pte_buffer_size_in_requests = dc->dcn_ip.pte_buffer_size_in_requests;
-	v->dispclk_ramping_margin = dc->dcn_ip.dispclk_ramping_margin;
-	v->max_inter_dcn_tile_repeaters = dc->dcn_ip.max_inter_dcn_tile_repeaters;
+			dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency;
+
+	v->dcfclkv_min0p65 = dc->dcn_soc->dcfclkv_min0p65;
+	v->dcfclkv_mid0p72 = dc->dcn_soc->dcfclkv_mid0p72;
+	v->dcfclkv_nom0p8 = dc->dcn_soc->dcfclkv_nom0p8;
+	v->dcfclkv_max0p9 = dc->dcn_soc->dcfclkv_max0p9;
+
+	v->max_dispclk_vmin0p65 = dc->dcn_soc->max_dispclk_vmin0p65;
+	v->max_dispclk_vmid0p72 = dc->dcn_soc->max_dispclk_vmid0p72;
+	v->max_dispclk_vnom0p8 = dc->dcn_soc->max_dispclk_vnom0p8;
+	v->max_dispclk_vmax0p9 = dc->dcn_soc->max_dispclk_vmax0p9;
+
+	v->max_dppclk_vmin0p65 = dc->dcn_soc->max_dppclk_vmin0p65;
+	v->max_dppclk_vmid0p72 = dc->dcn_soc->max_dppclk_vmid0p72;
+	v->max_dppclk_vnom0p8 = dc->dcn_soc->max_dppclk_vnom0p8;
+	v->max_dppclk_vmax0p9 = dc->dcn_soc->max_dppclk_vmax0p9;
+
+	v->socclk = dc->dcn_soc->socclk;
+
+	v->fabric_and_dram_bandwidth_vmin0p65 = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65;
+	v->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72;
+	v->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8;
+	v->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9;
+
+	v->phyclkv_min0p65 = dc->dcn_soc->phyclkv_min0p65;
+	v->phyclkv_mid0p72 = dc->dcn_soc->phyclkv_mid0p72;
+	v->phyclkv_nom0p8 = dc->dcn_soc->phyclkv_nom0p8;
+	v->phyclkv_max0p9 = dc->dcn_soc->phyclkv_max0p9;
+
+	v->downspreading = dc->dcn_soc->downspreading;
+	v->round_trip_ping_latency_cycles = dc->dcn_soc->round_trip_ping_latency_cycles;
+	v->urgent_out_of_order_return_per_channel = dc->dcn_soc->urgent_out_of_order_return_per_channel;
+	v->number_of_channels = dc->dcn_soc->number_of_channels;
+	v->vmm_page_size = dc->dcn_soc->vmm_page_size;
+	v->dram_clock_change_latency = dc->dcn_soc->dram_clock_change_latency;
+	v->return_bus_width = dc->dcn_soc->return_bus_width;
+
+	v->rob_buffer_size_in_kbyte = dc->dcn_ip->rob_buffer_size_in_kbyte;
+	v->det_buffer_size_in_kbyte = dc->dcn_ip->det_buffer_size_in_kbyte;
+	v->dpp_output_buffer_pixels = dc->dcn_ip->dpp_output_buffer_pixels;
+	v->opp_output_buffer_lines = dc->dcn_ip->opp_output_buffer_lines;
+	v->pixel_chunk_size_in_kbyte = dc->dcn_ip->pixel_chunk_size_in_kbyte;
+	v->pte_enable = dc->dcn_ip->pte_enable;
+	v->pte_chunk_size = dc->dcn_ip->pte_chunk_size;
+	v->meta_chunk_size = dc->dcn_ip->meta_chunk_size;
+	v->writeback_chunk_size = dc->dcn_ip->writeback_chunk_size;
+	v->odm_capability = dc->dcn_ip->odm_capability;
+	v->dsc_capability = dc->dcn_ip->dsc_capability;
+	v->line_buffer_size = dc->dcn_ip->line_buffer_size;
+	v->is_line_buffer_bpp_fixed = dc->dcn_ip->is_line_buffer_bpp_fixed;
+	v->line_buffer_fixed_bpp = dc->dcn_ip->line_buffer_fixed_bpp;
+	v->max_line_buffer_lines = dc->dcn_ip->max_line_buffer_lines;
+	v->writeback_luma_buffer_size = dc->dcn_ip->writeback_luma_buffer_size;
+	v->writeback_chroma_buffer_size = dc->dcn_ip->writeback_chroma_buffer_size;
+	v->max_num_dpp = dc->dcn_ip->max_num_dpp;
+	v->max_num_writeback = dc->dcn_ip->max_num_writeback;
+	v->max_dchub_topscl_throughput = dc->dcn_ip->max_dchub_topscl_throughput;
+	v->max_pscl_tolb_throughput = dc->dcn_ip->max_pscl_tolb_throughput;
+	v->max_lb_tovscl_throughput = dc->dcn_ip->max_lb_tovscl_throughput;
+	v->max_vscl_tohscl_throughput = dc->dcn_ip->max_vscl_tohscl_throughput;
+	v->max_hscl_ratio = dc->dcn_ip->max_hscl_ratio;
+	v->max_vscl_ratio = dc->dcn_ip->max_vscl_ratio;
+	v->max_hscl_taps = dc->dcn_ip->max_hscl_taps;
+	v->max_vscl_taps = dc->dcn_ip->max_vscl_taps;
+	v->under_scan_factor = dc->dcn_ip->under_scan_factor;
+	v->pte_buffer_size_in_requests = dc->dcn_ip->pte_buffer_size_in_requests;
+	v->dispclk_ramping_margin = dc->dcn_ip->dispclk_ramping_margin;
+	v->max_inter_dcn_tile_repeaters = dc->dcn_ip->max_inter_dcn_tile_repeaters;
 	v->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one =
-			dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one;
+			dc->dcn_ip->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one;
 	v->bug_forcing_luma_and_chroma_request_to_same_size_fixed =
-			dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed;
+			dc->dcn_ip->bug_forcing_luma_and_chroma_request_to_same_size_fixed;
 
 	v->voltage[5] = dcn_bw_no_support;
 	v->voltage[4] = dcn_bw_v_max0p9;
@@ -1021,7 +1021,7 @@ bool dcn_validate_bandwidth(
 		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
 		context->bw.dcn.calc_clk.dispclk_khz = (int)(v->dispclk * 1000);
 		if (dc->public.debug.max_disp_clk == true)
-			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc.max_dispclk_vmax0p9 * 1000);
+			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc->max_dispclk_vmax0p9 * 1000);
 		context->bw.dcn.calc_clk.dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
 
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
@@ -1118,15 +1118,15 @@ bool dcn_validate_bandwidth(
 		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
 
 		dc_core->dml.soc.sr_enter_plus_exit_time_us =
-				dc_core->dcn_soc.sr_enter_plus_exit_time;
-		dc_core->dml.soc.sr_exit_time_us = dc_core->dcn_soc.sr_exit_time;
+				dc_core->dcn_soc->sr_enter_plus_exit_time;
+		dc_core->dml.soc.sr_exit_time_us = dc_core->dcn_soc->sr_exit_time;
 	}
 
 	/*
 	 * BW limit is set to prevent display from impacting other system functions
 	 */
 
-	bw_limit = dc->dcn_soc.percent_disp_bw_limit * v->fabric_and_dram_bandwidth_vmax0p9;
+	bw_limit = dc->dcn_soc->percent_disp_bw_limit * v->fabric_and_dram_bandwidth_vmax0p9;
 	bw_limit_pass = (v->total_data_read_bandwidth / 1000.0) < bw_limit;
 
 	kernel_fpu_end();
@@ -1149,41 +1149,41 @@ unsigned int dcn_find_normalized_clock_vdd_Level(
 
 	switch (clocks_type) {
 	case DM_PP_CLOCK_TYPE_DISPLAY_CLK:
-		if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmax0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vnom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
-		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmid0p72*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
-		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmin0p65*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dispclk_vmin0p65*1000) {
 			vdd_level = dcn_bw_v_mid0p72;
 		} else
 			vdd_level = dcn_bw_v_min0p65;
 		break;
 	case DM_PP_CLOCK_TYPE_DISPLAYPHYCLK:
-		if (clocks_in_khz > dc->dcn_soc.phyclkv_max0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->phyclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_nom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
-		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_mid0p72*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
-		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_min0p65*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->phyclkv_min0p65*1000) {
 			vdd_level = dcn_bw_v_mid0p72;
 		} else
 			vdd_level = dcn_bw_v_min0p65;
 		break;
 
 	case DM_PP_CLOCK_TYPE_DPPCLK:
-		if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmax0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmax0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vnom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vnom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
-		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmid0p72*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
-		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmin0p65*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->max_dppclk_vmin0p65*1000) {
 			vdd_level = dcn_bw_v_mid0p72;
 		} else
 			vdd_level = dcn_bw_v_min0p65;
@@ -1191,15 +1191,16 @@ unsigned int dcn_find_normalized_clock_vdd_Level(
 
 	case DM_PP_CLOCK_TYPE_MEMORY_CLK:
 		{
-			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc.number_of_channels);
-			if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
+			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
+
+			if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
 			vdd_level = dcn_bw_v_max0p91;
 				BREAK_TO_DEBUGGER();
-			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
+			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
 				vdd_level = dcn_bw_v_max0p9;
-			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72*1000000/factor) {
+			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72*1000000/factor) {
 				vdd_level = dcn_bw_v_nom0p8;
-			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65*1000000/factor) {
+			} else if (clocks_in_khz > dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65*1000000/factor) {
 				vdd_level = dcn_bw_v_mid0p72;
 			} else
 				vdd_level = dcn_bw_v_min0p65;
@@ -1207,14 +1208,14 @@ unsigned int dcn_find_normalized_clock_vdd_Level(
 		break;
 
 	case DM_PP_CLOCK_TYPE_DCFCLK:
-		if (clocks_in_khz > dc->dcn_soc.dcfclkv_max0p9*1000) {
+		if (clocks_in_khz > dc->dcn_soc->dcfclkv_max0p9*1000) {
 			vdd_level = dcn_bw_v_max0p91;
 			BREAK_TO_DEBUGGER();
-		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_nom0p8*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_nom0p8*1000) {
 			vdd_level = dcn_bw_v_max0p9;
-		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_mid0p72*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_mid0p72*1000) {
 			vdd_level = dcn_bw_v_nom0p8;
-		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_min0p65*1000) {
+		} else if (clocks_in_khz > dc->dcn_soc->dcfclkv_min0p65*1000) {
 			vdd_level = dcn_bw_v_mid0p72;
 		} else
 			vdd_level = dcn_bw_v_min0p65;
@@ -1254,15 +1255,15 @@ unsigned int dcn_find_dcfclk_suits_all(
 	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
 	if (vdd_level == dcn_bw_v_max0p91) {
 		BREAK_TO_DEBUGGER();
-		dcf_clk = dc->dcn_soc.dcfclkv_max0p9*1000;
+		dcf_clk = dc->dcn_soc->dcfclkv_max0p9*1000;
 	} else if (vdd_level == dcn_bw_v_max0p9)
-		dcf_clk =  dc->dcn_soc.dcfclkv_max0p9*1000;
+		dcf_clk =  dc->dcn_soc->dcfclkv_max0p9*1000;
 	else if (vdd_level == dcn_bw_v_nom0p8)
-		dcf_clk =  dc->dcn_soc.dcfclkv_nom0p8*1000;
+		dcf_clk =  dc->dcn_soc->dcfclkv_nom0p8*1000;
 	else if (vdd_level == dcn_bw_v_mid0p72)
-		dcf_clk =  dc->dcn_soc.dcfclkv_mid0p72*1000;
+		dcf_clk =  dc->dcn_soc->dcfclkv_mid0p72*1000;
 	else
-		dcf_clk =  dc->dcn_soc.dcfclkv_min0p65*1000;
+		dcf_clk =  dc->dcn_soc->dcfclkv_min0p65*1000;
 
 	dm_logger_write(dc->ctx->logger, LOG_HW_MARKS,
 		"\tdcf_clk for voltage = %d\n", dcf_clk);
@@ -1282,27 +1283,27 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 			ctx, DM_PP_CLOCK_TYPE_FCLK, &clks) &&
 			clks.num_levels != 0) {
 		ASSERT(clks.num_levels >= 3);
-		dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 = 32 * (clks.data[0].clocks_in_khz / 1000.0) / 1000.0;
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 = 32 * (clks.data[0].clocks_in_khz / 1000.0) / 1000.0;
 		if (clks.num_levels > 2) {
-			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.number_of_channels *
+			dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
 					(clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
 		} else {
-			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.number_of_channels *
+			dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc->number_of_channels *
 					(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
 		}
-		dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc.number_of_channels *
+		dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc->number_of_channels *
 				(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
-		dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc.number_of_channels *
+		dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc->number_of_channels *
 				(clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
 	if (dm_pp_get_clock_levels_by_type_with_voltage(
 				ctx, DM_PP_CLOCK_TYPE_DCFCLK, &clks) &&
 				clks.num_levels >= 3) {
-		dc->dcn_soc.dcfclkv_min0p65 = clks.data[0].clocks_in_khz / 1000.0;
-		dc->dcn_soc.dcfclkv_mid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
-		dc->dcn_soc.dcfclkv_nom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
-		dc->dcn_soc.dcfclkv_max0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_min0p65 = clks.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_mid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_nom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc->dcfclkv_max0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
 
@@ -1315,17 +1316,17 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
 		nom_dcfclk_khz, mid_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
-	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc.number_of_channels);
+	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc->number_of_channels);
 
 	kernel_fpu_begin();
-	max_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
-	nom_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
-	mid_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 * 1000000 / factor;
-	min_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
-	max_dcfclk_khz = dc->dcn_soc.dcfclkv_max0p9 * 1000;
-	nom_dcfclk_khz = dc->dcn_soc.dcfclkv_nom0p8 * 1000;
-	min_dcfclk_khz = dc->dcn_soc.dcfclkv_min0p65 * 1000;
-	socclk_khz = dc->dcn_soc.socclk * 1000;
+	max_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
+	nom_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
+	mid_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 * 1000000 / factor;
+	min_fclk_khz = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
+	max_dcfclk_khz = dc->dcn_soc->dcfclkv_max0p9 * 1000;
+	nom_dcfclk_khz = dc->dcn_soc->dcfclkv_nom0p8 * 1000;
+	min_dcfclk_khz = dc->dcn_soc->dcfclkv_min0p65 * 1000;
+	socclk_khz = dc->dcn_soc->socclk * 1000;
 	kernel_fpu_end();
 
 	/* Now notify PPLib/SMU about which Watermarks sets they should select
@@ -1425,40 +1426,40 @@ void dcn_bw_sync_calcs_and_dml(struct core_dc *dc)
 			"vmm_page_size: %d Bytes\n"
 			"dram_clock_change_latency: %d ns\n"
 			"return_bus_width: %d Bytes\n",
-			dc->dcn_soc.sr_exit_time * 1000,
-			dc->dcn_soc.sr_enter_plus_exit_time * 1000,
-			dc->dcn_soc.urgent_latency * 1000,
-			dc->dcn_soc.write_back_latency * 1000,
-			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency,
-			dc->dcn_soc.max_request_size,
-			dc->dcn_soc.dcfclkv_max0p9 * 1000,
-			dc->dcn_soc.dcfclkv_nom0p8 * 1000,
-			dc->dcn_soc.dcfclkv_mid0p72 * 1000,
-			dc->dcn_soc.dcfclkv_min0p65 * 1000,
-			dc->dcn_soc.max_dispclk_vmax0p9 * 1000,
-			dc->dcn_soc.max_dispclk_vnom0p8 * 1000,
-			dc->dcn_soc.max_dispclk_vmid0p72 * 1000,
-			dc->dcn_soc.max_dispclk_vmin0p65 * 1000,
-			dc->dcn_soc.max_dppclk_vmax0p9 * 1000,
-			dc->dcn_soc.max_dppclk_vnom0p8 * 1000,
-			dc->dcn_soc.max_dppclk_vmid0p72 * 1000,
-			dc->dcn_soc.max_dppclk_vmin0p65 * 1000,
-			dc->dcn_soc.socclk * 1000,
-			dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000,
-			dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000,
-			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 * 1000,
-			dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000,
-			dc->dcn_soc.phyclkv_max0p9 * 1000,
-			dc->dcn_soc.phyclkv_nom0p8 * 1000,
-			dc->dcn_soc.phyclkv_mid0p72 * 1000,
-			dc->dcn_soc.phyclkv_min0p65 * 1000,
-			dc->dcn_soc.downspreading * 100,
-			dc->dcn_soc.round_trip_ping_latency_cycles,
-			dc->dcn_soc.urgent_out_of_order_return_per_channel,
-			dc->dcn_soc.number_of_channels,
-			dc->dcn_soc.vmm_page_size,
-			dc->dcn_soc.dram_clock_change_latency * 1000,
-			dc->dcn_soc.return_bus_width);
+			dc->dcn_soc->sr_exit_time * 1000,
+			dc->dcn_soc->sr_enter_plus_exit_time * 1000,
+			dc->dcn_soc->urgent_latency * 1000,
+			dc->dcn_soc->write_back_latency * 1000,
+			dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency,
+			dc->dcn_soc->max_request_size,
+			dc->dcn_soc->dcfclkv_max0p9 * 1000,
+			dc->dcn_soc->dcfclkv_nom0p8 * 1000,
+			dc->dcn_soc->dcfclkv_mid0p72 * 1000,
+			dc->dcn_soc->dcfclkv_min0p65 * 1000,
+			dc->dcn_soc->max_dispclk_vmax0p9 * 1000,
+			dc->dcn_soc->max_dispclk_vnom0p8 * 1000,
+			dc->dcn_soc->max_dispclk_vmid0p72 * 1000,
+			dc->dcn_soc->max_dispclk_vmin0p65 * 1000,
+			dc->dcn_soc->max_dppclk_vmax0p9 * 1000,
+			dc->dcn_soc->max_dppclk_vnom0p8 * 1000,
+			dc->dcn_soc->max_dppclk_vmid0p72 * 1000,
+			dc->dcn_soc->max_dppclk_vmin0p65 * 1000,
+			dc->dcn_soc->socclk * 1000,
+			dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9 * 1000,
+			dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8 * 1000,
+			dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72 * 1000,
+			dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65 * 1000,
+			dc->dcn_soc->phyclkv_max0p9 * 1000,
+			dc->dcn_soc->phyclkv_nom0p8 * 1000,
+			dc->dcn_soc->phyclkv_mid0p72 * 1000,
+			dc->dcn_soc->phyclkv_min0p65 * 1000,
+			dc->dcn_soc->downspreading * 100,
+			dc->dcn_soc->round_trip_ping_latency_cycles,
+			dc->dcn_soc->urgent_out_of_order_return_per_channel,
+			dc->dcn_soc->number_of_channels,
+			dc->dcn_soc->vmm_page_size,
+			dc->dcn_soc->dram_clock_change_latency * 1000,
+			dc->dcn_soc->return_bus_width);
 	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
 			"rob_buffer_size_in_kbyte: %d\n"
 			"det_buffer_size_in_kbyte: %d\n"
@@ -1494,120 +1495,120 @@ void dcn_bw_sync_calcs_and_dml(struct core_dc *dc)
 			"can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one: %d\n"
 			"bug_forcing_luma_and_chroma_request_to_same_size_fixed: %d\n"
 			"dcfclk_cstate_latency: %d\n",
-			dc->dcn_ip.rob_buffer_size_in_kbyte,
-			dc->dcn_ip.det_buffer_size_in_kbyte,
-			dc->dcn_ip.dpp_output_buffer_pixels,
-			dc->dcn_ip.opp_output_buffer_lines,
-			dc->dcn_ip.pixel_chunk_size_in_kbyte,
-			dc->dcn_ip.pte_enable,
-			dc->dcn_ip.pte_chunk_size,
-			dc->dcn_ip.meta_chunk_size,
-			dc->dcn_ip.writeback_chunk_size,
-			dc->dcn_ip.odm_capability,
-			dc->dcn_ip.dsc_capability,
-			dc->dcn_ip.line_buffer_size,
-			dc->dcn_ip.max_line_buffer_lines,
-			dc->dcn_ip.is_line_buffer_bpp_fixed,
-			dc->dcn_ip.line_buffer_fixed_bpp,
-			dc->dcn_ip.writeback_luma_buffer_size,
-			dc->dcn_ip.writeback_chroma_buffer_size,
-			dc->dcn_ip.max_num_dpp,
-			dc->dcn_ip.max_num_writeback,
-			dc->dcn_ip.max_dchub_topscl_throughput,
-			dc->dcn_ip.max_pscl_tolb_throughput,
-			dc->dcn_ip.max_lb_tovscl_throughput,
-			dc->dcn_ip.max_vscl_tohscl_throughput,
-			dc->dcn_ip.max_hscl_ratio,
-			dc->dcn_ip.max_vscl_ratio,
-			dc->dcn_ip.max_hscl_taps,
-			dc->dcn_ip.max_vscl_taps,
-			dc->dcn_ip.pte_buffer_size_in_requests,
-			dc->dcn_ip.dispclk_ramping_margin,
-			dc->dcn_ip.under_scan_factor * 100,
-			dc->dcn_ip.max_inter_dcn_tile_repeaters,
-			dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one,
-			dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed,
-			dc->dcn_ip.dcfclk_cstate_latency);
-	dc->dml.soc.vmin.socclk_mhz = dc->dcn_soc.socclk;
-	dc->dml.soc.vmid.socclk_mhz = dc->dcn_soc.socclk;
-	dc->dml.soc.vnom.socclk_mhz = dc->dcn_soc.socclk;
-	dc->dml.soc.vmax.socclk_mhz = dc->dcn_soc.socclk;
-
-	dc->dml.soc.vmin.dcfclk_mhz = dc->dcn_soc.dcfclkv_min0p65;
-	dc->dml.soc.vmid.dcfclk_mhz = dc->dcn_soc.dcfclkv_mid0p72;
-	dc->dml.soc.vnom.dcfclk_mhz = dc->dcn_soc.dcfclkv_nom0p8;
-	dc->dml.soc.vmax.dcfclk_mhz = dc->dcn_soc.dcfclkv_max0p9;
-
-	dc->dml.soc.vmin.dispclk_mhz = dc->dcn_soc.max_dispclk_vmin0p65;
-	dc->dml.soc.vmid.dispclk_mhz = dc->dcn_soc.max_dispclk_vmid0p72;
-	dc->dml.soc.vnom.dispclk_mhz = dc->dcn_soc.max_dispclk_vnom0p8;
-	dc->dml.soc.vmax.dispclk_mhz = dc->dcn_soc.max_dispclk_vmax0p9;
-
-	dc->dml.soc.vmin.dppclk_mhz = dc->dcn_soc.max_dppclk_vmin0p65;
-	dc->dml.soc.vmid.dppclk_mhz = dc->dcn_soc.max_dppclk_vmid0p72;
-	dc->dml.soc.vnom.dppclk_mhz = dc->dcn_soc.max_dppclk_vnom0p8;
-	dc->dml.soc.vmax.dppclk_mhz = dc->dcn_soc.max_dppclk_vmax0p9;
-
-	dc->dml.soc.vmin.phyclk_mhz = dc->dcn_soc.phyclkv_min0p65;
-	dc->dml.soc.vmid.phyclk_mhz = dc->dcn_soc.phyclkv_mid0p72;
-	dc->dml.soc.vnom.phyclk_mhz = dc->dcn_soc.phyclkv_nom0p8;
-	dc->dml.soc.vmax.phyclk_mhz = dc->dcn_soc.phyclkv_max0p9;
-
-	dc->dml.soc.vmin.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65;
-	dc->dml.soc.vmid.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72;
-	dc->dml.soc.vnom.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8;
-	dc->dml.soc.vmax.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9;
-
-	dc->dml.soc.sr_exit_time_us = dc->dcn_soc.sr_exit_time;
-	dc->dml.soc.sr_enter_plus_exit_time_us = dc->dcn_soc.sr_enter_plus_exit_time;
-	dc->dml.soc.urgent_latency_us = dc->dcn_soc.urgent_latency;
-	dc->dml.soc.writeback_latency_us = dc->dcn_soc.write_back_latency;
+			dc->dcn_ip->rob_buffer_size_in_kbyte,
+			dc->dcn_ip->det_buffer_size_in_kbyte,
+			dc->dcn_ip->dpp_output_buffer_pixels,
+			dc->dcn_ip->opp_output_buffer_lines,
+			dc->dcn_ip->pixel_chunk_size_in_kbyte,
+			dc->dcn_ip->pte_enable,
+			dc->dcn_ip->pte_chunk_size,
+			dc->dcn_ip->meta_chunk_size,
+			dc->dcn_ip->writeback_chunk_size,
+			dc->dcn_ip->odm_capability,
+			dc->dcn_ip->dsc_capability,
+			dc->dcn_ip->line_buffer_size,
+			dc->dcn_ip->max_line_buffer_lines,
+			dc->dcn_ip->is_line_buffer_bpp_fixed,
+			dc->dcn_ip->line_buffer_fixed_bpp,
+			dc->dcn_ip->writeback_luma_buffer_size,
+			dc->dcn_ip->writeback_chroma_buffer_size,
+			dc->dcn_ip->max_num_dpp,
+			dc->dcn_ip->max_num_writeback,
+			dc->dcn_ip->max_dchub_topscl_throughput,
+			dc->dcn_ip->max_pscl_tolb_throughput,
+			dc->dcn_ip->max_lb_tovscl_throughput,
+			dc->dcn_ip->max_vscl_tohscl_throughput,
+			dc->dcn_ip->max_hscl_ratio,
+			dc->dcn_ip->max_vscl_ratio,
+			dc->dcn_ip->max_hscl_taps,
+			dc->dcn_ip->max_vscl_taps,
+			dc->dcn_ip->pte_buffer_size_in_requests,
+			dc->dcn_ip->dispclk_ramping_margin,
+			dc->dcn_ip->under_scan_factor * 100,
+			dc->dcn_ip->max_inter_dcn_tile_repeaters,
+			dc->dcn_ip->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one,
+			dc->dcn_ip->bug_forcing_luma_and_chroma_request_to_same_size_fixed,
+			dc->dcn_ip->dcfclk_cstate_latency);
+	dc->dml.soc.vmin.socclk_mhz = dc->dcn_soc->socclk;
+	dc->dml.soc.vmid.socclk_mhz = dc->dcn_soc->socclk;
+	dc->dml.soc.vnom.socclk_mhz = dc->dcn_soc->socclk;
+	dc->dml.soc.vmax.socclk_mhz = dc->dcn_soc->socclk;
+
+	dc->dml.soc.vmin.dcfclk_mhz = dc->dcn_soc->dcfclkv_min0p65;
+	dc->dml.soc.vmid.dcfclk_mhz = dc->dcn_soc->dcfclkv_mid0p72;
+	dc->dml.soc.vnom.dcfclk_mhz = dc->dcn_soc->dcfclkv_nom0p8;
+	dc->dml.soc.vmax.dcfclk_mhz = dc->dcn_soc->dcfclkv_max0p9;
+
+	dc->dml.soc.vmin.dispclk_mhz = dc->dcn_soc->max_dispclk_vmin0p65;
+	dc->dml.soc.vmid.dispclk_mhz = dc->dcn_soc->max_dispclk_vmid0p72;
+	dc->dml.soc.vnom.dispclk_mhz = dc->dcn_soc->max_dispclk_vnom0p8;
+	dc->dml.soc.vmax.dispclk_mhz = dc->dcn_soc->max_dispclk_vmax0p9;
+
+	dc->dml.soc.vmin.dppclk_mhz = dc->dcn_soc->max_dppclk_vmin0p65;
+	dc->dml.soc.vmid.dppclk_mhz = dc->dcn_soc->max_dppclk_vmid0p72;
+	dc->dml.soc.vnom.dppclk_mhz = dc->dcn_soc->max_dppclk_vnom0p8;
+	dc->dml.soc.vmax.dppclk_mhz = dc->dcn_soc->max_dppclk_vmax0p9;
+
+	dc->dml.soc.vmin.phyclk_mhz = dc->dcn_soc->phyclkv_min0p65;
+	dc->dml.soc.vmid.phyclk_mhz = dc->dcn_soc->phyclkv_mid0p72;
+	dc->dml.soc.vnom.phyclk_mhz = dc->dcn_soc->phyclkv_nom0p8;
+	dc->dml.soc.vmax.phyclk_mhz = dc->dcn_soc->phyclkv_max0p9;
+
+	dc->dml.soc.vmin.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmin0p65;
+	dc->dml.soc.vmid.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmid0p72;
+	dc->dml.soc.vnom.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vnom0p8;
+	dc->dml.soc.vmax.dram_bw_per_chan_gbps = dc->dcn_soc->fabric_and_dram_bandwidth_vmax0p9;
+
+	dc->dml.soc.sr_exit_time_us = dc->dcn_soc->sr_exit_time;
+	dc->dml.soc.sr_enter_plus_exit_time_us = dc->dcn_soc->sr_enter_plus_exit_time;
+	dc->dml.soc.urgent_latency_us = dc->dcn_soc->urgent_latency;
+	dc->dml.soc.writeback_latency_us = dc->dcn_soc->write_back_latency;
 	dc->dml.soc.ideal_dram_bw_after_urgent_percent =
-			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency;
-	dc->dml.soc.max_request_size_bytes = dc->dcn_soc.max_request_size;
-	dc->dml.soc.downspread_percent = dc->dcn_soc.downspreading;
+			dc->dcn_soc->percent_of_ideal_drambw_received_after_urg_latency;
+	dc->dml.soc.max_request_size_bytes = dc->dcn_soc->max_request_size;
+	dc->dml.soc.downspread_percent = dc->dcn_soc->downspreading;
 	dc->dml.soc.round_trip_ping_latency_dcfclk_cycles =
-			dc->dcn_soc.round_trip_ping_latency_cycles;
+			dc->dcn_soc->round_trip_ping_latency_cycles;
 	dc->dml.soc.urgent_out_of_order_return_per_channel_bytes =
-			dc->dcn_soc.urgent_out_of_order_return_per_channel;
-	dc->dml.soc.num_chans = dc->dcn_soc.number_of_channels;
-	dc->dml.soc.vmm_page_size_bytes = dc->dcn_soc.vmm_page_size;
-	dc->dml.soc.dram_clock_change_latency_us = dc->dcn_soc.dram_clock_change_latency;
-	dc->dml.soc.return_bus_width_bytes = dc->dcn_soc.return_bus_width;
-
-	dc->dml.ip.rob_buffer_size_kbytes = dc->dcn_ip.rob_buffer_size_in_kbyte;
-	dc->dml.ip.det_buffer_size_kbytes = dc->dcn_ip.det_buffer_size_in_kbyte;
-	dc->dml.ip.dpp_output_buffer_pixels = dc->dcn_ip.dpp_output_buffer_pixels;
-	dc->dml.ip.opp_output_buffer_lines = dc->dcn_ip.opp_output_buffer_lines;
-	dc->dml.ip.pixel_chunk_size_kbytes = dc->dcn_ip.pixel_chunk_size_in_kbyte;
-	dc->dml.ip.pte_enable = dc->dcn_ip.pte_enable == dcn_bw_yes;
-	dc->dml.ip.pte_chunk_size_kbytes = dc->dcn_ip.pte_chunk_size;
-	dc->dml.ip.meta_chunk_size_kbytes = dc->dcn_ip.meta_chunk_size;
-	dc->dml.ip.writeback_chunk_size_kbytes = dc->dcn_ip.writeback_chunk_size;
-	dc->dml.ip.line_buffer_size_bits = dc->dcn_ip.line_buffer_size;
-	dc->dml.ip.max_line_buffer_lines = dc->dcn_ip.max_line_buffer_lines;
-	dc->dml.ip.IsLineBufferBppFixed = dc->dcn_ip.is_line_buffer_bpp_fixed == dcn_bw_yes;
-	dc->dml.ip.LineBufferFixedBpp = dc->dcn_ip.line_buffer_fixed_bpp;
-	dc->dml.ip.writeback_luma_buffer_size_kbytes = dc->dcn_ip.writeback_luma_buffer_size;
-	dc->dml.ip.writeback_chroma_buffer_size_kbytes = dc->dcn_ip.writeback_chroma_buffer_size;
-	dc->dml.ip.max_num_dpp = dc->dcn_ip.max_num_dpp;
-	dc->dml.ip.max_num_wb = dc->dcn_ip.max_num_writeback;
-	dc->dml.ip.max_dchub_pscl_bw_pix_per_clk = dc->dcn_ip.max_dchub_topscl_throughput;
-	dc->dml.ip.max_pscl_lb_bw_pix_per_clk = dc->dcn_ip.max_pscl_tolb_throughput;
-	dc->dml.ip.max_lb_vscl_bw_pix_per_clk = dc->dcn_ip.max_lb_tovscl_throughput;
-	dc->dml.ip.max_vscl_hscl_bw_pix_per_clk = dc->dcn_ip.max_vscl_tohscl_throughput;
-	dc->dml.ip.max_hscl_ratio = dc->dcn_ip.max_hscl_ratio;
-	dc->dml.ip.max_vscl_ratio = dc->dcn_ip.max_vscl_ratio;
-	dc->dml.ip.max_hscl_taps = dc->dcn_ip.max_hscl_taps;
-	dc->dml.ip.max_vscl_taps = dc->dcn_ip.max_vscl_taps;
+			dc->dcn_soc->urgent_out_of_order_return_per_channel;
+	dc->dml.soc.num_chans = dc->dcn_soc->number_of_channels;
+	dc->dml.soc.vmm_page_size_bytes = dc->dcn_soc->vmm_page_size;
+	dc->dml.soc.dram_clock_change_latency_us = dc->dcn_soc->dram_clock_change_latency;
+	dc->dml.soc.return_bus_width_bytes = dc->dcn_soc->return_bus_width;
+
+	dc->dml.ip.rob_buffer_size_kbytes = dc->dcn_ip->rob_buffer_size_in_kbyte;
+	dc->dml.ip.det_buffer_size_kbytes = dc->dcn_ip->det_buffer_size_in_kbyte;
+	dc->dml.ip.dpp_output_buffer_pixels = dc->dcn_ip->dpp_output_buffer_pixels;
+	dc->dml.ip.opp_output_buffer_lines = dc->dcn_ip->opp_output_buffer_lines;
+	dc->dml.ip.pixel_chunk_size_kbytes = dc->dcn_ip->pixel_chunk_size_in_kbyte;
+	dc->dml.ip.pte_enable = dc->dcn_ip->pte_enable == dcn_bw_yes;
+	dc->dml.ip.pte_chunk_size_kbytes = dc->dcn_ip->pte_chunk_size;
+	dc->dml.ip.meta_chunk_size_kbytes = dc->dcn_ip->meta_chunk_size;
+	dc->dml.ip.writeback_chunk_size_kbytes = dc->dcn_ip->writeback_chunk_size;
+	dc->dml.ip.line_buffer_size_bits = dc->dcn_ip->line_buffer_size;
+	dc->dml.ip.max_line_buffer_lines = dc->dcn_ip->max_line_buffer_lines;
+	dc->dml.ip.IsLineBufferBppFixed = dc->dcn_ip->is_line_buffer_bpp_fixed == dcn_bw_yes;
+	dc->dml.ip.LineBufferFixedBpp = dc->dcn_ip->line_buffer_fixed_bpp;
+	dc->dml.ip.writeback_luma_buffer_size_kbytes = dc->dcn_ip->writeback_luma_buffer_size;
+	dc->dml.ip.writeback_chroma_buffer_size_kbytes = dc->dcn_ip->writeback_chroma_buffer_size;
+	dc->dml.ip.max_num_dpp = dc->dcn_ip->max_num_dpp;
+	dc->dml.ip.max_num_wb = dc->dcn_ip->max_num_writeback;
+	dc->dml.ip.max_dchub_pscl_bw_pix_per_clk = dc->dcn_ip->max_dchub_topscl_throughput;
+	dc->dml.ip.max_pscl_lb_bw_pix_per_clk = dc->dcn_ip->max_pscl_tolb_throughput;
+	dc->dml.ip.max_lb_vscl_bw_pix_per_clk = dc->dcn_ip->max_lb_tovscl_throughput;
+	dc->dml.ip.max_vscl_hscl_bw_pix_per_clk = dc->dcn_ip->max_vscl_tohscl_throughput;
+	dc->dml.ip.max_hscl_ratio = dc->dcn_ip->max_hscl_ratio;
+	dc->dml.ip.max_vscl_ratio = dc->dcn_ip->max_vscl_ratio;
+	dc->dml.ip.max_hscl_taps = dc->dcn_ip->max_hscl_taps;
+	dc->dml.ip.max_vscl_taps = dc->dcn_ip->max_vscl_taps;
 	/*pte_buffer_size_in_requests missing in dml*/
-	dc->dml.ip.dispclk_ramp_margin_percent = dc->dcn_ip.dispclk_ramping_margin;
-	dc->dml.ip.underscan_factor = dc->dcn_ip.under_scan_factor;
-	dc->dml.ip.max_inter_dcn_tile_repeaters = dc->dcn_ip.max_inter_dcn_tile_repeaters;
+	dc->dml.ip.dispclk_ramp_margin_percent = dc->dcn_ip->dispclk_ramping_margin;
+	dc->dml.ip.underscan_factor = dc->dcn_ip->under_scan_factor;
+	dc->dml.ip.max_inter_dcn_tile_repeaters = dc->dcn_ip->max_inter_dcn_tile_repeaters;
 	dc->dml.ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one =
-		dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one == dcn_bw_yes;
+		dc->dcn_ip->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one == dcn_bw_yes;
 	dc->dml.ip.bug_forcing_LC_req_same_size_fixed =
-		dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed == dcn_bw_yes;
-	dc->dml.ip.dcfclk_cstate_latency = dc->dcn_ip.dcfclk_cstate_latency;
+		dc->dcn_ip->bug_forcing_luma_and_chroma_request_to_same_size_fixed == dcn_bw_yes;
+	dc->dml.ip.dcfclk_cstate_latency = dc->dcn_ip->dcfclk_cstate_latency;
 	kernel_fpu_end();
 }

commit 0d18d7bb80132467a3b88a2fe7277dc1fee65353
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Tue Aug 1 21:13:24 2017 -0400

    drm/amd/display: fix dlg ttu calculation input
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 13b7d8872f97..67da973c898b 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -401,7 +401,8 @@ static void pipe_ctx_to_e2e_pipe_params (
 static void dcn_bw_calc_rq_dlg_ttu(
 		const struct core_dc *dc,
 		const struct dcn_bw_internal_vars *v,
-		struct pipe_ctx *pipe)
+		struct pipe_ctx *pipe,
+		int in_idx)
 {
 	struct display_mode_lib *dml = (struct display_mode_lib *)(&dc->dml);
 	struct _vcs_dpi_display_dlg_regs_st *dlg_regs = &pipe->dlg_regs;
@@ -439,6 +440,21 @@ static void dcn_bw_calc_rq_dlg_ttu(
 	input.clks_cfg.socclk_mhz = v->socclk;
 	input.clks_cfg.voltage = v->voltage_level;
 //	dc->dml.logger = pool->base.logger;
+	input.dout.output_format = (v->output_format[in_idx] == dcn_bw_420) ? dm_420 : dm_444;
+	input.dout.output_type  = (v->output[in_idx] == dcn_bw_hdmi) ? dm_hdmi : dm_dp;
+	//input[in_idx].dout.output_standard;
+	switch (v->output_deep_color[in_idx]) {
+	case dcn_bw_encoder_12bpc:
+		input.dout.output_bpc = dm_out_12;
+	break;
+	case dcn_bw_encoder_10bpc:
+		input.dout.output_bpc = dm_out_10;
+	break;
+	case dcn_bw_encoder_8bpc:
+	default:
+		input.dout.output_bpc = dm_out_8;
+	break;
+	}
 
 	/*todo: soc->sr_enter_plus_exit_time??*/
 	dlg_sys_param.t_srx_delay_us = dc->dcn_ip.dcfclk_cstate_latency / v->dcf_clk_deep_sleep;
@@ -487,6 +503,21 @@ static void dcn_dml_wm_override(
 		input[in_idx].clks_cfg.refclk_mhz = pool->ref_clock_inKhz / 1000;
 		input[in_idx].clks_cfg.socclk_mhz = v->socclk;
 		input[in_idx].clks_cfg.voltage = v->voltage_level;
+		input[in_idx].dout.output_format = (v->output_format[in_idx] == dcn_bw_420) ? dm_420 : dm_444;
+		input[in_idx].dout.output_type  = (v->output[in_idx] == dcn_bw_hdmi) ? dm_hdmi : dm_dp;
+		//input[in_idx].dout.output_standard;
+		switch (v->output_deep_color[in_idx]) {
+		case dcn_bw_encoder_12bpc:
+			input[in_idx].dout.output_bpc = dm_out_12;
+		break;
+		case dcn_bw_encoder_10bpc:
+			input[in_idx].dout.output_bpc = dm_out_10;
+		break;
+		case dcn_bw_encoder_8bpc:
+		default:
+			input[in_idx].dout.output_bpc = dm_out_8;
+		break;
+		}
 		pipe_ctx_to_e2e_pipe_params(pipe, &input[in_idx].pipe);
 		dml_rq_dlg_get_rq_reg(
 			dml,
@@ -1060,7 +1091,7 @@ bool dcn_validate_bandwidth(
 							pipe, hsplit_pipe);
 					}
 
-					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe);
+					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe, input_idx);
 				} else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) {
 					/* merge previously split pipe */
 					pipe->bottom_pipe = hsplit_pipe->bottom_pipe;
@@ -1073,7 +1104,7 @@ bool dcn_validate_bandwidth(
 					resource_build_scaling_params(pipe);
 				}
 				/* for now important to do this after pipe split for building e2e params */
-				dcn_bw_calc_rq_dlg_ttu(dc, v, pipe);
+				dcn_bw_calc_rq_dlg_ttu(dc, v, pipe, input_idx);
 			}
 
 			input_idx++;

commit 6dd28867b1f964226c1c0b1600ecbfa4f8f98bba
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Mon Jul 31 15:35:01 2017 -0400

    drm/amd/display: fix PHYCLK in formula.
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1922c13d6f22..13b7d8872f97 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -856,18 +856,6 @@ bool dcn_validate_bandwidth(
 				- pipe->stream->timing.v_front_porch;
 		v->vactive[input_idx] = pipe->stream->timing.v_addressable;
 		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
-		if (pipe->stream->sink->sink_signal ==  SIGNAL_TYPE_HDMI_TYPE_A) {
-			switch (pipe->stream->timing.display_color_depth) {
-			case COLOR_DEPTH_101010:
-					v->pixel_clock[input_idx]  = (v->pixel_clock[input_idx] * 30) / 24;
-				break;
-			case COLOR_DEPTH_121212:
-				v->pixel_clock[input_idx]  = (v->pixel_clock[input_idx] * 36) / 24;
-				break;
-			default:
-				break;
-			}
-		}
 
 		if (!pipe->plane_state) {
 			v->dcc_enable[input_idx] = dcn_bw_yes;
@@ -938,6 +926,22 @@ bool dcn_validate_bandwidth(
 				PIXEL_ENCODING_YCBCR420 ? dcn_bw_420 : dcn_bw_444;
 		v->output[input_idx] = pipe->stream->sink->sink_signal ==
 				SIGNAL_TYPE_HDMI_TYPE_A ? dcn_bw_hdmi : dcn_bw_dp;
+		v->output_deep_color[input_idx] = dcn_bw_encoder_8bpc;
+		if (v->output[input_idx] == dcn_bw_hdmi) {
+			switch (pipe->stream->timing.display_color_depth) {
+			case COLOR_DEPTH_101010:
+				v->output_deep_color[input_idx] = dcn_bw_encoder_10bpc;
+				break;
+			case COLOR_DEPTH_121212:
+				v->output_deep_color[input_idx]  = dcn_bw_encoder_12bpc;
+				break;
+			case COLOR_DEPTH_161616:
+				v->output_deep_color[input_idx]  = dcn_bw_encoder_16bpc;
+				break;
+			default:
+				break;
+			}
+		}
 
 		input_idx++;
 	}

commit 86a66c4eb7365c96230bca218634439f7b057306
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 11:55:55 2017 -0400

    drm/amd/display: Move mi, ipp, xfm to plane_res
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.mi/\.plane_res.mi/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->mi/->plane_res.mi/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.ipp/\.plane_res.ipp/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->ipp/->plane_res.ipp/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.xfm/\.plane_res.xfm/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->xfm/->plane_res.xfm/g'
    
    To clean up bad renames:
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.min/\.min/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.min/->min/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mic/->mic/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.mis/\.mis/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mid/->mid/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.mid/\.mid/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mis/->mis/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.min/\.min/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.min/->min/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mic/->mic/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.mis/\.mis/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mid/->mid/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.mid/\.mid/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->plane_res\.mis/->mis/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.plane_res\.ipps/\.ipps/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/mpcc_cfg\.plane_res\.mi/mpcc_cfg\.mi/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/mi->plane_res\./mi->/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/cfg->plane_res\./cfg->/g'
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index cb762006e1c3..1922c13d6f22 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -547,9 +547,9 @@ static void split_stream_across_pipes(
 	*secondary_pipe = *primary_pipe;
 
 	secondary_pipe->pipe_idx = pipe_idx;
-	secondary_pipe->mi = pool->mis[secondary_pipe->pipe_idx];
-	secondary_pipe->ipp = pool->ipps[secondary_pipe->pipe_idx];
-	secondary_pipe->xfm = pool->transforms[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.mi = pool->mis[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.ipp = pool->ipps[secondary_pipe->pipe_idx];
+	secondary_pipe->plane_res.xfm = pool->transforms[secondary_pipe->pipe_idx];
 	if (primary_pipe->bottom_pipe) {
 		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;
 		secondary_pipe->bottom_pipe->top_pipe = secondary_pipe;

commit 6702a9ac53f88a373a9969b4dee292f4c5f023f5
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Sun Jul 30 11:51:21 2017 -0400

    drm/amd/display: Move scl_data to plane_res
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/\.scl_data/\.plane_res.scl_data/g'
    
    find -name Makefile -o -name Kconfig -o -name "*.c" -o -name "*.h" \
    -o -name "*.cpp" -o -name "*.hpp" | \
    xargs sed -i 's/->scl_data/->plane_res.scl_data/g'
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 404b39e7b44b..cb762006e1c3 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -244,10 +244,10 @@ static void pipe_ctx_to_e2e_pipe_params (
 	input->src.source_scan         = dm_horz;
 	input->src.sw_mode             = pipe->plane_state->tiling_info.gfx9.swizzle;
 
-	input->src.viewport_width      = pipe->scl_data.viewport.width;
-	input->src.viewport_height     = pipe->scl_data.viewport.height;
-	input->src.data_pitch          = pipe->scl_data.viewport.width;
-	input->src.data_pitch_c        = pipe->scl_data.viewport.width;
+	input->src.viewport_width      = pipe->plane_res.scl_data.viewport.width;
+	input->src.viewport_height     = pipe->plane_res.scl_data.viewport.height;
+	input->src.data_pitch          = pipe->plane_res.scl_data.viewport.width;
+	input->src.data_pitch_c        = pipe->plane_res.scl_data.viewport.width;
 	input->src.cur0_src_width      = 128; /* TODO: Cursor calcs, not curently stored */
 	input->src.cur0_bpp            = 32;
 
@@ -341,21 +341,21 @@ static void pipe_ctx_to_e2e_pipe_params (
 		break;
 	}
 
-	input->scale_taps.htaps                = pipe->scl_data.taps.h_taps;
-	input->scale_ratio_depth.hscl_ratio    = pipe->scl_data.ratios.horz.value/4294967296.0;
-	input->scale_ratio_depth.vscl_ratio    = pipe->scl_data.ratios.vert.value/4294967296.0;
-	input->scale_ratio_depth.vinit =  pipe->scl_data.inits.v.value/4294967296.0;
+	input->scale_taps.htaps                = pipe->plane_res.scl_data.taps.h_taps;
+	input->scale_ratio_depth.hscl_ratio    = pipe->plane_res.scl_data.ratios.horz.value/4294967296.0;
+	input->scale_ratio_depth.vscl_ratio    = pipe->plane_res.scl_data.ratios.vert.value/4294967296.0;
+	input->scale_ratio_depth.vinit =  pipe->plane_res.scl_data.inits.v.value/4294967296.0;
 	if (input->scale_ratio_depth.vinit < 1.0)
 			input->scale_ratio_depth.vinit = 1;
-	input->scale_taps.vtaps = pipe->scl_data.taps.v_taps;
-	input->scale_taps.vtaps_c = pipe->scl_data.taps.v_taps_c;
-	input->scale_taps.htaps_c              = pipe->scl_data.taps.h_taps_c;
-	input->scale_ratio_depth.hscl_ratio_c  = pipe->scl_data.ratios.horz_c.value/4294967296.0;
-	input->scale_ratio_depth.vscl_ratio_c  = pipe->scl_data.ratios.vert_c.value/4294967296.0;
-	input->scale_ratio_depth.vinit_c       = pipe->scl_data.inits.v_c.value/4294967296.0;
+	input->scale_taps.vtaps = pipe->plane_res.scl_data.taps.v_taps;
+	input->scale_taps.vtaps_c = pipe->plane_res.scl_data.taps.v_taps_c;
+	input->scale_taps.htaps_c              = pipe->plane_res.scl_data.taps.h_taps_c;
+	input->scale_ratio_depth.hscl_ratio_c  = pipe->plane_res.scl_data.ratios.horz_c.value/4294967296.0;
+	input->scale_ratio_depth.vscl_ratio_c  = pipe->plane_res.scl_data.ratios.vert_c.value/4294967296.0;
+	input->scale_ratio_depth.vinit_c       = pipe->plane_res.scl_data.inits.v_c.value/4294967296.0;
 	if (input->scale_ratio_depth.vinit_c < 1.0)
 			input->scale_ratio_depth.vinit_c = 1;
-	switch (pipe->scl_data.lb_params.depth) {
+	switch (pipe->plane_res.scl_data.lb_params.depth) {
 	case LB_PIXEL_DEPTH_30BPP:
 		input->scale_ratio_depth.lb_depth = 30; break;
 	case LB_PIXEL_DEPTH_36BPP:
@@ -367,11 +367,11 @@ static void pipe_ctx_to_e2e_pipe_params (
 
 	input->dest.vactive        = pipe->stream->timing.v_addressable;
 
-	input->dest.recout_width   = pipe->scl_data.recout.width;
-	input->dest.recout_height  = pipe->scl_data.recout.height;
+	input->dest.recout_width   = pipe->plane_res.scl_data.recout.width;
+	input->dest.recout_height  = pipe->plane_res.scl_data.recout.height;
 
-	input->dest.full_recout_width   = pipe->scl_data.recout.width;
-	input->dest.full_recout_height  = pipe->scl_data.recout.height;
+	input->dest.full_recout_width   = pipe->plane_res.scl_data.recout.width;
+	input->dest.full_recout_height  = pipe->plane_res.scl_data.recout.height;
 
 	input->dest.htotal         = pipe->stream->timing.h_total;
 	input->dest.hblank_start   = input->dest.htotal - pipe->stream->timing.h_front_porch;
@@ -885,38 +885,38 @@ bool dcn_validate_bandwidth(
 			v->source_scan[input_idx] = dcn_bw_hor;
 
 		} else {
-			v->viewport_height[input_idx] =  pipe->scl_data.viewport.height;
-			v->viewport_width[input_idx] = pipe->scl_data.viewport.width;
-			v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width;
-			v->scaler_recout_height[input_idx] = pipe->scl_data.recout.height;
+			v->viewport_height[input_idx] =  pipe->plane_res.scl_data.viewport.height;
+			v->viewport_width[input_idx] = pipe->plane_res.scl_data.viewport.width;
+			v->scaler_rec_out_width[input_idx] = pipe->plane_res.scl_data.recout.width;
+			v->scaler_recout_height[input_idx] = pipe->plane_res.scl_data.recout.height;
 			if (pipe->bottom_pipe && pipe->bottom_pipe->plane_state == pipe->plane_state) {
 				if (pipe->plane_state->rotation % 2 == 0) {
-					int viewport_end = pipe->scl_data.viewport.width
-							+ pipe->scl_data.viewport.x;
-					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.width
-							+ pipe->bottom_pipe->scl_data.viewport.x;
+					int viewport_end = pipe->plane_res.scl_data.viewport.width
+							+ pipe->plane_res.scl_data.viewport.x;
+					int viewport_b_end = pipe->bottom_pipe->plane_res.scl_data.viewport.width
+							+ pipe->bottom_pipe->plane_res.scl_data.viewport.x;
 
 					if (viewport_end > viewport_b_end)
 						v->viewport_width[input_idx] = viewport_end
-							- pipe->bottom_pipe->scl_data.viewport.x;
+							- pipe->bottom_pipe->plane_res.scl_data.viewport.x;
 					else
 						v->viewport_width[input_idx] = viewport_b_end
-									- pipe->scl_data.viewport.x;
+									- pipe->plane_res.scl_data.viewport.x;
 				} else  {
-					int viewport_end = pipe->scl_data.viewport.height
-						+ pipe->scl_data.viewport.y;
-					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.height
-						+ pipe->bottom_pipe->scl_data.viewport.y;
+					int viewport_end = pipe->plane_res.scl_data.viewport.height
+						+ pipe->plane_res.scl_data.viewport.y;
+					int viewport_b_end = pipe->bottom_pipe->plane_res.scl_data.viewport.height
+						+ pipe->bottom_pipe->plane_res.scl_data.viewport.y;
 
 					if (viewport_end > viewport_b_end)
 						v->viewport_height[input_idx] = viewport_end
-							- pipe->bottom_pipe->scl_data.viewport.y;
+							- pipe->bottom_pipe->plane_res.scl_data.viewport.y;
 					else
 						v->viewport_height[input_idx] = viewport_b_end
-									- pipe->scl_data.viewport.y;
+									- pipe->plane_res.scl_data.viewport.y;
 				}
-				v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width
-						+ pipe->bottom_pipe->scl_data.recout.width;
+				v->scaler_rec_out_width[input_idx] = pipe->plane_res.scl_data.recout.width
+						+ pipe->bottom_pipe->plane_res.scl_data.recout.width;
 			}
 
 			v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;
@@ -924,11 +924,11 @@ bool dcn_validate_bandwidth(
 					pipe->plane_state->format);
 			v->source_surface_mode[input_idx] = tl_sw_mode_to_bw_defs(
 					pipe->plane_state->tiling_info.gfx9.swizzle);
-			v->lb_bit_per_pixel[input_idx] = tl_lb_bpp_to_int(pipe->scl_data.lb_params.depth);
-			v->override_hta_ps[input_idx] = pipe->scl_data.taps.h_taps;
-			v->override_vta_ps[input_idx] = pipe->scl_data.taps.v_taps;
-			v->override_hta_pschroma[input_idx] = pipe->scl_data.taps.h_taps_c;
-			v->override_vta_pschroma[input_idx] = pipe->scl_data.taps.v_taps_c;
+			v->lb_bit_per_pixel[input_idx] = tl_lb_bpp_to_int(pipe->plane_res.scl_data.lb_params.depth);
+			v->override_hta_ps[input_idx] = pipe->plane_res.scl_data.taps.h_taps;
+			v->override_vta_ps[input_idx] = pipe->plane_res.scl_data.taps.v_taps;
+			v->override_hta_pschroma[input_idx] = pipe->plane_res.scl_data.taps.h_taps_c;
+			v->override_vta_pschroma[input_idx] = pipe->plane_res.scl_data.taps.v_taps_c;
 			v->source_scan[input_idx] = (pipe->plane_state->rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
 		}
 		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)

commit 3be5262e353b8ab97c528bfc7d0dd3c820e4ba27
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Thu Jul 27 09:55:38 2017 -0400

    drm/amd/display: Rename more dc_surface stuff to plane_state
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 7f7cb8ff124d..404b39e7b44b 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -233,16 +233,16 @@ static void pipe_ctx_to_e2e_pipe_params (
 		struct _vcs_dpi_display_pipe_params_st *input)
 {
 	input->src.is_hsplit = false;
-	if (pipe->top_pipe != NULL && pipe->top_pipe->surface == pipe->surface)
+	if (pipe->top_pipe != NULL && pipe->top_pipe->plane_state == pipe->plane_state)
 		input->src.is_hsplit = true;
-	else if (pipe->bottom_pipe != NULL && pipe->bottom_pipe->surface == pipe->surface)
+	else if (pipe->bottom_pipe != NULL && pipe->bottom_pipe->plane_state == pipe->plane_state)
 		input->src.is_hsplit = true;
 
-	input->src.dcc                 = pipe->surface->dcc.enable;
+	input->src.dcc                 = pipe->plane_state->dcc.enable;
 	input->src.dcc_rate            = 1;
-	input->src.meta_pitch          = pipe->surface->dcc.grph.meta_pitch;
+	input->src.meta_pitch          = pipe->plane_state->dcc.grph.meta_pitch;
 	input->src.source_scan         = dm_horz;
-	input->src.sw_mode             = pipe->surface->tiling_info.gfx9.swizzle;
+	input->src.sw_mode             = pipe->plane_state->tiling_info.gfx9.swizzle;
 
 	input->src.viewport_width      = pipe->scl_data.viewport.width;
 	input->src.viewport_height     = pipe->scl_data.viewport.height;
@@ -251,7 +251,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	input->src.cur0_src_width      = 128; /* TODO: Cursor calcs, not curently stored */
 	input->src.cur0_bpp            = 32;
 
-	switch (pipe->surface->tiling_info.gfx9.swizzle) {
+	switch (pipe->plane_state->tiling_info.gfx9.swizzle) {
 	/* for 4/8/16 high tiles */
 	case DC_SW_LINEAR:
 		input->src.is_display_sw = 1;
@@ -299,7 +299,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 		break;
 	}
 
-	switch (pipe->surface->rotation) {
+	switch (pipe->plane_state->rotation) {
 	case ROTATION_ANGLE_0:
 	case ROTATION_ANGLE_180:
 		input->src.source_scan = dm_horz;
@@ -314,7 +314,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	}
 
 	/* TODO: Fix pixel format mappings */
-	switch (pipe->surface->format) {
+	switch (pipe->plane_state->format) {
 	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
 	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCrCb:
 		input->src.source_format = dm_420_8;
@@ -455,7 +455,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 			true,
 			true,
 			v->pte_enable == dcn_bw_yes,
-			pipe->surface->flip_immediate);
+			pipe->plane_state->flip_immediate);
 }
 
 static void dcn_dml_wm_override(
@@ -478,7 +478,7 @@ static void dcn_dml_wm_override(
 	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
 		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 
-		if (!pipe->stream || !pipe->surface)
+		if (!pipe->stream || !pipe->plane_state)
 			continue;
 
 		input[in_idx].clks_cfg.dcfclk_mhz = v->dcfclk;
@@ -516,7 +516,7 @@ static void dcn_dml_wm_override(
 	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
 		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
 
-		if (!pipe->stream || !pipe->surface)
+		if (!pipe->stream || !pipe->plane_state)
 			continue;
 
 		dml_rq_dlg_get_dlg_reg(dml,
@@ -527,7 +527,7 @@ static void dcn_dml_wm_override(
 			true,
 			true,
 			v->pte_enable == dcn_bw_yes,
-			pipe->surface->flip_immediate);
+			pipe->plane_state->flip_immediate);
 		in_idx++;
 	}
 	dm_free(input);
@@ -541,7 +541,7 @@ static void split_stream_across_pipes(
 {
 	int pipe_idx = secondary_pipe->pipe_idx;
 
-	if (!primary_pipe->surface)
+	if (!primary_pipe->plane_state)
 		return;
 
 	*secondary_pipe = *primary_pipe;
@@ -843,7 +843,7 @@ bool dcn_validate_bandwidth(
 		if (!pipe->stream)
 			continue;
 		/* skip all but first of split pipes */
-		if (pipe->top_pipe && pipe->top_pipe->surface == pipe->surface)
+		if (pipe->top_pipe && pipe->top_pipe->plane_state == pipe->plane_state)
 			continue;
 
 		v->underscan_output[input_idx] = false; /* taken care of in recout already*/
@@ -869,7 +869,7 @@ bool dcn_validate_bandwidth(
 			}
 		}
 
-		if (!pipe->surface){
+		if (!pipe->plane_state) {
 			v->dcc_enable[input_idx] = dcn_bw_yes;
 			v->source_pixel_format[input_idx] = dcn_bw_rgb_sub_32;
 			v->source_surface_mode[input_idx] = dcn_bw_sw_4_kb_s;
@@ -889,8 +889,8 @@ bool dcn_validate_bandwidth(
 			v->viewport_width[input_idx] = pipe->scl_data.viewport.width;
 			v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width;
 			v->scaler_recout_height[input_idx] = pipe->scl_data.recout.height;
-			if (pipe->bottom_pipe && pipe->bottom_pipe->surface == pipe->surface) {
-				if (pipe->surface->rotation % 2 == 0) {
+			if (pipe->bottom_pipe && pipe->bottom_pipe->plane_state == pipe->plane_state) {
+				if (pipe->plane_state->rotation % 2 == 0) {
 					int viewport_end = pipe->scl_data.viewport.width
 							+ pipe->scl_data.viewport.x;
 					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.width
@@ -919,17 +919,17 @@ bool dcn_validate_bandwidth(
 						+ pipe->bottom_pipe->scl_data.recout.width;
 			}
 
-			v->dcc_enable[input_idx] = pipe->surface->dcc.enable ? dcn_bw_yes : dcn_bw_no;
+			v->dcc_enable[input_idx] = pipe->plane_state->dcc.enable ? dcn_bw_yes : dcn_bw_no;
 			v->source_pixel_format[input_idx] = tl_pixel_format_to_bw_defs(
-					pipe->surface->format);
+					pipe->plane_state->format);
 			v->source_surface_mode[input_idx] = tl_sw_mode_to_bw_defs(
-					pipe->surface->tiling_info.gfx9.swizzle);
+					pipe->plane_state->tiling_info.gfx9.swizzle);
 			v->lb_bit_per_pixel[input_idx] = tl_lb_bpp_to_int(pipe->scl_data.lb_params.depth);
 			v->override_hta_ps[input_idx] = pipe->scl_data.taps.h_taps;
 			v->override_vta_ps[input_idx] = pipe->scl_data.taps.v_taps;
 			v->override_hta_pschroma[input_idx] = pipe->scl_data.taps.h_taps_c;
 			v->override_vta_pschroma[input_idx] = pipe->scl_data.taps.v_taps_c;
-			v->source_scan[input_idx] = (pipe->surface->rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
+			v->source_scan[input_idx] = (pipe->plane_state->rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
 		}
 		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)
 			v->lb_bit_per_pixel[input_idx] = v->line_buffer_fixed_bpp;
@@ -996,7 +996,7 @@ bool dcn_validate_bandwidth(
 			if (!pipe->stream)
 				continue;
 			/* skip all but first of split pipes */
-			if (pipe->top_pipe && pipe->top_pipe->surface == pipe->surface)
+			if (pipe->top_pipe && pipe->top_pipe->plane_state == pipe->plane_state)
 				continue;
 
 			pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
@@ -1024,7 +1024,7 @@ bool dcn_validate_bandwidth(
 			pipe->pipe_dlg_param.vblank_start = asic_blank_start;
 			pipe->pipe_dlg_param.vblank_end = asic_blank_end;
 
-			if (pipe->surface) {
+			if (pipe->plane_state) {
 				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
 
 				if (v->dpp_per_plane[input_idx] == 2 ||
@@ -1036,7 +1036,7 @@ bool dcn_validate_bandwidth(
 					 TIMING_3D_FORMAT_TOP_AND_BOTTOM ||
 					 pipe->stream->timing.timing_3d_format ==
 					 TIMING_3D_FORMAT_SIDE_BY_SIDE))) {
-					if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
+					if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) {
 						/* update previously split pipe */
 						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
 						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx];
@@ -1057,12 +1057,12 @@ bool dcn_validate_bandwidth(
 					}
 
 					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe);
-				} else if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
+				} else if (hsplit_pipe && hsplit_pipe->plane_state == pipe->plane_state) {
 					/* merge previously split pipe */
 					pipe->bottom_pipe = hsplit_pipe->bottom_pipe;
 					if (hsplit_pipe->bottom_pipe)
 						hsplit_pipe->bottom_pipe->top_pipe = pipe;
-					hsplit_pipe->surface = NULL;
+					hsplit_pipe->plane_state = NULL;
 					hsplit_pipe->stream = NULL;
 					hsplit_pipe->top_pipe = NULL;
 					hsplit_pipe->bottom_pipe = NULL;

commit b701542d29d61c3514f51496168831c8476a8e2a
Author: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
Date:   Fri Jul 28 18:21:22 2017 -0400

    drm/amd/display: fix >340 Mhz with deep color pipe split no display
    
     the input to HW formula needs to take care the deep color.
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index c809ad3738e9..7f7cb8ff124d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -856,7 +856,18 @@ bool dcn_validate_bandwidth(
 				- pipe->stream->timing.v_front_porch;
 		v->vactive[input_idx] = pipe->stream->timing.v_addressable;
 		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
-
+		if (pipe->stream->sink->sink_signal ==  SIGNAL_TYPE_HDMI_TYPE_A) {
+			switch (pipe->stream->timing.display_color_depth) {
+			case COLOR_DEPTH_101010:
+					v->pixel_clock[input_idx]  = (v->pixel_clock[input_idx] * 30) / 24;
+				break;
+			case COLOR_DEPTH_121212:
+				v->pixel_clock[input_idx]  = (v->pixel_clock[input_idx] * 36) / 24;
+				break;
+			default:
+				break;
+			}
+		}
 
 		if (!pipe->surface){
 			v->dcc_enable[input_idx] = dcn_bw_yes;

commit e51bf71e8c827e1d71235f6b1fe12a952c3a0514
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Jul 28 18:21:22 2017 -0400

    drm/amd/display: fix bw_calc for hdmi and 420 outputs
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index be6e3ca9fb0d..c809ad3738e9 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -923,8 +923,10 @@ bool dcn_validate_bandwidth(
 		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)
 			v->lb_bit_per_pixel[input_idx] = v->line_buffer_fixed_bpp;
 		v->dcc_rate[input_idx] = 1; /*TODO: Worst case? does this change?*/
-		v->output_format[input_idx] = dcn_bw_444;
-		v->output[input_idx] = dcn_bw_dp;
+		v->output_format[input_idx] = pipe->stream->timing.pixel_encoding ==
+				PIXEL_ENCODING_YCBCR420 ? dcn_bw_420 : dcn_bw_444;
+		v->output[input_idx] = pipe->stream->sink->sink_signal ==
+				SIGNAL_TYPE_HDMI_TYPE_A ? dcn_bw_hdmi : dcn_bw_dp;
 
 		input_idx++;
 	}

commit cdc95cbb4cc29679e6116548d6e39d02a2b91649
Author: Andrey Grodzovsky <Andrey.Grodzovsky@amd.com>
Date:   Wed Jul 26 15:51:31 2017 -0400

    drm/amd/display: add preferred mode from Video Format Preference Data Block
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Vitaly Prosyak <Vitaly.Prosyak@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 875b98dae6e1..be6e3ca9fb0d 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -856,8 +856,6 @@ bool dcn_validate_bandwidth(
 				- pipe->stream->timing.v_front_porch;
 		v->vactive[input_idx] = pipe->stream->timing.v_addressable;
 		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
-		if (pipe->stream->timing.pixel_encoding == PIXEL_ENCODING_YCBCR420)
-			v->pixel_clock[input_idx] /= 2;
 
 
 		if (!pipe->surface){

commit 1c3fb02d2567ae348a3495c93bea991e9d028780
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Wed Jul 26 18:31:25 2017 -0400

    drm/amd/display: update clocks we report to PPlib
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 49b75765d900..875b98dae6e1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1267,13 +1267,14 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 {
 	struct dm_pp_wm_sets_with_clock_ranges_soc15 clk_ranges = {0};
 	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
-		nom_dcfclk_khz, min_dcfclk_khz, socclk_khz;
+		nom_dcfclk_khz, mid_fclk_khz, min_dcfclk_khz, socclk_khz;
 	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
 	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc.number_of_channels);
 
 	kernel_fpu_begin();
 	max_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
 	nom_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
+	mid_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 * 1000000 / factor;
 	min_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
 	max_dcfclk_khz = dc->dcn_soc.dcfclkv_max0p9 * 1000;
 	nom_dcfclk_khz = dc->dcn_soc.dcfclkv_nom0p8 * 1000;
@@ -1293,48 +1294,48 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 	clk_ranges.num_wm_mcif_sets = 4;
 	clk_ranges.wm_dmif_clocks_ranges[0].wm_set_id = WM_SET_A;
 	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_dcfclk_clk_in_khz = min_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_dcfclk_clk_in_khz = nom_dcfclk_khz - 1;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
 	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_mem_clk_in_khz = nom_fclk_khz - 1;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_mem_clk_in_khz = min_fclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[0].wm_set_id = WM_SET_A;
 	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_socclk_clk_in_khz = socclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_socclk_clk_in_khz = overdrive;
 	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_mem_clk_in_khz = nom_fclk_khz - 1;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_mem_clk_in_khz = min_fclk_khz;
 
 	clk_ranges.wm_dmif_clocks_ranges[1].wm_set_id = WM_SET_B;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_dcfclk_clk_in_khz = min_dcfclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_dcfclk_clk_in_khz = nom_dcfclk_khz - 1;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_memg_clk_in_khz = mid_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_mem_clk_in_khz = mid_fclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[1].wm_set_id = WM_SET_B;
 	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_socclk_clk_in_khz = socclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_memg_clk_in_khz = mid_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_mem_clk_in_khz = mid_fclk_khz;
 
 
 	clk_ranges.wm_dmif_clocks_ranges[2].wm_set_id = WM_SET_C;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_dcfclk_clk_in_khz = nom_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
 	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
 	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_mem_clk_in_khz = nom_fclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[2].wm_set_id = WM_SET_C;
 	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_socclk_clk_in_khz = socclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_socclk_clk_in_khz = overdrive;
 	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
-	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_mem_clk_in_khz = nom_fclk_khz;
 
 	clk_ranges.wm_dmif_clocks_ranges[3].wm_set_id = WM_SET_D;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_dcfclk_clk_in_khz = max_dcfclk_khz + 1;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_dcfclk_clk_in_khz = overdrive;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz + 1;
-	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_mem_clk_in_khz = overdrive;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_dcfclk_clk_in_khz = min_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_mem_clk_in_khz = max_fclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[3].wm_set_id = WM_SET_D;
 	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_socclk_clk_in_khz = socclk_khz;
 	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_socclk_clk_in_khz = overdrive;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz + 1;
-	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_mem_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_mem_clk_in_khz = max_fclk_khz;
 
 	/* Notify PP Lib/SMU which Watermarks to use for which clock ranges */
 	dm_pp_notify_wm_clock_changes_soc15(dc->ctx, &clk_ranges);

commit cc408d726c20f32e4fdd688f870dd2b17960d4a2
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Jul 21 17:46:50 2017 -0400

    drm/amd/display: mpc block redesign
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index ef10a8b49379..49b75765d900 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -547,11 +547,9 @@ static void split_stream_across_pipes(
 	*secondary_pipe = *primary_pipe;
 
 	secondary_pipe->pipe_idx = pipe_idx;
-	secondary_pipe->mpcc = pool->mpcc[secondary_pipe->pipe_idx];
 	secondary_pipe->mi = pool->mis[secondary_pipe->pipe_idx];
 	secondary_pipe->ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->xfm = pool->transforms[secondary_pipe->pipe_idx];
-	secondary_pipe->opp = pool->opps[secondary_pipe->pipe_idx];
 	if (primary_pipe->bottom_pipe) {
 		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;
 		secondary_pipe->bottom_pipe->top_pipe = secondary_pipe;

commit 4fa086b9b6640818c053c79d4d7104790ba76cb7
Author: Leo (Sunpeng) Li <sunpeng.li@amd.com>
Date:   Tue Jul 25 20:51:26 2017 -0400

    drm/amd/display: Roll core_stream into dc_stream
    
    Signed-off-by: Leo (Sunpeng) Li <sunpeng.li@amd.com>
    Reviewed-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1651b7548d40..ef10a8b49379 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -365,7 +365,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	}
 
 
-	input->dest.vactive        = pipe->stream->public.timing.v_addressable;
+	input->dest.vactive        = pipe->stream->timing.v_addressable;
 
 	input->dest.recout_width   = pipe->scl_data.recout.width;
 	input->dest.recout_height  = pipe->scl_data.recout.height;
@@ -373,24 +373,24 @@ static void pipe_ctx_to_e2e_pipe_params (
 	input->dest.full_recout_width   = pipe->scl_data.recout.width;
 	input->dest.full_recout_height  = pipe->scl_data.recout.height;
 
-	input->dest.htotal         = pipe->stream->public.timing.h_total;
-	input->dest.hblank_start   = input->dest.htotal - pipe->stream->public.timing.h_front_porch;
+	input->dest.htotal         = pipe->stream->timing.h_total;
+	input->dest.hblank_start   = input->dest.htotal - pipe->stream->timing.h_front_porch;
 	input->dest.hblank_end     = input->dest.hblank_start
-			- pipe->stream->public.timing.h_addressable
-			- pipe->stream->public.timing.h_border_left
-			- pipe->stream->public.timing.h_border_right;
+			- pipe->stream->timing.h_addressable
+			- pipe->stream->timing.h_border_left
+			- pipe->stream->timing.h_border_right;
 
-	input->dest.vtotal         = pipe->stream->public.timing.v_total;
-	input->dest.vblank_start   = input->dest.vtotal - pipe->stream->public.timing.v_front_porch;
+	input->dest.vtotal         = pipe->stream->timing.v_total;
+	input->dest.vblank_start   = input->dest.vtotal - pipe->stream->timing.v_front_porch;
 	input->dest.vblank_end     = input->dest.vblank_start
-			- pipe->stream->public.timing.v_addressable
-			- pipe->stream->public.timing.v_border_bottom
-			- pipe->stream->public.timing.v_border_top;
-
-	input->dest.vsync_plus_back_porch = pipe->stream->public.timing.v_total
-			- pipe->stream->public.timing.v_addressable
-			- pipe->stream->public.timing.v_front_porch;
-	input->dest.pixel_rate_mhz = pipe->stream->public.timing.pix_clk_khz/1000.0;
+			- pipe->stream->timing.v_addressable
+			- pipe->stream->timing.v_border_bottom
+			- pipe->stream->timing.v_border_top;
+
+	input->dest.vsync_plus_back_porch = pipe->stream->timing.v_total
+			- pipe->stream->timing.v_addressable
+			- pipe->stream->timing.v_front_porch;
+	input->dest.pixel_rate_mhz = pipe->stream->timing.pix_clk_khz/1000.0;
 	input->dest.vstartup_start = pipe->pipe_dlg_param.vstartup_start;
 	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
 	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
@@ -851,14 +851,14 @@ bool dcn_validate_bandwidth(
 		v->underscan_output[input_idx] = false; /* taken care of in recout already*/
 		v->interlace_output[input_idx] = false;
 
-		v->htotal[input_idx] = pipe->stream->public.timing.h_total;
-		v->vtotal[input_idx] = pipe->stream->public.timing.v_total;
-		v->v_sync_plus_back_porch[input_idx] = pipe->stream->public.timing.v_total
-				- pipe->stream->public.timing.v_addressable
-				- pipe->stream->public.timing.v_front_porch;
-		v->vactive[input_idx] = pipe->stream->public.timing.v_addressable;
-		v->pixel_clock[input_idx] = pipe->stream->public.timing.pix_clk_khz / 1000.0f;
-		if (pipe->stream->public.timing.pixel_encoding == PIXEL_ENCODING_YCBCR420)
+		v->htotal[input_idx] = pipe->stream->timing.h_total;
+		v->vtotal[input_idx] = pipe->stream->timing.v_total;
+		v->v_sync_plus_back_porch[input_idx] = pipe->stream->timing.v_total
+				- pipe->stream->timing.v_addressable
+				- pipe->stream->timing.v_front_porch;
+		v->vactive[input_idx] = pipe->stream->timing.v_addressable;
+		v->pixel_clock[input_idx] = pipe->stream->timing.pix_clk_khz / 1000.0f;
+		if (pipe->stream->timing.pixel_encoding == PIXEL_ENCODING_YCBCR420)
 			v->pixel_clock[input_idx] /= 2;
 
 
@@ -867,10 +867,10 @@ bool dcn_validate_bandwidth(
 			v->source_pixel_format[input_idx] = dcn_bw_rgb_sub_32;
 			v->source_surface_mode[input_idx] = dcn_bw_sw_4_kb_s;
 			v->lb_bit_per_pixel[input_idx] = 30;
-			v->viewport_width[input_idx] = pipe->stream->public.timing.h_addressable;
-			v->viewport_height[input_idx] = pipe->stream->public.timing.v_addressable;
-			v->scaler_rec_out_width[input_idx] = pipe->stream->public.timing.h_addressable;
-			v->scaler_recout_height[input_idx] = pipe->stream->public.timing.v_addressable;
+			v->viewport_width[input_idx] = pipe->stream->timing.h_addressable;
+			v->viewport_height[input_idx] = pipe->stream->timing.v_addressable;
+			v->scaler_rec_out_width[input_idx] = pipe->stream->timing.h_addressable;
+			v->scaler_recout_height[input_idx] = pipe->stream->timing.v_addressable;
 			v->override_hta_ps[input_idx] = 1;
 			v->override_vta_ps[input_idx] = 1;
 			v->override_hta_pschroma[input_idx] = 1;
@@ -995,22 +995,22 @@ bool dcn_validate_bandwidth(
 			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
 			pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
-			pipe->pipe_dlg_param.htotal = pipe->stream->public.timing.h_total;
-			pipe->pipe_dlg_param.vtotal = pipe->stream->public.timing.v_total;
-			vesa_sync_start = pipe->stream->public.timing.v_addressable +
-						pipe->stream->public.timing.v_border_bottom +
-						pipe->stream->public.timing.v_front_porch;
+			pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;
+			pipe->pipe_dlg_param.vtotal = pipe->stream->timing.v_total;
+			vesa_sync_start = pipe->stream->timing.v_addressable +
+						pipe->stream->timing.v_border_bottom +
+						pipe->stream->timing.v_front_porch;
 
-			asic_blank_end = (pipe->stream->public.timing.v_total -
+			asic_blank_end = (pipe->stream->timing.v_total -
 						vesa_sync_start -
-						pipe->stream->public.timing.v_border_top)
-			* (pipe->stream->public.timing.flags.INTERLACE ? 1 : 0);
+						pipe->stream->timing.v_border_top)
+			* (pipe->stream->timing.flags.INTERLACE ? 1 : 0);
 
 			asic_blank_start = asic_blank_end +
-						(pipe->stream->public.timing.v_border_top +
-						pipe->stream->public.timing.v_addressable +
-						pipe->stream->public.timing.v_border_bottom)
-			* (pipe->stream->public.timing.flags.INTERLACE ? 1 : 0);
+						(pipe->stream->timing.v_border_top +
+						pipe->stream->timing.v_addressable +
+						pipe->stream->timing.v_border_bottom)
+			* (pipe->stream->timing.flags.INTERLACE ? 1 : 0);
 
 			pipe->pipe_dlg_param.vblank_start = asic_blank_start;
 			pipe->pipe_dlg_param.vblank_end = asic_blank_end;
@@ -1019,13 +1019,13 @@ bool dcn_validate_bandwidth(
 				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
 
 				if (v->dpp_per_plane[input_idx] == 2 ||
-					((pipe->stream->public.view_format ==
+					((pipe->stream->view_format ==
 					  VIEW_3D_FORMAT_SIDE_BY_SIDE ||
-					  pipe->stream->public.view_format ==
+					  pipe->stream->view_format ==
 					  VIEW_3D_FORMAT_TOP_AND_BOTTOM) &&
-					(pipe->stream->public.timing.timing_3d_format ==
+					(pipe->stream->timing.timing_3d_format ==
 					 TIMING_3D_FORMAT_TOP_AND_BOTTOM ||
-					 pipe->stream->public.timing.timing_3d_format ==
+					 pipe->stream->timing.timing_3d_format ==
 					 TIMING_3D_FORMAT_SIDE_BY_SIDE))) {
 					if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
 						/* update previously split pipe */
@@ -1034,8 +1034,8 @@ bool dcn_validate_bandwidth(
 						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
 						hsplit_pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
 
-						hsplit_pipe->pipe_dlg_param.htotal = pipe->stream->public.timing.h_total;
-						hsplit_pipe->pipe_dlg_param.vtotal = pipe->stream->public.timing.v_total;
+						hsplit_pipe->pipe_dlg_param.htotal = pipe->stream->timing.h_total;
+						hsplit_pipe->pipe_dlg_param.vtotal = pipe->stream->timing.v_total;
 						hsplit_pipe->pipe_dlg_param.vblank_start = pipe->pipe_dlg_param.vblank_start;
 						hsplit_pipe->pipe_dlg_param.vblank_end = pipe->pipe_dlg_param.vblank_end;
 					} else {

commit e12cfcb1d447cc937d1abc6f4aab8bbe5f88542e
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Thu Jul 20 11:43:32 2017 -0400

    drm/amd/display: Roll core_surface into dc_surface
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 17b28280236f..1651b7548d40 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -238,11 +238,11 @@ static void pipe_ctx_to_e2e_pipe_params (
 	else if (pipe->bottom_pipe != NULL && pipe->bottom_pipe->surface == pipe->surface)
 		input->src.is_hsplit = true;
 
-	input->src.dcc                 = pipe->surface->public.dcc.enable;
+	input->src.dcc                 = pipe->surface->dcc.enable;
 	input->src.dcc_rate            = 1;
-	input->src.meta_pitch          = pipe->surface->public.dcc.grph.meta_pitch;
+	input->src.meta_pitch          = pipe->surface->dcc.grph.meta_pitch;
 	input->src.source_scan         = dm_horz;
-	input->src.sw_mode             = pipe->surface->public.tiling_info.gfx9.swizzle;
+	input->src.sw_mode             = pipe->surface->tiling_info.gfx9.swizzle;
 
 	input->src.viewport_width      = pipe->scl_data.viewport.width;
 	input->src.viewport_height     = pipe->scl_data.viewport.height;
@@ -251,7 +251,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	input->src.cur0_src_width      = 128; /* TODO: Cursor calcs, not curently stored */
 	input->src.cur0_bpp            = 32;
 
-	switch (pipe->surface->public.tiling_info.gfx9.swizzle) {
+	switch (pipe->surface->tiling_info.gfx9.swizzle) {
 	/* for 4/8/16 high tiles */
 	case DC_SW_LINEAR:
 		input->src.is_display_sw = 1;
@@ -299,7 +299,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 		break;
 	}
 
-	switch (pipe->surface->public.rotation) {
+	switch (pipe->surface->rotation) {
 	case ROTATION_ANGLE_0:
 	case ROTATION_ANGLE_180:
 		input->src.source_scan = dm_horz;
@@ -314,7 +314,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 	}
 
 	/* TODO: Fix pixel format mappings */
-	switch (pipe->surface->public.format) {
+	switch (pipe->surface->format) {
 	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
 	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCrCb:
 		input->src.source_format = dm_420_8;
@@ -455,7 +455,7 @@ static void dcn_bw_calc_rq_dlg_ttu(
 			true,
 			true,
 			v->pte_enable == dcn_bw_yes,
-			pipe->surface->public.flip_immediate);
+			pipe->surface->flip_immediate);
 }
 
 static void dcn_dml_wm_override(
@@ -527,7 +527,7 @@ static void dcn_dml_wm_override(
 			true,
 			true,
 			v->pte_enable == dcn_bw_yes,
-			pipe->surface->public.flip_immediate);
+			pipe->surface->flip_immediate);
 		in_idx++;
 	}
 	dm_free(input);
@@ -883,7 +883,7 @@ bool dcn_validate_bandwidth(
 			v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width;
 			v->scaler_recout_height[input_idx] = pipe->scl_data.recout.height;
 			if (pipe->bottom_pipe && pipe->bottom_pipe->surface == pipe->surface) {
-				if (pipe->surface->public.rotation % 2 == 0) {
+				if (pipe->surface->rotation % 2 == 0) {
 					int viewport_end = pipe->scl_data.viewport.width
 							+ pipe->scl_data.viewport.x;
 					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.width
@@ -912,17 +912,17 @@ bool dcn_validate_bandwidth(
 						+ pipe->bottom_pipe->scl_data.recout.width;
 			}
 
-			v->dcc_enable[input_idx] = pipe->surface->public.dcc.enable ? dcn_bw_yes : dcn_bw_no;
+			v->dcc_enable[input_idx] = pipe->surface->dcc.enable ? dcn_bw_yes : dcn_bw_no;
 			v->source_pixel_format[input_idx] = tl_pixel_format_to_bw_defs(
-					pipe->surface->public.format);
+					pipe->surface->format);
 			v->source_surface_mode[input_idx] = tl_sw_mode_to_bw_defs(
-					pipe->surface->public.tiling_info.gfx9.swizzle);
+					pipe->surface->tiling_info.gfx9.swizzle);
 			v->lb_bit_per_pixel[input_idx] = tl_lb_bpp_to_int(pipe->scl_data.lb_params.depth);
 			v->override_hta_ps[input_idx] = pipe->scl_data.taps.h_taps;
 			v->override_vta_ps[input_idx] = pipe->scl_data.taps.v_taps;
 			v->override_hta_pschroma[input_idx] = pipe->scl_data.taps.h_taps_c;
 			v->override_vta_pschroma[input_idx] = pipe->scl_data.taps.v_taps_c;
-			v->source_scan[input_idx] = (pipe->surface->public.rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
+			v->source_scan[input_idx] = (pipe->surface->rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
 		}
 		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)
 			v->lb_bit_per_pixel[input_idx] = v->line_buffer_fixed_bpp;

commit e63825be738ea0bb975aee43b603ac5f0190d7a8
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Sun Jul 23 16:45:45 2017 -0400

    drm/amd/display: fix YCbCr420 deep color mode not supported
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a1eabc47558e..17b28280236f 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -858,6 +858,8 @@ bool dcn_validate_bandwidth(
 				- pipe->stream->public.timing.v_front_porch;
 		v->vactive[input_idx] = pipe->stream->public.timing.v_addressable;
 		v->pixel_clock[input_idx] = pipe->stream->public.timing.pix_clk_khz / 1000.0f;
+		if (pipe->stream->public.timing.pixel_encoding == PIXEL_ENCODING_YCBCR420)
+			v->pixel_clock[input_idx] /= 2;
 
 
 		if (!pipe->surface){

commit 98e4a22f02da4a66f7652a3301dcb689b739e521
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Jul 18 20:13:05 2017 -0400

    drm/amd/display: revert dcn10 soc defaults to 17 19
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 0ea0dab49e0f..a1eabc47558e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -37,8 +37,8 @@
 /* Defaults from spreadsheet rev#247 */
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		/* latencies */
-		.sr_exit_time = 13, /*us*/
-		.sr_enter_plus_exit_time = 15, /*us*/
+		.sr_exit_time = 17, /*us*/
+		.sr_enter_plus_exit_time = 19, /*us*/
 		.urgent_latency = 4, /*us*/
 		.dram_clock_change_latency = 17, /*us*/
 		.write_back_latency = 12, /*us*/

commit 2ebad8eb19f68638c5f5dee385e05aa466e810e5
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Tue Jul 11 13:40:52 2017 -0400

    drm/amd/display: change non_dpm0 state's default SR latency
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index a1eabc47558e..0ea0dab49e0f 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -37,8 +37,8 @@
 /* Defaults from spreadsheet rev#247 */
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		/* latencies */
-		.sr_exit_time = 17, /*us*/
-		.sr_enter_plus_exit_time = 19, /*us*/
+		.sr_exit_time = 13, /*us*/
+		.sr_enter_plus_exit_time = 15, /*us*/
 		.urgent_latency = 4, /*us*/
 		.dram_clock_change_latency = 17, /*us*/
 		.write_back_latency = 12, /*us*/

commit b884a2ec5863c863a625ef4ee0efee59f969142d
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Thu Jul 6 14:20:25 2017 -0400

    drm/amd/display: call pplib to update clocks
    
    Allow pplib to update fclk and dcfclk for different voltage levels.
    PPlib's values for dispclk and phyclk is not correct, so we are not
    getting it from them. fclk is currently not used correctly, although
    does not effect the actual fclk we request.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 3118c248409f..a1eabc47558e 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1226,28 +1226,13 @@ unsigned int dcn_find_dcfclk_suits_all(
 void dcn_bw_update_from_pplib(struct core_dc *dc)
 {
 	struct dc_context *ctx = dc->ctx;
-	struct dm_pp_clock_levels_with_latency clks = {0};
-	struct dm_pp_clock_levels_with_voltage clks2 = {0};
+	struct dm_pp_clock_levels_with_voltage clks = {0};
 
 	kernel_fpu_begin();
 
+	/* TODO: This is not the proper way to obtain fabric_and_dram_bandwidth, should be min(fclk, memclk) */
+
 	if (dm_pp_get_clock_levels_by_type_with_voltage(
-				ctx, DM_PP_CLOCK_TYPE_DISPLAY_CLK, &clks2) &&
-				clks2.num_levels >= 3) {
-		dc->dcn_soc.max_dispclk_vmin0p65 = clks2.data[0].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dispclk_vmid0p72 = clks2.data[clks2.num_levels - 3].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dispclk_vnom0p8 = clks2.data[clks2.num_levels - 2].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dispclk_vmax0p9 = clks2.data[clks2.num_levels - 1].clocks_in_khz / 1000.0;
-	} else
-		BREAK_TO_DEBUGGER();
-/*
-	if (dm_pp_get_clock_levels_by_type_with_latency(
-			ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &clks) &&
-			clks.num_levels != 0) {
-			//this  is to get DRAM data_rate
-		//FabricAndDRAMBandwidth = min(64*FCLK , Data rate * single_Channel_Width * number of channels);
-	}*/
-	if (dm_pp_get_clock_levels_by_type_with_latency(
 			ctx, DM_PP_CLOCK_TYPE_FCLK, &clks) &&
 			clks.num_levels != 0) {
 		ASSERT(clks.num_levels >= 3);
@@ -1265,7 +1250,7 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 				(clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
-	if (dm_pp_get_clock_levels_by_type_with_latency(
+	if (dm_pp_get_clock_levels_by_type_with_voltage(
 				ctx, DM_PP_CLOCK_TYPE_DCFCLK, &clks) &&
 				clks.num_levels >= 3) {
 		dc->dcn_soc.dcfclkv_min0p65 = clks.data[0].clocks_in_khz / 1000.0;
@@ -1274,30 +1259,7 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 		dc->dcn_soc.dcfclkv_max0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
 	} else
 		BREAK_TO_DEBUGGER();
-	if (dm_pp_get_clock_levels_by_type_with_voltage(
-				ctx, DM_PP_CLOCK_TYPE_DISPLAYPHYCLK, &clks2) &&
-				clks2.num_levels >= 3) {
-		dc->dcn_soc.phyclkv_min0p65 = clks2.data[0].clocks_in_khz / 1000.0;
-		dc->dcn_soc.phyclkv_mid0p72 = clks2.data[clks2.num_levels - 3].clocks_in_khz / 1000.0;
-		dc->dcn_soc.phyclkv_nom0p8 = clks2.data[clks2.num_levels - 2].clocks_in_khz / 1000.0;
-		dc->dcn_soc.phyclkv_max0p9 = clks2.data[clks2.num_levels - 1].clocks_in_khz / 1000.0;
-	} else
-		BREAK_TO_DEBUGGER();
-	if (dm_pp_get_clock_levels_by_type_with_latency(
-				ctx, DM_PP_CLOCK_TYPE_DPPCLK, &clks) &&
-				clks.num_levels >= 3) {
-		dc->dcn_soc.max_dppclk_vmin0p65 = clks.data[0].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dppclk_vmid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dppclk_vnom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
-		dc->dcn_soc.max_dppclk_vmax0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
-	}
 
-	if (dm_pp_get_clock_levels_by_type_with_latency(
-				ctx, DM_PP_CLOCK_TYPE_SOCCLK, &clks) &&
-				clks.num_levels >= 3) {
-		dc->dcn_soc.socclk = clks.data[0].clocks_in_khz / 1000.0;
-	} else
-			BREAK_TO_DEBUGGER();
 	kernel_fpu_end();
 }
 

commit 133e8e1b35c7772f4903d367035e33e02becbb2d
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Wed Jul 5 15:30:18 2017 -0400

    drm/amd/display: Change how we disable pipe split
    
    Before this change, pipe split was disabled by bumping up dpp clock
    bounding box for DPM level 0 and 1, this allows validation to pass
    without splitting at a lower DPM level. This change reverts this
    and instead lowers display clock at DPM level 0, this forces
    configurations that need pipe split at DPM level 0 to go to
    DPM level 1, where they can be driven without split.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 24f8c4496a61..3118c248409f 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -822,8 +822,7 @@ bool dcn_validate_bandwidth(
 	v->phyclk_per_state[0] = v->phyclkv_min0p65;
 
 	if (dc->public.debug.disable_pipe_split) {
-		v->max_dppclk[1] = v->max_dppclk_vnom0p8;
-		v->max_dppclk[0] = v->max_dppclk_vnom0p8;
+		v->max_dispclk[0] = v->max_dppclk_vmin0p65;
 	}
 
 	if (v->voltage_override == dcn_bw_v_max0p9) {

commit 966443b592753f6498f896b4afc7c2df1352af72
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Tue Jun 27 12:09:01 2017 -0400

    drm/amd/display: block modes that require read bw greater than 30%
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 93384a3f3525..24f8c4496a61 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -95,6 +95,9 @@ const struct dcn_soc_bounding_box dcn10_soc_defaults = {
 		.vmm_page_size = 4096, /*bytes*/
 		.return_bus_width = 64, /*bytes*/
 		.max_request_size = 256, /*bytes*/
+
+		/* Depends on user class (client vs embedded, workstation, etc) */
+		.percent_disp_bw_limit = 0.3f /*%*/
 };
 
 const struct dcn_ip_params dcn10_ip_defaults = {
@@ -695,6 +698,8 @@ bool dcn_validate_bandwidth(
 	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;
 	int i, input_idx;
 	int vesa_sync_start, asic_blank_end, asic_blank_start;
+	bool bw_limit_pass;
+	float bw_limit;
 
 	if (dcn_bw_apply_registry_override(DC_TO_CORE(&dc->public)))
 		dcn_bw_sync_calcs_and_dml(DC_TO_CORE(&dc->public));
@@ -1072,8 +1077,19 @@ bool dcn_validate_bandwidth(
 		dc_core->dml.soc.sr_exit_time_us = dc_core->dcn_soc.sr_exit_time;
 	}
 
+	/*
+	 * BW limit is set to prevent display from impacting other system functions
+	 */
+
+	bw_limit = dc->dcn_soc.percent_disp_bw_limit * v->fabric_and_dram_bandwidth_vmax0p9;
+	bw_limit_pass = (v->total_data_read_bandwidth / 1000.0) < bw_limit;
+
 	kernel_fpu_end();
-	return v->voltage_level != 5;
+
+	if (bw_limit_pass && v->voltage_level != 5)
+		return true;
+	else
+		return false;
 }
 
 unsigned int dcn_find_normalized_clock_vdd_Level(

commit f42485bb6d1171272d5ecb5b6213505b14294472
Author: Eric Yang <Eric.Yang2@amd.com>
Date:   Tue Jul 4 14:13:25 2017 -0400

    drm/amd/display: move number of memory channel calc out of pplib call
    
    Move number of memory channel calculation out of dcn_bw_update_from_pplib
    
    Fill in fabric_and_dram_bandwidth for single channel case.
    
    Signed-off-by: Eric Yang <Eric.Yang2@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 58a4b2e90c15..93384a3f3525 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1215,10 +1215,6 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 	struct dm_pp_clock_levels_with_voltage clks2 = {0};
 
 	kernel_fpu_begin();
-	dc->dcn_soc.number_of_channels = dc->ctx->asic_id.vram_width / ddr4_dram_width;
-	ASSERT(dc->dcn_soc.number_of_channels && dc->dcn_soc.number_of_channels < 3);
-	if (dc->dcn_soc.number_of_channels == 0)/*old sbios bug*/
-		dc->dcn_soc.number_of_channels = 2;
 
 	if (dm_pp_get_clock_levels_by_type_with_voltage(
 				ctx, DM_PP_CLOCK_TYPE_DISPLAY_CLK, &clks2) &&

commit d5d4e09f5de96c986a47dc1ced334d6dc47a9959
Author: Tony Cheng <tony.cheng@amd.com>
Date:   Sun Jun 4 12:50:20 2017 -0400

    drm/amd/display: update DPM bounding box
    
    value based on STA  target aligned to FCLK for SS corners with 10% margin
    
    also
    - group all latency together
    - group all voltage state related together
    
    Signed-off-by: Tony Cheng <tony.cheng@amd.com>
    Reviewed-by: Eric Yang <eric.yang2@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 4486121e3986..58a4b2e90c15 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -36,40 +36,65 @@
 
 /* Defaults from spreadsheet rev#247 */
 const struct dcn_soc_bounding_box dcn10_soc_defaults = {
-		.sr_exit_time = 17, /*us*/ /*update based on HW Request for 118773*/
+		/* latencies */
+		.sr_exit_time = 17, /*us*/
 		.sr_enter_plus_exit_time = 19, /*us*/
 		.urgent_latency = 4, /*us*/
+		.dram_clock_change_latency = 17, /*us*/
 		.write_back_latency = 12, /*us*/
 		.percent_of_ideal_drambw_received_after_urg_latency = 80, /*%*/
-		.max_request_size = 256, /*bytes*/
-		.dcfclkv_max0p9 = 600, /*MHz*/
-		.dcfclkv_nom0p8 = 600, /*MHz*/
-		.dcfclkv_mid0p72 = 300, /*MHz*/
-		.dcfclkv_min0p65 = 300, /*MHz*/
-		.max_dispclk_vmax0p9 = 1086, /*MHz*/
-		.max_dispclk_vnom0p8 = 661, /*MHz*/
-		.max_dispclk_vmid0p72 = 608, /*MHz*/
-		.max_dispclk_vmin0p65 = 608, /*MHz*/
-		.max_dppclk_vmax0p9 = 661, /*MHz*/
-		.max_dppclk_vnom0p8 = 661, /*MHz*/
-		.max_dppclk_vmid0p72 = 435, /*MHz*/
-		.max_dppclk_vmin0p65 = 435, /*MHz*/
-		.socclk = 208, /*MHz*/
+
+		/* below default clocks derived from STA target base on
+		 * slow-slow corner + 10% margin with voltages aligned to FCLK.
+		 *
+		 * Use these value if fused value doesn't make sense as earlier
+		 * part don't have correct value fused */
+		/* default DCF CLK DPM on RV*/
+		.dcfclkv_max0p9 = 655,	/* MHz, = 3600/5.5 */
+		.dcfclkv_nom0p8 = 626,	/* MHz, = 3600/5.75 */
+		.dcfclkv_mid0p72 = 600,	/* MHz, = 3600/6, bypass */
+		.dcfclkv_min0p65 = 300,	/* MHz, = 3600/12, bypass */
+
+		/* default DISP CLK voltage state on RV */
+		.max_dispclk_vmax0p9 = 1108,	/* MHz, = 3600/3.25 */
+		.max_dispclk_vnom0p8 = 1029,	/* MHz, = 3600/3.5 */
+		.max_dispclk_vmid0p72 = 960,	/* MHz, = 3600/3.75 */
+		.max_dispclk_vmin0p65 = 626,	/* MHz, = 3600/5.75 */
+
+		/* default DPP CLK voltage state on RV */
+		.max_dppclk_vmax0p9 = 720,	/* MHz, = 3600/5 */
+		.max_dppclk_vnom0p8 = 686,	/* MHz, = 3600/5.25 */
+		.max_dppclk_vmid0p72 = 626,	/* MHz, = 3600/5.75 */
+		.max_dppclk_vmin0p65 = 400,	/* MHz, = 3600/9 */
+
+		/* default PHY CLK voltage state on RV */
+		.phyclkv_max0p9 = 900, /*MHz*/
+		.phyclkv_nom0p8 = 847, /*MHz*/
+		.phyclkv_mid0p72 = 800, /*MHz*/
+		.phyclkv_min0p65 = 600, /*MHz*/
+
+		/* BW depend on FCLK, MCLK, # of channels */
+		/* dual channel BW */
 		.fabric_and_dram_bandwidth_vmax0p9 = 38.4f, /*GB/s*/
-		.fabric_and_dram_bandwidth_vnom0p8 = 34.1f, /*GB/s*/
-		.fabric_and_dram_bandwidth_vmid0p72 = 29.8f, /*GB/s*/
+		.fabric_and_dram_bandwidth_vnom0p8 = 34.133f, /*GB/s*/
+		.fabric_and_dram_bandwidth_vmid0p72 = 29.866f, /*GB/s*/
 		.fabric_and_dram_bandwidth_vmin0p65 = 12.8f, /*GB/s*/
-		.phyclkv_max0p9 = 810, /*MHz*/
-		.phyclkv_nom0p8 = 810, /*MHz*/
-		.phyclkv_mid0p72 = 540, /*MHz*/
-		.phyclkv_min0p65 = 540, /*MHz*/
+		/* single channel BW
+		.fabric_and_dram_bandwidth_vmax0p9 = 19.2f,
+		.fabric_and_dram_bandwidth_vnom0p8 = 17.066f,
+		.fabric_and_dram_bandwidth_vmid0p72 = 14.933f,
+		.fabric_and_dram_bandwidth_vmin0p65 = 12.8f,
+		*/
+
+		.number_of_channels = 2,
+
+		.socclk = 208, /*MHz*/
 		.downspreading = 0.5f, /*%*/
 		.round_trip_ping_latency_cycles = 128, /*DCFCLK Cycles*/
 		.urgent_out_of_order_return_per_channel = 256, /*bytes*/
-		.number_of_channels = 2,
 		.vmm_page_size = 4096, /*bytes*/
-		.dram_clock_change_latency = 17, /*us*/
 		.return_bus_width = 64, /*bytes*/
+		.max_request_size = 256, /*bytes*/
 };
 
 const struct dcn_ip_params dcn10_ip_defaults = {

commit cdc5e04888cec337e44049f9b51d8894364ad52a
Author: Vitaly Prosyak <vitaly.prosyak@amd.com>
Date:   Wed Jun 28 13:36:25 2017 -0500

    drm/amd/display: Fix for hdmi frame pack stereo
    
    Signed-off-by: Vitaly Prosyak <vitaly.prosyak@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 00961bc37200..4486121e3986 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -987,7 +987,15 @@ bool dcn_validate_bandwidth(
 			if (pipe->surface) {
 				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
 
-				if (v->dpp_per_plane[input_idx] == 2) {
+				if (v->dpp_per_plane[input_idx] == 2 ||
+					((pipe->stream->public.view_format ==
+					  VIEW_3D_FORMAT_SIDE_BY_SIDE ||
+					  pipe->stream->public.view_format ==
+					  VIEW_3D_FORMAT_TOP_AND_BOTTOM) &&
+					(pipe->stream->public.timing.timing_3d_format ==
+					 TIMING_3D_FORMAT_TOP_AND_BOTTOM ||
+					 pipe->stream->public.timing.timing_3d_format ==
+					 TIMING_3D_FORMAT_SIDE_BY_SIDE))) {
 					if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
 						/* update previously split pipe */
 						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];

commit cfe4645e17f8dbe680c35c439d000313f2648482
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jun 14 18:58:04 2017 -0400

    drm/amd/display: fix dcn pipe reset sequence
    
    This change fixes dcn10 front end reset sequence. Previously we
    would reset front end during flip which led to issues
    in certain MPO and 4k/5k scenarios. We would also never properly
    power gate our front end.
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 3ec702fecfd1..00961bc37200 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -511,12 +511,14 @@ static void split_stream_across_pipes(
 		struct pipe_ctx *primary_pipe,
 		struct pipe_ctx *secondary_pipe)
 {
+	int pipe_idx = secondary_pipe->pipe_idx;
+
 	if (!primary_pipe->surface)
 		return;
 
-	secondary_pipe->stream = primary_pipe->stream;
-	secondary_pipe->tg = primary_pipe->tg;
+	*secondary_pipe = *primary_pipe;
 
+	secondary_pipe->pipe_idx = pipe_idx;
 	secondary_pipe->mpcc = pool->mpcc[secondary_pipe->pipe_idx];
 	secondary_pipe->mi = pool->mis[secondary_pipe->pipe_idx];
 	secondary_pipe->ipp = pool->ipps[secondary_pipe->pipe_idx];
@@ -528,8 +530,6 @@ static void split_stream_across_pipes(
 	}
 	primary_pipe->bottom_pipe = secondary_pipe;
 	secondary_pipe->top_pipe = primary_pipe;
-	secondary_pipe->surface = primary_pipe->surface;
-	secondary_pipe->pipe_dlg_param = primary_pipe->pipe_dlg_param;
 
 	resource_build_scaling_params(primary_pipe);
 	resource_build_scaling_params(secondary_pipe);
@@ -1011,10 +1011,13 @@ bool dcn_validate_bandwidth(
 					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe);
 				} else if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
 					/* merge previously split pipe */
-					if (pipe->bottom_pipe->bottom_pipe)
-						pipe->bottom_pipe->bottom_pipe->top_pipe = pipe;
-					memset(pipe->bottom_pipe, 0, sizeof(*pipe->bottom_pipe));
-					pipe->bottom_pipe = pipe->bottom_pipe->bottom_pipe;
+					pipe->bottom_pipe = hsplit_pipe->bottom_pipe;
+					if (hsplit_pipe->bottom_pipe)
+						hsplit_pipe->bottom_pipe->top_pipe = pipe;
+					hsplit_pipe->surface = NULL;
+					hsplit_pipe->stream = NULL;
+					hsplit_pipe->top_pipe = NULL;
+					hsplit_pipe->bottom_pipe = NULL;
 					resource_build_scaling_params(pipe);
 				}
 				/* for now important to do this after pipe split for building e2e params */

commit 139cb65c5202ba5074d52226931a7d4aaa3307bc
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jun 21 09:35:35 2017 -0400

    drm/amd/display: make variable latency into a regkey option
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 9cb08365e2b6..3ec702fecfd1 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -906,11 +906,16 @@ bool dcn_validate_bandwidth(
 	scaler_settings_calculation(v);
 	mode_support_and_system_configuration(v);
 
-	if (v->voltage_level == 0) {
+	if (v->voltage_level == 0 &&
+			(dc->public.debug.sr_exit_time_dpm0_ns
+				|| dc->public.debug.sr_enter_plus_exit_time_dpm0_ns)) {
 		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
 
-		v->sr_enter_plus_exit_time = 9.466f;
-		v->sr_exit_time = 7.849f;
+		if (dc->public.debug.sr_enter_plus_exit_time_dpm0_ns)
+			v->sr_enter_plus_exit_time =
+				dc->public.debug.sr_enter_plus_exit_time_dpm0_ns / 1000.0f;
+		if (dc->public.debug.sr_exit_time_dpm0_ns)
+			v->sr_exit_time =  dc->public.debug.sr_exit_time_dpm0_ns / 1000.0f;
 		dc_core->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
 		dc_core->dml.soc.sr_exit_time_us = v->sr_exit_time;
 		mode_support_and_system_configuration(v);

commit cb26f8925c50aa244ac34a72ac232e4c7dbd6f98
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue Jun 20 17:24:23 2017 -0400

    drm/amd/display: use different sr latencies for dpm0 dcn bw calc
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Charlene Liu <Charlene.Liu@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 0aa6662650cc..9cb08365e2b6 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -906,6 +906,16 @@ bool dcn_validate_bandwidth(
 	scaler_settings_calculation(v);
 	mode_support_and_system_configuration(v);
 
+	if (v->voltage_level == 0) {
+		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
+
+		v->sr_enter_plus_exit_time = 9.466f;
+		v->sr_exit_time = 7.849f;
+		dc_core->dml.soc.sr_enter_plus_exit_time_us = v->sr_enter_plus_exit_time;
+		dc_core->dml.soc.sr_exit_time_us = v->sr_exit_time;
+		mode_support_and_system_configuration(v);
+	}
+
 	if (v->voltage_level != 5) {
 		float bw_consumed = v->total_bandwidth_consumed_gbyte_per_second;
 		if (bw_consumed < v->fabric_and_dram_bandwidth_vmin0p65)
@@ -1013,6 +1023,14 @@ bool dcn_validate_bandwidth(
 					&dc->dml, context, pool);
 	}
 
+	if (v->voltage_level == 0) {
+		struct core_dc *dc_core = DC_TO_CORE(&dc->public);
+
+		dc_core->dml.soc.sr_enter_plus_exit_time_us =
+				dc_core->dcn_soc.sr_enter_plus_exit_time;
+		dc_core->dml.soc.sr_exit_time_us = dc_core->dcn_soc.sr_exit_time;
+	}
+
 	kernel_fpu_end();
 	return v->voltage_level != 5;
 }

commit 90f095c13efe2aed108ebd4754dd629946b68168
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Fri Jun 16 11:27:59 2017 -0400

    drm/amd/display: add pipe split disable regkey
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 2b62efce73c5..0aa6662650cc 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -791,7 +791,7 @@ bool dcn_validate_bandwidth(
 	v->phyclk_per_state[1] = v->phyclkv_mid0p72;
 	v->phyclk_per_state[0] = v->phyclkv_min0p65;
 
-	if (dc->public.debug.use_max_voltage) {
+	if (dc->public.debug.disable_pipe_split) {
 		v->max_dppclk[1] = v->max_dppclk_vnom0p8;
 		v->max_dppclk[0] = v->max_dppclk_vnom0p8;
 	}

commit 7f5c22d1652327b64375e88b184b0df502c7bdc7
Author: Vitaly Prosyak <vitaly.prosyak@amd.com>
Date:   Thu Jun 8 15:55:02 2017 -0500

    drm/amd/display: RV stereo support
    
    HDMI frame pack and DP frame alternate in band
    
    Signed-off-by: Vitaly Prosyak <vitaly.prosyak@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 66f0595a4c20..2b62efce73c5 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -972,9 +972,7 @@ bool dcn_validate_bandwidth(
 			if (pipe->surface) {
 				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
 
-				if (v->dpp_per_plane[input_idx] == 2 ||
-						(pipe->stream->public.timing.timing_3d_format == TIMING_3D_FORMAT_TOP_AND_BOTTOM ||
-						 pipe->stream->public.timing.timing_3d_format == TIMING_3D_FORMAT_SIDE_BY_SIDE)) {
+				if (v->dpp_per_plane[input_idx] == 2) {
 					if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
 						/* update previously split pipe */
 						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];

commit f0558542a72e72919dae2ac2187847ec312c2bcb
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Thu Jun 1 18:35:54 2017 -0400

    drm/amd/display: redesign mpc
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index f0f688b99d37..66f0595a4c20 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -517,6 +517,7 @@ static void split_stream_across_pipes(
 	secondary_pipe->stream = primary_pipe->stream;
 	secondary_pipe->tg = primary_pipe->tg;
 
+	secondary_pipe->mpcc = pool->mpcc[secondary_pipe->pipe_idx];
 	secondary_pipe->mi = pool->mis[secondary_pipe->pipe_idx];
 	secondary_pipe->ipp = pool->ipps[secondary_pipe->pipe_idx];
 	secondary_pipe->xfm = pool->transforms[secondary_pipe->pipe_idx];

commit c9742685c24acd6d71cdda3067bfc2f512fe2b7c
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Wed Jun 7 13:53:30 2017 -0400

    drm/amd/display: add bw logging for dcn
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 6d8bc6c74a73..f0f688b99d37 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -1312,6 +1312,144 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 void dcn_bw_sync_calcs_and_dml(struct core_dc *dc)
 {
 	kernel_fpu_begin();
+	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
+			"sr_exit_time: %d ns\n"
+			"sr_enter_plus_exit_time: %d ns\n"
+			"urgent_latency: %d ns\n"
+			"write_back_latency: %d ns\n"
+			"percent_of_ideal_drambw_received_after_urg_latency: %d %\n"
+			"max_request_size: %d bytes\n"
+			"dcfclkv_max0p9: %d kHz\n"
+			"dcfclkv_nom0p8: %d kHz\n"
+			"dcfclkv_mid0p72: %d kHz\n"
+			"dcfclkv_min0p65: %d kHz\n"
+			"max_dispclk_vmax0p9: %d kHz\n"
+			"max_dispclk_vnom0p8: %d kHz\n"
+			"max_dispclk_vmid0p72: %d kHz\n"
+			"max_dispclk_vmin0p65: %d kHz\n"
+			"max_dppclk_vmax0p9: %d kHz\n"
+			"max_dppclk_vnom0p8: %d kHz\n"
+			"max_dppclk_vmid0p72: %d kHz\n"
+			"max_dppclk_vmin0p65: %d kHz\n"
+			"socclk: %d kHz\n"
+			"fabric_and_dram_bandwidth_vmax0p9: %d MB/s\n"
+			"fabric_and_dram_bandwidth_vnom0p8: %d MB/s\n"
+			"fabric_and_dram_bandwidth_vmid0p72: %d MB/s\n"
+			"fabric_and_dram_bandwidth_vmin0p65: %d MB/s\n"
+			"phyclkv_max0p9: %d kHz\n"
+			"phyclkv_nom0p8: %d kHz\n"
+			"phyclkv_mid0p72: %d kHz\n"
+			"phyclkv_min0p65: %d kHz\n"
+			"downspreading: %d %\n"
+			"round_trip_ping_latency_cycles: %d DCFCLK Cycles\n"
+			"urgent_out_of_order_return_per_channel: %d Bytes\n"
+			"number_of_channels: %d\n"
+			"vmm_page_size: %d Bytes\n"
+			"dram_clock_change_latency: %d ns\n"
+			"return_bus_width: %d Bytes\n",
+			dc->dcn_soc.sr_exit_time * 1000,
+			dc->dcn_soc.sr_enter_plus_exit_time * 1000,
+			dc->dcn_soc.urgent_latency * 1000,
+			dc->dcn_soc.write_back_latency * 1000,
+			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency,
+			dc->dcn_soc.max_request_size,
+			dc->dcn_soc.dcfclkv_max0p9 * 1000,
+			dc->dcn_soc.dcfclkv_nom0p8 * 1000,
+			dc->dcn_soc.dcfclkv_mid0p72 * 1000,
+			dc->dcn_soc.dcfclkv_min0p65 * 1000,
+			dc->dcn_soc.max_dispclk_vmax0p9 * 1000,
+			dc->dcn_soc.max_dispclk_vnom0p8 * 1000,
+			dc->dcn_soc.max_dispclk_vmid0p72 * 1000,
+			dc->dcn_soc.max_dispclk_vmin0p65 * 1000,
+			dc->dcn_soc.max_dppclk_vmax0p9 * 1000,
+			dc->dcn_soc.max_dppclk_vnom0p8 * 1000,
+			dc->dcn_soc.max_dppclk_vmid0p72 * 1000,
+			dc->dcn_soc.max_dppclk_vmin0p65 * 1000,
+			dc->dcn_soc.socclk * 1000,
+			dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000,
+			dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000,
+			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 * 1000,
+			dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000,
+			dc->dcn_soc.phyclkv_max0p9 * 1000,
+			dc->dcn_soc.phyclkv_nom0p8 * 1000,
+			dc->dcn_soc.phyclkv_mid0p72 * 1000,
+			dc->dcn_soc.phyclkv_min0p65 * 1000,
+			dc->dcn_soc.downspreading * 100,
+			dc->dcn_soc.round_trip_ping_latency_cycles,
+			dc->dcn_soc.urgent_out_of_order_return_per_channel,
+			dc->dcn_soc.number_of_channels,
+			dc->dcn_soc.vmm_page_size,
+			dc->dcn_soc.dram_clock_change_latency * 1000,
+			dc->dcn_soc.return_bus_width);
+	dm_logger_write(dc->ctx->logger, LOG_BANDWIDTH_CALCS,
+			"rob_buffer_size_in_kbyte: %d\n"
+			"det_buffer_size_in_kbyte: %d\n"
+			"dpp_output_buffer_pixels: %d\n"
+			"opp_output_buffer_lines: %d\n"
+			"pixel_chunk_size_in_kbyte: %d\n"
+			"pte_enable: %d\n"
+			"pte_chunk_size: %d kbytes\n"
+			"meta_chunk_size: %d kbytes\n"
+			"writeback_chunk_size: %d kbytes\n"
+			"odm_capability: %d\n"
+			"dsc_capability: %d\n"
+			"line_buffer_size: %d bits\n"
+			"max_line_buffer_lines: %d\n"
+			"is_line_buffer_bpp_fixed: %d\n"
+			"line_buffer_fixed_bpp: %d\n"
+			"writeback_luma_buffer_size: %d kbytes\n"
+			"writeback_chroma_buffer_size: %d kbytes\n"
+			"max_num_dpp: %d\n"
+			"max_num_writeback: %d\n"
+			"max_dchub_topscl_throughput: %d pixels/dppclk\n"
+			"max_pscl_tolb_throughput: %d pixels/dppclk\n"
+			"max_lb_tovscl_throughput: %d pixels/dppclk\n"
+			"max_vscl_tohscl_throughput: %d pixels/dppclk\n"
+			"max_hscl_ratio: %d\n"
+			"max_vscl_ratio: %d\n"
+			"max_hscl_taps: %d\n"
+			"max_vscl_taps: %d\n"
+			"pte_buffer_size_in_requests: %d\n"
+			"dispclk_ramping_margin: %d %\n"
+			"under_scan_factor: %d %\n"
+			"max_inter_dcn_tile_repeaters: %d\n"
+			"can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one: %d\n"
+			"bug_forcing_luma_and_chroma_request_to_same_size_fixed: %d\n"
+			"dcfclk_cstate_latency: %d\n",
+			dc->dcn_ip.rob_buffer_size_in_kbyte,
+			dc->dcn_ip.det_buffer_size_in_kbyte,
+			dc->dcn_ip.dpp_output_buffer_pixels,
+			dc->dcn_ip.opp_output_buffer_lines,
+			dc->dcn_ip.pixel_chunk_size_in_kbyte,
+			dc->dcn_ip.pte_enable,
+			dc->dcn_ip.pte_chunk_size,
+			dc->dcn_ip.meta_chunk_size,
+			dc->dcn_ip.writeback_chunk_size,
+			dc->dcn_ip.odm_capability,
+			dc->dcn_ip.dsc_capability,
+			dc->dcn_ip.line_buffer_size,
+			dc->dcn_ip.max_line_buffer_lines,
+			dc->dcn_ip.is_line_buffer_bpp_fixed,
+			dc->dcn_ip.line_buffer_fixed_bpp,
+			dc->dcn_ip.writeback_luma_buffer_size,
+			dc->dcn_ip.writeback_chroma_buffer_size,
+			dc->dcn_ip.max_num_dpp,
+			dc->dcn_ip.max_num_writeback,
+			dc->dcn_ip.max_dchub_topscl_throughput,
+			dc->dcn_ip.max_pscl_tolb_throughput,
+			dc->dcn_ip.max_lb_tovscl_throughput,
+			dc->dcn_ip.max_vscl_tohscl_throughput,
+			dc->dcn_ip.max_hscl_ratio,
+			dc->dcn_ip.max_vscl_ratio,
+			dc->dcn_ip.max_hscl_taps,
+			dc->dcn_ip.max_vscl_taps,
+			dc->dcn_ip.pte_buffer_size_in_requests,
+			dc->dcn_ip.dispclk_ramping_margin,
+			dc->dcn_ip.under_scan_factor * 100,
+			dc->dcn_ip.max_inter_dcn_tile_repeaters,
+			dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one,
+			dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed,
+			dc->dcn_ip.dcfclk_cstate_latency);
 	dc->dml.soc.vmin.socclk_mhz = dc->dcn_soc.socclk;
 	dc->dml.soc.vmid.socclk_mhz = dc->dcn_soc.socclk;
 	dc->dml.soc.vnom.socclk_mhz = dc->dcn_soc.socclk;

commit ca3cba9c605348988223f69a83d1c1e7dde7583a
Author: Charlene Liu <charlene.liu@amd.com>
Date:   Mon May 8 17:46:57 2017 -0400

    drm/amd/display: single channel bandwidth verses dual channel bandwidth
    
    DPM0, FCLK=MCLK, single channel bandwidth = dual channel bandwidth
    for the rest of the DPM levels, single channel bandwidth = 1/2  dual channel bandwidth
    
    Signed-off-by: Charlene Liu <charlene.liu@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 26587bcdba96..6d8bc6c74a73 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -920,6 +920,10 @@ bool dcn_validate_bandwidth(
 		calc_wm_sets_and_perf_params(context, v);
 		context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
+		if (bw_consumed == v->fabric_and_dram_bandwidth_vmin0p65) {
+			context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 / 32);
+		}
+
 		context->bw.dcn.calc_clk.dram_ccm_us = (int)(v->dram_clock_change_margin);
 		context->bw.dcn.calc_clk.min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
 		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
@@ -1178,8 +1182,7 @@ void dcn_bw_update_from_pplib(struct core_dc *dc)
 			ctx, DM_PP_CLOCK_TYPE_FCLK, &clks) &&
 			clks.num_levels != 0) {
 		ASSERT(clks.num_levels >= 3);
-		dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 = dc->dcn_soc.number_of_channels *
-			(clks.data[0].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+		dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 = 32 * (clks.data[0].clocks_in_khz / 1000.0) / 1000.0;
 		if (clks.num_levels > 2) {
 			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.number_of_channels *
 					(clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
@@ -1240,7 +1243,7 @@ void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
 	kernel_fpu_begin();
 	max_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
 	nom_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
-	min_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000000 / factor;
+	min_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000000 / 32;
 	max_dcfclk_khz = dc->dcn_soc.dcfclkv_max0p9 * 1000;
 	nom_dcfclk_khz = dc->dcn_soc.dcfclkv_nom0p8 * 1000;
 	min_dcfclk_khz = dc->dcn_soc.dcfclkv_min0p65 * 1000;

commit d4b4597384eac0e2d293912cf4cebded27246301
Author: Yongqiang Sun <yongqiang.sun@amd.com>
Date:   Fri May 5 16:33:11 2017 -0400

    drm/amd/display: Add 64KB_S_T and 64KB_D_T swizzle mode.
    
    Signed-off-by: Yongqiang Sun <yongqiang.sun@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 1b9edfda2b98..26587bcdba96 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -236,6 +236,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 		break;
 	case DC_SW_64KB_S:
 	case DC_SW_64KB_S_X:
+	case DC_SW_64KB_S_T:
 		input->src.is_display_sw = 0;
 		input->src.macro_tile_size = dm_64k_tile;
 		break;
@@ -253,6 +254,7 @@ static void pipe_ctx_to_e2e_pipe_params (
 		break;
 	case DC_SW_64KB_D:
 	case DC_SW_64KB_D_X:
+	case DC_SW_64KB_D_T:
 		input->src.is_display_sw = 1;
 		input->src.macro_tile_size = dm_64k_tile;
 		break;

commit 9037d802a97812cb8d614b48f817a5532cf1558c
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue May 2 17:29:48 2017 -0400

    drm/amd/display: refactor bw related variable structure in val_ctx
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Tony Cheng <Tony.Cheng@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index 22d98ef69a77..1b9edfda2b98 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -470,17 +470,17 @@ static void dcn_dml_wm_override(
 	a.cpstate = dml_wm_cstate_pstate_e2e(dml, input, active_count);
 	a.pte_meta_urgent = dml_wm_pte_meta_urgent(dml, a.urgent);
 
-	context->watermarks.a.cstate_pstate.cstate_exit_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
 			a.cpstate.cstate_exit_us * 1000;
-	context->watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
 			a.cpstate.cstate_enter_plus_exit_us * 1000;
-	context->watermarks.a.cstate_pstate.pstate_change_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
 			a.cpstate.pstate_change_us * 1000;
-	context->watermarks.a.pte_meta_urgent_ns = a.pte_meta_urgent * 1000;
-	context->watermarks.a.urgent_ns = a.urgent * 1000;
-	context->watermarks.b = context->watermarks.a;
-	context->watermarks.c = context->watermarks.a;
-	context->watermarks.d = context->watermarks.a;
+	context->bw.dcn.watermarks.a.pte_meta_urgent_ns = a.pte_meta_urgent * 1000;
+	context->bw.dcn.watermarks.a.urgent_ns = a.urgent * 1000;
+	context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
+	context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
+	context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
 
 
 	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
@@ -543,28 +543,28 @@ static void calc_wm_sets_and_perf_params(
 		v->fabric_and_dram_bandwidth = v->fabric_and_dram_bandwidth_vnom0p8;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->watermarks.b.cstate_pstate.cstate_exit_ns =
+		context->bw.dcn.watermarks.b.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->watermarks.b.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw.dcn.watermarks.b.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->watermarks.b.cstate_pstate.pstate_change_ns =
+		context->bw.dcn.watermarks.b.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->watermarks.b.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->watermarks.b.urgent_ns = v->urgent_watermark * 1000;
+		context->bw.dcn.watermarks.b.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw.dcn.watermarks.b.urgent_ns = v->urgent_watermark * 1000;
 
 		v->dcfclk_per_state[1] = v->dcfclkv_nom0p8;
 		v->dcfclk_per_state[0] = v->dcfclkv_nom0p8;
 		v->dcfclk = v->dcfclkv_nom0p8;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->watermarks.c.cstate_pstate.cstate_exit_ns =
+		context->bw.dcn.watermarks.c.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->watermarks.c.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw.dcn.watermarks.c.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->watermarks.c.cstate_pstate.pstate_change_ns =
+		context->bw.dcn.watermarks.c.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->watermarks.c.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->watermarks.c.urgent_ns = v->urgent_watermark * 1000;
+		context->bw.dcn.watermarks.c.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw.dcn.watermarks.c.urgent_ns = v->urgent_watermark * 1000;
 	}
 
 	if (v->voltage_level < 3) {
@@ -578,14 +578,14 @@ static void calc_wm_sets_and_perf_params(
 		v->dcfclk = v->dcfclkv_max0p9;
 		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-		context->watermarks.d.cstate_pstate.cstate_exit_ns =
+		context->bw.dcn.watermarks.d.cstate_pstate.cstate_exit_ns =
 			v->stutter_exit_watermark * 1000;
-		context->watermarks.d.cstate_pstate.cstate_enter_plus_exit_ns =
+		context->bw.dcn.watermarks.d.cstate_pstate.cstate_enter_plus_exit_ns =
 				v->stutter_enter_plus_exit_watermark * 1000;
-		context->watermarks.d.cstate_pstate.pstate_change_ns =
+		context->bw.dcn.watermarks.d.cstate_pstate.pstate_change_ns =
 				v->dram_clock_change_watermark * 1000;
-		context->watermarks.d.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-		context->watermarks.d.urgent_ns = v->urgent_watermark * 1000;
+		context->bw.dcn.watermarks.d.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->bw.dcn.watermarks.d.urgent_ns = v->urgent_watermark * 1000;
 	}
 
 	v->fabric_and_dram_bandwidth_per_state[2] = v->fabric_and_dram_bandwidth_vnom0p8;
@@ -598,20 +598,20 @@ static void calc_wm_sets_and_perf_params(
 	v->dcfclk = v->dcfclk_per_state[v->voltage_level];
 	dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
 
-	context->watermarks.a.cstate_pstate.cstate_exit_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.cstate_exit_ns =
 		v->stutter_exit_watermark * 1000;
-	context->watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
 			v->stutter_enter_plus_exit_watermark * 1000;
-	context->watermarks.a.cstate_pstate.pstate_change_ns =
+	context->bw.dcn.watermarks.a.cstate_pstate.pstate_change_ns =
 			v->dram_clock_change_watermark * 1000;
-	context->watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
-	context->watermarks.a.urgent_ns = v->urgent_watermark * 1000;
+	context->bw.dcn.watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+	context->bw.dcn.watermarks.a.urgent_ns = v->urgent_watermark * 1000;
 	if (v->voltage_level >= 2) {
-		context->watermarks.b = context->watermarks.a;
-		context->watermarks.c = context->watermarks.a;
+		context->bw.dcn.watermarks.b = context->bw.dcn.watermarks.a;
+		context->bw.dcn.watermarks.c = context->bw.dcn.watermarks.a;
 	}
 	if (v->voltage_level >= 3)
-		context->watermarks.d = context->watermarks.a;
+		context->bw.dcn.watermarks.d = context->bw.dcn.watermarks.a;
 }
 
 static bool dcn_bw_apply_registry_override(struct core_dc *dc)
@@ -916,17 +916,16 @@ bool dcn_validate_bandwidth(
 
 		display_pipe_configuration(v);
 		calc_wm_sets_and_perf_params(context, v);
-		context->fclk_khz = (int)(bw_consumed * 1000000 /
+		context->bw.dcn.calc_clk.fclk_khz = (int)(bw_consumed * 1000000 /
 				(ddr4_dram_factor_single_Channel * v->number_of_channels));
-		context->dram_ccm_us = (int)(v->dram_clock_change_margin);
-		context->min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
-		context->dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
-		context->dcfclk_khz = (int)(v->dcfclk * 1000);
-		context->dispclk_khz = (int)(v->dispclk * 1000);
+		context->bw.dcn.calc_clk.dram_ccm_us = (int)(v->dram_clock_change_margin);
+		context->bw.dcn.calc_clk.min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
+		context->bw.dcn.calc_clk.dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
+		context->bw.dcn.calc_clk.dcfclk_khz = (int)(v->dcfclk * 1000);
+		context->bw.dcn.calc_clk.dispclk_khz = (int)(v->dispclk * 1000);
 		if (dc->public.debug.max_disp_clk == true)
-			context->dispclk_khz = (int)(dc->dcn_soc.max_dispclk_vmax0p9 * 1000);
-		context->dppclk_khz = (int)(v->dppclk * 1000);
-		context->dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
+			context->bw.dcn.calc_clk.dispclk_khz = (int)(dc->dcn_soc.max_dispclk_vmax0p9 * 1000);
+		context->bw.dcn.calc_clk.dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
 
 		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
 			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];

commit 64b44524d4b28349217718ff641b02ec5ecf7b26
Author: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
Date:   Tue May 2 16:58:39 2017 -0400

    drm/amd/display: bw debug options now apply to dml as well
    
    Signed-off-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Reviewed-by: Dmytro Laktyushkin <Dmytro.Laktyushkin@amd.com>
    Acked-by: Harry Wentland <Harry.Wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
index eb81edf3afa9..22d98ef69a77 100644
--- a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -614,23 +614,49 @@ static void calc_wm_sets_and_perf_params(
 		context->watermarks.d = context->watermarks.a;
 }
 
-static void dcn_bw_apply_registry_override(struct core_dc *dc)
+static bool dcn_bw_apply_registry_override(struct core_dc *dc)
 {
+	bool updated = false;
+
 	kernel_fpu_begin();
-	if (dc->public.debug.sr_exit_time_ns)
+	if ((int)(dc->dcn_soc.sr_exit_time * 1000) != dc->public.debug.sr_exit_time_ns
+			&& dc->public.debug.sr_exit_time_ns) {
+		updated = true;
 		dc->dcn_soc.sr_exit_time = dc->public.debug.sr_exit_time_ns / 1000.0;
-	if (dc->public.debug.sr_enter_plus_exit_time_ns)
+	}
+
+	if ((int)(dc->dcn_soc.sr_enter_plus_exit_time * 1000)
+				!= dc->public.debug.sr_enter_plus_exit_time_ns
+			&& dc->public.debug.sr_enter_plus_exit_time_ns) {
+		updated = true;
 		dc->dcn_soc.sr_enter_plus_exit_time =
 				dc->public.debug.sr_enter_plus_exit_time_ns / 1000.0;
-	if (dc->public.debug.urgent_latency_ns)
+	}
+
+	if ((int)(dc->dcn_soc.urgent_latency * 1000) != dc->public.debug.urgent_latency_ns
+			&& dc->public.debug.urgent_latency_ns) {
+		updated = true;
 		dc->dcn_soc.urgent_latency = dc->public.debug.urgent_latency_ns / 1000.0;
-	if (dc->public.debug.percent_of_ideal_drambw)
+	}
+
+	if ((int)(dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency * 1000)
+				!= dc->public.debug.percent_of_ideal_drambw
+			&& dc->public.debug.percent_of_ideal_drambw) {
+		updated = true;
 		dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency =
 				dc->public.debug.percent_of_ideal_drambw;
-	if (dc->public.debug.dram_clock_change_latency_ns)
+	}
+
+	if ((int)(dc->dcn_soc.dram_clock_change_latency * 1000)
+				!= dc->public.debug.dram_clock_change_latency_ns
+			&& dc->public.debug.dram_clock_change_latency_ns) {
+		updated = true;
 		dc->dcn_soc.dram_clock_change_latency =
 				dc->public.debug.dram_clock_change_latency_ns / 1000.0;
+	}
 	kernel_fpu_end();
+
+	return updated;
 }
 
 bool dcn_validate_bandwidth(
@@ -642,7 +668,8 @@ bool dcn_validate_bandwidth(
 	int i, input_idx;
 	int vesa_sync_start, asic_blank_end, asic_blank_start;
 
-	dcn_bw_apply_registry_override(DC_TO_CORE(&dc->public));
+	if (dcn_bw_apply_registry_override(DC_TO_CORE(&dc->public)))
+		dcn_bw_sync_calcs_and_dml(DC_TO_CORE(&dc->public));
 
 	memset(v, 0, sizeof(*v));
 	kernel_fpu_begin();

commit 74c49c7ac14f3a7cc500be959709f3473a6a49e7
Author: Harry Wentland <harry.wentland@amd.com>
Date:   Mon May 8 15:17:39 2017 -0400

    drm/amdgpu/display: Add calcs code for DCN
    
    Bandwidth and scaling calculations for DCN.
    
    Signed-off-by: Harry Wentland <harry.wentland@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
new file mode 100644
index 000000000000..eb81edf3afa9
--- /dev/null
+++ b/drivers/gpu/drm/amd/display/dc/calcs/dcn_calcs.c
@@ -0,0 +1,1366 @@
+/*
+ * Copyright 2017 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ * Authors: AMD
+ *
+ */
+
+#include "dm_services.h"
+#include "dcn_calcs.h"
+#include "dcn_calc_auto.h"
+#include "dc.h"
+#include "core_dc.h"
+#include "dal_asic_id.h"
+
+#include "resource.h"
+#include "dcn10/dcn10_resource.h"
+#include "dcn_calc_math.h"
+
+/* Defaults from spreadsheet rev#247 */
+const struct dcn_soc_bounding_box dcn10_soc_defaults = {
+		.sr_exit_time = 17, /*us*/ /*update based on HW Request for 118773*/
+		.sr_enter_plus_exit_time = 19, /*us*/
+		.urgent_latency = 4, /*us*/
+		.write_back_latency = 12, /*us*/
+		.percent_of_ideal_drambw_received_after_urg_latency = 80, /*%*/
+		.max_request_size = 256, /*bytes*/
+		.dcfclkv_max0p9 = 600, /*MHz*/
+		.dcfclkv_nom0p8 = 600, /*MHz*/
+		.dcfclkv_mid0p72 = 300, /*MHz*/
+		.dcfclkv_min0p65 = 300, /*MHz*/
+		.max_dispclk_vmax0p9 = 1086, /*MHz*/
+		.max_dispclk_vnom0p8 = 661, /*MHz*/
+		.max_dispclk_vmid0p72 = 608, /*MHz*/
+		.max_dispclk_vmin0p65 = 608, /*MHz*/
+		.max_dppclk_vmax0p9 = 661, /*MHz*/
+		.max_dppclk_vnom0p8 = 661, /*MHz*/
+		.max_dppclk_vmid0p72 = 435, /*MHz*/
+		.max_dppclk_vmin0p65 = 435, /*MHz*/
+		.socclk = 208, /*MHz*/
+		.fabric_and_dram_bandwidth_vmax0p9 = 38.4f, /*GB/s*/
+		.fabric_and_dram_bandwidth_vnom0p8 = 34.1f, /*GB/s*/
+		.fabric_and_dram_bandwidth_vmid0p72 = 29.8f, /*GB/s*/
+		.fabric_and_dram_bandwidth_vmin0p65 = 12.8f, /*GB/s*/
+		.phyclkv_max0p9 = 810, /*MHz*/
+		.phyclkv_nom0p8 = 810, /*MHz*/
+		.phyclkv_mid0p72 = 540, /*MHz*/
+		.phyclkv_min0p65 = 540, /*MHz*/
+		.downspreading = 0.5f, /*%*/
+		.round_trip_ping_latency_cycles = 128, /*DCFCLK Cycles*/
+		.urgent_out_of_order_return_per_channel = 256, /*bytes*/
+		.number_of_channels = 2,
+		.vmm_page_size = 4096, /*bytes*/
+		.dram_clock_change_latency = 17, /*us*/
+		.return_bus_width = 64, /*bytes*/
+};
+
+const struct dcn_ip_params dcn10_ip_defaults = {
+		.rob_buffer_size_in_kbyte = 64,
+		.det_buffer_size_in_kbyte = 164,
+		.dpp_output_buffer_pixels = 2560,
+		.opp_output_buffer_lines = 1,
+		.pixel_chunk_size_in_kbyte = 8,
+		.pte_enable = dcn_bw_yes,
+		.pte_chunk_size = 2, /*kbytes*/
+		.meta_chunk_size = 2, /*kbytes*/
+		.writeback_chunk_size = 2, /*kbytes*/
+		.odm_capability = dcn_bw_no,
+		.dsc_capability = dcn_bw_no,
+		.line_buffer_size = 589824, /*bit*/
+		.max_line_buffer_lines = 12,
+		.is_line_buffer_bpp_fixed = dcn_bw_no,
+		.line_buffer_fixed_bpp = dcn_bw_na,
+		.writeback_luma_buffer_size = 12, /*kbytes*/
+		.writeback_chroma_buffer_size = 8, /*kbytes*/
+		.max_num_dpp = 4,
+		.max_num_writeback = 2,
+		.max_dchub_topscl_throughput = 4, /*pixels/dppclk*/
+		.max_pscl_tolb_throughput = 2, /*pixels/dppclk*/
+		.max_lb_tovscl_throughput = 4, /*pixels/dppclk*/
+		.max_vscl_tohscl_throughput = 4, /*pixels/dppclk*/
+		.max_hscl_ratio = 4,
+		.max_vscl_ratio = 4,
+		.max_hscl_taps = 8,
+		.max_vscl_taps = 8,
+		.pte_buffer_size_in_requests = 42,
+		.dispclk_ramping_margin = 1, /*%*/
+		.under_scan_factor = 1.11f,
+		.max_inter_dcn_tile_repeaters = 8,
+		.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one = dcn_bw_no,
+		.bug_forcing_luma_and_chroma_request_to_same_size_fixed = dcn_bw_no,
+		.dcfclk_cstate_latency = 10 /*TODO clone of something else? sr_enter_plus_exit_time?*/
+};
+
+static enum dcn_bw_defs tl_sw_mode_to_bw_defs(enum swizzle_mode_values sw_mode)
+{
+	switch (sw_mode) {
+	case DC_SW_LINEAR:
+		return dcn_bw_sw_linear;
+	case DC_SW_4KB_S:
+		return dcn_bw_sw_4_kb_s;
+	case DC_SW_4KB_D:
+		return dcn_bw_sw_4_kb_d;
+	case DC_SW_64KB_S:
+		return dcn_bw_sw_64_kb_s;
+	case DC_SW_64KB_D:
+		return dcn_bw_sw_64_kb_d;
+	case DC_SW_VAR_S:
+		return dcn_bw_sw_var_s;
+	case DC_SW_VAR_D:
+		return dcn_bw_sw_var_d;
+	case DC_SW_64KB_S_T:
+		return dcn_bw_sw_64_kb_s_t;
+	case DC_SW_64KB_D_T:
+		return dcn_bw_sw_64_kb_d_t;
+	case DC_SW_4KB_S_X:
+		return dcn_bw_sw_4_kb_s_x;
+	case DC_SW_4KB_D_X:
+		return dcn_bw_sw_4_kb_d_x;
+	case DC_SW_64KB_S_X:
+		return dcn_bw_sw_64_kb_s_x;
+	case DC_SW_64KB_D_X:
+		return dcn_bw_sw_64_kb_d_x;
+	case DC_SW_VAR_S_X:
+		return dcn_bw_sw_var_s_x;
+	case DC_SW_VAR_D_X:
+		return dcn_bw_sw_var_d_x;
+	case DC_SW_256B_S:
+	case DC_SW_256_D:
+	case DC_SW_256_R:
+	case DC_SW_4KB_R:
+	case DC_SW_64KB_R:
+	case DC_SW_VAR_R:
+	case DC_SW_4KB_R_X:
+	case DC_SW_64KB_R_X:
+	case DC_SW_VAR_R_X:
+	default:
+		BREAK_TO_DEBUGGER(); /*not in formula*/
+		return dcn_bw_sw_4_kb_s;
+	}
+}
+
+static int tl_lb_bpp_to_int(enum lb_pixel_depth depth)
+{
+	switch (depth) {
+	case LB_PIXEL_DEPTH_18BPP:
+		return 18;
+	case LB_PIXEL_DEPTH_24BPP:
+		return 24;
+	case LB_PIXEL_DEPTH_30BPP:
+		return 30;
+	case LB_PIXEL_DEPTH_36BPP:
+		return 36;
+	default:
+		return 30;
+	}
+}
+
+static enum dcn_bw_defs tl_pixel_format_to_bw_defs(enum surface_pixel_format format)
+{
+	switch (format) {
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB1555:
+	case SURFACE_PIXEL_FORMAT_GRPH_RGB565:
+		return dcn_bw_rgb_sub_16;
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB8888:
+	case SURFACE_PIXEL_FORMAT_GRPH_ABGR8888:
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB2101010:
+	case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010:
+	case SURFACE_PIXEL_FORMAT_GRPH_ABGR2101010_XR_BIAS:
+		return dcn_bw_rgb_sub_32;
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616F:
+	case SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:
+		return dcn_bw_rgb_sub_64;
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCrCb:
+		return dcn_bw_yuv420_sub_8;
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_10bpc_YCbCr:
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_10bpc_YCrCb:
+		return dcn_bw_yuv420_sub_10;
+	default:
+		return dcn_bw_rgb_sub_32;
+	}
+}
+
+static void pipe_ctx_to_e2e_pipe_params (
+		const struct pipe_ctx *pipe,
+		struct _vcs_dpi_display_pipe_params_st *input)
+{
+	input->src.is_hsplit = false;
+	if (pipe->top_pipe != NULL && pipe->top_pipe->surface == pipe->surface)
+		input->src.is_hsplit = true;
+	else if (pipe->bottom_pipe != NULL && pipe->bottom_pipe->surface == pipe->surface)
+		input->src.is_hsplit = true;
+
+	input->src.dcc                 = pipe->surface->public.dcc.enable;
+	input->src.dcc_rate            = 1;
+	input->src.meta_pitch          = pipe->surface->public.dcc.grph.meta_pitch;
+	input->src.source_scan         = dm_horz;
+	input->src.sw_mode             = pipe->surface->public.tiling_info.gfx9.swizzle;
+
+	input->src.viewport_width      = pipe->scl_data.viewport.width;
+	input->src.viewport_height     = pipe->scl_data.viewport.height;
+	input->src.data_pitch          = pipe->scl_data.viewport.width;
+	input->src.data_pitch_c        = pipe->scl_data.viewport.width;
+	input->src.cur0_src_width      = 128; /* TODO: Cursor calcs, not curently stored */
+	input->src.cur0_bpp            = 32;
+
+	switch (pipe->surface->public.tiling_info.gfx9.swizzle) {
+	/* for 4/8/16 high tiles */
+	case DC_SW_LINEAR:
+		input->src.is_display_sw = 1;
+		input->src.macro_tile_size = dm_4k_tile;
+		break;
+	case DC_SW_4KB_S:
+	case DC_SW_4KB_S_X:
+		input->src.is_display_sw = 0;
+		input->src.macro_tile_size = dm_4k_tile;
+		break;
+	case DC_SW_64KB_S:
+	case DC_SW_64KB_S_X:
+		input->src.is_display_sw = 0;
+		input->src.macro_tile_size = dm_64k_tile;
+		break;
+	case DC_SW_VAR_S:
+	case DC_SW_VAR_S_X:
+		input->src.is_display_sw = 0;
+		input->src.macro_tile_size = dm_256k_tile;
+		break;
+
+	/* For 64bpp 2 high tiles */
+	case DC_SW_4KB_D:
+	case DC_SW_4KB_D_X:
+		input->src.is_display_sw = 1;
+		input->src.macro_tile_size = dm_4k_tile;
+		break;
+	case DC_SW_64KB_D:
+	case DC_SW_64KB_D_X:
+		input->src.is_display_sw = 1;
+		input->src.macro_tile_size = dm_64k_tile;
+		break;
+	case DC_SW_VAR_D:
+	case DC_SW_VAR_D_X:
+		input->src.is_display_sw = 1;
+		input->src.macro_tile_size = dm_256k_tile;
+		break;
+
+	/* Unsupported swizzle modes for dcn */
+	case DC_SW_256B_S:
+	default:
+		ASSERT(0); /* Not supported */
+		break;
+	}
+
+	switch (pipe->surface->public.rotation) {
+	case ROTATION_ANGLE_0:
+	case ROTATION_ANGLE_180:
+		input->src.source_scan = dm_horz;
+		break;
+	case ROTATION_ANGLE_90:
+	case ROTATION_ANGLE_270:
+		input->src.source_scan = dm_vert;
+		break;
+	default:
+		ASSERT(0); /* Not supported */
+		break;
+	}
+
+	/* TODO: Fix pixel format mappings */
+	switch (pipe->surface->public.format) {
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCbCr:
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_YCrCb:
+		input->src.source_format = dm_420_8;
+		input->src.viewport_width_c    = input->src.viewport_width / 2;
+		input->src.viewport_height_c   = input->src.viewport_height / 2;
+		break;
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_10bpc_YCbCr:
+	case SURFACE_PIXEL_FORMAT_VIDEO_420_10bpc_YCrCb:
+		input->src.source_format = dm_420_10;
+		input->src.viewport_width_c    = input->src.viewport_width / 2;
+		input->src.viewport_height_c   = input->src.viewport_height / 2;
+		break;
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616:
+	case SURFACE_PIXEL_FORMAT_GRPH_ARGB16161616F:
+	case SURFACE_PIXEL_FORMAT_GRPH_ABGR16161616F:
+		input->src.source_format = dm_444_64;
+		input->src.viewport_width_c    = input->src.viewport_width;
+		input->src.viewport_height_c   = input->src.viewport_height;
+		break;
+	default:
+		input->src.source_format = dm_444_32;
+		input->src.viewport_width_c    = input->src.viewport_width;
+		input->src.viewport_height_c   = input->src.viewport_height;
+		break;
+	}
+
+	input->scale_taps.htaps                = pipe->scl_data.taps.h_taps;
+	input->scale_ratio_depth.hscl_ratio    = pipe->scl_data.ratios.horz.value/4294967296.0;
+	input->scale_ratio_depth.vscl_ratio    = pipe->scl_data.ratios.vert.value/4294967296.0;
+	input->scale_ratio_depth.vinit =  pipe->scl_data.inits.v.value/4294967296.0;
+	if (input->scale_ratio_depth.vinit < 1.0)
+			input->scale_ratio_depth.vinit = 1;
+	input->scale_taps.vtaps = pipe->scl_data.taps.v_taps;
+	input->scale_taps.vtaps_c = pipe->scl_data.taps.v_taps_c;
+	input->scale_taps.htaps_c              = pipe->scl_data.taps.h_taps_c;
+	input->scale_ratio_depth.hscl_ratio_c  = pipe->scl_data.ratios.horz_c.value/4294967296.0;
+	input->scale_ratio_depth.vscl_ratio_c  = pipe->scl_data.ratios.vert_c.value/4294967296.0;
+	input->scale_ratio_depth.vinit_c       = pipe->scl_data.inits.v_c.value/4294967296.0;
+	if (input->scale_ratio_depth.vinit_c < 1.0)
+			input->scale_ratio_depth.vinit_c = 1;
+	switch (pipe->scl_data.lb_params.depth) {
+	case LB_PIXEL_DEPTH_30BPP:
+		input->scale_ratio_depth.lb_depth = 30; break;
+	case LB_PIXEL_DEPTH_36BPP:
+		input->scale_ratio_depth.lb_depth = 36; break;
+	default:
+		input->scale_ratio_depth.lb_depth = 24; break;
+	}
+
+
+	input->dest.vactive        = pipe->stream->public.timing.v_addressable;
+
+	input->dest.recout_width   = pipe->scl_data.recout.width;
+	input->dest.recout_height  = pipe->scl_data.recout.height;
+
+	input->dest.full_recout_width   = pipe->scl_data.recout.width;
+	input->dest.full_recout_height  = pipe->scl_data.recout.height;
+
+	input->dest.htotal         = pipe->stream->public.timing.h_total;
+	input->dest.hblank_start   = input->dest.htotal - pipe->stream->public.timing.h_front_porch;
+	input->dest.hblank_end     = input->dest.hblank_start
+			- pipe->stream->public.timing.h_addressable
+			- pipe->stream->public.timing.h_border_left
+			- pipe->stream->public.timing.h_border_right;
+
+	input->dest.vtotal         = pipe->stream->public.timing.v_total;
+	input->dest.vblank_start   = input->dest.vtotal - pipe->stream->public.timing.v_front_porch;
+	input->dest.vblank_end     = input->dest.vblank_start
+			- pipe->stream->public.timing.v_addressable
+			- pipe->stream->public.timing.v_border_bottom
+			- pipe->stream->public.timing.v_border_top;
+
+	input->dest.vsync_plus_back_porch = pipe->stream->public.timing.v_total
+			- pipe->stream->public.timing.v_addressable
+			- pipe->stream->public.timing.v_front_porch;
+	input->dest.pixel_rate_mhz = pipe->stream->public.timing.pix_clk_khz/1000.0;
+	input->dest.vstartup_start = pipe->pipe_dlg_param.vstartup_start;
+	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
+	input->dest.vupdate_offset = pipe->pipe_dlg_param.vupdate_offset;
+	input->dest.vupdate_width = pipe->pipe_dlg_param.vupdate_width;
+
+}
+
+static void dcn_bw_calc_rq_dlg_ttu(
+		const struct core_dc *dc,
+		const struct dcn_bw_internal_vars *v,
+		struct pipe_ctx *pipe)
+{
+	struct display_mode_lib *dml = (struct display_mode_lib *)(&dc->dml);
+	struct _vcs_dpi_display_dlg_regs_st *dlg_regs = &pipe->dlg_regs;
+	struct _vcs_dpi_display_ttu_regs_st *ttu_regs = &pipe->ttu_regs;
+	struct _vcs_dpi_display_rq_regs_st *rq_regs = &pipe->rq_regs;
+	struct _vcs_dpi_display_rq_params_st rq_param = {0};
+	struct _vcs_dpi_display_dlg_sys_params_st dlg_sys_param = {0};
+	struct _vcs_dpi_display_e2e_pipe_params_st input = { { { 0 } } };
+	float total_active_bw = 0;
+	float total_prefetch_bw = 0;
+	int total_flip_bytes = 0;
+	int i;
+
+	for (i = 0; i < number_of_planes; i++) {
+		total_active_bw += v->read_bandwidth[i];
+		total_prefetch_bw += v->prefetch_bandwidth[i];
+		total_flip_bytes += v->total_immediate_flip_bytes[i];
+	}
+	dlg_sys_param.total_flip_bw = v->return_bw - dcn_bw_max2(total_active_bw, total_prefetch_bw);
+	if (dlg_sys_param.total_flip_bw < 0.0)
+		dlg_sys_param.total_flip_bw = 0;
+
+	dlg_sys_param.t_mclk_wm_us = v->dram_clock_change_watermark;
+	dlg_sys_param.t_sr_wm_us = v->stutter_enter_plus_exit_watermark;
+	dlg_sys_param.t_urg_wm_us = v->urgent_watermark;
+	dlg_sys_param.t_extra_us = v->urgent_extra_latency;
+	dlg_sys_param.deepsleep_dcfclk_mhz = v->dcf_clk_deep_sleep;
+	dlg_sys_param.total_flip_bytes = total_flip_bytes;
+
+	pipe_ctx_to_e2e_pipe_params(pipe, &input.pipe);
+	input.clks_cfg.dcfclk_mhz = v->dcfclk;
+	input.clks_cfg.dispclk_mhz = v->dispclk;
+	input.clks_cfg.dppclk_mhz = v->dppclk;
+	input.clks_cfg.refclk_mhz = dc->res_pool->ref_clock_inKhz/1000;
+	input.clks_cfg.socclk_mhz = v->socclk;
+	input.clks_cfg.voltage = v->voltage_level;
+//	dc->dml.logger = pool->base.logger;
+
+	/*todo: soc->sr_enter_plus_exit_time??*/
+	dlg_sys_param.t_srx_delay_us = dc->dcn_ip.dcfclk_cstate_latency / v->dcf_clk_deep_sleep;
+
+	dml_rq_dlg_get_rq_params(dml, &rq_param, input.pipe.src);
+	extract_rq_regs(dml, rq_regs, rq_param);
+	dml_rq_dlg_get_dlg_params(
+			dml,
+			dlg_regs,
+			ttu_regs,
+			rq_param.dlg,
+			dlg_sys_param,
+			input,
+			true,
+			true,
+			v->pte_enable == dcn_bw_yes,
+			pipe->surface->public.flip_immediate);
+}
+
+static void dcn_dml_wm_override(
+		const struct dcn_bw_internal_vars *v,
+		struct display_mode_lib *dml,
+		struct validate_context *context,
+		const struct resource_pool *pool)
+{
+	int i, in_idx, active_count;
+
+	struct _vcs_dpi_display_e2e_pipe_params_st *input = dm_alloc(pool->pipe_count *
+					sizeof(struct _vcs_dpi_display_e2e_pipe_params_st));
+	struct wm {
+		double urgent;
+		struct _vcs_dpi_cstate_pstate_watermarks_st cpstate;
+		double pte_meta_urgent;
+	} a;
+
+
+	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
+		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
+
+		if (!pipe->stream || !pipe->surface)
+			continue;
+
+		input[in_idx].clks_cfg.dcfclk_mhz = v->dcfclk;
+		input[in_idx].clks_cfg.dispclk_mhz = v->dispclk;
+		input[in_idx].clks_cfg.dppclk_mhz = v->dppclk;
+		input[in_idx].clks_cfg.refclk_mhz = pool->ref_clock_inKhz / 1000;
+		input[in_idx].clks_cfg.socclk_mhz = v->socclk;
+		input[in_idx].clks_cfg.voltage = v->voltage_level;
+		pipe_ctx_to_e2e_pipe_params(pipe, &input[in_idx].pipe);
+		dml_rq_dlg_get_rq_reg(
+			dml,
+			&pipe->rq_regs,
+			input[in_idx].pipe.src);
+		in_idx++;
+	}
+	active_count = in_idx;
+
+	a.urgent = dml_wm_urgent_e2e(dml, input, active_count);
+	a.cpstate = dml_wm_cstate_pstate_e2e(dml, input, active_count);
+	a.pte_meta_urgent = dml_wm_pte_meta_urgent(dml, a.urgent);
+
+	context->watermarks.a.cstate_pstate.cstate_exit_ns =
+			a.cpstate.cstate_exit_us * 1000;
+	context->watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+			a.cpstate.cstate_enter_plus_exit_us * 1000;
+	context->watermarks.a.cstate_pstate.pstate_change_ns =
+			a.cpstate.pstate_change_us * 1000;
+	context->watermarks.a.pte_meta_urgent_ns = a.pte_meta_urgent * 1000;
+	context->watermarks.a.urgent_ns = a.urgent * 1000;
+	context->watermarks.b = context->watermarks.a;
+	context->watermarks.c = context->watermarks.a;
+	context->watermarks.d = context->watermarks.a;
+
+
+	for (i = 0, in_idx = 0; i < pool->pipe_count; i++) {
+		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
+
+		if (!pipe->stream || !pipe->surface)
+			continue;
+
+		dml_rq_dlg_get_dlg_reg(dml,
+			&pipe->dlg_regs,
+			&pipe->ttu_regs,
+			input, active_count,
+			in_idx,
+			true,
+			true,
+			v->pte_enable == dcn_bw_yes,
+			pipe->surface->public.flip_immediate);
+		in_idx++;
+	}
+	dm_free(input);
+}
+
+static void split_stream_across_pipes(
+		struct resource_context *res_ctx,
+		const struct resource_pool *pool,
+		struct pipe_ctx *primary_pipe,
+		struct pipe_ctx *secondary_pipe)
+{
+	if (!primary_pipe->surface)
+		return;
+
+	secondary_pipe->stream = primary_pipe->stream;
+	secondary_pipe->tg = primary_pipe->tg;
+
+	secondary_pipe->mi = pool->mis[secondary_pipe->pipe_idx];
+	secondary_pipe->ipp = pool->ipps[secondary_pipe->pipe_idx];
+	secondary_pipe->xfm = pool->transforms[secondary_pipe->pipe_idx];
+	secondary_pipe->opp = pool->opps[secondary_pipe->pipe_idx];
+	if (primary_pipe->bottom_pipe) {
+		secondary_pipe->bottom_pipe = primary_pipe->bottom_pipe;
+		secondary_pipe->bottom_pipe->top_pipe = secondary_pipe;
+	}
+	primary_pipe->bottom_pipe = secondary_pipe;
+	secondary_pipe->top_pipe = primary_pipe;
+	secondary_pipe->surface = primary_pipe->surface;
+	secondary_pipe->pipe_dlg_param = primary_pipe->pipe_dlg_param;
+
+	resource_build_scaling_params(primary_pipe);
+	resource_build_scaling_params(secondary_pipe);
+}
+
+static void calc_wm_sets_and_perf_params(
+		struct validate_context *context,
+		struct dcn_bw_internal_vars *v)
+{
+	/* Calculate set A last to keep internal var state consistent for required config */
+	if (v->voltage_level < 2) {
+		v->fabric_and_dram_bandwidth_per_state[1] = v->fabric_and_dram_bandwidth_vnom0p8;
+		v->fabric_and_dram_bandwidth_per_state[0] = v->fabric_and_dram_bandwidth_vnom0p8;
+		v->fabric_and_dram_bandwidth = v->fabric_and_dram_bandwidth_vnom0p8;
+		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
+
+		context->watermarks.b.cstate_pstate.cstate_exit_ns =
+			v->stutter_exit_watermark * 1000;
+		context->watermarks.b.cstate_pstate.cstate_enter_plus_exit_ns =
+				v->stutter_enter_plus_exit_watermark * 1000;
+		context->watermarks.b.cstate_pstate.pstate_change_ns =
+				v->dram_clock_change_watermark * 1000;
+		context->watermarks.b.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->watermarks.b.urgent_ns = v->urgent_watermark * 1000;
+
+		v->dcfclk_per_state[1] = v->dcfclkv_nom0p8;
+		v->dcfclk_per_state[0] = v->dcfclkv_nom0p8;
+		v->dcfclk = v->dcfclkv_nom0p8;
+		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
+
+		context->watermarks.c.cstate_pstate.cstate_exit_ns =
+			v->stutter_exit_watermark * 1000;
+		context->watermarks.c.cstate_pstate.cstate_enter_plus_exit_ns =
+				v->stutter_enter_plus_exit_watermark * 1000;
+		context->watermarks.c.cstate_pstate.pstate_change_ns =
+				v->dram_clock_change_watermark * 1000;
+		context->watermarks.c.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->watermarks.c.urgent_ns = v->urgent_watermark * 1000;
+	}
+
+	if (v->voltage_level < 3) {
+		v->fabric_and_dram_bandwidth_per_state[2] = v->fabric_and_dram_bandwidth_vmax0p9;
+		v->fabric_and_dram_bandwidth_per_state[1] = v->fabric_and_dram_bandwidth_vmax0p9;
+		v->fabric_and_dram_bandwidth_per_state[0] = v->fabric_and_dram_bandwidth_vmax0p9;
+		v->fabric_and_dram_bandwidth = v->fabric_and_dram_bandwidth_vmax0p9;
+		v->dcfclk_per_state[2] = v->dcfclkv_max0p9;
+		v->dcfclk_per_state[1] = v->dcfclkv_max0p9;
+		v->dcfclk_per_state[0] = v->dcfclkv_max0p9;
+		v->dcfclk = v->dcfclkv_max0p9;
+		dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
+
+		context->watermarks.d.cstate_pstate.cstate_exit_ns =
+			v->stutter_exit_watermark * 1000;
+		context->watermarks.d.cstate_pstate.cstate_enter_plus_exit_ns =
+				v->stutter_enter_plus_exit_watermark * 1000;
+		context->watermarks.d.cstate_pstate.pstate_change_ns =
+				v->dram_clock_change_watermark * 1000;
+		context->watermarks.d.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+		context->watermarks.d.urgent_ns = v->urgent_watermark * 1000;
+	}
+
+	v->fabric_and_dram_bandwidth_per_state[2] = v->fabric_and_dram_bandwidth_vnom0p8;
+	v->fabric_and_dram_bandwidth_per_state[1] = v->fabric_and_dram_bandwidth_vmid0p72;
+	v->fabric_and_dram_bandwidth_per_state[0] = v->fabric_and_dram_bandwidth_vmin0p65;
+	v->fabric_and_dram_bandwidth = v->fabric_and_dram_bandwidth_per_state[v->voltage_level];
+	v->dcfclk_per_state[2] = v->dcfclkv_nom0p8;
+	v->dcfclk_per_state[1] = v->dcfclkv_mid0p72;
+	v->dcfclk_per_state[0] = v->dcfclkv_min0p65;
+	v->dcfclk = v->dcfclk_per_state[v->voltage_level];
+	dispclkdppclkdcfclk_deep_sleep_prefetch_parameters_watermarks_and_performance_calculation(v);
+
+	context->watermarks.a.cstate_pstate.cstate_exit_ns =
+		v->stutter_exit_watermark * 1000;
+	context->watermarks.a.cstate_pstate.cstate_enter_plus_exit_ns =
+			v->stutter_enter_plus_exit_watermark * 1000;
+	context->watermarks.a.cstate_pstate.pstate_change_ns =
+			v->dram_clock_change_watermark * 1000;
+	context->watermarks.a.pte_meta_urgent_ns = v->ptemeta_urgent_watermark * 1000;
+	context->watermarks.a.urgent_ns = v->urgent_watermark * 1000;
+	if (v->voltage_level >= 2) {
+		context->watermarks.b = context->watermarks.a;
+		context->watermarks.c = context->watermarks.a;
+	}
+	if (v->voltage_level >= 3)
+		context->watermarks.d = context->watermarks.a;
+}
+
+static void dcn_bw_apply_registry_override(struct core_dc *dc)
+{
+	kernel_fpu_begin();
+	if (dc->public.debug.sr_exit_time_ns)
+		dc->dcn_soc.sr_exit_time = dc->public.debug.sr_exit_time_ns / 1000.0;
+	if (dc->public.debug.sr_enter_plus_exit_time_ns)
+		dc->dcn_soc.sr_enter_plus_exit_time =
+				dc->public.debug.sr_enter_plus_exit_time_ns / 1000.0;
+	if (dc->public.debug.urgent_latency_ns)
+		dc->dcn_soc.urgent_latency = dc->public.debug.urgent_latency_ns / 1000.0;
+	if (dc->public.debug.percent_of_ideal_drambw)
+		dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency =
+				dc->public.debug.percent_of_ideal_drambw;
+	if (dc->public.debug.dram_clock_change_latency_ns)
+		dc->dcn_soc.dram_clock_change_latency =
+				dc->public.debug.dram_clock_change_latency_ns / 1000.0;
+	kernel_fpu_end();
+}
+
+bool dcn_validate_bandwidth(
+		const struct core_dc *dc,
+		struct validate_context *context)
+{
+	const struct resource_pool *pool = dc->res_pool;
+	struct dcn_bw_internal_vars *v = &context->dcn_bw_vars;
+	int i, input_idx;
+	int vesa_sync_start, asic_blank_end, asic_blank_start;
+
+	dcn_bw_apply_registry_override(DC_TO_CORE(&dc->public));
+
+	memset(v, 0, sizeof(*v));
+	kernel_fpu_begin();
+	v->sr_exit_time = dc->dcn_soc.sr_exit_time;
+	v->sr_enter_plus_exit_time = dc->dcn_soc.sr_enter_plus_exit_time;
+	v->urgent_latency = dc->dcn_soc.urgent_latency;
+	v->write_back_latency = dc->dcn_soc.write_back_latency;
+	v->percent_of_ideal_drambw_received_after_urg_latency =
+			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency;
+
+	v->dcfclkv_min0p65 = dc->dcn_soc.dcfclkv_min0p65;
+	v->dcfclkv_mid0p72 = dc->dcn_soc.dcfclkv_mid0p72;
+	v->dcfclkv_nom0p8 = dc->dcn_soc.dcfclkv_nom0p8;
+	v->dcfclkv_max0p9 = dc->dcn_soc.dcfclkv_max0p9;
+
+	v->max_dispclk_vmin0p65 = dc->dcn_soc.max_dispclk_vmin0p65;
+	v->max_dispclk_vmid0p72 = dc->dcn_soc.max_dispclk_vmid0p72;
+	v->max_dispclk_vnom0p8 = dc->dcn_soc.max_dispclk_vnom0p8;
+	v->max_dispclk_vmax0p9 = dc->dcn_soc.max_dispclk_vmax0p9;
+
+	v->max_dppclk_vmin0p65 = dc->dcn_soc.max_dppclk_vmin0p65;
+	v->max_dppclk_vmid0p72 = dc->dcn_soc.max_dppclk_vmid0p72;
+	v->max_dppclk_vnom0p8 = dc->dcn_soc.max_dppclk_vnom0p8;
+	v->max_dppclk_vmax0p9 = dc->dcn_soc.max_dppclk_vmax0p9;
+
+	v->socclk = dc->dcn_soc.socclk;
+
+	v->fabric_and_dram_bandwidth_vmin0p65 = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65;
+	v->fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72;
+	v->fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8;
+	v->fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9;
+
+	v->phyclkv_min0p65 = dc->dcn_soc.phyclkv_min0p65;
+	v->phyclkv_mid0p72 = dc->dcn_soc.phyclkv_mid0p72;
+	v->phyclkv_nom0p8 = dc->dcn_soc.phyclkv_nom0p8;
+	v->phyclkv_max0p9 = dc->dcn_soc.phyclkv_max0p9;
+
+	v->downspreading = dc->dcn_soc.downspreading;
+	v->round_trip_ping_latency_cycles = dc->dcn_soc.round_trip_ping_latency_cycles;
+	v->urgent_out_of_order_return_per_channel = dc->dcn_soc.urgent_out_of_order_return_per_channel;
+	v->number_of_channels = dc->dcn_soc.number_of_channels;
+	v->vmm_page_size = dc->dcn_soc.vmm_page_size;
+	v->dram_clock_change_latency = dc->dcn_soc.dram_clock_change_latency;
+	v->return_bus_width = dc->dcn_soc.return_bus_width;
+
+	v->rob_buffer_size_in_kbyte = dc->dcn_ip.rob_buffer_size_in_kbyte;
+	v->det_buffer_size_in_kbyte = dc->dcn_ip.det_buffer_size_in_kbyte;
+	v->dpp_output_buffer_pixels = dc->dcn_ip.dpp_output_buffer_pixels;
+	v->opp_output_buffer_lines = dc->dcn_ip.opp_output_buffer_lines;
+	v->pixel_chunk_size_in_kbyte = dc->dcn_ip.pixel_chunk_size_in_kbyte;
+	v->pte_enable = dc->dcn_ip.pte_enable;
+	v->pte_chunk_size = dc->dcn_ip.pte_chunk_size;
+	v->meta_chunk_size = dc->dcn_ip.meta_chunk_size;
+	v->writeback_chunk_size = dc->dcn_ip.writeback_chunk_size;
+	v->odm_capability = dc->dcn_ip.odm_capability;
+	v->dsc_capability = dc->dcn_ip.dsc_capability;
+	v->line_buffer_size = dc->dcn_ip.line_buffer_size;
+	v->is_line_buffer_bpp_fixed = dc->dcn_ip.is_line_buffer_bpp_fixed;
+	v->line_buffer_fixed_bpp = dc->dcn_ip.line_buffer_fixed_bpp;
+	v->max_line_buffer_lines = dc->dcn_ip.max_line_buffer_lines;
+	v->writeback_luma_buffer_size = dc->dcn_ip.writeback_luma_buffer_size;
+	v->writeback_chroma_buffer_size = dc->dcn_ip.writeback_chroma_buffer_size;
+	v->max_num_dpp = dc->dcn_ip.max_num_dpp;
+	v->max_num_writeback = dc->dcn_ip.max_num_writeback;
+	v->max_dchub_topscl_throughput = dc->dcn_ip.max_dchub_topscl_throughput;
+	v->max_pscl_tolb_throughput = dc->dcn_ip.max_pscl_tolb_throughput;
+	v->max_lb_tovscl_throughput = dc->dcn_ip.max_lb_tovscl_throughput;
+	v->max_vscl_tohscl_throughput = dc->dcn_ip.max_vscl_tohscl_throughput;
+	v->max_hscl_ratio = dc->dcn_ip.max_hscl_ratio;
+	v->max_vscl_ratio = dc->dcn_ip.max_vscl_ratio;
+	v->max_hscl_taps = dc->dcn_ip.max_hscl_taps;
+	v->max_vscl_taps = dc->dcn_ip.max_vscl_taps;
+	v->under_scan_factor = dc->dcn_ip.under_scan_factor;
+	v->pte_buffer_size_in_requests = dc->dcn_ip.pte_buffer_size_in_requests;
+	v->dispclk_ramping_margin = dc->dcn_ip.dispclk_ramping_margin;
+	v->max_inter_dcn_tile_repeaters = dc->dcn_ip.max_inter_dcn_tile_repeaters;
+	v->can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one =
+			dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one;
+	v->bug_forcing_luma_and_chroma_request_to_same_size_fixed =
+			dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed;
+
+	v->voltage[5] = dcn_bw_no_support;
+	v->voltage[4] = dcn_bw_v_max0p9;
+	v->voltage[3] = dcn_bw_v_max0p9;
+	v->voltage[2] = dcn_bw_v_nom0p8;
+	v->voltage[1] = dcn_bw_v_mid0p72;
+	v->voltage[0] = dcn_bw_v_min0p65;
+	v->fabric_and_dram_bandwidth_per_state[5] = v->fabric_and_dram_bandwidth_vmax0p9;
+	v->fabric_and_dram_bandwidth_per_state[4] = v->fabric_and_dram_bandwidth_vmax0p9;
+	v->fabric_and_dram_bandwidth_per_state[3] = v->fabric_and_dram_bandwidth_vmax0p9;
+	v->fabric_and_dram_bandwidth_per_state[2] = v->fabric_and_dram_bandwidth_vnom0p8;
+	v->fabric_and_dram_bandwidth_per_state[1] = v->fabric_and_dram_bandwidth_vmid0p72;
+	v->fabric_and_dram_bandwidth_per_state[0] = v->fabric_and_dram_bandwidth_vmin0p65;
+	v->dcfclk_per_state[5] = v->dcfclkv_max0p9;
+	v->dcfclk_per_state[4] = v->dcfclkv_max0p9;
+	v->dcfclk_per_state[3] = v->dcfclkv_max0p9;
+	v->dcfclk_per_state[2] = v->dcfclkv_nom0p8;
+	v->dcfclk_per_state[1] = v->dcfclkv_mid0p72;
+	v->dcfclk_per_state[0] = v->dcfclkv_min0p65;
+	v->max_dispclk[5] = v->max_dispclk_vmax0p9;
+	v->max_dispclk[4] = v->max_dispclk_vmax0p9;
+	v->max_dispclk[3] = v->max_dispclk_vmax0p9;
+	v->max_dispclk[2] = v->max_dispclk_vnom0p8;
+	v->max_dispclk[1] = v->max_dispclk_vmid0p72;
+	v->max_dispclk[0] = v->max_dispclk_vmin0p65;
+	v->max_dppclk[5] = v->max_dppclk_vmax0p9;
+	v->max_dppclk[4] = v->max_dppclk_vmax0p9;
+	v->max_dppclk[3] = v->max_dppclk_vmax0p9;
+	v->max_dppclk[2] = v->max_dppclk_vnom0p8;
+	v->max_dppclk[1] = v->max_dppclk_vmid0p72;
+	v->max_dppclk[0] = v->max_dppclk_vmin0p65;
+	v->phyclk_per_state[5] = v->phyclkv_max0p9;
+	v->phyclk_per_state[4] = v->phyclkv_max0p9;
+	v->phyclk_per_state[3] = v->phyclkv_max0p9;
+	v->phyclk_per_state[2] = v->phyclkv_nom0p8;
+	v->phyclk_per_state[1] = v->phyclkv_mid0p72;
+	v->phyclk_per_state[0] = v->phyclkv_min0p65;
+
+	if (dc->public.debug.use_max_voltage) {
+		v->max_dppclk[1] = v->max_dppclk_vnom0p8;
+		v->max_dppclk[0] = v->max_dppclk_vnom0p8;
+	}
+
+	if (v->voltage_override == dcn_bw_v_max0p9) {
+		v->voltage_override_level = number_of_states - 1;
+	} else if (v->voltage_override == dcn_bw_v_nom0p8) {
+		v->voltage_override_level = number_of_states - 2;
+	} else if (v->voltage_override == dcn_bw_v_mid0p72) {
+		v->voltage_override_level = number_of_states - 3;
+	} else {
+		v->voltage_override_level = 0;
+	}
+	v->synchronized_vblank = dcn_bw_no;
+	v->ta_pscalculation = dcn_bw_override;
+	v->allow_different_hratio_vratio = dcn_bw_yes;
+
+
+	for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
+		struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
+
+		if (!pipe->stream)
+			continue;
+		/* skip all but first of split pipes */
+		if (pipe->top_pipe && pipe->top_pipe->surface == pipe->surface)
+			continue;
+
+		v->underscan_output[input_idx] = false; /* taken care of in recout already*/
+		v->interlace_output[input_idx] = false;
+
+		v->htotal[input_idx] = pipe->stream->public.timing.h_total;
+		v->vtotal[input_idx] = pipe->stream->public.timing.v_total;
+		v->v_sync_plus_back_porch[input_idx] = pipe->stream->public.timing.v_total
+				- pipe->stream->public.timing.v_addressable
+				- pipe->stream->public.timing.v_front_porch;
+		v->vactive[input_idx] = pipe->stream->public.timing.v_addressable;
+		v->pixel_clock[input_idx] = pipe->stream->public.timing.pix_clk_khz / 1000.0f;
+
+
+		if (!pipe->surface){
+			v->dcc_enable[input_idx] = dcn_bw_yes;
+			v->source_pixel_format[input_idx] = dcn_bw_rgb_sub_32;
+			v->source_surface_mode[input_idx] = dcn_bw_sw_4_kb_s;
+			v->lb_bit_per_pixel[input_idx] = 30;
+			v->viewport_width[input_idx] = pipe->stream->public.timing.h_addressable;
+			v->viewport_height[input_idx] = pipe->stream->public.timing.v_addressable;
+			v->scaler_rec_out_width[input_idx] = pipe->stream->public.timing.h_addressable;
+			v->scaler_recout_height[input_idx] = pipe->stream->public.timing.v_addressable;
+			v->override_hta_ps[input_idx] = 1;
+			v->override_vta_ps[input_idx] = 1;
+			v->override_hta_pschroma[input_idx] = 1;
+			v->override_vta_pschroma[input_idx] = 1;
+			v->source_scan[input_idx] = dcn_bw_hor;
+
+		} else {
+			v->viewport_height[input_idx] =  pipe->scl_data.viewport.height;
+			v->viewport_width[input_idx] = pipe->scl_data.viewport.width;
+			v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width;
+			v->scaler_recout_height[input_idx] = pipe->scl_data.recout.height;
+			if (pipe->bottom_pipe && pipe->bottom_pipe->surface == pipe->surface) {
+				if (pipe->surface->public.rotation % 2 == 0) {
+					int viewport_end = pipe->scl_data.viewport.width
+							+ pipe->scl_data.viewport.x;
+					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.width
+							+ pipe->bottom_pipe->scl_data.viewport.x;
+
+					if (viewport_end > viewport_b_end)
+						v->viewport_width[input_idx] = viewport_end
+							- pipe->bottom_pipe->scl_data.viewport.x;
+					else
+						v->viewport_width[input_idx] = viewport_b_end
+									- pipe->scl_data.viewport.x;
+				} else  {
+					int viewport_end = pipe->scl_data.viewport.height
+						+ pipe->scl_data.viewport.y;
+					int viewport_b_end = pipe->bottom_pipe->scl_data.viewport.height
+						+ pipe->bottom_pipe->scl_data.viewport.y;
+
+					if (viewport_end > viewport_b_end)
+						v->viewport_height[input_idx] = viewport_end
+							- pipe->bottom_pipe->scl_data.viewport.y;
+					else
+						v->viewport_height[input_idx] = viewport_b_end
+									- pipe->scl_data.viewport.y;
+				}
+				v->scaler_rec_out_width[input_idx] = pipe->scl_data.recout.width
+						+ pipe->bottom_pipe->scl_data.recout.width;
+			}
+
+			v->dcc_enable[input_idx] = pipe->surface->public.dcc.enable ? dcn_bw_yes : dcn_bw_no;
+			v->source_pixel_format[input_idx] = tl_pixel_format_to_bw_defs(
+					pipe->surface->public.format);
+			v->source_surface_mode[input_idx] = tl_sw_mode_to_bw_defs(
+					pipe->surface->public.tiling_info.gfx9.swizzle);
+			v->lb_bit_per_pixel[input_idx] = tl_lb_bpp_to_int(pipe->scl_data.lb_params.depth);
+			v->override_hta_ps[input_idx] = pipe->scl_data.taps.h_taps;
+			v->override_vta_ps[input_idx] = pipe->scl_data.taps.v_taps;
+			v->override_hta_pschroma[input_idx] = pipe->scl_data.taps.h_taps_c;
+			v->override_vta_pschroma[input_idx] = pipe->scl_data.taps.v_taps_c;
+			v->source_scan[input_idx] = (pipe->surface->public.rotation % 2) ? dcn_bw_vert : dcn_bw_hor;
+		}
+		if (v->is_line_buffer_bpp_fixed == dcn_bw_yes)
+			v->lb_bit_per_pixel[input_idx] = v->line_buffer_fixed_bpp;
+		v->dcc_rate[input_idx] = 1; /*TODO: Worst case? does this change?*/
+		v->output_format[input_idx] = dcn_bw_444;
+		v->output[input_idx] = dcn_bw_dp;
+
+		input_idx++;
+	}
+	v->number_of_active_planes = input_idx;
+
+	scaler_settings_calculation(v);
+	mode_support_and_system_configuration(v);
+
+	if (v->voltage_level != 5) {
+		float bw_consumed = v->total_bandwidth_consumed_gbyte_per_second;
+		if (bw_consumed < v->fabric_and_dram_bandwidth_vmin0p65)
+			bw_consumed = v->fabric_and_dram_bandwidth_vmin0p65;
+		else if (bw_consumed < v->fabric_and_dram_bandwidth_vmid0p72)
+			bw_consumed = v->fabric_and_dram_bandwidth_vmid0p72;
+		else if (bw_consumed < v->fabric_and_dram_bandwidth_vnom0p8)
+			bw_consumed = v->fabric_and_dram_bandwidth_vnom0p8;
+		else
+			bw_consumed = v->fabric_and_dram_bandwidth_vmax0p9;
+
+		display_pipe_configuration(v);
+		calc_wm_sets_and_perf_params(context, v);
+		context->fclk_khz = (int)(bw_consumed * 1000000 /
+				(ddr4_dram_factor_single_Channel * v->number_of_channels));
+		context->dram_ccm_us = (int)(v->dram_clock_change_margin);
+		context->min_active_dram_ccm_us = (int)(v->min_active_dram_clock_change_margin);
+		context->dcfclk_deep_sleep_khz = (int)(v->dcf_clk_deep_sleep * 1000);
+		context->dcfclk_khz = (int)(v->dcfclk * 1000);
+		context->dispclk_khz = (int)(v->dispclk * 1000);
+		if (dc->public.debug.max_disp_clk == true)
+			context->dispclk_khz = (int)(dc->dcn_soc.max_dispclk_vmax0p9 * 1000);
+		context->dppclk_khz = (int)(v->dppclk * 1000);
+		context->dppclk_div = (int)(v->dispclk_dppclk_ratio) == 2;
+
+		for (i = 0, input_idx = 0; i < pool->pipe_count; i++) {
+			struct pipe_ctx *pipe = &context->res_ctx.pipe_ctx[i];
+
+			/* skip inactive pipe */
+			if (!pipe->stream)
+				continue;
+			/* skip all but first of split pipes */
+			if (pipe->top_pipe && pipe->top_pipe->surface == pipe->surface)
+				continue;
+
+			pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
+			pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx];
+			pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
+			pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
+
+			pipe->pipe_dlg_param.htotal = pipe->stream->public.timing.h_total;
+			pipe->pipe_dlg_param.vtotal = pipe->stream->public.timing.v_total;
+			vesa_sync_start = pipe->stream->public.timing.v_addressable +
+						pipe->stream->public.timing.v_border_bottom +
+						pipe->stream->public.timing.v_front_porch;
+
+			asic_blank_end = (pipe->stream->public.timing.v_total -
+						vesa_sync_start -
+						pipe->stream->public.timing.v_border_top)
+			* (pipe->stream->public.timing.flags.INTERLACE ? 1 : 0);
+
+			asic_blank_start = asic_blank_end +
+						(pipe->stream->public.timing.v_border_top +
+						pipe->stream->public.timing.v_addressable +
+						pipe->stream->public.timing.v_border_bottom)
+			* (pipe->stream->public.timing.flags.INTERLACE ? 1 : 0);
+
+			pipe->pipe_dlg_param.vblank_start = asic_blank_start;
+			pipe->pipe_dlg_param.vblank_end = asic_blank_end;
+
+			if (pipe->surface) {
+				struct pipe_ctx *hsplit_pipe = pipe->bottom_pipe;
+
+				if (v->dpp_per_plane[input_idx] == 2 ||
+						(pipe->stream->public.timing.timing_3d_format == TIMING_3D_FORMAT_TOP_AND_BOTTOM ||
+						 pipe->stream->public.timing.timing_3d_format == TIMING_3D_FORMAT_SIDE_BY_SIDE)) {
+					if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
+						/* update previously split pipe */
+						hsplit_pipe->pipe_dlg_param.vupdate_width = v->v_update_width[input_idx];
+						hsplit_pipe->pipe_dlg_param.vupdate_offset = v->v_update_offset[input_idx];
+						hsplit_pipe->pipe_dlg_param.vready_offset = v->v_ready_offset[input_idx];
+						hsplit_pipe->pipe_dlg_param.vstartup_start = v->v_startup[input_idx];
+
+						hsplit_pipe->pipe_dlg_param.htotal = pipe->stream->public.timing.h_total;
+						hsplit_pipe->pipe_dlg_param.vtotal = pipe->stream->public.timing.v_total;
+						hsplit_pipe->pipe_dlg_param.vblank_start = pipe->pipe_dlg_param.vblank_start;
+						hsplit_pipe->pipe_dlg_param.vblank_end = pipe->pipe_dlg_param.vblank_end;
+					} else {
+						/* pipe not split previously needs split */
+						hsplit_pipe = find_idle_secondary_pipe(&context->res_ctx, pool);
+						ASSERT(hsplit_pipe);
+						split_stream_across_pipes(
+							&context->res_ctx, pool,
+							pipe, hsplit_pipe);
+					}
+
+					dcn_bw_calc_rq_dlg_ttu(dc, v, hsplit_pipe);
+				} else if (hsplit_pipe && hsplit_pipe->surface == pipe->surface) {
+					/* merge previously split pipe */
+					if (pipe->bottom_pipe->bottom_pipe)
+						pipe->bottom_pipe->bottom_pipe->top_pipe = pipe;
+					memset(pipe->bottom_pipe, 0, sizeof(*pipe->bottom_pipe));
+					pipe->bottom_pipe = pipe->bottom_pipe->bottom_pipe;
+					resource_build_scaling_params(pipe);
+				}
+				/* for now important to do this after pipe split for building e2e params */
+				dcn_bw_calc_rq_dlg_ttu(dc, v, pipe);
+			}
+
+			input_idx++;
+		}
+		if (dc->public.debug.use_dml_wm)
+			dcn_dml_wm_override(v, (struct display_mode_lib *)
+					&dc->dml, context, pool);
+	}
+
+	kernel_fpu_end();
+	return v->voltage_level != 5;
+}
+
+unsigned int dcn_find_normalized_clock_vdd_Level(
+	const struct core_dc *dc,
+	enum dm_pp_clock_type clocks_type,
+	int clocks_in_khz)
+{
+	int vdd_level = dcn_bw_v_min0p65;
+
+	if (clocks_in_khz == 0)/*todo some clock not in the considerations*/
+		return vdd_level;
+
+	switch (clocks_type) {
+	case DM_PP_CLOCK_TYPE_DISPLAY_CLK:
+		if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmax0p9*1000) {
+			vdd_level = dcn_bw_v_max0p91;
+			BREAK_TO_DEBUGGER();
+		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vnom0p8*1000) {
+			vdd_level = dcn_bw_v_max0p9;
+		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmid0p72*1000) {
+			vdd_level = dcn_bw_v_nom0p8;
+		} else if (clocks_in_khz > dc->dcn_soc.max_dispclk_vmin0p65*1000) {
+			vdd_level = dcn_bw_v_mid0p72;
+		} else
+			vdd_level = dcn_bw_v_min0p65;
+		break;
+	case DM_PP_CLOCK_TYPE_DISPLAYPHYCLK:
+		if (clocks_in_khz > dc->dcn_soc.phyclkv_max0p9*1000) {
+			vdd_level = dcn_bw_v_max0p91;
+			BREAK_TO_DEBUGGER();
+		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_nom0p8*1000) {
+			vdd_level = dcn_bw_v_max0p9;
+		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_mid0p72*1000) {
+			vdd_level = dcn_bw_v_nom0p8;
+		} else if (clocks_in_khz > dc->dcn_soc.phyclkv_min0p65*1000) {
+			vdd_level = dcn_bw_v_mid0p72;
+		} else
+			vdd_level = dcn_bw_v_min0p65;
+		break;
+
+	case DM_PP_CLOCK_TYPE_DPPCLK:
+		if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmax0p9*1000) {
+			vdd_level = dcn_bw_v_max0p91;
+			BREAK_TO_DEBUGGER();
+		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vnom0p8*1000) {
+			vdd_level = dcn_bw_v_max0p9;
+		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmid0p72*1000) {
+			vdd_level = dcn_bw_v_nom0p8;
+		} else if (clocks_in_khz > dc->dcn_soc.max_dppclk_vmin0p65*1000) {
+			vdd_level = dcn_bw_v_mid0p72;
+		} else
+			vdd_level = dcn_bw_v_min0p65;
+		break;
+
+	case DM_PP_CLOCK_TYPE_MEMORY_CLK:
+		{
+			unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc.number_of_channels);
+			if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9*1000000/factor) {
+			vdd_level = dcn_bw_v_max0p91;
+				BREAK_TO_DEBUGGER();
+			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8*1000000/factor) {
+				vdd_level = dcn_bw_v_max0p9;
+			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72*1000000/factor) {
+				vdd_level = dcn_bw_v_nom0p8;
+			} else if (clocks_in_khz > dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65*1000000/factor) {
+				vdd_level = dcn_bw_v_mid0p72;
+			} else
+				vdd_level = dcn_bw_v_min0p65;
+		}
+		break;
+
+	case DM_PP_CLOCK_TYPE_DCFCLK:
+		if (clocks_in_khz > dc->dcn_soc.dcfclkv_max0p9*1000) {
+			vdd_level = dcn_bw_v_max0p91;
+			BREAK_TO_DEBUGGER();
+		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_nom0p8*1000) {
+			vdd_level = dcn_bw_v_max0p9;
+		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_mid0p72*1000) {
+			vdd_level = dcn_bw_v_nom0p8;
+		} else if (clocks_in_khz > dc->dcn_soc.dcfclkv_min0p65*1000) {
+			vdd_level = dcn_bw_v_mid0p72;
+		} else
+			vdd_level = dcn_bw_v_min0p65;
+		break;
+
+	default:
+		 break;
+	}
+	return vdd_level;
+}
+
+unsigned int dcn_find_dcfclk_suits_all(
+	const struct core_dc *dc,
+	struct clocks_value *clocks)
+{
+	unsigned vdd_level, vdd_level_temp;
+	unsigned dcf_clk;
+
+	/*find a common supported voltage level*/
+	vdd_level = dcn_find_normalized_clock_vdd_Level(
+		dc, DM_PP_CLOCK_TYPE_DISPLAY_CLK, clocks->dispclk_in_khz);
+	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
+		dc, DM_PP_CLOCK_TYPE_DISPLAYPHYCLK, clocks->phyclk_in_khz);
+
+	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
+	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
+		dc, DM_PP_CLOCK_TYPE_DPPCLK, clocks->dppclk_in_khz);
+	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
+
+	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
+		dc, DM_PP_CLOCK_TYPE_MEMORY_CLK, clocks->dcfclock_in_khz);
+	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
+	vdd_level_temp = dcn_find_normalized_clock_vdd_Level(
+		dc, DM_PP_CLOCK_TYPE_DCFCLK, clocks->dcfclock_in_khz);
+
+	/*find that level conresponding dcfclk*/
+	vdd_level = dcn_bw_max(vdd_level, vdd_level_temp);
+	if (vdd_level == dcn_bw_v_max0p91) {
+		BREAK_TO_DEBUGGER();
+		dcf_clk = dc->dcn_soc.dcfclkv_max0p9*1000;
+	} else if (vdd_level == dcn_bw_v_max0p9)
+		dcf_clk =  dc->dcn_soc.dcfclkv_max0p9*1000;
+	else if (vdd_level == dcn_bw_v_nom0p8)
+		dcf_clk =  dc->dcn_soc.dcfclkv_nom0p8*1000;
+	else if (vdd_level == dcn_bw_v_mid0p72)
+		dcf_clk =  dc->dcn_soc.dcfclkv_mid0p72*1000;
+	else
+		dcf_clk =  dc->dcn_soc.dcfclkv_min0p65*1000;
+
+	dm_logger_write(dc->ctx->logger, LOG_HW_MARKS,
+		"\tdcf_clk for voltage = %d\n", dcf_clk);
+	return dcf_clk;
+}
+
+void dcn_bw_update_from_pplib(struct core_dc *dc)
+{
+	struct dc_context *ctx = dc->ctx;
+	struct dm_pp_clock_levels_with_latency clks = {0};
+	struct dm_pp_clock_levels_with_voltage clks2 = {0};
+
+	kernel_fpu_begin();
+	dc->dcn_soc.number_of_channels = dc->ctx->asic_id.vram_width / ddr4_dram_width;
+	ASSERT(dc->dcn_soc.number_of_channels && dc->dcn_soc.number_of_channels < 3);
+	if (dc->dcn_soc.number_of_channels == 0)/*old sbios bug*/
+		dc->dcn_soc.number_of_channels = 2;
+
+	if (dm_pp_get_clock_levels_by_type_with_voltage(
+				ctx, DM_PP_CLOCK_TYPE_DISPLAY_CLK, &clks2) &&
+				clks2.num_levels >= 3) {
+		dc->dcn_soc.max_dispclk_vmin0p65 = clks2.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dispclk_vmid0p72 = clks2.data[clks2.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dispclk_vnom0p8 = clks2.data[clks2.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dispclk_vmax0p9 = clks2.data[clks2.num_levels - 1].clocks_in_khz / 1000.0;
+	} else
+		BREAK_TO_DEBUGGER();
+/*
+	if (dm_pp_get_clock_levels_by_type_with_latency(
+			ctx, DM_PP_CLOCK_TYPE_MEMORY_CLK, &clks) &&
+			clks.num_levels != 0) {
+			//this  is to get DRAM data_rate
+		//FabricAndDRAMBandwidth = min(64*FCLK , Data rate * single_Channel_Width * number of channels);
+	}*/
+	if (dm_pp_get_clock_levels_by_type_with_latency(
+			ctx, DM_PP_CLOCK_TYPE_FCLK, &clks) &&
+			clks.num_levels != 0) {
+		ASSERT(clks.num_levels >= 3);
+		dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 = dc->dcn_soc.number_of_channels *
+			(clks.data[0].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+		if (clks.num_levels > 2) {
+			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.number_of_channels *
+					(clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+		} else {
+			dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72 = dc->dcn_soc.number_of_channels *
+					(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+		}
+		dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 = dc->dcn_soc.number_of_channels *
+				(clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+		dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 = dc->dcn_soc.number_of_channels *
+				(clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0) * ddr4_dram_factor_single_Channel / 1000.0;
+	} else
+		BREAK_TO_DEBUGGER();
+	if (dm_pp_get_clock_levels_by_type_with_latency(
+				ctx, DM_PP_CLOCK_TYPE_DCFCLK, &clks) &&
+				clks.num_levels >= 3) {
+		dc->dcn_soc.dcfclkv_min0p65 = clks.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc.dcfclkv_mid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc.dcfclkv_nom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc.dcfclkv_max0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
+	} else
+		BREAK_TO_DEBUGGER();
+	if (dm_pp_get_clock_levels_by_type_with_voltage(
+				ctx, DM_PP_CLOCK_TYPE_DISPLAYPHYCLK, &clks2) &&
+				clks2.num_levels >= 3) {
+		dc->dcn_soc.phyclkv_min0p65 = clks2.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc.phyclkv_mid0p72 = clks2.data[clks2.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc.phyclkv_nom0p8 = clks2.data[clks2.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc.phyclkv_max0p9 = clks2.data[clks2.num_levels - 1].clocks_in_khz / 1000.0;
+	} else
+		BREAK_TO_DEBUGGER();
+	if (dm_pp_get_clock_levels_by_type_with_latency(
+				ctx, DM_PP_CLOCK_TYPE_DPPCLK, &clks) &&
+				clks.num_levels >= 3) {
+		dc->dcn_soc.max_dppclk_vmin0p65 = clks.data[0].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dppclk_vmid0p72 = clks.data[clks.num_levels - 3].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dppclk_vnom0p8 = clks.data[clks.num_levels - 2].clocks_in_khz / 1000.0;
+		dc->dcn_soc.max_dppclk_vmax0p9 = clks.data[clks.num_levels - 1].clocks_in_khz / 1000.0;
+	}
+
+	if (dm_pp_get_clock_levels_by_type_with_latency(
+				ctx, DM_PP_CLOCK_TYPE_SOCCLK, &clks) &&
+				clks.num_levels >= 3) {
+		dc->dcn_soc.socclk = clks.data[0].clocks_in_khz / 1000.0;
+	} else
+			BREAK_TO_DEBUGGER();
+	kernel_fpu_end();
+}
+
+void dcn_bw_notify_pplib_of_wm_ranges(struct core_dc *dc)
+{
+	struct dm_pp_wm_sets_with_clock_ranges_soc15 clk_ranges = {0};
+	int max_fclk_khz, nom_fclk_khz, min_fclk_khz, max_dcfclk_khz,
+		nom_dcfclk_khz, min_dcfclk_khz, socclk_khz;
+	const int overdrive = 5000000; /* 5 GHz to cover Overdrive */
+	unsigned factor = (ddr4_dram_factor_single_Channel * dc->dcn_soc.number_of_channels);
+
+	kernel_fpu_begin();
+	max_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9 * 1000000 / factor;
+	nom_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8 * 1000000 / factor;
+	min_fclk_khz = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65 * 1000000 / factor;
+	max_dcfclk_khz = dc->dcn_soc.dcfclkv_max0p9 * 1000;
+	nom_dcfclk_khz = dc->dcn_soc.dcfclkv_nom0p8 * 1000;
+	min_dcfclk_khz = dc->dcn_soc.dcfclkv_min0p65 * 1000;
+	socclk_khz = dc->dcn_soc.socclk * 1000;
+	kernel_fpu_end();
+
+	/* Now notify PPLib/SMU about which Watermarks sets they should select
+	 * depending on DPM state they are in. And update BW MGR GFX Engine and
+	 * Memory clock member variables for Watermarks calculations for each
+	 * Watermark Set
+	 */
+	/* SOCCLK does not affect anytihng but writeback for DCN so for now we dont
+	 * care what the value is, hence min to overdrive level
+	 */
+	clk_ranges.num_wm_dmif_sets = 4;
+	clk_ranges.num_wm_mcif_sets = 4;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_set_id = WM_SET_A;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_dcfclk_clk_in_khz = min_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_dcfclk_clk_in_khz = nom_dcfclk_khz - 1;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[0].wm_max_mem_clk_in_khz = nom_fclk_khz - 1;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_set_id = WM_SET_A;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_socclk_clk_in_khz = socclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_socclk_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_min_memg_clk_in_khz = min_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[0].wm_max_mem_clk_in_khz = nom_fclk_khz - 1;
+
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_set_id = WM_SET_B;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_dcfclk_clk_in_khz = min_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_dcfclk_clk_in_khz = nom_dcfclk_khz - 1;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_min_memg_clk_in_khz = nom_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[1].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_set_id = WM_SET_B;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_socclk_clk_in_khz = socclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_socclk_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_min_memg_clk_in_khz = nom_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[1].wm_max_mem_clk_in_khz = max_fclk_khz;
+
+
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_set_id = WM_SET_C;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_dcfclk_clk_in_khz = nom_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_dcfclk_clk_in_khz = max_dcfclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
+	clk_ranges.wm_dmif_clocks_ranges[2].wm_max_mem_clk_in_khz = max_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_set_id = WM_SET_C;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_socclk_clk_in_khz = socclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_socclk_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_min_memg_clk_in_khz = nom_fclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[2].wm_max_mem_clk_in_khz = max_fclk_khz;
+
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_set_id = WM_SET_D;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_dcfclk_clk_in_khz = max_dcfclk_khz + 1;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_dcfclk_clk_in_khz = overdrive;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz + 1;
+	clk_ranges.wm_dmif_clocks_ranges[3].wm_max_mem_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_set_id = WM_SET_D;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_socclk_clk_in_khz = socclk_khz;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_socclk_clk_in_khz = overdrive;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_min_memg_clk_in_khz = max_fclk_khz + 1;
+	clk_ranges.wm_mcif_clocks_ranges[3].wm_max_mem_clk_in_khz = overdrive;
+
+	/* Notify PP Lib/SMU which Watermarks to use for which clock ranges */
+	dm_pp_notify_wm_clock_changes_soc15(dc->ctx, &clk_ranges);
+}
+
+void dcn_bw_sync_calcs_and_dml(struct core_dc *dc)
+{
+	kernel_fpu_begin();
+	dc->dml.soc.vmin.socclk_mhz = dc->dcn_soc.socclk;
+	dc->dml.soc.vmid.socclk_mhz = dc->dcn_soc.socclk;
+	dc->dml.soc.vnom.socclk_mhz = dc->dcn_soc.socclk;
+	dc->dml.soc.vmax.socclk_mhz = dc->dcn_soc.socclk;
+
+	dc->dml.soc.vmin.dcfclk_mhz = dc->dcn_soc.dcfclkv_min0p65;
+	dc->dml.soc.vmid.dcfclk_mhz = dc->dcn_soc.dcfclkv_mid0p72;
+	dc->dml.soc.vnom.dcfclk_mhz = dc->dcn_soc.dcfclkv_nom0p8;
+	dc->dml.soc.vmax.dcfclk_mhz = dc->dcn_soc.dcfclkv_max0p9;
+
+	dc->dml.soc.vmin.dispclk_mhz = dc->dcn_soc.max_dispclk_vmin0p65;
+	dc->dml.soc.vmid.dispclk_mhz = dc->dcn_soc.max_dispclk_vmid0p72;
+	dc->dml.soc.vnom.dispclk_mhz = dc->dcn_soc.max_dispclk_vnom0p8;
+	dc->dml.soc.vmax.dispclk_mhz = dc->dcn_soc.max_dispclk_vmax0p9;
+
+	dc->dml.soc.vmin.dppclk_mhz = dc->dcn_soc.max_dppclk_vmin0p65;
+	dc->dml.soc.vmid.dppclk_mhz = dc->dcn_soc.max_dppclk_vmid0p72;
+	dc->dml.soc.vnom.dppclk_mhz = dc->dcn_soc.max_dppclk_vnom0p8;
+	dc->dml.soc.vmax.dppclk_mhz = dc->dcn_soc.max_dppclk_vmax0p9;
+
+	dc->dml.soc.vmin.phyclk_mhz = dc->dcn_soc.phyclkv_min0p65;
+	dc->dml.soc.vmid.phyclk_mhz = dc->dcn_soc.phyclkv_mid0p72;
+	dc->dml.soc.vnom.phyclk_mhz = dc->dcn_soc.phyclkv_nom0p8;
+	dc->dml.soc.vmax.phyclk_mhz = dc->dcn_soc.phyclkv_max0p9;
+
+	dc->dml.soc.vmin.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmin0p65;
+	dc->dml.soc.vmid.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmid0p72;
+	dc->dml.soc.vnom.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vnom0p8;
+	dc->dml.soc.vmax.dram_bw_per_chan_gbps = dc->dcn_soc.fabric_and_dram_bandwidth_vmax0p9;
+
+	dc->dml.soc.sr_exit_time_us = dc->dcn_soc.sr_exit_time;
+	dc->dml.soc.sr_enter_plus_exit_time_us = dc->dcn_soc.sr_enter_plus_exit_time;
+	dc->dml.soc.urgent_latency_us = dc->dcn_soc.urgent_latency;
+	dc->dml.soc.writeback_latency_us = dc->dcn_soc.write_back_latency;
+	dc->dml.soc.ideal_dram_bw_after_urgent_percent =
+			dc->dcn_soc.percent_of_ideal_drambw_received_after_urg_latency;
+	dc->dml.soc.max_request_size_bytes = dc->dcn_soc.max_request_size;
+	dc->dml.soc.downspread_percent = dc->dcn_soc.downspreading;
+	dc->dml.soc.round_trip_ping_latency_dcfclk_cycles =
+			dc->dcn_soc.round_trip_ping_latency_cycles;
+	dc->dml.soc.urgent_out_of_order_return_per_channel_bytes =
+			dc->dcn_soc.urgent_out_of_order_return_per_channel;
+	dc->dml.soc.num_chans = dc->dcn_soc.number_of_channels;
+	dc->dml.soc.vmm_page_size_bytes = dc->dcn_soc.vmm_page_size;
+	dc->dml.soc.dram_clock_change_latency_us = dc->dcn_soc.dram_clock_change_latency;
+	dc->dml.soc.return_bus_width_bytes = dc->dcn_soc.return_bus_width;
+
+	dc->dml.ip.rob_buffer_size_kbytes = dc->dcn_ip.rob_buffer_size_in_kbyte;
+	dc->dml.ip.det_buffer_size_kbytes = dc->dcn_ip.det_buffer_size_in_kbyte;
+	dc->dml.ip.dpp_output_buffer_pixels = dc->dcn_ip.dpp_output_buffer_pixels;
+	dc->dml.ip.opp_output_buffer_lines = dc->dcn_ip.opp_output_buffer_lines;
+	dc->dml.ip.pixel_chunk_size_kbytes = dc->dcn_ip.pixel_chunk_size_in_kbyte;
+	dc->dml.ip.pte_enable = dc->dcn_ip.pte_enable == dcn_bw_yes;
+	dc->dml.ip.pte_chunk_size_kbytes = dc->dcn_ip.pte_chunk_size;
+	dc->dml.ip.meta_chunk_size_kbytes = dc->dcn_ip.meta_chunk_size;
+	dc->dml.ip.writeback_chunk_size_kbytes = dc->dcn_ip.writeback_chunk_size;
+	dc->dml.ip.line_buffer_size_bits = dc->dcn_ip.line_buffer_size;
+	dc->dml.ip.max_line_buffer_lines = dc->dcn_ip.max_line_buffer_lines;
+	dc->dml.ip.IsLineBufferBppFixed = dc->dcn_ip.is_line_buffer_bpp_fixed == dcn_bw_yes;
+	dc->dml.ip.LineBufferFixedBpp = dc->dcn_ip.line_buffer_fixed_bpp;
+	dc->dml.ip.writeback_luma_buffer_size_kbytes = dc->dcn_ip.writeback_luma_buffer_size;
+	dc->dml.ip.writeback_chroma_buffer_size_kbytes = dc->dcn_ip.writeback_chroma_buffer_size;
+	dc->dml.ip.max_num_dpp = dc->dcn_ip.max_num_dpp;
+	dc->dml.ip.max_num_wb = dc->dcn_ip.max_num_writeback;
+	dc->dml.ip.max_dchub_pscl_bw_pix_per_clk = dc->dcn_ip.max_dchub_topscl_throughput;
+	dc->dml.ip.max_pscl_lb_bw_pix_per_clk = dc->dcn_ip.max_pscl_tolb_throughput;
+	dc->dml.ip.max_lb_vscl_bw_pix_per_clk = dc->dcn_ip.max_lb_tovscl_throughput;
+	dc->dml.ip.max_vscl_hscl_bw_pix_per_clk = dc->dcn_ip.max_vscl_tohscl_throughput;
+	dc->dml.ip.max_hscl_ratio = dc->dcn_ip.max_hscl_ratio;
+	dc->dml.ip.max_vscl_ratio = dc->dcn_ip.max_vscl_ratio;
+	dc->dml.ip.max_hscl_taps = dc->dcn_ip.max_hscl_taps;
+	dc->dml.ip.max_vscl_taps = dc->dcn_ip.max_vscl_taps;
+	/*pte_buffer_size_in_requests missing in dml*/
+	dc->dml.ip.dispclk_ramp_margin_percent = dc->dcn_ip.dispclk_ramping_margin;
+	dc->dml.ip.underscan_factor = dc->dcn_ip.under_scan_factor;
+	dc->dml.ip.max_inter_dcn_tile_repeaters = dc->dcn_ip.max_inter_dcn_tile_repeaters;
+	dc->dml.ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one =
+		dc->dcn_ip.can_vstartup_lines_exceed_vsync_plus_back_porch_lines_minus_one == dcn_bw_yes;
+	dc->dml.ip.bug_forcing_LC_req_same_size_fixed =
+		dc->dcn_ip.bug_forcing_luma_and_chroma_request_to_same_size_fixed == dcn_bw_yes;
+	dc->dml.ip.dcfclk_cstate_latency = dc->dcn_ip.dcfclk_cstate_latency;
+	kernel_fpu_end();
+}
