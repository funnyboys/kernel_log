commit b8020b0304c8f44e5e29f0b1a04d31e0bf68d26a
Author: Joseph Greathouse <Joseph.Greathouse@amd.com>
Date:   Wed Sep 18 14:49:57 2019 -0500

    drm/amdkfd: Enable over-subscription with >1 GWS queue
    
    The current GWS usage model will only allows a single GWS-enabled
    process to be active on the GPU at once. This ensures that a
    barrier-using kernel gets a known amount of GPU hardware, to
    prevent deadlock due to inability to go beyond the GWS barrier.
    
    The HWS watches how many GWS entries are assigned to each process,
    and goes into over-subscription mode when two processes need more
    than the 64 that are available. The current KFD method for working
    with this is to allocate all 64 GWS entries to each GWS-capable
    process.
    
    When more than one GWS-enabled process is in the runlist, we must
    make sure the runlist is in over-subscription mode, so that the
    HWS gets a chained RUN_LIST packet and continues scheduling
    kernels.
    
    Signed-off-by: Joseph Greathouse <Joseph.Greathouse@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 50d919f814e9..4afa015c69b1 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -182,6 +182,7 @@ struct device_queue_manager {
 	unsigned int		processes_count;
 	unsigned int		active_queue_count;
 	unsigned int		active_cp_queue_count;
+	unsigned int		gws_queue_count;
 	unsigned int		total_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;

commit c7637c95abebc811d2479a5b924859025def544a
Author: Yong Zhao <Yong.Zhao@amd.com>
Date:   Wed Feb 5 16:53:37 2020 -0500

    drm/amdkfd: Delete unnecessary unmap queue package submissions
    
    The previous way of using SDMA queue count to infer whether we should unmap
    SDMA engines has bugs. The reason it did not cause issues is because MEC
    firmware unmaps all queues (CP + SDMA) when a unmap package for compute
    engine is received. Becasue of that, only one unmap queue package
    is needed, instead of one unmap queue package for CP and each SDMA engine,
    which results in much simpler driver code.
    
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 05e0afc04cd9..50d919f814e9 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -182,8 +182,6 @@ struct device_queue_manager {
 	unsigned int		processes_count;
 	unsigned int		active_queue_count;
 	unsigned int		active_cp_queue_count;
-	unsigned int		sdma_queue_count;
-	unsigned int		xgmi_sdma_queue_count;
 	unsigned int		total_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;

commit b42902f4af8fecd2781f71ad166e8a807edb1053
Author: Yong Zhao <Yong.Zhao@amd.com>
Date:   Wed Feb 5 14:48:38 2020 -0500

    drm/amdkfd: Count active CP queues directly
    
    The previous code of calculating active CP queues is problematic if
    some SDMA queues are inactive. Fix that by counting CP queues directly.
    
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 3f0fb0d28c01..05e0afc04cd9 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -181,6 +181,7 @@ struct device_queue_manager {
 	unsigned int		saved_flags;
 	unsigned int		processes_count;
 	unsigned int		active_queue_count;
+	unsigned int		active_cp_queue_count;
 	unsigned int		sdma_queue_count;
 	unsigned int		xgmi_sdma_queue_count;
 	unsigned int		total_queue_count;

commit e6945304187deae0a28ebc65008ec11277f1c0f0
Author: Yong Zhao <Yong.Zhao@amd.com>
Date:   Thu Jan 30 18:35:23 2020 -0500

    drm/amdkfd: Avoid ambiguity by indicating it's cp queue
    
    The queues represented in queue_bitmap are only CP queues.
    
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index ee3400e92c30..3f0fb0d28c01 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -219,7 +219,7 @@ void device_queue_manager_init_v10_navi10(
 		struct device_queue_manager_asic_ops *asic_ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-unsigned int get_queues_num(struct device_queue_manager *dqm);
+unsigned int get_cp_queues_num(struct device_queue_manager *dqm);
 unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);
 unsigned int get_num_sdma_queues(struct device_queue_manager *dqm);

commit 81b820b304a07fa4de9d46f095475555881af8fe
Author: Yong Zhao <Yong.Zhao@amd.com>
Date:   Thu Jan 30 18:25:50 2020 -0500

    drm/amdkfd: Rename queue_count to active_queue_count
    
    The name is easier to understand the code.
    
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 871d3b628d2d..ee3400e92c30 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -180,7 +180,7 @@ struct device_queue_manager {
 	struct list_head	queues;
 	unsigned int		saved_flags;
 	unsigned int		processes_count;
-	unsigned int		queue_count;
+	unsigned int		active_queue_count;
 	unsigned int		sdma_queue_count;
 	unsigned int		xgmi_sdma_queue_count;
 	unsigned int		total_queue_count;

commit 09c34e8d7a631cf541c82f46ee73433c28b6ce0e
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Fri Dec 20 02:07:54 2019 -0500

    drm/amdkfd: Improve HWS hang detection and handling
    
    Move HWS hang detection into unmap_queues_cpsch to catch hangs in all
    cases. If this happens during a reset, don't schedule another reset
    because the reset already in progress is expected to take care of it.
    
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Tested-by: Emily Deng <Emily.Deng@amd.com>
    Reviewed-by: shaoyunl  <shaoyun.liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 8991120c4fa2..871d3b628d2d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -104,6 +104,7 @@ struct device_queue_manager_ops {
 	int	(*initialize)(struct device_queue_manager *dqm);
 	int	(*start)(struct device_queue_manager *dqm);
 	int	(*stop)(struct device_queue_manager *dqm);
+	void	(*pre_reset)(struct device_queue_manager *dqm);
 	void	(*uninitialize)(struct device_queue_manager *dqm);
 	int	(*create_kernel_queue)(struct device_queue_manager *dqm,
 					struct kernel_queue *kq,
@@ -198,6 +199,7 @@ struct device_queue_manager {
 
 	/* hw exception  */
 	bool			is_hws_hang;
+	bool			is_resetting;
 	struct work_struct	hw_exception_work;
 	struct kfd_mem_obj	hiq_sdma_mqd;
 	bool			sched_running;

commit 63e088acfc330c7031c4c07e8503337d1f36639a
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Fri Dec 20 02:01:24 2019 -0500

    drm/amdkfd: Remove unused variable
    
    dqm->pipeline_mem wasn't used anywhere.
    
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: shaoyunl  <shaoyun.liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index a8c37e6da027..8991120c4fa2 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -190,7 +190,6 @@ struct device_queue_manager {
 	/* the pasid mapping for each kfd vmid */
 	uint16_t		vmid_pasid[VMID_NUM];
 	uint64_t		pipelines_addr;
-	struct kfd_mem_obj	*pipeline_mem;
 	uint64_t		fence_gpu_addr;
 	unsigned int		*fence_addr;
 	struct kfd_mem_obj	*fence_mem;

commit 2c99a547bcf9bb8532abd2953479949018449f93
Author: Philip Yang <Philip.Yang@amd.com>
Date:   Fri Oct 18 10:15:21 2019 -0400

    drm/amdkfd: don't use dqm lock during device reset/suspend/resume
    
    If device reset/suspend/resume failed for some reason, dqm lock is
    hold forever and this causes deadlock. Below is a kernel backtrace when
    application open kfd after suspend/resume failed.
    
    Instead of holding dqm lock in pre_reset and releasing dqm lock in
    post_reset, add dqm->sched_running flag which is modified in
    dqm->ops.start and dqm->ops.stop. The flag doesn't need lock protection
    because write/read are all inside dqm lock.
    
    For HWS case, map_queues_cpsch and unmap_queues_cpsch checks
    sched_running flag before sending the updated runlist.
    
    v2: For no-HWS case, when device is stopped, don't call
    load/destroy_mqd for eviction, restore and create queue, and avoid
    debugfs dump hdqs.
    
    Backtrace of dqm lock deadlock:
    
    [Thu Oct 17 16:43:37 2019] INFO: task rocminfo:3024 blocked for more
    than 120 seconds.
    [Thu Oct 17 16:43:37 2019]       Not tainted
    5.0.0-rc1-kfd-compute-rocm-dkms-no-npi-1131 #1
    [Thu Oct 17 16:43:37 2019] "echo 0 >
    /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [Thu Oct 17 16:43:37 2019] rocminfo        D    0  3024   2947
    0x80000000
    [Thu Oct 17 16:43:37 2019] Call Trace:
    [Thu Oct 17 16:43:37 2019]  ? __schedule+0x3d9/0x8a0
    [Thu Oct 17 16:43:37 2019]  schedule+0x32/0x70
    [Thu Oct 17 16:43:37 2019]  schedule_preempt_disabled+0xa/0x10
    [Thu Oct 17 16:43:37 2019]  __mutex_lock.isra.9+0x1e3/0x4e0
    [Thu Oct 17 16:43:37 2019]  ? __call_srcu+0x264/0x3b0
    [Thu Oct 17 16:43:37 2019]  ? process_termination_cpsch+0x24/0x2f0
    [amdgpu]
    [Thu Oct 17 16:43:37 2019]  process_termination_cpsch+0x24/0x2f0
    [amdgpu]
    [Thu Oct 17 16:43:37 2019]
    kfd_process_dequeue_from_all_devices+0x42/0x60 [amdgpu]
    [Thu Oct 17 16:43:37 2019]  kfd_process_notifier_release+0x1be/0x220
    [amdgpu]
    [Thu Oct 17 16:43:37 2019]  __mmu_notifier_release+0x3e/0xc0
    [Thu Oct 17 16:43:37 2019]  exit_mmap+0x160/0x1a0
    [Thu Oct 17 16:43:37 2019]  ? __handle_mm_fault+0xba3/0x1200
    [Thu Oct 17 16:43:37 2019]  ? exit_robust_list+0x5a/0x110
    [Thu Oct 17 16:43:37 2019]  mmput+0x4a/0x120
    [Thu Oct 17 16:43:37 2019]  do_exit+0x284/0xb20
    [Thu Oct 17 16:43:37 2019]  ? handle_mm_fault+0xfa/0x200
    [Thu Oct 17 16:43:37 2019]  do_group_exit+0x3a/0xa0
    [Thu Oct 17 16:43:37 2019]  __x64_sys_exit_group+0x14/0x20
    [Thu Oct 17 16:43:37 2019]  do_syscall_64+0x4f/0x100
    [Thu Oct 17 16:43:37 2019]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Suggested-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Philip Yang <Philip.Yang@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 2eaea6b04cbe..a8c37e6da027 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -201,6 +201,7 @@ struct device_queue_manager {
 	bool			is_hws_hang;
 	struct work_struct	hw_exception_work;
 	struct kfd_mem_obj	hiq_sdma_mqd;
+	bool			sched_running;
 };
 
 void device_queue_manager_init_cik(

commit d9d4623c87e9318549e0c85133c38909d9e8c47a
Author: Yong Zhao <Yong.Zhao@amd.com>
Date:   Wed Sep 25 23:49:46 2019 -0400

    drm/amdkfd: Record vmid pasid mapping in the driver for non HWS mode
    
    This makes possible the vmid pasid mapping query through software.
    
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 90db2c9275f6..2eaea6b04cbe 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -32,6 +32,8 @@
 #include "kfd_mqd_manager.h"
 
 
+#define VMID_NUM 16
+
 struct device_process_node {
 	struct qcm_process_device *qpd;
 	struct list_head list;
@@ -185,7 +187,8 @@ struct device_queue_manager {
 	unsigned int		*allocated_queues;
 	uint64_t		sdma_bitmap;
 	uint64_t		xgmi_sdma_bitmap;
-	unsigned int		vmid_bitmap;
+	/* the pasid mapping for each kfd vmid */
+	uint16_t		vmid_pasid[VMID_NUM];
 	uint64_t		pipelines_addr;
 	struct kfd_mem_obj	*pipeline_mem;
 	uint64_t		fence_gpu_addr;

commit 14328aa58ce523a59996c5a82681c43ec048cc33
Author: Philip Cox <Philip.Cox@amd.com>
Date:   Wed May 29 23:03:45 2019 -0500

    drm/amdkfd: Add navi10 support to amdkfd. (v3)
    
    KFD (kernel fusion driver) is the kernel driver
    for the compute backend for usermode compute
    stack.
    
    v2: squash in updates (Alex)
    v3: squash in rebase fixes (Alex)
    
    Signed-off-by: Oak Zeng <Oak.Zeng@amd.com>
    Signed-off-by: Philip Cox <Philip.Cox@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Hawking Zhang <Hawking.Zhang@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 88b4c007696e..90db2c9275f6 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -31,8 +31,6 @@
 #include "kfd_priv.h"
 #include "kfd_mqd_manager.h"
 
-#define KFD_UNMAP_LATENCY_MS			(4000)
-#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
 
 struct device_process_node {
 	struct qcm_process_device *qpd;
@@ -212,6 +210,8 @@ void device_queue_manager_init_vi_tonga(
 		struct device_queue_manager_asic_ops *asic_ops);
 void device_queue_manager_init_v9(
 		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_v10_navi10(
+		struct device_queue_manager_asic_ops *asic_ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 unsigned int get_queues_num(struct device_queue_manager *dqm);

commit 1b4670f6983156526c286723465fdf805070b45d
Author: Oak Zeng <Oak.Zeng@amd.com>
Date:   Thu Feb 7 14:02:27 2019 -0600

    drm/amdkfd: Introduce XGMI SDMA queue type
    
    Existing QUEUE_TYPE_SDMA means PCIe optimized SDMA queues.
    Introduce a new QUEUE_TYPE_SDMA_XGMI, which is optimized
    for non-PCIe transfer such as XGMI.
    
    Signed-off-by: Oak Zeng <Oak.Zeng@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 3742fd340ec3..88b4c007696e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -181,10 +181,12 @@ struct device_queue_manager {
 	unsigned int		processes_count;
 	unsigned int		queue_count;
 	unsigned int		sdma_queue_count;
+	unsigned int		xgmi_sdma_queue_count;
 	unsigned int		total_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;
 	uint64_t		sdma_bitmap;
+	uint64_t		xgmi_sdma_bitmap;
 	unsigned int		vmid_bitmap;
 	uint64_t		pipelines_addr;
 	struct kfd_mem_obj	*pipeline_mem;
@@ -216,6 +218,7 @@ unsigned int get_queues_num(struct device_queue_manager *dqm);
 unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);
 unsigned int get_num_sdma_queues(struct device_queue_manager *dqm);
+unsigned int get_num_xgmi_sdma_queues(struct device_queue_manager *dqm);
 
 static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {

commit 11614c36bc8f4fd22ff91e6150ac63e8bfce33b5
Author: Oak Zeng <ozeng@amd.com>
Date:   Tue Nov 27 21:58:54 2018 -0600

    drm/amdkfd: Allocate MQD trunk for HIQ and SDMA
    
    MEC FW for some new asic requires all SDMA MQDs to be in a continuous
    trunk of memory right after HIQ MQD. Add a field in device queue manager
    to hold the HIQ/SDMA MQD memory object and allocate MQD trunk on device
    queue manager initialization.
    
    Signed-off-by: Oak Zeng <ozeng@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index a5ef7a6650a5..3742fd340ec3 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -197,6 +197,7 @@ struct device_queue_manager {
 	/* hw exception  */
 	bool			is_hws_hang;
 	struct work_struct	hw_exception_work;
+	struct kfd_mem_obj	hiq_sdma_mqd;
 };
 
 void device_queue_manager_init_cik(

commit fdfa090bc90f34543b8efd05b05a143ae6d52406
Author: Oak Zeng <ozeng@amd.com>
Date:   Wed Dec 5 10:15:27 2018 -0600

    drm/amdkfd: Init mqd managers in device queue manager init
    
    Previously mqd managers was initialized on demand. As there
    are only a few type of mqd managers, the on demand initialization
    doesn't save too much memory. Initialize them on device
    queue initialization instead and delete the get_mqd_manager
    interface. This makes codes more organized for future changes.
    
    Signed-off-by: Oak Zeng <ozeng@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index a5d83ec1c6a8..a5ef7a6650a5 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -48,8 +48,6 @@ struct device_process_node {
  *
  * @update_queue: Queue update routine.
  *
- * @get_mqd_manager: Returns the mqd manager according to the mqd type.
- *
  * @exeute_queues: Dispatches the queues list to the H/W.
  *
  * @register_process: This routine associates a specific process with device.
@@ -97,10 +95,6 @@ struct device_queue_manager_ops {
 	int	(*update_queue)(struct device_queue_manager *dqm,
 				struct queue *q);
 
-	struct mqd_manager * (*get_mqd_manager)
-					(struct device_queue_manager *dqm,
-					enum KFD_MQD_TYPE type);
-
 	int	(*register_process)(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 

commit 972fcdb52fe865a2f639e3200b97e648f34a0f41
Author: Oak Zeng <ozeng@amd.com>
Date:   Mon Dec 3 13:56:14 2018 -0600

    drm/amdkfd: Introduce asic-specific mqd_manager_init function
    
    Global function mqd_manager_init just calls asic-specific functions and it
    is not necessary. Delete it and introduce a mqd_manager_init interface in
    dqm for asic-specific mqd manager init. Call mqd_manager_init interface
    directly to initialize mqd manager
    
    Signed-off-by: Oak Zeng <ozeng@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 2770f3ece89f..a5d83ec1c6a8 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -158,6 +158,8 @@ struct device_queue_manager_asic_ops {
 	void	(*init_sdma_vm)(struct device_queue_manager *dqm,
 				struct queue *q,
 				struct qcm_process_device *qpd);
+	struct mqd_manager *	(*mqd_manager_init)(enum KFD_MQD_TYPE type,
+				 struct kfd_dev *dev);
 };
 
 /**

commit cb77ee7cae96c4db5d7dfe127d3e0cf9d6056875
Author: Oak Zeng <Oak.Zeng@amd.com>
Date:   Thu Nov 1 11:06:25 2018 -0400

    drm/amdkfd: Use 64 bit sdma_bitmap
    
    Maximumly support 64 sdma queues
    
    Signed-off-by: Oak Zeng <Oak.Zeng@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 70e38a2e23b9..2770f3ece89f 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -188,7 +188,7 @@ struct device_queue_manager {
 	unsigned int		total_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;
-	unsigned int		sdma_bitmap;
+	uint64_t		sdma_bitmap;
 	unsigned int		vmid_bitmap;
 	uint64_t		pipelines_addr;
 	struct kfd_mem_obj	*pipeline_mem;

commit d50941892ed9d18792271b2b06c8842a7a14b54d
Author: Shaoyun Liu <Shaoyun.Liu@amd.com>
Date:   Fri Feb 9 16:29:14 2018 -0500

    drm/amdkfd: Make the number of SDMA queues variable
    
    Vega20 supports 8 SDMA queues per engine
    
    Signed-off-by: Shaoyun Liu <Shaoyun.Liu@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index e7bd19d09845..70e38a2e23b9 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -33,7 +33,6 @@
 
 #define KFD_UNMAP_LATENCY_MS			(4000)
 #define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
-#define KFD_SDMA_QUEUES_PER_ENGINE		(2)
 
 struct device_process_node {
 	struct qcm_process_device *qpd;

commit 5df099e8bc83f4f3af8711ee0b9b8faef359ffff
Author: Jay Cornwall <Jay.Cornwall@amd.com>
Date:   Tue May 2 17:39:37 2017 -0500

    drm/amdkfd: Add wavefront context save state retrieval ioctl
    
    Wavefront context save data is of interest to userspace clients for
    debugging static wavefront state. The MQD contains two parameters
    required to parse the control stack and the control stack itself
    is kept in the MQD from gfx9 onwards.
    
    Add an ioctl to fetch the context save area and control stack offsets
    and to copy the control stack to a userspace address if it is kept in
    the MQD.
    
    Signed-off-by: Jay Cornwall <Jay.Cornwall@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 00da3169a004..e7bd19d09845 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -82,6 +82,8 @@ struct device_process_node {
  *
  * @restore_process_queues: Restore all evicted queues queues of a process
  *
+ * @get_wave_state: Retrieves context save state and optionally copies the
+ * control stack, if kept in the MQD, to the given userspace address.
  */
 
 struct device_queue_manager_ops {
@@ -137,6 +139,12 @@ struct device_queue_manager_ops {
 				    struct qcm_process_device *qpd);
 	int (*restore_process_queues)(struct device_queue_manager *dqm,
 				      struct qcm_process_device *qpd);
+
+	int	(*get_wave_state)(struct device_queue_manager *dqm,
+				  struct queue *q,
+				  void __user *ctl_stack,
+				  u32 *ctl_stack_used_size,
+				  u32 *save_area_used_size);
 };
 
 struct device_queue_manager_asic_ops {

commit 98bb92222eef6ac022f352aa4224f4c94a119199
Author: Yong Zhao <yong.zhao@amd.com>
Date:   Fri Jul 13 16:17:44 2018 -0400

    drm/amdkfd: Make SDMA engine number an ASIC-dependent variable
    
    On Raven there is only one SDMA engine instead of previously assumed two,
    so we need to adapt our code to this new scenario.
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 52e708c4f9a5..00da3169a004 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -33,10 +33,7 @@
 
 #define KFD_UNMAP_LATENCY_MS			(4000)
 #define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
-
-#define CIK_SDMA_QUEUES				(4)
-#define CIK_SDMA_QUEUES_PER_ENGINE		(2)
-#define CIK_SDMA_ENGINE_NUM			(2)
+#define KFD_SDMA_QUEUES_PER_ENGINE		(2)
 
 struct device_process_node {
 	struct qcm_process_device *qpd;
@@ -214,6 +211,7 @@ void program_sh_mem_settings(struct device_queue_manager *dqm,
 unsigned int get_queues_num(struct device_queue_manager *dqm);
 unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);
+unsigned int get_num_sdma_queues(struct device_queue_manager *dqm);
 
 static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {

commit 8d5f355290880331e265d4c3f3b66c805969f18e
Author: Yong Zhao <yong.zhao@amd.com>
Date:   Wed Jul 11 22:33:07 2018 -0400

    drm/amdkfd: Replace mqd with mqd_mgr as the variable name for mqd_manager
    
    This will make reading code much easier.
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 70179a6826f4..52e708c4f9a5 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -172,7 +172,7 @@ struct device_queue_manager {
 	struct device_queue_manager_ops ops;
 	struct device_queue_manager_asic_ops asic_ops;
 
-	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
+	struct mqd_manager	*mqd_mgrs[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
 	struct kfd_dev		*dev;
 	struct mutex		lock_hidden; /* use dqm_lock/unlock(dqm) */

commit 73ea648d921ed5c58895c8592321456fe12b697f
Author: Shaoyun Liu <Shaoyun.Liu@amd.com>
Date:   Wed Jul 11 22:32:58 2018 -0400

    drm/amdkfd: Implement hang detection in KFD and call amdgpu
    
    The reset will be performed in a new hw_exception work thread to
    handle HWS hang without blocking the thread that detected the hang.
    
    Signed-off-by: Shaoyun Liu <Shaoyun.Liu@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 0a23ddac345e..70179a6826f4 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -193,6 +193,10 @@ struct device_queue_manager {
 	struct kfd_mem_obj	*fence_mem;
 	bool			active_runlist;
 	int			sched_policy;
+
+	/* hw exception  */
+	bool			is_hws_hang;
+	struct work_struct	hw_exception_work;
 };
 
 void device_queue_manager_init_cik(

commit efeaed4d98eb4dc9ce01e1dca6d3778d180b272c
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Wed Jul 11 22:32:44 2018 -0400

    drm/amdkfd: Reliably prevent reclaim-FS while holding DQM lock
    
    This is needed to prevent deadlocks when MMU notifiers run in
    reclaim-FS context and take the DQM lock for userptr evictions.
    Previously this was done by making all memory allocations under
    DQM locks GFP_NOIO. This is error prone. Using
    memalloc_nofs_save/restore will reliably affect all memory
    allocations anywhere in the kernel while the DQM lock is held.
    
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 59a6b1956932..0a23ddac345e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -26,6 +26,8 @@
 
 #include <linux/rwsem.h>
 #include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/sched/mm.h>
 #include "kfd_priv.h"
 #include "kfd_mqd_manager.h"
 
@@ -173,8 +175,9 @@ struct device_queue_manager {
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
 	struct kfd_dev		*dev;
-	struct mutex		lock;
+	struct mutex		lock_hidden; /* use dqm_lock/unlock(dqm) */
 	struct list_head	queues;
+	unsigned int		saved_flags;
 	unsigned int		processes_count;
 	unsigned int		queue_count;
 	unsigned int		sdma_queue_count;
@@ -219,4 +222,19 @@ get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd)
 	return (pdd->lds_base >> 60) & 0x0E;
 }
 
+/* The DQM lock can be taken in MMU notifiers. Make sure no reclaim-FS
+ * happens while holding this lock anywhere to prevent deadlocks when
+ * an MMU notifier runs in reclaim-FS context.
+ */
+static inline void dqm_lock(struct device_queue_manager *dqm)
+{
+	mutex_lock(&dqm->lock_hidden);
+	dqm->saved_flags = memalloc_nofs_save();
+}
+static inline void dqm_unlock(struct device_queue_manager *dqm)
+{
+	memalloc_nofs_restore(dqm->saved_flags);
+	mutex_unlock(&dqm->lock_hidden);
+}
+
 #endif /* KFD_DEVICE_QUEUE_MANAGER_H_ */

commit bed4f110251b4f9041e5e797e035bc40c34d60ea
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Tue Apr 10 17:33:09 2018 -0400

    drm/amdkfd: Add GFXv9 device queue manager
    
    Signed-off-by: John Bridgman <john.bridgman@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 412beff3281d..59a6b1956932 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -200,6 +200,8 @@ void device_queue_manager_init_vi(
 		struct device_queue_manager_asic_ops *asic_ops);
 void device_queue_manager_init_vi_tonga(
 		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_v9(
+		struct device_queue_manager_asic_ops *asic_ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 unsigned int get_queues_num(struct device_queue_manager *dqm);

commit 26103436da003327017af325483b6150a3b855cc
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Tue Feb 6 20:32:45 2018 -0500

    drm/amdkfd: Implement KFD process eviction/restore
    
    When the TTM memory manager in KGD evicts BOs, all user mode queues
    potentially accessing these BOs must be evicted temporarily. Once
    user mode queues are evicted, the eviction fence is signaled,
    allowing the migration of the BO to proceed.
    
    A delayed worker is scheduled to restore all the BOs belonging to
    the evicted process and restart its queues.
    
    During suspend/resume of the GPU we also evict all processes to allow
    KGD to save BOs in system memory, since VRAM will be lost.
    
    v2:
    * Account for eviction when updating of q->is_active in MQD manager
    
    Signed-off-by: Harish Kasiviswanathan <Harish.Kasiviswanathan@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 68be0aaff3f4..412beff3281d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -79,6 +79,10 @@ struct device_process_node {
  *
  * @process_termination: Clears all process queues belongs to that device.
  *
+ * @evict_process_queues: Evict all active queues of a process
+ *
+ * @restore_process_queues: Restore all evicted queues queues of a process
+ *
  */
 
 struct device_queue_manager_ops {
@@ -129,6 +133,11 @@ struct device_queue_manager_ops {
 
 	int (*process_termination)(struct device_queue_manager *dqm,
 			struct qcm_process_device *qpd);
+
+	int (*evict_process_queues)(struct device_queue_manager *dqm,
+				    struct qcm_process_device *qpd);
+	int (*restore_process_queues)(struct device_queue_manager *dqm,
+				      struct qcm_process_device *qpd);
 };
 
 struct device_queue_manager_asic_ops {

commit 97672cbe3de809ef8c4ea66cce675f5da3d3df44
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Thu Jan 4 17:17:44 2018 -0500

    drm/amdkfd: Add dGPU support to the device queue manager
    
    GFXv7 and v8 dGPUs use a different addressing mode for KFD compared
    to APUs (GPUVM64 vs HSA64). And dGPUs don't support MTYPE_CC. They
    use MTYPE_UC instead for memory that requires coherency.
    
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 9fdc9c2a107a..68be0aaff3f4 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -185,8 +185,12 @@ struct device_queue_manager {
 
 void device_queue_manager_init_cik(
 		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_cik_hawaii(
+		struct device_queue_manager_asic_ops *asic_ops);
 void device_queue_manager_init_vi(
 		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_vi_tonga(
+		struct device_queue_manager_asic_ops *asic_ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 unsigned int get_queues_num(struct device_queue_manager *dqm);

commit d146c5a7196b4c2c2586569971a55392b501b93b
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Thu Jan 4 17:17:43 2018 -0500

    drm/amdkfd: Make sched_policy a per-device setting
    
    Some dGPUs don't support HWS. Allow them to use a per-device
    sched_policy that may be different from the global default.
    
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index c61b693bfa8c..9fdc9c2a107a 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -180,6 +180,7 @@ struct device_queue_manager {
 	unsigned int		*fence_addr;
 	struct kfd_mem_obj	*fence_mem;
 	bool			active_runlist;
+	int			sched_policy;
 };
 
 void device_queue_manager_init_cik(

commit b46cb7d70e77ef5639bf2dcbef27be6e21e07a61
Author: Yong Zhao <yong.zhao@amd.com>
Date:   Fri Nov 24 18:10:54 2017 -0500

    drm/amdkfd: Delete a useless parameter from create_queue function pointer
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 8752edf9cd9b..c61b693bfa8c 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -84,8 +84,7 @@ struct device_process_node {
 struct device_queue_manager_ops {
 	int	(*create_queue)(struct device_queue_manager *dqm,
 				struct queue *q,
-				struct qcm_process_device *qpd,
-				int *allocate_vmid);
+				struct qcm_process_device *qpd);
 
 	int	(*destroy_queue)(struct device_queue_manager *dqm,
 				struct qcm_process_device *qpd,

commit d7b9bd2248d794275b53d34e665f7c5a08c4b396
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Tue Nov 14 16:41:20 2017 -0500

    drm/amdkfd: Add support for user-mode trap handlers
    
    A second-level user mode trap handler can be installed. The CWSR trap
    handler jumps to the secondary trap handler conditionally for any
    conditions not handled by it. This can be used e.g. for debugging or
    catching math exceptions.
    
    When CWSR is disabled, the user mode trap handler is installed as
    first level trap handler.
    
    Signed-off-by: Shaoyun.liu <shaoyun.liu@amd.com>
    Signed-off-by: Jay Cornwall <Jay.Cornwall@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 5b77cb69f732..8752edf9cd9b 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -123,6 +123,11 @@ struct device_queue_manager_ops {
 					   void __user *alternate_aperture_base,
 					   uint64_t alternate_aperture_size);
 
+	int	(*set_trap_handler)(struct device_queue_manager *dqm,
+				    struct qcm_process_device *qpd,
+				    uint64_t tba_addr,
+				    uint64_t tma_addr);
+
 	int (*process_termination)(struct device_queue_manager *dqm,
 			struct qcm_process_device *qpd);
 };

commit bfd5e378a98d0387b45a4864528a11b65d038f0c
Author: Yong Zhao <yong.zhao@amd.com>
Date:   Wed Nov 1 19:21:31 2017 -0400

    drm/amdkfd: Cleanup DQM ASIC-specific ops
    
    Remove empty initialize function.
    
    Rename register_process to update_qpd to avoid confusion with the
    non-ASIC-specific register_process.
    
    Shorten ops_asic_specific to asic_ops.
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 31c2b1f9d320..5b77cb69f732 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -128,9 +128,8 @@ struct device_queue_manager_ops {
 };
 
 struct device_queue_manager_asic_ops {
-	int	(*register_process)(struct device_queue_manager *dqm,
+	int	(*update_qpd)(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-	int	(*initialize)(struct device_queue_manager *dqm);
 	bool	(*set_cache_memory_policy)(struct device_queue_manager *dqm,
 					   struct qcm_process_device *qpd,
 					   enum cache_policy default_policy,
@@ -156,7 +155,7 @@ struct device_queue_manager_asic_ops {
 
 struct device_queue_manager {
 	struct device_queue_manager_ops ops;
-	struct device_queue_manager_asic_ops ops_asic_specific;
+	struct device_queue_manager_asic_ops asic_ops;
 
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
@@ -179,8 +178,10 @@ struct device_queue_manager {
 	bool			active_runlist;
 };
 
-void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops);
-void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops);
+void device_queue_manager_init_cik(
+		struct device_queue_manager_asic_ops *asic_ops);
+void device_queue_manager_init_vi(
+		struct device_queue_manager_asic_ops *asic_ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 unsigned int get_queues_num(struct device_queue_manager *dqm);

commit 9fd3f1bfae6c6c75f0c8aedb5d499d74cdb52eb9
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Wed Sep 27 00:09:52 2017 -0400

    drm/amdkfd: Improve process termination handling
    
    Separate device queue termination from process queue manager
    termination. Unmap all queues at once instead of one at a time.
    Unmap device queues before the PASID is unbound, in the
    kfd_process_iommu_unbind_callback.
    
    When resetting wavefronts in non-HWS mode, do it before the VMID is
    released.
    
    Signed-off-by: Ben Goz <ben.goz@amd.com>
    Signed-off-by: shaoyun liu <shaoyun.liu@amd.com>
    Signed-off-by: Amber Lin <Amber.Lin@amd.com>
    Signed-off-by: Yong Zhao <Yong.Zhao@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 60d46ce0b8a7..31c2b1f9d320 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -77,6 +77,8 @@ struct device_process_node {
  * @set_cache_memory_policy: Sets memory policy (cached/ non cached) for the
  * memory apertures.
  *
+ * @process_termination: Clears all process queues belongs to that device.
+ *
  */
 
 struct device_queue_manager_ops {
@@ -120,6 +122,9 @@ struct device_queue_manager_ops {
 					   enum cache_policy alternate_policy,
 					   void __user *alternate_aperture_base,
 					   uint64_t alternate_aperture_size);
+
+	int (*process_termination)(struct device_queue_manager *dqm,
+			struct qcm_process_device *qpd);
 };
 
 struct device_queue_manager_asic_ops {

commit 44008d7a871ce5a487cbcc4c412a5149145ea442
Author: Yong Zhao <yong.zhao@amd.com>
Date:   Wed Sep 20 18:10:18 2017 -0400

    drm/amdkfd: Use VMID bitmap from KGD v2
    
    The hard-coded values related to VMID were removed in KFD, as those
    values can be calculated in the KFD initialization function.
    
    v2: remove unnecessary local variable
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 99e23053f59d..60d46ce0b8a7 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -32,10 +32,6 @@
 #define KFD_UNMAP_LATENCY_MS			(4000)
 #define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
 
-#define CIK_VMID_NUM				(8)
-#define KFD_VMID_START_OFFSET			(8)
-#define VMID_PER_DEVICE				CIK_VMID_NUM
-#define KFD_DQM_FIRST_PIPE			(0)
 #define CIK_SDMA_QUEUES				(4)
 #define CIK_SDMA_QUEUES_PER_ENGINE		(2)
 #define CIK_SDMA_ENGINE_NUM			(2)

commit b90e3fbecc9030efb8a6aed1d54a79d0c3d0820a
Author: Felix Kuehling <Felix.Kuehling@amd.com>
Date:   Wed Sep 20 18:10:16 2017 -0400

    drm/amdkfd: Adjust dequeue latencies and timeouts
    
    Adjust latencies and timeouts for dequeueing with HWS and consolidate
    them in one place. Make them longer to allow long running waves to
    complete without causing a timeout. The timeout is twice as long as the
    latency plus some buffer to make sure we don't detect a timeout
    prematurely.
    
    Change timeouts for dequeueing HQDs without HWS. KFD_UNMAP_LATENCY is
    more consistent with what the HWS does for user queues.
    
    Signed-off-by: Yong Zhao <yong.zhao@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index faf820a06400..99e23053f59d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -29,7 +29,9 @@
 #include "kfd_priv.h"
 #include "kfd_mqd_manager.h"
 
-#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS	(500)
+#define KFD_UNMAP_LATENCY_MS			(4000)
+#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS (2 * KFD_UNMAP_LATENCY_MS + 1000)
+
 #define CIK_VMID_NUM				(8)
 #define KFD_VMID_START_OFFSET			(8)
 #define VMID_PER_DEVICE				CIK_VMID_NUM

commit 13c4a2c78e7ed932cff019f54cdd4f6976dad140
Author: Jay Cornwall <Jay.Cornwall@amd.com>
Date:   Thu Jul 13 20:21:54 2017 -0500

    drm/amdkfd: Remove unused references to shared_resources.num_mec
    
    Dead code.
    
    Change-Id: Ic0bb1bcca87e96bc5e8fa9894727b0de152e8818
    Signed-off-by: Jay Cornwall <Jay.Cornwall@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 66b9615bc3c1..faf820a06400 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -180,7 +180,6 @@ void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops);
 void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-unsigned int get_mec_num(struct device_queue_manager *dqm);
 unsigned int get_queues_num(struct device_queue_manager *dqm);
 unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);

commit d0b63bb3385c5683c7531044425f4507ca5251b2
Author: Andres Rodriguez <andresx7@gmail.com>
Date:   Fri Feb 3 16:28:48 2017 -0500

    drm/amdkfd: allow split HQD on per-queue granularity v5
    
    Update the KGD to KFD interface to allow sharing pipes with queue
    granularity instead of pipe granularity.
    
    This allows for more interesting pipe/queue splits.
    
    v2: fix overflow check for res.queue_mask
    v3: fix shift overflow when setting res.queue_mask
    v4: fix comment in is_pipeline_enabled()
    v5: clamp res.queue_mask to the first MEC only
    
    Reviewed-by: Edward O'Callaghan <funfunctor@folklore1984.net>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Christian König <christian.koenig@amd.com>
    Signed-off-by: Andres Rodriguez <andresx7@gmail.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index a625b9137da2..66b9615bc3c1 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -30,8 +30,6 @@
 #include "kfd_mqd_manager.h"
 
 #define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS	(500)
-#define QUEUES_PER_PIPE				(8)
-#define PIPE_PER_ME_CP_SCHEDULING		(3)
 #define CIK_VMID_NUM				(8)
 #define KFD_VMID_START_OFFSET			(8)
 #define VMID_PER_DEVICE				CIK_VMID_NUM
@@ -182,10 +180,10 @@ void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops);
 void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-int init_pipelines(struct device_queue_manager *dqm,
-		unsigned int pipes_num, unsigned int first_pipe);
-unsigned int get_first_pipe(struct device_queue_manager *dqm);
-unsigned int get_pipes_num(struct device_queue_manager *dqm);
+unsigned int get_mec_num(struct device_queue_manager *dqm);
+unsigned int get_queues_num(struct device_queue_manager *dqm);
+unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
+unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);
 
 static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {

commit a104299b94bdb562c2ce9a4445bac24c147c25cd
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Tue Jun 21 11:10:26 2016 +0200

    drm/amdkfd: Clean up inline handling
    
    - inline functions need to be static inline, otherwise gcc can opt to
      not inline and the linker gets unhappy.
    - no forward decls for inline functions, just include the right headers.
    
    Cc: Oded Gabbay <oded.gabbay@gmail.com>
    Cc: Ben Goz <ben.goz@amd.com>
    Reviewed-by: Oded Gabbay <oded.gabbay@gmail.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/1466500235-21282-2-git-send-email-daniel.vetter@ffwll.ch

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index ec4036a09f3e..a625b9137da2 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -187,12 +187,12 @@ int init_pipelines(struct device_queue_manager *dqm,
 unsigned int get_first_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_num(struct device_queue_manager *dqm);
 
-extern inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
+static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {
 	return (pdd->lds_base >> 16) & 0xFF;
 }
 
-extern inline unsigned int
+static inline unsigned int
 get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd)
 {
 	return (pdd->lds_base >> 60) & 0x0E;

commit 992839ad64f21ff4e5ed0a71691098ab7cfcb9dc
Author: Yair Shachar <yair.shachar@amd.com>
Date:   Wed May 20 13:43:04 2015 +0300

    drm/amdkfd: Add static user-mode queues support
    
    This patch adds support for static user-mode queues in QCM.
    Queues which are designated as static can NOT be preempted by
    the CP microcode when it is executing its scheduling algorithm.
    
    This is needed for supporting the debugger feature, because we
    can't allow the CP to preempt queues which are currently being debugged.
    
    The number of queues that can be designated as static is limited by the
    number of HQDs (Hardware Queue Descriptors).
    
    Signed-off-by: Yair Shachar <yair.shachar@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 57278e2d72e0..ec4036a09f3e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -88,9 +88,11 @@ struct device_queue_manager_ops {
 				struct queue *q,
 				struct qcm_process_device *qpd,
 				int *allocate_vmid);
+
 	int	(*destroy_queue)(struct device_queue_manager *dqm,
 				struct qcm_process_device *qpd,
 				struct queue *q);
+
 	int	(*update_queue)(struct device_queue_manager *dqm,
 				struct queue *q);
 
@@ -100,8 +102,10 @@ struct device_queue_manager_ops {
 
 	int	(*register_process)(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
+
 	int	(*unregister_process)(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
+
 	int	(*initialize)(struct device_queue_manager *dqm);
 	int	(*start)(struct device_queue_manager *dqm);
 	int	(*stop)(struct device_queue_manager *dqm);
@@ -109,9 +113,11 @@ struct device_queue_manager_ops {
 	int	(*create_kernel_queue)(struct device_queue_manager *dqm,
 					struct kernel_queue *kq,
 					struct qcm_process_device *qpd);
+
 	void	(*destroy_kernel_queue)(struct device_queue_manager *dqm,
 					struct kernel_queue *kq,
 					struct qcm_process_device *qpd);
+
 	bool	(*set_cache_memory_policy)(struct device_queue_manager *dqm,
 					   struct qcm_process_device *qpd,
 					   enum cache_policy default_policy,

commit 3e3f6e1a90a890e0cea4ec6d6f98e1fa94255de8
Author: Oded Gabbay <oded.gabbay@gmail.com>
Date:   Tue May 5 11:51:39 2015 +0300

    drm/amdkfd: make the sdma vm init to be asic specific
    
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 650ae1c9ceca..57278e2d72e0 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -130,6 +130,9 @@ struct device_queue_manager_asic_ops {
 					   enum cache_policy alternate_policy,
 					   void __user *alternate_aperture_base,
 					   uint64_t alternate_aperture_size);
+	void	(*init_sdma_vm)(struct device_queue_manager *dqm,
+				struct queue *q,
+				struct qcm_process_device *qpd);
 };
 
 /**

commit d42af779fb2cf23bbe218c91b44f4979f18bc910
Author: Oded Gabbay <oded.gabbay@gmail.com>
Date:   Tue May 5 11:51:31 2015 +0300

    drm/amdkfd: Use new struct for asic specific ops
    
    This patch creates a new structure for asic specific operations, instead
    of using the existing structure of operations.
    
    This is done to make the code flow more logic, readable and maintainable.
    
    The change is done only to the device queue manager module at this point.
    
    Signed-off-by: Oded Gabbay <oded.gabbay@gmail.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 488f51d19427..650ae1c9ceca 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -120,6 +120,18 @@ struct device_queue_manager_ops {
 					   uint64_t alternate_aperture_size);
 };
 
+struct device_queue_manager_asic_ops {
+	int	(*register_process)(struct device_queue_manager *dqm,
+					struct qcm_process_device *qpd);
+	int	(*initialize)(struct device_queue_manager *dqm);
+	bool	(*set_cache_memory_policy)(struct device_queue_manager *dqm,
+					   struct qcm_process_device *qpd,
+					   enum cache_policy default_policy,
+					   enum cache_policy alternate_policy,
+					   void __user *alternate_aperture_base,
+					   uint64_t alternate_aperture_size);
+};
+
 /**
  * struct device_queue_manager
  *
@@ -134,7 +146,7 @@ struct device_queue_manager_ops {
 
 struct device_queue_manager {
 	struct device_queue_manager_ops ops;
-	struct device_queue_manager_ops ops_asic_specific;
+	struct device_queue_manager_asic_ops ops_asic_specific;
 
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
@@ -157,8 +169,8 @@ struct device_queue_manager {
 	bool			active_runlist;
 };
 
-void device_queue_manager_init_cik(struct device_queue_manager_ops *ops);
-void device_queue_manager_init_vi(struct device_queue_manager_ops *ops);
+void device_queue_manager_init_cik(struct device_queue_manager_asic_ops *ops);
+void device_queue_manager_init_vi(struct device_queue_manager_asic_ops *ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 int init_pipelines(struct device_queue_manager *dqm,

commit 64ea8f4af57cee9f8b0bf542819b41ee82acfcb9
Author: Oded Gabbay <oded.gabbay@amd.com>
Date:   Tue Feb 17 11:30:31 2015 +0200

    drm/amdkfd: don't set get_pipes_num() as inline
    
    get_pipes_num() calls BUG_ON so we can't set it as inline because it produces a
    warning as BUG_ON() uses static variables when it is expanded.
    
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 386bd7df7c30..488f51d19427 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -164,6 +164,7 @@ void program_sh_mem_settings(struct device_queue_manager *dqm,
 int init_pipelines(struct device_queue_manager *dqm,
 		unsigned int pipes_num, unsigned int first_pipe);
 unsigned int get_first_pipe(struct device_queue_manager *dqm);
+unsigned int get_pipes_num(struct device_queue_manager *dqm);
 
 extern inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {
@@ -176,10 +177,4 @@ get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd)
 	return (pdd->lds_base >> 60) & 0x0E;
 }
 
-extern inline unsigned int get_pipes_num(struct device_queue_manager *dqm)
-{
-	BUG_ON(!dqm || !dqm->dev);
-	return dqm->dev->shared_resources.compute_pipe_count;
-}
-
 #endif /* KFD_DEVICE_QUEUE_MANAGER_H_ */

commit 1365aa6266fad0669487240af3f098593796172c
Author: Oded Gabbay <oded.gabbay@amd.com>
Date:   Tue Feb 17 11:58:27 2015 +0200

    drm/amdkfd: Initialize only amdkfd's assigned pipelines
    
    This patch fixes a bug in the initialization of the pipelines. The
    init_pipelines() function was called with a constant value of 0 in the
    first_pipe argument. This is an error because amdkfd doesn't handle pipe 0.
    
    The correct way is to pass the value that get_first_pipe() returns as the
    argument for first_pipe.
    
    This bug appeared in 3.19 (first version with amdkfd) and it causes around 15%
    drop in CPU performance of Kaveri (A10-7850).
    
    v2: Don't set get_first_pipe() as inline because it calls BUG_ON()
    
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Cc: stable@vger.kernel.org
    Tested-by: Michel Dänzer <michel.daenzer@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index d64f86cda34f..386bd7df7c30 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -163,6 +163,7 @@ void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
 int init_pipelines(struct device_queue_manager *dqm,
 		unsigned int pipes_num, unsigned int first_pipe);
+unsigned int get_first_pipe(struct device_queue_manager *dqm);
 
 extern inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {

commit b3869b17fd63bacb53ac4db4ff4ba093701e17be
Merge: 7b83741bf76c c59c961ca511
Author: Dave Airlie <airlied@gmail.com>
Date:   Thu Jan 29 11:45:31 2015 +1000

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux into drm-next
    
    This backmerges drm-fixes into drm-next mainly for the amdkfd
    stuff, I'm not 100% confident, but it builds and the amdkfd
    folks can fix anything up.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    Conflicts:
            drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
            drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h

commit 0b3674ae1c8b9539dde694a70391e974aedde8c2
Author: Oded Gabbay <oded.gabbay@amd.com>
Date:   Thu Jan 22 13:42:28 2015 +0200

    drm/amdkfd: Fix sparse errors
    
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 19347956eeb9..e7b17b28330e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -160,10 +160,24 @@ void device_queue_manager_init_cik(struct device_queue_manager_ops *ops);
 void device_queue_manager_init_vi(struct device_queue_manager_ops *ops);
 void program_sh_mem_settings(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd);
-inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *qpd);
-inline unsigned int get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd);
 int init_pipelines(struct device_queue_manager *dqm,
 		unsigned int pipes_num, unsigned int first_pipe);
-inline unsigned int get_pipes_num(struct device_queue_manager *dqm);
+
+extern inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
+{
+	return (pdd->lds_base >> 16) & 0xFF;
+}
+
+extern inline unsigned int
+get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd)
+{
+	return (pdd->lds_base >> 60) & 0x0E;
+}
+
+extern inline unsigned int get_pipes_num(struct device_queue_manager *dqm)
+{
+	BUG_ON(!dqm || !dqm->dev);
+	return dqm->dev->shared_resources.compute_pipe_count;
+}
 
 #endif /* KFD_DEVICE_QUEUE_MANAGER_H_ */

commit b8cbab042cd69f3918a51620986514681c6cbad0
Author: Oded Gabbay <oded.gabbay@amd.com>
Date:   Sun Jan 18 13:18:01 2015 +0200

    drm/amdkfd: Allow user to limit only queues per device
    
    This patch replaces the two current amdkfd module parameters with a new one.
    
    The current parameters that are being replaced are:
    
    - Maximum number of HSA processes
    - Maximum number of queues per process
    
    The new parameter that replaces them is called "Maximum queues per device"
    
    This replacement achieves two goals:
    
    - Allows the user to have as many HSA processes as it wants (until
      a maximum of 512 HSA processes in Kaveri).
    
    - Removes the limitation the user had on maximum number of queues per HSA
      process. E.g. the user can now have processes which only have one queue and
      other processes which have hundreds of queues, while before the user
      couldn't have more than 128 queues per process (as default).
    
    The default value of the new parameter is 4096 (32 * 128, which were the
    defaults of the old parameters). There is almost no additional GART memory
    required for the default case. As a reminder, this amount of queues requires a
    little bit below 4MB of GART memory.
    
    v2:
    In addition, This patch defines a new counter for queues accounting in the DQM
    structure. This is done because the current counter only counts active queues
    which allows the user to create more queues than the
    max_num_of_queues_per_device module parameter allows.
    
    However, we need the current counter for the runlist packet build process, so
    the solution is to have a dedicated counter for this accounting.
    
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Reviewed-by: Ben Goz <ben.goz@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index c3f189e8ae35..52035bf0c1cb 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -130,6 +130,7 @@ struct device_queue_manager {
 	struct list_head	queues;
 	unsigned int		processes_count;
 	unsigned int		queue_count;
+	unsigned int		total_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;
 	unsigned int		vmid_bitmap;

commit a22fc85495575d81c36db24b12f66fd314b7ced1
Author: Ben Goz <ben.goz@amd.com>
Date:   Mon Jan 12 14:28:46 2015 +0200

    drm/amdkfd: Add initial VI support for DQM
    
    This patch starts to add support for the VI APU in the DQM module.
    
    Because most (more than 90%) of the DQM code is shared among AMD's APUs, we
    chose a design that performs most/all the code in the shared DQM file
    (kfd_device_queue_manager.c). If there is H/W specific code to be executed,
    than it is written in an asic-specific extension function for that H/W.
    
    That asic-specific extension function is called from the shared function at the
    appropriate time. This requires that for every asic-specific extension function
    that is implemented in a specific ASIC, there will be an equivalent
    implementation in ALL ASICs, even if those implementations are just stubs.
    
    That way we achieve:
    
    - Maintainability: by having one copy of most of the code, we only need to
      fix bugs at one locations
    
    - Readability: very clear what is the shared code and what is done per ASIC
    
    - Extensibility: very easy to add new H/W specific files/functions
    
    Signed-off-by: Ben Goz <ben.goz@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 72d2ca056e19..19347956eeb9 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -134,6 +134,7 @@ struct device_queue_manager_ops {
 
 struct device_queue_manager {
 	struct device_queue_manager_ops ops;
+	struct device_queue_manager_ops ops_asic_specific;
 
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;
@@ -155,6 +156,14 @@ struct device_queue_manager {
 	bool			active_runlist;
 };
 
-
+void device_queue_manager_init_cik(struct device_queue_manager_ops *ops);
+void device_queue_manager_init_vi(struct device_queue_manager_ops *ops);
+void program_sh_mem_settings(struct device_queue_manager *dqm,
+					struct qcm_process_device *qpd);
+inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *qpd);
+inline unsigned int get_sh_mem_bases_nybble_64(struct kfd_process_device *pdd);
+int init_pipelines(struct device_queue_manager *dqm,
+		unsigned int pipes_num, unsigned int first_pipe);
+inline unsigned int get_pipes_num(struct device_queue_manager *dqm);
 
 #endif /* KFD_DEVICE_QUEUE_MANAGER_H_ */

commit 45c9a5e4297b9a07d94ff8195ff6f21ba3581ad6
Author: Oded Gabbay <oded.gabbay@amd.com>
Date:   Mon Jan 12 14:26:10 2015 +0200

    drm/amdkfd: Encapsulate DQM functions in ops structure
    
    This patch does some re-org on the device_queue_manager structure. It takes out
    all the function pointers from the structure and puts them in a new structure,
    called device_queue_manager_ops. Then, it puts an instance of that structure
    inside device_queue_manager.
    
    This re-org is done to prepare the DQM module to support more than one AMD APU
    (Kaveri).
    
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 554c06ee8892..72d2ca056e19 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -46,7 +46,7 @@ struct device_process_node {
 };
 
 /**
- * struct device_queue_manager
+ * struct device_queue_manager_ops
  *
  * @create_queue: Queue creation routine.
  *
@@ -81,15 +81,9 @@ struct device_process_node {
  * @set_cache_memory_policy: Sets memory policy (cached/ non cached) for the
  * memory apertures.
  *
- * This struct is a base class for the kfd queues scheduler in the
- * device level. The device base class should expose the basic operations
- * for queue creation and queue destruction. This base class hides the
- * scheduling mode of the driver and the specific implementation of the
- * concrete device. This class is the only class in the queues scheduler
- * that configures the H/W.
  */
 
-struct device_queue_manager {
+struct device_queue_manager_ops {
 	int	(*create_queue)(struct device_queue_manager *dqm,
 				struct queue *q,
 				struct qcm_process_device *qpd,
@@ -124,7 +118,22 @@ struct device_queue_manager {
 					   enum cache_policy alternate_policy,
 					   void __user *alternate_aperture_base,
 					   uint64_t alternate_aperture_size);
+};
+
+/**
+ * struct device_queue_manager
+ *
+ * This struct is a base class for the kfd queues scheduler in the
+ * device level. The device base class should expose the basic operations
+ * for queue creation and queue destruction. This base class hides the
+ * scheduling mode of the driver and the specific implementation of the
+ * concrete device. This class is the only class in the queues scheduler
+ * that configures the H/W.
+ *
+ */
 
+struct device_queue_manager {
+	struct device_queue_manager_ops ops;
 
 	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
 	struct packet_manager	packets;

commit bcea308175748339b872cc50972e0a31c1999c64
Author: Ben Goz <ben.goz@amd.com>
Date:   Sat Jan 3 22:12:32 2015 +0200

    drm/amdkfd: Add SDMA user-mode queues support to QCM
    
    This patch adds support for SDMA user-mode queues to the QCM - the Queue
    management system that manages queues-per-device and queues-per-process.
    
    v2: Remove calls to interface function that initializes sdma engines.
    
    v3: Use the new names of some of the defines.
    
    Signed-off-by: Ben Goz <ben.goz@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index c3f189e8ae35..554c06ee8892 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -36,6 +36,9 @@
 #define KFD_VMID_START_OFFSET			(8)
 #define VMID_PER_DEVICE				CIK_VMID_NUM
 #define KFD_DQM_FIRST_PIPE			(0)
+#define CIK_SDMA_QUEUES				(4)
+#define CIK_SDMA_QUEUES_PER_ENGINE		(2)
+#define CIK_SDMA_ENGINE_NUM			(2)
 
 struct device_process_node {
 	struct qcm_process_device *qpd;
@@ -130,8 +133,10 @@ struct device_queue_manager {
 	struct list_head	queues;
 	unsigned int		processes_count;
 	unsigned int		queue_count;
+	unsigned int		sdma_queue_count;
 	unsigned int		next_pipe_to_allocate;
 	unsigned int		*allocated_queues;
+	unsigned int		sdma_bitmap;
 	unsigned int		vmid_bitmap;
 	uint64_t		pipelines_addr;
 	struct kfd_mem_obj	*pipeline_mem;

commit 64c7f8cf792776aaca036fb983006b6b21204934
Author: Ben Goz <ben.goz@amd.com>
Date:   Thu Jul 17 01:27:00 2014 +0300

    amdkfd: Add device queue manager module
    
    The queue scheduler divides into two sections, one section is process bounded
    and the other section is device bounded.
    The device bounded section is handled by this module.
    The DQM module handles queue setup, update and tear-down from the device side.
    It also supports suspend/resume operation.
    
    v3: Changed device_init, added the use of the new gart allocation functions an
    Added documentation.
    
    v4:
    
    Fixed a race in DQM queue scheduler where dqm->lock must be held when accessing
    dqm->queue_count and dqm->processes_count. This fixes runlist IB allocation
    failures when DQM is under load.
    
    Fixed race in DQM queue destruction where queues being destroyed must be
    removed from qpd->queues_list prior to preemption, or concurrent queue
    creation activity may reschedule them while their MQD is destroyed.
    
    Fixed EOP queue size setting in CP_HPD_EOP_CONTROL, because the size is
    specified as (log2(size_dwords)-1). The previous calculation assumed the
    size was specified in bytes, which caused interference between EOP queues
    when multiple MEC pipelines were active.
    
    v5:
    
    Move amdkfd from drm/radeon/ to drm/amd/
    Change format of mqd structure to match latest KV firmware
    Add support for AQL queues creation to enable working with open-source HSA
    runtime
    Remove unused unmap_queue function
    Various fixes (Style, typos)
    
    Signed-off-by: Ben Goz <ben.goz@amd.com>
    Signed-off-by: Jay Cornwall <jay.cornwall@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index e495b38a8cfd..c3f189e8ae35 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -42,6 +42,50 @@ struct device_process_node {
 	struct list_head list;
 };
 
+/**
+ * struct device_queue_manager
+ *
+ * @create_queue: Queue creation routine.
+ *
+ * @destroy_queue: Queue destruction routine.
+ *
+ * @update_queue: Queue update routine.
+ *
+ * @get_mqd_manager: Returns the mqd manager according to the mqd type.
+ *
+ * @exeute_queues: Dispatches the queues list to the H/W.
+ *
+ * @register_process: This routine associates a specific process with device.
+ *
+ * @unregister_process: destroys the associations between process to device.
+ *
+ * @initialize: Initializes the pipelines and memory module for that device.
+ *
+ * @start: Initializes the resources/modules the the device needs for queues
+ * execution. This function is called on device initialization and after the
+ * system woke up after suspension.
+ *
+ * @stop: This routine stops execution of all the active queue running on the
+ * H/W and basically this function called on system suspend.
+ *
+ * @uninitialize: Destroys all the device queue manager resources allocated in
+ * initialize routine.
+ *
+ * @create_kernel_queue: Creates kernel queue. Used for debug queue.
+ *
+ * @destroy_kernel_queue: Destroys kernel queue. Used for debug queue.
+ *
+ * @set_cache_memory_policy: Sets memory policy (cached/ non cached) for the
+ * memory apertures.
+ *
+ * This struct is a base class for the kfd queues scheduler in the
+ * device level. The device base class should expose the basic operations
+ * for queue creation and queue destruction. This base class hides the
+ * scheduling mode of the driver and the specific implementation of the
+ * concrete device. This class is the only class in the queues scheduler
+ * that configures the H/W.
+ */
+
 struct device_queue_manager {
 	int	(*create_queue)(struct device_queue_manager *dqm,
 				struct queue *q,
@@ -52,8 +96,9 @@ struct device_queue_manager {
 				struct queue *q);
 	int	(*update_queue)(struct device_queue_manager *dqm,
 				struct queue *q);
-	struct mqd_manager * (*get_mqd_manager)(
-					struct device_queue_manager *dqm,
+
+	struct mqd_manager * (*get_mqd_manager)
+					(struct device_queue_manager *dqm,
 					enum KFD_MQD_TYPE type);
 
 	int	(*register_process)(struct device_queue_manager *dqm,

commit ed6e6a3487bd736cbcfc74fe0f9d9220bae00c72
Author: Ben Goz <ben.goz@amd.com>
Date:   Thu Jul 17 00:45:35 2014 +0300

    amdkfd: Add kernel queue module
    
    The kernel queue module enables the amdkfd to establish kernel queues, not
    exposed to user space.
    
    The kernel queues are used for HIQ (HSA Interface Queue) and DIQ (Debug
    Interface Queue) operations
    
    v3: Removed use of internal typedefs and added use of the new gart allocation
    functions
    
    v4: Fixed a miscalculation in kernel queue wrapping
    
    v5:
    
    Move amdkfd from drm/radeon/ to drm/amd/
    Change format of mqd structure to match latest KV firmware
    Add support for AQL queues creation to enable working with open-source HSA
    runtime
    Add define for kernel queue size
    Various fixes
    
    Signed-off-by: Ben Goz <ben.goz@amd.com>
    Signed-off-by: Oded Gabbay <oded.gabbay@amd.com>

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
new file mode 100644
index 000000000000..e495b38a8cfd
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -0,0 +1,101 @@
+/*
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef KFD_DEVICE_QUEUE_MANAGER_H_
+#define KFD_DEVICE_QUEUE_MANAGER_H_
+
+#include <linux/rwsem.h>
+#include <linux/list.h>
+#include "kfd_priv.h"
+#include "kfd_mqd_manager.h"
+
+#define QUEUE_PREEMPT_DEFAULT_TIMEOUT_MS	(500)
+#define QUEUES_PER_PIPE				(8)
+#define PIPE_PER_ME_CP_SCHEDULING		(3)
+#define CIK_VMID_NUM				(8)
+#define KFD_VMID_START_OFFSET			(8)
+#define VMID_PER_DEVICE				CIK_VMID_NUM
+#define KFD_DQM_FIRST_PIPE			(0)
+
+struct device_process_node {
+	struct qcm_process_device *qpd;
+	struct list_head list;
+};
+
+struct device_queue_manager {
+	int	(*create_queue)(struct device_queue_manager *dqm,
+				struct queue *q,
+				struct qcm_process_device *qpd,
+				int *allocate_vmid);
+	int	(*destroy_queue)(struct device_queue_manager *dqm,
+				struct qcm_process_device *qpd,
+				struct queue *q);
+	int	(*update_queue)(struct device_queue_manager *dqm,
+				struct queue *q);
+	struct mqd_manager * (*get_mqd_manager)(
+					struct device_queue_manager *dqm,
+					enum KFD_MQD_TYPE type);
+
+	int	(*register_process)(struct device_queue_manager *dqm,
+					struct qcm_process_device *qpd);
+	int	(*unregister_process)(struct device_queue_manager *dqm,
+					struct qcm_process_device *qpd);
+	int	(*initialize)(struct device_queue_manager *dqm);
+	int	(*start)(struct device_queue_manager *dqm);
+	int	(*stop)(struct device_queue_manager *dqm);
+	void	(*uninitialize)(struct device_queue_manager *dqm);
+	int	(*create_kernel_queue)(struct device_queue_manager *dqm,
+					struct kernel_queue *kq,
+					struct qcm_process_device *qpd);
+	void	(*destroy_kernel_queue)(struct device_queue_manager *dqm,
+					struct kernel_queue *kq,
+					struct qcm_process_device *qpd);
+	bool	(*set_cache_memory_policy)(struct device_queue_manager *dqm,
+					   struct qcm_process_device *qpd,
+					   enum cache_policy default_policy,
+					   enum cache_policy alternate_policy,
+					   void __user *alternate_aperture_base,
+					   uint64_t alternate_aperture_size);
+
+
+	struct mqd_manager	*mqds[KFD_MQD_TYPE_MAX];
+	struct packet_manager	packets;
+	struct kfd_dev		*dev;
+	struct mutex		lock;
+	struct list_head	queues;
+	unsigned int		processes_count;
+	unsigned int		queue_count;
+	unsigned int		next_pipe_to_allocate;
+	unsigned int		*allocated_queues;
+	unsigned int		vmid_bitmap;
+	uint64_t		pipelines_addr;
+	struct kfd_mem_obj	*pipeline_mem;
+	uint64_t		fence_gpu_addr;
+	unsigned int		*fence_addr;
+	struct kfd_mem_obj	*fence_mem;
+	bool			active_runlist;
+};
+
+
+
+#endif /* KFD_DEVICE_QUEUE_MANAGER_H_ */
