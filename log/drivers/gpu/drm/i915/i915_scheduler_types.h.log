commit 18e4af04d2183e99e6808f55dcef30f66ac0b155
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 13 18:35:04 2020 +0100

    drm/i915: Drop no-semaphore boosting
    
    Now that we have fast timeslicing on semaphores, we no longer need to
    prioritise none-semaphore work as we will yield any work blocked on a
    semaphore to the next in the queue. Previously with no timeslicing,
    blocking on the semaphore caused extremely bad scheduling with multiple
    clients utilising multiple rings. Now, there is no impact and we can
    remove the complication.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200513173504.28322-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 6ab2c5289bed..f72e6c397b08 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -65,8 +65,7 @@ struct i915_sched_node {
 	struct list_head link;
 	struct i915_sched_attr attr;
 	unsigned int flags;
-#define I915_SCHED_HAS_SEMAPHORE_CHAIN	BIT(0)
-#define I915_SCHED_HAS_EXTERNAL_CHAIN	BIT(1)
+#define I915_SCHED_HAS_EXTERNAL_CHAIN	BIT(0)
 	intel_engine_mask_t semaphores;
 };
 

commit fcae496153a3ce719082746a01c81dd581e7a239
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri May 8 10:29:27 2020 +0100

    drm/i915: Prevent using semaphores to chain up to external fences
    
    The downside of using semaphores is that we lose metadata passing
    along the signaling chain. This is particularly nasty when we
    need to pass along a fatal error such as EFAULT or EDEADLK. For
    fatal errors we want to scrub the request before it is executed,
    which means that we cannot preload the request onto HW and have
    it wait upon a semaphore.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200508092933.738-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 7186875088a0..6ab2c5289bed 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -66,6 +66,7 @@ struct i915_sched_node {
 	struct i915_sched_attr attr;
 	unsigned int flags;
 #define I915_SCHED_HAS_SEMAPHORE_CHAIN	BIT(0)
+#define I915_SCHED_HAS_EXTERNAL_CHAIN	BIT(1)
 	intel_engine_mask_t semaphores;
 };
 

commit 6b6cd2ebd8d071e55998e32b648bb8081f7f02bb
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu May 7 16:51:09 2020 +0100

    drm/i915: Mark concurrent submissions with a weak-dependency
    
    We recorded the dependencies for WAIT_FOR_SUBMIT in order that we could
    correctly perform priority inheritance from the parallel branches to the
    common trunk. However, for the purpose of timeslicing and reset
    handling, the dependency is weak -- as we the pair of requests are
    allowed to run in parallel and not in strict succession.
    
    The real significance though is that this allows us to rearrange
    groups of WAIT_FOR_SUBMIT linked requests along the single engine, and
    so can resolve user level inter-batch scheduling dependencies from user
    semaphores.
    
    Fixes: c81471f5e95c ("drm/i915: Copy across scheduler behaviour flags across submit fences")
    Testcase: igt/gem_exec_fence/submit
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: <stable@vger.kernel.org> # v5.6+
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200507155109.8892-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index d18e70550054..7186875088a0 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -78,6 +78,7 @@ struct i915_dependency {
 	unsigned long flags;
 #define I915_DEPENDENCY_ALLOC		BIT(0)
 #define I915_DEPENDENCY_EXTERNAL	BIT(1)
+#define I915_DEPENDENCY_WEAK		BIT(2)
 };
 
 #endif /* _I915_SCHEDULER_TYPES_H_ */

commit 9c4a14f8cceeffa01334e3229888746aa7dd95fe
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 10 08:14:25 2019 +0100

    drm/i915: Note the addition of timeslicing to the pretend scheduler
    
    Since writing the comment that the scheduler is entirely passive, we've
    added minimal timeslicing which adds the most primitive of active
    elements (a timeout and reschedule).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Cc: Ramalingam C <ramalingam.c@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191010071434.31195-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index aad81acba9dc..d18e70550054 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -49,6 +49,15 @@ struct i915_sched_attr {
  * DAG of each request, we are able to insert it into a sorted queue when it
  * is ready, and are able to reorder its portion of the graph to accommodate
  * dynamic priority changes.
+ *
+ * Ok, there is now one active element to the "scheduler" in the backends.
+ * We let a new context run for a small amount of time before re-evaluating
+ * the run order. As we re-evaluate, we maintain the strict ordering of
+ * dependencies, but attempt to rotate the active contexts (the current context
+ * is put to the back of its priority queue, then reshuffling its dependents).
+ * This provides minimal timeslicing and prevents a userspace hog (e.g.
+ * something waiting on a user semaphore [VkEvent]) from denying service to
+ * others.
  */
 struct i915_sched_node {
 	struct list_head signalers_list; /* those before us, we depend upon */

commit 8ee36e048c98d4015804a23f884be2576f778a93
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jun 20 15:20:52 2019 +0100

    drm/i915/execlists: Minimalistic timeslicing
    
    If we have multiple contexts of equal priority pending execution,
    activate a timer to demote the currently executing context in favour of
    the next in the queue when that timeslice expires. This enforces
    fairness between contexts (so long as they allow preemption -- forced
    preemption, in the future, will kick those who do not obey) and allows
    us to avoid userspace blocking forward progress with e.g. unbounded
    MI_SEMAPHORE_WAIT.
    
    For the starting point here, we use the jiffie as our timeslice so that
    we should be reasonably efficient wrt frequent CPU wakeups.
    
    Testcase: igt/gem_exec_scheduler/semaphore-resolve
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190620142052.19311-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 3e309631bd0b..aad81acba9dc 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -62,6 +62,7 @@ struct i915_sched_node {
 
 struct i915_dependency {
 	struct i915_sched_node *signaler;
+	struct i915_sched_node *waiter;
 	struct list_head signal_link;
 	struct list_head wait_link;
 	struct list_head dfs_link;

commit 6e7eb7a80769e7250e31652b96918cf7f3e0d285
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 15 14:00:50 2019 +0100

    drm/i915: Bump signaler priority on adding a waiter
    
    The handling of the no-preemption priority level imposes the restriction
    that we need to maintain the implied ordering even though preemption is
    disabled. Otherwise we may end up with an AB-BA deadlock across multiple
    engine due to a real preemption event reordering the no-preemption
    WAITs. To resolve this issue we currently promote all requests to WAIT
    on unsubmission, however this interferes with the timeslicing
    requirement that we do not apply any implicit promotion that will defeat
    the round-robin timeslice list. (If we automatically promote the active
    request it will go back to the head of the queue and not the tail!)
    
    So we need implicit promotion to prevent reordering around semaphores
    where we are not allowed to preempt, and we must avoid implicit
    promotion on unsubmission. So instead of at unsubmit, if we apply that
    implicit promotion on adding the dependency, we avoid the semaphore
    deadlock and we also reduce the gains made by the promotion for user
    space waiting. Furthermore, by keeping the earlier dependencies at a
    higher level, we reduce the search space for timeslicing without
    altering runtime scheduling too badly (no dependencies at all will be
    assigned a higher priority for rrul).
    
    v2: Limit the bump to external edges (as originally intended) i.e.
    between contexts and out to the user.
    
    Testcase: igt/gem_concurrent_blit
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190515130052.4475-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 166a457884b2..3e309631bd0b 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -66,7 +66,8 @@ struct i915_dependency {
 	struct list_head wait_link;
 	struct list_head dfs_link;
 	unsigned long flags;
-#define I915_DEPENDENCY_ALLOC BIT(0)
+#define I915_DEPENDENCY_ALLOC		BIT(0)
+#define I915_DEPENDENCY_EXTERNAL	BIT(1)
 };
 
 #endif /* _I915_SCHEDULER_TYPES_H_ */

commit 112ed2d31a46f4704085ad925435b77e62b8abee
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 24 18:48:39 2019 +0100

    drm/i915: Move GraphicsTechnology files under gt/
    
    Start partitioning off the code that talks to the hardware (GT) from the
    uapi layers and move the device facing code under gt/
    
    One casualty is s/intel_ringbuffer.h/intel_engine.h/ with the plan to
    subdivide that header and body further (and split out the submission
    code from the ringbuffer and logical context handling). This patch aims
    to be simple motion so git can fixup inflight patches with little mess.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Acked-by: Jani Nikula <jani.nikula@intel.com>
    Acked-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190424174839.7141-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index f1af3916a808..166a457884b2 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -9,8 +9,8 @@
 
 #include <linux/list.h>
 
+#include "gt/intel_engine_types.h"
 #include "i915_priolist_types.h"
-#include "intel_engine_types.h"
 
 struct drm_i915_private;
 struct i915_request;

commit 7881e6057586b0bdaaffef13d9f88c95a86ba484
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 1 17:26:41 2019 +0100

    drm/i915: Only emit one semaphore per request
    
    Ideally we only need one semaphore per ring to accommodate waiting on
    multiple engines in parallel. However, since we do not know which fences
    we will finally be waiting on, we emit a semaphore for every fence. It
    turns out to be quite easy to trick ourselves into exhausting our
    ringbuffer causing an error, just by feeding in a batch that depends on
    several thousand contexts.
    
    Since we never can be waiting on more than one semaphore in parallel
    (other than perhaps the desire to busywait on multiple engines), just
    pick the first fence for our semaphore. If we pick the wrong fence to
    busywait on, we just miss an opportunity to reduce latency.
    
    An adaption might be to use sched.flags as either a semaphore counter,
    or to track the first busywait on each engine, converting it back to a
    single use bit prior to closing the request.
    
    v2: Track first semaphore used per-engine (this caters for our basic
    igt that semaphores are working).
    
    Reported-by: Mika Kuoppala <mika.kuoppala@intel.com>
    Testcase: igt/gem_exec_fence/long-history
    Fixes: e88619646971 ("drm/i915: Use HW semaphores for inter-engine synchronisation on gen8+")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190401162641.10963-3-chris@chris-wilson.co.uk
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 76a004a17b2e..f1af3916a808 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -10,6 +10,7 @@
 #include <linux/list.h>
 
 #include "i915_priolist_types.h"
+#include "intel_engine_types.h"
 
 struct drm_i915_private;
 struct i915_request;
@@ -55,7 +56,8 @@ struct i915_sched_node {
 	struct list_head link;
 	struct i915_sched_attr attr;
 	unsigned int flags;
-#define I915_SCHED_HAS_SEMAPHORE	BIT(0)
+#define I915_SCHED_HAS_SEMAPHORE_CHAIN	BIT(0)
+	intel_engine_mask_t semaphores;
 };
 
 struct i915_dependency {

commit 8b74594aa45506ea6de9f827cfb2865718fa210e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 1 17:26:40 2019 +0100

    drm/i915: Split out i915_priolist_types into its own header
    
    For more intel_engine_mask_t detangling. This time so that we can use
    intel_engine_mask_t inside the scheduling structs.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190401162641.10963-2-chris@chris-wilson.co.uk
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
index 5c94b3eb5c81..76a004a17b2e 100644
--- a/drivers/gpu/drm/i915/i915_scheduler_types.h
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -8,34 +8,13 @@
 #define _I915_SCHEDULER_TYPES_H_
 
 #include <linux/list.h>
-#include <linux/rbtree.h>
 
-#include <uapi/drm/i915_drm.h>
+#include "i915_priolist_types.h"
 
 struct drm_i915_private;
 struct i915_request;
 struct intel_engine_cs;
 
-enum {
-	I915_PRIORITY_MIN = I915_CONTEXT_MIN_USER_PRIORITY - 1,
-	I915_PRIORITY_NORMAL = I915_CONTEXT_DEFAULT_PRIORITY,
-	I915_PRIORITY_MAX = I915_CONTEXT_MAX_USER_PRIORITY + 1,
-
-	I915_PRIORITY_INVALID = INT_MIN
-};
-
-#define I915_USER_PRIORITY_SHIFT 3
-#define I915_USER_PRIORITY(x) ((x) << I915_USER_PRIORITY_SHIFT)
-
-#define I915_PRIORITY_COUNT BIT(I915_USER_PRIORITY_SHIFT)
-#define I915_PRIORITY_MASK (I915_PRIORITY_COUNT - 1)
-
-#define I915_PRIORITY_WAIT		((u8)BIT(0))
-#define I915_PRIORITY_NEWCLIENT		((u8)BIT(1))
-#define I915_PRIORITY_NOSEMAPHORE	((u8)BIT(2))
-
-#define __NO_PREEMPTION (I915_PRIORITY_WAIT)
-
 struct i915_sched_attr {
 	/**
 	 * @priority: execution and service priority
@@ -88,11 +67,4 @@ struct i915_dependency {
 #define I915_DEPENDENCY_ALLOC BIT(0)
 };
 
-struct i915_priolist {
-	struct list_head requests[I915_PRIORITY_COUNT];
-	struct rb_node node;
-	unsigned long used;
-	int priority;
-};
-
 #endif /* _I915_SCHEDULER_TYPES_H_ */

commit 3a891a62679424e5625a551b9af9c33af6ea59b3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 1 17:26:39 2019 +0100

    drm/i915: Move intel_engine_mask_t around for use by i915_request_types.h
    
    We want to use intel_engine_mask_t inside i915_request.h, which means
    extracting it from the general header file mess and placing it inside a
    types.h. A knock on effect is that the compiler wants to warn about
    type-contraction of ALL_ENGINES into intel_engine_maskt_t, so prepare
    for the worst.
    
    v2: Use intel_engine_mask_t consistently
    v3: Move I915_NUM_ENGINES to its natural home at the end of the enum
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190401162641.10963-1-chris@chris-wilson.co.uk
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>

diff --git a/drivers/gpu/drm/i915/i915_scheduler_types.h b/drivers/gpu/drm/i915/i915_scheduler_types.h
new file mode 100644
index 000000000000..5c94b3eb5c81
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_scheduler_types.h
@@ -0,0 +1,98 @@
+/*
+ * SPDX-License-Identifier: MIT
+ *
+ * Copyright Â© 2018 Intel Corporation
+ */
+
+#ifndef _I915_SCHEDULER_TYPES_H_
+#define _I915_SCHEDULER_TYPES_H_
+
+#include <linux/list.h>
+#include <linux/rbtree.h>
+
+#include <uapi/drm/i915_drm.h>
+
+struct drm_i915_private;
+struct i915_request;
+struct intel_engine_cs;
+
+enum {
+	I915_PRIORITY_MIN = I915_CONTEXT_MIN_USER_PRIORITY - 1,
+	I915_PRIORITY_NORMAL = I915_CONTEXT_DEFAULT_PRIORITY,
+	I915_PRIORITY_MAX = I915_CONTEXT_MAX_USER_PRIORITY + 1,
+
+	I915_PRIORITY_INVALID = INT_MIN
+};
+
+#define I915_USER_PRIORITY_SHIFT 3
+#define I915_USER_PRIORITY(x) ((x) << I915_USER_PRIORITY_SHIFT)
+
+#define I915_PRIORITY_COUNT BIT(I915_USER_PRIORITY_SHIFT)
+#define I915_PRIORITY_MASK (I915_PRIORITY_COUNT - 1)
+
+#define I915_PRIORITY_WAIT		((u8)BIT(0))
+#define I915_PRIORITY_NEWCLIENT		((u8)BIT(1))
+#define I915_PRIORITY_NOSEMAPHORE	((u8)BIT(2))
+
+#define __NO_PREEMPTION (I915_PRIORITY_WAIT)
+
+struct i915_sched_attr {
+	/**
+	 * @priority: execution and service priority
+	 *
+	 * All clients are equal, but some are more equal than others!
+	 *
+	 * Requests from a context with a greater (more positive) value of
+	 * @priority will be executed before those with a lower @priority
+	 * value, forming a simple QoS.
+	 *
+	 * The &drm_i915_private.kernel_context is assigned the lowest priority.
+	 */
+	int priority;
+};
+
+/*
+ * "People assume that time is a strict progression of cause to effect, but
+ * actually, from a nonlinear, non-subjective viewpoint, it's more like a big
+ * ball of wibbly-wobbly, timey-wimey ... stuff." -The Doctor, 2015
+ *
+ * Requests exist in a complex web of interdependencies. Each request
+ * has to wait for some other request to complete before it is ready to be run
+ * (e.g. we have to wait until the pixels have been rendering into a texture
+ * before we can copy from it). We track the readiness of a request in terms
+ * of fences, but we also need to keep the dependency tree for the lifetime
+ * of the request (beyond the life of an individual fence). We use the tree
+ * at various points to reorder the requests whilst keeping the requests
+ * in order with respect to their various dependencies.
+ *
+ * There is no active component to the "scheduler". As we know the dependency
+ * DAG of each request, we are able to insert it into a sorted queue when it
+ * is ready, and are able to reorder its portion of the graph to accommodate
+ * dynamic priority changes.
+ */
+struct i915_sched_node {
+	struct list_head signalers_list; /* those before us, we depend upon */
+	struct list_head waiters_list; /* those after us, they depend upon us */
+	struct list_head link;
+	struct i915_sched_attr attr;
+	unsigned int flags;
+#define I915_SCHED_HAS_SEMAPHORE	BIT(0)
+};
+
+struct i915_dependency {
+	struct i915_sched_node *signaler;
+	struct list_head signal_link;
+	struct list_head wait_link;
+	struct list_head dfs_link;
+	unsigned long flags;
+#define I915_DEPENDENCY_ALLOC BIT(0)
+};
+
+struct i915_priolist {
+	struct list_head requests[I915_PRIORITY_COUNT];
+	struct rb_node node;
+	unsigned long used;
+	int priority;
+};
+
+#endif /* _I915_SCHEDULER_TYPES_H_ */
