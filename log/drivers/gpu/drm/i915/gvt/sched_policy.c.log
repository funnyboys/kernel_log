commit a61ac1e75105a077ec1efd6923ae3c619f862304
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Mar 6 10:08:10 2020 +0800

    drm/i915/gvt: Wean gvt off using dev_priv
    
    Teach gvt to use intel_gt directly as it currently assumes direct HW
    access.
    
    [Zhenyu: rebase, fix compiling]
    
    Cc: Ding Zhuocheng <zhuocheng.ding@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200304032307.2983-3-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 737096a82e14..036b74fe9298 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -39,7 +39,7 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
 
-	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
+	for_each_engine(engine, vgpu->gvt->gt, i) {
 		if (!list_empty(workload_q_head(vgpu, engine)))
 			return true;
 	}
@@ -152,8 +152,8 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	scheduler->need_reschedule = true;
 
 	/* still have uncompleted workload? */
-	for_each_engine(engine, gvt->dev_priv, i) {
-		if (scheduler->current_workload[i])
+	for_each_engine(engine, gvt->gt, i) {
+		if (scheduler->current_workload[engine->id])
 			return;
 	}
 
@@ -169,8 +169,8 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	scheduler->need_reschedule = false;
 
 	/* wake up workload dispatch thread */
-	for_each_engine(engine, gvt->dev_priv, i)
-		wake_up(&scheduler->waitq[i]);
+	for_each_engine(engine, gvt->gt, i)
+		wake_up(&scheduler->waitq[engine->id]);
 }
 
 static struct intel_vgpu *find_busy_vgpu(struct gvt_sched_data *sched_data)
@@ -445,7 +445,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 	struct intel_gvt_workload_scheduler *scheduler =
 		&vgpu->gvt->scheduler;
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct drm_i915_private *dev_priv = vgpu->gvt->gt->i915;
 	struct intel_engine_cs *engine;
 	enum intel_engine_id id;
 
@@ -468,7 +468,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 
 	intel_runtime_pm_get(&dev_priv->runtime_pm);
 	spin_lock_bh(&scheduler->mmio_context_lock);
-	for_each_engine(engine, &vgpu->gvt->dev_priv->gt, id) {
+	for_each_engine(engine, vgpu->gvt->gt, id) {
 		if (scheduler->engine_owner[engine->id] == vgpu) {
 			intel_gvt_switch_mmio(vgpu, NULL, engine);
 			scheduler->engine_owner[engine->id] = NULL;

commit 8fde41076f6df53db84cb13051efed6482986ce3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Mar 4 11:23:06 2020 +0800

    drm/i915/gvt: Wean gvt off dev_priv->engine[]
    
    Stop trying to escape out of the gvt layer to find the engine that we
    initially setup for use with gvt. Record the engines during initialisation
    and use them henceforth.
    
    add/remove: 1/4 grow/shrink: 22/28 up/down: 341/-1410 (-1069)
    
    [Zhenyu: rebase, fix nonpriv register check fault, fix gvt engine
    thread run failure.]
    
    Cc: Ding Zhuocheng <zhuocheng.ding@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200304032307.2983-2-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 2369d4a9af94..737096a82e14 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -40,7 +40,7 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 	struct intel_engine_cs *engine;
 
 	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
-		if (!list_empty(workload_q_head(vgpu, i)))
+		if (!list_empty(workload_q_head(vgpu, engine)))
 			return true;
 	}
 
@@ -444,9 +444,10 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 {
 	struct intel_gvt_workload_scheduler *scheduler =
 		&vgpu->gvt->scheduler;
-	int ring_id;
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id id;
 
 	if (!vgpu_data->active)
 		return;
@@ -467,10 +468,10 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 
 	intel_runtime_pm_get(&dev_priv->runtime_pm);
 	spin_lock_bh(&scheduler->mmio_context_lock);
-	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
-		if (scheduler->engine_owner[ring_id] == vgpu) {
-			intel_gvt_switch_mmio(vgpu, NULL, ring_id);
-			scheduler->engine_owner[ring_id] = NULL;
+	for_each_engine(engine, &vgpu->gvt->dev_priv->gt, id) {
+		if (scheduler->engine_owner[engine->id] == vgpu) {
+			intel_gvt_switch_mmio(vgpu, NULL, engine);
+			scheduler->engine_owner[engine->id] = NULL;
 		}
 	}
 	spin_unlock_bh(&scheduler->mmio_context_lock);

commit d858d5695f3897d55df68452066a90d7560cb845
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Thu Jun 13 16:21:54 2019 -0700

    drm/i915: update rpm_get/put to use the rpm structure
    
    The functions where internally already only using the structure, so we
    need to just flip the interface.
    
    v2: rebase
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Imre Deak <imre.deak@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190613232156.34940-7-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 1c763a27a412..2369d4a9af94 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -465,7 +465,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		scheduler->current_vgpu = NULL;
 	}
 
-	intel_runtime_pm_get(dev_priv);
+	intel_runtime_pm_get(&dev_priv->runtime_pm);
 	spin_lock_bh(&scheduler->mmio_context_lock);
 	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
 		if (scheduler->engine_owner[ring_id] == vgpu) {
@@ -474,6 +474,6 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		}
 	}
 	spin_unlock_bh(&scheduler->mmio_context_lock);
-	intel_runtime_pm_put_unchecked(dev_priv);
+	intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
 	mutex_unlock(&vgpu->gvt->sched_lock);
 }

commit ff00d85b4df97bf9cc7082b3d0dc2c317b946aa2
Merge: 0cdc1d07b461 2e679d48f38c
Author: Rodrigo Vivi <vivijim@rdvivi-cozumel.jf.intel.com>
Date:   Thu Jan 24 14:50:02 2019 -0800

    Merge tag 'gvt-next-2019-01-24' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2019-01-24
    
    - split kvmgt as seperate module (Zhenyu)
    - Coffeelake GVT support (Fred)
    - const treatment and change for kernel type (Jani)
    
    Signed-off-by: Rodrigo Vivi <vivijim@rdvivi-cozumel.jf.intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190124054048.GO7203@zhen-hp.sh.intel.com

commit 2e679d48f38c378650db403b4ba2248adf0691b2
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Mon Jan 21 11:51:41 2019 +0200

    drm/i915/gvt: switch to kernel types
    
    Mixed C99 and kernel types use is getting ugly. Prefer kernel types.
    
    sed -i 's/\buint\(8\|16\|32\|64\)_t\b/u\1/g'
    
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index c32e7d5e8629..951cfee85902 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -94,7 +94,7 @@ static void gvt_balance_timeslice(struct gvt_sched_data *sched_data)
 {
 	struct vgpu_sched_data *vgpu_data;
 	struct list_head *pos;
-	static uint64_t stage_check;
+	static u64 stage_check;
 	int stage = stage_check++ % GVT_TS_BALANCE_STAGE_NUM;
 
 	/* The timeslice accumulation reset at stage 0, which is

commit 16e4dd0342a804090fd0958bb271d3a6b57056ac
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jan 14 14:21:10 2019 +0000

    drm/i915: Markup paired operations on wakerefs
    
    The majority of runtime-pm operations are bounded and scoped within a
    function; these are easy to verify that the wakeref are handled
    correctly. We can employ the compiler to help us, and reduce the number
    of wakerefs tracked when debugging, by passing around cookies provided
    by the various rpm_get functions to their rpm_put counterpart. This
    makes the pairing explicit, and given the required wakeref cookie the
    compiler can verify that we pass an initialised value to the rpm_put
    (quite handy for double checking error paths).
    
    For regular builds, the compiler should be able to eliminate the unused
    local variables and the program growth should be minimal. Fwiw, it came
    out as a net improvement as gcc was able to refactor rpm_get and
    rpm_get_if_in_use together,
    
    v2: Just s/rpm_put/rpm_put_unchecked/ everywhere, leaving the manual
    mark up for smaller more targeted patches.
    v3: Mention the cookie in Returns
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Jani Nikula <jani.nikula@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190114142129.24398-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index c32e7d5e8629..f04b3b965bfc 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -474,6 +474,6 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		}
 	}
 	spin_unlock_bh(&scheduler->mmio_context_lock);
-	intel_runtime_pm_put(dev_priv);
+	intel_runtime_pm_put_unchecked(dev_priv);
 	mutex_unlock(&vgpu->gvt->sched_lock);
 }

commit 54ff01fd0d44b9681615f77c15fe9ea6dfadb501
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Aug 30 11:33:43 2018 +0800

    drm/i915/gvt: Give new born vGPU higher scheduling chance
    
    This trys to give new born vGPU with higher scheduling chance
    not only with adding to sched list head and also have higher
    priority for workload sched for 2 seconds after starting to
    schedule it. In order for fast GPU execution during VM boot,
    and ensure guest driver setup with required state given in time.
    
    This fixes recent failure seen on one VM with multiple linux VMs
    running on kernel with commit 2621cefaa42b3("drm/i915: Provide a timeout to i915_gem_wait_for_idle() on setup"),
    which had shorter setup timeout that caused context state init failed.
    
    v2: change to 2s for higher scheduling period
    
    Cc: Yuan Hang <hang.yuan@intel.com>
    Reviewed-by: Hang Yuan <hang.yuan@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 985fe81794dd..c32e7d5e8629 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -47,11 +47,15 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 	return false;
 }
 
+/* We give 2 seconds higher prio for vGPU during start */
+#define GVT_SCHED_VGPU_PRI_TIME  2
+
 struct vgpu_sched_data {
 	struct list_head lru_list;
 	struct intel_vgpu *vgpu;
 	bool active;
-
+	bool pri_sched;
+	ktime_t pri_time;
 	ktime_t sched_in_time;
 	ktime_t sched_time;
 	ktime_t left_ts;
@@ -183,6 +187,14 @@ static struct intel_vgpu *find_busy_vgpu(struct gvt_sched_data *sched_data)
 		if (!vgpu_has_pending_workload(vgpu_data->vgpu))
 			continue;
 
+		if (vgpu_data->pri_sched) {
+			if (ktime_before(ktime_get(), vgpu_data->pri_time)) {
+				vgpu = vgpu_data->vgpu;
+				break;
+			} else
+				vgpu_data->pri_sched = false;
+		}
+
 		/* Return the vGPU only if it has time slice left */
 		if (vgpu_data->left_ts > 0) {
 			vgpu = vgpu_data->vgpu;
@@ -202,6 +214,7 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct vgpu_sched_data *vgpu_data;
 	struct intel_vgpu *vgpu = NULL;
+
 	/* no active vgpu or has already had a target */
 	if (list_empty(&sched_data->lru_runq_head) || scheduler->next_vgpu)
 		goto out;
@@ -209,12 +222,13 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 	vgpu = find_busy_vgpu(sched_data);
 	if (vgpu) {
 		scheduler->next_vgpu = vgpu;
-
-		/* Move the last used vGPU to the tail of lru_list */
 		vgpu_data = vgpu->sched_data;
-		list_del_init(&vgpu_data->lru_list);
-		list_add_tail(&vgpu_data->lru_list,
-				&sched_data->lru_runq_head);
+		if (!vgpu_data->pri_sched) {
+			/* Move the last used vGPU to the tail of lru_list */
+			list_del_init(&vgpu_data->lru_list);
+			list_add_tail(&vgpu_data->lru_list,
+				      &sched_data->lru_runq_head);
+		}
 	} else {
 		scheduler->next_vgpu = gvt->idle_vgpu;
 	}
@@ -328,11 +342,17 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 {
 	struct gvt_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
+	ktime_t now;
 
 	if (!list_empty(&vgpu_data->lru_list))
 		return;
 
-	list_add_tail(&vgpu_data->lru_list, &sched_data->lru_runq_head);
+	now = ktime_get();
+	vgpu_data->pri_time = ktime_add(now,
+					ktime_set(GVT_SCHED_VGPU_PRI_TIME, 0));
+	vgpu_data->pri_sched = true;
+
+	list_add(&vgpu_data->lru_list, &sched_data->lru_runq_head);
 
 	if (!hrtimer_active(&sched_data->timer))
 		hrtimer_start(&sched_data->timer, ktime_add_ns(ktime_get(),

commit b2b599fb54f90ae395ddc51f0d49e4f28244a8f8
Author: Hang Yuan <hang.yuan@linux.intel.com>
Date:   Wed Aug 29 17:15:56 2018 +0800

    drm/i915/gvt: move intel_runtime_pm_get out of spin_lock in stop_schedule
    
    pm_runtime_get_sync in intel_runtime_pm_get might sleep if i915
    device is not active. When stop vgpu schedule, the device may be
    inactive. So need to move runtime_pm_get out of spin_lock/unlock.
    
    Fixes: b24881e0b0b6("drm/i915/gvt: Add runtime_pm_get/put into gvt_switch_mmio
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Hang Yuan <hang.yuan@linux.intel.com>
    Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 09d7bb72b4ff..985fe81794dd 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -426,6 +426,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		&vgpu->gvt->scheduler;
 	int ring_id;
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 
 	if (!vgpu_data->active)
 		return;
@@ -444,6 +445,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		scheduler->current_vgpu = NULL;
 	}
 
+	intel_runtime_pm_get(dev_priv);
 	spin_lock_bh(&scheduler->mmio_context_lock);
 	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
 		if (scheduler->engine_owner[ring_id] == vgpu) {
@@ -452,5 +454,6 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		}
 	}
 	spin_unlock_bh(&scheduler->mmio_context_lock);
+	intel_runtime_pm_put(dev_priv);
 	mutex_unlock(&vgpu->gvt->sched_lock);
 }

commit 9a512e23f173a3598709b74d6ccf9a6616403967
Author: Colin Xu <colin.xu@intel.com>
Date:   Sat May 19 12:28:55 2018 +0800

    drm/i915/gvt: Use sched_lock to protect gvt scheduler logic.
    
    The scheduler lock(gvt->sched_lock) is used to protect gvt
    scheduler logic, including the gvt scheduler structure(gvt->scheduler
    and per vgpu schedule data(vgpu->sched_data, vgpu->sched_ctl).
    
    v9:
      - Change commit author since the patches are improved a lot compared
        with original version.
        Original author: Pei Zhang <pei.zhang@intel.com>
      - Rebase to latest gvt-staging.
    v8:
      - Correct coding wqstyle.
      - Rebase to latest gvt-staging.
    v7:
      - Remove gtt_lock since already proteced by gvt_lock and vgpu_lock.
    v6:
      - Rebase to latest gvt-staging.
    v5:
      - Rebase to latest gvt-staging.
    v4:
      - Rebase to latest gvt-staging.
    v3: update to latest code base
    
    Signed-off-by: Pei Zhang <pei.zhang@intel.com>
    Signed-off-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index d053cbe1dc94..09d7bb72b4ff 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -228,7 +228,7 @@ void intel_gvt_schedule(struct intel_gvt *gvt)
 	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
 	ktime_t cur_time;
 
-	mutex_lock(&gvt->lock);
+	mutex_lock(&gvt->sched_lock);
 	cur_time = ktime_get();
 
 	if (test_and_clear_bit(INTEL_GVT_REQUEST_SCHED,
@@ -244,7 +244,7 @@ void intel_gvt_schedule(struct intel_gvt *gvt)
 	vgpu_update_timeslice(gvt->scheduler.current_vgpu, cur_time);
 	tbs_sched_func(sched_data);
 
-	mutex_unlock(&gvt->lock);
+	mutex_unlock(&gvt->sched_lock);
 }
 
 static enum hrtimer_restart tbs_timer_fn(struct hrtimer *timer_data)
@@ -359,39 +359,65 @@ static struct intel_gvt_sched_policy_ops tbs_schedule_ops = {
 
 int intel_gvt_init_sched_policy(struct intel_gvt *gvt)
 {
+	int ret;
+
+	mutex_lock(&gvt->sched_lock);
 	gvt->scheduler.sched_ops = &tbs_schedule_ops;
+	ret = gvt->scheduler.sched_ops->init(gvt);
+	mutex_unlock(&gvt->sched_lock);
 
-	return gvt->scheduler.sched_ops->init(gvt);
+	return ret;
 }
 
 void intel_gvt_clean_sched_policy(struct intel_gvt *gvt)
 {
+	mutex_lock(&gvt->sched_lock);
 	gvt->scheduler.sched_ops->clean(gvt);
+	mutex_unlock(&gvt->sched_lock);
 }
 
+/* for per-vgpu scheduler policy, there are 2 per-vgpu data:
+ * sched_data, and sched_ctl. We see these 2 data as part of
+ * the global scheduler which are proteced by gvt->sched_lock.
+ * Caller should make their decision if the vgpu_lock should
+ * be hold outside.
+ */
+
 int intel_vgpu_init_sched_policy(struct intel_vgpu *vgpu)
 {
-	return vgpu->gvt->scheduler.sched_ops->init_vgpu(vgpu);
+	int ret;
+
+	mutex_lock(&vgpu->gvt->sched_lock);
+	ret = vgpu->gvt->scheduler.sched_ops->init_vgpu(vgpu);
+	mutex_unlock(&vgpu->gvt->sched_lock);
+
+	return ret;
 }
 
 void intel_vgpu_clean_sched_policy(struct intel_vgpu *vgpu)
 {
+	mutex_lock(&vgpu->gvt->sched_lock);
 	vgpu->gvt->scheduler.sched_ops->clean_vgpu(vgpu);
+	mutex_unlock(&vgpu->gvt->sched_lock);
 }
 
 void intel_vgpu_start_schedule(struct intel_vgpu *vgpu)
 {
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
+	mutex_lock(&vgpu->gvt->sched_lock);
 	if (!vgpu_data->active) {
 		gvt_dbg_core("vgpu%d: start schedule\n", vgpu->id);
 		vgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);
 	}
+	mutex_unlock(&vgpu->gvt->sched_lock);
 }
 
 void intel_gvt_kick_schedule(struct intel_gvt *gvt)
 {
+	mutex_lock(&gvt->sched_lock);
 	intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);
+	mutex_unlock(&gvt->sched_lock);
 }
 
 void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
@@ -406,6 +432,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 
 	gvt_dbg_core("vgpu%d: stop schedule\n", vgpu->id);
 
+	mutex_lock(&vgpu->gvt->sched_lock);
 	scheduler->sched_ops->stop_schedule(vgpu);
 
 	if (scheduler->next_vgpu == vgpu)
@@ -425,4 +452,5 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		}
 	}
 	spin_unlock_bh(&scheduler->mmio_context_lock);
+	mutex_unlock(&vgpu->gvt->sched_lock);
 }

commit 89babe7cf18e4f93c6ba1e6abfe2e5aa5e4fc66c
Author: Zhipeng Gong <zhipeng.gong@intel.com>
Date:   Wed Apr 4 08:43:53 2018 +0800

    drm/i915/gvt: Update time slice more frequently
    
    When there is only one vGPU in GVT-g and it submits workloads
    continuously, it will not be scheduled out, vgpu_update_timeslice
    is not called and its sched_in_time is not updated in a long time,
    which can be several seconds or longer.
    Once GVT-g pauses to submit workload for this vGPU due to heavy
    host CPU workload, this vGPU get scheduled out and
    vgpu_update_timeslice is called, its left_ts will be subtract
    by a big value from sched_out_time - sched_in_time.
    When GVT-g is going to submit workload for this vGPU again,
    it will not be scheduled in until gvt_balance_timeslice reaches
    stage 0 and reset its left_ts, which introduces several
    hunderand milliseconds latency.
    
    This patch updates time slice in every ms to update sched_in_time
    timely.
    
    v2: revise commit message
    v3: use more concise expr. (Zhenyu)
    
    Signed-off-by: Zhipeng Gong <zhipeng.gong@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Min He <min.he@intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 8876a57f407c..d053cbe1dc94 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -53,7 +53,6 @@ struct vgpu_sched_data {
 	bool active;
 
 	ktime_t sched_in_time;
-	ktime_t sched_out_time;
 	ktime_t sched_time;
 	ktime_t left_ts;
 	ktime_t allocated_ts;
@@ -69,15 +68,19 @@ struct gvt_sched_data {
 	ktime_t expire_time;
 };
 
-static void vgpu_update_timeslice(struct intel_vgpu *pre_vgpu)
+static void vgpu_update_timeslice(struct intel_vgpu *vgpu, ktime_t cur_time)
 {
 	ktime_t delta_ts;
-	struct vgpu_sched_data *vgpu_data = pre_vgpu->sched_data;
+	struct vgpu_sched_data *vgpu_data;
 
-	delta_ts = vgpu_data->sched_out_time - vgpu_data->sched_in_time;
+	if (!vgpu || vgpu == vgpu->gvt->idle_vgpu)
+		return;
 
-	vgpu_data->sched_time += delta_ts;
-	vgpu_data->left_ts -= delta_ts;
+	vgpu_data = vgpu->sched_data;
+	delta_ts = ktime_sub(cur_time, vgpu_data->sched_in_time);
+	vgpu_data->sched_time = ktime_add(vgpu_data->sched_time, delta_ts);
+	vgpu_data->left_ts = ktime_sub(vgpu_data->left_ts, delta_ts);
+	vgpu_data->sched_in_time = cur_time;
 }
 
 #define GVT_TS_BALANCE_PERIOD_MS 100
@@ -151,11 +154,7 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	}
 
 	cur_time = ktime_get();
-	if (scheduler->current_vgpu) {
-		vgpu_data = scheduler->current_vgpu->sched_data;
-		vgpu_data->sched_out_time = cur_time;
-		vgpu_update_timeslice(scheduler->current_vgpu);
-	}
+	vgpu_update_timeslice(scheduler->current_vgpu, cur_time);
 	vgpu_data = scheduler->next_vgpu->sched_data;
 	vgpu_data->sched_in_time = cur_time;
 
@@ -227,13 +226,13 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 void intel_gvt_schedule(struct intel_gvt *gvt)
 {
 	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
+	ktime_t cur_time;
 
 	mutex_lock(&gvt->lock);
+	cur_time = ktime_get();
 
 	if (test_and_clear_bit(INTEL_GVT_REQUEST_SCHED,
 				(void *)&gvt->service_request)) {
-		ktime_t cur_time = ktime_get();
-
 		if (cur_time >= sched_data->expire_time) {
 			gvt_balance_timeslice(sched_data);
 			sched_data->expire_time = ktime_add_ms(
@@ -242,6 +241,7 @@ void intel_gvt_schedule(struct intel_gvt *gvt)
 	}
 	clear_bit(INTEL_GVT_REQUEST_EVENT_SCHED, (void *)&gvt->service_request);
 
+	vgpu_update_timeslice(gvt->scheduler.current_vgpu, cur_time);
 	tbs_sched_func(sched_data);
 
 	mutex_unlock(&gvt->lock);

commit 292bb0d38a5714440b59ef910404408d5e9a8017
Author: Zhipeng Gong <zhipeng.gong@intel.com>
Date:   Wed Apr 4 08:43:52 2018 +0800

    drm/i915/gvt: Use real time to do timer check
    
    intel_gvt_schedule check timer through a counter and is supposed
    to wake up to increase the counter every ms.
    In a system with heavy workload, gvt_service_thread can not get
    a chance to run right after wake up and will be delayed several
    milliseconds. As a result, one hundred counter interval means
    several hundred milliseconds in real time.
    
    This patch use real time instead of counter to do timer check.
    
    v2: remove static variable. (Zhenyu)
    v3: correct expire_time update. (Zhenyu)
    
    Signed-off-by: Zhipeng Gong <zhipeng.gong@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Min He <min.he@intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 75b7bc7b344c..8876a57f407c 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -66,6 +66,7 @@ struct gvt_sched_data {
 	struct hrtimer timer;
 	unsigned long period;
 	struct list_head lru_runq_head;
+	ktime_t expire_time;
 };
 
 static void vgpu_update_timeslice(struct intel_vgpu *pre_vgpu)
@@ -226,14 +227,18 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 void intel_gvt_schedule(struct intel_gvt *gvt)
 {
 	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
-	static uint64_t timer_check;
 
 	mutex_lock(&gvt->lock);
 
 	if (test_and_clear_bit(INTEL_GVT_REQUEST_SCHED,
 				(void *)&gvt->service_request)) {
-		if (!(timer_check++ % GVT_TS_BALANCE_PERIOD_MS))
+		ktime_t cur_time = ktime_get();
+
+		if (cur_time >= sched_data->expire_time) {
 			gvt_balance_timeslice(sched_data);
+			sched_data->expire_time = ktime_add_ms(
+				cur_time, GVT_TS_BALANCE_PERIOD_MS);
+		}
 	}
 	clear_bit(INTEL_GVT_REQUEST_EVENT_SCHED, (void *)&gvt->service_request);
 

commit 9803984581de2b09290338687a3d21a7cd16685b
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Feb 22 15:16:13 2018 +0800

    drm/i915/gvt: Fix vGPU sched timeslice calculation warning
    
    Fix below warning by using proper ktime helper to calculate timeslice.
    
    CHECK   drivers/gpu/drm/i915//gvt/sched_policy.c
    drivers/gpu/drm/i915//gvt/sched_policy.c:108 gvt_balance_timeslice() debug: sval_binop_signed: invalid divide LLONG_MIN/-1
    drivers/gpu/drm/i915//gvt/sched_policy.c:108 gvt_balance_timeslice() debug: sval_binop_signed: invalid divide LLONG_MIN/-1
    
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index cc1ce361cd76..75b7bc7b344c 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -103,9 +103,8 @@ static void gvt_balance_timeslice(struct gvt_sched_data *sched_data)
 
 		list_for_each(pos, &sched_data->lru_runq_head) {
 			vgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);
-			fair_timeslice = ms_to_ktime(GVT_TS_BALANCE_PERIOD_MS) *
-						vgpu_data->sched_ctl.weight /
-						total_weight;
+			fair_timeslice = ktime_divns(ms_to_ktime(GVT_TS_BALANCE_PERIOD_MS),
+						     total_weight) * vgpu_data->sched_ctl.weight;
 
 			vgpu_data->allocated_ts = fair_timeslice;
 			vgpu_data->left_ts = vgpu_data->allocated_ts;

commit 9212b13f28374815d9def65e3c877a35092e1c6e
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Fri Jan 26 15:09:08 2018 +0800

    drm/i915/gvt: only reset execlist state of one engine during VM engine reset
    
    Only reset vgpu execlist state of the exact engine which gets reset
    request from VM. After read context status from HWSP enabled, KMD will use
    the saved CSB read pointer but not always read from MMIO. When one engine
    reset happen, only the read pointer of this engine will be reset, in GVT-g
    host side also need to align with this policy, otherwise VM may get wrong
    CSB status after one engine reset compeleted.
    
    v2: Split refine and fix patch, code refine(Zhenyu)
    v3: Move active flag of vgpu scheduler into sched_data(Zhenyu)
    
    Cc: Fred Gao <fred.gao@intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index d031f6486ce3..cc1ce361cd76 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -50,6 +50,7 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 struct vgpu_sched_data {
 	struct list_head lru_list;
 	struct intel_vgpu *vgpu;
+	bool active;
 
 	ktime_t sched_in_time;
 	ktime_t sched_out_time;
@@ -332,6 +333,7 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 	if (!hrtimer_active(&sched_data->timer))
 		hrtimer_start(&sched_data->timer, ktime_add_ns(ktime_get(),
 			sched_data->period), HRTIMER_MODE_ABS);
+	vgpu_data->active = true;
 }
 
 static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
@@ -339,6 +341,7 @@ static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
 	list_del_init(&vgpu_data->lru_list);
+	vgpu_data->active = false;
 }
 
 static struct intel_gvt_sched_policy_ops tbs_schedule_ops = {
@@ -374,9 +377,12 @@ void intel_vgpu_clean_sched_policy(struct intel_vgpu *vgpu)
 
 void intel_vgpu_start_schedule(struct intel_vgpu *vgpu)
 {
-	gvt_dbg_core("vgpu%d: start schedule\n", vgpu->id);
+	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
-	vgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);
+	if (!vgpu_data->active) {
+		gvt_dbg_core("vgpu%d: start schedule\n", vgpu->id);
+		vgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);
+	}
 }
 
 void intel_gvt_kick_schedule(struct intel_gvt *gvt)
@@ -389,6 +395,10 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 	struct intel_gvt_workload_scheduler *scheduler =
 		&vgpu->gvt->scheduler;
 	int ring_id;
+	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
+
+	if (!vgpu_data->active)
+		return;
 
 	gvt_dbg_core("vgpu%d: stop schedule\n", vgpu->id);
 

commit 61a669473f82cd5cac66146644b829b370c791ca
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Mon Jan 15 16:36:12 2018 +0800

    drm/i915/gvt: cancel scheduler timer when no vGPU exists
    
    Stop gvt scheduler timer if no vGPU exists, otherwise it keeps
    gvt service thread busy to handle request schedule event but no
    actual schedule activity required.
    
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index eea1a2f92099..d031f6486ce3 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -308,8 +308,15 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 
 static void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)
 {
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
+
 	kfree(vgpu->sched_data);
 	vgpu->sched_data = NULL;
+
+	/* this vgpu id has been removed */
+	if (idr_is_empty(&gvt->vgpu_idr))
+		hrtimer_cancel(&sched_data->timer);
 }
 
 static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)

commit c130456cefd4cf9531f12f3e9b23805d34706ac0
Author: Changbin Du <changbin.du@intel.com>
Date:   Wed Nov 29 15:40:07 2017 +0800

    drm/i915/gvt: Kick scheduler when new workload queued
    
    The current schedule policy rely on a 1ms timer to execute workload. This
    can introduce maximum 1ms unnecessary latency. This is especially bad for
    small media workloads.
    
    And I don't think we need this timer for QoS, but the change is not simply
    remove the code. So I made a new API intel_gvt_kick_schedule() for future
    change.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 03532dfc0cd5..eea1a2f92099 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -372,6 +372,11 @@ void intel_vgpu_start_schedule(struct intel_vgpu *vgpu)
 	vgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);
 }
 
+void intel_gvt_kick_schedule(struct intel_gvt *gvt)
+{
+	intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);
+}
+
 void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 {
 	struct intel_gvt_workload_scheduler *scheduler =

commit ba3ee00683bc2dad4c14fba805c2241ae23acff9
Author: Changbin Du <changbin.du@intel.com>
Date:   Fri Sep 22 10:00:09 2017 +0800

    drm/i915/gvt: Fix GPU hang after reusing vGPU instance across different guest OS
    
    We have implemented delayed ring mmio switch mechanism to reduce
    unnecessary mmio switch. While the vGPU is being destroyed or
    detached from VM, we need to force the ring switch to host context.
    
    The later deadline is missed. Then it got a chance that word load
    from VM2 might execute under the ring context of VM1 which was
    attached to a same vGPU instance. Finally, the GPU is hang.
    
    This patch guarantee the two deadline are performed.
    
    v2: Remove unused variable 'scheduler'
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 436377da41ba..03532dfc0cd5 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -308,20 +308,8 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 
 static void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)
 {
-	struct intel_gvt_workload_scheduler *scheduler = &vgpu->gvt->scheduler;
-	int ring_id;
-
 	kfree(vgpu->sched_data);
 	vgpu->sched_data = NULL;
-
-	spin_lock_bh(&scheduler->mmio_context_lock);
-	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
-		if (scheduler->engine_owner[ring_id] == vgpu) {
-			intel_gvt_switch_mmio(vgpu, NULL, ring_id);
-			scheduler->engine_owner[ring_id] = NULL;
-		}
-	}
-	spin_unlock_bh(&scheduler->mmio_context_lock);
 }
 
 static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
@@ -388,6 +376,7 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 {
 	struct intel_gvt_workload_scheduler *scheduler =
 		&vgpu->gvt->scheduler;
+	int ring_id;
 
 	gvt_dbg_core("vgpu%d: stop schedule\n", vgpu->id);
 
@@ -401,4 +390,13 @@ void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
 		scheduler->need_reschedule = true;
 		scheduler->current_vgpu = NULL;
 	}
+
+	spin_lock_bh(&scheduler->mmio_context_lock);
+	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
+		if (scheduler->engine_owner[ring_id] == vgpu) {
+			intel_gvt_switch_mmio(vgpu, NULL, ring_id);
+			scheduler->engine_owner[ring_id] = NULL;
+		}
+	}
+	spin_unlock_bh(&scheduler->mmio_context_lock);
 }

commit 7dfb9ba33f6f3dee710f47cddf0a098d20afbc25
Merge: 9a30a26122c3 615c16a9d864
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Fri Jun 16 10:03:00 2017 +0300

    Merge tag 'gvt-next-2017-06-08' of https://github.com/01org/gvt-linux into drm-intel-next-queued
    
    gvt-next-2017-06-08
    
    First gvt-next pull for 4.13:
    - optimization for per-VM mmio save/restore (Changbin)
    - optimization for mmio hash table (Changbin)
    - scheduler optimization with event (Ping)
    - vGPU reset refinement (Fred)
    - other misc refactor and cleanups, etc.
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170608093547.bjgs436e3iokrzdm@zhen-hp.sh.intel.com

commit c713cb2f9b7e1e9ffa8a379cecb13bc6eacd49b6
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Wed May 24 20:30:17 2017 +0800

    drm/i915/gvt: Support event based scheduling
    
    This patch decouple the time slice calculation and scheduler, let
    other event be able to trigger scheduling without impact the
    calculation for QoS.
    
    v2: add only one new enum definition.
    v3: fix typo.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index f642a3f0cfa0..6f2073d74de2 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -198,11 +198,6 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct vgpu_sched_data *vgpu_data;
 	struct intel_vgpu *vgpu = NULL;
-	static uint64_t timer_check;
-
-	if (!(timer_check++ % GVT_TS_BALANCE_PERIOD_MS))
-		gvt_balance_timeslice(sched_data);
-
 	/* no active vgpu or has already had a target */
 	if (list_empty(&sched_data->lru_runq_head) || scheduler->next_vgpu)
 		goto out;
@@ -227,9 +222,19 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 void intel_gvt_schedule(struct intel_gvt *gvt)
 {
 	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
+	static uint64_t timer_check;
 
 	mutex_lock(&gvt->lock);
+
+	if (test_and_clear_bit(INTEL_GVT_REQUEST_SCHED,
+				(void *)&gvt->service_request)) {
+		if (!(timer_check++ % GVT_TS_BALANCE_PERIOD_MS))
+			gvt_balance_timeslice(sched_data);
+	}
+	clear_bit(INTEL_GVT_REQUEST_EVENT_SCHED, (void *)&gvt->service_request);
+
 	tbs_sched_func(sched_data);
+
 	mutex_unlock(&gvt->lock);
 }
 

commit 0e86cc9ccc3bf557348befaddf5cb613cf3c4458
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu May 4 10:52:38 2017 +0800

    drm/i915/gvt: implement per-vm mmio switching optimization
    
    Commit ab9da627906a ("drm/i915: make context status notifier head be
    per engine") gives us a chance to inspect every single request. Then
    we can eliminate unnecessary mmio switching for same vGPU. We only
    need mmio switching for different VMs (including host).
    
    This patch introduced a new general API intel_gvt_switch_mmio() to
    replace the old intel_gvt_load/restore_render_mmio(). This function
    can be further optimized for vGPU to vGPU switching.
    
    To support individual ring switch, we track the owner who occupy
    each ring. When another VM or host request a ring we do the mmio
    context switching. Otherwise no need to switch the ring.
    
    This optimization is very useful if only one guest has plenty of
    workloads and the host is mostly idle. The best case is no mmio
    switching will happen.
    
    v2:
      o fix missing ring switch issue. (chuanxiao)
      o support individual ring switch.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Reviewed-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 79ba4b3440aa..f642a3f0cfa0 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -299,8 +299,20 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 
 static void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)
 {
+	struct intel_gvt_workload_scheduler *scheduler = &vgpu->gvt->scheduler;
+	int ring_id;
+
 	kfree(vgpu->sched_data);
 	vgpu->sched_data = NULL;
+
+	spin_lock_bh(&scheduler->mmio_context_lock);
+	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
+		if (scheduler->engine_owner[ring_id] == vgpu) {
+			intel_gvt_switch_mmio(vgpu, NULL, ring_id);
+			scheduler->engine_owner[ring_id] = NULL;
+		}
+	}
+	spin_unlock_bh(&scheduler->mmio_context_lock);
 }
 
 static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)

commit 3d72e27a3af88b6f939aa87070c36304f713aa28
Merge: 2ea659a9ef48 ae157902aea1
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Mon May 15 13:03:30 2017 +0300

    Merge tag 'gvt-fixes-2017-05-11' of https://github.com/01org/gvt-linux into drm-intel-fixes
    
    gvt-fixes-2017-05-11
    
    - vGPU scheduler performance regression fix (Ping)
    - bypass in-context mmio restore (Chuanxiao)
    - one typo fix (Colin)
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170511054736.swpcmnzdoqi75cnl@zhen-hp.sh.intel.com

commit ae157902aea13926e9596d091cad6bd87b3be084
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Wed Apr 19 14:02:31 2017 +0800

    drm/i915/gvt: avoid unnecessary vgpu switch
    
    It's no need to switch vgpu if next vgpu is the same with current
    vgpu, otherwise it will make performance drop in some case.
    
    v2: correct the comments.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 34b9acdf3479..6ac77f8cc405 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -53,9 +53,13 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
 
-	/* no target to schedule */
-	if (!scheduler->next_vgpu)
+	/* no need to schedule if next_vgpu is the same with current_vgpu,
+	 * let scheduler chose next_vgpu again by setting it to NULL.
+	 */
+	if (scheduler->next_vgpu == scheduler->current_vgpu) {
+		scheduler->next_vgpu = NULL;
 		return;
+	}
 
 	gvt_dbg_sched("try to schedule next vgpu %d\n",
 			scheduler->next_vgpu->id);

commit 954180aa69325feade02fa79f056fe1561f31fbb
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed Apr 12 14:22:50 2017 +0800

    drm/i915/gvt: remove some debug messages in scheduler timer handler
    
    As those debug messages might appear in every timer call for scheduler,
    it's too noisy, eat too much log and aren't meaningful. So remove them.
    
    Cc: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index f459ec8b06a1..79ba4b3440aa 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -133,9 +133,6 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	if (!scheduler->next_vgpu)
 		return;
 
-	gvt_dbg_sched("try to schedule next vgpu %d\n",
-			scheduler->next_vgpu->id);
-
 	/*
 	 * after the flag is set, workload dispatch thread will
 	 * stop dispatching workload for current vgpu
@@ -144,10 +141,8 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 
 	/* still have uncompleted workload? */
 	for_each_engine(engine, gvt->dev_priv, i) {
-		if (scheduler->current_workload[i]) {
-			gvt_dbg_sched("still have running workload\n");
+		if (scheduler->current_workload[i])
 			return;
-		}
 	}
 
 	cur_time = ktime_get();

commit 0b063bd3ea9c13df78c82aa742e581c39f9d6156
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Sat Apr 1 10:53:02 2017 +0800

    drm/i915/gvt: cleanup some too chatty scheduler message
    
    It's too chatty to have three places to tell us which one
    is next vgpu for schedule. My log file was bloated to eat
    all disk space..
    
    Cc: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index f84959170674..f459ec8b06a1 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -150,9 +150,6 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 		}
 	}
 
-	gvt_dbg_sched("switch to next vgpu %d\n",
-			scheduler->next_vgpu->id);
-
 	cur_time = ktime_get();
 	if (scheduler->current_vgpu) {
 		vgpu_data = scheduler->current_vgpu->sched_data;
@@ -224,17 +221,12 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 		list_del_init(&vgpu_data->lru_list);
 		list_add_tail(&vgpu_data->lru_list,
 				&sched_data->lru_runq_head);
-
-		gvt_dbg_sched("pick next vgpu %d\n", vgpu->id);
 	} else {
 		scheduler->next_vgpu = gvt->idle_vgpu;
 	}
 out:
-	if (scheduler->next_vgpu) {
-		gvt_dbg_sched("try to schedule next vgpu %d\n",
-				scheduler->next_vgpu->id);
+	if (scheduler->next_vgpu)
 		try_to_schedule_next_vgpu(gvt);
-	}
 }
 
 void intel_gvt_schedule(struct intel_gvt *gvt)

commit b35f34d1da4e77637869c8041a355da810f69fb6
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 00:36:40 2017 +0800

    drm/i915/gvt: control the scheduler by timeslice usage
    
    The timeslice usage will determine vGPU whether has chance to
    schedule or not at every vGPU switch checkpoint.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index dd5b38c3e4ed..f84959170674 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -187,8 +187,11 @@ static struct intel_vgpu *find_busy_vgpu(struct gvt_sched_data *sched_data)
 		if (!vgpu_has_pending_workload(vgpu_data->vgpu))
 			continue;
 
-		vgpu = vgpu_data->vgpu;
-		break;
+		/* Return the vGPU only if it has time slice left */
+		if (vgpu_data->left_ts > 0) {
+			vgpu = vgpu_data->vgpu;
+			break;
+		}
 	}
 
 	return vgpu;
@@ -223,6 +226,8 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 				&sched_data->lru_runq_head);
 
 		gvt_dbg_sched("pick next vgpu %d\n", vgpu->id);
+	} else {
+		scheduler->next_vgpu = gvt->idle_vgpu;
 	}
 out:
 	if (scheduler->next_vgpu) {

commit 39d467c2b71ccb4a55af830c117fafe0525caa12
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 10:03:24 2017 +0800

    drm/i915/gvt: add basic function for weight control
    
    This method tries to guarantee precision in second level, with the
    adjustment conducted in every 100ms. At the end of each vGPU switch
    calculate the sched time and subtract it from the time slice
    allocated; the allocated time slice for every 100ms together with
    remaining timeslice, will be used to decide how much timeslice
    allocated to this vGPU in the next 100ms slice, with the end goal
    to guarantee weight ratio in second level.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 5aa7a2539e4d..dd5b38c3e4ed 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -67,6 +67,60 @@ struct gvt_sched_data {
 	struct list_head lru_runq_head;
 };
 
+static void vgpu_update_timeslice(struct intel_vgpu *pre_vgpu)
+{
+	ktime_t delta_ts;
+	struct vgpu_sched_data *vgpu_data = pre_vgpu->sched_data;
+
+	delta_ts = vgpu_data->sched_out_time - vgpu_data->sched_in_time;
+
+	vgpu_data->sched_time += delta_ts;
+	vgpu_data->left_ts -= delta_ts;
+}
+
+#define GVT_TS_BALANCE_PERIOD_MS 100
+#define GVT_TS_BALANCE_STAGE_NUM 10
+
+static void gvt_balance_timeslice(struct gvt_sched_data *sched_data)
+{
+	struct vgpu_sched_data *vgpu_data;
+	struct list_head *pos;
+	static uint64_t stage_check;
+	int stage = stage_check++ % GVT_TS_BALANCE_STAGE_NUM;
+
+	/* The timeslice accumulation reset at stage 0, which is
+	 * allocated again without adding previous debt.
+	 */
+	if (stage == 0) {
+		int total_weight = 0;
+		ktime_t fair_timeslice;
+
+		list_for_each(pos, &sched_data->lru_runq_head) {
+			vgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);
+			total_weight += vgpu_data->sched_ctl.weight;
+		}
+
+		list_for_each(pos, &sched_data->lru_runq_head) {
+			vgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);
+			fair_timeslice = ms_to_ktime(GVT_TS_BALANCE_PERIOD_MS) *
+						vgpu_data->sched_ctl.weight /
+						total_weight;
+
+			vgpu_data->allocated_ts = fair_timeslice;
+			vgpu_data->left_ts = vgpu_data->allocated_ts;
+		}
+	} else {
+		list_for_each(pos, &sched_data->lru_runq_head) {
+			vgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);
+
+			/* timeslice for next 100ms should add the left/debt
+			 * slice of previous stages.
+			 */
+			vgpu_data->left_ts += vgpu_data->allocated_ts;
+		}
+	}
+}
+
 static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
@@ -103,6 +157,7 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	if (scheduler->current_vgpu) {
 		vgpu_data = scheduler->current_vgpu->sched_data;
 		vgpu_data->sched_out_time = cur_time;
+		vgpu_update_timeslice(scheduler->current_vgpu);
 	}
 	vgpu_data = scheduler->next_vgpu->sched_data;
 	vgpu_data->sched_in_time = cur_time;
@@ -148,6 +203,10 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct vgpu_sched_data *vgpu_data;
 	struct intel_vgpu *vgpu = NULL;
+	static uint64_t timer_check;
+
+	if (!(timer_check++ % GVT_TS_BALANCE_PERIOD_MS))
+		gvt_balance_timeslice(sched_data);
 
 	/* no active vgpu or has already had a target */
 	if (list_empty(&sched_data->lru_runq_head) || scheduler->next_vgpu)

commit bc90d097ae144fab1f789f8523b621de7125c6a8
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 00:36:37 2017 +0800

    drm/i915/gvt: define weight according to vGPU type
    
    The weight defines proportional control of physical GPU resource
    shared between vGPUs. So far the weight is tied to a specific vGPU
    type, i.e when creating multiple vGPUs with different types, they
    will inherit different weights.
    
    e.g. The weight of type GVTg_V5_2 is 8, the weight of type GVTg_V5_4
    is 4, so vGPU of type GVTg_V5_2 has double vGPU resource of vGPU type
    GVTg_V5_4.
    
    TODO: allow user control the weight setting in the future.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 15e0c3b53d93..5aa7a2539e4d 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -237,6 +237,7 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 	if (!data)
 		return -ENOMEM;
 
+	data->sched_ctl.weight = vgpu->sched_ctl.weight;
 	data->vgpu = vgpu;
 	INIT_LIST_HEAD(&data->lru_list);
 

commit 32356920dae17ef3b04fe02a113418c983fc66ce
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 00:36:36 2017 +0800

    drm/i915/gvt: factor out the scheduler
    
    Factor out the scheduler to a more clear structure, the basic
    logic is to find out next vGPU first and then schedule it.
    vGPUs were ordered in a LRU list, scheduler scan from the LRU
    list head and choose the first vGPU who has pending workload.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index ff19abc6f77e..15e0c3b53d93 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -48,7 +48,7 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 }
 
 struct vgpu_sched_data {
-	struct list_head list;
+	struct list_head lru_list;
 	struct intel_vgpu *vgpu;
 
 	ktime_t sched_in_time;
@@ -64,7 +64,7 @@ struct gvt_sched_data {
 	struct intel_gvt *gvt;
 	struct hrtimer timer;
 	unsigned long period;
-	struct list_head runq_head;
+	struct list_head lru_runq_head;
 };
 
 static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
@@ -118,36 +118,17 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 		wake_up(&scheduler->waitq[i]);
 }
 
-/* in nanosecond */
-#define GVT_DEFAULT_TIME_SLICE 1000000
-
-static void tbs_sched_func(struct gvt_sched_data *sched_data)
+static struct intel_vgpu *find_busy_vgpu(struct gvt_sched_data *sched_data)
 {
 	struct vgpu_sched_data *vgpu_data;
-
-	struct intel_gvt *gvt = sched_data->gvt;
-	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-
 	struct intel_vgpu *vgpu = NULL;
-	struct list_head *pos, *head;
-
-	/* no vgpu or has already had a target */
-	if (list_empty(&sched_data->runq_head) || scheduler->next_vgpu)
-		goto out;
-
-	if (scheduler->current_vgpu) {
-		vgpu_data = scheduler->current_vgpu->sched_data;
-		head = &vgpu_data->list;
-	} else {
-		head = &sched_data->runq_head;
-	}
+	struct list_head *head = &sched_data->lru_runq_head;
+	struct list_head *pos;
 
 	/* search a vgpu with pending workload */
 	list_for_each(pos, head) {
-		if (pos == &sched_data->runq_head)
-			continue;
 
-		vgpu_data = container_of(pos, struct vgpu_sched_data, list);
+		vgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);
 		if (!vgpu_has_pending_workload(vgpu_data->vgpu))
 			continue;
 
@@ -155,8 +136,33 @@ static void tbs_sched_func(struct gvt_sched_data *sched_data)
 		break;
 	}
 
+	return vgpu;
+}
+
+/* in nanosecond */
+#define GVT_DEFAULT_TIME_SLICE 1000000
+
+static void tbs_sched_func(struct gvt_sched_data *sched_data)
+{
+	struct intel_gvt *gvt = sched_data->gvt;
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	struct vgpu_sched_data *vgpu_data;
+	struct intel_vgpu *vgpu = NULL;
+
+	/* no active vgpu or has already had a target */
+	if (list_empty(&sched_data->lru_runq_head) || scheduler->next_vgpu)
+		goto out;
+
+	vgpu = find_busy_vgpu(sched_data);
 	if (vgpu) {
 		scheduler->next_vgpu = vgpu;
+
+		/* Move the last used vGPU to the tail of lru_list */
+		vgpu_data = vgpu->sched_data;
+		list_del_init(&vgpu_data->lru_list);
+		list_add_tail(&vgpu_data->lru_list,
+				&sched_data->lru_runq_head);
+
 		gvt_dbg_sched("pick next vgpu %d\n", vgpu->id);
 	}
 out:
@@ -200,7 +206,7 @@ static int tbs_sched_init(struct intel_gvt *gvt)
 	if (!data)
 		return -ENOMEM;
 
-	INIT_LIST_HEAD(&data->runq_head);
+	INIT_LIST_HEAD(&data->lru_runq_head);
 	hrtimer_init(&data->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 	data->timer.function = tbs_timer_fn;
 	data->period = GVT_DEFAULT_TIME_SLICE;
@@ -232,7 +238,7 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 		return -ENOMEM;
 
 	data->vgpu = vgpu;
-	INIT_LIST_HEAD(&data->list);
+	INIT_LIST_HEAD(&data->lru_list);
 
 	vgpu->sched_data = data;
 
@@ -250,10 +256,10 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 	struct gvt_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
-	if (!list_empty(&vgpu_data->list))
+	if (!list_empty(&vgpu_data->lru_list))
 		return;
 
-	list_add_tail(&vgpu_data->list, &sched_data->runq_head);
+	list_add_tail(&vgpu_data->lru_list, &sched_data->lru_runq_head);
 
 	if (!hrtimer_active(&sched_data->timer))
 		hrtimer_start(&sched_data->timer, ktime_add_ns(ktime_get(),
@@ -264,7 +270,7 @@ static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
 {
 	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
-	list_del_init(&vgpu_data->list);
+	list_del_init(&vgpu_data->lru_list);
 }
 
 static struct intel_gvt_sched_policy_ops tbs_schedule_ops = {

commit f6504cce54b26e4318697a854a50cf1a1cb3c066
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 00:36:35 2017 +0800

    drm/i915/gvt: add some statistic routine for scheduler
    
    Add some statistic routine to collect the time when vGPU is
    scheduled in/out and the time of the last ctx submission.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index c8ade8fc511d..ff19abc6f77e 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -47,11 +47,33 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 	return false;
 }
 
+struct vgpu_sched_data {
+	struct list_head list;
+	struct intel_vgpu *vgpu;
+
+	ktime_t sched_in_time;
+	ktime_t sched_out_time;
+	ktime_t sched_time;
+	ktime_t left_ts;
+	ktime_t allocated_ts;
+
+	struct vgpu_sched_ctl sched_ctl;
+};
+
+struct gvt_sched_data {
+	struct intel_gvt *gvt;
+	struct hrtimer timer;
+	unsigned long period;
+	struct list_head runq_head;
+};
+
 static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
+	struct vgpu_sched_data *vgpu_data;
+	ktime_t cur_time;
 
 	/* no target to schedule */
 	if (!scheduler->next_vgpu)
@@ -77,6 +99,14 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	gvt_dbg_sched("switch to next vgpu %d\n",
 			scheduler->next_vgpu->id);
 
+	cur_time = ktime_get();
+	if (scheduler->current_vgpu) {
+		vgpu_data = scheduler->current_vgpu->sched_data;
+		vgpu_data->sched_out_time = cur_time;
+	}
+	vgpu_data = scheduler->next_vgpu->sched_data;
+	vgpu_data->sched_in_time = cur_time;
+
 	/* switch current vgpu */
 	scheduler->current_vgpu = scheduler->next_vgpu;
 	scheduler->next_vgpu = NULL;
@@ -88,25 +118,12 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 		wake_up(&scheduler->waitq[i]);
 }
 
-struct tbs_vgpu_data {
-	struct list_head list;
-	struct intel_vgpu *vgpu;
-	/* put some per-vgpu sched stats here */
-};
-
-struct tbs_sched_data {
-	struct intel_gvt *gvt;
-	struct hrtimer timer;
-	unsigned long period;
-	struct list_head runq_head;
-};
-
 /* in nanosecond */
 #define GVT_DEFAULT_TIME_SLICE 1000000
 
-static void tbs_sched_func(struct tbs_sched_data *sched_data)
+static void tbs_sched_func(struct gvt_sched_data *sched_data)
 {
-	struct tbs_vgpu_data *vgpu_data;
+	struct vgpu_sched_data *vgpu_data;
 
 	struct intel_gvt *gvt = sched_data->gvt;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
@@ -130,7 +147,7 @@ static void tbs_sched_func(struct tbs_sched_data *sched_data)
 		if (pos == &sched_data->runq_head)
 			continue;
 
-		vgpu_data = container_of(pos, struct tbs_vgpu_data, list);
+		vgpu_data = container_of(pos, struct vgpu_sched_data, list);
 		if (!vgpu_has_pending_workload(vgpu_data->vgpu))
 			continue;
 
@@ -152,7 +169,7 @@ static void tbs_sched_func(struct tbs_sched_data *sched_data)
 
 void intel_gvt_schedule(struct intel_gvt *gvt)
 {
-	struct tbs_sched_data *sched_data = gvt->scheduler.sched_data;
+	struct gvt_sched_data *sched_data = gvt->scheduler.sched_data;
 
 	mutex_lock(&gvt->lock);
 	tbs_sched_func(sched_data);
@@ -161,9 +178,9 @@ void intel_gvt_schedule(struct intel_gvt *gvt)
 
 static enum hrtimer_restart tbs_timer_fn(struct hrtimer *timer_data)
 {
-	struct tbs_sched_data *data;
+	struct gvt_sched_data *data;
 
-	data = container_of(timer_data, struct tbs_sched_data, timer);
+	data = container_of(timer_data, struct gvt_sched_data, timer);
 
 	intel_gvt_request_service(data->gvt, INTEL_GVT_REQUEST_SCHED);
 
@@ -177,7 +194,7 @@ static int tbs_sched_init(struct intel_gvt *gvt)
 	struct intel_gvt_workload_scheduler *scheduler =
 		&gvt->scheduler;
 
-	struct tbs_sched_data *data;
+	struct gvt_sched_data *data;
 
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
 	if (!data)
@@ -198,7 +215,7 @@ static void tbs_sched_clean(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler =
 		&gvt->scheduler;
-	struct tbs_sched_data *data = scheduler->sched_data;
+	struct gvt_sched_data *data = scheduler->sched_data;
 
 	hrtimer_cancel(&data->timer);
 
@@ -208,7 +225,7 @@ static void tbs_sched_clean(struct intel_gvt *gvt)
 
 static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 {
-	struct tbs_vgpu_data *data;
+	struct vgpu_sched_data *data;
 
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
 	if (!data)
@@ -230,8 +247,8 @@ static void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)
 
 static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 {
-	struct tbs_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;
-	struct tbs_vgpu_data *vgpu_data = vgpu->sched_data;
+	struct gvt_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;
+	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
 	if (!list_empty(&vgpu_data->list))
 		return;
@@ -245,7 +262,7 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 
 static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
 {
-	struct tbs_vgpu_data *vgpu_data = vgpu->sched_data;
+	struct vgpu_sched_data *vgpu_data = vgpu->sched_data;
 
 	list_del_init(&vgpu_data->list);
 }

commit 91d0101ad30bd1bd7f7f805f4fa314c6b70bb602
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Mar 30 00:36:34 2017 +0800

    drm/i915/gvt: use hrtimer replace delayed_work in scheduler
    
    Currently the scheduler is triggered by delayed_work, which doesn't
    provide precision at microsecond level. Move to hrtimer instead for
    more accurate control.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 34b9acdf3479..c8ade8fc511d 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -96,17 +96,16 @@ struct tbs_vgpu_data {
 
 struct tbs_sched_data {
 	struct intel_gvt *gvt;
-	struct delayed_work work;
+	struct hrtimer timer;
 	unsigned long period;
 	struct list_head runq_head;
 };
 
-#define GVT_DEFAULT_TIME_SLICE (msecs_to_jiffies(1))
+/* in nanosecond */
+#define GVT_DEFAULT_TIME_SLICE 1000000
 
-static void tbs_sched_func(struct work_struct *work)
+static void tbs_sched_func(struct tbs_sched_data *sched_data)
 {
-	struct tbs_sched_data *sched_data = container_of(work,
-			struct tbs_sched_data, work.work);
 	struct tbs_vgpu_data *vgpu_data;
 
 	struct intel_gvt *gvt = sched_data->gvt;
@@ -115,8 +114,6 @@ static void tbs_sched_func(struct work_struct *work)
 	struct intel_vgpu *vgpu = NULL;
 	struct list_head *pos, *head;
 
-	mutex_lock(&gvt->lock);
-
 	/* no vgpu or has already had a target */
 	if (list_empty(&sched_data->runq_head) || scheduler->next_vgpu)
 		goto out;
@@ -151,17 +148,30 @@ static void tbs_sched_func(struct work_struct *work)
 				scheduler->next_vgpu->id);
 		try_to_schedule_next_vgpu(gvt);
 	}
+}
 
-	/*
-	 * still have vgpu on runq
-	 * or last schedule haven't finished due to running workload
-	 */
-	if (!list_empty(&sched_data->runq_head) || scheduler->next_vgpu)
-		schedule_delayed_work(&sched_data->work, sched_data->period);
+void intel_gvt_schedule(struct intel_gvt *gvt)
+{
+	struct tbs_sched_data *sched_data = gvt->scheduler.sched_data;
 
+	mutex_lock(&gvt->lock);
+	tbs_sched_func(sched_data);
 	mutex_unlock(&gvt->lock);
 }
 
+static enum hrtimer_restart tbs_timer_fn(struct hrtimer *timer_data)
+{
+	struct tbs_sched_data *data;
+
+	data = container_of(timer_data, struct tbs_sched_data, timer);
+
+	intel_gvt_request_service(data->gvt, INTEL_GVT_REQUEST_SCHED);
+
+	hrtimer_add_expires_ns(&data->timer, data->period);
+
+	return HRTIMER_RESTART;
+}
+
 static int tbs_sched_init(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler =
@@ -174,11 +184,13 @@ static int tbs_sched_init(struct intel_gvt *gvt)
 		return -ENOMEM;
 
 	INIT_LIST_HEAD(&data->runq_head);
-	INIT_DELAYED_WORK(&data->work, tbs_sched_func);
+	hrtimer_init(&data->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	data->timer.function = tbs_timer_fn;
 	data->period = GVT_DEFAULT_TIME_SLICE;
 	data->gvt = gvt;
 
 	scheduler->sched_data = data;
+
 	return 0;
 }
 
@@ -188,7 +200,8 @@ static void tbs_sched_clean(struct intel_gvt *gvt)
 		&gvt->scheduler;
 	struct tbs_sched_data *data = scheduler->sched_data;
 
-	cancel_delayed_work(&data->work);
+	hrtimer_cancel(&data->timer);
+
 	kfree(data);
 	scheduler->sched_data = NULL;
 }
@@ -205,6 +218,7 @@ static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
 	INIT_LIST_HEAD(&data->list);
 
 	vgpu->sched_data = data;
+
 	return 0;
 }
 
@@ -223,7 +237,10 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 		return;
 
 	list_add_tail(&vgpu_data->list, &sched_data->runq_head);
-	schedule_delayed_work(&sched_data->work, 0);
+
+	if (!hrtimer_active(&sched_data->timer))
+		hrtimer_start(&sched_data->timer, ktime_add_ns(ktime_get(),
+			sched_data->period), HRTIMER_MODE_ABS);
 }
 
 static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)

commit 2958b9013fcbabeeba221161d0712f5259f1e15d
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Fri Mar 17 12:11:20 2017 +0800

    drm/i915/gvt: Fix gvt scheduler interval time
    
    Fix to correctly assign 1ms for gvt scheduler interval time,
    as previous code using HZ is pretty broken. And use no delay
    for start gvt scheduler function.
    
    Fixes: 4b63960ebd3f ("drm/i915/gvt: vGPU schedule policy framework")
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: stable@vger.kernel.org # v4.10+
    Acked-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 06c9584ac5f0..34b9acdf3479 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -101,7 +101,7 @@ struct tbs_sched_data {
 	struct list_head runq_head;
 };
 
-#define GVT_DEFAULT_TIME_SLICE (1 * HZ / 1000)
+#define GVT_DEFAULT_TIME_SLICE (msecs_to_jiffies(1))
 
 static void tbs_sched_func(struct work_struct *work)
 {
@@ -223,7 +223,7 @@ static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
 		return;
 
 	list_add_tail(&vgpu_data->list, &sched_data->runq_head);
-	schedule_delayed_work(&sched_data->work, sched_data->period);
+	schedule_delayed_work(&sched_data->work, 0);
 }
 
 static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)

commit 6e3745cea7521499e573aa193c52d801a9cb0c10
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu Feb 9 10:13:15 2017 +0800

    drm/i915/gvt: remove a noisy unimportant log in sched_policy
    
    Remove below unimportant log which is too noisy.
      'no current vgpu search from q head'
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 678b0be85376..06c9584ac5f0 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -125,7 +125,6 @@ static void tbs_sched_func(struct work_struct *work)
 		vgpu_data = scheduler->current_vgpu->sched_data;
 		head = &vgpu_data->list;
 	} else {
-		gvt_dbg_sched("no current vgpu search from q head\n");
 		head = &sched_data->runq_head;
 	}
 

commit 337d0665270ad1ebcbd58ddc1ce7769852f2391b
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Oct 27 09:54:22 2016 +0800

    drm/i915/gvt: remove unused variable 'execlist'
    
    Remove the variable 'execlist' as it's unused in function
    vgpu_has_pending_workload.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 1df6a5460f3e..678b0be85376 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -36,12 +36,10 @@
 
 static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 {
-	struct intel_vgpu_execlist *execlist;
 	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
 
 	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
-		execlist = &vgpu->execlist[i];
 		if (!list_empty(workload_q_head(vgpu, i)))
 			return true;
 	}

commit 999ccb4017c2c818afae18a90060385ec1db903b
Author: Du, Changbin <changbin.du@intel.com>
Date:   Thu Oct 20 14:08:47 2016 +0800

    drm/i915/gvt: mark symbols static where possible
    
    Mark all local functions & variables as static.
    
    Signed-off-by: Du, Changbin <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index b605ac6137eb..1df6a5460f3e 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -236,7 +236,7 @@ static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
 	list_del_init(&vgpu_data->list);
 }
 
-struct intel_gvt_sched_policy_ops tbs_schedule_ops = {
+static struct intel_gvt_sched_policy_ops tbs_schedule_ops = {
 	.init = tbs_sched_init,
 	.clean = tbs_sched_clean,
 	.init_vgpu = tbs_sched_init_vgpu,

commit 0fac21e7e978f8556d3f9bb1b2fadfc722bfe992
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Oct 20 13:30:33 2016 +0800

    drm/i915/gvt: properly access enabled intel_engine_cs
    
    Switch to use new for_each_engine() helper to properly access
    enabled intel_engine_cs as i915 core has changed that to be
    dynamic managed. At GVT-g init time would still depend on ring
    mask to determine engine list as it's earlier.
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index 278db0c180fc..b605ac6137eb 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -37,9 +37,10 @@
 static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 {
 	struct intel_vgpu_execlist *execlist;
-	int i;
+	enum intel_engine_id i;
+	struct intel_engine_cs *engine;
 
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
+	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
 		execlist = &vgpu->execlist[i];
 		if (!list_empty(workload_q_head(vgpu, i)))
 			return true;
@@ -51,7 +52,8 @@ static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	int i;
+	enum intel_engine_id i;
+	struct intel_engine_cs *engine;
 
 	/* no target to schedule */
 	if (!scheduler->next_vgpu)
@@ -67,7 +69,7 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	scheduler->need_reschedule = true;
 
 	/* still have uncompleted workload? */
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
+	for_each_engine(engine, gvt->dev_priv, i) {
 		if (scheduler->current_workload[i]) {
 			gvt_dbg_sched("still have running workload\n");
 			return;
@@ -84,7 +86,7 @@ static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
 	scheduler->need_reschedule = false;
 
 	/* wake up workload dispatch thread */
-	for (i = 0; i < I915_NUM_ENGINES; i++)
+	for_each_engine(engine, gvt->dev_priv, i)
 		wake_up(&scheduler->waitq[i]);
 }
 

commit feddf6e866c9cdbdec45b09f0a9566ea538a0da3
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Oct 20 17:15:03 2016 +0800

    drm/i915/gvt: clean up intel_gvt.h as interface for i915 core
    
    i915 core should only call functions and structures exposed through
    intel_gvt.h. Remove internal gvt.h and i915_pvinfo.h.
    
    Change for internal intel_gvt structure as private handler which
    not requires to expose gvt internal structure for i915 core.
    
    v2: Fix per Chris's comment
    - carefully handle dev_priv->gvt assignment
    - add necessary bracket for macro helper
    - forward declartion struct intel_gvt
    - keep free operation within same file handling alloc
    
    v3: fix use after free and remove intel_gvt.initialized
    
    v4: change to_gvt() to an inline
    
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
index c607354c3798..278db0c180fc 100644
--- a/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -32,6 +32,7 @@
  */
 
 #include "i915_drv.h"
+#include "gvt.h"
 
 static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
 {

commit 4b63960ebd3f4c41caca6a8dca68751b34e61e9b
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun May 1 17:09:58 2016 -0400

    drm/i915/gvt: vGPU schedule policy framework
    
    This patch introduces a vGPU schedule policy framework, with a timer based
    schedule policy module for now
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/sched_policy.c b/drivers/gpu/drm/i915/gvt/sched_policy.c
new file mode 100644
index 000000000000..c607354c3798
--- /dev/null
+++ b/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -0,0 +1,291 @@
+/*
+ * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * Authors:
+ *    Anhua Xu
+ *    Kevin Tian <kevin.tian@intel.com>
+ *
+ * Contributors:
+ *    Min He <min.he@intel.com>
+ *    Bing Niu <bing.niu@intel.com>
+ *    Zhi Wang <zhi.a.wang@intel.com>
+ *
+ */
+
+#include "i915_drv.h"
+
+static bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)
+{
+	struct intel_vgpu_execlist *execlist;
+	int i;
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		execlist = &vgpu->execlist[i];
+		if (!list_empty(workload_q_head(vgpu, i)))
+			return true;
+	}
+
+	return false;
+}
+
+static void try_to_schedule_next_vgpu(struct intel_gvt *gvt)
+{
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	int i;
+
+	/* no target to schedule */
+	if (!scheduler->next_vgpu)
+		return;
+
+	gvt_dbg_sched("try to schedule next vgpu %d\n",
+			scheduler->next_vgpu->id);
+
+	/*
+	 * after the flag is set, workload dispatch thread will
+	 * stop dispatching workload for current vgpu
+	 */
+	scheduler->need_reschedule = true;
+
+	/* still have uncompleted workload? */
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		if (scheduler->current_workload[i]) {
+			gvt_dbg_sched("still have running workload\n");
+			return;
+		}
+	}
+
+	gvt_dbg_sched("switch to next vgpu %d\n",
+			scheduler->next_vgpu->id);
+
+	/* switch current vgpu */
+	scheduler->current_vgpu = scheduler->next_vgpu;
+	scheduler->next_vgpu = NULL;
+
+	scheduler->need_reschedule = false;
+
+	/* wake up workload dispatch thread */
+	for (i = 0; i < I915_NUM_ENGINES; i++)
+		wake_up(&scheduler->waitq[i]);
+}
+
+struct tbs_vgpu_data {
+	struct list_head list;
+	struct intel_vgpu *vgpu;
+	/* put some per-vgpu sched stats here */
+};
+
+struct tbs_sched_data {
+	struct intel_gvt *gvt;
+	struct delayed_work work;
+	unsigned long period;
+	struct list_head runq_head;
+};
+
+#define GVT_DEFAULT_TIME_SLICE (1 * HZ / 1000)
+
+static void tbs_sched_func(struct work_struct *work)
+{
+	struct tbs_sched_data *sched_data = container_of(work,
+			struct tbs_sched_data, work.work);
+	struct tbs_vgpu_data *vgpu_data;
+
+	struct intel_gvt *gvt = sched_data->gvt;
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+
+	struct intel_vgpu *vgpu = NULL;
+	struct list_head *pos, *head;
+
+	mutex_lock(&gvt->lock);
+
+	/* no vgpu or has already had a target */
+	if (list_empty(&sched_data->runq_head) || scheduler->next_vgpu)
+		goto out;
+
+	if (scheduler->current_vgpu) {
+		vgpu_data = scheduler->current_vgpu->sched_data;
+		head = &vgpu_data->list;
+	} else {
+		gvt_dbg_sched("no current vgpu search from q head\n");
+		head = &sched_data->runq_head;
+	}
+
+	/* search a vgpu with pending workload */
+	list_for_each(pos, head) {
+		if (pos == &sched_data->runq_head)
+			continue;
+
+		vgpu_data = container_of(pos, struct tbs_vgpu_data, list);
+		if (!vgpu_has_pending_workload(vgpu_data->vgpu))
+			continue;
+
+		vgpu = vgpu_data->vgpu;
+		break;
+	}
+
+	if (vgpu) {
+		scheduler->next_vgpu = vgpu;
+		gvt_dbg_sched("pick next vgpu %d\n", vgpu->id);
+	}
+out:
+	if (scheduler->next_vgpu) {
+		gvt_dbg_sched("try to schedule next vgpu %d\n",
+				scheduler->next_vgpu->id);
+		try_to_schedule_next_vgpu(gvt);
+	}
+
+	/*
+	 * still have vgpu on runq
+	 * or last schedule haven't finished due to running workload
+	 */
+	if (!list_empty(&sched_data->runq_head) || scheduler->next_vgpu)
+		schedule_delayed_work(&sched_data->work, sched_data->period);
+
+	mutex_unlock(&gvt->lock);
+}
+
+static int tbs_sched_init(struct intel_gvt *gvt)
+{
+	struct intel_gvt_workload_scheduler *scheduler =
+		&gvt->scheduler;
+
+	struct tbs_sched_data *data;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&data->runq_head);
+	INIT_DELAYED_WORK(&data->work, tbs_sched_func);
+	data->period = GVT_DEFAULT_TIME_SLICE;
+	data->gvt = gvt;
+
+	scheduler->sched_data = data;
+	return 0;
+}
+
+static void tbs_sched_clean(struct intel_gvt *gvt)
+{
+	struct intel_gvt_workload_scheduler *scheduler =
+		&gvt->scheduler;
+	struct tbs_sched_data *data = scheduler->sched_data;
+
+	cancel_delayed_work(&data->work);
+	kfree(data);
+	scheduler->sched_data = NULL;
+}
+
+static int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)
+{
+	struct tbs_vgpu_data *data;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->vgpu = vgpu;
+	INIT_LIST_HEAD(&data->list);
+
+	vgpu->sched_data = data;
+	return 0;
+}
+
+static void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)
+{
+	kfree(vgpu->sched_data);
+	vgpu->sched_data = NULL;
+}
+
+static void tbs_sched_start_schedule(struct intel_vgpu *vgpu)
+{
+	struct tbs_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;
+	struct tbs_vgpu_data *vgpu_data = vgpu->sched_data;
+
+	if (!list_empty(&vgpu_data->list))
+		return;
+
+	list_add_tail(&vgpu_data->list, &sched_data->runq_head);
+	schedule_delayed_work(&sched_data->work, sched_data->period);
+}
+
+static void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)
+{
+	struct tbs_vgpu_data *vgpu_data = vgpu->sched_data;
+
+	list_del_init(&vgpu_data->list);
+}
+
+struct intel_gvt_sched_policy_ops tbs_schedule_ops = {
+	.init = tbs_sched_init,
+	.clean = tbs_sched_clean,
+	.init_vgpu = tbs_sched_init_vgpu,
+	.clean_vgpu = tbs_sched_clean_vgpu,
+	.start_schedule = tbs_sched_start_schedule,
+	.stop_schedule = tbs_sched_stop_schedule,
+};
+
+int intel_gvt_init_sched_policy(struct intel_gvt *gvt)
+{
+	gvt->scheduler.sched_ops = &tbs_schedule_ops;
+
+	return gvt->scheduler.sched_ops->init(gvt);
+}
+
+void intel_gvt_clean_sched_policy(struct intel_gvt *gvt)
+{
+	gvt->scheduler.sched_ops->clean(gvt);
+}
+
+int intel_vgpu_init_sched_policy(struct intel_vgpu *vgpu)
+{
+	return vgpu->gvt->scheduler.sched_ops->init_vgpu(vgpu);
+}
+
+void intel_vgpu_clean_sched_policy(struct intel_vgpu *vgpu)
+{
+	vgpu->gvt->scheduler.sched_ops->clean_vgpu(vgpu);
+}
+
+void intel_vgpu_start_schedule(struct intel_vgpu *vgpu)
+{
+	gvt_dbg_core("vgpu%d: start schedule\n", vgpu->id);
+
+	vgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);
+}
+
+void intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)
+{
+	struct intel_gvt_workload_scheduler *scheduler =
+		&vgpu->gvt->scheduler;
+
+	gvt_dbg_core("vgpu%d: stop schedule\n", vgpu->id);
+
+	scheduler->sched_ops->stop_schedule(vgpu);
+
+	if (scheduler->next_vgpu == vgpu)
+		scheduler->next_vgpu = NULL;
+
+	if (scheduler->current_vgpu == vgpu) {
+		/* stop workload dispatching */
+		scheduler->need_reschedule = true;
+		scheduler->current_vgpu = NULL;
+	}
+}
