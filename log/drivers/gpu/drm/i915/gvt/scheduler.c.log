commit faa392181a0bd42c5478175cef601adeecdc91b6
Merge: cfa3b8068b09 9ca1f474cea0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 15:04:15 2020 -0700

    Merge tag 'drm-next-2020-06-02' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm updates from Dave Airlie:
     "Highlights:
    
       - Core DRM had a lot of refactoring around managed drm resources to
         make drivers simpler.
    
       - Intel Tigerlake support is on by default
    
       - amdgpu now support p2p PCI buffer sharing and encrypted GPU memory
    
      Details:
    
      core:
       - uapi: error out EBUSY when existing master
       - uapi: rework SET/DROP MASTER permission handling
       - remove drm_pci.h
       - drm_pci* are now legacy
       - introduced managed DRM resources
       - subclassing support for drm_framebuffer
       - simple encoder helper
       - edid improvements
       - vblank + writeback documentation improved
       - drm/mm - optimise tree searches
       - port drivers to use devm_drm_dev_alloc
    
      dma-buf:
       - add flag for p2p buffer support
    
      mst:
       - ACT timeout improvements
       - remove drm_dp_mst_has_audio
       - don't use 2nd TX slot - spec recommends against it
    
      bridge:
       - dw-hdmi various improvements
       - chrontel ch7033 support
       - fix stack issues with old gcc
    
      hdmi:
       - add unpack function for drm infoframe
    
      fbdev:
       - misc fbdev driver fixes
    
      i915:
       - uapi: global sseu pinning
       - uapi: OA buffer polling
       - uapi: remove generated perf code
       - uapi: per-engine default property values in sysfs
       - Tigerlake GEN12 enabled.
       - Lots of gem refactoring
       - Tigerlake enablement patches
       - move to drm_device logging
       - Icelake gamma HW readout
       - push MST link retrain to hotplug work
       - bandwidth atomic helpers
       - ICL fixes
       - RPS/GT refactoring
       - Cherryview full-ppgtt support
       - i915 locking guidelines documented
       - require linear fb stride to be 512 multiple on gen9
       - Tigerlake SAGV support
    
      amdgpu:
       - uapi: encrypted GPU memory handling
       - uapi: add MEM_SYNC IB flag
       - p2p dma-buf support
       - export VRAM dma-bufs
       - FRU chip access support
       - RAS/SR-IOV updates
       - Powerplay locking fixes
       - VCN DPG (powergating) enablement
       - GFX10 clockgating fixes
       - DC fixes
       - GPU reset fixes
       - navi SDMA fix
       - expose FP16 for modesetting
       - DP 1.4 compliance fixes
       - gfx10 soft recovery
       - Improved Critical Thermal Faults handling
       - resizable BAR on gmc10
    
      amdkfd:
       - uapi: GWS resource management
       - track GPU memory per process
       - report PCI domain in topology
    
      radeon:
       - safe reg list generator fixes
    
      nouveau:
       - HD audio fixes on recent systems
       - vGPU detection (fail probe if we're on one, for now)
       - Interlaced mode fixes (mostly avoidance on Turing, which doesn't support it)
       - SVM improvements/fixes
       - NVIDIA format modifier support
       - Misc other fixes.
    
      adv7511:
       - HDMI SPDIF support
    
      ast:
       - allocate crtc state size
       - fix double assignment
       - fix suspend
    
      bochs:
       - drop connector register
    
      cirrus:
       - move to tiny drivers.
    
      exynos:
       - fix imported dma-buf mapping
       - enable runtime PM
       - fixes and cleanups
    
      mediatek:
       - DPI pin mode swap
       - config mipi_tx current/impedance
    
      lima:
       - devfreq + cooling device support
       - task handling improvements
       - runtime PM support
    
      pl111:
       - vexpress init improvements
       - fix module auto-load
    
      rcar-du:
       - DT bindings conversion to YAML
       - Planes zpos sanity check and fix
       - MAINTAINERS entry for LVDS panel driver
    
      mcde:
       - fix return value
    
      mgag200:
       - use managed config init
    
      stm:
       - read endpoints from DT
    
      vboxvideo:
       - use PCI managed functions
       - drop WC mtrr
    
      vkms:
       - enable cursor by default
    
      rockchip:
       - afbc support
    
      virtio:
       - various cleanups
    
      qxl:
       - fix cursor notify port
    
      hisilicon:
       - 128-byte stride alignment fix
    
      sun4i:
       - improved format handling"
    
    * tag 'drm-next-2020-06-02' of git://anongit.freedesktop.org/drm/drm: (1401 commits)
      drm/amd/display: Fix potential integer wraparound resulting in a hang
      drm/amd/display: drop cursor position check in atomic test
      drm/amdgpu: fix device attribute node create failed with multi gpu
      drm/nouveau: use correct conflicting framebuffer API
      drm/vblank: Fix -Wformat compile warnings on some arches
      drm/amdgpu: Sync with VM root BO when switching VM to CPU update mode
      drm/amd/display: Handle GPU reset for DC block
      drm/amdgpu: add apu flags (v2)
      drm/amd/powerpay: Disable gfxoff when setting manual mode on picasso and raven
      drm/amdgpu: fix pm sysfs node handling (v2)
      drm/amdgpu: move gpu_info parsing after common early init
      drm/amdgpu: move discovery gfx config fetching
      drm/nouveau/dispnv50: fix runtime pm imbalance on error
      drm/nouveau: fix runtime pm imbalance on error
      drm/nouveau: fix runtime pm imbalance on error
      drm/nouveau/debugfs: fix runtime pm imbalance on error
      drm/nouveau/nouveau/hmm: fix migrate zero page to GPU
      drm/nouveau/nouveau/hmm: fix nouveau_dmem_chunk allocations
      drm/nouveau/kms/nv50-: Share DP SST mode_valid() handling with MST
      drm/nouveau/kms/nv50-: Move 8BPC limit for MST into nv50_mstc_get_modes()
      ...

commit f159c647b13b3cc9ae682f455b2858d2e1abd6be
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Fri May 15 19:35:45 2020 -0700

    drm/i915: Mark check_shadow_context_ppgtt as maybe unused
    
    When CONFIG_DRM_I915_DEBUG_GEM is not set, clang warns:
    
    drivers/gpu/drm/i915/gvt/scheduler.c:884:1: warning: function
    'check_shadow_context_ppgtt' is not needed and will not be emitted
    [-Wunneeded-internal-declaration]
    check_shadow_context_ppgtt(struct execlist_ring_context *c, struct
    intel_vgpu_mm *m)
    ^
    1 warning generated.
    
    This warning is similar to -Wunused-function but rather than warning
    that the function is completely unused, it warns that it is used in some
    expression within the file but that expression will be evaluated to a
    constant or be optimized away in the final assembly, essentially making
    it appeared used but really isn't. Usually, this happens when a function
    or variable is only used in sizeof, where it will appear to be used but
    will be evaluated at compile time and not be required to be emitted.
    
    In this case, the function is only used in GEM_BUG_ON, which is defined
    as BUILD_BUG_ON_INVALID, which intentionally follows this pattern. To
    fix this warning, add __maybe_unused to make it clear that this is
    intentional depending on the configuration.
    
    Fixes: bec3df930fbd ("drm/i915/gvt: Support PPGTT table load command")
    Link: https://github.com/ClangBuiltLinux/linux/issues/1027
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200516023545.3332334-1-natechancellor@gmail.com
    (cherry picked from commit 993fa32eb3d5ffb79e86a770ca982eb9c9f54011)
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index c00189432b58..3a9bd8e4d8db 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -876,7 +876,7 @@ static void update_guest_pdps(struct intel_vgpu *vgpu,
 				gpa + i * 8, &pdp[7 - i], 4);
 }
 
-static bool
+static __maybe_unused bool
 check_shadow_context_ppgtt(struct execlist_ring_context *c, struct intel_vgpu_mm *m)
 {
 	if (m->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {

commit 1be8f347d70b5027b7b223c665756d85feaf36b6
Merge: 7a00e68b4317 47e51832ae93
Author: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Date:   Thu May 14 18:02:22 2020 +0300

    Merge tag 'gvt-next-2020-05-12' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2020-05-12
    
    - Support PPGTT update via LRI cmd (Zhenyu)
    - Remove extra kmap for shadow ctx update (Zhenyu)
    - Move workload cleanup out of execlist handling code (Zhenyu)
    
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200512094017.GX18545@zhen-hp.sh.intel.com

commit 475e8423024d4713c8b791b2fa27b09fbcb7d2ca
Merge: a9d094dcf784 72a7a9925e2b
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Mon May 11 23:55:25 2020 -0700

    Merge tag 'gvt-fixes-2020-05-12' of https://github.com/intel/gvt-linux into drm-intel-fixes
    
    gvt-fixes-2020-05-12
    
    - Correct transcoder and DPLL initial clock to fix recent guest
      display probe failure. (Colin)
    - Fix kernel oops on older guest using aliasing ppgtt. (Zhenyu)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200512024803.GQ18545@zhen-hp.sh.intel.com

commit 72a7a9925e2beea09b109dffb3384c9bf920d9da
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed May 6 17:59:18 2020 +0800

    drm/i915/gvt: Fix kernel oops for 3-level ppgtt guest
    
    As i915 won't allocate extra PDP for current default PML4 table,
    so for 3-level ppgtt guest, we would hit kernel pointer access
    failure on extra PDP pointers. So this trys to bypass that now.
    It won't impact real shadow PPGTT setup, so guest context still
    works.
    
    This is verified on 4.15 guest kernel with i915.enable_ppgtt=1
    to force on old aliasing ppgtt behavior.
    
    Fixes: 4f15665ccbba ("drm/i915: Add ppgtt to GVT GEM context")
    Reviewed-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200506095918.124913-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index cb11c3184085..0f1d6998cf9c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -379,7 +379,11 @@ static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 		for (i = 0; i < GVT_RING_CTX_NR_PDPS; i++) {
 			struct i915_page_directory * const pd =
 				i915_pd_entry(ppgtt->pd, i);
-
+			/* skip now as current i915 ppgtt alloc won't allocate
+			   top level pdp for non 4-level table, won't impact
+			   shadow ppgtt. */
+			if (!pd)
+				break;
 			px_dma(pd) = mm->ppgtt_mm.shadow_pdps[i];
 		}
 	}

commit 47e51832ae93534d872511ba557115722582d94c
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed May 6 17:59:48 2020 +0800

    drm/i915/gvt: use context lrc_reg_state for shadow ppgtt override
    
    We can replace kmap by using context's lrc_reg_state directly
    for shadow ppgtt table override.
    
    Reviewed-by: Yan Zhao <yan.y.zhao@intel.com>
    Cc: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200506095948.124979-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index da6a2a424283..54c10dfcd3d2 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -58,10 +58,8 @@ static void set_context_pdp_root_pointer(
 
 static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 {
-	struct drm_i915_gem_object *ctx_obj =
-		workload->req->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
-	struct page *page;
+	struct intel_context *ctx = workload->req->context;
 
 	if (WARN_ON(!workload->shadow_mm))
 		return;
@@ -69,11 +67,9 @@ static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 	if (WARN_ON(!atomic_read(&workload->shadow_mm->pincount)))
 		return;
 
-	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap(page);
+	shadow_ring_context = (struct execlist_ring_context *)ctx->lrc_reg_state;
 	set_context_pdp_root_pointer(shadow_ring_context,
 			(void *)workload->shadow_mm->ppgtt_mm.shadow_pdps);
-	kunmap(page);
 }
 
 /*

commit bec3df930fbd40fcc7bcead43a39cfd3c5b0419f
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Fri May 8 11:14:09 2020 +0800

    drm/i915/gvt: Support PPGTT table load command
    
    The PPGTT in context image can be overridden by LRI cmd with another
    PPGTT's pdps. In such case, the load mm is used instead of the one in
    the context image. So we need to load its shadow mm in GVT and replace
    ppgtt pointers in command.
    
    This feature is used by guest IGD driver to share gfx VM between
    different contexts. Verified by IGT "gem_ctx_clone" test.
    
    v4:
    - consolidate shadow mm handlers (Yan)
    - fix cmd shadow mm pin error path
    
    v3: (Zhenyu Wang)
    - Cleanup PDP register offset check
    - Add debug check for guest context ppgtt update
    - Skip 3-level ppgtt guest handling code. The reason is that all
      guests now use 4-level ppgtt table and the only left case for
      3-level table is ancient aliasing ppgtt case. But those guest
      kernel has no use of PPGTT LRI command. So 3-level ppgtt guest
      for this feature becomes simply un-testable.
    
    v2: (Zhenyu Wang)
    - Change to list for handling possible multiple ppgtt table loads
      in one submission. Make sure shadow mm is to replace for each one.
    
    Reviewed-by: Yan Zhao <yan.y.zhao@intel.com>
    Cc: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Tina Zhang <tina.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200508031409.2562-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d25fbae87ca9..da6a2a424283 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -647,10 +647,11 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 	}
 }
 
-static int prepare_workload(struct intel_vgpu_workload *workload)
+static int
+intel_vgpu_shadow_mm_pin(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct intel_vgpu_mm *m;
 	int ret = 0;
 
 	ret = intel_vgpu_pin_mm(workload->shadow_mm);
@@ -665,6 +666,52 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 		return -EINVAL;
 	}
 
+	if (!list_empty(&workload->lri_shadow_mm)) {
+		list_for_each_entry(m, &workload->lri_shadow_mm,
+				    ppgtt_mm.link) {
+			ret = intel_vgpu_pin_mm(m);
+			if (ret) {
+				list_for_each_entry_from_reverse(m,
+								 &workload->lri_shadow_mm,
+								 ppgtt_mm.link)
+					intel_vgpu_unpin_mm(m);
+				gvt_vgpu_err("LRI shadow ppgtt fail to pin\n");
+				break;
+			}
+		}
+	}
+
+	if (ret)
+		intel_vgpu_unpin_mm(workload->shadow_mm);
+
+	return ret;
+}
+
+static void
+intel_vgpu_shadow_mm_unpin(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu_mm *m;
+
+	if (!list_empty(&workload->lri_shadow_mm)) {
+		list_for_each_entry(m, &workload->lri_shadow_mm,
+				    ppgtt_mm.link)
+			intel_vgpu_unpin_mm(m);
+	}
+	intel_vgpu_unpin_mm(workload->shadow_mm);
+}
+
+static int prepare_workload(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	int ret = 0;
+
+	ret = intel_vgpu_shadow_mm_pin(workload);
+	if (ret) {
+		gvt_vgpu_err("fail to pin shadow mm\n");
+		return ret;
+	}
+
 	update_shadow_pdps(workload);
 
 	set_context_ppgtt_from_shadow(workload, s->shadow[workload->engine->id]);
@@ -711,7 +758,7 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 err_shadow_batch:
 	release_shadow_batch_buffer(workload);
 err_unpin_mm:
-	intel_vgpu_unpin_mm(workload->shadow_mm);
+	intel_vgpu_shadow_mm_unpin(workload);
 	return ret;
 }
 
@@ -821,6 +868,37 @@ pick_next_workload(struct intel_gvt *gvt, struct intel_engine_cs *engine)
 	return workload;
 }
 
+static void update_guest_pdps(struct intel_vgpu *vgpu,
+			      u64 ring_context_gpa, u32 pdp[8])
+{
+	u64 gpa;
+	int i;
+
+	gpa = ring_context_gpa + RING_CTX_OFF(pdps[0].val);
+
+	for (i = 0; i < 8; i++)
+		intel_gvt_hypervisor_write_gpa(vgpu,
+				gpa + i * 8, &pdp[7 - i], 4);
+}
+
+static bool
+check_shadow_context_ppgtt(struct execlist_ring_context *c, struct intel_vgpu_mm *m)
+{
+	if (m->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
+		u64 shadow_pdp = c->pdps[7].val | (u64) c->pdps[6].val << 32;
+
+		if (shadow_pdp != m->ppgtt_mm.shadow_pdps[0]) {
+			gvt_dbg_mm("4-level context ppgtt not match LRI command\n");
+			return false;
+		}
+		return true;
+	} else {
+		/* see comment in LRI handler in cmd_parser.c */
+		gvt_dbg_mm("invalid shadow mm type\n");
+		return false;
+	}
+}
+
 static void update_guest_context(struct intel_vgpu_workload *workload)
 {
 	struct i915_request *rq = workload->req;
@@ -906,6 +984,15 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 
 	shadow_ring_context = (void *) ctx->lrc_reg_state;
 
+	if (!list_empty(&workload->lri_shadow_mm)) {
+		struct intel_vgpu_mm *m = list_last_entry(&workload->lri_shadow_mm,
+							  struct intel_vgpu_mm,
+							  ppgtt_mm.link);
+		GEM_BUG_ON(!check_shadow_context_ppgtt(shadow_ring_context, m));
+		update_guest_pdps(vgpu, workload->ring_context_gpa,
+				  (void *)m->ppgtt_mm.guest_pdps);
+	}
+
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa + \
 		RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)
@@ -1014,7 +1101,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	workload->complete(workload);
 
-	intel_vgpu_unpin_mm(workload->shadow_mm);
+	intel_vgpu_shadow_mm_unpin(workload);
 	intel_vgpu_destroy_workload(workload);
 
 	atomic_dec(&s->running_workload_num);
@@ -1409,6 +1496,16 @@ void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
 	release_shadow_batch_buffer(workload);
 	release_shadow_wa_ctx(&workload->wa_ctx);
 
+	if (!list_empty(&workload->lri_shadow_mm)) {
+		struct intel_vgpu_mm *m, *mm;
+		list_for_each_entry_safe(m, mm, &workload->lri_shadow_mm,
+					 ppgtt_mm.link) {
+			list_del(&m->ppgtt_mm.link);
+			intel_vgpu_mm_put(m);
+		}
+	}
+
+	GEM_BUG_ON(!list_empty(&workload->lri_shadow_mm));
 	if (workload->shadow_mm)
 		intel_vgpu_mm_put(workload->shadow_mm);
 
@@ -1427,6 +1524,7 @@ alloc_workload(struct intel_vgpu *vgpu)
 
 	INIT_LIST_HEAD(&workload->list);
 	INIT_LIST_HEAD(&workload->shadow_bb);
+	INIT_LIST_HEAD(&workload->lri_shadow_mm);
 
 	init_waitqueue_head(&workload->shadow_ctx_status_wq);
 	atomic_set(&workload->shadow_ctx_active, 0);

commit 40dcee1b7c086715d8ce7f6c9c9bdae45f4855b0
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed May 6 17:43:17 2020 +0800

    drm/i915/gvt: move workload destroy out of execlist complete
    
    To let execlist.c only handle execlist handling and keep other
    workload cleanup function in scheduler.c to align with other
    workload specific handling there. This doesn't change current
    code behavior.
    
    Reviewed-by: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200506094318.105604-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 92a055c9d8d8..d25fbae87ca9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1014,6 +1014,9 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	workload->complete(workload);
 
+	intel_vgpu_unpin_mm(workload->shadow_mm);
+	intel_vgpu_destroy_workload(workload);
+
 	atomic_dec(&s->running_workload_num);
 	wake_up(&scheduler->workload_complete_wq);
 

commit 53b2622e774681943230fb9f31096512fff99fe7
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Apr 28 19:47:49 2020 +0100

    drm/i915/execlists: Avoid reusing the same logical CCID
    
    The bspec is confusing on the nature of the upper 32bits of the LRC
    descriptor. Once upon a time, it said that it uses the upper 32b to
    decide if it should perform a lite-restore, and so we must ensure that
    each unique context submitted to HW is given a unique CCID [for the
    duration of it being on the HW]. Currently, this is achieved by using
    a small circular tag, and assigning every context submitted to HW a
    new id. However, this tag is being cleared on repinning an inflight
    context such that we end up re-using the 0 tag for multiple contexts.
    
    To avoid accidentally clearing the CCID in the upper 32bits of the LRC
    descriptor, split the descriptor into two dwords so we can update the
    GGTT address separately from the CCID.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1796
    Fixes: 2935ed5339c4 ("drm/i915: Remove logical HW ID")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: <stable@vger.kernel.org> # v5.5+
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200428184751.11257-1-chris@chris-wilson.co.uk
    (cherry picked from commit 2632f174a2e1a5fd40a70404fa8ccfd0b1f79ebd)
    (cherry picked from commit a4b70fcc587860f4b972f68217d8ebebe295ec15)
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index cb11c3184085..6eb6710b35e9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -290,7 +290,7 @@ static void
 shadow_context_descriptor_update(struct intel_context *ce,
 				 struct intel_vgpu_workload *workload)
 {
-	u64 desc = ce->lrc_desc;
+	u64 desc = ce->lrc.desc;
 
 	/*
 	 * Update bits 0-11 of the context descriptor which includes flags
@@ -300,7 +300,7 @@ shadow_context_descriptor_update(struct intel_context *ce,
 	desc |= (u64)workload->ctx_desc.addressing_mode <<
 		GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	ce->lrc_desc = desc;
+	ce->lrc.desc = desc;
 }
 
 static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)

commit 8b46ed57f34df111c224d4319c891b43e0e03806
Merge: 79eb8c7f015a fb55c7355223
Author: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Date:   Thu Apr 30 10:53:20 2020 +0300

    Merge tag 'gvt-next-2020-04-22' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2020-04-22
    
    - remove non-upstream xen support bits (Christoph)
    - guest context shadow copy optimization (Yan)
    - guest context tracking for shadow skip optimization (Yan)
    
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200422051230.GH11247@zhen-hp.sh.intel.com

commit 2632f174a2e1a5fd40a70404fa8ccfd0b1f79ebd
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Apr 28 19:47:49 2020 +0100

    drm/i915/execlists: Avoid reusing the same logical CCID
    
    The bspec is confusing on the nature of the upper 32bits of the LRC
    descriptor. Once upon a time, it said that it uses the upper 32b to
    decide if it should perform a lite-restore, and so we must ensure that
    each unique context submitted to HW is given a unique CCID [for the
    duration of it being on the HW]. Currently, this is achieved by using
    a small circular tag, and assigning every context submitted to HW a
    new id. However, this tag is being cleared on repinning an inflight
    context such that we end up re-using the 0 tag for multiple contexts.
    
    To avoid accidentally clearing the CCID in the upper 32bits of the LRC
    descriptor, split the descriptor into two dwords so we can update the
    GGTT address separately from the CCID.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1796
    Fixes: 2935ed5339c4 ("drm/i915: Remove logical HW ID")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: <stable@vger.kernel.org> # v5.5+
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200428184751.11257-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 2f5c59111821..38234073e0fc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -290,7 +290,7 @@ static void
 shadow_context_descriptor_update(struct intel_context *ce,
 				 struct intel_vgpu_workload *workload)
 {
-	u64 desc = ce->lrc_desc;
+	u64 desc = ce->lrc.desc;
 
 	/*
 	 * Update bits 0-11 of the context descriptor which includes flags
@@ -300,7 +300,7 @@ shadow_context_descriptor_update(struct intel_context *ce,
 	desc |= (u64)workload->ctx_desc.addressing_mode <<
 		GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	ce->lrc_desc = desc;
+	ce->lrc.desc = desc;
 }
 
 static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)

commit 50689771c8f073e97f7758e5b696c64f3044bbd8
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 22 20:05:58 2020 +0100

    drm/i915: Only close vma we open
    
    The history of i915_vma_close() is confusing, as is its use. As the
    lifetime of the i915_vma is currently bounded by the object it is
    attached to, we needed a means of identify when a vma was no longer in
    use by userspace (via the user's fd). This is further complicated by
    that only ppgtt vma should be closed at the user's behest, as the ggtt
    were always shared.
    
    Now that we attach the vma to a lut on the user's context, the open
    count does indicate how many unique and open context/vm are referencing
    this vma from the user. As such, we can and should just use the
    open_count to track when the vma is still in use by userspace.
    
    It's a poor man's replacement for reference counting.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/issues/1193
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200422190558.30509-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index cb11c3184085..2f5c59111821 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -595,10 +595,9 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			if (bb->va && !IS_ERR(bb->va))
 				i915_gem_object_unpin_map(bb->obj);
 
-			if (bb->vma && !IS_ERR(bb->vma)) {
+			if (bb->vma && !IS_ERR(bb->vma))
 				i915_vma_unpin(bb->vma);
-				i915_vma_close(bb->vma);
-			}
+
 			i915_gem_object_put(bb->obj);
 		}
 		list_del(&bb->list);

commit fb55c735522352704c35d899d0b253453cf0e799
Author: Yan Zhao <yan.y.zhao@intel.com>
Date:   Fri Apr 17 05:13:34 2020 -0400

    drm/i915/gvt: skip populate shadow context if guest context not changed
    
    Software is not expected to populate engine context except when using
    restore inhibit bit or golden state to initialize it for the first time.
    
    Therefore, if a newly submitted guest context is the same as the last
    shadowed one, no need to populate its engine context from guest again.
    
    Currently using lrca + ring_context_gpa to identify whether two guest
    contexts are the same.
    
    The reason of why context id is not included as an identifier is that
    i915 recently changed the code and context id is only unique for a
    context when OA is enabled. And when OA is on, context id is generated
    based on lrca. Therefore, in that case, if two contexts are of the same
    lrca, they have identical context ids as well.
    (This patch also works with old guest kernel like 4.20.)
    
    for guest context, if its ggtt entry is modified after last context
    shadowing, it is also deemed as not the same context as last shadowed one.
    
    v7:
    -removed local variable "valid". use the one in s->last_ctx diretly
    
    v6:
    -change type of lrca of last ctx to be u32. as currently it's all
    protected by vgpu lock (Kevin Tian)
    -reset valid of last ctx to false once it needs to be repopulated before
    population completes successfully (Kevin Tian)
    
    v5:
    -merge all 3 patches into one patch  (Zhenyu Wang)
    
    v4:
    - split the series into 3 patches.
    - don't turn on optimization until last patch in this series (Kevin Tian)
    - define lrca to be atomic in this patch rather than update its type in
    the second patch (Kevin Tian)
    
    v3: updated commit message to describe engine context and context id
    clearly (Kevin Tian)
    v2: rebased to 5.6.0-rc4+Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Kevin Tian <kevin.tian@intel.com>
    Cc: Kevin Tian <kevin.tian@intel.com>
    Suggested-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200417091334.32628-1-yan.y.zhao@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f650ad3367b6..92a055c9d8d8 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -135,7 +135,10 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	unsigned long context_gpa, context_page_num;
 	unsigned long gpa_base; /* first gpa of consecutive GPAs */
 	unsigned long gpa_size; /* size of consecutive GPAs */
+	struct intel_vgpu_submission *s = &vgpu->submission;
 	int i;
+	bool skip = false;
+	int ring_id = workload->engine->id;
 
 	GEM_BUG_ON(!intel_context_is_pinned(ctx));
 
@@ -175,13 +178,31 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 
 	sr_oa_regs(workload, (u32 *)shadow_ring_context, false);
 
-	if (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val))
-		return 0;
+	gvt_dbg_sched("ring %s workload lrca %x, ctx_id %x, ctx gpa %llx",
+			workload->engine->name, workload->ctx_desc.lrca,
+			workload->ctx_desc.context_id,
+			workload->ring_context_gpa);
 
-	gvt_dbg_sched("ring %s workload lrca %x",
-		      workload->engine->name,
-		      workload->ctx_desc.lrca);
+	/* only need to ensure this context is not pinned/unpinned during the
+	 * period from last submission to this this submission.
+	 * Upon reaching this function, the currently submitted context is not
+	 * supposed to get unpinned. If a misbehaving guest driver ever does
+	 * this, it would corrupt itself.
+	 */
+	if (s->last_ctx[ring_id].valid &&
+			(s->last_ctx[ring_id].lrca ==
+				workload->ctx_desc.lrca) &&
+			(s->last_ctx[ring_id].ring_context_gpa ==
+				workload->ring_context_gpa))
+		skip = true;
 
+	s->last_ctx[ring_id].lrca = workload->ctx_desc.lrca;
+	s->last_ctx[ring_id].ring_context_gpa = workload->ring_context_gpa;
+
+	if (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val) || skip)
+		return 0;
+
+	s->last_ctx[ring_id].valid = false;
 	context_page_num = workload->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
@@ -220,6 +241,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 		gpa_size = I915_GTT_PAGE_SIZE;
 		dst = context_base + (i << I915_GTT_PAGE_SHIFT);
 	}
+	s->last_ctx[ring_id].valid = true;
 	return 0;
 }
 
@@ -1296,6 +1318,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	atomic_set(&s->running_workload_num, 0);
 	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
+	memset(s->last_ctx, 0, sizeof(s->last_ctx));
+
 	i915_vm_put(&ppgtt->vm);
 	return 0;
 

commit e5e113079efdffb9a39e16a88d109c3d47efdfcc
Author: Yan Zhao <yan.y.zhao@intel.com>
Date:   Tue Apr 14 23:58:27 2020 -0400

    drm/i915/gvt: combine access to consecutive guest context pages
    
    IOVA(GPA)s of context pages are checked and if they are consecutive,
    read/write them together in one intel_gvt_hypervisor_read_gpa() /
    intel_gvt_hypervisor_write_gpa().
    
    Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200415035827.26476-1-yan.y.zhao@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 4639a56f9a3c..f650ad3367b6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -133,6 +133,8 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	void *dst;
 	void *context_base;
 	unsigned long context_gpa, context_page_num;
+	unsigned long gpa_base; /* first gpa of consecutive GPAs */
+	unsigned long gpa_size; /* size of consecutive GPAs */
 	int i;
 
 	GEM_BUG_ON(!intel_context_is_pinned(ctx));
@@ -186,8 +188,11 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	if (IS_BROADWELL(gvt->gt->i915) && workload->engine->id == RCS0)
 		context_page_num = 19;
 
-	i = 2;
-	while (i < context_page_num) {
+	/* find consecutive GPAs from gma until the first inconsecutive GPA.
+	 * read from the continuous GPAs into dst virtual address
+	 */
+	gpa_size = 0;
+	for (i = 2; i < context_page_num; i++) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
 				(u32)((workload->ctx_desc.lrca + i) <<
 				I915_GTT_PAGE_SHIFT));
@@ -196,10 +201,24 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			return -EFAULT;
 		}
 
+		if (gpa_size == 0) {
+			gpa_base = context_gpa;
+			dst = context_base + (i << I915_GTT_PAGE_SHIFT);
+		} else if (context_gpa != gpa_base + gpa_size)
+			goto read;
+
+		gpa_size += I915_GTT_PAGE_SIZE;
+
+		if (i == context_page_num - 1)
+			goto read;
+
+		continue;
+
+read:
+		intel_gvt_hypervisor_read_gpa(vgpu, gpa_base, dst, gpa_size);
+		gpa_base = context_gpa;
+		gpa_size = I915_GTT_PAGE_SIZE;
 		dst = context_base + (i << I915_GTT_PAGE_SHIFT);
-		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
-				I915_GTT_PAGE_SIZE);
-		i++;
 	}
 	return 0;
 }
@@ -789,6 +808,8 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	void *context_base;
 	void *src;
 	unsigned long context_gpa, context_page_num;
+	unsigned long gpa_base; /* first gpa of consecutive GPAs */
+	unsigned long gpa_size; /* size of consecutive GPAs*/
 	int i;
 	u32 ring_base;
 	u32 head, tail;
@@ -822,11 +843,14 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	if (IS_BROADWELL(rq->i915) && rq->engine->id == RCS0)
 		context_page_num = 19;
 
-	i = 2;
 	context_base = (void *) ctx->lrc_reg_state -
 			(LRC_STATE_PN << I915_GTT_PAGE_SHIFT);
 
-	while (i < context_page_num) {
+	/* find consecutive GPAs from gma until the first inconsecutive GPA.
+	 * write to the consecutive GPAs from src virtual address
+	 */
+	gpa_size = 0;
+	for (i = 2; i < context_page_num; i++) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
 				(u32)((workload->ctx_desc.lrca + i) <<
 					I915_GTT_PAGE_SHIFT));
@@ -835,10 +859,24 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			return;
 		}
 
+		if (gpa_size == 0) {
+			gpa_base = context_gpa;
+			src = context_base + (i << I915_GTT_PAGE_SHIFT);
+		} else if (context_gpa != gpa_base + gpa_size)
+			goto write;
+
+		gpa_size += I915_GTT_PAGE_SIZE;
+
+		if (i == context_page_num - 1)
+			goto write;
+
+		continue;
+
+write:
+		intel_gvt_hypervisor_write_gpa(vgpu, gpa_base, src, gpa_size);
+		gpa_base = context_gpa;
+		gpa_size = I915_GTT_PAGE_SIZE;
 		src = context_base + (i << I915_GTT_PAGE_SHIFT);
-		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
-				I915_GTT_PAGE_SIZE);
-		i++;
 	}
 
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa +

commit 6c2f73e26a253ae827d9754572bfee4a912e559c
Author: Yan Zhao <yan.y.zhao@intel.com>
Date:   Tue Apr 14 23:57:28 2020 -0400

    drm/i915/gvt: access shadow ctx via its virtual address directly
    
    as shadow context is pinned in intel_vgpu_setup_submission() and
    unpinned in intel_vgpu_clean_submission(), its base virtual address of
    is safely obtained from lrc_reg_state. no need to call kmap()/kunmap()
    repeatedly.
    
    Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200415035728.26424-1-yan.y.zhao@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index e1e6345700cc..4639a56f9a3c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -128,16 +128,19 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
-	struct drm_i915_gem_object *ctx_obj =
-		workload->req->context->state->obj;
+	struct intel_context *ctx = workload->req->context;
 	struct execlist_ring_context *shadow_ring_context;
-	struct page *page;
 	void *dst;
+	void *context_base;
 	unsigned long context_gpa, context_page_num;
 	int i;
 
-	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap(page);
+	GEM_BUG_ON(!intel_context_is_pinned(ctx));
+
+	context_base = (void *) ctx->lrc_reg_state -
+				(LRC_STATE_PN << I915_GTT_PAGE_SHIFT);
+
+	shadow_ring_context = (void *) ctx->lrc_reg_state;
 
 	sr_oa_regs(workload, (u32 *)shadow_ring_context, true);
 #define COPY_REG(name) \
@@ -169,7 +172,6 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			I915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
 	sr_oa_regs(workload, (u32 *)shadow_ring_context, false);
-	kunmap(page);
 
 	if (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val))
 		return 0;
@@ -194,11 +196,9 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			return -EFAULT;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, i);
-		dst = kmap(page);
+		dst = context_base + (i << I915_GTT_PAGE_SHIFT);
 		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
 				I915_GTT_PAGE_SIZE);
-		kunmap(page);
 		i++;
 	}
 	return 0;
@@ -784,9 +784,9 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 {
 	struct i915_request *rq = workload->req;
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct drm_i915_gem_object *ctx_obj = rq->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
-	struct page *page;
+	struct intel_context *ctx = workload->req->context;
+	void *context_base;
 	void *src;
 	unsigned long context_gpa, context_page_num;
 	int i;
@@ -797,6 +797,8 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d workload lrca %x\n", rq->engine->id,
 		      workload->ctx_desc.lrca);
 
+	GEM_BUG_ON(!intel_context_is_pinned(ctx));
+
 	head = workload->rb_head;
 	tail = workload->rb_tail;
 	wrap_count = workload->guest_rb_head >> RB_HEAD_WRAP_CNT_OFF;
@@ -821,6 +823,8 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 		context_page_num = 19;
 
 	i = 2;
+	context_base = (void *) ctx->lrc_reg_state -
+			(LRC_STATE_PN << I915_GTT_PAGE_SHIFT);
 
 	while (i < context_page_num) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
@@ -831,19 +835,16 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			return;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, i);
-		src = kmap(page);
+		src = context_base + (i << I915_GTT_PAGE_SHIFT);
 		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
 				I915_GTT_PAGE_SIZE);
-		kunmap(page);
 		i++;
 	}
 
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa +
 		RING_CTX_OFF(ring_header.val), &workload->rb_tail, 4);
 
-	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap(page);
+	shadow_ring_context = (void *) ctx->lrc_reg_state;
 
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa + \
@@ -860,8 +861,6 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			(void *)shadow_ring_context +
 			sizeof(*shadow_ring_context),
 			I915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
-
-	kunmap(page);
 }
 
 void intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,

commit 17d0c1062a0c60e17c96538adf4a84c208930d9d
Merge: 2bdd4c28baff eb0ff8074e0b
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Tue Mar 31 09:25:14 2020 -0700

    Merge tag 'gvt-next-fixes-2020-03-31' of https://github.com/intel/gvt-linux into drm-intel-next-fixes
    
    gvt-next-fixes-2020-03-31
    
    - Fix non-privilege access warning (Tina)
    - Fix display port type (Tina)
    - BDW cmd parser missed SWTESS_BASE_ADDRESS (Yan)
    - Bypass length check of LRI (Yan)
    - Fix one klocwork warning (Tina)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200331070025.GB16629@zhen-hp.sh.intel.com

commit eb0ff8074e0baecba2cd0c7813f6cfa99bafc430
Author: Tina Zhang <tina.zhang@intel.com>
Date:   Tue Mar 24 20:30:21 2020 +0800

    drm/i915/gvt: Fix klocwork issues about data size
    
    Add llu suffix and cast operator to fix the klocwork warning about
    "Operands in a bitwise operation have different size"
    
    Signed-off-by: Tina Zhang <tina.zhang@intel.com>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200324123021.15831-1-tina.zhang@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index e1e6345700cc..9d67f33f37a0 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -296,8 +296,8 @@ shadow_context_descriptor_update(struct intel_context *ce,
 	 * Update bits 0-11 of the context descriptor which includes flags
 	 * like GEN8_CTX_* cached in desc_template
 	 */
-	desc &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
-	desc |= workload->ctx_desc.addressing_mode <<
+	desc &= ~(0x3ull << GEN8_CTX_ADDRESSING_MODE_SHIFT);
+	desc |= (u64)workload->ctx_desc.addressing_mode <<
 		GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
 	ce->lrc_desc = desc;

commit 75e675f81f33fa0a1c66180c32eb05a5061ed19a
Merge: 765e7cd9a6fd a61ac1e75105
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Tue Mar 10 15:46:28 2020 -0700

    Merge tag 'gvt-next-2020-03-10' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2020-03-10
    
    - Fix CFL dmabuf display after vfio edid enabling (Tina)
    - Clean up scan non-priv batch debugfs entry (Chris)
    - Use intel engines initialized in gvt, cleanup previous ring id (Chris)
    - Use intel_gt instead (Chris)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200310081928.GG28483@zhen-hp.sh.intel.com

commit a61ac1e75105a077ec1efd6923ae3c619f862304
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Mar 6 10:08:10 2020 +0800

    drm/i915/gvt: Wean gvt off using dev_priv
    
    Teach gvt to use intel_gt directly as it currently assumes direct HW
    access.
    
    [Zhenyu: rebase, fix compiling]
    
    Cc: Ding Zhuocheng <zhuocheng.ding@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200304032307.2983-3-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f43ac73c20c6..e1e6345700cc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -84,7 +84,7 @@ static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 static void sr_oa_regs(struct intel_vgpu_workload *workload,
 		u32 *reg_state, bool save)
 {
-	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->gt->i915;
 	u32 ctx_oactxctrl = dev_priv->perf.ctx_oactxctrl_offset;
 	u32 ctx_flexeu0 = dev_priv->perf.ctx_flexeu0_offset;
 	int i = 0;
@@ -181,7 +181,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	context_page_num = workload->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && workload->engine->id == RCS0)
+	if (IS_BROADWELL(gvt->gt->i915) && workload->engine->id == RCS0)
 		context_page_num = 19;
 
 	i = 2;
@@ -868,7 +868,7 @@ void intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,
 				intel_engine_mask_t engine_mask)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct drm_i915_private *dev_priv = vgpu->gvt->gt->i915;
 	struct intel_engine_cs *engine;
 	struct intel_vgpu_workload *pos, *n;
 	intel_engine_mask_t tmp;
@@ -1065,7 +1065,7 @@ void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
 
 	gvt_dbg_core("clean workload scheduler\n");
 
-	for_each_engine(engine, gvt->dev_priv, i) {
+	for_each_engine(engine, gvt->gt, i) {
 		atomic_notifier_chain_unregister(
 					&engine->context_status_notifier,
 					&gvt->shadow_ctx_notifier_block[i]);
@@ -1084,7 +1084,7 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 	init_waitqueue_head(&scheduler->workload_complete_wq);
 
-	for_each_engine(engine, gvt->dev_priv, i) {
+	for_each_engine(engine, gvt->gt, i) {
 		init_waitqueue_head(&scheduler->waitq[i]);
 
 		scheduler->thread[i] = kthread_run(workload_thread, engine,
@@ -1142,7 +1142,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
 
 	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(s->shadow[0]->vm));
-	for_each_engine(engine, vgpu->gvt->dev_priv, id)
+	for_each_engine(engine, vgpu->gvt->gt, id)
 		intel_context_unpin(s->shadow[id]);
 
 	kmem_cache_destroy(s->workloads);
@@ -1199,7 +1199,7 @@ i915_context_ppgtt_root_save(struct intel_vgpu_submission *s,
  */
 int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
-	struct drm_i915_private *i915 = vgpu->gvt->dev_priv;
+	struct drm_i915_private *i915 = vgpu->gvt->gt->i915;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct intel_engine_cs *engine;
 	struct i915_ppgtt *ppgtt;
@@ -1212,7 +1212,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 	i915_context_ppgtt_root_save(s, ppgtt);
 
-	for_each_engine(engine, &i915->gt, i) {
+	for_each_engine(engine, vgpu->gvt->gt, i) {
 		struct intel_context *ce;
 
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
@@ -1264,7 +1264,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 out_shadow_ctx:
 	i915_context_ppgtt_root_restore(s, ppgtt);
-	for_each_engine(engine, &i915->gt, i) {
+	for_each_engine(engine, vgpu->gvt->gt, i) {
 		if (IS_ERR(s->shadow[i]))
 			break;
 
@@ -1291,7 +1291,7 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 				     intel_engine_mask_t engine_mask,
 				     unsigned int interface)
 {
-	struct drm_i915_private *i915 = vgpu->gvt->dev_priv;
+	struct drm_i915_private *i915 = vgpu->gvt->gt->i915;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	const struct intel_vgpu_submission_ops *ops[] = {
 		[INTEL_VGPU_EXECLIST_SUBMISSION] =

commit 8fde41076f6df53db84cb13051efed6482986ce3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Mar 4 11:23:06 2020 +0800

    drm/i915/gvt: Wean gvt off dev_priv->engine[]
    
    Stop trying to escape out of the gvt layer to find the engine that we
    initially setup for use with gvt. Record the engines during initialisation
    and use them henceforth.
    
    add/remove: 1/4 grow/shrink: 22/28 up/down: 341/-1410 (-1069)
    
    [Zhenyu: rebase, fix nonpriv register check fault, fix gvt engine
    thread run failure.]
    
    Cc: Ding Zhuocheng <zhuocheng.ding@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200304032307.2983-2-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index cc89afd7b5f1..f43ac73c20c6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -98,7 +98,7 @@ static void sr_oa_regs(struct intel_vgpu_workload *workload,
 		i915_mmio_reg_offset(EU_PERF_CNTL6),
 	};
 
-	if (workload->ring_id != RCS0)
+	if (workload->engine->id != RCS0)
 		return;
 
 	if (save) {
@@ -128,7 +128,6 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
-	int ring_id = workload->ring_id;
 	struct drm_i915_gem_object *ctx_obj =
 		workload->req->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
@@ -154,7 +153,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	COPY_REG_MASKED(ctx_ctrl);
 	COPY_REG(ctx_timestamp);
 
-	if (ring_id == RCS0) {
+	if (workload->engine->id == RCS0) {
 		COPY_REG(bb_per_ctx_ptr);
 		COPY_REG(rcs_indirect_ctx);
 		COPY_REG(rcs_indirect_ctx_offset);
@@ -175,14 +174,14 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	if (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val))
 		return 0;
 
-	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
-			workload->ctx_desc.lrca);
-
-	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
+	gvt_dbg_sched("ring %s workload lrca %x",
+		      workload->engine->name,
+		      workload->ctx_desc.lrca);
 
+	context_page_num = workload->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS0)
+	if (IS_BROADWELL(gvt->dev_priv) && workload->engine->id == RCS0)
 		context_page_num = 19;
 
 	i = 2;
@@ -210,38 +209,43 @@ static inline bool is_gvt_request(struct i915_request *rq)
 	return intel_context_force_single_submission(rq->context);
 }
 
-static void save_ring_hw_state(struct intel_vgpu *vgpu, int ring_id)
+static void save_ring_hw_state(struct intel_vgpu *vgpu,
+			       const struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	u32 ring_base = dev_priv->engine[ring_id]->mmio_base;
+	struct intel_uncore *uncore = engine->uncore;
 	i915_reg_t reg;
 
-	reg = RING_INSTDONE(ring_base);
-	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
-	reg = RING_ACTHD(ring_base);
-	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
-	reg = RING_ACTHD_UDW(ring_base);
-	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
+	reg = RING_INSTDONE(engine->mmio_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =
+		intel_uncore_read(uncore, reg);
+
+	reg = RING_ACTHD(engine->mmio_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =
+		intel_uncore_read(uncore, reg);
+
+	reg = RING_ACTHD_UDW(engine->mmio_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =
+		intel_uncore_read(uncore, reg);
 }
 
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
-	struct i915_request *req = data;
+	struct i915_request *rq = data;
 	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
-				shadow_ctx_notifier_block[req->engine->id]);
+				shadow_ctx_notifier_block[rq->engine->id]);
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	enum intel_engine_id ring_id = req->engine->id;
+	enum intel_engine_id ring_id = rq->engine->id;
 	struct intel_vgpu_workload *workload;
 	unsigned long flags;
 
-	if (!is_gvt_request(req)) {
+	if (!is_gvt_request(rq)) {
 		spin_lock_irqsave(&scheduler->mmio_context_lock, flags);
 		if (action == INTEL_CONTEXT_SCHEDULE_IN &&
 		    scheduler->engine_owner[ring_id]) {
 			/* Switch ring from vGPU to host. */
 			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
-					      NULL, ring_id);
+					      NULL, rq->engine);
 			scheduler->engine_owner[ring_id] = NULL;
 		}
 		spin_unlock_irqrestore(&scheduler->mmio_context_lock, flags);
@@ -259,7 +263,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		if (workload->vgpu != scheduler->engine_owner[ring_id]) {
 			/* Switch ring from host to vGPU or vGPU to vGPU. */
 			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
-					      workload->vgpu, ring_id);
+					      workload->vgpu, rq->engine);
 			scheduler->engine_owner[ring_id] = workload->vgpu;
 		} else
 			gvt_dbg_sched("skip ring %d mmio switch for vgpu%d\n",
@@ -268,11 +272,11 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
-		save_ring_hw_state(workload->vgpu, ring_id);
+		save_ring_hw_state(workload->vgpu, rq->engine);
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
-		save_ring_hw_state(workload->vgpu, ring_id);
+		save_ring_hw_state(workload->vgpu, rq->engine);
 		break;
 	default:
 		WARN_ON(1);
@@ -391,7 +395,7 @@ intel_gvt_workload_req_alloc(struct intel_vgpu_workload *workload)
 	if (workload->req)
 		return 0;
 
-	rq = i915_request_create(s->shadow[workload->ring_id]);
+	rq = i915_request_create(s->shadow[workload->engine->id]);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
 		return PTR_ERR(rq);
@@ -420,15 +424,16 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (workload->shadow)
 		return 0;
 
-	if (!test_and_set_bit(workload->ring_id, s->shadow_ctx_desc_updated))
-		shadow_context_descriptor_update(s->shadow[workload->ring_id],
+	if (!test_and_set_bit(workload->engine->id, s->shadow_ctx_desc_updated))
+		shadow_context_descriptor_update(s->shadow[workload->engine->id],
 						 workload);
 
 	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
 		return ret;
 
-	if (workload->ring_id == RCS0 && workload->wa_ctx.indirect_ctx.size) {
+	if (workload->engine->id == RCS0 &&
+	    workload->wa_ctx.indirect_ctx.size) {
 		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
 		if (ret)
 			goto err_shadow;
@@ -436,6 +441,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 
 	workload->shadow = true;
 	return 0;
+
 err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
 	return ret;
@@ -567,12 +573,8 @@ static int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 
 static void update_vreg_in_ctx(struct intel_vgpu_workload *workload)
 {
-	struct intel_vgpu *vgpu = workload->vgpu;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	u32 ring_base;
-
-	ring_base = dev_priv->engine[workload->ring_id]->mmio_base;
-	vgpu_vreg_t(vgpu, RING_START(ring_base)) = workload->rb_start;
+	vgpu_vreg_t(workload->vgpu, RING_START(workload->engine->mmio_base)) =
+		workload->rb_start;
 }
 
 static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
@@ -608,7 +610,6 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	int ring = workload->ring_id;
 	int ret = 0;
 
 	ret = intel_vgpu_pin_mm(workload->shadow_mm);
@@ -625,7 +626,7 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 
 	update_shadow_pdps(workload);
 
-	set_context_ppgtt_from_shadow(workload, s->shadow[ring]);
+	set_context_ppgtt_from_shadow(workload, s->shadow[workload->engine->id]);
 
 	ret = intel_vgpu_sync_oos_pages(workload->vgpu);
 	if (ret) {
@@ -677,11 +678,10 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct i915_request *rq;
-	int ring_id = workload->ring_id;
 	int ret;
 
-	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
-		ring_id, workload);
+	gvt_dbg_sched("ring id %s prepare to dispatch workload %p\n",
+		      workload->engine->name, workload);
 
 	mutex_lock(&vgpu->vgpu_lock);
 
@@ -710,8 +710,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	}
 
 	if (!IS_ERR_OR_NULL(workload->req)) {
-		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
-				ring_id, workload->req);
+		gvt_dbg_sched("ring id %s submit workload to i915 %p\n",
+			      workload->engine->name, workload->req);
 		i915_request_add(workload->req);
 		workload->dispatched = true;
 	}
@@ -722,8 +722,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
-static struct intel_vgpu_workload *pick_next_workload(
-		struct intel_gvt *gvt, int ring_id)
+static struct intel_vgpu_workload *
+pick_next_workload(struct intel_gvt *gvt, struct intel_engine_cs *engine)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
@@ -735,27 +735,27 @@ static struct intel_vgpu_workload *pick_next_workload(
 	 * bail out
 	 */
 	if (!scheduler->current_vgpu) {
-		gvt_dbg_sched("ring id %d stop - no current vgpu\n", ring_id);
+		gvt_dbg_sched("ring %s stop - no current vgpu\n", engine->name);
 		goto out;
 	}
 
 	if (scheduler->need_reschedule) {
-		gvt_dbg_sched("ring id %d stop - will reschedule\n", ring_id);
+		gvt_dbg_sched("ring %s stop - will reschedule\n", engine->name);
 		goto out;
 	}
 
 	if (!scheduler->current_vgpu->active ||
-	    list_empty(workload_q_head(scheduler->current_vgpu, ring_id)))
+	    list_empty(workload_q_head(scheduler->current_vgpu, engine)))
 		goto out;
 
 	/*
 	 * still have current workload, maybe the workload disptacher
 	 * fail to submit it for some reason, resubmit it.
 	 */
-	if (scheduler->current_workload[ring_id]) {
-		workload = scheduler->current_workload[ring_id];
-		gvt_dbg_sched("ring id %d still have current workload %p\n",
-				ring_id, workload);
+	if (scheduler->current_workload[engine->id]) {
+		workload = scheduler->current_workload[engine->id];
+		gvt_dbg_sched("ring %s still have current workload %p\n",
+			      engine->name, workload);
 		goto out;
 	}
 
@@ -765,13 +765,14 @@ static struct intel_vgpu_workload *pick_next_workload(
 	 * will wait the current workload is finished when trying to
 	 * schedule out a vgpu.
 	 */
-	scheduler->current_workload[ring_id] = container_of(
-			workload_q_head(scheduler->current_vgpu, ring_id)->next,
-			struct intel_vgpu_workload, list);
+	scheduler->current_workload[engine->id] =
+		list_first_entry(workload_q_head(scheduler->current_vgpu,
+						 engine),
+				 struct intel_vgpu_workload, list);
 
-	workload = scheduler->current_workload[ring_id];
+	workload = scheduler->current_workload[engine->id];
 
-	gvt_dbg_sched("ring id %d pick new workload %p\n", ring_id, workload);
+	gvt_dbg_sched("ring %s pick new workload %p\n", engine->name, workload);
 
 	atomic_inc(&workload->vgpu->submission.running_workload_num);
 out:
@@ -783,14 +784,12 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 {
 	struct i915_request *rq = workload->req;
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_gvt *gvt = vgpu->gvt;
 	struct drm_i915_gem_object *ctx_obj = rq->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *src;
 	unsigned long context_gpa, context_page_num;
 	int i;
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
 	u32 ring_base;
 	u32 head, tail;
 	u16 wrap_count;
@@ -811,14 +810,14 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 
 	head = (wrap_count << RB_HEAD_WRAP_CNT_OFF) | tail;
 
-	ring_base = dev_priv->engine[workload->ring_id]->mmio_base;
+	ring_base = rq->engine->mmio_base;
 	vgpu_vreg_t(vgpu, RING_TAIL(ring_base)) = tail;
 	vgpu_vreg_t(vgpu, RING_HEAD(ring_base)) = head;
 
 	context_page_num = rq->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && rq->engine->id == RCS0)
+	if (IS_BROADWELL(rq->i915) && rq->engine->id == RCS0)
 		context_page_num = 19;
 
 	i = 2;
@@ -966,54 +965,47 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	mutex_unlock(&vgpu->vgpu_lock);
 }
 
-struct workload_thread_param {
-	struct intel_gvt *gvt;
-	int ring_id;
-};
-
-static int workload_thread(void *priv)
+static int workload_thread(void *arg)
 {
-	struct workload_thread_param *p = (struct workload_thread_param *)priv;
-	struct intel_gvt *gvt = p->gvt;
-	int ring_id = p->ring_id;
+	struct intel_engine_cs *engine = arg;
+	const bool need_force_wake = INTEL_GEN(engine->i915) >= 9;
+	struct intel_gvt *gvt = engine->i915->gvt;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
 	struct intel_vgpu *vgpu = NULL;
 	int ret;
-	bool need_force_wake = (INTEL_GEN(gvt->dev_priv) >= 9);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
-	struct intel_runtime_pm *rpm = &gvt->dev_priv->runtime_pm;
-
-	kfree(p);
 
-	gvt_dbg_core("workload thread for ring %d started\n", ring_id);
+	gvt_dbg_core("workload thread for ring %s started\n", engine->name);
 
 	while (!kthread_should_stop()) {
-		add_wait_queue(&scheduler->waitq[ring_id], &wait);
+		intel_wakeref_t wakeref;
+
+		add_wait_queue(&scheduler->waitq[engine->id], &wait);
 		do {
-			workload = pick_next_workload(gvt, ring_id);
+			workload = pick_next_workload(gvt, engine);
 			if (workload)
 				break;
 			wait_woken(&wait, TASK_INTERRUPTIBLE,
 				   MAX_SCHEDULE_TIMEOUT);
 		} while (!kthread_should_stop());
-		remove_wait_queue(&scheduler->waitq[ring_id], &wait);
+		remove_wait_queue(&scheduler->waitq[engine->id], &wait);
 
 		if (!workload)
 			break;
 
-		gvt_dbg_sched("ring id %d next workload %p vgpu %d\n",
-				workload->ring_id, workload,
-				workload->vgpu->id);
+		gvt_dbg_sched("ring %s next workload %p vgpu %d\n",
+			      engine->name, workload,
+			      workload->vgpu->id);
 
-		intel_runtime_pm_get(rpm);
+		wakeref = intel_runtime_pm_get(engine->uncore->rpm);
 
-		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
-				workload->ring_id, workload);
+		gvt_dbg_sched("ring %s will dispatch workload %p\n",
+			      engine->name, workload);
 
 		if (need_force_wake)
-			intel_uncore_forcewake_get(&gvt->dev_priv->uncore,
-					FORCEWAKE_ALL);
+			intel_uncore_forcewake_get(engine->uncore,
+						   FORCEWAKE_ALL);
 		/*
 		 * Update the vReg of the vGPU which submitted this
 		 * workload. The vGPU may use these registers for checking
@@ -1030,21 +1022,21 @@ static int workload_thread(void *priv)
 			goto complete;
 		}
 
-		gvt_dbg_sched("ring id %d wait workload %p\n",
-				workload->ring_id, workload);
+		gvt_dbg_sched("ring %s wait workload %p\n",
+			      engine->name, workload);
 		i915_request_wait(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",
-				workload, workload->status);
+			      workload, workload->status);
 
-		complete_current_workload(gvt, ring_id);
+		complete_current_workload(gvt, engine->id);
 
 		if (need_force_wake)
-			intel_uncore_forcewake_put(&gvt->dev_priv->uncore,
-					FORCEWAKE_ALL);
+			intel_uncore_forcewake_put(engine->uncore,
+						   FORCEWAKE_ALL);
 
-		intel_runtime_pm_put_unchecked(rpm);
+		intel_runtime_pm_put(engine->uncore->rpm, wakeref);
 		if (ret && (vgpu_is_vm_unhealthy(ret)))
 			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
 	}
@@ -1084,7 +1076,6 @@ void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
 int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	struct workload_thread_param *param = NULL;
 	struct intel_engine_cs *engine;
 	enum intel_engine_id i;
 	int ret;
@@ -1096,17 +1087,8 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 	for_each_engine(engine, gvt->dev_priv, i) {
 		init_waitqueue_head(&scheduler->waitq[i]);
 
-		param = kzalloc(sizeof(*param), GFP_KERNEL);
-		if (!param) {
-			ret = -ENOMEM;
-			goto err;
-		}
-
-		param->gvt = gvt;
-		param->ring_id = i;
-
-		scheduler->thread[i] = kthread_run(workload_thread, param,
-			"gvt workload %d", i);
+		scheduler->thread[i] = kthread_run(workload_thread, engine,
+						   "gvt:%s", engine->name);
 		if (IS_ERR(scheduler->thread[i])) {
 			gvt_err("fail to create workload thread\n");
 			ret = PTR_ERR(scheduler->thread[i]);
@@ -1118,11 +1100,11 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 		atomic_notifier_chain_register(&engine->context_status_notifier,
 					&gvt->shadow_ctx_notifier_block[i]);
 	}
+
 	return 0;
+
 err:
 	intel_gvt_clean_workload_scheduler(gvt);
-	kfree(param);
-	param = NULL;
 	return ret;
 }
 
@@ -1230,7 +1212,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 	i915_context_ppgtt_root_save(s, ppgtt);
 
-	for_each_engine(engine, i915, i) {
+	for_each_engine(engine, &i915->gt, i) {
 		struct intel_context *ce;
 
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
@@ -1282,7 +1264,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 out_shadow_ctx:
 	i915_context_ppgtt_root_restore(s, ppgtt);
-	for_each_engine(engine, i915, i) {
+	for_each_engine(engine, &i915->gt, i) {
 		if (IS_ERR(s->shadow[i]))
 			break;
 
@@ -1443,7 +1425,7 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 /**
  * intel_vgpu_create_workload - create a vGPU workload
  * @vgpu: a vGPU
- * @ring_id: ring index
+ * @engine: the engine
  * @desc: a guest context descriptor
  *
  * This function is called when creating a vGPU workload.
@@ -1454,14 +1436,14 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
  *
  */
 struct intel_vgpu_workload *
-intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
+intel_vgpu_create_workload(struct intel_vgpu *vgpu,
+			   const struct intel_engine_cs *engine,
 			   struct execlist_ctx_descriptor_format *desc)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct list_head *q = workload_q_head(vgpu, ring_id);
+	struct list_head *q = workload_q_head(vgpu, engine);
 	struct intel_vgpu_workload *last_workload = NULL;
 	struct intel_vgpu_workload *workload = NULL;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	u64 ring_context_gpa;
 	u32 head, tail, start, ctl, ctx_ctl, per_ctx, indirect_ctx;
 	u32 guest_head;
@@ -1488,10 +1470,10 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	list_for_each_entry_reverse(last_workload, q, list) {
 
 		if (same_context(&last_workload->ctx_desc, desc)) {
-			gvt_dbg_el("ring id %d cur workload == last\n",
-					ring_id);
+			gvt_dbg_el("ring %s cur workload == last\n",
+				   engine->name);
 			gvt_dbg_el("ctx head %x real head %lx\n", head,
-					last_workload->rb_tail);
+				   last_workload->rb_tail);
 			/*
 			 * cannot use guest context head pointer here,
 			 * as it might not be updated at this time
@@ -1501,7 +1483,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 		}
 	}
 
-	gvt_dbg_el("ring id %d begin a new workload\n", ring_id);
+	gvt_dbg_el("ring %s begin a new workload\n", engine->name);
 
 	/* record some ring buffer register values for scan and shadow */
 	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
@@ -1521,7 +1503,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	if (IS_ERR(workload))
 		return workload;
 
-	workload->ring_id = ring_id;
+	workload->engine = engine;
 	workload->ctx_desc = *desc;
 	workload->ring_context_gpa = ring_context_gpa;
 	workload->rb_head = head;
@@ -1530,7 +1512,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	workload->rb_start = start;
 	workload->rb_ctl = ctl;
 
-	if (ring_id == RCS0) {
+	if (engine->id == RCS0) {
 		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
 			RING_CTX_OFF(bb_per_ctx_ptr.val), &per_ctx, 4);
 		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
@@ -1568,8 +1550,8 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 		}
 	}
 
-	gvt_dbg_el("workload %p ring id %d head %x tail %x start %x ctl %x\n",
-			workload, ring_id, head, tail, start, ctl);
+	gvt_dbg_el("workload %p ring %s head %x tail %x start %x ctl %x\n",
+		   workload, engine->name, head, tail, start, ctl);
 
 	ret = prepare_mm(workload);
 	if (ret) {
@@ -1580,10 +1562,11 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	/* Only scan and shadow the first workload in the queue
 	 * as there is only one pre-allocated buf-obj for shadow.
 	 */
-	if (list_empty(workload_q_head(vgpu, ring_id))) {
-		intel_runtime_pm_get(&dev_priv->runtime_pm);
-		ret = intel_gvt_scan_and_shadow_workload(workload);
-		intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
+	if (list_empty(q)) {
+		intel_wakeref_t wakeref;
+
+		with_intel_runtime_pm(engine->gt->uncore->rpm, wakeref)
+			ret = intel_gvt_scan_and_shadow_workload(workload);
 	}
 
 	if (ret) {
@@ -1603,7 +1586,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 void intel_vgpu_queue_workload(struct intel_vgpu_workload *workload)
 {
 	list_add_tail(&workload->list,
-		workload_q_head(workload->vgpu, workload->ring_id));
+		      workload_q_head(workload->vgpu, workload->engine));
 	intel_gvt_kick_schedule(workload->vgpu->gvt);
-	wake_up(&workload->vgpu->gvt->scheduler.waitq[workload->ring_id]);
+	wake_up(&workload->vgpu->gvt->scheduler.waitq[workload->engine->id]);
 }

commit cfdd30b4100bdcec2b3d2c013b0f33715bb76e59
Merge: 7a0a6ee73150 a8bb49b64c4f
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Wed Feb 26 14:58:14 2020 -0800

    Merge tag 'gvt-next-2020-02-26' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2020-02-26
    
    - Enable VFIO edid for all platform (Zhenyu)
    - Code cleanup for attr group and unused vblank complete (Zhenyu, Julian)
    - Make gvt oblivious of kvmgt data structures (Julian)
    - Make WARN* drm specific (Pankaj)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200226103840.GD10413@zhen-hp.sh.intel.com

commit 12d5861973c70fb9a890d81d051de1cb1886eeee
Author: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
Date:   Thu Feb 20 22:25:07 2020 +0530

    drm/i915/gvt: Make WARN* drm specific where vgpu ptr is available
    
    Drm specific drm_WARN* calls include device information in the
    backtrace, so we know what device the warnings originate from.
    
    Covert all the calls of WARN* with device specific drm_WARN*
    variants in functions where drm_device struct pointer is readily
    available.
    
    The conversion was done automatically with below coccinelle semantic
    patch. checkpatch errors/warnings are fixed manually.
    
    @@
    identifier func, T;
    @@
    func(struct intel_vgpu *T,...) {
    +struct drm_i915_private *i915 = T->gvt->dev_priv;
    <+...
    (
    -WARN(
    +drm_WARN(&i915->drm,
    ...)
    |
    -WARN_ON(
    +drm_WARN_ON(&i915->drm,
    ...)
    |
    -WARN_ONCE(
    +drm_WARN_ONCE(&i915->drm,
    ...)
    |
    -WARN_ON_ONCE(
    +drm_WARN_ON_ONCE(&i915->drm,
    ...)
    )
    ...+>
    
    }
    
    Signed-off-by: Pankaj Bharadiya <pankaj.laxminarayan.bharadiya@intel.com>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20200220165507.16823-9-pankaj.laxminarayan.bharadiya@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 685d1e04a5ff..cc89afd7b5f1 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1309,6 +1309,7 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 				     intel_engine_mask_t engine_mask,
 				     unsigned int interface)
 {
+	struct drm_i915_private *i915 = vgpu->gvt->dev_priv;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	const struct intel_vgpu_submission_ops *ops[] = {
 		[INTEL_VGPU_EXECLIST_SUBMISSION] =
@@ -1316,10 +1317,11 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 	};
 	int ret;
 
-	if (WARN_ON(interface >= ARRAY_SIZE(ops)))
+	if (drm_WARN_ON(&i915->drm, interface >= ARRAY_SIZE(ops)))
 		return -EINVAL;
 
-	if (WARN_ON(interface == 0 && engine_mask != ALL_ENGINES))
+	if (drm_WARN_ON(&i915->drm,
+			interface == 0 && engine_mask != ALL_ENGINES))
 		return -EINVAL;
 
 	if (s->active)

commit 202c98e7169248480d97a79416cbfb1154903d99
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Tue Feb 18 14:33:24 2020 -0800

    drm/i915/guc: Apply new uC status tracking to GuC submission as well
    
    To be able to differentiate the before and after of our commitment to
    GuC submission, which will be used in follow-up patches to early set-up
    the submission structures.
    
    v2: move functions to guc_submission.h (Michal)
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Reviewed-by: John Harrison <John.C.Harrison@Intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200218223327.11058-7-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5fe00ee6bd1b..e8c0885df978 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1247,7 +1247,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		intel_context_set_single_submission(ce);
 
 		/* Max ring buffer size */
-		if (!intel_uc_uses_guc_submission(&engine->gt->uc)) {
+		if (!intel_uc_wants_guc_submission(&engine->gt->uc)) {
 			const unsigned int ring_size = 512 * SZ_4K;
 
 			ce->ring = __intel_context_ring_size(ring_size);

commit 065273f76dd03b2a5434d95f90247effe066f275
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Tue Feb 18 14:33:20 2020 -0800

    drm/i915/guc: Kill USES_GUC_SUBMISSION macro
    
    use intel_uc_uses_guc_submission() directly instead, to be consistent in
    the way we check what we want to do with the GuC.
    
    v2: do not go through ctx->vm->gt, use i915->gt instead
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Cc: Matthew Brost <matthew.brost@intel.com>
    Reviewed-by: Michal Wajdeczko <michal.wajdeczko@intel.com> #v1
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200218223327.11058-3-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 685d1e04a5ff..5fe00ee6bd1b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1246,7 +1246,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		ce->vm = i915_vm_get(&ppgtt->vm);
 		intel_context_set_single_submission(ce);
 
-		if (!USES_GUC_SUBMISSION(i915)) { /* Max ring buffer size */
+		/* Max ring buffer size */
+		if (!intel_uc_uses_guc_submission(&engine->gt->uc)) {
 			const unsigned int ring_size = 512 * SZ_4K;
 
 			ce->ring = __intel_context_ring_size(ring_size);

commit 2c86e55d2ab55b036d901384eae43fdae4487459
Author: Matthew Auld <matthew.auld@intel.com>
Date:   Tue Jan 7 13:40:09 2020 +0000

    drm/i915/gtt: split up i915_gem_gtt
    
    Attempt to split i915_gem_gtt.[ch] into more manageable chunks.
    
    Suggested-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Matthew Auld <matthew.auld@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200107134009.3255354-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b3299f88e24e..685d1e04a5ff 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1224,7 +1224,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	enum intel_engine_id i;
 	int ret;
 
-	ppgtt = i915_ppgtt_create(i915);
+	ppgtt = i915_ppgtt_create(&i915->gt);
 	if (IS_ERR(ppgtt))
 		return PTR_ERR(ppgtt);
 

commit e6ba76480299a0d77c51d846f7467b1673aad25b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Dec 21 16:03:24 2019 +0000

    drm/i915: Remove i915->kernel_context
    
    Allocate only an internal intel_context for the kernel_context, forgoing
    a global GEM context for internal use as we only require a separate
    address space (for our own protection).
    
    Now having weaned GT from requiring ce->gem_context, we can stop
    referencing it entirely. This also means we no longer have to create random
    and unnecessary GEM contexts for internal use.
    
    GEM contexts are now entirely for tracking GEM clients, and intel_context
    the execution environment on the GPU.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Acked-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191221160324.1073045-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 228c66534e21..b3299f88e24e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -35,12 +35,12 @@
 
 #include <linux/kthread.h>
 
-#include "gem/i915_gem_context.h"
 #include "gem/i915_gem_pm.h"
 #include "gt/intel_context.h"
 #include "gt/intel_ring.h"
 
 #include "i915_drv.h"
+#include "i915_gem_gtt.h"
 #include "gvt.h"
 
 #define RING_CTX_OFF(x) \
@@ -1220,16 +1220,14 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	struct drm_i915_private *i915 = vgpu->gvt->dev_priv;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct intel_engine_cs *engine;
-	struct i915_gem_context *ctx;
 	struct i915_ppgtt *ppgtt;
 	enum intel_engine_id i;
 	int ret;
 
-	ctx = i915_gem_context_create_kernel(i915, I915_PRIORITY_MAX);
-	if (IS_ERR(ctx))
-		return PTR_ERR(ctx);
+	ppgtt = i915_ppgtt_create(i915);
+	if (IS_ERR(ppgtt))
+		return PTR_ERR(ppgtt);
 
-	ppgtt = i915_vm_to_ppgtt(i915_gem_context_get_vm_rcu(ctx));
 	i915_context_ppgtt_root_save(s, ppgtt);
 
 	for_each_engine(engine, i915, i) {
@@ -1238,12 +1236,14 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
 		s->shadow[i] = ERR_PTR(-EINVAL);
 
-		ce = intel_context_create(ctx, engine);
+		ce = intel_context_create(engine);
 		if (IS_ERR(ce)) {
 			ret = PTR_ERR(ce);
 			goto out_shadow_ctx;
 		}
 
+		i915_vm_put(ce->vm);
+		ce->vm = i915_vm_get(&ppgtt->vm);
 		intel_context_set_single_submission(ce);
 
 		if (!USES_GUC_SUBMISSION(i915)) { /* Max ring buffer size */
@@ -1278,7 +1278,6 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
 	i915_vm_put(&ppgtt->vm);
-	i915_gem_context_put(ctx);
 	return 0;
 
 out_shadow_ctx:
@@ -1291,7 +1290,6 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		intel_context_put(s->shadow[i]);
 	}
 	i915_vm_put(&ppgtt->vm);
-	i915_gem_context_put(ctx);
 	return ret;
 }
 

commit 9f3ccd40acf4a348aab4eda140cdb4d2f1f773b4
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Dec 20 10:12:29 2019 +0000

    drm/i915: Drop GEM context as a direct link from i915_request
    
    Keep the intel_context as being the primary state for i915_request, with
    the GEM context a backpointer from the low level state for the rarer
    cases we need client information. Our goal is to remove such references
    to clients from the backend, and leave the HW submission agnostic to
    client interfaces and self-contained.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191220101230.256839-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5b2a7d072ec9..228c66534e21 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -59,7 +59,7 @@ static void set_context_pdp_root_pointer(
 static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 {
 	struct drm_i915_gem_object *ctx_obj =
-		workload->req->hw_context->state->obj;
+		workload->req->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 
@@ -130,7 +130,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	struct intel_gvt *gvt = vgpu->gvt;
 	int ring_id = workload->ring_id;
 	struct drm_i915_gem_object *ctx_obj =
-		workload->req->hw_context->state->obj;
+		workload->req->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *dst;
@@ -205,9 +205,9 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
-static inline bool is_gvt_request(struct i915_request *req)
+static inline bool is_gvt_request(struct i915_request *rq)
 {
-	return i915_gem_context_force_single_submission(req->gem_context);
+	return intel_context_force_single_submission(rq->context);
 }
 
 static void save_ring_hw_state(struct intel_vgpu *vgpu, int ring_id)
@@ -307,7 +307,7 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	u32 *cs;
 	int err;
 
-	if (IS_GEN(req->i915, 9) && is_inhibit_context(req->hw_context))
+	if (IS_GEN(req->i915, 9) && is_inhibit_context(req->context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/*
@@ -363,11 +363,10 @@ static void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 }
 
 static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
-					  struct i915_gem_context *ctx)
+					  struct intel_context *ce)
 {
 	struct intel_vgpu_mm *mm = workload->shadow_mm;
-	struct i915_ppgtt *ppgtt =
-		i915_vm_to_ppgtt(i915_gem_context_get_vm_rcu(ctx));
+	struct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(ce->vm);
 	int i = 0;
 
 	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
@@ -380,8 +379,6 @@ static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 			px_dma(pd) = mm->ppgtt_mm.shadow_pdps[i];
 		}
 	}
-
-	i915_vm_put(&ppgtt->vm);
 }
 
 static int
@@ -529,7 +526,7 @@ static void update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 		container_of(wa_ctx, struct intel_vgpu_workload, wa_ctx);
 	struct i915_request *rq = workload->req;
 	struct execlist_ring_context *shadow_ring_context =
-		(struct execlist_ring_context *)rq->hw_context->lrc_reg_state;
+		(struct execlist_ring_context *)rq->context->lrc_reg_state;
 
 	shadow_ring_context->bb_per_ctx_ptr.val =
 		(shadow_ring_context->bb_per_ctx_ptr.val &
@@ -628,7 +625,7 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 
 	update_shadow_pdps(workload);
 
-	set_context_ppgtt_from_shadow(workload, s->shadow[ring]->gem_context);
+	set_context_ppgtt_from_shadow(workload, s->shadow[ring]);
 
 	ret = intel_vgpu_sync_oos_pages(workload->vgpu);
 	if (ret) {
@@ -787,7 +784,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	struct i915_request *rq = workload->req;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
-	struct drm_i915_gem_object *ctx_obj = rq->hw_context->state->obj;
+	struct drm_i915_gem_object *ctx_obj = rq->context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *src;
@@ -1232,8 +1229,6 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	if (IS_ERR(ctx))
 		return PTR_ERR(ctx);
 
-	i915_gem_context_set_force_single_submission(ctx);
-
 	ppgtt = i915_vm_to_ppgtt(i915_gem_context_get_vm_rcu(ctx));
 	i915_context_ppgtt_root_save(s, ppgtt);
 
@@ -1249,6 +1244,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 			goto out_shadow_ctx;
 		}
 
+		intel_context_set_single_submission(ce);
+
 		if (!USES_GUC_SUBMISSION(i915)) { /* Max ring buffer size */
 			const unsigned int ring_size = 512 * SZ_4K;
 

commit 9f37940756b1b7f56bcfb53de17b78f55fb18e48
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Wed Oct 30 18:30:39 2019 -0700

    drm/i915: drop lrc header page
    
    Recent GuC binaries (including all the ones we're currently using)
    don't require this shared area anymore, having moved the relevant
    entries into the stage pool instead. i915 itself doesn't write
    anything into it either, so we can safely drop it.
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Cc: Matthew Brost <matthew.brost@intel.com>
    Acked-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Brost <matthew.brost@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191031013040.25803-1-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 377811f8853f..5b2a7d072ec9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -195,7 +195,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			return -EFAULT;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
+		page = i915_gem_object_get_page(ctx_obj, i);
 		dst = kmap(page);
 		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
 				I915_GTT_PAGE_SIZE);
@@ -835,7 +835,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			return;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
+		page = i915_gem_object_get_page(ctx_obj, i);
 		src = kmap(page);
 		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
 				I915_GTT_PAGE_SIZE);

commit 2871ea85c119e6fb1127b30f0061436b285d3a2c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 24 11:03:44 2019 +0100

    drm/i915/gt: Split intel_ring_submission
    
    Split the legacy submission backend from the common CS ring buffer
    handling.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191024100344.5041-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a5b942ee3ceb..377811f8853f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -38,6 +38,7 @@
 #include "gem/i915_gem_context.h"
 #include "gem/i915_gem_pm.h"
 #include "gt/intel_context.h"
+#include "gt/intel_ring.h"
 
 #include "i915_drv.h"
 #include "gvt.h"

commit 8eb8e322ec07392e8c8008437216c38c310ff6c7
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed Oct 23 17:43:27 2019 +0800

    drm/i915/gvt: fix dead locking in early workload shadow
    
    As early workload scan and shadow happens in execlist mmio handler,
    which has already taken vgpu_lock. So remove extra lock taking here.
    
    Fixes: 952f89f098c7 ("drm/i915/gvt: Wean off struct_mutex")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 36bb7639e82f..a5b942ee3ceb 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1584,9 +1584,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	 */
 	if (list_empty(workload_q_head(vgpu, ring_id))) {
 		intel_runtime_pm_get(&dev_priv->runtime_pm);
-		mutex_lock(&vgpu->vgpu_lock);
 		ret = intel_gvt_scan_and_shadow_workload(workload);
-		mutex_unlock(&vgpu->vgpu_lock);
 		intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
 	}
 

commit 952f89f098c75ae4147fc440aaa3b9a209240cea
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 16 19:39:01 2019 +0100

    drm/i915/gvt: Wean off struct_mutex
    
    Use the local vgpu_lock while preparing workloads to avoid taking the
    obsolete i915->drm.struct_mutex
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191016183902.13614-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9ebb2534558b..36bb7639e82f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -415,10 +415,9 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	int ret;
 
-	lockdep_assert_held(&dev_priv->drm.struct_mutex);
+	lockdep_assert_held(&vgpu->vgpu_lock);
 
 	if (workload->shadow)
 		return 0;
@@ -580,8 +579,6 @@ static void update_vreg_in_ctx(struct intel_vgpu_workload *workload)
 
 static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 {
-	struct intel_vgpu *vgpu = workload->vgpu;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct intel_vgpu_shadow_bb *bb, *pos;
 
 	if (list_empty(&workload->shadow_bb))
@@ -590,8 +587,6 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 	bb = list_first_entry(&workload->shadow_bb,
 			struct intel_vgpu_shadow_bb, list);
 
-	mutex_lock(&dev_priv->drm.struct_mutex);
-
 	list_for_each_entry_safe(bb, pos, &workload->shadow_bb, list) {
 		if (bb->obj) {
 			if (bb->accessing)
@@ -609,8 +604,6 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 		list_del(&bb->list);
 		kfree(bb);
 	}
-
-	mutex_unlock(&dev_priv->drm.struct_mutex);
 }
 
 static int prepare_workload(struct intel_vgpu_workload *workload)
@@ -685,7 +678,6 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
@@ -694,7 +686,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 		ring_id, workload);
 
 	mutex_lock(&vgpu->vgpu_lock);
-	mutex_lock(&dev_priv->drm.struct_mutex);
 
 	ret = intel_gvt_workload_req_alloc(workload);
 	if (ret)
@@ -729,7 +720,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 err_req:
 	if (ret)
 		workload->status = ret;
-	mutex_unlock(&dev_priv->drm.struct_mutex);
 	mutex_unlock(&vgpu->vgpu_lock);
 	return ret;
 }
@@ -1594,9 +1584,9 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	 */
 	if (list_empty(workload_q_head(vgpu, ring_id))) {
 		intel_runtime_pm_get(&dev_priv->runtime_pm);
-		mutex_lock(&dev_priv->drm.struct_mutex);
+		mutex_lock(&vgpu->vgpu_lock);
 		ret = intel_gvt_scan_and_shadow_workload(workload);
-		mutex_unlock(&dev_priv->drm.struct_mutex);
+		mutex_unlock(&vgpu->vgpu_lock);
 		intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
 	}
 

commit a50134b1983b8860e0e74e41579cbb19a7304ca7
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Thu Oct 17 17:18:52 2019 +0100

    drm/i915: Make for_each_engine_masked work on intel_gt
    
    Medium term goal is to eliminate the i915->engine[] array and to get there
    we have recently introduced equivalent array in intel_gt. Now we need to
    migrate the code further towards this state.
    
    This next step is to eliminate usage of i915->engines[] from the
    for_each_engine_masked iterator.
    
    For this to work we also need to use engine->id as index when populating
    the gt->engine[] array and adjust the default engine set indexing to use
    engine->legacy_idx instead of assuming gt->engines[] indexing.
    
    v2:
      * Populate gt->engine[] earlier.
      * Check that we don't duplicate engine->legacy_idx
    
    v3:
      * Work around the initialization order issue between default_engines()
        and intel_engines_driver_register() which sets engine->legacy_idx for
        now. It will be fixed properly later.
    
    v4:
      * Merge with forgotten v2.5.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191017161852.8836-1-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 6850f1f40241..9ebb2534558b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -887,7 +887,7 @@ void intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,
 	intel_engine_mask_t tmp;
 
 	/* free the unsubmited workloads in the queues. */
-	for_each_engine_masked(engine, dev_priv, engine_mask, tmp) {
+	for_each_engine_masked(engine, &dev_priv->gt, engine_mask, tmp) {
 		list_for_each_entry_safe(pos, n,
 			&s->workload_q_head[engine->id], list) {
 			list_del_init(&pos->list);

commit 97ea56540ffc59dac275ccb64b8a5e457348a250
Merge: da0c9ea146cb 9445ad17109b
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Oct 8 12:54:38 2019 +1000

    Merge tag 'drm-intel-next-2019-10-07' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    UAPI Changes:
    - Never allow userptr into the mappable GGTT (Chris)
      No existing users. Avoid anyone from even trying to
      spare a deadlock scenario.
    
    Cross-subsystem Changes:
    
    Core Changes:
    
    Driver Changes:
    
    - Eliminate struct_mutex use as BKL! (Chris)
      Only used for execbuf serialisation.
    
    - Initialize DDI TC and TBT ports (D-I) on Tigerlake (Lucas)
    - Fix DKL link training for 2.7GHz and 1.62GHz (Jose)
    - Add Tigerlake DKL PHY programming sequences (Clinton)
    - Add Tigerlake Thunderbolt PLL divider values (Imre)
    
    - drm/i915: Use helpers for drm_mm_node booleans (Chris)
    - Restrict L3 remapping sysfs interface to dwords (Chris)
    - Fix audio power up sequence for gen10+ display (Kai)
    - Skip redundant execlist resubmission (Chris)
    - Only unwedge if we can reset GPU first (Chris)
    - Initialise breadcrumb lists on the virtual engine (Chris)
    - Don't rely on kernel context existing during early errors (Matt A)
    - Update Icelake+ MG_DP_MODE programming table (Clinton)
    - Update DMC firmware for Icelake (Anusha)
    - Downgrade DP MST error after unplugging TypeC cable (Srinivasan)
    - Limit MST modes based on plane size too (Ville)
    - Polish intel_tv_mode_valid() (Ville)
    - Fix g4x sprite scaling stride check with GTT remapping (Ville)
    - Don't advertize non-exisiting crtcs (Ville)
    - Clean up encoder->crtc_mask setup (Ville)
    - Use tc_port instead of port parameter to MG registers (Jose)
    - Remove static variable for aux last status (Jani)
    - Implement a better i945gm vblank irq vs. C-states workaround (Ville)
    
    - Make the object creation interface consistent (CQ)
    - Rename intel_vga_msr_write() to intel_vga_reset_io_mem() (Jani, Ville)
    - Eliminate previous drm_dbg/drm_err usage (Jani)
    - Move gmbus setup down to intel_modeset_init() (Jani)
    - Abstract all vgaarb access to intel_vga.[ch] (Jani)
    - Split out i915_switcheroo.[ch] from i915_drv.c (Jani)
    - Use intel_gt in has_reset* (Chris)
    - Eliminate return value for i915_gem_init_early (Matt A)
    - Selftest improvements (Chris)
    - Update HuC firmware header version number format (Daniele)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    From: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191007134801.GA24313@jlahtine-desk.ger.corp.intel.com

commit a4e7ccdac38ec8335d9e4e2656c1a041c77feae1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:09 2019 +0100

    drm/i915: Move context management under GEM
    
    Keep track of the GEM contexts underneath i915->gem.contexts and assign
    them their own lock for the purposes of list management.
    
    v2: Focus on lock tracking; ctx->vm is protected by ctx->mutex
    v3: Correct split with removal of logical HW ID
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-15-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7052c90d937f..f6419f0bd4fd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -365,7 +365,8 @@ static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 					  struct i915_gem_context *ctx)
 {
 	struct intel_vgpu_mm *mm = workload->shadow_mm;
-	struct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(ctx->vm);
+	struct i915_ppgtt *ppgtt =
+		i915_vm_to_ppgtt(i915_gem_context_get_vm_rcu(ctx));
 	int i = 0;
 
 	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
@@ -378,6 +379,8 @@ static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 			px_dma(pd) = mm->ppgtt_mm.shadow_pdps[i];
 		}
 	}
+
+	i915_vm_put(&ppgtt->vm);
 }
 
 static int
@@ -1213,20 +1216,18 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct intel_engine_cs *engine;
 	struct i915_gem_context *ctx;
+	struct i915_ppgtt *ppgtt;
 	enum intel_engine_id i;
 	int ret;
 
-	mutex_lock(&i915->drm.struct_mutex);
-
 	ctx = i915_gem_context_create_kernel(i915, I915_PRIORITY_MAX);
-	if (IS_ERR(ctx)) {
-		ret = PTR_ERR(ctx);
-		goto out_unlock;
-	}
+	if (IS_ERR(ctx))
+		return PTR_ERR(ctx);
 
 	i915_gem_context_set_force_single_submission(ctx);
 
-	i915_context_ppgtt_root_save(s, i915_vm_to_ppgtt(ctx->vm));
+	ppgtt = i915_vm_to_ppgtt(i915_gem_context_get_vm_rcu(ctx));
+	i915_context_ppgtt_root_save(s, ppgtt);
 
 	for_each_engine(engine, i915, i) {
 		struct intel_context *ce;
@@ -1271,12 +1272,12 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	atomic_set(&s->running_workload_num, 0);
 	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
+	i915_vm_put(&ppgtt->vm);
 	i915_gem_context_put(ctx);
-	mutex_unlock(&i915->drm.struct_mutex);
 	return 0;
 
 out_shadow_ctx:
-	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(ctx->vm));
+	i915_context_ppgtt_root_restore(s, ppgtt);
 	for_each_engine(engine, i915, i) {
 		if (IS_ERR(s->shadow[i]))
 			break;
@@ -1284,9 +1285,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		intel_context_unpin(s->shadow[i]);
 		intel_context_put(s->shadow[i]);
 	}
+	i915_vm_put(&ppgtt->vm);
 	i915_gem_context_put(ctx);
-out_unlock:
-	mutex_unlock(&i915->drm.struct_mutex);
 	return ret;
 }
 

commit b1e3177bd1d8f41e2a9cc847e56a96cdc0eefe62
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:00 2019 +0100

    drm/i915: Coordinate i915_active with its own mutex
    
    Forgo the struct_mutex serialisation for i915_active, and interpose its
    own mutex handling for active/retire.
    
    This is a multi-layered sleight-of-hand. First, we had to ensure that no
    active/retire callbacks accidentally inverted the mutex ordering rules,
    nor assumed that they were themselves serialised by struct_mutex. More
    challenging though, is the rule over updating elements of the active
    rbtree. Instead of the whole i915_active now being serialised by
    struct_mutex, allocations/rotations of the tree are serialised by the
    i915_active.mutex and individual nodes are serialised by the caller
    using the i915_timeline.mutex (we need to use nested spinlocks to
    interact with the dma_fence callback lists).
    
    The pain point here is that instead of a single mutex around execbuf, we
    now have to take a mutex for active tracker (one for each vma, context,
    etc) and a couple of spinlocks for each fence update. The improvement in
    fine grained locking allowing for multiple concurrent clients
    (eventually!) should be worth it in typical loads.
    
    v2: Add some comments that barely elucidate anything :(
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-6-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1a28e3666951..7052c90d937f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -385,11 +385,8 @@ intel_gvt_workload_req_alloc(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct i915_request *rq;
 
-	lockdep_assert_held(&dev_priv->drm.struct_mutex);
-
 	if (workload->req)
 		return 0;
 

commit 574cc4539762561d96b456dbc0544d8898bd4c6e
Merge: 3c2edc36a774 945b584c94f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 19 16:24:24 2019 -0700

    Merge tag 'drm-next-2019-09-18' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm updates from Dave Airlie:
     "This is the main pull request for 5.4-rc1 merge window. I don't think
      there is anything outstanding so next week should just be fixes, but
      we'll see if I missed anything. I landed some fixes earlier in the
      week but got delayed writing summary and sending it out, due to a mix
      of sick kid and jetlag!
    
      There are some fixes pending, but I'd rather get the main merge out of
      the way instead of delaying it longer.
    
      It's also pretty large in commit count and new amd header file size.
      The largest thing is four new amdgpu products (navi12/14, arcturus and
      renoir APU support).
    
      Otherwise it's pretty much lots of work across the board, i915 has
      started landing tigerlake support, lots of icelake fixes and lots of
      locking reworking for future gpu support, lots of header file rework
      (drmP.h is nearly gone), some old legacy hacks (DRM_WAIT_ON) have been
      put into the places they are needed.
    
      uapi:
       - content protection type property for HDCP
    
      core:
       - rework include dependencies
       - lots of drmP.h removals
       - link rate calculation robustness fix
       - make fb helper map only when required
       - add connector->DDC adapter link
       - DRM_WAIT_ON removed
       - drop DRM_AUTH usage from drivers
    
      dma-buf:
       - reservation object fence helper
    
      dma-fence:
       - shrink dma_fence struct
       - merge signal functions
       - store timestamps in dma_fence
       - selftests
    
      ttm:
       - embed drm_get_object struct into ttm_buffer_object
       - release_notify callback
    
      bridges:
       - sii902x - audio graph card support
       - tc358767 - aux data handling rework
       - ti-snd64dsi86 - debugfs support, DSI mode flags support
    
      panels:
       - Support for GiantPlus GPM940B0, Sharp LQ070Y3DG3B, Ortustech
         COM37H3M, Novatek NT39016, Sharp LS020B1DD01D, Raydium RM67191, Boe
         Himax8279d, Sharp LD-D5116Z01B
       - TI nspire, NEC NL8048HL11, LG Philips LB035Q02, Sharp LS037V7DW01,
         Sony ACX565AKM, Toppoly TD028TTEC1 Toppoly TD043MTEA1
    
      i915:
       - Initial tigerlake platform support
       - Locking simplification work, general all over refactoring.
       - Selftests
       - HDCP debug info improvements
       - DSI properties
       - Icelake display PLL fixes, colorspace fixes, bandwidth fixes, DSI
         suspend/resume
       - GuC fixes
       - Perf fixes
       - ElkhartLake enablement
       - DP MST fixes
       - GVT - command parser enhancements
    
      amdgpu:
       - add wipe memory on release flag for buffer creation
       - Navi12/14 support (may be marked experimental)
       - Arcturus support
       - Renoir APU support
       - mclk DPM for Navi
       - DC display fixes
       - Raven scatter/gather support
       - RAS support for GFX
       - Navi12 + Arcturus power features
       - GPU reset for Picasso
       - smu11 i2c controller support
    
      amdkfd:
       - navi12/14 support
       - Arcturus support
    
      radeon:
       - kexec fix
    
      nouveau:
       - improved display color management
       - detect lack of GPU power cables
    
      vmwgfx:
       - evicition priority support
       - remove unused security feature
    
      msm:
       - msm8998 display support
       - better async commit support for cursor updates
    
      etnaviv:
       - per-process address space support
       - performance counter fixes
       - softpin support
    
      mcde:
       - DCS transfers fix
    
      exynos:
       - drmP.h cleanup
    
      lima:
       - reduce logging
    
      kirin:
       - misc clenaups
    
      komeda:
       - dual-link support
       - DT memory regions
    
      hisilicon:
       - misc fixes
    
      imx:
       - IPUv3 image converter fixes
       - 32-bit RGB V4L2 pixel format support
    
      ingenic:
       - more support for panel related cases
    
      mgag200:
       - cursor support fix
    
      panfrost:
       - export GPU features register to userspace
       - gpu heap allocations
       - per-fd address space support
    
      pl111:
       - CLD pads wiring support removed from DT
    
      rockchip:
       - rework to use DRM PSR helpers
       - fix bug in VOP_WIN_GET macro
       - DSI DT binding rework
    
      sun4i:
       - improve support for color encoding and range
       - DDC enabled GPIO
    
      tinydrm:
       - rework SPI support
       - improve MIPI-DBI support
       - moved to drm/tiny
    
      vkms:
       - rework CRC tracking
    
      dw-hdmi:
       - get_eld and i2s improvements
    
      gm12u320:
       - misc fixes
    
      meson:
       - global code cleanup
       - vpu feature detect
    
      omap:
       - alpha/pixel blend mode properties
    
      rcar-du:
       - misc fixes"
    
    * tag 'drm-next-2019-09-18' of git://anongit.freedesktop.org/drm/drm: (2112 commits)
      drm/nouveau/bar/gm20b: Avoid BAR1 teardown during init
      drm/nouveau: Fix ordering between TTM and GEM release
      drm/nouveau/prime: Extend DMA reservation object lock
      drm/nouveau: Fix fallout from reservation object rework
      drm/nouveau/kms/nv50-: Don't create MSTMs for eDP connectors
      drm/i915: Use NOEVICT for first pass on attemping to pin a GGTT mmap
      drm/i915: to make vgpu ppgtt notificaiton as atomic operation
      drm/i915: Flush the existing fence before GGTT read/write
      drm/i915: Hold irq-off for the entire fake lock period
      drm/i915/gvt: update RING_START reg of vGPU when the context is submitted to i915
      drm/i915/gvt: update vgpu workload head pointer correctly
      drm/mcde: Fix DSI transfers
      drm/msm: Use the correct dma_sync calls harder
      drm/msm: remove unlikely() from WARN_ON() conditions
      drm/msm/dsi: Fix return value check for clk_get_parent
      drm/msm: add atomic traces
      drm/msm/dpu: async commit support
      drm/msm: async commit support
      drm/msm: split power control from prepare/complete_commit
      drm/msm: add kms->flush_commit()
      ...

commit 0ac072cced08390b9b31fe3c56c364c3ef2f6d6a
Merge: 578d2342ec70 4a5322560aa2
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Fri Sep 6 09:52:43 2019 -0700

    Merge tag 'gvt-next-fixes-2019-09-06' of https://github.com/intel/gvt-linux into drm-intel-next-fixes
    
    gvt-next-fixes-2019-09-06
    
    - Fix guest context head pointer update for hang (Xiaolin)
    - Fix guest context ring state for reset (Weinan)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190906054255.GC3458@zhen-hp.sh.intel.com

commit 4a5322560aa235efa84c0aa34c00e5749a0792fd
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Mon Sep 2 13:57:59 2019 +0800

    drm/i915/gvt: update RING_START reg of vGPU when the context is submitted to i915
    
    The guest may use this register to identify the running state of one
    context. Emulate it as the value in context image as if the context runs
    on the GPU hardware.
    
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index affd38238725..30807a458050 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -573,6 +573,16 @@ static int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 	return 0;
 }
 
+static void update_vreg_in_ctx(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	u32 ring_base;
+
+	ring_base = dev_priv->engine[workload->ring_id]->mmio_base;
+	vgpu_vreg_t(vgpu, RING_START(ring_base)) = workload->rb_start;
+}
+
 static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
@@ -1002,6 +1012,13 @@ static int workload_thread(void *priv)
 		if (need_force_wake)
 			intel_uncore_forcewake_get(&gvt->dev_priv->uncore,
 					FORCEWAKE_ALL);
+		/*
+		 * Update the vReg of the vGPU which submitted this
+		 * workload. The vGPU may use these registers for checking
+		 * the context state. The value comes from GPU commands
+		 * in this workload.
+		 */
+		update_vreg_in_ctx(workload);
 
 		ret = dispatch_workload(workload);
 

commit 0a3242bdb47713e09cb004a0ba4947d3edf82d8a
Author: Xiaolin Zhang <xiaolin.zhang@intel.com>
Date:   Tue Aug 27 16:39:23 2019 +0800

    drm/i915/gvt: update vgpu workload head pointer correctly
    
    when creating a vGPU workload, the guest context head pointer should
    be updated correctly by comparing with the exsiting workload in the
    guest worklod queue including the current running context.
    
    in some situation, there is a running context A and then received 2 new
    vGPU workload context B and A. in the new workload context A, it's head
    pointer should be updated with the running context A's tail.
    
    v2: walk through guest workload list in backward way.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Xiaolin Zhang <xiaolin.zhang@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7c99bbc3e2b8..affd38238725 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1371,9 +1371,6 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 #define same_context(a, b) (((a)->context_id == (b)->context_id) && \
 		((a)->lrca == (b)->lrca))
 
-#define get_last_workload(q) \
-	(list_empty(q) ? NULL : container_of(q->prev, \
-	struct intel_vgpu_workload, list))
 /**
  * intel_vgpu_create_workload - create a vGPU workload
  * @vgpu: a vGPU
@@ -1393,7 +1390,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct list_head *q = workload_q_head(vgpu, ring_id);
-	struct intel_vgpu_workload *last_workload = get_last_workload(q);
+	struct intel_vgpu_workload *last_workload = NULL;
 	struct intel_vgpu_workload *workload = NULL;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	u64 ring_context_gpa;
@@ -1416,15 +1413,20 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	head &= RB_HEAD_OFF_MASK;
 	tail &= RB_TAIL_OFF_MASK;
 
-	if (last_workload && same_context(&last_workload->ctx_desc, desc)) {
-		gvt_dbg_el("ring id %d cur workload == last\n", ring_id);
-		gvt_dbg_el("ctx head %x real head %lx\n", head,
-				last_workload->rb_tail);
-		/*
-		 * cannot use guest context head pointer here,
-		 * as it might not be updated at this time
-		 */
-		head = last_workload->rb_tail;
+	list_for_each_entry_reverse(last_workload, q, list) {
+
+		if (same_context(&last_workload->ctx_desc, desc)) {
+			gvt_dbg_el("ring id %d cur workload == last\n",
+					ring_id);
+			gvt_dbg_el("ctx head %x real head %lx\n", head,
+					last_workload->rb_tail);
+			/*
+			 * cannot use guest context head pointer here,
+			 * as it might not be updated at this time
+			 */
+			head = last_workload->rb_tail;
+			break;
+		}
 	}
 
 	gvt_dbg_el("ring id %d begin a new workload\n", ring_id);

commit 829e8def7bd7b1e58028113ee5c2877da89d8f27
Merge: 8e40983dec63 ae4530062620
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Wed Aug 21 22:47:35 2019 -0700

    Merge drm/drm-next into drm-intel-next-queued
    
    We need the rename of reservation_object to dma_resv.
    
    The solution on this merge came from linux-next:
    From: Stephen Rothwell <sfr@canb.auug.org.au>
    Date: Wed, 14 Aug 2019 12:48:39 +1000
    Subject: [PATCH] drm: fix up fallout from "dma-buf: rename reservation_object to dma_resv"
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    ---
     drivers/gpu/drm/i915/gt/intel_engine_pool.c | 8 ++++----
     3 files changed, 7 insertions(+), 7 deletions(-)
    
    diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pool.c b/drivers/gpu/drm/i915/gt/intel_engine_pool.c
    index 03d90b49584a..4cd54c569911 100644
    --- a/drivers/gpu/drm/i915/gt/intel_engine_pool.c
    +++ b/drivers/gpu/drm/i915/gt/intel_engine_pool.c
    @@ -43,12 +43,12 @@ static int pool_active(struct i915_active *ref)
     {
            struct intel_engine_pool_node *node =
                    container_of(ref, typeof(*node), active);
    -       struct reservation_object *resv = node->obj->base.resv;
    +       struct dma_resv *resv = node->obj->base.resv;
            int err;
    
    -       if (reservation_object_trylock(resv)) {
    -               reservation_object_add_excl_fence(resv, NULL);
    -               reservation_object_unlock(resv);
    +       if (dma_resv_trylock(resv)) {
    +               dma_resv_add_excl_fence(resv, NULL);
    +               dma_resv_unlock(resv);
            }
    
            err = i915_gem_object_pin_pages(node->obj);
    
    which is a simplified version from a previous one which had:
    Reviewed-by: Christian König <christian.koenig@amd.com>
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

commit 48ae397b6b935c6733f15476c338df27eac9293c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 9 19:25:17 2019 +0100

    drm/i915: Push the ring creation flags to the backend
    
    Push the ring creation flags from the outer GEM context to the inner
    intel_context to avoid an unsightly back-reference from inside the
    backend.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190809182518.20486-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 4cb3c71327fe..dfb64a6ec838 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1227,8 +1227,6 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	}
 
 	i915_gem_context_set_force_single_submission(ctx);
-	if (!USES_GUC_SUBMISSION(i915))
-		ctx->ring_size = 512 * PAGE_SIZE; /* Max ring buffer size */
 
 	i915_context_ppgtt_root_save(s, i915_vm_to_ppgtt(ctx->vm));
 
@@ -1244,6 +1242,12 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 			goto out_shadow_ctx;
 		}
 
+		if (!USES_GUC_SUBMISSION(i915)) { /* Max ring buffer size */
+			const unsigned int ring_size = 512 * SZ_4K;
+
+			ce->ring = __intel_context_ring_size(ring_size);
+		}
+
 		ret = intel_context_pin(ce);
 		intel_context_put(ce);
 		if (ret)

commit 72e277759340dacdf3d68b0b2b4eae267e601d55
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 9 19:25:15 2019 +0100

    drm/i915: Remove i915_gem_context_create_gvt()
    
    As we are phasing out using the GEM context for internal clients that
    need to manipulate logical context state directly, remove the
    constructor for the GVT context. We are not using it for anything other
    than default setup and allocation of an i915_ppgtt.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190809182518.20486-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0a0a17bea540..4cb3c71327fe 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1211,19 +1211,28 @@ i915_context_ppgtt_root_save(struct intel_vgpu_submission *s,
  */
 int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
+	struct drm_i915_private *i915 = vgpu->gvt->dev_priv;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct intel_engine_cs *engine;
 	struct i915_gem_context *ctx;
 	enum intel_engine_id i;
 	int ret;
 
-	ctx = i915_gem_context_create_gvt(&vgpu->gvt->dev_priv->drm);
-	if (IS_ERR(ctx))
-		return PTR_ERR(ctx);
+	mutex_lock(&i915->drm.struct_mutex);
+
+	ctx = i915_gem_context_create_kernel(i915, I915_PRIORITY_MAX);
+	if (IS_ERR(ctx)) {
+		ret = PTR_ERR(ctx);
+		goto out_unlock;
+	}
+
+	i915_gem_context_set_force_single_submission(ctx);
+	if (!USES_GUC_SUBMISSION(i915))
+		ctx->ring_size = 512 * PAGE_SIZE; /* Max ring buffer size */
 
 	i915_context_ppgtt_root_save(s, i915_vm_to_ppgtt(ctx->vm));
 
-	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
+	for_each_engine(engine, i915, i) {
 		struct intel_context *ce;
 
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
@@ -1261,11 +1270,12 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
 	i915_gem_context_put(ctx);
+	mutex_unlock(&i915->drm.struct_mutex);
 	return 0;
 
 out_shadow_ctx:
 	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(ctx->vm));
-	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
+	for_each_engine(engine, i915, i) {
 		if (IS_ERR(s->shadow[i]))
 			break;
 
@@ -1273,6 +1283,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		intel_context_put(s->shadow[i]);
 	}
 	i915_gem_context_put(ctx);
+out_unlock:
+	mutex_unlock(&i915->drm.struct_mutex);
 	return ret;
 }
 

commit eac4471d0882da14be652ef0a55b914145fab15a
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Aug 8 13:32:36 2019 +0300

    drm/i915: Use after free in error path in intel_vgpu_create_workload()
    
    We can't free "workload" until after the printk or it's a use after
    free.
    
    Fixes: 2089a76ade90 ("drm/i915/gvt: Checking workload's gma earlier")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9f3fd7d96a69..75baff657e43 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1528,9 +1528,9 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 			if (!intel_gvt_ggtt_validate_range(vgpu,
 				workload->wa_ctx.indirect_ctx.guest_gma,
 				workload->wa_ctx.indirect_ctx.size)) {
-				kmem_cache_free(s->workloads, workload);
 				gvt_vgpu_err("invalid wa_ctx at: 0x%lx\n",
 				    workload->wa_ctx.indirect_ctx.guest_gma);
+				kmem_cache_free(s->workloads, workload);
 				return ERR_PTR(-EINVAL);
 			}
 		}
@@ -1542,9 +1542,9 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 			if (!intel_gvt_ggtt_validate_range(vgpu,
 				workload->wa_ctx.per_ctx.guest_gma,
 				CACHELINE_BYTES)) {
-				kmem_cache_free(s->workloads, workload);
 				gvt_vgpu_err("invalid per_ctx at: 0x%lx\n",
 					workload->wa_ctx.per_ctx.guest_gma);
+				kmem_cache_free(s->workloads, workload);
 				return ERR_PTR(-EINVAL);
 			}
 		}

commit 387758298bfdec9fbb74406207eb648bb6391670
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 8 12:06:11 2019 +0100

    drm/i915: Allocate kernel_contexts directly
    
    Ignore the central i915->kernel_context for allocating an engine, as
    that GEM context is being phased out. For internal clients, we just need
    the per-engine logical state, so allocate it at the point of use.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190808110612.23539-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 431fd427c49c..0a0a17bea540 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1229,7 +1229,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
 		s->shadow[i] = ERR_PTR(-EINVAL);
 
-		ce = i915_gem_context_get_engine(ctx, i);
+		ce = intel_context_create(ctx, engine);
 		if (IS_ERR(ce)) {
 			ret = PTR_ERR(ce);
 			goto out_shadow_ctx;
@@ -1270,6 +1270,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 			break;
 
 		intel_context_unpin(s->shadow[i]);
+		intel_context_put(s->shadow[i]);
 	}
 	i915_gem_context_put(ctx);
 	return ret;

commit a37f08a882b01a6e86a07512a082b14d20ee0773
Author: Umesh Nerlige Ramappa <umesh.nerlige.ramappa@intel.com>
Date:   Tue Aug 6 16:30:02 2019 -0700

    drm/i915/perf: Refactor oa object to better manage resources
    
    The oa object manages the oa buffer and must be allocated when the user
    intends to read performance counter snapshots. This can be achieved by
    making the oa object part of the stream object which is allocated when a
    stream is opened by the user.
    
    Attributes in the oa object that are gen-specific are moved to the perf
    object so that they can be initialized on driver load.
    
    The split provides a better separation of the objects used in perf
    implementation of i915 driver so that resources are allocated and
    initialized only when needed.
    
    v2: Fix checkpatch warnings
    v3: Addressed Lionel's review comment
    v4: Rebase
    v5: Fix rebase/merge issue with ratelimit_state_init
    
    Signed-off-by: Umesh Nerlige Ramappa <umesh.nerlige.ramappa@intel.com>
    Reviewed-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190806233002.984-1-umesh.nerlige.ramappa@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 4c018fb1359c..431fd427c49c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -84,8 +84,8 @@ static void sr_oa_regs(struct intel_vgpu_workload *workload,
 		u32 *reg_state, bool save)
 {
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
-	u32 ctx_oactxctrl = dev_priv->perf.oa.ctx_oactxctrl_offset;
-	u32 ctx_flexeu0 = dev_priv->perf.oa.ctx_flexeu0_offset;
+	u32 ctx_oactxctrl = dev_priv->perf.ctx_oactxctrl_offset;
+	u32 ctx_flexeu0 = dev_priv->perf.ctx_flexeu0_offset;
 	int i = 0;
 	u32 flex_mmio[] = {
 		i915_mmio_reg_offset(EU_PERF_CNTL0),

commit dce14e36aea23183ccd315fbc6b0fca027bf73f5
Merge: e21a712a9685 e0e712fe42ef
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Aug 6 12:41:39 2019 +1000

    Merge tag 'drm-intel-next-2019-07-30' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    - More changes on simplifying locking mechanisms (Chris)
    - Selftests fixes and improvements (Chris)
    - More work around engine tracking for better handling (Chris, Tvrtko)
    - HDCP debug and info improvements (Ram, Ashuman)
    - Add DSI properties (Vandita)
    - Rework on sdvo support for better debuggability before fixing bugs (Ville)
    - Display PLLs fixes and improvements, specially targeting Ice Lake (Imre, Matt, Ville)
    - Perf fixes and improvements (Lionel)
    - Enumerate scratch buffers (Lionel)
    - Add infra to hold off preemption on a request (Lionel)
    - Ice Lake color space fixes (Uma)
    - Type-C fixes and improvements (Lucas)
    - Fix and improvements around workarounds (Chris, John, Tvrtko)
    - GuC related fixes and improvements (Chris, Daniele, Michal, Tvrtko)
    - Fix on VLV/CHV display power domain (Ville)
    - Improvements around Watermark (Ville)
    - Favor intel_ types on intel_atomic functions (Ville)
    - Don’t pass stack garbage to pcode (Ville)
    - Improve display tracepoints (Steven)
    - Don’t overestimate 4:2:0 link symbol clock (Ville)
    - Add support for 4th pipe and transcoder (Lucas)
    - Introduce initial support for Tiger Lake platform (Daniele, Lucas, Mahesh, Jose, Imre, Mika, Vandita, Rodrigo, Michel)
    - PPGTT allocation simplification (Chris)
    - Standardize function names and suffixes to make clean, symmetric and let checkpatch happy (Janusz)
    - Skip SINK_COUNT read on CH7511 (Ville)
    - Fix on kernel documentation (Chris, Michal)
    - Add modular FIA (Anusha, Lucas)
    - Fix EHL display (Matt, Vivek)
    - Enable hotplug retry (Imre, Jose)
    - Disable preemption under GVT (Chris)
    - OA; Reconfigure context on the fly (Chris)
    - Fixes and improvements around engine reset. (Chris)
    - Small clean up on display pipe fault mask (Ville)
    - Make sure cdclk is high enough for DP audio on VLV/CHV (Ville)
    - Drop some wmb() and improve pwrite flush (Chris)
    - Fix critical PSR regression (DK)
    - Remove unused variables (YueHaibing)
    - Use dev_get_drvdata for simplification (Chunhong)
    - Use upstream version of header tests (Jani)
    
    drm-intel-next-2019-07-08:
    - Signal fence completion from i915_request_wait (Chris)
    - Fixes and improvements around rings pin/unpin (Chris)
    - Display uncore prep patches (Daniele)
    - Execlists preemption improvements (Chris)
    - Selftests fixes and improvements (Chris)
    - More Elkhartlake enabling work (Vandita, Jose, Matt, Vivek)
    - Defer address space cleanup to an RCU worker (Chris)
    - Implicit dev_priv removal and GT compartmentalization and other related follow-ups (Tvrtko, Chris)
    - Prevent dereference of engine before NULL check in error capture (Chris)
    - GuC related fixes (Daniele, Robert)
    - Many changes on active tracking, timelines and locking mechanisms (Chris)
    - Disable SAMPLER_STATE prefetching on Gen11 (HW W/a) (Kenneth)
    - I915_perf fixes (Lionel)
    - Add Ice Lake PCI ID (Mika)
    - eDP backlight fix (Lee)
    - Fix various gen2 tracepoints (Ville)
    - Some irq vfunc clean-up and improvements (Ville)
    - Move OA files to separated folder (Michal)
    - Display self contained headers clean-up (Jani)
    - Preparation for 4th pile (Lucas)
    - Move atomic commit, watermark and other places to use more intel_crtc_state (Maarten)
    - Many Ice Lake Type C and Thunderbolt fixes (Imre)
    - Fix some Ice Lake hw w/a whitelist regs (Lionel)
    - Fix memleak in runtime wakeref tracking (Mika)
    - Remove unused Private PPAT manager (Michal)
    - Don't check PPGTT presence on PPGTT-only platforms (Michal)
    - Fix ICL DSI suspend/resume (Chris)
    - Fix ICL Bandwidth issues (Ville)
    - Add N & CTS values for 10/12 bit deep color (Aditya)
    - Moving more GT related stuff under gt folder (Chris)
    - Forcewake related fixes (Chris)
    - Show support for accurate sw PMU busyness tracking (Chris)
    - Handle gtt double alloc failures (Chris)
    - Upgrade to new GuC version (Michal)
    - Improve w/a debug dumps and pull engine w/a initialization into a common (Chris)
    - Look for instdone on all engines at hangcheck (Tvrtko)
    - Engine lookup simplification  (Chris)
    - Many plane color formats fixes and improvements (Ville)
    - Fix some compilation issues (YueHaibing)
    - GTT page directory clean up and improvements (Mika)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190801201314.GA23635@intel.com

commit a1c9ca223c3df1b8993abedde777f5462165387c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jul 30 14:30:26 2019 +0100

    drm/i915: Remove lrc default desc from GEM context
    
    We only compute the lrc_descriptor() on pinning the context, i.e.
    infrequently, so we do not benefit from storing the template as the
    addressing mode is also fixed for the lifetime of the intel_context.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Prathap Kumar Valsan <prathap.kumar.valsan@intel.com>
    Acked-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190730133035.1977-9-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f68798ab1e7c..4c018fb1359c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -291,9 +291,6 @@ shadow_context_descriptor_update(struct intel_context *ce,
 	 * Update bits 0-11 of the context descriptor which includes flags
 	 * like GEN8_CTX_* cached in desc_template
 	 */
-	desc &= U64_MAX << 12;
-	desc |= ce->gem_context->desc_template & ((1ULL << 12) - 1);
-
 	desc &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
 	desc |= workload->ctx_desc.addressing_mode <<
 		GEN8_CTX_ADDRESSING_MODE_SHIFT;

commit f5d974f9d2a811ef08c044b6fce95c94a6a6e19b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jul 30 15:32:09 2019 +0100

    drm/i915/gt: Provide a local intel_context.vm
    
    Track the currently bound address space used by the HW context. Minor
    conversions to use the local intel_context.vm are made, leaving behind
    some more surgery required to make intel_context the primary through the
    selftests.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190730143209.4549-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 2144fb46d0e1..f68798ab1e7c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1156,7 +1156,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 
 	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
 
-	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(s->shadow[0]->gem_context->vm));
+	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(s->shadow[0]->vm));
 	for_each_engine(engine, vgpu->gvt->dev_priv, id)
 		intel_context_unpin(s->shadow[id]);
 

commit 4187414808095f645ca0661f8dde77617e2e7cb3
Author: Colin Xu <colin.xu@intel.com>
Date:   Thu Jul 4 16:45:06 2019 +0800

    drm/i915/gvt: Adding ppgtt to GVT GEM context after shadow pdps settled.
    
    Windows guest can't run after force-TDR with host log:
    ...
    gvt: vgpu 1: workload shadow ppgtt isn't ready
    gvt: vgpu 1: fail to dispatch workload, skip
    ...
    
    The error is raised by set_context_ppgtt_from_shadow(), when it checks
    and found the shadow_mm isn't marked as shadowed.
    
    In work thread before each submission, a shadow_mm is set to shadowed in:
    shadow_ppgtt_mm()
    <-intel_vgpu_pin_mm()
    <-prepare_workload()
    <-dispatch_workload()
    <-workload_thread()
    However checking whether or not shadow_mm is shadowed is prior to it:
    set_context_ppgtt_from_shadow()
    <-dispatch_workload()
    <-workload_thread()
    
    In normal case, create workload will check the existence of shadow_mm,
    if not it will create a new one and marked as shadowed. If already exist
    it will reuse the old one. Since shadow_mm is reused, checking of shadowed
    in set_context_ppgtt_from_shadow() actually always see the state set in
    creation, but not the state set in intel_vgpu_pin_mm().
    
    When force-TDR, all engines are reset, since it's not dmlr level, all
    ppgtt_mm are invalidated but not destroyed. Invalidation will mark all
    reused shadow_mm as not shadowed but still keeps in ppgtt_mm_list_head.
    If workload submission phase those shadow_mm are reused with shadowed
    not set, then set_context_ppgtt_from_shadow() will report error.
    
    Pin for context after shadow_mm pinned and shadow pdps settled.
    
    v2:
    Move set_context_ppgtt_from_shadow() after prepare_workload(). (zhenyu)
    v3:
    Move set_context_ppgtt_from_shadow() after shadow pdps updated.(zhenyu)
    
    Fixes: 4f15665ccbba ("drm/i915: Add ppgtt to GVT GEM context")
    Cc: stable@vger.kernel.org
    Signed-off-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 196b4155a309..9f3fd7d96a69 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -364,16 +364,13 @@ static void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 	wa_ctx->indirect_ctx.shadow_va = NULL;
 }
 
-static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
-					 struct i915_gem_context *ctx)
+static void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
+					  struct i915_gem_context *ctx)
 {
 	struct intel_vgpu_mm *mm = workload->shadow_mm;
 	struct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(ctx->vm);
 	int i = 0;
 
-	if (mm->type != INTEL_GVT_MM_PPGTT || !mm->ppgtt_mm.shadowed)
-		return -EINVAL;
-
 	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
 		px_dma(ppgtt->pd) = mm->ppgtt_mm.shadow_pdps[0];
 	} else {
@@ -384,8 +381,6 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 			px_dma(pd) = mm->ppgtt_mm.shadow_pdps[i];
 		}
 	}
-
-	return 0;
 }
 
 static int
@@ -614,6 +609,8 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 static int prepare_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	int ring = workload->ring_id;
 	int ret = 0;
 
 	ret = intel_vgpu_pin_mm(workload->shadow_mm);
@@ -622,8 +619,16 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 		return ret;
 	}
 
+	if (workload->shadow_mm->type != INTEL_GVT_MM_PPGTT ||
+	    !workload->shadow_mm->ppgtt_mm.shadowed) {
+		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
+		return -EINVAL;
+	}
+
 	update_shadow_pdps(workload);
 
+	set_context_ppgtt_from_shadow(workload, s->shadow[ring]->gem_context);
+
 	ret = intel_vgpu_sync_oos_pages(workload->vgpu);
 	if (ret) {
 		gvt_vgpu_err("fail to vgpu sync oos pages\n");
@@ -674,7 +679,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
@@ -685,13 +689,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	mutex_lock(&vgpu->vgpu_lock);
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
-	ret = set_context_ppgtt_from_shadow(workload,
-					    s->shadow[ring_id]->gem_context);
-	if (ret < 0) {
-		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
-		goto err_req;
-	}
-
 	ret = intel_gvt_workload_req_alloc(workload);
 	if (ret)
 		goto err_req;

commit ef5b0b444e6297d03ac0bdc0c82f65396ef4dccd
Author: Xiaolin Zhang <xiaolin.zhang@intel.com>
Date:   Thu Jun 20 10:29:24 2019 -0400

    drm/i915/gvt: grab runtime pm first for forcewake use
    
    in workload_thread, it should grab runtime pm wakelock and later
    uncore forcewake get will check rpm wakelock held successfully.
    otherwise, sometimes, rpm wakelock not hold and print call trace below:
    
     Call Trace:
      intel_uncore_forcewake_get+0x15/0x20 [i915]
      workload_thread+0x5f9/0x16f0 [i915]
      ? __switch_to_asm+0x34/0x70
      ? __switch_to_asm+0x40/0x70
      ? __switch_to_asm+0x34/0x70
      ? __switch_to_asm+0x40/0x70
      ? __switch_to_asm+0x34/0x70
      ? __switch_to+0x85/0x3f0
      ? __switch_to_asm+0x40/0x70
      ? do_wait_intr_irq+0x90/0x90
      kthread+0x121/0x140
      ? intel_vgpu_clean_workloads+0x100/0x100 [i915]
      ? kthread_park+0x90/0x90
      ret_from_fork+0x35/0x40
     --[ end trace 86525f742a02e12c ]--
    
    v2: adapted to use rpm structure.
    
    Fixes: 251d46b0875c ("drm/i915/gvt: Pin the per-engine GVT shadow contexts")
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Xiaolin Zhang <xiaolin.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 6469366c1753..196b4155a309 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -990,6 +990,7 @@ static int workload_thread(void *priv)
 	int ret;
 	bool need_force_wake = (INTEL_GEN(gvt->dev_priv) >= 9);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
+	struct intel_runtime_pm *rpm = &gvt->dev_priv->runtime_pm;
 
 	kfree(p);
 
@@ -1013,6 +1014,8 @@ static int workload_thread(void *priv)
 				workload->ring_id, workload,
 				workload->vgpu->id);
 
+		intel_runtime_pm_get(rpm);
+
 		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
 				workload->ring_id, workload);
 
@@ -1042,6 +1045,7 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_put(&gvt->dev_priv->uncore,
 					FORCEWAKE_ALL);
 
+		intel_runtime_pm_put_unchecked(rpm);
 		if (ret && (vgpu_is_vm_unhealthy(ret)))
 			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
 	}

commit 2089a76ade9005a06c5e08e8454f45f3625fdc1c
Author: Xiong Zhang <xiong.y.zhang@intel.com>
Date:   Mon May 27 13:45:53 2019 +0800

    drm/i915/gvt: Checking workload's gma earlier
    
    Workload contains RB and WA_CTX which are in ggtt space,
    if they aren't in valid ggtt space, the workload shouldn't be
    shadowed and scanned. So checking them earlier to avoid shadow
    them.
    
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 2144fb46d0e1..6469366c1753 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1492,6 +1492,12 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
 			RING_CTX_OFF(ctx_ctrl.val), &ctx_ctl, 4);
 
+	if (!intel_gvt_ggtt_validate_range(vgpu, start,
+				_RING_CTL_BUF_SIZE(ctl))) {
+		gvt_vgpu_err("context contain invalid rb at: 0x%x\n", start);
+		return ERR_PTR(-EINVAL);
+	}
+
 	workload = alloc_workload(vgpu);
 	if (IS_ERR(workload))
 		return workload;
@@ -1516,9 +1522,31 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 		workload->wa_ctx.indirect_ctx.size =
 			(indirect_ctx & INDIRECT_CTX_SIZE_MASK) *
 			CACHELINE_BYTES;
+
+		if (workload->wa_ctx.indirect_ctx.size != 0) {
+			if (!intel_gvt_ggtt_validate_range(vgpu,
+				workload->wa_ctx.indirect_ctx.guest_gma,
+				workload->wa_ctx.indirect_ctx.size)) {
+				kmem_cache_free(s->workloads, workload);
+				gvt_vgpu_err("invalid wa_ctx at: 0x%lx\n",
+				    workload->wa_ctx.indirect_ctx.guest_gma);
+				return ERR_PTR(-EINVAL);
+			}
+		}
+
 		workload->wa_ctx.per_ctx.guest_gma =
 			per_ctx & PER_CTX_ADDR_MASK;
 		workload->wa_ctx.per_ctx.valid = per_ctx & 1;
+		if (workload->wa_ctx.per_ctx.valid) {
+			if (!intel_gvt_ggtt_validate_range(vgpu,
+				workload->wa_ctx.per_ctx.guest_gma,
+				CACHELINE_BYTES)) {
+				kmem_cache_free(s->workloads, workload);
+				gvt_vgpu_err("invalid per_ctx at: 0x%lx\n",
+					workload->wa_ctx.per_ctx.guest_gma);
+				return ERR_PTR(-EINVAL);
+			}
+		}
 	}
 
 	gvt_dbg_el("workload %p ring id %d head %x tail %x start %x ctl %x\n",

commit 417f2544f48c19f5958790658c4aa30b0986647f
Merge: 39a207d0cfce 1ee008f240ad
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Jun 21 13:59:49 2019 +1000

    Merge tag 'drm-intel-next-2019-06-19' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    Features:
    - HDR support (Uma, Ville)
    - Add I2C symlink under HDMI connector similar to DP (Oleg)
    - Add ICL multi-segmented gamma support (Shashank, Uma)
    - Update register whitelist support for new hardware (Robert, John)
    - GuC firmware update with updated ABI interface (Michal, Oscar)
    - Add support for new DMC header versions (Lucas)
    - In-kernel blitter client for selftest use (Matthew)
    - Add Mule Creec Canyon (MCC) PCH support to go with EHL (Matt)
    - EHL platform feature updates (Matt)
    - Use Command Transport Buffers with GuC on all gens (Daniele)
    - New i915.force_probe module parameter to replace i915.alpha_support (Jani)
    
    Refactoring:
    - Better runtime PM code abstraction/encapsulation (Daniele)
    - VBT parsing cleanup and improvements (Jani)
    - Move display code to its own subdirectory (Jani)
    - Header cleanup (Jani, Daniele)
    - Prep work for subsclice mask expansion (Stuart)
    - Use uncore mmio register accessors more, remove unused macro wrappers (Tvrtko)
    - Remove unused atomic property get/set stubs (Maarten)
    - GTT cleanups and improvements (Mika)
    - Pass intel_ types instead of drm_ types in plenty of display code (Ville)
    - Engine reset, hangcheck, fault code cleanups and improvements (Tvrtko)
    - Consider AML variants simply as either KBL or CFL ULX (Ville)
    - State checker cleanups and improvements (Ville)
    - GEM code reorganization to more files under gem subdirectory (Chris)
    - Reducing dependency on a coarse struct_mutex (Chris)
    
    Fixes:
    - Fix use of uninitialized/incorrect error pointers (Colin, Dan)
    - Fix DSI fastboot on some VLV/CHV platforms (Hans)
    - Fix DSI error path (Hans)
    - Add ICL port A combo PHY HW state check (Imre)
    - Fix ICL AUX-B HW not done issue (Imre)
    - Fix perf whitelist on gen10+ (Lionel)
    - Fix PSR exit by forcing manual exit on older gens (José)
    - Match voltage ranges instead of exact values (Lucas)
    - Fix SDVO HDMI audio, with cleanups (Ville)
    - Fix plane state dumps (Ville)
    - Fix driver cleanup code to support driver hot unbind (Janusz)
    - Add checks for ICL memory bandwidth requirements (Ville)
    - Fix toggling between no C8 planes vs. at least one C8 plane (Ville)
    - Improved checks on PLL usage conditions, refactoring (Ville)
    - Avoid clobbering M/N values in fastset fuzzy checks (Ville)
    - Take a runtime pm wakeref for atomic commits (Chris)
    - Do not allow runtime pm autosuspend to remove userspace GGTT mmaps too quickly (Chris)
    - Avoid refcount_inc on known zero count to avoid debug flagging (Chris)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/87v9x1lpdh.fsf@intel.com

commit 52d2d44eee8091e740d0d275df1311fb8373c9a9
Merge: 2454fcea338a 9e0babf2c06c
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Wed Jun 19 12:04:55 2019 +0200

    Merge v5.2-rc5 into drm-next
    
    Maarten needs -rc4 backmerged so he can pull in the fbcon notifier
    removal topic branch into drm-misc-next.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

commit b5b7bef9ca9e63cdc563dce447505feb2992cca5
Author: Mika Kuoppala <mika.kuoppala@linux.intel.com>
Date:   Fri Jun 14 19:43:42 2019 +0300

    drm/i915/gtt: Use a common type for page directories
    
    All page directories are identical in function, only the position in the
    hierarchy differ. Use same base type for directory functionality.
    
    v2: cleanup, size always 512, init to null
    
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Matthew Auld <matthew.william.auld@gmail.com>
    Cc: Abdiel Janulgue <abdiel.janulgue@linux.intel.com>
    Signed-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190614164350.30415-2-mika.kuoppala@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8f0bd1e195d1..5f4fa5d590bd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -375,11 +375,13 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 		return -EINVAL;
 
 	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
-		px_dma(&ppgtt->pml4) = mm->ppgtt_mm.shadow_pdps[0];
+		px_dma(ppgtt->pd) = mm->ppgtt_mm.shadow_pdps[0];
 	} else {
 		for (i = 0; i < GVT_RING_CTX_NR_PDPS; i++) {
-			px_dma(ppgtt->pdp.page_directory[i]) =
-				mm->ppgtt_mm.shadow_pdps[i];
+			struct i915_page_directory * const pd =
+				i915_pd_entry(ppgtt->pd, i);
+
+			px_dma(pd) = mm->ppgtt_mm.shadow_pdps[i];
 		}
 	}
 
@@ -1107,11 +1109,14 @@ i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s,
 	int i;
 
 	if (i915_vm_is_4lvl(&ppgtt->vm)) {
-		px_dma(&ppgtt->pml4) = s->i915_context_pml4;
+		px_dma(ppgtt->pd) = s->i915_context_pml4;
 	} else {
-		for (i = 0; i < GEN8_3LVL_PDPES; i++)
-			px_dma(ppgtt->pdp.page_directory[i]) =
-				s->i915_context_pdps[i];
+		for (i = 0; i < GEN8_3LVL_PDPES; i++) {
+			struct i915_page_directory * const pd =
+				i915_pd_entry(ppgtt->pd, i);
+
+			px_dma(pd) = s->i915_context_pdps[i];
+		}
 	}
 }
 
@@ -1165,11 +1170,14 @@ i915_context_ppgtt_root_save(struct intel_vgpu_submission *s,
 	int i;
 
 	if (i915_vm_is_4lvl(&ppgtt->vm)) {
-		s->i915_context_pml4 = px_dma(&ppgtt->pml4);
+		s->i915_context_pml4 = px_dma(ppgtt->pd);
 	} else {
-		for (i = 0; i < GEN8_3LVL_PDPES; i++)
-			s->i915_context_pdps[i] =
-				px_dma(ppgtt->pdp.page_directory[i]);
+		for (i = 0; i < GEN8_3LVL_PDPES; i++) {
+			struct i915_page_directory * const pd =
+				i915_pd_entry(ppgtt->pd, i);
+
+			s->i915_context_pdps[i] = px_dma(pd);
+		}
 	}
 }
 

commit d858d5695f3897d55df68452066a90d7560cb845
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Thu Jun 13 16:21:54 2019 -0700

    drm/i915: update rpm_get/put to use the rpm structure
    
    The functions where internally already only using the structure, so we
    need to just flip the interface.
    
    v2: rebase
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Imre Deak <imre.deak@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190613232156.34940-7-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 080cb740e028..8f0bd1e195d1 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1501,11 +1501,11 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	 * as there is only one pre-allocated buf-obj for shadow.
 	 */
 	if (list_empty(workload_q_head(vgpu, ring_id))) {
-		intel_runtime_pm_get(dev_priv);
+		intel_runtime_pm_get(&dev_priv->runtime_pm);
 		mutex_lock(&dev_priv->drm.struct_mutex);
 		ret = intel_gvt_scan_and_shadow_workload(workload);
 		mutex_unlock(&dev_priv->drm.struct_mutex);
-		intel_runtime_pm_put_unchecked(dev_priv);
+		intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
 	}
 
 	if (ret) {

commit ab53497b57573e0a1b2b5349651108fd69c28a2e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 11 10:12:38 2019 +0100

    drm/i915: Rename i915_hw_ppgtt to i915_ppgtt
    
    Keeping the _hw_ in there does not help to distinguish it from its
    only brethren i915_ggtt, so drop it.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190611091238.15808-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ce6d2f06e8f7..080cb740e028 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -368,7 +368,7 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 					 struct i915_gem_context *ctx)
 {
 	struct intel_vgpu_mm *mm = workload->shadow_mm;
-	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(ctx->vm);
+	struct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(ctx->vm);
 	int i = 0;
 
 	if (mm->type != INTEL_GVT_MM_PPGTT || !mm->ppgtt_mm.shadowed)
@@ -1102,7 +1102,7 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 static void
 i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s,
-				struct i915_hw_ppgtt *ppgtt)
+				struct i915_ppgtt *ppgtt)
 {
 	int i;
 
@@ -1160,7 +1160,7 @@ void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
 
 static void
 i915_context_ppgtt_root_save(struct intel_vgpu_submission *s,
-			     struct i915_hw_ppgtt *ppgtt)
+			     struct i915_ppgtt *ppgtt)
 {
 	int i;
 

commit e568ac3874be7dcef3da0cc3bd6b91ca9dd14aa0
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 11 10:12:37 2019 +0100

    drm/i915: Pull kref into i915_address_space
    
    Make the kref common to both derived structs (i915_ggtt and i915_ppgtt)
    so that we can safely reference count an abstract ctx->vm address space.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190611091238.15808-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a09ad98a3f43..ce6d2f06e8f7 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -368,7 +368,7 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 					 struct i915_gem_context *ctx)
 {
 	struct intel_vgpu_mm *mm = workload->shadow_mm;
-	struct i915_hw_ppgtt *ppgtt = ctx->ppgtt;
+	struct i915_hw_ppgtt *ppgtt = i915_vm_to_ppgtt(ctx->vm);
 	int i = 0;
 
 	if (mm->type != INTEL_GVT_MM_PPGTT || !mm->ppgtt_mm.shadowed)
@@ -1130,7 +1130,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 
 	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
 
-	i915_context_ppgtt_root_restore(s, s->shadow[0]->gem_context->ppgtt);
+	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(s->shadow[0]->gem_context->vm));
 	for_each_engine(engine, vgpu->gvt->dev_priv, id)
 		intel_context_unpin(s->shadow[id]);
 
@@ -1195,7 +1195,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	if (IS_ERR(ctx))
 		return PTR_ERR(ctx);
 
-	i915_context_ppgtt_root_save(s, ctx->ppgtt);
+	i915_context_ppgtt_root_save(s, i915_vm_to_ppgtt(ctx->vm));
 
 	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
 		struct intel_context *ce;
@@ -1238,7 +1238,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	return 0;
 
 out_shadow_ctx:
-	i915_context_ppgtt_root_restore(s, ctx->ppgtt);
+	i915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(ctx->vm));
 	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
 		if (IS_ERR(s->shadow[i]))
 			break;

commit 15e7f52a4596b496ce3da2fa4c1f94c6fb0023f2
Author: Xiaolin Zhang <xiaolin.zhang@intel.com>
Date:   Mon Jun 3 10:55:53 2019 +0800

    drm/i915/gvt: save RING_HEAD into vreg when vgpu switched out
    
    Save RING_HEAD into vgpu reg when vgpu switched out and report
    it's value back to guest.
    
    v6: addressed comment for ring head wrap count support. (Zhenyu)
    v5: ring head wrap count support.
    v4: updated HEAD/TAIL with guest value, not host value. (Yan Zhao)
    v3: save RING HEAD/TAIL vgpu reg in save_ring_hw_state. (Zhenyu Wang)
    v2: save RING_TAIL as well during vgpu mmio switch to meet ring_is_idle
    condition. (Fred Gao)
    v1: based on input from Weinan. (Weinan Li)
    
    [zhenyuw: Include this fix for possible future guest kernel that
    would utilize RING_HEAD for hangcheck.]
    
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Xiaolin Zhang <xiaolin.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 13632dba8b2a..0f919f0a43d4 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -812,10 +812,31 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	void *src;
 	unsigned long context_gpa, context_page_num;
 	int i;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	u32 ring_base;
+	u32 head, tail;
+	u16 wrap_count;
 
 	gvt_dbg_sched("ring id %d workload lrca %x\n", rq->engine->id,
 		      workload->ctx_desc.lrca);
 
+	head = workload->rb_head;
+	tail = workload->rb_tail;
+	wrap_count = workload->guest_rb_head >> RB_HEAD_WRAP_CNT_OFF;
+
+	if (tail < head) {
+		if (wrap_count == RB_HEAD_WRAP_CNT_MAX)
+			wrap_count = 0;
+		else
+			wrap_count += 1;
+	}
+
+	head = (wrap_count << RB_HEAD_WRAP_CNT_OFF) | tail;
+
+	ring_base = dev_priv->engine[workload->ring_id]->mmio_base;
+	vgpu_vreg_t(vgpu, RING_TAIL(ring_base)) = tail;
+	vgpu_vreg_t(vgpu, RING_HEAD(ring_base)) = head;
+
 	context_page_num = rq->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
@@ -1415,6 +1436,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	u64 ring_context_gpa;
 	u32 head, tail, start, ctl, ctx_ctl, per_ctx, indirect_ctx;
+	u32 guest_head;
 	int ret;
 
 	ring_context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
@@ -1430,6 +1452,8 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
 			RING_CTX_OFF(ring_tail.val), &tail, 4);
 
+	guest_head = head;
+
 	head &= RB_HEAD_OFF_MASK;
 	tail &= RB_TAIL_OFF_MASK;
 
@@ -1462,6 +1486,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	workload->ctx_desc = *desc;
 	workload->ring_context_gpa = ring_context_gpa;
 	workload->rb_head = head;
+	workload->guest_rb_head = guest_head;
 	workload->rb_tail = tail;
 	workload->rb_start = start;
 	workload->rb_ctl = ctl;

commit c017cf6b1a5c7a218f7171bb8061132d9a23a918
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:56 2019 +0100

    drm/i915: Drop the deferred active reference
    
    An old optimisation to reduce the number of atomics per batch sadly
    relies on struct_mutex for coordination. In order to remove struct_mutex
    from serialising object/context closing, always taking and releasing an
    active reference on first use / last use greatly simplifies the locking.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-15-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8a00e2d0c81c..a09ad98a3f43 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -600,7 +600,7 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 				i915_vma_unpin(bb->vma);
 				i915_vma_close(bb->vma);
 			}
-			__i915_gem_object_release_unless_active(bb->obj);
+			i915_gem_object_put(bb->obj);
 		}
 		list_del(&bb->list);
 		kfree(bb);

commit 6951e5893b4821f68a48022842f67c3033ca7b30
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:51 2019 +0100

    drm/i915: Move GEM object domain management from struct_mutex to local
    
    Use the per-object local lock to control the cache domain of the
    individual GEM objects, not struct_mutex. This is a huge leap forward
    for us in terms of object-level synchronisation; execbuffers are
    coordinated using the ww_mutex and pread/pwrite is finally fully
    serialised again.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-10-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d66bf77f55fd..8a00e2d0c81c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -509,18 +509,18 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			}
 
 			ret = i915_gem_object_set_to_gtt_domain(bb->obj,
-					false);
+								false);
 			if (ret)
 				goto err;
 
-			i915_gem_object_finish_access(bb->obj);
-			bb->accessing = false;
-
 			ret = i915_vma_move_to_active(bb->vma,
 						      workload->req,
 						      0);
 			if (ret)
 				goto err;
+
+			i915_gem_object_finish_access(bb->obj);
+			bb->accessing = false;
 		}
 	}
 	return 0;

commit 10be98a77c558f8cfb823cd2777171fbb35040f6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:49 2019 +0100

    drm/i915: Move more GEM objects under gem/
    
    Continuing the theme of separating out the GEM clutter.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-8-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 3a691447f76c..d66bf77f55fd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -35,8 +35,11 @@
 
 #include <linux/kthread.h>
 
+#include "gem/i915_gem_context.h"
+#include "gem/i915_gem_pm.h"
+#include "gt/intel_context.h"
+
 #include "i915_drv.h"
-#include "i915_gem_pm.h"
 #include "gvt.h"
 
 #define RING_CTX_OFF(x) \

commit f0e4a06397526d3352a3c80b0575ac22ab24da94
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:48 2019 +0100

    drm/i915: Move GEM domain management to its own file
    
    Continuing the decluttering of i915_gem.c, that of the read/write
    domains, perhaps the biggest of GEM's follies?
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-7-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 38897d241f5f..3a691447f76c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -482,7 +482,7 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 						bb->obj->base.size);
 				bb->clflush &= ~CLFLUSH_AFTER;
 			}
-			i915_gem_obj_finish_shmem_access(bb->obj);
+			i915_gem_object_finish_access(bb->obj);
 			bb->accessing = false;
 
 		} else {
@@ -510,7 +510,7 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			if (ret)
 				goto err;
 
-			i915_gem_obj_finish_shmem_access(bb->obj);
+			i915_gem_object_finish_access(bb->obj);
 			bb->accessing = false;
 
 			ret = i915_vma_move_to_active(bb->vma,
@@ -588,7 +588,7 @@ static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 	list_for_each_entry_safe(bb, pos, &workload->shadow_bb, list) {
 		if (bb->obj) {
 			if (bb->accessing)
-				i915_gem_obj_finish_shmem_access(bb->obj);
+				i915_gem_object_finish_access(bb->obj);
 
 			if (bb->va && !IS_ERR(bb->va))
 				i915_gem_object_unpin_map(bb->obj);

commit 14ee642c2ab0a3d8a1ded11fade692d8b77172b9
Merge: 88cd7a2c1b29 c0a74c732568
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue May 28 09:03:58 2019 +1000

    Merge tag 'drm-intel-next-2019-05-24' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    Features:
    - Engine discovery query (Tvrtko)
    - Support for DP YCbCr4:2:0 outputs (Gwan-gyeong)
    - HDCP revocation support, refactoring (Ramalingam)
    - Remove DRM_AUTH from IOCTLs which also have DRM_RENDER_ALLOW (Christian König)
    - Asynchronous display power disabling (Imre)
    - Perma-pin uC firmware and re-enable global reset (Fernando)
    - GTT remapping for display, for bigger fb size and stride (Ville)
    - Enable pipe HDR mode on ICL if only HDR planes are used (Ville)
    - Kconfig to tweak the busyspin durations for i915_wait_request (Chris)
    - Allow multiple user handles to the same VM (Chris)
    - GT/GEM runtime pm improvements using wakerefs (Chris)
    - Gen 4&5 render context support (Chris)
    - Allow userspace to clone contexts on creation (Chris)
    - SINGLE_TIMELINE flags for context creation (Chris)
    - Allow specification of parallel execbuf (Chris)
    
    Refactoring:
    - Header refactoring (Jani)
    - Move GraphicsTechnology files under gt/ (Chris)
    - Sideband code refactoring (Chris)
    
    Fixes:
    - ICL DSI state readout and checker fixes (Vandita)
    - GLK DSI picture corruption fix (Stanislav)
    - HDMI deep color fixes (Clinton, Aditya)
    - Fix driver unbinding from a device in use (Janusz)
    - Fix clock gating with pipe scaling (Radhakrishna)
    - Disable broken FBC on GLK (Daniel Drake)
    - Miscellaneous GuC fixes (Michal)
    - Fix MG PHY DP register programming (Imre)
    - Add missing combo PHY lane power setup (Imre)
    - Workarounds for early ICL VBT issues (Imre)
    - Fix fastset vs. pfit on/off on HSW EDP transcoder (Ville)
    - Add readout and state check for pch_pfit.force_thru (Ville)
    - Miscellaneous display fixes and refactoring (Ville)
    - Display workaround fixes (Ville)
    - Enable audio even if ELD is bogus (Ville)
    - Fix use-after-free in reporting create.size (Chris)
    - Sideband fixes to avoid BYT hard lockups (Chris)
    - Workaround fixes and improvements (Chris)
    
    Maintainer shortcomings:
    - Failure to adequately describe and give credit for all changes (Jani)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/87sgt3n45z.fsf@intel.com

commit df2ea3c296b1f3d66f297d240124c2ebd74c3db3
Author: Yan Zhao <yan.y.zhao@intel.com>
Date:   Tue May 7 22:14:04 2019 -0400

    drm/i915/gvt: use cmd to restore in-context mmios to hw for gen9 platform
    
    for restore-inhibit context, hardware will not load in-context mmios
    (engine context part) to hardware, but hardware will save the mmio
    values in hardware back to context image. So, in order to save correct
    values of vGPU back to context image, values of vGPU mmios have to be
    loaded into hardware first for restore-inhibit context.
    
    In this patch, the mechanism is applied to all gen9 platform.
    
    The reason excluding gen8 platforms is only because of lacking of testing
    on those platforms.
    
    v3: for mocs registers, goto in-context mmios save-restore path for skl
    platform as well (weinan li)
    v2: update vreg when scanning indirect context for inhibit context for
    gen9
    
    Cc: Weinan Li <weinan.z.li@intel.com>
    Acked-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ccd71152c9bc..13632dba8b2a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -300,9 +300,7 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	u32 *cs;
 	int err;
 
-	if ((IS_KABYLAKE(req->i915) || IS_BROXTON(req->i915)
-		|| IS_COFFEELAKE(req->i915))
-		&& is_inhibit_context(req->hw_context))
+	if (IS_GEN(req->i915, 9) && is_inhibit_context(req->hw_context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/*

commit a8c2d5ab9e71be3f9431c47bd45329a36e1fc650
Author: Weinan <weinan.z.li@intel.com>
Date:   Fri May 10 15:57:20 2019 +0800

    drm/i915/gvt: emit init breadcrumb for gvt request
    
    "To track whether a request has started on HW, we can emit a breadcrumb at
    the beginning of the request and check its timeline's HWSP to see if the
    breadcrumb has advanced past the start of this request." It means all the
    request which timeline's has_init_breadcrumb is true, then the
    emit_init_breadcrumb process must have before emitting the real commands,
    otherwise, the scheduler might get a wrong state of this request during
    reset. If the request is exactly the guilty one, the scheduler won't
    terminate it with the wrong state. To avoid this, do emit_init_breadcrumb
    for all the requests from gvt.
    
    v2: cc to stable kernel
    
    Fixes: 8547444137ec ("drm/i915: Identify active requests")
    Cc: stable@vger.kernel.org
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7c99bbc3e2b8..ccd71152c9bc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -298,12 +298,31 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	struct i915_request *req = workload->req;
 	void *shadow_ring_buffer_va;
 	u32 *cs;
+	int err;
 
 	if ((IS_KABYLAKE(req->i915) || IS_BROXTON(req->i915)
 		|| IS_COFFEELAKE(req->i915))
 		&& is_inhibit_context(req->hw_context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
+	/*
+	 * To track whether a request has started on HW, we can emit a
+	 * breadcrumb at the beginning of the request and check its
+	 * timeline's HWSP to see if the breadcrumb has advanced past the
+	 * start of this request. Actually, the request must have the
+	 * init_breadcrumb if its timeline set has_init_bread_crumb, or the
+	 * scheduler might get a wrong state of it during reset. Since the
+	 * requests from gvt always set the has_init_breadcrumb flag, here
+	 * need to do the emit_init_breadcrumb for all the requests.
+	 */
+	if (req->engine->emit_init_breadcrumb) {
+		err = req->engine->emit_init_breadcrumb(req);
+		if (err) {
+			gvt_vgpu_err("fail to emit init breadcrumb\n");
+			return err;
+		}
+	}
+
 	/* allocate shadow ring buffer */
 	cs = intel_ring_begin(workload->req, workload->rb_len / sizeof(u32));
 	if (IS_ERR(cs)) {

commit 5e2a0419ef7cb25d0f9a5fd6a62372bb47ce948d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Apr 26 17:33:34 2019 +0100

    drm/i915: Switch back to an array of logical per-engine HW contexts
    
    We switched to a tree of per-engine HW context to accommodate the
    introduction of virtual engines. However, we plan to also support
    multiple instances of the same engine within the GEM context, defeating
    our use of the engine as a key to looking up the HW context. Just
    allocate a logical per-engine instance and always use an index into the
    ctx->engines[]. Later on, this ctx->engines[] may be replaced by a user
    specified map.
    
    v2: Add for_each_gem_engine() helper to iterator within the engines lock
    v3: intel_context_create_request() helper
    v4: s/unsigned long/unsigned int/ 4 billion engines is quite enough.
    v5: Push iterator locking to caller
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190426163336.15906-7-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index da6b52de5b16..7ae42f2ebfe8 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1183,7 +1183,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
 		s->shadow[i] = ERR_PTR(-EINVAL);
 
-		ce = intel_context_instance(ctx, engine);
+		ce = i915_gem_context_get_engine(ctx, i);
 		if (IS_ERR(ce)) {
 			ret = PTR_ERR(ce);
 			goto out_shadow_ctx;

commit fa9f668141f4e5590837845ffc1dc4f5aca7a0a5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Apr 26 17:33:29 2019 +0100

    drm/i915: Export intel_context_instance()
    
    We want to pass in a intel_context into intel_context_pin() and that
    requires us to first be able to lookup the intel_context!
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190426163336.15906-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5da50201eb41..da6b52de5b16 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1183,12 +1183,17 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
 		s->shadow[i] = ERR_PTR(-EINVAL);
 
-		ce = intel_context_pin(ctx, engine);
+		ce = intel_context_instance(ctx, engine);
 		if (IS_ERR(ce)) {
 			ret = PTR_ERR(ce);
 			goto out_shadow_ctx;
 		}
 
+		ret = intel_context_pin(ce);
+		intel_context_put(ce);
+		if (ret)
+			goto out_shadow_ctx;
+
 		s->shadow[i] = ce;
 	}
 

commit 251d46b0875c7bb9ff4571a5248550a7427e0b50
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Apr 26 17:33:28 2019 +0100

    drm/i915/gvt: Pin the per-engine GVT shadow contexts
    
    Our eventual goal is to rid request construction of struct_mutex, with
    the short term step of lifting the struct_mutex requirements into the
    higher levels (i.e. the caller must ensure that the context is already
    pinned into the GTT). In this patch, we pin GVT's shadow context upon
    allocation and so keep them pinned into the GGTT for as long as the
    virtual machine is alive, and so we can use the simpler request
    construction path safe in the knowledge that the hard work is already
    done.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190426163336.15906-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8998fa5ab198..5da50201eb41 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -36,6 +36,7 @@
 #include <linux/kthread.h>
 
 #include "i915_drv.h"
+#include "i915_gem_pm.h"
 #include "gvt.h"
 
 #define RING_CTX_OFF(x) \
@@ -277,18 +278,23 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
-static void shadow_context_descriptor_update(struct intel_context *ce)
+static void
+shadow_context_descriptor_update(struct intel_context *ce,
+				 struct intel_vgpu_workload *workload)
 {
-	u64 desc = 0;
-
-	desc = ce->lrc_desc;
+	u64 desc = ce->lrc_desc;
 
-	/* Update bits 0-11 of the context descriptor which includes flags
+	/*
+	 * Update bits 0-11 of the context descriptor which includes flags
 	 * like GEN8_CTX_* cached in desc_template
 	 */
 	desc &= U64_MAX << 12;
 	desc |= ce->gem_context->desc_template & ((1ULL << 12) - 1);
 
+	desc &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
+	desc |= workload->ctx_desc.addressing_mode <<
+		GEN8_CTX_ADDRESSING_MODE_SHIFT;
+
 	ce->lrc_desc = desc;
 }
 
@@ -365,26 +371,22 @@ intel_gvt_workload_req_alloc(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
 	struct i915_request *rq;
-	int ret = 0;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
 
 	if (workload->req)
-		goto out;
+		return 0;
 
-	rq = i915_request_alloc(engine, shadow_ctx);
+	rq = i915_request_create(s->shadow[workload->ring_id]);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
-		ret = PTR_ERR(rq);
-		goto out;
+		return PTR_ERR(rq);
 	}
+
 	workload->req = i915_request_get(rq);
-out:
-	return ret;
+	return 0;
 }
 
 /**
@@ -399,10 +401,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
-	struct intel_context *ce;
 	int ret;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
@@ -410,29 +409,13 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (workload->shadow)
 		return 0;
 
-	/* pin shadow context by gvt even the shadow context will be pinned
-	 * when i915 alloc request. That is because gvt will update the guest
-	 * context from shadow context when workload is completed, and at that
-	 * moment, i915 may already unpined the shadow context to make the
-	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
-	 * the guest context, gvt can unpin the shadow_ctx safely.
-	 */
-	ce = intel_context_pin(shadow_ctx, engine);
-	if (IS_ERR(ce)) {
-		gvt_vgpu_err("fail to pin shadow context\n");
-		return PTR_ERR(ce);
-	}
-
-	shadow_ctx->desc_template &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
-	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
-				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
-
 	if (!test_and_set_bit(workload->ring_id, s->shadow_ctx_desc_updated))
-		shadow_context_descriptor_update(ce);
+		shadow_context_descriptor_update(s->shadow[workload->ring_id],
+						 workload);
 
 	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
-		goto err_unpin;
+		return ret;
 
 	if (workload->ring_id == RCS0 && workload->wa_ctx.indirect_ctx.size) {
 		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
@@ -444,8 +427,6 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return 0;
 err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
-err_unpin:
-	intel_context_unpin(ce);
 	return ret;
 }
 
@@ -672,7 +653,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
@@ -683,7 +663,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	mutex_lock(&vgpu->vgpu_lock);
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
-	ret = set_context_ppgtt_from_shadow(workload, shadow_ctx);
+	ret = set_context_ppgtt_from_shadow(workload,
+					    s->shadow[ring_id]->gem_context);
 	if (ret < 0) {
 		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
 		goto err_req;
@@ -911,11 +892,6 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 				intel_vgpu_trigger_virtual_event(vgpu, event);
 		}
 
-		/* unpin shadow ctx as the shadow_ctx update is done */
-		mutex_lock(&rq->i915->drm.struct_mutex);
-		intel_context_unpin(rq->hw_context);
-		mutex_unlock(&rq->i915->drm.struct_mutex);
-
 		i915_request_put(fetch_and_zero(&workload->req));
 	}
 
@@ -994,8 +970,6 @@ static int workload_thread(void *priv)
 				workload->ring_id, workload,
 				workload->vgpu->id);
 
-		intel_runtime_pm_get(gvt->dev_priv);
-
 		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
 				workload->ring_id, workload);
 
@@ -1025,7 +999,6 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_put(&gvt->dev_priv->uncore,
 					FORCEWAKE_ALL);
 
-		intel_runtime_pm_put_unchecked(gvt->dev_priv);
 		if (ret && (vgpu_is_vm_unhealthy(ret)))
 			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
 	}
@@ -1108,17 +1081,17 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 }
 
 static void
-i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s)
+i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s,
+				struct i915_hw_ppgtt *ppgtt)
 {
-	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
 	int i;
 
-	if (i915_vm_is_4lvl(&i915_ppgtt->vm)) {
-		px_dma(&i915_ppgtt->pml4) = s->i915_context_pml4;
+	if (i915_vm_is_4lvl(&ppgtt->vm)) {
+		px_dma(&ppgtt->pml4) = s->i915_context_pml4;
 	} else {
 		for (i = 0; i < GEN8_3LVL_PDPES; i++)
-			px_dma(i915_ppgtt->pdp.page_directory[i]) =
-						s->i915_context_pdps[i];
+			px_dma(ppgtt->pdp.page_directory[i]) =
+				s->i915_context_pdps[i];
 	}
 }
 
@@ -1132,10 +1105,15 @@ i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s)
 void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id id;
 
 	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
-	i915_context_ppgtt_root_restore(s);
-	i915_gem_context_put(s->shadow_ctx);
+
+	i915_context_ppgtt_root_restore(s, s->shadow[0]->gem_context->ppgtt);
+	for_each_engine(engine, vgpu->gvt->dev_priv, id)
+		intel_context_unpin(s->shadow[id]);
+
 	kmem_cache_destroy(s->workloads);
 }
 
@@ -1161,17 +1139,17 @@ void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
 }
 
 static void
-i915_context_ppgtt_root_save(struct intel_vgpu_submission *s)
+i915_context_ppgtt_root_save(struct intel_vgpu_submission *s,
+			     struct i915_hw_ppgtt *ppgtt)
 {
-	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
 	int i;
 
-	if (i915_vm_is_4lvl(&i915_ppgtt->vm))
-		s->i915_context_pml4 = px_dma(&i915_ppgtt->pml4);
-	else {
+	if (i915_vm_is_4lvl(&ppgtt->vm)) {
+		s->i915_context_pml4 = px_dma(&ppgtt->pml4);
+	} else {
 		for (i = 0; i < GEN8_3LVL_PDPES; i++)
 			s->i915_context_pdps[i] =
-				px_dma(i915_ppgtt->pdp.page_directory[i]);
+				px_dma(ppgtt->pdp.page_directory[i]);
 	}
 }
 
@@ -1188,16 +1166,31 @@ i915_context_ppgtt_root_save(struct intel_vgpu_submission *s)
 int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
+	struct i915_gem_context *ctx;
+	enum intel_engine_id i;
 	int ret;
 
-	s->shadow_ctx = i915_gem_context_create_gvt(
-			&vgpu->gvt->dev_priv->drm);
-	if (IS_ERR(s->shadow_ctx))
-		return PTR_ERR(s->shadow_ctx);
+	ctx = i915_gem_context_create_gvt(&vgpu->gvt->dev_priv->drm);
+	if (IS_ERR(ctx))
+		return PTR_ERR(ctx);
+
+	i915_context_ppgtt_root_save(s, ctx->ppgtt);
 
-	i915_context_ppgtt_root_save(s);
+	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
+		struct intel_context *ce;
+
+		INIT_LIST_HEAD(&s->workload_q_head[i]);
+		s->shadow[i] = ERR_PTR(-EINVAL);
+
+		ce = intel_context_pin(ctx, engine);
+		if (IS_ERR(ce)) {
+			ret = PTR_ERR(ce);
+			goto out_shadow_ctx;
+		}
+
+		s->shadow[i] = ce;
+	}
 
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
@@ -1213,16 +1206,21 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		goto out_shadow_ctx;
 	}
 
-	for_each_engine(engine, vgpu->gvt->dev_priv, i)
-		INIT_LIST_HEAD(&s->workload_q_head[i]);
-
 	atomic_set(&s->running_workload_num, 0);
 	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
+	i915_gem_context_put(ctx);
 	return 0;
 
 out_shadow_ctx:
-	i915_gem_context_put(s->shadow_ctx);
+	i915_context_ppgtt_root_restore(s, ctx->ppgtt);
+	for_each_engine(engine, vgpu->gvt->dev_priv, i) {
+		if (IS_ERR(s->shadow[i]))
+			break;
+
+		intel_context_unpin(s->shadow[i]);
+	}
+	i915_gem_context_put(ctx);
 	return ret;
 }
 

commit 0cf8f58d0a340a2ac744f6e0e3402a89780ecf8b
Author: Aleksei Gimbitskii <aleksei.gimbitskii@intel.com>
Date:   Tue Apr 23 15:04:08 2019 +0300

    drm/i915/gvt: Remove typedef and let the enumeration starts from zero
    
    Typedef is not recommended in the Linux kernel.The klocwork static code
    analyzer takes the enumeration as the full range of intel_gvt_gtt_type_t.
    But the intel_gvt_gtt_type_t will never be used in full range. For
    example, the GTT_TYPE_INVALID will never be used as an index of an array.
    Remove the typedef and let the enumeration starts from zero to pass
    klocwork analysis.
    
    This patch fixed the critial issues #483, #551, #665 reported by
    klockwork.
    
    v3:
    - Remove the typedef and let the enumeration starts from zero.
    
    Signed-off-by: Aleksei Gimbitskii <aleksei.gimbitskii@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    CC: Colin Xu <colin.xu@intel.com>
    Reviewed-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8998fa5ab198..7c99bbc3e2b8 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1343,7 +1343,7 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 	struct execlist_ctx_descriptor_format *desc = &workload->ctx_desc;
 	struct intel_vgpu_mm *mm;
 	struct intel_vgpu *vgpu = workload->vgpu;
-	intel_gvt_gtt_type_t root_entry_type;
+	enum intel_gvt_gtt_type root_entry_type;
 	u64 pdps[GVT_RING_CTX_NR_PDPS];
 
 	switch (desc->addressing_mode) {

commit b1c4f7feada5a5cf4e13db1631fb4784b1ddcb31
Merge: b3edf499dd5b ad2c467aa92e
Author: Dave Airlie <airlied@redhat.com>
Date:   Wed Apr 24 10:02:20 2019 +1000

    Merge tag 'drm-intel-next-2019-04-17' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    UAPI Changes:
    
    - uAPI "Fixes:" patch for the upcoming kernel 5.1, included here too
    
      We have an Ack from the media folks (only current user) for this
      late tweak
    
    Cross-subsystem Changes:
    
    - ALSA: hda: Fix racy display power access (Takashi, Chris)
    
    Driver Changes:
    
    - DDI and MIPI-DSI clocks fixes for Icelake (Vandita)
    - Fix Icelake frequency change/locking (RPS) (Mika)
    - Temporarily disable ppGTT read-only bit on Icelake (Mika)
    - Add missing Icelake W/As (Mika)
    - Enable 12 deep CSB status FIFO on Icelake (Mika)
    - Inherit more Icelake code for Elkhartlake (Bob, Jani)
    
    - Handle catastrophic error on engine reset (Mika)
    - Shortcut readiness to reset check (Mika)
    - Regression fix for GEM_BUSY causing us to report a mixed uabi-class request as not busy (Chris)
    - Revert back to max link rate and lane count on eDP (Jani)
    - Fix pipe BPP readout for BXT/GLK DSI (Ville)
    - Set DP min_bpp to 8*3 for non-RGB output formats (Ville)
    - Enable coarse preemption boundaries for Gen8 (Chris)
    - Do not enable FEC without DSC (Ville)
    - Restore correct BXT DDI latency optim setting calculation (Ville)
    - Always reset context's RING registers to avoid running workload twice during reset (Chris)
    - Set GPU wedged on driver unload (Janusz)
    - Consolidate two similar barries from timeline into one (Chris)
    - Only reset the pinned kernel contexts on resume (Chris)
    - Wakeref tracking improvements (Chris, Imre)
    - Lockdep fixes for shrinker interactions (Chris)
    - Bump ready tasks ahead of busywaits in prep of semaphore use (Chris)
    
    - Huge step in splitting display code into fine grained files (Jani)
    - Refactor the IRQ init/reset macros for code saving (Paulo)
    - Convert IRQ initialization code to uncore MMIO access (Paulo)
    - Convert workarounds code to use uncore MMIO access (Chris)
    - Nuke drm_crtc_state and use intel_atomic_state instead (Manasi)
    - Update SKL clock-gating WA (Radhakrishna, Ville)
    - Isolate GuC reset code flow (Chris)
    - Expose force_dsc_enable through debugfs (Manasi)
    - Header standalone compile testing framework (Jani)
    - Code cleanups to reduce driver footprint (Chris)
    - PSR code fixes and cleanups (Jose)
    - Sparse and kerneldoc updates (Chris)
    - Suppress spurious combo PHY B warning (Vile)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    From: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190418080426.GA6409@jlahtine-desk.ger.corp.intel.com

commit f06ddb53096b4cddad2c530125a78a3c2a1d28a4
Merge: ecc4946f11a0 dc4060a5dc25
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Apr 15 15:51:49 2019 +1000

    BackMerge v5.1-rc5 into drm-next
    
    Need rc5 for udl fix to add udl cleanups on top.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit 3a891a62679424e5625a551b9af9c33af6ea59b3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 1 17:26:39 2019 +0100

    drm/i915: Move intel_engine_mask_t around for use by i915_request_types.h
    
    We want to use intel_engine_mask_t inside i915_request.h, which means
    extracting it from the general header file mess and placing it inside a
    types.h. A knock on effect is that the compiler wants to warn about
    type-contraction of ALL_ENGINES into intel_engine_maskt_t, so prepare
    for the worst.
    
    v2: Use intel_engine_mask_t consistently
    v3: Move I915_NUM_ENGINES to its natural home at the end of the enum
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190401162641.10963-1-chris@chris-wilson.co.uk
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 3faf2438b9bc..b385edbeaa30 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -838,13 +838,13 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 }
 
 void intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,
-				unsigned long engine_mask)
+				intel_engine_mask_t engine_mask)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine;
 	struct intel_vgpu_workload *pos, *n;
-	unsigned int tmp;
+	intel_engine_mask_t tmp;
 
 	/* free the unsubmited workloads in the queues. */
 	for_each_engine_masked(engine, dev_priv, engine_mask, tmp) {
@@ -1137,7 +1137,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
  *
  */
 void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
-		unsigned long engine_mask)
+				 intel_engine_mask_t engine_mask)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 
@@ -1227,7 +1227,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
  *
  */
 int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
-				     unsigned long engine_mask,
+				     intel_engine_mask_t engine_mask,
 				     unsigned int interface)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;

commit dade58ed5af6365ac50ff4259c2a0bf31219e285
Author: Yan Zhao <yan.y.zhao@intel.com>
Date:   Wed Mar 27 00:54:51 2019 -0400

    drm/i915/gvt: do not deliver a workload if its creation fails
    
    in workload creation routine, if any failure occurs, do not queue this
    workload for delivery. if this failure is fatal, enter into failsafe
    mode.
    
    Fixes: 6d76303553ba ("drm/i915/gvt: Move common vGPU workload creation into scheduler.c")
    Cc: stable@vger.kernel.org #4.19+
    Cc: zhenyuw@linux.intel.com
    Signed-off-by: Yan Zhao <yan.y.zhao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 159192c097cc..05b953793316 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1486,8 +1486,9 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 		intel_runtime_pm_put_unchecked(dev_priv);
 	}
 
-	if (ret && (vgpu_is_vm_unhealthy(ret))) {
-		enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
+	if (ret) {
+		if (vgpu_is_vm_unhealthy(ret))
+			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
 		intel_vgpu_destroy_workload(workload);
 		return ERR_PTR(ret);
 	}

commit 76444b6e62edd73abee11e5809a960b39e00e238
Merge: 000c4f90e3f0 72aabfb862e4
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Thu Mar 21 12:33:24 2019 -0700

    Merge tag 'gvt-fixes-2019-03-21' of https://github.com/intel/gvt-linux into drm-intel-fixes
    
    gvt-fixes-2019-03-21
    
    - Fix MI_FLUSH_DW cmd parser on index check (Zhenyu)
    - Fix Windows guest font render error (Colin)
    - Fix unexpected workload submission for inactive vGPU (Weinan)
    - Fix incorrect workload submission in error path (Zhenyu)
    - Fix warning for shadow ppgtt mm reclaim list walk with locking (Zhenyu)
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190321035018.GF10798@zhen-hp.sh.intel.com

commit 3ceea6a1b4d2426b49a9ebcc099cc147dc68e20b
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Tue Mar 19 11:35:36 2019 -0700

    drm/i915: use intel_uncore for all forcewake get/put
    
    Now that the internal code all works on intel_uncore, flip the
    external-facing interface.
    
    v2: fix GVT.
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Paulo Zanoni <paulo.r.zanoni@intel.com>
    Reviewed-by: Paulo Zanoni <paulo.r.zanoni@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190319183543.13679-4-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7550e09939ae..3faf2438b9bc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -988,7 +988,7 @@ static int workload_thread(void *priv)
 				workload->ring_id, workload);
 
 		if (need_force_wake)
-			intel_uncore_forcewake_get(gvt->dev_priv,
+			intel_uncore_forcewake_get(&gvt->dev_priv->uncore,
 					FORCEWAKE_ALL);
 
 		ret = dispatch_workload(workload);
@@ -1010,7 +1010,7 @@ static int workload_thread(void *priv)
 		complete_current_workload(gvt, ring_id);
 
 		if (need_force_wake)
-			intel_uncore_forcewake_put(gvt->dev_priv,
+			intel_uncore_forcewake_put(&gvt->dev_priv->uncore,
 					FORCEWAKE_ALL);
 
 		intel_runtime_pm_put_unchecked(gvt->dev_priv);

commit a9fe9ca44c918b44f8fb85d4571d3386f390be4f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Mar 14 22:38:38 2019 +0000

    drm/i915/gtt: Rename i915_vm_is_48b to i915_vm_is_4lvl
    
    Large ppGTT are differentiated by the requirement to go to four levels
    to address more than 32b. Given the introduction of more 4 level ppGTT
    with different sizes of addressable bits, rename i915_vm_is_48b() to
    better reflect the commonality of using 4 levels.
    
    Based on a patch by Bob Paauwe.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Bob Paauwe <bob.j.paauwe@intel.com>
    Cc: Matthew Auld <matthew.william.auld@gmail.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190314223839.28258-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 709bcaaed765..7550e09939ae 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1101,9 +1101,9 @@ i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s)
 	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
 	int i;
 
-	if (i915_vm_is_48bit(&i915_ppgtt->vm))
+	if (i915_vm_is_4lvl(&i915_ppgtt->vm)) {
 		px_dma(&i915_ppgtt->pml4) = s->i915_context_pml4;
-	else {
+	} else {
 		for (i = 0; i < GEN8_3LVL_PDPES; i++)
 			px_dma(i915_ppgtt->pdp.page_directory[i]) =
 						s->i915_context_pdps[i];
@@ -1154,7 +1154,7 @@ i915_context_ppgtt_root_save(struct intel_vgpu_submission *s)
 	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
 	int i;
 
-	if (i915_vm_is_48bit(&i915_ppgtt->vm))
+	if (i915_vm_is_4lvl(&i915_ppgtt->vm))
 		s->i915_context_pml4 = px_dma(&i915_ppgtt->pml4);
 	else {
 		for (i = 0; i < GEN8_3LVL_PDPES; i++)

commit 8a68d464366efb5b294fa11ccf23b51306cc2695
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Mar 5 18:03:30 2019 +0000

    drm/i915: Store the BIT(engine->id) as the engine's mask
    
    In the next patch, we are introducing a broad virtual engine to encompass
    multiple physical engines, losing the 1:1 nature of BIT(engine->id). To
    reflect the broader set of engines implied by the virtual instance, lets
    store the full bitmask.
    
    v2: Use intel_engine_mask_t (s/ring_mask/engine_mask/)
    v3: Tvrtko voted for moah churn so teach everyone to not mention ring
    and use $class$instance throughout.
    v4: Comment upon the disparity in bspec for using VCS1,VCS2 in gen8 and
    VCS[0-4] in later gen. We opt to keep the code consistent and use
    0-index naming throughout.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190305180332.30900-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1bb8f936fdaa..709bcaaed765 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -93,7 +93,7 @@ static void sr_oa_regs(struct intel_vgpu_workload *workload,
 		i915_mmio_reg_offset(EU_PERF_CNTL6),
 	};
 
-	if (workload->ring_id != RCS)
+	if (workload->ring_id != RCS0)
 		return;
 
 	if (save) {
@@ -149,7 +149,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	COPY_REG_MASKED(ctx_ctrl);
 	COPY_REG(ctx_timestamp);
 
-	if (ring_id == RCS) {
+	if (ring_id == RCS0) {
 		COPY_REG(bb_per_ctx_ptr);
 		COPY_REG(rcs_indirect_ctx);
 		COPY_REG(rcs_indirect_ctx_offset);
@@ -177,7 +177,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS0)
 		context_page_num = 19;
 
 	i = 2;
@@ -440,8 +440,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (ret)
 		goto err_unpin;
 
-	if ((workload->ring_id == RCS) &&
-	    (workload->wa_ctx.indirect_ctx.size != 0)) {
+	if (workload->ring_id == RCS0 && workload->wa_ctx.indirect_ctx.size) {
 		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
 		if (ret)
 			goto err_shadow;
@@ -791,7 +790,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	context_page_num = rq->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && rq->engine->id == RCS)
+	if (IS_BROADWELL(gvt->dev_priv) && rq->engine->id == RCS0)
 		context_page_num = 19;
 
 	i = 2;
@@ -891,8 +890,8 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 				workload->status = 0;
 		}
 
-		if (!workload->status && !(vgpu->resetting_eng &
-					   ENGINE_MASK(ring_id))) {
+		if (!workload->status &&
+		    !(vgpu->resetting_eng & BIT(ring_id))) {
 			update_guest_context(workload);
 
 			for_each_set_bit(event, workload->pending_events,
@@ -915,7 +914,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	list_del_init(&workload->list);
 
-	if (workload->status || (vgpu->resetting_eng & ENGINE_MASK(ring_id))) {
+	if (workload->status || vgpu->resetting_eng & BIT(ring_id)) {
 		/* if workload->status is not successful means HW GPU
 		 * has occurred GPU hang or something wrong with i915/GVT,
 		 * and GVT won't inject context switch interrupt to guest.
@@ -929,7 +928,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		 * cleaned up during the resetting process later, so doing
 		 * the workload clean up here doesn't have any impact.
 		 **/
-		intel_vgpu_clean_workloads(vgpu, ENGINE_MASK(ring_id));
+		intel_vgpu_clean_workloads(vgpu, BIT(ring_id));
 	}
 
 	workload->complete(workload);
@@ -1438,7 +1437,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	workload->rb_start = start;
 	workload->rb_ctl = ctl;
 
-	if (ring_id == RCS) {
+	if (ring_id == RCS0) {
 		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
 			RING_CTX_OFF(bb_per_ctx_ptr.val), &per_ctx, 4);
 		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +

commit 1e18d5e6731d674fee0bb4b66f5ea61e504452a3
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Fri Mar 1 15:04:12 2019 +0800

    drm/i915/gvt: Only assign ppgtt root at dispatch time
    
    This moves ppgtt root hook out of scan and shadow function,
    as it's only required at dispatch time. Also make sure this
    checks against shadow mm to be ready, otherwise bail to fail
    earlier.
    
    Reviewed-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Cc: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 817d46f25f09..0f2f2c43c0e0 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -345,7 +345,7 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 	int i = 0;
 
 	if (mm->type != INTEL_GVT_MM_PPGTT || !mm->ppgtt_mm.shadowed)
-		return -1;
+		return -EINVAL;
 
 	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
 		px_dma(&ppgtt->pml4) = mm->ppgtt_mm.shadow_pdps[0];
@@ -409,12 +409,6 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (workload->shadow)
 		return 0;
 
-	ret = set_context_ppgtt_from_shadow(workload, shadow_ctx);
-	if (ret < 0) {
-		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
-		return ret;
-	}
-
 	/* pin shadow context by gvt even the shadow context will be pinned
 	 * when i915 alloc request. That is because gvt will update the guest
 	 * context from shadow context when workload is completed, and at that
@@ -677,6 +671,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
@@ -687,6 +683,12 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	mutex_lock(&vgpu->vgpu_lock);
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
+	ret = set_context_ppgtt_from_shadow(workload, shadow_ctx);
+	if (ret < 0) {
+		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
+		goto err_req;
+	}
+
 	ret = intel_gvt_workload_req_alloc(workload);
 	if (ret)
 		goto err_req;

commit f552e7bd028f93157b206bd1c7e3e195e919e8c2
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Fri Mar 1 15:55:04 2019 +0800

    drm/i915/gvt: Don't submit request for error workload dispatch
    
    As vGPU shadow ctx is loaded with guest context state, arbitrarily
    submitting request in error workload dispatch path would cause trouble.
    So don't try to submit in error path now like in previous code.
    This is to fix VM failure when GPU hang happens.
    
    Fixes: f0e994372518 ("drm/i915/gvt: Fix workload request allocation before request add")
    Reviewed-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index af97df10495e..817d46f25f09 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -677,6 +677,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
 
@@ -702,6 +703,14 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	ret = prepare_workload(workload);
 out:
+	if (ret) {
+		/* We might still need to add request with
+		 * clean ctx to retire it properly..
+		 */
+		rq = fetch_and_zero(&workload->req);
+		i915_request_put(rq);
+	}
+
 	if (!IS_ERR_OR_NULL(workload->req)) {
 		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 				ring_id, workload->req);

commit 9f4984773240cf004cbffb99316af3269bbb5e26
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Wed Feb 27 15:36:58 2019 +0800

    drm/i915/gvt: stop scheduling workload when vgpu is inactive
    
    There is one corner case that workload_thread may pick and dispatch one
    workload of vgpu after it's already deactivated. Below is the scenario:
    
    1. deactive_vgpu got the vgpu_lock, it found pending workload was
    submitted, then it released the vgpu_lock and wait for vgpu idle.
    2. before deactive_vgpu got the vgpu_lock back, workload_thread might pick
    one new valid workload, then it was blocked by the vgpu_lock.
    3. deactive_vgpu got the vgpu_lock again, finished the last processes of
    deactivating, then release the vgpu_lock.
    4. workload_thread got the vgpu_lock, then it will try to dispatch the
    fetched workload. It's not expected one workload of deactivated vgpu is
    dispatched.
    
    The solution is to add condition check of the vgpu's active flag and stop
    to schedule when it's inactive.
    
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 55bb7885e228..af97df10495e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -738,7 +738,8 @@ static struct intel_vgpu_workload *pick_next_workload(
 		goto out;
 	}
 
-	if (list_empty(workload_q_head(scheduler->current_vgpu, ring_id)))
+	if (!scheduler->current_vgpu->active ||
+	    list_empty(workload_q_head(scheduler->current_vgpu, ring_id)))
 		goto out;
 
 	/*

commit c06de56121e3ac0f0f1f4a081c041654ffcacd62
Merge: 8d451a4b6e9f a3b22b9f11d9
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Feb 18 13:27:15 2019 +1000

    Merge v5.0-rc7 into drm-next
    
    Backmerging for nouveau and imx that needed some fixes for next pulls.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit ff00d85b4df97bf9cc7082b3d0dc2c317b946aa2
Merge: 0cdc1d07b461 2e679d48f38c
Author: Rodrigo Vivi <vivijim@rdvivi-cozumel.jf.intel.com>
Date:   Thu Jan 24 14:50:02 2019 -0800

    Merge tag 'gvt-next-2019-01-24' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2019-01-24
    
    - split kvmgt as seperate module (Zhenyu)
    - Coffeelake GVT support (Fred)
    - const treatment and change for kernel type (Jani)
    
    Signed-off-by: Rodrigo Vivi <vivijim@rdvivi-cozumel.jf.intel.com>
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190124054048.GO7203@zhen-hp.sh.intel.com

commit 0f75551216091223efe1f18295f655aff6415385
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Tue Jan 22 13:46:27 2019 +0800

    drm/i915/gvt: release shadow batch buffer and wa_ctx before destroy one workload
    
    GVT-g will shadow the privilege batch buffer and the indirect context
    during command scan, move the release process into
    intel_vgpu_destroy_workload() to ensure the resources are recycled
    properly.
    
    Fixes: 0cce2823ed37 ("drm/i915/gvt/kvmgt:Refine error handling for prepare_execlist_workload")
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5567ddc7760f..55bb7885e228 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -332,6 +332,9 @@ static void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 
 	i915_gem_object_unpin_map(wa_ctx->indirect_ctx.obj);
 	i915_gem_object_put(wa_ctx->indirect_ctx.obj);
+
+	wa_ctx->indirect_ctx.obj = NULL;
+	wa_ctx->indirect_ctx.shadow_va = NULL;
 }
 
 static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
@@ -911,11 +914,6 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	list_del_init(&workload->list);
 
-	if (!workload->status) {
-		release_shadow_batch_buffer(workload);
-		release_shadow_wa_ctx(&workload->wa_ctx);
-	}
-
 	if (workload->status || (vgpu->resetting_eng & ENGINE_MASK(ring_id))) {
 		/* if workload->status is not successful means HW GPU
 		 * has occurred GPU hang or something wrong with i915/GVT,
@@ -1283,6 +1281,9 @@ void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu_submission *s = &workload->vgpu->submission;
 
+	release_shadow_batch_buffer(workload);
+	release_shadow_wa_ctx(&workload->wa_ctx);
+
 	if (workload->shadow_mm)
 		intel_vgpu_mm_put(workload->shadow_mm);
 

commit 16e4dd0342a804090fd0958bb271d3a6b57056ac
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jan 14 14:21:10 2019 +0000

    drm/i915: Markup paired operations on wakerefs
    
    The majority of runtime-pm operations are bounded and scoped within a
    function; these are easy to verify that the wakeref are handled
    correctly. We can employ the compiler to help us, and reduce the number
    of wakerefs tracked when debugging, by passing around cookies provided
    by the various rpm_get functions to their rpm_put counterpart. This
    makes the pairing explicit, and given the required wakeref cookie the
    compiler can verify that we pass an initialised value to the rpm_put
    (quite handy for double checking error paths).
    
    For regular builds, the compiler should be able to eliminate the unused
    local variables and the program growth should be minimal. Fwiw, it came
    out as a net improvement as gcc was able to refactor rpm_get and
    rpm_get_if_in_use together,
    
    v2: Just s/rpm_put/rpm_put_unchecked/ everywhere, leaving the manual
    mark up for smaller more targeted patches.
    v3: Mention the cookie in Returns
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Jani Nikula <jani.nikula@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190114142129.24398-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1ad8c5e1455d..3816dcae2185 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -997,7 +997,7 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_put(gvt->dev_priv,
 					FORCEWAKE_ALL);
 
-		intel_runtime_pm_put(gvt->dev_priv);
+		intel_runtime_pm_put_unchecked(gvt->dev_priv);
 		if (ret && (vgpu_is_vm_unhealthy(ret)))
 			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
 	}
@@ -1451,7 +1451,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 		mutex_lock(&dev_priv->drm.struct_mutex);
 		ret = intel_gvt_scan_and_shadow_workload(workload);
 		mutex_unlock(&dev_priv->drm.struct_mutex);
-		intel_runtime_pm_put(dev_priv);
+		intel_runtime_pm_put_unchecked(dev_priv);
 	}
 
 	if (ret && (vgpu_is_vm_unhealthy(ret))) {

commit c3b5a8430daadf5b8ec9757d6c81149903cbe99f
Author: fred gao <fred.gao@intel.com>
Date:   Wed Jan 9 09:20:07 2019 +0800

    drm/i915/gvt: Enable gfx virtualiztion for CFL
    
    Use INTEL_GEN to simplify the code for SKL+ platforms.
    
    v2:
    - split the enabling code into final one to identify any regression.
    
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Fei Jiang <fei.jiang@intel.com>
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1ad8c5e1455d..fb7445baa139 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -299,7 +299,8 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	void *shadow_ring_buffer_va;
 	u32 *cs;
 
-	if ((IS_KABYLAKE(req->i915) || IS_BROXTON(req->i915))
+	if ((IS_KABYLAKE(req->i915) || IS_BROXTON(req->i915)
+		|| IS_COFFEELAKE(req->i915))
 		&& is_inhibit_context(req->hw_context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
@@ -939,9 +940,7 @@ static int workload_thread(void *priv)
 	struct intel_vgpu_workload *workload = NULL;
 	struct intel_vgpu *vgpu = NULL;
 	int ret;
-	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv)
-			|| IS_KABYLAKE(gvt->dev_priv)
-			|| IS_BROXTON(gvt->dev_priv);
+	bool need_force_wake = (INTEL_GEN(gvt->dev_priv) >= 9);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
 
 	kfree(p);

commit f0e9943725186ddbdc9718a559c26c5f507262f2
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Sat Dec 29 11:13:10 2018 +0800

    drm/i915/gvt: Fix workload request allocation before request add
    
    In commit 6bb2a2af8b1b ("drm/i915/gvt: Fix crash after request->hw_context change"),
    forgot to handle workload scan path in ELSP handler case which was to
    optimize scanning earlier instead of in gvt submission thread, so request
    alloc and add was splitting then which is against right process.
    
    This trys to do a partial revert of that commit which still has workload
    request alloc helper and make sure shadow state population is handled after
    request alloc for target state buffer.
    
    v3: Fix missed workload status setting in request alloc error path
    v2: Fix dispatch workload err path that should add request after alloc anyway.
    
    Fixes: 6bb2a2af8b1b ("drm/i915/gvt: Fix crash after request->hw_context change")
    Cc: Bin Yang <bin.yang@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Tested-by: Bin Yang <bin.yang@intel.com>
    Reviewed-by: Xiaolin Zhang <xiaolin.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1ad8c5e1455d..5567ddc7760f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -356,6 +356,33 @@ static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
 	return 0;
 }
 
+static int
+intel_gvt_workload_req_alloc(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
+	struct i915_request *rq;
+	int ret = 0;
+
+	lockdep_assert_held(&dev_priv->drm.struct_mutex);
+
+	if (workload->req)
+		goto out;
+
+	rq = i915_request_alloc(engine, shadow_ctx);
+	if (IS_ERR(rq)) {
+		gvt_vgpu_err("fail to allocate gem request\n");
+		ret = PTR_ERR(rq);
+		goto out;
+	}
+	workload->req = i915_request_get(rq);
+out:
+	return ret;
+}
+
 /**
  * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
  * shadow it as well, include ringbuffer,wa_ctx and ctx.
@@ -372,12 +399,11 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
 	struct intel_context *ce;
-	struct i915_request *rq;
 	int ret;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
 
-	if (workload->req)
+	if (workload->shadow)
 		return 0;
 
 	ret = set_context_ppgtt_from_shadow(workload, shadow_ctx);
@@ -417,22 +443,8 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 			goto err_shadow;
 	}
 
-	rq = i915_request_alloc(engine, shadow_ctx);
-	if (IS_ERR(rq)) {
-		gvt_vgpu_err("fail to allocate gem request\n");
-		ret = PTR_ERR(rq);
-		goto err_shadow;
-	}
-	workload->req = i915_request_get(rq);
-
-	ret = populate_shadow_context(workload);
-	if (ret)
-		goto err_req;
-
+	workload->shadow = true;
 	return 0;
-err_req:
-	rq = fetch_and_zero(&workload->req);
-	i915_request_put(rq);
 err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
 err_unpin:
@@ -671,23 +683,31 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	mutex_lock(&vgpu->vgpu_lock);
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
+	ret = intel_gvt_workload_req_alloc(workload);
+	if (ret)
+		goto err_req;
+
 	ret = intel_gvt_scan_and_shadow_workload(workload);
 	if (ret)
 		goto out;
 
-	ret = prepare_workload(workload);
+	ret = populate_shadow_context(workload);
+	if (ret) {
+		release_shadow_wa_ctx(&workload->wa_ctx);
+		goto out;
+	}
 
+	ret = prepare_workload(workload);
 out:
-	if (ret)
-		workload->status = ret;
-
 	if (!IS_ERR_OR_NULL(workload->req)) {
 		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 				ring_id, workload->req);
 		i915_request_add(workload->req);
 		workload->dispatched = true;
 	}
-
+err_req:
+	if (ret)
+		workload->status = ret;
 	mutex_unlock(&dev_priv->drm.struct_mutex);
 	mutex_unlock(&vgpu->vgpu_lock);
 	return ret;

commit f39a89b8f738b683e720185cdbbd1e6d626fada9
Author: Xiong Zhang <xiong.y.zhang@intel.com>
Date:   Thu Nov 29 16:25:54 2018 +0800

    drm/i915/gvt: Fix shadow ctx ppgtt destroy function
    
    Recently gvt shadow ctx create ppgtt table and this ppgtt's root
    pointer is modified at workload dispatch, then we lose the original
    ppgtt's root pointer, this causes the ppgtt destroy function abnormal
    as it will release the wrong root table.
    
    This patch save i915 context ppgtt root pointer at shadow
    ctx creation and restore it at shadow ctx destruction.
    
    v2: Split save and restore function (Zhenyu)
    
    Fixes:4f15665ccbba("drm/i915: Add ppgtt to GVT GEM context")
    Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b8fbe3fabea3..1ad8c5e1455d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1079,6 +1079,21 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 	return ret;
 }
 
+static void
+i915_context_ppgtt_root_restore(struct intel_vgpu_submission *s)
+{
+	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
+	int i;
+
+	if (i915_vm_is_48bit(&i915_ppgtt->vm))
+		px_dma(&i915_ppgtt->pml4) = s->i915_context_pml4;
+	else {
+		for (i = 0; i < GEN8_3LVL_PDPES; i++)
+			px_dma(i915_ppgtt->pdp.page_directory[i]) =
+						s->i915_context_pdps[i];
+	}
+}
+
 /**
  * intel_vgpu_clean_submission - free submission-related resource for vGPU
  * @vgpu: a vGPU
@@ -1091,6 +1106,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 	struct intel_vgpu_submission *s = &vgpu->submission;
 
 	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
+	i915_context_ppgtt_root_restore(s);
 	i915_gem_context_put(s->shadow_ctx);
 	kmem_cache_destroy(s->workloads);
 }
@@ -1116,6 +1132,21 @@ void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
 	s->ops->reset(vgpu, engine_mask);
 }
 
+static void
+i915_context_ppgtt_root_save(struct intel_vgpu_submission *s)
+{
+	struct i915_hw_ppgtt *i915_ppgtt = s->shadow_ctx->ppgtt;
+	int i;
+
+	if (i915_vm_is_48bit(&i915_ppgtt->vm))
+		s->i915_context_pml4 = px_dma(&i915_ppgtt->pml4);
+	else {
+		for (i = 0; i < GEN8_3LVL_PDPES; i++)
+			s->i915_context_pdps[i] =
+				px_dma(i915_ppgtt->pdp.page_directory[i]);
+	}
+}
+
 /**
  * intel_vgpu_setup_submission - setup submission-related resource for vGPU
  * @vgpu: a vGPU
@@ -1138,6 +1169,8 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	if (IS_ERR(s->shadow_ctx))
 		return PTR_ERR(s->shadow_ctx);
 
+	i915_context_ppgtt_root_save(s);
+
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
 	s->workloads = kmem_cache_create_usercopy("gvt-g_vgpu_workload",

commit 4f15665ccbba434b2c81a3ba833941de6afea95e
Author: Xiong Zhang <xiong.y.zhang@intel.com>
Date:   Thu Oct 18 13:40:31 2018 +0800

    drm/i915: Add ppgtt to GVT GEM context
    
    Currently the guest couldn't boot up under GVT-g environment as the
    following call trace exists:
    [  272.504762] BUG: unable to handle kernel NULL pointer dereference at 0000000000000100
    [  272.504834] Call Trace:
    [  272.504852]  execlists_context_pin+0x2b2/0x520 [i915]
    [  272.504869]  intel_gvt_scan_and_shadow_workload+0x50/0x4d0 [i915]
    [  272.504887]  intel_vgpu_create_workload+0x3e2/0x570 [i915]
    [  272.504901]  intel_vgpu_submit_execlist+0xc0/0x2a0 [i915]
    [  272.504916]  elsp_mmio_write+0xc7/0x130 [i915]
    [  272.504930]  intel_vgpu_mmio_reg_rw+0x24a/0x4c0 [i915]
    [  272.504944]  intel_vgpu_emulate_mmio_write+0xac/0x240 [i915]
    [  272.504947]  intel_vgpu_rw+0x22d/0x270 [kvmgt]
    [  272.504949]  intel_vgpu_write+0x164/0x1f0 [kvmgt]
    
    GVT GEM context is created by i915_gem_context_create_gvt() which
    doesn't allocate ppgtt. So GVT GEM context structure doesn't have
    a valid i915_hw_ppgtt.
    
    This patch create ppgtt table at GVT GEM context creation, then assign
    shadow ppgtt's root table address to this ppgtt when shadow ppgtt will
    be used on GPU. So GVT GEM context has valid ppgtt address. But note
    that this ppgtt only contain valid ppgtt root table address, the table
    entry in this ppgtt structure are invalid.
    
    Fixes:4a3d3f6785be("drm/i915: Match code to comment and enforce ppgtt for execlists")
    
    Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/1539841231-3157-1-git-send-email-xiong.y.zhang@intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ea34003d6dd2..b8fbe3fabea3 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -334,6 +334,28 @@ static void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 	i915_gem_object_put(wa_ctx->indirect_ctx.obj);
 }
 
+static int set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,
+					 struct i915_gem_context *ctx)
+{
+	struct intel_vgpu_mm *mm = workload->shadow_mm;
+	struct i915_hw_ppgtt *ppgtt = ctx->ppgtt;
+	int i = 0;
+
+	if (mm->type != INTEL_GVT_MM_PPGTT || !mm->ppgtt_mm.shadowed)
+		return -1;
+
+	if (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {
+		px_dma(&ppgtt->pml4) = mm->ppgtt_mm.shadow_pdps[0];
+	} else {
+		for (i = 0; i < GVT_RING_CTX_NR_PDPS; i++) {
+			px_dma(ppgtt->pdp.page_directory[i]) =
+				mm->ppgtt_mm.shadow_pdps[i];
+		}
+	}
+
+	return 0;
+}
+
 /**
  * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
  * shadow it as well, include ringbuffer,wa_ctx and ctx.
@@ -358,6 +380,12 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (workload->req)
 		return 0;
 
+	ret = set_context_ppgtt_from_shadow(workload, shadow_ctx);
+	if (ret < 0) {
+		gvt_vgpu_err("workload shadow ppgtt isn't ready\n");
+		return ret;
+	}
+
 	/* pin shadow context by gvt even the shadow context will be pinned
 	 * when i915 alloc request. That is because gvt will update the guest
 	 * context from shadow context when workload is completed, and at that

commit b1c1566822ab489a945dfdafee651aa29de160c7
Merge: 1f3eb3461f58 a28957b8f10b
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Sep 11 11:52:54 2018 +1000

    Merge tag 'drm-intel-next-2018-09-06-2' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    Merge tag 'gvt-next-2018-09-04'
    drm-intel-next-2018-09-06-1:
    UAPI Changes:
    - GGTT coherency GETPARAM: GGTT has turned out to be non-coherent for some
      platforms, which we've failed to communicate to userspace so far. SNA was
      modified to do extra flushing on non-coherent GGTT access, while Mesa will
      mitigate by always requiring WC mapping (which is non-coherent anyway).
    - Neuter Resource Streamer uAPI: There never really were users for the feature,
      so neuter it while keeping the interface bits for compatibility. This is a
      long due item from past.
    
    Cross-subsystem Changes:
    - Backmerge of branch drm-next-4.19 for DP_DPCD_REV_14 changes
    
    Core Changes:
    - None
    
    Driver Changes:
    
    - A load of Icelake (ICL) enabling patches (Paulo, Manasi)
    - Enabled full PPGTT for IVB,VLV and HSW (Chris)
    - Bugzilla #107113: Distribute DDB based on display resolutions (Mahesh)
    - Bugzillas #100023,#107476,#94921: Support limited range DP displays (Jani)
    - Bugzilla #107503: Increase LSPCON timeout (Fredrik)
    - Avoid boosting GPU due to an occasional stall in interactive workloads (Chris)
    - Apply GGTT coherency W/A only for affected systems instead of all (Chris)
    - Fix for infinite link training loop for faulty USB-C MST hubs (Nathan)
    - Keep KMS functional on Gen4 and earlier when GPU is wedged (Chris)
    - Stop holding ppGTT reference from closed VMAs (Chris)
    - Clear error registers after error capture (Lionel)
    - Various Icelake fixes (Anusha, Jyoti, Ville, Tvrtko)
    - Add missing Coffeelake (CFL) PCI IDs (Rodrigo)
    - Flush execlists tasklet directly from reset-finish (Chris)
    - Fix LPE audio runtime PM (Chris)
    - Fix detection of out of range surface positions (GLK/CNL) (Ville)
    - Remove wait-for-idle for PSR2 (Dhinakaran)
    - Power down existing display hardware resources when display is disabled (Chris)
    - Don't allow runtime power management if RC6 doesn't exist (Chris)
    - Add debugging checks for runtime power management paths (Imre)
    - Increase symmetry in display power init/fini paths (Imre)
    - Isolate GVT specific macros from i915_reg.h (Lucas)
    - Increase symmetry in power management enable/disable paths (Chris)
    - Increase IP disable timeout to 100 ms to avoid DRM_ERROR (Imre)
    - Fix memory leak from HDMI HDCP write function (Brian, Rodrigo)
    - Reject Y/Yf tiling on interlaced modes (Ville)
    - Use a cached mapping for the physical HWS on older gens (Chris)
    - Force slow path of writing relocations to buffer if unable to write to userspace (Chris)
    - Do a full device reset after being wedged (Chris)
    - Keep forcewake counts over reset (in case of debugfs user) (Imre, Chris)
    - Avoid false-positive errors from power wells during init (Imre)
    - Reset engines forcibly in exchange of declaring whole device wedged (Mika)
    - Reduce context HW ID lifetime in preparation for Icelake (Chris)
    - Attempt to recover from module load failures (Chris)
    - Keep select interrupts over a reset to avoid missing/losing them (Chris)
    - GuC submission backend improvements (Jakub)
    - Terminate context images with BB_END (Chris, Lionel)
    - Make GCC evaluate GGTT view struct size assertions again (Ville)
    - Add selftest to exercise suspend/hibernate code-paths for GEM (Chris)
    - Use a full emulation of a user ppgtt context in selftests (Chris)
    - Exercise resetting in the middle of a wait-on-fence in selftests (Chris)
    - Fix coherency issues on selftests for Baytrail (Chris)
    - Various other GEM fixes / self-test updates (Chris, Matt)
    - GuC doorbell self-tests (Daniele)
    - PSR mode control through debugfs for IGTs (Maarten)
    - Degrade expected WM latency errors to DRM_DEBUG_KMS (Chris)
    - Cope with errors better in MST link training (Dhinakaran)
    - Fix WARN on KBL external displays (Azhar)
    - Power well code cleanups (Imre)
    - Fixes to PSR debugging (Dhinakaran)
    - Make forcewake errors louder for easier catching in CI (WARNs) (Chris)
    - Fortify tiling code against programmer errors (Chris)
    - Bunch of fixes for CI exposed corner cases (multiple authors, mostly Chris)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180907105446.GA22860@jlahtine-desk.ger.corp.intel.com

commit 5781cf82553ce1c91aa2173f9def10680275cddb
Merge: d4da8a4d4004 69ca5af4ff9a
Author: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Date:   Thu Sep 6 16:51:50 2018 +0300

    Merge tag 'gvt-next-2018-09-04' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    gvt-next-2018-09-04
    
    - guest context shadow optimization for restore inhibit one (Yan)
    - cmd parser optimization (Yan)
    - W=1 warning fixes (Zhenyu)
    
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    
    # Conflicts:
    #       drivers/gpu/drm/i915/gvt/reg.h
    From: Zhenyu Wang <zhenyuw@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180904030154.GG20737@zhen-hp.sh.intel.com

commit f9090d4c22130c861b9e00e063812ac69d93a4a2
Author: Hang Yuan <hang.yuan@linux.intel.com>
Date:   Tue Aug 7 18:29:21 2018 +0800

    drm/i915/gvt: free workload in vgpu release
    
    Some workloads may be prepared in vgpu's queue but not be scheduled
    to run yet. If vgpu is released at this time, they will not be freed
    in workload complete callback and so need to be freed in vgpu release
    operation.
    
    Add new vgpu_release operation in gvt_ops to stop vgpu and release
    runtime resources. gvt_ops vgpu_deactivate operation will only stop
    vgpu.
    
    v2: add new gvt ops to clean vgpu running status (Xiong Zhang)
    
    Signed-off-by: Hang Yuan <hang.yuan@linux.intel.com>
    Reviewed-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b0e566956b8d..43aa058e29fc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -784,7 +784,8 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	kunmap(page);
 }
 
-static void clean_workloads(struct intel_vgpu *vgpu, unsigned long engine_mask)
+void intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,
+				unsigned long engine_mask)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
@@ -879,7 +880,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		 * cleaned up during the resetting process later, so doing
 		 * the workload clean up here doesn't have any impact.
 		 **/
-		clean_workloads(vgpu, ENGINE_MASK(ring_id));
+		intel_vgpu_clean_workloads(vgpu, ENGINE_MASK(ring_id));
 	}
 
 	workload->complete(workload);
@@ -1081,7 +1082,7 @@ void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
 	if (!s->active)
 		return;
 
-	clean_workloads(vgpu, engine_mask);
+	intel_vgpu_clean_workloads(vgpu, engine_mask);
 	s->ops->reset(vgpu, engine_mask);
 }
 

commit a752b070a67823174565322cc48b2668daf9a8da
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Tue Jul 31 11:02:12 2018 +0800

    drm/i915/gvt: Fix function comment doc errors
    
    Caught by W=1 to fix left wrong function comment doc.
    
    Reviewed-by: Hang Yuan <hang.yuan@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 049c0ea324fa..cd6f63b5a40a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1135,6 +1135,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 /**
  * intel_vgpu_select_submission_ops - select virtual submission interface
  * @vgpu: a vGPU
+ * @engine_mask: either ALL_ENGINES or target engine mask
  * @interface: expected vGPU virtual submission interface
  *
  * This function is called when guest configures submission interface.
@@ -1187,7 +1188,7 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 
 /**
  * intel_vgpu_destroy_workload - destroy a vGPU workload
- * @vgpu: a vGPU
+ * @workload: workload to destroy
  *
  * This function is called when destroy a vGPU workload.
  *
@@ -1279,6 +1280,7 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 /**
  * intel_vgpu_create_workload - create a vGPU workload
  * @vgpu: a vGPU
+ * @ring_id: ring index
  * @desc: a guest context descriptor
  *
  * This function is called when creating a vGPU workload.

commit 8bfa02c885ee538353f52b36d16de90655e1bcde
Author: Zhao Yan <yan.y.zhao@intel.com>
Date:   Wed Aug 1 00:15:31 2018 -0400

    drm/i915/gvt: only copy the first page for restore inhibit context
    
    if a context is a restore inhibit context, gfx hw only load the first page
    for ring context, so we only need to copy from guest the 1 page too.
    
    v3: use "return" instead of "goto" for inhibit case. (zhenyu wang)
    v2: move judgement of restore inhibit to a macro in  mmio_context.h
    
    Signed-off-by: Zhao Yan <yan.y.zhao@intel.com>
    Acked-by: Hang Yuan <hang.yuan@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 928818f218f7..049c0ea324fa 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -132,35 +132,6 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	unsigned long context_gpa, context_page_num;
 	int i;
 
-	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
-			workload->ctx_desc.lrca);
-
-	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
-
-	context_page_num = context_page_num >> PAGE_SHIFT;
-
-	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
-		context_page_num = 19;
-
-	i = 2;
-
-	while (i < context_page_num) {
-		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
-				(u32)((workload->ctx_desc.lrca + i) <<
-				I915_GTT_PAGE_SHIFT));
-		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
-			gvt_vgpu_err("Invalid guest context descriptor\n");
-			return -EFAULT;
-		}
-
-		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
-		dst = kmap(page);
-		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
-				I915_GTT_PAGE_SIZE);
-		kunmap(page);
-		i++;
-	}
-
 	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
 	shadow_ring_context = kmap(page);
 
@@ -195,6 +166,37 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 
 	sr_oa_regs(workload, (u32 *)shadow_ring_context, false);
 	kunmap(page);
+
+	if (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val))
+		return 0;
+
+	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
+			workload->ctx_desc.lrca);
+
+	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
+
+	context_page_num = context_page_num >> PAGE_SHIFT;
+
+	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+		context_page_num = 19;
+
+	i = 2;
+	while (i < context_page_num) {
+		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
+				(u32)((workload->ctx_desc.lrca + i) <<
+				I915_GTT_PAGE_SHIFT));
+		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
+			gvt_vgpu_err("Invalid guest context descriptor\n");
+			return -EFAULT;
+		}
+
+		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
+		dst = kmap(page);
+		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
+				I915_GTT_PAGE_SIZE);
+		kunmap(page);
+		i++;
+	}
 	return 0;
 }
 

commit a523697857cdfb6a548f6caf38f42f4fe0f7d757
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Jul 6 11:39:44 2018 +0100

    drm/i915: Start returning an error from i915_vma_move_to_active()
    
    Handling such a late error in request construction is tricky, but to
    accommodate future patches which may allocate here, we potentially could
    err. To handle the error after already adjusting global state to track
    the new request, we must finish and submit the request. But we don't
    want to use the request as not everything is being tracked by it, so we
    opt to cancel the commands inside the request.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180706103947.15919-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 928818f218f7..b0e566956b8d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -476,7 +476,11 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			i915_gem_obj_finish_shmem_access(bb->obj);
 			bb->accessing = false;
 
-			i915_vma_move_to_active(bb->vma, workload->req, 0);
+			ret = i915_vma_move_to_active(bb->vma,
+						      workload->req,
+						      0);
+			if (ret)
+				goto err;
 		}
 	}
 	return 0;

commit 47d9d3be5925688dfcc7407b0ae70013ba2dce2e
Author: Colin Xu <colin.xu@intel.com>
Date:   Mon Jun 11 15:39:36 2018 +0800

    drm/i915/gvt: Enable force wake support for BXT.
    
    BXT forcewake is handled in the same way as SKL/KBL.
    
    v2: Add missing inhibit_context restore for BXT.
    
    Signed-off-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 462cf560492e..928818f218f7 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -297,7 +297,8 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	void *shadow_ring_buffer_va;
 	u32 *cs;
 
-	if (IS_KABYLAKE(req->i915) && is_inhibit_context(req->hw_context))
+	if ((IS_KABYLAKE(req->i915) || IS_BROXTON(req->i915))
+		&& is_inhibit_context(req->hw_context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/* allocate shadow ring buffer */
@@ -904,7 +905,8 @@ static int workload_thread(void *priv)
 	struct intel_vgpu *vgpu = NULL;
 	int ret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv)
-			|| IS_KABYLAKE(gvt->dev_priv);
+			|| IS_KABYLAKE(gvt->dev_priv)
+			|| IS_BROXTON(gvt->dev_priv);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
 
 	kfree(p);

commit 1417fad75cb4eddc8d50604be88ca9a8a8de4c71
Author: Xinyun Liu <xinyun.liu@intel.com>
Date:   Thu Jun 7 22:48:42 2018 +0800

    drm/i915/gvt: use array to avoid potential buffer overflow
    
    Array 'pdp_pair' of size 1 may use index value(s) 1..7.
    Changed to pdps[8] to avoid confusion.
    
    Signed-off-by: Xinyun Liu <xinyun.liu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index cf5a22cb6e06..462cf560492e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -45,11 +45,10 @@ static void set_context_pdp_root_pointer(
 		struct execlist_ring_context *ring_context,
 		u32 pdp[8])
 {
-	struct execlist_mmio_pair *pdp_pair = &ring_context->pdp3_UDW;
 	int i;
 
 	for (i = 0; i < 8; i++)
-		pdp_pair[i].val = pdp[7 - i];
+		ring_context->pdps[i].val = pdp[7 - i];
 }
 
 static void update_shadow_pdps(struct intel_vgpu_workload *workload)
@@ -1230,7 +1229,7 @@ static void read_guest_pdps(struct intel_vgpu *vgpu,
 	u64 gpa;
 	int i;
 
-	gpa = ring_context_gpa + RING_CTX_OFF(pdp3_UDW.val);
+	gpa = ring_context_gpa + RING_CTX_OFF(pdps[0].val);
 
 	for (i = 0; i < 8; i++)
 		intel_gvt_hypervisor_read_gpa(vgpu,

commit 0766e2efc6968179ceec9828caf740ec0ac58a15
Merge: 9a512e23f173 14c3f8425080
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Jun 7 10:24:50 2018 +0800

    Merge tag 'drm-intel-next-2018-06-06' into gvt-next
    
    Backmerge for recent request->hw_context change and
    new vGPU huge page capability definition.
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

commit 6bb2a2af8b1b38a5e33984f5fdda2638317ed8fd
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Mon May 21 16:17:52 2018 +0800

    drm/i915/gvt: Fix crash after request->hw_context change
    
    When we do shadowing, workload's request might not be allocated yet,
    so we still require shadow context's object. And when complete workload,
    delay to zero workload's request pointer after used for update guest context.
    
    v2: Move request alloc earlier as already try to track shadow status
    depending on request state, which also facilitate to use request->hw_context
    for target engine context reference.
    
    Fixes: 1fc44d9b1afb ("drm/i915: Store a pointer to intel_context in i915_request")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180521081752.31056-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index e1760030dda1..7f5e01df95ee 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -348,6 +348,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
 	struct intel_context *ce;
+	struct i915_request *rq;
 	int ret;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
@@ -386,46 +387,26 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 			goto err_shadow;
 	}
 
-	ret = populate_shadow_context(workload);
-	if (ret)
-		goto err_shadow;
-
-	return 0;
-
-err_shadow:
-	release_shadow_wa_ctx(&workload->wa_ctx);
-err_unpin:
-	intel_context_unpin(ce);
-	return ret;
-}
-
-static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
-{
-	int ring_id = workload->ring_id;
-	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
-	struct i915_request *rq;
-	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
-	int ret;
-
-	rq = i915_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
+	rq = i915_request_alloc(engine, shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
 		ret = PTR_ERR(rq);
-		goto err_unpin;
+		goto err_shadow;
 	}
-
-	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
-
 	workload->req = i915_request_get(rq);
-	ret = copy_workload_to_ring_buffer(workload);
+
+	ret = populate_shadow_context(workload);
 	if (ret)
-		goto err_unpin;
-	return 0;
+		goto err_req;
 
-err_unpin:
+	return 0;
+err_req:
+	rq = fetch_and_zero(&workload->req);
+	i915_request_put(rq);
+err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
+err_unpin:
+	intel_context_unpin(ce);
 	return ret;
 }
 
@@ -609,7 +590,7 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 		goto err_unpin_mm;
 	}
 
-	ret = intel_gvt_generate_request(workload);
+	ret = copy_workload_to_ring_buffer(workload);
 	if (ret) {
 		gvt_vgpu_err("fail to generate request\n");
 		goto err_unpin_mm;
@@ -823,7 +804,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		scheduler->current_workload[ring_id];
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_request *rq;
+	struct i915_request *rq = workload->req;
 	int event;
 
 	mutex_lock(&gvt->lock);
@@ -832,7 +813,6 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	 * switch to make sure request is completed.
 	 * For the workload w/o request, directly complete the workload.
 	 */
-	rq = fetch_and_zero(&workload->req);
 	if (rq) {
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
@@ -863,7 +843,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		intel_context_unpin(rq->hw_context);
 		mutex_unlock(&rq->i915->drm.struct_mutex);
 
-		i915_request_put(rq);
+		i915_request_put(fetch_and_zero(&workload->req));
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",

commit 1fc44d9b1afb0afe46acd99bdfdf793805a850e1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu May 17 22:26:32 2018 +0100

    drm/i915: Store a pointer to intel_context in i915_request
    
    To ease the frequent and ugly pointer dance of
    &request->gem_context->engine[request->engine->id] during request
    submission, store that pointer as request->hw_context. One major
    advantage that we will exploit later is that this decouples the logical
    context state from the engine itself.
    
    v2: Set mock_context->ops so we don't crash and burn in selftests.
        Cleanups from Tvrtko.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180517212633.24934-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 17f9f8d7e148..e1760030dda1 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -54,11 +54,8 @@ static void set_context_pdp_root_pointer(
 
 static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 {
-	struct intel_vgpu *vgpu = workload->vgpu;
-	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->__engine[ring_id].state->obj;
+		workload->req->hw_context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 
@@ -128,9 +125,8 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
 	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->__engine[ring_id].state->obj;
+		workload->req->hw_context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *dst;
@@ -280,10 +276,8 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
-static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
-		struct intel_engine_cs *engine)
+static void shadow_context_descriptor_update(struct intel_context *ce)
 {
-	struct intel_context *ce = to_intel_context(ctx, engine);
 	u64 desc = 0;
 
 	desc = ce->lrc_desc;
@@ -292,7 +286,7 @@ static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
 	 * like GEN8_CTX_* cached in desc_template
 	 */
 	desc &= U64_MAX << 12;
-	desc |= ctx->desc_template & ((1ULL << 12) - 1);
+	desc |= ce->gem_context->desc_template & ((1ULL << 12) - 1);
 
 	ce->lrc_desc = desc;
 }
@@ -300,12 +294,11 @@ static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
 static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
+	struct i915_request *req = workload->req;
 	void *shadow_ring_buffer_va;
 	u32 *cs;
-	struct i915_request *req = workload->req;
 
-	if (IS_KABYLAKE(req->i915) &&
-	    is_inhibit_context(req->gem_context, req->engine->id))
+	if (IS_KABYLAKE(req->i915) && is_inhibit_context(req->hw_context))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/* allocate shadow ring buffer */
@@ -353,60 +346,56 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	int ring_id = workload->ring_id;
-	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	struct intel_ring *ring;
+	struct intel_engine_cs *engine = dev_priv->engine[workload->ring_id];
+	struct intel_context *ce;
 	int ret;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
 
-	if (workload->shadowed)
+	if (workload->req)
 		return 0;
 
+	/* pin shadow context by gvt even the shadow context will be pinned
+	 * when i915 alloc request. That is because gvt will update the guest
+	 * context from shadow context when workload is completed, and at that
+	 * moment, i915 may already unpined the shadow context to make the
+	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
+	 * the guest context, gvt can unpin the shadow_ctx safely.
+	 */
+	ce = intel_context_pin(shadow_ctx, engine);
+	if (IS_ERR(ce)) {
+		gvt_vgpu_err("fail to pin shadow context\n");
+		return PTR_ERR(ce);
+	}
+
 	shadow_ctx->desc_template &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
 	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	if (!test_and_set_bit(ring_id, s->shadow_ctx_desc_updated))
-		shadow_context_descriptor_update(shadow_ctx,
-					dev_priv->engine[ring_id]);
+	if (!test_and_set_bit(workload->ring_id, s->shadow_ctx_desc_updated))
+		shadow_context_descriptor_update(ce);
 
 	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
-		goto err_scan;
+		goto err_unpin;
 
 	if ((workload->ring_id == RCS) &&
 	    (workload->wa_ctx.indirect_ctx.size != 0)) {
 		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
 		if (ret)
-			goto err_scan;
-	}
-
-	/* pin shadow context by gvt even the shadow context will be pinned
-	 * when i915 alloc request. That is because gvt will update the guest
-	 * context from shadow context when workload is completed, and at that
-	 * moment, i915 may already unpined the shadow context to make the
-	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
-	 * the guest context, gvt can unpin the shadow_ctx safely.
-	 */
-	ring = intel_context_pin(shadow_ctx, engine);
-	if (IS_ERR(ring)) {
-		ret = PTR_ERR(ring);
-		gvt_vgpu_err("fail to pin shadow context\n");
-		goto err_shadow;
+			goto err_shadow;
 	}
 
 	ret = populate_shadow_context(workload);
 	if (ret)
-		goto err_unpin;
-	workload->shadowed = true;
+		goto err_shadow;
+
 	return 0;
 
-err_unpin:
-	intel_context_unpin(shadow_ctx, engine);
 err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
-err_scan:
+err_unpin:
+	intel_context_unpin(ce);
 	return ret;
 }
 
@@ -414,7 +403,6 @@ static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
 {
 	int ring_id = workload->ring_id;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
-	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct i915_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
@@ -437,7 +425,6 @@ static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
 	return 0;
 
 err_unpin:
-	intel_context_unpin(shadow_ctx, engine);
 	release_shadow_wa_ctx(&workload->wa_ctx);
 	return ret;
 }
@@ -517,21 +504,13 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
-static int update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
+static void update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 {
-	struct intel_vgpu_workload *workload = container_of(wa_ctx,
-					struct intel_vgpu_workload,
-					wa_ctx);
-	int ring_id = workload->ring_id;
-	struct intel_vgpu_submission *s = &workload->vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
-	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->__engine[ring_id].state->obj;
-	struct execlist_ring_context *shadow_ring_context;
-	struct page *page;
-
-	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap_atomic(page);
+	struct intel_vgpu_workload *workload =
+		container_of(wa_ctx, struct intel_vgpu_workload, wa_ctx);
+	struct i915_request *rq = workload->req;
+	struct execlist_ring_context *shadow_ring_context =
+		(struct execlist_ring_context *)rq->hw_context->lrc_reg_state;
 
 	shadow_ring_context->bb_per_ctx_ptr.val =
 		(shadow_ring_context->bb_per_ctx_ptr.val &
@@ -539,9 +518,6 @@ static int update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 	shadow_ring_context->rcs_indirect_ctx.val =
 		(shadow_ring_context->rcs_indirect_ctx.val &
 		(~INDIRECT_CTX_ADDR_MASK)) | wa_ctx->indirect_ctx.shadow_gma;
-
-	kunmap_atomic(shadow_ring_context);
-	return 0;
 }
 
 static int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
@@ -670,12 +646,9 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	int ring_id = workload->ring_id;
-	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	int ret = 0;
+	int ret;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
 		ring_id, workload);
@@ -687,10 +660,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 		goto out;
 
 	ret = prepare_workload(workload);
-	if (ret) {
-		intel_context_unpin(shadow_ctx, engine);
-		goto out;
-	}
 
 out:
 	if (ret)
@@ -765,27 +734,23 @@ static struct intel_vgpu_workload *pick_next_workload(
 
 static void update_guest_context(struct intel_vgpu_workload *workload)
 {
+	struct i915_request *rq = workload->req;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
-	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
-	int ring_id = workload->ring_id;
-	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->__engine[ring_id].state->obj;
+	struct drm_i915_gem_object *ctx_obj = rq->hw_context->state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *src;
 	unsigned long context_gpa, context_page_num;
 	int i;
 
-	gvt_dbg_sched("ring id %d workload lrca %x\n", ring_id,
-			workload->ctx_desc.lrca);
-
-	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
+	gvt_dbg_sched("ring id %d workload lrca %x\n", rq->engine->id,
+		      workload->ctx_desc.lrca);
 
+	context_page_num = rq->engine->context_size;
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
-	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+	if (IS_BROADWELL(gvt->dev_priv) && rq->engine->id == RCS)
 		context_page_num = 19;
 
 	i = 2;
@@ -858,6 +823,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		scheduler->current_workload[ring_id];
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_request *rq;
 	int event;
 
 	mutex_lock(&gvt->lock);
@@ -866,11 +832,8 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	 * switch to make sure request is completed.
 	 * For the workload w/o request, directly complete the workload.
 	 */
-	if (workload->req) {
-		struct drm_i915_private *dev_priv =
-			workload->vgpu->gvt->dev_priv;
-		struct intel_engine_cs *engine =
-			dev_priv->engine[workload->ring_id];
+	rq = fetch_and_zero(&workload->req);
+	if (rq) {
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
 
@@ -886,8 +849,6 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 				workload->status = 0;
 		}
 
-		i915_request_put(fetch_and_zero(&workload->req));
-
 		if (!workload->status && !(vgpu->resetting_eng &
 					   ENGINE_MASK(ring_id))) {
 			update_guest_context(workload);
@@ -896,10 +857,13 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 					 INTEL_GVT_EVENT_MAX)
 				intel_vgpu_trigger_virtual_event(vgpu, event);
 		}
-		mutex_lock(&dev_priv->drm.struct_mutex);
+
 		/* unpin shadow ctx as the shadow_ctx update is done */
-		intel_context_unpin(s->shadow_ctx, engine);
-		mutex_unlock(&dev_priv->drm.struct_mutex);
+		mutex_lock(&rq->i915->drm.struct_mutex);
+		intel_context_unpin(rq->hw_context);
+		mutex_unlock(&rq->i915->drm.struct_mutex);
+
+		i915_request_put(rq);
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",
@@ -1270,7 +1234,6 @@ alloc_workload(struct intel_vgpu *vgpu)
 	atomic_set(&workload->shadow_ctx_active, 0);
 
 	workload->status = -EINPROGRESS;
-	workload->shadowed = false;
 	workload->vgpu = vgpu;
 
 	return workload;

commit 4e0d64dba816adf18c17488d38ede67a3d0e9b40
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu May 17 22:26:30 2018 +0100

    drm/i915: Move request->ctx aside
    
    In the next patch, we want to store the intel_context pointer inside
    i915_request, as it is frequently access via a convoluted dance when
    submitting the request to hw. Having two context pointers inside
    i915_request leads to confusion so first rename the existing
    i915_gem_context pointer to i915_request.gem_context.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180517212633.24934-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index c2d183b91500..17f9f8d7e148 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -205,7 +205,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 
 static inline bool is_gvt_request(struct i915_request *req)
 {
-	return i915_gem_context_force_single_submission(req->ctx);
+	return i915_gem_context_force_single_submission(req->gem_context);
 }
 
 static void save_ring_hw_state(struct intel_vgpu *vgpu, int ring_id)
@@ -305,7 +305,7 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	struct i915_request *req = workload->req;
 
 	if (IS_KABYLAKE(req->i915) &&
-	    is_inhibit_context(req->ctx, req->engine->id))
+	    is_inhibit_context(req->gem_context, req->engine->id))
 		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/* allocate shadow ring buffer */

commit 9a512e23f173a3598709b74d6ccf9a6616403967
Author: Colin Xu <colin.xu@intel.com>
Date:   Sat May 19 12:28:55 2018 +0800

    drm/i915/gvt: Use sched_lock to protect gvt scheduler logic.
    
    The scheduler lock(gvt->sched_lock) is used to protect gvt
    scheduler logic, including the gvt scheduler structure(gvt->scheduler
    and per vgpu schedule data(vgpu->sched_data, vgpu->sched_ctl).
    
    v9:
      - Change commit author since the patches are improved a lot compared
        with original version.
        Original author: Pei Zhang <pei.zhang@intel.com>
      - Rebase to latest gvt-staging.
    v8:
      - Correct coding wqstyle.
      - Rebase to latest gvt-staging.
    v7:
      - Remove gtt_lock since already proteced by gvt_lock and vgpu_lock.
    v6:
      - Rebase to latest gvt-staging.
    v5:
      - Rebase to latest gvt-staging.
    v4:
      - Rebase to latest gvt-staging.
    v3: update to latest code base
    
    Signed-off-by: Pei Zhang <pei.zhang@intel.com>
    Signed-off-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index c0848dcf79c7..847f295e6755 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -715,7 +715,7 @@ static struct intel_vgpu_workload *pick_next_workload(
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
 
-	mutex_lock(&gvt->lock);
+	mutex_lock(&gvt->sched_lock);
 
 	/*
 	 * no current vgpu / will be scheduled out / no workload
@@ -761,7 +761,7 @@ static struct intel_vgpu_workload *pick_next_workload(
 
 	atomic_inc(&workload->vgpu->submission.running_workload_num);
 out:
-	mutex_unlock(&gvt->lock);
+	mutex_unlock(&gvt->sched_lock);
 	return workload;
 }
 
@@ -862,8 +862,8 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	int event;
 
-	mutex_lock(&gvt->lock);
 	mutex_lock(&vgpu->vgpu_lock);
+	mutex_lock(&gvt->sched_lock);
 
 	/* For the workload w/ request, needs to wait for the context
 	 * switch to make sure request is completed.
@@ -941,8 +941,8 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	if (gvt->scheduler.need_reschedule)
 		intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);
 
+	mutex_unlock(&gvt->sched_lock);
 	mutex_unlock(&vgpu->vgpu_lock);
-	mutex_unlock(&gvt->lock);
 }
 
 struct workload_thread_param {

commit f25a49ab8ab9c1b5587837c8a386b276403f315c
Author: Colin Xu <colin.xu@intel.com>
Date:   Sat May 19 12:28:54 2018 +0800

    drm/i915/gvt: Use vgpu_lock to protect per vgpu access
    
    The patch set splits out 2 small locks from the original big gvt lock:
      - vgpu_lock protects per-vGPU data and logic, especially the vGPU
        trap emulation path.
      - sched_lock protects gvt scheudler structure, context schedule logic
        and vGPU's schedule data.
    
    Use vgpu_lock to replace the gvt big lock. By doing this, the
    mmio read/write trap path, vgpu virtual event emulation and other
    vgpu related process, would be protected under per vgpu_lock.
    
    v9:
      - Change commit author since the patches are improved a lot compared
        with original version.
        Original author: Pei Zhang <pei.zhang@intel.com>
      - Rebase to latest gvt-staging.
    v8:
      - Correct coding and comment style.
      - Rebase to latest gvt-staging.
    v7:
      - Remove gtt_lock since already proteced by gvt_lock and vgpu_lock.
      - Fix a typo in intel_gvt_deactivate_vgpu, unlock the wrong lock.
    v6:
      - Rebase to latest gvt-staging.
    v5:
      - Rebase to latest gvt-staging.
      - intel_vgpu_page_track_handler should use vgpu_lock.
    v4:
      - Rebase to latest gvt-staging.
      - Protect vgpu->active access with vgpu_lock.
      - Do not wait gpu idle in vgpu_lock.
    v3: update to latest code base
    v2: add gvt->lock in function gvt_check_vblank_emulation
    
    Performance comparison on Kabylake platform.
      - Configuration:
        Host: Ubuntu 16.04.
        Guest 1 & 2: Ubuntu 16.04.
    
    glmark2 score comparison:
      - Configuration:
        Host: glxgears.
        Guests: glmark2.
    +--------------------------------+-----------------+
    | Setup                          | glmark2 score   |
    +--------------------------------+-----------------+
    | unified lock, iommu=on         | 58~62 (avg. 60) |
    +--------------------------------+-----------------+
    | unified lock, iommu=igfx_off   | 57~61 (avg. 59) |
    +--------------------------------+-----------------+
    | per-logic lock, iommu=on       | 60~68 (avg. 64) |
    +--------------------------------+-----------------+
    | per-logic lock, iommu=igfx_off | 61~67 (avg. 64) |
    +--------------------------------+-----------------+
    
    lock_stat comparison:
      - Configuration:
        Stop lock stat immediately after boot up.
        Boot 2 VM Guests.
        Run glmark2 in guests.
        Start perf lock_stat for 20 seconds and stop again.
      - Legend: c - contentions; w - waittime-avg
    +------------+-----------------+-----------+---------------+------------+
    |            | gvt_lock        |sched_lock | vgpu_lock     | gtt_lock   |
    + lock type; +-----------------+-----------+---------------+------------+
    | iommu set  | c     | w       | c  | w    | c    | w      | c   | w    |
    +------------+-------+---------+----+------+------+--------+-----+------+
    | unified;   | 20697 | 839     |N/A | N/A  | N/A  | N/A    | N/A | N/A  |
    | on         |       |         |    |      |      |        |     |      |
    +------------+-------+---------+----+------+------+--------+-----+------+
    | unified;   | 21838 | 658.15  |N/A | N/A  | N/A  | N/A    | N/A | N/A  |
    | igfx_off   |       |         |    |      |      |        |     |      |
    +------------+-------+---------+----+------+------+--------+-----+------+
    | per-logic; | 1553  | 1599.96 |9458|429.97| 5846 | 274.33 | 0   | 0.00 |
    | on         |       |         |    |      |      |        |     |      |
    +------------+-------+---------+----+------+------+--------+-----+------+
    | per-logic; | 1911  | 1678.32 |8335|445.16| 5451 | 244.80 | 0   | 0.00 |
    | igfx_off   |       |         |    |      |      |        |     |      |
    +------------+-------+---------+----+------+------+--------+-----+------+
    
    Signed-off-by: Pei Zhang <pei.zhang@intel.com>
    Signed-off-by: Colin Xu <colin.xu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index c2d183b91500..c0848dcf79c7 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -680,6 +680,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
 		ring_id, workload);
 
+	mutex_lock(&vgpu->vgpu_lock);
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
 	ret = intel_gvt_scan_and_shadow_workload(workload);
@@ -704,6 +705,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	}
 
 	mutex_unlock(&dev_priv->drm.struct_mutex);
+	mutex_unlock(&vgpu->vgpu_lock);
 	return ret;
 }
 
@@ -861,14 +863,14 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	int event;
 
 	mutex_lock(&gvt->lock);
+	mutex_lock(&vgpu->vgpu_lock);
 
 	/* For the workload w/ request, needs to wait for the context
 	 * switch to make sure request is completed.
 	 * For the workload w/o request, directly complete the workload.
 	 */
 	if (workload->req) {
-		struct drm_i915_private *dev_priv =
-			workload->vgpu->gvt->dev_priv;
+		struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 		struct intel_engine_cs *engine =
 			dev_priv->engine[workload->ring_id];
 		wait_event(workload->shadow_ctx_status_wq,
@@ -939,6 +941,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	if (gvt->scheduler.need_reschedule)
 		intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);
 
+	mutex_unlock(&vgpu->vgpu_lock);
 	mutex_unlock(&gvt->lock);
 }
 
@@ -991,9 +994,7 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_get(gvt->dev_priv,
 					FORCEWAKE_ALL);
 
-		mutex_lock(&gvt->lock);
 		ret = dispatch_workload(workload);
-		mutex_unlock(&gvt->lock);
 
 		if (ret) {
 			vgpu = workload->vgpu;

commit 41e403d04e7050c8d88682939febcdbe117d4c82
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Wed Mar 21 15:40:32 2018 +0800

    Revert "drm/i915/gvt: set max priority for gvt context"
    
    This reverts commit 11474e9091cf2002e948647fd9f63a7f027e488a.
    
    There are issues which will block the host preemption before, instead of
    disabling it use one workaround "setting max priority for gvt context"
    to avoid the gvt context be preempted by the host. Now the issues have been
    cleared, so revert this patch to enable host preemption.
    
    v2:
    - refine description(Zhenyu)
    
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ffb45a9ee228..c2d183b91500 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1156,9 +1156,6 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	if (IS_ERR(s->shadow_ctx))
 		return PTR_ERR(s->shadow_ctx);
 
-	if (HAS_LOGICAL_RING_PREEMPTION(vgpu->gvt->dev_priv))
-		s->shadow_ctx->sched.priority = INT_MAX;
-
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
 	s->workloads = kmem_cache_create_usercopy("gvt-g_vgpu_workload",

commit ab82a0635cdf0b91a134aaae34abd4e864595c5b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Apr 30 14:15:01 2018 +0100

    drm/i915: Wrap engine->context_pin() and engine->context_unpin()
    
    Make life easier in upcoming patches by moving the context_pin and
    context_unpin vfuncs into inline helpers.
    
    v2: Fixup mock_engine to mark the context as pinned on use.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180430131503.5375-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 35f7cfd7a6b4..ffb45a9ee228 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -58,7 +58,7 @@ static void update_shadow_pdps(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->engine[ring_id].state->obj;
+		shadow_ctx->__engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 
@@ -130,7 +130,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->engine[ring_id].state->obj;
+		shadow_ctx->__engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *dst;
@@ -283,7 +283,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
 		struct intel_engine_cs *engine)
 {
-	struct intel_context *ce = &ctx->engine[engine->id];
+	struct intel_context *ce = to_intel_context(ctx, engine);
 	u64 desc = 0;
 
 	desc = ce->lrc_desc;
@@ -389,7 +389,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
 	 * the guest context, gvt can unpin the shadow_ctx safely.
 	 */
-	ring = engine->context_pin(engine, shadow_ctx);
+	ring = intel_context_pin(shadow_ctx, engine);
 	if (IS_ERR(ring)) {
 		ret = PTR_ERR(ring);
 		gvt_vgpu_err("fail to pin shadow context\n");
@@ -403,7 +403,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return 0;
 
 err_unpin:
-	engine->context_unpin(engine, shadow_ctx);
+	intel_context_unpin(shadow_ctx, engine);
 err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
 err_scan:
@@ -437,7 +437,7 @@ static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
 	return 0;
 
 err_unpin:
-	engine->context_unpin(engine, shadow_ctx);
+	intel_context_unpin(shadow_ctx, engine);
 	release_shadow_wa_ctx(&workload->wa_ctx);
 	return ret;
 }
@@ -526,7 +526,7 @@ static int update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 	struct intel_vgpu_submission *s = &workload->vgpu->submission;
 	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->engine[ring_id].state->obj;
+		shadow_ctx->__engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 
@@ -688,7 +688,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	ret = prepare_workload(workload);
 	if (ret) {
-		engine->context_unpin(engine, shadow_ctx);
+		intel_context_unpin(shadow_ctx, engine);
 		goto out;
 	}
 
@@ -771,7 +771,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	int ring_id = workload->ring_id;
 	struct drm_i915_gem_object *ctx_obj =
-		shadow_ctx->engine[ring_id].state->obj;
+		shadow_ctx->__engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
 	struct page *page;
 	void *src;
@@ -898,7 +898,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		}
 		mutex_lock(&dev_priv->drm.struct_mutex);
 		/* unpin shadow ctx as the shadow_ctx update is done */
-		engine->context_unpin(engine, s->shadow_ctx);
+		intel_context_unpin(s->shadow_ctx, engine);
 		mutex_unlock(&dev_priv->drm.struct_mutex);
 	}
 

commit 1f7e305093d24ea7e9f342ec671adbabf27f51d2
Merge: 011f22eb545a 3eda0d22ead0
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Mon Apr 23 13:17:26 2018 +0300

    Merge tag 'gvt-next-2018-04-23' of https://github.com/intel/gvt-linux into drm-intel-next-queued
    
    - Minor condition check improvment (Gustavo A. R. Silva)
    - Non-priviliged batch buffer scan (Yan Zhao)
    - Scheduling optimizations (Zhipeng Gong)
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/5dafba29-b2bd-6b94-630e-db5c009da7e3@intel.com

commit 41e7ccc19c2a52b13b77f2a489a466b140d2d235
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Thu Mar 22 13:21:54 2018 -0500

    drm/i915/gvt/scheduler: Remove unnecessary NULL checks in sr_oa_regs
    
    The checks are misleading and not required [1].
    
    [1] https://lkml.org/lkml/2018/3/19/1792
    
    Addresses-Coverity-ID: 1466017
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1bd7aa0c694a..d1a8fd88eed9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -97,7 +97,7 @@ static void sr_oa_regs(struct intel_vgpu_workload *workload,
 		i915_mmio_reg_offset(EU_PERF_CNTL6),
 	};
 
-	if (!workload || !reg_state || workload->ring_id != RCS)
+	if (workload->ring_id != RCS)
 		return;
 
 	if (save) {

commit 96bebe39b2f4533af14c509061cd2b551ca81e8d
Author: Zhao Yan <yan.y.zhao@intel.com>
Date:   Wed Apr 4 13:57:09 2018 +0800

    drm/i915/gvt: scan non-privileged batch buffer for debug purpose
    
    For perfomance purpose, scanning of non-privileged batch buffer is turned
    off by default. But for debugging purpose, it can be turned on via debugfs.
    After scanning, we submit the original non-privileged batch buffer into
    hardware, so that the scanning is only a peeking window of guest submitted
    commands and will not affect the execution results.
    
    v4:
    - refine debugfs print format&content (zhenyu wang)
    - print engine id instread of engine name to prevent potential memory leak
      in debugfs warning message. (zhenyu wang)
    
    v3:
    - change vgpu->scan_nonprivbb from type bool to u32, so it is able to
      selectively turn on/off scanning of non-privileged batch buffer on engine
      level. e.g.
      if vgpu->scan_nonprivbb=3, then it will scan non-privileged batch buffer
      on engine 0 and 1.
    - in debugfs interface to set vgpu->scan_nonprivbb, print warning message
      to warn user and explicitly tell state change in kernel log (zhenyu wang)
    v2:
    - rebase
    - update comments for start_gma_offset (henry)
    
    Signed-off-by: Zhao Yan <yan.y.zhao@intel.com>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 638abe84857c..1bd7aa0c694a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -452,12 +452,6 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 	int ret;
 
 	list_for_each_entry(bb, &workload->shadow_bb, list) {
-		bb->vma = i915_gem_object_ggtt_pin(bb->obj, NULL, 0, 0, 0);
-		if (IS_ERR(bb->vma)) {
-			ret = PTR_ERR(bb->vma);
-			goto err;
-		}
-
 		/* For privilge batch buffer and not wa_ctx, the bb_start_cmd_va
 		 * is only updated into ring_scan_buffer, not real ring address
 		 * allocated in later copy_workload_to_ring_buffer. pls be noted
@@ -469,25 +463,53 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			bb->bb_start_cmd_va = workload->shadow_ring_buffer_va
 				+ bb->bb_offset;
 
-		/* relocate shadow batch buffer */
-		bb->bb_start_cmd_va[1] = i915_ggtt_offset(bb->vma);
-		if (gmadr_bytes == 8)
-			bb->bb_start_cmd_va[2] = 0;
+		if (bb->ppgtt) {
+			/* for non-priv bb, scan&shadow is only for
+			 * debugging purpose, so the content of shadow bb
+			 * is the same as original bb. Therefore,
+			 * here, rather than switch to shadow bb's gma
+			 * address, we directly use original batch buffer's
+			 * gma address, and send original bb to hardware
+			 * directly
+			 */
+			if (bb->clflush & CLFLUSH_AFTER) {
+				drm_clflush_virt_range(bb->va,
+						bb->obj->base.size);
+				bb->clflush &= ~CLFLUSH_AFTER;
+			}
+			i915_gem_obj_finish_shmem_access(bb->obj);
+			bb->accessing = false;
+
+		} else {
+			bb->vma = i915_gem_object_ggtt_pin(bb->obj,
+					NULL, 0, 0, 0);
+			if (IS_ERR(bb->vma)) {
+				ret = PTR_ERR(bb->vma);
+				goto err;
+			}
 
-		/* No one is going to touch shadow bb from now on. */
-		if (bb->clflush & CLFLUSH_AFTER) {
-			drm_clflush_virt_range(bb->va, bb->obj->base.size);
-			bb->clflush &= ~CLFLUSH_AFTER;
-		}
+			/* relocate shadow batch buffer */
+			bb->bb_start_cmd_va[1] = i915_ggtt_offset(bb->vma);
+			if (gmadr_bytes == 8)
+				bb->bb_start_cmd_va[2] = 0;
 
-		ret = i915_gem_object_set_to_gtt_domain(bb->obj, false);
-		if (ret)
-			goto err;
+			/* No one is going to touch shadow bb from now on. */
+			if (bb->clflush & CLFLUSH_AFTER) {
+				drm_clflush_virt_range(bb->va,
+						bb->obj->base.size);
+				bb->clflush &= ~CLFLUSH_AFTER;
+			}
 
-		i915_gem_obj_finish_shmem_access(bb->obj);
-		bb->accessing = false;
+			ret = i915_gem_object_set_to_gtt_domain(bb->obj,
+					false);
+			if (ret)
+				goto err;
 
-		i915_vma_move_to_active(bb->vma, workload->req, 0);
+			i915_gem_obj_finish_shmem_access(bb->obj);
+			bb->accessing = false;
+
+			i915_vma_move_to_active(bb->vma, workload->req, 0);
+		}
 	}
 	return 0;
 err:

commit b7268c5eed0ab4f052d614b4b0e3fe8a51c9d5a1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 18 19:40:52 2018 +0100

    drm/i915: Pack params to engine->schedule() into a struct
    
    Today we only want to pass along the priority to engine->schedule(), but
    in the future we want to have much more control over the various aspects
    of the GPU during a context's execution, for example controlling the
    frequency allowed. As we need an ever growing number of parameters for
    scheduling, move those into a struct for convenience.
    
    v2: Move the anonymous struct into its own function for legibility and
    ye olde gcc.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180418184052.7129-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 638abe84857c..f3d21849b0cb 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1135,7 +1135,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		return PTR_ERR(s->shadow_ctx);
 
 	if (HAS_LOGICAL_RING_PREEMPTION(vgpu->gvt->dev_priv))
-		s->shadow_ctx->priority = INT_MAX;
+		s->shadow_ctx->sched.priority = INT_MAX;
 
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 

commit 2b4f44eec2be2688511c2b617d0e1b4f94c45ba4
Merge: 33d009cd8894 3eb2ce825ea1
Author: Dave Airlie <airlied@redhat.com>
Date:   Wed Mar 28 14:30:41 2018 +1000

    Backmerge tag 'v4.16-rc7' into drm-next
    
    Linux 4.16-rc7
    
    This was requested by Daniel, and things were getting
    a bit hard to reconcile, most of the conflicts were
    trivial though.

commit d8303075699292008ae5b2c8fc728d455b994c26
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Mon Mar 19 17:09:05 2018 +0800

    drm/i915/gvt: force to set all context control bits from guest
    
    Our shadow context content is from guest but with masked control reg like
    CTX_CONTEXT_CONTROL, we need to make sure all settings from guest would be set
    when this context is on hw, this trys to force mask enable bits for all to
    ensure every bits setting would be effective on hw.
    
    One regression found related to once inhibit bit is set, gpu engine are working
    on inhibit state until MI_LOAD_REG_IMM command or context image clear inhibit
    bit with mask bit set to 1, and val bit set to 0. In gvt-g currently workload
    has the highest priority, so gvt-g workload could trigger preempt context
    easily, preempt context set inhibit bit, then gvt-g workload is scheduled in,
    but gvt-g workload shadow context image usually doesn't set inhibit mask bit,
    so gpu is still in inhibit state when gvt workload is running. This caused gpu
    hang.
    
    Suggested-by: Zhang, Xiong <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Reviewed-by: Zhang, Xiong <xiong.y.zhang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1127bd77fc6e..a55b4975c154 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -124,8 +124,14 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_read_gpa(vgpu, workload->ring_context_gpa \
 		+ RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)
+#define COPY_REG_MASKED(name) {\
+		intel_gvt_hypervisor_read_gpa(vgpu, workload->ring_context_gpa \
+					      + RING_CTX_OFF(name.val),\
+					      &shadow_ring_context->name.val, 4);\
+		shadow_ring_context->name.val |= 0xffff << 16;\
+	}
 
-	COPY_REG(ctx_ctrl);
+	COPY_REG_MASKED(ctx_ctrl);
 	COPY_REG(ctx_timestamp);
 
 	if (ring_id == RCS) {
@@ -134,6 +140,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 		COPY_REG(rcs_indirect_ctx_offset);
 	}
 #undef COPY_REG
+#undef COPY_REG_MASKED
 
 	intel_gvt_hypervisor_read_gpa(vgpu,
 			workload->ring_context_gpa +

commit b20c0d5ce1047ba03a6709a07f31f4d7178de35c
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Wed Feb 7 18:12:15 2018 +0800

    drm/i915/gvt: Update PDPs after a vGPU mm object is pinned.
    
    The PDPs of a shadow page will only be valid after a vGPU mm is pinned.
    So the PDPs in the shadow context should be updated then.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9b92b4e25a20..1127bd77fc6e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -52,6 +52,29 @@ static void set_context_pdp_root_pointer(
 		pdp_pair[i].val = pdp[7 - i];
 }
 
+static void update_shadow_pdps(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	int ring_id = workload->ring_id;
+	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
+	struct drm_i915_gem_object *ctx_obj =
+		shadow_ctx->engine[ring_id].state->obj;
+	struct execlist_ring_context *shadow_ring_context;
+	struct page *page;
+
+	if (WARN_ON(!workload->shadow_mm))
+		return;
+
+	if (WARN_ON(!atomic_read(&workload->shadow_mm->pincount)))
+		return;
+
+	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
+	shadow_ring_context = kmap(page);
+	set_context_pdp_root_pointer(shadow_ring_context,
+			(void *)workload->shadow_mm->ppgtt_mm.shadow_pdps);
+	kunmap(page);
+}
+
 static int populate_shadow_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
@@ -112,9 +135,6 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	}
 #undef COPY_REG
 
-	set_context_pdp_root_pointer(shadow_ring_context,
-				     (void *)workload->shadow_mm->ppgtt_mm.shadow_pdps);
-
 	intel_gvt_hypervisor_read_gpa(vgpu,
 			workload->ring_context_gpa +
 			sizeof(*shadow_ring_context),
@@ -509,6 +529,8 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 		return ret;
 	}
 
+	update_shadow_pdps(workload);
+
 	ret = intel_vgpu_sync_oos_pages(workload->vgpu);
 	if (ret) {
 		gvt_vgpu_err("fail to vgpu sync oos pages\n");

commit 850555d1d31e45fc3e9a2982f81717387e8d5e1b
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed Feb 14 11:35:01 2018 +0800

    drm/i915/gvt: fix user copy warning by whitelist workload rb_tail field
    
    This is to fix warning got as:
    
    [ 6730.476938] ------------[ cut here ]------------
    [ 6730.476979] Bad or missing usercopy whitelist? Kernel memory exposure attempt detected from SLAB object 'gvt-g_vgpu_workload' (offset 120, size 4)!
    [ 6730.477021] WARNING: CPU: 2 PID: 441 at mm/usercopy.c:81 usercopy_warn+0x7e/0xa0
    [ 6730.477042] Modules linked in: tun(E) bridge(E) stp(E) llc(E) kvmgt(E) x86_pkg_temp_thermal(E) vfio_mdev(E) intel_powerclamp(E) mdev(E) coretemp(E) vfio_iommu_type1(E) vfio(E) kvm_intel(E) kvm(E) hid_generic(E) irqbypass(E) crct10dif_pclmul(E) crc32_pclmul(E) usbhid(E) i915(E) crc32c_intel(E) hid(E) ghash_clmulni_intel(E) pcbc(E) aesni_intel(E) aes_x86_64(E) crypto_simd(E) cryptd(E) glue_helper(E) intel_cstate(E) idma64(E) evdev(E) virt_dma(E) iTCO_wdt(E) intel_uncore(E) intel_rapl_perf(E) intel_lpss_pci(E) sg(E) shpchp(E) mei_me(E) pcspkr(E) iTCO_vendor_support(E) intel_lpss(E) intel_pch_thermal(E) prime_numbers(E) mei(E) mfd_core(E) video(E) acpi_pad(E) button(E) binfmt_misc(E) ip_tables(E) x_tables(E) autofs4(E) ext4(E) crc16(E) mbcache(E) jbd2(E) fscrypto(E) sd_mod(E) e1000e(E) xhci_pci(E) sdhci_pci(E)
    [ 6730.477244]  ptp(E) cqhci(E) xhci_hcd(E) pps_core(E) sdhci(E) mmc_core(E) i2c_i801(E) usbcore(E) thermal(E) fan(E)
    [ 6730.477276] CPU: 2 PID: 441 Comm: gvt workload 0 Tainted: G            E    4.16.0-rc1-gvt-staging-0213+ #127
    [ 6730.477303] Hardware name:  /NUC6i5SYB, BIOS SYSKLi35.86A.0039.2016.0316.1747 03/16/2016
    [ 6730.477326] RIP: 0010:usercopy_warn+0x7e/0xa0
    [ 6730.477340] RSP: 0018:ffffba6301223d18 EFLAGS: 00010286
    [ 6730.477355] RAX: 0000000000000000 RBX: ffff8f41caae9838 RCX: 0000000000000006
    [ 6730.477375] RDX: 0000000000000007 RSI: 0000000000000082 RDI: ffff8f41dad166f0
    [ 6730.477395] RBP: 0000000000000004 R08: 0000000000000576 R09: 0000000000000000
    [ 6730.477415] R10: ffffffffb1293fb2 R11: 00000000ffffffff R12: 0000000000000001
    [ 6730.477447] R13: ffff8f41caae983c R14: ffff8f41caae9838 R15: 00007f183ca2b000
    [ 6730.477467] FS:  0000000000000000(0000) GS:ffff8f41dad00000(0000) knlGS:0000000000000000
    [ 6730.477489] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 6730.477506] CR2: 0000559462817291 CR3: 000000028b46c006 CR4: 00000000003626e0
    [ 6730.477526] Call Trace:
    [ 6730.477537]  __check_object_size+0x9c/0x1a0
    [ 6730.477562]  __kvm_write_guest_page+0x45/0x90 [kvm]
    [ 6730.477585]  kvm_write_guest+0x46/0x80 [kvm]
    [ 6730.477599]  kvmgt_rw_gpa+0x9b/0xf0 [kvmgt]
    [ 6730.477642]  workload_thread+0xa38/0x1040 [i915]
    [ 6730.477659]  ? do_wait_intr_irq+0xc0/0xc0
    [ 6730.477673]  ? finish_wait+0x80/0x80
    [ 6730.477707]  ? clean_workloads+0x120/0x120 [i915]
    [ 6730.477722]  kthread+0x111/0x130
    [ 6730.477733]  ? _kthread_create_worker_on_cpu+0x60/0x60
    [ 6730.477750]  ? exit_to_usermode_loop+0x6f/0xb0
    [ 6730.477766]  ret_from_fork+0x35/0x40
    [ 6730.477777] Code: 48 c7 c0 20 e3 25 b1 48 0f 44 c2 41 50 51 41 51 48 89 f9 49 89 f1 4d 89 d8 4c 89 d2 48 89 c6 48 c7 c7 78 e3 25 b1 e8 b2 bc e4 ff <0f> ff 48 83 c4 18 c3 48 c7 c6 09 d0 26 b1 49 89 f1 49 89 f3 eb
    [ 6730.477849] ---[ end trace cae869c1c323e45a ]---
    
    By whitelist guest page write from workload struct allocated from kmem cache.
    
    Reviewed-by: Hang Yuan <hang.yuan@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    (cherry picked from commit 5627705406874df57fdfad3b4e0c9aedd3b007df)

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index fdf1c0bf0d55..d74d6f05c62c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1105,10 +1105,12 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
-	s->workloads = kmem_cache_create("gvt-g_vgpu_workload",
-			sizeof(struct intel_vgpu_workload), 0,
-			SLAB_HWCACHE_ALIGN,
-			NULL);
+	s->workloads = kmem_cache_create_usercopy("gvt-g_vgpu_workload",
+						  sizeof(struct intel_vgpu_workload), 0,
+						  SLAB_HWCACHE_ALIGN,
+						  offsetof(struct intel_vgpu_workload, rb_tail),
+						  sizeof_field(struct intel_vgpu_workload, rb_tail),
+						  NULL);
 
 	if (!s->workloads) {
 		ret = -ENOMEM;

commit ef75c685869ea2059f85855a7dc00148a704c36c
Author: fred gao <fred.gao@intel.com>
Date:   Thu Mar 15 13:21:10 2018 +0800

    drm/i915/gvt: Correct the privilege shadow batch buffer address
    
    Once the ring buffer is copied to ring_scan_buffer and scanned,
    the shadow batch buffer start address is only updated into
    ring_scan_buffer, not the real ring address allocated through
    intel_ring_begin in later copy_workload_to_ring_buffer.
    
    This patch is only to set the right shadow batch buffer address
    from Ring buffer, not include the shadow_wa_ctx.
    
    v2:
    - refine some comments. (Zhenyu)
    v3:
    - fix typo in title. (Zhenyu)
    v4:
    - remove the unnecessary comments. (Zhenyu)
    - add comments in bb_start_cmd_va update. (Zhenyu)
    
    Fixes: 0a53bc07f044 ("drm/i915/gvt: Separate cmd scan from request allocation")
    Cc: stable@vger.kernel.org  # v4.15
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Yulei Zhang <yulei.zhang@intel.com>
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 8caf72c1e794..fdf1c0bf0d55 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -426,6 +426,17 @@ static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 			goto err;
 		}
 
+		/* For privilge batch buffer and not wa_ctx, the bb_start_cmd_va
+		 * is only updated into ring_scan_buffer, not real ring address
+		 * allocated in later copy_workload_to_ring_buffer. pls be noted
+		 * shadow_ring_buffer_va is now pointed to real ring buffer va
+		 * in copy_workload_to_ring_buffer.
+		 */
+
+		if (bb->bb_offset)
+			bb->bb_start_cmd_va = workload->shadow_ring_buffer_va
+				+ bb->bb_offset;
+
 		/* relocate shadow batch buffer */
 		bb->bb_start_cmd_va[1] = i915_ggtt_offset(bb->vma);
 		if (gmadr_bytes == 8)

commit fa3dd623e559e8e7004179f9594b090318df0d05
Author: Min He <min.he@intel.com>
Date:   Fri Mar 2 10:00:25 2018 +0800

    drm/i915/gvt: keep oa config in shadow ctx
    
    When populating shadow ctx from guest, we should handle oa related
    registers in hw ctx, so that they will not be overlapped by guest oa
    configs. This patch made it possible to capture oa data from host for
    both host and guests.
    
    Signed-off-by: Min He <min.he@intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b55b3580ca1d..8caf72c1e794 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -52,6 +52,54 @@ static void set_context_pdp_root_pointer(
 		pdp_pair[i].val = pdp[7 - i];
 }
 
+/*
+ * when populating shadow ctx from guest, we should not overrride oa related
+ * registers, so that they will not be overlapped by guest oa configs. Thus
+ * made it possible to capture oa data from host for both host and guests.
+ */
+static void sr_oa_regs(struct intel_vgpu_workload *workload,
+		u32 *reg_state, bool save)
+{
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	u32 ctx_oactxctrl = dev_priv->perf.oa.ctx_oactxctrl_offset;
+	u32 ctx_flexeu0 = dev_priv->perf.oa.ctx_flexeu0_offset;
+	int i = 0;
+	u32 flex_mmio[] = {
+		i915_mmio_reg_offset(EU_PERF_CNTL0),
+		i915_mmio_reg_offset(EU_PERF_CNTL1),
+		i915_mmio_reg_offset(EU_PERF_CNTL2),
+		i915_mmio_reg_offset(EU_PERF_CNTL3),
+		i915_mmio_reg_offset(EU_PERF_CNTL4),
+		i915_mmio_reg_offset(EU_PERF_CNTL5),
+		i915_mmio_reg_offset(EU_PERF_CNTL6),
+	};
+
+	if (!workload || !reg_state || workload->ring_id != RCS)
+		return;
+
+	if (save) {
+		workload->oactxctrl = reg_state[ctx_oactxctrl + 1];
+
+		for (i = 0; i < ARRAY_SIZE(workload->flex_mmio); i++) {
+			u32 state_offset = ctx_flexeu0 + i * 2;
+
+			workload->flex_mmio[i] = reg_state[state_offset + 1];
+		}
+	} else {
+		reg_state[ctx_oactxctrl] =
+			i915_mmio_reg_offset(GEN8_OACTXCONTROL);
+		reg_state[ctx_oactxctrl + 1] = workload->oactxctrl;
+
+		for (i = 0; i < ARRAY_SIZE(workload->flex_mmio); i++) {
+			u32 state_offset = ctx_flexeu0 + i * 2;
+			u32 mmio = flex_mmio[i];
+
+			reg_state[state_offset] = mmio;
+			reg_state[state_offset + 1] = workload->flex_mmio[i];
+		}
+	}
+}
+
 static int populate_shadow_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
@@ -98,6 +146,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
 	shadow_ring_context = kmap(page);
 
+	sr_oa_regs(workload, (u32 *)shadow_ring_context, true);
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_read_gpa(vgpu, workload->ring_context_gpa \
 		+ RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)
@@ -122,6 +171,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			sizeof(*shadow_ring_context),
 			I915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
+	sr_oa_regs(workload, (u32 *)shadow_ring_context, false);
 	kunmap(page);
 	return 0;
 }

commit cd7e61b93d068a80bfe6cb55bf00f17332d831a1
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Fri Feb 23 14:46:45 2018 +0800

    drm/i915/gvt: init mmio by lri command in vgpu inhibit context
    
    There is one issue relates to Coarse Power Gating(CPG) on KBL NUC in GVT-g,
    vgpu can't get the correct default context by updating the registers before
    inhibit context submission. It always get back the hardware default value
    unless the inhibit context submission happened before the 1st time
    forcewake put. With this wrong default context, vgpu will run with
    incorrect state and meet unknown issues.
    
    The solution is initialize these mmios by adding lri command in ring buffer
    of the inhibit context, then gpu hardware has no chance to go down RC6 when
    lri commands are right being executed, and then vgpu can get correct
    default context for further use.
    
    v3:
    - fix code fault, use 'for' to loop through mmio render list(Zhenyu)
    
    v4:
    - save the count of engine mmio need to be restored for inhibit context and
      refine some comments. (Kevin)
    
    v5:
    - code rebase
    
    Cc: Kevin Tian <kevin.tian@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f4765ed4e92a..9b92b4e25a20 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -225,6 +225,11 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	void *shadow_ring_buffer_va;
 	u32 *cs;
+	struct i915_request *req = workload->req;
+
+	if (IS_KABYLAKE(req->i915) &&
+	    is_inhibit_context(req->ctx, req->engine->id))
+		intel_vgpu_restore_inhibit_context(vgpu, req);
 
 	/* allocate shadow ring buffer */
 	cs = intel_ring_begin(workload->req, workload->rb_len / sizeof(u32));

commit e6e9c46fd2351a07f31b3bf3101c57170c13aeab
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Jan 30 19:19:46 2018 +0800

    drm/i915/gvt: Factor out intel_vgpu_{get, put}_ppgtt_mm interface
    
    Factor out these two interfaces so we can kill some duplicated code in
    scheduler.c.
    
    v2:
      - rename to intel_vgpu_{get,put}_ppgtt_mm
      - refine handle_g2v_notification
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 989304ef18e3..f4765ed4e92a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1198,18 +1198,10 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 
 	read_guest_pdps(workload->vgpu, workload->ring_context_gpa, (void *)pdps);
 
-	mm = intel_vgpu_find_ppgtt_mm(workload->vgpu, pdps);
-	if (mm) {
-		intel_vgpu_mm_get(mm);
-	} else {
-
-		mm = intel_vgpu_create_ppgtt_mm(workload->vgpu, root_entry_type,
-						pdps);
-		if (IS_ERR(mm)) {
-			gvt_vgpu_err("fail to create mm object.\n");
-			return PTR_ERR(mm);
-		}
-	}
+	mm = intel_vgpu_get_ppgtt_mm(workload->vgpu, root_entry_type, pdps);
+	if (IS_ERR(mm))
+		return PTR_ERR(mm);
+
 	workload->shadow_mm = mm;
 	return 0;
 }

commit 1bc258519dc72070f21291cdd37aeaa192082abd
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Jan 30 19:19:41 2018 +0800

    drm/i915/gvt: Refine the intel_vgpu_mm reference management
    
    If we manage an object with a reference count, then its life cycle
    must flow the reference count operations. Meanwhile, change the
    operation functions to generic name *put* and *get*.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5668c3d0f542..989304ef18e3 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1132,7 +1132,7 @@ void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
 	struct intel_vgpu_submission *s = &workload->vgpu->submission;
 
 	if (workload->shadow_mm)
-		intel_gvt_mm_unreference(workload->shadow_mm);
+		intel_vgpu_mm_put(workload->shadow_mm);
 
 	kmem_cache_free(s->workloads, workload);
 }
@@ -1200,7 +1200,7 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 
 	mm = intel_vgpu_find_ppgtt_mm(workload->vgpu, pdps);
 	if (mm) {
-		intel_gvt_mm_reference(mm);
+		intel_vgpu_mm_get(mm);
 	} else {
 
 		mm = intel_vgpu_create_ppgtt_mm(workload->vgpu, root_entry_type,

commit ede9d0cfcb789b6fd86ecb71b4721a19c53956e6
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Jan 30 19:19:40 2018 +0800

    drm/i915/gvt: Rework shadow graphic memory management code
    
    This is a big one and the GVT shadow graphic memory management code is
    heavily refined. The new code is more straightforward with less code.
    
    The struct intel_vgpu_mm is restructured to be clearly defined, use
    accurate names and some of the original fields are removed which are
    really redundant.
    
    Now we only manage ppgtt mm object with mm->ppgtt_mm.lru_list. No need
    to mix ppgtt and ggtt together, since one vGPU only has one ggtt object.
    
    v4: Don't invoke ppgtt_free_all_shadow_page before intel_vgpu_destroy_all_ppgtt_mm.
    v3: Add GVT_RING_CTX_NR_PDPS to avoid confusing about the PDPs.
    v2: Split some changes into small standalone patches.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 92df1b44fe1d..5668c3d0f542 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -113,7 +113,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 #undef COPY_REG
 
 	set_context_pdp_root_pointer(shadow_ring_context,
-				     workload->shadow_mm->shadow_page_table);
+				     (void *)workload->shadow_mm->ppgtt_mm.shadow_pdps);
 
 	intel_gvt_hypervisor_read_gpa(vgpu,
 			workload->ring_context_gpa +
@@ -1181,27 +1181,30 @@ static int prepare_mm(struct intel_vgpu_workload *workload)
 	struct execlist_ctx_descriptor_format *desc = &workload->ctx_desc;
 	struct intel_vgpu_mm *mm;
 	struct intel_vgpu *vgpu = workload->vgpu;
-	int page_table_level;
-	u32 pdp[8];
+	intel_gvt_gtt_type_t root_entry_type;
+	u64 pdps[GVT_RING_CTX_NR_PDPS];
 
-	if (desc->addressing_mode == 1) { /* legacy 32-bit */
-		page_table_level = 3;
-	} else if (desc->addressing_mode == 3) { /* legacy 64 bit */
-		page_table_level = 4;
-	} else {
+	switch (desc->addressing_mode) {
+	case 1: /* legacy 32-bit */
+		root_entry_type = GTT_TYPE_PPGTT_ROOT_L3_ENTRY;
+		break;
+	case 3: /* legacy 64-bit */
+		root_entry_type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;
+		break;
+	default:
 		gvt_vgpu_err("Advanced Context mode(SVM) is not supported!\n");
 		return -EINVAL;
 	}
 
-	read_guest_pdps(workload->vgpu, workload->ring_context_gpa, pdp);
+	read_guest_pdps(workload->vgpu, workload->ring_context_gpa, (void *)pdps);
 
-	mm = intel_vgpu_find_ppgtt_mm(workload->vgpu, page_table_level, pdp);
+	mm = intel_vgpu_find_ppgtt_mm(workload->vgpu, pdps);
 	if (mm) {
 		intel_gvt_mm_reference(mm);
 	} else {
 
-		mm = intel_vgpu_create_mm(workload->vgpu, INTEL_GVT_MM_PPGTT,
-				pdp, page_table_level, 0);
+		mm = intel_vgpu_create_ppgtt_mm(workload->vgpu, root_entry_type,
+						pdps);
 		if (IS_ERR(mm)) {
 			gvt_vgpu_err("fail to create mm object.\n");
 			return PTR_ERR(mm);

commit bba73071b6f71be0a101658d7c13866e30b264a6
Merge: c71b53cc66c5 f073d78eeb8e
Author: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Date:   Thu Mar 1 11:14:24 2018 +0200

    Merge drm-next into drm-intel-next-queued (this time for real)
    
    To pull in the HDCP changes, especially wait_for changes to drm/i915
    that Chris wants to build on top of.
    
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

commit e61e0f51ba7974bb575cdc23220b573e5cd4ff2a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Feb 21 09:56:36 2018 +0000

    drm/i915: Rename drm_i915_gem_request to i915_request
    
    We want to de-emphasize the link between the request (dependency,
    execution and fence tracking) from GEM and so rename the struct from
    drm_i915_gem_request to i915_request. That is we may implement the GEM
    user interface on top of requests, but they are an abstraction for
    tracking execution rather than an implementation detail of GEM. (Since
    they are not tied to HW, we keep the i915 prefix as opposed to intel.)
    
    In short, the spatch:
    @@
    
    @@
    - struct drm_i915_gem_request
    + struct i915_request
    
    A corollary to contracting the type name, we also harmonise on using
    'rq' shorthand for local variables where space if of the essence and
    repetition makes 'request' unwieldy. For globals and struct members,
    'request' is still much preferred for its clarity.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Michał Winiarski <michal.winiarski@intel.com>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20180221095636.6649-1-chris@chris-wilson.co.uk
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Michał Winiarski <michal.winiarski@intel.com>
    Acked-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0056638b0c16..a22a686f14c2 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -126,7 +126,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
-static inline bool is_gvt_request(struct drm_i915_gem_request *req)
+static inline bool is_gvt_request(struct i915_request *req)
 {
 	return i915_gem_context_force_single_submission(req->ctx);
 }
@@ -148,7 +148,7 @@ static void save_ring_hw_state(struct intel_vgpu *vgpu, int ring_id)
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
-	struct drm_i915_gem_request *req = (struct drm_i915_gem_request *)data;
+	struct i915_request *req = data;
 	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
 				shadow_ctx_notifier_block[req->engine->id]);
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
@@ -333,13 +333,13 @@ static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	struct drm_i915_gem_request *rq;
+	struct i915_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	int ret;
 
-	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
+	rq = i915_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
 		ret = PTR_ERR(rq);
@@ -348,7 +348,7 @@ static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
 
 	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
 
-	workload->req = i915_gem_request_get(rq);
+	workload->req = i915_request_get(rq);
 	ret = copy_workload_to_ring_buffer(workload);
 	if (ret)
 		goto err_unpin;
@@ -582,7 +582,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	if (!IS_ERR_OR_NULL(workload->req)) {
 		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 				ring_id, workload->req);
-		i915_add_request(workload->req);
+		i915_request_add(workload->req);
 		workload->dispatched = true;
 	}
 
@@ -769,7 +769,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 				workload->status = 0;
 		}
 
-		i915_gem_request_put(fetch_and_zero(&workload->req));
+		i915_request_put(fetch_and_zero(&workload->req));
 
 		if (!workload->status && !(vgpu->resetting_eng &
 					   ENGINE_MASK(ring_id))) {
@@ -886,7 +886,7 @@ static int workload_thread(void *priv)
 
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
-		i915_wait_request(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
+		i915_request_wait(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",

commit 9212b13f28374815d9def65e3c877a35092e1c6e
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Fri Jan 26 15:09:08 2018 +0800

    drm/i915/gvt: only reset execlist state of one engine during VM engine reset
    
    Only reset vgpu execlist state of the exact engine which gets reset
    request from VM. After read context status from HWSP enabled, KMD will use
    the saved CSB read pointer but not always read from MMIO. When one engine
    reset happen, only the read pointer of this engine will be reset, in GVT-g
    host side also need to align with this policy, otherwise VM may get wrong
    CSB status after one engine reset compeleted.
    
    v2: Split refine and fix patch, code refine(Zhenyu)
    v3: Move active flag of vgpu scheduler into sched_data(Zhenyu)
    
    Cc: Fred Gao <fred.gao@intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f0997a2f0513..b55b3580ca1d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1092,17 +1092,17 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 	if (WARN_ON(interface >= ARRAY_SIZE(ops)))
 		return -EINVAL;
 
-	if (s->active) {
+	if (WARN_ON(interface == 0 && engine_mask != ALL_ENGINES))
+		return -EINVAL;
+
+	if (s->active)
 		s->ops->clean(vgpu, engine_mask);
-		s->active = false;
-		gvt_dbg_core("vgpu%d: de-select ops [ %s ] \n",
-				vgpu->id, s->ops->name);
-	}
 
 	if (interface == 0) {
 		s->ops = NULL;
 		s->virtual_submission_interface = 0;
-		gvt_dbg_core("vgpu%d: no submission ops\n", vgpu->id);
+		s->active = false;
+		gvt_dbg_core("vgpu%d: remove submission ops\n", vgpu->id);
 		return 0;
 	}
 

commit 7569a06dc80ec05c96783f541fa706ea3bebec79
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Fri Jan 26 15:09:07 2018 +0800

    drm/i915/gvt: refine intel_vgpu_submission_ops as per engine ops
    
    Using per engine ops will be more flexible, here refine sub-ops(init,
    clean) as per engine operation align with reset operation. This change also
    will be used in next fix patch for VM engine reset.
    
    Cc: Fred Gao <fred.gao@intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0056638b0c16..f0997a2f0513 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -991,7 +991,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 
-	intel_vgpu_select_submission_ops(vgpu, 0);
+	intel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);
 	i915_gem_context_put(s->shadow_ctx);
 	kmem_cache_destroy(s->workloads);
 }
@@ -1079,6 +1079,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
  *
  */
 int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
+				     unsigned long engine_mask,
 				     unsigned int interface)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
@@ -1092,7 +1093,7 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 		return -EINVAL;
 
 	if (s->active) {
-		s->ops->clean(vgpu);
+		s->ops->clean(vgpu, engine_mask);
 		s->active = false;
 		gvt_dbg_core("vgpu%d: de-select ops [ %s ] \n",
 				vgpu->id, s->ops->name);
@@ -1105,7 +1106,7 @@ int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
 		return 0;
 	}
 
-	ret = ops[interface]->init(vgpu);
+	ret = ops[interface]->init(vgpu, engine_mask);
 	if (ret)
 		return ret;
 

commit 6647852abc1fd74e9c5e0dcf404ea4cb9c929630
Merge: 3e72be177cf1 3f1f0b1c57dd
Author: Rodrigo Vivi <rodrigo.vivi@intel.com>
Date:   Fri Dec 8 10:15:30 2017 -0800

    Merge airlied/drm-next into drm-intel-next-queued
    
    Chris requested this backmerge for a reconciliation on
    drm_print.h between drm-misc-next and drm-intel-next-queued
    
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

commit 1603660b3342269c95fcafee1945790342a8c28e
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Mon Dec 4 10:42:58 2017 +0800

    drm/i915/gvt: set max priority for gvt context
    
    This is to workaround guest driver hang regression after
    preemption enable that gvt hasn't enabled handling of that
    for guest workload. So in effect this disables preemption
    for gvt context now.
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9100aebcb105..ab9a500ba3e9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1038,6 +1038,9 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	if (IS_ERR(s->shadow_ctx))
 		return PTR_ERR(s->shadow_ctx);
 
+	if (HAS_LOGICAL_RING_PREEMPTION(vgpu->gvt->dev_priv))
+		s->shadow_ctx->priority = INT_MAX;
+
 	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
 	s->workloads = kmem_cache_create("gvt-g_vgpu_workload",

commit da5f99eaccc10e30bf82eb02b1be74703b878720
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Fri Dec 1 14:59:53 2017 +0800

    drm/i915/gvt: Don't mark vgpu context as inactive when preempted
    
    We shouldn't mark inactive for vGPU context if preempted,
    which would still be re-scheduled later. So keep active state.
    
    Fixes: d6c0511300dc ("drm/i915/execlists: Distinguish the incomplete context notifies")
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 76d2812f2f03..9100aebcb105 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -188,10 +188,12 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
-	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
 		save_ring_hw_state(workload->vgpu, ring_id);
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
+	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
+		save_ring_hw_state(workload->vgpu, ring_id);
+		break;
 	default:
 		WARN_ON(1);
 		return NOTIFY_OK;

commit c130456cefd4cf9531f12f3e9b23805d34706ac0
Author: Changbin Du <changbin.du@intel.com>
Date:   Wed Nov 29 15:40:07 2017 +0800

    drm/i915/gvt: Kick scheduler when new workload queued
    
    The current schedule policy rely on a 1ms timer to execute workload. This
    can introduce maximum 1ms unnecessary latency. This is especially bad for
    small media workloads.
    
    And I don't think we need this timer for QoS, but the change is not simply
    remove the code. So I made a new API intel_gvt_kick_schedule() for future
    change.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index abf71be092f8..76d2812f2f03 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1337,5 +1337,6 @@ void intel_vgpu_queue_workload(struct intel_vgpu_workload *workload)
 {
 	list_add_tail(&workload->list,
 		workload_q_head(workload->vgpu, workload->ring_id));
+	intel_gvt_kick_schedule(workload->vgpu->gvt);
 	wake_up(&workload->vgpu->gvt->scheduler.waitq[workload->ring_id]);
 }

commit 59a716c6477c2a095adf274e8f76b9889af7bc7b
Author: Changbin Du <changbin.du@intel.com>
Date:   Wed Nov 29 15:40:06 2017 +0800

    drm/i915/gvt: Convert macro queue_workload to a function
    
    Convert the macro to a function which should always be preferred.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 13ccc00f0d40..abf71be092f8 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -1328,3 +1328,14 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 
 	return workload;
 }
+
+/**
+ * intel_vgpu_queue_workload - Qeue a vGPU workload
+ * @workload: the workload to queue in
+ */
+void intel_vgpu_queue_workload(struct intel_vgpu_workload *workload)
+{
+	list_add_tail(&workload->list,
+		workload_q_head(workload->vgpu, workload->ring_id));
+	wake_up(&workload->vgpu->gvt->scheduler.waitq[workload->ring_id]);
+}

commit ca797d29cd63e7b71b4eea29aff3b1cefd1ecb59
Merge: 2c1c55cb75a9 010d118c2061
Author: Dave Airlie <airlied@redhat.com>
Date:   Mon Dec 4 09:40:35 2017 +1000

    Merge tag 'drm-intel-next-2017-11-17-1' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    More change sets for 4.16:
    
    - Many improvements for selftests and other igt tests (Chris)
    - Forcewake with PUNIT->PMIC bus fixes and robustness (Hans)
    - Define an engine class for uABI (Tvrtko)
    - Context switch fixes and improvements (Chris)
    - GT powersavings and power gating simplification and fixes (Chris)
    - Other general driver clean-ups (Chris, Lucas, Ville)
    - Removing old, useless and/or bad workarounds (Chris, Oscar, Radhakrishna)
    - IPS, pipe config, etc in preparation for another Fast Boot attempt (Maarten)
    - OA perf fixes and support to Coffee Lake and Cannonlake (Lionel)
    - Fixes around GPU fault registers (Michel)
    - GEM Proxy (Tina)
    - Refactor of Geminilake and Cannonlake plane color handling (James)
    - Generalize transcoder loop (Mika Kahola)
    - New HW Workaround for Cannonlake and Geminilake (Rodrigo)
    - Resume GuC before using GEM (Chris)
    - Stolen Memory handling improvements (Ville)
    - Initialize entry in PPAT for older compilers (Chris)
    - Other fixes and robustness improvements on execbuf (Chris)
    - Improve logs of GEM_BUG_ON (Mika Kuoppala)
    - Rework with massive rename of GuC functions and files (Sagar)
    - Don't sanitize frame start delay if pipe is off (Ville)
    - Cannonlake clock fixes (Rodrigo)
    - Cannonlake HDMI 2.0 support (Rodrigo)
    - Add a GuC doorbells selftest (Michel)
    - Add might_sleep() check to our wait_for() (Chris)
    
    Many GVT changes for 4.16:
    
    - CSB HWSP update support (Weinan)
    - GVT debug helpers, dyndbg and debugfs (Chuanxiao, Shuo)
    - full virtualized opregion (Xiaolin)
    - VM health check for sane fallback (Fred)
    - workload submission code refactor for future enabling (Zhi)
    - Updated repo URL in MAINTAINERS (Zhenyu)
    - other many misc fixes
    
    * tag 'drm-intel-next-2017-11-17-1' of git://anongit.freedesktop.org/drm/drm-intel: (260 commits)
      drm/i915: Update DRIVER_DATE to 20171117
      drm/i915: Add a policy note for removing workarounds
      drm/i915/selftests: Report ENOMEM clearly for an allocation failure
      Revert "drm/i915: Display WA #1133 WaFbcSkipSegments:cnl, glk"
      drm/i915: Calculate g4x intermediate watermarks correctly
      drm/i915: Calculate vlv/chv intermediate watermarks correctly, v3.
      drm/i915: Pass crtc_state to ips toggle functions, v2
      drm/i915: Pass idle crtc_state to intel_dp_sink_crc
      drm/i915: Enable FIFO underrun reporting after initial fastset, v4.
      drm/i915: Mark the userptr invalidate workqueue as WQ_MEM_RECLAIM
      drm/i915: Add might_sleep() check to wait_for()
      drm/i915/selftests: Add a GuC doorbells selftest
      drm/i915/cnl: Extend HDMI 2.0 support to CNL.
      drm/i915/cnl: Simplify dco_fraction calculation.
      drm/i915/cnl: Don't blindly replace qdiv.
      drm/i915/cnl: Fix wrpll math for higher freqs.
      drm/i915/cnl: Fix, simplify and unify wrpll variable sizes.
      drm/i915/cnl: Remove useless conversion.
      drm/i915/cnl: Remove spurious central_freq.
      drm/i915/selftests: exercise_ggtt may have nothing to do
      ...

commit c3c80f0736bb2767fab983d1ae53252b2ddd405d
Author: fred gao <fred.gao@intel.com>
Date:   Tue Nov 14 17:09:35 2017 +0800

    drm/i915/gvt: Move request alloc to dispatch_workload path only
    
    Previously the performance is improved through the workload auditing
    and shadowing ahead of vGPU scheduling, however, there is the case that
    more requests are allocated in submit_context before the previous request
    is added, the timeline will hold its seqno which is later.
    
    This patch is to move the request alloc to dispatch_workload function,
    where is the same place as request is added.
    
    It will fix the issue of kernel BUG for (timeline->seqno != request->fence.seqno)
    check when add_request.
    
    Fixes: 89ea20b930cb ("drm/i915/gvt: Factor out scan and shadow from workload dispatch")
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    (cherry picked from commit f2880e04f3a5419366926182fc97a3c2e4fd8f2a)

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f031234290f6..3ac1dc97a7a0 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -254,7 +254,6 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	struct drm_i915_gem_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_ring *ring;
 	int ret;
@@ -300,6 +299,26 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	ret = populate_shadow_context(workload);
 	if (ret)
 		goto err_unpin;
+	workload->shadowed = true;
+	return 0;
+
+err_unpin:
+	engine->context_unpin(engine, shadow_ctx);
+err_shadow:
+	release_shadow_wa_ctx(&workload->wa_ctx);
+err_scan:
+	return ret;
+}
+
+int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
+{
+	int ring_id = workload->ring_id;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
+	struct drm_i915_gem_request *rq;
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct i915_gem_context *shadow_ctx = vgpu->shadow_ctx;
+	int ret;
 
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
@@ -314,14 +333,11 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	ret = copy_workload_to_ring_buffer(workload);
 	if (ret)
 		goto err_unpin;
-	workload->shadowed = true;
 	return 0;
 
 err_unpin:
 	engine->context_unpin(engine, shadow_ctx);
-err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
-err_scan:
 	return ret;
 }
 

commit 679fd3ebabc23dec33d42525b19479f16f355e61
Author: Changbin Du <changbin.du@intel.com>
Date:   Mon Nov 13 14:58:31 2017 +0800

    drm/i915/gvt: Fix unsafe locking caused by spin_unlock_bh
    
    The caller of shadow_context_status_change may disable irqs. So it is not
    safe to use spin_unlock_bh in such context. Let's switch to irqsave version
    for safety.
    
    ------------[ cut here ]------------
    WARNING: CPU: 2 PID: 4504 at kernel/softirq.c:161 __local_bh_enable_ip+0x46/0x60
    [  168.797710] Hardware name: Dell Inc. OptiPlex 7040/0Y7WYT, BIOS 1.2.8 01/26/2016
    [  168.797712] task: ffff8c693d22db80 task.stack: ffffb51b482bc000
    [  168.797718] RIP: 0010:__local_bh_enable_ip+0x46/0x60
    [  168.797721] RSP: 0018:ffffb51b482bfa10 EFLAGS: 00010046
    [  168.797724] RAX: 0000000000000046 RBX: ffff8c6900278000 RCX: 00000000ffffffff
    [  168.797726] RDX: 0000000000000001 RSI: 0000000000000200 RDI: ffffffffc06a0330
    [  168.797728] RBP: ffffb51b482bfa10 R08: 0000000000000000 R09: ffff8c690027cb90
    [  168.797730] R10: ffffb51b482bfa40 R11: 00000004072f0001 R12: 0000000000000000
    [  168.797732] R13: 0000000000000000 R14: ffff8c690027ca9c R15: 0000000000000000
    [  168.797735] FS:  00007ff187c56700(0000) GS:ffff8c6959d00000(0000) knlGS:0000000000000000
    [  168.797738] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  168.797740] CR2: 0000562bc0c3991f CR3: 0000000430614006 CR4: 00000000003606e0
    [  168.797742] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [  168.797744] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [  168.797745] Call Trace:
    [  168.797755]  _raw_spin_unlock_bh+0x1e/0x20
    [  168.797826]  shadow_context_status_change+0x120/0x1e0 [i915]
    [  168.797831]  notifier_call_chain+0x4a/0x70
    [  168.797834]  atomic_notifier_call_chain+0x1a/0x20
    [  168.797896]  execlists_cancel_port_requests+0x4f/0x80 [i915]
    [  168.797956]  reset_common_ring+0x30/0x100 [i915]
    [  168.798007]  i915_gem_reset_engine+0x114/0x330 [i915]
    [  168.798060]  ? i915_gem_retire_requests+0x75/0x180 [i915]
    [  168.798111]  i915_gem_reset+0x3e/0xb0 [i915]
    [  168.798149]  i915_reset+0x10b/0x1c0 [i915]
    [  168.798187]  i915_reset_device+0x209/0x220 [i915]
    [  168.798225]  ? gen8_gt_irq_ack+0x170/0x170 [i915]
    [  168.798229]  ? __queue_work+0x430/0x430
    [  168.798270]  i915_handle_error+0x285/0x420 [i915]
    [  168.798275]  ? mntput+0x24/0x40
    [  168.798281]  ? terminate_walk+0x8e/0xf0
    [  168.798328]  i915_wedged_set+0x84/0xc0 [i915]
    [  168.798333]  simple_attr_write+0xab/0xc0
    [  168.798337]  full_proxy_write+0x54/0x90
    [  168.798343]  __vfs_write+0x37/0x170
    [  168.798349]  ? common_file_perm+0x4c/0x100
    [  168.798355]  ? apparmor_file_permission+0x1a/0x20
    [  168.798361]  ? security_file_permission+0x3b/0xc0
    [  168.798365]  vfs_write+0xb8/0x1b0
    [  168.798370]  SyS_write+0x55/0xc0
    [  168.798376]  entry_SYSCALL_64_fastpath+0x1e/0xa9
    
    Fixes: 0e86cc9 ("drm/i915/gvt: implement per-vm mmio switching optimization")
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f6ded475bb2c..f031234290f6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -140,9 +140,10 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	enum intel_engine_id ring_id = req->engine->id;
 	struct intel_vgpu_workload *workload;
+	unsigned long flags;
 
 	if (!is_gvt_request(req)) {
-		spin_lock_bh(&scheduler->mmio_context_lock);
+		spin_lock_irqsave(&scheduler->mmio_context_lock, flags);
 		if (action == INTEL_CONTEXT_SCHEDULE_IN &&
 		    scheduler->engine_owner[ring_id]) {
 			/* Switch ring from vGPU to host. */
@@ -150,7 +151,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 					      NULL, ring_id);
 			scheduler->engine_owner[ring_id] = NULL;
 		}
-		spin_unlock_bh(&scheduler->mmio_context_lock);
+		spin_unlock_irqrestore(&scheduler->mmio_context_lock, flags);
 
 		return NOTIFY_OK;
 	}
@@ -161,7 +162,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 
 	switch (action) {
 	case INTEL_CONTEXT_SCHEDULE_IN:
-		spin_lock_bh(&scheduler->mmio_context_lock);
+		spin_lock_irqsave(&scheduler->mmio_context_lock, flags);
 		if (workload->vgpu != scheduler->engine_owner[ring_id]) {
 			/* Switch ring from host to vGPU or vGPU to vGPU. */
 			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
@@ -170,7 +171,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		} else
 			gvt_dbg_sched("skip ring %d mmio switch for vgpu%d\n",
 				      ring_id, workload->vgpu->id);
-		spin_unlock_bh(&scheduler->mmio_context_lock);
+		spin_unlock_irqrestore(&scheduler->mmio_context_lock, flags);
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:

commit 7b30255698edc91f9235faf586d21625ca7bbbac
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 20 13:29:58 2017 +0000

    drm/i915/gvt: Cleanup unwanted public symbols
    
    drivers/gpu/drm/i915/gvt/execlist.c:531:6: warning: symbol 'clean_execlist' was not declared. Should it be static?
    drivers/gpu/drm/i915/gvt/execlist.c:545:6: warning: symbol 'reset_execlist' was not declared. Should it be static?
    drivers/gpu/drm/i915/gvt/execlist.c:556:5: warning: symbol 'init_execlist' was not declared. Should it be static?
    drivers/gpu/drm/i915/gvt/scheduler.c:248:6: warning: symbol 'release_shadow_wa_ctx' was not declared. Should it be static?
    
    References: 06bb372f9ace ("drm/i915/gvt: Introduce intel_vgpu_reset_submission")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: intel-gvt-dev@lists.freedesktop.org
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a742b364c2c3..13ccc00f0d40 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -245,7 +245,7 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
-void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
+static void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 {
 	if (!wa_ctx->indirect_ctx.obj)
 		return;

commit f2880e04f3a5419366926182fc97a3c2e4fd8f2a
Author: fred gao <fred.gao@intel.com>
Date:   Tue Nov 14 17:09:35 2017 +0800

    drm/i915/gvt: Move request alloc to dispatch_workload path only
    
    Previously the performance is improved through the workload auditing
    and shadowing ahead of vGPU scheduling, however, there is the case that
    more requests are allocated in submit_context before the previous request
    is added, the timeline will hold its seqno which is later.
    
    This patch is to move the request alloc to dispatch_workload function,
    where is the same place as request is added.
    
    It will fix the issue of kernel BUG for (timeline->seqno != request->fence.seqno)
    check when add_request.
    
    Fixes: 89ea20b930cb ("drm/i915/gvt: Factor out scan and shadow from workload dispatch")
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9749113fccdd..a742b364c2c3 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -270,7 +270,6 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	int ring_id = workload->ring_id;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	struct drm_i915_gem_request *rq;
 	struct intel_ring *ring;
 	int ret;
 
@@ -315,6 +314,27 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	ret = populate_shadow_context(workload);
 	if (ret)
 		goto err_unpin;
+	workload->shadowed = true;
+	return 0;
+
+err_unpin:
+	engine->context_unpin(engine, shadow_ctx);
+err_shadow:
+	release_shadow_wa_ctx(&workload->wa_ctx);
+err_scan:
+	return ret;
+}
+
+static int intel_gvt_generate_request(struct intel_vgpu_workload *workload)
+{
+	int ring_id = workload->ring_id;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
+	struct drm_i915_gem_request *rq;
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
+	int ret;
 
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
@@ -329,14 +349,11 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	ret = copy_workload_to_ring_buffer(workload);
 	if (ret)
 		goto err_unpin;
-	workload->shadowed = true;
 	return 0;
 
 err_unpin:
 	engine->context_unpin(engine, shadow_ctx);
-err_shadow:
 	release_shadow_wa_ctx(&workload->wa_ctx);
-err_scan:
 	return ret;
 }
 
@@ -496,6 +513,12 @@ static int prepare_workload(struct intel_vgpu_workload *workload)
 		goto err_unpin_mm;
 	}
 
+	ret = intel_gvt_generate_request(workload);
+	if (ret) {
+		gvt_vgpu_err("fail to generate request\n");
+		goto err_unpin_mm;
+	}
+
 	ret = prepare_shadow_batch_buffer(workload);
 	if (ret) {
 		gvt_vgpu_err("fail to prepare_shadow_batch_buffer\n");

commit 295764cd2ff41e2c1bc8af4050de77cec5e7a1c0
Author: Xiong Zhang <xiong.y.zhang@intel.com>
Date:   Tue Nov 7 05:23:02 2017 +0800

    drm/i915/gvt: Limit read hw reg to active vgpu
    
    mmio_read_from_hw() let vgpu could read hw reg, if vgpu's workload
    is running on hw, things is good. Otherwise vgpu will get other
    vgpu's reg val, it is unsafe.
    
    This patch limit such hw access to active vgpu. If vgpu isn't
    running on hw, the reg read of this vgpu will get the last active
    val which saved at schedule_out.
    
    v2: ring timestamp is walking continuously even if the ring is idle.
        so read hw directly. (Zhenyu)
    
    Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7a1ffaa9ae06..9749113fccdd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -131,6 +131,20 @@ static inline bool is_gvt_request(struct drm_i915_gem_request *req)
 	return i915_gem_context_force_single_submission(req->ctx);
 }
 
+static void save_ring_hw_state(struct intel_vgpu *vgpu, int ring_id)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	u32 ring_base = dev_priv->engine[ring_id]->mmio_base;
+	i915_reg_t reg;
+
+	reg = RING_INSTDONE(ring_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
+	reg = RING_ACTHD(ring_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
+	reg = RING_ACTHD_UDW(ring_base);
+	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
+}
+
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
@@ -175,6 +189,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
 	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
+		save_ring_hw_state(workload->vgpu, ring_id);
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	default:

commit 9556e118889293f6d5d226b64688ee2adfd8964c
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue Oct 10 13:51:32 2017 +0800

    drm/i915/gvt: Use I915_GTT_PAGE_SIZE
    
    As there is already an I915_GTT_PAGE_SIZE marco in i915, let GVT-g use it
    as well. Also this patch re-names some GTT marcos with additional prefix.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f2d4c90ea1d4..7a1ffaa9ae06 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -81,7 +81,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	while (i < context_page_num) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
 				(u32)((workload->ctx_desc.lrca + i) <<
-				GTT_PAGE_SHIFT));
+				I915_GTT_PAGE_SHIFT));
 		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
 			gvt_vgpu_err("Invalid guest context descriptor\n");
 			return -EFAULT;
@@ -90,7 +90,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
 		dst = kmap(page);
 		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
-				GTT_PAGE_SIZE);
+				I915_GTT_PAGE_SIZE);
 		kunmap(page);
 		i++;
 	}
@@ -120,7 +120,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			sizeof(*shadow_ring_context),
 			(void *)shadow_ring_context +
 			sizeof(*shadow_ring_context),
-			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
+			I915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
 	kunmap(page);
 	return 0;
@@ -635,7 +635,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	while (i < context_page_num) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
 				(u32)((workload->ctx_desc.lrca + i) <<
-					GTT_PAGE_SHIFT));
+					I915_GTT_PAGE_SHIFT));
 		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
 			gvt_vgpu_err("invalid guest context descriptor\n");
 			return;
@@ -644,7 +644,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
 		src = kmap(page);
 		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
-				GTT_PAGE_SIZE);
+				I915_GTT_PAGE_SIZE);
 		kunmap(page);
 		i++;
 	}
@@ -669,7 +669,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			sizeof(*shadow_ring_context),
 			(void *)shadow_ring_context +
 			sizeof(*shadow_ring_context),
-			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
+			I915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
 	kunmap(page);
 }
@@ -1198,7 +1198,7 @@ intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
 	int ret;
 
 	ring_context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
-			(u32)((desc->lrca + 1) << GTT_PAGE_SHIFT));
+			(u32)((desc->lrca + 1) << I915_GTT_PAGE_SHIFT));
 	if (ring_context_gpa == INTEL_GVT_INVALID_ADDR) {
 		gvt_vgpu_err("invalid guest context LRCA: %x\n", desc->lrca);
 		return ERR_PTR(-EINVAL);

commit f52c380a48f527930c86ea6fd7242873c93ba682
Author: Zhi Wang <zhi.wang.linux@gmail.com>
Date:   Sun Sep 24 21:53:03 2017 +0800

    drm/i915/gvt: Refine shadow batch buffer
    
    1) Use standard i915 GEM object sequence to access the shadow batch buffer.
    2) Manage i915 vma life cycle to solve one FIXME.
    
    v2:
    - Refine code structure.
    - Refine the usage of GEM APIs.
    - Add the missing lock/unlock in release_shadow_batch_buffer.
    
    Test on my SKL NuC.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 391690c0c28c..f2d4c90ea1d4 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -325,31 +325,46 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
+static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload);
+
 static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 {
 	struct intel_gvt *gvt = workload->vgpu->gvt;
 	const int gmadr_bytes = gvt->device_info.gmadr_bytes_in_cmd;
-	struct intel_shadow_bb_entry *entry_obj;
+	struct intel_vgpu_shadow_bb *bb;
+	int ret;
 
-	/* pin the gem object to ggtt */
-	list_for_each_entry(entry_obj, &workload->shadow_bb, list) {
-		struct i915_vma *vma;
+	list_for_each_entry(bb, &workload->shadow_bb, list) {
+		bb->vma = i915_gem_object_ggtt_pin(bb->obj, NULL, 0, 0, 0);
+		if (IS_ERR(bb->vma)) {
+			ret = PTR_ERR(bb->vma);
+			goto err;
+		}
 
-		vma = i915_gem_object_ggtt_pin(entry_obj->obj, NULL, 0, 4, 0);
-		if (IS_ERR(vma))
-			return PTR_ERR(vma);
+		/* relocate shadow batch buffer */
+		bb->bb_start_cmd_va[1] = i915_ggtt_offset(bb->vma);
+		if (gmadr_bytes == 8)
+			bb->bb_start_cmd_va[2] = 0;
 
-		/* FIXME: we are not tracking our pinned VMA leaving it
-		 * up to the core to fix up the stray pin_count upon
-		 * free.
-		 */
+		/* No one is going to touch shadow bb from now on. */
+		if (bb->clflush & CLFLUSH_AFTER) {
+			drm_clflush_virt_range(bb->va, bb->obj->base.size);
+			bb->clflush &= ~CLFLUSH_AFTER;
+		}
 
-		/* update the relocate gma with shadow batch buffer*/
-		entry_obj->bb_start_cmd_va[1] = i915_ggtt_offset(vma);
-		if (gmadr_bytes == 8)
-			entry_obj->bb_start_cmd_va[2] = 0;
+		ret = i915_gem_object_set_to_gtt_domain(bb->obj, false);
+		if (ret)
+			goto err;
+
+		i915_gem_obj_finish_shmem_access(bb->obj);
+		bb->accessing = false;
+
+		i915_vma_move_to_active(bb->vma, workload->req, 0);
 	}
 	return 0;
+err:
+	release_shadow_batch_buffer(workload);
+	return ret;
 }
 
 static int update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
@@ -410,22 +425,37 @@ static int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
 
 static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
 {
-	/* release all the shadow batch buffer */
-	if (!list_empty(&workload->shadow_bb)) {
-		struct intel_shadow_bb_entry *entry_obj =
-			list_first_entry(&workload->shadow_bb,
-					 struct intel_shadow_bb_entry,
-					 list);
-		struct intel_shadow_bb_entry *temp;
-
-		list_for_each_entry_safe(entry_obj, temp, &workload->shadow_bb,
-					 list) {
-			i915_gem_object_unpin_map(entry_obj->obj);
-			i915_gem_object_put(entry_obj->obj);
-			list_del(&entry_obj->list);
-			kfree(entry_obj);
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_vgpu_shadow_bb *bb, *pos;
+
+	if (list_empty(&workload->shadow_bb))
+		return;
+
+	bb = list_first_entry(&workload->shadow_bb,
+			struct intel_vgpu_shadow_bb, list);
+
+	mutex_lock(&dev_priv->drm.struct_mutex);
+
+	list_for_each_entry_safe(bb, pos, &workload->shadow_bb, list) {
+		if (bb->obj) {
+			if (bb->accessing)
+				i915_gem_obj_finish_shmem_access(bb->obj);
+
+			if (bb->va && !IS_ERR(bb->va))
+				i915_gem_object_unpin_map(bb->obj);
+
+			if (bb->vma && !IS_ERR(bb->vma)) {
+				i915_vma_unpin(bb->vma);
+				i915_vma_close(bb->vma);
+			}
+			__i915_gem_object_release_unless_active(bb->obj);
 		}
+		list_del(&bb->list);
+		kfree(bb);
 	}
+
+	mutex_unlock(&dev_priv->drm.struct_mutex);
 }
 
 static int prepare_workload(struct intel_vgpu_workload *workload)

commit e2c43c0111d54d4857d052fcfca9f3f16bf1b1b2
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Wed Sep 13 01:58:35 2017 +0800

    drm/i915/gvt: Move clean_workloads() into scheduler.c
    
    Move clean_workloads() into scheduler.c since it's not specific to
    execlist.
    
    v2:
    
    - Remove clean_workloads in intel_vgpu_select_submission_ops. (Zhenyu)
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 88ce57116a4c..391690c0c28c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -644,6 +644,25 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	kunmap(page);
 }
 
+static void clean_workloads(struct intel_vgpu *vgpu, unsigned long engine_mask)
+{
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine;
+	struct intel_vgpu_workload *pos, *n;
+	unsigned int tmp;
+
+	/* free the unsubmited workloads in the queues. */
+	for_each_engine_masked(engine, dev_priv, engine_mask, tmp) {
+		list_for_each_entry_safe(pos, n,
+			&s->workload_q_head[engine->id], list) {
+			list_del_init(&pos->list);
+			intel_vgpu_destroy_workload(pos);
+		}
+		clear_bit(engine->id, s->shadow_ctx_desc_updated);
+	}
+}
+
 static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
@@ -707,6 +726,23 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		release_shadow_wa_ctx(&workload->wa_ctx);
 	}
 
+	if (workload->status || (vgpu->resetting_eng & ENGINE_MASK(ring_id))) {
+		/* if workload->status is not successful means HW GPU
+		 * has occurred GPU hang or something wrong with i915/GVT,
+		 * and GVT won't inject context switch interrupt to guest.
+		 * So this error is a vGPU hang actually to the guest.
+		 * According to this we should emunlate a vGPU hang. If
+		 * there are pending workloads which are already submitted
+		 * from guest, we should clean them up like HW GPU does.
+		 *
+		 * if it is in middle of engine resetting, the pending
+		 * workloads won't be submitted to HW GPU and will be
+		 * cleaned up during the resetting process later, so doing
+		 * the workload clean up here doesn't have any impact.
+		 **/
+		clean_workloads(vgpu, ENGINE_MASK(ring_id));
+	}
+
 	workload->complete(workload);
 
 	atomic_dec(&s->running_workload_num);
@@ -906,6 +942,7 @@ void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
 	if (!s->active)
 		return;
 
+	clean_workloads(vgpu, engine_mask);
 	s->ops->reset(vgpu, engine_mask);
 }
 

commit 06bb372f9ace47296aeaaca8e130d948ea2855cf
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Wed Sep 13 01:41:35 2017 +0800

    drm/i915/gvt: Introduce intel_vgpu_reset_submission
    
    Introduce an generic API to reset vGPU virtual submission interface.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f3be88fa88dd..88ce57116a4c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -889,6 +889,26 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 	kmem_cache_destroy(s->workloads);
 }
 
+
+/**
+ * intel_vgpu_reset_submission - reset submission-related resource for vGPU
+ * @vgpu: a vGPU
+ * @engine_mask: engines expected to be reset
+ *
+ * This function is called when a vGPU is being destroyed.
+ *
+ */
+void intel_vgpu_reset_submission(struct intel_vgpu *vgpu,
+		unsigned long engine_mask)
+{
+	struct intel_vgpu_submission *s = &vgpu->submission;
+
+	if (!s->active)
+		return;
+
+	s->ops->reset(vgpu, engine_mask);
+}
+
 /**
  * intel_vgpu_setup_submission - setup submission-related resource for vGPU
  * @vgpu: a vGPU

commit ad1d36369b07f6b9db81897802ee5d8764eaa922
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Wed Sep 13 00:31:29 2017 +0800

    drm/i915/gvt: Introduce vGPU submission ops
    
    Introduce vGPU submission ops to support easy switching submission mode
    of one vGPU between different OSes.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 69893f29ff6d..f3be88fa88dd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -884,6 +884,7 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
 
+	intel_vgpu_select_submission_ops(vgpu, 0);
 	i915_gem_context_put(s->shadow_ctx);
 	kmem_cache_destroy(s->workloads);
 }
@@ -935,6 +936,58 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	return ret;
 }
 
+/**
+ * intel_vgpu_select_submission_ops - select virtual submission interface
+ * @vgpu: a vGPU
+ * @interface: expected vGPU virtual submission interface
+ *
+ * This function is called when guest configures submission interface.
+ *
+ * Returns:
+ * Zero on success, negative error code if failed.
+ *
+ */
+int intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,
+				     unsigned int interface)
+{
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	const struct intel_vgpu_submission_ops *ops[] = {
+		[INTEL_VGPU_EXECLIST_SUBMISSION] =
+			&intel_vgpu_execlist_submission_ops,
+	};
+	int ret;
+
+	if (WARN_ON(interface >= ARRAY_SIZE(ops)))
+		return -EINVAL;
+
+	if (s->active) {
+		s->ops->clean(vgpu);
+		s->active = false;
+		gvt_dbg_core("vgpu%d: de-select ops [ %s ] \n",
+				vgpu->id, s->ops->name);
+	}
+
+	if (interface == 0) {
+		s->ops = NULL;
+		s->virtual_submission_interface = 0;
+		gvt_dbg_core("vgpu%d: no submission ops\n", vgpu->id);
+		return 0;
+	}
+
+	ret = ops[interface]->init(vgpu);
+	if (ret)
+		return ret;
+
+	s->ops = ops[interface];
+	s->virtual_submission_interface = interface;
+	s->active = true;
+
+	gvt_dbg_core("vgpu%d: activate ops [ %s ]\n",
+			vgpu->id, s->ops->name);
+
+	return 0;
+}
+
 /**
  * intel_vgpu_destroy_workload - destroy a vGPU workload
  * @vgpu: a vGPU

commit 6d76303553bab75ffc53993c56aad06251d8de60
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue Sep 12 22:33:12 2017 +0800

    drm/i915/gvt: Move common vGPU workload creation into scheduler.c
    
    Move common vGPU workload creation functions into scheduler.c since
    they are not specific to execlist emulation.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a7a67cc65a07..69893f29ff6d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -792,13 +792,8 @@ static int workload_thread(void *priv)
 					FORCEWAKE_ALL);
 
 		intel_runtime_pm_put(gvt->dev_priv);
-		if (ret && (vgpu_is_vm_unhealthy(ret))) {
-			mutex_lock(&gvt->lock);
-			intel_vgpu_clean_execlist(vgpu);
-			mutex_unlock(&gvt->lock);
+		if (ret && (vgpu_is_vm_unhealthy(ret)))
 			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
-		}
-
 	}
 	return 0;
 }
@@ -957,9 +952,90 @@ void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
 	kmem_cache_free(s->workloads, workload);
 }
 
+static struct intel_vgpu_workload *
+alloc_workload(struct intel_vgpu *vgpu)
+{
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct intel_vgpu_workload *workload;
+
+	workload = kmem_cache_zalloc(s->workloads, GFP_KERNEL);
+	if (!workload)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&workload->list);
+	INIT_LIST_HEAD(&workload->shadow_bb);
+
+	init_waitqueue_head(&workload->shadow_ctx_status_wq);
+	atomic_set(&workload->shadow_ctx_active, 0);
+
+	workload->status = -EINPROGRESS;
+	workload->shadowed = false;
+	workload->vgpu = vgpu;
+
+	return workload;
+}
+
+#define RING_CTX_OFF(x) \
+	offsetof(struct execlist_ring_context, x)
+
+static void read_guest_pdps(struct intel_vgpu *vgpu,
+		u64 ring_context_gpa, u32 pdp[8])
+{
+	u64 gpa;
+	int i;
+
+	gpa = ring_context_gpa + RING_CTX_OFF(pdp3_UDW.val);
+
+	for (i = 0; i < 8; i++)
+		intel_gvt_hypervisor_read_gpa(vgpu,
+				gpa + i * 8, &pdp[7 - i], 4);
+}
+
+static int prepare_mm(struct intel_vgpu_workload *workload)
+{
+	struct execlist_ctx_descriptor_format *desc = &workload->ctx_desc;
+	struct intel_vgpu_mm *mm;
+	struct intel_vgpu *vgpu = workload->vgpu;
+	int page_table_level;
+	u32 pdp[8];
+
+	if (desc->addressing_mode == 1) { /* legacy 32-bit */
+		page_table_level = 3;
+	} else if (desc->addressing_mode == 3) { /* legacy 64 bit */
+		page_table_level = 4;
+	} else {
+		gvt_vgpu_err("Advanced Context mode(SVM) is not supported!\n");
+		return -EINVAL;
+	}
+
+	read_guest_pdps(workload->vgpu, workload->ring_context_gpa, pdp);
+
+	mm = intel_vgpu_find_ppgtt_mm(workload->vgpu, page_table_level, pdp);
+	if (mm) {
+		intel_gvt_mm_reference(mm);
+	} else {
+
+		mm = intel_vgpu_create_mm(workload->vgpu, INTEL_GVT_MM_PPGTT,
+				pdp, page_table_level, 0);
+		if (IS_ERR(mm)) {
+			gvt_vgpu_err("fail to create mm object.\n");
+			return PTR_ERR(mm);
+		}
+	}
+	workload->shadow_mm = mm;
+	return 0;
+}
+
+#define same_context(a, b) (((a)->context_id == (b)->context_id) && \
+		((a)->lrca == (b)->lrca))
+
+#define get_last_workload(q) \
+	(list_empty(q) ? NULL : container_of(q->prev, \
+	struct intel_vgpu_workload, list))
 /**
  * intel_vgpu_create_workload - create a vGPU workload
  * @vgpu: a vGPU
+ * @desc: a guest context descriptor
  *
  * This function is called when creating a vGPU workload.
  *
@@ -969,24 +1045,108 @@ void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
  *
  */
 struct intel_vgpu_workload *
-intel_vgpu_create_workload(struct intel_vgpu *vgpu)
+intel_vgpu_create_workload(struct intel_vgpu *vgpu, int ring_id,
+			   struct execlist_ctx_descriptor_format *desc)
 {
 	struct intel_vgpu_submission *s = &vgpu->submission;
-	struct intel_vgpu_workload *workload;
+	struct list_head *q = workload_q_head(vgpu, ring_id);
+	struct intel_vgpu_workload *last_workload = get_last_workload(q);
+	struct intel_vgpu_workload *workload = NULL;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	u64 ring_context_gpa;
+	u32 head, tail, start, ctl, ctx_ctl, per_ctx, indirect_ctx;
+	int ret;
 
-	workload = kmem_cache_zalloc(s->workloads, GFP_KERNEL);
-	if (!workload)
-		return ERR_PTR(-ENOMEM);
+	ring_context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
+			(u32)((desc->lrca + 1) << GTT_PAGE_SHIFT));
+	if (ring_context_gpa == INTEL_GVT_INVALID_ADDR) {
+		gvt_vgpu_err("invalid guest context LRCA: %x\n", desc->lrca);
+		return ERR_PTR(-EINVAL);
+	}
 
-	INIT_LIST_HEAD(&workload->list);
-	INIT_LIST_HEAD(&workload->shadow_bb);
+	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(ring_header.val), &head, 4);
 
-	init_waitqueue_head(&workload->shadow_ctx_status_wq);
-	atomic_set(&workload->shadow_ctx_active, 0);
+	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(ring_tail.val), &tail, 4);
 
-	workload->status = -EINPROGRESS;
-	workload->shadowed = false;
-	workload->vgpu = vgpu;
+	head &= RB_HEAD_OFF_MASK;
+	tail &= RB_TAIL_OFF_MASK;
+
+	if (last_workload && same_context(&last_workload->ctx_desc, desc)) {
+		gvt_dbg_el("ring id %d cur workload == last\n", ring_id);
+		gvt_dbg_el("ctx head %x real head %lx\n", head,
+				last_workload->rb_tail);
+		/*
+		 * cannot use guest context head pointer here,
+		 * as it might not be updated at this time
+		 */
+		head = last_workload->rb_tail;
+	}
+
+	gvt_dbg_el("ring id %d begin a new workload\n", ring_id);
+
+	/* record some ring buffer register values for scan and shadow */
+	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(rb_start.val), &start, 4);
+	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(rb_ctrl.val), &ctl, 4);
+	intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(ctx_ctrl.val), &ctx_ctl, 4);
+
+	workload = alloc_workload(vgpu);
+	if (IS_ERR(workload))
+		return workload;
+
+	workload->ring_id = ring_id;
+	workload->ctx_desc = *desc;
+	workload->ring_context_gpa = ring_context_gpa;
+	workload->rb_head = head;
+	workload->rb_tail = tail;
+	workload->rb_start = start;
+	workload->rb_ctl = ctl;
+
+	if (ring_id == RCS) {
+		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(bb_per_ctx_ptr.val), &per_ctx, 4);
+		intel_gvt_hypervisor_read_gpa(vgpu, ring_context_gpa +
+			RING_CTX_OFF(rcs_indirect_ctx.val), &indirect_ctx, 4);
+
+		workload->wa_ctx.indirect_ctx.guest_gma =
+			indirect_ctx & INDIRECT_CTX_ADDR_MASK;
+		workload->wa_ctx.indirect_ctx.size =
+			(indirect_ctx & INDIRECT_CTX_SIZE_MASK) *
+			CACHELINE_BYTES;
+		workload->wa_ctx.per_ctx.guest_gma =
+			per_ctx & PER_CTX_ADDR_MASK;
+		workload->wa_ctx.per_ctx.valid = per_ctx & 1;
+	}
+
+	gvt_dbg_el("workload %p ring id %d head %x tail %x start %x ctl %x\n",
+			workload, ring_id, head, tail, start, ctl);
+
+	ret = prepare_mm(workload);
+	if (ret) {
+		kmem_cache_free(s->workloads, workload);
+		return ERR_PTR(ret);
+	}
+
+	/* Only scan and shadow the first workload in the queue
+	 * as there is only one pre-allocated buf-obj for shadow.
+	 */
+	if (list_empty(workload_q_head(vgpu, ring_id))) {
+		intel_runtime_pm_get(dev_priv);
+		mutex_lock(&dev_priv->drm.struct_mutex);
+		ret = intel_gvt_scan_and_shadow_workload(workload);
+		mutex_unlock(&dev_priv->drm.struct_mutex);
+		intel_runtime_pm_put(dev_priv);
+	}
+
+	if (ret && (vgpu_is_vm_unhealthy(ret))) {
+		enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
+		intel_vgpu_destroy_workload(workload);
+		return ERR_PTR(ret);
+	}
 
 	return workload;
 }

commit d8235b5e55845de19983cec38af245cc200b81e2
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue Sep 12 22:06:39 2017 +0800

    drm/i915/gvt: Move common workload preparation into prepare_workload()
    
    Move common workload preparation into prepare_workload() in scheduler.c,
    as they are not specific to execlist emulation.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 3d1435f55c7b..a7a67cc65a07 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -325,13 +325,157 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
+static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
+{
+	struct intel_gvt *gvt = workload->vgpu->gvt;
+	const int gmadr_bytes = gvt->device_info.gmadr_bytes_in_cmd;
+	struct intel_shadow_bb_entry *entry_obj;
+
+	/* pin the gem object to ggtt */
+	list_for_each_entry(entry_obj, &workload->shadow_bb, list) {
+		struct i915_vma *vma;
+
+		vma = i915_gem_object_ggtt_pin(entry_obj->obj, NULL, 0, 4, 0);
+		if (IS_ERR(vma))
+			return PTR_ERR(vma);
+
+		/* FIXME: we are not tracking our pinned VMA leaving it
+		 * up to the core to fix up the stray pin_count upon
+		 * free.
+		 */
+
+		/* update the relocate gma with shadow batch buffer*/
+		entry_obj->bb_start_cmd_va[1] = i915_ggtt_offset(vma);
+		if (gmadr_bytes == 8)
+			entry_obj->bb_start_cmd_va[2] = 0;
+	}
+	return 0;
+}
+
+static int update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)
+{
+	struct intel_vgpu_workload *workload = container_of(wa_ctx,
+					struct intel_vgpu_workload,
+					wa_ctx);
+	int ring_id = workload->ring_id;
+	struct intel_vgpu_submission *s = &workload->vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
+	struct drm_i915_gem_object *ctx_obj =
+		shadow_ctx->engine[ring_id].state->obj;
+	struct execlist_ring_context *shadow_ring_context;
+	struct page *page;
+
+	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
+	shadow_ring_context = kmap_atomic(page);
+
+	shadow_ring_context->bb_per_ctx_ptr.val =
+		(shadow_ring_context->bb_per_ctx_ptr.val &
+		(~PER_CTX_ADDR_MASK)) | wa_ctx->per_ctx.shadow_gma;
+	shadow_ring_context->rcs_indirect_ctx.val =
+		(shadow_ring_context->rcs_indirect_ctx.val &
+		(~INDIRECT_CTX_ADDR_MASK)) | wa_ctx->indirect_ctx.shadow_gma;
+
+	kunmap_atomic(shadow_ring_context);
+	return 0;
+}
+
+static int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
+{
+	struct i915_vma *vma;
+	unsigned char *per_ctx_va =
+		(unsigned char *)wa_ctx->indirect_ctx.shadow_va +
+		wa_ctx->indirect_ctx.size;
+
+	if (wa_ctx->indirect_ctx.size == 0)
+		return 0;
+
+	vma = i915_gem_object_ggtt_pin(wa_ctx->indirect_ctx.obj, NULL,
+				       0, CACHELINE_BYTES, 0);
+	if (IS_ERR(vma))
+		return PTR_ERR(vma);
+
+	/* FIXME: we are not tracking our pinned VMA leaving it
+	 * up to the core to fix up the stray pin_count upon
+	 * free.
+	 */
+
+	wa_ctx->indirect_ctx.shadow_gma = i915_ggtt_offset(vma);
+
+	wa_ctx->per_ctx.shadow_gma = *((unsigned int *)per_ctx_va + 1);
+	memset(per_ctx_va, 0, CACHELINE_BYTES);
+
+	update_wa_ctx_2_shadow_ctx(wa_ctx);
+	return 0;
+}
+
+static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)
+{
+	/* release all the shadow batch buffer */
+	if (!list_empty(&workload->shadow_bb)) {
+		struct intel_shadow_bb_entry *entry_obj =
+			list_first_entry(&workload->shadow_bb,
+					 struct intel_shadow_bb_entry,
+					 list);
+		struct intel_shadow_bb_entry *temp;
+
+		list_for_each_entry_safe(entry_obj, temp, &workload->shadow_bb,
+					 list) {
+			i915_gem_object_unpin_map(entry_obj->obj);
+			i915_gem_object_put(entry_obj->obj);
+			list_del(&entry_obj->list);
+			kfree(entry_obj);
+		}
+	}
+}
+
 static int prepare_workload(struct intel_vgpu_workload *workload)
 {
+	struct intel_vgpu *vgpu = workload->vgpu;
 	int ret = 0;
 
-	if (workload->prepare)
+	ret = intel_vgpu_pin_mm(workload->shadow_mm);
+	if (ret) {
+		gvt_vgpu_err("fail to vgpu pin mm\n");
+		return ret;
+	}
+
+	ret = intel_vgpu_sync_oos_pages(workload->vgpu);
+	if (ret) {
+		gvt_vgpu_err("fail to vgpu sync oos pages\n");
+		goto err_unpin_mm;
+	}
+
+	ret = intel_vgpu_flush_post_shadow(workload->vgpu);
+	if (ret) {
+		gvt_vgpu_err("fail to flush post shadow\n");
+		goto err_unpin_mm;
+	}
+
+	ret = prepare_shadow_batch_buffer(workload);
+	if (ret) {
+		gvt_vgpu_err("fail to prepare_shadow_batch_buffer\n");
+		goto err_unpin_mm;
+	}
+
+	ret = prepare_shadow_wa_ctx(&workload->wa_ctx);
+	if (ret) {
+		gvt_vgpu_err("fail to prepare_shadow_wa_ctx\n");
+		goto err_shadow_batch;
+	}
+
+	if (workload->prepare) {
 		ret = workload->prepare(workload);
+		if (ret)
+			goto err_shadow_wa_ctx;
+	}
 
+	return 0;
+err_shadow_wa_ctx:
+	release_shadow_wa_ctx(&workload->wa_ctx);
+err_shadow_batch:
+	release_shadow_batch_buffer(workload);
+err_unpin_mm:
+	intel_vgpu_unpin_mm(workload->shadow_mm);
 	return ret;
 }
 
@@ -557,6 +701,12 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	scheduler->current_workload[ring_id] = NULL;
 
 	list_del_init(&workload->list);
+
+	if (!workload->status) {
+		release_shadow_batch_buffer(workload);
+		release_shadow_wa_ctx(&workload->wa_ctx);
+	}
+
 	workload->complete(workload);
 
 	atomic_dec(&s->running_workload_num);

commit 497aa3f5e3bdb6bea5994f7075e2f2df2377d70e
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue Sep 12 21:51:10 2017 +0800

    drm/i915/gvt: Factor out prepare_workload()
    
    Factor out prepare_workload() for the following re-factor.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 10ccb05d0e8d..3d1435f55c7b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -325,6 +325,16 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
+static int prepare_workload(struct intel_vgpu_workload *workload)
+{
+	int ret = 0;
+
+	if (workload->prepare)
+		ret = workload->prepare(workload);
+
+	return ret;
+}
+
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
@@ -344,12 +354,10 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	if (ret)
 		goto out;
 
-	if (workload->prepare) {
-		ret = workload->prepare(workload);
-		if (ret) {
-			engine->context_unpin(engine, shadow_ctx);
-			goto out;
-		}
+	ret = prepare_workload(workload);
+	if (ret) {
+		engine->context_unpin(engine, shadow_ctx);
+		goto out;
 	}
 
 out:

commit 21527a8dafc40fc499ae57492c1c5d0098cbcf08
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue Sep 12 21:42:09 2017 +0800

    drm/i915/gvt: Factor out vGPU workload creation/destroy
    
    Factor out vGPU workload creation/destroy functions since they are not
    specific to execlist emulation.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 02af14023383..10ccb05d0e8d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -781,3 +781,54 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 	i915_gem_context_put(s->shadow_ctx);
 	return ret;
 }
+
+/**
+ * intel_vgpu_destroy_workload - destroy a vGPU workload
+ * @vgpu: a vGPU
+ *
+ * This function is called when destroy a vGPU workload.
+ *
+ */
+void intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu_submission *s = &workload->vgpu->submission;
+
+	if (workload->shadow_mm)
+		intel_gvt_mm_unreference(workload->shadow_mm);
+
+	kmem_cache_free(s->workloads, workload);
+}
+
+/**
+ * intel_vgpu_create_workload - create a vGPU workload
+ * @vgpu: a vGPU
+ *
+ * This function is called when creating a vGPU workload.
+ *
+ * Returns:
+ * struct intel_vgpu_workload * on success, negative error code in
+ * pointer if failed.
+ *
+ */
+struct intel_vgpu_workload *
+intel_vgpu_create_workload(struct intel_vgpu *vgpu)
+{
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct intel_vgpu_workload *workload;
+
+	workload = kmem_cache_zalloc(s->workloads, GFP_KERNEL);
+	if (!workload)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&workload->list);
+	INIT_LIST_HEAD(&workload->shadow_bb);
+
+	init_waitqueue_head(&workload->shadow_ctx_status_wq);
+	atomic_set(&workload->shadow_ctx_active, 0);
+
+	workload->status = -EINPROGRESS;
+	workload->shadowed = false;
+	workload->vgpu = vgpu;
+
+	return workload;
+}

commit e011c6ce2b4fc7c577ade41485d74431a4e6ea1a
Author: fred gao <fred.gao@intel.com>
Date:   Tue Sep 19 15:11:28 2017 +0800

    drm/i915/gvt: Add VM healthy check for workload_thread
    
    When a scan error occurs in dispatch_workload, this patch is to
    check the healthy state and free all the queued workloads before
    the failsafe mode is entered.
    
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0771b715f825..02af14023383 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -634,6 +634,13 @@ static int workload_thread(void *priv)
 					FORCEWAKE_ALL);
 
 		intel_runtime_pm_put(gvt->dev_priv);
+		if (ret && (vgpu_is_vm_unhealthy(ret))) {
+			mutex_lock(&gvt->lock);
+			intel_vgpu_clean_execlist(vgpu);
+			mutex_unlock(&gvt->lock);
+			enter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);
+		}
+
 	}
 	return 0;
 }

commit 5c56883a9531cd89561fb9a11a33697f2847c82a
Author: fred gao <fred.gao@intel.com>
Date:   Wed Sep 20 05:36:47 2017 +0800

    drm/i915/gvt: Change the return type during command scan
    
    Generally, there are 3 types of errors during command scan: a) some
    commands might be unknown with EBADRQC;  b) some cmd access invalid
    address with EFAULT; c) some unexpected force nonpriv cmd with EPERM.
    later the healthy state can be judged through the return error.
    
    v2:
    - remove some internal i915 errors rating.  (Zhenyu)
    
    v3:
    - the healthy state is judged through the internal defined return
      error. (Zhenyu)
    - force non priv cmd error can be ignored. (Kevin)
    
    v4:
    - reuse standard defined errno instead of recreate, e.g EBADRQC for
      unknown cmd, EFAULT for invalid address, EPERM for nonpriv. (Zhenyu)
    
    v5:
    - remove some irrelevant code for the patch.
    - fix typo of vgpu_is_vm_unhealthy. (Zhenyu)
    
    v6:
    - move the healthy check and failsafe code into another patch. (Zhenyu)
    
    v7:
    - polish title and commit message. (Zhenyu)
    
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7cb1cf4223ed..0771b715f825 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -84,7 +84,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 				GTT_PAGE_SHIFT));
 		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
 			gvt_vgpu_err("Invalid guest context descriptor\n");
-			return -EINVAL;
+			return -EFAULT;
 		}
 
 		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);

commit 91d5d85442b2a65e5f4e1726565c1c1a8ba9976f
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun Sep 10 21:33:20 2017 +0800

    drm/i915/gvt: Move tlb_handle_pending into intel_vgpu_submission
    
    Move tlb_handle_pending into intel_vgpu_submssion since it belongs to a
    part of vGPU submission stuffs
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 864a2bc06e45..7cb1cf4223ed 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -766,6 +766,7 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 		INIT_LIST_HEAD(&s->workload_q_head[i]);
 
 	atomic_set(&s->running_workload_num, 0);
+	bitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);
 
 	return 0;
 

commit 1406a14b0ed977fc18f43398b391e4bb5d744174
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun Sep 10 21:15:18 2017 +0800

    drm/i915/gvt: Introduce intel_vgpu_submission
    
    Introduce intel_vgpu_submission to hold all members related to submission
    in struct intel_vgpu before.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 81952139b00c..864a2bc06e45 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -57,7 +57,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
 	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
+	struct i915_gem_context *shadow_ctx = vgpu->submission.shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
 		shadow_ctx->engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
@@ -249,12 +249,13 @@ void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
  */
 int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 {
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
-	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct drm_i915_gem_request *rq;
-	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_ring *ring;
 	int ret;
 
@@ -267,7 +268,7 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	if (!test_and_set_bit(ring_id, vgpu->shadow_ctx_desc_updated))
+	if (!test_and_set_bit(ring_id, s->shadow_ctx_desc_updated))
 		shadow_context_descriptor_update(shadow_ctx,
 					dev_priv->engine[ring_id]);
 
@@ -326,9 +327,11 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
-	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	int ret = 0;
 
@@ -414,7 +417,7 @@ static struct intel_vgpu_workload *pick_next_workload(
 
 	gvt_dbg_sched("ring id %d pick new workload %p\n", ring_id, workload);
 
-	atomic_inc(&workload->vgpu->running_workload_num);
+	atomic_inc(&workload->vgpu->submission.running_workload_num);
 out:
 	mutex_unlock(&gvt->lock);
 	return workload;
@@ -424,8 +427,9 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
 	struct intel_gvt *gvt = vgpu->gvt;
+	struct intel_vgpu_submission *s = &vgpu->submission;
+	struct i915_gem_context *shadow_ctx = s->shadow_ctx;
 	int ring_id = workload->ring_id;
-	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_gem_object *ctx_obj =
 		shadow_ctx->engine[ring_id].state->obj;
 	struct execlist_ring_context *shadow_ring_context;
@@ -491,15 +495,14 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	struct intel_vgpu_workload *workload;
-	struct intel_vgpu *vgpu;
+	struct intel_vgpu_workload *workload =
+		scheduler->current_workload[ring_id];
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_vgpu_submission *s = &vgpu->submission;
 	int event;
 
 	mutex_lock(&gvt->lock);
 
-	workload = scheduler->current_workload[ring_id];
-	vgpu = workload->vgpu;
-
 	/* For the workload w/ request, needs to wait for the context
 	 * switch to make sure request is completed.
 	 * For the workload w/o request, directly complete the workload.
@@ -536,7 +539,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		}
 		mutex_lock(&dev_priv->drm.struct_mutex);
 		/* unpin shadow ctx as the shadow_ctx update is done */
-		engine->context_unpin(engine, workload->vgpu->shadow_ctx);
+		engine->context_unpin(engine, s->shadow_ctx);
 		mutex_unlock(&dev_priv->drm.struct_mutex);
 	}
 
@@ -548,7 +551,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	list_del_init(&workload->list);
 	workload->complete(workload);
 
-	atomic_dec(&vgpu->running_workload_num);
+	atomic_dec(&s->running_workload_num);
 	wake_up(&scheduler->workload_complete_wq);
 
 	if (gvt->scheduler.need_reschedule)
@@ -637,14 +640,15 @@ static int workload_thread(void *priv)
 
 void intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu)
 {
+	struct intel_vgpu_submission *s = &vgpu->submission;
 	struct intel_gvt *gvt = vgpu->gvt;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 
-	if (atomic_read(&vgpu->running_workload_num)) {
+	if (atomic_read(&s->running_workload_num)) {
 		gvt_dbg_sched("wait vgpu idle\n");
 
 		wait_event(scheduler->workload_complete_wq,
-				!atomic_read(&vgpu->running_workload_num));
+				!atomic_read(&s->running_workload_num));
 	}
 }
 
@@ -718,8 +722,10 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
  */
 void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
-	i915_gem_context_put(vgpu->shadow_ctx);
-	kmem_cache_destroy(vgpu->workloads);
+	struct intel_vgpu_submission *s = &vgpu->submission;
+
+	i915_gem_context_put(s->shadow_ctx);
+	kmem_cache_destroy(s->workloads);
 }
 
 /**
@@ -734,35 +740,36 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
  */
 int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
+	struct intel_vgpu_submission *s = &vgpu->submission;
 	enum intel_engine_id i;
 	struct intel_engine_cs *engine;
 	int ret;
 
-	vgpu->shadow_ctx = i915_gem_context_create_gvt(
+	s->shadow_ctx = i915_gem_context_create_gvt(
 			&vgpu->gvt->dev_priv->drm);
-	if (IS_ERR(vgpu->shadow_ctx))
-		return PTR_ERR(vgpu->shadow_ctx);
+	if (IS_ERR(s->shadow_ctx))
+		return PTR_ERR(s->shadow_ctx);
 
-	bitmap_zero(vgpu->shadow_ctx_desc_updated, I915_NUM_ENGINES);
+	bitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
-	vgpu->workloads = kmem_cache_create("gvt-g_vgpu_workload",
+	s->workloads = kmem_cache_create("gvt-g_vgpu_workload",
 			sizeof(struct intel_vgpu_workload), 0,
 			SLAB_HWCACHE_ALIGN,
 			NULL);
 
-	if (!vgpu->workloads) {
+	if (!s->workloads) {
 		ret = -ENOMEM;
 		goto out_shadow_ctx;
 	}
 
 	for_each_engine(engine, vgpu->gvt->dev_priv, i)
-		INIT_LIST_HEAD(&vgpu->workload_q_head[i]);
+		INIT_LIST_HEAD(&s->workload_q_head[i]);
 
-	atomic_set(&vgpu->running_workload_num, 0);
+	atomic_set(&s->running_workload_num, 0);
 
 	return 0;
 
 out_shadow_ctx:
-	i915_gem_context_put(vgpu->shadow_ctx);
+	i915_gem_context_put(s->shadow_ctx);
 	return ret;
 }

commit 9a9829e9eb8bc4b4e870ce15a8904a32991608d5
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun Sep 10 20:28:09 2017 +0800

    drm/i915/gvt: Move workload cache init/clean into intel_vgpu_{setup, clean}_submission()
    
    Move vGPU workload cache initialization/de-initialization into
    intel_vgpu_{setup, clean}_submission() since they are not specific to
    execlist stuffs.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5913bcb7b73e..81952139b00c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -719,6 +719,7 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
 	i915_gem_context_put(vgpu->shadow_ctx);
+	kmem_cache_destroy(vgpu->workloads);
 }
 
 /**
@@ -733,7 +734,9 @@ void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
  */
 int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
-	atomic_set(&vgpu->running_workload_num, 0);
+	enum intel_engine_id i;
+	struct intel_engine_cs *engine;
+	int ret;
 
 	vgpu->shadow_ctx = i915_gem_context_create_gvt(
 			&vgpu->gvt->dev_priv->drm);
@@ -742,5 +745,24 @@ int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 
 	bitmap_zero(vgpu->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
+	vgpu->workloads = kmem_cache_create("gvt-g_vgpu_workload",
+			sizeof(struct intel_vgpu_workload), 0,
+			SLAB_HWCACHE_ALIGN,
+			NULL);
+
+	if (!vgpu->workloads) {
+		ret = -ENOMEM;
+		goto out_shadow_ctx;
+	}
+
+	for_each_engine(engine, vgpu->gvt->dev_priv, i)
+		INIT_LIST_HEAD(&vgpu->workload_q_head[i]);
+
+	atomic_set(&vgpu->running_workload_num, 0);
+
 	return 0;
+
+out_shadow_ctx:
+	i915_gem_context_put(vgpu->shadow_ctx);
+	return ret;
 }

commit 874b6a910e6cc094629bd2634d14061cf5eb7690
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun Sep 10 20:08:18 2017 +0800

    drm/i915/gvt: Rename intel_vgpu_{init, clean}_gvt_context()
    
    To move workload related functions into scheduler.c, an expected way is
    to collect all the init/clean functions related to vGPU workload
    submission into fewer functions.
    
    Rename intel_vgpu_{init, clean}_gvt_context() for above usage in future.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 42cc61230ca7..5913bcb7b73e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -709,12 +709,29 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 	return ret;
 }
 
-void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
+/**
+ * intel_vgpu_clean_submission - free submission-related resource for vGPU
+ * @vgpu: a vGPU
+ *
+ * This function is called when a vGPU is being destroyed.
+ *
+ */
+void intel_vgpu_clean_submission(struct intel_vgpu *vgpu)
 {
 	i915_gem_context_put(vgpu->shadow_ctx);
 }
 
-int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
+/**
+ * intel_vgpu_setup_submission - setup submission-related resource for vGPU
+ * @vgpu: a vGPU
+ *
+ * This function is called when a vGPU is being created.
+ *
+ * Returns:
+ * Zero on success, negative error code if failed.
+ *
+ */
+int intel_vgpu_setup_submission(struct intel_vgpu *vgpu)
 {
 	atomic_set(&vgpu->running_workload_num, 0);
 

commit d2b4b97933f5adacfba42dc3b9200d0e21fbe2c4
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Nov 10 14:26:33 2017 +0000

    drm/i915: Record the default hw state after reset upon load
    
    Take a copy of the HW state after a reset upon module loading by
    executing a context switch from a blank context to the kernel context,
    thus saving the default hw state over the blank context image.
    We can then use the default hw state to initialise any future context,
    ensuring that each starts with the default view of hw state.
    
    v2: Unmap our default state from the GTT after stealing it from the
    context. This should stop us from accidentally overwriting it via the
    GTT (and frees up some precious GTT space).
    
    Testcase: igt/gem_ctx_isolation
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Ville Syrjälä <ville.syrjala@linux.intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171110142634.10551-7-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f6ded475bb2c..42cc61230ca7 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -723,8 +723,6 @@ int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
 	if (IS_ERR(vgpu->shadow_ctx))
 		return PTR_ERR(vgpu->shadow_ctx);
 
-	vgpu->shadow_ctx->engine[RCS].initialised = true;
-
 	bitmap_zero(vgpu->shadow_ctx_desc_updated, I915_NUM_ENGINES);
 
 	return 0;

commit d6c0511300dcff19969844495ba293c4efb50b42
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Oct 3 21:34:47 2017 +0100

    drm/i915/execlists: Distinguish the incomplete context notifies
    
    Let the listener know that the context we just scheduled out was not
    complete, and will be scheduled back in at a later point.
    
    v2: Handle CONTEXT_STATUS_PREEMPTED in gvt by aliasing it to
    CONTEXT_STATUS_OUT for the moment, gvt can expand upon the difference
    later.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: "Zhenyu Wang" <zhenyuw@linux.intel.com>
    Cc: "Wang, Zhi A" <zhi.a.wang@intel.com>
    Cc: Michał Winiarski <michal.winiarski@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171003203453.15692-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d5892d24f0b6..f6ded475bb2c 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -174,6 +174,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
+	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	default:

commit 0b29c75a01e5413d9c1f368b8357b589c7b5a922
Author: Michel Thierry <michel.thierry@intel.com>
Date:   Wed Sep 13 09:56:00 2017 +0100

    drm/i915/lrc: Clarify the format of the context image
    
    Not only the context image consist of two parts (the PPHWSP, and the
    logical context state), but we also allocate a header at the start of
    for sharing data with GuC. Thus every lrc looks like this:
    
      | [guc]          | [hwsp] [logical state] |
      |<- our header ->|<- context image      ->|
    
    So far, we have oversimplified whenever we use each of these parts of the
    context, just because the GuC header happens to be in page 0, and the
    (PP)HWSP is in page 1. But this had led to using the same define for more
    than one meaning (as a page index in the lrc and as 1 page).
    
    This patch adds defines for the GuC shared page, the PPHWSP page and the
    start of the logical state. It also updated the places where the old
    define was being used. Since we are not changing the size (or format) of
    the context, there are no functional changes.
    
    v2: Use PPHWSP index for hws again.
    
    Suggested-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Michel Thierry <michel.thierry@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: Oscar Mateo <oscar.mateo@intel.com>
    Cc: intel-gvt-dev@lists.freedesktop.org
    Link: http://patchwork.freedesktop.org/patch/msgid/20170712193032.27080-1-michel.thierry@intel.com
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20170913085605.18299-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 6fb9b589276d..d5892d24f0b6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -87,7 +87,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			return -EINVAL;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
+		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
 		dst = kmap(page);
 		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
 				GTT_PAGE_SIZE);
@@ -454,7 +454,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			return;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
+		page = i915_gem_object_get_page(ctx_obj, LRC_HEADER_PAGES + i);
 		src = kmap(page);
 		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
 				GTT_PAGE_SIZE);

commit 0f43702a334b2848ae2e942dbc0677ddc4566b57
Author: fred gao <fred.gao@intel.com>
Date:   Fri Aug 18 15:41:10 2017 +0800

    drm/i915/gvt: Refine error handling in dispatch_workload
    
    When an error occurs in dispatch_workload, this patch is to do the
    proper cleanup and rollback to the original states before the workload
    is abandoned.
    
    v2:
    - split the mixed several error paths for better review. (Zhenyu)
    
    v3:
    - original PTR_ERR(cs) is good and code cleanup. (Zhenyu)
    
    v4:
    - reuse the existing i915_add_request for error handling. (Zhenyu)
    
    v5:
    - remove the duplicate error handling release_shadow_wa_ctx and
      move the engine->context_unpin upper. (Zhenyu)
    
    v6:
    - keep the old label "out". (Zhenyu)
    
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 29171961af5e..6fb9b589276d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -342,8 +342,10 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	if (workload->prepare) {
 		ret = workload->prepare(workload);
-		if (ret)
+		if (ret) {
+			engine->context_unpin(engine, shadow_ctx);
 			goto out;
+		}
 	}
 
 out:

commit a3cfdca920b274618d6046d85a474308ee28e5bb
Author: fred gao <fred.gao@intel.com>
Date:   Fri Aug 18 15:41:07 2017 +0800

    drm/i915/gvt: Add error handling for intel_gvt_scan_and_shadow_workload
    
    When an error occurs after shadow_indirect_ctx, this patch is to do the
    proper cleanup and rollback to the original states for shadowed indirect
    context before the workload is abandoned.
    
    v2:
    - split the mixed several error paths for better review. (Zhenyu)
    
    v3:
    - no return check for clean up functions. (Changbin)
    
    v4:
    - expose and reuse the existing release_shadow_wa_ctx. (Zhenyu)
    
    v5:
    - move the release function to scheduler.c file. (Zhenyu)
    
    v6:
    - move error handling code of intel_gvt_scan_and_shadow_workload
      to here. (Zhenyu)
    
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0e480f59f659..29171961af5e 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -229,6 +229,15 @@ static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
+void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)
+{
+	if (!wa_ctx->indirect_ctx.obj)
+		return;
+
+	i915_gem_object_unpin_map(wa_ctx->indirect_ctx.obj);
+	i915_gem_object_put(wa_ctx->indirect_ctx.obj);
+}
+
 /**
  * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
  * shadow it as well, include ringbuffer,wa_ctx and ctx.
@@ -263,13 +272,13 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 
 	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
-		goto out;
+		goto err_scan;
 
 	if ((workload->ring_id == RCS) &&
 	    (workload->wa_ctx.indirect_ctx.size != 0)) {
 		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
 		if (ret)
-			goto out;
+			goto err_scan;
 	}
 
 	/* pin shadow context by gvt even the shadow context will be pinned
@@ -283,18 +292,18 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	if (IS_ERR(ring)) {
 		ret = PTR_ERR(ring);
 		gvt_vgpu_err("fail to pin shadow context\n");
-		goto out;
+		goto err_shadow;
 	}
 
 	ret = populate_shadow_context(workload);
 	if (ret)
-		goto out;
+		goto err_unpin;
 
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
 		ret = PTR_ERR(rq);
-		goto out;
+		goto err_unpin;
 	}
 
 	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
@@ -302,10 +311,15 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	workload->req = i915_gem_request_get(rq);
 	ret = copy_workload_to_ring_buffer(workload);
 	if (ret)
-		goto out;
+		goto err_unpin;
 	workload->shadowed = true;
+	return 0;
 
-out:
+err_unpin:
+	engine->context_unpin(engine, shadow_ctx);
+err_shadow:
+	release_shadow_wa_ctx(&workload->wa_ctx);
+err_scan:
 	return ret;
 }
 

commit 0a53bc07f044c4c51eb0dc1386c504db80ca8d00
Author: fred gao <fred.gao@intel.com>
Date:   Fri Aug 18 15:41:06 2017 +0800

    drm/i915/gvt: Separate cmd scan from request allocation
    
    Currently i915 request structure and shadow ring buffer are allocated
    before command scan, so it will have to restore to previous states once
    any error happens afterwards in the long dispatch_workload path.
    
    This patch is to introduce a reserved ring buffer created at the beginning
    of vGPU initialization. Workload will be coped to this reserved buffer and
    be scanned first, the i915 request and shadow ring buffer are only
    allocated after the result of scan is successful.
    
    To balance the memory usage and buffer alloc time, the coming bigger ring
    buffer will be reallocated and kept until more bigger buffer is coming.
    
    v2:
    - use kmalloc for the smaller ring buffer, realloc if required. (Zhenyu)
    
    v3:
    - remove the dynamically allocated ring buffer. (Zhenyu)
    
    v4:
    - code style polish.
    - kfree previous allocated buffer once kmalloc failed. (Zhenyu)
    
    Signed-off-by: fred gao <fred.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 391800d2067b..0e480f59f659 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -201,6 +201,34 @@ static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
 	ce->lrc_desc = desc;
 }
 
+static int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	void *shadow_ring_buffer_va;
+	u32 *cs;
+
+	/* allocate shadow ring buffer */
+	cs = intel_ring_begin(workload->req, workload->rb_len / sizeof(u32));
+	if (IS_ERR(cs)) {
+		gvt_vgpu_err("fail to alloc size =%ld shadow  ring buffer\n",
+			workload->rb_len);
+		return PTR_ERR(cs);
+	}
+
+	shadow_ring_buffer_va = workload->shadow_ring_buffer_va;
+
+	/* get shadow ring buffer va */
+	workload->shadow_ring_buffer_va = cs;
+
+	memcpy(cs, shadow_ring_buffer_va,
+			workload->rb_len);
+
+	cs += workload->rb_len / sizeof(u32);
+	intel_ring_advance(workload->req, cs);
+
+	return 0;
+}
+
 /**
  * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
  * shadow it as well, include ringbuffer,wa_ctx and ctx.
@@ -214,8 +242,10 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct drm_i915_gem_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_ring *ring;
 	int ret;
 
 	lockdep_assert_held(&dev_priv->drm.struct_mutex);
@@ -231,17 +261,6 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 		shadow_context_descriptor_update(shadow_ctx,
 					dev_priv->engine[ring_id]);
 
-	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
-	if (IS_ERR(rq)) {
-		gvt_vgpu_err("fail to allocate gem request\n");
-		ret = PTR_ERR(rq);
-		goto out;
-	}
-
-	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
-
-	workload->req = i915_gem_request_get(rq);
-
 	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
 		goto out;
@@ -253,10 +272,37 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 			goto out;
 	}
 
+	/* pin shadow context by gvt even the shadow context will be pinned
+	 * when i915 alloc request. That is because gvt will update the guest
+	 * context from shadow context when workload is completed, and at that
+	 * moment, i915 may already unpined the shadow context to make the
+	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
+	 * the guest context, gvt can unpin the shadow_ctx safely.
+	 */
+	ring = engine->context_pin(engine, shadow_ctx);
+	if (IS_ERR(ring)) {
+		ret = PTR_ERR(ring);
+		gvt_vgpu_err("fail to pin shadow context\n");
+		goto out;
+	}
+
 	ret = populate_shadow_context(workload);
 	if (ret)
 		goto out;
 
+	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
+	if (IS_ERR(rq)) {
+		gvt_vgpu_err("fail to allocate gem request\n");
+		ret = PTR_ERR(rq);
+		goto out;
+	}
+
+	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
+
+	workload->req = i915_gem_request_get(rq);
+	ret = copy_workload_to_ring_buffer(workload);
+	if (ret)
+		goto out;
 	workload->shadowed = true;
 
 out:
@@ -269,8 +315,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
-	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_ring *ring;
 	int ret = 0;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
@@ -288,20 +332,6 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 			goto out;
 	}
 
-	/* pin shadow context by gvt even the shadow context will be pinned
-	 * when i915 alloc request. That is because gvt will update the guest
-	 * context from shadow context when workload is completed, and at that
-	 * moment, i915 may already unpined the shadow context to make the
-	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
-	 * the guest context, gvt can unpin the shadow_ctx safely.
-	 */
-	ring = engine->context_pin(engine, shadow_ctx);
-	if (IS_ERR(ring)) {
-		ret = PTR_ERR(ring);
-		gvt_vgpu_err("fail to pin shadow context\n");
-		goto out;
-	}
-
 out:
 	if (ret)
 		workload->status = ret;

commit 735f463af70e9601881ec879961ec42aef051733
Merge: 3aadb888b1b6 a42894ebb50d
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Aug 22 10:03:07 2017 +1000

    Merge tag 'drm-intel-next-2017-08-18' of git://anongit.freedesktop.org/git/drm-intel into drm-next
    
    Final pile of features for 4.14
    
    - New ioctl to change NOA configurations, plus prep (Lionel)
    - CCS (color compression) scanout support, based on the fancy new
      modifier additions (Ville&Ben)
    - Document i915 register macro style (Jani)
    - Many more gen10/cnl patches (Rodrigo, Pualo, ...)
    - More gpu reset vs. modeset duct-tape to restore the old way.
    - prep work for cnl: hpd_pin reorg (Rodrigo), support for more power
      wells (Imre), i2c pin reorg (Anusha)
    - drm_syncobj support (Jason Ekstrand)
    - forcewake vs gpu reset fix (Chris)
    - execbuf speedup for the no-relocs fastpath, anv/vk low-overhead ftw (Chris)
    - switch to idr/radixtree instead of the resizing ht for execbuf id->vma
      lookups (Chris)
    
    gvt:
    - MMIO save/restore optimization (Changbin)
    - Split workload scan vs. dispatch for more parallel exec (Ping)
    - vGPU full 48bit ppgtt support (Joonas, Tina)
    - vGPU hw id expose for perf (Zhenyu)
    
    Bunch of work all over to make the igt CI runs more complete/stable.
    Watch https://intel-gfx-ci.01.org/tree/drm-tip/shards-all.html for
    progress in getting this ready. Next week we're going into production
    mode (i.e. will send results to intel-gfx) on hsw, more platforms to
    come.
    
    Also, a new maintainer tram, I'm stepping out. Huge thanks to Jani for
    being an awesome co-maintainer the past few years, and all the best
    for Jani, Joonas&Rodrigo as the new maintainers!
    
    * tag 'drm-intel-next-2017-08-18' of git://anongit.freedesktop.org/git/drm-intel: (179 commits)
      drm/i915: Update DRIVER_DATE to 20170818
      drm/i915/bxt: use NULL for GPIO connection ID
      drm/i915: Mark the GT as busy before idling the previous request
      drm/i915: Trivial grammar fix s/opt of/opt out of/ in comment
      drm/i915: Replace execbuf vma ht with an idr
      drm/i915: Simplify eb_lookup_vmas()
      drm/i915: Convert execbuf to use struct-of-array packing for critical fields
      drm/i915: Check context status before looking up our obj/vma
      drm/i915: Don't use MI_STORE_DWORD_IMM on Sandybridge/vcs
      drm/i915: Stop touching forcewake following a gen6+ engine reset
      MAINTAINERS: drm/i915 has a new maintainer team
      drm/i915: Split pin mapping into per platform functions
      drm/i915/opregion: let user specify override VBT via firmware load
      drm/i915/cnl: Reuse skl_wm_get_hw_state on Cannonlake.
      drm/i915/gen10: implement gen 10 watermarks calculations
      drm/i915/cnl: Fix LSPCON support.
      drm/i915/vbt: ignore extraneous child devices for a port
      drm/i915/cnl: Setup PAT Index.
      drm/i915/edp: Allow alternate fixed mode for eDP if available.
      drm/i915: Add support for drm syncobjs
      ...

commit 0c697fafc66830ca7d5dc19123a1d0641deaa1f6
Merge: 09ef2378dc42 ef954844c7ac
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Aug 15 16:16:58 2017 +1000

    Backmerge tag 'v4.13-rc5' into drm-next
    
    Linux 4.13-rc5
    
    There's a really nasty nouveau collision, hopefully someone can take a look
    once I pushed this out.

commit 9dfb8e5b9112483530429c96d463e6d45e0106ed
Author: Kechen Lu <kechen.lu@intel.com>
Date:   Thu Aug 10 07:41:36 2017 +0800

    drm/i915/gvt: Add shadow context descriptor updating
    
    The current context logic only updates the descriptor of context when
    it's being pinned to graphics memory space. But this cannot satisfy the
    requirement of shadow context. The addressing mode of the pinned shadow
    context descriptor may be changed according to the guest addressing mode.
    And this won't be updated, as the already pinned shadow context has no
    chance to update its descriptor. And this will lead to GPU hang issue,
    as shadow context is used with wrong descriptor. This patch fixes this
    issue by letting the pinned shadow context descriptor update its
    addressing mode on demand.
    
    This patch fixes GPU HANG issue which happends after changing the
    grub parameter i915.enable_ppgtt form 0x01 to 0x03 or vice versa and
    then rebooting the guest.
    
    Signed-off-by: Tina Zhang <tina.zhang@intel.com>
    Signed-off-by: Kechen Lu <kechen.lu@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ca1926d564c9..025aba8a72e0 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -184,6 +184,23 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
+static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
+		struct intel_engine_cs *engine)
+{
+	struct intel_context *ce = &ctx->engine[engine->id];
+	u64 desc = 0;
+
+	desc = ce->lrc_desc;
+
+	/* Update bits 0-11 of the context descriptor which includes flags
+	 * like GEN8_CTX_* cached in desc_template
+	 */
+	desc &= U64_MAX << 12;
+	desc |= ctx->desc_template & ((1ULL << 12) - 1);
+
+	ce->lrc_desc = desc;
+}
+
 /**
  * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
  * shadow it as well, include ringbuffer,wa_ctx and ctx.
@@ -210,6 +227,10 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
+	if (!test_and_set_bit(ring_id, vgpu->shadow_ctx_desc_updated))
+		shadow_context_descriptor_update(shadow_ctx,
+					dev_priv->engine[ring_id]);
+
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
@@ -656,5 +677,7 @@ int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
 
 	vgpu->shadow_ctx->engine[RCS].initialised = true;
 
+	bitmap_zero(vgpu->shadow_ctx_desc_updated, I915_NUM_ENGINES);
+
 	return 0;
 }

commit 87e919d741f9bf07f8aad6f096c6ebc3345a9856
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Tue Jul 4 14:53:03 2017 +0800

    drm/i915/gvt: To check whether workload scan and shadow has mutex hold
    
    The function workload scan and shadow have to hold the drm.struct_mutex
    before called. To avoid misusing of this function, add a lockdep assert
    in it.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index bd59c6d09319..ca1926d564c9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -201,6 +201,8 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	int ret;
 
+	lockdep_assert_held(&dev_priv->drm.struct_mutex);
+
 	if (workload->shadowed)
 		return 0;
 

commit d0302e74003bf1f0fc41c06948b745204c4704ea
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Jun 29 12:22:43 2017 +0800

    drm/i915/gvt: Audit and shadow workload during ELSP writing
    
    Let the workload audit and shadow ahead of vGPU scheduling, that
    will eliminate GPU idle time and improve performance for multi-VM.
    
    The performance of Heaven running simultaneously in 3VMs has
    improved 20% after this patch.
    
    v2:Remove condition current->vgpu==vgpu when shadow during ELSP
    writing.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7929c0285d1d..bd59c6d09319 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -201,6 +201,9 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	struct intel_vgpu *vgpu = workload->vgpu;
 	int ret;
 
+	if (workload->shadowed)
+		return 0;
+
 	shadow_ctx->desc_template &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
 	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
@@ -228,6 +231,10 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	}
 
 	ret = populate_shadow_context(workload);
+	if (ret)
+		goto out;
+
+	workload->shadowed = true;
 
 out:
 	return ret;

commit 89ea20b930cb7372095645acae8928a75f7d5be5
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Thu Jun 29 12:22:42 2017 +0800

    drm/i915/gvt: Factor out scan and shadow from workload dispatch
    
    To perform the workload scan and shadow in ELSP writing stage for
    performance consideration, the workload scan and shadow stuffs
    should be factored out from dispatch_workload().
    
    v2:Put context pin before i915_add_request;
       Refine the comments;
       Rename some APIs;
    
    v3:workload->status should set only when error happens.
    v4:i915_add_request is must to have after i915_gem_request_alloc.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Reviewed-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 0e2e36ad6196..7929c0285d1d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -184,42 +184,27 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
-static int dispatch_workload(struct intel_vgpu_workload *workload)
+/**
+ * intel_gvt_scan_and_shadow_workload - audit the workload by scanning and
+ * shadow it as well, include ringbuffer,wa_ctx and ctx.
+ * @workload: an abstract entity for each execlist submission.
+ *
+ * This function is called before the workload submitting to i915, to make
+ * sure the content of the workload is valid.
+ */
+int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 {
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
-	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct drm_i915_gem_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_ring *ring;
 	int ret;
 
-	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
-		ring_id, workload);
-
 	shadow_ctx->desc_template &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
 	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	mutex_lock(&dev_priv->drm.struct_mutex);
-
-	/* pin shadow context by gvt even the shadow context will be pinned
-	 * when i915 alloc request. That is because gvt will update the guest
-	 * context from shadow context when workload is completed, and at that
-	 * moment, i915 may already unpined the shadow context to make the
-	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
-	 * the guest context, gvt can unpin the shadow_ctx safely.
-	 */
-	ring = engine->context_pin(engine, shadow_ctx);
-	if (IS_ERR(ring)) {
-		ret = PTR_ERR(ring);
-		gvt_vgpu_err("fail to pin shadow context\n");
-		workload->status = ret;
-		mutex_unlock(&dev_priv->drm.struct_mutex);
-		return ret;
-	}
-
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
@@ -231,7 +216,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	workload->req = i915_gem_request_get(rq);
 
-	ret = intel_gvt_scan_and_shadow_workload(workload);
+	ret = intel_gvt_scan_and_shadow_ringbuffer(workload);
 	if (ret)
 		goto out;
 
@@ -243,6 +228,27 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	}
 
 	ret = populate_shadow_context(workload);
+
+out:
+	return ret;
+}
+
+static int dispatch_workload(struct intel_vgpu_workload *workload)
+{
+	int ring_id = workload->ring_id;
+	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_ring *ring;
+	int ret = 0;
+
+	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
+		ring_id, workload);
+
+	mutex_lock(&dev_priv->drm.struct_mutex);
+
+	ret = intel_gvt_scan_and_shadow_workload(workload);
 	if (ret)
 		goto out;
 
@@ -252,19 +258,30 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 			goto out;
 	}
 
-	gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
-			ring_id, workload->req);
+	/* pin shadow context by gvt even the shadow context will be pinned
+	 * when i915 alloc request. That is because gvt will update the guest
+	 * context from shadow context when workload is completed, and at that
+	 * moment, i915 may already unpined the shadow context to make the
+	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
+	 * the guest context, gvt can unpin the shadow_ctx safely.
+	 */
+	ring = engine->context_pin(engine, shadow_ctx);
+	if (IS_ERR(ring)) {
+		ret = PTR_ERR(ring);
+		gvt_vgpu_err("fail to pin shadow context\n");
+		goto out;
+	}
 
-	ret = 0;
-	workload->dispatched = true;
 out:
 	if (ret)
 		workload->status = ret;
 
-	if (!IS_ERR_OR_NULL(rq))
-		i915_add_request(rq);
-	else
-		engine->context_unpin(engine, shadow_ctx);
+	if (!IS_ERR_OR_NULL(workload->req)) {
+		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
+				ring_id, workload->req);
+		i915_add_request(workload->req);
+		workload->dispatched = true;
+	}
 
 	mutex_unlock(&dev_priv->drm.struct_mutex);
 	return ret;

commit 6184cc8ddbb318758a000da68c5285fc2dd74338
Author: Chuanxiao Dong <chuanxiao.dong@intel.com>
Date:   Tue Aug 1 17:47:25 2017 +0800

    drm/i915/gvt: change resetting to resetting_eng
    
    Use resetting_eng to identify which engine is resetting
    so the rest ones' workload won't be impacted
    
    v2:
    - use ENGINE_MASK(ring_id) instead of (1 << ring_id). (Zhenyu)
    
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 4f7057d62d88..22e08eb2d0b7 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -432,7 +432,8 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 		i915_gem_request_put(fetch_and_zero(&workload->req));
 
-		if (!workload->status && !vgpu->resetting) {
+		if (!workload->status && !(vgpu->resetting_eng &
+					   ENGINE_MASK(ring_id))) {
 			update_guest_context(workload);
 
 			for_each_set_bit(event, workload->pending_events,

commit 2d62c799f8ffac4f7ffba6a4e7f148827dfc24c7
Merge: 5771a8c08880 58947144af34
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Jul 20 11:31:43 2017 +1000

    Merge tag 'drm-intel-next-2017-07-17' of git://anongit.freedesktop.org/git/drm-intel into drm-next
    
    2nd round of 4.14 features:
    
    - prep for deferred fbdev setup
    - refactor fixed 16.16 computations and skl+ wm code (Mahesh Kumar)
    - more cnl paches (Rodrigo, Imre et al)
    - tighten context cleanup and handling (Chris Wilson)
    - fix interlaced handling on skl+ (Mahesh Kumar)
    - small bits as usual
    
    * tag 'drm-intel-next-2017-07-17' of git://anongit.freedesktop.org/git/drm-intel: (84 commits)
      drm/i915: Update DRIVER_DATE to 20170717
      drm/i915: Protect against deferred fbdev setup
      drm/i915/fbdev: Always forward hotplug events
      drm/i915/skl+: unify cpp value in WM calculation
      drm/i915/skl+: WM calculation don't require height
      drm/i915: Addition wrapper for fixed16.16 operation
      drm/i915: cleanup fixed-point wrappers naming
      drm/i915: Always perform internal fixed16 division in 64 bits
      drm/i915: take-out common clamping code of fixed16 wrappers
      drm/i915/cnl: Add missing type case.
      drm/i915/cnl: Add max allowed Cannonlake DC.
      drm/i915: Make DP-MST connector info work
      drm/i915/cnl: Get DDI clock based on PLLs.
      drm/i915/cnl: Inherit RPS stuff from previous platforms.
      drm/i915/cnl: Gen10 render context size.
      drm/i915/cnl: Don't trust VBT's alternate pin for port D for now.
      drm/i915: Fix the kernel panic when using aliasing ppgtt
      drm/i915/cnl: Cannonlake color init.
      drm/i915/cnl: Add force wake for gen10+.
      x86/gpu: CNL uses the same GMS values as SKL
      ...

commit 0cf5ec41839d82ee7f8fbb47f137b7afc562b9f1
Author: Chuanxiao Dong <chuanxiao.dong@intel.com>
Date:   Fri Jun 23 13:01:11 2017 +0800

    drm/i915/gvt: Use fence error from GVT request for workload status
    
    The req->fence.error will be set if this request caused GPU hang so
    we can use this value to workload->status to indicate whether this
    GVT request caused any problem. If it caused GPU hang, we shouldn't
    trigger any context switch back to the guest.
    
    v2:
    - only take -EIO from fence->error. (Zhenyu)
    
    Fixes: 8f1117abb408 (drm/i915/gvt: handle workload lifecycle properly)
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5aeba13a5de4..4f7057d62d88 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -174,15 +174,6 @@ static int shadow_context_status_change(struct notifier_block *nb,
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
-		/* If the status is -EINPROGRESS means this workload
-		 * doesn't meet any issue during dispatching so when
-		 * get the SCHEDULE_OUT set the status to be zero for
-		 * good. If the status is NOT -EINPROGRESS means there
-		 * is something wrong happened during dispatching and
-		 * the status should not be set to zero
-		 */
-		if (workload->status == -EINPROGRESS)
-			workload->status = 0;
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	default:
@@ -427,6 +418,18 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
 
+		/* If this request caused GPU hang, req->fence.error will
+		 * be set to -EIO. Use -EIO to set workload status so
+		 * that when this request caused GPU hang, didn't trigger
+		 * context switch interrupt to guest.
+		 */
+		if (likely(workload->status == -EINPROGRESS)) {
+			if (workload->req->fence.error == -EIO)
+				workload->status = -EIO;
+			else
+				workload->status = 0;
+		}
+
 		i915_gem_request_put(fetch_and_zero(&workload->req));
 
 		if (!workload->status && !vgpu->resetting) {

commit 4cc74389a551dc95fce72d58c11e55a93b6ecd19
Author: Weinan Li <weinan.z.li@intel.com>
Date:   Mon Jun 19 08:49:17 2017 +0800

    drm/i915/gvt: remove scheduler_mutex in per-engine workload_thread
    
    For the vGPU workloads, now GVT-g use per vGPU scheduler, the per-ring
    work_thread only pick workload belongs to the current vGPU. And with time
    slice based scheduler, it waits all the engines become idle before do vGPU
    switch. So we can run free dispatch in per-ring work_thread, different ring
    running in different 'vGPU' won't happen.
    
    For the workloads between vGPU and Host, this scheduler_mutex can't block
    host to dispatch workload into other ring engines.
    
    Here remove this mutex since it impacts the performance when applications
    use more than 1 ring engines in 1 vgpu.
    
    ring0 running in vGPU1, ring1 running in Host. Will happen.
    ring0 running in vGPU1, ring1 running in vGPU2. Won't happen.
    
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 488fdea348a9..5aeba13a5de4 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -464,8 +464,6 @@ struct workload_thread_param {
 	int ring_id;
 };
 
-static DEFINE_MUTEX(scheduler_mutex);
-
 static int workload_thread(void *priv)
 {
 	struct workload_thread_param *p = (struct workload_thread_param *)priv;
@@ -497,8 +495,6 @@ static int workload_thread(void *priv)
 		if (!workload)
 			break;
 
-		mutex_lock(&scheduler_mutex);
-
 		gvt_dbg_sched("ring id %d next workload %p vgpu %d\n",
 				workload->ring_id, workload,
 				workload->vgpu->id);
@@ -537,9 +533,6 @@ static int workload_thread(void *priv)
 					FORCEWAKE_ALL);
 
 		intel_runtime_pm_put(gvt->dev_priv);
-
-		mutex_unlock(&scheduler_mutex);
-
 	}
 	return 0;
 }

commit 5f09a9c8ab6b16eefbcf81635330d68481af1edc
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 20 12:05:46 2017 +0100

    drm/i915: Allow contexts to be unreferenced locklessly
    
    If we move the actual cleanup of the context to a worker, we can allow
    the final free to be called from any context and avoid undue latency in
    the caller.
    
    v2: Negotiate handling the delayed contexts free by flushing the
    workqueue before calling i915_gem_context_fini() and performing the final
    free of the kernel context directly
    v3: Flush deferred frees before new context allocations
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170620110547.15947-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 488fdea348a9..2b8a80c9c18b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -620,7 +620,7 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
 {
-	i915_gem_context_put_unlocked(vgpu->shadow_ctx);
+	i915_gem_context_put(vgpu->shadow_ctx);
 }
 
 int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)

commit f100daec9c9a5bbf1a715323cb6102e99933fdb3
Author: Ping Gao <ping.a.gao@intel.com>
Date:   Wed May 24 09:14:11 2017 +0800

    drm/i915/gvt: Trigger scheduling after context complete
    
    The time based scheduler poll context busy status at every
    micro-second during vGPU switch, it will make GPU idle for a while
    when the context is very small and completed before the next
    micro-second arrival. Trigger scheduling immediately after context
    complete will eliminate GPU idle and improve performance.
    
    Create two vGPU with same type, run Heaven simultaneously:
    Before this patch:
     +---------+----------+----------+
     |         |  vGPU1   |   vGPU2  |
     +---------+----------+----------+
     |  Heaven |  357     |    354   |
     +-------------------------------+
    
    After this patch:
     +---------+----------+----------+
     |         |  vGPU1   |   vGPU2  |
     +---------+----------+----------+
     |  Heaven |  397     |    398   |
     +-------------------------------+
    
    v2: Let need_reschedule protect by gvt-lock.
    
    Signed-off-by: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Weinan Li <weinan.z.li@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index aa7e06df88b6..488fdea348a9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -452,6 +452,10 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	atomic_dec(&vgpu->running_workload_num);
 	wake_up(&scheduler->workload_complete_wq);
+
+	if (gvt->scheduler.need_reschedule)
+		intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);
+
 	mutex_unlock(&gvt->lock);
 }
 

commit 0e86cc9ccc3bf557348befaddf5cb613cf3c4458
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu May 4 10:52:38 2017 +0800

    drm/i915/gvt: implement per-vm mmio switching optimization
    
    Commit ab9da627906a ("drm/i915: make context status notifier head be
    per engine") gives us a chance to inspect every single request. Then
    we can eliminate unnecessary mmio switching for same vGPU. We only
    need mmio switching for different VMs (including host).
    
    This patch introduced a new general API intel_gvt_switch_mmio() to
    replace the old intel_gvt_load/restore_render_mmio(). This function
    can be further optimized for vGPU to vGPU switching.
    
    To support individual ring switch, we track the owner who occupy
    each ring. When another VM or host request a ring we do the mmio
    context switching. Otherwise no need to switch the ring.
    
    This optimization is very useful if only one guest has plenty of
    workloads and the host is mostly idle. The best case is no mmio
    switching will happen.
    
    v2:
      o fix missing ring switch issue. (chuanxiao)
      o support individual ring switch.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Reviewed-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 6ae286cb5804..aa7e06df88b6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -138,21 +138,42 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
 				shadow_ctx_notifier_block[req->engine->id]);
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	struct intel_vgpu_workload *workload =
-		scheduler->current_workload[req->engine->id];
+	enum intel_engine_id ring_id = req->engine->id;
+	struct intel_vgpu_workload *workload;
+
+	if (!is_gvt_request(req)) {
+		spin_lock_bh(&scheduler->mmio_context_lock);
+		if (action == INTEL_CONTEXT_SCHEDULE_IN &&
+		    scheduler->engine_owner[ring_id]) {
+			/* Switch ring from vGPU to host. */
+			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
+					      NULL, ring_id);
+			scheduler->engine_owner[ring_id] = NULL;
+		}
+		spin_unlock_bh(&scheduler->mmio_context_lock);
 
-	if (!is_gvt_request(req) || unlikely(!workload))
+		return NOTIFY_OK;
+	}
+
+	workload = scheduler->current_workload[ring_id];
+	if (unlikely(!workload))
 		return NOTIFY_OK;
 
 	switch (action) {
 	case INTEL_CONTEXT_SCHEDULE_IN:
-		intel_gvt_load_render_mmio(workload->vgpu,
-					   workload->ring_id);
+		spin_lock_bh(&scheduler->mmio_context_lock);
+		if (workload->vgpu != scheduler->engine_owner[ring_id]) {
+			/* Switch ring from host to vGPU or vGPU to vGPU. */
+			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
+					      workload->vgpu, ring_id);
+			scheduler->engine_owner[ring_id] = workload->vgpu;
+		} else
+			gvt_dbg_sched("skip ring %d mmio switch for vgpu%d\n",
+				      ring_id, workload->vgpu->id);
+		spin_unlock_bh(&scheduler->mmio_context_lock);
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
-		intel_gvt_restore_render_mmio(workload->vgpu,
-					      workload->ring_id);
 		/* If the status is -EINPROGRESS means this workload
 		 * doesn't meet any issue during dispatching so when
 		 * get the SCHEDULE_OUT set the status to be zero for

commit 266a240bf0abf1e00e72e571f3724ec753a35f19
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu May 4 10:33:08 2017 +0100

    drm/i915: Use engine->context_pin() to report the intel_ring
    
    Since unifying ringbuffer/execlist submission to use
    engine->pin_context, we ensure that the intel_ring is available before
    we start constructing the request. We can therefore move the assignment
    of the request->ring to the central i915_gem_request_alloc() and not
    require it in every engine->request_alloc() callback. Another small step
    towards simplification (of the core, but at a cost of handling error
    pointers in less important callers of engine->pin_context).
    
    v2: Rearrange a few branches to reduce impact of PTR_ERR() on gcc's code
    generation.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Oscar Mateo <oscar.mateo@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Oscar Mateo <oscar.mateo@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170504093308.4137-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 1256fe21850b..6ae286cb5804 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -180,6 +180,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct drm_i915_gem_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_ring *ring;
 	int ret;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
@@ -198,8 +199,9 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
 	 * the guest context, gvt can unpin the shadow_ctx safely.
 	 */
-	ret = engine->context_pin(engine, shadow_ctx);
-	if (ret) {
+	ring = engine->context_pin(engine, shadow_ctx);
+	if (IS_ERR(ring)) {
+		ret = PTR_ERR(ring);
 		gvt_vgpu_err("fail to pin shadow context\n");
 		workload->status = ret;
 		mutex_unlock(&dev_priv->drm.struct_mutex);

commit ad15f74ac6c7ed1c7c252a760b017da042ec83fb
Merge: 8a2d6ae1f737 8b03d1ed2c43
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Wed May 3 21:41:35 2017 +0200

    Merge tag 'tags/drm-for-v4.12' into drm-intel-next-queued
    
    Backmerge the main drm-next pull to sync up.
    
    Chris also pointed out that
    
    commit ade0b0c965f59176daddbef9c4717354034f9bce
    Author: Chris Wilson <chris@chris-wilson.co.uk>
    Date:   Sat Apr 22 09:15:37 2017 +0100
    
        drm/i915: Confirm the request is still active before adding it to the await
    
    is double-applied in the git merge, so make sure we get this right.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>

commit 63ffbcdadcf2b5dde2cd6db6715fc94e77cd43b6
Author: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
Date:   Fri Apr 28 10:53:36 2017 +0300

    drm/i915: Sanitize engine context sizes
    
    Pre-calculate engine context size based on engine class and device
    generation and store it in the engine instance.
    
    v2:
    - Squash and get rid of hw_context_size (Chris)
    
    v3:
    - Move after MMIO init for probing on Gen7 and 8 (Chris)
    - Retained rounding (Tvrtko)
    v4:
    - Rebase for deferred legacy context allocation
    
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Paulo Zanoni <paulo.r.zanoni@intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Oscar Mateo <oscar.mateo@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: intel-gvt-dev@lists.freedesktop.org
    Acked-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a77db2332e68..ac538dcfff61 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -69,8 +69,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
 			workload->ctx_desc.lrca);
 
-	context_page_num = intel_lr_context_size(
-			gvt->dev_priv->engine[ring_id]);
+	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
 
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
@@ -333,8 +332,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d workload lrca %x\n", ring_id,
 			workload->ctx_desc.lrca);
 
-	context_page_num = intel_lr_context_size(
-			gvt->dev_priv->engine[ring_id]);
+	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
 
 	context_page_num = context_page_num >> PAGE_SHIFT;
 

commit f8a77153b0cb6eef34f16670df678dae49ce3776
Merge: ab6eb211b07a c821ee6d2bb4
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Wed Apr 26 12:19:55 2017 +0300

    Merge tag 'gvt-next-fixes-2017-04-20' of https://github.com/01org/gvt-linux into drm-intel-next-fixes
    
    gvt-next-fixes-2017-04-20
    
    - some code optimization from Changbin
    - debug message cleanup after QoS merge
    - misc fixes for display mmio init, reset vgpu warning, etc.
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit 954180aa69325feade02fa79f056fe1561f31fbb
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed Apr 12 14:22:50 2017 +0800

    drm/i915/gvt: remove some debug messages in scheduler timer handler
    
    As those debug messages might appear in every timer call for scheduler,
    it's too noisy, eat too much log and aren't meaningful. So remove them.
    
    Cc: Ping Gao <ping.a.gao@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 375038252761..0b685dd26cb3 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -274,11 +274,8 @@ static struct intel_vgpu_workload *pick_next_workload(
 		goto out;
 	}
 
-	if (list_empty(workload_q_head(scheduler->current_vgpu, ring_id))) {
-		gvt_dbg_sched("ring id %d stop - no available workload\n",
-				ring_id);
+	if (list_empty(workload_q_head(scheduler->current_vgpu, ring_id)))
 		goto out;
-	}
 
 	/*
 	 * still have current workload, maybe the workload disptacher

commit b769fefb68cd70385d68220ae341e5a10723fbc0
Merge: 1420f63b8207 39da7c509acf
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Apr 11 07:40:42 2017 +1000

    Backmerge tag 'v4.11-rc6' into drm-next
    
    Linux 4.11-rc6
    
    drm-misc needs 4.11-rc5, may as well fix conflicts with rc6.

commit e3476c0021c31b64070c40f02bfb7faab55591b4
Author: Xu Han <xu.han@intel.com>
Date:   Wed Mar 29 10:13:59 2017 +0800

    drm/i915/gvt: Add KBL dispatch logic in each function.
    
    Extend function dispatch logic to support KBL platform.
    
    Signed-off-by: Xu Han <xu.han@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index ad8876bd15b3..375038252761 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -448,7 +448,8 @@ static int workload_thread(void *priv)
 	struct intel_vgpu_workload *workload = NULL;
 	struct intel_vgpu *vgpu = NULL;
 	int ret;
-	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
+	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv)
+			|| IS_KABYLAKE(gvt->dev_priv);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
 
 	kfree(p);

commit e5c1ff14757afe21733ddee9cc4bbaeaeadbf803
Merge: 65d1086c4479 c02ed2e75ef4
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Mar 28 17:34:19 2017 +1000

    Backmerge tag 'v4.11-rc4' into drm-next
    
    Linux 4.11-rc4
    
    The i915 GVT team need the rc4 code to base some more code on.

commit 69653f626ef31107878c14883a603c1afca5edb9
Merge: c02ed2e75ef4 bc2d4b62db67
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Mon Mar 27 11:01:16 2017 +0300

    Merge tag 'gvt-fixes-2017-03-23' of https://github.com/01org/gvt-linux into drm-intel-fixes
    
    gvt-fixes-2017-03-23
    
    - KVM reference fix from Alex
    - shadow gtt entry partial update fix from Xiaoguang
    - gvt context notification check (Changbin)
    - other misc fixes.
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit 65d1086c44791112188f6aebbdc3a27cab3736d3
Merge: edd849e5448c 97da3854c526
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Mar 23 12:05:13 2017 +1000

    BackMerge tag 'v4.11-rc3' into drm-next
    
    Linux 4.11-rc3 as requested by Daniel

commit bc2d4b62db67f817b09c782219996630e9c2f5e2
Author: Changbin Du <changbin.du@intel.com>
Date:   Wed Mar 22 12:35:31 2017 +0800

    drm/i915/gvt: Use force single submit flag to distinguish gvt request from i915 request
    
    In my previous Commit ab9da627906a ("drm/i915: make context status
    notifier head be per engine") rely on scheduler->current_workload[x]
    to distinguish gvt spacial request from i915 request. But this is
    not always true since no synchronization between workload_thread and
    lrc irq handler.
    
        lrc irq handler               workload_thread
             ----                          ----
      pick i915 requests;
                                    intel_vgpu_submit_execlist();
                                    current_workload[x] = xxx;
      shadow_context_status_change();
    
    Then current_workload[x] is not null but current request is of i915 self.
    So instead we check ctx flag CONTEXT_FORCE_SINGLE_SUBMISSION. Only gvt
    request set this flag and always set.
    
    v2: Reverse the order of multi-condition 'if' statement.
    
    Fixes: ab9da6279 ("drm/i915: make context status notifier head be per engine")
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Reviewed-by: Yulei Zhang <yulei.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 39a83eb7aecc..d0c04155f7d5 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -127,6 +127,11 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
+static inline bool is_gvt_request(struct drm_i915_gem_request *req)
+{
+	return i915_gem_context_force_single_submission(req->ctx);
+}
+
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
@@ -139,7 +144,7 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	struct intel_vgpu_workload *workload =
 		scheduler->current_workload[req->engine->id];
 
-	if (unlikely(!workload))
+	if (!is_gvt_request(req) || unlikely(!workload))
 		return NOTIFY_OK;
 
 	switch (action) {

commit 590379aef2e3fb7d00a093ed556c0a2714f86916
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Mar 21 14:47:20 2017 +0000

    drm/i915: make context status notifier head be per engine
    
    GVTg has introduced the context status notifier to schedule the GVTg
    workload. At that time, the notifier is bound to GVTg context only,
    so GVTg is not aware of host workloads.
    
    Now we are going to improve GVTg's guest workload scheduler policy,
    and add Guc emulation support for new Gen graphics. Both these two
    features require acknowledgment for all contexts running on hardware.
    (But will not alter host workload.) So here try to make some change.
    
    The change is simple:
      1. Move the context status notifier head from i915_gem_context to
         intel_engine_cs. Which means there is a notifier head per engine
         instead of per context. Execlist driver still call notifier for
         each context sched-in/out events of current engine.
      2. At GVTg side, it binds a notifier_block for each physical engine
         at GVTg initialization period. Then GVTg can hear all context
         status events.
    
    In this patch, GVTg do nothing for host context event, but later
    will add a function there. But in any case, the notifier callback is
    a noop if this is no active vGPU.
    
    Since intel_gvt_init() is called at early initialization stage and
    require the status notifier head has been initiated, I initiate it in
    intel_engine_setup().
    
    v2: remove a redundant newline. (chris)
    
    Fixes: 3c7ba6359d70 ("drm/i915: Introduce execlist context status change notification")
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=100232
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170313024711.28591-1-changbin.du@intel.com
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    (cherry picked from commit 3fc03069bc6e6c316f19bb526e3c8ce784677477)
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170321144720.17020-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 39a83eb7aecc..c4353ed86d4b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -130,12 +130,10 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
-	struct intel_vgpu *vgpu = container_of(nb,
-			struct intel_vgpu, shadow_ctx_notifier_block);
-	struct drm_i915_gem_request *req =
-		(struct drm_i915_gem_request *)data;
-	struct intel_gvt_workload_scheduler *scheduler =
-		&vgpu->gvt->scheduler;
+	struct drm_i915_gem_request *req = (struct drm_i915_gem_request *)data;
+	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
+				shadow_ctx_notifier_block[req->engine->id]);
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload =
 		scheduler->current_workload[req->engine->id];
 
@@ -534,15 +532,16 @@ void intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu)
 void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	int i;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id i;
 
 	gvt_dbg_core("clean workload scheduler\n");
 
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
-		if (scheduler->thread[i]) {
-			kthread_stop(scheduler->thread[i]);
-			scheduler->thread[i] = NULL;
-		}
+	for_each_engine(engine, gvt->dev_priv, i) {
+		atomic_notifier_chain_unregister(
+					&engine->context_status_notifier,
+					&gvt->shadow_ctx_notifier_block[i]);
+		kthread_stop(scheduler->thread[i]);
 	}
 }
 
@@ -550,18 +549,15 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct workload_thread_param *param = NULL;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id i;
 	int ret;
-	int i;
 
 	gvt_dbg_core("init workload scheduler\n");
 
 	init_waitqueue_head(&scheduler->workload_complete_wq);
 
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
-		/* check ring mask at init time */
-		if (!HAS_ENGINE(gvt->dev_priv, i))
-			continue;
-
+	for_each_engine(engine, gvt->dev_priv, i) {
 		init_waitqueue_head(&scheduler->waitq[i]);
 
 		param = kzalloc(sizeof(*param), GFP_KERNEL);
@@ -580,6 +576,11 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 			ret = PTR_ERR(scheduler->thread[i]);
 			goto err;
 		}
+
+		gvt->shadow_ctx_notifier_block[i].notifier_call =
+					shadow_context_status_change;
+		atomic_notifier_chain_register(&engine->context_status_notifier,
+					&gvt->shadow_ctx_notifier_block[i]);
 	}
 	return 0;
 err:
@@ -591,9 +592,6 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
 {
-	atomic_notifier_chain_unregister(&vgpu->shadow_ctx->status_notifier,
-			&vgpu->shadow_ctx_notifier_block);
-
 	i915_gem_context_put_unlocked(vgpu->shadow_ctx);
 }
 
@@ -608,10 +606,5 @@ int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
 
 	vgpu->shadow_ctx->engine[RCS].initialised = true;
 
-	vgpu->shadow_ctx_notifier_block.notifier_call =
-		shadow_context_status_change;
-
-	atomic_notifier_chain_register(&vgpu->shadow_ctx->status_notifier,
-				       &vgpu->shadow_ctx_notifier_block);
 	return 0;
 }

commit e642c85b03de10eb1b411884ba27550651d0100b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Mar 17 11:47:09 2017 +0000

    drm/i915: Remove superfluous i915_add_request_no_flush() helper
    
    The only time we need to emit a flush inside request emission is after
    an execbuffer, for which we can use the full __i915_add_request(). All
    other instances want the simpler i915_add_request() without flushing, so
    remove the useless helper.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170317114709.8388-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b14c5c2109d4..31d2240fdb1f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -212,7 +212,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 		workload->status = ret;
 
 	if (!IS_ERR_OR_NULL(rq))
-		i915_add_request_no_flush(rq);
+		i915_add_request(rq);
 	mutex_unlock(&dev_priv->drm.struct_mutex);
 	return ret;
 }

commit 3cd23b828b37bd5ccf4b40132f12dbf20d7afdb6
Author: Chuanxiao Dong <chuanxiao.dong@intel.com>
Date:   Thu Mar 16 09:47:58 2017 +0800

    drm/i915/gvt: GVT pin/unpin shadow context
    
    When handling guest request, GVT needs to populate/update shadow_ctx
    with guest context. This behavior needs to make sure the shadow_ctx
    is pinned. The current implementation is relying on i195 allocate request
    to pin but this way cannot guarantee the i915 not to unpin the shadow_ctx
    when GVT update the guest context from shadow_ctx. So GVT should pin/unpin
    the shadow_ctx by itself.
    
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 907e6bc794f6..39a83eb7aecc 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -175,6 +175,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
 	struct drm_i915_gem_request *rq;
 	struct intel_vgpu *vgpu = workload->vgpu;
 	int ret;
@@ -188,6 +189,21 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	mutex_lock(&dev_priv->drm.struct_mutex);
 
+	/* pin shadow context by gvt even the shadow context will be pinned
+	 * when i915 alloc request. That is because gvt will update the guest
+	 * context from shadow context when workload is completed, and at that
+	 * moment, i915 may already unpined the shadow context to make the
+	 * shadow_ctx pages invalid. So gvt need to pin itself. After update
+	 * the guest context, gvt can unpin the shadow_ctx safely.
+	 */
+	ret = engine->context_pin(engine, shadow_ctx);
+	if (ret) {
+		gvt_vgpu_err("fail to pin shadow context\n");
+		workload->status = ret;
+		mutex_unlock(&dev_priv->drm.struct_mutex);
+		return ret;
+	}
+
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_vgpu_err("fail to allocate gem request\n");
@@ -231,6 +247,9 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	if (!IS_ERR_OR_NULL(rq))
 		i915_add_request_no_flush(rq);
+	else
+		engine->context_unpin(engine, shadow_ctx);
+
 	mutex_unlock(&dev_priv->drm.struct_mutex);
 	return ret;
 }
@@ -380,6 +399,10 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	 * For the workload w/o request, directly complete the workload.
 	 */
 	if (workload->req) {
+		struct drm_i915_private *dev_priv =
+			workload->vgpu->gvt->dev_priv;
+		struct intel_engine_cs *engine =
+			dev_priv->engine[workload->ring_id];
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
 
@@ -392,6 +415,10 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 					 INTEL_GVT_EVENT_MAX)
 				intel_vgpu_trigger_virtual_event(vgpu, event);
 		}
+		mutex_lock(&dev_priv->drm.struct_mutex);
+		/* unpin shadow ctx as the shadow_ctx update is done */
+		engine->context_unpin(engine, workload->vgpu->shadow_ctx);
+		mutex_unlock(&dev_priv->drm.struct_mutex);
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",

commit 17f1b1a6d4e5f41df161eb2a90af22c003a243fc
Author: Tina Zhang <tina.zhang@intel.com>
Date:   Wed Mar 15 23:16:01 2017 -0400

    drm/i915/gvt: scan shadow indirect context image when valid
    
    The shadow indirect context image should be only scanned when valid.
    So far, Only RCS ring has the shadow indirect context image. This patch
    limits the scan logic only for RCS ring.
    
    v2. refine description of the subject
    v3. fix alignment. (Zhenyu)
    
    Signed-off-by: Tina Zhang <tina.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index dd8f8cc69a76..907e6bc794f6 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -203,9 +203,12 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	if (ret)
 		goto out;
 
-	ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
-	if (ret)
-		goto out;
+	if ((workload->ring_id == RCS) &&
+	    (workload->wa_ctx.indirect_ctx.size != 0)) {
+		ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
+		if (ret)
+			goto out;
+	}
 
 	ret = populate_shadow_context(workload);
 	if (ret)

commit 3dce2aca02929f180ab66171b333fa48fe485a03
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Mar 8 22:08:08 2017 +0000

    drm/i915/gvt: Remove bogus retry around i915_wait_request
    
    commit 8f1117abb408 ("drm/i915/gvt: handle workload lifecycle properly")
    includes some nonsense to retry a indefinite wait - i915_wait_request()
    does not return until the request is completed when used from an
    uninterruptible context.
    
    Fixes: 8f1117abb408 ("drm/i915/gvt: handle workload lifecycle properly"
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d29e4352d529..dd8f8cc69a76 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -468,19 +468,7 @@ static int workload_thread(void *priv)
 
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
-retry:
-		i915_wait_request(workload->req,
-					 0, MAX_SCHEDULE_TIMEOUT);
-		/* I915 has replay mechanism and a request will be replayed
-		 * if there is i915 reset. So the seqno will be updated anyway.
-		 * If the seqno is not updated yet after waiting, which means
-		 * the replay may still be in progress and we can wait again.
-		 */
-		if (!i915_gem_request_completed(workload->req)) {
-			gvt_dbg_sched("workload %p not completed, wait again\n",
-					workload);
-			goto retry;
-		}
+		i915_wait_request(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",

commit 695fbc08d80f93ecca18a1abd8f52c2ab77fdc8d
Author: Tina Zhang <tina.zhang@intel.com>
Date:   Fri Mar 10 04:26:53 2017 -0500

    drm/i915/gvt: replace the gvt_err with gvt_vgpu_err
    
    gvt_err should be used only for the very few critical error message
    during host i915 drvier initialization. This patch
    1. removes the redundant gvt_err;
    2. creates a new gvt_vgpu_err to show errors caused by vgpu;
    3. replaces the most gvt_err with gvt_vgpu_err;
    4. leaves very few gvt_err for dumping gvt error during host gvt
       initialization.
    
    v2. change name to gvt_vgpu_err and add vgpu id to the message. (Kevin)
        add gpu id to gvt_vgpu_err. (Zhi)
    v3. remove gpu id from gvt_vgpu_err caller. (Zhi)
    v4. add vgpu check to the gvt_vgpu_err macro. (Zhiyuan)
    v5. add comments for v3 and v4.
    v6. split the big patch into two, with this patch only for checking
        gvt_vgpu_err. (Zhenyu)
    v7. rebase to staging branch
    v8. rebase to fix branch
    
    Signed-off-by: Tina Zhang <tina.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d3a56c949025..d29e4352d529 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -84,7 +84,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 				(u32)((workload->ctx_desc.lrca + i) <<
 				GTT_PAGE_SHIFT));
 		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
-			gvt_err("Invalid guest context descriptor\n");
+			gvt_vgpu_err("Invalid guest context descriptor\n");
 			return -EINVAL;
 		}
 
@@ -176,6 +176,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
 	struct drm_i915_gem_request *rq;
+	struct intel_vgpu *vgpu = workload->vgpu;
 	int ret;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
@@ -189,7 +190,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
-		gvt_err("fail to allocate gem request\n");
+		gvt_vgpu_err("fail to allocate gem request\n");
 		ret = PTR_ERR(rq);
 		goto out;
 	}
@@ -322,7 +323,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 				(u32)((workload->ctx_desc.lrca + i) <<
 					GTT_PAGE_SHIFT));
 		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
-			gvt_err("invalid guest context descriptor\n");
+			gvt_vgpu_err("invalid guest context descriptor\n");
 			return;
 		}
 
@@ -417,6 +418,7 @@ static int workload_thread(void *priv)
 	int ring_id = p->ring_id;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
+	struct intel_vgpu *vgpu = NULL;
 	int ret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
@@ -459,7 +461,8 @@ static int workload_thread(void *priv)
 		mutex_unlock(&gvt->lock);
 
 		if (ret) {
-			gvt_err("fail to dispatch workload, skip\n");
+			vgpu = workload->vgpu;
+			gvt_vgpu_err("fail to dispatch workload, skip\n");
 			goto complete;
 		}
 

commit 3fc03069bc6e6c316f19bb526e3c8ce784677477
Author: Changbin Du <changbin.du@intel.com>
Date:   Mon Mar 13 10:47:11 2017 +0800

    drm/i915: make context status notifier head be per engine
    
    GVTg has introduced the context status notifier to schedule the GVTg
    workload. At that time, the notifier is bound to GVTg context only,
    so GVTg is not aware of host workloads.
    
    Now we are going to improve GVTg's guest workload scheduler policy,
    and add Guc emulation support for new Gen graphics. Both these two
    features require acknowledgment for all contexts running on hardware.
    (But will not alter host workload.) So here try to make some change.
    
    The change is simple:
      1. Move the context status notifier head from i915_gem_context to
         intel_engine_cs. Which means there is a notifier head per engine
         instead of per context. Execlist driver still call notifier for
         each context sched-in/out events of current engine.
      2. At GVTg side, it binds a notifier_block for each physical engine
         at GVTg initialization period. Then GVTg can hear all context
         status events.
    
    In this patch, GVTg do nothing for host context event, but later
    will add a function there. But in any case, the notifier callback is
    a noop if this is no active vGPU.
    
    Since intel_gvt_init() is called at early initialization stage and
    require the status notifier head has been initiated, I initiate it in
    intel_engine_setup().
    
    v2: remove a redundant newline. (chris)
    
    Fixes: 3c7ba6359d70 ("drm/i915: Introduce execlist context status change notification")
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=100232
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170313024711.28591-1-changbin.du@intel.com
    Acked-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d6b6d0efdd1a..b14c5c2109d4 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -130,12 +130,10 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
-	struct intel_vgpu *vgpu = container_of(nb,
-			struct intel_vgpu, shadow_ctx_notifier_block);
-	struct drm_i915_gem_request *req =
-		(struct drm_i915_gem_request *)data;
-	struct intel_gvt_workload_scheduler *scheduler =
-		&vgpu->gvt->scheduler;
+	struct drm_i915_gem_request *req = (struct drm_i915_gem_request *)data;
+	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
+				shadow_ctx_notifier_block[req->engine->id]);
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload =
 		scheduler->current_workload[req->engine->id];
 
@@ -493,15 +491,16 @@ void intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu)
 void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	int i;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id i;
 
 	gvt_dbg_core("clean workload scheduler\n");
 
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
-		if (scheduler->thread[i]) {
-			kthread_stop(scheduler->thread[i]);
-			scheduler->thread[i] = NULL;
-		}
+	for_each_engine(engine, gvt->dev_priv, i) {
+		atomic_notifier_chain_unregister(
+					&engine->context_status_notifier,
+					&gvt->shadow_ctx_notifier_block[i]);
+		kthread_stop(scheduler->thread[i]);
 	}
 }
 
@@ -509,18 +508,15 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct workload_thread_param *param = NULL;
+	struct intel_engine_cs *engine;
+	enum intel_engine_id i;
 	int ret;
-	int i;
 
 	gvt_dbg_core("init workload scheduler\n");
 
 	init_waitqueue_head(&scheduler->workload_complete_wq);
 
-	for (i = 0; i < I915_NUM_ENGINES; i++) {
-		/* check ring mask at init time */
-		if (!HAS_ENGINE(gvt->dev_priv, i))
-			continue;
-
+	for_each_engine(engine, gvt->dev_priv, i) {
 		init_waitqueue_head(&scheduler->waitq[i]);
 
 		param = kzalloc(sizeof(*param), GFP_KERNEL);
@@ -539,6 +535,11 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 			ret = PTR_ERR(scheduler->thread[i]);
 			goto err;
 		}
+
+		gvt->shadow_ctx_notifier_block[i].notifier_call =
+					shadow_context_status_change;
+		atomic_notifier_chain_register(&engine->context_status_notifier,
+					&gvt->shadow_ctx_notifier_block[i]);
 	}
 	return 0;
 err:
@@ -550,9 +551,6 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
 {
-	atomic_notifier_chain_unregister(&vgpu->shadow_ctx->status_notifier,
-			&vgpu->shadow_ctx_notifier_block);
-
 	i915_gem_context_put_unlocked(vgpu->shadow_ctx);
 }
 
@@ -567,10 +565,5 @@ int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
 
 	vgpu->shadow_ctx->engine[RCS].initialised = true;
 
-	vgpu->shadow_ctx_notifier_block.notifier_call =
-		shadow_context_status_change;
-
-	atomic_notifier_chain_register(&vgpu->shadow_ctx->status_notifier,
-				       &vgpu->shadow_ctx_notifier_block);
 	return 0;
 }

commit 8f1117abb408808af9cc4c948925c726bec4755a
Author: Chuanxiao Dong <chuanxiao.dong@intel.com>
Date:   Mon Mar 6 13:05:24 2017 +0800

    drm/i915/gvt: handle workload lifecycle properly
    
    Currently i915 has a request replay mechanism which can make sure
    the request can be replayed after a GPU reset. With this mechanism,
    gvt should wait until the GVT request seqno passed before complete
    the current workload. So that there should be a context switch interrupt
    come before gvt free the workload. In this way, workload lifecylce
    matches with the i915 request lifecycle. The workload can only be freed
    after the request is completed.
    
    v2: use gvt_dbg_sched instead of gvt_err to print when wait again
    
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index e355a82ccabd..d3a56c949025 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -151,6 +151,15 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	case INTEL_CONTEXT_SCHEDULE_OUT:
 		intel_gvt_restore_render_mmio(workload->vgpu,
 					      workload->ring_id);
+		/* If the status is -EINPROGRESS means this workload
+		 * doesn't meet any issue during dispatching so when
+		 * get the SCHEDULE_OUT set the status to be zero for
+		 * good. If the status is NOT -EINPROGRESS means there
+		 * is something wrong happened during dispatching and
+		 * the status should not be set to zero
+		 */
+		if (workload->status == -EINPROGRESS)
+			workload->status = 0;
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	default:
@@ -362,15 +371,23 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 	workload = scheduler->current_workload[ring_id];
 	vgpu = workload->vgpu;
 
-	if (!workload->status && !vgpu->resetting) {
+	/* For the workload w/ request, needs to wait for the context
+	 * switch to make sure request is completed.
+	 * For the workload w/o request, directly complete the workload.
+	 */
+	if (workload->req) {
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
 
-		update_guest_context(workload);
+		i915_gem_request_put(fetch_and_zero(&workload->req));
 
-		for_each_set_bit(event, workload->pending_events,
-				 INTEL_GVT_EVENT_MAX)
-			intel_vgpu_trigger_virtual_event(vgpu, event);
+		if (!workload->status && !vgpu->resetting) {
+			update_guest_context(workload);
+
+			for_each_set_bit(event, workload->pending_events,
+					 INTEL_GVT_EVENT_MAX)
+				intel_vgpu_trigger_virtual_event(vgpu, event);
+		}
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",
@@ -400,7 +417,6 @@ static int workload_thread(void *priv)
 	int ring_id = p->ring_id;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
-	long lret;
 	int ret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
@@ -449,23 +465,24 @@ static int workload_thread(void *priv)
 
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
-
-		lret = i915_wait_request(workload->req,
+retry:
+		i915_wait_request(workload->req,
 					 0, MAX_SCHEDULE_TIMEOUT);
-		if (lret < 0) {
-			workload->status = lret;
-			gvt_err("fail to wait workload, skip\n");
-		} else {
-			workload->status = 0;
+		/* I915 has replay mechanism and a request will be replayed
+		 * if there is i915 reset. So the seqno will be updated anyway.
+		 * If the seqno is not updated yet after waiting, which means
+		 * the replay may still be in progress and we can wait again.
+		 */
+		if (!i915_gem_request_completed(workload->req)) {
+			gvt_dbg_sched("workload %p not completed, wait again\n",
+					workload);
+			goto retry;
 		}
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",
 				workload, workload->status);
 
-		if (workload->req)
-			i915_gem_request_put(fetch_and_zero(&workload->req));
-
 		complete_current_workload(gvt, ring_id);
 
 		if (need_force_wake)

commit 9272f73f79bd780502134f227fa52fd280ecda17
Author: Chuanxiao Dong <chuanxiao.dong@intel.com>
Date:   Fri Feb 17 19:29:52 2017 +0800

    drm/i915/gvt: add a NULL pointer check to avoid kernel panic
    
    Due to the request replay, context switch interrupt may come after
    gvt free the workload thus can cause a kernel NULL pointer kernel
    panic. This patch will add a simple check to avoid this for a short
    term.
    
    From long term, gvt workload lifecycle doesn't match with i915 request
    and need to find a proper way to manage this.
    
    v4: simplify the NULL pointer check.
    v5: add unlikely to optimize.
    
    Signed-off-by: Chuanxiao Dong <chuanxiao.dong@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index d6b6d0efdd1a..e355a82ccabd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -139,6 +139,9 @@ static int shadow_context_status_change(struct notifier_block *nb,
 	struct intel_vgpu_workload *workload =
 		scheduler->current_workload[req->engine->id];
 
+	if (unlikely(!workload))
+		return NOTIFY_OK;
+
 	switch (action) {
 	case INTEL_CONTEXT_SCHEDULE_IN:
 		intel_gvt_load_render_mmio(workload->vgpu,

commit 03806edc3582392727a6dc617662ac57e6b24b01
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Mon Feb 13 17:07:19 2017 +0800

    drm/i915/gvt: Fix shadow context descriptor
    
    We need to be careful to only update addr mode for gvt shadow context
    descriptor but keep other valid config. This fixes GPU hang caused by
    invalid descriptor submitted for gvt workload.
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 3fbcd9db65b9..d6b6d0efdd1a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -169,7 +169,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
 		ring_id, workload);
 
-	shadow_ctx->desc_template = workload->ctx_desc.addressing_mode <<
+	shadow_ctx->desc_template &= ~(0x3 << GEN8_CTX_ADDRESSING_MODE_SHIFT);
+	shadow_ctx->desc_template |= workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
 	mutex_lock(&dev_priv->drm.struct_mutex);

commit 3ce3274bff89c6ea5bed3f8f47abdbf672e216b2
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu Feb 9 10:13:16 2017 +0800

    drm/i915/gvt: remove a redundant end of line in debug log
    
    Remove a redundant end of line in below log.
      'will complete workload %p\n, status: %d\n'
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7ea68a75dc46..3fbcd9db65b9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -456,7 +456,7 @@ static int workload_thread(void *priv)
 		}
 
 complete:
-		gvt_dbg_sched("will complete workload %p\n, status: %d\n",
+		gvt_dbg_sched("will complete workload %p, status: %d\n",
 				workload, workload->status);
 
 		if (workload->req)

commit b0df0b251b25b0bf89ef3e518330fcac300add86
Merge: f0493e653f96 ff9f8a7cf935
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Jan 27 11:00:42 2017 +1000

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux into drm-next
    
    Backmerge Linus master to get the connector locking revert.
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux: (645 commits)
      sysctl: fix proc_doulongvec_ms_jiffies_minmax()
      Revert "drm/probe-helpers: Drop locking from poll_enable"
      MAINTAINERS: add Dan Streetman to zbud maintainers
      MAINTAINERS: add Dan Streetman to zswap maintainers
      mm: do not export ioremap_page_range symbol for external module
      mn10300: fix build error of missing fpu_save()
      romfs: use different way to generate fsid for BLOCK or MTD
      frv: add missing atomic64 operations
      mm, page_alloc: fix premature OOM when racing with cpuset mems update
      mm, page_alloc: move cpuset seqcount checking to slowpath
      mm, page_alloc: fix fast-path race with cpuset update or removal
      mm, page_alloc: fix check for NULL preferred_zone
      kernel/panic.c: add missing \n
      fbdev: color map copying bounds checking
      frv: add atomic64_add_unless()
      mm/mempolicy.c: do not put mempolicy before using its nodemask
      radix-tree: fix private list warnings
      Documentation/filesystems/proc.txt: add VmPin
      mm, memcg: do not retry precharge charges
      proc: add a schedule point in proc_pid_readdir()
      ...

commit 440a9b9fae37dfd7e4c7d76db34fada57f9afd92
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu Jan 5 16:49:03 2017 +0800

    drm/i915/gvt: dec vgpu->running_workload_num after the workload is really done
    
    The vgpu->running_workload_num is used to determine whether a vgpu has
    any workload running or not. So we should make sure the workload is
    really done before we dec running_workload_num. Function
    complete_current_workload is not the right place to do it, since this
    function is still processing the workload. This patch move the dec op
    afterward.
    
    v2: move dec op before wake_up(&scheduler->workload_complete_wq) (Min He)
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Reviewed-by: Min He <min.he@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index c694dd039f3b..e91885dffeff 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -350,13 +350,15 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload;
+	struct intel_vgpu *vgpu;
 	int event;
 
 	mutex_lock(&gvt->lock);
 
 	workload = scheduler->current_workload[ring_id];
+	vgpu = workload->vgpu;
 
-	if (!workload->status && !workload->vgpu->resetting) {
+	if (!workload->status && !vgpu->resetting) {
 		wait_event(workload->shadow_ctx_status_wq,
 			   !atomic_read(&workload->shadow_ctx_active));
 
@@ -364,8 +366,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 		for_each_set_bit(event, workload->pending_events,
 				 INTEL_GVT_EVENT_MAX)
-			intel_vgpu_trigger_virtual_event(workload->vgpu,
-					event);
+			intel_vgpu_trigger_virtual_event(vgpu, event);
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",
@@ -373,11 +374,10 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 
 	scheduler->current_workload[ring_id] = NULL;
 
-	atomic_dec(&workload->vgpu->running_workload_num);
-
 	list_del_init(&workload->list);
 	workload->complete(workload);
 
+	atomic_dec(&vgpu->running_workload_num);
 	wake_up(&scheduler->workload_complete_wq);
 	mutex_unlock(&gvt->lock);
 }

commit 2e51ef32b0d66fcd5fe45c437cf7c6aef8350746
Author: Changbin Du <changbin.du@intel.com>
Date:   Thu Jan 5 13:28:05 2017 +0800

    drm/i915/gvt: fix use after free for workload
    
    In the function workload_thread(), we invoke complete_current_workload()
    to cleanup the just processed workload (workload will be freed there).
    So we cannot access workload->req after that. This patch move
    complete_current_workload() afterward.
    
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 4db242250235..c694dd039f3b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -459,11 +459,11 @@ static int workload_thread(void *priv)
 		gvt_dbg_sched("will complete workload %p\n, status: %d\n",
 				workload, workload->status);
 
-		complete_current_workload(gvt, ring_id);
-
 		if (workload->req)
 			i915_gem_request_put(fetch_and_zero(&workload->req));
 
+		complete_current_workload(gvt, ring_id);
+
 		if (need_force_wake)
 			intel_uncore_forcewake_put(gvt->dev_priv,
 					FORCEWAKE_ALL);

commit a402eae64d0ad12b1c4a411f250d6c161e67f623
Merge: 7800fb69ddf3 0c744ea4f77d
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Wed Jan 4 11:34:01 2017 +0100

    Merge tag 'v4.10-rc2' into drm-intel-next-queued
    
    Backmerge Linux 4.10-rc2 to resync with our -fixes cherry-picks. I've
    done the backmerge directly because Dave is on vacation.
    
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>

commit 70ffe9956c5cf328380053c803b783df711e8024
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Dec 18 15:37:22 2016 +0000

    drm/i915: Mark the shadow gvt context as closed
    
    As the shadow gvt is not user accessible and does not have an associated
    vm, we can mark it as closed during its construction. This saves leaking
    the internal knowledge of i915_gem_context into gvt/.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161218153724.8439-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f898df38dd9a..c61c8a738202 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -547,18 +547,10 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 
 void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-
 	atomic_notifier_chain_unregister(&vgpu->shadow_ctx->status_notifier,
 			&vgpu->shadow_ctx_notifier_block);
 
-	mutex_lock(&dev_priv->drm.struct_mutex);
-
-	/* a little hacky to mark as ctx closed */
-	vgpu->shadow_ctx->closed = true;
-	i915_gem_context_put(vgpu->shadow_ctx);
-
-	mutex_unlock(&dev_priv->drm.struct_mutex);
+	i915_gem_context_put_unlocked(vgpu->shadow_ctx);
 }
 
 int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)

commit 53d6f812c0dbf1c9cad89b1c2118e61c13ca9677
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Nov 24 15:55:49 2016 +0800

    drm/i915/gvt: fix lock not released bug for dispatch_workload() err path
    
    Need to be careful to release struct_mutext when request alloc
    failed and take consistent handling for return status as with
    normal go out path. Ensure to check correct workload request in
    complete path too.
    
    v2: Add Fixes note
    
    Fixes: 90d27a1b180e ("drm/i915/gvt: fix deadlock in workload_thread")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Pei Zhang <pei.zhang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f898df38dd9a..4db242250235 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -177,8 +177,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_err("fail to allocate gem request\n");
-		workload->status = PTR_ERR(rq);
-		return workload->status;
+		ret = PTR_ERR(rq);
+		goto out;
 	}
 
 	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
@@ -212,7 +212,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	if (ret)
 		workload->status = ret;
 
-	i915_add_request_no_flush(rq);
+	if (!IS_ERR_OR_NULL(rq))
+		i915_add_request_no_flush(rq);
 	mutex_unlock(&dev_priv->drm.struct_mutex);
 	return ret;
 }
@@ -460,7 +461,8 @@ static int workload_thread(void *priv)
 
 		complete_current_workload(gvt, ring_id);
 
-		i915_gem_request_put(fetch_and_zero(&workload->req));
+		if (workload->req)
+			i915_gem_request_put(fetch_and_zero(&workload->req));
 
 		if (need_force_wake)
 			intel_uncore_forcewake_put(gvt->dev_priv,

commit 90d27a1b180e51ef071350a302648b41fe884ff2
Author: Pei Zhang <pei.zhang@intel.com>
Date:   Mon Nov 14 18:02:57 2016 +0800

    drm/i915/gvt: fix deadlock in workload_thread
    
    It's a classical abba type deadlock when using 2 mutex objects, which
    are gvt.lock(a) and drm.struct_mutex(b). Deadlock happens in threads:
    1. intel_gvt_create/destroy_vgpu: P(a)->P(b)
    2. workload_thread: P(b)->P(a)
    
    Fix solution is align the lock acquire sequence in both threads. This
    patch choose to adjust the sequence in workload_thread function.
    
    This fixed lockup symptom for guest-reboot stress test.
    
    v2: adjust sequence in workload_thread based on zhenyu's suggestion.
        adjust sequence in create/destroy_vgpu function.
    v3: fix to still require struct_mutex for dispatch_workload()
    
    Signed-off-by: Pei Zhang <pei.zhang@intel.com>
    [zhenyuw: fix unused variables warnings.]
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 7d87c43661c5..f898df38dd9a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -160,8 +160,6 @@ static int shadow_context_status_change(struct notifier_block *nb,
 
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
-	struct intel_vgpu *vgpu = workload->vgpu;
-	struct intel_gvt *gvt = vgpu->gvt;
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
@@ -174,6 +172,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	shadow_ctx->desc_template = workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
+	mutex_lock(&dev_priv->drm.struct_mutex);
+
 	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
 	if (IS_ERR(rq)) {
 		gvt_err("fail to allocate gem request\n");
@@ -185,40 +185,35 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	workload->req = i915_gem_request_get(rq);
 
-	mutex_lock(&gvt->lock);
-
 	ret = intel_gvt_scan_and_shadow_workload(workload);
 	if (ret)
-		goto err;
+		goto out;
 
 	ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
 	if (ret)
-		goto err;
+		goto out;
 
 	ret = populate_shadow_context(workload);
 	if (ret)
-		goto err;
+		goto out;
 
 	if (workload->prepare) {
 		ret = workload->prepare(workload);
 		if (ret)
-			goto err;
+			goto out;
 	}
 
-	mutex_unlock(&gvt->lock);
-
 	gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 			ring_id, workload->req);
 
-	i915_add_request_no_flush(rq);
+	ret = 0;
 	workload->dispatched = true;
-	return 0;
-err:
-	workload->status = ret;
-
-	mutex_unlock(&gvt->lock);
+out:
+	if (ret)
+		workload->status = ret;
 
 	i915_add_request_no_flush(rq);
+	mutex_unlock(&dev_priv->drm.struct_mutex);
 	return ret;
 }
 
@@ -438,9 +433,9 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_get(gvt->dev_priv,
 					FORCEWAKE_ALL);
 
-		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
+		mutex_lock(&gvt->lock);
 		ret = dispatch_workload(workload);
-		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
+		mutex_unlock(&gvt->lock);
 
 		if (ret) {
 			gvt_err("fail to dispatch workload, skip\n");
@@ -463,9 +458,7 @@ static int workload_thread(void *priv)
 		gvt_dbg_sched("will complete workload %p\n, status: %d\n",
 				workload, workload->status);
 
-		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
 		complete_current_workload(gvt, ring_id);
-		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
 
 		i915_gem_request_put(fetch_and_zero(&workload->req));
 

commit c754936fe66c45d2075970dc1e6ebdfeec4df6f3
Author: Xiaoguang Chen <xiaoguang.chen@intel.com>
Date:   Thu Nov 3 18:38:30 2016 +0800

    drm/i915/gvt: use kmap instead of kmap_atomic around guest memory access
    
    kmap_atomic doesn't allow sleep until unmapped. However,
    it's necessary to allow sleep during reading/writing guest
    memory, so use kmap instead.
    
    Signed-off-by: Bing Niu <bing.niu@intel.com>
    Signed-off-by: Xiaoguang Chen <xiaoguang.chen@intel.com>
    Signed-off-by: Jike Song <jike.song@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 843a5de4300d..7d87c43661c5 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -89,15 +89,15 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 		}
 
 		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
-		dst = kmap_atomic(page);
+		dst = kmap(page);
 		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
 				GTT_PAGE_SIZE);
-		kunmap_atomic(dst);
+		kunmap(page);
 		i++;
 	}
 
 	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap_atomic(page);
+	shadow_ring_context = kmap(page);
 
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_read_gpa(vgpu, workload->ring_context_gpa \
@@ -123,7 +123,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			sizeof(*shadow_ring_context),
 			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
-	kunmap_atomic(shadow_ring_context);
+	kunmap(page);
 	return 0;
 }
 
@@ -318,10 +318,10 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 		}
 
 		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
-		src = kmap_atomic(page);
+		src = kmap(page);
 		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
 				GTT_PAGE_SIZE);
-		kunmap_atomic(src);
+		kunmap(page);
 		i++;
 	}
 
@@ -329,7 +329,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 		RING_CTX_OFF(ring_header.val), &workload->rb_tail, 4);
 
 	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
-	shadow_ring_context = kmap_atomic(page);
+	shadow_ring_context = kmap(page);
 
 #define COPY_REG(name) \
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa + \
@@ -347,7 +347,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			sizeof(*shadow_ring_context),
 			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
 
-	kunmap_atomic(shadow_ring_context);
+	kunmap(page);
 }
 
 static void complete_current_workload(struct intel_gvt *gvt, int ring_id)

commit 9b172345caa4e21423c649c4b98b32f53b6c89cb
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Wed Nov 2 15:00:15 2016 +0800

    drm/i915/gvt: Fix workload status after wait
    
    From commit e95433c73a11759203af1cae5958f998c9673370, workload status setting
    was changed to only capture on error path, but we need to set it properly in
    normal path too, otherwise we'll fail to complete workload which could lead
    guest VM vGPU reset.
    
    v2: uses braces and add Fixes tag.
    
    Fixes: e95433c73a11 ("drm/i915: Rearrange i915_wait_request() accounting with callers")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 18acb45dd14d..843a5de4300d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -455,6 +455,8 @@ static int workload_thread(void *priv)
 		if (lret < 0) {
 			workload->status = lret;
 			gvt_err("fail to wait workload, skip\n");
+		} else {
+			workload->status = 0;
 		}
 
 complete:

commit e95433c73a11759203af1cae5958f998c9673370
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 28 13:58:27 2016 +0100

    drm/i915: Rearrange i915_wait_request() accounting with callers
    
    Our low-level wait routine has evolved from our generic wait interface
    that handled unlocked, RPS boosting, waits with time tracking. If we
    push our GEM fence tracking to use reservation_objects (required for
    handling multiple timelines), we lose the ability to pass the required
    information down to i915_wait_request(). However, if we push the extra
    functionality from i915_wait_request() to the individual callsites
    (i915_gem_object_wait_rendering and i915_gem_wait_ioctl) that make use
    of those extras, we can both simplify our low level wait and prepare for
    extending the GEM interface for use of reservation_objects.
    
    v2: Rewrite i915_wait_request() kerneldocs
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.william.auld@gmail.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161028125858.23563-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index f7e320b9b17a..18acb45dd14d 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -400,6 +400,7 @@ static int workload_thread(void *priv)
 	int ring_id = p->ring_id;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload = NULL;
+	long lret;
 	int ret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
 	DEFINE_WAIT_FUNC(wait, woken_wake_function);
@@ -449,10 +450,12 @@ static int workload_thread(void *priv)
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
 
-		workload->status = i915_wait_request(workload->req,
-						     0, NULL, NULL);
-		if (workload->status != 0)
+		lret = i915_wait_request(workload->req,
+					 0, MAX_SCHEDULE_TIMEOUT);
+		if (lret < 0) {
+			workload->status = lret;
 			gvt_err("fail to wait workload, skip\n");
+		}
 
 complete:
 		gvt_dbg_sched("will complete workload %p\n, status: %d\n",

commit e45d7b7f47a4849a5d3d55a2cf5802a72924d37b
Author: Du, Changbin <changbin.du@intel.com>
Date:   Thu Oct 27 11:10:31 2016 +0800

    drm/i915/gvt: fix nested sleeping issue
    
    We cannot use blocking method mutex_lock inside a wait loop.
    Here we invoke pick_next_workload() which needs acquire a
    mutex in our "condition" experssion. Then we go into a another
    of the going-to-sleep sequence and changing the task state.
    This is a dangerous. Let's rewrite the wait sequence to avoid
    nested sleeping.
    
    v2: fix do...while loop exit condition (zhenyu)
    v3: rebase to gvt-staging branch
    
    Signed-off-by: Du, Changbin <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index e96eaeebeb0a..f7e320b9b17a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -402,19 +402,24 @@ static int workload_thread(void *priv)
 	struct intel_vgpu_workload *workload = NULL;
 	int ret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
+	DEFINE_WAIT_FUNC(wait, woken_wake_function);
 
 	kfree(p);
 
 	gvt_dbg_core("workload thread for ring %d started\n", ring_id);
 
 	while (!kthread_should_stop()) {
-		ret = wait_event_interruptible(scheduler->waitq[ring_id],
-				kthread_should_stop() ||
-				(workload = pick_next_workload(gvt, ring_id)));
-
-		WARN_ON_ONCE(ret);
-
-		if (kthread_should_stop())
+		add_wait_queue(&scheduler->waitq[ring_id], &wait);
+		do {
+			workload = pick_next_workload(gvt, ring_id);
+			if (workload)
+				break;
+			wait_woken(&wait, TASK_INTERRUPTIBLE,
+				   MAX_SCHEDULE_TIMEOUT);
+		} while (!kthread_should_stop());
+		remove_wait_queue(&scheduler->waitq[ring_id], &wait);
+
+		if (!workload)
 			break;
 
 		mutex_lock(&scheduler_mutex);

commit 999ccb4017c2c818afae18a90060385ec1db903b
Author: Du, Changbin <changbin.du@intel.com>
Date:   Thu Oct 20 14:08:47 2016 +0800

    drm/i915/gvt: mark symbols static where possible
    
    Mark all local functions & variables as static.
    
    Signed-off-by: Du, Changbin <changbin.du@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index a6ba60141ff4..e96eaeebeb0a 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -41,7 +41,8 @@
 #define RING_CTX_OFF(x) \
 	offsetof(struct execlist_ring_context, x)
 
-void set_context_pdp_root_pointer(struct execlist_ring_context *ring_context,
+static void set_context_pdp_root_pointer(
+		struct execlist_ring_context *ring_context,
 		u32 pdp[8])
 {
 	struct execlist_mmio_pair *pdp_pair = &ring_context->pdp3_UDW;

commit 0fac21e7e978f8556d3f9bb1b2fadfc722bfe992
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Oct 20 13:30:33 2016 +0800

    drm/i915/gvt: properly access enabled intel_engine_cs
    
    Switch to use new for_each_engine() helper to properly access
    enabled intel_engine_cs as i915 core has changed that to be
    dynamic managed. At GVT-g init time would still depend on ring
    mask to determine engine list as it's earlier.
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 12f825512e9b..a6ba60141ff4 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -510,6 +510,10 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 	init_waitqueue_head(&scheduler->workload_complete_wq);
 
 	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		/* check ring mask at init time */
+		if (!HAS_ENGINE(gvt->dev_priv, i))
+			continue;
+
 		init_waitqueue_head(&scheduler->waitq[i]);
 
 		param = kzalloc(sizeof(*param), GFP_KERNEL);

commit 66bbc3b2b16b4d15de0bd737147538bcf4d355b6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 19 11:11:44 2016 +0100

    drm/i915/gvt: Stop waiting whilst holding struct_mutex
    
    For whatever reason, the gvt scheduler runs synchronously. At the very
    least, lets run synchronously without holding the struct_mutex.
    
    v2: cut'n'paste mutex_lock instead of unlock.
    Replace long hold of struct_mutex with a mutex to serialise the worker
    threads.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 9c508c307ff0..12f825512e9b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -390,6 +390,8 @@ struct workload_thread_param {
 	int ring_id;
 };
 
+static DEFINE_MUTEX(scheduler_mutex);
+
 static int workload_thread(void *priv)
 {
 	struct workload_thread_param *p = (struct workload_thread_param *)priv;
@@ -414,17 +416,14 @@ static int workload_thread(void *priv)
 		if (kthread_should_stop())
 			break;
 
+		mutex_lock(&scheduler_mutex);
+
 		gvt_dbg_sched("ring id %d next workload %p vgpu %d\n",
 				workload->ring_id, workload,
 				workload->vgpu->id);
 
 		intel_runtime_pm_get(gvt->dev_priv);
 
-		/*
-		 * Always take i915 big lock first
-		 */
-		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
-
 		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
 				workload->ring_id, workload);
 
@@ -432,7 +431,10 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_get(gvt->dev_priv,
 					FORCEWAKE_ALL);
 
+		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
 		ret = dispatch_workload(workload);
+		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
+
 		if (ret) {
 			gvt_err("fail to dispatch workload, skip\n");
 			goto complete;
@@ -442,8 +444,7 @@ static int workload_thread(void *priv)
 				workload->ring_id, workload);
 
 		workload->status = i915_wait_request(workload->req,
-						     I915_WAIT_LOCKED,
-						     NULL, NULL);
+						     0, NULL, NULL);
 		if (workload->status != 0)
 			gvt_err("fail to wait workload, skip\n");
 
@@ -451,7 +452,9 @@ static int workload_thread(void *priv)
 		gvt_dbg_sched("will complete workload %p\n, status: %d\n",
 				workload, workload->status);
 
+		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
 		complete_current_workload(gvt, ring_id);
+		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
 
 		i915_gem_request_put(fetch_and_zero(&workload->req));
 
@@ -459,9 +462,10 @@ static int workload_thread(void *priv)
 			intel_uncore_forcewake_put(gvt->dev_priv,
 					FORCEWAKE_ALL);
 
-		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
-
 		intel_runtime_pm_put(gvt->dev_priv);
+
+		mutex_unlock(&scheduler_mutex);
+
 	}
 	return 0;
 }

commit f460c251ea37836c57584a18981820fbde809d1d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 19 11:11:43 2016 +0100

    drm/i915/gvt: Stop checking for impossible interrupts from a kthread
    
    The kthread will not be interrupted, don't even bother checking.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 3c2d8e9b2d3f..9c508c307ff0 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -423,12 +423,7 @@ static int workload_thread(void *priv)
 		/*
 		 * Always take i915 big lock first
 		 */
-		ret = i915_mutex_lock_interruptible(&gvt->dev_priv->drm);
-		if (ret < 0) {
-			gvt_err("i915 submission is not available, retry\n");
-			schedule_timeout(1);
-			continue;
-		}
+		mutex_lock(&gvt->dev_priv->drm.struct_mutex);
 
 		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
 				workload->ring_id, workload);
@@ -447,7 +442,7 @@ static int workload_thread(void *priv)
 				workload->ring_id, workload);
 
 		workload->status = i915_wait_request(workload->req,
-						     I915_WAIT_INTERRUPTIBLE | I915_WAIT_LOCKED,
+						     I915_WAIT_LOCKED,
 						     NULL, NULL);
 		if (workload->status != 0)
 			gvt_err("fail to wait workload, skip\n");

commit 0eb742d7af224481ab7abb7b38bd7166e47661e2
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 20 17:29:36 2016 +0800

    drm/i915/gvt: Hold a reference on the request
    
    The workload took a pointer to the request, and even waited upon,
    without holding a reference on the request. Take that reference
    explicitly and fix up the error path following request allocation that
    missed flushing the request.
    
    v2: [zhenyuw]
    - drop request put in error path for dispatch, as main thread
    caller will handle it identically to a real request.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 01d23ad03637..3c2d8e9b2d3f 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -164,6 +164,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	int ring_id = workload->ring_id;
 	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
 	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct drm_i915_gem_request *rq;
 	int ret;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
@@ -172,17 +173,16 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	shadow_ctx->desc_template = workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	workload->req = i915_gem_request_alloc(dev_priv->engine[ring_id],
-					       shadow_ctx);
-	if (IS_ERR_OR_NULL(workload->req)) {
+	rq = i915_gem_request_alloc(dev_priv->engine[ring_id], shadow_ctx);
+	if (IS_ERR(rq)) {
 		gvt_err("fail to allocate gem request\n");
-		workload->status = PTR_ERR(workload->req);
-		workload->req = NULL;
+		workload->status = PTR_ERR(rq);
 		return workload->status;
 	}
 
-	gvt_dbg_sched("ring id %d get i915 gem request %p\n",
-			ring_id, workload->req);
+	gvt_dbg_sched("ring id %d get i915 gem request %p\n", ring_id, rq);
+
+	workload->req = i915_gem_request_get(rq);
 
 	mutex_lock(&gvt->lock);
 
@@ -209,16 +209,15 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 			ring_id, workload->req);
 
-	i915_add_request_no_flush(workload->req);
-
+	i915_add_request_no_flush(rq);
 	workload->dispatched = true;
 	return 0;
 err:
 	workload->status = ret;
-	if (workload->req)
-		workload->req = NULL;
 
 	mutex_unlock(&gvt->lock);
+
+	i915_add_request_no_flush(rq);
 	return ret;
 }
 
@@ -459,6 +458,8 @@ static int workload_thread(void *priv)
 
 		complete_current_workload(gvt, ring_id);
 
+		i915_gem_request_put(fetch_and_zero(&workload->req));
+
 		if (need_force_wake)
 			intel_uncore_forcewake_put(gvt->dev_priv,
 					FORCEWAKE_ALL);

commit feddf6e866c9cdbdec45b09f0a9566ea538a0da3
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Thu Oct 20 17:15:03 2016 +0800

    drm/i915/gvt: clean up intel_gvt.h as interface for i915 core
    
    i915 core should only call functions and structures exposed through
    intel_gvt.h. Remove internal gvt.h and i915_pvinfo.h.
    
    Change for internal intel_gvt structure as private handler which
    not requires to expose gvt internal structure for i915 core.
    
    v2: Fix per Chris's comment
    - carefully handle dev_priv->gvt assignment
    - add necessary bracket for macro helper
    - forward declartion struct intel_gvt
    - keep free operation within same file handling alloc
    
    v3: fix use after free and remove intel_gvt.initialized
    
    v4: change to_gvt() to an inline
    
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index b15cdf5978a9..01d23ad03637 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -33,10 +33,11 @@
  *
  */
 
-#include "i915_drv.h"
-
 #include <linux/kthread.h>
 
+#include "i915_drv.h"
+#include "gvt.h"
+
 #define RING_CTX_OFF(x) \
 	offsetof(struct execlist_ring_context, x)
 

commit 1140f9ed051011e06a2a15c73efe57ac0b0cdc8d
Author: Zhenyu Wang <zhenyuw@linux.intel.com>
Date:   Tue Oct 18 09:40:07 2016 +0800

    drm/i915/gvt: Fix build failure after intel_engine_cs change
    
    Change GVT-g code reference for intel_engine_cs from static array to
    allocated pointer after commit 3b3f1650b1ca ("drm/i915: Allocate
    intel_engine_cs structure only for the enabled engines").
    
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161018014007.29369-1-zhenyuw@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 732672b7d22b..b15cdf5978a9 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -68,7 +68,7 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			workload->ctx_desc.lrca);
 
 	context_page_num = intel_lr_context_size(
-			&gvt->dev_priv->engine[ring_id]);
+			gvt->dev_priv->engine[ring_id]);
 
 	context_page_num = context_page_num >> PAGE_SHIFT;
 
@@ -171,7 +171,7 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	shadow_ctx->desc_template = workload->ctx_desc.addressing_mode <<
 				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
 
-	workload->req = i915_gem_request_alloc(&dev_priv->engine[ring_id],
+	workload->req = i915_gem_request_alloc(dev_priv->engine[ring_id],
 					       shadow_ctx);
 	if (IS_ERR_OR_NULL(workload->req)) {
 		gvt_err("fail to allocate gem request\n");
@@ -298,7 +298,7 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 			workload->ctx_desc.lrca);
 
 	context_page_num = intel_lr_context_size(
-			&gvt->dev_priv->engine[ring_id]);
+			gvt->dev_priv->engine[ring_id]);
 
 	context_page_num = context_page_num >> PAGE_SHIFT;
 

commit be1da7070aeaee23ff659c1a8cd992789ff86da4
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Tue May 3 18:26:57 2016 -0400

    drm/i915/gvt: vGPU command scanner
    
    This patch introduces a command scanner to scan guest command buffers.
    
    Signed-off-by: Yulei Zhang <yulei.zhang@intel.com>
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 2f96302c7b21..732672b7d22b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -185,6 +185,14 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 
 	mutex_lock(&gvt->lock);
 
+	ret = intel_gvt_scan_and_shadow_workload(workload);
+	if (ret)
+		goto err;
+
+	ret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);
+	if (ret)
+		goto err;
+
 	ret = populate_shadow_context(workload);
 	if (ret)
 		goto err;
@@ -345,6 +353,7 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 {
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	struct intel_vgpu_workload *workload;
+	int event;
 
 	mutex_lock(&gvt->lock);
 
@@ -355,6 +364,11 @@ static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
 			   !atomic_read(&workload->shadow_ctx_active));
 
 		update_guest_context(workload);
+
+		for_each_set_bit(event, workload->pending_events,
+				 INTEL_GVT_EVENT_MAX)
+			intel_vgpu_trigger_virtual_event(workload->vgpu,
+					event);
 	}
 
 	gvt_dbg_sched("ring id %d complete workload %p status %d\n",

commit 178657139307126b22d226df0823223d6dfe91ba
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun May 1 19:02:37 2016 -0400

    drm/i915/gvt: vGPU context switch
    
    As different VM may configure different render MMIOs when executing
    workload, to schedule workloads between different VM, the render MMIOs
    have to be switched.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 2302a97cebf6..2f96302c7b21 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -139,9 +139,13 @@ static int shadow_context_status_change(struct notifier_block *nb,
 
 	switch (action) {
 	case INTEL_CONTEXT_SCHEDULE_IN:
+		intel_gvt_load_render_mmio(workload->vgpu,
+					   workload->ring_id);
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
+		intel_gvt_restore_render_mmio(workload->vgpu,
+					      workload->ring_id);
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	default:

commit e473405783c064a9d859d108010581bae8e9af40
Author: Zhi Wang <zhi.a.wang@intel.com>
Date:   Sun May 1 07:42:16 2016 -0400

    drm/i915/gvt: vGPU workload scheduler
    
    This patch introduces the vGPU workload scheduler routines.
    
    GVT workload scheduler is responsible for picking and executing GVT workload
    from current scheduled vGPU. Before the workload is submitted to host i915,
    the guest execlist context will be shadowed in the host GVT shadow context.
    the instructions in guest ring buffer will be copied into GVT shadow ring
    buffer. Then GVT-g workload scheduler will scan the instructions in guest
    ring buffer and submit it to host i915.
    
    Signed-off-by: Zhi Wang <zhi.a.wang@intel.com>
    Signed-off-by: Zhenyu Wang <zhenyuw@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
new file mode 100644
index 000000000000..2302a97cebf6
--- /dev/null
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -0,0 +1,554 @@
+/*
+ * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ * Authors:
+ *    Zhi Wang <zhi.a.wang@intel.com>
+ *
+ * Contributors:
+ *    Ping Gao <ping.a.gao@intel.com>
+ *    Tina Zhang <tina.zhang@intel.com>
+ *    Chanbin Du <changbin.du@intel.com>
+ *    Min He <min.he@intel.com>
+ *    Bing Niu <bing.niu@intel.com>
+ *    Zhenyu Wang <zhenyuw@linux.intel.com>
+ *
+ */
+
+#include "i915_drv.h"
+
+#include <linux/kthread.h>
+
+#define RING_CTX_OFF(x) \
+	offsetof(struct execlist_ring_context, x)
+
+void set_context_pdp_root_pointer(struct execlist_ring_context *ring_context,
+		u32 pdp[8])
+{
+	struct execlist_mmio_pair *pdp_pair = &ring_context->pdp3_UDW;
+	int i;
+
+	for (i = 0; i < 8; i++)
+		pdp_pair[i].val = pdp[7 - i];
+}
+
+static int populate_shadow_context(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_gvt *gvt = vgpu->gvt;
+	int ring_id = workload->ring_id;
+	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
+	struct drm_i915_gem_object *ctx_obj =
+		shadow_ctx->engine[ring_id].state->obj;
+	struct execlist_ring_context *shadow_ring_context;
+	struct page *page;
+	void *dst;
+	unsigned long context_gpa, context_page_num;
+	int i;
+
+	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
+			workload->ctx_desc.lrca);
+
+	context_page_num = intel_lr_context_size(
+			&gvt->dev_priv->engine[ring_id]);
+
+	context_page_num = context_page_num >> PAGE_SHIFT;
+
+	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+		context_page_num = 19;
+
+	i = 2;
+
+	while (i < context_page_num) {
+		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
+				(u32)((workload->ctx_desc.lrca + i) <<
+				GTT_PAGE_SHIFT));
+		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
+			gvt_err("Invalid guest context descriptor\n");
+			return -EINVAL;
+		}
+
+		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
+		dst = kmap_atomic(page);
+		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
+				GTT_PAGE_SIZE);
+		kunmap_atomic(dst);
+		i++;
+	}
+
+	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
+	shadow_ring_context = kmap_atomic(page);
+
+#define COPY_REG(name) \
+	intel_gvt_hypervisor_read_gpa(vgpu, workload->ring_context_gpa \
+		+ RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)
+
+	COPY_REG(ctx_ctrl);
+	COPY_REG(ctx_timestamp);
+
+	if (ring_id == RCS) {
+		COPY_REG(bb_per_ctx_ptr);
+		COPY_REG(rcs_indirect_ctx);
+		COPY_REG(rcs_indirect_ctx_offset);
+	}
+#undef COPY_REG
+
+	set_context_pdp_root_pointer(shadow_ring_context,
+				     workload->shadow_mm->shadow_page_table);
+
+	intel_gvt_hypervisor_read_gpa(vgpu,
+			workload->ring_context_gpa +
+			sizeof(*shadow_ring_context),
+			(void *)shadow_ring_context +
+			sizeof(*shadow_ring_context),
+			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
+
+	kunmap_atomic(shadow_ring_context);
+	return 0;
+}
+
+static int shadow_context_status_change(struct notifier_block *nb,
+		unsigned long action, void *data)
+{
+	struct intel_vgpu *vgpu = container_of(nb,
+			struct intel_vgpu, shadow_ctx_notifier_block);
+	struct drm_i915_gem_request *req =
+		(struct drm_i915_gem_request *)data;
+	struct intel_gvt_workload_scheduler *scheduler =
+		&vgpu->gvt->scheduler;
+	struct intel_vgpu_workload *workload =
+		scheduler->current_workload[req->engine->id];
+
+	switch (action) {
+	case INTEL_CONTEXT_SCHEDULE_IN:
+		atomic_set(&workload->shadow_ctx_active, 1);
+		break;
+	case INTEL_CONTEXT_SCHEDULE_OUT:
+		atomic_set(&workload->shadow_ctx_active, 0);
+		break;
+	default:
+		WARN_ON(1);
+		return NOTIFY_OK;
+	}
+	wake_up(&workload->shadow_ctx_status_wq);
+	return NOTIFY_OK;
+}
+
+static int dispatch_workload(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_gvt *gvt = vgpu->gvt;
+	int ring_id = workload->ring_id;
+	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	int ret;
+
+	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
+		ring_id, workload);
+
+	shadow_ctx->desc_template = workload->ctx_desc.addressing_mode <<
+				    GEN8_CTX_ADDRESSING_MODE_SHIFT;
+
+	workload->req = i915_gem_request_alloc(&dev_priv->engine[ring_id],
+					       shadow_ctx);
+	if (IS_ERR_OR_NULL(workload->req)) {
+		gvt_err("fail to allocate gem request\n");
+		workload->status = PTR_ERR(workload->req);
+		workload->req = NULL;
+		return workload->status;
+	}
+
+	gvt_dbg_sched("ring id %d get i915 gem request %p\n",
+			ring_id, workload->req);
+
+	mutex_lock(&gvt->lock);
+
+	ret = populate_shadow_context(workload);
+	if (ret)
+		goto err;
+
+	if (workload->prepare) {
+		ret = workload->prepare(workload);
+		if (ret)
+			goto err;
+	}
+
+	mutex_unlock(&gvt->lock);
+
+	gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
+			ring_id, workload->req);
+
+	i915_add_request_no_flush(workload->req);
+
+	workload->dispatched = true;
+	return 0;
+err:
+	workload->status = ret;
+	if (workload->req)
+		workload->req = NULL;
+
+	mutex_unlock(&gvt->lock);
+	return ret;
+}
+
+static struct intel_vgpu_workload *pick_next_workload(
+		struct intel_gvt *gvt, int ring_id)
+{
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	struct intel_vgpu_workload *workload = NULL;
+
+	mutex_lock(&gvt->lock);
+
+	/*
+	 * no current vgpu / will be scheduled out / no workload
+	 * bail out
+	 */
+	if (!scheduler->current_vgpu) {
+		gvt_dbg_sched("ring id %d stop - no current vgpu\n", ring_id);
+		goto out;
+	}
+
+	if (scheduler->need_reschedule) {
+		gvt_dbg_sched("ring id %d stop - will reschedule\n", ring_id);
+		goto out;
+	}
+
+	if (list_empty(workload_q_head(scheduler->current_vgpu, ring_id))) {
+		gvt_dbg_sched("ring id %d stop - no available workload\n",
+				ring_id);
+		goto out;
+	}
+
+	/*
+	 * still have current workload, maybe the workload disptacher
+	 * fail to submit it for some reason, resubmit it.
+	 */
+	if (scheduler->current_workload[ring_id]) {
+		workload = scheduler->current_workload[ring_id];
+		gvt_dbg_sched("ring id %d still have current workload %p\n",
+				ring_id, workload);
+		goto out;
+	}
+
+	/*
+	 * pick a workload as current workload
+	 * once current workload is set, schedule policy routines
+	 * will wait the current workload is finished when trying to
+	 * schedule out a vgpu.
+	 */
+	scheduler->current_workload[ring_id] = container_of(
+			workload_q_head(scheduler->current_vgpu, ring_id)->next,
+			struct intel_vgpu_workload, list);
+
+	workload = scheduler->current_workload[ring_id];
+
+	gvt_dbg_sched("ring id %d pick new workload %p\n", ring_id, workload);
+
+	atomic_inc(&workload->vgpu->running_workload_num);
+out:
+	mutex_unlock(&gvt->lock);
+	return workload;
+}
+
+static void update_guest_context(struct intel_vgpu_workload *workload)
+{
+	struct intel_vgpu *vgpu = workload->vgpu;
+	struct intel_gvt *gvt = vgpu->gvt;
+	int ring_id = workload->ring_id;
+	struct i915_gem_context *shadow_ctx = workload->vgpu->shadow_ctx;
+	struct drm_i915_gem_object *ctx_obj =
+		shadow_ctx->engine[ring_id].state->obj;
+	struct execlist_ring_context *shadow_ring_context;
+	struct page *page;
+	void *src;
+	unsigned long context_gpa, context_page_num;
+	int i;
+
+	gvt_dbg_sched("ring id %d workload lrca %x\n", ring_id,
+			workload->ctx_desc.lrca);
+
+	context_page_num = intel_lr_context_size(
+			&gvt->dev_priv->engine[ring_id]);
+
+	context_page_num = context_page_num >> PAGE_SHIFT;
+
+	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+		context_page_num = 19;
+
+	i = 2;
+
+	while (i < context_page_num) {
+		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
+				(u32)((workload->ctx_desc.lrca + i) <<
+					GTT_PAGE_SHIFT));
+		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
+			gvt_err("invalid guest context descriptor\n");
+			return;
+		}
+
+		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
+		src = kmap_atomic(page);
+		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
+				GTT_PAGE_SIZE);
+		kunmap_atomic(src);
+		i++;
+	}
+
+	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa +
+		RING_CTX_OFF(ring_header.val), &workload->rb_tail, 4);
+
+	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
+	shadow_ring_context = kmap_atomic(page);
+
+#define COPY_REG(name) \
+	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa + \
+		RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)
+
+	COPY_REG(ctx_ctrl);
+	COPY_REG(ctx_timestamp);
+
+#undef COPY_REG
+
+	intel_gvt_hypervisor_write_gpa(vgpu,
+			workload->ring_context_gpa +
+			sizeof(*shadow_ring_context),
+			(void *)shadow_ring_context +
+			sizeof(*shadow_ring_context),
+			GTT_PAGE_SIZE - sizeof(*shadow_ring_context));
+
+	kunmap_atomic(shadow_ring_context);
+}
+
+static void complete_current_workload(struct intel_gvt *gvt, int ring_id)
+{
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	struct intel_vgpu_workload *workload;
+
+	mutex_lock(&gvt->lock);
+
+	workload = scheduler->current_workload[ring_id];
+
+	if (!workload->status && !workload->vgpu->resetting) {
+		wait_event(workload->shadow_ctx_status_wq,
+			   !atomic_read(&workload->shadow_ctx_active));
+
+		update_guest_context(workload);
+	}
+
+	gvt_dbg_sched("ring id %d complete workload %p status %d\n",
+			ring_id, workload, workload->status);
+
+	scheduler->current_workload[ring_id] = NULL;
+
+	atomic_dec(&workload->vgpu->running_workload_num);
+
+	list_del_init(&workload->list);
+	workload->complete(workload);
+
+	wake_up(&scheduler->workload_complete_wq);
+	mutex_unlock(&gvt->lock);
+}
+
+struct workload_thread_param {
+	struct intel_gvt *gvt;
+	int ring_id;
+};
+
+static int workload_thread(void *priv)
+{
+	struct workload_thread_param *p = (struct workload_thread_param *)priv;
+	struct intel_gvt *gvt = p->gvt;
+	int ring_id = p->ring_id;
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	struct intel_vgpu_workload *workload = NULL;
+	int ret;
+	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv);
+
+	kfree(p);
+
+	gvt_dbg_core("workload thread for ring %d started\n", ring_id);
+
+	while (!kthread_should_stop()) {
+		ret = wait_event_interruptible(scheduler->waitq[ring_id],
+				kthread_should_stop() ||
+				(workload = pick_next_workload(gvt, ring_id)));
+
+		WARN_ON_ONCE(ret);
+
+		if (kthread_should_stop())
+			break;
+
+		gvt_dbg_sched("ring id %d next workload %p vgpu %d\n",
+				workload->ring_id, workload,
+				workload->vgpu->id);
+
+		intel_runtime_pm_get(gvt->dev_priv);
+
+		/*
+		 * Always take i915 big lock first
+		 */
+		ret = i915_mutex_lock_interruptible(&gvt->dev_priv->drm);
+		if (ret < 0) {
+			gvt_err("i915 submission is not available, retry\n");
+			schedule_timeout(1);
+			continue;
+		}
+
+		gvt_dbg_sched("ring id %d will dispatch workload %p\n",
+				workload->ring_id, workload);
+
+		if (need_force_wake)
+			intel_uncore_forcewake_get(gvt->dev_priv,
+					FORCEWAKE_ALL);
+
+		ret = dispatch_workload(workload);
+		if (ret) {
+			gvt_err("fail to dispatch workload, skip\n");
+			goto complete;
+		}
+
+		gvt_dbg_sched("ring id %d wait workload %p\n",
+				workload->ring_id, workload);
+
+		workload->status = i915_wait_request(workload->req,
+						     I915_WAIT_INTERRUPTIBLE | I915_WAIT_LOCKED,
+						     NULL, NULL);
+		if (workload->status != 0)
+			gvt_err("fail to wait workload, skip\n");
+
+complete:
+		gvt_dbg_sched("will complete workload %p\n, status: %d\n",
+				workload, workload->status);
+
+		complete_current_workload(gvt, ring_id);
+
+		if (need_force_wake)
+			intel_uncore_forcewake_put(gvt->dev_priv,
+					FORCEWAKE_ALL);
+
+		mutex_unlock(&gvt->dev_priv->drm.struct_mutex);
+
+		intel_runtime_pm_put(gvt->dev_priv);
+	}
+	return 0;
+}
+
+void intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+
+	if (atomic_read(&vgpu->running_workload_num)) {
+		gvt_dbg_sched("wait vgpu idle\n");
+
+		wait_event(scheduler->workload_complete_wq,
+				!atomic_read(&vgpu->running_workload_num));
+	}
+}
+
+void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
+{
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	int i;
+
+	gvt_dbg_core("clean workload scheduler\n");
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		if (scheduler->thread[i]) {
+			kthread_stop(scheduler->thread[i]);
+			scheduler->thread[i] = NULL;
+		}
+	}
+}
+
+int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
+{
+	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
+	struct workload_thread_param *param = NULL;
+	int ret;
+	int i;
+
+	gvt_dbg_core("init workload scheduler\n");
+
+	init_waitqueue_head(&scheduler->workload_complete_wq);
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		init_waitqueue_head(&scheduler->waitq[i]);
+
+		param = kzalloc(sizeof(*param), GFP_KERNEL);
+		if (!param) {
+			ret = -ENOMEM;
+			goto err;
+		}
+
+		param->gvt = gvt;
+		param->ring_id = i;
+
+		scheduler->thread[i] = kthread_run(workload_thread, param,
+			"gvt workload %d", i);
+		if (IS_ERR(scheduler->thread[i])) {
+			gvt_err("fail to create workload thread\n");
+			ret = PTR_ERR(scheduler->thread[i]);
+			goto err;
+		}
+	}
+	return 0;
+err:
+	intel_gvt_clean_workload_scheduler(gvt);
+	kfree(param);
+	param = NULL;
+	return ret;
+}
+
+void intel_vgpu_clean_gvt_context(struct intel_vgpu *vgpu)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+
+	atomic_notifier_chain_unregister(&vgpu->shadow_ctx->status_notifier,
+			&vgpu->shadow_ctx_notifier_block);
+
+	mutex_lock(&dev_priv->drm.struct_mutex);
+
+	/* a little hacky to mark as ctx closed */
+	vgpu->shadow_ctx->closed = true;
+	i915_gem_context_put(vgpu->shadow_ctx);
+
+	mutex_unlock(&dev_priv->drm.struct_mutex);
+}
+
+int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
+{
+	atomic_set(&vgpu->running_workload_num, 0);
+
+	vgpu->shadow_ctx = i915_gem_context_create_gvt(
+			&vgpu->gvt->dev_priv->drm);
+	if (IS_ERR(vgpu->shadow_ctx))
+		return PTR_ERR(vgpu->shadow_ctx);
+
+	vgpu->shadow_ctx->engine[RCS].initialised = true;
+
+	vgpu->shadow_ctx_notifier_block.notifier_call =
+		shadow_context_status_change;
+
+	atomic_notifier_chain_register(&vgpu->shadow_ctx->status_notifier,
+				       &vgpu->shadow_ctx_notifier_block);
+	return 0;
+}
