commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 81f7656f41c3..c31a6744daee 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -203,7 +203,7 @@ i915_mmu_notifier_find(struct i915_mm_struct *mm)
 	mmap_write_lock(mm->mm);
 	mutex_lock(&mm->i915->mm_lock);
 	if (mm->mn == NULL && !err) {
-		/* Protected by mmap_sem (write-lock) */
+		/* Protected by mmap_lock (write-lock) */
 		err = __mmu_notifier_register(&mn->mn, mm->mm);
 		if (!err) {
 			/* Protected by mm_lock */
@@ -522,8 +522,8 @@ __i915_gem_userptr_get_pages_schedule(struct drm_i915_gem_object *obj)
 
 	/* Spawn a worker so that we can acquire the
 	 * user pages without holding our mutex. Access
-	 * to the user pages requires mmap_sem, and we have
-	 * a strict lock ordering of mmap_sem, struct_mutex -
+	 * to the user pages requires mmap_lock, and we have
+	 * a strict lock ordering of mmap_lock, struct_mutex -
 	 * we already hold struct_mutex here and so cannot
 	 * call gup without encountering a lock inversion.
 	 *

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 33776b3f3fa5..81f7656f41c3 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -200,7 +200,7 @@ i915_mmu_notifier_find(struct i915_mm_struct *mm)
 	if (IS_ERR(mn))
 		err = PTR_ERR(mn);
 
-	down_write(&mm->mm->mmap_sem);
+	mmap_write_lock(mm->mm);
 	mutex_lock(&mm->i915->mm_lock);
 	if (mm->mn == NULL && !err) {
 		/* Protected by mmap_sem (write-lock) */
@@ -217,7 +217,7 @@ i915_mmu_notifier_find(struct i915_mm_struct *mm)
 		err = 0;
 	}
 	mutex_unlock(&mm->i915->mm_lock);
-	up_write(&mm->mm->mmap_sem);
+	mmap_write_unlock(mm->mm);
 
 	if (mn && !IS_ERR(mn))
 		kfree(mn);
@@ -468,7 +468,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 		if (mmget_not_zero(mm)) {
 			while (pinned < npages) {
 				if (!locked) {
-					down_read(&mm->mmap_sem);
+					mmap_read_lock(mm);
 					locked = 1;
 				}
 				ret = pin_user_pages_remote
@@ -483,7 +483,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 				pinned += ret;
 			}
 			if (locked)
-				up_read(&mm->mmap_sem);
+				mmap_read_unlock(mm);
 			mmput(mm);
 		}
 	}

commit 2170ecfa768850bb29487baa3101c993ab7d7402
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Wed Jun 3 15:56:37 2020 -0700

    drm/i915: convert get_user_pages() --> pin_user_pages()
    
    This code was using get_user_pages*(), in a "Case 2" scenario (DMA/RDMA),
    using the categorization from [1].  That means that it's time to convert
    the get_user_pages*() + put_page() calls to pin_user_pages*() +
    unpin_user_pages() calls.
    
    There is some helpful background in [2]: basically, this is a small part
    of fixing a long-standing disconnect between pinning pages, and file
    systems' use of those pages.
    
    [1] Documentation/core-api/pin_user_pages.rst
    
    [2] "Explicit pinning of user-space pages":
        https://lwn.net/Articles/807108/
    
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Souptick Joarder <jrdr.linux@gmail.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: "Joonas Lahtinen" <joonas.lahtinen@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Link: http://lkml.kernel.org/r/20200519002124.2025955-5-jhubbard@nvidia.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index f80cf6ac20c5..33776b3f3fa5 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -471,7 +471,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 					down_read(&mm->mmap_sem);
 					locked = 1;
 				}
-				ret = get_user_pages_remote
+				ret = pin_user_pages_remote
 					(work->task, mm,
 					 obj->userptr.ptr + pinned * PAGE_SIZE,
 					 npages - pinned,
@@ -507,7 +507,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 	}
 	mutex_unlock(&obj->mm.lock);
 
-	release_pages(pvec, pinned);
+	unpin_user_pages(pvec, pinned);
 	kvfree(pvec);
 
 	i915_gem_object_put(obj);
@@ -564,6 +564,7 @@ static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
 	struct sg_table *pages;
 	bool active;
 	int pinned;
+	unsigned int gup_flags = 0;
 
 	/* If userspace should engineer that these pages are replaced in
 	 * the vma between us binding this page into the GTT and completion
@@ -606,11 +607,14 @@ static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
 		 *
 		 * We may or may not care.
 		 */
-		if (pvec) /* defer to worker if malloc fails */
-			pinned = __get_user_pages_fast(obj->userptr.ptr,
-						       num_pages,
-						       !i915_gem_object_is_readonly(obj),
-						       pvec);
+		if (pvec) {
+			/* defer to worker if malloc fails */
+			if (!i915_gem_object_is_readonly(obj))
+				gup_flags |= FOLL_WRITE;
+			pinned = pin_user_pages_fast_only(obj->userptr.ptr,
+							  num_pages, gup_flags,
+							  pvec);
+		}
 	}
 
 	active = false;
@@ -628,7 +632,7 @@ static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
 		__i915_gem_userptr_set_active(obj, true);
 
 	if (IS_ERR(pages))
-		release_pages(pvec, pinned);
+		unpin_user_pages(pvec, pinned);
 	kvfree(pvec);
 
 	return PTR_ERR_OR_ZERO(pages);
@@ -683,7 +687,7 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 		}
 
 		mark_page_accessed(page);
-		put_page(page);
+		unpin_user_page(page);
 	}
 	obj->mm.dirty = false;
 

commit 17839856fd588f4ab6b789f482ed3ffd7c403e1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 27 18:29:34 2020 -0700

    gup: document and work around "COW can break either way" issue
    
    Doing a "get_user_pages()" on a copy-on-write page for reading can be
    ambiguous: the page can be COW'ed at any time afterwards, and the
    direction of a COW event isn't defined.
    
    Yes, whoever writes to it will generally do the COW, but if the thread
    that did the get_user_pages() unmapped the page before the write (and
    that could happen due to memory pressure in addition to any outright
    action), the writer could also just take over the old page instead.
    
    End result: the get_user_pages() call might result in a page pointer
    that is no longer associated with the original VM, and is associated
    with - and controlled by - another VM having taken it over instead.
    
    So when doing a get_user_pages() on a COW mapping, the only really safe
    thing to do would be to break the COW when getting the page, even when
    only getting it for reading.
    
    At the same time, some users simply don't even care.
    
    For example, the perf code wants to look up the page not because it
    cares about the page, but because the code simply wants to look up the
    physical address of the access for informational purposes, and doesn't
    really care about races when a page might be unmapped and remapped
    elsewhere.
    
    This adds logic to force a COW event by setting FOLL_WRITE on any
    copy-on-write mapping when FOLL_GET (or FOLL_PIN) is used to get a page
    pointer as a result.
    
    The current semantics end up being:
    
     - __get_user_pages_fast(): no change. If you don't ask for a write,
       you won't break COW. You'd better know what you're doing.
    
     - get_user_pages_fast(): the fast-case "look it up in the page tables
       without anything getting mmap_sem" now refuses to follow a read-only
       page, since it might need COW breaking.  Which happens in the slow
       path - the fast path doesn't know if the memory might be COW or not.
    
     - get_user_pages() (including the slow-path fallback for gup_fast()):
       for a COW mapping, turn on FOLL_WRITE for FOLL_GET/FOLL_PIN, with
       very similar semantics to FOLL_FORCE.
    
    If it turns out that we want finer granularity (ie "only break COW when
    it might actually matter" - things like the zero page are special and
    don't need to be broken) we might need to push these semantics deeper
    into the lookup fault path.  So if people care enough, it's possible
    that we might end up adding a new internal FOLL_BREAK_COW flag to go
    with the internal FOLL_COW flag we already have for tracking "I had a
    COW".
    
    Alternatively, if it turns out that different callers might want to
    explicitly control the forced COW break behavior, we might even want to
    make such a flag visible to the users of get_user_pages() instead of
    using the above default semantics.
    
    But for now, this is mostly commentary on the issue (this commit message
    being a lot bigger than the patch, and that patch in turn is almost all
    comments), with that minimal "enable COW breaking early" logic using the
    existing FOLL_WRITE behavior.
    
    [ It might be worth noting that we've always had this ambiguity, and it
      could arguably be seen as a user-space issue.
    
      You only get private COW mappings that could break either way in
      situations where user space is doing cooperative things (ie fork()
      before an execve() etc), but it _is_ surprising and very subtle, and
      fork() is supposed to give you independent address spaces.
    
      So let's treat this as a kernel issue and make the semantics of
      get_user_pages() easier to understand. Note that obviously a true
      shared mapping will still get a page that can change under us, so this
      does _not_ mean that get_user_pages() somehow returns any "stable"
      page ]
    
    Reported-by: Jann Horn <jannh@google.com>
    Tested-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Kirill Shutemov <kirill@shutemov.name>
    Acked-by: Jan Kara <jack@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 7ffd7afeb7a5..f80cf6ac20c5 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -598,6 +598,14 @@ static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
 				      GFP_KERNEL |
 				      __GFP_NORETRY |
 				      __GFP_NOWARN);
+		/*
+		 * Using __get_user_pages_fast() with a read-only
+		 * access is questionable. A read-only page may be
+		 * COW-broken, and then this might end up giving
+		 * the wrong side of the COW..
+		 *
+		 * We may or may not care.
+		 */
 		if (pvec) /* defer to worker if malloc fails */
 			pinned = __get_user_pages_fast(obj->userptr.ptr,
 						       num_pages,

commit 83d2bdb6a0e088a0ec8fe1e2877c8aa1a4a80330
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Tue Feb 25 15:31:31 2020 +0200

    drm/i915: significantly reduce the use of <drm/i915_drm.h>
    
    The #include has been splattered all over the place, but there are
    precious few places, all .c files, that actually need it.
    
    v2: remove leftover double newlines
    
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200225133131.3301-1-jani.nikula@intel.com

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 63ead7a2b64a..7ffd7afeb7a5 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -10,8 +10,6 @@
 #include <linux/swap.h>
 #include <linux/sched/mm.h>
 
-#include <drm/i915_drm.h>
-
 #include "i915_drv.h"
 #include "i915_gem_ioctls.h"
 #include "i915_gem_object.h"

commit f6c26b555e14fb2b35239cb0d66963e996ec100c
Author: Janusz Krzysztofik <janusz.krzysztofik@linux.intel.com>
Date:   Tue Feb 4 16:23:02 2020 +0000

    drm/i915: Never allow userptr into the new mapping types
    
    Commit 4f2a572eda67 ("drm/i915/userptr: Never allow userptr into the
    mappable GGTT") made I915_GEM_MMAP_GTT IOCTLs to fail when attempted
    on a userptr object in order to protect from a lockdep splat.  Later
    on, new mapping types were introduced by commit cc662126b413
    ("drm/i915: Introduce DRM_I915_GEM_MMAP_OFFSET").  Those new mapping
    types suffer from the same lockdep splat issue but they now succeed
    when tried on top of a userptr object.  Fix it.
    
    v2: Don't play with the -ENODEV driver response (Chris)
    
    Signed-off-by: Janusz Krzysztofik <janusz.krzysztofik@linux.intel.com>
    Cc: Abdiel Janulgue <abdiel.janulgue@linux.intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200204162302.1299516-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index a74d4ac6a39f..63ead7a2b64a 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -704,7 +704,7 @@ i915_gem_userptr_dmabuf_export(struct drm_i915_gem_object *obj)
 static const struct drm_i915_gem_object_ops i915_gem_userptr_ops = {
 	.flags = I915_GEM_OBJECT_HAS_STRUCT_PAGE |
 		 I915_GEM_OBJECT_IS_SHRINKABLE |
-		 I915_GEM_OBJECT_NO_GGTT |
+		 I915_GEM_OBJECT_NO_MMAP |
 		 I915_GEM_OBJECT_ASYNC_CANCEL,
 	.get_pages = i915_gem_userptr_get_pages,
 	.put_pages = i915_gem_userptr_put_pages,

commit 8e78871bc1e5efec22c950d3fd24ddb63d4ff28a
Author: Matthew Auld <matthew.auld@intel.com>
Date:   Fri Jan 17 13:24:13 2020 +0000

    drm/i915/userptr: fix size calculation
    
    If we create a rather large userptr object(e.g 1ULL << 32) we might
    shift past the type-width of num_pages: (int)num_pages << PAGE_SHIFT,
    resulting in a totally bogus sg_table, which fortunately will eventually
    manifest as:
    
    gen8_ppgtt_insert_huge:463 GEM_BUG_ON(iter->sg->length < page_size)
    kernel BUG at drivers/gpu/drm/i915/gt/gen8_ppgtt.c:463!
    
    v2: more unsigned long
        prefer I915_GTT_PAGE_SIZE
    
    Fixes: 5cc9ed4b9a7a ("drm/i915: Introduce mapping of user pages into video memory (userptr) ioctl")
    Signed-off-by: Matthew Auld <matthew.auld@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200117132413.1170563-2-matthew.auld@intel.com

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index b64b0f383a56..a74d4ac6a39f 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -403,7 +403,7 @@ struct get_pages_work {
 
 static struct sg_table *
 __i915_gem_userptr_alloc_pages(struct drm_i915_gem_object *obj,
-			       struct page **pvec, int num_pages)
+			       struct page **pvec, unsigned long num_pages)
 {
 	unsigned int max_segment = i915_sg_segment_size();
 	struct sg_table *st;
@@ -449,9 +449,10 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 {
 	struct get_pages_work *work = container_of(_work, typeof(*work), work);
 	struct drm_i915_gem_object *obj = work->obj;
-	const int npages = obj->base.size >> PAGE_SHIFT;
+	const unsigned long npages = obj->base.size >> PAGE_SHIFT;
+	unsigned long pinned;
 	struct page **pvec;
-	int pinned, ret;
+	int ret;
 
 	ret = -ENOMEM;
 	pinned = 0;
@@ -559,7 +560,7 @@ __i915_gem_userptr_get_pages_schedule(struct drm_i915_gem_object *obj)
 
 static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
 {
-	const int num_pages = obj->base.size >> PAGE_SHIFT;
+	const unsigned long num_pages = obj->base.size >> PAGE_SHIFT;
 	struct mm_struct *mm = obj->userptr.mm->mm;
 	struct page **pvec;
 	struct sg_table *pages;

commit 24860ad748688b1156472d734b88b5d8d32fa42a
Author: Matthew Auld <matthew.auld@intel.com>
Date:   Fri Jan 17 13:24:12 2020 +0000

    drm/i915/userptr: add user_size limit check
    
    Don't allow a mismatch between obj->base.size/vma->size and the actual
    number of pages for the backing store, which is limited to INT_MAX
    pages.
    
    v2: document what are missing before we can safely drop the limit check
    
    Signed-off-by: Matthew Auld <matthew.auld@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200117132413.1170563-1-matthew.auld@intel.com

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index e5558af111e2..b64b0f383a56 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -769,6 +769,23 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 			    I915_USERPTR_UNSYNCHRONIZED))
 		return -EINVAL;
 
+	/*
+	 * XXX: There is a prevalence of the assumption that we fit the
+	 * object's page count inside a 32bit _signed_ variable. Let's document
+	 * this and catch if we ever need to fix it. In the meantime, if you do
+	 * spot such a local variable, please consider fixing!
+	 *
+	 * Aside from our own locals (for which we have no excuse!):
+	 * - sg_table embeds unsigned int for num_pages
+	 * - get_user_pages*() mixed ints with longs
+	 */
+
+	if (args->user_size >> PAGE_SHIFT > INT_MAX)
+		return -E2BIG;
+
+	if (overflows_type(args->user_size, obj->base.size))
+		return -E2BIG;
+
 	if (!args->user_size)
 		return -EINVAL;
 

commit e6ba76480299a0d77c51d846f7467b1673aad25b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Dec 21 16:03:24 2019 +0000

    drm/i915: Remove i915->kernel_context
    
    Allocate only an internal intel_context for the kernel_context, forgoing
    a global GEM context for internal use as we only require a separate
    address space (for our own protection).
    
    Now having weaned GT from requiring ce->gem_context, we can stop
    referencing it entirely. This also means we no longer have to create random
    and unnecessary GEM contexts for internal use.
    
    GEM contexts are now entirely for tracking GEM clients, and intel_context
    the execution environment on the GPU.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Acked-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191221160324.1073045-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index f7f66c62cf0e..e5558af111e2 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -779,15 +779,11 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 		return -EFAULT;
 
 	if (args->flags & I915_USERPTR_READ_ONLY) {
-		struct i915_address_space *vm;
-
 		/*
 		 * On almost all of the older hw, we cannot tell the GPU that
 		 * a page is readonly.
 		 */
-		vm = rcu_dereference_protected(dev_priv->kernel_context->vm,
-					       true); /* static vm */
-		if (!vm || !vm->has_read_only)
+		if (!dev_priv->gt.vm->has_read_only)
 			return -ENODEV;
 	}
 

commit 93e89ac8537bad2cd560c64b70dbb87e68714e54
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Dec 10 13:37:19 2019 +0000

    drm/i915/gem: Wait on unbind barriers when invalidating userptr
    
    When we are told we have to drop all references to userptr, wait for any
    barriers required for unbinding.
    
    <4> [2055.808787] WARNING: CPU: 3 PID: 6239 at mm/mmu_notifier.c:472 __mmu_notifier_invalidate_range_start+0x1f2/0x250
    <4> [2055.808792] Modules linked in: vgem mei_hdcp snd_hda_codec_hdmi snd_hda_codec_realtek snd_hda_codec_generic x86_pkg_temp_thermal coretemp crct10dif_pclmul crc32_pclmul ghash_clmulni_intel r8169 lpc_ich realtek i915 snd_hda_intel snd_intel_dspcfg snd_hda_codec snd_hwdep snd_hda_core pinctrl_broxton snd_pcm pinctrl_intel mei_me intel_lpss_pci mei prime_numbers [last unloaded: vgem]
    <4> [2055.808834] CPU: 3 PID: 6239 Comm: gem_userptr_bli Tainted: G     U            5.5.0-rc1-CI-CI_DRM_7522+ #1
    <4> [2055.808839] Hardware name:  /NUC6CAYB, BIOS AYAPLCEL.86A.0049.2018.0508.1356 05/08/2018
    <4> [2055.808847] RIP: 0010:__mmu_notifier_invalidate_range_start+0x1f2/0x250
    <4> [2055.808853] Code: c2 48 c7 c7 70 17 2e 82 44 89 45 d4 48 8b 70 28 e8 ec 01 ef ff 41 f6 46 20 01 44 8b 45 d4 75 0a 41 83 f8 f5 44 89 7d d4 74 89 <0f> 0b 44 89 45 d4 eb 81 0f 0b 49 8b 46 18 49 8b 76 10 4c 89 ff 48
    <4> [2055.808858] RSP: 0018:ffffc90002937d40 EFLAGS: 00010202
    <4> [2055.808865] RAX: 0000000000000061 RBX: ffff8882703a33e0 RCX: 0000000000000001
    <4> [2055.808870] RDX: 0000000000000000 RSI: ffff888277da8cb8 RDI: 00000000ffffffff
    <4> [2055.808874] RBP: ffffc90002937d70 R08: 00000000fffffff5 R09: 0000000000000000
    <4> [2055.808879] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000001
    <4> [2055.808884] R13: ffffffff822e1716 R14: ffffc90002937d80 R15: 00000000fffffff5
    <4> [2055.808890] FS:  00007fda75004e40(0000) GS:ffff888277d80000(0000) knlGS:0000000000000000
    <4> [2055.808895] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    <4> [2055.808900] CR2: 000055ad72ec3000 CR3: 00000002697b2000 CR4: 00000000003406e0
    <4> [2055.808904] Call Trace:
    <4> [2055.808920]  unmap_vmas+0x13e/0x150
    <4> [2055.808937]  unmap_region+0xa3/0x100
    <4> [2055.808964]  __do_munmap+0x26d/0x490
    <4> [2055.808980]  __vm_munmap+0x66/0xc0
    <4> [2055.808994]  __x64_sys_munmap+0x12/0x20
    <4> [2055.809001]  do_syscall_64+0x4f/0x220
    
    Closes: https://gitlab.freedesktop.org/drm/intel/issues/771
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191210133719.3874455-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 54ebc7ab71bc..f7f66c62cf0e 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -129,7 +129,8 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 		spin_unlock(&mn->lock);
 
 		ret = i915_gem_object_unbind(obj,
-					     I915_GEM_OBJECT_UNBIND_ACTIVE);
+					     I915_GEM_OBJECT_UNBIND_ACTIVE |
+					     I915_GEM_OBJECT_UNBIND_BARRIER);
 		if (ret == 0)
 			ret = __i915_gem_object_put_pages(obj);
 		i915_gem_object_put(obj);

commit d323c9b88260bc2cf3e3ed4bc741ab69643e0233
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 11 13:32:04 2019 +0000

    drm/i915/userptr: Handle unlocked gup retries
    
    Enable gup to retry and fault the pages outside of the mmap_sem lock in
    our worker. As we are inside our worker, outside of any critical path,
    we can allow the mmap_sem lock to be dropped in order to service a page
    fault; this in turn allows the mm to populate the page using a slow
    fault handler.
    
    References: 5b56d49fc31d ("mm: add locked parameter to get_user_pages_remote()")
    Testcase: igt/gem_userptr/userfault
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191111133205.11590-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index dd104b0e2071..54ebc7ab71bc 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -459,26 +459,31 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 	if (pvec != NULL) {
 		struct mm_struct *mm = obj->userptr.mm->mm;
 		unsigned int flags = 0;
+		int locked = 0;
 
 		if (!i915_gem_object_is_readonly(obj))
 			flags |= FOLL_WRITE;
 
 		ret = -EFAULT;
 		if (mmget_not_zero(mm)) {
-			down_read(&mm->mmap_sem);
 			while (pinned < npages) {
+				if (!locked) {
+					down_read(&mm->mmap_sem);
+					locked = 1;
+				}
 				ret = get_user_pages_remote
 					(work->task, mm,
 					 obj->userptr.ptr + pinned * PAGE_SIZE,
 					 npages - pinned,
 					 flags,
-					 pvec + pinned, NULL, NULL);
+					 pvec + pinned, NULL, &locked);
 				if (ret < 0)
 					break;
 
 				pinned += ret;
 			}
-			up_read(&mm->mmap_sem);
+			if (locked)
+				up_read(&mm->mmap_sem);
 			mmput(mm);
 		}
 	}

commit 0d4bbe3d407f79438dc4f87943db21f7134cfc65
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 11 13:32:03 2019 +0000

    drm/i915/userptr: Try to acquire the page lock around set_page_dirty()
    
    set_page_dirty says:
    
            For pages with a mapping this should be done under the page lock
            for the benefit of asynchronous memory errors who prefer a
            consistent dirty state. This rule can be broken in some special
            cases, but should be better not to.
    
    Under those rules, it is only safe for us to use the plain set_page_dirty
    calls for shmemfs/anonymous memory. Userptr may be used with real
    mappings and so needs to use the locked version (set_page_dirty_lock).
    
    However, following a try_to_unmap() we may want to remove the userptr and
    so call put_pages(). However, try_to_unmap() acquires the page lock and
    so we must avoid recursively locking the pages ourselves -- which means
    that we cannot safely acquire the lock around set_page_dirty(). Since we
    can't be sure of the lock, we have to risk skip dirtying the page, or
    else risk calling set_page_dirty() without a lock and so risk fs
    corruption.
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=203317
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=112012
    Fixes: 5cc9ed4b9a7a ("drm/i915: Introduce mapping of user pages into video memory (userptr) ioctl")
    References: cb6d7c7dc7ff ("drm/i915/userptr: Acquire the page lock around set_page_dirty()")
    References: 505a8ec7e11a ("Revert "drm/i915/userptr: Acquire the page lock around set_page_dirty()"")
    References: 6dcc693bc57f ("ext4: warn when page is dirtied without buffers")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191111133205.11590-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index ee65c6acf0e2..dd104b0e2071 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -646,8 +646,28 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 		obj->mm.dirty = false;
 
 	for_each_sgt_page(page, sgt_iter, pages) {
-		if (obj->mm.dirty)
+		if (obj->mm.dirty && trylock_page(page)) {
+			/*
+			 * As this may not be anonymous memory (e.g. shmem)
+			 * but exist on a real mapping, we have to lock
+			 * the page in order to dirty it -- holding
+			 * the page reference is not sufficient to
+			 * prevent the inode from being truncated.
+			 * Play safe and take the lock.
+			 *
+			 * However...!
+			 *
+			 * The mmu-notifier can be invalidated for a
+			 * migrate_page, that is alreadying holding the lock
+			 * on the page. Such a try_to_unmap() will result
+			 * in us calling put_pages() and so recursively try
+			 * to lock the page. We avoid that deadlock with
+			 * a trylock_page() and in exchange we risk missing
+			 * some page dirtying.
+			 */
 			set_page_dirty(page);
+			unlock_page(page);
+		}
 
 		mark_page_accessed(page);
 		put_page(page);

commit f86dbacb30029f4e0396e8b18b0ca60fabaec6c4
Author: Daniel Vetter <daniel.vetter@ffwll.ch>
Date:   Tue Nov 5 10:01:48 2019 +0100

    drm/i915: Switch obj->mm.lock lockdep annotations on its head
    
    The trouble with having a plain nesting flag for locks which do not
    naturally nest (unlike block devices and their partitions, which is
    the original motivation for nesting levels) is that lockdep will
    never spot a true deadlock if you screw up.
    
    This patch is an attempt at trying better, by highlighting a bit more
    of the actual nature of the nesting that's going on. Essentially we
    have two kinds of objects:
    
    - objects without pages allocated, which cannot be on any lru and are
      hence inaccessible to the shrinker.
    
    - objects which have pages allocated, which are on an lru, and which
      the shrinker can decide to throw out.
    
    For the former type of object, memory allocations while holding
    obj->mm.lock are permissible. For the latter they are not. And
    get/put_pages transitions between the two types of objects.
    
    This is still not entirely fool-proof since the rules might change.
    But as long as we run such a code ever at runtime lockdep should be
    able to observe the inconsistency and complain (like with any other
    lockdep class that we've split up in multiple classes). But there are
    a few clear benefits:
    
    - We can drop the nesting flag parameter from
      __i915_gem_object_put_pages, because that function by definition is
      never going allocate memory, and calling it on an object which
      doesn't have its pages allocated would be a bug.
    
    - We strictly catch more bugs, since there's not only one place in the
      entire tree which is annotated with the special class. All the
      other places that had explicit lockdep nesting annotations we're now
      going to leave up to lockdep again.
    
    - Specifically this catches stuff like calling get_pages from
      put_pages (which isn't really a good idea, if we can call get_pages
      so could the shrinker). I've seen patches do exactly that.
    
    Of course I fully expect CI will show me for the fool I am with this
    one here :-)
    
    v2: There can only be one (lockdep only has a cache for the first
    subclass, not for deeper ones, and we don't want to make these locks
    even slower). Still separate enums for better documentation.
    
    Real fix: don't forget about phys objs and pin_map(), and fix the
    shrinker to have the right annotations ... silly me.
    
    v3: Forgot usertptr too ...
    
    v4: Improve comment for pages_pin_count, drop the IMPORTANT comment
    and instead prime lockdep (Chris).
    
    v5: Appease checkpatch, no double empty lines (Chris)
    
    v6: More rebasing over selftest changes. Also somehow I forgot to
    push this patch :-/
    
    Also format comments consistently while at it.
    
    v7: Fix typo in commit message (Joonas)
    
    Also drop the priming, with the lmem merge we now have allocations
    while holding the lmem lock, which wreaks the generic priming I've
    done in earlier patches. Should probably be resurrected when lmem is
    fixed. See
    
    commit 232a6ebae419193f5b8da4fa869ae5089ab105c2
    Author: Matthew Auld <matthew.auld@intel.com>
    Date:   Tue Oct 8 17:01:14 2019 +0100
    
        drm/i915: introduce intel_memory_region
    
    I'm keeping the priming patch locally so it wont get lost.
    
    Cc: Matthew Auld <matthew.auld@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: "Tang, CQ" <cq.tang@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk> (v5)
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com> (v6)
    Signed-off-by: Daniel Vetter <daniel.vetter@intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Signed-off-by: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191105090148.30269-1-daniel.vetter@ffwll.ch
    [mlankhorst: Fix commit typos pointed out by Michael Ruhl]

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 1e045c337044..ee65c6acf0e2 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -131,7 +131,7 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 		ret = i915_gem_object_unbind(obj,
 					     I915_GEM_OBJECT_UNBIND_ACTIVE);
 		if (ret == 0)
-			ret = __i915_gem_object_put_pages(obj, I915_MM_SHRINKER);
+			ret = __i915_gem_object_put_pages(obj);
 		i915_gem_object_put(obj);
 		if (ret)
 			return ret;
@@ -483,7 +483,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)
 		}
 	}
 
-	mutex_lock(&obj->mm.lock);
+	mutex_lock_nested(&obj->mm.lock, I915_MM_GET_PAGES);
 	if (obj->userptr.work == &work->work) {
 		struct sg_table *pages = ERR_PTR(ret);
 

commit 7867d709959927e5df04a34f98880e5e394b411d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Oct 22 15:45:01 2019 +0100

    drm/i915/gem: Distinguish each object type
    
    Separate each object class into a separate lock type to avoid lockdep
    cross-contamination between paths (i.e. userptr!).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191022144501.26486-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 4f970474013f..1e045c337044 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -725,6 +725,7 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 		       void *data,
 		       struct drm_file *file)
 {
+	static struct lock_class_key lock_class;
 	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct drm_i915_gem_userptr *args = data;
 	struct drm_i915_gem_object *obj;
@@ -769,7 +770,7 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 		return -ENOMEM;
 
 	drm_gem_private_object_init(dev, &obj->base, args->user_size);
-	i915_gem_object_init(obj, &i915_gem_userptr_ops);
+	i915_gem_object_init(obj, &i915_gem_userptr_ops, &lock_class);
 	obj->read_domains = I915_GEM_DOMAIN_CPU;
 	obj->write_domain = I915_GEM_DOMAIN_CPU;
 	i915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);

commit 97ea56540ffc59dac275ccb64b8a5e457348a250
Merge: da0c9ea146cb 9445ad17109b
Author: Dave Airlie <airlied@redhat.com>
Date:   Tue Oct 8 12:54:38 2019 +1000

    Merge tag 'drm-intel-next-2019-10-07' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    UAPI Changes:
    - Never allow userptr into the mappable GGTT (Chris)
      No existing users. Avoid anyone from even trying to
      spare a deadlock scenario.
    
    Cross-subsystem Changes:
    
    Core Changes:
    
    Driver Changes:
    
    - Eliminate struct_mutex use as BKL! (Chris)
      Only used for execbuf serialisation.
    
    - Initialize DDI TC and TBT ports (D-I) on Tigerlake (Lucas)
    - Fix DKL link training for 2.7GHz and 1.62GHz (Jose)
    - Add Tigerlake DKL PHY programming sequences (Clinton)
    - Add Tigerlake Thunderbolt PLL divider values (Imre)
    
    - drm/i915: Use helpers for drm_mm_node booleans (Chris)
    - Restrict L3 remapping sysfs interface to dwords (Chris)
    - Fix audio power up sequence for gen10+ display (Kai)
    - Skip redundant execlist resubmission (Chris)
    - Only unwedge if we can reset GPU first (Chris)
    - Initialise breadcrumb lists on the virtual engine (Chris)
    - Don't rely on kernel context existing during early errors (Matt A)
    - Update Icelake+ MG_DP_MODE programming table (Clinton)
    - Update DMC firmware for Icelake (Anusha)
    - Downgrade DP MST error after unplugging TypeC cable (Srinivasan)
    - Limit MST modes based on plane size too (Ville)
    - Polish intel_tv_mode_valid() (Ville)
    - Fix g4x sprite scaling stride check with GTT remapping (Ville)
    - Don't advertize non-exisiting crtcs (Ville)
    - Clean up encoder->crtc_mask setup (Ville)
    - Use tc_port instead of port parameter to MG registers (Jose)
    - Remove static variable for aux last status (Jani)
    - Implement a better i945gm vblank irq vs. C-states workaround (Ville)
    
    - Make the object creation interface consistent (CQ)
    - Rename intel_vga_msr_write() to intel_vga_reset_io_mem() (Jani, Ville)
    - Eliminate previous drm_dbg/drm_err usage (Jani)
    - Move gmbus setup down to intel_modeset_init() (Jani)
    - Abstract all vgaarb access to intel_vga.[ch] (Jani)
    - Split out i915_switcheroo.[ch] from i915_drv.c (Jani)
    - Use intel_gt in has_reset* (Chris)
    - Eliminate return value for i915_gem_init_early (Matt A)
    - Selftest improvements (Chris)
    - Update HuC firmware header version number format (Daniele)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    From: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191007134801.GA24313@jlahtine-desk.ger.corp.intel.com

commit a4e7ccdac38ec8335d9e4e2656c1a041c77feae1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:09 2019 +0100

    drm/i915: Move context management under GEM
    
    Keep track of the GEM contexts underneath i915->gem.contexts and assign
    them their own lock for the purposes of list management.
    
    v2: Focus on lock tracking; ctx->vm is protected by ctx->mutex
    v3: Correct split with removal of logical HW ID
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-15-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 3798502097b4..ef5e723fd057 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -766,7 +766,8 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 		 * On almost all of the older hw, we cannot tell the GPU that
 		 * a page is readonly.
 		 */
-		vm = dev_priv->kernel_context->vm;
+		vm = rcu_dereference_protected(dev_priv->kernel_context->vm,
+					       true); /* static vm */
 		if (!vm || !vm->has_read_only)
 			return -ENODEV;
 	}

commit 2850748ef8763ab46958e43a4d1c445f29eeb37d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:39:58 2019 +0100

    drm/i915: Pull i915_vma_pin under the vm->mutex
    
    Replace the struct_mutex requirement for pinning the i915_vma with the
    local vm->mutex instead. Note that the vm->mutex is tainted by the
    shrinker (we require unbinding from inside fs-reclaim) and so we cannot
    allocate while holding that mutex. Instead we have to preallocate
    workers to do allocate and apply the PTE updates after we have we
    reserved their slot in the drm_mm (using fences to order the PTE writes
    with the GPU work and with later unbind).
    
    In adding the asynchronous vma binding, one subtle requirement is to
    avoid coupling the binding fence into the backing object->resv. That is
    the asynchronous binding only applies to the vma timeline itself and not
    to the pages as that is a more global timeline (the binding of one vma
    does not need to be ordered with another vma, nor does the implicit GEM
    fencing depend on a vma, only on writes to the backing store). Keeping
    the vma binding distinct from the backing store timelines is verified by
    a number of async gem_exec_fence and gem_exec_schedule tests. The way we
    do this is quite simple, we keep the fence for the vma binding separate
    and only wait on it as required, and never add it to the obj->resv
    itself.
    
    Another consequence in reducing the locking around the vma is the
    destruction of the vma is no longer globally serialised by struct_mutex.
    A natural solution would be to add a kref to i915_vma, but that requires
    decoupling the reference cycles, possibly by introducing a new
    i915_mm_pages object that is own by both obj->mm and vma->pages.
    However, we have not taken that route due to the overshadowing lmem/ttm
    discussions, and instead play a series of complicated games with
    trylocks to (hopefully) ensure that only one destruction path is called!
    
    v2: Add some commentary, and some helpers to reduce patch churn.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index a0d974ac78a7..3798502097b4 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -92,7 +92,6 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 	struct i915_mmu_notifier *mn =
 		container_of(_mn, struct i915_mmu_notifier, mn);
 	struct interval_tree_node *it;
-	struct mutex *unlock = NULL;
 	unsigned long end;
 	int ret = 0;
 
@@ -129,33 +128,13 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 		}
 		spin_unlock(&mn->lock);
 
-		if (!unlock) {
-			unlock = &mn->mm->i915->drm.struct_mutex;
-
-			switch (mutex_trylock_recursive(unlock)) {
-			default:
-			case MUTEX_TRYLOCK_FAILED:
-				if (mutex_lock_killable_nested(unlock, I915_MM_SHRINKER)) {
-					i915_gem_object_put(obj);
-					return -EINTR;
-				}
-				/* fall through */
-			case MUTEX_TRYLOCK_SUCCESS:
-				break;
-
-			case MUTEX_TRYLOCK_RECURSIVE:
-				unlock = ERR_PTR(-EEXIST);
-				break;
-			}
-		}
-
 		ret = i915_gem_object_unbind(obj,
 					     I915_GEM_OBJECT_UNBIND_ACTIVE);
 		if (ret == 0)
 			ret = __i915_gem_object_put_pages(obj, I915_MM_SHRINKER);
 		i915_gem_object_put(obj);
 		if (ret)
-			goto unlock;
+			return ret;
 
 		spin_lock(&mn->lock);
 
@@ -168,10 +147,6 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 	}
 	spin_unlock(&mn->lock);
 
-unlock:
-	if (!IS_ERR_OR_NULL(unlock))
-		mutex_unlock(unlock);
-
 	return ret;
 
 }

commit a4311745bba9763e3c965643d4531bd5765b0513
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Sep 28 09:25:46 2019 +0100

    drm/i915/userptr: Never allow userptr into the mappable GGTT
    
    Daniel Vetter uncovered a nasty cycle in using the mmu-notifiers to
    invalidate userptr objects which also happen to be pulled into GGTT
    mmaps. That is when we unbind the userptr object (on mmu invalidation),
    we revoke all CPU mmaps, which may then recurse into mmu invalidation.
    
    We looked for ways of breaking the cycle, but the revocation on
    invalidation is required and cannot be avoided. The only solution we
    could see was to not allow such GGTT bindings of userptr objects in the
    first place. In practice, no one really wants to use a GGTT mmapping of
    a CPU pointer...
    
    Just before Daniel's explosive lockdep patches land in v5.4-rc1, we got
    a genuine blip from CI:
    
    <4>[  246.793958] ======================================================
    <4>[  246.793972] WARNING: possible circular locking dependency detected
    <4>[  246.793989] 5.3.0-gbd6c56f50d15-drmtip_372+ #1 Tainted: G     U
    <4>[  246.794003] ------------------------------------------------------
    <4>[  246.794017] kswapd0/145 is trying to acquire lock:
    <4>[  246.794030] 000000003f565be6 (&dev->struct_mutex/1){+.+.}, at: userptr_mn_invalidate_range_start+0x18f/0x220 [i915]
    <4>[  246.794250]
                      but task is already holding lock:
    <4>[  246.794263] 000000001799cef9 (&anon_vma->rwsem){++++}, at: page_lock_anon_vma_read+0xe6/0x2a0
    <4>[  246.794291]
                      which lock already depends on the new lock.
    
    <4>[  246.794307]
                      the existing dependency chain (in reverse order) is:
    <4>[  246.794322]
                      -> #3 (&anon_vma->rwsem){++++}:
    <4>[  246.794344]        down_write+0x33/0x70
    <4>[  246.794357]        __vma_adjust+0x3d9/0x7b0
    <4>[  246.794370]        __split_vma+0x16a/0x180
    <4>[  246.794385]        mprotect_fixup+0x2a5/0x320
    <4>[  246.794399]        do_mprotect_pkey+0x208/0x2e0
    <4>[  246.794413]        __x64_sys_mprotect+0x16/0x20
    <4>[  246.794429]        do_syscall_64+0x55/0x1c0
    <4>[  246.794443]        entry_SYSCALL_64_after_hwframe+0x49/0xbe
    <4>[  246.794456]
                      -> #2 (&mapping->i_mmap_rwsem){++++}:
    <4>[  246.794478]        down_write+0x33/0x70
    <4>[  246.794493]        unmap_mapping_pages+0x48/0x130
    <4>[  246.794519]        i915_vma_revoke_mmap+0x81/0x1b0 [i915]
    <4>[  246.794519]        i915_vma_unbind+0x11d/0x4a0 [i915]
    <4>[  246.794519]        i915_vma_destroy+0x31/0x300 [i915]
    <4>[  246.794519]        __i915_gem_free_objects+0xb8/0x4b0 [i915]
    <4>[  246.794519]        drm_file_free.part.0+0x1e6/0x290
    <4>[  246.794519]        drm_release+0xa6/0xe0
    <4>[  246.794519]        __fput+0xc2/0x250
    <4>[  246.794519]        task_work_run+0x82/0xb0
    <4>[  246.794519]        do_exit+0x35b/0xdb0
    <4>[  246.794519]        do_group_exit+0x34/0xb0
    <4>[  246.794519]        __x64_sys_exit_group+0xf/0x10
    <4>[  246.794519]        do_syscall_64+0x55/0x1c0
    <4>[  246.794519]        entry_SYSCALL_64_after_hwframe+0x49/0xbe
    <4>[  246.794519]
                      -> #1 (&vm->mutex){+.+.}:
    <4>[  246.794519]        i915_gem_shrinker_taints_mutex+0x6d/0xe0 [i915]
    <4>[  246.794519]        i915_address_space_init+0x9f/0x160 [i915]
    <4>[  246.794519]        i915_ggtt_init_hw+0x55/0x170 [i915]
    <4>[  246.794519]        i915_driver_probe+0xc9f/0x1620 [i915]
    <4>[  246.794519]        i915_pci_probe+0x43/0x1b0 [i915]
    <4>[  246.794519]        pci_device_probe+0x9e/0x120
    <4>[  246.794519]        really_probe+0xea/0x3d0
    <4>[  246.794519]        driver_probe_device+0x10b/0x120
    <4>[  246.794519]        device_driver_attach+0x4a/0x50
    <4>[  246.794519]        __driver_attach+0x97/0x130
    <4>[  246.794519]        bus_for_each_dev+0x74/0xc0
    <4>[  246.794519]        bus_add_driver+0x13f/0x210
    <4>[  246.794519]        driver_register+0x56/0xe0
    <4>[  246.794519]        do_one_initcall+0x58/0x300
    <4>[  246.794519]        do_init_module+0x56/0x1f6
    <4>[  246.794519]        load_module+0x25bd/0x2a40
    <4>[  246.794519]        __se_sys_finit_module+0xd3/0xf0
    <4>[  246.794519]        do_syscall_64+0x55/0x1c0
    <4>[  246.794519]        entry_SYSCALL_64_after_hwframe+0x49/0xbe
    <4>[  246.794519]
                      -> #0 (&dev->struct_mutex/1){+.+.}:
    <4>[  246.794519]        __lock_acquire+0x15d8/0x1e90
    <4>[  246.794519]        lock_acquire+0xa6/0x1c0
    <4>[  246.794519]        __mutex_lock+0x9d/0x9b0
    <4>[  246.794519]        userptr_mn_invalidate_range_start+0x18f/0x220 [i915]
    <4>[  246.794519]        __mmu_notifier_invalidate_range_start+0x85/0x110
    <4>[  246.794519]        try_to_unmap_one+0x76b/0x860
    <4>[  246.794519]        rmap_walk_anon+0x104/0x280
    <4>[  246.794519]        try_to_unmap+0xc0/0xf0
    <4>[  246.794519]        shrink_page_list+0x561/0xc10
    <4>[  246.794519]        shrink_inactive_list+0x220/0x440
    <4>[  246.794519]        shrink_node_memcg+0x36e/0x740
    <4>[  246.794519]        shrink_node+0xcb/0x490
    <4>[  246.794519]        balance_pgdat+0x241/0x580
    <4>[  246.794519]        kswapd+0x16c/0x530
    <4>[  246.794519]        kthread+0x119/0x130
    <4>[  246.794519]        ret_from_fork+0x24/0x50
    <4>[  246.794519]
                      other info that might help us debug this:
    
    <4>[  246.794519] Chain exists of:
                        &dev->struct_mutex/1 --> &mapping->i_mmap_rwsem --> &anon_vma->rwsem
    
    <4>[  246.794519]  Possible unsafe locking scenario:
    
    <4>[  246.794519]        CPU0                    CPU1
    <4>[  246.794519]        ----                    ----
    <4>[  246.794519]   lock(&anon_vma->rwsem);
    <4>[  246.794519]                                lock(&mapping->i_mmap_rwsem);
    <4>[  246.794519]                                lock(&anon_vma->rwsem);
    <4>[  246.794519]   lock(&dev->struct_mutex/1);
    <4>[  246.794519]
                       *** DEADLOCK ***
    
    v2: Say no to mmap_ioctl
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111744
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111870
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: stable@vger.kernel.org
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190928082546.3473-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 74da35611d7c..a0d974ac78a7 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -710,6 +710,7 @@ i915_gem_userptr_dmabuf_export(struct drm_i915_gem_object *obj)
 static const struct drm_i915_gem_object_ops i915_gem_userptr_ops = {
 	.flags = I915_GEM_OBJECT_HAS_STRUCT_PAGE |
 		 I915_GEM_OBJECT_IS_SHRINKABLE |
+		 I915_GEM_OBJECT_NO_GGTT |
 		 I915_GEM_OBJECT_ASYNC_CANCEL,
 	.get_pages = i915_gem_userptr_get_pages,
 	.put_pages = i915_gem_userptr_put_pages,

commit 574cc4539762561d96b456dbc0544d8898bd4c6e
Merge: 3c2edc36a774 945b584c94f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 19 16:24:24 2019 -0700

    Merge tag 'drm-next-2019-09-18' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm updates from Dave Airlie:
     "This is the main pull request for 5.4-rc1 merge window. I don't think
      there is anything outstanding so next week should just be fixes, but
      we'll see if I missed anything. I landed some fixes earlier in the
      week but got delayed writing summary and sending it out, due to a mix
      of sick kid and jetlag!
    
      There are some fixes pending, but I'd rather get the main merge out of
      the way instead of delaying it longer.
    
      It's also pretty large in commit count and new amd header file size.
      The largest thing is four new amdgpu products (navi12/14, arcturus and
      renoir APU support).
    
      Otherwise it's pretty much lots of work across the board, i915 has
      started landing tigerlake support, lots of icelake fixes and lots of
      locking reworking for future gpu support, lots of header file rework
      (drmP.h is nearly gone), some old legacy hacks (DRM_WAIT_ON) have been
      put into the places they are needed.
    
      uapi:
       - content protection type property for HDCP
    
      core:
       - rework include dependencies
       - lots of drmP.h removals
       - link rate calculation robustness fix
       - make fb helper map only when required
       - add connector->DDC adapter link
       - DRM_WAIT_ON removed
       - drop DRM_AUTH usage from drivers
    
      dma-buf:
       - reservation object fence helper
    
      dma-fence:
       - shrink dma_fence struct
       - merge signal functions
       - store timestamps in dma_fence
       - selftests
    
      ttm:
       - embed drm_get_object struct into ttm_buffer_object
       - release_notify callback
    
      bridges:
       - sii902x - audio graph card support
       - tc358767 - aux data handling rework
       - ti-snd64dsi86 - debugfs support, DSI mode flags support
    
      panels:
       - Support for GiantPlus GPM940B0, Sharp LQ070Y3DG3B, Ortustech
         COM37H3M, Novatek NT39016, Sharp LS020B1DD01D, Raydium RM67191, Boe
         Himax8279d, Sharp LD-D5116Z01B
       - TI nspire, NEC NL8048HL11, LG Philips LB035Q02, Sharp LS037V7DW01,
         Sony ACX565AKM, Toppoly TD028TTEC1 Toppoly TD043MTEA1
    
      i915:
       - Initial tigerlake platform support
       - Locking simplification work, general all over refactoring.
       - Selftests
       - HDCP debug info improvements
       - DSI properties
       - Icelake display PLL fixes, colorspace fixes, bandwidth fixes, DSI
         suspend/resume
       - GuC fixes
       - Perf fixes
       - ElkhartLake enablement
       - DP MST fixes
       - GVT - command parser enhancements
    
      amdgpu:
       - add wipe memory on release flag for buffer creation
       - Navi12/14 support (may be marked experimental)
       - Arcturus support
       - Renoir APU support
       - mclk DPM for Navi
       - DC display fixes
       - Raven scatter/gather support
       - RAS support for GFX
       - Navi12 + Arcturus power features
       - GPU reset for Picasso
       - smu11 i2c controller support
    
      amdkfd:
       - navi12/14 support
       - Arcturus support
    
      radeon:
       - kexec fix
    
      nouveau:
       - improved display color management
       - detect lack of GPU power cables
    
      vmwgfx:
       - evicition priority support
       - remove unused security feature
    
      msm:
       - msm8998 display support
       - better async commit support for cursor updates
    
      etnaviv:
       - per-process address space support
       - performance counter fixes
       - softpin support
    
      mcde:
       - DCS transfers fix
    
      exynos:
       - drmP.h cleanup
    
      lima:
       - reduce logging
    
      kirin:
       - misc clenaups
    
      komeda:
       - dual-link support
       - DT memory regions
    
      hisilicon:
       - misc fixes
    
      imx:
       - IPUv3 image converter fixes
       - 32-bit RGB V4L2 pixel format support
    
      ingenic:
       - more support for panel related cases
    
      mgag200:
       - cursor support fix
    
      panfrost:
       - export GPU features register to userspace
       - gpu heap allocations
       - per-fd address space support
    
      pl111:
       - CLD pads wiring support removed from DT
    
      rockchip:
       - rework to use DRM PSR helpers
       - fix bug in VOP_WIN_GET macro
       - DSI DT binding rework
    
      sun4i:
       - improve support for color encoding and range
       - DDC enabled GPIO
    
      tinydrm:
       - rework SPI support
       - improve MIPI-DBI support
       - moved to drm/tiny
    
      vkms:
       - rework CRC tracking
    
      dw-hdmi:
       - get_eld and i2s improvements
    
      gm12u320:
       - misc fixes
    
      meson:
       - global code cleanup
       - vpu feature detect
    
      omap:
       - alpha/pixel blend mode properties
    
      rcar-du:
       - misc fixes"
    
    * tag 'drm-next-2019-09-18' of git://anongit.freedesktop.org/drm/drm: (2112 commits)
      drm/nouveau/bar/gm20b: Avoid BAR1 teardown during init
      drm/nouveau: Fix ordering between TTM and GEM release
      drm/nouveau/prime: Extend DMA reservation object lock
      drm/nouveau: Fix fallout from reservation object rework
      drm/nouveau/kms/nv50-: Don't create MSTMs for eDP connectors
      drm/i915: Use NOEVICT for first pass on attemping to pin a GGTT mmap
      drm/i915: to make vgpu ppgtt notificaiton as atomic operation
      drm/i915: Flush the existing fence before GGTT read/write
      drm/i915: Hold irq-off for the entire fake lock period
      drm/i915/gvt: update RING_START reg of vGPU when the context is submitted to i915
      drm/i915/gvt: update vgpu workload head pointer correctly
      drm/mcde: Fix DSI transfers
      drm/msm: Use the correct dma_sync calls harder
      drm/msm: remove unlikely() from WARN_ON() conditions
      drm/msm/dsi: Fix return value check for clk_get_parent
      drm/msm: add atomic traces
      drm/msm/dpu: async commit support
      drm/msm: async commit support
      drm/msm: split power control from prepare/complete_commit
      drm/msm: add kms->flush_commit()
      ...

commit 505a8ec7e11ae5236c4a154a1e24ef49a8349600
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Sep 12 13:56:34 2019 +0100

    Revert "drm/i915/userptr: Acquire the page lock around set_page_dirty()"
    
    The userptr put_pages can be called from inside try_to_unmap, and so
    enters with the page lock held on one of the object's backing pages. We
    cannot take the page lock ourselves for fear of recursion.
    
    Reported-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Reported-by: Martin Wilck <Martin.Wilck@suse.com>
    Reported-by: Leo Kraav <leho@kraav.com>
    Fixes: aa56a292ce62 ("drm/i915/userptr: Acquire the page lock around set_page_dirty()")
    References: https://bugzilla.kernel.org/show_bug.cgi?id=203317
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Jani Nikula <jani.nikula@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 2caa594322bc..528b61678334 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -664,15 +664,7 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 
 	for_each_sgt_page(page, sgt_iter, pages) {
 		if (obj->mm.dirty)
-			/*
-			 * As this may not be anonymous memory (e.g. shmem)
-			 * but exist on a real mapping, we have to lock
-			 * the page in order to dirty it -- holding
-			 * the page reference is not sufficient to
-			 * prevent the inode from being truncated.
-			 * Play safe and take the lock.
-			 */
-			set_page_dirty_lock(page);
+			set_page_dirty(page);
 
 		mark_page_accessed(page);
 		put_page(page);

commit 6da4a2c411e8d9be6f848f9207beb69f13113546
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Tue Aug 6 13:07:30 2019 +0300

    drm/i915: remove unnecessary includes of intel_display_types.h header
    
    With its original name intel_drv.h the intel_display_types.h header was
    superfluously cargo-cult included all over the place, while it's really
    mostly about display internals. Remove the unnecessary includes.
    
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/e3d737f0ab87c55969e62c1e077e15c04c238297.1565085692.git.jani.nikula@intel.com

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 51720b733cfe..74da35611d7c 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -12,12 +12,10 @@
 
 #include <drm/i915_drm.h>
 
-#include "display/intel_display_types.h"
-
+#include "i915_drv.h"
 #include "i915_gem_ioctls.h"
 #include "i915_gem_object.h"
 #include "i915_scatterlist.h"
-#include "i915_trace.h"
 
 struct i915_mm_struct {
 	struct mm_struct *mm;

commit 1d455f8de8e8a211cc91e19484eeda2e454531a1
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Tue Aug 6 14:39:33 2019 +0300

    drm/i915: rename intel_drv.h to display/intel_display_types.h
    
    Everything about the file is about display, and mostly about types
    related to display. Move under display/ as intel_display_types.h to
    reflect the facts.
    
    There's still plenty to clean up, but start off with moving the file
    where it logically belongs and naming according to contents.
    
    v2: fix the include guard name in the renamed file
    
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190806113933.11799-1-jani.nikula@intel.com

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index b9d2bb15e4a6..51720b733cfe 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -12,11 +12,12 @@
 
 #include <drm/i915_drm.h>
 
+#include "display/intel_display_types.h"
+
 #include "i915_gem_ioctls.h"
 #include "i915_gem_object.h"
 #include "i915_scatterlist.h"
 #include "i915_trace.h"
-#include "intel_drv.h"
 
 struct i915_mm_struct {
 	struct mm_struct *mm;

commit aa56a292ce623734ddd30f52d73f527d1f3529b5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jul 8 15:03:27 2019 +0100

    drm/i915/userptr: Acquire the page lock around set_page_dirty()
    
    set_page_dirty says:
    
            For pages with a mapping this should be done under the page lock
            for the benefit of asynchronous memory errors who prefer a
            consistent dirty state. This rule can be broken in some special
            cases, but should be better not to.
    
    Under those rules, it is only safe for us to use the plain set_page_dirty
    calls for shmemfs/anonymous memory. Userptr may be used with real
    mappings and so needs to use the locked version (set_page_dirty_lock).
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=203317
    Fixes: 5cc9ed4b9a7a ("drm/i915: Introduce mapping of user pages into video memory (userptr) ioctl")
    References: 6dcc693bc57f ("ext4: warn when page is dirtied without buffers")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190708140327.26825-1-chris@chris-wilson.co.uk
    (cherry picked from commit cb6d7c7dc7ff8cace666ddec66334117a6068ce2)
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 528b61678334..2caa594322bc 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -664,7 +664,15 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 
 	for_each_sgt_page(page, sgt_iter, pages) {
 		if (obj->mm.dirty)
-			set_page_dirty(page);
+			/*
+			 * As this may not be anonymous memory (e.g. shmem)
+			 * but exist on a real mapping, we have to lock
+			 * the page in order to dirty it -- holding
+			 * the page reference is not sufficient to
+			 * prevent the inode from being truncated.
+			 * Play safe and take the lock.
+			 */
+			set_page_dirty_lock(page);
 
 		mark_page_accessed(page);
 		put_page(page);

commit 681c774d349018c0f47a770133b952f50328d957
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jul 9 09:17:18 2019 +0100

    drm/i915/userptr: Don't mark readonly objects as dirty
    
    If we map an object as readonly into the GTT, we know that the GPU
    cannot have written to it and so the object is not dirty and we don't
    need to flush the writes back to the system.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190709081718.27843-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 32d208ede343..b9d2bb15e4a6 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -663,6 +663,14 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 	__i915_gem_object_release_shmem(obj, pages, true);
 	i915_gem_gtt_finish_pages(obj, pages);
 
+	/*
+	 * We always mark objects as dirty when they are used by the GPU,
+	 * just in case. However, if we set the vma as being read-only we know
+	 * that the object will never have been written to.
+	 */
+	if (i915_gem_object_is_readonly(obj))
+		obj->mm.dirty = false;
+
 	for_each_sgt_page(page, sgt_iter, pages) {
 		if (obj->mm.dirty)
 			/*

commit cb6d7c7dc7ff8cace666ddec66334117a6068ce2
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jul 8 15:03:27 2019 +0100

    drm/i915/userptr: Acquire the page lock around set_page_dirty()
    
    set_page_dirty says:
    
            For pages with a mapping this should be done under the page lock
            for the benefit of asynchronous memory errors who prefer a
            consistent dirty state. This rule can be broken in some special
            cases, but should be better not to.
    
    Under those rules, it is only safe for us to use the plain set_page_dirty
    calls for shmemfs/anonymous memory. Userptr may be used with real
    mappings and so needs to use the locked version (set_page_dirty_lock).
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=203317
    Fixes: 5cc9ed4b9a7a ("drm/i915: Introduce mapping of user pages into video memory (userptr) ioctl")
    References: 6dcc693bc57f ("ext4: warn when page is dirtied without buffers")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190708140327.26825-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 16ccec7fb7da..32d208ede343 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -665,7 +665,15 @@ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
 
 	for_each_sgt_page(page, sgt_iter, pages) {
 		if (obj->mm.dirty)
-			set_page_dirty(page);
+			/*
+			 * As this may not be anonymous memory (e.g. shmem)
+			 * but exist on a real mapping, we have to lock
+			 * the page in order to dirty it -- holding
+			 * the page reference is not sufficient to
+			 * prevent the inode from being truncated.
+			 * Play safe and take the lock.
+			 */
+			set_page_dirty_lock(page);
 
 		mark_page_accessed(page);
 		put_page(page);

commit c03467ba40f783ebe756114bb68e13a6b404c03a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Jul 3 10:17:17 2019 +0100

    drm/i915/gem: Free pages before rcu-freeing the object
    
    As we have dropped the final reference to the object, we do not need to
    wait until after the rcu grace period to drop its pages. We still require
    struct_mutex to completely unbind the object to release the pages, so we
    still need a free-worker to manage that from process context. By
    scheduling the release of pages before waiting for the rcu should mean
    that we are not trapping those pages from beyond the reach of the
    shrinker.
    
    v2: Pass along the request to skip if the vma is busy to the underlying
    unbind routine, to avoid checking the reservation underneath the
    i915->mm.obj_lock which may be used from inside irq context.
    
    v3: Flip the bit for unbinding while active, for later convenience.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111035
    Fixes: a93615f900bd ("drm/i915: Throw away the active object retirement complexity")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190703091726.11690-6-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index 528b61678334..16ccec7fb7da 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -150,7 +150,8 @@ userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
 			}
 		}
 
-		ret = i915_gem_object_unbind(obj);
+		ret = i915_gem_object_unbind(obj,
+					     I915_GEM_OBJECT_UNBIND_ACTIVE);
 		if (ret == 0)
 			ret = __i915_gem_object_put_pages(obj, I915_MM_SHRINKER);
 		i915_gem_object_put(obj);

commit e568ac3874be7dcef3da0cc3bd6b91ca9dd14aa0
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 11 10:12:37 2019 +0100

    drm/i915: Pull kref into i915_address_space
    
    Make the kref common to both derived structs (i915_ggtt and i915_ppgtt)
    so that we can safely reference count an abstract ctx->vm address space.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190611091238.15808-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index cfa990edb351..528b61678334 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -768,14 +768,14 @@ i915_gem_userptr_ioctl(struct drm_device *dev,
 		return -EFAULT;
 
 	if (args->flags & I915_USERPTR_READ_ONLY) {
-		struct i915_hw_ppgtt *ppgtt;
+		struct i915_address_space *vm;
 
 		/*
 		 * On almost all of the older hw, we cannot tell the GPU that
 		 * a page is readonly.
 		 */
-		ppgtt = dev_priv->kernel_context->ppgtt;
-		if (!ppgtt || !ppgtt->vm.has_read_only)
+		vm = dev_priv->kernel_context->vm;
+		if (!vm || !vm->has_read_only)
 			return -ENODEV;
 	}
 

commit 37d63f8fdb4941b6a82ef9e59bee62a494225c7e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:50 2019 +0100

    drm/i915: Pull scatterlist utils out of i915_gem.h
    
    Out scatterlist utility routines can be pulled out of i915_gem.h for a
    bit more decluttering.
    
    v2: Push I915_GTT_PAGE_SIZE out of i915_scatterlist itself and into the
    caller.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-9-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
index ccac73b72597..cfa990edb351 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -14,6 +14,7 @@
 
 #include "i915_gem_ioctls.h"
 #include "i915_gem_object.h"
+#include "i915_scatterlist.h"
 #include "i915_trace.h"
 #include "intel_drv.h"
 

commit 10be98a77c558f8cfb823cd2777171fbb35040f6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:49 2019 +0100

    drm/i915: Move more GEM objects under gem/
    
    Continuing the theme of separating out the GEM clutter.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-8-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gem/i915_gem_userptr.c b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
new file mode 100644
index 000000000000..ccac73b72597
--- /dev/null
+++ b/drivers/gpu/drm/i915/gem/i915_gem_userptr.c
@@ -0,0 +1,832 @@
+/*
+ * SPDX-License-Identifier: MIT
+ *
+ * Copyright  2012-2014 Intel Corporation
+ */
+
+#include <linux/mmu_context.h>
+#include <linux/mmu_notifier.h>
+#include <linux/mempolicy.h>
+#include <linux/swap.h>
+#include <linux/sched/mm.h>
+
+#include <drm/i915_drm.h>
+
+#include "i915_gem_ioctls.h"
+#include "i915_gem_object.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+
+struct i915_mm_struct {
+	struct mm_struct *mm;
+	struct drm_i915_private *i915;
+	struct i915_mmu_notifier *mn;
+	struct hlist_node node;
+	struct kref kref;
+	struct work_struct work;
+};
+
+#if defined(CONFIG_MMU_NOTIFIER)
+#include <linux/interval_tree.h>
+
+struct i915_mmu_notifier {
+	spinlock_t lock;
+	struct hlist_node node;
+	struct mmu_notifier mn;
+	struct rb_root_cached objects;
+	struct i915_mm_struct *mm;
+};
+
+struct i915_mmu_object {
+	struct i915_mmu_notifier *mn;
+	struct drm_i915_gem_object *obj;
+	struct interval_tree_node it;
+};
+
+static void add_object(struct i915_mmu_object *mo)
+{
+	GEM_BUG_ON(!RB_EMPTY_NODE(&mo->it.rb));
+	interval_tree_insert(&mo->it, &mo->mn->objects);
+}
+
+static void del_object(struct i915_mmu_object *mo)
+{
+	if (RB_EMPTY_NODE(&mo->it.rb))
+		return;
+
+	interval_tree_remove(&mo->it, &mo->mn->objects);
+	RB_CLEAR_NODE(&mo->it.rb);
+}
+
+static void
+__i915_gem_userptr_set_active(struct drm_i915_gem_object *obj, bool value)
+{
+	struct i915_mmu_object *mo = obj->userptr.mmu_object;
+
+	/*
+	 * During mm_invalidate_range we need to cancel any userptr that
+	 * overlaps the range being invalidated. Doing so requires the
+	 * struct_mutex, and that risks recursion. In order to cause
+	 * recursion, the user must alias the userptr address space with
+	 * a GTT mmapping (possible with a MAP_FIXED) - then when we have
+	 * to invalidate that mmaping, mm_invalidate_range is called with
+	 * the userptr address *and* the struct_mutex held.  To prevent that
+	 * we set a flag under the i915_mmu_notifier spinlock to indicate
+	 * whether this object is valid.
+	 */
+	if (!mo)
+		return;
+
+	spin_lock(&mo->mn->lock);
+	if (value)
+		add_object(mo);
+	else
+		del_object(mo);
+	spin_unlock(&mo->mn->lock);
+}
+
+static int
+userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,
+				  const struct mmu_notifier_range *range)
+{
+	struct i915_mmu_notifier *mn =
+		container_of(_mn, struct i915_mmu_notifier, mn);
+	struct interval_tree_node *it;
+	struct mutex *unlock = NULL;
+	unsigned long end;
+	int ret = 0;
+
+	if (RB_EMPTY_ROOT(&mn->objects.rb_root))
+		return 0;
+
+	/* interval ranges are inclusive, but invalidate range is exclusive */
+	end = range->end - 1;
+
+	spin_lock(&mn->lock);
+	it = interval_tree_iter_first(&mn->objects, range->start, end);
+	while (it) {
+		struct drm_i915_gem_object *obj;
+
+		if (!mmu_notifier_range_blockable(range)) {
+			ret = -EAGAIN;
+			break;
+		}
+
+		/*
+		 * The mmu_object is released late when destroying the
+		 * GEM object so it is entirely possible to gain a
+		 * reference on an object in the process of being freed
+		 * since our serialisation is via the spinlock and not
+		 * the struct_mutex - and consequently use it after it
+		 * is freed and then double free it. To prevent that
+		 * use-after-free we only acquire a reference on the
+		 * object if it is not in the process of being destroyed.
+		 */
+		obj = container_of(it, struct i915_mmu_object, it)->obj;
+		if (!kref_get_unless_zero(&obj->base.refcount)) {
+			it = interval_tree_iter_next(it, range->start, end);
+			continue;
+		}
+		spin_unlock(&mn->lock);
+
+		if (!unlock) {
+			unlock = &mn->mm->i915->drm.struct_mutex;
+
+			switch (mutex_trylock_recursive(unlock)) {
+			default:
+			case MUTEX_TRYLOCK_FAILED:
+				if (mutex_lock_killable_nested(unlock, I915_MM_SHRINKER)) {
+					i915_gem_object_put(obj);
+					return -EINTR;
+				}
+				/* fall through */
+			case MUTEX_TRYLOCK_SUCCESS:
+				break;
+
+			case MUTEX_TRYLOCK_RECURSIVE:
+				unlock = ERR_PTR(-EEXIST);
+				break;
+			}
+		}
+
+		ret = i915_gem_object_unbind(obj);
+		if (ret == 0)
+			ret = __i915_gem_object_put_pages(obj, I915_MM_SHRINKER);
+		i915_gem_object_put(obj);
+		if (ret)
+			goto unlock;
+
+		spin_lock(&mn->lock);
+
+		/*
+		 * As we do not (yet) protect the mmu from concurrent insertion
+		 * over this range, there is no guarantee that this search will
+		 * terminate given a pathologic workload.
+		 */
+		it = interval_tree_iter_first(&mn->objects, range->start, end);
+	}
+	spin_unlock(&mn->lock);
+
+unlock:
+	if (!IS_ERR_OR_NULL(unlock))
+		mutex_unlock(unlock);
+
+	return ret;
+
+}
+
+static const struct mmu_notifier_ops i915_gem_userptr_notifier = {
+	.invalidate_range_start = userptr_mn_invalidate_range_start,
+};
+
+static struct i915_mmu_notifier *
+i915_mmu_notifier_create(struct i915_mm_struct *mm)
+{
+	struct i915_mmu_notifier *mn;
+
+	mn = kmalloc(sizeof(*mn), GFP_KERNEL);
+	if (mn == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	spin_lock_init(&mn->lock);
+	mn->mn.ops = &i915_gem_userptr_notifier;
+	mn->objects = RB_ROOT_CACHED;
+	mn->mm = mm;
+
+	return mn;
+}
+
+static void
+i915_gem_userptr_release__mmu_notifier(struct drm_i915_gem_object *obj)
+{
+	struct i915_mmu_object *mo;
+
+	mo = fetch_and_zero(&obj->userptr.mmu_object);
+	if (!mo)
+		return;
+
+	spin_lock(&mo->mn->lock);
+	del_object(mo);
+	spin_unlock(&mo->mn->lock);
+	kfree(mo);
+}
+
+static struct i915_mmu_notifier *
+i915_mmu_notifier_find(struct i915_mm_struct *mm)
+{
+	struct i915_mmu_notifier *mn;
+	int err = 0;
+
+	mn = mm->mn;
+	if (mn)
+		return mn;
+
+	mn = i915_mmu_notifier_create(mm);
+	if (IS_ERR(mn))
+		err = PTR_ERR(mn);
+
+	down_write(&mm->mm->mmap_sem);
+	mutex_lock(&mm->i915->mm_lock);
+	if (mm->mn == NULL && !err) {
+		/* Protected by mmap_sem (write-lock) */
+		err = __mmu_notifier_register(&mn->mn, mm->mm);
+		if (!err) {
+			/* Protected by mm_lock */
+			mm->mn = fetch_and_zero(&mn);
+		}
+	} else if (mm->mn) {
+		/*
+		 * Someone else raced and successfully installed the mmu
+		 * notifier, we can cancel our own errors.
+		 */
+		err = 0;
+	}
+	mutex_unlock(&mm->i915->mm_lock);
+	up_write(&mm->mm->mmap_sem);
+
+	if (mn && !IS_ERR(mn))
+		kfree(mn);
+
+	return err ? ERR_PTR(err) : mm->mn;
+}
+
+static int
+i915_gem_userptr_init__mmu_notifier(struct drm_i915_gem_object *obj,
+				    unsigned flags)
+{
+	struct i915_mmu_notifier *mn;
+	struct i915_mmu_object *mo;
+
+	if (flags & I915_USERPTR_UNSYNCHRONIZED)
+		return capable(CAP_SYS_ADMIN) ? 0 : -EPERM;
+
+	if (WARN_ON(obj->userptr.mm == NULL))
+		return -EINVAL;
+
+	mn = i915_mmu_notifier_find(obj->userptr.mm);
+	if (IS_ERR(mn))
+		return PTR_ERR(mn);
+
+	mo = kzalloc(sizeof(*mo), GFP_KERNEL);
+	if (!mo)
+		return -ENOMEM;
+
+	mo->mn = mn;
+	mo->obj = obj;
+	mo->it.start = obj->userptr.ptr;
+	mo->it.last = obj->userptr.ptr + obj->base.size - 1;
+	RB_CLEAR_NODE(&mo->it.rb);
+
+	obj->userptr.mmu_object = mo;
+	return 0;
+}
+
+static void
+i915_mmu_notifier_free(struct i915_mmu_notifier *mn,
+		       struct mm_struct *mm)
+{
+	if (mn == NULL)
+		return;
+
+	mmu_notifier_unregister(&mn->mn, mm);
+	kfree(mn);
+}
+
+#else
+
+static void
+__i915_gem_userptr_set_active(struct drm_i915_gem_object *obj, bool value)
+{
+}
+
+static void
+i915_gem_userptr_release__mmu_notifier(struct drm_i915_gem_object *obj)
+{
+}
+
+static int
+i915_gem_userptr_init__mmu_notifier(struct drm_i915_gem_object *obj,
+				    unsigned flags)
+{
+	if ((flags & I915_USERPTR_UNSYNCHRONIZED) == 0)
+		return -ENODEV;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return 0;
+}
+
+static void
+i915_mmu_notifier_free(struct i915_mmu_notifier *mn,
+		       struct mm_struct *mm)
+{
+}
+
+#endif
+
+static struct i915_mm_struct *
+__i915_mm_struct_find(struct drm_i915_private *dev_priv, struct mm_struct *real)
+{
+	struct i915_mm_struct *mm;
+
+	/* Protected by dev_priv->mm_lock */
+	hash_for_each_possible(dev_priv->mm_structs, mm, node, (unsigned long)real)
+		if (mm->mm == real)
+			return mm;
+
+	return NULL;
+}
+
+static int
+i915_gem_userptr_init__mm_struct(struct drm_i915_gem_object *obj)
+{
+	struct drm_i915_private *dev_priv = to_i915(obj->base.dev);
+	struct i915_mm_struct *mm;
+	int ret = 0;
+
+	/* During release of the GEM object we hold the struct_mutex. This
+	 * precludes us from calling mmput() at that time as that may be
+	 * the last reference and so call exit_mmap(). exit_mmap() will
+	 * attempt to reap the vma, and if we were holding a GTT mmap
+	 * would then call drm_gem_vm_close() and attempt to reacquire
+	 * the struct mutex. So in order to avoid that recursion, we have
+	 * to defer releasing the mm reference until after we drop the
+	 * struct_mutex, i.e. we need to schedule a worker to do the clean
+	 * up.
+	 */
+	mutex_lock(&dev_priv->mm_lock);
+	mm = __i915_mm_struct_find(dev_priv, current->mm);
+	if (mm == NULL) {
+		mm = kmalloc(sizeof(*mm), GFP_KERNEL);
+		if (mm == NULL) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		kref_init(&mm->kref);
+		mm->i915 = to_i915(obj->base.dev);
+
+		mm->mm = current->mm;
+		mmgrab(current->mm);
+
+		mm->mn = NULL;
+
+		/* Protected by dev_priv->mm_lock */
+		hash_add(dev_priv->mm_structs,
+			 &mm->node, (unsigned long)mm->mm);
+	} else
+		kref_get(&mm->kref);
+
+	obj->userptr.mm = mm;
+out:
+	mutex_unlock(&dev_priv->mm_lock);
+	return ret;
+}
+
+static void
+__i915_mm_struct_free__worker(struct work_struct *work)
+{
+	struct i915_mm_struct *mm = container_of(work, typeof(*mm), work);
+	i915_mmu_notifier_free(mm->mn, mm->mm);
+	mmdrop(mm->mm);
+	kfree(mm);
+}
+
+static void
+__i915_mm_struct_free(struct kref *kref)
+{
+	struct i915_mm_struct *mm = container_of(kref, typeof(*mm), kref);
+
+	/* Protected by dev_priv->mm_lock */
+	hash_del(&mm->node);
+	mutex_unlock(&mm->i915->mm_lock);
+
+	INIT_WORK(&mm->work, __i915_mm_struct_free__worker);
+	queue_work(mm->i915->mm.userptr_wq, &mm->work);
+}
+
+static void
+i915_gem_userptr_release__mm_struct(struct drm_i915_gem_object *obj)
+{
+	if (obj->userptr.mm == NULL)
+		return;
+
+	kref_put_mutex(&obj->userptr.mm->kref,
+		       __i915_mm_struct_free,
+		       &to_i915(obj->base.dev)->mm_lock);
+	obj->userptr.mm = NULL;
+}
+
+struct get_pages_work {
+	struct work_struct work;
+	struct drm_i915_gem_object *obj;
+	struct task_struct *task;
+};
+
+static struct sg_table *
+__i915_gem_userptr_alloc_pages(struct drm_i915_gem_object *obj,
+			       struct page **pvec, int num_pages)
+{
+	unsigned int max_segment = i915_sg_segment_size();
+	struct sg_table *st;
+	unsigned int sg_page_sizes;
+	int ret;
+
+	st = kmalloc(sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return ERR_PTR(-ENOMEM);
+
+alloc_table:
+	ret = __sg_alloc_table_from_pages(st, pvec, num_pages,
+					  0, num_pages << PAGE_SHIFT,
+					  max_segment,
+					  GFP_KERNEL);
+	if (ret) {
+		kfree(st);
+		return ERR_PTR(ret);
+	}
+
+	ret = i915_gem_gtt_prepare_pages(obj, st);
+	if (ret) {
+		sg_free_table(st);
+
+		if (max_segment > PAGE_SIZE) {
+			max_segment = PAGE_SIZE;
+			goto alloc_table;
+		}
+
+		kfree(st);
+		return ERR_PTR(ret);
+	}
+
+	sg_page_sizes = i915_sg_page_sizes(st->sgl);
+
+	__i915_gem_object_set_pages(obj, st, sg_page_sizes);
+
+	return st;
+}
+
+static void
+__i915_gem_userptr_get_pages_worker(struct work_struct *_work)
+{
+	struct get_pages_work *work = container_of(_work, typeof(*work), work);
+	struct drm_i915_gem_object *obj = work->obj;
+	const int npages = obj->base.size >> PAGE_SHIFT;
+	struct page **pvec;
+	int pinned, ret;
+
+	ret = -ENOMEM;
+	pinned = 0;
+
+	pvec = kvmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);
+	if (pvec != NULL) {
+		struct mm_struct *mm = obj->userptr.mm->mm;
+		unsigned int flags = 0;
+
+		if (!i915_gem_object_is_readonly(obj))
+			flags |= FOLL_WRITE;
+
+		ret = -EFAULT;
+		if (mmget_not_zero(mm)) {
+			down_read(&mm->mmap_sem);
+			while (pinned < npages) {
+				ret = get_user_pages_remote
+					(work->task, mm,
+					 obj->userptr.ptr + pinned * PAGE_SIZE,
+					 npages - pinned,
+					 flags,
+					 pvec + pinned, NULL, NULL);
+				if (ret < 0)
+					break;
+
+				pinned += ret;
+			}
+			up_read(&mm->mmap_sem);
+			mmput(mm);
+		}
+	}
+
+	mutex_lock(&obj->mm.lock);
+	if (obj->userptr.work == &work->work) {
+		struct sg_table *pages = ERR_PTR(ret);
+
+		if (pinned == npages) {
+			pages = __i915_gem_userptr_alloc_pages(obj, pvec,
+							       npages);
+			if (!IS_ERR(pages)) {
+				pinned = 0;
+				pages = NULL;
+			}
+		}
+
+		obj->userptr.work = ERR_CAST(pages);
+		if (IS_ERR(pages))
+			__i915_gem_userptr_set_active(obj, false);
+	}
+	mutex_unlock(&obj->mm.lock);
+
+	release_pages(pvec, pinned);
+	kvfree(pvec);
+
+	i915_gem_object_put(obj);
+	put_task_struct(work->task);
+	kfree(work);
+}
+
+static struct sg_table *
+__i915_gem_userptr_get_pages_schedule(struct drm_i915_gem_object *obj)
+{
+	struct get_pages_work *work;
+
+	/* Spawn a worker so that we can acquire the
+	 * user pages without holding our mutex. Access
+	 * to the user pages requires mmap_sem, and we have
+	 * a strict lock ordering of mmap_sem, struct_mutex -
+	 * we already hold struct_mutex here and so cannot
+	 * call gup without encountering a lock inversion.
+	 *
+	 * Userspace will keep on repeating the operation
+	 * (thanks to EAGAIN) until either we hit the fast
+	 * path or the worker completes. If the worker is
+	 * cancelled or superseded, the task is still run
+	 * but the results ignored. (This leads to
+	 * complications that we may have a stray object
+	 * refcount that we need to be wary of when
+	 * checking for existing objects during creation.)
+	 * If the worker encounters an error, it reports
+	 * that error back to this function through
+	 * obj->userptr.work = ERR_PTR.
+	 */
+	work = kmalloc(sizeof(*work), GFP_KERNEL);
+	if (work == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	obj->userptr.work = &work->work;
+
+	work->obj = i915_gem_object_get(obj);
+
+	work->task = current;
+	get_task_struct(work->task);
+
+	INIT_WORK(&work->work, __i915_gem_userptr_get_pages_worker);
+	queue_work(to_i915(obj->base.dev)->mm.userptr_wq, &work->work);
+
+	return ERR_PTR(-EAGAIN);
+}
+
+static int i915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)
+{
+	const int num_pages = obj->base.size >> PAGE_SHIFT;
+	struct mm_struct *mm = obj->userptr.mm->mm;
+	struct page **pvec;
+	struct sg_table *pages;
+	bool active;
+	int pinned;
+
+	/* If userspace should engineer that these pages are replaced in
+	 * the vma between us binding this page into the GTT and completion
+	 * of rendering... Their loss. If they change the mapping of their
+	 * pages they need to create a new bo to point to the new vma.
+	 *
+	 * However, that still leaves open the possibility of the vma
+	 * being copied upon fork. Which falls under the same userspace
+	 * synchronisation issue as a regular bo, except that this time
+	 * the process may not be expecting that a particular piece of
+	 * memory is tied to the GPU.
+	 *
+	 * Fortunately, we can hook into the mmu_notifier in order to
+	 * discard the page references prior to anything nasty happening
+	 * to the vma (discard or cloning) which should prevent the more
+	 * egregious cases from causing harm.
+	 */
+
+	if (obj->userptr.work) {
+		/* active flag should still be held for the pending work */
+		if (IS_ERR(obj->userptr.work))
+			return PTR_ERR(obj->userptr.work);
+		else
+			return -EAGAIN;
+	}
+
+	pvec = NULL;
+	pinned = 0;
+
+	if (mm == current->mm) {
+		pvec = kvmalloc_array(num_pages, sizeof(struct page *),
+				      GFP_KERNEL |
+				      __GFP_NORETRY |
+				      __GFP_NOWARN);
+		if (pvec) /* defer to worker if malloc fails */
+			pinned = __get_user_pages_fast(obj->userptr.ptr,
+						       num_pages,
+						       !i915_gem_object_is_readonly(obj),
+						       pvec);
+	}
+
+	active = false;
+	if (pinned < 0) {
+		pages = ERR_PTR(pinned);
+		pinned = 0;
+	} else if (pinned < num_pages) {
+		pages = __i915_gem_userptr_get_pages_schedule(obj);
+		active = pages == ERR_PTR(-EAGAIN);
+	} else {
+		pages = __i915_gem_userptr_alloc_pages(obj, pvec, num_pages);
+		active = !IS_ERR(pages);
+	}
+	if (active)
+		__i915_gem_userptr_set_active(obj, true);
+
+	if (IS_ERR(pages))
+		release_pages(pvec, pinned);
+	kvfree(pvec);
+
+	return PTR_ERR_OR_ZERO(pages);
+}
+
+static void
+i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj,
+			   struct sg_table *pages)
+{
+	struct sgt_iter sgt_iter;
+	struct page *page;
+
+	/* Cancel any inflight work and force them to restart their gup */
+	obj->userptr.work = NULL;
+	__i915_gem_userptr_set_active(obj, false);
+	if (!pages)
+		return;
+
+	__i915_gem_object_release_shmem(obj, pages, true);
+	i915_gem_gtt_finish_pages(obj, pages);
+
+	for_each_sgt_page(page, sgt_iter, pages) {
+		if (obj->mm.dirty)
+			set_page_dirty(page);
+
+		mark_page_accessed(page);
+		put_page(page);
+	}
+	obj->mm.dirty = false;
+
+	sg_free_table(pages);
+	kfree(pages);
+}
+
+static void
+i915_gem_userptr_release(struct drm_i915_gem_object *obj)
+{
+	i915_gem_userptr_release__mmu_notifier(obj);
+	i915_gem_userptr_release__mm_struct(obj);
+}
+
+static int
+i915_gem_userptr_dmabuf_export(struct drm_i915_gem_object *obj)
+{
+	if (obj->userptr.mmu_object)
+		return 0;
+
+	return i915_gem_userptr_init__mmu_notifier(obj, 0);
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_userptr_ops = {
+	.flags = I915_GEM_OBJECT_HAS_STRUCT_PAGE |
+		 I915_GEM_OBJECT_IS_SHRINKABLE |
+		 I915_GEM_OBJECT_ASYNC_CANCEL,
+	.get_pages = i915_gem_userptr_get_pages,
+	.put_pages = i915_gem_userptr_put_pages,
+	.dmabuf_export = i915_gem_userptr_dmabuf_export,
+	.release = i915_gem_userptr_release,
+};
+
+/*
+ * Creates a new mm object that wraps some normal memory from the process
+ * context - user memory.
+ *
+ * We impose several restrictions upon the memory being mapped
+ * into the GPU.
+ * 1. It must be page aligned (both start/end addresses, i.e ptr and size).
+ * 2. It must be normal system memory, not a pointer into another map of IO
+ *    space (e.g. it must not be a GTT mmapping of another object).
+ * 3. We only allow a bo as large as we could in theory map into the GTT,
+ *    that is we limit the size to the total size of the GTT.
+ * 4. The bo is marked as being snoopable. The backing pages are left
+ *    accessible directly by the CPU, but reads and writes by the GPU may
+ *    incur the cost of a snoop (unless you have an LLC architecture).
+ *
+ * Synchronisation between multiple users and the GPU is left to userspace
+ * through the normal set-domain-ioctl. The kernel will enforce that the
+ * GPU relinquishes the VMA before it is returned back to the system
+ * i.e. upon free(), munmap() or process termination. However, the userspace
+ * malloc() library may not immediately relinquish the VMA after free() and
+ * instead reuse it whilst the GPU is still reading and writing to the VMA.
+ * Caveat emptor.
+ *
+ * Also note, that the object created here is not currently a "first class"
+ * object, in that several ioctls are banned. These are the CPU access
+ * ioctls: mmap(), pwrite and pread. In practice, you are expected to use
+ * direct access via your pointer rather than use those ioctls. Another
+ * restriction is that we do not allow userptr surfaces to be pinned to the
+ * hardware and so we reject any attempt to create a framebuffer out of a
+ * userptr.
+ *
+ * If you think this is a good interface to use to pass GPU memory between
+ * drivers, please use dma-buf instead. In fact, wherever possible use
+ * dma-buf instead.
+ */
+int
+i915_gem_userptr_ioctl(struct drm_device *dev,
+		       void *data,
+		       struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = to_i915(dev);
+	struct drm_i915_gem_userptr *args = data;
+	struct drm_i915_gem_object *obj;
+	int ret;
+	u32 handle;
+
+	if (!HAS_LLC(dev_priv) && !HAS_SNOOP(dev_priv)) {
+		/* We cannot support coherent userptr objects on hw without
+		 * LLC and broken snooping.
+		 */
+		return -ENODEV;
+	}
+
+	if (args->flags & ~(I915_USERPTR_READ_ONLY |
+			    I915_USERPTR_UNSYNCHRONIZED))
+		return -EINVAL;
+
+	if (!args->user_size)
+		return -EINVAL;
+
+	if (offset_in_page(args->user_ptr | args->user_size))
+		return -EINVAL;
+
+	if (!access_ok((char __user *)(unsigned long)args->user_ptr, args->user_size))
+		return -EFAULT;
+
+	if (args->flags & I915_USERPTR_READ_ONLY) {
+		struct i915_hw_ppgtt *ppgtt;
+
+		/*
+		 * On almost all of the older hw, we cannot tell the GPU that
+		 * a page is readonly.
+		 */
+		ppgtt = dev_priv->kernel_context->ppgtt;
+		if (!ppgtt || !ppgtt->vm.has_read_only)
+			return -ENODEV;
+	}
+
+	obj = i915_gem_object_alloc();
+	if (obj == NULL)
+		return -ENOMEM;
+
+	drm_gem_private_object_init(dev, &obj->base, args->user_size);
+	i915_gem_object_init(obj, &i915_gem_userptr_ops);
+	obj->read_domains = I915_GEM_DOMAIN_CPU;
+	obj->write_domain = I915_GEM_DOMAIN_CPU;
+	i915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);
+
+	obj->userptr.ptr = args->user_ptr;
+	if (args->flags & I915_USERPTR_READ_ONLY)
+		i915_gem_object_set_readonly(obj);
+
+	/* And keep a pointer to the current->mm for resolving the user pages
+	 * at binding. This means that we need to hook into the mmu_notifier
+	 * in order to detect if the mmu is destroyed.
+	 */
+	ret = i915_gem_userptr_init__mm_struct(obj);
+	if (ret == 0)
+		ret = i915_gem_userptr_init__mmu_notifier(obj, args->flags);
+	if (ret == 0)
+		ret = drm_gem_handle_create(file, &obj->base, &handle);
+
+	/* drop reference from allocate - handle holds it now */
+	i915_gem_object_put(obj);
+	if (ret)
+		return ret;
+
+	args->handle = handle;
+	return 0;
+}
+
+int i915_gem_init_userptr(struct drm_i915_private *dev_priv)
+{
+	mutex_init(&dev_priv->mm_lock);
+	hash_init(dev_priv->mm_structs);
+
+	dev_priv->mm.userptr_wq =
+		alloc_workqueue("i915-userptr-acquire",
+				WQ_HIGHPRI | WQ_UNBOUND,
+				0);
+	if (!dev_priv->mm.userptr_wq)
+		return -ENOMEM;
+
+	return 0;
+}
+
+void i915_gem_cleanup_userptr(struct drm_i915_private *dev_priv)
+{
+	destroy_workqueue(dev_priv->mm.userptr_wq);
+}
