commit dfe324f34c53af095bfe54b322e1a338421a6b0e
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 2 13:24:30 2019 +0100

    drm/i915/selftests: Extract random_offset() for use with a prng
    
    For selftests, we desire repeatability and so prefer using a prng with
    known seed over true randomness. Extract random_offset() as a selftest
    utility that can take the prng state.
    
    Suggested-by: Matthew Auld <matthew.auld@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191002122430.23205-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index 716a3f19f030..abdfadcf626b 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -29,6 +29,7 @@
 #include <linux/types.h>
 
 #include "i915_random.h"
+#include "i915_utils.h"
 
 u64 i915_prandom_u64_state(struct rnd_state *rnd)
 {
@@ -87,3 +88,22 @@ unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
 	i915_random_reorder(order, count, state);
 	return order;
 }
+
+u64 igt_random_offset(struct rnd_state *state,
+		      u64 start, u64 end,
+		      u64 len, u64 align)
+{
+	u64 range, addr;
+
+	BUG_ON(range_overflows(start, len, end));
+	BUG_ON(round_up(start, align) > round_down(end - len, align));
+
+	range = round_down(end - len, align) - round_up(start, align);
+	if (range) {
+		addr = i915_prandom_u64_state(state);
+		div64_u64_rem(addr, range, &addr);
+		start += addr;
+	}
+
+	return round_up(start, align);
+}

commit 8ba306a6a362ef6f3c005ec8819c8890a6fadcd1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jan 28 18:18:10 2019 +0000

    drm/i915: Share per-timeline HWSP using a slab suballocator
    
    If we restrict ourselves to only using a cacheline for each timeline's
    HWSP (we could go smaller, but want to avoid needless polluting
    cachelines on different engines between different contexts), then we can
    suballocate a single 4k page into 64 different timeline HWSP. By
    treating each fresh allocation as a slab of 64 entries, we can keep it
    around for the next 64 allocation attempts until we need to refresh the
    slab cache.
    
    John Harrison noted the issue of fragmentation leading to the same worst
    case performance of one page per timeline as before, which can be
    mitigated by adopting a freelist.
    
    v2: Keep all partially allocated HWSP on a freelist
    
    This is still without migration, so it is possible for the system to end
    up with each timeline in its own page, but we ensure that no new
    allocation would needless allocate a fresh page!
    
    v3: Throw a selftest at the allocator to try and catch invalid cacheline
    reuse.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190128181812.22804-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index 1f415ce47018..716a3f19f030 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -41,18 +41,37 @@ u64 i915_prandom_u64_state(struct rnd_state *rnd)
 	return x;
 }
 
-void i915_random_reorder(unsigned int *order, unsigned int count,
-			 struct rnd_state *state)
+void i915_prandom_shuffle(void *arr, size_t elsz, size_t count,
+			  struct rnd_state *state)
 {
-	unsigned int i, j;
+	char stack[128];
+
+	if (WARN_ON(elsz > sizeof(stack) || count > U32_MAX))
+		return;
+
+	if (!elsz || !count)
+		return;
+
+	/* Fisher-Yates shuffle courtesy of Knuth */
+	while (--count) {
+		size_t swp;
+
+		swp = i915_prandom_u32_max_state(count + 1, state);
+		if (swp == count)
+			continue;
 
-	for (i = 0; i < count; i++) {
-		BUILD_BUG_ON(sizeof(unsigned int) > sizeof(u32));
-		j = i915_prandom_u32_max_state(count, state);
-		swap(order[i], order[j]);
+		memcpy(stack, arr + count * elsz, elsz);
+		memcpy(arr + count * elsz, arr + swp * elsz, elsz);
+		memcpy(arr + swp * elsz, stack, elsz);
 	}
 }
 
+void i915_random_reorder(unsigned int *order, unsigned int count,
+			 struct rnd_state *state)
+{
+	i915_prandom_shuffle(order, sizeof(*order), count, state);
+}
+
 unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
 {
 	unsigned int *order, i;

commit 9861b6682804a9db82600e0adf592bfa6deec377
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Dec 23 11:04:07 2017 +0000

    drm/i915/selftests: Allow random array allocation to fail
    
    In the selftests, we don't want to force an oom and would rather
    ENOMEM be reported. In this case, we would rather the allocation for the
    random array to fail.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171223110407.21402-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index 2088ae57aa89..1f415ce47018 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -57,7 +57,8 @@ unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
 {
 	unsigned int *order, i;
 
-	order = kmalloc_array(count, sizeof(*order), GFP_KERNEL | __GFP_NOWARN);
+	order = kmalloc_array(count, sizeof(*order),
+			      GFP_KERNEL | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);
 	if (!order)
 		return order;
 

commit c65c8b0f7a1f90e84c273e111fc9391c50c3c482
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 22 12:06:00 2017 +0000

    drm/i915/selftests: Use NOWARN for large allocations
    
    We may try to do a large kmalloc for the permutation array, falling back
    to a smaller array/test if the first allocation fails. Since we are
    intentionally trying a large allocation which may fail, pass __GFP_NOWARN.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=103842
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171122120600.27025-1-chris@chris-wilson.co.uk
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index b85872cc7fbe..2088ae57aa89 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -57,7 +57,7 @@ unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
 {
 	unsigned int *order, i;
 
-	order = kmalloc_array(count, sizeof(*order), GFP_KERNEL);
+	order = kmalloc_array(count, sizeof(*order), GFP_KERNEL | __GFP_NOWARN);
 	if (!order)
 		return order;
 

commit 32f35b863451884e856f0f577474740561a87fad
Merge: ae7617f0ef18 754270c7c562
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Thu Sep 28 15:56:49 2017 +0300

    Merge drm-upstream/drm-next into drm-intel-next-queued
    
    Need MST sideband message transaction to power up/down nodes.
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit 0ee931c4e31a5efb134c76440405e9219f896e33
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Sep 13 16:28:29 2017 -0700

    mm: treewide: remove GFP_TEMPORARY allocation flag
    
    GFP_TEMPORARY was introduced by commit e12ba74d8ff3 ("Group short-lived
    and reclaimable kernel allocations") along with __GFP_RECLAIMABLE.  It's
    primary motivation was to allow users to tell that an allocation is
    short lived and so the allocator can try to place such allocations close
    together and prevent long term fragmentation.  As much as this sounds
    like a reasonable semantic it becomes much less clear when to use the
    highlevel GFP_TEMPORARY allocation flag.  How long is temporary? Can the
    context holding that memory sleep? Can it take locks? It seems there is
    no good answer for those questions.
    
    The current implementation of GFP_TEMPORARY is basically GFP_KERNEL |
    __GFP_RECLAIMABLE which in itself is tricky because basically none of
    the existing caller provide a way to reclaim the allocated memory.  So
    this is rather misleading and hard to evaluate for any benefits.
    
    I have checked some random users and none of them has added the flag
    with a specific justification.  I suspect most of them just copied from
    other existing users and others just thought it might be a good idea to
    use without any measuring.  This suggests that GFP_TEMPORARY just
    motivates for cargo cult usage without any reasoning.
    
    I believe that our gfp flags are quite complex already and especially
    those with highlevel semantic should be clearly defined to prevent from
    confusion and abuse.  Therefore I propose dropping GFP_TEMPORARY and
    replace all existing users to simply use GFP_KERNEL.  Please note that
    SLAB users with shrinkers will still get __GFP_RECLAIMABLE heuristic and
    so they will be placed properly for memory fragmentation prevention.
    
    I can see reasons we might want some gfp flag to reflect shorterm
    allocations but I propose starting from a clear semantic definition and
    only then add users with proper justification.
    
    This was been brought up before LSF this year by Matthew [1] and it
    turned out that GFP_TEMPORARY really doesn't have a clear semantic.  It
    seems to be a heuristic without any measured advantage for most (if not
    all) its current users.  The follow up discussion has revealed that
    opinions on what might be temporary allocation differ a lot between
    developers.  So rather than trying to tweak existing users into a
    semantic which they haven't expected I propose to simply remove the flag
    and start from scratch if we really need a semantic for short term
    allocations.
    
    [1] http://lkml.kernel.org/r/20170118054945.GD18349@bombadil.infradead.org
    
    [akpm@linux-foundation.org: fix typo]
    [akpm@linux-foundation.org: coding-style fixes]
    [sfr@canb.auug.org.au: drm/i915: fix up]
      Link: http://lkml.kernel.org/r/20170816144703.378d4f4d@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20170728091904.14627-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index d044bf9a6feb..222c511bea49 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -62,7 +62,7 @@ unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
 {
 	unsigned int *order, i;
 
-	order = kmalloc_array(count, sizeof(*order), GFP_TEMPORARY);
+	order = kmalloc_array(count, sizeof(*order), GFP_KERNEL);
 	if (!order)
 		return order;
 

commit 7ce5b6850b47824a2b8d0a17b5fe75f9942e5cd1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Sep 13 11:51:54 2017 +0100

    drm/i915/selftests: Use mul_u32_u32() for 32b x 32b -> 64b result
    
    As realised by commit 9e3d6223d209 ("math64, timers: Fix 32bit
    mul_u64_u32_shr() and friends"), GCC does not always generate ideal code
    for performing a 32b x 32b multiply returning a 64b result (i.e. where
    we idiomatically use u64 result = (u64)x * (u32)x).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20170913105154.2910-2-chris@chris-wilson.co.uk
    Reviewed-by: Ville Syrjälä <ville.syrjala@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index d044bf9a6feb..ea0f5dbc0eb7 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -41,11 +41,6 @@ u64 i915_prandom_u64_state(struct rnd_state *rnd)
 	return x;
 }
 
-static inline u32 i915_prandom_u32_max_state(u32 ep_ro, struct rnd_state *state)
-{
-	return upper_32_bits((u64)prandom_u32_state(state) * ep_ro);
-}
-
 void i915_random_reorder(unsigned int *order, unsigned int count,
 			 struct rnd_state *state)
 {

commit 4797948071f607c5b43753cb8f1b7ddcf22e146d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 3 10:39:21 2017 +0100

    drm/i915: Squash repeated awaits on the same fence
    
    Track the latest fence waited upon on each context, and only add a new
    asynchronous wait if the new fence is more recent than the recorded
    fence for that context. This requires us to filter out unordered
    timelines, which are noted by DMA_FENCE_NO_CONTEXT. However, in the
    absence of a universal identifier, we have to use our own
    i915->mm.unordered_timeline token.
    
    v2: Throw around the debug crutches
    v3: Inline the likely case of the pre-allocation cache being full.
    v4: Drop the pre-allocation support, we can lose the most recent fence
    in case of allocation failure -- it just means we may emit more awaits
    than strictly necessary but will not break.
    v5: Trim allocation size for leaf nodes, they only need an array of u32
    not pointers.
    v6: Create mock_timeline to tidy selftest writing
    v7: s/intel_timeline_sync_get/intel_timeline_sync_is_later/ (Tvrtko)
    v8: Prune the stale sync points when we idle.
    v9: Include a small benchmark in the kselftests
    v10: Separate the idr implementation into its own compartment. (Tvrkto)
    v11: Refactor igt_sync kselftests to avoid deep nesting (Tvrkto)
    v12: __sync_leaf_idx() to assert that p->height is 0 when checking leaves
    v13: kselftests to investigate struct i915_syncmap itself (Tvrtko)
    v14: Foray into ascii art graphs
    v15: Take into account that the random lookup/insert does 2 prng calls,
    not 1, when benchmarking, and use for_each_set_bit() (Tvrtko)
    v16: Improved ascii art
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170503093924.5320-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
index c17c83c30637..d044bf9a6feb 100644
--- a/drivers/gpu/drm/i915/selftests/i915_random.c
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -30,6 +30,17 @@
 
 #include "i915_random.h"
 
+u64 i915_prandom_u64_state(struct rnd_state *rnd)
+{
+	u64 x;
+
+	x = prandom_u32_state(rnd);
+	x <<= 32;
+	x |= prandom_u32_state(rnd);
+
+	return x;
+}
+
 static inline u32 i915_prandom_u32_max_state(u32 ep_ro, struct rnd_state *state)
 {
 	return upper_32_bits((u64)prandom_u32_state(state) * ep_ro);

commit 953c7f82eb890085c60dbe22578e883d6837c674
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Feb 13 17:15:12 2017 +0000

    drm/i915: Provide a hook for selftests
    
    Some pieces of code are independent of hardware but are very tricky to
    exercise through the normal userspace ABI or via debugfs hooks. Being
    able to create mock unit tests and execute them through CI is vital.
    Start by adding a central point where we can execute unit tests and
    a parameter to enable them. This is disabled by default as the
    expectation is that these tests will occasionally explode.
    
    To facilitate integration with igt, any parameter beginning with
    i915.igt__ is interpreted as a subtest executable independently via
    igt/drv_selftest.
    
    Two classes of selftests are recognised: mock unit tests and integration
    tests. Mock unit tests are run as soon as the module is loaded, before
    the device is probed. At that point there is no driver instantiated and
    all hw interactions must be "mocked". This is very useful for writing
    universal tests to exercise code not typically run on a broad range of
    architectures. Alternatively, you can hook into the live selftests and
    run when the device has been instantiated - hw interactions are real.
    
    v2: Add a macro for compiling conditional code for mock objects inside
    real objects.
    v3: Differentiate between mock unit tests and late integration test.
    v4: List the tests in natural order, use igt to sort after modparam.
    v5: s/late/live/
    v6: s/unsigned long/unsigned int/
    v7: Use igt_ prefixes for long helpers.
    v8: Deobfuscate macros overriding functions, stop using -I$(src)
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170213171558.20942-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/selftests/i915_random.c b/drivers/gpu/drm/i915/selftests/i915_random.c
new file mode 100644
index 000000000000..c17c83c30637
--- /dev/null
+++ b/drivers/gpu/drm/i915/selftests/i915_random.c
@@ -0,0 +1,63 @@
+/*
+ * Copyright © 2016 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/kernel.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "i915_random.h"
+
+static inline u32 i915_prandom_u32_max_state(u32 ep_ro, struct rnd_state *state)
+{
+	return upper_32_bits((u64)prandom_u32_state(state) * ep_ro);
+}
+
+void i915_random_reorder(unsigned int *order, unsigned int count,
+			 struct rnd_state *state)
+{
+	unsigned int i, j;
+
+	for (i = 0; i < count; i++) {
+		BUILD_BUG_ON(sizeof(unsigned int) > sizeof(u32));
+		j = i915_prandom_u32_max_state(count, state);
+		swap(order[i], order[j]);
+	}
+}
+
+unsigned int *i915_random_order(unsigned int count, struct rnd_state *state)
+{
+	unsigned int *order, i;
+
+	order = kmalloc_array(count, sizeof(*order), GFP_TEMPORARY);
+	if (!order)
+		return order;
+
+	for (i = 0; i < count; i++)
+		order[i] = i;
+
+	i915_random_reorder(order, count, state);
+	return order;
+}
