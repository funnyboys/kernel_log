commit b0a997ae5248b293b6f6d1996ea49c57f7b94227
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun May 10 11:24:29 2020 +0100

    drm/i915: Emit await(batch) before MI_BB_START
    
    Be consistent and ensure that we always emit the asynchronous waits
    prior to issuing instructions that use the address. This ensures that if
    we do emit GPU commands to do the await, they are before our use!
    
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200510102431.21959-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index 708cb7808865..f59e7875cc5e 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -219,6 +219,14 @@ int intel_renderstate_emit(struct intel_renderstate *so,
 	if (!so->vma)
 		return 0;
 
+	i915_vma_lock(so->vma);
+	err = i915_request_await_object(rq, so->vma->obj, false);
+	if (err == 0)
+		err = i915_vma_move_to_active(so->vma, rq, 0);
+	i915_vma_unlock(so->vma);
+	if (err)
+		return err;
+
 	err = engine->emit_bb_start(rq,
 				    so->batch_offset, so->batch_size,
 				    I915_DISPATCH_SECURE);
@@ -233,13 +241,7 @@ int intel_renderstate_emit(struct intel_renderstate *so,
 			return err;
 	}
 
-	i915_vma_lock(so->vma);
-	err = i915_request_await_object(rq, so->vma->obj, false);
-	if (err == 0)
-		err = i915_vma_move_to_active(so->vma, rq, 0);
-	i915_vma_unlock(so->vma);
-
-	return err;
+	return 0;
 }
 
 void intel_renderstate_fini(struct intel_renderstate *so)

commit 50689771c8f073e97f7758e5b696c64f3044bbd8
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 22 20:05:58 2020 +0100

    drm/i915: Only close vma we open
    
    The history of i915_vma_close() is confusing, as is its use. As the
    lifetime of the i915_vma is currently bounded by the object it is
    attached to, we needed a means of identify when a vma was no longer in
    use by userspace (via the user's fd). This is further complicated by
    that only ppgtt vma should be closed at the user's behest, as the ggtt
    were always shared.
    
    Now that we attach the vma to a lut on the user's context, the open
    count does indicate how many unique and open context/vm are referencing
    this vma from the user. As such, we can and should just use the
    open_count to track when the vma is still in use by userspace.
    
    It's a poor man's replacement for reference counting.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/issues/1193
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200422190558.30509-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index 26e78db33675..708cb7808865 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -194,7 +194,7 @@ int intel_renderstate_init(struct intel_renderstate *so,
 
 	err = i915_vma_pin(so->vma, 0, 0, PIN_GLOBAL | PIN_HIGH);
 	if (err)
-		goto err_vma;
+		goto err_obj;
 
 	err = render_state_setup(so, engine->i915);
 	if (err)
@@ -204,8 +204,6 @@ int intel_renderstate_init(struct intel_renderstate *so,
 
 err_unpin:
 	i915_vma_unpin(so->vma);
-err_vma:
-	i915_vma_close(so->vma);
 err_obj:
 	i915_gem_object_put(obj);
 	so->vma = NULL;

commit edf040f4ee619c9f47da1781eafc7f2dde36d57c
Author: Wambui Karuga <wambui.karugax@gmail.com>
Date:   Sat Mar 14 21:33:41 2020 +0300

    drm/i915/renderstate: use struct drm_device based logging macros.
    
    Replace the use of the printk based drm logging macros with the struct
    drm_device based logging macros.
    
    Signed-off-by: Wambui Karuga <wambui.karugax@gmail.com>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200314183344.17603-5-wambui.karugax@gmail.com

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index 5954ecc3207f..26e78db33675 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -102,7 +102,7 @@ static int render_state_setup(struct intel_renderstate *so,
 	}
 
 	if (rodata->reloc[reloc_index] != -1) {
-		DRM_ERROR("only %d relocs resolved\n", reloc_index);
+		drm_err(&i915->drm, "only %d relocs resolved\n", reloc_index);
 		goto err;
 	}
 

commit 42d105113018a5c1bd2c86897b1737adf16b3f5f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Dec 2 20:43:14 2019 +0000

    drm/i915: Lift i915_vma_pin() out of intel_renderstate_emit()
    
    Once inside a request, inside the timeline->mutex, pinning is verboten.
    
    <4> [896.032829] ======================================================
    <4> [896.032831] WARNING: possible circular locking dependency detected
    <4> [896.032835] 5.4.0-rc8-CI-Patchwork_15533+ #1 Tainted: G     U
    <4> [896.032838] ------------------------------------------------------
    <4> [896.032841] gem_exec_parall/3720 is trying to acquire lock:
    <4> [896.032844] ffff888401863270 (&kernel#2){+.+.}, at: i915_request_create+0x16/0x1c0 [i915]
    <4> [896.032915]
    but task is already holding lock:
    <4> [896.032917] ffff8883ec1c93c0 (&vm->mutex){+.+.}, at: i915_vma_pin+0xf3/0x11c0 [i915]
    <4> [896.032952]
    which lock already depends on the new lock.
    
    <4> [896.032954]
    the existing dependency chain (in reverse order) is:
    <4> [896.032956]
    -> #1 (&vm->mutex){+.+.}:
    <4> [896.032961]        __mutex_lock+0x9a/0x9d0
    <4> [896.032995]        i915_vma_pin+0xf3/0x11c0 [i915]
    <4> [896.033033]        intel_renderstate_emit+0xb9/0x9e0 [i915]
    <4> [896.033081]        i915_gem_init+0x5a9/0xa50 [i915]
    <4> [896.033112]        i915_driver_probe+0xb00/0x15f0 [i915]
    <4> [896.033144]        i915_pci_probe+0x43/0x1c0 [i915]
    <4> [896.033149]        pci_device_probe+0x9e/0x120
    <4> [896.033154]        really_probe+0xea/0x420
    <4> [896.033158]        driver_probe_device+0x10b/0x120
    <4> [896.033161]        device_driver_attach+0x4a/0x50
    <4> [896.033164]        __driver_attach+0x97/0x130
    <4> [896.033168]        bus_for_each_dev+0x74/0xc0
    <4> [896.033171]        bus_add_driver+0x142/0x220
    <4> [896.033174]        driver_register+0x56/0xf0
    <4> [896.033178]        do_one_initcall+0x58/0x2ff
    <4> [896.033183]        do_init_module+0x56/0x1f8
    <4> [896.033187]        load_module+0x243e/0x29f0
    <4> [896.033190]        __do_sys_finit_module+0xe9/0x110
    <4> [896.033194]        do_syscall_64+0x4f/0x210
    <4> [896.033197]        entry_SYSCALL_64_after_hwframe+0x49/0xbe
    <4> [896.033200]
    -> #0 (&kernel#2){+.+.}:
    <4> [896.033206]        __lock_acquire+0x1328/0x15d0
    <4> [896.033209]        lock_acquire+0xa7/0x1c0
    <4> [896.033213]        __mutex_lock+0x9a/0x9d0
    <4> [896.033255]        i915_request_create+0x16/0x1c0 [i915]
    <4> [896.033287]        intel_engine_flush_barriers+0x4c/0x100 [i915]
    <4> [896.033327]        ggtt_flush+0x37/0x60 [i915]
    <4> [896.033366]        i915_gem_evict_something+0x46b/0x5a0 [i915]
    <4> [896.033407]        i915_gem_gtt_insert+0x21d/0x6a0 [i915]
    <4> [896.033449]        i915_vma_pin+0xb36/0x11c0 [i915]
    <4> [896.033488]        gen6_ppgtt_pin+0xd5/0x170 [i915]
    <4> [896.033523]        ring_context_pin+0x2e/0xc0 [i915]
    <4> [896.033554]        __intel_context_do_pin+0x6b/0x190 [i915]
    <4> [896.033591]        i915_gem_do_execbuffer+0x1814/0x26c0 [i915]
    <4> [896.033627]        i915_gem_execbuffer2_ioctl+0x11b/0x460 [i915]
    <4> [896.033632]        drm_ioctl_kernel+0xa7/0xf0
    <4> [896.033635]        drm_ioctl+0x2e1/0x390
    <4> [896.033638]        do_vfs_ioctl+0xa0/0x6f0
    <4> [896.033641]        ksys_ioctl+0x35/0x60
    <4> [896.033644]        __x64_sys_ioctl+0x11/0x20
    <4> [896.033647]        do_syscall_64+0x4f/0x210
    <4> [896.033650]        entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Lift the object allocation and pin prior to the request construction.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191202204316.2665847-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index c4edc35e7d89..5954ecc3207f 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -29,16 +29,6 @@
 #include "intel_renderstate.h"
 #include "intel_ring.h"
 
-struct intel_renderstate {
-	const struct intel_renderstate_rodata *rodata;
-	struct drm_i915_gem_object *obj;
-	struct i915_vma *vma;
-	u32 batch_offset;
-	u32 batch_size;
-	u32 aux_offset;
-	u32 aux_size;
-};
-
 static const struct intel_renderstate_rodata *
 render_state_get_rodata(const struct intel_engine_cs *engine)
 {
@@ -84,11 +74,11 @@ static int render_state_setup(struct intel_renderstate *so,
 	u32 *d;
 	int ret;
 
-	ret = i915_gem_object_prepare_write(so->obj, &needs_clflush);
+	ret = i915_gem_object_prepare_write(so->vma->obj, &needs_clflush);
 	if (ret)
 		return ret;
 
-	d = kmap_atomic(i915_gem_object_get_dirty_page(so->obj, 0));
+	d = kmap_atomic(i915_gem_object_get_dirty_page(so->vma->obj, 0));
 
 	while (i < rodata->batch_items) {
 		u32 s = rodata->batch[i];
@@ -166,7 +156,7 @@ static int render_state_setup(struct intel_renderstate *so,
 
 	ret = 0;
 out:
-	i915_gem_object_finish_access(so->obj);
+	i915_gem_object_finish_access(so->vma->obj);
 	return ret;
 
 err:
@@ -177,61 +167,84 @@ static int render_state_setup(struct intel_renderstate *so,
 
 #undef OUT_BATCH
 
-int intel_renderstate_emit(struct i915_request *rq)
+int intel_renderstate_init(struct intel_renderstate *so,
+			   struct intel_engine_cs *engine)
 {
-	struct intel_engine_cs *engine = rq->engine;
-	struct intel_renderstate so = {}; /* keep the compiler happy */
+	struct drm_i915_gem_object *obj;
 	int err;
 
-	so.rodata = render_state_get_rodata(engine);
-	if (!so.rodata)
+	memset(so, 0, sizeof(*so));
+
+	so->rodata = render_state_get_rodata(engine);
+	if (!so->rodata)
 		return 0;
 
-	if (so.rodata->batch_items * 4 > PAGE_SIZE)
+	if (so->rodata->batch_items * 4 > PAGE_SIZE)
 		return -EINVAL;
 
-	so.obj = i915_gem_object_create_internal(engine->i915, PAGE_SIZE);
-	if (IS_ERR(so.obj))
-		return PTR_ERR(so.obj);
+	obj = i915_gem_object_create_internal(engine->i915, PAGE_SIZE);
+	if (IS_ERR(obj))
+		return PTR_ERR(obj);
 
-	so.vma = i915_vma_instance(so.obj, &engine->gt->ggtt->vm, NULL);
-	if (IS_ERR(so.vma)) {
-		err = PTR_ERR(so.vma);
+	so->vma = i915_vma_instance(obj, &engine->gt->ggtt->vm, NULL);
+	if (IS_ERR(so->vma)) {
+		err = PTR_ERR(so->vma);
 		goto err_obj;
 	}
 
-	err = i915_vma_pin(so.vma, 0, 0, PIN_GLOBAL | PIN_HIGH);
+	err = i915_vma_pin(so->vma, 0, 0, PIN_GLOBAL | PIN_HIGH);
 	if (err)
 		goto err_vma;
 
-	err = render_state_setup(&so, rq->i915);
+	err = render_state_setup(so, engine->i915);
 	if (err)
 		goto err_unpin;
 
+	return 0;
+
+err_unpin:
+	i915_vma_unpin(so->vma);
+err_vma:
+	i915_vma_close(so->vma);
+err_obj:
+	i915_gem_object_put(obj);
+	so->vma = NULL;
+	return err;
+}
+
+int intel_renderstate_emit(struct intel_renderstate *so,
+			   struct i915_request *rq)
+{
+	struct intel_engine_cs *engine = rq->engine;
+	int err;
+
+	if (!so->vma)
+		return 0;
+
 	err = engine->emit_bb_start(rq,
-				    so.batch_offset, so.batch_size,
+				    so->batch_offset, so->batch_size,
 				    I915_DISPATCH_SECURE);
 	if (err)
-		goto err_unpin;
+		return err;
 
-	if (so.aux_size > 8) {
+	if (so->aux_size > 8) {
 		err = engine->emit_bb_start(rq,
-					    so.aux_offset, so.aux_size,
+					    so->aux_offset, so->aux_size,
 					    I915_DISPATCH_SECURE);
 		if (err)
-			goto err_unpin;
+			return err;
 	}
 
-	i915_vma_lock(so.vma);
-	err = i915_request_await_object(rq, so.vma->obj, false);
+	i915_vma_lock(so->vma);
+	err = i915_request_await_object(rq, so->vma->obj, false);
 	if (err == 0)
-		err = i915_vma_move_to_active(so.vma, rq, 0);
-	i915_vma_unlock(so.vma);
-err_unpin:
-	i915_vma_unpin(so.vma);
-err_vma:
-	i915_vma_close(so.vma);
-err_obj:
-	i915_gem_object_put(so.obj);
+		err = i915_vma_move_to_active(so->vma, rq, 0);
+	i915_vma_unlock(so->vma);
+
 	return err;
 }
+
+void intel_renderstate_fini(struct intel_renderstate *so)
+{
+	i915_vma_unpin_and_release(&so->vma, 0);
+}

commit 2871ea85c119e6fb1127b30f0061436b285d3a2c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 24 11:03:44 2019 +0100

    drm/i915/gt: Split intel_ring_submission
    
    Split the legacy submission backend from the common CS ring buffer
    handling.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191024100344.5041-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index 6d05f9c64178..c4edc35e7d89 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -27,6 +27,7 @@
 
 #include "i915_drv.h"
 #include "intel_renderstate.h"
+#include "intel_ring.h"
 
 struct intel_renderstate {
 	const struct intel_renderstate_rodata *rodata;

commit 70d6894d1456de95a3b8b3c80f6d0714fc04fcec
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Aug 19 12:20:33 2019 +0100

    drm/i915: Serialize against vma moves
    
    Make sure that when submitting requests, we always serialize against
    potential vma moves and clflushes.
    
    Time for a i915_request_await_vma() interface!
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190819112033.30638-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index be37d4501c67..6d05f9c64178 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -222,7 +222,9 @@ int intel_renderstate_emit(struct i915_request *rq)
 	}
 
 	i915_vma_lock(so.vma);
-	err = i915_vma_move_to_active(so.vma, rq, 0);
+	err = i915_request_await_object(rq, so.vma->obj, false);
+	if (err == 0)
+		err = i915_vma_move_to_active(so.vma, rq, 0);
 	i915_vma_unlock(so.vma);
 err_unpin:
 	i915_vma_unpin(so.vma);

commit a56277216637c359e9125b9c16be09f21ed2a395
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jul 29 12:37:20 2019 +0100

    drm/i915: Inline engine->init_context into its caller
    
    We only use the init_context vfunc once while recording the default
    context state, and we use the same sequence in each backend (eliding
    steps that do not apply). Remove the vfunc for simplicity and
    de-duplication.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190729113720.24830-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
index 06a8dc40b19f..be37d4501c67 100644
--- a/drivers/gpu/drm/i915/gt/intel_renderstate.c
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -41,7 +41,7 @@ struct intel_renderstate {
 static const struct intel_renderstate_rodata *
 render_state_get_rodata(const struct intel_engine_cs *engine)
 {
-	if (engine->id != RCS0)
+	if (engine->class != RENDER_CLASS)
 		return NULL;
 
 	switch (INTEL_GEN(engine->i915)) {

commit 2006058e9988421a113e8edc004a8e0eae1a6d3f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jul 4 10:19:25 2019 +0100

    drm/i915: Move the renderstate setup under gt/
    
    The render state is used to initialise the default RCS context, and only
    used during early setup from within the gt code. As such, it makes a
    good candidate for placing within gt/, even if it is not yet entirely
    clean of our GEM heritage.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190704091925.7391-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_renderstate.c b/drivers/gpu/drm/i915/gt/intel_renderstate.c
new file mode 100644
index 000000000000..06a8dc40b19f
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/intel_renderstate.c
@@ -0,0 +1,234 @@
+/*
+ * Copyright © 2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Mika Kuoppala <mika.kuoppala@intel.com>
+ *
+ */
+
+#include "i915_drv.h"
+#include "intel_renderstate.h"
+
+struct intel_renderstate {
+	const struct intel_renderstate_rodata *rodata;
+	struct drm_i915_gem_object *obj;
+	struct i915_vma *vma;
+	u32 batch_offset;
+	u32 batch_size;
+	u32 aux_offset;
+	u32 aux_size;
+};
+
+static const struct intel_renderstate_rodata *
+render_state_get_rodata(const struct intel_engine_cs *engine)
+{
+	if (engine->id != RCS0)
+		return NULL;
+
+	switch (INTEL_GEN(engine->i915)) {
+	case 6:
+		return &gen6_null_state;
+	case 7:
+		return &gen7_null_state;
+	case 8:
+		return &gen8_null_state;
+	case 9:
+		return &gen9_null_state;
+	}
+
+	return NULL;
+}
+
+/*
+ * Macro to add commands to auxiliary batch.
+ * This macro only checks for page overflow before inserting the commands,
+ * this is sufficient as the null state generator makes the final batch
+ * with two passes to build command and state separately. At this point
+ * the size of both are known and it compacts them by relocating the state
+ * right after the commands taking care of alignment so we should sufficient
+ * space below them for adding new commands.
+ */
+#define OUT_BATCH(batch, i, val)				\
+	do {							\
+		if ((i) >= PAGE_SIZE / sizeof(u32))		\
+			goto err;				\
+		(batch)[(i)++] = (val);				\
+	} while(0)
+
+static int render_state_setup(struct intel_renderstate *so,
+			      struct drm_i915_private *i915)
+{
+	const struct intel_renderstate_rodata *rodata = so->rodata;
+	unsigned int i = 0, reloc_index = 0;
+	unsigned int needs_clflush;
+	u32 *d;
+	int ret;
+
+	ret = i915_gem_object_prepare_write(so->obj, &needs_clflush);
+	if (ret)
+		return ret;
+
+	d = kmap_atomic(i915_gem_object_get_dirty_page(so->obj, 0));
+
+	while (i < rodata->batch_items) {
+		u32 s = rodata->batch[i];
+
+		if (i * 4  == rodata->reloc[reloc_index]) {
+			u64 r = s + so->vma->node.start;
+			s = lower_32_bits(r);
+			if (HAS_64BIT_RELOC(i915)) {
+				if (i + 1 >= rodata->batch_items ||
+				    rodata->batch[i + 1] != 0)
+					goto err;
+
+				d[i++] = s;
+				s = upper_32_bits(r);
+			}
+
+			reloc_index++;
+		}
+
+		d[i++] = s;
+	}
+
+	if (rodata->reloc[reloc_index] != -1) {
+		DRM_ERROR("only %d relocs resolved\n", reloc_index);
+		goto err;
+	}
+
+	so->batch_offset = i915_ggtt_offset(so->vma);
+	so->batch_size = rodata->batch_items * sizeof(u32);
+
+	while (i % CACHELINE_DWORDS)
+		OUT_BATCH(d, i, MI_NOOP);
+
+	so->aux_offset = i * sizeof(u32);
+
+	if (HAS_POOLED_EU(i915)) {
+		/*
+		 * We always program 3x6 pool config but depending upon which
+		 * subslice is disabled HW drops down to appropriate config
+		 * shown below.
+		 *
+		 * In the below table 2x6 config always refers to
+		 * fused-down version, native 2x6 is not available and can
+		 * be ignored
+		 *
+		 * SNo  subslices config                eu pool configuration
+		 * -----------------------------------------------------------
+		 * 1    3 subslices enabled (3x6)  -    0x00777000  (9+9)
+		 * 2    ss0 disabled (2x6)         -    0x00777000  (3+9)
+		 * 3    ss1 disabled (2x6)         -    0x00770000  (6+6)
+		 * 4    ss2 disabled (2x6)         -    0x00007000  (9+3)
+		 */
+		u32 eu_pool_config = 0x00777000;
+
+		OUT_BATCH(d, i, GEN9_MEDIA_POOL_STATE);
+		OUT_BATCH(d, i, GEN9_MEDIA_POOL_ENABLE);
+		OUT_BATCH(d, i, eu_pool_config);
+		OUT_BATCH(d, i, 0);
+		OUT_BATCH(d, i, 0);
+		OUT_BATCH(d, i, 0);
+	}
+
+	OUT_BATCH(d, i, MI_BATCH_BUFFER_END);
+	so->aux_size = i * sizeof(u32) - so->aux_offset;
+	so->aux_offset += so->batch_offset;
+	/*
+	 * Since we are sending length, we need to strictly conform to
+	 * all requirements. For Gen2 this must be a multiple of 8.
+	 */
+	so->aux_size = ALIGN(so->aux_size, 8);
+
+	if (needs_clflush)
+		drm_clflush_virt_range(d, i * sizeof(u32));
+	kunmap_atomic(d);
+
+	ret = 0;
+out:
+	i915_gem_object_finish_access(so->obj);
+	return ret;
+
+err:
+	kunmap_atomic(d);
+	ret = -EINVAL;
+	goto out;
+}
+
+#undef OUT_BATCH
+
+int intel_renderstate_emit(struct i915_request *rq)
+{
+	struct intel_engine_cs *engine = rq->engine;
+	struct intel_renderstate so = {}; /* keep the compiler happy */
+	int err;
+
+	so.rodata = render_state_get_rodata(engine);
+	if (!so.rodata)
+		return 0;
+
+	if (so.rodata->batch_items * 4 > PAGE_SIZE)
+		return -EINVAL;
+
+	so.obj = i915_gem_object_create_internal(engine->i915, PAGE_SIZE);
+	if (IS_ERR(so.obj))
+		return PTR_ERR(so.obj);
+
+	so.vma = i915_vma_instance(so.obj, &engine->gt->ggtt->vm, NULL);
+	if (IS_ERR(so.vma)) {
+		err = PTR_ERR(so.vma);
+		goto err_obj;
+	}
+
+	err = i915_vma_pin(so.vma, 0, 0, PIN_GLOBAL | PIN_HIGH);
+	if (err)
+		goto err_vma;
+
+	err = render_state_setup(&so, rq->i915);
+	if (err)
+		goto err_unpin;
+
+	err = engine->emit_bb_start(rq,
+				    so.batch_offset, so.batch_size,
+				    I915_DISPATCH_SECURE);
+	if (err)
+		goto err_unpin;
+
+	if (so.aux_size > 8) {
+		err = engine->emit_bb_start(rq,
+					    so.aux_offset, so.aux_size,
+					    I915_DISPATCH_SECURE);
+		if (err)
+			goto err_unpin;
+	}
+
+	i915_vma_lock(so.vma);
+	err = i915_vma_move_to_active(so.vma, rq, 0);
+	i915_vma_unlock(so.vma);
+err_unpin:
+	i915_vma_unpin(so.vma);
+err_vma:
+	i915_vma_close(so.vma);
+err_obj:
+	i915_gem_object_put(so.obj);
+	return err;
+}
