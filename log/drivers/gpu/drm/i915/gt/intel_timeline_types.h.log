commit e26b6d4341476f6944f57bb78fffcdf332c5948b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Dec 22 12:07:52 2019 +0000

    drm/i915/gt: Pull GT initialisation under intel_gt_init()
    
    Begin pulling the GT setup underneath a single GT umbrella; let intel_gt
    take ownership of its engines! As hinted, the complication is the
    lifetime of the probed engine versus the active lifetime of the GT
    backends. We need to detect the engine layout early and keep it until
    the end so that we can sanitize state on takeover and release.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Acked-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191222120752.1368352-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index 24d040f14e89..02181c5020db 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -15,9 +15,9 @@
 
 #include "i915_active_types.h"
 
-struct drm_i915_private;
 struct i915_vma;
 struct i915_syncmap;
+struct intel_gt;
 struct intel_timeline_hwsp;
 
 struct intel_timeline {

commit 85bedbf191e82aac0d7f05623bccfeccdcd91cea
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Dec 17 01:16:59 2019 +0000

    drm/i915/gt: Eliminate the trylock for reading a timeline's hwsp
    
    As we stash a pointer to the HWSP cacheline on the request, when reading
    it we only need confirm that the cacheline is still valid by checking
    that the request and timeline are still intact.
    
    v2: Protect hwsp_cachline with RCU
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191217011659.3092130-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index aaf15cbe1ce1..24d040f14e89 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -10,14 +10,15 @@
 #include <linux/list.h>
 #include <linux/kref.h>
 #include <linux/mutex.h>
+#include <linux/rcupdate.h>
 #include <linux/types.h>
 
 #include "i915_active_types.h"
 
 struct drm_i915_private;
 struct i915_vma;
-struct intel_timeline_cacheline;
 struct i915_syncmap;
+struct intel_timeline_hwsp;
 
 struct intel_timeline {
 	u64 fence_context;
@@ -87,4 +88,13 @@ struct intel_timeline {
 	struct rcu_head rcu;
 };
 
+struct intel_timeline_cacheline {
+	struct i915_active active;
+
+	struct intel_timeline_hwsp *hwsp;
+	void *vaddr;
+
+	struct rcu_head rcu;
+};
+
 #endif /* __I915_TIMELINE_TYPES_H__ */

commit 4f88f8747fa43c97c3b3712d8d87295ea757cc51
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 25 10:58:58 2019 +0000

    drm/i915/gt: Schedule request retirement when timeline idles
    
    The major drawback of commit 7e34f4e4aad3 ("drm/i915/gen8+: Add RC6 CTX
    corruption WA") is that it disables RC6 while Skylake (and friends) is
    active, and we do not consider the GPU idle until all outstanding
    requests have been retired and the engine switched over to the kernel
    context. If userspace is idle, this task falls onto our background idle
    worker, which only runs roughly once a second, meaning that userspace has
    to have been idle for a couple of seconds before we enable RC6 again.
    Naturally, this causes us to consume considerably more energy than
    before as powersaving is effectively disabled while a display server
    (here's looking at you Xorg) is running.
    
    As execlists will get a completion event as each context is completed,
    we can use this interrupt to queue a retire worker bound to this engine
    to cleanup idle timelines. We will then immediately notice the idle
    engine (without userspace intervention or the aid of the background
    retire worker) and start parking the GPU. Thus during light workloads,
    we will do much more work to idle the GPU faster...  Hopefully with
    commensurate power saving!
    
    v2: Watch context completions and only look at those local to the engine
    when retiring to reduce the amount of excess work we perform.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=112315
    References: 7e34f4e4aad3 ("drm/i915/gen8+: Add RC6 CTX corruption WA")
    References: 2248a28384fe ("drm/i915/gen8+: Add RC6 CTX corruption WA")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191125105858.1718307-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index 5244615ed1cb..aaf15cbe1ce1 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -66,6 +66,9 @@ struct intel_timeline {
 	 */
 	struct i915_active_fence last_request;
 
+	/** A chain of completed timelines ready for early retirement. */
+	struct intel_timeline *retire;
+
 	/**
 	 * We track the most recent seqno that we wait on in every context so
 	 * that we only have to emit a new await and dependency on a more

commit a6edbca74b305adc165e67065d7ee766006e6a48
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 20 16:55:13 2019 +0000

    drm/i915/gt: Close race between engine_park and intel_gt_retire_requests
    
    The general concept was that intel_timeline.active_count was locked by
    the intel_timeline.mutex. The exception was for power management, where
    the engine->kernel_context->timeline could be manipulated under the
    global wakeref.mutex.
    
    This was quite solid, as we always manipulated the timeline only while
    we held an engine wakeref.
    
    And then we started retiring requests outside of struct_mutex, only
    using the timelines.active_list and the timeline->mutex. There we
    started manipulating intel_timeline.active_count outside of an engine
    wakeref, and so introduced a race between __engine_park() and
    intel_gt_retire_requests(), a race that could result in the
    engine->kernel_context not being added to the active timelines and so
    losing requests, which caused us to keep the system permanently powered
    up [and unloadable].
    
    The race would be easy to close if we could take the engine wakeref for
    the timeline before we retire -- except timelines are not bound to any
    engine and so we would need to keep all active engines awake. The
    alternative is to guard intel_timeline_enter/intel_timeline_exit for use
    outside of the timeline->mutex.
    
    Fixes: e5dadff4b093 ("drm/i915: Protect request retirement with timeline->mutex")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191120165514.3955081-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index 98d9ee166379..5244615ed1cb 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -42,7 +42,7 @@ struct intel_timeline {
 	 * from the intel_context caller plus internal atomicity.
 	 */
 	atomic_t pin_count;
-	unsigned int active_count;
+	atomic_t active_count;
 
 	const u32 *hwsp_seqno;
 	struct i915_vma *hwsp_ggtt;

commit b1e3177bd1d8f41e2a9cc847e56a96cdc0eefe62
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:00 2019 +0100

    drm/i915: Coordinate i915_active with its own mutex
    
    Forgo the struct_mutex serialisation for i915_active, and interpose its
    own mutex handling for active/retire.
    
    This is a multi-layered sleight-of-hand. First, we had to ensure that no
    active/retire callbacks accidentally inverted the mutex ordering rules,
    nor assumed that they were themselves serialised by struct_mutex. More
    challenging though, is the rule over updating elements of the active
    rbtree. Instead of the whole i915_active now being serialised by
    struct_mutex, allocations/rotations of the tree are serialised by the
    i915_active.mutex and individual nodes are serialised by the caller
    using the i915_timeline.mutex (we need to use nested spinlocks to
    interact with the dma_fence callback lists).
    
    The pain point here is that instead of a single mutex around execbuf, we
    now have to take a mutex for active tracker (one for each vma, context,
    etc) and a couple of spinlocks for each fence update. The improvement in
    fine grained locking allowing for multiple concurrent clients
    (eventually!) should be worth it in typical loads.
    
    v2: Add some comments that barely elucidate anything :(
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-6-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index c668c4c50e75..98d9ee166379 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -58,12 +58,13 @@ struct intel_timeline {
 	 */
 	struct list_head requests;
 
-	/* Contains an RCU guarded pointer to the last request. No reference is
+	/*
+	 * Contains an RCU guarded pointer to the last request. No reference is
 	 * held to the request, users must carefully acquire a reference to
-	 * the request using i915_active_request_get_request_rcu(), or hold the
-	 * struct_mutex.
+	 * the request using i915_active_fence_get(), or manage the RCU
+	 * protection themselves (cf the i915_active_fence API).
 	 */
-	struct i915_active_request last_request;
+	struct i915_active_fence last_request;
 
 	/**
 	 * We track the most recent seqno that we wait on in every context so

commit d19d71fc2b15bf30ff3e56932eae23ff096c1396
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Sep 19 12:19:10 2019 +0100

    drm/i915: Mark i915_request.timeline as a volatile, rcu pointer
    
    The request->timeline is only valid until the request is retired (i.e.
    before it is completed). Upon retiring the request, the context may be
    unpinned and freed, and along with it the timeline may be freed. We
    therefore need to be very careful when chasing rq->timeline that the
    pointer does not disappear beneath us. The vast majority of users are in
    a protected context, either during request construction or retirement,
    where the timeline->mutex is held and the timeline cannot disappear. It
    is those few off the beaten path (where we access a second timeline) that
    need extra scrutiny -- to be added in the next patch after first adding
    the warnings about dangerous access.
    
    One complication, where we cannot use the timeline->mutex itself, is
    during request submission onto hardware (under spinlocks). Here, we want
    to check on the timeline to finalize the breadcrumb, and so we need to
    impose a second rule to ensure that the request->timeline is indeed
    valid. As we are submitting the request, it's context and timeline must
    be pinned, as it will be used by the hardware. Since it is pinned, we
    know the request->timeline must still be valid, and we cannot submit the
    idle barrier until after we release the engine->active.lock, ergo while
    submitting and holding that spinlock, a second thread cannot release the
    timeline.
    
    v2: Don't be lazy inside selftests; hold the timeline->mutex for as long
    as we need it, and tidy up acquiring the timeline with a bit of
    refactoring (i915_active_add_request)
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190919111912.21631-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index 2b1baf2fcc8e..c668c4c50e75 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -80,6 +80,7 @@ struct intel_timeline {
 	struct intel_gt *gt;
 
 	struct kref kref;
+	struct rcu_head rcu;
 };
 
 #endif /* __I915_TIMELINE_TYPES_H__ */

commit ccb23d2dcc300d3fa176de654610ea82f635915d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 15 21:57:08 2019 +0100

    drm/i915/gt: Guard timeline pinning without relying on struct_mutex
    
    In preparation for removing struct_mutex from around context retirement,
    we need to make timeline pinning and unpinning safe. Since multiple
    engines/contexts can share a single timeline, we cannot rely on
    borrowing the context mutex (otherwise we could state that the timeline
    is only pinned/unpinned inside the context pin/unpin and so guarded by
    it). However, we only perform a sequence of atomic operations inside the
    timeline pin/unpin and the sequence of those operations is safe for a
    concurrent unpin / pin, so we can relax the struct_mutex requirement.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190815205709.24285-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index b1a9f0c54bf0..2b1baf2fcc8e 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -41,7 +41,7 @@ struct intel_timeline {
 	 * but the pin_count is protected by a combination of serialisation
 	 * from the intel_context caller plus internal atomicity.
 	 */
-	unsigned int pin_count;
+	atomic_t pin_count;
 	unsigned int active_count;
 
 	const u32 *hwsp_seqno;

commit 531958f6f357cf7f812974152954484ff77d3964
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 15 21:57:06 2019 +0100

    drm/i915/gt: Track timeline activeness in enter/exit
    
    Lift moving the timeline to/from the active_list on enter/exit in order
    to shorten the active tracking span in comparison to the existing
    pin/unpin.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190815205709.24285-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
index 9a71aea7a338..b1a9f0c54bf0 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -25,7 +25,25 @@ struct intel_timeline {
 
 	struct mutex mutex; /* protects the flow of requests */
 
+	/*
+	 * pin_count and active_count track essentially the same thing:
+	 * How many requests are in flight or may be under construction.
+	 *
+	 * We need two distinct counters so that we can assign different
+	 * lifetimes to the events for different use-cases. For example,
+	 * we want to permanently keep the timeline pinned for the kernel
+	 * context so that we can issue requests at any time without having
+	 * to acquire space in the GGTT. However, we want to keep tracking
+	 * the activity (to be able to detect when we become idle) along that
+	 * permanently pinned timeline and so end up requiring two counters.
+	 *
+	 * Note that the active_count is protected by the intel_timeline.mutex,
+	 * but the pin_count is protected by a combination of serialisation
+	 * from the intel_context caller plus internal atomicity.
+	 */
 	unsigned int pin_count;
+	unsigned int active_count;
+
 	const u32 *hwsp_seqno;
 	struct i915_vma *hwsp_ggtt;
 	u32 hwsp_offset;

commit f0c02c1b91888ccac539388eacb0659bf263a557
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Fri Jun 21 08:08:10 2019 +0100

    drm/i915: Rename i915_timeline to intel_timeline and move under gt
    
    Move all timeline code under gt and rename to intel_gt prefix.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Suggested-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190621070811.7006-32-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_timeline_types.h b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
new file mode 100644
index 000000000000..9a71aea7a338
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/intel_timeline_types.h
@@ -0,0 +1,67 @@
+/*
+ * SPDX-License-Identifier: MIT
+ *
+ * Copyright © 2016 Intel Corporation
+ */
+
+#ifndef __I915_TIMELINE_TYPES_H__
+#define __I915_TIMELINE_TYPES_H__
+
+#include <linux/list.h>
+#include <linux/kref.h>
+#include <linux/mutex.h>
+#include <linux/types.h>
+
+#include "i915_active_types.h"
+
+struct drm_i915_private;
+struct i915_vma;
+struct intel_timeline_cacheline;
+struct i915_syncmap;
+
+struct intel_timeline {
+	u64 fence_context;
+	u32 seqno;
+
+	struct mutex mutex; /* protects the flow of requests */
+
+	unsigned int pin_count;
+	const u32 *hwsp_seqno;
+	struct i915_vma *hwsp_ggtt;
+	u32 hwsp_offset;
+
+	struct intel_timeline_cacheline *hwsp_cacheline;
+
+	bool has_initial_breadcrumb;
+
+	/**
+	 * List of breadcrumbs associated with GPU requests currently
+	 * outstanding.
+	 */
+	struct list_head requests;
+
+	/* Contains an RCU guarded pointer to the last request. No reference is
+	 * held to the request, users must carefully acquire a reference to
+	 * the request using i915_active_request_get_request_rcu(), or hold the
+	 * struct_mutex.
+	 */
+	struct i915_active_request last_request;
+
+	/**
+	 * We track the most recent seqno that we wait on in every context so
+	 * that we only have to emit a new await and dependency on a more
+	 * recent sync point. As the contexts may be executed out-of-order, we
+	 * have to track each individually and can not rely on an absolute
+	 * global_seqno. When we know that all tracked fences are completed
+	 * (i.e. when the driver is idle), we know that the syncmap is
+	 * redundant and we can discard it without loss of generality.
+	 */
+	struct i915_syncmap *sync;
+
+	struct list_head link;
+	struct intel_gt *gt;
+
+	struct kref kref;
+};
+
+#endif /* __I915_TIMELINE_TYPES_H__ */
