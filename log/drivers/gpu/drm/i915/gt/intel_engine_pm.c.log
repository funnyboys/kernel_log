commit 977253df6433f85d5e2cb3ab0f8eb4127f8173dd
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon May 4 19:07:45 2020 +0100

    drm/i915/gt: Stop holding onto the pinned_default_state
    
    As we only restore the default context state upon banning a context, we
    only need enough of the state to run the ring and nothing more. That is
    we only need our bare protocontext.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200504180745.15645-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 811debefebc0..d0a1078ef632 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -21,18 +21,11 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	struct intel_engine_cs *engine =
 		container_of(wf, typeof(*engine), wakeref);
 	struct intel_context *ce;
-	void *map;
 
 	ENGINE_TRACE(engine, "\n");
 
 	intel_gt_pm_get(engine->gt);
 
-	/* Pin the default state for fast resets from atomic context. */
-	map = NULL;
-	if (engine->default_state)
-		map = shmem_pin_map(engine->default_state);
-	engine->pinned_default_state = map;
-
 	/* Discard stale context state from across idling */
 	ce = engine->kernel_context;
 	if (ce) {
@@ -42,6 +35,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 		if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM) && ce->state) {
 			struct drm_i915_gem_object *obj = ce->state->obj;
 			int type = i915_coherent_map_type(engine->i915);
+			void *map;
 
 			map = i915_gem_object_pin_map(obj, type);
 			if (!IS_ERR(map)) {
@@ -260,12 +254,6 @@ static int __engine_park(struct intel_wakeref *wf)
 	if (engine->park)
 		engine->park(engine);
 
-	if (engine->pinned_default_state) {
-		shmem_unpin_map(engine->default_state,
-				engine->pinned_default_state);
-		engine->pinned_default_state = NULL;
-	}
-
 	engine->execlists.no_priolist = false;
 
 	/* While gt calls i915_vma_parked(), we have to break the lock cycle */

commit 16e87459673a5cbef35cc0f2e15c664b10a4cdb6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Apr 30 12:18:12 2020 +0100

    drm/i915/gt: Move the batch buffer pool from the engine to the gt
    
    Since the introduction of 'soft-rc6', we aim to park the device quickly
    and that results in frequent idling of the whole device. Currently upon
    idling we free the batch buffer pool, and so this renders the cache
    ineffective for many workloads. If we want to have an effective cache of
    recently allocated buffers available for reuse, we need to decouple that
    cache from the engine powermanagement and make it timer based. As there
    is no reason then to keep it within the engine (where it once made
    retirement order easier to track), we can move it up the hierarchy to the
    owner of the memory allocations.
    
    v2: Hook up to debugfs/drop_caches to clear the cache on demand.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200430111819.10262-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 446e35ac0224..811debefebc0 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -10,7 +10,6 @@
 #include "intel_engine.h"
 #include "intel_engine_heartbeat.h"
 #include "intel_engine_pm.h"
-#include "intel_engine_pool.h"
 #include "intel_gt.h"
 #include "intel_gt_pm.h"
 #include "intel_rc6.h"
@@ -254,7 +253,6 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	intel_engine_park_heartbeat(engine);
 	intel_engine_disarm_breadcrumbs(engine);
-	intel_engine_pool_park(&engine->pool);
 
 	/* Must be reset upon idling, or we may miss the busy wakeup. */
 	GEM_BUG_ON(engine->execlists.queue_priority_hint != INT_MIN);

commit be1cb55a07bfc528d826962f1e421a44ed5f8311
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 29 18:24:29 2020 +0100

    drm/i915/gt: Keep a no-frills swappable copy of the default context state
    
    We need to keep the default context state around to instantiate new
    contexts (aka golden rendercontext), and we also keep it pinned while
    the engine is active so that we can quickly reset a hanging context.
    However, the default contexts are large enough to merit keeping in
    swappable memory as opposed to kernel memory, so we store them inside
    shmemfs. Currently, we use the normal GEM objects to create the default
    context image, but we can throw away all but the shmemfs file.
    
    This greatly simplifies the tricky power management code which wants to
    run underneath the normal GT locking, and we definitely do not want to
    use any high level objects that may appear to recurse back into the GT.
    Though perhaps the primary advantage of the complex GEM object is that
    we aggressively cache the mapping, but here we are recreating the
    vm_area everytime time we unpark. At the worst, we add a lightweight
    cache, but first find a microbenchmark that is impacted.
    
    Having started to create some utility functions to make working with
    shmemfs objects easier, we can start putting them to wider use, where
    GEM objects are overkill, such as storing persistent error state.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Cc: Ramalingam C <ramalingam.c@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200429172429.6054-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 3be679741d22..446e35ac0224 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -15,6 +15,7 @@
 #include "intel_gt_pm.h"
 #include "intel_rc6.h"
 #include "intel_ring.h"
+#include "shmem_utils.h"
 
 static int __engine_unpark(struct intel_wakeref *wf)
 {
@@ -30,10 +31,8 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	/* Pin the default state for fast resets from atomic context. */
 	map = NULL;
 	if (engine->default_state)
-		map = i915_gem_object_pin_map(engine->default_state,
-					      I915_MAP_WB);
-	if (!IS_ERR_OR_NULL(map))
-		engine->pinned_default_state = map;
+		map = shmem_pin_map(engine->default_state);
+	engine->pinned_default_state = map;
 
 	/* Discard stale context state from across idling */
 	ce = engine->kernel_context;
@@ -264,7 +263,8 @@ static int __engine_park(struct intel_wakeref *wf)
 		engine->park(engine);
 
 	if (engine->pinned_default_state) {
-		i915_gem_object_unpin_map(engine->default_state);
+		shmem_unpin_map(engine->default_state,
+				engine->pinned_default_state);
 		engine->pinned_default_state = NULL;
 	}
 

commit 0d961c4610ff24fdd6d98782b443e27d176aa006
Author: Chen Zhou <chenzhou10@huawei.com>
Date:   Wed Apr 1 10:25:06 2020 +0800

    drm/i915/gt: fix spelling mistake "undeflow" -> "underflow"
    
    There is a spelling mistake in comment, fix it.
    
    Signed-off-by: Chen Zhou <chenzhou10@huawei.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200401022506.52965-1-chenzhou10@huawei.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index b6cf284e3a2d..3be679741d22 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -181,7 +181,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * Ergo, if we put ourselves on the timelines.active_list
 	 * (se intel_timeline_enter()) before we increment the
 	 * engine->wakeref.count, we may see the request completion and retire
-	 * it causing an undeflow of the engine->wakeref.
+	 * it causing an underflow of the engine->wakeref.
 	 */
 	flags = __timeline_mark_lock(ce);
 	GEM_BUG_ON(atomic_read(&ce->timeline->active_count) < 0);

commit bf9cc891d32b1c784bcd4d181321bb279254bdf8
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Jan 22 12:41:54 2020 +0000

    drm/i915/gt: Include a tell-tale for engine parking
    
    We have two trace messages that rely on the function name for
    distinction. However, if gcc inlines the function, the two traces end up
    with the same function name and are indistinguishable. Add a different
    message to each to clarify which one we hit, i.e. which phase of engine
    parking we are processing.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200122124154.483444-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index ea90ab3e396e..b6cf284e3a2d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -112,7 +112,7 @@ __queue_and_release_pm(struct i915_request *rq,
 {
 	struct intel_gt_timelines *timelines = &engine->gt->timelines;
 
-	ENGINE_TRACE(engine, "\n");
+	ENGINE_TRACE(engine, "parking\n");
 
 	/*
 	 * We have to serialise all potential retirement paths with our
@@ -249,7 +249,7 @@ static int __engine_park(struct intel_wakeref *wf)
 	if (!switch_to_kernel_context(engine))
 		return -EBUSY;
 
-	ENGINE_TRACE(engine, "\n");
+	ENGINE_TRACE(engine, "parked\n");
 
 	call_idle_barriers(engine); /* cleanup after wedging */
 

commit 1d0e2c9359fec6212af8c5fb29d5d56269f96968
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jan 2 13:17:07 2020 +0000

    drm/i915/gt: Always poison the kernel_context image before unparking
    
    Keep scrubbing the kernel_context image with poison before we reset it
    in order to demonstrate that we will be resilient in the case where it
    is accidentally overwritten on idle.
    
    Suggested-by: Imre Deak <imre.deak@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200102131707.1463945-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 1b9f73948f22..ea90ab3e396e 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -37,8 +37,24 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 	/* Discard stale context state from across idling */
 	ce = engine->kernel_context;
-	if (ce)
+	if (ce) {
+		GEM_BUG_ON(test_bit(CONTEXT_VALID_BIT, &ce->flags));
+
+		/* First poison the image to verify we never fully trust it */
+		if (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM) && ce->state) {
+			struct drm_i915_gem_object *obj = ce->state->obj;
+			int type = i915_coherent_map_type(engine->i915);
+
+			map = i915_gem_object_pin_map(obj, type);
+			if (!IS_ERR(map)) {
+				memset(map, CONTEXT_REDZONE, obj->base.size);
+				i915_gem_object_flush_map(obj);
+				i915_gem_object_unpin_map(obj);
+			}
+		}
+
 		ce->ops->reset(ce);
+	}
 
 	if (engine->unpark)
 		engine->unpark(engine);

commit fb218f2027048e199a76cccb9a131b2afd48ccbe
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jan 2 13:17:06 2020 +0000

    drm/i915/gt: Discard stale context state from across idling
    
    Before we idle, on parking, we switch to the kernel context such that we
    have a scratch context loaded while the GPU idle, protecting any
    precious user state. Be paranoid and assume that the idle state may have
    been trashed, and reset the kernel_context image after idling.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200102131707.1463945-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index cd82f0baef49..1b9f73948f22 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -20,6 +20,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 {
 	struct intel_engine_cs *engine =
 		container_of(wf, typeof(*engine), wakeref);
+	struct intel_context *ce;
 	void *map;
 
 	ENGINE_TRACE(engine, "\n");
@@ -34,6 +35,11 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	if (!IS_ERR_OR_NULL(map))
 		engine->pinned_default_state = map;
 
+	/* Discard stale context state from across idling */
+	ce = engine->kernel_context;
+	if (ce)
+		ce->ops->reset(ce);
+
 	if (engine->unpark)
 		engine->unpark(engine);
 

commit 45b152f752ea81935ff1f68d28b1914b8cfbe759
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Dec 29 18:31:49 2019 +0000

    drm/i915/gt: Avoid using the GPU before initialisation
    
    Mark the GT as wedged so that we are not tempted to use it prior to
    initialisation.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191229183153.3719869-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 010620b78202..cd82f0baef49 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -123,16 +123,16 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	unsigned long flags;
 	bool result = true;
 
+	/* GPU is pointing to the void, as good as in the kernel context. */
+	if (intel_gt_is_wedged(engine->gt))
+		return true;
+
 	GEM_BUG_ON(!intel_context_is_barrier(ce));
 
 	/* Already inside the kernel context, safe to power down. */
 	if (engine->wakeref_serial == engine->serial)
 		return true;
 
-	/* GPU is pointing to the void, as good as in the kernel context. */
-	if (intel_gt_is_wedged(engine->gt))
-		return true;
-
 	/*
 	 * Note, we do this without taking the timeline->mutex. We cannot
 	 * as we may be called while retiring the kernel context and so

commit e6ba76480299a0d77c51d846f7467b1673aad25b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sat Dec 21 16:03:24 2019 +0000

    drm/i915: Remove i915->kernel_context
    
    Allocate only an internal intel_context for the kernel_context, forgoing
    a global GEM context for internal use as we only require a separate
    address space (for our own protection).
    
    Now having weaned GT from requiring ce->gem_context, we can stop
    referencing it entirely. This also means we no longer have to create random
    and unnecessary GEM contexts for internal use.
    
    GEM contexts are now entirely for tracking GEM clients, and intel_context
    the execution environment on the GPU.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Acked-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191221160324.1073045-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 8fb7b34fc5a6..010620b78202 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -6,6 +6,7 @@
 
 #include "i915_drv.h"
 
+#include "intel_context.h"
 #include "intel_engine.h"
 #include "intel_engine_heartbeat.h"
 #include "intel_engine_pm.h"
@@ -122,6 +123,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	unsigned long flags;
 	bool result = true;
 
+	GEM_BUG_ON(!intel_context_is_barrier(ce));
+
 	/* Already inside the kernel context, safe to power down. */
 	if (engine->wakeref_serial == engine->serial)
 		return true;

commit b81e4d9b594188a8babf06442e1d6f28de78c68f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 19 12:43:53 2019 +0000

    drm/i915/gt: Track engine round-trip times
    
    Knowing the round trip time of an engine is useful for tracking the
    health of the system as well as providing a metric for the baseline
    responsiveness of the engine. We can use the latter metric for
    automatically tuning our waits in selftests and when idling so we don't
    confuse a slower system with a dead one.
    
    Upon idling the engine, we send one last pulse to switch the context
    away from precious user state to the volatile kernel context. We know
    the engine is idle at this point, and the pulse is non-preemptible, so
    this provides us with a good measurement of the round trip time. It also
    provides us with faster engine parking for ringbuffer submission, which
    is a welcome bonus (e.g. softer-rc6).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Stuart Summers <stuart.summers@intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191219105043.4169050-1-chris@chris-wilson.co.uk
    Link: https://patchwork.freedesktop.org/patch/msgid/20191219124353.8607-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index bcbda8e52d41..8fb7b34fc5a6 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -73,6 +73,15 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 
 #endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
 
+static void duration(struct dma_fence *fence, struct dma_fence_cb *cb)
+{
+	struct i915_request *rq = to_request(fence);
+
+	ewma__engine_latency_add(&rq->engine->latency,
+				 ktime_us_delta(rq->fence.timestamp,
+						rq->duration.emitted));
+}
+
 static void
 __queue_and_release_pm(struct i915_request *rq,
 		       struct intel_timeline *tl,
@@ -163,7 +172,18 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 
 	/* Install ourselves as a preemption barrier */
 	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
-	__i915_request_commit(rq);
+	if (likely(!__i915_request_commit(rq))) { /* engine should be idle! */
+		/*
+		 * Use an interrupt for precise measurement of duration,
+		 * otherwise we rely on someone else retiring all the requests
+		 * which may delay the signaling (i.e. we will likely wait
+		 * until the background request retirement running every
+		 * second or two).
+		 */
+		BUILD_BUG_ON(sizeof(rq->duration) > sizeof(rq->submitq));
+		dma_fence_add_callback(&rq->fence, &rq->duration.cb, duration);
+		rq->duration.emitted = ktime_get();
+	}
 
 	/* Expose ourselves to the world */
 	__queue_and_release_pm(rq, ce->timeline, engine);

commit 639f2f24895fb37dd67dfecabd2c74019ed64140
Author: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
Date:   Fri Dec 13 07:51:52 2019 -0800

    drm/i915: Introduce new macros for tracing
    
    New macros ENGINE_TRACE(), CE_TRACE(), RQ_TRACE() and
    GT_TRACE() are introduce to tag device name and engine
    name with contexts and requests tracing in i915.
    
    Cc: Sudeep Dutt <sudeep.dutt@intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Jani Nikula <jani.nikula@intel.com>
    Signed-off-by: Venkata Sandeep Dhanalakota <venkata.s.dhanalakota@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191213155152.69182-2-venkata.s.dhanalakota@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 889eb37e386a..bcbda8e52d41 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -21,7 +21,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 		container_of(wf, typeof(*engine), wakeref);
 	void *map;
 
-	GEM_TRACE("%s\n", engine->name);
+	ENGINE_TRACE(engine, "\n");
 
 	intel_gt_pm_get(engine->gt);
 
@@ -80,7 +80,7 @@ __queue_and_release_pm(struct i915_request *rq,
 {
 	struct intel_gt_timelines *timelines = &engine->gt->timelines;
 
-	GEM_TRACE("%s\n", engine->name);
+	ENGINE_TRACE(engine, "\n");
 
 	/*
 	 * We have to serialise all potential retirement paths with our
@@ -204,7 +204,7 @@ static int __engine_park(struct intel_wakeref *wf)
 	if (!switch_to_kernel_context(engine))
 		return -EBUSY;
 
-	GEM_TRACE("%s\n", engine->name);
+	ENGINE_TRACE(engine, "\n");
 
 	call_idle_barriers(engine); /* cleanup after wedging */
 

commit 023265ed75d8792ca1d555430a8985511d3f8788
Merge: 2b68392e638d e42617b825f8
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Wed Dec 11 10:35:37 2019 +0200

    Merge drm/drm-next into drm-intel-next-queued
    
    Sync up with v5.5-rc1 to get the updated lock_release() API among other
    things. Fix the conflict reported by Stephen Rothwell [1].
    
    [1] http://lore.kernel.org/r/20191210093957.5120f717@canb.auug.org.au
    
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit 7ada90eb9c7ae4a8eb066f8e9b4d80122f0363db
Merge: 9feb1af97e73 9c1867d730a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 6 10:28:09 2019 -0800

    Merge tag 'drm-next-2019-12-06' of git://anongit.freedesktop.org/drm/drm
    
    Pull more drm updates from Dave Airlie:
     "Rob pointed out I missed his pull request for msm-next, it's been in
      next for a while outside of my tree so shouldn't cause any unexpected
      issues, it has some OCMEM support in drivers/soc that is acked by
      other maintainers as it's outside my tree.
    
      Otherwise it's a usual fixes pull, i915, amdgpu, the main ones, with
      some tegra, omap, mgag200 and one core fix.
    
      Summary:
    
      msm-next:
       - OCMEM support for a3xx and a4xx GPUs.
       - a510 support + display support
    
      core:
       - mst payload deletion fix
    
      i915:
       - uapi alignment fix
       - fix for power usage regression due to security fixes
       - change default preemption timeout to 640ms from 100ms
       - EHL voltage level display fixes
       - TGL DGL PHY fix
       - gvt - MI_ATOMIC cmd parser fix, CFL non-priv warning
       - CI spotted deadlock fix
       - EHL port D programming fix
    
      amdgpu:
       - VRAM lost fixes on BACO for CI/VI
       - navi14 DC fixes
       - misc SR-IOV, gfx10 fixes
       - XGMI fixes for arcturus
       - SRIOV fixes
    
      amdkfd:
       - KFD on ppc64le enabled
       - page table optimisations
    
      radeon:
       - fix for r1xx/2xx register checker.
    
      tegra:
       - displayport regression fixes
       - DMA API regression fixes
    
      mgag200:
       - fix devices that can't scanout except at 0 addr
    
      omap:
       - fix dma_addr refcounting"
    
    * tag 'drm-next-2019-12-06' of git://anongit.freedesktop.org/drm/drm: (100 commits)
      drm/dp_mst: Correct the bug in drm_dp_update_payload_part1()
      drm/omap: fix dma_addr refcounting
      drm/tegra: Run hub cleanup on ->remove()
      drm/tegra: sor: Make the +5V HDMI supply optional
      drm/tegra: Silence expected errors on IOMMU attach
      drm/tegra: vic: Export module device table
      drm/tegra: sor: Implement system suspend/resume
      drm/tegra: Use proper IOVA address for cursor image
      drm/tegra: gem: Remove premature import restrictions
      drm/tegra: gem: Properly pin imported buffers
      drm/tegra: hub: Remove bogus connection mutex check
      ia64: agp: Replace empty define with do while
      agp: Add bridge parameter documentation
      agp: remove unused variable num_segments
      agp: move AGPGART_MINOR to include/linux/miscdevice.h
      agp: remove unused variable size in agp_generic_create_gatt_table
      drm/dp_mst: Fix build on systems with STACKTRACE_SUPPORT=n
      drm/radeon: fix r1xx/r2xx register checker for POT textures
      drm/amdgpu: fix GFX10 missing CSIB set(v3)
      drm/amdgpu: should stop GFX ring in hw_fini
      ...

commit a6ed68d6468bd5a3da78a103344ded1435fed57a
Merge: 8c39f71ee201 acc61b892936
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 27 17:45:48 2019 -0800

    Merge tag 'drm-next-2019-11-27' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm updates from Dave Airlie:
     "Lots of stuff in here, though it hasn't been too insane this merge
      apart from dealing with the security fun.
    
      uapi:
       - export different colorspace properties on DP vs HDMI
       - new fourcc for ARM 16x16 block format
       - syncobj: allow querying last submitted timeline value
       - DRM_FORMAT_BIG_ENDIAN defined as unsigned
    
      core:
       - allow using gem vma manager in ttm
       - connector/encoder/bridge doc fixes
       - allow more than 3 encoders for a connector
       - displayport mst suspend/resume reprobing support
       - vram lazy unmapping, uniform vram mm and gem vram
       - edid cleanups + AVI informframe bar info
       - displayport helpers - dpcd parser added
    
      dp_cec:
       - Allow a connector to be associated with a cec device
    
      ttm:
       - pipelining with no_gpu_wait fix
       - always keep BOs on the LRU
    
      sched:
       - allow free_job routine to sleep
    
      i915:
       - Block userptr from mappable GTT
       - i915 perf uapi versioning
       - OA stream dynamic reconfiguration
       - make context persistence optional
       - introduce DRM_I915_UNSTABLE Kconfig
       - add fake lmem testing under unstable
       - BT.2020 support for DP MSA
       - struct mutex elimination
       - Tigerlake display/PLL/power management improvements
       - Jasper Lake PCH support
       - refactor PMU for multiple GPUs
       - Icelake firmware update
       - Split out vga + switcheroo code
    
      amdgpu:
       - implement dma-buf import/export without helpers
       - vega20 RAS enablement
       - DC i2c over aux fixes
       - renoir GPU reset
       - DC HDCP support
       - BACO support for CI/VI asics
       - MSI-X support
       - Arcturus EEPROM support
       - Arcturus VCN encode support
       - VCN dynamic powergating on RV/RV2
    
      amdkfd:
       - add navi12/14/renoir support to kfd
    
      radeon:
       - SI dpm fix ported from amdgpu
       - fix bad DMA on ppc platforms
    
      gma500:
       - memory leak fixes
    
      qxl:
       - convert to new gem mmap
    
      exynos:
       - build warning fix
    
      komeda:
       - add aclk sysfs attribute
    
      v3d:
       - userspace cleanup uapi change
    
      i810:
       - fix for underflow in dispatch ioctls
    
      ast:
       - refactor show_cursor
    
      mgag200:
       - refactor show_cursor
    
      arcgpu:
       - encoder finding improvements
    
      mediatek:
       - mipi_tx, dsi and partial crtc support for MT8183 SoC
       - rotation support
    
      meson:
       - add suspend/resume support
    
      omap:
       - misc refactors
    
      tegra:
       - DisplayPort support for Tegra 210, 186 and 194.
       - IOMMU-backed DMA API fixes
    
      panfrost:
       - fix lockdep issue
       - simplify devfreq integration
    
      rcar-du:
       - R8A774B1 SoC support
       - fixes for H2 ES2.0
    
      sun4i:
       - vcc-dsi regulator support
    
      virtio-gpu:
       - vmexit vs spinlock fix
       - move to gem shmem helpers
       - handle large command buffers with cma"
    
    * tag 'drm-next-2019-11-27' of git://anongit.freedesktop.org/drm/drm: (1855 commits)
      drm/amdgpu: invalidate mmhub semaphore workaround in gmc9/gmc10
      drm/amdgpu: initialize vm_inv_eng0_sem for gfxhub and mmhub
      drm/amd/amdgpu/sriov skip RLCG s/r list for arcturus VF.
      drm/amd/amdgpu/sriov temporarily skip ras,dtm,hdcp for arcturus VF
      drm/amdgpu/gfx10: re-init clear state buffer after gpu reset
      merge fix for "ftrace: Rework event_create_dir()"
      drm/amdgpu: Update Arcturus golden registers
      drm/amdgpu/gfx10: fix out-of-bound mqd_backup array access
      drm/amdgpu/gfx10: explicitly wait for cp idle after halt/unhalt
      Revert "drm/amd/display: enable S/G for RAVEN chip"
      drm/amdgpu: disable gfxoff on original raven
      drm/amdgpu: remove experimental flag for Navi14
      drm/amdgpu: disable gfxoff when using register read interface
      drm/amdgpu/powerplay: properly set PP_GFXOFF_MASK (v2)
      drm/amdgpu: fix bad DMA from INTERRUPT_CNTL2
      drm/radeon: fix bad DMA from INTERRUPT_CNTL2
      drm/amd/display: Fix debugfs on MST connectors
      drm/amdgpu/nv: add asic func for fetching vbios from rom directly
      drm/amdgpu: put flush_delayed_work at first
      drm/amdgpu/vcn2.5: fix the enc loop with hw fini
      ...

commit df9f85d8582ebda052835c55ae940e4f866e1ef5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 27 13:45:27 2019 +0000

    drm/i915: Serialise i915_active_fence_set() with itself
    
    The expected downside to commit 58b4c1a07ada ("drm/i915: Reduce nested
    prepare_remote_context() to a trylock") was that it would need to return
    -EAGAIN to userspace in order to resolve potential mutex inversion. Such
    an unsightly round trip is unnecessary if we could atomically insert a
    barrier into the i915_active_fence, so make it happen.
    
    Currently, we use the timeline->mutex (or some other named outer lock)
    to order insertion into the i915_active_fence (and so individual nodes
    of i915_active). Inside __i915_active_fence_set, we only need then
    serialise with the interrupt handler in order to claim the timeline for
    ourselves.
    
    However, if we remove the outer lock, we need to ensure the order is
    intact between not only multiple threads trying to insert themselves
    into the timeline, but also with the interrupt handler completing the
    previous occupant. We use xchg() on insert so that we have an ordered
    sequence of insertions (and each caller knows the previous fence on
    which to wait, preserving the chain of all fences in the timeline), but
    we then have to cmpxchg() in the interrupt handler to avoid overwriting
    the new occupant. The only nasty side-effect is having to temporarily
    strip off the RCU-annotations to apply the atomic operations, otherwise
    the rules are much more conventional!
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=112402
    Fixes: 58b4c1a07ada ("drm/i915: Reduce nested prepare_remote_context() to a trylock")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191127134527.3438410-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 0e1ad4a4bd97..21183ad87368 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -183,7 +183,7 @@ static void call_idle_barriers(struct intel_engine_cs *engine)
 			container_of((struct list_head *)node,
 				     typeof(*cb), node);
 
-		cb->func(NULL, cb);
+		cb->func(ERR_PTR(-EAGAIN), cb);
 	}
 }
 

commit a09c2860ae4fcf4d7e25202661f3eb868547cddb
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 25 10:58:57 2019 +0000

    drm/i915/gt: Adapt engine_park synchronisation rules for engine_retire
    
    In the next patch, we will introduce a new asynchronous retirement
    worker, fed by execlists CS events. Here we may queue a retirement as
    soon as a request is submitted to HW (and completes instantly), and we
    also want to process that retirement as early as possible and cannot
    afford to postpone (as there may not be another opportunity to retire it
    for a few seconds). To allow the new async retirer to run in parallel
    with our submission, pull the __i915_request_queue (that passes the
    request to HW) inside the timelines spinlock so that the retirement
    cannot release the timeline before we have completed the submission.
    
    v2: Actually to play nicely with engine_retire, we have to raise the
    timeline.active_lock before releasing the HW. intel_gt_retire_requsts()
    is still serialised by the outer lock so they cannot see this
    intermediate state, and engine_retire is serialised by HW submission.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191125105858.1718307-2-chris@chris-wilson.co.uk
    (cherry picked from commit 88a4655e75ac8f55eea5e3f38b176cba9cf653b5)
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 373a4b9f159c..0e1ad4a4bd97 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -74,16 +74,33 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 #endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
 
 static void
-__intel_timeline_enter_and_release_pm(struct intel_timeline *tl,
-				      struct intel_engine_cs *engine)
+__queue_and_release_pm(struct i915_request *rq,
+		       struct intel_timeline *tl,
+		       struct intel_engine_cs *engine)
 {
 	struct intel_gt_timelines *timelines = &engine->gt->timelines;
 
+	GEM_TRACE("%s\n", engine->name);
+
+	/*
+	 * We have to serialise all potential retirement paths with our
+	 * submission, as we don't want to underflow either the
+	 * engine->wakeref.counter or our timeline->active_count.
+	 *
+	 * Equally, we cannot allow a new submission to start until
+	 * after we finish queueing, nor could we allow that submitter
+	 * to retire us before we are ready!
+	 */
 	spin_lock(&timelines->lock);
 
+	/* Let intel_gt_retire_requests() retire us (acquired under lock) */
 	if (!atomic_fetch_inc(&tl->active_count))
 		list_add_tail(&tl->link, &timelines->active_list);
 
+	/* Hand the request over to HW and so engine_retire() */
+	__i915_request_queue(rq, NULL);
+
+	/* Let new submissions commence (and maybe retire this timeline) */
 	__intel_wakeref_defer_park(&engine->wakeref);
 
 	spin_unlock(&timelines->lock);
@@ -148,10 +165,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
 	__i915_request_commit(rq);
 
-	__i915_request_queue(rq, NULL);
-
-	/* Expose ourselves to intel_gt_retire_requests() and new submission */
-	__intel_timeline_enter_and_release_pm(ce->timeline, engine);
+	/* Expose ourselves to the world */
+	__queue_and_release_pm(rq, ce->timeline, engine);
 
 	result = false;
 out_unlock:

commit bf201f5eda23eb57f7217d20ea3e18da8cbcf52b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 20 16:55:14 2019 +0000

    drm/i915/gt: Unlock engine-pm after queuing the kernel context switch
    
    In commit a79ca656b648 ("drm/i915: Push the wakeref->count deferral to
    the backend"), I erroneously concluded that we last modify the engine
    inside __i915_request_commit() meaning that we could enable concurrent
    submission for userspace as we enqueued this request. However, this
    falls into a trap with other users of the engine->kernel_context waking
    up and submitting their request before the idle-switch is queued, with
    the result that the kernel_context is executed out-of-sequence most
    likely upsetting the GPU and certainly ourselves when we try to retire
    the out-of-sequence requests.
    
    As such we need to hold onto the effective engine->kernel_context mutex
    lock (via the engine pm mutex proxy) until we have finish queuing the
    request to the engine.
    
    v2: Serialise against concurrent intel_gt_retire_requests()
    v3: Describe the hairy locking scheme with intel_gt_retire_requests()
    for future reference.
    v4: Combine timeline->lock and engine pm release; it's hairy.
    
    Fixes: a79ca656b648 ("drm/i915: Push the wakeref->count deferral to the backend")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191120165514.3955081-2-chris@chris-wilson.co.uk
    (cherry picked from commit 5cba288466e9b229feb68295675246e7522fb5eb)
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 7269c87c1372..373a4b9f159c 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -73,8 +73,25 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 
 #endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
 
+static void
+__intel_timeline_enter_and_release_pm(struct intel_timeline *tl,
+				      struct intel_engine_cs *engine)
+{
+	struct intel_gt_timelines *timelines = &engine->gt->timelines;
+
+	spin_lock(&timelines->lock);
+
+	if (!atomic_fetch_inc(&tl->active_count))
+		list_add_tail(&tl->link, &timelines->active_list);
+
+	__intel_wakeref_defer_park(&engine->wakeref);
+
+	spin_unlock(&timelines->lock);
+}
+
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
+	struct intel_context *ce = engine->kernel_context;
 	struct i915_request *rq;
 	unsigned long flags;
 	bool result = true;
@@ -98,16 +115,31 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * This should hold true as we can only park the engine after
 	 * retiring the last request, thus all rings should be empty and
 	 * all timelines idle.
+	 *
+	 * For unlocking, there are 2 other parties and the GPU who have a
+	 * stake here.
+	 *
+	 * A new gpu user will be waiting on the engine-pm to start their
+	 * engine_unpark. New waiters are predicated on engine->wakeref.count
+	 * and so intel_wakeref_defer_park() acts like a mutex_unlock of the
+	 * engine->wakeref.
+	 *
+	 * The other party is intel_gt_retire_requests(), which is walking the
+	 * list of active timelines looking for completions. Meanwhile as soon
+	 * as we call __i915_request_queue(), the GPU may complete our request.
+	 * Ergo, if we put ourselves on the timelines.active_list
+	 * (se intel_timeline_enter()) before we increment the
+	 * engine->wakeref.count, we may see the request completion and retire
+	 * it causing an undeflow of the engine->wakeref.
 	 */
-	flags = __timeline_mark_lock(engine->kernel_context);
+	flags = __timeline_mark_lock(ce);
+	GEM_BUG_ON(atomic_read(&ce->timeline->active_count) < 0);
 
-	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
+	rq = __i915_request_create(ce, GFP_NOWAIT);
 	if (IS_ERR(rq))
 		/* Context switch failed, hope for the best! Maybe reset? */
 		goto out_unlock;
 
-	intel_timeline_enter(i915_request_timeline(rq));
-
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
 	i915_request_add_active_barriers(rq);
@@ -116,13 +148,14 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
 	__i915_request_commit(rq);
 
-	/* Release our exclusive hold on the engine */
-	__intel_wakeref_defer_park(&engine->wakeref);
 	__i915_request_queue(rq, NULL);
 
+	/* Expose ourselves to intel_gt_retire_requests() and new submission */
+	__intel_timeline_enter_and_release_pm(ce->timeline, engine);
+
 	result = false;
 out_unlock:
-	__timeline_mark_unlock(engine->kernel_context, flags);
+	__timeline_mark_unlock(ce, flags);
 	return result;
 }
 

commit ee33baa83109612d9a31fc2c2a7b74967767b358
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 20 12:54:33 2019 +0000

    drm/i915: Mark up the calling context for intel_wakeref_put()
    
    Previously, we assumed we could use mutex_trylock() within an atomic
    context, falling back to a worker if contended. However, such trickery
    is illegal inside interrupt context, and so we need to always use a
    worker under such circumstances. As we normally are in process context,
    we can typically use a plain mutex, and only defer to a work when we
    know we are being called from an interrupt path.
    
    Fixes: 51fbd8de87dc ("drm/i915/pmu: Atomically acquire the gt_pm wakeref")
    References: a0855d24fc22d ("locking/mutex: Complain upon mutex API misuse in IRQ contexts")
    References: https://bugs.freedesktop.org/show_bug.cgi?id=111626
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191120125433.3767149-1-chris@chris-wilson.co.uk
    (cherry picked from commit 07779a76ee1f93f930cf697b22be73d16e14f50c)
    Signed-off-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 3c0f490ff2c7..7269c87c1372 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -177,7 +177,8 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	engine->execlists.no_priolist = false;
 
-	intel_gt_pm_put(engine->gt);
+	/* While gt calls i915_vma_parked(), we have to break the lock cycle */
+	intel_gt_pm_put_async(engine->gt);
 	return 0;
 }
 

commit 88a4655e75ac8f55eea5e3f38b176cba9cf653b5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 25 10:58:57 2019 +0000

    drm/i915/gt: Adapt engine_park synchronisation rules for engine_retire
    
    In the next patch, we will introduce a new asynchronous retirement
    worker, fed by execlists CS events. Here we may queue a retirement as
    soon as a request is submitted to HW (and completes instantly), and we
    also want to process that retirement as early as possible and cannot
    afford to postpone (as there may not be another opportunity to retire it
    for a few seconds). To allow the new async retirer to run in parallel
    with our submission, pull the __i915_request_queue (that passes the
    request to HW) inside the timelines spinlock so that the retirement
    cannot release the timeline before we have completed the submission.
    
    v2: Actually to play nicely with engine_retire, we have to raise the
    timeline.active_lock before releasing the HW. intel_gt_retire_requsts()
    is still serialised by the outer lock so they cannot see this
    intermediate state, and engine_retire is serialised by HW submission.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191125105858.1718307-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 373a4b9f159c..0e1ad4a4bd97 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -74,16 +74,33 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 #endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
 
 static void
-__intel_timeline_enter_and_release_pm(struct intel_timeline *tl,
-				      struct intel_engine_cs *engine)
+__queue_and_release_pm(struct i915_request *rq,
+		       struct intel_timeline *tl,
+		       struct intel_engine_cs *engine)
 {
 	struct intel_gt_timelines *timelines = &engine->gt->timelines;
 
+	GEM_TRACE("%s\n", engine->name);
+
+	/*
+	 * We have to serialise all potential retirement paths with our
+	 * submission, as we don't want to underflow either the
+	 * engine->wakeref.counter or our timeline->active_count.
+	 *
+	 * Equally, we cannot allow a new submission to start until
+	 * after we finish queueing, nor could we allow that submitter
+	 * to retire us before we are ready!
+	 */
 	spin_lock(&timelines->lock);
 
+	/* Let intel_gt_retire_requests() retire us (acquired under lock) */
 	if (!atomic_fetch_inc(&tl->active_count))
 		list_add_tail(&tl->link, &timelines->active_list);
 
+	/* Hand the request over to HW and so engine_retire() */
+	__i915_request_queue(rq, NULL);
+
+	/* Let new submissions commence (and maybe retire this timeline) */
 	__intel_wakeref_defer_park(&engine->wakeref);
 
 	spin_unlock(&timelines->lock);
@@ -148,10 +165,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
 	__i915_request_commit(rq);
 
-	__i915_request_queue(rq, NULL);
-
-	/* Expose ourselves to intel_gt_retire_requests() and new submission */
-	__intel_timeline_enter_and_release_pm(ce->timeline, engine);
+	/* Expose ourselves to the world */
+	__queue_and_release_pm(rq, ce->timeline, engine);
 
 	result = false;
 out_unlock:

commit 5cba288466e9b229feb68295675246e7522fb5eb
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 20 16:55:14 2019 +0000

    drm/i915/gt: Unlock engine-pm after queuing the kernel context switch
    
    In commit a79ca656b648 ("drm/i915: Push the wakeref->count deferral to
    the backend"), I erroneously concluded that we last modify the engine
    inside __i915_request_commit() meaning that we could enable concurrent
    submission for userspace as we enqueued this request. However, this
    falls into a trap with other users of the engine->kernel_context waking
    up and submitting their request before the idle-switch is queued, with
    the result that the kernel_context is executed out-of-sequence most
    likely upsetting the GPU and certainly ourselves when we try to retire
    the out-of-sequence requests.
    
    As such we need to hold onto the effective engine->kernel_context mutex
    lock (via the engine pm mutex proxy) until we have finish queuing the
    request to the engine.
    
    v2: Serialise against concurrent intel_gt_retire_requests()
    v3: Describe the hairy locking scheme with intel_gt_retire_requests()
    for future reference.
    v4: Combine timeline->lock and engine pm release; it's hairy.
    
    Fixes: a79ca656b648 ("drm/i915: Push the wakeref->count deferral to the backend")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191120165514.3955081-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 7269c87c1372..373a4b9f159c 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -73,8 +73,25 @@ static inline void __timeline_mark_unlock(struct intel_context *ce,
 
 #endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
 
+static void
+__intel_timeline_enter_and_release_pm(struct intel_timeline *tl,
+				      struct intel_engine_cs *engine)
+{
+	struct intel_gt_timelines *timelines = &engine->gt->timelines;
+
+	spin_lock(&timelines->lock);
+
+	if (!atomic_fetch_inc(&tl->active_count))
+		list_add_tail(&tl->link, &timelines->active_list);
+
+	__intel_wakeref_defer_park(&engine->wakeref);
+
+	spin_unlock(&timelines->lock);
+}
+
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
+	struct intel_context *ce = engine->kernel_context;
 	struct i915_request *rq;
 	unsigned long flags;
 	bool result = true;
@@ -98,16 +115,31 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * This should hold true as we can only park the engine after
 	 * retiring the last request, thus all rings should be empty and
 	 * all timelines idle.
+	 *
+	 * For unlocking, there are 2 other parties and the GPU who have a
+	 * stake here.
+	 *
+	 * A new gpu user will be waiting on the engine-pm to start their
+	 * engine_unpark. New waiters are predicated on engine->wakeref.count
+	 * and so intel_wakeref_defer_park() acts like a mutex_unlock of the
+	 * engine->wakeref.
+	 *
+	 * The other party is intel_gt_retire_requests(), which is walking the
+	 * list of active timelines looking for completions. Meanwhile as soon
+	 * as we call __i915_request_queue(), the GPU may complete our request.
+	 * Ergo, if we put ourselves on the timelines.active_list
+	 * (se intel_timeline_enter()) before we increment the
+	 * engine->wakeref.count, we may see the request completion and retire
+	 * it causing an undeflow of the engine->wakeref.
 	 */
-	flags = __timeline_mark_lock(engine->kernel_context);
+	flags = __timeline_mark_lock(ce);
+	GEM_BUG_ON(atomic_read(&ce->timeline->active_count) < 0);
 
-	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
+	rq = __i915_request_create(ce, GFP_NOWAIT);
 	if (IS_ERR(rq))
 		/* Context switch failed, hope for the best! Maybe reset? */
 		goto out_unlock;
 
-	intel_timeline_enter(i915_request_timeline(rq));
-
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
 	i915_request_add_active_barriers(rq);
@@ -116,13 +148,14 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
 	__i915_request_commit(rq);
 
-	/* Release our exclusive hold on the engine */
-	__intel_wakeref_defer_park(&engine->wakeref);
 	__i915_request_queue(rq, NULL);
 
+	/* Expose ourselves to intel_gt_retire_requests() and new submission */
+	__intel_timeline_enter_and_release_pm(ce->timeline, engine);
+
 	result = false;
 out_unlock:
-	__timeline_mark_unlock(engine->kernel_context, flags);
+	__timeline_mark_unlock(ce, flags);
 	return result;
 }
 

commit 07779a76ee1f93f930cf697b22be73d16e14f50c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Nov 20 12:54:33 2019 +0000

    drm/i915: Mark up the calling context for intel_wakeref_put()
    
    Previously, we assumed we could use mutex_trylock() within an atomic
    context, falling back to a worker if contended. However, such trickery
    is illegal inside interrupt context, and so we need to always use a
    worker under such circumstances. As we normally are in process context,
    we can typically use a plain mutex, and only defer to a work when we
    know we are being called from an interrupt path.
    
    Fixes: 51fbd8de87dc ("drm/i915/pmu: Atomically acquire the gt_pm wakeref")
    References: a0855d24fc22d ("locking/mutex: Complain upon mutex API misuse in IRQ contexts")
    References: https://bugs.freedesktop.org/show_bug.cgi?id=111626
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191120125433.3767149-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 3c0f490ff2c7..7269c87c1372 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -177,7 +177,8 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	engine->execlists.no_priolist = false;
 
-	intel_gt_pm_put(engine->gt);
+	/* While gt calls i915_vma_parked(), we have to break the lock cycle */
+	intel_gt_pm_put_async(engine->gt);
 	return 0;
 }
 

commit 2871ea85c119e6fb1127b30f0061436b285d3a2c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 24 11:03:44 2019 +0100

    drm/i915/gt: Split intel_ring_submission
    
    Split the legacy submission backend from the common CS ring buffer
    handling.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191024100344.5041-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 6fbfa2162e54..3c0f490ff2c7 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -13,6 +13,7 @@
 #include "intel_gt.h"
 #include "intel_gt_pm.h"
 #include "intel_rc6.h"
+#include "intel_ring.h"
 
 static int __engine_unpark(struct intel_wakeref *wf)
 {

commit 058179e72e0956a2dfe4927db6cbe5fbfb2406aa
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 23 14:31:08 2019 +0100

    drm/i915/gt: Replace hangcheck by heartbeats
    
    Replace sampling the engine state every so often with a periodic
    heartbeat request to measure the health of an engine. This is coupled
    with the forced-preemption to allow long running requests to survive so
    long as they do not block other users.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Jon Bloomfield <jon.bloomfield@intel.com>
    Reviewed-by: Jon Bloomfield <jon.bloomfield@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191023133108.21401-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 7d76611d9df1..6fbfa2162e54 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -7,6 +7,7 @@
 #include "i915_drv.h"
 
 #include "intel_engine.h"
+#include "intel_engine_heartbeat.h"
 #include "intel_engine_pm.h"
 #include "intel_engine_pool.h"
 #include "intel_gt.h"
@@ -34,7 +35,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	if (engine->unpark)
 		engine->unpark(engine);
 
-	intel_engine_init_hangcheck(engine);
+	intel_engine_unpark_heartbeat(engine);
 	return 0;
 }
 
@@ -158,6 +159,7 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	call_idle_barriers(engine); /* cleanup after wedging */
 
+	intel_engine_park_heartbeat(engine);
 	intel_engine_disarm_breadcrumbs(engine);
 	intel_engine_pool_park(&engine->pool);
 
@@ -188,6 +190,7 @@ void intel_engine_init__pm(struct intel_engine_cs *engine)
 	struct intel_runtime_pm *rpm = engine->uncore->rpm;
 
 	intel_wakeref_init(&engine->wakeref, rpm, &wf_ops);
+	intel_engine_init_heartbeat(engine);
 }
 
 #if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)

commit b5e8e954eb672e7128e50589c43817b0ffce6f75
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Oct 21 18:43:39 2019 +0100

    drm/i915/gt: Introduce barrier pulses along engines
    
    To flush idle barriers, and even inflight requests, we want to send a
    preemptive 'pulse' along an engine. We use a no-op request along the
    pinned kernel_context at high priority so that it should run or else
    kick off the stuck requests. We can use this to ensure idle barriers are
    immediately flushed, as part of a context cancellation mechanism, or as
    part of a heartbeat mechanism to detect and reset a stuck GPU.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191021174339.5389-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 67eb6183648a..7d76611d9df1 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -111,7 +111,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	i915_request_add_active_barriers(rq);
 
 	/* Install ourselves as a preemption barrier */
-	rq->sched.attr.priority = I915_PRIORITY_UNPREEMPTABLE;
+	rq->sched.attr.priority = I915_PRIORITY_BARRIER;
 	__i915_request_commit(rq);
 
 	/* Release our exclusive hold on the engine */

commit 5facae4f3549b5cf7c0e10ec312a65ffd43b5726
Author: Qian Cai <cai@lca.pw>
Date:   Thu Sep 19 12:09:40 2019 -0400

    locking/lockdep: Remove unused @nested argument from lock_release()
    
    Since the following commit:
    
      b4adfe8e05f1 ("locking/lockdep: Remove unused argument in __lock_release")
    
    @nested is no longer used in lock_release(), so remove it from all
    lock_release() calls and friends.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: alexander.levin@microsoft.com
    Cc: daniel@iogearbox.net
    Cc: davem@davemloft.net
    Cc: dri-devel@lists.freedesktop.org
    Cc: duyuyang@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: hannes@cmpxchg.org
    Cc: intel-gfx@lists.freedesktop.org
    Cc: jack@suse.com
    Cc: jlbec@evilplan.or
    Cc: joonas.lahtinen@linux.intel.com
    Cc: joseph.qi@linux.alibaba.com
    Cc: jslaby@suse.com
    Cc: juri.lelli@redhat.com
    Cc: maarten.lankhorst@linux.intel.com
    Cc: mark@fasheh.com
    Cc: mhocko@kernel.org
    Cc: mripard@kernel.org
    Cc: ocfs2-devel@oss.oracle.com
    Cc: rodrigo.vivi@intel.com
    Cc: sean@poorly.run
    Cc: st@kernel.org
    Cc: tj@kernel.org
    Cc: tytso@mit.edu
    Cc: vdavydov.dev@gmail.com
    Cc: vincent.guittot@linaro.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1568909380-32199-1-git-send-email-cai@lca.pw
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 65b5ca74b394..7f647243b3b9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -52,7 +52,7 @@ static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 static inline void __timeline_mark_unlock(struct intel_context *ce,
 					  unsigned long flags)
 {
-	mutex_release(&ce->timeline->mutex.dep_map, 0, _THIS_IP_);
+	mutex_release(&ce->timeline->mutex.dep_map, _THIS_IP_);
 	local_irq_restore(flags);
 }
 

commit cd6a851385be878f086204aa0e7cd3bec9ea909f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Oct 7 16:45:31 2019 +0100

    drm/i915/gt: Prefer local path to runtime powermanagement
    
    Avoid going to the base i915 device when we already have a path from gt
    to the runtime powermanagement interface. The benefit is that it looks a
    bit more self-consistent to always be acquiring the gt->uncore->rpm for
    use with the gt->uncore.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191007154531.1750-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 8e5e513eddc9..67eb6183648a 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -185,7 +185,7 @@ static const struct intel_wakeref_ops wf_ops = {
 
 void intel_engine_init__pm(struct intel_engine_cs *engine)
 {
-	struct intel_runtime_pm *rpm = &engine->i915->runtime_pm;
+	struct intel_runtime_pm *rpm = engine->uncore->rpm;
 
 	intel_wakeref_init(&engine->wakeref, rpm, &wf_ops);
 }

commit b72348406927740d4cfb55f2c1e19c6769ffc666
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:01 2019 +0100

    drm/i915: Move idle barrier cleanup into engine-pm
    
    Now that we now longer need to guarantee that the active callback is
    under the struct_mutex, we can lift it out of the i915_gem_park() and
    into the engine parking itself.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-7-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 66fc49b76ea8..8e5e513eddc9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -124,6 +124,19 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	return result;
 }
 
+static void call_idle_barriers(struct intel_engine_cs *engine)
+{
+	struct llist_node *node, *next;
+
+	llist_for_each_safe(node, next, llist_del_all(&engine->barrier_tasks)) {
+		struct dma_fence_cb *cb =
+			container_of((struct list_head *)node,
+				     typeof(*cb), node);
+
+		cb->func(NULL, cb);
+	}
+}
+
 static int __engine_park(struct intel_wakeref *wf)
 {
 	struct intel_engine_cs *engine =
@@ -143,6 +156,8 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	GEM_TRACE("%s\n", engine->name);
 
+	call_idle_barriers(engine); /* cleanup after wedging */
+
 	intel_engine_disarm_breadcrumbs(engine);
 	intel_engine_pool_park(&engine->pool);
 

commit c113236718e89561f309705f1a78126b3df93a21
Author: Andi Shyti <andi.shyti@intel.com>
Date:   Fri Sep 27 12:08:49 2019 +0100

    drm/i915: Extract GT render sleep (rc6) management
    
    Continuing the theme of breaking intel_pm.c up in a reasonable chunk of
    powermanagement utilities, pull out the rc6 setup into its GT handler.
    
    Based on a patch by Chris Wilson.
    
    Signed-off-by: Andi Shyti <andi.shyti@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190919143840.20384-1-andi.shyti@intel.com
    Link: https://patchwork.freedesktop.org/patch/msgid/20190927110849.28734-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index ce61c83d68e9..66fc49b76ea8 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -11,6 +11,7 @@
 #include "intel_engine_pool.h"
 #include "intel_gt.h"
 #include "intel_gt_pm.h"
+#include "intel_rc6.h"
 
 static int __engine_unpark(struct intel_wakeref *wf)
 {

commit d19d71fc2b15bf30ff3e56932eae23ff096c1396
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Sep 19 12:19:10 2019 +0100

    drm/i915: Mark i915_request.timeline as a volatile, rcu pointer
    
    The request->timeline is only valid until the request is retired (i.e.
    before it is completed). Upon retiring the request, the context may be
    unpinned and freed, and along with it the timeline may be freed. We
    therefore need to be very careful when chasing rq->timeline that the
    pointer does not disappear beneath us. The vast majority of users are in
    a protected context, either during request construction or retirement,
    where the timeline->mutex is held and the timeline cannot disappear. It
    is those few off the beaten path (where we access a second timeline) that
    need extra scrutiny -- to be added in the next patch after first adding
    the warnings about dangerous access.
    
    One complication, where we cannot use the timeline->mutex itself, is
    during request submission onto hardware (under spinlocks). Here, we want
    to check on the timeline to finalize the breadcrumb, and so we need to
    impose a second rule to ensure that the request->timeline is indeed
    valid. As we are submitting the request, it's context and timeline must
    be pinned, as it will be used by the hardware. Since it is pinned, we
    know the request->timeline must still be valid, and we cannot submit the
    idle barrier until after we release the engine->active.lock, ergo while
    submitting and holding that spinlock, a second thread cannot release the
    timeline.
    
    v2: Don't be lazy inside selftests; hold the timeline->mutex for as long
    as we need it, and tidy up acquiring the timeline with a bit of
    refactoring (i915_active_add_request)
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190919111912.21631-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 65b5ca74b394..ce61c83d68e9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -103,7 +103,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 		/* Context switch failed, hope for the best! Maybe reset? */
 		goto out_unlock;
 
-	intel_timeline_enter(rq->timeline);
+	intel_timeline_enter(i915_request_timeline(rq));
 
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;

commit ff36c5c4fd0b3783ed982ecf292a42d20c2f2725
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 23 14:26:46 2019 +0100

    drm/i915: Hold irq-off for the entire fake lock period
    
    Sadly lockdep records when the irqs are re-enabled and then marks up the
    fake lock as being irq-unsafe. Our hand is forced and so we must mark up
    the entire fake lock critical section as irq-off.
    
    Hopefully this is the last tweak required!
    
    v2: Not quite, we need to mark the timeline spinlock as irqsafe. That
    was a genuine bug being hidden by the earlier lockdep splat.
    
    Fixes: d67739268cf0 ("drm/i915/gt: Mark up the nested engine-pm timeline lock as irqsafe")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190823132700.25286-2-chris@chris-wilson.co.uk
    (cherry picked from commit 6dcb85a0ad990455ae7c596e3fc966ad9c1ba9c5)
    Signed-off-by: Rodrigo Vivi <rodrigo.vivi@intel.com>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index a372d4ea9370..65b5ca74b394 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -39,27 +39,32 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 #if IS_ENABLED(CONFIG_LOCKDEP)
 
-static inline void __timeline_mark_lock(struct intel_context *ce)
+static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
 	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
-	local_irq_restore(flags);
+
+	return flags;
 }
 
-static inline void __timeline_mark_unlock(struct intel_context *ce)
+static inline void __timeline_mark_unlock(struct intel_context *ce,
+					  unsigned long flags)
 {
 	mutex_release(&ce->timeline->mutex.dep_map, 0, _THIS_IP_);
+	local_irq_restore(flags);
 }
 
 #else
 
-static inline void __timeline_mark_lock(struct intel_context *ce)
+static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
+	return 0;
 }
 
-static inline void __timeline_mark_unlock(struct intel_context *ce)
+static inline void __timeline_mark_unlock(struct intel_context *ce,
+					  unsigned long flags)
 {
 }
 
@@ -68,6 +73,8 @@ static inline void __timeline_mark_unlock(struct intel_context *ce)
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
 	struct i915_request *rq;
+	unsigned long flags;
+	bool result = true;
 
 	/* Already inside the kernel context, safe to power down. */
 	if (engine->wakeref_serial == engine->serial)
@@ -89,12 +96,12 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * retiring the last request, thus all rings should be empty and
 	 * all timelines idle.
 	 */
-	__timeline_mark_lock(engine->kernel_context);
+	flags = __timeline_mark_lock(engine->kernel_context);
 
 	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
 	if (IS_ERR(rq))
 		/* Context switch failed, hope for the best! Maybe reset? */
-		return true;
+		goto out_unlock;
 
 	intel_timeline_enter(rq->timeline);
 
@@ -110,9 +117,10 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	__intel_wakeref_defer_park(&engine->wakeref);
 	__i915_request_queue(rq, NULL);
 
-	__timeline_mark_unlock(engine->kernel_context);
-
-	return false;
+	result = false;
+out_unlock:
+	__timeline_mark_unlock(engine->kernel_context, flags);
+	return result;
 }
 
 static int __engine_park(struct intel_wakeref *wf)

commit 6dcb85a0ad990455ae7c596e3fc966ad9c1ba9c5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 23 14:26:46 2019 +0100

    drm/i915: Hold irq-off for the entire fake lock period
    
    Sadly lockdep records when the irqs are re-enabled and then marks up the
    fake lock as being irq-unsafe. Our hand is forced and so we must mark up
    the entire fake lock critical section as irq-off.
    
    Hopefully this is the last tweak required!
    
    v2: Not quite, we need to mark the timeline spinlock as irqsafe. That
    was a genuine bug being hidden by the earlier lockdep splat.
    
    Fixes: d67739268cf0 ("drm/i915/gt: Mark up the nested engine-pm timeline lock as irqsafe")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190823132700.25286-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index a372d4ea9370..65b5ca74b394 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -39,27 +39,32 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 #if IS_ENABLED(CONFIG_LOCKDEP)
 
-static inline void __timeline_mark_lock(struct intel_context *ce)
+static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
 	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
-	local_irq_restore(flags);
+
+	return flags;
 }
 
-static inline void __timeline_mark_unlock(struct intel_context *ce)
+static inline void __timeline_mark_unlock(struct intel_context *ce,
+					  unsigned long flags)
 {
 	mutex_release(&ce->timeline->mutex.dep_map, 0, _THIS_IP_);
+	local_irq_restore(flags);
 }
 
 #else
 
-static inline void __timeline_mark_lock(struct intel_context *ce)
+static inline unsigned long __timeline_mark_lock(struct intel_context *ce)
 {
+	return 0;
 }
 
-static inline void __timeline_mark_unlock(struct intel_context *ce)
+static inline void __timeline_mark_unlock(struct intel_context *ce,
+					  unsigned long flags)
 {
 }
 
@@ -68,6 +73,8 @@ static inline void __timeline_mark_unlock(struct intel_context *ce)
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
 	struct i915_request *rq;
+	unsigned long flags;
+	bool result = true;
 
 	/* Already inside the kernel context, safe to power down. */
 	if (engine->wakeref_serial == engine->serial)
@@ -89,12 +96,12 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * retiring the last request, thus all rings should be empty and
 	 * all timelines idle.
 	 */
-	__timeline_mark_lock(engine->kernel_context);
+	flags = __timeline_mark_lock(engine->kernel_context);
 
 	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
 	if (IS_ERR(rq))
 		/* Context switch failed, hope for the best! Maybe reset? */
-		return true;
+		goto out_unlock;
 
 	intel_timeline_enter(rq->timeline);
 
@@ -110,9 +117,10 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	__intel_wakeref_defer_park(&engine->wakeref);
 	__i915_request_queue(rq, NULL);
 
-	__timeline_mark_unlock(engine->kernel_context);
-
-	return false;
+	result = false;
+out_unlock:
+	__timeline_mark_unlock(engine->kernel_context, flags);
+	return result;
 }
 
 static int __engine_park(struct intel_wakeref *wf)

commit d67739268cf0ee928cdc5e8224090c59efacd653
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Aug 19 08:58:19 2019 +0100

    drm/i915/gt: Mark up the nested engine-pm timeline lock as irqsafe
    
    We use a fake timeline->mutex lock to reassure lockdep that the timeline
    is always locked when emitting requests. However, the use inside
    __engine_park() may be inside hardirq and so lockdep now complains about
    the mixed irq-state of the nested locked. Disable irqs around the
    lockdep tracking to keep it happy.
    
    Fixes: 6c69a45445af ("drm/i915/gt: Mark context->active_count as protected by timeline->mutex")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190819075835.20065-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 5f03f7dcad72..a372d4ea9370 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -37,9 +37,15 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_LOCKDEP)
+
 static inline void __timeline_mark_lock(struct intel_context *ce)
 {
+	unsigned long flags;
+
+	local_irq_save(flags);
 	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
+	local_irq_restore(flags);
 }
 
 static inline void __timeline_mark_unlock(struct intel_context *ce)
@@ -47,6 +53,18 @@ static inline void __timeline_mark_unlock(struct intel_context *ce)
 	mutex_release(&ce->timeline->mutex.dep_map, 0, _THIS_IP_);
 }
 
+#else
+
+static inline void __timeline_mark_lock(struct intel_context *ce)
+{
+}
+
+static inline void __timeline_mark_unlock(struct intel_context *ce)
+{
+}
+
+#endif /* !IS_ENABLED(CONFIG_LOCKDEP) */
+
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
 	struct i915_request *rq;

commit 6c69a45445af924378371ce3ae3fbe20c4657b94
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 16 13:09:59 2019 +0100

    drm/i915/gt: Mark context->active_count as protected by timeline->mutex
    
    We use timeline->mutex to protect modifications to
    context->active_count, and the associated enable/disable callbacks.
    Due to complications with engine-pm barrier there is a path where we used
    a "superlock" to provide serialised protect and so could not
    unconditionally assert with lockdep that it was always held. However,
    we can mark the mutex as taken (noting that we may be nested underneath
    ourselves) which means we can be reassured the right timeline->mutex is
    always treated as held and let lockdep roam free.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190816121000.8507-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index f3f0109f9e22..5f03f7dcad72 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -37,6 +37,16 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	return 0;
 }
 
+static inline void __timeline_mark_lock(struct intel_context *ce)
+{
+	mutex_acquire(&ce->timeline->mutex.dep_map, 2, 0, _THIS_IP_);
+}
+
+static inline void __timeline_mark_unlock(struct intel_context *ce)
+{
+	mutex_release(&ce->timeline->mutex.dep_map, 0, _THIS_IP_);
+}
+
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
 	struct i915_request *rq;
@@ -61,6 +71,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * retiring the last request, thus all rings should be empty and
 	 * all timelines idle.
 	 */
+	__timeline_mark_lock(engine->kernel_context);
+
 	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
 	if (IS_ERR(rq))
 		/* Context switch failed, hope for the best! Maybe reset? */
@@ -80,6 +92,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	__intel_wakeref_defer_park(&engine->wakeref);
 	__i915_request_queue(rq, NULL);
 
+	__timeline_mark_unlock(engine->kernel_context);
+
 	return false;
 }
 

commit 531958f6f357cf7f812974152954484ff77d3964
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 15 21:57:06 2019 +0100

    drm/i915/gt: Track timeline activeness in enter/exit
    
    Lift moving the timeline to/from the active_list on enter/exit in order
    to shorten the active tracking span in comparison to the existing
    pin/unpin.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190815205709.24285-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 49ad02c3720f..f3f0109f9e22 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -66,6 +66,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 		/* Context switch failed, hope for the best! Maybe reset? */
 		return true;
 
+	intel_timeline_enter(rq->timeline);
+
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
 	i915_request_add_active_barriers(rq);

commit a79ca656b648e6851dd2f1eb0dcef86b278a8908
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Aug 13 20:07:05 2019 +0100

    drm/i915: Push the wakeref->count deferral to the backend
    
    If the backend wishes to defer the wakeref parking, make it responsible
    for unlocking the wakeref (i.e. bumping the counter). This allows it to
    time the unlock much more carefully in case it happens to needs the
    wakeref to be active during its deferral.
    
    For instance, during engine parking we may choose to emit an idle
    barrier (a request). To do so, we borrow the engine->kernel_context
    timeline and to ensure exclusive access we keep the
    engine->wakeref.count as 0. However, to submit that request to HW may
    require a intel_engine_pm_get() (e.g. to keep the submission tasklet
    alive) and before we allow that we have to rewake our wakeref to avoid a
    recursive deadlock.
    
    <4> [257.742916] IRQs not enabled as expected
    <4> [257.742930] WARNING: CPU: 0 PID: 0 at kernel/softirq.c:169 __local_bh_enable_ip+0xa9/0x100
    <4> [257.742936] Modules linked in: vgem snd_hda_codec_hdmi snd_hda_codec_realtek snd_hda_codec_generic i915 btusb btrtl btbcm btintel snd_hda_intel snd_intel_nhlt bluetooth snd_hda_codec coretemp snd_hwdep crct10dif_pclmul snd_hda_core crc32_pclmul ecdh_generic ecc ghash_clmulni_intel snd_pcm r8169 realtek lpc_ich prime_numbers i2c_hid
    <4> [257.742991] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G     U  W         5.3.0-rc3-g5d0a06cd532c-drmtip_340+ #1
    <4> [257.742998] Hardware name: GIGABYTE GB-BXBT-1900/MZBAYAB-00, BIOS F6 02/17/2015
    <4> [257.743008] RIP: 0010:__local_bh_enable_ip+0xa9/0x100
    <4> [257.743017] Code: 37 5b 5d c3 8b 80 50 08 00 00 85 c0 75 a9 80 3d 0b be 25 01 00 75 a0 48 c7 c7 f3 0c 06 ac c6 05 fb bd 25 01 01 e8 77 84 ff ff <0f> 0b eb 89 48 89 ef e8 3b 41 06 00 eb 98 e8 e4 5c f4 ff 5b 5d c3
    <4> [257.743025] RSP: 0018:ffffa78600003cb8 EFLAGS: 00010086
    <4> [257.743035] RAX: 0000000000000000 RBX: 0000000000000200 RCX: 0000000000010302
    <4> [257.743042] RDX: 0000000080010302 RSI: 0000000000000000 RDI: 00000000ffffffff
    <4> [257.743050] RBP: ffffffffc0494bb3 R08: 0000000000000000 R09: 0000000000000001
    <4> [257.743058] R10: 0000000014c8f0e9 R11: 00000000fee2ff8e R12: ffffa23ba8c38008
    <4> [257.743065] R13: ffffa23bacc579c0 R14: ffffa23bb7db0f60 R15: ffffa23b9cc8c430
    <4> [257.743074] FS:  0000000000000000(0000) GS:ffffa23bbba00000(0000) knlGS:0000000000000000
    <4> [257.743082] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    <4> [257.743089] CR2: 00007fe477b20778 CR3: 000000011f72a000 CR4: 00000000001006f0
    <4> [257.743096] Call Trace:
    <4> [257.743104]  <IRQ>
    <4> [257.743265]  __i915_request_commit+0x240/0x5d0 [i915]
    <4> [257.743427]  ? __i915_request_create+0x228/0x4c0 [i915]
    <4> [257.743584]  __engine_park+0x64/0x250 [i915]
    <4> [257.743730]  ____intel_wakeref_put_last+0x1c/0x70 [i915]
    <4> [257.743878]  i915_sample+0x2ee/0x310 [i915]
    <4> [257.744030]  ? i915_pmu_cpu_offline+0xb0/0xb0 [i915]
    <4> [257.744040]  __hrtimer_run_queues+0x11e/0x4b0
    <4> [257.744068]  hrtimer_interrupt+0xea/0x250
    <4> [257.744079]  ? lockdep_hardirqs_off+0x79/0xd0
    <4> [257.744101]  smp_apic_timer_interrupt+0x96/0x280
    <4> [257.744114]  apic_timer_interrupt+0xf/0x20
    <4> [257.744125] RIP: 0010:__do_softirq+0xb3/0x4ae
    
    v2: Keep the priority_hint assert
    v3: That assert was desperately trying to point out my bug. Sorry, little
    assert.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111378
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190813190705.23869-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 6b15e3335dd6..49ad02c3720f 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -68,10 +68,16 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
-
 	i915_request_add_active_barriers(rq);
+
+	/* Install ourselves as a preemption barrier */
+	rq->sched.attr.priority = I915_PRIORITY_UNPREEMPTABLE;
 	__i915_request_commit(rq);
 
+	/* Release our exclusive hold on the engine */
+	__intel_wakeref_defer_park(&engine->wakeref);
+	__i915_request_queue(rq, NULL);
+
 	return false;
 }
 

commit c7302f204490f3eb4ef839bec228315bcd3ba43f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 8 21:27:58 2019 +0100

    drm/i915: Defer final intel_wakeref_put to process context
    
    As we need to acquire a mutex to serialise the final
    intel_wakeref_put, we need to ensure that we are in process context at
    that time. However, we want to allow operation on the intel_wakeref from
    inside timer and other hardirq context, which means that need to defer
    that final put to a workqueue.
    
    Inside the final wakeref puts, we are safe to operate in any context, as
    we are simply marking up the HW and state tracking for the potential
    sleep. It's only the serialisation with the potential sleeping getting
    that requires careful wait avoidance. This allows us to retain the
    immediate processing as before (we only need to sleep over the same
    races as the current mutex_lock).
    
    v2: Add a selftest to ensure we exercise the code while lockdep watches.
    v3: That test was extremely loud and complained about many things!
    v4: Not a whale!
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111295
    References: https://bugs.freedesktop.org/show_bug.cgi?id=111245
    References: https://bugs.freedesktop.org/show_bug.cgi?id=111256
    Fixes: 18398904ca9e ("drm/i915: Only recover active engines")
    Fixes: 51fbd8de87dc ("drm/i915/pmu: Atomically acquire the gt_pm wakeref")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190808202758.10453-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 0336204988d6..6b15e3335dd6 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -37,28 +37,6 @@ static int __engine_unpark(struct intel_wakeref *wf)
 	return 0;
 }
 
-void intel_engine_pm_get(struct intel_engine_cs *engine)
-{
-	intel_wakeref_get(&engine->i915->runtime_pm, &engine->wakeref, __engine_unpark);
-}
-
-void intel_engine_park(struct intel_engine_cs *engine)
-{
-	/*
-	 * We are committed now to parking this engine, make sure there
-	 * will be no more interrupts arriving later and the engine
-	 * is truly idle.
-	 */
-	if (wait_for(intel_engine_is_idle(engine), 10)) {
-		struct drm_printer p = drm_debug_printer(__func__);
-
-		dev_err(engine->i915->drm.dev,
-			"%s is not idle before parking\n",
-			engine->name);
-		intel_engine_dump(engine, &p, NULL);
-	}
-}
-
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 {
 	struct i915_request *rq;
@@ -136,12 +114,18 @@ static int __engine_park(struct intel_wakeref *wf)
 	return 0;
 }
 
-void intel_engine_pm_put(struct intel_engine_cs *engine)
-{
-	intel_wakeref_put(&engine->i915->runtime_pm, &engine->wakeref, __engine_park);
-}
+static const struct intel_wakeref_ops wf_ops = {
+	.get = __engine_unpark,
+	.put = __engine_park,
+};
 
 void intel_engine_init__pm(struct intel_engine_cs *engine)
 {
-	intel_wakeref_init(&engine->wakeref);
+	struct intel_runtime_pm *rpm = &engine->i915->runtime_pm;
+
+	intel_wakeref_init(&engine->wakeref, rpm, &wf_ops);
 }
+
+#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
+#include "selftest_engine_pm.c"
+#endif

commit b40d73784ffc33f3c6431e7ceec3b20fffcd95c3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Aug 4 13:48:26 2019 +0100

    drm/i915: Replace struct_mutex for batch pool serialisation
    
    Switch to tracking activity via i915_active on individual nodes, only
    keeping a list of retired objects in the cache, and reaping the cache
    when the engine itself idles.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190804124826.30272-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index ce54092475da..0336204988d6 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -8,6 +8,7 @@
 
 #include "intel_engine.h"
 #include "intel_engine_pm.h"
+#include "intel_engine_pool.h"
 #include "intel_gt.h"
 #include "intel_gt_pm.h"
 
@@ -116,6 +117,7 @@ static int __engine_park(struct intel_wakeref *wf)
 	GEM_TRACE("%s\n", engine->name);
 
 	intel_engine_disarm_breadcrumbs(engine);
+	intel_engine_pool_park(&engine->pool);
 
 	/* Must be reset upon idling, or we may miss the busy wakeup. */
 	GEM_BUG_ON(engine->execlists.queue_priority_hint != INT_MIN);

commit d8af05ff38ae7a42819b285ffef314942414ef8b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 2 11:00:15 2019 +0100

    drm/i915: Allow sharing the idle-barrier from other kernel requests
    
    By placing our idle-barriers in the i915_active fence tree, we expose
    those for reuse by other components that are issuing requests along the
    kernel_context. Reusing the proto-barrier active_node is perfectly fine
    as the new request implies a context-switch, and so an opportune point
    to run the idle-barrier. However, the proto-barrier is not equivalent
    to a normal active_node and care must be taken to avoid dereferencing the
    ERR_PTR used as its request marker.
    
    v2: Comment the more egregious cheek
    v3: A glossary!
    
    Reported-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Fixes: ce476c80b8bf ("drm/i915: Keep contexts pinned until after the next kernel context switch")
    Fixes: a9877da2d629 ("drm/i915/oa: Reconfigure contexts on the fly")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190802100015.1281-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index e74fbf04a68d..ce54092475da 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -90,7 +90,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
 
-	i915_request_add_barriers(rq);
+	i915_request_add_active_barriers(rq);
 	__i915_request_commit(rq);
 
 	return false;

commit cb823ed9915b0d4064f3f51e936fbe13c089948a
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Jul 12 20:29:53 2019 +0100

    drm/i915/gt: Use intel_gt as the primary object for handling resets
    
    Having taken the first step in encapsulating the functionality by moving
    the related files under gt/, the next step is to start encapsulating by
    passing around the relevant structs rather than the global
    drm_i915_private. In this step, we pass intel_gt to intel_reset.c
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Reviewed-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190712192953.9187-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 84e432abe8e0..e74fbf04a68d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -8,6 +8,7 @@
 
 #include "intel_engine.h"
 #include "intel_engine_pm.h"
+#include "intel_gt.h"
 #include "intel_gt_pm.h"
 
 static int __engine_unpark(struct intel_wakeref *wf)
@@ -66,7 +67,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 		return true;
 
 	/* GPU is pointing to the void, as good as in the kernel context. */
-	if (i915_reset_failed(engine->i915))
+	if (intel_gt_is_wedged(engine->gt))
 		return true;
 
 	/*

commit 092be382a2602067766f190a113514d469162456
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Jun 26 16:45:49 2019 +0100

    drm/i915: Lift intel_engines_resume() to callers
    
    Since the reset path wants to recover the engines itself, it only wants
    to reinitialise the hardware using i915_gem_init_hw(). Pull the call to
    intel_engines_resume() to the module init/resume path so we can avoid it
    during reset.
    
    Fixes: 79ffac8599c4 ("drm/i915: Invert the GEM wakeref hierarchy")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190626154549.10066-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 5253c382034d..84e432abe8e0 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -142,27 +142,3 @@ void intel_engine_init__pm(struct intel_engine_cs *engine)
 {
 	intel_wakeref_init(&engine->wakeref);
 }
-
-int intel_engines_resume(struct drm_i915_private *i915)
-{
-	struct intel_engine_cs *engine;
-	enum intel_engine_id id;
-	int err = 0;
-
-	intel_gt_pm_get(&i915->gt);
-	for_each_engine(engine, i915, id) {
-		intel_engine_pm_get(engine);
-		engine->serial++; /* kernel context lost */
-		err = engine->resume(engine);
-		intel_engine_pm_put(engine);
-		if (err) {
-			dev_err(i915->drm.dev,
-				"Failed to restart %s (%d)\n",
-				engine->name, err);
-			break;
-		}
-	}
-	intel_gt_pm_put(&i915->gt);
-
-	return err;
-}

commit 0c91621cad492e362c37330e1a0985bcdda00fda
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 25 14:01:10 2019 +0100

    drm/i915/gt: Pass intel_gt to pm routines
    
    Switch from passing the i915 container to newly named struct intel_gt.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190625130128.11009-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 2ce00d3dc42a..5253c382034d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -18,7 +18,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 	GEM_TRACE("%s\n", engine->name);
 
-	intel_gt_pm_get(engine->i915);
+	intel_gt_pm_get(engine->gt);
 
 	/* Pin the default state for fast resets from atomic context. */
 	map = NULL;
@@ -129,7 +129,7 @@ static int __engine_park(struct intel_wakeref *wf)
 
 	engine->execlists.no_priolist = false;
 
-	intel_gt_pm_put(engine->i915);
+	intel_gt_pm_put(engine->gt);
 	return 0;
 }
 
@@ -149,7 +149,7 @@ int intel_engines_resume(struct drm_i915_private *i915)
 	enum intel_engine_id id;
 	int err = 0;
 
-	intel_gt_pm_get(i915);
+	intel_gt_pm_get(&i915->gt);
 	for_each_engine(engine, i915, id) {
 		intel_engine_pm_get(engine);
 		engine->serial++; /* kernel context lost */
@@ -162,7 +162,7 @@ int intel_engines_resume(struct drm_i915_private *i915)
 			break;
 		}
 	}
-	intel_gt_pm_put(i915);
+	intel_gt_pm_put(&i915->gt);
 
 	return err;
 }

commit 44d89409a12eb8333735958509d7d591b461d13d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 18 08:41:35 2019 +0100

    drm/i915: Make the semaphore saturation mask global
    
    The idea behind keeping the saturation mask local to a context backfired
    spectacularly. The premise with the local mask was that we would be more
    proactive in attempting to use semaphores after each time the context
    idled, and that all new contexts would attempt to use semaphores
    ignoring the current state of the system. This turns out to be horribly
    optimistic. If the system state is still oversaturated and the existing
    workloads have all stopped using semaphores, the new workloads would
    attempt to use semaphores and be deprioritised behind real work. The
    new contexts would not switch off using semaphores until their initial
    batch of low priority work had completed. Given sufficient backload load
    of equal user priority, this would completely starve the new work of any
    GPU time.
    
    To compensate, remove the local tracking in favour of keeping it as
    global state on the engine -- once the system is saturated and
    semaphores are disabled, everyone stops attempting to use semaphores
    until the system is idle again. One of the reason for preferring local
    context tracking was that it worked with virtual engines, so for
    switching to global state we could either do a complete check of all the
    virtual siblings or simply disable semaphores for those requests. This
    takes the simpler approach of disabling semaphores on virtual engines.
    
    The downside is that the decision that the engine is saturated is a
    local measure -- we are only checking whether or not this context was
    scheduled in a timely fashion, it may be legitimately delayed due to user
    priorities. We still have the same dilemma though, that we do not want
    to employ the semaphore poll unless it will be used.
    
    v2: Explain why we need to assume the worst wrt virtual engines.
    
    Fixes: ca6e56f654e7 ("drm/i915: Disable semaphore busywaits on saturated systems")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    Cc: Dmitry Ermilov <dmitry.ermilov@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190618074153.16055-8-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index d14e352b0b17..2ce00d3dc42a 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -100,6 +100,8 @@ static int __engine_park(struct intel_wakeref *wf)
 	struct intel_engine_cs *engine =
 		container_of(wf, typeof(*engine), wakeref);
 
+	engine->saturated = 0;
+
 	/*
 	 * If one and only one request is completed between pm events,
 	 * we know that we are inside the kernel context and it is

commit ce476c80b8bfa8a8e4c9182cdb686c5aea2431a6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Jun 14 17:46:04 2019 +0100

    drm/i915: Keep contexts pinned until after the next kernel context switch
    
    We need to keep the context image pinned in memory until after the GPU
    has finished writing into it. Since it continues to write as we signal
    the final breadcrumb, we need to keep it pinned until the request after
    it is complete. Currently we know the order in which requests execute on
    each engine, and so to remove that presumption we need to identify a
    request/context-switch we know must occur after our completion. Any
    request queued after the signal must imply a context switch, for
    simplicity we use a fresh request from the kernel context.
    
    The sequence of operations for keeping the context pinned until saved is:
    
     - On context activation, we preallocate a node for each physical engine
       the context may operate on. This is to avoid allocations during
       unpinning, which may be from inside FS_RECLAIM context (aka the
       shrinker)
    
     - On context deactivation on retirement of the last active request (which
       is before we know the context has been saved), we add the
       preallocated node onto a barrier list on each engine
    
     - On engine idling, we emit a switch to kernel context. When this
       switch completes, we know that all previous contexts must have been
       saved, and so on retiring this request we can finally unpin all the
       contexts that were marked as deactivated prior to the switch.
    
    We can enhance this in future by flushing all the idle contexts on a
    regular heartbeat pulse of a switch to kernel context, which will also
    be used to check for hung engines.
    
    v2: intel_context_active_acquire/_release
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190614164606.15633-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 903bee3d6c6d..d14e352b0b17 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -88,6 +88,8 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 
 	/* Check again on the next retirement. */
 	engine->wakeref_serial = engine->serial + 1;
+
+	i915_request_add_barriers(rq);
 	__i915_request_commit(rq);
 
 	return false;

commit 58a111f03a6e3f66f68260e09eb5cf1eefe2a387
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Thu Jun 13 16:21:56 2019 -0700

    drm/i915: make intel_wakeref work on the rpm struct
    
    intel_runtime_pm is the only thing they use from the i915 structure,
    so use that directly.
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190613232156.34940-9-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index ccf034764741..903bee3d6c6d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -37,7 +37,7 @@ static int __engine_unpark(struct intel_wakeref *wf)
 
 void intel_engine_pm_get(struct intel_engine_cs *engine)
 {
-	intel_wakeref_get(engine->i915, &engine->wakeref, __engine_unpark);
+	intel_wakeref_get(&engine->i915->runtime_pm, &engine->wakeref, __engine_unpark);
 }
 
 void intel_engine_park(struct intel_engine_cs *engine)
@@ -131,7 +131,7 @@ static int __engine_park(struct intel_wakeref *wf)
 
 void intel_engine_pm_put(struct intel_engine_cs *engine)
 {
-	intel_wakeref_put(engine->i915, &engine->wakeref, __engine_park);
+	intel_wakeref_put(&engine->i915->runtime_pm, &engine->wakeref, __engine_park);
 }
 
 void intel_engine_init__pm(struct intel_engine_cs *engine)

commit c34c5bca331adc6dcdff81577383df463b4c475c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri May 3 09:09:42 2019 +0100

    drm/i915/execlists: Flush the tasklet on parking
    
    Tidy up the cleanup sequence by always ensure that the tasklet is
    flushed on parking (before we cleanup). The parking provides a
    convenient point to ensure that the backend is truly idle.
    
    v2: Do the full check for idleness before parking, to be sure we flush
    any residual interrupt.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190503080942.30151-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
index 3976aea3c1d1..ccf034764741 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_pm.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -10,7 +10,7 @@
 #include "intel_engine_pm.h"
 #include "intel_gt_pm.h"
 
-static int intel_engine_unpark(struct intel_wakeref *wf)
+static int __engine_unpark(struct intel_wakeref *wf)
 {
 	struct intel_engine_cs *engine =
 		container_of(wf, typeof(*engine), wakeref);
@@ -37,7 +37,24 @@ static int intel_engine_unpark(struct intel_wakeref *wf)
 
 void intel_engine_pm_get(struct intel_engine_cs *engine)
 {
-	intel_wakeref_get(engine->i915, &engine->wakeref, intel_engine_unpark);
+	intel_wakeref_get(engine->i915, &engine->wakeref, __engine_unpark);
+}
+
+void intel_engine_park(struct intel_engine_cs *engine)
+{
+	/*
+	 * We are committed now to parking this engine, make sure there
+	 * will be no more interrupts arriving later and the engine
+	 * is truly idle.
+	 */
+	if (wait_for(intel_engine_is_idle(engine), 10)) {
+		struct drm_printer p = drm_debug_printer(__func__);
+
+		dev_err(engine->i915->drm.dev,
+			"%s is not idle before parking\n",
+			engine->name);
+		intel_engine_dump(engine, &p, NULL);
+	}
 }
 
 static bool switch_to_kernel_context(struct intel_engine_cs *engine)
@@ -56,7 +73,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	 * Note, we do this without taking the timeline->mutex. We cannot
 	 * as we may be called while retiring the kernel context and so
 	 * already underneath the timeline->mutex. Instead we rely on the
-	 * exclusive property of the intel_engine_park that prevents anyone
+	 * exclusive property of the __engine_park that prevents anyone
 	 * else from creating a request on this engine. This also requires
 	 * that the ring is empty and we avoid any waits while constructing
 	 * the context, as they assume protection by the timeline->mutex.
@@ -76,7 +93,7 @@ static bool switch_to_kernel_context(struct intel_engine_cs *engine)
 	return false;
 }
 
-static int intel_engine_park(struct intel_wakeref *wf)
+static int __engine_park(struct intel_wakeref *wf)
 {
 	struct intel_engine_cs *engine =
 		container_of(wf, typeof(*engine), wakeref);
@@ -114,7 +131,7 @@ static int intel_engine_park(struct intel_wakeref *wf)
 
 void intel_engine_pm_put(struct intel_engine_cs *engine)
 {
-	intel_wakeref_put(engine->i915, &engine->wakeref, intel_engine_park);
+	intel_wakeref_put(engine->i915, &engine->wakeref, __engine_park);
 }
 
 void intel_engine_init__pm(struct intel_engine_cs *engine)

commit 79ffac8599c4d8aa84d313920d3d86d7361c252b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 24 21:07:17 2019 +0100

    drm/i915: Invert the GEM wakeref hierarchy
    
    In the current scheme, on submitting a request we take a single global
    GEM wakeref, which trickles down to wake up all GT power domains. This
    is undesirable as we would like to be able to localise our power
    management to the available power domains and to remove the global GEM
    operations from the heart of the driver. (The intent there is to push
    global GEM decisions to the boundary as used by the GEM user interface.)
    
    Now during request construction, each request is responsible via its
    logical context to acquire a wakeref on each power domain it intends to
    utilize. Currently, each request takes a wakeref on the engine(s) and
    the engines themselves take a chipset wakeref. This gives us a
    transition on each engine which we can extend if we want to insert more
    powermangement control (such as soft rc6). The global GEM operations
    that currently require a struct_mutex are reduced to listening to pm
    events from the chipset GT wakeref. As we reduce the struct_mutex
    requirement, these listeners should evaporate.
    
    Perhaps the biggest immediate change is that this removes the
    struct_mutex requirement around GT power management, allowing us greater
    flexibility in request construction. Another important knock-on effect,
    is that by tracking engine usage, we can insert a switch back to the
    kernel context on that engine immediately, avoiding any extra delay or
    inserting global synchronisation barriers. This makes tracking when an
    engine and its associated contexts are idle much easier -- important for
    when we forgo our assumed execution ordering and need idle barriers to
    unpin used contexts. In the process, it means we remove a large chunk of
    code whose only purpose was to switch back to the kernel context.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190424200717.1686-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_pm.c b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
new file mode 100644
index 000000000000..3976aea3c1d1
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/intel_engine_pm.c
@@ -0,0 +1,147 @@
+/*
+ * SPDX-License-Identifier: MIT
+ *
+ * Copyright  2019 Intel Corporation
+ */
+
+#include "i915_drv.h"
+
+#include "intel_engine.h"
+#include "intel_engine_pm.h"
+#include "intel_gt_pm.h"
+
+static int intel_engine_unpark(struct intel_wakeref *wf)
+{
+	struct intel_engine_cs *engine =
+		container_of(wf, typeof(*engine), wakeref);
+	void *map;
+
+	GEM_TRACE("%s\n", engine->name);
+
+	intel_gt_pm_get(engine->i915);
+
+	/* Pin the default state for fast resets from atomic context. */
+	map = NULL;
+	if (engine->default_state)
+		map = i915_gem_object_pin_map(engine->default_state,
+					      I915_MAP_WB);
+	if (!IS_ERR_OR_NULL(map))
+		engine->pinned_default_state = map;
+
+	if (engine->unpark)
+		engine->unpark(engine);
+
+	intel_engine_init_hangcheck(engine);
+	return 0;
+}
+
+void intel_engine_pm_get(struct intel_engine_cs *engine)
+{
+	intel_wakeref_get(engine->i915, &engine->wakeref, intel_engine_unpark);
+}
+
+static bool switch_to_kernel_context(struct intel_engine_cs *engine)
+{
+	struct i915_request *rq;
+
+	/* Already inside the kernel context, safe to power down. */
+	if (engine->wakeref_serial == engine->serial)
+		return true;
+
+	/* GPU is pointing to the void, as good as in the kernel context. */
+	if (i915_reset_failed(engine->i915))
+		return true;
+
+	/*
+	 * Note, we do this without taking the timeline->mutex. We cannot
+	 * as we may be called while retiring the kernel context and so
+	 * already underneath the timeline->mutex. Instead we rely on the
+	 * exclusive property of the intel_engine_park that prevents anyone
+	 * else from creating a request on this engine. This also requires
+	 * that the ring is empty and we avoid any waits while constructing
+	 * the context, as they assume protection by the timeline->mutex.
+	 * This should hold true as we can only park the engine after
+	 * retiring the last request, thus all rings should be empty and
+	 * all timelines idle.
+	 */
+	rq = __i915_request_create(engine->kernel_context, GFP_NOWAIT);
+	if (IS_ERR(rq))
+		/* Context switch failed, hope for the best! Maybe reset? */
+		return true;
+
+	/* Check again on the next retirement. */
+	engine->wakeref_serial = engine->serial + 1;
+	__i915_request_commit(rq);
+
+	return false;
+}
+
+static int intel_engine_park(struct intel_wakeref *wf)
+{
+	struct intel_engine_cs *engine =
+		container_of(wf, typeof(*engine), wakeref);
+
+	/*
+	 * If one and only one request is completed between pm events,
+	 * we know that we are inside the kernel context and it is
+	 * safe to power down. (We are paranoid in case that runtime
+	 * suspend causes corruption to the active context image, and
+	 * want to avoid that impacting userspace.)
+	 */
+	if (!switch_to_kernel_context(engine))
+		return -EBUSY;
+
+	GEM_TRACE("%s\n", engine->name);
+
+	intel_engine_disarm_breadcrumbs(engine);
+
+	/* Must be reset upon idling, or we may miss the busy wakeup. */
+	GEM_BUG_ON(engine->execlists.queue_priority_hint != INT_MIN);
+
+	if (engine->park)
+		engine->park(engine);
+
+	if (engine->pinned_default_state) {
+		i915_gem_object_unpin_map(engine->default_state);
+		engine->pinned_default_state = NULL;
+	}
+
+	engine->execlists.no_priolist = false;
+
+	intel_gt_pm_put(engine->i915);
+	return 0;
+}
+
+void intel_engine_pm_put(struct intel_engine_cs *engine)
+{
+	intel_wakeref_put(engine->i915, &engine->wakeref, intel_engine_park);
+}
+
+void intel_engine_init__pm(struct intel_engine_cs *engine)
+{
+	intel_wakeref_init(&engine->wakeref);
+}
+
+int intel_engines_resume(struct drm_i915_private *i915)
+{
+	struct intel_engine_cs *engine;
+	enum intel_engine_id id;
+	int err = 0;
+
+	intel_gt_pm_get(i915);
+	for_each_engine(engine, i915, id) {
+		intel_engine_pm_get(engine);
+		engine->serial++; /* kernel context lost */
+		err = engine->resume(engine);
+		intel_engine_pm_put(engine);
+		if (err) {
+			dev_err(i915->drm.dev,
+				"Failed to restart %s (%d)\n",
+				engine->name, err);
+			break;
+		}
+	}
+	intel_gt_pm_put(i915);
+
+	return err;
+}
