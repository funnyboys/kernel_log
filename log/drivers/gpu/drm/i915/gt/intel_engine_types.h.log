commit 0f4013fb28ec00986f3934bc6c7d0c2a0eec695c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 13 08:47:46 2020 +0100

    drm/i915/gt: Transfer old virtual breadcrumbs to irq_worker
    
    The second try at staging the transfer of the breadcrumb. In part one,
    we realised we could not simply move to the second engine as we were
    only holding the breadcrumb lock on the first. So in commit 6c81e21a4742
    ("drm/i915/gt: Stage the transfer of the virtual breadcrumb"), we
    removed it from the first engine and marked up this request to reattach
    the signaling on the new engine. However, this failed to take into
    account that we only attach the breadcrumb if the new request is added
    at the start of the queue, which if we are transferring, it is because
    we know there to be a request to be signaled (and hence we would not be
    attached).
    
    In this attempt, we try to transfer the completed requests to the
    irq_worker on its rq->engine->breadcrumbs. This preserves the coupling
    between the rq and its breadcrumbs, so that
    i915_request_cancel_breadcrumb() does not attempt to manipulate the list
    under the wrong lock.
    
    v2: Code sharing is fun.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1862
    Fixes: 6c81e21a4742 ("drm/i915/gt: Stage the transfer of the virtual breadcrumb")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200513074809.18194-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index c39db7e249e2..2b6cdf47d428 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -377,6 +377,8 @@ struct intel_engine_cs {
 		spinlock_t irq_lock;
 		struct list_head signalers;
 
+		struct list_head signaled_requests;
+
 		struct irq_work irq_work; /* for use from inside irq_lock */
 
 		unsigned int irq_enabled;

commit 7a0ba6b43bc0b0078df5504393fd3966a5c7e808
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu May 14 07:29:05 2020 +0100

    drm/i915: Show per-engine default property values in sysfs
    
    By providing the default values configured into the kernel via sysfs, it
    is much more convenient for userspace to restore those sane defaults, or
    at least know what are considered good baseline. This is useful, for
    example, to cleanup after any failed userspace prior to commencing new
    jobs.
    
    /sys/class/drm/card0/engine/rcs0/
    ├── capabilities
    ├── class
    ├── .defaults
    │   ├── heartbeat_interval_ms
    │   ├── max_busywait_duration_ns
    │   ├── preempt_timeout_ms
    │   ├── stop_timeout_ms
    │   └── timeslice_duration_ms
    ├── heartbeat_interval_ms
    ├── instance
    ├── known_capabilities
    ├── max_busywait_duration_ns
    ├── mmio_base
    ├── name
    ├── preempt_timeout_ms
    ├── stop_timeout_ms
    └── timeslice_duration_ms
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Maciej Patelczyk <maciej.patelczyk@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200514062905.28668-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index c113b7805e65..c39db7e249e2 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -565,7 +565,7 @@ struct intel_engine_cs {
 		unsigned long preempt_timeout_ms;
 		unsigned long stop_timeout_ms;
 		unsigned long timeslice_duration_ms;
-	} props;
+	} props, defaults;
 };
 
 static inline bool

commit 977253df6433f85d5e2cb3ab0f8eb4127f8173dd
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon May 4 19:07:45 2020 +0100

    drm/i915/gt: Stop holding onto the pinned_default_state
    
    As we only restore the default context state upon banning a context, we
    only need enough of the state to run the ring and nothing more. That is
    we only need our bare protocontext.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200504180745.15645-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index b1048f039552..c113b7805e65 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -344,7 +344,6 @@ struct intel_engine_cs {
 	unsigned long wakeref_serial;
 	struct intel_wakeref wakeref;
 	struct file *default_state;
-	void *pinned_default_state;
 
 	struct {
 		struct intel_ring *ring;

commit b68be5c623cdd62b9a8b99eb1443ba9cba812d6d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 5 09:46:29 2020 +0100

    drm/i915/execlists: Record the active CCID from before reset
    
    If we cannot trust the reset will flush out the CS event queue such that
    process_csb() reports an accurate view of HW, we will need to search the
    active and pending contexts to determine which was actually running at
    the time we issued the reset.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200505084629.31365-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 6c676774dcd9..b1048f039552 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -179,6 +179,11 @@ struct intel_engine_execlists {
 	 */
 	u32 error_interrupt;
 
+	/**
+	 * @reset_ccid: Active CCID [EXECLISTS_STATUS_HI] at the time of reset
+	 */
+	u32 reset_ccid;
+
 	/**
 	 * @no_priolist: priority lists disabled
 	 */

commit a211da9c771bf97395a3ced83a3aa383372b13a7
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri May 1 13:22:49 2020 +0100

    drm/i915/gt: Make timeslicing an explicit engine property
    
    In order to allow userspace to rely on timeslicing to reorder their
    batches, we must support preemption of those user batches. Declare
    timeslicing as an explicit property that is a combination of having the
    kernel support and HW support.
    
    Suggested-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Fixes: 8ee36e048c98 ("drm/i915/execlists: Minimalistic timeslicing")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200501122249.12417-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 3c3225c0332f..6c676774dcd9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -492,10 +492,11 @@ struct intel_engine_cs {
 #define I915_ENGINE_SUPPORTS_STATS   BIT(1)
 #define I915_ENGINE_HAS_PREEMPTION   BIT(2)
 #define I915_ENGINE_HAS_SEMAPHORES   BIT(3)
-#define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(4)
-#define I915_ENGINE_IS_VIRTUAL       BIT(5)
-#define I915_ENGINE_HAS_RELATIVE_MMIO BIT(6)
-#define I915_ENGINE_REQUIRES_CMD_PARSER BIT(7)
+#define I915_ENGINE_HAS_TIMESLICES   BIT(4)
+#define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(5)
+#define I915_ENGINE_IS_VIRTUAL       BIT(6)
+#define I915_ENGINE_HAS_RELATIVE_MMIO BIT(7)
+#define I915_ENGINE_REQUIRES_CMD_PARSER BIT(8)
 	unsigned int flags;
 
 	/*
@@ -593,6 +594,15 @@ intel_engine_has_semaphores(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_HAS_SEMAPHORES;
 }
 
+static inline bool
+intel_engine_has_timeslices(const struct intel_engine_cs *engine)
+{
+	if (!IS_ACTIVE(CONFIG_DRM_I915_TIMESLICE_DURATION))
+		return false;
+
+	return engine->flags & I915_ENGINE_HAS_TIMESLICES;
+}
+
 static inline bool
 intel_engine_needs_breadcrumb_tasklet(const struct intel_engine_cs *engine)
 {

commit 16e87459673a5cbef35cc0f2e15c664b10a4cdb6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Apr 30 12:18:12 2020 +0100

    drm/i915/gt: Move the batch buffer pool from the engine to the gt
    
    Since the introduction of 'soft-rc6', we aim to park the device quickly
    and that results in frequent idling of the whole device. Currently upon
    idling we free the batch buffer pool, and so this renders the cache
    ineffective for many workloads. If we want to have an effective cache of
    recently allocated buffers available for reuse, we need to decouple that
    cache from the engine powermanagement and make it timer based. As there
    is no reason then to keep it within the engine (where it once made
    retirement order easier to track), we can move it up the hierarchy to the
    owner of the memory allocations.
    
    v2: Hook up to debugfs/drop_caches to clear the cache on demand.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200430111819.10262-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index f760e2ef285b..3c3225c0332f 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -22,7 +22,6 @@
 #include "i915_pmu.h"
 #include "i915_priolist_types.h"
 #include "i915_selftest.h"
-#include "intel_engine_pool_types.h"
 #include "intel_sseu.h"
 #include "intel_timeline_types.h"
 #include "intel_wakeref.h"
@@ -405,13 +404,6 @@ struct intel_engine_cs {
 		struct i915_pmu_sample sample[I915_ENGINE_SAMPLE_COUNT];
 	} pmu;
 
-	/*
-	 * A pool of objects to use as shadow copies of client batch buffers
-	 * when the command parser is enabled. Prevents the client from
-	 * modifying the batch contents after software parsing.
-	 */
-	struct intel_engine_pool pool;
-
 	struct intel_hw_status_page status_page;
 	struct i915_ctx_workarounds wa_ctx;
 	struct i915_wa_list ctx_wa_list;

commit 36d516be867ce3657401ab26b05a39a20f56fa13
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 29 21:54:44 2020 +0100

    drm/i915/gt: Switch to manual evaluation of RPS
    
    As with the realisation for soft-rc6, we respond to idling the engines
    within microseconds, far faster than the response times for HW RC6 and
    RPS. Furthermore, our fast parking upon idle, prevents HW RPS from
    running for many desktop workloads, as the RPS evaluation intervals are
    on the order of tens of milliseconds, but the typical workload is just a
    couple of milliseconds, but yet we still need to determine the best
    frequency for user latency versus power.
    
    Recognising that the HW evaluation intervals are a poor fit, and that
    they were deprecated [in bspec at least] from gen10, start to wean
    ourselves off them and replace the EI with a timer and our accurate
    busy-stats. The principle benefit of manually evaluating RPS intervals
    is that we can be more responsive for better performance and powersaving
    for both spiky workloads and steady-state.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1698
    Fixes: 98479ada421a ("drm/i915/gt: Treat idling as a RPS downclock event")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Reviewed-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200429205446.3259-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 83b1f95ebf12..f760e2ef285b 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -555,6 +555,11 @@ struct intel_engine_cs {
 		 * Idle is defined as active == 0, active is active > 0.
 		 */
 		ktime_t start;
+
+		/**
+		 * @rps: Utilisation at last RPS sampling.
+		 */
+		ktime_t rps;
 	} stats;
 
 	struct {

commit 426d0073fb6d1a9513978cea4c9e8396f3721fba
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 29 21:54:41 2020 +0100

    drm/i915/gt: Always enable busy-stats for execlists
    
    In the near future, we will utilize the busy-stats on each engine to
    approximate the C0 cycles of each, and use that as an input to a manual
    RPS mechanism. That entails having busy-stats always enabled and so we
    can remove the enable/disable routines and simplify the pmu setup. As a
    consequence of always having the stats enabled, we can also show the
    current active time via sysfs/engine/xcs/active_time_ns.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200429205446.3259-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 483d8ff39a0d..83b1f95ebf12 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -531,28 +531,16 @@ struct intel_engine_cs {
 	u32 (*get_cmd_length_mask)(u32 cmd_header);
 
 	struct {
-		/**
-		 * @lock: Lock protecting the below fields.
-		 */
-		seqlock_t lock;
-		/**
-		 * @enabled: Reference count indicating number of listeners.
-		 */
-		unsigned int enabled;
 		/**
 		 * @active: Number of contexts currently scheduled in.
 		 */
-		unsigned int active;
-		/**
-		 * @enabled_at: Timestamp when busy stats were enabled.
-		 */
-		ktime_t enabled_at;
+		atomic_t active;
+
 		/**
-		 * @start: Timestamp of the last idle to active transition.
-		 *
-		 * Idle is defined as active == 0, active is active > 0.
+		 * @lock: Lock protecting the below fields.
 		 */
-		ktime_t start;
+		seqlock_t lock;
+
 		/**
 		 * @total: Total time this engine was busy.
 		 *
@@ -560,6 +548,13 @@ struct intel_engine_cs {
 		 * where engine is currently busy (active > 0).
 		 */
 		ktime_t total;
+
+		/**
+		 * @start: Timestamp of the last idle to active transition.
+		 *
+		 * Idle is defined as active == 0, active is active > 0.
+		 */
+		ktime_t start;
 	} stats;
 
 	struct {

commit be1cb55a07bfc528d826962f1e421a44ed5f8311
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 29 18:24:29 2020 +0100

    drm/i915/gt: Keep a no-frills swappable copy of the default context state
    
    We need to keep the default context state around to instantiate new
    contexts (aka golden rendercontext), and we also keep it pinned while
    the engine is active so that we can quickly reset a hanging context.
    However, the default contexts are large enough to merit keeping in
    swappable memory as opposed to kernel memory, so we store them inside
    shmemfs. Currently, we use the normal GEM objects to create the default
    context image, but we can throw away all but the shmemfs file.
    
    This greatly simplifies the tricky power management code which wants to
    run underneath the normal GT locking, and we definitely do not want to
    use any high level objects that may appear to recurse back into the GT.
    Though perhaps the primary advantage of the complex GEM object is that
    we aggressively cache the mapping, but here we are recreating the
    vm_area everytime time we unpark. At the worst, we add a lightweight
    cache, but first find a microbenchmark that is impacted.
    
    Having started to create some utility functions to make working with
    shmemfs objects easier, we can start putting them to wider use, where
    GEM objects are overkill, such as storing persistent error state.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Matthew Auld <matthew.auld@intel.com>
    Cc: Ramalingam C <ramalingam.c@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200429172429.6054-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index cfe4feaee982..483d8ff39a0d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -339,7 +339,7 @@ struct intel_engine_cs {
 
 	unsigned long wakeref_serial;
 	struct intel_wakeref wakeref;
-	struct drm_i915_gem_object *default_state;
+	struct file *default_state;
 	void *pinned_default_state;
 
 	struct {

commit 5c4a53e3b1cbc38d0906e382f1037290658759bb
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Apr 28 19:47:50 2020 +0100

    drm/i915/execlists: Track inflight CCID
    
    The presumption is that by using a circular counter that is twice as
    large as the maximum ELSP submission, we would never reuse the same CCID
    for two inflight contexts.
    
    However, if we continually preempt an active context such that it always
    remains inflight, it can be resubmitted with an arbitrary number of
    paired contexts. As each of its paired contexts will use a new CCID,
    eventually it will wrap and submit two ELSP with the same CCID.
    
    Rather than use a simple circular counter, switch over to a small bitmap
    of inflight ids so we can avoid reusing one that is still potentially
    active.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1796
    Fixes: 2935ed5339c4 ("drm/i915: Remove logical HW ID")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: <stable@vger.kernel.org> # v5.5+
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200428184751.11257-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 470bdc73220a..cfe4feaee982 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -309,8 +309,7 @@ struct intel_engine_cs {
 	u32 context_size;
 	u32 mmio_base;
 
-	unsigned int context_tag;
-#define NUM_CONTEXT_TAG roundup_pow_of_two(2 * EXECLIST_MAX_PORTS)
+	unsigned long context_tag;
 
 	struct rb_node uabi_node;
 

commit 2632f174a2e1a5fd40a70404fa8ccfd0b1f79ebd
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Apr 28 19:47:49 2020 +0100

    drm/i915/execlists: Avoid reusing the same logical CCID
    
    The bspec is confusing on the nature of the upper 32bits of the LRC
    descriptor. Once upon a time, it said that it uses the upper 32b to
    decide if it should perform a lite-restore, and so we must ensure that
    each unique context submitted to HW is given a unique CCID [for the
    duration of it being on the HW]. Currently, this is achieved by using
    a small circular tag, and assigning every context submitted to HW a
    new id. However, this tag is being cleared on repinning an inflight
    context such that we end up re-using the 0 tag for multiple contexts.
    
    To avoid accidentally clearing the CCID in the upper 32bits of the LRC
    descriptor, split the descriptor into two dwords so we can update the
    GGTT address separately from the CCID.
    
    Closes: https://gitlab.freedesktop.org/drm/intel/-/issues/1796
    Fixes: 2935ed5339c4 ("drm/i915: Remove logical HW ID")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: <stable@vger.kernel.org> # v5.5+
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200428184751.11257-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index bf395227c99f..470bdc73220a 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -156,6 +156,11 @@ struct intel_engine_execlists {
 	 */
 	struct i915_priolist default_priolist;
 
+	/**
+	 * @ccid: identifier for contexts submitted to this engine
+	 */
+	u32 ccid;
+
 	/**
 	 * @yield: CCID at the time of the last semaphore-wait interrupt.
 	 *

commit 23122a4d992b5d5a660155e2c4640e5a93cc957d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Apr 16 12:41:17 2020 +0100

    drm/i915/gt: Scrub execlists state on resume
    
    Before we resume, we reset the HW so we restart from a known good state.
    However, as a part of the reset process, we drain our pending CS event
    queue -- and if we are resuming that does not correspond to internal
    state. On setup, we are scrubbing the CS pointers, but alas only on
    setup.
    
    Apply the sanitization not just to setup, but to all resumes.
    
    Reported-by: Venkata Ramana Nayana <venkata.ramana.nayana@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Venkata Ramana Nayana <venkata.ramana.nayana@intel.com>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200416114117.3460-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 01d4bd781a2f..bf395227c99f 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -419,6 +419,7 @@ struct intel_engine_cs {
 	void		(*irq_enable)(struct intel_engine_cs *engine);
 	void		(*irq_disable)(struct intel_engine_cs *engine);
 
+	void		(*sanitize)(struct intel_engine_cs *engine);
 	int		(*resume)(struct intel_engine_cs *engine);
 
 	struct {

commit c4e8ba7390346a77ffe33ec3f210bc62e0b6c8c6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Apr 7 14:08:11 2020 +0100

    drm/i915/gt: Yield the timeslice if caught waiting on a user semaphore
    
    If we find ourselves waiting on a MI_SEMAPHORE_WAIT, either within the
    user batch or in our own preamble, the engine raises a
    GT_WAIT_ON_SEMAPHORE interrupt. We can unmask that interrupt and so
    respond to a semaphore wait by yielding the timeslice, if we have
    another context to yield to!
    
    The only real complication is that the interrupt is only generated for
    the start of the semaphore wait, and is asynchronous to our
    process_csb() -- that is, we may not have registered the timeslice before
    we see the interrupt. To ensure we don't miss a potential semaphore
    blocking forward progress (e.g. selftests/live_timeslice_preempt) we mark
    the interrupt and apply it to the next timeslice regardless of whether it
    was active at the time.
    
    v2: We use semaphores in preempt-to-busy, within the timeslicing
    implementation itself! Ergo, when we do insert a preemption due to an
    expired timeslice, the new context may start with the missed semaphore
    flagged by the retired context and be yielded, ad infinitum. To avoid
    this, read the context id at the time of the semaphore interrupt and
    only yield if that context is still active.
    
    Fixes: 8ee36e048c98 ("drm/i915/execlists: Minimalistic timeslicing")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Kenneth Graunke <kenneth@whitecape.org>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200407130811.17321-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index de8e6edcf999..01d4bd781a2f 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -156,6 +156,15 @@ struct intel_engine_execlists {
 	 */
 	struct i915_priolist default_priolist;
 
+	/**
+	 * @yield: CCID at the time of the last semaphore-wait interrupt.
+	 *
+	 * Instead of leaving a semaphore busy-spinning on an engine, we would
+	 * like to switch to another ready context, i.e. yielding the semaphore
+	 * timeslice.
+	 */
+	u32 yield;
+
 	/**
 	 * @error_interrupt: CS Master EIR
 	 *

commit 43acd6516ca970c7a1d9d4e3e1d85bf570abc190
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Apr 2 19:40:37 2020 +0100

    drm/i915: Keep a per-engine request pool
    
    Add a tiny per-engine request mempool so that we should always have a
    request available for powermanagement allocations from tricky
    contexts. This reserve is expected to be only used for kernel
    contexts when barriers must be emitted [almost] without fail.
    
    The main consumer for this reserved request is expected to be engine-pm,
    for which we know that there will always be at least the previous pm
    request that we can reuse under mempressure (so there should always be
    a spare request for engine_park()).
    
    This is an alternative to using a comparatively bulky mempool, which
    requires custom handling for both our reserved allocation requirement
    and to protect our TYPESAFE_BY_RCU slab cache. The advantage of mempool
    would be that it would allow us to keep a larger per-engine request
    pool. However, converting over to mempool is straightforward should the
    need arise.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Janusz Krzysztofik <janusz.krzysztofik@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-and-tested-by: Janusz Krzysztofik <janusz.krzysztofik@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200402184037.21630-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 80cdde712842..de8e6edcf999 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -308,6 +308,9 @@ struct intel_engine_cs {
 		struct list_head hold; /* ready requests, but on hold */
 	} active;
 
+	/* keep a request in reserve for a [pm] barrier under oom */
+	struct i915_request *request_pool;
+
 	struct llist_head barrier_tasks;
 
 	struct intel_context *kernel_context; /* pinned */

commit 062444bbc6859dbe9991673ffd377a4af4d76d51
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Feb 28 13:17:13 2020 +0000

    drm/i915/gt: Expose busywait duration to sysfs
    
    We busywait on an inflight request (one that is currently executing on
    HW, and so might complete quickly) prior to setting up an interrupt and
    sleeping. The trade off is that we keep an expensive CPU core busy in
    order to avoid wake up latency: where that trade off should lie is best
    left to the sysadmin.
    
    The busywait mechanism can be compiled out with
    
            ./scripts/config --set-val DRM_I915_SPIN_REQUEST 0
    
    The maximum busywait duration can be adjusted per-engine using,
    
            /sys/class/drm/card?/engine/*/ms_busywait_duration_ns
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Steve Carbonari <steven.carbonari@intel.com>
    Tested-by: Steve Carbonari <steven.carbonari@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200228131716.3243616-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index b23366a81048..80cdde712842 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -547,6 +547,7 @@ struct intel_engine_cs {
 
 	struct {
 		unsigned long heartbeat_interval_ms;
+		unsigned long max_busywait_duration_ns;
 		unsigned long preempt_timeout_ms;
 		unsigned long stop_timeout_ms;
 		unsigned long timeslice_duration_ms;

commit b656000782d7c8a9e623707da77f8fc2f4681741
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Feb 9 23:08:36 2020 +0000

    drm/i915/selftests: Drop live_preempt_hang
    
    live_preempt_hang's use of hang injection has been superseded by
    live_preempt_reset's use of an non-preemptible spinner. The latter does
    not require intrusive hacks into the code.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200209230838.361154-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 45e36d963ea7..b23366a81048 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -127,7 +127,6 @@ DECLARE_EWMA(_engine_latency, 6, 4)
 struct st_preempt_hang {
 	struct completion completion;
 	unsigned int count;
-	bool inject_hang;
 };
 
 /**

commit f70431023eb6e5827dbf25050a8afd75bef1f2b0
Author: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
Date:   Wed Jan 29 20:16:38 2020 +0200

    drm/i915: add extra slice common debug registers
    
    Could be helpful for debugging purposes.
    
    Signed-off-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200129181638.1528150-1-lionel.g.landwerlin@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index abd1de3b83a8..45e36d963ea7 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -75,6 +75,7 @@ struct intel_instdone {
 	u32 instdone;
 	/* The following exist only in the RCS engine */
 	u32 slice_common;
+	u32 slice_common_extra[2];
 	u32 sampler[I915_MAX_SLICES][I915_MAX_SUBSLICES];
 	u32 row[I915_MAX_SLICES][I915_MAX_SUBSLICES];
 };

commit 70a76a9b8e9d553c02deaf8503cd01a316016be0
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jan 28 20:43:15 2020 +0000

    drm/i915/gt: Hook up CS_MASTER_ERROR_INTERRUPT
    
    Now that we have offline error capture and can reset an engine from
    inside an atomic context while also preserving the GPU state for
    post-mortem analysis, it is time to handle error interrupts thrown by
    the command parser.
    
    This provides a much, much faster mechanism for us to detect known
    problems than using heartbeats/hangchecks, and also provides a mechanism
    for when those are disabled. However, it is limited to problems the HW
    can detect in the CS and so not a complete solution for detecting lockups.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200128204318.4182039-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 92be41a6903c..abd1de3b83a8 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -156,6 +156,16 @@ struct intel_engine_execlists {
 	 */
 	struct i915_priolist default_priolist;
 
+	/**
+	 * @error_interrupt: CS Master EIR
+	 *
+	 * The CS generates an interrupt when it detects an error. We capture
+	 * the first error interrupt, record the EIR and schedule the tasklet.
+	 * In the tasklet, we process the pending CS events to ensure we have
+	 * the guilty request, and then reset the engine.
+	 */
+	u32 error_interrupt;
+
 	/**
 	 * @no_priolist: priority lists disabled
 	 */

commit 0b3bd0cdc329a1e2e00995cffd61aacf58c87cb4
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Thu Jan 16 13:45:08 2020 +0000

    drm/i915: Align engine->uabi_class/instance with i915_drm.h
    
    In our ABI we have defined I915_ENGINE_CLASS_INVALID_NONE and
    I915_ENGINE_CLASS_INVALID_VIRTUAL as negative values which creates
    implicit coupling with type widths used in, also ABI, struct
    i915_engine_class_instance.
    
    One place where we export engine->uabi_class
    I915_ENGINE_CLASS_INVALID_VIRTUAL is from our our tracepoints. Because the
    type of the former is u8 in contrast to u16 defined in the ABI, 254 will
    be returned instead of 65534 which userspace would legitimately expect.
    
    Another place is I915_CONTEXT_PARAM_ENGINES.
    
    Therefore we need to align the type used to store engine ABI class and
    instance.
    
    v2:
     * Update the commit message mentioning get_engines and cc stable.
       (Chris)
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Fixes: 6d06779e8672 ("drm/i915: Load balancing across a virtual engine")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: <stable@vger.kernel.org> # v5.3+
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200116134508.25211-1-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 77e68c7643de..92be41a6903c 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -278,8 +278,8 @@ struct intel_engine_cs {
 	u8 class;
 	u8 instance;
 
-	u8 uabi_class;
-	u8 uabi_instance;
+	u16 uabi_class;
+	u16 uabi_instance;
 
 	u32 uabi_capabilities;
 	u32 context_size;

commit 32ff621fd74496f0c33644125fb69ff175859b1f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jan 16 18:47:53 2020 +0000

    drm/i915/gt: Allow temporary suspension of inflight requests
    
    In order to support out-of-line error capture, we need to remove the
    active request from HW and put it to one side while a worker compresses
    and stores all the details associated with that request. (As that
    compression may take an arbitrary user-controlled amount of time, we
    want to let the engine continue running on other workloads while the
    hanging request is dumped.) Not only do we need to remove the active
    request, but we also have to remove its context and all requests that
    were dependent on it (both in flight, queued and future submission).
    
    Finally once the capture is complete, we need to be able to resubmit the
    request and its dependents and allow them to execute.
    
    v2: Replace stack recursion with a simple list.
    v3: Check all the parents, not just the first, when searching for a
    stuck ancestor!
    
    References: https://gitlab.freedesktop.org/drm/intel/issues/738
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200116184754.2860848-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 00287515e7af..77e68c7643de 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -295,6 +295,7 @@ struct intel_engine_cs {
 	struct {
 		spinlock_t lock;
 		struct list_head requests;
+		struct list_head hold; /* ready requests, but on hold */
 	} active;
 
 	struct llist_head barrier_tasks;

commit e26b6d4341476f6944f57bb78fffcdf332c5948b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Dec 22 12:07:52 2019 +0000

    drm/i915/gt: Pull GT initialisation under intel_gt_init()
    
    Begin pulling the GT setup underneath a single GT umbrella; let intel_gt
    take ownership of its engines! As hinted, the complication is the
    lifetime of the probed engine versus the active lifetime of the GT
    backends. We need to detect the engine layout early and keep it until
    the end so that we can sanitize state on takeover and release.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Andi Shyti <andi.shyti@intel.com>
    Acked-by: Andi Shyti <andi.shyti@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191222120752.1368352-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 7f227da09d66..00287515e7af 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -400,7 +400,10 @@ struct intel_engine_cs {
 
 	struct {
 		void (*prepare)(struct intel_engine_cs *engine);
-		void (*reset)(struct intel_engine_cs *engine, bool stalled);
+
+		void (*rewind)(struct intel_engine_cs *engine, bool stalled);
+		void (*cancel)(struct intel_engine_cs *engine);
+
 		void (*finish)(struct intel_engine_cs *engine);
 	} reset;
 
@@ -450,15 +453,7 @@ struct intel_engine_cs {
 	void		(*schedule)(struct i915_request *request,
 				    const struct i915_sched_attr *attr);
 
-	/*
-	 * Cancel all requests on the hardware, or queued for execution.
-	 * This should only cancel the ready requests that have been
-	 * submitted to the engine (via the engine->submit_request callback).
-	 * This is called when marking the device as wedged.
-	 */
-	void		(*cancel_requests)(struct intel_engine_cs *engine);
-
-	void		(*destroy)(struct intel_engine_cs *engine);
+	void		(*release)(struct intel_engine_cs *engine);
 
 	struct intel_engine_execlists execlists;
 

commit b81e4d9b594188a8babf06442e1d6f28de78c68f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 19 12:43:53 2019 +0000

    drm/i915/gt: Track engine round-trip times
    
    Knowing the round trip time of an engine is useful for tracking the
    health of the system as well as providing a metric for the baseline
    responsiveness of the engine. We can use the latter metric for
    automatically tuning our waits in selftests and when idling so we don't
    confuse a slower system with a dead one.
    
    Upon idling the engine, we send one last pulse to switch the context
    away from precious user state to the volatile kernel context. We know
    the engine is idle at this point, and the pulse is non-preemptible, so
    this provides us with a good measurement of the round trip time. It also
    provides us with faster engine parking for ringbuffer submission, which
    is a welcome bonus (e.g. softer-rc6).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Stuart Summers <stuart.summers@intel.com>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191219105043.4169050-1-chris@chris-wilson.co.uk
    Link: https://patchwork.freedesktop.org/patch/msgid/20191219124353.8607-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 17f1f1441efc..7f227da09d66 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -7,6 +7,7 @@
 #ifndef __INTEL_ENGINE_TYPES__
 #define __INTEL_ENGINE_TYPES__
 
+#include <linux/average.h>
 #include <linux/hashtable.h>
 #include <linux/irq_work.h>
 #include <linux/kref.h>
@@ -119,6 +120,9 @@ enum intel_engine_id {
 #define INVALID_ENGINE ((enum intel_engine_id)-1)
 };
 
+/* A simple estimator for the round-trip latency of an engine */
+DECLARE_EWMA(_engine_latency, 6, 4)
+
 struct st_preempt_hang {
 	struct completion completion;
 	unsigned int count;
@@ -316,6 +320,13 @@ struct intel_engine_cs {
 		struct intel_timeline *timeline;
 	} legacy;
 
+	/*
+	 * We track the average duration of the idle pulse on parking the
+	 * engine to keep an estimate of the how the fast the engine is
+	 * under ideal conditions.
+	 */
+	struct ewma__engine_latency latency;
+
 	/* Rather than have every client wait upon all user interrupts,
 	 * with the herd waking after every interrupt and each doing the
 	 * heavyweight seqno dance, we delegate the task (of being the

commit 4f88f8747fa43c97c3b3712d8d87295ea757cc51
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Nov 25 10:58:58 2019 +0000

    drm/i915/gt: Schedule request retirement when timeline idles
    
    The major drawback of commit 7e34f4e4aad3 ("drm/i915/gen8+: Add RC6 CTX
    corruption WA") is that it disables RC6 while Skylake (and friends) is
    active, and we do not consider the GPU idle until all outstanding
    requests have been retired and the engine switched over to the kernel
    context. If userspace is idle, this task falls onto our background idle
    worker, which only runs roughly once a second, meaning that userspace has
    to have been idle for a couple of seconds before we enable RC6 again.
    Naturally, this causes us to consume considerably more energy than
    before as powersaving is effectively disabled while a display server
    (here's looking at you Xorg) is running.
    
    As execlists will get a completion event as each context is completed,
    we can use this interrupt to queue a retire worker bound to this engine
    to cleanup idle timelines. We will then immediately notice the idle
    engine (without userspace intervention or the aid of the background
    retire worker) and start parking the GPU. Thus during light workloads,
    we will do much more work to idle the GPU faster...  Hopefully with
    commensurate power saving!
    
    v2: Watch context completions and only look at those local to the engine
    when retiring to reduce the amount of excess work we perform.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=112315
    References: 7e34f4e4aad3 ("drm/i915/gen8+: Add RC6 CTX corruption WA")
    References: 2248a28384fe ("drm/i915/gen8+: Add RC6 CTX corruption WA")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191125105858.1718307-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 758f0e8ec672..17f1f1441efc 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -451,6 +451,14 @@ struct intel_engine_cs {
 
 	struct intel_engine_execlists execlists;
 
+	/*
+	 * Keep track of completed timelines on this engine for early
+	 * retirement with the goal of quickly enabling powersaving as
+	 * soon as the engine is idle.
+	 */
+	struct intel_timeline *retire;
+	struct work_struct retire_work;
+
 	/* status_notifier: list of callbacks for context-switch changes */
 	struct atomic_notifier_head context_status_notifier;
 

commit dfce90259d74d34cff4cb0c75ecfc0336c09520f
Merge: 2248a28384fe ea0b163b13ff
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Nov 14 11:06:01 2019 +1000

    Backmerge i915 security patches from commit 'ea0b163b13ff' into drm-next
    
    This backmerges the branch that ended up in Linus' tree. It removes
    all the changes for the rc6 patches from Linus' tree in favour of
    a patch that is based on a large refactor that occured.
    
    Otherwise it all looks good.
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit 311a50e76a33d1e029563c24b2ff6db0c02b5afe
Author: Jon Bloomfield <jon.bloomfield@intel.com>
Date:   Wed Aug 1 09:33:59 2018 -0700

    drm/i915: Add support for mandatory cmdparsing
    
    The existing cmdparser for gen7 can be bypassed by specifying
    batch_len=0 in the execbuf call. This is safe because bypassing
    simply reduces the cmd-set available.
    
    In a later patch we will introduce cmdparsing for gen9, as a
    security measure, which must be strictly enforced since without
    it we are vulnerable to DoS attacks.
    
    Introduce the concept of 'required' cmd parsing that cannot be
    bypassed by submitting zero-length bb's.
    
    v2: rebase (Mika)
    v2: rebase (Mika)
    v3: fix conflict on engine flags (Mika)
    
    Signed-off-by: Jon Bloomfield <jon.bloomfield@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: Takashi Iwai <tiwai@suse.de>
    Cc: Tyler Hicks <tyhicks@canonical.com>
    Signed-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Chris Wilson <chris.p.wilson@intel.com>

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index a82cea95c2f2..9dd8c299cb2d 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -475,12 +475,13 @@ struct intel_engine_cs {
 
 	struct intel_engine_hangcheck hangcheck;
 
-#define I915_ENGINE_NEEDS_CMD_PARSER BIT(0)
+#define I915_ENGINE_USING_CMD_PARSER BIT(0)
 #define I915_ENGINE_SUPPORTS_STATS   BIT(1)
 #define I915_ENGINE_HAS_PREEMPTION   BIT(2)
 #define I915_ENGINE_HAS_SEMAPHORES   BIT(3)
 #define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(4)
 #define I915_ENGINE_IS_VIRTUAL       BIT(5)
+#define I915_ENGINE_REQUIRES_CMD_PARSER BIT(7)
 	unsigned int flags;
 
 	/*
@@ -541,9 +542,15 @@ struct intel_engine_cs {
 };
 
 static inline bool
-intel_engine_needs_cmd_parser(const struct intel_engine_cs *engine)
+intel_engine_using_cmd_parser(const struct intel_engine_cs *engine)
 {
-	return engine->flags & I915_ENGINE_NEEDS_CMD_PARSER;
+	return engine->flags & I915_ENGINE_USING_CMD_PARSER;
+}
+
+static inline bool
+intel_engine_requires_cmd_parser(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_REQUIRES_CMD_PARSER;
 }
 
 static inline bool

commit b79029b2e859d8cef534643a1254a833459038f1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Oct 29 09:16:32 2019 +0000

    drm/i915/gt: Make timeslice duration configurable
    
    Execlists uses a scheduling quantum (a timeslice) to alternate execution
    between ready-to-run contexts of equal priority. This ensures that all
    users (though only if they of equal importance) have the opportunity to
    run and prevents livelocks where contexts may have implicit ordering due
    to userspace semaphores. However, not all workloads necessarily benefit
    from timeslicing and in the extreme some sysadmin may want to disable or
    reduce the timeslicing granularity.
    
    The timeslicing mechanism can be compiled out^W^W disabled (but should
    DCE!) with
    
            ./scripts/config --set-val DRM_I915_TIMESLICE_DURATION 0
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191029091632.26281-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index e8ea12b96755..c5d1047a4bc5 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -523,6 +523,7 @@ struct intel_engine_cs {
 		unsigned long heartbeat_interval_ms;
 		unsigned long preempt_timeout_ms;
 		unsigned long stop_timeout_ms;
+		unsigned long timeslice_duration_ms;
 	} props;
 };
 

commit 2871ea85c119e6fb1127b30f0061436b285d3a2c
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Oct 24 11:03:44 2019 +0100

    drm/i915/gt: Split intel_ring_submission
    
    Split the legacy submission backend from the common CS ring buffer
    handling.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191024100344.5041-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 26998901feff..e8ea12b96755 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -59,6 +59,7 @@ struct i915_gem_context;
 struct i915_request;
 struct i915_sched_attr;
 struct intel_gt;
+struct intel_ring;
 struct intel_uncore;
 
 typedef u8 intel_engine_mask_t;
@@ -77,32 +78,6 @@ struct intel_instdone {
 	u32 row[I915_MAX_SLICES][I915_MAX_SUBSLICES];
 };
 
-struct intel_ring {
-	struct kref ref;
-	struct i915_vma *vma;
-	void *vaddr;
-
-	/*
-	 * As we have two types of rings, one global to the engine used
-	 * by ringbuffer submission and those that are exclusive to a
-	 * context used by execlists, we have to play safe and allow
-	 * atomic updates to the pin_count. However, the actual pinning
-	 * of the context is either done during initialisation for
-	 * ringbuffer submission or serialised as part of the context
-	 * pinning for execlists, and so we do not need a mutex ourselves
-	 * to serialise intel_ring_pin/intel_ring_unpin.
-	 */
-	atomic_t pin_count;
-
-	u32 head;
-	u32 tail;
-	u32 emit;
-
-	u32 space;
-	u32 size;
-	u32 effective_size;
-};
-
 /*
  * we use a single page to load ctx workarounds so all of these
  * values are referred in terms of dwords

commit 058179e72e0956a2dfe4927db6cbe5fbfb2406aa
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 23 14:31:08 2019 +0100

    drm/i915/gt: Replace hangcheck by heartbeats
    
    Replace sampling the engine state every so often with a periodic
    heartbeat request to measure the health of an engine. This is coupled
    with the forced-preemption to allow long running requests to survive so
    long as they do not block other users.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Jon Bloomfield <jon.bloomfield@intel.com>
    Reviewed-by: Jon Bloomfield <jon.bloomfield@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191023133108.21401-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 1251dac91f31..26998901feff 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -15,6 +15,7 @@
 #include <linux/rbtree.h>
 #include <linux/timer.h>
 #include <linux/types.h>
+#include <linux/workqueue.h>
 
 #include "i915_gem.h"
 #include "i915_pmu.h"
@@ -76,14 +77,6 @@ struct intel_instdone {
 	u32 row[I915_MAX_SLICES][I915_MAX_SUBSLICES];
 };
 
-struct intel_engine_hangcheck {
-	u64 acthd;
-	u32 last_ring;
-	u32 last_head;
-	unsigned long action_timestamp;
-	struct intel_instdone instdone;
-};
-
 struct intel_ring {
 	struct kref ref;
 	struct i915_vma *vma;
@@ -331,6 +324,11 @@ struct intel_engine_cs {
 
 	intel_engine_mask_t saturated; /* submitting semaphores too late? */
 
+	struct {
+		struct delayed_work work;
+		struct i915_request *systole;
+	} heartbeat;
+
 	unsigned long serial;
 
 	unsigned long wakeref_serial;
@@ -481,8 +479,6 @@ struct intel_engine_cs {
 	/* status_notifier: list of callbacks for context-switch changes */
 	struct atomic_notifier_head context_status_notifier;
 
-	struct intel_engine_hangcheck hangcheck;
-
 #define I915_ENGINE_NEEDS_CMD_PARSER BIT(0)
 #define I915_ENGINE_SUPPORTS_STATS   BIT(1)
 #define I915_ENGINE_HAS_PREEMPTION   BIT(2)
@@ -549,6 +545,7 @@ struct intel_engine_cs {
 	} stats;
 
 	struct {
+		unsigned long heartbeat_interval_ms;
 		unsigned long preempt_timeout_ms;
 		unsigned long stop_timeout_ms;
 	} props;

commit 3a7a92aba8fb77162e1e9963360fd81fc15c39a5
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 23 14:31:05 2019 +0100

    drm/i915/execlists: Force preemption
    
    If the preempted context takes too long to relinquish control, e.g. it
    is stuck inside a shader with arbitration disabled, evict that context
    with an engine reset. This ensures that preemptions are reasonably
    responsive, providing a tighter QoS for the more important context at
    the cost of flagging unresponsive contexts more frequently (i.e. instead
    of using an ~10s hangcheck, we now evict at ~100ms).  The challenge of
    lies in picking a timeout that can be reasonably serviced by HW for
    typical workloads, balancing the existing clients against the needs for
    responsiveness.
    
    Note that coupled with timeslicing, this will lead to rapid GPU "hang"
    detection with multiple active contexts vying for GPU time.
    
    The forced preemption mechanism can be compiled out with
    
            ./scripts/config --set-val DRM_I915_PREEMPT_TIMEOUT 0
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191023133108.21401-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 87d5c4ef3ae7..1251dac91f31 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -174,6 +174,11 @@ struct intel_engine_execlists {
 	 */
 	struct timer_list timer;
 
+	/**
+	 * @preempt: reset the current context if it fails to give way
+	 */
+	struct timer_list preempt;
+
 	/**
 	 * @default_priolist: priority list for I915_PRIORITY_NORMAL
 	 */
@@ -544,6 +549,7 @@ struct intel_engine_cs {
 	} stats;
 
 	struct {
+		unsigned long preempt_timeout_ms;
 		unsigned long stop_timeout_ms;
 	} props;
 };

commit a8c51ed22b0ef20205180baa8ed30e05ea6f8505
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Oct 23 14:31:04 2019 +0100

    drm/i915/gt: Try to more gracefully quiesce the system before resets
    
    If we are doing a normal GPU reset triggered after detecting a long
    period of stalled work, we can take our time and allow the engines to
    quiesce. Since we've stopped submission to the engine, and if we wait
    long enough an innocent context should complete, leaving the engine idle.
    So by waiting a short amount of time, we should prevent clobbering other
    users when resetting a stuck context.
    
    Suggested-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Suggested-by: Jon Bloomfield <jon.bloomfield@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191023133108.21401-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 3451be034caf..87d5c4ef3ae7 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -542,6 +542,10 @@ struct intel_engine_cs {
 		 */
 		ktime_t total;
 	} stats;
+
+	struct {
+		unsigned long stop_timeout_ms;
+	} props;
 };
 
 static inline bool

commit a50134b1983b8860e0e74e41579cbb19a7304ca7
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Thu Oct 17 17:18:52 2019 +0100

    drm/i915: Make for_each_engine_masked work on intel_gt
    
    Medium term goal is to eliminate the i915->engine[] array and to get there
    we have recently introduced equivalent array in intel_gt. Now we need to
    migrate the code further towards this state.
    
    This next step is to eliminate usage of i915->engines[] from the
    for_each_engine_masked iterator.
    
    For this to work we also need to use engine->id as index when populating
    the gt->engine[] array and adjust the default engine set indexing to use
    engine->legacy_idx instead of assuming gt->engines[] indexing.
    
    v2:
      * Populate gt->engine[] earlier.
      * Check that we don't duplicate engine->legacy_idx
    
    v3:
      * Work around the initialization order issue between default_engines()
        and intel_engines_driver_register() which sets engine->legacy_idx for
        now. It will be fixed properly later.
    
    v4:
      * Merge with forgotten v2.5.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191017161852.8836-1-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 6199064f332b..3451be034caf 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -148,6 +148,7 @@ enum intel_engine_id {
 	VECS1,
 #define _VECS(n) (VECS0 + (n))
 	I915_NUM_ENGINES
+#define INVALID_ENGINE ((enum intel_engine_id)-1)
 };
 
 struct st_preempt_hang {

commit 2935ed5339c495066d901b97de384d86c55a3f6d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Oct 4 14:40:08 2019 +0100

    drm/i915: Remove logical HW ID
    
    With the introduction of ctx->engines[] we allow multiple logical
    contexts to be used on the same engine (e.g. with virtual engines).
    According to bspec, aach logical context requires a unique tag in order
    for context-switching to occur correctly between them. [Simple
    experiments show that it is not so easy to trick the HW into performing
    a lite-restore with matching logical IDs, though my memory from early
    Broadwell experiments do suggest that it should be generating
    lite-restores.]
    
    We only need to keep a unique tag for the active lifetime of the
    context, and for as long as we need to identify that context. The HW
    uses the tag to determine if it should use a lite-restore (why not the
    LRCA?) and passes the tag back for various status identifies. The only
    status we need to track is for OA, so when using perf, we assign the
    specific context a unique tag.
    
    v2: Calculate required number of tags to fill ELSP.
    
    Fixes: 976b55f0e1db ("drm/i915: Allow a context to define its set of engines")
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111895
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20191004134015.13204-14-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 943f0663837e..6199064f332b 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -303,10 +303,12 @@ struct intel_engine_cs {
 	u8 uabi_class;
 	u8 uabi_instance;
 
+	u32 uabi_capabilities;
 	u32 context_size;
 	u32 mmio_base;
 
-	u32 uabi_capabilities;
+	unsigned int context_tag;
+#define NUM_CONTEXT_TAG roundup_pow_of_two(2 * EXECLIST_MAX_PORTS)
 
 	struct rb_node uabi_node;
 

commit cdb736fa8b8b65740030ba8f9dc87f0fc3313858
Author: Mika Kuoppala <mika.kuoppala@linux.intel.com>
Date:   Fri Sep 6 16:49:57 2019 +0300

    drm/i915: Use engine relative LRIs on context setup
    
    Daniele pointed out that relative mmio works differently in
    on context restore. Instead of adding the engine mmio base to offset,
    it masks out the base and adds bits [12:2] to current engine base.
    
    This should allow us to construct context register state to be
    applicable to all instances, including virtual. And avoid the trouble
    of updating the registers on virtual instances when submitting work.
    
    v2: only enable for gen12 for now (Mika)
    v3: make enabling readable (Chris)
    
    Bspec: 20206
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Lucas De Marchi <lucas.demarchi@intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Suggested-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Signed-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190906134957.25909-1-mika.kuoppala@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 15e02cb58a67..943f0663837e 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -481,6 +481,7 @@ struct intel_engine_cs {
 #define I915_ENGINE_HAS_SEMAPHORES   BIT(3)
 #define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(4)
 #define I915_ENGINE_IS_VIRTUAL       BIT(5)
+#define I915_ENGINE_HAS_RELATIVE_MMIO BIT(6)
 	unsigned int flags;
 
 	/*
@@ -576,6 +577,12 @@ intel_engine_is_virtual(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_IS_VIRTUAL;
 }
 
+static inline bool
+intel_engine_has_relative_mmio(const struct intel_engine_cs * const engine)
+{
+	return engine->flags & I915_ENGINE_HAS_RELATIVE_MMIO;
+}
+
 #define instdone_has_slice(dev_priv___, sseu___, slice___) \
 	((IS_GEN(dev_priv___, 7) ? 1 : ((sseu___)->slice_mask)) & BIT(slice___))
 

commit eaef5b3c411337db73a205d0e009c72f7a39f3a3
Author: Stuart Summers <stuart.summers@intel.com>
Date:   Fri Aug 23 09:03:05 2019 -0700

    drm/i915: Refactor instdone loops on new subslice functions
    
    Refactor instdone loops to use the new intel_sseu_has_subslice
    function.
    
    Signed-off-by: Stuart Summers <stuart.summers@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190823160307.180813-10-stuart.summers@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index a82cea95c2f2..15e02cb58a67 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -576,20 +576,18 @@ intel_engine_is_virtual(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_IS_VIRTUAL;
 }
 
-#define instdone_slice_mask(dev_priv__) \
-	(IS_GEN(dev_priv__, 7) ? \
-	 1 : RUNTIME_INFO(dev_priv__)->sseu.slice_mask)
-
-#define instdone_subslice_mask(dev_priv__) \
-	(IS_GEN(dev_priv__, 7) ? \
-	 1 : RUNTIME_INFO(dev_priv__)->sseu.subslice_mask[0])
-
-#define for_each_instdone_slice_subslice(dev_priv__, slice__, subslice__) \
-	for ((slice__) = 0, (subslice__) = 0; \
-	     (slice__) < I915_MAX_SLICES; \
-	     (subslice__) = ((subslice__) + 1) < I915_MAX_SUBSLICES ? (subslice__) + 1 : 0, \
-	       (slice__) += ((subslice__) == 0)) \
-		for_each_if((BIT(slice__) & instdone_slice_mask(dev_priv__)) && \
-			    (BIT(subslice__) & instdone_subslice_mask(dev_priv__)))
-
+#define instdone_has_slice(dev_priv___, sseu___, slice___) \
+	((IS_GEN(dev_priv___, 7) ? 1 : ((sseu___)->slice_mask)) & BIT(slice___))
+
+#define instdone_has_subslice(dev_priv__, sseu__, slice__, subslice__) \
+	(IS_GEN(dev_priv__, 7) ? (1 & BIT(subslice__)) : \
+	 intel_sseu_has_subslice(sseu__, 0, subslice__))
+
+#define for_each_instdone_slice_subslice(dev_priv_, sseu_, slice_, subslice_) \
+	for ((slice_) = 0, (subslice_) = 0; (slice_) < I915_MAX_SLICES; \
+	     (subslice_) = ((subslice_) + 1) % I915_MAX_SUBSLICES, \
+	     (slice_) += ((subslice_) == 0)) \
+		for_each_if((instdone_has_slice(dev_priv_, sseu_, slice_)) && \
+			    (instdone_has_subslice(dev_priv_, sseu_, slice_, \
+						    subslice_)))
 #endif /* __INTEL_ENGINE_TYPES_H__ */

commit 3d7b3039741de382fba0f0b9aca7869202749f08
Author: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
Date:   Thu Aug 15 18:23:39 2019 -0700

    drm/i915: Move engine IDs out of i915_reg.h
    
    To remove the dependency between the GT headers and i915_reg.h, move the
    definition of the engine IDs/classes to intel_engine_types.h
    
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Reviewed-by: Lucas De Marchi <lucas.demarchi@intel.com>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190816012343.36433-3-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 5441aa9cb863..a82cea95c2f2 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -26,6 +26,26 @@
 #include "intel_wakeref.h"
 #include "intel_workarounds_types.h"
 
+/* Legacy HW Engine ID */
+
+#define RCS0_HW		0
+#define VCS0_HW		1
+#define BCS0_HW		2
+#define VECS0_HW	3
+#define VCS1_HW		4
+#define VCS2_HW		6
+#define VCS3_HW		7
+#define VECS1_HW	12
+
+/* Gen11+ HW Engine class + instance */
+#define RENDER_CLASS		0
+#define VIDEO_DECODE_CLASS	1
+#define VIDEO_ENHANCEMENT_CLASS	2
+#define COPY_ENGINE_CLASS	3
+#define OTHER_CLASS		4
+#define MAX_ENGINE_CLASS	4
+#define MAX_ENGINE_INSTANCE	3
+
 #define I915_MAX_SLICES	3
 #define I915_MAX_SUBSLICES 8
 

commit df403069029dc61e0fc09cbeb0b5900705edec5b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 16 18:16:08 2019 +0100

    drm/i915/execlists: Lift process_csb() out of the irq-off spinlock
    
    If we only call process_csb() from the tasklet, though we lose the
    ability to bypass ksoftirqd interrupt processing on direct submission
    paths, we can push it out of the irq-off spinlock.
    
    The penalty is that we then allow schedule_out to be called concurrently
    with schedule_in requiring us to handle the usage count (baked into the
    pointer itself) atomically.
    
    As we do kick the tasklets (via local_bh_enable()) after our submission,
    there is a possibility there to see if we can pull the local softirq
    processing back from the ksoftirqd.
    
    v2: Store the 'switch_priority_hint' on submission, so that we can
    safely check during process_csb().
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190816171608.11760-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 9965a32601d6..5441aa9cb863 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -204,6 +204,16 @@ struct intel_engine_execlists {
 	 */
 	unsigned int port_mask;
 
+	/**
+	 * @switch_priority_hint: Second context priority.
+	 *
+	 * We submit multiple contexts to the HW simultaneously and would
+	 * like to occasionally switch between them to emulate timeslicing.
+	 * To know when timeslicing is suitable, we track the priority of
+	 * the context submitted second.
+	 */
+	int switch_priority_hint;
+
 	/**
 	 * @queue_priority_hint: Highest pending priority.
 	 *

commit e5dadff4b09376e8ed92ecc0c12f1b9b3b1fbd19
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 15 21:57:09 2019 +0100

    drm/i915: Protect request retirement with timeline->mutex
    
    Forgo the struct_mutex requirement for request retirement as we have
    been transitioning over to only using the timeline->mutex for
    controlling the lifetime of a request on that timeline.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190815205709.24285-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index a0f372807dd4..9965a32601d6 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -69,9 +69,6 @@ struct intel_ring {
 	struct i915_vma *vma;
 	void *vaddr;
 
-	struct list_head request_list;
-	struct list_head active_link;
-
 	/*
 	 * As we have two types of rings, one global to the engine used
 	 * by ringbuffer submission and those that are exclusive to a

commit 75d0a7f31eec8ec4a53b4485905800e09dc5091f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Aug 9 19:25:18 2019 +0100

    drm/i915: Lift timeline into intel_context
    
    Move the timeline from being inside the intel_ring to intel_context
    itself. This saves much pointer dancing and makes the relations of the
    context to its timeline much clearer.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190809182518.20486-4-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index c79fd1dafa88..a0f372807dd4 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -69,7 +69,6 @@ struct intel_ring {
 	struct i915_vma *vma;
 	void *vaddr;
 
-	struct intel_timeline *timeline;
 	struct list_head request_list;
 	struct list_head active_link;
 
@@ -286,8 +285,6 @@ struct intel_engine_cs {
 
 	struct intel_sseu sseu;
 
-	struct intel_ring *buffer;
-
 	struct {
 		spinlock_t lock;
 		struct list_head requests;
@@ -306,6 +303,11 @@ struct intel_engine_cs {
 	struct drm_i915_gem_object *default_state;
 	void *pinned_default_state;
 
+	struct {
+		struct intel_ring *ring;
+		struct intel_timeline *timeline;
+	} legacy;
+
 	/* Rather than have every client wait upon all user interrupts,
 	 * with the herd waking after every interrupt and each doing the
 	 * heavyweight seqno dance, we delegate the task (of being the

commit f1c4d157ab9b13c1228f3b8ad7747126785460d0
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Aug 8 12:06:12 2019 +0100

    drm/i915: Fix up the inverse mapping for default ctx->engines[]
    
    The order in which we store the engines inside default_engines() for the
    legacy ctx->engines[] has to match the legacy I915_EXEC_RING selector
    mapping in execbuf::user_map. If we present VCS2 as being the second
    instance of the video engine, legacy userspace calls that I915_EXEC_BSD2
    and so we need to insert it into the second video slot.
    
    v2: Record the legacy mapping (hopefully we can remove this need in the
    future)
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=111328
    Fixes: 2edda80db3d0 ("drm/i915: Rename engines to match their user interface")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com> #v1
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190808110612.23539-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index dacaa707c797..c79fd1dafa88 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -264,8 +264,11 @@ struct intel_engine_cs {
 	char name[INTEL_ENGINE_CS_MAX_NAME];
 
 	enum intel_engine_id id;
+	enum intel_engine_id legacy_idx;
+
 	unsigned int hw_id;
 	unsigned int guc_id;
+
 	intel_engine_mask_t mask;
 
 	u8 class;

commit 750e76b4f9f63c95bf4c283ccf8e2af0e258d3bb
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Aug 6 13:43:00 2019 +0100

    drm/i915/gt: Move the [class][inst] lookup for engines onto the GT
    
    To maintain a fast lookup from a GT centric irq handler, we want the
    engine lookup tables on the intel_gt. To avoid having multiple copies of
    the same multi-dimension lookup table, move the generic user engine
    lookup into an rbtree (for fast and flexible indexing).
    
    v2: Split uabi_instance cf uabi_class
    v3: Set uabi_class/uabi_instance after collating all engines to provide a
    stable uabi across parallel unordered construction.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Reviewed-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com> #v2
    Link: https://patchwork.freedesktop.org/patch/msgid/20190806124300.24945-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 2e273e8f5064..dacaa707c797 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -12,6 +12,7 @@
 #include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/llist.h>
+#include <linux/rbtree.h>
 #include <linux/timer.h>
 #include <linux/types.h>
 
@@ -267,15 +268,19 @@ struct intel_engine_cs {
 	unsigned int guc_id;
 	intel_engine_mask_t mask;
 
-	u8 uabi_class;
-
 	u8 class;
 	u8 instance;
+
+	u8 uabi_class;
+	u8 uabi_instance;
+
 	u32 context_size;
 	u32 mmio_base;
 
 	u32 uabi_capabilities;
 
+	struct rb_node uabi_node;
+
 	struct intel_sseu sseu;
 
 	struct intel_ring *buffer;

commit b40d73784ffc33f3c6431e7ceec3b20fffcd95c3
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Sun Aug 4 13:48:26 2019 +0100

    drm/i915: Replace struct_mutex for batch pool serialisation
    
    Switch to tracking activity via i915_active on individual nodes, only
    keeping a list of retired objects in the cache, and reaping the cache
    when the engine itself idles.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190804124826.30272-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index da61dd329210..2e273e8f5064 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -16,12 +16,12 @@
 #include <linux/types.h>
 
 #include "i915_gem.h"
-#include "i915_gem_batch_pool.h"
 #include "i915_pmu.h"
 #include "i915_priolist_types.h"
 #include "i915_selftest.h"
-#include "gt/intel_timeline_types.h"
+#include "intel_engine_pool_types.h"
 #include "intel_sseu.h"
+#include "intel_timeline_types.h"
 #include "intel_wakeref.h"
 #include "intel_workarounds_types.h"
 
@@ -354,7 +354,7 @@ struct intel_engine_cs {
 	 * when the command parser is enabled. Prevents the client from
 	 * modifying the batch contents after software parsing.
 	 */
-	struct i915_gem_batch_pool batch_pool;
+	struct intel_engine_pool pool;
 
 	struct intel_hw_status_page status_page;
 	struct i915_ctx_workarounds wa_ctx;

commit a56277216637c359e9125b9c16be09f21ed2a395
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Mon Jul 29 12:37:20 2019 +0100

    drm/i915: Inline engine->init_context into its caller
    
    We only use the init_context vfunc once while recording the default
    context state, and we use the same sequence in each backend (eliding
    steps that do not apply). Remove the vfunc for simplicity and
    de-duplication.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190729113720.24830-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 8be63019d707..da61dd329210 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -383,7 +383,6 @@ struct intel_engine_cs {
 	const struct intel_context_ops *cops;
 
 	int		(*request_alloc)(struct i915_request *rq);
-	int		(*init_context)(struct i915_request *rq);
 
 	int		(*emit_flush)(struct i915_request *request, u32 mode);
 #define EMIT_INVALIDATE	BIT(0)

commit 71b0846c17b9c86ffa9a18f09f8cfda4fe41ee86
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jul 9 17:54:26 2019 -0700

    drm/i915/guc: Remove preemption support for current fw
    
    Preemption via GuC submission is not being supported with its current
    legacy incarnation. The current FW does support a similar pre-emption
    flow via H2G, but it is class-based instead of being instance-based,
    which doesn't fit well with the i915 tracking. To fix this, the
    firmware is being updated to better support our needs with a new flow,
    so we can safely remove the old code.
    
    v2 (Daniele): resurrect & rebase, reword commit message, remove
    preempt_context as well
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: Matthew Brost <matthew.brost@intel.com>
    Cc: John Harrison <John.C.Harrison@Intel.com>
    Acked-by: Matthew Brost <matthew.brost@intel.com>
    Reviewed-by: Michał Winiarski <michal.winiarski@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190710005437.3496-2-daniele.ceraolospurio@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 7e056114344e..8be63019d707 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -288,7 +288,6 @@ struct intel_engine_cs {
 	struct llist_head barrier_tasks;
 
 	struct intel_context *kernel_context; /* pinned */
-	struct intel_context *preempt_context; /* pinned; optional */
 
 	intel_engine_mask_t saturated; /* submitting semaphores too late? */
 

commit f0c02c1b91888ccac539388eacb0659bf263a557
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Fri Jun 21 08:08:10 2019 +0100

    drm/i915: Rename i915_timeline to intel_timeline and move under gt
    
    Move all timeline code under gt and rename to intel_gt prefix.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Suggested-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190621070811.7006-32-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index fb65e96fa36b..7e056114344e 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -20,7 +20,7 @@
 #include "i915_pmu.h"
 #include "i915_priolist_types.h"
 #include "i915_selftest.h"
-#include "i915_timeline_types.h"
+#include "gt/intel_timeline_types.h"
 #include "intel_sseu.h"
 #include "intel_wakeref.h"
 #include "intel_workarounds_types.h"
@@ -68,7 +68,7 @@ struct intel_ring {
 	struct i915_vma *vma;
 	void *vaddr;
 
-	struct i915_timeline *timeline;
+	struct intel_timeline *timeline;
 	struct list_head request_list;
 	struct list_head active_link;
 

commit f937f5613b6f108b2f93470f357c0880c1b57ffb
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Fri Jun 21 08:07:49 2019 +0100

    drm/i915: Store backpointer to intel_gt in the engine
    
    It will come useful in the next patch.
    
    v2:
     * Do mock_engine as well.
    
    v3:
     * And the virtual engine...
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190621070811.7006-11-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 9940bad37812..fb65e96fa36b 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -36,6 +36,7 @@ struct drm_i915_reg_table;
 struct i915_gem_context;
 struct i915_request;
 struct i915_sched_attr;
+struct intel_gt;
 struct intel_uncore;
 
 typedef u8 intel_engine_mask_t;
@@ -257,6 +258,7 @@ struct intel_engine_execlists {
 
 struct intel_engine_cs {
 	struct drm_i915_private *i915;
+	struct intel_gt *gt;
 	struct intel_uncore *uncore;
 	char name[INTEL_ENGINE_CS_MAX_NAME];
 

commit 8ee36e048c98d4015804a23f884be2576f778a93
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jun 20 15:20:52 2019 +0100

    drm/i915/execlists: Minimalistic timeslicing
    
    If we have multiple contexts of equal priority pending execution,
    activate a timer to demote the currently executing context in favour of
    the next in the queue when that timeslice expires. This enforces
    fairness between contexts (so long as they allow preemption -- forced
    preemption, in the future, will kick those who do not obey) and allows
    us to avoid userspace blocking forward progress with e.g. unbounded
    MI_SEMAPHORE_WAIT.
    
    For the starting point here, we use the jiffie as our timeslice so that
    we should be reasonably efficient wrt frequent CPU wakeups.
    
    Testcase: igt/gem_exec_scheduler/semaphore-resolve
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190620142052.19311-2-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index b4f7b81a3c3e..9940bad37812 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -12,6 +12,7 @@
 #include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/llist.h>
+#include <linux/timer.h>
 #include <linux/types.h>
 
 #include "i915_gem.h"
@@ -149,6 +150,11 @@ struct intel_engine_execlists {
 	 */
 	struct tasklet_struct tasklet;
 
+	/**
+	 * @timer: kick the current context if its timeslice expires
+	 */
+	struct timer_list timer;
+
 	/**
 	 * @default_priolist: priority list for I915_PRIORITY_NORMAL
 	 */

commit 22b7a426bbe1ebe1520f92da4cd1617d1e1b5fc4
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Jun 20 15:20:51 2019 +0100

    drm/i915/execlists: Preempt-to-busy
    
    When using a global seqno, we required a precise stop-the-workd event to
    handle preemption and unwind the global seqno counter. To accomplish
    this, we would preempt to a special out-of-band context and wait for the
    machine to report that it was idle. Given an idle machine, we could very
    precisely see which requests had completed and which we needed to feed
    back into the run queue.
    
    However, now that we have scrapped the global seqno, we no longer need
    to precisely unwind the global counter and only track requests by their
    per-context seqno. This allows us to loosely unwind inflight requests
    while scheduling a preemption, with the enormous caveat that the
    requests we put back on the run queue are still _inflight_ (until the
    preemption request is complete). This makes request tracking much more
    messy, as at any point then we can see a completed request that we
    believe is not currently scheduled for execution. We also have to be
    careful not to rewind RING_TAIL past RING_HEAD on preempting to the
    running context, and for this we use a semaphore to prevent completion
    of the request before continuing.
    
    To accomplish this feat, we change how we track requests scheduled to
    the HW. Instead of appending our requests onto a single list as we
    submit, we track each submission to ELSP as its own block. Then upon
    receiving the CS preemption event, we promote the pending block to the
    inflight block (discarding what was previously being tracked). As normal
    CS completion events arrive, we then remove stale entries from the
    inflight tracker.
    
    v2: Be a tinge paranoid and ensure we flush the write into the HWS page
    for the GPU semaphore to pick in a timely fashion.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190620142052.19311-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 43e975a26016..b4f7b81a3c3e 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -172,51 +172,28 @@ struct intel_engine_execlists {
 	 */
 	u32 __iomem *ctrl_reg;
 
+#define EXECLIST_MAX_PORTS 2
+	/**
+	 * @active: the currently known context executing on HW
+	 */
+	struct i915_request * const *active;
 	/**
-	 * @port: execlist port states
+	 * @inflight: the set of contexts submitted and acknowleged by HW
 	 *
-	 * For each hardware ELSP (ExecList Submission Port) we keep
-	 * track of the last request and the number of times we submitted
-	 * that port to hw. We then count the number of times the hw reports
-	 * a context completion or preemption. As only one context can
-	 * be active on hw, we limit resubmission of context to port[0]. This
-	 * is called Lite Restore, of the context.
+	 * The set of inflight contexts is managed by reading CS events
+	 * from the HW. On a context-switch event (not preemption), we
+	 * know the HW has transitioned from port0 to port1, and we
+	 * advance our inflight/active tracking accordingly.
 	 */
-	struct execlist_port {
-		/**
-		 * @request_count: combined request and submission count
-		 */
-		struct i915_request *request_count;
-#define EXECLIST_COUNT_BITS 2
-#define port_request(p) ptr_mask_bits((p)->request_count, EXECLIST_COUNT_BITS)
-#define port_count(p) ptr_unmask_bits((p)->request_count, EXECLIST_COUNT_BITS)
-#define port_pack(rq, count) ptr_pack_bits(rq, count, EXECLIST_COUNT_BITS)
-#define port_unpack(p, count) ptr_unpack_bits((p)->request_count, count, EXECLIST_COUNT_BITS)
-#define port_set(p, packed) ((p)->request_count = (packed))
-#define port_isset(p) ((p)->request_count)
-#define port_index(p, execlists) ((p) - (execlists)->port)
-
-		/**
-		 * @context_id: context ID for port
-		 */
-		GEM_DEBUG_DECL(u32 context_id);
-
-#define EXECLIST_MAX_PORTS 2
-	} port[EXECLIST_MAX_PORTS];
-
+	struct i915_request *inflight[EXECLIST_MAX_PORTS + 1 /* sentinel */];
 	/**
-	 * @active: is the HW active? We consider the HW as active after
-	 * submitting any context for execution and until we have seen the
-	 * last context completion event. After that, we do not expect any
-	 * more events until we submit, and so can park the HW.
+	 * @pending: the next set of contexts submitted to ELSP
 	 *
-	 * As we have a small number of different sources from which we feed
-	 * the HW, we track the state of each inside a single bitfield.
+	 * We store the array of contexts that we submit to HW (via ELSP) and
+	 * promote them to the inflight array once HW has signaled the
+	 * preemption or idle-to-active event.
 	 */
-	unsigned int active;
-#define EXECLISTS_ACTIVE_USER 0
-#define EXECLISTS_ACTIVE_PREEMPT 1
-#define EXECLISTS_ACTIVE_HWACK 2
+	struct i915_request *pending[EXECLIST_MAX_PORTS + 1];
 
 	/**
 	 * @port_mask: number of execlist ports - 1
@@ -257,11 +234,6 @@ struct intel_engine_execlists {
 	 */
 	u32 *csb_status;
 
-	/**
-	 * @preempt_complete_status: expected CSB upon completing preemption
-	 */
-	u32 preempt_complete_status;
-
 	/**
 	 * @csb_size: context status buffer FIFO size
 	 */

commit 09c5ab384f6fb30f834a5777888b4486dd7f015d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Jun 19 18:01:35 2019 +0100

    drm/i915: Keep rings pinned while the context is active
    
    Remember to keep the rings pinned as well as the context image until the
    GPU is no longer active.
    
    v2: Introduce a ring->pin_count primarily to hide the
    mock_ring that doesn't fit into the normal GGTT vma picture.
    
    v3: Order is important in teardown, ringbuffer submission needs to drop
    the pin count on the engine->kernel_context before it can gleefully free
    its ring.
    
    Bugzilla: https://bugs.freedesktop.org/show_bug.cgi?id=110946
    Fixes: ce476c80b8bf ("drm/i915: Keep contexts pinned until after the next kernel context switch")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190619170135.15281-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 868b220214f8..43e975a26016 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -70,6 +70,18 @@ struct intel_ring {
 	struct list_head request_list;
 	struct list_head active_link;
 
+	/*
+	 * As we have two types of rings, one global to the engine used
+	 * by ringbuffer submission and those that are exclusive to a
+	 * context used by execlists, we have to play safe and allow
+	 * atomic updates to the pin_count. However, the actual pinning
+	 * of the context is either done during initialisation for
+	 * ringbuffer submission or serialised as part of the context
+	 * pinning for execlists, and so we do not need a mutex ourselves
+	 * to serialise intel_ring_pin/intel_ring_unpin.
+	 */
+	atomic_t pin_count;
+
 	u32 head;
 	u32 tail;
 	u32 emit;

commit 44d89409a12eb8333735958509d7d591b461d13d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Jun 18 08:41:35 2019 +0100

    drm/i915: Make the semaphore saturation mask global
    
    The idea behind keeping the saturation mask local to a context backfired
    spectacularly. The premise with the local mask was that we would be more
    proactive in attempting to use semaphores after each time the context
    idled, and that all new contexts would attempt to use semaphores
    ignoring the current state of the system. This turns out to be horribly
    optimistic. If the system state is still oversaturated and the existing
    workloads have all stopped using semaphores, the new workloads would
    attempt to use semaphores and be deprioritised behind real work. The
    new contexts would not switch off using semaphores until their initial
    batch of low priority work had completed. Given sufficient backload load
    of equal user priority, this would completely starve the new work of any
    GPU time.
    
    To compensate, remove the local tracking in favour of keeping it as
    global state on the engine -- once the system is saturated and
    semaphores are disabled, everyone stops attempting to use semaphores
    until the system is idle again. One of the reason for preferring local
    context tracking was that it worked with virtual engines, so for
    switching to global state we could either do a complete check of all the
    virtual siblings or simply disable semaphores for those requests. This
    takes the simpler approach of disabling semaphores on virtual engines.
    
    The downside is that the decision that the engine is saturated is a
    local measure -- we are only checking whether or not this context was
    scheduled in a timely fashion, it may be legitimately delayed due to user
    priorities. We still have the same dilemma though, that we do not want
    to employ the semaphore poll unless it will be used.
    
    v2: Explain why we need to assume the worst wrt virtual engines.
    
    Fixes: ca6e56f654e7 ("drm/i915: Disable semaphore busywaits on saturated systems")
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    Cc: Dmitry Ermilov <dmitry.ermilov@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190618074153.16055-8-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index b2faca8e5dec..868b220214f8 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -298,6 +298,8 @@ struct intel_engine_cs {
 	struct intel_context *kernel_context; /* pinned */
 	struct intel_context *preempt_context; /* pinned; optional */
 
+	intel_engine_mask_t saturated; /* submitting semaphores too late? */
+
 	unsigned long serial;
 
 	unsigned long wakeref_serial;

commit 422d7df4f090bbbc4d49e66d533a259ba63ec70d
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Jun 14 17:46:06 2019 +0100

    drm/i915: Replace engine->timeline with a plain list
    
    To continue the onslaught of removing the assumption of a global
    execution ordering, another casualty is the engine->timeline. Without an
    actual timeline to track, it is overkill and we can replace it with a
    much less grand plain list. We still need a list of requests inflight,
    for the simple purpose of finding inflight requests (for retiring,
    resetting, preemption etc).
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190614164606.15633-3-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 33a31aa2d2ae..b2faca8e5dec 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -288,7 +288,11 @@ struct intel_engine_cs {
 
 	struct intel_ring *buffer;
 
-	struct i915_timeline timeline;
+	struct {
+		spinlock_t lock;
+		struct list_head requests;
+	} active;
+
 	struct llist_head barrier_tasks;
 
 	struct intel_context *kernel_context; /* pinned */

commit ce476c80b8bfa8a8e4c9182cdb686c5aea2431a6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Fri Jun 14 17:46:04 2019 +0100

    drm/i915: Keep contexts pinned until after the next kernel context switch
    
    We need to keep the context image pinned in memory until after the GPU
    has finished writing into it. Since it continues to write as we signal
    the final breadcrumb, we need to keep it pinned until the request after
    it is complete. Currently we know the order in which requests execute on
    each engine, and so to remove that presumption we need to identify a
    request/context-switch we know must occur after our completion. Any
    request queued after the signal must imply a context switch, for
    simplicity we use a fresh request from the kernel context.
    
    The sequence of operations for keeping the context pinned until saved is:
    
     - On context activation, we preallocate a node for each physical engine
       the context may operate on. This is to avoid allocations during
       unpinning, which may be from inside FS_RECLAIM context (aka the
       shrinker)
    
     - On context deactivation on retirement of the last active request (which
       is before we know the context has been saved), we add the
       preallocated node onto a barrier list on each engine
    
     - On engine idling, we emit a switch to kernel context. When this
       switch completes, we know that all previous contexts must have been
       saved, and so on retiring this request we can finally unpin all the
       contexts that were marked as deactivated prior to the switch.
    
    We can enhance this in future by flushing all the idle contexts on a
    regular heartbeat pulse of a switch to kernel context, which will also
    be used to check for hung engines.
    
    v2: intel_context_active_acquire/_release
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190614164606.15633-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 01223864237a..33a31aa2d2ae 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -11,6 +11,7 @@
 #include <linux/irq_work.h>
 #include <linux/kref.h>
 #include <linux/list.h>
+#include <linux/llist.h>
 #include <linux/types.h>
 
 #include "i915_gem.h"
@@ -288,6 +289,7 @@ struct intel_engine_cs {
 	struct intel_ring *buffer;
 
 	struct i915_timeline timeline;
+	struct llist_head barrier_tasks;
 
 	struct intel_context *kernel_context; /* pinned */
 	struct intel_context *preempt_context; /* pinned; optional */
@@ -435,17 +437,6 @@ struct intel_engine_cs {
 
 	struct intel_engine_execlists execlists;
 
-	/* Contexts are pinned whilst they are active on the GPU. The last
-	 * context executed remains active whilst the GPU is idle - the
-	 * switch away and write to the context object only occurs on the
-	 * next execution.  Contexts are only unpinned on retirement of the
-	 * following request ensuring that we can always write to the object
-	 * on the context switch even after idling. Across suspend, we switch
-	 * to the kernel context and trash it as the save may not happen
-	 * before the hardware is powered down.
-	 */
-	struct intel_context *last_retired_context;
-
 	/* status_notifier: list of callbacks for context-switch changes */
 	struct atomic_notifier_head context_status_notifier;
 

commit a10f361d176ce53c72d5b1b2e2913a1a222ee393
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Wed May 29 11:21:50 2019 +0300

    Revert "drm/i915: Expand subslice mask"
    
    This reverts commit 1ac159e23c2c ("drm/i915: Expand subslice mask"),
    which kills ICL due to GEM_BUG_ON() sanity checks before CI even gets a
    chance to do anything.
    
    The commit exposes an issue in commit 1e40d4aea57b ("drm/i915/cnl:
    Implement WaProgramMgsrForCorrectSliceSpecificMmioReads"), which will
    also need to be addressed.
    
    There's a proposed fix [1], but considering the seeming uncertainty with
    the fix as well as the size of the regressing commit (in this context,
    the one that actually brings down ICL), this warrants a revert to get
    ICL working, and gives us time to get all of this right without
    rushing. Even if this means shooting the messenger.
    
    <3>[    9.426327] intel_sseu_get_subslices:46 GEM_BUG_ON(slice >= sseu->max_slices)
    <4>[    9.426355] ------------[ cut here ]------------
    <2>[    9.426357] kernel BUG at drivers/gpu/drm/i915/gt/intel_sseu.c:46!
    <4>[    9.426371] invalid opcode: 0000 [#1] PREEMPT SMP NOPTI
    <4>[    9.426377] CPU: 1 PID: 364 Comm: systemd-udevd Not tainted 5.2.0-rc2-CI-CI_DRM_6159+ #1
    <4>[    9.426385] Hardware name: Intel Corporation Ice Lake Client Platform/IceLake U DDR4 SODIMM PD RVP TLC, BIOS ICLSFWR1.R00.3183.A00.1905020411 05/02/2019
    <4>[    9.426444] RIP: 0010:intel_sseu_get_subslices+0x8a/0xe0 [i915]
    <4>[    9.426452] Code: d5 76 b7 e0 48 8b 35 9d 24 21 00 49 c7 c0 07 f0 72 a0 b9 2e 00 00 00 48 c7 c2 00 8e 6d a0 48 c7 c7 a5 14 5b a0 e8 36 3c be e0 <0f> 0b 48 c7 c1 80 d5 6f a0 ba 30 00 00 00 48 c7 c6 00 8e 6d a0 48
    <4>[    9.426468] RSP: 0018:ffffc9000037b9c8 EFLAGS: 00010282
    <4>[    9.426475] RAX: 000000000000000f RBX: 0000000000000000 RCX: 0000000000000000
    <4>[    9.426482] RDX: 0000000000000001 RSI: 0000000000000008 RDI: ffff88849e346f98
    <4>[    9.426490] RBP: ffff88848a200000 R08: 0000000000000004 R09: ffff88849d50b000
    <4>[    9.426497] R10: 0000000000000000 R11: ffff88849e346f98 R12: ffff88848a209e78
    <4>[    9.426505] R13: 0000000003000000 R14: ffff88848a20b1a8 R15: 0000000000000000
    <4>[    9.426513] FS:  00007f73d5ae8680(0000) GS:ffff88849fc80000(0000) knlGS:0000000000000000
    <4>[    9.426521] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    <4>[    9.426527] CR2: 0000561417b01260 CR3: 0000000494764003 CR4: 0000000000760ee0
    <4>[    9.426535] PKRU: 55555554
    <4>[    9.426538] Call Trace:
    <4>[    9.426585]  wa_init_mcr+0xd5/0x110 [i915]
    <4>[    9.426597]  ? lock_acquire+0xa6/0x1c0
    <4>[    9.426645]  icl_gt_workarounds_init+0x21/0x1a0 [i915]
    <4>[    9.426694]  ? i915_driver_load+0xfcf/0x18a0 [i915]
    <4>[    9.426739]  gt_init_workarounds+0x14c/0x230 [i915]
    <4>[    9.426748]  ? _raw_spin_unlock_irq+0x24/0x50
    <4>[    9.426789]  intel_gt_init_workarounds+0x1b/0x30 [i915]
    <4>[    9.426835]  i915_driver_load+0xfd7/0x18a0 [i915]
    <4>[    9.426843]  ? lock_acquire+0xa6/0x1c0
    <4>[    9.426850]  ? __pm_runtime_resume+0x4f/0x80
    <4>[    9.426857]  ? _raw_spin_unlock_irqrestore+0x4c/0x60
    <4>[    9.426863]  ? _raw_spin_unlock_irqrestore+0x4c/0x60
    <4>[    9.426870]  ? lockdep_hardirqs_on+0xe3/0x1b0
    <4>[    9.426915]  i915_pci_probe+0x29/0xa0 [i915]
    <4>[    9.426923]  pci_device_probe+0x9e/0x120
    <4>[    9.426930]  really_probe+0xea/0x3c0
    <4>[    9.426936]  driver_probe_device+0x10b/0x120
    <4>[    9.426942]  device_driver_attach+0x4a/0x50
    <4>[    9.426948]  __driver_attach+0x97/0x130
    <4>[    9.426954]  ? device_driver_attach+0x50/0x50
    <4>[    9.426960]  bus_for_each_dev+0x74/0xc0
    <4>[    9.426966]  bus_add_driver+0x13f/0x210
    <4>[    9.426971]  ? 0xffffffffa083b000
    <4>[    9.426976]  driver_register+0x56/0xe0
    <4>[    9.426982]  ? 0xffffffffa083b000
    <4>[    9.426987]  do_one_initcall+0x58/0x300
    <4>[    9.426994]  ? do_init_module+0x1d/0x1f6
    <4>[    9.427001]  ? rcu_read_lock_sched_held+0x6f/0x80
    <4>[    9.427007]  ? kmem_cache_alloc_trace+0x261/0x290
    <4>[    9.427014]  do_init_module+0x56/0x1f6
    <4>[    9.427020]  load_module+0x24d1/0x2990
    <4>[    9.427032]  ? __se_sys_finit_module+0xd3/0xf0
    <4>[    9.427037]  __se_sys_finit_module+0xd3/0xf0
    <4>[    9.427047]  do_syscall_64+0x55/0x1c0
    <4>[    9.427053]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
    <4>[    9.427059] RIP: 0033:0x7f73d5609839
    <4>[    9.427064] Code: 00 f3 c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 40 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 1f f6 2c 00 f7 d8 64 89 01 48
    <4>[    9.427082] RSP: 002b:00007ffdf34477b8 EFLAGS: 00000246 ORIG_RAX: 0000000000000139
    <4>[    9.427091] RAX: ffffffffffffffda RBX: 00005559fd5d7b40 RCX: 00007f73d5609839
    <4>[    9.427099] RDX: 0000000000000000 RSI: 00007f73d52e8145 RDI: 000000000000000f
    <4>[    9.427106] RBP: 00007f73d52e8145 R08: 0000000000000000 R09: 00007ffdf34478d0
    <4>[    9.427114] R10: 000000000000000f R11: 0000000000000246 R12: 0000000000000000
    <4>[    9.427121] R13: 00005559fd5c90f0 R14: 0000000000020000 R15: 00005559fd5d7b40
    <4>[    9.427131] Modules linked in: i915(+) mei_hdcp x86_pkg_temp_thermal coretemp snd_hda_intel crct10dif_pclmul crc32_pclmul snd_hda_codec snd_hwdep e1000e snd_hda_core ghash_clmulni_intel ptp snd_pcm cdc_ether usbnet mii pps_core mei_me mei prime_numbers btusb btrtl btbcm btintel bluetooth ecdh_generic ecc
    <4>[    9.427254] ---[ end trace af3eeb543bd66e66 ]---
    
    [1] http://patchwork.freedesktop.org/patch/msgid/20190528200655.11605-1-chris@chris-wilson.co.uk
    
    References: https://intel-gfx-ci.01.org/tree/drm-tip/CI_DRM_6159/fi-icl-u2/pstore0-1517155098_Oops_1.log
    References: 1e40d4aea57b ("drm/i915/cnl: Implement WaProgramMgsrForCorrectSliceSpecificMmioReads")
    Fixes: 1ac159e23c2c ("drm/i915: Expand subslice mask")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Cc: Manasi Navare <manasi.d.navare@intel.com>
    Cc: Michel Thierry <michel.thierry@intel.com>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Cc: Oscar Mateo <oscar.mateo@intel.com>
    Cc: Stuart Summers <stuart.summers@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@linux.intel.com>
    Cc: Yunwei Zhang <yunwei.zhang@intel.com>
    Acked-by: Daniel Vetter <daniel@ffwll.ch>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190529082150.31526-1-jani.nikula@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 4f311ceeab89..01223864237a 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -552,20 +552,20 @@ intel_engine_is_virtual(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_IS_VIRTUAL;
 }
 
-#define instdone_has_slice(dev_priv___, sseu___, slice___) \
-	((IS_GEN(dev_priv___, 7) ? 1 : ((sseu___)->slice_mask)) & \
-	BIT(slice___))
-
-#define instdone_has_subslice(dev_priv__, sseu__, slice__, subslice__) \
-	(IS_GEN(dev_priv__, 7) ? (1 & BIT(subslice__)) : \
-	 intel_sseu_has_subslice(sseu__, slice__, subslice__))
-
-#define for_each_instdone_slice_subslice(dev_priv_, sseu_, slice_, subslice_) \
-	for ((slice_) = 0, (subslice_) = 0; (slice_) < I915_MAX_SLICES; \
-	     (subslice_) = ((subslice_) + 1) % I915_MAX_SUBSLICES, \
-	     (slice_) += ((subslice_) == 0)) \
-		for_each_if((instdone_has_slice(dev_priv_, sseu_, slice_)) && \
-			    (instdone_has_subslice(dev_priv_, sseu_, slice_, \
-						    subslice_)))
+#define instdone_slice_mask(dev_priv__) \
+	(IS_GEN(dev_priv__, 7) ? \
+	 1 : RUNTIME_INFO(dev_priv__)->sseu.slice_mask)
+
+#define instdone_subslice_mask(dev_priv__) \
+	(IS_GEN(dev_priv__, 7) ? \
+	 1 : RUNTIME_INFO(dev_priv__)->sseu.subslice_mask[0])
+
+#define for_each_instdone_slice_subslice(dev_priv__, slice__, subslice__) \
+	for ((slice__) = 0, (subslice__) = 0; \
+	     (slice__) < I915_MAX_SLICES; \
+	     (subslice__) = ((subslice__) + 1) < I915_MAX_SUBSLICES ? (subslice__) + 1 : 0, \
+	       (slice__) += ((subslice__) == 0)) \
+		for_each_if((BIT(slice__) & instdone_slice_mask(dev_priv__)) && \
+			    (BIT(subslice__) & instdone_subslice_mask(dev_priv__)))
 
 #endif /* __INTEL_ENGINE_TYPES_H__ */

commit 1ac159e23c2c033f1fcbf7d60286b90335a4e9b2
Author: Stuart Summers <stuart.summers@intel.com>
Date:   Fri May 24 08:40:22 2019 -0700

    drm/i915: Expand subslice mask
    
    Currently, the subslice_mask runtime parameter is stored as an
    array of subslices per slice. Expand the subslice mask array to
    better match what is presented to userspace through the
    I915_QUERY_TOPOLOGY_INFO ioctl. The index into this array is
    then calculated:
      slice * subslice stride + subslice index / 8
    
    v2: fix spacing in set_sseu_info args
        use set_sseu_info to initialize sseu data when building
        device status in debugfs
        rename variables in intel_engine_types.h to avoid checkpatch
        warnings
    v3: update headers in intel_sseu.h
    v4: add const to some sseu_dev_info variables
        use sseu->eu_stride for EU stride calculations
    v5: address review comments from Tvrtko and Daniele
    v6: remove extra space in intel_sseu_get_subslices
        return the correct subslice enable in for_each_instdone
        add GEM_BUG_ON to ensure user doesn't pass invalid ss_mask size
        use printk formatted string for subslice mask
    v7: remove string.h header and rebase
    
    Cc: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Cc: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Acked-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Reviewed-by: Daniele Ceraolo Spurio <daniele.ceraolospurio@intel.com>
    Signed-off-by: Stuart Summers <stuart.summers@intel.com>
    Signed-off-by: Manasi Navare <manasi.d.navare@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190524154022.13575-6-stuart.summers@intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 01223864237a..4f311ceeab89 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -552,20 +552,20 @@ intel_engine_is_virtual(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_IS_VIRTUAL;
 }
 
-#define instdone_slice_mask(dev_priv__) \
-	(IS_GEN(dev_priv__, 7) ? \
-	 1 : RUNTIME_INFO(dev_priv__)->sseu.slice_mask)
-
-#define instdone_subslice_mask(dev_priv__) \
-	(IS_GEN(dev_priv__, 7) ? \
-	 1 : RUNTIME_INFO(dev_priv__)->sseu.subslice_mask[0])
-
-#define for_each_instdone_slice_subslice(dev_priv__, slice__, subslice__) \
-	for ((slice__) = 0, (subslice__) = 0; \
-	     (slice__) < I915_MAX_SLICES; \
-	     (subslice__) = ((subslice__) + 1) < I915_MAX_SUBSLICES ? (subslice__) + 1 : 0, \
-	       (slice__) += ((subslice__) == 0)) \
-		for_each_if((BIT(slice__) & instdone_slice_mask(dev_priv__)) && \
-			    (BIT(subslice__) & instdone_subslice_mask(dev_priv__)))
+#define instdone_has_slice(dev_priv___, sseu___, slice___) \
+	((IS_GEN(dev_priv___, 7) ? 1 : ((sseu___)->slice_mask)) & \
+	BIT(slice___))
+
+#define instdone_has_subslice(dev_priv__, sseu__, slice__, subslice__) \
+	(IS_GEN(dev_priv__, 7) ? (1 & BIT(subslice__)) : \
+	 intel_sseu_has_subslice(sseu__, slice__, subslice__))
+
+#define for_each_instdone_slice_subslice(dev_priv_, sseu_, slice_, subslice_) \
+	for ((slice_) = 0, (subslice_) = 0; (slice_) < I915_MAX_SLICES; \
+	     (subslice_) = ((subslice_) + 1) % I915_MAX_SUBSLICES, \
+	     (slice_) += ((subslice_) == 0)) \
+		for_each_if((instdone_has_slice(dev_priv_, sseu_, slice_)) && \
+			    (instdone_has_subslice(dev_priv_, sseu_, slice_, \
+						    subslice_)))
 
 #endif /* __INTEL_ENGINE_TYPES_H__ */

commit 5e5d2e209e085be73a83f342798eae68f58e7674
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 28 10:29:42 2019 +0100

    drm/i915: Split GEM object type definition to its own header
    
    For convenience in avoiding inline spaghetti, keep the type definition
    as a separate header.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Matthew Auld <matthew.auld@intel.com>
    Acked-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Acked-by: Jani Nikula <jani.nikula@intel.com>
    Acked-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190528092956.14910-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 40e774acc2cd..01223864237a 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -29,6 +29,7 @@
 #define I915_CMD_HASH_ORDER 9
 
 struct dma_fence;
+struct drm_i915_gem_object;
 struct drm_i915_reg_table;
 struct i915_gem_context;
 struct i915_request;

commit c5d3e39caa456b1e061644b739131f2b54c84c08
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Wed May 22 10:00:54 2019 +0100

    drm/i915: Engine discovery query
    
    Engine discovery query allows userspace to enumerate engines, probe their
    configuration features, all without needing to maintain the internal PCI
    ID based database.
    
    A new query for the generic i915 query ioctl is added named
    DRM_I915_QUERY_ENGINE_INFO, together with accompanying structure
    drm_i915_query_engine_info. The address of latter should be passed to the
    kernel in the query.data_ptr field, and should be large enough for the
    kernel to fill out all known engines as struct drm_i915_engine_info
    elements trailing the query.
    
    As with other queries, setting the item query length to zero allows
    userspace to query minimum required buffer size.
    
    Enumerated engines have common type mask which can be used to query all
    hardware engines, versus engines userspace can submit to using the execbuf
    uAPI.
    
    Engines also have capabilities which are per engine class namespace of
    bits describing features not present on all engine instances.
    
    v2:
     * Fixed HEVC assignment.
     * Reorder some fields, rename type to flags, increase width. (Lionel)
     * No need to allocate temporary storage if we do it engine by engine.
       (Lionel)
    
    v3:
     * Describe engine flags and mark mbz fields. (Lionel)
     * HEVC only applies to VCS.
    
    v4:
     * Squash SFC flag into main patch.
     * Tidy some comments.
    
    v5:
     * Add uabi_ prefix to engine capabilities. (Chris Wilson)
     * Report exact size of engine info array. (Chris Wilson)
     * Drop the engine flags. (Joonas Lahtinen)
     * Added some more reserved fields.
     * Move flags after class/instance.
    
    v6:
     * Do not check engine info array was zeroed by userspace but zero the
       unused fields for them instead.
    
    v7:
     * Simplify length calculation loop. (Lionel Landwerlin)
    
    v8:
     * Remove MBZ comments where not applicable.
     * Rename ABI flags to match engine class define naming.
     * Rename SFC ABI flag to reflect it applies to VCS and VECS.
     * SFC is wired to even _logical_ engine instances.
     * SFC applies to VCS and VECS.
     * HEVC is present on all instances on Gen11. (Tony)
     * Simplify length calculation even more. (Chris Wilson)
     * Move info_ptr assigment closer to loop for clarity. (Chris Wilson)
     * Use vdbox_sfc_access from runtime info.
     * Rebase for RUNTIME_INFO.
     * Refactor for lower indentation.
     * Rename uAPI class/instance to engine_class/instance to avoid C++
       keyword.
    
    v9:
     * Rebase for s/num_rings/num_engines/ in RUNTIME_INFO.
    
    v10:
     * Use new copy_query_item.
    
    v11:
     * Consolidate with struct i915_engine_class_instnace.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Jon Bloomfield <jon.bloomfield@intel.com>
    Cc: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    Cc: Lionel Landwerlin <lionel.g.landwerlin@intel.com>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Tony Ye <tony.ye@intel.com>
    Reviewed-by: Lionel Landwerlin <lionel.g.landwerlin@intel.com> # v7
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190522090054.6007-1-tvrtko.ursulin@linux.intel.com

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index f3fc2e8acc90..40e774acc2cd 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -280,6 +280,8 @@ struct intel_engine_cs {
 	u32 context_size;
 	u32 mmio_base;
 
+	u32 uabi_capabilities;
+
 	struct intel_sseu sseu;
 
 	struct intel_ring *buffer;

commit ee1136908e9b28173f9794be25465a13b2bb9b18
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 21 22:11:33 2019 +0100

    drm/i915/execlists: Virtual engine bonding
    
    Some users require that when a master batch is executed on one particular
    engine, a companion batch is run simultaneously on a specific slave
    engine. For this purpose, we introduce virtual engine bonding, allowing
    maps of master:slaves to be constructed to constrain which physical
    engines a virtual engine may select given a fence on a master engine.
    
    For the moment, we continue to ignore the issue of preemption deferring
    the master request for later. Ideally, we would like to then also remove
    the slave and run something else rather than have it stall the pipeline.
    With load balancing, we should be able to move workload around it, but
    there is a similar stall on the master pipeline while it may wait for
    the slave to be executed. At the cost of more latency for the bonded
    request, it may be interesting to launch both on their engines in
    lockstep. (Bubbles abound.)
    
    Opens: Also what about bonding an engine as its own master? It doesn't
    break anything internally, so allow the silliness.
    
    v2: Emancipate the bonds
    v3: Couple in delayed scheduling for the selftests
    v4: Handle invalid mutually exclusive bonding
    v5: Mention what the uapi does
    v6: s/nbond/num_bonds/
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190521211134.16117-9-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 7b47e00fa082..f3fc2e8acc90 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -405,6 +405,13 @@ struct intel_engine_cs {
 	 */
 	void		(*submit_request)(struct i915_request *rq);
 
+	/*
+	 * Called on signaling of a SUBMIT_FENCE, passing along the signaling
+	 * request down to the bonded pairs.
+	 */
+	void            (*bond_execute)(struct i915_request *rq,
+					struct dma_fence *signal);
+
 	/*
 	 * Call when the priority on a request has changed and it and its
 	 * dependencies may need rescheduling. Note the request itself may

commit 6d06779e86724322d79eb53b26989edd9db188f6
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue May 21 22:11:30 2019 +0100

    drm/i915: Load balancing across a virtual engine
    
    Having allowed the user to define a set of engines that they will want
    to only use, we go one step further and allow them to bind those engines
    into a single virtual instance. Submitting a batch to the virtual engine
    will then forward it to any one of the set in a manner as best to
    distribute load.  The virtual engine has a single timeline across all
    engines (it operates as a single queue), so it is not able to concurrently
    run batches across multiple engines by itself; that is left up to the user
    to submit multiple concurrent batches to multiple queues. Multiple users
    will be load balanced across the system.
    
    The mechanism used for load balancing in this patch is a late greedy
    balancer. When a request is ready for execution, it is added to each
    engine's queue, and when an engine is ready for its next request it
    claims it from the virtual engine. The first engine to do so, wins, i.e.
    the request is executed at the earliest opportunity (idle moment) in the
    system.
    
    As not all HW is created equal, the user is still able to skip the
    virtual engine and execute the batch on a specific engine, all within the
    same queue. It will then be executed in order on the correct engine,
    with execution on other virtual engines being moved away due to the load
    detection.
    
    A couple of areas for potential improvement left!
    
    - The virtual engine always take priority over equal-priority tasks.
    Mostly broken up by applying FQ_CODEL rules for prioritising new clients,
    and hopefully the virtual and real engines are not then congested (i.e.
    all work is via virtual engines, or all work is to the real engine).
    
    - We require the breadcrumb irq around every virtual engine request. For
    normal engines, we eliminate the need for the slow round trip via
    interrupt by using the submit fence and queueing in order. For virtual
    engines, we have to allow any job to transfer to a new ring, and cannot
    coalesce the submissions, so require the completion fence instead,
    forcing the persistent use of interrupts.
    
    - We only drip feed single requests through each virtual engine and onto
    the physical engines, even if there was enough work to fill all ELSP,
    leaving small stalls with an idle CS event at the end of every request.
    Could we be greedy and fill both slots? Being lazy is virtuous for load
    distribution on less-than-full workloads though.
    
    Other areas of improvement are more general, such as reducing lock
    contention, reducing dispatch overhead, looking at direct submission
    rather than bouncing around tasklets etc.
    
    sseu: Lift the restriction to allow sseu to be reconfigured on virtual
    engines composed of RENDER_CLASS (rcs).
    
    v2: macroize check_user_mbz()
    v3: Cancel virtual engines on wedging
    v4: Commence commenting
    v5: Replace 64b sibling_mask with a list of class:instance
    v6: Drop the one-element array in the uabi
    v7: Assert it is an virtual engine in to_virtual_engine()
    v8: Skip over holes in [class][inst] so we can selftest with (vcs0, vcs2)
    
    Link: https://github.com/intel/media-driver/pull/283
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190521211134.16117-6-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index e381c1c73902..7b47e00fa082 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -227,6 +227,7 @@ struct intel_engine_execlists {
 	 * @queue: queue of requests, in priority lists
 	 */
 	struct rb_root_cached queue;
+	struct rb_root_cached virtual;
 
 	/**
 	 * @csb_write: control register for Context Switch buffer
@@ -445,6 +446,7 @@ struct intel_engine_cs {
 #define I915_ENGINE_HAS_PREEMPTION   BIT(2)
 #define I915_ENGINE_HAS_SEMAPHORES   BIT(3)
 #define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(4)
+#define I915_ENGINE_IS_VIRTUAL       BIT(5)
 	unsigned int flags;
 
 	/*
@@ -534,6 +536,12 @@ intel_engine_needs_breadcrumb_tasklet(const struct intel_engine_cs *engine)
 	return engine->flags & I915_ENGINE_NEEDS_BREADCRUMB_TASKLET;
 }
 
+static inline bool
+intel_engine_is_virtual(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_IS_VIRTUAL;
+}
+
 #define instdone_slice_mask(dev_priv__) \
 	(IS_GEN(dev_priv__, 7) ? \
 	 1 : RUNTIME_INFO(dev_priv__)->sseu.slice_mask)

commit 519a019491b833209b2920f0fe9db4aa22da6bbe
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 8 09:06:25 2019 +0100

    drm/i915/hangcheck: Replace hangcheck.seqno with RING_HEAD
    
    After realising we need to sample RING_START to detect context switches
    from preemption events that do not allow for the seqno to advance, we
    can also realise that the seqno itself is just a distance along the ring
    and so can be replaced by sampling RING_HEAD.
    
    v2: Bonus comment for the mystery separate CS_STALL before MI_USER_INTERRUPT
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190508080704.24223-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index c0ab11b12e14..e381c1c73902 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -54,8 +54,7 @@ struct intel_instdone {
 struct intel_engine_hangcheck {
 	u64 acthd;
 	u32 last_ring;
-	u32 last_seqno;
-	u32 next_seqno;
+	u32 last_head;
 	unsigned long action_timestamp;
 	struct intel_instdone instdone;
 };

commit f4107766a96ed743c032de1edd753dbcb1839e38
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 1 12:45:28 2019 +0100

    drm/i915/hangcheck: Track context changes
    
    Given sufficient preemption, we may see a busy system that doesn't
    advance seqno while performing work across multiple contexts, and given
    sufficient pathology not even notice a change in ACTHD. What does change
    between the preempting contexts is their RING, so take note of that and
    treat a change in the ring address as being an indication of forward
    progress.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Mika Kuoppala <mika.kuoppala@linux.intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190501114541.10077-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 9d64e33f8427..c0ab11b12e14 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -53,6 +53,7 @@ struct intel_instdone {
 
 struct intel_engine_hangcheck {
 	u64 acthd;
+	u32 last_ring;
 	u32 last_seqno;
 	u32 next_seqno;
 	unsigned long action_timestamp;

commit 45b9c968c5b8a954576038d3cddf9771ebda0a8f
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed May 1 11:32:04 2019 +0100

    drm/i915: Move the engine->destroy() vfunc onto the engine
    
    Make the engine responsible for cleaning itself up!
    
    This removes the i915->gt.cleanup vfunc that has been annoying the
    casual reader and myself for the last several years, and helps keep a
    future patch to add more cleanup tidy.
    
    v2: Assert that engine->destroy is set after the backend starts
    allocating its own state.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190501103204.18632-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index d972c339309c..9d64e33f8427 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -420,7 +420,7 @@ struct intel_engine_cs {
 	 */
 	void		(*cancel_requests)(struct intel_engine_cs *engine);
 
-	void		(*cleanup)(struct intel_engine_cs *engine);
+	void		(*destroy)(struct intel_engine_cs *engine);
 
 	struct intel_engine_execlists execlists;
 

commit 79ffac8599c4d8aa84d313920d3d86d7361c252b
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 24 21:07:17 2019 +0100

    drm/i915: Invert the GEM wakeref hierarchy
    
    In the current scheme, on submitting a request we take a single global
    GEM wakeref, which trickles down to wake up all GT power domains. This
    is undesirable as we would like to be able to localise our power
    management to the available power domains and to remove the global GEM
    operations from the heart of the driver. (The intent there is to push
    global GEM decisions to the boundary as used by the GEM user interface.)
    
    Now during request construction, each request is responsible via its
    logical context to acquire a wakeref on each power domain it intends to
    utilize. Currently, each request takes a wakeref on the engine(s) and
    the engines themselves take a chipset wakeref. This gives us a
    transition on each engine which we can extend if we want to insert more
    powermangement control (such as soft rc6). The global GEM operations
    that currently require a struct_mutex are reduced to listening to pm
    events from the chipset GT wakeref. As we reduce the struct_mutex
    requirement, these listeners should evaporate.
    
    Perhaps the biggest immediate change is that this removes the
    struct_mutex requirement around GT power management, allowing us greater
    flexibility in request construction. Another important knock-on effect,
    is that by tracking engine usage, we can insert a switch back to the
    kernel context on that engine immediately, avoiding any extra delay or
    inserting global synchronisation barriers. This makes tracking when an
    engine and its associated contexts are idle much easier -- important for
    when we forgo our assumed execution ordering and need idle barriers to
    unpin used contexts. In the process, it means we remove a large chunk of
    code whose only purpose was to switch back to the kernel context.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190424200717.1686-5-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 3adf58da6d2c..d972c339309c 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -20,6 +20,7 @@
 #include "i915_selftest.h"
 #include "i915_timeline_types.h"
 #include "intel_sseu.h"
+#include "intel_wakeref.h"
 #include "intel_workarounds_types.h"
 
 #define I915_MAX_SLICES	3
@@ -287,6 +288,10 @@ struct intel_engine_cs {
 	struct intel_context *kernel_context; /* pinned */
 	struct intel_context *preempt_context; /* pinned; optional */
 
+	unsigned long serial;
+
+	unsigned long wakeref_serial;
+	struct intel_wakeref wakeref;
 	struct drm_i915_gem_object *default_state;
 	void *pinned_default_state;
 
@@ -359,7 +364,7 @@ struct intel_engine_cs {
 	void		(*irq_enable)(struct intel_engine_cs *engine);
 	void		(*irq_disable)(struct intel_engine_cs *engine);
 
-	int		(*init_hw)(struct intel_engine_cs *engine);
+	int		(*resume)(struct intel_engine_cs *engine);
 
 	struct {
 		void (*prepare)(struct intel_engine_cs *engine);

commit 112ed2d31a46f4704085ad925435b77e62b8abee
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Apr 24 18:48:39 2019 +0100

    drm/i915: Move GraphicsTechnology files under gt/
    
    Start partitioning off the code that talks to the hardware (GT) from the
    uapi layers and move the device facing code under gt/
    
    One casualty is s/intel_ringbuffer.h/intel_engine.h/ with the plan to
    subdivide that header and body further (and split out the submission
    code from the ringbuffer and logical context handling). This patch aims
    to be simple motion so git can fixup inflight patches with little mess.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Acked-by: Jani Nikula <jani.nikula@intel.com>
    Acked-by: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190424174839.7141-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
new file mode 100644
index 000000000000..3adf58da6d2c
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -0,0 +1,548 @@
+/*
+ * SPDX-License-Identifier: MIT
+ *
+ * Copyright © 2019 Intel Corporation
+ */
+
+#ifndef __INTEL_ENGINE_TYPES__
+#define __INTEL_ENGINE_TYPES__
+
+#include <linux/hashtable.h>
+#include <linux/irq_work.h>
+#include <linux/kref.h>
+#include <linux/list.h>
+#include <linux/types.h>
+
+#include "i915_gem.h"
+#include "i915_gem_batch_pool.h"
+#include "i915_pmu.h"
+#include "i915_priolist_types.h"
+#include "i915_selftest.h"
+#include "i915_timeline_types.h"
+#include "intel_sseu.h"
+#include "intel_workarounds_types.h"
+
+#define I915_MAX_SLICES	3
+#define I915_MAX_SUBSLICES 8
+
+#define I915_CMD_HASH_ORDER 9
+
+struct dma_fence;
+struct drm_i915_reg_table;
+struct i915_gem_context;
+struct i915_request;
+struct i915_sched_attr;
+struct intel_uncore;
+
+typedef u8 intel_engine_mask_t;
+#define ALL_ENGINES ((intel_engine_mask_t)~0ul)
+
+struct intel_hw_status_page {
+	struct i915_vma *vma;
+	u32 *addr;
+};
+
+struct intel_instdone {
+	u32 instdone;
+	/* The following exist only in the RCS engine */
+	u32 slice_common;
+	u32 sampler[I915_MAX_SLICES][I915_MAX_SUBSLICES];
+	u32 row[I915_MAX_SLICES][I915_MAX_SUBSLICES];
+};
+
+struct intel_engine_hangcheck {
+	u64 acthd;
+	u32 last_seqno;
+	u32 next_seqno;
+	unsigned long action_timestamp;
+	struct intel_instdone instdone;
+};
+
+struct intel_ring {
+	struct kref ref;
+	struct i915_vma *vma;
+	void *vaddr;
+
+	struct i915_timeline *timeline;
+	struct list_head request_list;
+	struct list_head active_link;
+
+	u32 head;
+	u32 tail;
+	u32 emit;
+
+	u32 space;
+	u32 size;
+	u32 effective_size;
+};
+
+/*
+ * we use a single page to load ctx workarounds so all of these
+ * values are referred in terms of dwords
+ *
+ * struct i915_wa_ctx_bb:
+ *  offset: specifies batch starting position, also helpful in case
+ *    if we want to have multiple batches at different offsets based on
+ *    some criteria. It is not a requirement at the moment but provides
+ *    an option for future use.
+ *  size: size of the batch in DWORDS
+ */
+struct i915_ctx_workarounds {
+	struct i915_wa_ctx_bb {
+		u32 offset;
+		u32 size;
+	} indirect_ctx, per_ctx;
+	struct i915_vma *vma;
+};
+
+#define I915_MAX_VCS	4
+#define I915_MAX_VECS	2
+
+/*
+ * Engine IDs definitions.
+ * Keep instances of the same type engine together.
+ */
+enum intel_engine_id {
+	RCS0 = 0,
+	BCS0,
+	VCS0,
+	VCS1,
+	VCS2,
+	VCS3,
+#define _VCS(n) (VCS0 + (n))
+	VECS0,
+	VECS1,
+#define _VECS(n) (VECS0 + (n))
+	I915_NUM_ENGINES
+};
+
+struct st_preempt_hang {
+	struct completion completion;
+	unsigned int count;
+	bool inject_hang;
+};
+
+/**
+ * struct intel_engine_execlists - execlist submission queue and port state
+ *
+ * The struct intel_engine_execlists represents the combined logical state of
+ * driver and the hardware state for execlist mode of submission.
+ */
+struct intel_engine_execlists {
+	/**
+	 * @tasklet: softirq tasklet for bottom handler
+	 */
+	struct tasklet_struct tasklet;
+
+	/**
+	 * @default_priolist: priority list for I915_PRIORITY_NORMAL
+	 */
+	struct i915_priolist default_priolist;
+
+	/**
+	 * @no_priolist: priority lists disabled
+	 */
+	bool no_priolist;
+
+	/**
+	 * @submit_reg: gen-specific execlist submission register
+	 * set to the ExecList Submission Port (elsp) register pre-Gen11 and to
+	 * the ExecList Submission Queue Contents register array for Gen11+
+	 */
+	u32 __iomem *submit_reg;
+
+	/**
+	 * @ctrl_reg: the enhanced execlists control register, used to load the
+	 * submit queue on the HW and to request preemptions to idle
+	 */
+	u32 __iomem *ctrl_reg;
+
+	/**
+	 * @port: execlist port states
+	 *
+	 * For each hardware ELSP (ExecList Submission Port) we keep
+	 * track of the last request and the number of times we submitted
+	 * that port to hw. We then count the number of times the hw reports
+	 * a context completion or preemption. As only one context can
+	 * be active on hw, we limit resubmission of context to port[0]. This
+	 * is called Lite Restore, of the context.
+	 */
+	struct execlist_port {
+		/**
+		 * @request_count: combined request and submission count
+		 */
+		struct i915_request *request_count;
+#define EXECLIST_COUNT_BITS 2
+#define port_request(p) ptr_mask_bits((p)->request_count, EXECLIST_COUNT_BITS)
+#define port_count(p) ptr_unmask_bits((p)->request_count, EXECLIST_COUNT_BITS)
+#define port_pack(rq, count) ptr_pack_bits(rq, count, EXECLIST_COUNT_BITS)
+#define port_unpack(p, count) ptr_unpack_bits((p)->request_count, count, EXECLIST_COUNT_BITS)
+#define port_set(p, packed) ((p)->request_count = (packed))
+#define port_isset(p) ((p)->request_count)
+#define port_index(p, execlists) ((p) - (execlists)->port)
+
+		/**
+		 * @context_id: context ID for port
+		 */
+		GEM_DEBUG_DECL(u32 context_id);
+
+#define EXECLIST_MAX_PORTS 2
+	} port[EXECLIST_MAX_PORTS];
+
+	/**
+	 * @active: is the HW active? We consider the HW as active after
+	 * submitting any context for execution and until we have seen the
+	 * last context completion event. After that, we do not expect any
+	 * more events until we submit, and so can park the HW.
+	 *
+	 * As we have a small number of different sources from which we feed
+	 * the HW, we track the state of each inside a single bitfield.
+	 */
+	unsigned int active;
+#define EXECLISTS_ACTIVE_USER 0
+#define EXECLISTS_ACTIVE_PREEMPT 1
+#define EXECLISTS_ACTIVE_HWACK 2
+
+	/**
+	 * @port_mask: number of execlist ports - 1
+	 */
+	unsigned int port_mask;
+
+	/**
+	 * @queue_priority_hint: Highest pending priority.
+	 *
+	 * When we add requests into the queue, or adjust the priority of
+	 * executing requests, we compute the maximum priority of those
+	 * pending requests. We can then use this value to determine if
+	 * we need to preempt the executing requests to service the queue.
+	 * However, since the we may have recorded the priority of an inflight
+	 * request we wanted to preempt but since completed, at the time of
+	 * dequeuing the priority hint may no longer may match the highest
+	 * available request priority.
+	 */
+	int queue_priority_hint;
+
+	/**
+	 * @queue: queue of requests, in priority lists
+	 */
+	struct rb_root_cached queue;
+
+	/**
+	 * @csb_write: control register for Context Switch buffer
+	 *
+	 * Note this register may be either mmio or HWSP shadow.
+	 */
+	u32 *csb_write;
+
+	/**
+	 * @csb_status: status array for Context Switch buffer
+	 *
+	 * Note these register may be either mmio or HWSP shadow.
+	 */
+	u32 *csb_status;
+
+	/**
+	 * @preempt_complete_status: expected CSB upon completing preemption
+	 */
+	u32 preempt_complete_status;
+
+	/**
+	 * @csb_size: context status buffer FIFO size
+	 */
+	u8 csb_size;
+
+	/**
+	 * @csb_head: context status buffer head
+	 */
+	u8 csb_head;
+
+	I915_SELFTEST_DECLARE(struct st_preempt_hang preempt_hang;)
+};
+
+#define INTEL_ENGINE_CS_MAX_NAME 8
+
+struct intel_engine_cs {
+	struct drm_i915_private *i915;
+	struct intel_uncore *uncore;
+	char name[INTEL_ENGINE_CS_MAX_NAME];
+
+	enum intel_engine_id id;
+	unsigned int hw_id;
+	unsigned int guc_id;
+	intel_engine_mask_t mask;
+
+	u8 uabi_class;
+
+	u8 class;
+	u8 instance;
+	u32 context_size;
+	u32 mmio_base;
+
+	struct intel_sseu sseu;
+
+	struct intel_ring *buffer;
+
+	struct i915_timeline timeline;
+
+	struct intel_context *kernel_context; /* pinned */
+	struct intel_context *preempt_context; /* pinned; optional */
+
+	struct drm_i915_gem_object *default_state;
+	void *pinned_default_state;
+
+	/* Rather than have every client wait upon all user interrupts,
+	 * with the herd waking after every interrupt and each doing the
+	 * heavyweight seqno dance, we delegate the task (of being the
+	 * bottom-half of the user interrupt) to the first client. After
+	 * every interrupt, we wake up one client, who does the heavyweight
+	 * coherent seqno read and either goes back to sleep (if incomplete),
+	 * or wakes up all the completed clients in parallel, before then
+	 * transferring the bottom-half status to the next client in the queue.
+	 *
+	 * Compared to walking the entire list of waiters in a single dedicated
+	 * bottom-half, we reduce the latency of the first waiter by avoiding
+	 * a context switch, but incur additional coherent seqno reads when
+	 * following the chain of request breadcrumbs. Since it is most likely
+	 * that we have a single client waiting on each seqno, then reducing
+	 * the overhead of waking that client is much preferred.
+	 */
+	struct intel_breadcrumbs {
+		spinlock_t irq_lock;
+		struct list_head signalers;
+
+		struct irq_work irq_work; /* for use from inside irq_lock */
+
+		unsigned int irq_enabled;
+
+		bool irq_armed;
+	} breadcrumbs;
+
+	struct intel_engine_pmu {
+		/**
+		 * @enable: Bitmask of enable sample events on this engine.
+		 *
+		 * Bits correspond to sample event types, for instance
+		 * I915_SAMPLE_QUEUED is bit 0 etc.
+		 */
+		u32 enable;
+		/**
+		 * @enable_count: Reference count for the enabled samplers.
+		 *
+		 * Index number corresponds to @enum drm_i915_pmu_engine_sample.
+		 */
+		unsigned int enable_count[I915_ENGINE_SAMPLE_COUNT];
+		/**
+		 * @sample: Counter values for sampling events.
+		 *
+		 * Our internal timer stores the current counters in this field.
+		 *
+		 * Index number corresponds to @enum drm_i915_pmu_engine_sample.
+		 */
+		struct i915_pmu_sample sample[I915_ENGINE_SAMPLE_COUNT];
+	} pmu;
+
+	/*
+	 * A pool of objects to use as shadow copies of client batch buffers
+	 * when the command parser is enabled. Prevents the client from
+	 * modifying the batch contents after software parsing.
+	 */
+	struct i915_gem_batch_pool batch_pool;
+
+	struct intel_hw_status_page status_page;
+	struct i915_ctx_workarounds wa_ctx;
+	struct i915_wa_list ctx_wa_list;
+	struct i915_wa_list wa_list;
+	struct i915_wa_list whitelist;
+
+	u32             irq_keep_mask; /* always keep these interrupts */
+	u32		irq_enable_mask; /* bitmask to enable ring interrupt */
+	void		(*irq_enable)(struct intel_engine_cs *engine);
+	void		(*irq_disable)(struct intel_engine_cs *engine);
+
+	int		(*init_hw)(struct intel_engine_cs *engine);
+
+	struct {
+		void (*prepare)(struct intel_engine_cs *engine);
+		void (*reset)(struct intel_engine_cs *engine, bool stalled);
+		void (*finish)(struct intel_engine_cs *engine);
+	} reset;
+
+	void		(*park)(struct intel_engine_cs *engine);
+	void		(*unpark)(struct intel_engine_cs *engine);
+
+	void		(*set_default_submission)(struct intel_engine_cs *engine);
+
+	const struct intel_context_ops *cops;
+
+	int		(*request_alloc)(struct i915_request *rq);
+	int		(*init_context)(struct i915_request *rq);
+
+	int		(*emit_flush)(struct i915_request *request, u32 mode);
+#define EMIT_INVALIDATE	BIT(0)
+#define EMIT_FLUSH	BIT(1)
+#define EMIT_BARRIER	(EMIT_INVALIDATE | EMIT_FLUSH)
+	int		(*emit_bb_start)(struct i915_request *rq,
+					 u64 offset, u32 length,
+					 unsigned int dispatch_flags);
+#define I915_DISPATCH_SECURE BIT(0)
+#define I915_DISPATCH_PINNED BIT(1)
+	int		 (*emit_init_breadcrumb)(struct i915_request *rq);
+	u32		*(*emit_fini_breadcrumb)(struct i915_request *rq,
+						 u32 *cs);
+	unsigned int	emit_fini_breadcrumb_dw;
+
+	/* Pass the request to the hardware queue (e.g. directly into
+	 * the legacy ringbuffer or to the end of an execlist).
+	 *
+	 * This is called from an atomic context with irqs disabled; must
+	 * be irq safe.
+	 */
+	void		(*submit_request)(struct i915_request *rq);
+
+	/*
+	 * Call when the priority on a request has changed and it and its
+	 * dependencies may need rescheduling. Note the request itself may
+	 * not be ready to run!
+	 */
+	void		(*schedule)(struct i915_request *request,
+				    const struct i915_sched_attr *attr);
+
+	/*
+	 * Cancel all requests on the hardware, or queued for execution.
+	 * This should only cancel the ready requests that have been
+	 * submitted to the engine (via the engine->submit_request callback).
+	 * This is called when marking the device as wedged.
+	 */
+	void		(*cancel_requests)(struct intel_engine_cs *engine);
+
+	void		(*cleanup)(struct intel_engine_cs *engine);
+
+	struct intel_engine_execlists execlists;
+
+	/* Contexts are pinned whilst they are active on the GPU. The last
+	 * context executed remains active whilst the GPU is idle - the
+	 * switch away and write to the context object only occurs on the
+	 * next execution.  Contexts are only unpinned on retirement of the
+	 * following request ensuring that we can always write to the object
+	 * on the context switch even after idling. Across suspend, we switch
+	 * to the kernel context and trash it as the save may not happen
+	 * before the hardware is powered down.
+	 */
+	struct intel_context *last_retired_context;
+
+	/* status_notifier: list of callbacks for context-switch changes */
+	struct atomic_notifier_head context_status_notifier;
+
+	struct intel_engine_hangcheck hangcheck;
+
+#define I915_ENGINE_NEEDS_CMD_PARSER BIT(0)
+#define I915_ENGINE_SUPPORTS_STATS   BIT(1)
+#define I915_ENGINE_HAS_PREEMPTION   BIT(2)
+#define I915_ENGINE_HAS_SEMAPHORES   BIT(3)
+#define I915_ENGINE_NEEDS_BREADCRUMB_TASKLET BIT(4)
+	unsigned int flags;
+
+	/*
+	 * Table of commands the command parser needs to know about
+	 * for this engine.
+	 */
+	DECLARE_HASHTABLE(cmd_hash, I915_CMD_HASH_ORDER);
+
+	/*
+	 * Table of registers allowed in commands that read/write registers.
+	 */
+	const struct drm_i915_reg_table *reg_tables;
+	int reg_table_count;
+
+	/*
+	 * Returns the bitmask for the length field of the specified command.
+	 * Return 0 for an unrecognized/invalid command.
+	 *
+	 * If the command parser finds an entry for a command in the engine's
+	 * cmd_tables, it gets the command's length based on the table entry.
+	 * If not, it calls this function to determine the per-engine length
+	 * field encoding for the command (i.e. different opcode ranges use
+	 * certain bits to encode the command length in the header).
+	 */
+	u32 (*get_cmd_length_mask)(u32 cmd_header);
+
+	struct {
+		/**
+		 * @lock: Lock protecting the below fields.
+		 */
+		seqlock_t lock;
+		/**
+		 * @enabled: Reference count indicating number of listeners.
+		 */
+		unsigned int enabled;
+		/**
+		 * @active: Number of contexts currently scheduled in.
+		 */
+		unsigned int active;
+		/**
+		 * @enabled_at: Timestamp when busy stats were enabled.
+		 */
+		ktime_t enabled_at;
+		/**
+		 * @start: Timestamp of the last idle to active transition.
+		 *
+		 * Idle is defined as active == 0, active is active > 0.
+		 */
+		ktime_t start;
+		/**
+		 * @total: Total time this engine was busy.
+		 *
+		 * Accumulated time not counting the most recent block in cases
+		 * where engine is currently busy (active > 0).
+		 */
+		ktime_t total;
+	} stats;
+};
+
+static inline bool
+intel_engine_needs_cmd_parser(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_NEEDS_CMD_PARSER;
+}
+
+static inline bool
+intel_engine_supports_stats(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_SUPPORTS_STATS;
+}
+
+static inline bool
+intel_engine_has_preemption(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_HAS_PREEMPTION;
+}
+
+static inline bool
+intel_engine_has_semaphores(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_HAS_SEMAPHORES;
+}
+
+static inline bool
+intel_engine_needs_breadcrumb_tasklet(const struct intel_engine_cs *engine)
+{
+	return engine->flags & I915_ENGINE_NEEDS_BREADCRUMB_TASKLET;
+}
+
+#define instdone_slice_mask(dev_priv__) \
+	(IS_GEN(dev_priv__, 7) ? \
+	 1 : RUNTIME_INFO(dev_priv__)->sseu.slice_mask)
+
+#define instdone_subslice_mask(dev_priv__) \
+	(IS_GEN(dev_priv__, 7) ? \
+	 1 : RUNTIME_INFO(dev_priv__)->sseu.subslice_mask[0])
+
+#define for_each_instdone_slice_subslice(dev_priv__, slice__, subslice__) \
+	for ((slice__) = 0, (subslice__) = 0; \
+	     (slice__) < I915_MAX_SLICES; \
+	     (subslice__) = ((subslice__) + 1) < I915_MAX_SUBSLICES ? (subslice__) + 1 : 0, \
+	       (slice__) += ((subslice__) == 0)) \
+		for_each_if((BIT(slice__) & instdone_slice_mask(dev_priv__)) && \
+			    (BIT(subslice__) & instdone_subslice_mask(dev_priv__)))
+
+#endif /* __INTEL_ENGINE_TYPES_H__ */
