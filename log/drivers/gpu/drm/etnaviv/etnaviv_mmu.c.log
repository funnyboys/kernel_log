commit 18fa692d8020083cd57ce031a4b5a7a4ec8bc50a
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Wed Oct 16 16:10:21 2019 +0200

    drm/etnaviv: reinstate MMUv1 command buffer window check
    
    The switch to per-process address spaces erroneously dropped the check
    which validated that the command buffer is mapped through the linear
    apperture as required by the hardware. This turned a system
    misconfiguration with a helpful error message into a very hard to
    debug issue. Reinstate the check at the appropriate location.
    
    Fixes: 17e4660ae3d7 (drm/etnaviv: implement per-process address spaces on MMUv2)
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 35ebae6a1be7..3607d348c298 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -328,12 +328,23 @@ etnaviv_iommu_context_init(struct etnaviv_iommu_global *global,
 
 	ret = etnaviv_cmdbuf_suballoc_map(suballoc, ctx, &ctx->cmdbuf_mapping,
 					  global->memory_base);
-	if (ret) {
-		global->ops->free(ctx);
-		return NULL;
+	if (ret)
+		goto out_free;
+
+	if (global->version == ETNAVIV_IOMMU_V1 &&
+	    ctx->cmdbuf_mapping.iova > 0x80000000) {
+		dev_err(global->dev,
+		        "command buffer outside valid memory window\n");
+		goto out_unmap;
 	}
 
 	return ctx;
+
+out_unmap:
+	etnaviv_cmdbuf_suballoc_unmap(ctx, &ctx->cmdbuf_mapping);
+out_free:
+	global->ops->free(ctx);
+	return NULL;
 }
 
 void etnaviv_iommu_restore(struct etnaviv_gpu *gpu,

commit 17eae23b08207a8954f77ca74caeaeb72a04da67
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Aug 2 14:19:38 2019 +0200

    drm/etnaviv: allow to request specific virtual address for gem mapping
    
    Allow the mapping code to request a specific virtual address for the gem
    mapping. If the virtual address is zero we fall back to the old mode of
    allocating a virtual address for the mapping.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 82822e30bf30..35ebae6a1be7 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -220,9 +220,16 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu_context *context,
 	return ret;
 }
 
+static int etnaviv_iommu_insert_exact(struct etnaviv_iommu_context *context,
+		   struct drm_mm_node *node, size_t size, u64 va)
+{
+	return drm_mm_insert_node_in_range(&context->mm, node, size, 0, 0, va,
+					   va + size, DRM_MM_INSERT_LOWEST);
+}
+
 int etnaviv_iommu_map_gem(struct etnaviv_iommu_context *context,
 	struct etnaviv_gem_object *etnaviv_obj, u32 memory_base,
-	struct etnaviv_vram_mapping *mapping)
+	struct etnaviv_vram_mapping *mapping, u64 va)
 {
 	struct sg_table *sgt = etnaviv_obj->sgt;
 	struct drm_mm_node *node;
@@ -248,7 +255,12 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu_context *context,
 
 	node = &mapping->vram_node;
 
-	ret = etnaviv_iommu_find_iova(context, node, etnaviv_obj->base.size);
+	if (va)
+		ret = etnaviv_iommu_insert_exact(context, node,
+						 etnaviv_obj->base.size, va);
+	else
+		ret = etnaviv_iommu_find_iova(context, node,
+					      etnaviv_obj->base.size);
 	if (ret < 0)
 		goto unlock;
 

commit 17e4660ae3d7e14120f0b355d3d1995cd10a3e43
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Jul 5 19:17:27 2019 +0200

    drm/etnaviv: implement per-process address spaces on MMUv2
    
    This builds on top of the MMU contexts introduced earlier. Instead of having
    one context per GPU core, each GPU client receives its own context.
    
    On MMUv1 this still means a single shared pagetable set is used by all
    clients, but on MMUv2 there is now a distinct set of pagetables for each
    client. As the command fetch is also translated via the MMU on MMUv2 the
    kernel command ringbuffer is mapped into each of the client pagetables.
    
    As the MMU context switch is a bit of a heavy operation, due to the needed
    cache and TLB flushing, this patch implements a lazy way of switching the
    MMU context. The kernel does not have its own MMU context, but reuses the
    last client context for all of its operations. This has some visible impact,
    as the GPU can now only be started once a client has submitted some work and
    we got the client MMU context assigned. Also the MMU context has a different
    lifetime than the general client context, as the GPU might still execute the
    kernel command buffer in the context of a client even after the client has
    completed all GPU work and has been terminated. Only when the GPU is runtime
    suspended or switches to another clients MMU context is the old context
    freed up.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 2f64eef773ed..82822e30bf30 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -290,6 +290,8 @@ static void etnaviv_iommu_context_free(struct kref *kref)
 	struct etnaviv_iommu_context *context =
 		container_of(kref, struct etnaviv_iommu_context, refcount);
 
+	etnaviv_cmdbuf_suballoc_unmap(context, &context->cmdbuf_mapping);
+
 	context->global->ops->free(context);
 }
 void etnaviv_iommu_context_put(struct etnaviv_iommu_context *context)
@@ -298,12 +300,28 @@ void etnaviv_iommu_context_put(struct etnaviv_iommu_context *context)
 }
 
 struct etnaviv_iommu_context *
-etnaviv_iommu_context_init(struct etnaviv_iommu_global *global)
+etnaviv_iommu_context_init(struct etnaviv_iommu_global *global,
+			   struct etnaviv_cmdbuf_suballoc *suballoc)
 {
+	struct etnaviv_iommu_context *ctx;
+	int ret;
+
 	if (global->version == ETNAVIV_IOMMU_V1)
-		return etnaviv_iommuv1_context_alloc(global);
+		ctx = etnaviv_iommuv1_context_alloc(global);
 	else
-		return etnaviv_iommuv2_context_alloc(global);
+		ctx = etnaviv_iommuv2_context_alloc(global);
+
+	if (!ctx)
+		return NULL;
+
+	ret = etnaviv_cmdbuf_suballoc_map(suballoc, ctx, &ctx->cmdbuf_mapping,
+					  global->memory_base);
+	if (ret) {
+		global->ops->free(ctx);
+		return NULL;
+	}
+
+	return ctx;
 }
 
 void etnaviv_iommu_restore(struct etnaviv_gpu *gpu,
@@ -319,6 +337,12 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu_context *context,
 {
 	mutex_lock(&context->lock);
 
+	if (mapping->use > 0) {
+		mapping->use++;
+		mutex_unlock(&context->lock);
+		return 0;
+	}
+
 	/*
 	 * For MMUv1 we don't add the suballoc region to the pagetables, as
 	 * those GPUs can only work with cmdbufs accessed through the linear
@@ -340,7 +364,6 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu_context *context,
 		mapping->iova = node->start;
 		ret = etnaviv_context_map(context, node->start, paddr, size,
 					  ETNAVIV_PROT_READ);
-
 		if (ret < 0) {
 			drm_mm_remove_node(node);
 			mutex_unlock(&context->lock);
@@ -363,15 +386,14 @@ void etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu_context *context,
 {
 	struct drm_mm_node *node = &mapping->vram_node;
 
-	if (!mapping->use)
-		return;
-
-	mapping->use = 0;
+	mutex_lock(&context->lock);
+	mapping->use--;
 
-	if (context->global->version == ETNAVIV_IOMMU_V1)
+	if (mapping->use > 0 || context->global->version == ETNAVIV_IOMMU_V1) {
+		mutex_unlock(&context->lock);
 		return;
+	}
 
-	mutex_lock(&context->lock);
 	etnaviv_context_unmap(context, node->start, node->size);
 	drm_mm_remove_node(node);
 	mutex_unlock(&context->lock);

commit 27b67278e007b5475bc14918794bf73cf297a026
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Jul 5 19:17:24 2019 +0200

    drm/etnaviv: rework MMU handling
    
    This reworks the MMU handling to make it possible to have multiple MMU contexts.
    A context is basically one instance of GPU page tables. Currently we have one
    set of page tables per GPU, which isn't all that clever, as it has the
    following two consequences:
    
    1. All GPU clients (aka processes) are sharing the same pagetables, which means
    there is no isolation between clients, but only between GPU assigned memory
    spaces and the rest of the system. Better than nothing, but also not great.
    
    2. Clients operating on the same set of buffers with different etnaviv GPU
    cores, e.g. a workload using both the 2D and 3D GPU, need to map the used
    buffers into the pagetable sets of each used GPU.
    
    This patch reworks all the MMU handling to introduce the abstraction of the
    MMU context. A context can be shared across different GPU cores, as long as
    they have compatible MMU implementations, which is the case for all systems
    with Vivante GPUs seen in the wild.
    
    As MMUv1 is not able to change pagetables on the fly, without a
    "stop the world" operation, which stops GPU, changes pagetables via CPU
    interaction, restarts GPU, the implementation introduces a shared context on
    MMUv1, which is returned whenever there is a request for a new context.
    
    This patch assigns a MMU context to each GPU, so on MMUv2 systems there is
    still one set of pagetables per GPU, but due to the shared context MMUv1
    systems see a change in behavior as now a single pagetable set is used
    across all GPU cores.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index bbd1624a3df8..2f64eef773ed 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -3,6 +3,7 @@
  * Copyright (C) 2015-2018 Etnaviv Project
  */
 
+#include <linux/dma-mapping.h>
 #include <linux/scatterlist.h>
 
 #include "common.xml.h"
@@ -10,10 +11,9 @@
 #include "etnaviv_drv.h"
 #include "etnaviv_gem.h"
 #include "etnaviv_gpu.h"
-#include "etnaviv_iommu.h"
 #include "etnaviv_mmu.h"
 
-static void etnaviv_domain_unmap(struct etnaviv_iommu_domain *domain,
+static void etnaviv_context_unmap(struct etnaviv_iommu_context *context,
 				 unsigned long iova, size_t size)
 {
 	size_t unmapped_page, unmapped = 0;
@@ -26,7 +26,8 @@ static void etnaviv_domain_unmap(struct etnaviv_iommu_domain *domain,
 	}
 
 	while (unmapped < size) {
-		unmapped_page = domain->ops->unmap(domain, iova, pgsize);
+		unmapped_page = context->global->ops->unmap(context, iova,
+							    pgsize);
 		if (!unmapped_page)
 			break;
 
@@ -35,7 +36,7 @@ static void etnaviv_domain_unmap(struct etnaviv_iommu_domain *domain,
 	}
 }
 
-static int etnaviv_domain_map(struct etnaviv_iommu_domain *domain,
+static int etnaviv_context_map(struct etnaviv_iommu_context *context,
 			      unsigned long iova, phys_addr_t paddr,
 			      size_t size, int prot)
 {
@@ -51,7 +52,8 @@ static int etnaviv_domain_map(struct etnaviv_iommu_domain *domain,
 	}
 
 	while (size) {
-		ret = domain->ops->map(domain, iova, paddr, pgsize, prot);
+		ret = context->global->ops->map(context, iova, paddr, pgsize,
+						prot);
 		if (ret)
 			break;
 
@@ -62,21 +64,19 @@ static int etnaviv_domain_map(struct etnaviv_iommu_domain *domain,
 
 	/* unroll mapping in case something went wrong */
 	if (ret)
-		etnaviv_domain_unmap(domain, orig_iova, orig_size - size);
+		etnaviv_context_unmap(context, orig_iova, orig_size - size);
 
 	return ret;
 }
 
-static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
+static int etnaviv_iommu_map(struct etnaviv_iommu_context *context, u32 iova,
 			     struct sg_table *sgt, unsigned len, int prot)
-{
-	struct etnaviv_iommu_domain *domain = iommu->domain;
-	struct scatterlist *sg;
+{	struct scatterlist *sg;
 	unsigned int da = iova;
 	unsigned int i, j;
 	int ret;
 
-	if (!domain || !sgt)
+	if (!context || !sgt)
 		return -EINVAL;
 
 	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
@@ -85,7 +85,7 @@ static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 
 		VERB("map[%d]: %08x %08x(%zx)", i, iova, pa, bytes);
 
-		ret = etnaviv_domain_map(domain, da, pa, bytes, prot);
+		ret = etnaviv_context_map(context, da, pa, bytes, prot);
 		if (ret)
 			goto fail;
 
@@ -100,16 +100,15 @@ static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 	for_each_sg(sgt->sgl, sg, i, j) {
 		size_t bytes = sg_dma_len(sg) + sg->offset;
 
-		etnaviv_domain_unmap(domain, da, bytes);
+		etnaviv_context_unmap(context, da, bytes);
 		da += bytes;
 	}
 	return ret;
 }
 
-static void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
+static void etnaviv_iommu_unmap(struct etnaviv_iommu_context *context, u32 iova,
 				struct sg_table *sgt, unsigned len)
 {
-	struct etnaviv_iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;
 	unsigned int da = iova;
 	int i;
@@ -117,7 +116,7 @@ static void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
 	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
 		size_t bytes = sg_dma_len(sg) + sg->offset;
 
-		etnaviv_domain_unmap(domain, da, bytes);
+		etnaviv_context_unmap(context, da, bytes);
 
 		VERB("unmap[%d]: %08x(%zx)", i, iova, bytes);
 
@@ -127,24 +126,24 @@ static void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
 	}
 }
 
-static void etnaviv_iommu_remove_mapping(struct etnaviv_iommu *mmu,
+static void etnaviv_iommu_remove_mapping(struct etnaviv_iommu_context *context,
 	struct etnaviv_vram_mapping *mapping)
 {
 	struct etnaviv_gem_object *etnaviv_obj = mapping->object;
 
-	etnaviv_iommu_unmap(mmu, mapping->vram_node.start,
+	etnaviv_iommu_unmap(context, mapping->vram_node.start,
 			    etnaviv_obj->sgt, etnaviv_obj->base.size);
 	drm_mm_remove_node(&mapping->vram_node);
 }
 
-static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
+static int etnaviv_iommu_find_iova(struct etnaviv_iommu_context *context,
 				   struct drm_mm_node *node, size_t size)
 {
 	struct etnaviv_vram_mapping *free = NULL;
 	enum drm_mm_insert_mode mode = DRM_MM_INSERT_LOW;
 	int ret;
 
-	lockdep_assert_held(&mmu->lock);
+	lockdep_assert_held(&context->lock);
 
 	while (1) {
 		struct etnaviv_vram_mapping *m, *n;
@@ -152,17 +151,17 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		struct list_head list;
 		bool found;
 
-		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
+		ret = drm_mm_insert_node_in_range(&context->mm, node,
 						  size, 0, 0, 0, U64_MAX, mode);
 		if (ret != -ENOSPC)
 			break;
 
 		/* Try to retire some entries */
-		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, mode);
+		drm_mm_scan_init(&scan, &context->mm, size, 0, 0, mode);
 
 		found = 0;
 		INIT_LIST_HEAD(&list);
-		list_for_each_entry(free, &mmu->mappings, mmu_node) {
+		list_for_each_entry(free, &context->mappings, mmu_node) {
 			/* If this vram node has not been used, skip this. */
 			if (!free->vram_node.mm)
 				continue;
@@ -204,8 +203,8 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		 * this mapping.
 		 */
 		list_for_each_entry_safe(m, n, &list, scan_node) {
-			etnaviv_iommu_remove_mapping(mmu, m);
-			m->mmu = NULL;
+			etnaviv_iommu_remove_mapping(context, m);
+			m->context = NULL;
 			list_del_init(&m->mmu_node);
 			list_del_init(&m->scan_node);
 		}
@@ -221,7 +220,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 	return ret;
 }
 
-int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
+int etnaviv_iommu_map_gem(struct etnaviv_iommu_context *context,
 	struct etnaviv_gem_object *etnaviv_obj, u32 memory_base,
 	struct etnaviv_vram_mapping *mapping)
 {
@@ -231,17 +230,17 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 
 	lockdep_assert_held(&etnaviv_obj->lock);
 
-	mutex_lock(&mmu->lock);
+	mutex_lock(&context->lock);
 
 	/* v1 MMU can optimize single entry (contiguous) scatterlists */
-	if (mmu->version == ETNAVIV_IOMMU_V1 &&
+	if (context->global->version == ETNAVIV_IOMMU_V1 &&
 	    sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
 		u32 iova;
 
 		iova = sg_dma_address(sgt->sgl) - memory_base;
 		if (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {
 			mapping->iova = iova;
-			list_add_tail(&mapping->mmu_node, &mmu->mappings);
+			list_add_tail(&mapping->mmu_node, &context->mappings);
 			ret = 0;
 			goto unlock;
 		}
@@ -249,12 +248,12 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 
 	node = &mapping->vram_node;
 
-	ret = etnaviv_iommu_find_iova(mmu, node, etnaviv_obj->base.size);
+	ret = etnaviv_iommu_find_iova(context, node, etnaviv_obj->base.size);
 	if (ret < 0)
 		goto unlock;
 
 	mapping->iova = node->start;
-	ret = etnaviv_iommu_map(mmu, node->start, sgt, etnaviv_obj->base.size,
+	ret = etnaviv_iommu_map(context, node->start, sgt, etnaviv_obj->base.size,
 				ETNAVIV_PROT_READ | ETNAVIV_PROT_WRITE);
 
 	if (ret < 0) {
@@ -262,84 +261,63 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 		goto unlock;
 	}
 
-	list_add_tail(&mapping->mmu_node, &mmu->mappings);
-	mmu->flush_seq++;
+	list_add_tail(&mapping->mmu_node, &context->mappings);
+	context->flush_seq++;
 unlock:
-	mutex_unlock(&mmu->lock);
+	mutex_unlock(&context->lock);
 
 	return ret;
 }
 
-void etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,
+void etnaviv_iommu_unmap_gem(struct etnaviv_iommu_context *context,
 	struct etnaviv_vram_mapping *mapping)
 {
 	WARN_ON(mapping->use);
 
-	mutex_lock(&mmu->lock);
+	mutex_lock(&context->lock);
 
 	/* If the vram node is on the mm, unmap and remove the node */
-	if (mapping->vram_node.mm == &mmu->mm)
-		etnaviv_iommu_remove_mapping(mmu, mapping);
+	if (mapping->vram_node.mm == &context->mm)
+		etnaviv_iommu_remove_mapping(context, mapping);
 
 	list_del(&mapping->mmu_node);
-	mmu->flush_seq++;
-	mutex_unlock(&mmu->lock);
+	context->flush_seq++;
+	mutex_unlock(&context->lock);
 }
 
-void etnaviv_iommu_destroy(struct etnaviv_iommu *mmu)
+static void etnaviv_iommu_context_free(struct kref *kref)
 {
-	drm_mm_takedown(&mmu->mm);
-	mmu->domain->ops->free(mmu->domain);
-	kfree(mmu);
-}
+	struct etnaviv_iommu_context *context =
+		container_of(kref, struct etnaviv_iommu_context, refcount);
 
-struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu)
+	context->global->ops->free(context);
+}
+void etnaviv_iommu_context_put(struct etnaviv_iommu_context *context)
 {
-	enum etnaviv_iommu_version version;
-	struct etnaviv_iommu *mmu;
-
-	mmu = kzalloc(sizeof(*mmu), GFP_KERNEL);
-	if (!mmu)
-		return ERR_PTR(-ENOMEM);
-
-	if (!(gpu->identity.minor_features1 & chipMinorFeatures1_MMU_VERSION)) {
-		mmu->domain = etnaviv_iommuv1_domain_alloc(gpu);
-		version = ETNAVIV_IOMMU_V1;
-	} else {
-		mmu->domain = etnaviv_iommuv2_domain_alloc(gpu);
-		version = ETNAVIV_IOMMU_V2;
-	}
-
-	if (!mmu->domain) {
-		dev_err(gpu->dev, "Failed to allocate GPU IOMMU domain\n");
-		kfree(mmu);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	mmu->gpu = gpu;
-	mmu->version = version;
-	mutex_init(&mmu->lock);
-	INIT_LIST_HEAD(&mmu->mappings);
-
-	drm_mm_init(&mmu->mm, mmu->domain->base, mmu->domain->size);
-
-	return mmu;
+	kref_put(&context->refcount, etnaviv_iommu_context_free);
 }
 
-void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
+struct etnaviv_iommu_context *
+etnaviv_iommu_context_init(struct etnaviv_iommu_global *global)
 {
-	if (gpu->mmu->version == ETNAVIV_IOMMU_V1)
-		etnaviv_iommuv1_restore(gpu);
+	if (global->version == ETNAVIV_IOMMU_V1)
+		return etnaviv_iommuv1_context_alloc(global);
 	else
-		etnaviv_iommuv2_restore(gpu);
+		return etnaviv_iommuv2_context_alloc(global);
+}
+
+void etnaviv_iommu_restore(struct etnaviv_gpu *gpu,
+			   struct etnaviv_iommu_context *context)
+{
+	context->global->ops->restore(gpu, context);
 }
 
-int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu *mmu,
+int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu_context *context,
 				  struct etnaviv_vram_mapping *mapping,
 				  u32 memory_base, dma_addr_t paddr,
 				  size_t size)
 {
-	mutex_lock(&mmu->lock);
+	mutex_lock(&context->lock);
 
 	/*
 	 * For MMUv1 we don't add the suballoc region to the pagetables, as
@@ -347,40 +325,40 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu *mmu,
 	 * window. Instead we manufacture a mapping to make it look uniform
 	 * to the upper layers.
 	 */
-	if (mmu->version == ETNAVIV_IOMMU_V1) {
+	if (context->global->version == ETNAVIV_IOMMU_V1) {
 		mapping->iova = paddr - memory_base;
 	} else {
 		struct drm_mm_node *node = &mapping->vram_node;
 		int ret;
 
-		ret = etnaviv_iommu_find_iova(mmu, node, size);
+		ret = etnaviv_iommu_find_iova(context, node, size);
 		if (ret < 0) {
-			mutex_unlock(&mmu->lock);
+			mutex_unlock(&context->lock);
 			return ret;
 		}
 
 		mapping->iova = node->start;
-		ret = etnaviv_domain_map(mmu->domain, node->start, paddr, size,
-					 ETNAVIV_PROT_READ);
+		ret = etnaviv_context_map(context, node->start, paddr, size,
+					  ETNAVIV_PROT_READ);
 
 		if (ret < 0) {
 			drm_mm_remove_node(node);
-			mutex_unlock(&mmu->lock);
+			mutex_unlock(&context->lock);
 			return ret;
 		}
 
-		mmu->flush_seq++;
+		context->flush_seq++;
 	}
 
-	list_add_tail(&mapping->mmu_node, &mmu->mappings);
+	list_add_tail(&mapping->mmu_node, &context->mappings);
 	mapping->use = 1;
 
-	mutex_unlock(&mmu->lock);
+	mutex_unlock(&context->lock);
 
 	return 0;
 }
 
-void etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu *mmu,
+void etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu_context *context,
 		  struct etnaviv_vram_mapping *mapping)
 {
 	struct drm_mm_node *node = &mapping->vram_node;
@@ -390,21 +368,104 @@ void etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu *mmu,
 
 	mapping->use = 0;
 
-	if (mmu->version == ETNAVIV_IOMMU_V1)
+	if (context->global->version == ETNAVIV_IOMMU_V1)
 		return;
 
-	mutex_lock(&mmu->lock);
-	etnaviv_domain_unmap(mmu->domain, node->start, node->size);
+	mutex_lock(&context->lock);
+	etnaviv_context_unmap(context, node->start, node->size);
 	drm_mm_remove_node(node);
-	mutex_unlock(&mmu->lock);
+	mutex_unlock(&context->lock);
+}
+
+size_t etnaviv_iommu_dump_size(struct etnaviv_iommu_context *context)
+{
+	return context->global->ops->dump_size(context);
+}
+
+void etnaviv_iommu_dump(struct etnaviv_iommu_context *context, void *buf)
+{
+	context->global->ops->dump(context, buf);
 }
 
-size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
+int etnaviv_iommu_global_init(struct etnaviv_gpu *gpu)
 {
-	return iommu->domain->ops->dump_size(iommu->domain);
+	enum etnaviv_iommu_version version = ETNAVIV_IOMMU_V1;
+	struct etnaviv_drm_private *priv = gpu->drm->dev_private;
+	struct etnaviv_iommu_global *global;
+	struct device *dev = gpu->drm->dev;
+
+	if (gpu->identity.minor_features1 & chipMinorFeatures1_MMU_VERSION)
+		version = ETNAVIV_IOMMU_V2;
+
+	if (priv->mmu_global) {
+		if (priv->mmu_global->version != version) {
+			dev_err(gpu->dev,
+				"MMU version doesn't match global version\n");
+			return -ENXIO;
+		}
+
+		priv->mmu_global->use++;
+		return 0;
+	}
+
+	global = kzalloc(sizeof(*global), GFP_KERNEL);
+	if (!global)
+		return -ENOMEM;
+
+	global->bad_page_cpu = dma_alloc_wc(dev, SZ_4K, &global->bad_page_dma,
+					    GFP_KERNEL);
+	if (!global->bad_page_cpu)
+		goto free_global;
+
+	memset32(global->bad_page_cpu, 0xdead55aa, SZ_4K / sizeof(u32));
+
+	if (version == ETNAVIV_IOMMU_V2) {
+		global->v2.pta_cpu = dma_alloc_wc(dev, ETNAVIV_PTA_SIZE,
+					       &global->v2.pta_dma, GFP_KERNEL);
+		if (!global->v2.pta_cpu)
+			goto free_bad_page;
+	}
+
+	global->dev = dev;
+	global->version = version;
+	global->use = 1;
+	mutex_init(&global->lock);
+
+	if (version == ETNAVIV_IOMMU_V1)
+		global->ops = &etnaviv_iommuv1_ops;
+	else
+		global->ops = &etnaviv_iommuv2_ops;
+
+	priv->mmu_global = global;
+
+	return 0;
+
+free_bad_page:
+	dma_free_wc(dev, SZ_4K, global->bad_page_cpu, global->bad_page_dma);
+free_global:
+	kfree(global);
+
+	return -ENOMEM;
 }
 
-void etnaviv_iommu_dump(struct etnaviv_iommu *iommu, void *buf)
+void etnaviv_iommu_global_fini(struct etnaviv_gpu *gpu)
 {
-	iommu->domain->ops->dump(iommu->domain, buf);
+	struct etnaviv_drm_private *priv = gpu->drm->dev_private;
+	struct etnaviv_iommu_global *global = priv->mmu_global;
+
+	if (--global->use > 0)
+		return;
+
+	if (global->v2.pta_cpu)
+		dma_free_wc(global->dev, ETNAVIV_PTA_SIZE,
+			    global->v2.pta_cpu, global->v2.pta_dma);
+
+	if (global->bad_page_cpu)
+		dma_free_wc(global->dev, SZ_4K,
+			    global->bad_page_cpu, global->bad_page_dma);
+
+	mutex_destroy(&global->lock);
+	kfree(global);
+
+	priv->mmu_global = NULL;
 }

commit 4900dda90af2cb13bc1d4c12ce94b98acc8fe64e
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Jul 5 19:17:23 2019 +0200

    drm/etnaviv: replace MMU flush marker with flush sequence
    
    If a MMU is shared between multiple GPUs, all of them need to flush their
    TLBs, so a single marker that gets reset on the first flush won't do.
    Replace the flush marker with a sequence number, so that it's possible to
    check if the TLB is in sync with the current page table state for each GPU.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 09f516093b91..bbd1624a3df8 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -263,7 +263,7 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 	}
 
 	list_add_tail(&mapping->mmu_node, &mmu->mappings);
-	mmu->need_flush = true;
+	mmu->flush_seq++;
 unlock:
 	mutex_unlock(&mmu->lock);
 
@@ -282,7 +282,7 @@ void etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,
 		etnaviv_iommu_remove_mapping(mmu, mapping);
 
 	list_del(&mapping->mmu_node);
-	mmu->need_flush = true;
+	mmu->flush_seq++;
 	mutex_unlock(&mmu->lock);
 }
 
@@ -369,7 +369,7 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu *mmu,
 			return ret;
 		}
 
-		mmu->need_flush = true;
+		mmu->flush_seq++;
 	}
 
 	list_add_tail(&mapping->mmu_node, &mmu->mappings);

commit db82a0435b8be32d544bbed91c43c2f21b5f4ea7
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Jul 5 19:17:21 2019 +0200

    drm/etnaviv: split out cmdbuf mapping into address space
    
    This allows to decouple the cmdbuf suballocator create and mapping
    the region into the GPU address space. Allowing multiple AS to share
    a single cmdbuf suballoc.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-by: Guido G端nther <agx@sigxcpu.org>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 731275999a57..09f516093b91 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -334,52 +334,71 @@ void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
 		etnaviv_iommuv2_restore(gpu);
 }
 
-int etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,
-				  struct drm_mm_node *vram_node, size_t size,
-				  u32 *iova)
+int etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu *mmu,
+				  struct etnaviv_vram_mapping *mapping,
+				  u32 memory_base, dma_addr_t paddr,
+				  size_t size)
 {
-	struct etnaviv_iommu *mmu = gpu->mmu;
+	mutex_lock(&mmu->lock);
 
+	/*
+	 * For MMUv1 we don't add the suballoc region to the pagetables, as
+	 * those GPUs can only work with cmdbufs accessed through the linear
+	 * window. Instead we manufacture a mapping to make it look uniform
+	 * to the upper layers.
+	 */
 	if (mmu->version == ETNAVIV_IOMMU_V1) {
-		*iova = paddr - gpu->memory_base;
-		return 0;
+		mapping->iova = paddr - memory_base;
 	} else {
+		struct drm_mm_node *node = &mapping->vram_node;
 		int ret;
 
-		mutex_lock(&mmu->lock);
-		ret = etnaviv_iommu_find_iova(mmu, vram_node, size);
+		ret = etnaviv_iommu_find_iova(mmu, node, size);
 		if (ret < 0) {
 			mutex_unlock(&mmu->lock);
 			return ret;
 		}
-		ret = etnaviv_domain_map(mmu->domain, vram_node->start, paddr,
-					 size, ETNAVIV_PROT_READ);
+
+		mapping->iova = node->start;
+		ret = etnaviv_domain_map(mmu->domain, node->start, paddr, size,
+					 ETNAVIV_PROT_READ);
+
 		if (ret < 0) {
-			drm_mm_remove_node(vram_node);
+			drm_mm_remove_node(node);
 			mutex_unlock(&mmu->lock);
 			return ret;
 		}
-		gpu->mmu->need_flush = true;
-		mutex_unlock(&mmu->lock);
 
-		*iova = (u32)vram_node->start;
-		return 0;
+		mmu->need_flush = true;
 	}
+
+	list_add_tail(&mapping->mmu_node, &mmu->mappings);
+	mapping->use = 1;
+
+	mutex_unlock(&mmu->lock);
+
+	return 0;
 }
 
-void etnaviv_iommu_put_suballoc_va(struct etnaviv_gpu *gpu,
-				   struct drm_mm_node *vram_node, size_t size,
-				   u32 iova)
+void etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu *mmu,
+		  struct etnaviv_vram_mapping *mapping)
 {
-	struct etnaviv_iommu *mmu = gpu->mmu;
+	struct drm_mm_node *node = &mapping->vram_node;
 
-	if (mmu->version == ETNAVIV_IOMMU_V2) {
-		mutex_lock(&mmu->lock);
-		etnaviv_domain_unmap(mmu->domain, iova, size);
-		drm_mm_remove_node(vram_node);
-		mutex_unlock(&mmu->lock);
-	}
+	if (!mapping->use)
+		return;
+
+	mapping->use = 0;
+
+	if (mmu->version == ETNAVIV_IOMMU_V1)
+		return;
+
+	mutex_lock(&mmu->lock);
+	etnaviv_domain_unmap(mmu->domain, node->start, node->size);
+	drm_mm_remove_node(node);
+	mutex_unlock(&mmu->lock);
 }
+
 size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
 {
 	return iommu->domain->ops->dump_size(iommu->domain);

commit 6eae41fea75039136707c02cf99431462d590c5f
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Sun Jun 30 07:21:03 2019 +0200

    drm/etnaviv: drop use of drmP.h
    
    Drop use of the deprecated drmP.h header file.
    Fix fallout in all .c files.
    
    The etnaviv_drv.h header file was made self-contained,
    and missing includes was then added to the .c files that needed them.
    In a few cases the list of include files was sorted.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Cc: Lucas Stach <l.stach@pengutronix.de>
    Cc: Russell King <linux+etnaviv@armlinux.org.uk>
    Cc: Christian Gmeiner <christian.gmeiner@gmail.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: etnaviv@lists.freedesktop.org
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 8069f9f36a2e..731275999a57 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -3,6 +3,8 @@
  * Copyright (C) 2015-2018 Etnaviv Project
  */
 
+#include <linux/scatterlist.h>
+
 #include "common.xml.h"
 #include "etnaviv_cmdbuf.h"
 #include "etnaviv_drv.h"

commit f6ffbd4fc1a1caafe2ab840993b917fba5324598
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue May 8 16:20:54 2018 +0200

    drm/etnaviv: replace license text with SPDX tags
    
    This replaces the repetitive GPL-2.0 license text in code and header files
    with the SPDX tags. Generated hardware headers aren't changed, as any changes
    there need to be done in the upstream rnndb repository.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian Gmeiner <christian.gmeiner@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index e8e8c4fe3242..8069f9f36a2e 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -1,17 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
- * Copyright (C) 2015 Etnaviv Project
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License version 2 as published by
- * the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program.  If not, see <http://www.gnu.org/licenses/>.
+ * Copyright (C) 2015-2018 Etnaviv Project
  */
 
 #include "common.xml.h"

commit ccae45928fc43d78d6ba7d0c6965b142c922a446
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Mar 9 12:53:34 2018 +0100

    drm/etnaviv: remove cycling through MMU address space
    
    This was useful on MMUv1 GPUs, which don't generate proper faults,
    when the GPU write caches weren't fully understood and not properly
    handled by the kernel driver. As this has been fixed for quite some
    time, the cycling though the MMU address space needlessly spreads
    out the MMU mappings.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 49e049713a52..e8e8c4fe3242 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -162,22 +162,10 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		bool found;
 
 		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
-						  size, 0, 0,
-						  mmu->last_iova, U64_MAX,
-						  mode);
+						  size, 0, 0, 0, U64_MAX, mode);
 		if (ret != -ENOSPC)
 			break;
 
-		/*
-		 * If we did not search from the start of the MMU region,
-		 * try again in case there are free slots.
-		 */
-		if (mmu->last_iova) {
-			mmu->last_iova = 0;
-			mmu->need_flush = true;
-			continue;
-		}
-
 		/* Try to retire some entries */
 		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, mode);
 
@@ -274,7 +262,6 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 	if (ret < 0)
 		goto unlock;
 
-	mmu->last_iova = node->start + etnaviv_obj->base.size;
 	mapping->iova = node->start;
 	ret = etnaviv_iommu_map(mmu, node->start, sgt, etnaviv_obj->base.size,
 				ETNAVIV_PROT_READ | ETNAVIV_PROT_WRITE);
@@ -381,7 +368,6 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,
 			mutex_unlock(&mmu->lock);
 			return ret;
 		}
-		mmu->last_iova = vram_node->start + size;
 		gpu->mmu->need_flush = true;
 		mutex_unlock(&mmu->lock);
 

commit ba5a42196b3e73a734f934e479e573a4dfd40dc5
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Jan 18 12:14:14 2018 +0100

    drm/etnaviv: use correct format specifier for size_t
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index d113fe06e6b5..49e049713a52 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -29,7 +29,7 @@ static void etnaviv_domain_unmap(struct etnaviv_iommu_domain *domain,
 	size_t pgsize = SZ_4K;
 
 	if (!IS_ALIGNED(iova | size, pgsize)) {
-		pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
+		pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%zx\n",
 		       iova, size, pgsize);
 		return;
 	}
@@ -54,7 +54,7 @@ static int etnaviv_domain_map(struct etnaviv_iommu_domain *domain,
 	int ret = 0;
 
 	if (!IS_ALIGNED(iova | paddr | size, pgsize)) {
-		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%zx\n",
 		       iova, &paddr, size, pgsize);
 		return -EINVAL;
 	}

commit ff9815957768b6e76d6895a0597d1ede05b4378b
Author: Markus Elfring <elfring@users.sourceforge.net>
Date:   Mon Oct 23 21:27:30 2017 +0200

    drm/etnaviv: Improve unlocking of a mutex in etnaviv_iommu_map_gem()
    
    Add a jump target so that a call of the function "mutex_unlock" is stored
    only once at the end of this function implementation.
    Replace three calls by goto statements.
    
    This issue was detected by using the Coccinelle software.
    
    Signed-off-by: Markus Elfring <elfring@users.sourceforge.net>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 35074b944778..d113fe06e6b5 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -263,18 +263,16 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 		if (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {
 			mapping->iova = iova;
 			list_add_tail(&mapping->mmu_node, &mmu->mappings);
-			mutex_unlock(&mmu->lock);
-			return 0;
+			ret = 0;
+			goto unlock;
 		}
 	}
 
 	node = &mapping->vram_node;
 
 	ret = etnaviv_iommu_find_iova(mmu, node, etnaviv_obj->base.size);
-	if (ret < 0) {
-		mutex_unlock(&mmu->lock);
-		return ret;
-	}
+	if (ret < 0)
+		goto unlock;
 
 	mmu->last_iova = node->start + etnaviv_obj->base.size;
 	mapping->iova = node->start;
@@ -283,12 +281,12 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 
 	if (ret < 0) {
 		drm_mm_remove_node(node);
-		mutex_unlock(&mmu->lock);
-		return ret;
+		goto unlock;
 	}
 
 	list_add_tail(&mapping->mmu_node, &mmu->mappings);
 	mmu->need_flush = true;
+unlock:
 	mutex_unlock(&mmu->lock);
 
 	return ret;

commit b670908384bda92c42076cf36614ee4f97763253
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Sep 7 17:06:28 2017 +0200

    drm/etnaviv: remove IOMMU dependency
    
    Using the IOMMU API to manage the internal GPU MMU has been an
    historical accident and it keeps getting in the way, as well as
    entangling the driver with the inner workings of the IOMMU
    subsystem.
    
    Clean this up by removing the usage of iommu_domain, which is the
    last piece linking etnaviv to the IOMMU subsystem.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 95e1671aee53..35074b944778 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -22,7 +22,7 @@
 #include "etnaviv_iommu.h"
 #include "etnaviv_mmu.h"
 
-static void etnaviv_domain_unmap(struct iommu_domain *domain,
+static void etnaviv_domain_unmap(struct etnaviv_iommu_domain *domain,
 				 unsigned long iova, size_t size)
 {
 	size_t unmapped_page, unmapped = 0;
@@ -44,8 +44,9 @@ static void etnaviv_domain_unmap(struct iommu_domain *domain,
 	}
 }
 
-static int etnaviv_domain_map(struct iommu_domain *domain, unsigned long iova,
-		     phys_addr_t paddr, size_t size, int prot)
+static int etnaviv_domain_map(struct etnaviv_iommu_domain *domain,
+			      unsigned long iova, phys_addr_t paddr,
+			      size_t size, int prot)
 {
 	unsigned long orig_iova = iova;
 	size_t pgsize = SZ_4K;
@@ -78,7 +79,7 @@ static int etnaviv_domain_map(struct iommu_domain *domain, unsigned long iova,
 static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 			     struct sg_table *sgt, unsigned len, int prot)
 {
-	struct iommu_domain *domain = iommu->domain;
+	struct etnaviv_iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;
 	unsigned int da = iova;
 	unsigned int i, j;
@@ -117,7 +118,7 @@ static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 static void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
 				struct sg_table *sgt, unsigned len)
 {
-	struct iommu_domain *domain = iommu->domain;
+	struct etnaviv_iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;
 	unsigned int da = iova;
 	int i;
@@ -278,7 +279,7 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 	mmu->last_iova = node->start + etnaviv_obj->base.size;
 	mapping->iova = node->start;
 	ret = etnaviv_iommu_map(mmu, node->start, sgt, etnaviv_obj->base.size,
-				IOMMU_READ | IOMMU_WRITE);
+				ETNAVIV_PROT_READ | ETNAVIV_PROT_WRITE);
 
 	if (ret < 0) {
 		drm_mm_remove_node(node);
@@ -312,7 +313,7 @@ void etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,
 void etnaviv_iommu_destroy(struct etnaviv_iommu *mmu)
 {
 	drm_mm_takedown(&mmu->mm);
-	iommu_domain_free(mmu->domain);
+	mmu->domain->ops->free(mmu->domain);
 	kfree(mmu);
 }
 
@@ -344,9 +345,7 @@ struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu)
 	mutex_init(&mmu->lock);
 	INIT_LIST_HEAD(&mmu->mappings);
 
-	drm_mm_init(&mmu->mm, mmu->domain->geometry.aperture_start,
-		    mmu->domain->geometry.aperture_end -
-		    mmu->domain->geometry.aperture_start + 1);
+	drm_mm_init(&mmu->mm, mmu->domain->base, mmu->domain->size);
 
 	return mmu;
 }
@@ -378,7 +377,7 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,
 			return ret;
 		}
 		ret = etnaviv_domain_map(mmu->domain, vram_node->start, paddr,
-					 size, IOMMU_READ);
+					 size, ETNAVIV_PROT_READ);
 		if (ret < 0) {
 			drm_mm_remove_node(vram_node);
 			mutex_unlock(&mmu->lock);
@@ -408,18 +407,10 @@ void etnaviv_iommu_put_suballoc_va(struct etnaviv_gpu *gpu,
 }
 size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
 {
-	struct etnaviv_iommu_ops *ops;
-
-	ops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);
-
-	return ops->dump_size(iommu->domain);
+	return iommu->domain->ops->dump_size(iommu->domain);
 }
 
 void etnaviv_iommu_dump(struct etnaviv_iommu *iommu, void *buf)
 {
-	struct etnaviv_iommu_ops *ops;
-
-	ops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);
-
-	ops->dump(iommu->domain, buf);
+	iommu->domain->ops->dump(iommu->domain, buf);
 }

commit 27d38062a20326a7b86d1f3ebba42c2a37d8415d
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Sep 7 17:06:28 2017 +0200

    drm/etnaviv: mmu: mark local functions static
    
    And clean up the header file a bit.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-By: Wladimir J. van der Laan <laanwj@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index e7b8b7aff9c6..95e1671aee53 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -75,8 +75,8 @@ static int etnaviv_domain_map(struct iommu_domain *domain, unsigned long iova,
 	return ret;
 }
 
-int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
-		struct sg_table *sgt, unsigned len, int prot)
+static int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
+			     struct sg_table *sgt, unsigned len, int prot)
 {
 	struct iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;
@@ -114,8 +114,8 @@ int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 	return ret;
 }
 
-void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
-			 struct sg_table *sgt, unsigned len)
+static void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
+				struct sg_table *sgt, unsigned len)
 {
 	struct iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;

commit 50073cf98d16354b2426f6c712c7b2cb0df2871b
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Sep 7 16:52:13 2017 +0200

    drm/etnaviv: mmu: stop using iommu map/unmap functions
    
    This is a preparation to remove the etnaviv dependency on the IOMMU
    subsystem by importing the relevant parts of the iommu map/unamp
    functions into the driver.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-By: Wladimir J. van der Laan <laanwj@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index f3ed07db9b2d..e7b8b7aff9c6 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -22,6 +22,59 @@
 #include "etnaviv_iommu.h"
 #include "etnaviv_mmu.h"
 
+static void etnaviv_domain_unmap(struct iommu_domain *domain,
+				 unsigned long iova, size_t size)
+{
+	size_t unmapped_page, unmapped = 0;
+	size_t pgsize = SZ_4K;
+
+	if (!IS_ALIGNED(iova | size, pgsize)) {
+		pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
+		       iova, size, pgsize);
+		return;
+	}
+
+	while (unmapped < size) {
+		unmapped_page = domain->ops->unmap(domain, iova, pgsize);
+		if (!unmapped_page)
+			break;
+
+		iova += unmapped_page;
+		unmapped += unmapped_page;
+	}
+}
+
+static int etnaviv_domain_map(struct iommu_domain *domain, unsigned long iova,
+		     phys_addr_t paddr, size_t size, int prot)
+{
+	unsigned long orig_iova = iova;
+	size_t pgsize = SZ_4K;
+	size_t orig_size = size;
+	int ret = 0;
+
+	if (!IS_ALIGNED(iova | paddr | size, pgsize)) {
+		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+		       iova, &paddr, size, pgsize);
+		return -EINVAL;
+	}
+
+	while (size) {
+		ret = domain->ops->map(domain, iova, paddr, pgsize, prot);
+		if (ret)
+			break;
+
+		iova += pgsize;
+		paddr += pgsize;
+		size -= pgsize;
+	}
+
+	/* unroll mapping in case something went wrong */
+	if (ret)
+		etnaviv_domain_unmap(domain, orig_iova, orig_size - size);
+
+	return ret;
+}
+
 int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 		struct sg_table *sgt, unsigned len, int prot)
 {
@@ -40,7 +93,7 @@ int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 
 		VERB("map[%d]: %08x %08x(%zx)", i, iova, pa, bytes);
 
-		ret = iommu_map(domain, da, pa, bytes, prot);
+		ret = etnaviv_domain_map(domain, da, pa, bytes, prot);
 		if (ret)
 			goto fail;
 
@@ -55,14 +108,14 @@ int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 	for_each_sg(sgt->sgl, sg, i, j) {
 		size_t bytes = sg_dma_len(sg) + sg->offset;
 
-		iommu_unmap(domain, da, bytes);
+		etnaviv_domain_unmap(domain, da, bytes);
 		da += bytes;
 	}
 	return ret;
 }
 
-int etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
-		struct sg_table *sgt, unsigned len)
+void etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
+			 struct sg_table *sgt, unsigned len)
 {
 	struct iommu_domain *domain = iommu->domain;
 	struct scatterlist *sg;
@@ -71,11 +124,8 @@ int etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
 
 	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
 		size_t bytes = sg_dma_len(sg) + sg->offset;
-		size_t unmapped;
 
-		unmapped = iommu_unmap(domain, da, bytes);
-		if (unmapped < bytes)
-			return unmapped;
+		etnaviv_domain_unmap(domain, da, bytes);
 
 		VERB("unmap[%d]: %08x(%zx)", i, iova, bytes);
 
@@ -83,8 +133,6 @@ int etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
 
 		da += bytes;
 	}
-
-	return 0;
 }
 
 static void etnaviv_iommu_remove_mapping(struct etnaviv_iommu *mmu,
@@ -329,8 +377,8 @@ int etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,
 			mutex_unlock(&mmu->lock);
 			return ret;
 		}
-		ret = iommu_map(mmu->domain, vram_node->start, paddr, size,
-				IOMMU_READ);
+		ret = etnaviv_domain_map(mmu->domain, vram_node->start, paddr,
+					 size, IOMMU_READ);
 		if (ret < 0) {
 			drm_mm_remove_node(vram_node);
 			mutex_unlock(&mmu->lock);
@@ -353,7 +401,7 @@ void etnaviv_iommu_put_suballoc_va(struct etnaviv_gpu *gpu,
 
 	if (mmu->version == ETNAVIV_IOMMU_V2) {
 		mutex_lock(&mmu->lock);
-		iommu_unmap(mmu->domain,iova, size);
+		etnaviv_domain_unmap(mmu->domain, iova, size);
 		drm_mm_remove_node(vram_node);
 		mutex_unlock(&mmu->lock);
 	}

commit 3bc3e0ecef69bf8061f89b6f2971ca9d88c669ea
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Sep 7 15:22:22 2017 +0200

    drm/etnaviv: remove iommu fault handler
    
    The handler has never been used, so it's really just dead code.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Philipp Zabel <p.zabel@pengutronix.de>
    Reviewed-By: Wladimir J. van der Laan <laanwj@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index f103e787de94..f3ed07db9b2d 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -22,13 +22,6 @@
 #include "etnaviv_iommu.h"
 #include "etnaviv_mmu.h"
 
-static int etnaviv_fault_handler(struct iommu_domain *iommu, struct device *dev,
-		unsigned long iova, int flags, void *arg)
-{
-	DBG("*** fault: iova=%08lx, flags=%d", iova, flags);
-	return 0;
-}
-
 int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
 		struct sg_table *sgt, unsigned len, int prot)
 {
@@ -307,8 +300,6 @@ struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu)
 		    mmu->domain->geometry.aperture_end -
 		    mmu->domain->geometry.aperture_start + 1);
 
-	iommu_set_fault_handler(mmu->domain, etnaviv_fault_handler, gpu->dev);
-
 	return mmu;
 }
 

commit 4e64e5539d152e202ad6eea2b6f65f3ab58d9428
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Feb 2 21:04:38 2017 +0000

    drm: Improve drm_mm search (and fix topdown allocation) with rbtrees
    
    The drm_mm range manager claimed to support top-down insertion, but it
    was neither searching for the top-most hole that could fit the
    allocation request nor fitting the request to the hole correctly.
    
    In order to search the range efficiently, we create a secondary index
    for the holes using either their size or their address. This index
    allows us to find the smallest hole or the hole at the bottom or top of
    the range efficiently, whilst keeping the hole stack to rapidly service
    evictions.
    
    v2: Search for holes both high and low. Rename flags to mode.
    v3: Discover rb_entry_safe() and use it!
    v4: Kerneldoc for enum drm_mm_insert_mode.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: "Christian K旦nig" <christian.koenig@amd.com>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Daniel Vetter <daniel.vetter@intel.com>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Sean Paul <seanpaul@chromium.org>
    Cc: Lucas Stach <l.stach@pengutronix.de>
    Cc: Christian Gmeiner <christian.gmeiner@gmail.com>
    Cc: Rob Clark <robdclark@gmail.com>
    Cc: Thierry Reding <thierry.reding@gmail.com>
    Cc: Stephen Warren <swarren@wwwdotorg.org>
    Cc: Alexandre Courbot <gnurou@gmail.com>
    Cc: Eric Anholt <eric@anholt.net>
    Cc: Sinclair Yeh <syeh@vmware.com>
    Cc: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Alex Deucher <alexander.deucher@amd.com>
    Reviewed-by: Sinclair Yeh <syeh@vmware.com> # vmwgfx
    Reviewed-by: Lucas Stach <l.stach@pengutronix.de> #etnaviv
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20170202210438.28702-1-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index ff826c16fb89..f103e787de94 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -108,6 +108,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 				   struct drm_mm_node *node, size_t size)
 {
 	struct etnaviv_vram_mapping *free = NULL;
+	enum drm_mm_insert_mode mode = DRM_MM_INSERT_LOW;
 	int ret;
 
 	lockdep_assert_held(&mmu->lock);
@@ -119,9 +120,9 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		bool found;
 
 		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
-			size, 0, mmu->last_iova, ~0UL,
-			DRM_MM_SEARCH_DEFAULT);
-
+						  size, 0, 0,
+						  mmu->last_iova, U64_MAX,
+						  mode);
 		if (ret != -ENOSPC)
 			break;
 
@@ -136,7 +137,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		}
 
 		/* Try to retire some entries */
-		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, 0);
+		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, mode);
 
 		found = 0;
 		INIT_LIST_HEAD(&list);
@@ -188,6 +189,8 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 			list_del_init(&m->scan_node);
 		}
 
+		mode = DRM_MM_INSERT_EVICT;
+
 		/*
 		 * We removed enough mappings so that the new allocation will
 		 * succeed, retry the allocation one more time.

commit 99743ae4c5f52f8f8ceb17783056fcc9b4f8b64c
Merge: 18566acac18f 82260364fd0c
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Feb 3 05:41:58 2017 +1000

    Merge branch 'drm-etnaviv-next' of https://git.pengutronix.de/git/lst/linux into drm-next
    
    It includes code cleanups from Bhumika and Liviu, a significant shader
    performance fix and additions to the cmdstream validator from Wladimir
    and the addition of a cmdbuf suballocator by myself.
    The suballocator improves performance on all chips by reducing the CPU
    overhead of the kernel driver and side steps the GC3000 FE MMU flush
    erratum, now making the workarounds in IOVA allocation we had before
    unnecessary, which results in a nice cleanup of the code in that area.
    
    * 'drm-etnaviv-next' of https://git.pengutronix.de/git/lst/linux:
      drm/etnaviv: Remove duplicate header file include
      Revert "drm/etnaviv: trick drm_mm into giving out a low IOVA"
      drm/etnaviv: add cmdbuf suballocator
      drm/etnaviv: get cmdbuf physical address through the cmdbuf abstraction
      drm/etnaviv: wire up iova handling in new cmdbuf abstraction
      drm/etnaviv: move cmdbuf de-/allocation into own file
      drm/etnaviv: always flush MMU TLBs on map/unmap
      drm/etnaviv: constify etnaviv_iommu_ops structures
      drm/etnaviv: set up initial PULSE_EATER register
      drm/etnaviv: add new GC3000 sensitive states

commit e17d0bf23f124f3e341415377a2ccbe0195f4158
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue Jan 17 11:04:45 2017 +0100

    Revert "drm/etnaviv: trick drm_mm into giving out a low IOVA"
    
    Now that commandstreams are handled through the cmdbuf suballocator
    the workaround to make the IOVA games work is not needed anymore.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian Gmeiner <christian.gmeiner@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index dcc86d8eeb98..6a8c28e64866 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -117,14 +117,9 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		struct list_head list;
 		bool found;
 
-		/*
-		 * XXX: The DRM_MM_SEARCH_BELOW is really a hack to trick
-		 * drm_mm into giving out a low IOVA after address space
-		 * rollover. This needs a proper fix.
-		 */
 		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
 			size, 0, mmu->last_iova, ~0UL,
-			mmu->last_iova ? DRM_MM_SEARCH_DEFAULT : DRM_MM_SEARCH_BELOW);
+			DRM_MM_SEARCH_DEFAULT);
 
 		if (ret != -ENOSPC)
 			break;

commit e66774dd6f6a3d44559599e4eeb785734c28d034
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Mon Jan 16 17:29:57 2017 +0100

    drm/etnaviv: add cmdbuf suballocator
    
    There are 3 big benefits to suballocating a single big DMA buffer
    for command submission:
    
    1. Avoid hammering CMA. The old way of allocating and freeing a DMA
       buffer for each submission was hitting some of the real slow
       pathes in CMA, as this allocator was not designed for a concurrent
       small buffers load.
    
    2. Less TLB flushes on IOMMUv2. If a new command buffer is mapped into
       the GPU address space the MMU TLBs need to be flushed. By having
       one big buffer statically mapped to the GPU, a lot of those flushes
       can be avoided.
    
    3. No funky workarounds for GC3000. The FE TLB flush on GC3000 isn't
       reliable. To work around that we tried to lay out the cmdbufs in
       the GPU address space in a way to avoid this issue. This hasn't
       always worked if the address space is crowded. A single statically
       mapped buffer avoids the erratum completely.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian Gmeiner <christian.gmeiner@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 056685bd33b2..dcc86d8eeb98 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -321,55 +321,50 @@ void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
 		etnaviv_iommuv2_restore(gpu);
 }
 
-u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,
-				struct etnaviv_cmdbuf *buf)
+int etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,
+				  struct drm_mm_node *vram_node, size_t size,
+				  u32 *iova)
 {
 	struct etnaviv_iommu *mmu = gpu->mmu;
 
 	if (mmu->version == ETNAVIV_IOMMU_V1) {
-		return buf->paddr - gpu->memory_base;
+		*iova = paddr - gpu->memory_base;
+		return 0;
 	} else {
 		int ret;
 
-		if (buf->vram_node.allocated)
-			return (u32)buf->vram_node.start;
-
 		mutex_lock(&mmu->lock);
-		ret = etnaviv_iommu_find_iova(mmu, &buf->vram_node,
-					      buf->size + SZ_64K);
+		ret = etnaviv_iommu_find_iova(mmu, vram_node, size);
 		if (ret < 0) {
 			mutex_unlock(&mmu->lock);
-			return 0;
+			return ret;
 		}
-		ret = iommu_map(mmu->domain, buf->vram_node.start, buf->paddr,
-				buf->size, IOMMU_READ);
+		ret = iommu_map(mmu->domain, vram_node->start, paddr, size,
+				IOMMU_READ);
 		if (ret < 0) {
-			drm_mm_remove_node(&buf->vram_node);
+			drm_mm_remove_node(vram_node);
 			mutex_unlock(&mmu->lock);
-			return 0;
+			return ret;
 		}
-		/*
-		 * At least on GC3000 the FE MMU doesn't properly flush old TLB
-		 * entries. Make sure to space the command buffers out in a way
-		 * that the FE MMU prefetch won't load invalid entries.
-		 */
-		mmu->last_iova = buf->vram_node.start + buf->size + SZ_64K;
+		mmu->last_iova = vram_node->start + size;
 		gpu->mmu->need_flush = true;
 		mutex_unlock(&mmu->lock);
 
-		return (u32)buf->vram_node.start;
+		*iova = (u32)vram_node->start;
+		return 0;
 	}
 }
 
-void etnaviv_iommu_put_cmdbuf_va(struct etnaviv_gpu *gpu,
-				 struct etnaviv_cmdbuf *buf)
+void etnaviv_iommu_put_suballoc_va(struct etnaviv_gpu *gpu,
+				   struct drm_mm_node *vram_node, size_t size,
+				   u32 iova)
 {
 	struct etnaviv_iommu *mmu = gpu->mmu;
 
-	if (mmu->version == ETNAVIV_IOMMU_V2 && buf->vram_node.allocated) {
+	if (mmu->version == ETNAVIV_IOMMU_V2) {
 		mutex_lock(&mmu->lock);
-		iommu_unmap(mmu->domain, buf->vram_node.start, buf->size);
-		drm_mm_remove_node(&buf->vram_node);
+		iommu_unmap(mmu->domain,iova, size);
+		drm_mm_remove_node(vram_node);
 		mutex_unlock(&mmu->lock);
 	}
 }

commit ea1f5729aa1bbe68f9a394e259288d6ff894b0aa
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Mon Jan 16 16:09:51 2017 +0100

    drm/etnaviv: move cmdbuf de-/allocation into own file
    
    This will get more complex with the following changes, so move it
    into its own place.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian Gmeiner <christian.gmeiner@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index d1216516b947..056685bd33b2 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -15,6 +15,7 @@
  */
 
 #include "common.xml.h"
+#include "etnaviv_cmdbuf.h"
 #include "etnaviv_drv.h"
 #include "etnaviv_gem.h"
 #include "etnaviv_gpu.h"

commit d46450737c007b16bb81f9d5dea56d1f4b9a2c21
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue Jan 17 10:59:37 2017 +0100

    drm/etnaviv: always flush MMU TLBs on map/unmap
    
    This ensures that the GPU isn't able to write into already freed
    objects, as doing this in the IOVA reaper isn't enough, as the
    gem_free_object path will also cause unmaps to happen.
    
    On MMUv2 this also ensures that stale entries, which may have
    been prefetched into the TLB will be purged.
    
    The flush is low overhead, as it gets batched up with the next
    user command buffer, so this isn't incuring an overhead for
    each buffer map/unmap.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Christian Gmeiner <christian.gmeiner@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index fe0e85b41310..d1216516b947 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -193,11 +193,8 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 
 		/*
 		 * We removed enough mappings so that the new allocation will
-		 * succeed.  Ensure that the MMU will be flushed before the
-		 * associated commit requesting this mapping, and retry the
-		 * allocation one more time.
+		 * succeed, retry the allocation one more time.
 		 */
-		mmu->need_flush = true;
 	}
 
 	return ret;
@@ -249,6 +246,7 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 	}
 
 	list_add_tail(&mapping->mmu_node, &mmu->mappings);
+	mmu->need_flush = true;
 	mutex_unlock(&mmu->lock);
 
 	return ret;
@@ -266,6 +264,7 @@ void etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,
 		etnaviv_iommu_remove_mapping(mmu, mapping);
 
 	list_del(&mapping->mmu_node);
+	mmu->need_flush = true;
 	mutex_unlock(&mmu->lock);
 }
 

commit b0df0b251b25b0bf89ef3e518330fcac300add86
Merge: f0493e653f96 ff9f8a7cf935
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Jan 27 11:00:42 2017 +1000

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux into drm-next
    
    Backmerge Linus master to get the connector locking revert.
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux: (645 commits)
      sysctl: fix proc_doulongvec_ms_jiffies_minmax()
      Revert "drm/probe-helpers: Drop locking from poll_enable"
      MAINTAINERS: add Dan Streetman to zbud maintainers
      MAINTAINERS: add Dan Streetman to zswap maintainers
      mm: do not export ioremap_page_range symbol for external module
      mn10300: fix build error of missing fpu_save()
      romfs: use different way to generate fsid for BLOCK or MTD
      frv: add missing atomic64 operations
      mm, page_alloc: fix premature OOM when racing with cpuset mems update
      mm, page_alloc: move cpuset seqcount checking to slowpath
      mm, page_alloc: fix fast-path race with cpuset update or removal
      mm, page_alloc: fix check for NULL preferred_zone
      kernel/panic.c: add missing \n
      fbdev: color map copying bounds checking
      frv: add atomic64_add_unless()
      mm/mempolicy.c: do not put mempolicy before using its nodemask
      radix-tree: fix private list warnings
      Documentation/filesystems/proc.txt: add VmPin
      mm, memcg: do not retry precharge charges
      proc: add a schedule point in proc_pid_readdir()
      ...

commit 3546fb0cdac25a79c89d87020566fab52b92867d
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Mon Dec 12 16:15:17 2016 +0100

    drm/etnaviv: trick drm_mm into giving out a low IOVA
    
    After rollover of the IOVA space, we want to get a low IOVA address,
    otherwise the the games we play by remembering the last IOVA are
    pointless. When we search for a free hole with DRM_MM_SEARCH_DEFAULT,
    drm_mm will pop the next entry from the free holes stack, which will
    likely be a high IOVA. By using DRM_MM_SEARCH_BELOW we can trick
    drm_mm into reversing the search and provide us with a low IOVA.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Reviewed-by: Wladimir van der Laan <laanwj@gmail.com>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 169ac96e8f08..fe0e85b41310 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -116,9 +116,14 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		struct list_head list;
 		bool found;
 
+		/*
+		 * XXX: The DRM_MM_SEARCH_BELOW is really a hack to trick
+		 * drm_mm into giving out a low IOVA after address space
+		 * rollover. This needs a proper fix.
+		 */
 		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
 			size, 0, mmu->last_iova, ~0UL,
-			DRM_MM_SEARCH_DEFAULT);
+			mmu->last_iova ? DRM_MM_SEARCH_DEFAULT : DRM_MM_SEARCH_BELOW);
 
 		if (ret != -ENOSPC)
 			break;

commit 0b04d474a611e2831d142e246422a03a10998ae1
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:33 2016 +0000

    drm: Compute tight evictions for drm_mm_scan
    
    Compute the minimal required hole during scan and only evict those nodes
    that overlap. This enables us to reduce the number of nodes we need to
    evict to the bare minimum.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: http://patchwork.freedesktop.org/patch/msgid/20161222083641.2691-31-chris@chris-wilson.co.uk

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 379ea3a96f0a..ae2733a609ba 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -135,7 +135,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		}
 
 		/* Try to retire some entries */
-		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0);
+		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, 0);
 
 		found = 0;
 		INIT_LIST_HEAD(&list);

commit 9a71e277888b39b8f0e8364813ec1ba58a5a4371
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Thu Dec 22 08:36:29 2016 +0000

    drm: Extract struct drm_mm_scan from struct drm_mm
    
    The scan state occupies a large proportion of the struct drm_mm and is
    rarely used and only contains temporary state. That makes it suitable to
    moving to its struct and onto the stack of the callers.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    [danvet: Fix up etnaviv to compile, was missing a BUG_ON.]
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 169ac96e8f08..379ea3a96f0a 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -113,6 +113,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 
 	while (1) {
 		struct etnaviv_vram_mapping *m, *n;
+		struct drm_mm_scan scan;
 		struct list_head list;
 		bool found;
 
@@ -134,7 +135,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		}
 
 		/* Try to retire some entries */
-		drm_mm_init_scan(&mmu->mm, size, 0, 0);
+		drm_mm_scan_init(&scan, &mmu->mm, size, 0, 0);
 
 		found = 0;
 		INIT_LIST_HEAD(&list);
@@ -151,7 +152,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 				continue;
 
 			list_add(&free->scan_node, &list);
-			if (drm_mm_scan_add_block(&free->vram_node)) {
+			if (drm_mm_scan_add_block(&scan, &free->vram_node)) {
 				found = true;
 				break;
 			}
@@ -160,7 +161,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		if (!found) {
 			/* Nothing found, clean up and fail */
 			list_for_each_entry_safe(m, n, &list, scan_node)
-				BUG_ON(drm_mm_scan_remove_block(&m->vram_node));
+				BUG_ON(drm_mm_scan_remove_block(&scan, &m->vram_node));
 			break;
 		}
 
@@ -171,7 +172,7 @@ static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
 		 * can leave the block pinned.
 		 */
 		list_for_each_entry_safe(m, n, &list, scan_node)
-			if (!drm_mm_scan_remove_block(&m->vram_node))
+			if (!drm_mm_scan_remove_block(&scan, &m->vram_node))
 				list_del_init(&m->scan_node);
 
 		/*

commit 8814d2dce00f77c5eeb7278981ac6fd08835629e
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Oct 6 17:03:28 2016 +0200

    drm/etnaviv: block 64K of address space behind each cmdstream
    
    To make sure we don't place anything there which might confuse
    the FE prefetcher. This gets rid of another case of FE MMU faults
    when the address space gets crowded before triggering the reaper.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index d3796ed8d8c5..169ac96e8f08 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -330,7 +330,8 @@ u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,
 			return (u32)buf->vram_node.start;
 
 		mutex_lock(&mmu->lock);
-		ret = etnaviv_iommu_find_iova(mmu, &buf->vram_node, buf->size);
+		ret = etnaviv_iommu_find_iova(mmu, &buf->vram_node,
+					      buf->size + SZ_64K);
 		if (ret < 0) {
 			mutex_unlock(&mmu->lock);
 			return 0;

commit 7c971c62dd543a323a0cabc0c559caea1759f59f
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Thu Sep 15 13:06:43 2016 +0200

    drm/etnaviv: space out IOVA layout for cmdbufs on MMUv2
    
    At least on the GC3000 the FE MMU is not properly flushing stale TLB
    entries. Make sure to map the cmdbufs with a big enough spacing in
    the IOVAs to not hit old/prefetched TLB entries when jumping to a
    newly mapped cmdbuf.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 304c0b4a9d58..d3796ed8d8c5 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -342,7 +342,12 @@ u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,
 			mutex_unlock(&mmu->lock);
 			return 0;
 		}
-		mmu->last_iova = buf->vram_node.start + buf->size;
+		/*
+		 * At least on GC3000 the FE MMU doesn't properly flush old TLB
+		 * entries. Make sure to space the command buffers out in a way
+		 * that the FE MMU prefetch won't load invalid entries.
+		 */
+		mmu->last_iova = buf->vram_node.start + buf->size + SZ_64K;
 		gpu->mmu->need_flush = true;
 		mutex_unlock(&mmu->lock);
 

commit afb7b3b1deb471698f54e04a1815bc803ec9a161
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Sat Aug 20 00:16:58 2016 +0200

    drm/etnaviv: implement IOMMUv2 translation
    
    All other parts are now in place, so implement the actual translation
    step and hook it up, so the driver claims support for cores with
    the new MMU.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 98c84ef862c7..304c0b4a9d58 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -313,7 +313,7 @@ void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
 	if (gpu->mmu->version == ETNAVIV_IOMMU_V1)
 		etnaviv_iommuv1_restore(gpu);
 	else
-		dev_err(gpu->dev, "IOMMUv2 restore not implemented\n");
+		etnaviv_iommuv2_restore(gpu);
 }
 
 u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,

commit e68f270f210776eed956884d1fc3ce1aab0912a3
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Aug 19 23:49:10 2016 +0200

    drm/etnaviv: map cmdbuf through MMU on version 2
    
    With MMUv2 all buffers need to be mapped through the MMU once it
    is enabled. Align the buffer size to 4K, as the MMU is only able to
    map page aligned buffers.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 69ea0a0a70c2..98c84ef862c7 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -319,9 +319,49 @@ void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
 u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,
 				struct etnaviv_cmdbuf *buf)
 {
-	return buf->paddr - gpu->memory_base;
+	struct etnaviv_iommu *mmu = gpu->mmu;
+
+	if (mmu->version == ETNAVIV_IOMMU_V1) {
+		return buf->paddr - gpu->memory_base;
+	} else {
+		int ret;
+
+		if (buf->vram_node.allocated)
+			return (u32)buf->vram_node.start;
+
+		mutex_lock(&mmu->lock);
+		ret = etnaviv_iommu_find_iova(mmu, &buf->vram_node, buf->size);
+		if (ret < 0) {
+			mutex_unlock(&mmu->lock);
+			return 0;
+		}
+		ret = iommu_map(mmu->domain, buf->vram_node.start, buf->paddr,
+				buf->size, IOMMU_READ);
+		if (ret < 0) {
+			drm_mm_remove_node(&buf->vram_node);
+			mutex_unlock(&mmu->lock);
+			return 0;
+		}
+		mmu->last_iova = buf->vram_node.start + buf->size;
+		gpu->mmu->need_flush = true;
+		mutex_unlock(&mmu->lock);
+
+		return (u32)buf->vram_node.start;
+	}
 }
 
+void etnaviv_iommu_put_cmdbuf_va(struct etnaviv_gpu *gpu,
+				 struct etnaviv_cmdbuf *buf)
+{
+	struct etnaviv_iommu *mmu = gpu->mmu;
+
+	if (mmu->version == ETNAVIV_IOMMU_V2 && buf->vram_node.allocated) {
+		mutex_lock(&mmu->lock);
+		iommu_unmap(mmu->domain, buf->vram_node.start, buf->size);
+		drm_mm_remove_node(&buf->vram_node);
+		mutex_unlock(&mmu->lock);
+	}
+}
 size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
 {
 	struct etnaviv_iommu_ops *ops;

commit 90969c9aa97700663d03c51031652e131df3bd9b
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Fri Aug 19 23:43:40 2016 +0200

    drm/etnaviv: split out iova search and MMU reaping logic
    
    With MMUv2 the command buffers need to be mapped through the MMU.
    Split out the iova search and MMU reaping logic so it can be reused
    for the cmdbuf mapping, where no GEM object is involved.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index d62125eb30d8..69ea0a0a70c2 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -103,41 +103,21 @@ static void etnaviv_iommu_remove_mapping(struct etnaviv_iommu *mmu,
 	drm_mm_remove_node(&mapping->vram_node);
 }
 
-int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
-	struct etnaviv_gem_object *etnaviv_obj, u32 memory_base,
-	struct etnaviv_vram_mapping *mapping)
+static int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,
+				   struct drm_mm_node *node, size_t size)
 {
 	struct etnaviv_vram_mapping *free = NULL;
-	struct sg_table *sgt = etnaviv_obj->sgt;
-	struct drm_mm_node *node;
 	int ret;
 
-	lockdep_assert_held(&etnaviv_obj->lock);
-
-	mutex_lock(&mmu->lock);
+	lockdep_assert_held(&mmu->lock);
 
-	/* v1 MMU can optimize single entry (contiguous) scatterlists */
-	if (mmu->version == ETNAVIV_IOMMU_V1 &&
-	    sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
-		u32 iova;
-
-		iova = sg_dma_address(sgt->sgl) - memory_base;
-		if (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {
-			mapping->iova = iova;
-			list_add_tail(&mapping->mmu_node, &mmu->mappings);
-			mutex_unlock(&mmu->lock);
-			return 0;
-		}
-	}
-
-	node = &mapping->vram_node;
 	while (1) {
 		struct etnaviv_vram_mapping *m, *n;
 		struct list_head list;
 		bool found;
 
 		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
-			etnaviv_obj->base.size, 0, mmu->last_iova, ~0UL,
+			size, 0, mmu->last_iova, ~0UL,
 			DRM_MM_SEARCH_DEFAULT);
 
 		if (ret != -ENOSPC)
@@ -154,7 +134,7 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 		}
 
 		/* Try to retire some entries */
-		drm_mm_init_scan(&mmu->mm, etnaviv_obj->base.size, 0, 0);
+		drm_mm_init_scan(&mmu->mm, size, 0, 0);
 
 		found = 0;
 		INIT_LIST_HEAD(&list);
@@ -215,6 +195,38 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 		mmu->need_flush = true;
 	}
 
+	return ret;
+}
+
+int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
+	struct etnaviv_gem_object *etnaviv_obj, u32 memory_base,
+	struct etnaviv_vram_mapping *mapping)
+{
+	struct sg_table *sgt = etnaviv_obj->sgt;
+	struct drm_mm_node *node;
+	int ret;
+
+	lockdep_assert_held(&etnaviv_obj->lock);
+
+	mutex_lock(&mmu->lock);
+
+	/* v1 MMU can optimize single entry (contiguous) scatterlists */
+	if (mmu->version == ETNAVIV_IOMMU_V1 &&
+	    sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
+		u32 iova;
+
+		iova = sg_dma_address(sgt->sgl) - memory_base;
+		if (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {
+			mapping->iova = iova;
+			list_add_tail(&mapping->mmu_node, &mmu->mappings);
+			mutex_unlock(&mmu->lock);
+			return 0;
+		}
+	}
+
+	node = &mapping->vram_node;
+
+	ret = etnaviv_iommu_find_iova(mmu, node, etnaviv_obj->base.size);
 	if (ret < 0) {
 		mutex_unlock(&mmu->lock);
 		return ret;

commit e07c0db5e84a5f1a16af8567d5fdde2ca6d2c80e
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Wed Aug 17 14:57:51 2016 +0200

    drm/etnaviv: move gpu_va() to etnaviv mmu
    
    The GPU virtual address for the command buffers differs depending on
    the IOMMU version. Move the calculation of the iova into etnaviv
    mmu, to enable proper dispatch.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index e744c6d81a2d..d62125eb30d8 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -304,6 +304,12 @@ void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
 		dev_err(gpu->dev, "IOMMUv2 restore not implemented\n");
 }
 
+u32 etnaviv_iommu_get_cmdbuf_va(struct etnaviv_gpu *gpu,
+				struct etnaviv_cmdbuf *buf)
+{
+	return buf->paddr - gpu->memory_base;
+}
+
 size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
 {
 	struct etnaviv_iommu_ops *ops;

commit dd34bb9655176873dc6fdfc612c71f7c2f078caa
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue Aug 16 12:09:08 2016 +0200

    drm/etnaviv: move IOMMU domain allocation into etnaviv MMU
    
    The GPU code doesn't need to deal with the IOMMU directly, instead
    it can all be hidden behind the etnaviv mmu interface. Move the
    last remaining part into etnaviv mmu.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 55d4229f6932..e744c6d81a2d 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -14,6 +14,7 @@
  * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include "common.xml.h"
 #include "etnaviv_drv.h"
 #include "etnaviv_gem.h"
 #include "etnaviv_gpu.h"
@@ -258,26 +259,39 @@ void etnaviv_iommu_destroy(struct etnaviv_iommu *mmu)
 	kfree(mmu);
 }
 
-struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu,
-	struct iommu_domain *domain, enum etnaviv_iommu_version version)
+struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu)
 {
+	enum etnaviv_iommu_version version;
 	struct etnaviv_iommu *mmu;
 
 	mmu = kzalloc(sizeof(*mmu), GFP_KERNEL);
 	if (!mmu)
 		return ERR_PTR(-ENOMEM);
 
-	mmu->domain = domain;
+	if (!(gpu->identity.minor_features1 & chipMinorFeatures1_MMU_VERSION)) {
+		mmu->domain = etnaviv_iommuv1_domain_alloc(gpu);
+		version = ETNAVIV_IOMMU_V1;
+	} else {
+		mmu->domain = etnaviv_iommuv2_domain_alloc(gpu);
+		version = ETNAVIV_IOMMU_V2;
+	}
+
+	if (!mmu->domain) {
+		dev_err(gpu->dev, "Failed to allocate GPU IOMMU domain\n");
+		kfree(mmu);
+		return ERR_PTR(-ENOMEM);
+	}
+
 	mmu->gpu = gpu;
 	mmu->version = version;
 	mutex_init(&mmu->lock);
 	INIT_LIST_HEAD(&mmu->mappings);
 
-	drm_mm_init(&mmu->mm, domain->geometry.aperture_start,
-		    domain->geometry.aperture_end -
-		      domain->geometry.aperture_start + 1);
+	drm_mm_init(&mmu->mm, mmu->domain->geometry.aperture_start,
+		    mmu->domain->geometry.aperture_end -
+		    mmu->domain->geometry.aperture_start + 1);
 
-	iommu_set_fault_handler(domain, etnaviv_fault_handler, gpu->dev);
+	iommu_set_fault_handler(mmu->domain, etnaviv_fault_handler, gpu->dev);
 
 	return mmu;
 }

commit e095c8feb8feed9e2c8ef76f8ec8491f46985e24
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue Aug 16 11:54:51 2016 +0200

    drm/etnaviv: indirect IOMMU restore through etnaviv MMU
    
    So we can call the v2 restore code once it is there.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index e2013b5b3f6a..55d4229f6932 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -17,6 +17,7 @@
 #include "etnaviv_drv.h"
 #include "etnaviv_gem.h"
 #include "etnaviv_gpu.h"
+#include "etnaviv_iommu.h"
 #include "etnaviv_mmu.h"
 
 static int etnaviv_fault_handler(struct iommu_domain *iommu, struct device *dev,
@@ -281,6 +282,14 @@ struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu,
 	return mmu;
 }
 
+void etnaviv_iommu_restore(struct etnaviv_gpu *gpu)
+{
+	if (gpu->mmu->version == ETNAVIV_IOMMU_V1)
+		etnaviv_iommuv1_restore(gpu);
+	else
+		dev_err(gpu->dev, "IOMMUv2 restore not implemented\n");
+}
+
 size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
 {
 	struct etnaviv_iommu_ops *ops;

commit 1486b1cb80cb8676d573a27ced5423658fdd32de
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Wed Aug 17 16:15:25 2016 +0200

    drm/etnaviv: only try to use the linear window on MMUv1
    
    As the comment above the code states, the linear window is only
    available on MMUv1. Don't try to use it on MMUv2.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 29a723fabc17..e2013b5b3f6a 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -115,7 +115,8 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 	mutex_lock(&mmu->lock);
 
 	/* v1 MMU can optimize single entry (contiguous) scatterlists */
-	if (sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
+	if (mmu->version == ETNAVIV_IOMMU_V1 &&
+	    sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
 		u32 iova;
 
 		iova = sg_dma_address(sgt->sgl) - memory_base;

commit b6325f409959c7e1065ef1537f2e54cf4d7ab465
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 21 15:20:50 2016 +0000

    drm: etnaviv: clean up vram_mapping submission/retire path
    
    Currently, we scan the list of mappings each time we want to operate on
    the vram_mapping struct.  Rather than repeatedly scanning these, look
    them up once in the submission path, and then use _reference and
    _unreference methods as necessary to manage this object.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
index 6743bc648dc8..29a723fabc17 100644
--- a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -193,7 +193,7 @@ int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
 
 		/*
 		 * Unmap the blocks which need to be reaped from the MMU.
-		 * Clear the mmu pointer to prevent the get_iova finding
+		 * Clear the mmu pointer to prevent the mapping_get finding
 		 * this mapping.
 		 */
 		list_for_each_entry_safe(m, n, &list, scan_node) {

commit a8c21a5451d831e67b7a6fb910f9ca8bc7b43554
Author: The etnaviv authors <dri-devel@lists.freedesktop.org>
Date:   Thu Dec 3 18:21:29 2015 +0100

    drm/etnaviv: add initial etnaviv DRM driver
    
    This adds the etnaviv DRM driver and hooks it up in Makefiles
    and Kconfig.
    
    Signed-off-by: Christian Gmeiner <christian.gmeiner@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/drivers/gpu/drm/etnaviv/etnaviv_mmu.c b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
new file mode 100644
index 000000000000..6743bc648dc8
--- /dev/null
+++ b/drivers/gpu/drm/etnaviv/etnaviv_mmu.c
@@ -0,0 +1,299 @@
+/*
+ * Copyright (C) 2015 Etnaviv Project
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published by
+ * the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "etnaviv_drv.h"
+#include "etnaviv_gem.h"
+#include "etnaviv_gpu.h"
+#include "etnaviv_mmu.h"
+
+static int etnaviv_fault_handler(struct iommu_domain *iommu, struct device *dev,
+		unsigned long iova, int flags, void *arg)
+{
+	DBG("*** fault: iova=%08lx, flags=%d", iova, flags);
+	return 0;
+}
+
+int etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,
+		struct sg_table *sgt, unsigned len, int prot)
+{
+	struct iommu_domain *domain = iommu->domain;
+	struct scatterlist *sg;
+	unsigned int da = iova;
+	unsigned int i, j;
+	int ret;
+
+	if (!domain || !sgt)
+		return -EINVAL;
+
+	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
+		u32 pa = sg_dma_address(sg) - sg->offset;
+		size_t bytes = sg_dma_len(sg) + sg->offset;
+
+		VERB("map[%d]: %08x %08x(%zx)", i, iova, pa, bytes);
+
+		ret = iommu_map(domain, da, pa, bytes, prot);
+		if (ret)
+			goto fail;
+
+		da += bytes;
+	}
+
+	return 0;
+
+fail:
+	da = iova;
+
+	for_each_sg(sgt->sgl, sg, i, j) {
+		size_t bytes = sg_dma_len(sg) + sg->offset;
+
+		iommu_unmap(domain, da, bytes);
+		da += bytes;
+	}
+	return ret;
+}
+
+int etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,
+		struct sg_table *sgt, unsigned len)
+{
+	struct iommu_domain *domain = iommu->domain;
+	struct scatterlist *sg;
+	unsigned int da = iova;
+	int i;
+
+	for_each_sg(sgt->sgl, sg, sgt->nents, i) {
+		size_t bytes = sg_dma_len(sg) + sg->offset;
+		size_t unmapped;
+
+		unmapped = iommu_unmap(domain, da, bytes);
+		if (unmapped < bytes)
+			return unmapped;
+
+		VERB("unmap[%d]: %08x(%zx)", i, iova, bytes);
+
+		BUG_ON(!PAGE_ALIGNED(bytes));
+
+		da += bytes;
+	}
+
+	return 0;
+}
+
+static void etnaviv_iommu_remove_mapping(struct etnaviv_iommu *mmu,
+	struct etnaviv_vram_mapping *mapping)
+{
+	struct etnaviv_gem_object *etnaviv_obj = mapping->object;
+
+	etnaviv_iommu_unmap(mmu, mapping->vram_node.start,
+			    etnaviv_obj->sgt, etnaviv_obj->base.size);
+	drm_mm_remove_node(&mapping->vram_node);
+}
+
+int etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,
+	struct etnaviv_gem_object *etnaviv_obj, u32 memory_base,
+	struct etnaviv_vram_mapping *mapping)
+{
+	struct etnaviv_vram_mapping *free = NULL;
+	struct sg_table *sgt = etnaviv_obj->sgt;
+	struct drm_mm_node *node;
+	int ret;
+
+	lockdep_assert_held(&etnaviv_obj->lock);
+
+	mutex_lock(&mmu->lock);
+
+	/* v1 MMU can optimize single entry (contiguous) scatterlists */
+	if (sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {
+		u32 iova;
+
+		iova = sg_dma_address(sgt->sgl) - memory_base;
+		if (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {
+			mapping->iova = iova;
+			list_add_tail(&mapping->mmu_node, &mmu->mappings);
+			mutex_unlock(&mmu->lock);
+			return 0;
+		}
+	}
+
+	node = &mapping->vram_node;
+	while (1) {
+		struct etnaviv_vram_mapping *m, *n;
+		struct list_head list;
+		bool found;
+
+		ret = drm_mm_insert_node_in_range(&mmu->mm, node,
+			etnaviv_obj->base.size, 0, mmu->last_iova, ~0UL,
+			DRM_MM_SEARCH_DEFAULT);
+
+		if (ret != -ENOSPC)
+			break;
+
+		/*
+		 * If we did not search from the start of the MMU region,
+		 * try again in case there are free slots.
+		 */
+		if (mmu->last_iova) {
+			mmu->last_iova = 0;
+			mmu->need_flush = true;
+			continue;
+		}
+
+		/* Try to retire some entries */
+		drm_mm_init_scan(&mmu->mm, etnaviv_obj->base.size, 0, 0);
+
+		found = 0;
+		INIT_LIST_HEAD(&list);
+		list_for_each_entry(free, &mmu->mappings, mmu_node) {
+			/* If this vram node has not been used, skip this. */
+			if (!free->vram_node.mm)
+				continue;
+
+			/*
+			 * If the iova is pinned, then it's in-use,
+			 * so we must keep its mapping.
+			 */
+			if (free->use)
+				continue;
+
+			list_add(&free->scan_node, &list);
+			if (drm_mm_scan_add_block(&free->vram_node)) {
+				found = true;
+				break;
+			}
+		}
+
+		if (!found) {
+			/* Nothing found, clean up and fail */
+			list_for_each_entry_safe(m, n, &list, scan_node)
+				BUG_ON(drm_mm_scan_remove_block(&m->vram_node));
+			break;
+		}
+
+		/*
+		 * drm_mm does not allow any other operations while
+		 * scanning, so we have to remove all blocks first.
+		 * If drm_mm_scan_remove_block() returns false, we
+		 * can leave the block pinned.
+		 */
+		list_for_each_entry_safe(m, n, &list, scan_node)
+			if (!drm_mm_scan_remove_block(&m->vram_node))
+				list_del_init(&m->scan_node);
+
+		/*
+		 * Unmap the blocks which need to be reaped from the MMU.
+		 * Clear the mmu pointer to prevent the get_iova finding
+		 * this mapping.
+		 */
+		list_for_each_entry_safe(m, n, &list, scan_node) {
+			etnaviv_iommu_remove_mapping(mmu, m);
+			m->mmu = NULL;
+			list_del_init(&m->mmu_node);
+			list_del_init(&m->scan_node);
+		}
+
+		/*
+		 * We removed enough mappings so that the new allocation will
+		 * succeed.  Ensure that the MMU will be flushed before the
+		 * associated commit requesting this mapping, and retry the
+		 * allocation one more time.
+		 */
+		mmu->need_flush = true;
+	}
+
+	if (ret < 0) {
+		mutex_unlock(&mmu->lock);
+		return ret;
+	}
+
+	mmu->last_iova = node->start + etnaviv_obj->base.size;
+	mapping->iova = node->start;
+	ret = etnaviv_iommu_map(mmu, node->start, sgt, etnaviv_obj->base.size,
+				IOMMU_READ | IOMMU_WRITE);
+
+	if (ret < 0) {
+		drm_mm_remove_node(node);
+		mutex_unlock(&mmu->lock);
+		return ret;
+	}
+
+	list_add_tail(&mapping->mmu_node, &mmu->mappings);
+	mutex_unlock(&mmu->lock);
+
+	return ret;
+}
+
+void etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,
+	struct etnaviv_vram_mapping *mapping)
+{
+	WARN_ON(mapping->use);
+
+	mutex_lock(&mmu->lock);
+
+	/* If the vram node is on the mm, unmap and remove the node */
+	if (mapping->vram_node.mm == &mmu->mm)
+		etnaviv_iommu_remove_mapping(mmu, mapping);
+
+	list_del(&mapping->mmu_node);
+	mutex_unlock(&mmu->lock);
+}
+
+void etnaviv_iommu_destroy(struct etnaviv_iommu *mmu)
+{
+	drm_mm_takedown(&mmu->mm);
+	iommu_domain_free(mmu->domain);
+	kfree(mmu);
+}
+
+struct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu,
+	struct iommu_domain *domain, enum etnaviv_iommu_version version)
+{
+	struct etnaviv_iommu *mmu;
+
+	mmu = kzalloc(sizeof(*mmu), GFP_KERNEL);
+	if (!mmu)
+		return ERR_PTR(-ENOMEM);
+
+	mmu->domain = domain;
+	mmu->gpu = gpu;
+	mmu->version = version;
+	mutex_init(&mmu->lock);
+	INIT_LIST_HEAD(&mmu->mappings);
+
+	drm_mm_init(&mmu->mm, domain->geometry.aperture_start,
+		    domain->geometry.aperture_end -
+		      domain->geometry.aperture_start + 1);
+
+	iommu_set_fault_handler(domain, etnaviv_fault_handler, gpu->dev);
+
+	return mmu;
+}
+
+size_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)
+{
+	struct etnaviv_iommu_ops *ops;
+
+	ops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);
+
+	return ops->dump_size(iommu->domain);
+}
+
+void etnaviv_iommu_dump(struct etnaviv_iommu *iommu, void *buf)
+{
+	struct etnaviv_iommu_ops *ops;
+
+	ops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);
+
+	ops->dump(iommu->domain, buf);
+}
