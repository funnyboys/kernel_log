commit 87730ccbddcb48478b1b88e88b14e73424130764
Author: Leonid Ravich <Leonid.Ravich@emc.com>
Date:   Wed Jul 1 21:48:12 2020 +0300

    dmaengine: ioat setting ioat timeout as module parameter
    
    DMA transaction time to completion is a function of PCI bandwidth,
    transaction size and a queue depth.  So hard coded value for timeouts
    might be wrong for some scenarios.
    
    Signed-off-by: Leonid Ravich <Leonid.Ravich@emc.com>
    Reviewed-by: Dave Jiang <dave.jiang@intel.com>
    Link: https://lore.kernel.org/r/20200701184816.29138-1-leonid.ravich@dell.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index e6b622e1ba92..f7f31fdf14cf 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -104,8 +104,6 @@ struct ioatdma_chan {
 	#define IOAT_RUN 5
 	#define IOAT_CHAN_ACTIVE 6
 	struct timer_list timer;
-	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
-	#define IDLE_TIMEOUT msecs_to_jiffies(2000)
 	#define RESET_DELAY msecs_to_jiffies(100)
 	struct ioatdma_device *ioat_dma;
 	dma_addr_t completion_dma;

commit a02254f8a676f5fdb98ebeb6cc420a71e652574a
Author: Leonid Ravich <Leonid.Ravich@emc.com>
Date:   Thu Apr 16 20:06:22 2020 +0300

    dmaengine: ioat: Decreasing allocation chunk size 2M->512K
    
    requreing kmalloc of 2M high chance to fail in
    fragmented memory.
    IOAT ring requires 64k * 64B memory
    which will be achived by 512k * 8 allocation
    instead of 2M * 2.
    
    Acked-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Leonid Ravich <Leonid.Ravich@emc.com>
    Link: https://lore.kernel.org/r/20200416170628.16196-2-leonid.ravich@dell.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5216c6b7c99e..e6b622e1ba92 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -83,7 +83,7 @@ struct ioatdma_device {
 
 #define IOAT_MAX_ORDER 16
 #define IOAT_MAX_DESCS (1 << IOAT_MAX_ORDER)
-#define IOAT_CHUNK_SIZE (SZ_2M)
+#define IOAT_CHUNK_SIZE (SZ_512K)
 #define IOAT_DESCS_PER_CHUNK (IOAT_CHUNK_SIZE / IOAT_DESC_SZ)
 
 struct ioat_descs {

commit bd2bf302eef21aafa6da2cf829b87a9e33150658
Author: Leonid Ravich <Leonid.Ravich@emc.com>
Date:   Thu Apr 16 20:06:21 2020 +0300

    dmaengine: ioat: fixing chunk sizing macros dependency
    
    changing macros which assumption is chunk size of 2M,
    which can be other size
    prepare for changing allocation chunk size.
    
    Acked-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Leonid Ravich <Leonid.Ravich@emc.com>
    Link: https://lore.kernel.org/r/20200416170628.16196-1-leonid.ravich@dell.com
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index b8e8e0b9693c..5216c6b7c99e 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -81,6 +81,11 @@ struct ioatdma_device {
 	u32 msixpba;
 };
 
+#define IOAT_MAX_ORDER 16
+#define IOAT_MAX_DESCS (1 << IOAT_MAX_ORDER)
+#define IOAT_CHUNK_SIZE (SZ_2M)
+#define IOAT_DESCS_PER_CHUNK (IOAT_CHUNK_SIZE / IOAT_DESC_SZ)
+
 struct ioat_descs {
 	void *virt;
 	dma_addr_t hw;
@@ -128,7 +133,7 @@ struct ioatdma_chan {
 	u16 produce;
 	struct ioat_ring_ent **ring;
 	spinlock_t prep_lock;
-	struct ioat_descs descs[2];
+	struct ioat_descs descs[IOAT_MAX_DESCS / IOAT_DESCS_PER_CHUNK];
 	int desc_chunks;
 	int intr_coalesce;
 	int prev_intr_coalesce;
@@ -301,9 +306,6 @@ static inline bool is_ioat_bug(unsigned long err)
 	return !!err;
 }
 
-#define IOAT_MAX_ORDER 16
-#define IOAT_MAX_DESCS 65536
-#define IOAT_DESCS_PER_2M 32768
 
 static inline u32 ioat_ring_size(struct ioatdma_chan *ioat_chan)
 {

commit 9ab65aff02e842b09fbdcd7a7df02b63ed63442a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 15:51:37 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 7
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details the full
      gnu general public license is included in this distribution in the
      file called copying
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 9 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Jilayne Lovejoy <opensource@jilayne.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190519154041.244154651@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index aaafd0e882b5..b8e8e0b9693c 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -1,18 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Copyright(c) 2004 - 2009 Intel Corporation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License as published by the Free
- * Software Foundation; either version 2 of the License, or (at your option)
- * any later version.
- *
- * This program is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * The full GNU General Public License is included in this distribution in the
- * file called COPYING.
  */
 #ifndef IOATDMA_H
 #define IOATDMA_H

commit 4d75873f814055359bb6722c4e35a185d02157a8
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Fri Feb 22 09:59:54 2019 -0700

    dmaengine: ioatdma: Add Snow Ridge ioatdma device id
    
    Add Snowridge Xeon-D ioatdma PCI device id. Also applies for Icelake
    SP Xeon. This introduces ioatdma v3.4 platform. Also bumping driver version
    to 5.0 since we are adding additional code for 3.4 support.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vkoul@kernel.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 1ab42ec2b7ff..aaafd0e882b5 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -27,7 +27,7 @@
 #include "registers.h"
 #include "hw.h"
 
-#define IOAT_DMA_VERSION  "4.00"
+#define IOAT_DMA_VERSION  "5.00"
 
 #define IOAT_DMA_DCA_ANY_CPU		~0
 

commit bcdc4bd356c76a5bab2f480a73f089dc8e0e4e89
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Oct 24 03:02:23 2017 -0700

    dmaengine: Convert timers to use timer_setup()
    
    In preparation for unconditionally passing the struct timer_list pointer to
    all timer callbacks, switch to using the new timer_setup() and from_timer()
    to pass the timer pointer explicitly.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 56200eefcf5e..1ab42ec2b7ff 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -406,10 +406,9 @@ enum dma_status
 ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 		struct dma_tx_state *txstate);
 void ioat_cleanup_event(unsigned long data);
-void ioat_timer_event(unsigned long data);
+void ioat_timer_event(struct timer_list *t);
 int ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs);
 void ioat_issue_pending(struct dma_chan *chan);
-void ioat_timer_event(unsigned long data);
 
 /* IOAT Init functions */
 bool is_bwd_ioat(struct pci_dev *pdev);

commit 268e2519f5b7101d707a0df32e628e9990bc0da6
Author: Ujjal Singh <ujjal.singh@intel.com>
Date:   Tue Aug 22 20:31:18 2017 -0400

    dmaengine: ioatdma: Add intr_coalesce sysfs entry
    
    We observed performance increase with DMA copy from memory
    to MMIO by changing the interrupt coalescing value to 0.
    The previous set value was projected on the C5xxx Xeon
    platform and no longer holds true. Removing hard coded
    value and providing a tune-able in sysfs in order to allow
    user to tune this on a per channel basis. By default this
    value will be set to 0.
    Example of sysfs variable importing for interrupt coalescing
    value from command line:
    echo 5> /sys/devices/pci0000:00/0000:00:04.0/dma/dma0chan0/
    quickdata/intr_coalesce
    
    Reported-by: Nithin Sujir <nsujir@tintri.com>
    Signed-off-by: Ujjal Singh <ujjal.singh@intel.com>
    Acked-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index a9bc1a15b0d1..56200eefcf5e 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -142,11 +142,14 @@ struct ioatdma_chan {
 	spinlock_t prep_lock;
 	struct ioat_descs descs[2];
 	int desc_chunks;
+	int intr_coalesce;
+	int prev_intr_coalesce;
 };
 
 struct ioat_sysfs_entry {
 	struct attribute attr;
 	ssize_t (*show)(struct dma_chan *, char *);
+	ssize_t (*store)(struct dma_chan *, const char *, size_t);
 };
 
 /**

commit c997e30e7f65f00832abc5d92f7fd3d6ca325402
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Thu Mar 10 16:18:40 2016 -0700

    dmaengine: IOATDMA: revise channel reset workaround on CB3.3 platforms
    
    Previously we unloaded the interrupts and reloaded in order to work around
    a channel reset bug that cleared the MSIX table. This approach just isn't
    practical when a reset needs to happen in the error handler that just
    happens to be running in interrupt context (bottom half). It looks like we
    can work around the hardware issue by just storing a shadow copy of the
    MSIX table and restore it after reset.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 57a9b83db455..a9bc1a15b0d1 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -86,6 +86,11 @@ struct ioatdma_device {
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
 	u32 cap;
+
+	/* shadow version for CB3.3 chan reset errata workaround */
+	u64 msixtba0;
+	u64 msixdata0;
+	u32 msixpba;
 };
 
 struct ioat_descs {

commit dd4645ebb7d100bb04ba38ec58b499cbe95322fa
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Feb 10 15:00:32 2016 -0700

    dmaengine: IOATDMA: Allocate DMA descriptor ring in contig DMA memory
    
    Future IOATDMA hardware will take advantage of descriptors residing in
    contiguous memory. Setting the descriptor ring in max config DMA memory
    of 2MB. Each channel will need 2 of these chunks. This should provide 64k
    of 64B descriptors.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5f2f9fbcf184..57a9b83db455 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -62,7 +62,6 @@ enum ioat_irq_mode {
  * struct ioatdma_device - internal representation of a IOAT device
  * @pdev: PCI-Express device
  * @reg_base: MMIO register space base address
- * @dma_pool: for allocating DMA descriptors
  * @completion_pool: DMA buffers for completion ops
  * @sed_hw_pool: DMA super descriptor pools
  * @dma_dev: embedded struct dma_device
@@ -76,7 +75,6 @@ enum ioat_irq_mode {
 struct ioatdma_device {
 	struct pci_dev *pdev;
 	void __iomem *reg_base;
-	struct dma_pool *dma_pool;
 	struct dma_pool *completion_pool;
 #define MAX_SED_POOLS	5
 	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
@@ -90,6 +88,11 @@ struct ioatdma_device {
 	u32 cap;
 };
 
+struct ioat_descs {
+	void *virt;
+	dma_addr_t hw;
+};
+
 struct ioatdma_chan {
 	struct dma_chan dma_chan;
 	void __iomem *reg_base;
@@ -132,6 +135,8 @@ struct ioatdma_chan {
 	u16 produce;
 	struct ioat_ring_ent **ring;
 	spinlock_t prep_lock;
+	struct ioat_descs descs[2];
+	int desc_chunks;
 };
 
 struct ioat_sysfs_entry {
@@ -301,6 +306,8 @@ static inline bool is_ioat_bug(unsigned long err)
 }
 
 #define IOAT_MAX_ORDER 16
+#define IOAT_MAX_DESCS 65536
+#define IOAT_DESCS_PER_2M 32768
 
 static inline u32 ioat_ring_size(struct ioatdma_chan *ioat_chan)
 {

commit cd60cd96137f6cb3ea82cace9225626619e7a52d
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Feb 10 15:00:26 2016 -0700

    dmaengine: IOATDMA: Removing descriptor ring reshape
    
    Moving to contingous memory backed descriptor rings. This makes is really
    difficult and complex to do reshape. Going to remove this as I don't think
    we need to do it anymore.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index f471092440d3..5f2f9fbcf184 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -100,7 +100,6 @@ struct ioatdma_chan {
 	#define IOAT_COMPLETION_ACK 1
 	#define IOAT_RESET_PENDING 2
 	#define IOAT_KOBJ_INIT_FAIL 3
-	#define IOAT_RESHAPE_PENDING 4
 	#define IOAT_RUN 5
 	#define IOAT_CHAN_ACTIVE 6
 	struct timer_list timer;
@@ -302,10 +301,6 @@ static inline bool is_ioat_bug(unsigned long err)
 }
 
 #define IOAT_MAX_ORDER 16
-#define ioat_get_alloc_order() \
-	(min(ioat_ring_alloc_order, IOAT_MAX_ORDER))
-#define ioat_get_max_alloc_order() \
-	(min(ioat_ring_max_alloc_order, IOAT_MAX_ORDER))
 
 static inline u32 ioat_ring_size(struct ioatdma_chan *ioat_chan)
 {

commit 679cfbf79b4eb7d7d81195e6b9ab98106fd78a54
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Feb 10 15:00:21 2016 -0700

    dmaengine: IOATDMA: Convert pci_pool_* to dma_pool_*
    
    Converting old pci_pool_* calls to "new" dma_pool_* to make everything
    uniform.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index b8f48074789f..f471092440d3 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -76,8 +76,8 @@ enum ioat_irq_mode {
 struct ioatdma_device {
 	struct pci_dev *pdev;
 	void __iomem *reg_base;
-	struct pci_pool *dma_pool;
-	struct pci_pool *completion_pool;
+	struct dma_pool *dma_pool;
+	struct dma_pool *completion_pool;
 #define MAX_SED_POOLS	5
 	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
 	struct dma_device dma_dev;

commit d3cd63f91b84c60e0b30ee8a45b8f291dac7bbff
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Fri Nov 6 13:24:01 2015 -0700

    dmaengine: IOATDMA: Cleanup pre v3.0 chansts register reads
    
    Remove pre-3.0 channel status reads. 3.0 and later chansts register
    is 64bit and can be read 64bit. This was clarified with the hardware
    architects and since the driver now only support 3.0+ we don't need the
    legacy support
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 8f4e607d5817..b8f48074789f 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -235,43 +235,11 @@ ioat_chan_by_index(struct ioatdma_device *ioat_dma, int index)
 	return ioat_dma->idx[index];
 }
 
-static inline u64 ioat_chansts_32(struct ioatdma_chan *ioat_chan)
-{
-	u8 ver = ioat_chan->ioat_dma->version;
-	u64 status;
-	u32 status_lo;
-
-	/* We need to read the low address first as this causes the
-	 * chipset to latch the upper bits for the subsequent read
-	 */
-	status_lo = readl(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET_LOW(ver));
-	status = readl(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET_HIGH(ver));
-	status <<= 32;
-	status |= status_lo;
-
-	return status;
-}
-
-#if BITS_PER_LONG == 64
-
 static inline u64 ioat_chansts(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->ioat_dma->version;
-	u64 status;
-
-	 /* With IOAT v3.3 the status register is 64bit.  */
-	if (ver >= IOAT_VER_3_3)
-		status = readq(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET(ver));
-	else
-		status = ioat_chansts_32(ioat_chan);
-
-	return status;
+	return readq(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET);
 }
 
-#else
-#define ioat_chansts ioat_chansts_32
-#endif
-
 static inline u64 ioat_chansts_to_addr(u64 status)
 {
 	return status & IOAT_CHANSTS_COMPLETED_DESCRIPTOR_ADDR;

commit ad4a7b5065c1b4f5176e7d031c3cc2b36f776884
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Aug 26 13:17:24 2015 -0700

    dmaengine: ioatdma: adding shutdown support
    
    The ioatdma needs to be queisced and block all additional op submission
    during reboots. When NET_DMA was used, this caused issue as ops were still
    being sent to ioatdma during reboots even though PCI BME has been turned
    off. Even though NET_DMA has been deprecated, we need to prevent similar
    situations. The shutdown handler should address that.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 1bc084986646..8f4e607d5817 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -82,8 +82,9 @@ struct ioatdma_device {
 	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
 	struct dma_device dma_dev;
 	u8 version;
-	struct msix_entry msix_entries[4];
-	struct ioatdma_chan *idx[4];
+#define IOAT_MAX_CHANS 4
+	struct msix_entry msix_entries[IOAT_MAX_CHANS];
+	struct ioatdma_chan *idx[IOAT_MAX_CHANS];
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
 	u32 cap;
@@ -95,6 +96,7 @@ struct ioatdma_chan {
 	dma_addr_t last_completion;
 	spinlock_t cleanup_lock;
 	unsigned long state;
+	#define IOAT_CHAN_DOWN 0
 	#define IOAT_COMPLETION_ACK 1
 	#define IOAT_RESET_PENDING 2
 	#define IOAT_KOBJ_INIT_FAIL 3

commit 09659a5978e16a7f3676fd6cb41e21daa77ce9a6
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:49:11 2015 -0700

    dmaengine: ioatdma: Clean up IOAT_COMPLETION_PENDING flag
    
    IOAT_COMPLETION_PENDING flag was deprecated for v2 and v3 drivers but was
    not cleaned up. Doing that now. The commit deprecated this flag was
    4dec23d7 ioatdma: fix race between updating ioat->head and
    IOAT_COMPLETION_PENDING.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 9bc1395f23b3..1bc084986646 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -95,7 +95,6 @@ struct ioatdma_chan {
 	dma_addr_t last_completion;
 	spinlock_t cleanup_lock;
 	unsigned long state;
-	#define IOAT_COMPLETION_PENDING 0
 	#define IOAT_COMPLETION_ACK 1
 	#define IOAT_RESET_PENDING 2
 	#define IOAT_KOBJ_INIT_FAIL 3

commit c7b0e8d7b5d1523ca3a85d3d3d9d113bb19a5668
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:49:05 2015 -0700

    dmaengine: ioatdma: fixup kernel doc errors from dma.h
    
    ./scripts/kerne-doc is reporting errors on dma.h. Clean up all reported
    errors.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index df569d810b8f..9bc1395f23b3 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -63,11 +63,15 @@ enum ioat_irq_mode {
  * @pdev: PCI-Express device
  * @reg_base: MMIO register space base address
  * @dma_pool: for allocating DMA descriptors
+ * @completion_pool: DMA buffers for completion ops
+ * @sed_hw_pool: DMA super descriptor pools
  * @dma_dev: embedded struct dma_device
  * @version: version of ioatdma device
  * @msix_entries: irq handlers
  * @idx: per channel data
  * @dca: direct cache access context
+ * @irq_mode: interrupt mode (INTX, MSI, MSIX)
+ * @cap: read DMA capabilities register
  */
 struct ioatdma_device {
 	struct pci_dev *pdev;
@@ -138,9 +142,9 @@ struct ioat_sysfs_entry {
 /**
  * struct ioat_sed_ent - wrapper around super extended hardware descriptor
  * @hw: hardware SED
- * @sed_dma: dma address for the SED
- * @list: list member
+ * @dma: dma address for the SED
  * @parent: point to the dma descriptor that's the parent
+ * @hw_pool: descriptor pool index
  */
 struct ioat_sed_ent {
 	struct ioat_sed_raw_descriptor *hw;
@@ -152,7 +156,6 @@ struct ioat_sed_ent {
 /**
  * struct ioat_ring_ent - wrapper around hardware descriptor
  * @hw: hardware DMA descriptor (for memcpy)
- * @fill: hardware fill descriptor
  * @xor: hardware xor descriptor
  * @xor_ex: hardware xor extension descriptor
  * @pq: hardware pq descriptor
@@ -163,6 +166,7 @@ struct ioat_sed_ent {
  * @len: total transaction length for unmap
  * @result: asynchronous result of validate operations
  * @id: identifier for debug
+ * @sed: pointer to super extended descriptor sw desc
  */
 
 struct ioat_ring_ent {

commit ef97bd0f59741ca1a555b69b8708f6601e35c3ed
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:49:00 2015 -0700

    dmanegine: ioatdma: remove function ptrs in ioatdma_device
    
    Since we are a "single" device driver now we no longer require the function
    pointers in ioatdma_device. Remove.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 2e1f05464703..df569d810b8f 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -68,14 +68,6 @@ enum ioat_irq_mode {
  * @msix_entries: irq handlers
  * @idx: per channel data
  * @dca: direct cache access context
- * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
- * @enumerate_channels: hw version specific channel enumeration
- * @reset_hw: hw version specific channel (re)initialization
- * @cleanup_fn: select between the v2 and v3 cleanup routines
- * @timer_fn: select between the v2 and v3 timer watchdog routines
- * @self_test: hardware version specific self test for each supported op type
- *
- * Note: the v3 cleanup routine supports raid operations
  */
 struct ioatdma_device {
 	struct pci_dev *pdev;
@@ -91,12 +83,6 @@ struct ioatdma_device {
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
 	u32 cap;
-	void (*intr_quirk)(struct ioatdma_device *ioat_dma);
-	int (*enumerate_channels)(struct ioatdma_device *ioat_dma);
-	int (*reset_hw)(struct ioatdma_chan *ioat_chan);
-	void (*cleanup_fn)(unsigned long data);
-	void (*timer_fn)(unsigned long data);
-	int (*self_test)(struct ioatdma_device *ioat_dma);
 };
 
 struct ioatdma_chan {

commit 3372de5813e4da8305002ff6ffbfc0c7012cb319
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:55 2015 -0700

    dmaengine: ioatdma: removal of dma_v3.c and relevant ioat3 references
    
    Moving the relevant functions to their respective .c files and removal of
    dma_v3.c file. Also removed various ioat3 references when appropriate.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index a319befad1a3..2e1f05464703 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -438,24 +438,15 @@ ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 		struct dma_tx_state *txstate);
 void ioat_cleanup_event(unsigned long data);
 void ioat_timer_event(unsigned long data);
-enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
-				   struct dma_tx_state *txstate);
-bool ioat_cleanup_preamble(struct ioatdma_chan *ioat_chan,
-			   dma_addr_t *phys_complete);
 int ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs);
 void ioat_issue_pending(struct dma_chan *chan);
-bool reshape_ring(struct ioatdma_chan *ioat, int order);
-void __ioat_issue_pending(struct ioatdma_chan *ioat_chan);
 void ioat_timer_event(unsigned long data);
-int ioat_quiesce(struct ioatdma_chan *ioat_chan, unsigned long tmo);
-int ioat_reset_sync(struct ioatdma_chan *ioat_chan, unsigned long tmo);
-void __ioat_restart_chan(struct ioatdma_chan *ioat_chan);
 
 /* IOAT Init functions */
 bool is_bwd_ioat(struct pci_dev *pdev);
+struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 void ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *ioat_dma);
 int ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);
 void ioat_stop(struct ioatdma_chan *ioat_chan);
-struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 #endif /* IOATDMA_H */

commit 599d49de7f69cb5a23e913db24e168ba2f09bd05
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:49 2015 -0700

    dmaengine: ioatdma: move dma prep functions to single location
    
    Move all DMA descriptor prepping functions to prep.c file. Fixup all
    broken bits caused by the move.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index d2ffa5775d53..a319befad1a3 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -37,6 +37,14 @@
 
 #define chan_num(ch) ((int)((ch)->reg_base - (ch)->ioat_dma->reg_base) / 0x80)
 
+/* ioat hardware assumes at least two sources for raid operations */
+#define src_cnt_to_sw(x) ((x) + 2)
+#define src_cnt_to_hw(x) ((x) - 2)
+#define ndest_to_sw(x) ((x) + 1)
+#define ndest_to_hw(x) ((x) - 1)
+#define src16_cnt_to_sw(x) ((x) + 9)
+#define src16_cnt_to_hw(x) ((x) - 9)
+
 /*
  * workaround for IOAT ver.3.0 null descriptor issue
  * (channel returns error when size is 0)
@@ -190,15 +198,22 @@ struct ioat_ring_ent {
 	struct ioat_sed_ent *sed;
 };
 
+extern const struct sysfs_ops ioat_sysfs_ops;
+extern struct ioat_sysfs_entry ioat_version_attr;
+extern struct ioat_sysfs_entry ioat_cap_attr;
+extern int ioat_pending_level;
+extern int ioat_ring_alloc_order;
+extern struct kobj_type ioat_ktype;
+extern struct kmem_cache *ioat_cache;
+extern int ioat_ring_max_alloc_order;
+extern struct kmem_cache *ioat_sed_cache;
+
 static inline struct ioatdma_chan *to_ioat_chan(struct dma_chan *c)
 {
 	return container_of(c, struct ioatdma_chan, dma_chan);
 }
 
-
-
 /* wrapper around hardware descriptor format + additional software fields */
-
 #ifdef DEBUG
 #define set_desc_id(desc, i) ((desc)->id = (i))
 #define desc_id(desc) ((desc)->id)
@@ -381,13 +396,10 @@ ioat_set_chainaddr(struct ioatdma_chan *ioat_chan, u64 addr)
 	       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_HIGH);
 }
 
-irqreturn_t ioat_dma_do_interrupt(int irq, void *data);
-irqreturn_t ioat_dma_do_interrupt_msix(int irq, void *data);
-struct ioat_ring_ent **
-ioat_alloc_ring(struct dma_chan *c, int order, gfp_t flags);
-void ioat_start_null_desc(struct ioatdma_chan *ioat_chan);
-void ioat_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan);
-int ioat_reset_hw(struct ioatdma_chan *ioat_chan);
+/* IOAT Prep functions */
+struct dma_async_tx_descriptor *
+ioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,
+			   dma_addr_t dma_src, size_t len, unsigned long flags);
 struct dma_async_tx_descriptor *
 ioat_prep_interrupt_lock(struct dma_chan *c, unsigned long flags);
 struct dma_async_tx_descriptor *
@@ -412,53 +424,38 @@ struct dma_async_tx_descriptor *
 ioat_prep_pqxor_val(struct dma_chan *chan, dma_addr_t *src,
 		     unsigned int src_cnt, size_t len,
 		     enum sum_check_flags *result, unsigned long flags);
+
+/* IOAT Operation functions */
+irqreturn_t ioat_dma_do_interrupt(int irq, void *data);
+irqreturn_t ioat_dma_do_interrupt_msix(int irq, void *data);
+struct ioat_ring_ent **
+ioat_alloc_ring(struct dma_chan *c, int order, gfp_t flags);
+void ioat_start_null_desc(struct ioatdma_chan *ioat_chan);
+void ioat_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan);
+int ioat_reset_hw(struct ioatdma_chan *ioat_chan);
 enum dma_status
 ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 		struct dma_tx_state *txstate);
 void ioat_cleanup_event(unsigned long data);
 void ioat_timer_event(unsigned long data);
-bool is_bwd_ioat(struct pci_dev *pdev);
-int ioat_probe(struct ioatdma_device *ioat_dma);
-int ioat_register(struct ioatdma_device *ioat_dma);
-int ioat_dma_self_test(struct ioatdma_device *ioat_dma);
-void ioat_dma_remove(struct ioatdma_device *ioat_dma);
-struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-void ioat_init_channel(struct ioatdma_device *ioat_dma,
-		       struct ioatdma_chan *ioat_chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 				   struct dma_tx_state *txstate);
 bool ioat_cleanup_preamble(struct ioatdma_chan *ioat_chan,
 			   dma_addr_t *phys_complete);
-void ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);
-void ioat_kobject_del(struct ioatdma_device *ioat_dma);
-int ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);
-void ioat_stop(struct ioatdma_chan *ioat_chan);
-int ioat_dma_probe(struct ioatdma_device *ioat_dma, int dca);
-int ioat3_dma_probe(struct ioatdma_device *ioat_dma, int dca);
-struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 int ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs);
-int ioat_enumerate_channels(struct ioatdma_device *ioat_dma);
-struct dma_async_tx_descriptor *
-ioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,
-			   dma_addr_t dma_src, size_t len, unsigned long flags);
 void ioat_issue_pending(struct dma_chan *chan);
-int ioat_alloc_chan_resources(struct dma_chan *c);
-void ioat_free_chan_resources(struct dma_chan *c);
-void __ioat_restart_chan(struct ioatdma_chan *ioat_chan);
 bool reshape_ring(struct ioatdma_chan *ioat, int order);
 void __ioat_issue_pending(struct ioatdma_chan *ioat_chan);
 void ioat_timer_event(unsigned long data);
 int ioat_quiesce(struct ioatdma_chan *ioat_chan, unsigned long tmo);
 int ioat_reset_sync(struct ioatdma_chan *ioat_chan, unsigned long tmo);
+void __ioat_restart_chan(struct ioatdma_chan *ioat_chan);
 
-extern const struct sysfs_ops ioat_sysfs_ops;
-extern struct ioat_sysfs_entry ioat_version_attr;
-extern struct ioat_sysfs_entry ioat_cap_attr;
-extern int ioat_pending_level;
-extern int ioat_ring_alloc_order;
-extern struct kobj_type ioat_ktype;
-extern struct kmem_cache *ioat_cache;
-extern int ioat_ring_max_alloc_order;
-extern struct kmem_cache *ioat_sed_cache;
-
+/* IOAT Init functions */
+bool is_bwd_ioat(struct pci_dev *pdev);
+void ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);
+void ioat_kobject_del(struct ioatdma_device *ioat_dma);
+int ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);
+void ioat_stop(struct ioatdma_chan *ioat_chan);
+struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 #endif /* IOATDMA_H */

commit c0f28ce66ecfd9fa0ae662a2c7f3e68e537e77f4
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:43 2015 -0700

    dmaengine: ioatdma: move all the init routines
    
    Moving all the init routines to init.c and fixup anything broken during
    the move.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 2566ec6ae8a4..d2ffa5775d53 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -381,6 +381,43 @@ ioat_set_chainaddr(struct ioatdma_chan *ioat_chan, u64 addr)
 	       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_HIGH);
 }
 
+irqreturn_t ioat_dma_do_interrupt(int irq, void *data);
+irqreturn_t ioat_dma_do_interrupt_msix(int irq, void *data);
+struct ioat_ring_ent **
+ioat_alloc_ring(struct dma_chan *c, int order, gfp_t flags);
+void ioat_start_null_desc(struct ioatdma_chan *ioat_chan);
+void ioat_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan);
+int ioat_reset_hw(struct ioatdma_chan *ioat_chan);
+struct dma_async_tx_descriptor *
+ioat_prep_interrupt_lock(struct dma_chan *c, unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
+	       unsigned int src_cnt, size_t len, unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_xor_val(struct dma_chan *chan, dma_addr_t *src,
+		    unsigned int src_cnt, size_t len,
+		    enum sum_check_flags *result, unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
+	      unsigned int src_cnt, const unsigned char *scf, size_t len,
+	      unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
+		  unsigned int src_cnt, const unsigned char *scf, size_t len,
+		  enum sum_check_flags *pqres, unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_pqxor(struct dma_chan *chan, dma_addr_t dst, dma_addr_t *src,
+		 unsigned int src_cnt, size_t len, unsigned long flags);
+struct dma_async_tx_descriptor *
+ioat_prep_pqxor_val(struct dma_chan *chan, dma_addr_t *src,
+		     unsigned int src_cnt, size_t len,
+		     enum sum_check_flags *result, unsigned long flags);
+enum dma_status
+ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
+		struct dma_tx_state *txstate);
+void ioat_cleanup_event(unsigned long data);
+void ioat_timer_event(unsigned long data);
+bool is_bwd_ioat(struct pci_dev *pdev);
 int ioat_probe(struct ioatdma_device *ioat_dma);
 int ioat_register(struct ioatdma_device *ioat_dma);
 int ioat_dma_self_test(struct ioatdma_device *ioat_dma);
@@ -421,5 +458,7 @@ extern int ioat_pending_level;
 extern int ioat_ring_alloc_order;
 extern struct kobj_type ioat_ktype;
 extern struct kmem_cache *ioat_cache;
+extern int ioat_ring_max_alloc_order;
+extern struct kmem_cache *ioat_sed_cache;
 
 #endif /* IOATDMA_H */

commit 885b201056e942f7deb66496b5c501d2a35d6c04
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:32 2015 -0700

    dmaengine: ioatdma: remove dma_v2.*
    
    Clean out dma_v2 and remove ioat2 calls since we are moving everything
    to just ioat.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 11bbcf27f86f..2566ec6ae8a4 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -18,13 +18,14 @@
 #define IOATDMA_H
 
 #include <linux/dmaengine.h>
-#include "hw.h"
-#include "registers.h"
 #include <linux/init.h>
 #include <linux/dmapool.h>
 #include <linux/cache.h>
 #include <linux/pci_ids.h>
-#include <net/tcp.h>
+#include <linux/circ_buf.h>
+#include <linux/interrupt.h>
+#include "registers.h"
+#include "hw.h"
 
 #define IOAT_DMA_VERSION  "4.00"
 
@@ -154,6 +155,41 @@ struct ioat_sed_ent {
 	unsigned int hw_pool;
 };
 
+/**
+ * struct ioat_ring_ent - wrapper around hardware descriptor
+ * @hw: hardware DMA descriptor (for memcpy)
+ * @fill: hardware fill descriptor
+ * @xor: hardware xor descriptor
+ * @xor_ex: hardware xor extension descriptor
+ * @pq: hardware pq descriptor
+ * @pq_ex: hardware pq extension descriptor
+ * @pqu: hardware pq update descriptor
+ * @raw: hardware raw (un-typed) descriptor
+ * @txd: the generic software descriptor for all engines
+ * @len: total transaction length for unmap
+ * @result: asynchronous result of validate operations
+ * @id: identifier for debug
+ */
+
+struct ioat_ring_ent {
+	union {
+		struct ioat_dma_descriptor *hw;
+		struct ioat_xor_descriptor *xor;
+		struct ioat_xor_ext_descriptor *xor_ex;
+		struct ioat_pq_descriptor *pq;
+		struct ioat_pq_ext_descriptor *pq_ex;
+		struct ioat_pq_update_descriptor *pqu;
+		struct ioat_raw_descriptor *raw;
+	};
+	size_t len;
+	struct dma_async_tx_descriptor txd;
+	enum sum_check_flags *result;
+	#ifdef DEBUG
+	int id;
+	#endif
+	struct ioat_sed_ent *sed;
+};
+
 static inline struct ioatdma_chan *to_ioat_chan(struct dma_chan *c)
 {
 	return container_of(c, struct ioatdma_chan, dma_chan);
@@ -291,6 +327,60 @@ static inline bool is_ioat_bug(unsigned long err)
 	return !!err;
 }
 
+#define IOAT_MAX_ORDER 16
+#define ioat_get_alloc_order() \
+	(min(ioat_ring_alloc_order, IOAT_MAX_ORDER))
+#define ioat_get_max_alloc_order() \
+	(min(ioat_ring_max_alloc_order, IOAT_MAX_ORDER))
+
+static inline u32 ioat_ring_size(struct ioatdma_chan *ioat_chan)
+{
+	return 1 << ioat_chan->alloc_order;
+}
+
+/* count of descriptors in flight with the engine */
+static inline u16 ioat_ring_active(struct ioatdma_chan *ioat_chan)
+{
+	return CIRC_CNT(ioat_chan->head, ioat_chan->tail,
+			ioat_ring_size(ioat_chan));
+}
+
+/* count of descriptors pending submission to hardware */
+static inline u16 ioat_ring_pending(struct ioatdma_chan *ioat_chan)
+{
+	return CIRC_CNT(ioat_chan->head, ioat_chan->issued,
+			ioat_ring_size(ioat_chan));
+}
+
+static inline u32 ioat_ring_space(struct ioatdma_chan *ioat_chan)
+{
+	return ioat_ring_size(ioat_chan) - ioat_ring_active(ioat_chan);
+}
+
+static inline u16
+ioat_xferlen_to_descs(struct ioatdma_chan *ioat_chan, size_t len)
+{
+	u16 num_descs = len >> ioat_chan->xfercap_log;
+
+	num_descs += !!(len & ((1 << ioat_chan->xfercap_log) - 1));
+	return num_descs;
+}
+
+static inline struct ioat_ring_ent *
+ioat_get_ring_ent(struct ioatdma_chan *ioat_chan, u16 idx)
+{
+	return ioat_chan->ring[idx & (ioat_ring_size(ioat_chan) - 1)];
+}
+
+static inline void
+ioat_set_chainaddr(struct ioatdma_chan *ioat_chan, u64 addr)
+{
+	writel(addr & 0x00000000FFFFFFFF,
+	       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_LOW);
+	writel(addr >> 32,
+	       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_HIGH);
+}
+
 int ioat_probe(struct ioatdma_device *ioat_dma);
 int ioat_register(struct ioatdma_device *ioat_dma);
 int ioat_dma_self_test(struct ioatdma_device *ioat_dma);
@@ -306,7 +396,30 @@ void ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *ioat_dma);
 int ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);
 void ioat_stop(struct ioatdma_chan *ioat_chan);
+int ioat_dma_probe(struct ioatdma_device *ioat_dma, int dca);
+int ioat3_dma_probe(struct ioatdma_device *ioat_dma, int dca);
+struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+int ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs);
+int ioat_enumerate_channels(struct ioatdma_device *ioat_dma);
+struct dma_async_tx_descriptor *
+ioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,
+			   dma_addr_t dma_src, size_t len, unsigned long flags);
+void ioat_issue_pending(struct dma_chan *chan);
+int ioat_alloc_chan_resources(struct dma_chan *c);
+void ioat_free_chan_resources(struct dma_chan *c);
+void __ioat_restart_chan(struct ioatdma_chan *ioat_chan);
+bool reshape_ring(struct ioatdma_chan *ioat, int order);
+void __ioat_issue_pending(struct ioatdma_chan *ioat_chan);
+void ioat_timer_event(unsigned long data);
+int ioat_quiesce(struct ioatdma_chan *ioat_chan, unsigned long tmo);
+int ioat_reset_sync(struct ioatdma_chan *ioat_chan, unsigned long tmo);
+
 extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;
 extern struct ioat_sysfs_entry ioat_cap_attr;
+extern int ioat_pending_level;
+extern int ioat_ring_alloc_order;
+extern struct kobj_type ioat_ktype;
+extern struct kmem_cache *ioat_cache;
+
 #endif /* IOATDMA_H */

commit 55f878ec47e3ab560a046c9030a97b1048b74e8b
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:27 2015 -0700

    dmaengine: ioatdma: fixup ioatdma_device namings
    
    Changing the variable names for ioatdma_device to be consistently named
    ioat_dma instead of device/dma in order to avoid confusion and distinct
    from struct device. This will clearly indicate that it is an
    ioatdma_device. This also make all the naming consistent that the dma
    device is ioat_dma and all the channels are ioat_chan.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 43290d1c88ed..11bbcf27f86f 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -30,11 +30,11 @@
 
 #define IOAT_DMA_DCA_ANY_CPU		~0
 
-#define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
-#define to_dev(ioat_chan) (&(ioat_chan)->device->pdev->dev)
-#define to_pdev(ioat_chan) ((ioat_chan)->device->pdev)
+#define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, dma_dev)
+#define to_dev(ioat_chan) (&(ioat_chan)->ioat_dma->pdev->dev)
+#define to_pdev(ioat_chan) ((ioat_chan)->ioat_dma->pdev)
 
-#define chan_num(ch) ((int)((ch)->reg_base - (ch)->device->reg_base) / 0x80)
+#define chan_num(ch) ((int)((ch)->reg_base - (ch)->ioat_dma->reg_base) / 0x80)
 
 /*
  * workaround for IOAT ver.3.0 null descriptor issue
@@ -54,7 +54,7 @@ enum ioat_irq_mode {
  * @pdev: PCI-Express device
  * @reg_base: MMIO register space base address
  * @dma_pool: for allocating DMA descriptors
- * @common: embedded struct dma_device
+ * @dma_dev: embedded struct dma_device
  * @version: version of ioatdma device
  * @msix_entries: irq handlers
  * @idx: per channel data
@@ -75,19 +75,19 @@ struct ioatdma_device {
 	struct pci_pool *completion_pool;
 #define MAX_SED_POOLS	5
 	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
-	struct dma_device common;
+	struct dma_device dma_dev;
 	u8 version;
 	struct msix_entry msix_entries[4];
 	struct ioatdma_chan *idx[4];
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
 	u32 cap;
-	void (*intr_quirk)(struct ioatdma_device *device);
-	int (*enumerate_channels)(struct ioatdma_device *device);
+	void (*intr_quirk)(struct ioatdma_device *ioat_dma);
+	int (*enumerate_channels)(struct ioatdma_device *ioat_dma);
 	int (*reset_hw)(struct ioatdma_chan *ioat_chan);
 	void (*cleanup_fn)(unsigned long data);
 	void (*timer_fn)(unsigned long data);
-	int (*self_test)(struct ioatdma_device *device);
+	int (*self_test)(struct ioatdma_device *ioat_dma);
 };
 
 struct ioatdma_chan {
@@ -107,7 +107,7 @@ struct ioatdma_chan {
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
 	#define IDLE_TIMEOUT msecs_to_jiffies(2000)
 	#define RESET_DELAY msecs_to_jiffies(100)
-	struct ioatdma_device *device;
+	struct ioatdma_device *ioat_dma;
 	dma_addr_t completion_dma;
 	u64 *completion;
 	struct tasklet_struct cleanup_task;
@@ -188,14 +188,14 @@ __dump_desc_dbg(struct ioatdma_chan *ioat_chan, struct ioat_dma_descriptor *hw,
 	({ if (d) __dump_desc_dbg(c, d->hw, &d->txd, desc_id(d)); 0; })
 
 static inline struct ioatdma_chan *
-ioat_chan_by_index(struct ioatdma_device *device, int index)
+ioat_chan_by_index(struct ioatdma_device *ioat_dma, int index)
 {
-	return device->idx[index];
+	return ioat_dma->idx[index];
 }
 
 static inline u64 ioat_chansts_32(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->device->version;
+	u8 ver = ioat_chan->ioat_dma->version;
 	u64 status;
 	u32 status_lo;
 
@@ -214,7 +214,7 @@ static inline u64 ioat_chansts_32(struct ioatdma_chan *ioat_chan)
 
 static inline u64 ioat_chansts(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->device->version;
+	u8 ver = ioat_chan->ioat_dma->version;
 	u64 status;
 
 	 /* With IOAT v3.3 the status register is 64bit.  */
@@ -242,7 +242,7 @@ static inline u32 ioat_chanerr(struct ioatdma_chan *ioat_chan)
 
 static inline void ioat_suspend(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->device->version;
+	u8 ver = ioat_chan->ioat_dma->version;
 
 	writeb(IOAT_CHANCMD_SUSPEND,
 	       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
@@ -250,7 +250,7 @@ static inline void ioat_suspend(struct ioatdma_chan *ioat_chan)
 
 static inline void ioat_reset(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->device->version;
+	u8 ver = ioat_chan->ioat_dma->version;
 
 	writeb(IOAT_CHANCMD_RESET,
 	       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
@@ -258,7 +258,7 @@ static inline void ioat_reset(struct ioatdma_chan *ioat_chan)
 
 static inline bool ioat_reset_pending(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = ioat_chan->device->version;
+	u8 ver = ioat_chan->ioat_dma->version;
 	u8 cmd;
 
 	cmd = readb(ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
@@ -291,20 +291,20 @@ static inline bool is_ioat_bug(unsigned long err)
 	return !!err;
 }
 
-int ioat_probe(struct ioatdma_device *device);
-int ioat_register(struct ioatdma_device *device);
-int ioat_dma_self_test(struct ioatdma_device *device);
-void ioat_dma_remove(struct ioatdma_device *device);
+int ioat_probe(struct ioatdma_device *ioat_dma);
+int ioat_register(struct ioatdma_device *ioat_dma);
+int ioat_dma_self_test(struct ioatdma_device *ioat_dma);
+void ioat_dma_remove(struct ioatdma_device *ioat_dma);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-void ioat_init_channel(struct ioatdma_device *device,
+void ioat_init_channel(struct ioatdma_device *ioat_dma,
 		       struct ioatdma_chan *ioat_chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 				   struct dma_tx_state *txstate);
 bool ioat_cleanup_preamble(struct ioatdma_chan *ioat_chan,
 			   dma_addr_t *phys_complete);
-void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
-void ioat_kobject_del(struct ioatdma_device *device);
-int ioat_dma_setup_interrupts(struct ioatdma_device *device);
+void ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);
+void ioat_kobject_del(struct ioatdma_device *ioat_dma);
+int ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);
 void ioat_stop(struct ioatdma_chan *ioat_chan);
 extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;

commit 5a976888c953a50336c2266bab894c1c098462b3
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:21 2015 -0700

    dmaengine: ioatdma: clean up local dma channel data structure
    
    Kill the common ioatdma channel structure and everything that is not
    dma_chan to be ioat_dma_chan. Since we don't have to worry about v1
    and v2 ioatdma anymore this makes it much cleaner and obvious for
    maintenance.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 1e96eaa29068..43290d1c88ed 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -78,20 +78,20 @@ struct ioatdma_device {
 	struct dma_device common;
 	u8 version;
 	struct msix_entry msix_entries[4];
-	struct ioat_chan_common *idx[4];
+	struct ioatdma_chan *idx[4];
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
 	u32 cap;
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
-	int (*reset_hw)(struct ioat_chan_common *chan);
+	int (*reset_hw)(struct ioatdma_chan *ioat_chan);
 	void (*cleanup_fn)(unsigned long data);
 	void (*timer_fn)(unsigned long data);
 	int (*self_test)(struct ioatdma_device *device);
 };
 
-struct ioat_chan_common {
-	struct dma_chan common;
+struct ioatdma_chan {
+	struct dma_chan dma_chan;
 	void __iomem *reg_base;
 	dma_addr_t last_completion;
 	spinlock_t cleanup_lock;
@@ -112,6 +112,27 @@ struct ioat_chan_common {
 	u64 *completion;
 	struct tasklet_struct cleanup_task;
 	struct kobject kobj;
+
+/* ioat v2 / v3 channel attributes
+ * @xfercap_log; log2 of channel max transfer length (for fast division)
+ * @head: allocated index
+ * @issued: hardware notification point
+ * @tail: cleanup index
+ * @dmacount: identical to 'head' except for occasionally resetting to zero
+ * @alloc_order: log2 of the number of allocated descriptors
+ * @produce: number of descriptors to produce at submit time
+ * @ring: software ring buffer implementation of hardware ring
+ * @prep_lock: serializes descriptor preparation (producers)
+ */
+	size_t xfercap_log;
+	u16 head;
+	u16 issued;
+	u16 tail;
+	u16 dmacount;
+	u16 alloc_order;
+	u16 produce;
+	struct ioat_ring_ent **ring;
+	spinlock_t prep_lock;
 };
 
 struct ioat_sysfs_entry {
@@ -133,11 +154,13 @@ struct ioat_sed_ent {
 	unsigned int hw_pool;
 };
 
-static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
+static inline struct ioatdma_chan *to_ioat_chan(struct dma_chan *c)
 {
-	return container_of(c, struct ioat_chan_common, common);
+	return container_of(c, struct ioatdma_chan, dma_chan);
 }
 
+
+
 /* wrapper around hardware descriptor format + additional software fields */
 
 #ifdef DEBUG
@@ -149,10 +172,10 @@ static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
 #endif
 
 static inline void
-__dump_desc_dbg(struct ioat_chan_common *chan, struct ioat_dma_descriptor *hw,
+__dump_desc_dbg(struct ioatdma_chan *ioat_chan, struct ioat_dma_descriptor *hw,
 		struct dma_async_tx_descriptor *tx, int id)
 {
-	struct device *dev = to_dev(chan);
+	struct device *dev = to_dev(ioat_chan);
 
 	dev_dbg(dev, "desc[%d]: (%#llx->%#llx) cookie: %d flags: %#x"
 		" ctl: %#10.8x (op: %#x int_en: %d compl: %d)\n", id,
@@ -162,25 +185,25 @@ __dump_desc_dbg(struct ioat_chan_common *chan, struct ioat_dma_descriptor *hw,
 }
 
 #define dump_desc_dbg(c, d) \
-	({ if (d) __dump_desc_dbg(&c->base, d->hw, &d->txd, desc_id(d)); 0; })
+	({ if (d) __dump_desc_dbg(c, d->hw, &d->txd, desc_id(d)); 0; })
 
-static inline struct ioat_chan_common *
+static inline struct ioatdma_chan *
 ioat_chan_by_index(struct ioatdma_device *device, int index)
 {
 	return device->idx[index];
 }
 
-static inline u64 ioat_chansts_32(struct ioat_chan_common *chan)
+static inline u64 ioat_chansts_32(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = chan->device->version;
+	u8 ver = ioat_chan->device->version;
 	u64 status;
 	u32 status_lo;
 
 	/* We need to read the low address first as this causes the
 	 * chipset to latch the upper bits for the subsequent read
 	 */
-	status_lo = readl(chan->reg_base + IOAT_CHANSTS_OFFSET_LOW(ver));
-	status = readl(chan->reg_base + IOAT_CHANSTS_OFFSET_HIGH(ver));
+	status_lo = readl(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET_LOW(ver));
+	status = readl(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET_HIGH(ver));
 	status <<= 32;
 	status |= status_lo;
 
@@ -189,16 +212,16 @@ static inline u64 ioat_chansts_32(struct ioat_chan_common *chan)
 
 #if BITS_PER_LONG == 64
 
-static inline u64 ioat_chansts(struct ioat_chan_common *chan)
+static inline u64 ioat_chansts(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = chan->device->version;
+	u8 ver = ioat_chan->device->version;
 	u64 status;
 
 	 /* With IOAT v3.3 the status register is 64bit.  */
 	if (ver >= IOAT_VER_3_3)
-		status = readq(chan->reg_base + IOAT_CHANSTS_OFFSET(ver));
+		status = readq(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET(ver));
 	else
-		status = ioat_chansts_32(chan);
+		status = ioat_chansts_32(ioat_chan);
 
 	return status;
 }
@@ -212,31 +235,33 @@ static inline u64 ioat_chansts_to_addr(u64 status)
 	return status & IOAT_CHANSTS_COMPLETED_DESCRIPTOR_ADDR;
 }
 
-static inline u32 ioat_chanerr(struct ioat_chan_common *chan)
+static inline u32 ioat_chanerr(struct ioatdma_chan *ioat_chan)
 {
-	return readl(chan->reg_base + IOAT_CHANERR_OFFSET);
+	return readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);
 }
 
-static inline void ioat_suspend(struct ioat_chan_common *chan)
+static inline void ioat_suspend(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = chan->device->version;
+	u8 ver = ioat_chan->device->version;
 
-	writeb(IOAT_CHANCMD_SUSPEND, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+	writeb(IOAT_CHANCMD_SUSPEND,
+	       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
 }
 
-static inline void ioat_reset(struct ioat_chan_common *chan)
+static inline void ioat_reset(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = chan->device->version;
+	u8 ver = ioat_chan->device->version;
 
-	writeb(IOAT_CHANCMD_RESET, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+	writeb(IOAT_CHANCMD_RESET,
+	       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
 }
 
-static inline bool ioat_reset_pending(struct ioat_chan_common *chan)
+static inline bool ioat_reset_pending(struct ioatdma_chan *ioat_chan)
 {
-	u8 ver = chan->device->version;
+	u8 ver = ioat_chan->device->version;
 	u8 cmd;
 
-	cmd = readb(chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+	cmd = readb(ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
 	return (cmd & IOAT_CHANCMD_RESET) == IOAT_CHANCMD_RESET;
 }
 
@@ -272,15 +297,15 @@ int ioat_dma_self_test(struct ioatdma_device *device);
 void ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 void ioat_init_channel(struct ioatdma_device *device,
-		       struct ioat_chan_common *chan, int idx);
+		       struct ioatdma_chan *ioat_chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 				   struct dma_tx_state *txstate);
-bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
+bool ioat_cleanup_preamble(struct ioatdma_chan *ioat_chan,
 			   dma_addr_t *phys_complete);
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *device);
 int ioat_dma_setup_interrupts(struct ioatdma_device *device);
-void ioat_stop(struct ioat_chan_common *chan);
+void ioat_stop(struct ioatdma_chan *ioat_chan);
 extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;
 extern struct ioat_sysfs_entry ioat_cap_attr;

commit 85596a19478da5125f3471a0c474b3f05a78e390
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Aug 11 08:48:10 2015 -0700

    dmaengine: ioatdma: remove ioat1 specific code
    
    Cleaning up of ioat1 specific code as it is no longer supported
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 30f5c7eede16..1e96eaa29068 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -28,12 +28,9 @@
 
 #define IOAT_DMA_VERSION  "4.00"
 
-#define IOAT_LOW_COMPLETION_MASK	0xffffffc0
 #define IOAT_DMA_DCA_ANY_CPU		~0
 
 #define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
-#define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
-#define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, txd)
 #define to_dev(ioat_chan) (&(ioat_chan)->device->pdev->dev)
 #define to_pdev(ioat_chan) ((ioat_chan)->device->pdev)
 
@@ -122,23 +119,6 @@ struct ioat_sysfs_entry {
 	ssize_t (*show)(struct dma_chan *, char *);
 };
 
-/**
- * struct ioat_dma_chan - internal representation of a DMA channel
- */
-struct ioat_dma_chan {
-	struct ioat_chan_common base;
-
-	size_t xfercap;	/* XFERCAP register value expanded out */
-
-	spinlock_t desc_lock;
-	struct list_head free_desc;
-	struct list_head used_desc;
-
-	int pending;
-	u16 desccount;
-	u16 active;
-};
-
 /**
  * struct ioat_sed_ent - wrapper around super extended hardware descriptor
  * @hw: hardware SED
@@ -158,34 +138,8 @@ static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
 	return container_of(c, struct ioat_chan_common, common);
 }
 
-static inline struct ioat_dma_chan *to_ioat_chan(struct dma_chan *c)
-{
-	struct ioat_chan_common *chan = to_chan_common(c);
-
-	return container_of(chan, struct ioat_dma_chan, base);
-}
-
 /* wrapper around hardware descriptor format + additional software fields */
 
-/**
- * struct ioat_desc_sw - wrapper around hardware descriptor
- * @hw: hardware DMA descriptor (for memcpy)
- * @node: this descriptor will either be on the free list,
- *     or attached to a transaction list (tx_list)
- * @txd: the generic software descriptor for all engines
- * @id: identifier for debug
- */
-struct ioat_desc_sw {
-	struct ioat_dma_descriptor *hw;
-	struct list_head node;
-	size_t len;
-	struct list_head tx_list;
-	struct dma_async_tx_descriptor txd;
-	#ifdef DEBUG
-	int id;
-	#endif
-};
-
 #ifdef DEBUG
 #define set_desc_id(desc, i) ((desc)->id = (i))
 #define desc_id(desc) ((desc)->id)
@@ -253,13 +207,6 @@ static inline u64 ioat_chansts(struct ioat_chan_common *chan)
 #define ioat_chansts ioat_chansts_32
 #endif
 
-static inline void ioat_start(struct ioat_chan_common *chan)
-{
-	u8 ver = chan->device->version;
-
-	writeb(IOAT_CHANCMD_START, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
-}
-
 static inline u64 ioat_chansts_to_addr(u64 status)
 {
 	return status & IOAT_CHANSTS_COMPLETED_DESCRIPTOR_ADDR;
@@ -293,16 +240,6 @@ static inline bool ioat_reset_pending(struct ioat_chan_common *chan)
 	return (cmd & IOAT_CHANCMD_RESET) == IOAT_CHANCMD_RESET;
 }
 
-static inline void ioat_set_chainaddr(struct ioat_dma_chan *ioat, u64 addr)
-{
-	struct ioat_chan_common *chan = &ioat->base;
-
-	writel(addr & 0x00000000FFFFFFFF,
-	       chan->reg_base + IOAT1_CHAINADDR_OFFSET_LOW);
-	writel(addr >> 32,
-	       chan->reg_base + IOAT1_CHAINADDR_OFFSET_HIGH);
-}
-
 static inline bool is_ioat_active(unsigned long status)
 {
 	return ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_ACTIVE);
@@ -331,11 +268,9 @@ static inline bool is_ioat_bug(unsigned long err)
 
 int ioat_probe(struct ioatdma_device *device);
 int ioat_register(struct ioatdma_device *device);
-int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
 int ioat_dma_self_test(struct ioatdma_device *device);
 void ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-dma_addr_t ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,

commit 3b62286d0ef785815994e2558e8cfb686597b0cd
Author: Jarkko Nikula <jarkko.nikula@linux.intel.com>
Date:   Mon Mar 16 09:37:24 2015 +0200

    dmaengine: Remove FSF mailing addresses
    
    Free Software Foundation mailing address has been moved in the past and some
    of the addresses here are outdated. Remove them from file headers since the
    COPYING file in the kernel sources includes it.
    
    Signed-off-by: Jarkko Nikula <jarkko.nikula@linux.intel.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index d63f68b1aa35..30f5c7eede16 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -11,10 +11,6 @@
  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
  * more details.
  *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 59
- * Temple Place - Suite 330, Boston, MA  02111-1307, USA.
- *
  * The full GNU General Public License is included in this distribution in the
  * file called COPYING.
  */

commit 7bced397510ab569d31de4c70b39e13355046387
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon Dec 30 12:37:29 2013 -0800

    net_dma: simple removal
    
    Per commit "77873803363c net_dma: mark broken" net_dma is no longer used
    and there is no plan to fix it.
    
    This is the mechanical removal of bits in CONFIG_NET_DMA ifdef guards.
    Reverting the remainder of the net_dma induced changes is deferred to
    subsequent patches.
    
    Marked for stable due to Roman's report of a memory leak in
    dma_pin_iovec_pages():
    
        https://lkml.org/lkml/2014/9/3/177
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: David Whipple <whipple@securedatainnovations.ch>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index e982f00a9843..d63f68b1aa35 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -214,13 +214,6 @@ __dump_desc_dbg(struct ioat_chan_common *chan, struct ioat_dma_descriptor *hw,
 #define dump_desc_dbg(c, d) \
 	({ if (d) __dump_desc_dbg(&c->base, d->hw, &d->txd, desc_id(d)); 0; })
 
-static inline void ioat_set_tcp_copy_break(unsigned long copybreak)
-{
-	#ifdef CONFIG_NET_DMA
-	sysctl_tcp_dma_copybreak = copybreak;
-	#endif
-}
-
 static inline struct ioat_chan_common *
 ioat_chan_by_index(struct ioatdma_device *device, int index)
 {

commit da87ca4d4ca101f177fffd84f1f0a5e4c0343557
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Feb 19 16:19:35 2014 -0800

    ioat: fix tasklet tear down
    
    Since commit 77873803363c "net_dma: mark broken" we no longer pin dma
    engines active for the network-receive-offload use case.  As a result
    the ->free_chan_resources() that occurs after the driver self test no
    longer has a NET_DMA induced ->alloc_chan_resources() to back it up.  A
    late firing irq can lead to ksoftirqd spinning indefinitely due to the
    tasklet_disable() performed by ->free_chan_resources().  Only
    ->alloc_chan_resources() can clear this condition in affected kernels.
    
    This problem has been present since commit 3e037454bcfa "I/OAT: Add
    support for MSI and MSI-X" in 2.6.24, but is now exposed. Given the
    NET_DMA use case is deprecated we can revisit moving the driver to use
    threaded irqs.  For now, just tear down the irq and tasklet properly by:
    
    1/ Disable the irq from triggering the tasklet
    
    2/ Disable the irq from re-arming
    
    3/ Flush inflight interrupts
    
    4/ Flush the timer
    
    5/ Flush inflight tasklets
    
    References:
    https://lkml.org/lkml/2014/1/27/282
    https://lkml.org/lkml/2014/2/19/672
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: <stable@vger.kernel.org>
    Reported-by: Mike Galbraith <bitbucket@online.de>
    Reported-by: Stanislav Fomichev <stfomichev@yandex-team.ru>
    Tested-by: Mike Galbraith <bitbucket@online.de>
    Tested-by: Stanislav Fomichev <stfomichev@yandex-team.ru>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 11fb877ddca9..e982f00a9843 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -356,6 +356,7 @@ bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *device);
 int ioat_dma_setup_interrupts(struct ioatdma_device *device);
+void ioat_stop(struct ioat_chan_common *chan);
 extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;
 extern struct ioat_sysfs_entry ioat_cap_attr;

commit 4c5d9619e06b960d14f5640341f40e71f78801c2
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Nov 13 16:29:52 2013 -0800

    ioat: kill msix_single_vector support
    
    Once we have determined that we will not have all of our desired msix
    vectors there is no point in attempting a single msix allocation.  The
    driver will already need to read registers to determine the source of
    the interrupt the fact that it is msix is moot.  Fallback directly to
    msi.
    
    Reported-by: Alexander Gordeev <agordeev@redhat.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index df552d841481..11fb877ddca9 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -52,7 +52,6 @@
 enum ioat_irq_mode {
 	IOAT_NOIRQ = 0,
 	IOAT_MSIX,
-	IOAT_MSIX_SINGLE,
 	IOAT_MSI,
 	IOAT_INTX
 };

commit 59056e85d7dd337674c65d9dac65008cb46a98cd
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Nov 13 10:57:18 2013 -0800

    ioatdma: clean up sed pool kmem_cache
    
    Use a single cache for all sed allocations.  No need to make it per
    channel.  This also avoids the slub_debug warnings for multiple caches
    with the same name.
    
    Switching to dmam_pool_create() to fix leaking the dma pools on
    initialization failure and lets us kill ioat3_dma_remove().
    
    Cc: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 4300d5af188f..df552d841481 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -83,7 +83,6 @@ struct ioatdma_device {
 	struct pci_pool *completion_pool;
 #define MAX_SED_POOLS	5
 	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
-	struct kmem_cache *sed_pool;
 	struct dma_device common;
 	u8 version;
 	struct msix_entry msix_entries[4];

commit 54f8d501e842879143e867e70996574a54d1e130
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Fri Oct 18 19:35:32 2013 +0200

    dmaengine: remove DMA unmap from drivers
    
    Remove support for DMA unmapping from drivers as it is no longer
    needed (DMA core code is now handling it).
    
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    [djbw: fix up chan2parent() unused warning in drivers/dma/dw/core.c]
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 54fb7b9ff9aa..4300d5af188f 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -342,16 +342,6 @@ static inline bool is_ioat_bug(unsigned long err)
 	return !!err;
 }
 
-static inline void ioat_unmap(struct pci_dev *pdev, dma_addr_t addr, size_t len,
-			      int direction, enum dma_ctrl_flags flags, bool dst)
-{
-	if ((dst && (flags & DMA_COMPL_DEST_UNMAP_SINGLE)) ||
-	    (!dst && (flags & DMA_COMPL_SRC_UNMAP_SINGLE)))
-		pci_unmap_single(pdev, addr, len, direction);
-	else
-		pci_unmap_page(pdev, addr, len, direction);
-}
-
 int ioat_probe(struct ioatdma_device *device);
 int ioat_register(struct ioatdma_device *device);
 int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
@@ -363,8 +353,6 @@ void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 				   struct dma_tx_state *txstate);
-void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
-		    size_t len, struct ioat_dma_descriptor *hw);
 bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
 			   dma_addr_t *phys_complete);
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);

commit 75c6f0ab480657269b5014e0e457c7b18ba8597e
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Wed Apr 10 16:44:39 2013 -0700

    ioatdma: Adding write back descriptor error status support for ioatdma 3.3
    
    v3.3 provides support for write back descriptor error status. This allows
    reporting of errors in a descriptor field. In supporting this, certain
    errors such as P/Q validation errors no longer halts the channel. The DMA
    engine can continue to execute until the end of the chain and allow software
    to report the "errors" up the stack. We are also going to mask those error
    interrupts and handle them when the "chain" has completed at the end.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 35d74028773a..54fb7b9ff9aa 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -90,6 +90,7 @@ struct ioatdma_device {
 	struct ioat_chan_common *idx[4];
 	struct dca_provider *dca;
 	enum ioat_irq_mode irq_mode;
+	u32 cap;
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
 	int (*reset_hw)(struct ioat_chan_common *chan);

commit 7727eaa4490b7244934fe31f05e7329f30715267
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Mon Apr 15 10:25:56 2013 -0700

    ioatdma: Adding support for 16 src PQ ops and super extended descriptors
    
    v3.3 introduced 16 sources PQ operations. This also introduced super extended
    descriptors to support the 16 srcs operations. This patch adds support for
    the 16 sources ops and in turn adds the super extended descriptors for those
    ops.
    
    5 SED pools are created depending on the descriptor sizes. An SED can be a 64
    bytes sized descriptor or larger and must be physically contiguous. A kmem
    cache pool is created for allocating the software descriptor that manages the
    hardware descriptor. The super extended descriptor will take place of extended
    descriptor under certain operations and be "attached" to the op descriptor
    during operation. This is a new feature for ioatdma v3.3.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 976eba8c06c7..35d74028773a 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -81,6 +81,9 @@ struct ioatdma_device {
 	void __iomem *reg_base;
 	struct pci_pool *dma_pool;
 	struct pci_pool *completion_pool;
+#define MAX_SED_POOLS	5
+	struct dma_pool *sed_hw_pool[MAX_SED_POOLS];
+	struct kmem_cache *sed_pool;
 	struct dma_device common;
 	u8 version;
 	struct msix_entry msix_entries[4];
@@ -141,6 +144,20 @@ struct ioat_dma_chan {
 	u16 active;
 };
 
+/**
+ * struct ioat_sed_ent - wrapper around super extended hardware descriptor
+ * @hw: hardware SED
+ * @sed_dma: dma address for the SED
+ * @list: list member
+ * @parent: point to the dma descriptor that's the parent
+ */
+struct ioat_sed_ent {
+	struct ioat_sed_raw_descriptor *hw;
+	dma_addr_t dma;
+	struct ioat_ring_ent *parent;
+	unsigned int hw_pool;
+};
+
 static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
 {
 	return container_of(c, struct ioat_chan_common, common);

commit 3f09ede4237fe4691ac687c6c43cb4c1a530777b
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Mar 26 15:43:09 2013 -0700

    ioatdma: Removing PQ val disable for cb3.3
    
    The PQ Val ops work on the newer hardware so we should actually provide support
    for it and remove the disabling bits.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index b16902cd2684..976eba8c06c7 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -39,6 +39,7 @@
 #define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
 #define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, txd)
 #define to_dev(ioat_chan) (&(ioat_chan)->device->pdev->dev)
+#define to_pdev(ioat_chan) ((ioat_chan)->device->pdev)
 
 #define chan_num(ch) ((int)((ch)->reg_base - (ch)->device->reg_base) / 0x80)
 

commit 8a52b9ff1154a68b6a2a8da9a31a87e52f5f6418
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Mar 26 15:42:47 2013 -0700

    ioatdma: channel reset scheme fixup on Intel Atom S1200 platforms
    
    The Intel Atom S1200 family ioatdma changed the channel reset behavior.
    It does a reset similar to PCI FLR by resetting all the MSIX
    registers. We have to re-init msix interrupts because of this. This
    workaround is only specific to this platform and is not expected to carry
    over to the later generations.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 9285caadf825..b16902cd2684 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -48,6 +48,14 @@
  */
 #define NULL_DESC_BUFFER_SIZE 1
 
+enum ioat_irq_mode {
+	IOAT_NOIRQ = 0,
+	IOAT_MSIX,
+	IOAT_MSIX_SINGLE,
+	IOAT_MSI,
+	IOAT_INTX
+};
+
 /**
  * struct ioatdma_device - internal representation of a IOAT device
  * @pdev: PCI-Express device
@@ -77,6 +85,7 @@ struct ioatdma_device {
 	struct msix_entry msix_entries[4];
 	struct ioat_chan_common *idx[4];
 	struct dca_provider *dca;
+	enum ioat_irq_mode irq_mode;
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
 	int (*reset_hw)(struct ioat_chan_common *chan);
@@ -341,6 +350,7 @@ bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
 			   dma_addr_t *phys_complete);
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *device);
+int ioat_dma_setup_interrupts(struct ioatdma_device *device);
 extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;
 extern struct ioat_sysfs_entry ioat_cap_attr;

commit d92a8d7cbb6941d5d985ccb3453a2ac5c92f60e4
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Tue Mar 26 15:42:41 2013 -0700

    ioatdma: Add 64bit chansts register read for ioat v3.3.
    
    The channel status register for v3.3 is now 64bit. Use readq if available
    on v3.3 platforms.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 95ae7b3139ec..9285caadf825 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -201,7 +201,7 @@ ioat_chan_by_index(struct ioatdma_device *device, int index)
 	return device->idx[index];
 }
 
-static inline u64 ioat_chansts(struct ioat_chan_common *chan)
+static inline u64 ioat_chansts_32(struct ioat_chan_common *chan)
 {
 	u8 ver = chan->device->version;
 	u64 status;
@@ -218,6 +218,26 @@ static inline u64 ioat_chansts(struct ioat_chan_common *chan)
 	return status;
 }
 
+#if BITS_PER_LONG == 64
+
+static inline u64 ioat_chansts(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+	u64 status;
+
+	 /* With IOAT v3.3 the status register is 64bit.  */
+	if (ver >= IOAT_VER_3_3)
+		status = readq(chan->reg_base + IOAT_CHANSTS_OFFSET(ver));
+	else
+		status = ioat_chansts_32(chan);
+
+	return status;
+}
+
+#else
+#define ioat_chansts ioat_chansts_32
+#endif
+
 static inline void ioat_start(struct ioat_chan_common *chan)
 {
 	u8 ver = chan->device->version;

commit 50f9f97e70fa4679fa197cb6dea358329298b987
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Mon Mar 4 10:59:54 2013 -0700

    ioatdma: make debug output more readable
    
    Making OP field a hex instead of integer to make it more readable. Also add
    the dump out of the NEXT field.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Acked-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 53a4cbb78f47..95ae7b3139ec 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -179,7 +179,7 @@ __dump_desc_dbg(struct ioat_chan_common *chan, struct ioat_dma_descriptor *hw,
 	struct device *dev = to_dev(chan);
 
 	dev_dbg(dev, "desc[%d]: (%#llx->%#llx) cookie: %d flags: %#x"
-		" ctl: %#x (op: %d int_en: %d compl: %d)\n", id,
+		" ctl: %#10.8x (op: %#x int_en: %d compl: %d)\n", id,
 		(unsigned long long) tx->phys,
 		(unsigned long long) hw->next, tx->cookie, tx->flags,
 		hw->ctl, hw->ctl_f.op, hw->ctl_f.int_en, hw->ctl_f.compl_write);

commit 5115f3c19d17851aaff5a857f55b4a019c908775
Merge: c41b3810c09e 17166a3b6e88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 09:24:48 2013 -0800

    Merge branch 'next' of git://git.infradead.org/users/vkoul/slave-dma
    
    Pull slave-dmaengine updates from Vinod Koul:
     "This is fairly big pull by my standards as I had missed last merge
      window.  So we have the support for device tree for slave-dmaengine,
      large updates to dw_dmac driver from Andy for reusing on different
      architectures.  Along with this we have fixes on bunch of the drivers"
    
    Fix up trivial conflicts, usually due to #include line movement next to
    each other.
    
    * 'next' of git://git.infradead.org/users/vkoul/slave-dma: (111 commits)
      Revert "ARM: SPEAr13xx: Pass DW DMAC platform data from DT"
      ARM: dts: pl330: Add #dma-cells for generic dma binding support
      DMA: PL330: Register the DMA controller with the generic DMA helpers
      DMA: PL330: Add xlate function
      DMA: PL330: Add new pl330 filter for DT case.
      dma: tegra20-apb-dma: remove unnecessary assignment
      edma: do not waste memory for dma_mask
      dma: coh901318: set residue only if dma is in progress
      dma: coh901318: avoid unbalanced locking
      dmaengine.h: remove redundant else keyword
      dma: of-dma: protect list write operation by spin_lock
      dmaengine: ste_dma40: do not remove descriptors for cyclic transfers
      dma: of-dma.c: fix memory leakage
      dw_dmac: apply default dma_mask if needed
      dmaengine: ioat - fix spare sparse complain
      dmaengine: move drivers/of/dma.c -> drivers/dma/of-dma.c
      ioatdma: fix race between updating ioat->head and IOAT_COMPLETION_PENDING
      dw_dmac: add support for Lynxpoint DMA controllers
      dw_dmac: return proper residue value
      dw_dmac: fill individual length of descriptor
      ...

commit 4dec23d7718e6f1f5e1773698d112025169e7d49
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Thu Feb 7 14:38:32 2013 -0700

    ioatdma: fix race between updating ioat->head and IOAT_COMPLETION_PENDING
    
    There is a race that can hit during __cleanup() when the ioat->head pointer is
    incremented during descriptor submission. The __cleanup() can clear the
    PENDING flag when it does not see any active descriptors. This causes new
    submitted descriptors to be ignored because the COMPLETION_PENDING flag is
    cleared. This was introduced when code was adapted from ioatdma v1 to ioatdma
    v2. For v2 and v3, IOAT_COMPLETION_PENDING flag will be abandoned and a new
    flag IOAT_CHAN_ACTIVE will be utilized. This flag will also be protected under
    the prep_lock when being modified in order to avoid the race.
    
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Reviewed-by: Dan Williams <djbw@fb.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5e8fe01ba69d..1020598b80d3 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -97,6 +97,7 @@ struct ioat_chan_common {
 	#define IOAT_KOBJ_INIT_FAIL 3
 	#define IOAT_RESHAPE_PENDING 4
 	#define IOAT_RUN 5
+	#define IOAT_CHAN_ACTIVE 6
 	struct timer_list timer;
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
 	#define IDLE_TIMEOUT msecs_to_jiffies(2000)

commit 4bf27b8b333bcd291664fd0f7d129099d474a23b
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Dec 21 15:09:59 2012 -0800

    Drivers: dma: remove __dev* attributes.
    
    CONFIG_HOTPLUG is going away as an option.  As a result, the __dev*
    markings need to be removed.
    
    This change removes the use of __devinit, __devexit_p, __devinitconst,
    and __devexit from these drivers.
    
    Based on patches originally written by Bill Pemberton, but redone by me
    in order to handle some of the coding style issues better, by hand.
    
    Cc: Bill Pemberton <wfp5p@virginia.edu>
    Cc: Viresh Kumar <viresh.linux@gmail.com>
    Cc: Dan Williams <djbw@fb.com>
    Cc: Vinod Koul <vinod.koul@intel.com>
    Cc: Barry Song <baohua.song@csr.com>
    Cc: Jeff Kirsher <jeffrey.t.kirsher@intel.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Cc: Jassi Brar <jassisinghbrar@gmail.com>
    Cc: Dave Jiang <dave.jiang@intel.com>
    Cc: Bill Pemberton <wfp5p@virginia.edu>
    Cc: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5e8fe01ba69d..087935f1565f 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -303,13 +303,12 @@ static inline void ioat_unmap(struct pci_dev *pdev, dma_addr_t addr, size_t len,
 		pci_unmap_page(pdev, addr, len, direction);
 }
 
-int __devinit ioat_probe(struct ioatdma_device *device);
-int __devinit ioat_register(struct ioatdma_device *device);
-int __devinit ioat1_dma_probe(struct ioatdma_device *dev, int dca);
-int __devinit ioat_dma_self_test(struct ioatdma_device *device);
-void __devexit ioat_dma_remove(struct ioatdma_device *device);
-struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
-					      void __iomem *iobase);
+int ioat_probe(struct ioatdma_device *device);
+int ioat_register(struct ioatdma_device *device);
+int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
+int ioat_dma_self_test(struct ioatdma_device *device);
+void ioat_dma_remove(struct ioatdma_device *device);
+struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 dma_addr_t ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx);

commit 94fb175c0414902ad9dbd956addf3a5feafbc85b
Merge: a9e1e53bcfb2 a2bd1140a264
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 10 15:30:16 2012 -0700

    Merge tag 'dmaengine-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine
    
    Pull dmaengine fixes from Dan Williams:
    
    1/ regression fix for Xen as it now trips over a broken assumption
       about the dma address size on 32-bit builds
    
    2/ new quirk for netdma to ignore dma channels that cannot meet
       netdma alignment requirements
    
    3/ fixes for two long standing issues in ioatdma (ring size overflow)
       and iop-adma (potential stack corruption)
    
    * tag 'dmaengine-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/djbw/dmaengine:
      netdma: adding alignment check for NETDMA ops
      ioatdma: DMA copy alignment needed to address IOAT DMA silicon errata
      ioat: ring size variables need to be 32bit to avoid overflow
      iop-adma: Corrected array overflow in RAID6 Xscale(R) test.
      ioat: fix size of 'completion' for Xen

commit 275029353953c2117941ade84f02a2303912fad1
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Mar 23 13:36:42 2012 -0700

    ioat: fix size of 'completion' for Xen
    
    Starting with v3.2 Jonathan reports that Xen crashes loading the ioatdma
    driver.  A debug run shows:
    
      ioatdma 0000:00:16.4: desc[0]: (0x300cc7000->0x300cc7040) cookie: 0 flags: 0x2 ctl: 0x29 (op: 0 int_en: 1 compl: 1)
      ...
      ioatdma 0000:00:16.4: ioat_get_current_completion: phys_complete: 0xcc7000
    
    ...which shows that in this environment GFP_KERNEL memory may be backed
    by a 64-bit dma address.  This breaks the driver's assumption that an
    unsigned long should be able to contain the physical address for
    descriptor memory.  Switch to dma_addr_t which beyond being the right
    size, is the true type for the data i.e. an io-virtual address
    inidicating the engine's last processed descriptor.
    
    [stable: 3.2+]
    Cc: <stable@vger.kernel.org>
    Reported-by: Jonathan Nieder <jrnieder@gmail.com>
    Reported-by: William Dauchy <wdauchy@gmail.com>
    Tested-by: William Dauchy <wdauchy@gmail.com>
    Tested-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5216c8a92a21..8bebddd189c7 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -88,7 +88,7 @@ struct ioatdma_device {
 struct ioat_chan_common {
 	struct dma_chan common;
 	void __iomem *reg_base;
-	unsigned long last_completion;
+	dma_addr_t last_completion;
 	spinlock_t cleanup_lock;
 	dma_cookie_t completed_cookie;
 	unsigned long state;
@@ -333,7 +333,7 @@ int __devinit ioat_dma_self_test(struct ioatdma_device *device);
 void __devexit ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
 					      void __iomem *iobase);
-unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
+dma_addr_t ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx);
 enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
@@ -341,7 +341,7 @@ enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
 		    size_t len, struct ioat_dma_descriptor *hw);
 bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
-			   unsigned long *phys_complete);
+			   dma_addr_t *phys_complete);
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *device);
 extern const struct sysfs_ops ioat_sysfs_ops;

commit 96a2af41c78b1fbb1f567a3486bdc63f7b31c5fd
Author: Russell King - ARM Linux <linux@arm.linux.org.uk>
Date:   Tue Mar 6 22:35:27 2012 +0000

    dmaengine: consolidate tx_status functions
    
    Now that we have the completed cookie in the dma_chan structure, we
    can consolidate the tx_status functions by providing a function to set
    the txstate structure and returning the DMA status.  We also provide
    a separate helper to set the residue for cookies which are still in
    progress.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Jassi Brar <jassisinghbrar@gmail.com>
    [imx-sdma.c & mxs-dma.c]
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 9653b6b6a715..c7888bccd974 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -142,27 +142,6 @@ static inline struct ioat_dma_chan *to_ioat_chan(struct dma_chan *c)
 	return container_of(chan, struct ioat_dma_chan, base);
 }
 
-/**
- * ioat_tx_status - poll the status of an ioat transaction
- * @c: channel handle
- * @cookie: transaction identifier
- * @txstate: if set, updated with the transaction state
- */
-static inline enum dma_status
-ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
-		 struct dma_tx_state *txstate)
-{
-	dma_cookie_t last_used;
-	dma_cookie_t last_complete;
-
-	last_used = c->cookie;
-	last_complete = c->completed_cookie;
-
-	dma_set_tx_state(txstate, last_complete, last_used, 0);
-
-	return dma_async_is_complete(cookie, last_complete, last_used);
-}
-
 /* wrapper around hardware descriptor format + additional software fields */
 
 /**

commit 4d4e58de32a192fea65ab84509d17d199bd291c8
Author: Russell King - ARM Linux <linux@arm.linux.org.uk>
Date:   Tue Mar 6 22:34:06 2012 +0000

    dmaengine: move last completed cookie into generic dma_chan structure
    
    Every DMA engine implementation declares a last completed dma cookie
    in their private dma channel structures.  This is pointless, and
    forces driver specific code.  Move this out into the common dma_chan
    structure.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Jassi Brar <jassisinghbrar@gmail.com>
    [imx-sdma.c & mxs-dma.c]
    Tested-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Vinod Koul <vinod.koul@linux.intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5216c8a92a21..9653b6b6a715 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -90,7 +90,6 @@ struct ioat_chan_common {
 	void __iomem *reg_base;
 	unsigned long last_completion;
 	spinlock_t cleanup_lock;
-	dma_cookie_t completed_cookie;
 	unsigned long state;
 	#define IOAT_COMPLETION_PENDING 0
 	#define IOAT_COMPLETION_ACK 1
@@ -153,12 +152,11 @@ static inline enum dma_status
 ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 		 struct dma_tx_state *txstate)
 {
-	struct ioat_chan_common *chan = to_chan_common(c);
 	dma_cookie_t last_used;
 	dma_cookie_t last_complete;
 
 	last_used = c->cookie;
-	last_complete = chan->completed_cookie;
+	last_complete = c->completed_cookie;
 
 	dma_set_tx_state(txstate, last_complete, last_used, 0);
 

commit 556ab45f9a775bfa4762bacc0a4afb5b44b067bc
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Jul 23 15:47:56 2010 -0700

    ioat2: catch and recover from broken vtd configurations v6
    
    On some platforms (MacPro3,1) the BIOS assigns the ioatdma device to the
    incorrect iommu causing faults when the driver initializes.  Add a quirk
    to catch this misconfiguration and try falling back to untranslated
    operation (which works in the MacPro3,1 case).
    
    Assuming there are other platforms with misconfigured iommus teach the
    ioatdma driver to treat initialization failures as non-fatal (just fail
    the driver load and emit a warning instead of triggering a BUG_ON).
    
    This can be classified as a boot regression since 2.6.32 on affected
    platforms since the ioatdma module did not autoload prior to that
    kernel.
    
    Cc: <stable@kernel.org>
    Acked-by: David Woodhouse <David.Woodhouse@intel.com>
    Reported-by: Chris Li <lkml@chrisli.org>
    Tested-by: Chris Li <lkml@chrisli.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 6d3a73b57e54..5216c8a92a21 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -97,6 +97,7 @@ struct ioat_chan_common {
 	#define IOAT_RESET_PENDING 2
 	#define IOAT_KOBJ_INIT_FAIL 3
 	#define IOAT_RESHAPE_PENDING 4
+	#define IOAT_RUN 5
 	struct timer_list timer;
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
 	#define IDLE_TIMEOUT msecs_to_jiffies(2000)

commit 0b28330e39bbe0ffee4c56b09fc415fcec595ea3
Merge: 058276303dbc caa20d974c86
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Mon May 17 16:30:58 2010 -0700

    Merge branch 'ioat' into dmaengine

commit 074cc47679f8b0931d7d5384e95822d82768f149
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat May 1 15:22:55 2010 -0700

    ioat2,3: convert to producer/consumer locking
    
    Use separate locks for the descriptor prep (producer) and descriptor
    cleanup (consumer) paths.  Allows the producer path to run concurrently
    with the cleanup path.  Inspired by Documentation/circular-buffer.txt.
    
    Cc: David Howells <dhowells@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 86b97ac8774e..0c76578e9911 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -96,6 +96,7 @@ struct ioat_chan_common {
 	#define IOAT_COMPLETION_ACK 1
 	#define IOAT_RESET_PENDING 2
 	#define IOAT_KOBJ_INIT_FAIL 3
+	#define IOAT_RESHAPE_PENDING 4
 	struct timer_list timer;
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
 	#define IDLE_TIMEOUT msecs_to_jiffies(2000)

commit bca3469205402d9fb14060d255d8786ae2256640
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Mar 26 16:52:10 2010 -0700

    dmaengine: provide helper for setting txstate
    
    Simple conditional struct filler to cut out some duplicated code.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 23399672239e..26f48ef94c5c 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -158,11 +158,7 @@ ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 	last_used = c->cookie;
 	last_complete = chan->completed_cookie;
 
-	if (txstate) {
-		txstate->last = last_complete;
-		txstate->used = last_used;
-		txstate->residue = 0;
-	}
+	dma_set_tx_state(txstate, last_complete, last_used, 0);
 
 	return dma_async_is_complete(cookie, last_complete, last_used);
 }

commit 0793448187643b50af89d36b08470baf45a3cab4
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Fri Mar 26 16:50:49 2010 -0700

    DMAENGINE: generic channel status v2
    
    Convert the device_is_tx_complete() operation on the
    DMA engine to a generic device_tx_status()operation which
    can return three states, DMA_TX_RUNNING, DMA_TX_COMPLETE,
    DMA_TX_PAUSED.
    
    [dan.j.williams@intel.com: update for timberdale]
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Acked-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Cc: Nicolas Ferre <nicolas.ferre@atmel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Li Yang <leoli@freescale.com>
    Cc: Guennadi Liakhovetski <g.liakhovetski@gmx.de>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Haavard Skinnemoen <haavard.skinnemoen@atmel.com>
    Cc: Magnus Damm <damm@opensource.se>
    Cc: Liam Girdwood <lrg@slimlogic.co.uk>
    Cc: Joe Perches <joe@perches.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 86b97ac8774e..23399672239e 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -142,15 +142,14 @@ static inline struct ioat_dma_chan *to_ioat_chan(struct dma_chan *c)
 }
 
 /**
- * ioat_is_complete - poll the status of an ioat transaction
+ * ioat_tx_status - poll the status of an ioat transaction
  * @c: channel handle
  * @cookie: transaction identifier
- * @done: if set, updated with last completed transaction
- * @used: if set, updated with last used transaction
+ * @txstate: if set, updated with the transaction state
  */
 static inline enum dma_status
-ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
-		 dma_cookie_t *done, dma_cookie_t *used)
+ioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,
+		 struct dma_tx_state *txstate)
 {
 	struct ioat_chan_common *chan = to_chan_common(c);
 	dma_cookie_t last_used;
@@ -159,10 +158,11 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
 	last_used = c->cookie;
 	last_complete = chan->completed_cookie;
 
-	if (done)
-		*done = last_complete;
-	if (used)
-		*used = last_used;
+	if (txstate) {
+		txstate->last = last_complete;
+		txstate->used = last_used;
+		txstate->residue = 0;
+	}
 
 	return dma_async_is_complete(cookie, last_complete, last_used);
 }
@@ -338,8 +338,8 @@ struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
 unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx);
-enum dma_status ioat_is_dma_complete(struct dma_chan *c, dma_cookie_t cookie,
-				     dma_cookie_t *done, dma_cookie_t *used);
+enum dma_status ioat_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,
+				   struct dma_tx_state *txstate);
 void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
 		    size_t len, struct ioat_dma_descriptor *hw);
 bool ioat_cleanup_preamble(struct ioat_chan_common *chan,

commit 52cf25d0ab7f78eeecc59ac652ed5090f69b619e
Author: Emese Revfy <re.emese@gmail.com>
Date:   Tue Jan 19 02:58:23 2010 +0100

    Driver core: Constify struct sysfs_ops in struct kobj_type
    
    Constify struct sysfs_ops.
    
    This is part of the ops structure constification
    effort started by Arjan van de Ven et al.
    
    Benefits of this constification:
    
     * prevents modification of data that is shared
       (referenced) by many other structure instances
       at runtime
    
     * detects/prevents accidental (but not intentional)
       modification attempts on archs that enforce
       read-only kernel data at runtime
    
     * potentially better optimized code as the compiler
       can assume that the const data cannot be changed
    
     * the compiler/linker move const data into .rodata
       and therefore exclude them from false sharing
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    Acked-by: David Teigland <teigland@redhat.com>
    Acked-by: Matt Domsch <Matt_Domsch@dell.com>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Acked-by: Hans J. Koch <hjk@linutronix.de>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Acked-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 4f747a254074..86b97ac8774e 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -346,7 +346,7 @@ bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
 			   unsigned long *phys_complete);
 void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
 void ioat_kobject_del(struct ioatdma_device *device);
-extern struct sysfs_ops ioat_sysfs_ops;
+extern const struct sysfs_ops ioat_sysfs_ops;
 extern struct ioat_sysfs_entry ioat_version_attr;
 extern struct ioat_sysfs_entry ioat_cap_attr;
 #endif /* IOATDMA_H */

commit aa4d72ae946a4fa40486b871717778734184fa29
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Mar 3 21:21:13 2010 -0700

    ioat: cleanup ->timer_fn() and ->cleanup_fn() prototypes
    
    If the calling convention of ->timer_fn() and ->cleanup_fn() are unified
    across hardware versions we can drop parameters to ioat_init_channel() and
    unify ioat_is_dma_complete() implementations.
    
    Both ->timer_fn() and ->cleanup_fn() are modified to expect a struct
    dma_chan pointer.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index bbc3e78ef333..4f747a254074 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -61,7 +61,7 @@
  * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
  * @enumerate_channels: hw version specific channel enumeration
  * @reset_hw: hw version specific channel (re)initialization
- * @cleanup_tasklet: select between the v2 and v3 cleanup routines
+ * @cleanup_fn: select between the v2 and v3 cleanup routines
  * @timer_fn: select between the v2 and v3 timer watchdog routines
  * @self_test: hardware version specific self test for each supported op type
  *
@@ -80,7 +80,7 @@ struct ioatdma_device {
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
 	int (*reset_hw)(struct ioat_chan_common *chan);
-	void (*cleanup_tasklet)(unsigned long data);
+	void (*cleanup_fn)(unsigned long data);
 	void (*timer_fn)(unsigned long data);
 	int (*self_test)(struct ioatdma_device *device);
 };
@@ -337,10 +337,9 @@ struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
 					      void __iomem *iobase);
 unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
-		       struct ioat_chan_common *chan, int idx,
-		       void (*timer_fn)(unsigned long),
-		       void (*tasklet)(unsigned long),
-		       unsigned long ioat);
+		       struct ioat_chan_common *chan, int idx);
+enum dma_status ioat_is_dma_complete(struct dma_chan *c, dma_cookie_t cookie,
+				     dma_cookie_t *done, dma_cookie_t *used);
 void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
 		    size_t len, struct ioat_dma_descriptor *hw);
 bool ioat_cleanup_preamble(struct ioat_chan_common *chan,

commit a6d52d70677e99bdb89b6921c265d0a58c22e597
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Dec 19 15:36:02 2009 -0700

    ioat2,3: put channel hardware in known state at init
    
    Put the ioat2 and ioat3 state machines in the halted state with all
    errors cleared.
    
    The ioat1 init path is not disturbed for stability, there are no
    reported ioat1 initiaization issues.
    
    Cc: <stable@kernel.org>
    Reported-by: Roland Dreier <rdreier@cisco.com>
    Tested-by: Roland Dreier <rdreier@cisco.com>
    Acked-by: Simon Horman <horms@verge.net.au>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 45edde996480..bbc3e78ef333 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -60,6 +60,7 @@
  * @dca: direct cache access context
  * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
  * @enumerate_channels: hw version specific channel enumeration
+ * @reset_hw: hw version specific channel (re)initialization
  * @cleanup_tasklet: select between the v2 and v3 cleanup routines
  * @timer_fn: select between the v2 and v3 timer watchdog routines
  * @self_test: hardware version specific self test for each supported op type
@@ -78,6 +79,7 @@ struct ioatdma_device {
 	struct dca_provider *dca;
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
+	int (*reset_hw)(struct ioat_chan_common *chan);
 	void (*cleanup_tasklet)(unsigned long data);
 	void (*timer_fn)(unsigned long data);
 	int (*self_test)(struct ioatdma_device *device);
@@ -264,6 +266,22 @@ static inline void ioat_suspend(struct ioat_chan_common *chan)
 	writeb(IOAT_CHANCMD_SUSPEND, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
 }
 
+static inline void ioat_reset(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+
+	writeb(IOAT_CHANCMD_RESET, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+}
+
+static inline bool ioat_reset_pending(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+	u8 cmd;
+
+	cmd = readb(chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+	return (cmd & IOAT_CHANCMD_RESET) == IOAT_CHANCMD_RESET;
+}
+
 static inline void ioat_set_chainaddr(struct ioat_dma_chan *ioat, u64 addr)
 {
 	struct ioat_chan_common *chan = &ioat->base;

commit b57014def9afc2bd8a62299d2f51b77dad5ae0c7
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Nov 19 17:10:07 2009 -0700

    ioat2,3: report all uncorrectable errors
    
    Modify is_ioat_bug() to catch all errors that are uncorrectable, or not
    currently handled.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index c14fdfeb7f33..45edde996480 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -297,9 +297,7 @@ static inline bool is_ioat_suspended(unsigned long status)
 /* channel was fatally programmed */
 static inline bool is_ioat_bug(unsigned long err)
 {
-	return !!(err & (IOAT_CHANERR_SRC_ADDR_ERR|IOAT_CHANERR_DEST_ADDR_ERR|
-			 IOAT_CHANERR_NEXT_ADDR_ERR|IOAT_CHANERR_CONTROL_ERR|
-			 IOAT_CHANERR_LENGTH_ERR));
+	return !!err;
 }
 
 static inline void ioat_unmap(struct pci_dev *pdev, dma_addr_t addr, size_t len,

commit 3208ca52f3bfa36914c44db207d0a34071f9897f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Thu Sep 10 11:27:36 2009 -0700

    ioat: driver version 4.0
    
    A new ring implementation and the addition of raid functionality
    constitutes a bump in the driver major version number.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 6a675a2a2d1c..c14fdfeb7f33 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -30,7 +30,7 @@
 #include <linux/pci_ids.h>
 #include <net/tcp.h>
 
-#define IOAT_DMA_VERSION  "3.64"
+#define IOAT_DMA_VERSION  "4.00"
 
 #define IOAT_LOW_COMPLETION_MASK	0xffffffc0
 #define IOAT_DMA_DCA_ANY_CPU		~0

commit bbb20089a3275a19e475dbc21320c3742e3ca423
Merge: 3e48e656903e 657a77fa7284
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:55:21 2009 -0700

    Merge branch 'dmaengine' into async-tx-next
    
    Conflicts:
            crypto/async_tx/async_xor.c
            drivers/dma/ioat/dma_v2.h
            drivers/dma/ioat/pci.c
            drivers/md/raid5.c

commit ea25968a32a621b02c3715d6b649f0c6ef53c24e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:53:02 2009 -0700

    ioat: implement a private tx_list
    
    Drop ioatdma's use of tx_list from struct dma_async_tx_descriptor in
    preparation for removal of this field.
    
    Cc: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index d9d6a7e3cd76..8966fa5453a7 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -157,7 +157,7 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
  * struct ioat_desc_sw - wrapper around hardware descriptor
  * @hw: hardware DMA descriptor
  * @node: this descriptor will either be on the free list,
- *     or attached to a transaction list (async_tx.tx_list)
+ *     or attached to a transaction list (tx_list)
  * @txd: the generic software descriptor for all engines
  * @id: identifier for debug
  */
@@ -165,6 +165,7 @@ struct ioat_desc_sw {
 	struct ioat_dma_descriptor *hw;
 	struct list_head node;
 	size_t len;
+	struct list_head tx_list;
 	struct dma_async_tx_descriptor txd;
 	#ifdef DEBUG
 	int id;

commit 9de6fc717bdc574cf5faf9d46ce0f9d6265c7952
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:58 2009 -0700

    ioat3: xor self test
    
    This adds a hardware specific self test to be called from ioat_probe.
    In the ioat3 case we will have tests for all the different raid
    operations, while ioat1 and ioat2 will continue to just test memcpy.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index c2939b289185..0e37e426c729 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -62,10 +62,10 @@
  * @enumerate_channels: hw version specific channel enumeration
  * @cleanup_tasklet: select between the v2 and v3 cleanup routines
  * @timer_fn: select between the v2 and v3 timer watchdog routines
+ * @self_test: hardware version specific self test for each supported op type
  *
  * Note: the v3 cleanup routine supports raid operations
  */
-
 struct ioatdma_device {
 	struct pci_dev *pdev;
 	void __iomem *reg_base;
@@ -80,6 +80,7 @@ struct ioatdma_device {
 	int (*enumerate_channels)(struct ioatdma_device *device);
 	void (*cleanup_tasklet)(unsigned long data);
 	void (*timer_fn)(unsigned long data);
+	int (*self_test)(struct ioatdma_device *device);
 };
 
 struct ioat_chan_common {
@@ -313,6 +314,7 @@ static inline void ioat_unmap(struct pci_dev *pdev, dma_addr_t addr, size_t len,
 int __devinit ioat_probe(struct ioatdma_device *device);
 int __devinit ioat_register(struct ioatdma_device *device);
 int __devinit ioat1_dma_probe(struct ioatdma_device *dev, int dca);
+int __devinit ioat_dma_self_test(struct ioatdma_device *device);
 void __devexit ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
 					      void __iomem *iobase);

commit 5669e31c5a4874f1634bc0ffba268a6e2fa0cdd2
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:56 2009 -0700

    ioat: add 'ioat' sysfs attributes
    
    Export driver attributes for diagnostic purposes:
    'ring_size': total number of descriptors available to the engine
    'ring_active': number of descriptors in-flight
    'capabilities': supported operation types for this channel
    'version': Intel(R) QuickData specfication revision
    
    This also allows some chattiness to be removed from the driver startup
    as this information is now available via sysfs.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index c6d58bf541d1..c2939b289185 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -92,6 +92,7 @@ struct ioat_chan_common {
 	#define IOAT_COMPLETION_PENDING 0
 	#define IOAT_COMPLETION_ACK 1
 	#define IOAT_RESET_PENDING 2
+	#define IOAT_KOBJ_INIT_FAIL 3
 	struct timer_list timer;
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
 	#define IDLE_TIMEOUT msecs_to_jiffies(2000)
@@ -100,8 +101,13 @@ struct ioat_chan_common {
 	dma_addr_t completion_dma;
 	u64 *completion;
 	struct tasklet_struct cleanup_task;
+	struct kobject kobj;
 };
 
+struct ioat_sysfs_entry {
+	struct attribute attr;
+	ssize_t (*show)(struct dma_chan *, char *);
+};
 
 /**
  * struct ioat_dma_chan - internal representation of a DMA channel
@@ -117,6 +123,7 @@ struct ioat_dma_chan {
 
 	int pending;
 	u16 desccount;
+	u16 active;
 };
 
 static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
@@ -319,4 +326,9 @@ void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
 		    size_t len, struct ioat_dma_descriptor *hw);
 bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
 			   unsigned long *phys_complete);
+void ioat_kobject_add(struct ioatdma_device *device, struct kobj_type *type);
+void ioat_kobject_del(struct ioatdma_device *device);
+extern struct sysfs_ops ioat_sysfs_ops;
+extern struct ioat_sysfs_entry ioat_version_attr;
+extern struct ioat_sysfs_entry ioat_cap_attr;
 #endif /* IOATDMA_H */

commit bf40a6869c9198bdf56fe173961feb89e9f0d961
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:55 2009 -0700

    ioat3: split ioat3 support to its own file, add memset
    
    Up until this point the driver for Intel(R) QuickData Technology
    engines, specification versions 2 and 3, were mostly identical save for
    a few quirks.  Version 3.2 hardware adds many new capabilities (like
    raid offload support) requiring some infrastructure that is not relevant
    for v2.  For better code organization of the new funcionality move v3
    and v3.2 support to its own file dma_v3.c, and export some routines from
    the base files (dma.c and dma_v2.c) that can be reused directly.
    
    The first new capability included in this code reorganization is support
    for v3.2 memset operations.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 0d94e7804c13..c6d58bf541d1 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -60,6 +60,10 @@
  * @dca: direct cache access context
  * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
  * @enumerate_channels: hw version specific channel enumeration
+ * @cleanup_tasklet: select between the v2 and v3 cleanup routines
+ * @timer_fn: select between the v2 and v3 timer watchdog routines
+ *
+ * Note: the v3 cleanup routine supports raid operations
  */
 
 struct ioatdma_device {
@@ -74,6 +78,8 @@ struct ioatdma_device {
 	struct dca_provider *dca;
 	void (*intr_quirk)(struct ioatdma_device *device);
 	int (*enumerate_channels)(struct ioatdma_device *device);
+	void (*cleanup_tasklet)(unsigned long data);
+	void (*timer_fn)(unsigned long data);
 };
 
 struct ioat_chan_common {
@@ -287,6 +293,16 @@ static inline bool is_ioat_bug(unsigned long err)
 			 IOAT_CHANERR_LENGTH_ERR));
 }
 
+static inline void ioat_unmap(struct pci_dev *pdev, dma_addr_t addr, size_t len,
+			      int direction, enum dma_ctrl_flags flags, bool dst)
+{
+	if ((dst && (flags & DMA_COMPL_DEST_UNMAP_SINGLE)) ||
+	    (!dst && (flags & DMA_COMPL_SRC_UNMAP_SINGLE)))
+		pci_unmap_single(pdev, addr, len, direction);
+	else
+		pci_unmap_page(pdev, addr, len, direction);
+}
+
 int __devinit ioat_probe(struct ioatdma_device *device);
 int __devinit ioat_register(struct ioatdma_device *device);
 int __devinit ioat1_dma_probe(struct ioatdma_device *dev, int dca);

commit 2aec048cdc4a5a81163a42a61df903f76a27e737
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:42:54 2009 -0700

    ioat3: hardware version 3.2 register / descriptor definitions
    
    ioat3.2 adds raid5 and raid6 offload capabilities.
    
    Signed-off-by: Tom Picard <tom.s.picard@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index d9d6a7e3cd76..0d94e7804c13 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -155,7 +155,7 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
 
 /**
  * struct ioat_desc_sw - wrapper around hardware descriptor
- * @hw: hardware DMA descriptor
+ * @hw: hardware DMA descriptor (for memcpy)
  * @node: this descriptor will either be on the free list,
  *     or attached to a transaction list (async_tx.tx_list)
  * @txd: the generic software descriptor for all engines

commit a309218acee8606f7e235da20cc826eb06d9b0f6
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:02:01 2009 -0700

    ioat2,3: dynamically resize descriptor ring
    
    Increment the allocation order of the descriptor ring every time we run
    out of descriptors up to a maximum of allocation order specified by the
    module parameter 'ioat_max_alloc_order'.  After each idle period
    decrement the allocation order to a minimum order of
    'ioat_ring_alloc_order' (i.e. the default ring size, tunable as a module
    parameter).
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index dbfccac3e80c..d9d6a7e3cd76 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -88,6 +88,7 @@ struct ioat_chan_common {
 	#define IOAT_RESET_PENDING 2
 	struct timer_list timer;
 	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
+	#define IDLE_TIMEOUT msecs_to_jiffies(2000)
 	#define RESET_DELAY msecs_to_jiffies(100)
 	struct ioatdma_device *device;
 	dma_addr_t completion_dma;

commit 09c8a5b85e5f1e74a19bdd7c85547429d51df1cd
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:01:49 2009 -0700

    ioat: switch watchdog and reset handler from workqueue to timer
    
    In order to support dynamic resizing of the descriptor ring or polling
    for a descriptor in the presence of a hung channel the reset handler
    needs to make progress while in a non-preemptible context.  The current
    workqueue implementation precludes polling channel reset completion
    under spin_lock().
    
    This conversion also allows us to return to opportunistic cleanup in the
    ioat2 case as the timer implementation guarantees at least one cleanup
    after every descriptor is submitted.  This means the worst case
    completion latency becomes the timer frequency (for exceptional
    circumstances), but with the benefit of avoiding busy waiting when the
    lock is contended.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index ec851cf5345c..dbfccac3e80c 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -23,6 +23,7 @@
 
 #include <linux/dmaengine.h>
 #include "hw.h"
+#include "registers.h"
 #include <linux/init.h>
 #include <linux/dmapool.h>
 #include <linux/cache.h>
@@ -33,7 +34,6 @@
 
 #define IOAT_LOW_COMPLETION_MASK	0xffffffc0
 #define IOAT_DMA_DCA_ANY_CPU		~0
-#define IOAT_WATCHDOG_PERIOD		(2 * HZ)
 
 #define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
 #define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
@@ -42,9 +42,6 @@
 
 #define chan_num(ch) ((int)((ch)->reg_base - (ch)->device->reg_base) / 0x80)
 
-#define RESET_DELAY  msecs_to_jiffies(100)
-#define WATCHDOG_DELAY  round_jiffies(msecs_to_jiffies(2000))
-
 /*
  * workaround for IOAT ver.3.0 null descriptor issue
  * (channel returns error when size is 0)
@@ -72,7 +69,6 @@ struct ioatdma_device {
 	struct pci_pool *completion_pool;
 	struct dma_device common;
 	u8 version;
-	struct delayed_work work;
 	struct msix_entry msix_entries[4];
 	struct ioat_chan_common *idx[4];
 	struct dca_provider *dca;
@@ -81,24 +77,21 @@ struct ioatdma_device {
 };
 
 struct ioat_chan_common {
+	struct dma_chan common;
 	void __iomem *reg_base;
-
 	unsigned long last_completion;
-	unsigned long last_completion_time;
-
 	spinlock_t cleanup_lock;
 	dma_cookie_t completed_cookie;
-	unsigned long watchdog_completion;
-	int watchdog_tcp_cookie;
-	u32 watchdog_last_tcp_cookie;
-	struct delayed_work work;
-
+	unsigned long state;
+	#define IOAT_COMPLETION_PENDING 0
+	#define IOAT_COMPLETION_ACK 1
+	#define IOAT_RESET_PENDING 2
+	struct timer_list timer;
+	#define COMPLETION_TIMEOUT msecs_to_jiffies(100)
+	#define RESET_DELAY msecs_to_jiffies(100)
 	struct ioatdma_device *device;
-	struct dma_chan common;
-
 	dma_addr_t completion_dma;
 	u64 *completion;
-	unsigned long last_compl_desc_addr_hw;
 	struct tasklet_struct cleanup_task;
 };
 
@@ -148,7 +141,6 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
 
 	last_used = c->cookie;
 	last_complete = chan->completed_cookie;
-	chan->watchdog_tcp_cookie = cookie;
 
 	if (done)
 		*done = last_complete;
@@ -215,6 +207,85 @@ ioat_chan_by_index(struct ioatdma_device *device, int index)
 	return device->idx[index];
 }
 
+static inline u64 ioat_chansts(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+	u64 status;
+	u32 status_lo;
+
+	/* We need to read the low address first as this causes the
+	 * chipset to latch the upper bits for the subsequent read
+	 */
+	status_lo = readl(chan->reg_base + IOAT_CHANSTS_OFFSET_LOW(ver));
+	status = readl(chan->reg_base + IOAT_CHANSTS_OFFSET_HIGH(ver));
+	status <<= 32;
+	status |= status_lo;
+
+	return status;
+}
+
+static inline void ioat_start(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+
+	writeb(IOAT_CHANCMD_START, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+}
+
+static inline u64 ioat_chansts_to_addr(u64 status)
+{
+	return status & IOAT_CHANSTS_COMPLETED_DESCRIPTOR_ADDR;
+}
+
+static inline u32 ioat_chanerr(struct ioat_chan_common *chan)
+{
+	return readl(chan->reg_base + IOAT_CHANERR_OFFSET);
+}
+
+static inline void ioat_suspend(struct ioat_chan_common *chan)
+{
+	u8 ver = chan->device->version;
+
+	writeb(IOAT_CHANCMD_SUSPEND, chan->reg_base + IOAT_CHANCMD_OFFSET(ver));
+}
+
+static inline void ioat_set_chainaddr(struct ioat_dma_chan *ioat, u64 addr)
+{
+	struct ioat_chan_common *chan = &ioat->base;
+
+	writel(addr & 0x00000000FFFFFFFF,
+	       chan->reg_base + IOAT1_CHAINADDR_OFFSET_LOW);
+	writel(addr >> 32,
+	       chan->reg_base + IOAT1_CHAINADDR_OFFSET_HIGH);
+}
+
+static inline bool is_ioat_active(unsigned long status)
+{
+	return ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_ACTIVE);
+}
+
+static inline bool is_ioat_idle(unsigned long status)
+{
+	return ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_DONE);
+}
+
+static inline bool is_ioat_halted(unsigned long status)
+{
+	return ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_HALTED);
+}
+
+static inline bool is_ioat_suspended(unsigned long status)
+{
+	return ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_SUSPENDED);
+}
+
+/* channel was fatally programmed */
+static inline bool is_ioat_bug(unsigned long err)
+{
+	return !!(err & (IOAT_CHANERR_SRC_ADDR_ERR|IOAT_CHANERR_DEST_ADDR_ERR|
+			 IOAT_CHANERR_NEXT_ADDR_ERR|IOAT_CHANERR_CONTROL_ERR|
+			 IOAT_CHANERR_LENGTH_ERR));
+}
+
 int __devinit ioat_probe(struct ioatdma_device *device);
 int __devinit ioat_register(struct ioatdma_device *device);
 int __devinit ioat1_dma_probe(struct ioatdma_device *dev, int dca);
@@ -224,8 +295,11 @@ struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
 unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx,
-		       work_func_t work_fn, void (*tasklet)(unsigned long),
-		       unsigned long tasklet_data);
+		       void (*timer_fn)(unsigned long),
+		       void (*tasklet)(unsigned long),
+		       unsigned long ioat);
 void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
 		    size_t len, struct ioat_dma_descriptor *hw);
+bool ioat_cleanup_preamble(struct ioat_chan_common *chan,
+			   unsigned long *phys_complete);
 #endif /* IOATDMA_H */

commit ad643f54c8514998333bc6c7b201fda2267496be
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:01:38 2009 -0700

    ioat1: trim ioat_dma_desc_sw
    
    Save 4 bytes per software descriptor by transmitting tx_cnt in an unused
    portion of the hardware descriptor.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index e47083b52ee7..ec851cf5345c 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -165,14 +165,12 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
  * @hw: hardware DMA descriptor
  * @node: this descriptor will either be on the free list,
  *     or attached to a transaction list (async_tx.tx_list)
- * @tx_cnt: number of descriptors required to complete the transaction
  * @txd: the generic software descriptor for all engines
  * @id: identifier for debug
  */
 struct ioat_desc_sw {
 	struct ioat_dma_descriptor *hw;
 	struct list_head node;
-	int tx_cnt;
 	size_t len;
 	struct dma_async_tx_descriptor txd;
 	#ifdef DEBUG

commit 345d852391cf3fdc73f23a9ca522c6e7b5eb5a52
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:01:30 2009 -0700

    ioat: ___devinit annotate the initialization paths
    
    Mark all single use initialization routines with __devinit.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5fd6e2de84db..e47083b52ee7 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -217,11 +217,12 @@ ioat_chan_by_index(struct ioatdma_device *device, int index)
 	return device->idx[index];
 }
 
-int ioat_probe(struct ioatdma_device *device);
-int ioat_register(struct ioatdma_device *device);
-int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
-void ioat_dma_remove(struct ioatdma_device *device);
-struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+int __devinit ioat_probe(struct ioatdma_device *device);
+int __devinit ioat_register(struct ioatdma_device *device);
+int __devinit ioat1_dma_probe(struct ioatdma_device *dev, int dca);
+void __devexit ioat_dma_remove(struct ioatdma_device *device);
+struct dca_provider * __devinit ioat_dca_init(struct pci_dev *pdev,
+					      void __iomem *iobase);
 unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
 void ioat_init_channel(struct ioatdma_device *device,
 		       struct ioat_chan_common *chan, int idx,

commit 4fb9b9e8d55880523db550043dfb204696dd0422
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:01:04 2009 -0700

    ioat: cleanup completion status reads
    
    The cleanup path makes an effort to only perform an atomic read of the
    64-bit completion address.  However in the 32-bit case it does not
    matter if we read the upper-32 and lower-32 non-atomically because the
    upper-32 will always be zero.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 9f9edc2cd079..5fd6e2de84db 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -96,14 +96,8 @@ struct ioat_chan_common {
 	struct ioatdma_device *device;
 	struct dma_chan common;
 
-	dma_addr_t completion_addr;
-	union {
-		u64 full; /* HW completion writeback */
-		struct {
-			u32 low;
-			u32 high;
-		};
-	} *completion_virt;
+	dma_addr_t completion_dma;
+	u64 *completion;
 	unsigned long last_compl_desc_addr_hw;
 	struct tasklet_struct cleanup_task;
 };

commit 6df9183a153291a2585a8dfe67597fc18c201147
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:00:55 2009 -0700

    ioat: add some dev_dbg() calls
    
    Provide some output for debugging the driver.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index fa15e77652a0..9f9edc2cd079 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -173,6 +173,7 @@ ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
  *     or attached to a transaction list (async_tx.tx_list)
  * @tx_cnt: number of descriptors required to complete the transaction
  * @txd: the generic software descriptor for all engines
+ * @id: identifier for debug
  */
 struct ioat_desc_sw {
 	struct ioat_dma_descriptor *hw;
@@ -180,8 +181,35 @@ struct ioat_desc_sw {
 	int tx_cnt;
 	size_t len;
 	struct dma_async_tx_descriptor txd;
+	#ifdef DEBUG
+	int id;
+	#endif
 };
 
+#ifdef DEBUG
+#define set_desc_id(desc, i) ((desc)->id = (i))
+#define desc_id(desc) ((desc)->id)
+#else
+#define set_desc_id(desc, i)
+#define desc_id(desc) (0)
+#endif
+
+static inline void
+__dump_desc_dbg(struct ioat_chan_common *chan, struct ioat_dma_descriptor *hw,
+		struct dma_async_tx_descriptor *tx, int id)
+{
+	struct device *dev = to_dev(chan);
+
+	dev_dbg(dev, "desc[%d]: (%#llx->%#llx) cookie: %d flags: %#x"
+		" ctl: %#x (op: %d int_en: %d compl: %d)\n", id,
+		(unsigned long long) tx->phys,
+		(unsigned long long) hw->next, tx->cookie, tx->flags,
+		hw->ctl, hw->ctl_f.op, hw->ctl_f.int_en, hw->ctl_f.compl_write);
+}
+
+#define dump_desc_dbg(c, d) \
+	({ if (d) __dump_desc_dbg(&c->base, d->hw, &d->txd, desc_id(d)); 0; })
+
 static inline void ioat_set_tcp_copy_break(unsigned long copybreak)
 {
 	#ifdef CONFIG_NET_DMA

commit 38e12f64a165e83617c21dae3c15972fd8d639f5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 12:00:46 2009 -0700

    ioat1: kill unused unmap parameters
    
    The unified ioat1/ioat2 ioat_dma_unmap() implementation derives the
    source and dest addresses from the unmap descriptor.  There is no longer
    a need to track this information in struct ioat_desc_sw.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 84065dfa4d40..fa15e77652a0 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -179,8 +179,6 @@ struct ioat_desc_sw {
 	struct list_head node;
 	int tx_cnt;
 	size_t len;
-	dma_addr_t src;
-	dma_addr_t dst;
 	struct dma_async_tx_descriptor txd;
 };
 

commit 5cbafa65b92ee4f5b8ba915cddf94b91f186b989
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Aug 26 13:01:44 2009 -0700

    ioat2,3: convert to a true ring buffer
    
    Replace the current linked list munged into a ring with a native ring
    buffer implementation.  The benefit of this approach is reduced overhead
    as many parameters can be derived from ring position with simple pointer
    comparisons and descriptor allocation/freeing becomes just a
    manipulation of head/tail pointers.
    
    It requires a contiguous allocation for the software descriptor
    information.
    
    Since this arrangement is significantly different from the ioat1 chain,
    move ioat2,3 support into its own file and header.  Common routines are
    exported from driver/dma/ioat/dma.[ch].
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5b31db73ad8e..84065dfa4d40 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -62,6 +62,7 @@
  * @idx: per channel data
  * @dca: direct cache access context
  * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
+ * @enumerate_channels: hw version specific channel enumeration
  */
 
 struct ioatdma_device {
@@ -76,6 +77,7 @@ struct ioatdma_device {
 	struct ioat_chan_common *idx[4];
 	struct dca_provider *dca;
 	void (*intr_quirk)(struct ioatdma_device *device);
+	int (*enumerate_channels)(struct ioatdma_device *device);
 };
 
 struct ioat_chan_common {
@@ -106,6 +108,7 @@ struct ioat_chan_common {
 	struct tasklet_struct cleanup_task;
 };
 
+
 /**
  * struct ioat_dma_chan - internal representation of a DMA channel
  */
@@ -119,7 +122,6 @@ struct ioat_dma_chan {
 	struct list_head used_desc;
 
 	int pending;
-	u16 dmacount;
 	u16 desccount;
 };
 
@@ -135,6 +137,33 @@ static inline struct ioat_dma_chan *to_ioat_chan(struct dma_chan *c)
 	return container_of(chan, struct ioat_dma_chan, base);
 }
 
+/**
+ * ioat_is_complete - poll the status of an ioat transaction
+ * @c: channel handle
+ * @cookie: transaction identifier
+ * @done: if set, updated with last completed transaction
+ * @used: if set, updated with last used transaction
+ */
+static inline enum dma_status
+ioat_is_complete(struct dma_chan *c, dma_cookie_t cookie,
+		 dma_cookie_t *done, dma_cookie_t *used)
+{
+	struct ioat_chan_common *chan = to_chan_common(c);
+	dma_cookie_t last_used;
+	dma_cookie_t last_complete;
+
+	last_used = c->cookie;
+	last_complete = chan->completed_cookie;
+	chan->watchdog_tcp_cookie = cookie;
+
+	if (done)
+		*done = last_complete;
+	if (used)
+		*used = last_used;
+
+	return dma_async_is_complete(cookie, last_complete, last_used);
+}
+
 /* wrapper around hardware descriptor format + additional software fields */
 
 /**
@@ -162,11 +191,22 @@ static inline void ioat_set_tcp_copy_break(unsigned long copybreak)
 	#endif
 }
 
+static inline struct ioat_chan_common *
+ioat_chan_by_index(struct ioatdma_device *device, int index)
+{
+	return device->idx[index];
+}
+
+int ioat_probe(struct ioatdma_device *device);
+int ioat_register(struct ioatdma_device *device);
 int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
-int ioat2_dma_probe(struct ioatdma_device *dev, int dca);
-int ioat3_dma_probe(struct ioatdma_device *dev, int dca);
 void ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-struct dca_provider *ioat2_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+unsigned long ioat_get_current_completion(struct ioat_chan_common *chan);
+void ioat_init_channel(struct ioatdma_device *device,
+		       struct ioat_chan_common *chan, int idx,
+		       work_func_t work_fn, void (*tasklet)(unsigned long),
+		       unsigned long tasklet_data);
+void ioat_dma_unmap(struct ioat_chan_common *chan, enum dma_ctrl_flags flags,
+		    size_t len, struct ioat_dma_descriptor *hw);
 #endif /* IOATDMA_H */

commit dcbc853af6f0c056088e4df0794d9bf36184809e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:44:50 2009 -0700

    ioat: prepare the code for ioat[12]_dma_chan split
    
    Prepare the code for the conversion of the ioat2 linked-list-ring into a
    native ring buffer.  After this conversion ioat2 channels will share
    less of the ioat1 infrastructure, but there will still be places where
    sharing is possible.  struct ioat_chan_common is created to house the
    channel attributes that will remain common between ioat1 and ioat2
    channels.
    
    For every routine that accesses both common and hardware specific fields
    the old unified 'ioat_chan' pointer is split into an 'ioat' and  'chan'
    pointer.  Where 'chan' references common fields and 'ioat' the
    hardware/version specific.
    
    [ Impact: pure structure member movement/variable renames, no logic changes ]
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 9f0c853b6a77..5b31db73ad8e 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -35,7 +35,6 @@
 #define IOAT_DMA_DCA_ANY_CPU		~0
 #define IOAT_WATCHDOG_PERIOD		(2 * HZ)
 
-#define to_ioat_chan(chan) container_of(chan, struct ioat_dma_chan, common)
 #define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
 #define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
 #define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, txd)
@@ -74,37 +73,24 @@ struct ioatdma_device {
 	u8 version;
 	struct delayed_work work;
 	struct msix_entry msix_entries[4];
-	struct ioat_dma_chan *idx[4];
+	struct ioat_chan_common *idx[4];
 	struct dca_provider *dca;
 	void (*intr_quirk)(struct ioatdma_device *device);
 };
 
-/**
- * struct ioat_dma_chan - internal representation of a DMA channel
- */
-struct ioat_dma_chan {
-
+struct ioat_chan_common {
 	void __iomem *reg_base;
 
-	dma_cookie_t completed_cookie;
 	unsigned long last_completion;
 	unsigned long last_completion_time;
 
-	size_t xfercap;	/* XFERCAP register value expanded out */
-
 	spinlock_t cleanup_lock;
-	spinlock_t desc_lock;
-	struct list_head free_desc;
-	struct list_head used_desc;
+	dma_cookie_t completed_cookie;
 	unsigned long watchdog_completion;
 	int watchdog_tcp_cookie;
 	u32 watchdog_last_tcp_cookie;
 	struct delayed_work work;
 
-	int pending;
-	u16 dmacount;
-	u16 desccount;
-
 	struct ioatdma_device *device;
 	struct dma_chan common;
 
@@ -120,6 +106,35 @@ struct ioat_dma_chan {
 	struct tasklet_struct cleanup_task;
 };
 
+/**
+ * struct ioat_dma_chan - internal representation of a DMA channel
+ */
+struct ioat_dma_chan {
+	struct ioat_chan_common base;
+
+	size_t xfercap;	/* XFERCAP register value expanded out */
+
+	spinlock_t desc_lock;
+	struct list_head free_desc;
+	struct list_head used_desc;
+
+	int pending;
+	u16 dmacount;
+	u16 desccount;
+};
+
+static inline struct ioat_chan_common *to_chan_common(struct dma_chan *c)
+{
+	return container_of(c, struct ioat_chan_common, common);
+}
+
+static inline struct ioat_dma_chan *to_ioat_chan(struct dma_chan *c)
+{
+	struct ioat_chan_common *chan = to_chan_common(c);
+
+	return container_of(chan, struct ioat_dma_chan, base);
+}
+
 /* wrapper around hardware descriptor format + additional software fields */
 
 /**

commit 77867fff033ea549096c49d863c564ad7d8be36f
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:44:04 2009 -0700

    ioat: fix type mismatch for ->dmacount
    
    ->dmacount tracks the sequence number of active descriptors.  It is
    written to the DMACOUNT register to update the channel's view of pending
    descriptors in the chain.  The register is 16-bits so ->dmacount should
    be unsigned and 16-bit as well.  Also modify ->desccount to maintain
    alignment.
    
    This was never a problem in practice because we never compared dmacount
    values, but this is a bug waiting to happen.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 1226e35f2709..9f0c853b6a77 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -102,8 +102,8 @@ struct ioat_dma_chan {
 	struct delayed_work work;
 
 	int pending;
-	int dmacount;
-	int desccount;
+	u16 dmacount;
+	u16 desccount;
 
 	struct ioatdma_device *device;
 	struct dma_chan common;

commit f2427e276ffec5ce599c6bc116e0927269a360ef
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:42:38 2009 -0700

    ioat: split ioat_dma_probe into core/version-specific routines
    
    Towards the removal of ioatdma_device.version split the initialization
    path into distinct versions.  This conversion:
    1/ moves version specific probe code to version specific routines
    2/ removes the need for ioat_device
    3/ turns off the ioat1 msi quirk if the device is reinitialized for intx
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 6e27ddb1e98a..1226e35f2709 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -61,6 +61,8 @@
  * @version: version of ioatdma device
  * @msix_entries: irq handlers
  * @idx: per channel data
+ * @dca: direct cache access context
+ * @intr_quirk: interrupt setup quirk (for ioat_v1 devices)
  */
 
 struct ioatdma_device {
@@ -73,6 +75,8 @@ struct ioatdma_device {
 	struct delayed_work work;
 	struct msix_entry msix_entries[4];
 	struct ioat_dma_chan *idx[4];
+	struct dca_provider *dca;
+	void (*intr_quirk)(struct ioatdma_device *device);
 };
 
 /**
@@ -136,25 +140,16 @@ struct ioat_desc_sw {
 	struct dma_async_tx_descriptor txd;
 };
 
-static inline void ioat_set_tcp_copy_break(struct ioatdma_device *dev)
+static inline void ioat_set_tcp_copy_break(unsigned long copybreak)
 {
 	#ifdef CONFIG_NET_DMA
-	switch (dev->version) {
-	case IOAT_VER_1_2:
-		sysctl_tcp_dma_copybreak = 4096;
-		break;
-	case IOAT_VER_2_0:
-		sysctl_tcp_dma_copybreak = 2048;
-		break;
-	case IOAT_VER_3_0:
-		sysctl_tcp_dma_copybreak = 262144;
-		break;
-	}
+	sysctl_tcp_dma_copybreak = copybreak;
 	#endif
 }
 
-struct ioatdma_device *ioat_dma_probe(struct pci_dev *pdev,
-				      void __iomem *iobase);
+int ioat1_dma_probe(struct ioatdma_device *dev, int dca);
+int ioat2_dma_probe(struct ioatdma_device *dev, int dca);
+int ioat3_dma_probe(struct ioatdma_device *dev, int dca);
 void ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 struct dca_provider *ioat2_dca_init(struct pci_dev *pdev, void __iomem *iobase);

commit b31b78f1ab7806759622b703357e39a21f757281
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:42:32 2009 -0700

    ioat: kill function prototype ifdef guards
    
    The only .c files that utilize these protected prototypes depend on
    CONFIG_INTEL_IOATDMA=y, so there is no value gained in providing empty
    prototypes.
    
    [ Impact: pure cleanup ]
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index c5eabae4c1b9..6e27ddb1e98a 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -153,19 +153,10 @@ static inline void ioat_set_tcp_copy_break(struct ioatdma_device *dev)
 	#endif
 }
 
-#if defined(CONFIG_INTEL_IOATDMA) || defined(CONFIG_INTEL_IOATDMA_MODULE)
 struct ioatdma_device *ioat_dma_probe(struct pci_dev *pdev,
 				      void __iomem *iobase);
 void ioat_dma_remove(struct ioatdma_device *device);
 struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 struct dca_provider *ioat2_dca_init(struct pci_dev *pdev, void __iomem *iobase);
 struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
-#else
-#define ioat_dma_probe(pdev, iobase)    NULL
-#define ioat_dma_remove(device)         do { } while (0)
-#define ioat_dca_init(pdev, iobase)	NULL
-#define ioat2_dca_init(pdev, iobase)	NULL
-#define ioat3_dca_init(pdev, iobase)	NULL
-#endif
-
 #endif /* IOATDMA_H */

commit bc3c70258526a635325f1f15138a96297879bc1a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:33:42 2009 -0700

    ioat: cleanup some long deref chains and 80 column collisions
    
    * reduce device->common. to dma-> in ioat_dma_{probe,remove,selftest}
    * ioat_lookup_chan_by_index to ioat_chan_by_index
    * multi-line function definitions
    * ioat_desc_sw.async_tx to ioat_desc_sw.txd
    * desc->txd. to tx-> in cleanup routine
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 5e8d7cfabc21..c5eabae4c1b9 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -38,7 +38,8 @@
 #define to_ioat_chan(chan) container_of(chan, struct ioat_dma_chan, common)
 #define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
 #define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
-#define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, async_tx)
+#define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, txd)
+#define to_dev(ioat_chan) (&(ioat_chan)->device->pdev->dev)
 
 #define chan_num(ch) ((int)((ch)->reg_base - (ch)->device->reg_base) / 0x80)
 
@@ -123,7 +124,7 @@ struct ioat_dma_chan {
  * @node: this descriptor will either be on the free list,
  *     or attached to a transaction list (async_tx.tx_list)
  * @tx_cnt: number of descriptors required to complete the transaction
- * @async_tx: the generic software descriptor for all engines
+ * @txd: the generic software descriptor for all engines
  */
 struct ioat_desc_sw {
 	struct ioat_dma_descriptor *hw;
@@ -132,7 +133,7 @@ struct ioat_desc_sw {
 	size_t len;
 	dma_addr_t src;
 	dma_addr_t dst;
-	struct dma_async_tx_descriptor async_tx;
+	struct dma_async_tx_descriptor txd;
 };
 
 static inline void ioat_set_tcp_copy_break(struct ioatdma_device *dev)

commit e6c0b69a43150c1a37cf342ce5faedf12583bf79
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:29:44 2009 -0700

    ioat: convert ioat_probe to pcim/devm
    
    The driver currently duplicates much of what these routines offer, so
    just use the common code.  For example ->irq_mode tracks what interrupt
    mode was initialized, which duplicates the ->msix_enabled and
    ->msi_enabled handling in pcim_release.
    
    This also adds a check to the return value of dma_async_device_register,
    which can fail.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index ccb400f5e279..5e8d7cfabc21 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -31,14 +31,6 @@
 
 #define IOAT_DMA_VERSION  "3.64"
 
-enum ioat_interrupt {
-	none = 0,
-	msix_multi_vector = 1,
-	msix_single_vector = 2,
-	msi = 3,
-	intx = 4,
-};
-
 #define IOAT_LOW_COMPLETION_MASK	0xffffffc0
 #define IOAT_DMA_DCA_ANY_CPU		~0
 #define IOAT_WATCHDOG_PERIOD		(2 * HZ)
@@ -59,7 +51,6 @@ enum ioat_interrupt {
  */
 #define NULL_DESC_BUFFER_SIZE 1
 
-
 /**
  * struct ioatdma_device - internal representation of a IOAT device
  * @pdev: PCI-Express device
@@ -67,7 +58,6 @@ enum ioat_interrupt {
  * @dma_pool: for allocating DMA descriptors
  * @common: embedded struct dma_device
  * @version: version of ioatdma device
- * @irq_mode: which style irq to use
  * @msix_entries: irq handlers
  * @idx: per channel data
  */
@@ -79,7 +69,6 @@ struct ioatdma_device {
 	struct pci_pool *completion_pool;
 	struct dma_device common;
 	u8 version;
-	enum ioat_interrupt irq_mode;
 	struct delayed_work work;
 	struct msix_entry msix_entries[4];
 	struct ioat_dma_chan *idx[4];

commit 1f27adc2f050836c12deb4d99afe507636537a0b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:29:02 2009 -0700

    ioat: move definitions to dma.h
    
    Some of these defines may be useful outside of dma.c and the header is
    private so there are no namespace pollution concerns.
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index e80e787fe64f..ccb400f5e279 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -43,6 +43,22 @@ enum ioat_interrupt {
 #define IOAT_DMA_DCA_ANY_CPU		~0
 #define IOAT_WATCHDOG_PERIOD		(2 * HZ)
 
+#define to_ioat_chan(chan) container_of(chan, struct ioat_dma_chan, common)
+#define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, common)
+#define to_ioat_desc(lh) container_of(lh, struct ioat_desc_sw, node)
+#define tx_to_ioat_desc(tx) container_of(tx, struct ioat_desc_sw, async_tx)
+
+#define chan_num(ch) ((int)((ch)->reg_base - (ch)->device->reg_base) / 0x80)
+
+#define RESET_DELAY  msecs_to_jiffies(100)
+#define WATCHDOG_DELAY  round_jiffies(msecs_to_jiffies(2000))
+
+/*
+ * workaround for IOAT ver.3.0 null descriptor issue
+ * (channel returns error when size is 0)
+ */
+#define NULL_DESC_BUFFER_SIZE 1
+
 
 /**
  * struct ioatdma_device - internal representation of a IOAT device

commit 584ec22759c06cdfc189c03a727f20038526245b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 28 14:32:12 2009 -0700

    ioat: move to drivers/dma/ioat/
    
    When first created the ioat driver was the only inhabitant of
    drivers/dma/.  Now, it is the only multi-file (more than a .c and a .h)
    driver in the directory.  Moving it to an ioat/ subdirectory allows the
    naming convention to be cleaned up, and allows for future splitting of
    the source files by hardware version (v1, v2, and v3).
    
    Signed-off-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
new file mode 100644
index 000000000000..e80e787fe64f
--- /dev/null
+++ b/drivers/dma/ioat/dma.h
@@ -0,0 +1,165 @@
+/*
+ * Copyright(c) 2004 - 2009 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; either version 2 of the License, or (at your option)
+ * any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+ *
+ * The full GNU General Public License is included in this distribution in the
+ * file called COPYING.
+ */
+#ifndef IOATDMA_H
+#define IOATDMA_H
+
+#include <linux/dmaengine.h>
+#include "hw.h"
+#include <linux/init.h>
+#include <linux/dmapool.h>
+#include <linux/cache.h>
+#include <linux/pci_ids.h>
+#include <net/tcp.h>
+
+#define IOAT_DMA_VERSION  "3.64"
+
+enum ioat_interrupt {
+	none = 0,
+	msix_multi_vector = 1,
+	msix_single_vector = 2,
+	msi = 3,
+	intx = 4,
+};
+
+#define IOAT_LOW_COMPLETION_MASK	0xffffffc0
+#define IOAT_DMA_DCA_ANY_CPU		~0
+#define IOAT_WATCHDOG_PERIOD		(2 * HZ)
+
+
+/**
+ * struct ioatdma_device - internal representation of a IOAT device
+ * @pdev: PCI-Express device
+ * @reg_base: MMIO register space base address
+ * @dma_pool: for allocating DMA descriptors
+ * @common: embedded struct dma_device
+ * @version: version of ioatdma device
+ * @irq_mode: which style irq to use
+ * @msix_entries: irq handlers
+ * @idx: per channel data
+ */
+
+struct ioatdma_device {
+	struct pci_dev *pdev;
+	void __iomem *reg_base;
+	struct pci_pool *dma_pool;
+	struct pci_pool *completion_pool;
+	struct dma_device common;
+	u8 version;
+	enum ioat_interrupt irq_mode;
+	struct delayed_work work;
+	struct msix_entry msix_entries[4];
+	struct ioat_dma_chan *idx[4];
+};
+
+/**
+ * struct ioat_dma_chan - internal representation of a DMA channel
+ */
+struct ioat_dma_chan {
+
+	void __iomem *reg_base;
+
+	dma_cookie_t completed_cookie;
+	unsigned long last_completion;
+	unsigned long last_completion_time;
+
+	size_t xfercap;	/* XFERCAP register value expanded out */
+
+	spinlock_t cleanup_lock;
+	spinlock_t desc_lock;
+	struct list_head free_desc;
+	struct list_head used_desc;
+	unsigned long watchdog_completion;
+	int watchdog_tcp_cookie;
+	u32 watchdog_last_tcp_cookie;
+	struct delayed_work work;
+
+	int pending;
+	int dmacount;
+	int desccount;
+
+	struct ioatdma_device *device;
+	struct dma_chan common;
+
+	dma_addr_t completion_addr;
+	union {
+		u64 full; /* HW completion writeback */
+		struct {
+			u32 low;
+			u32 high;
+		};
+	} *completion_virt;
+	unsigned long last_compl_desc_addr_hw;
+	struct tasklet_struct cleanup_task;
+};
+
+/* wrapper around hardware descriptor format + additional software fields */
+
+/**
+ * struct ioat_desc_sw - wrapper around hardware descriptor
+ * @hw: hardware DMA descriptor
+ * @node: this descriptor will either be on the free list,
+ *     or attached to a transaction list (async_tx.tx_list)
+ * @tx_cnt: number of descriptors required to complete the transaction
+ * @async_tx: the generic software descriptor for all engines
+ */
+struct ioat_desc_sw {
+	struct ioat_dma_descriptor *hw;
+	struct list_head node;
+	int tx_cnt;
+	size_t len;
+	dma_addr_t src;
+	dma_addr_t dst;
+	struct dma_async_tx_descriptor async_tx;
+};
+
+static inline void ioat_set_tcp_copy_break(struct ioatdma_device *dev)
+{
+	#ifdef CONFIG_NET_DMA
+	switch (dev->version) {
+	case IOAT_VER_1_2:
+		sysctl_tcp_dma_copybreak = 4096;
+		break;
+	case IOAT_VER_2_0:
+		sysctl_tcp_dma_copybreak = 2048;
+		break;
+	case IOAT_VER_3_0:
+		sysctl_tcp_dma_copybreak = 262144;
+		break;
+	}
+	#endif
+}
+
+#if defined(CONFIG_INTEL_IOATDMA) || defined(CONFIG_INTEL_IOATDMA_MODULE)
+struct ioatdma_device *ioat_dma_probe(struct pci_dev *pdev,
+				      void __iomem *iobase);
+void ioat_dma_remove(struct ioatdma_device *device);
+struct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+struct dca_provider *ioat2_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+struct dca_provider *ioat3_dca_init(struct pci_dev *pdev, void __iomem *iobase);
+#else
+#define ioat_dma_probe(pdev, iobase)    NULL
+#define ioat_dma_remove(device)         do { } while (0)
+#define ioat_dca_init(pdev, iobase)	NULL
+#define ioat2_dca_init(pdev, iobase)	NULL
+#define ioat3_dca_init(pdev, iobase)	NULL
+#endif
+
+#endif /* IOATDMA_H */
