commit 6c8991f41546c3c472503dff1ea9daaddf9331c2
Author: Sabrina Dubroca <sd@queasysnail.net>
Date:   Wed Dec 4 15:35:53 2019 +0100

    net: ipv6_stub: use ip6_dst_lookup_flow instead of ip6_dst_lookup
    
    ipv6_stub uses the ip6_dst_lookup function to allow other modules to
    perform IPv6 lookups. However, this function skips the XFRM layer
    entirely.
    
    All users of ipv6_stub->ip6_dst_lookup use ip_route_output_flow (via the
    ip_route_output_key and ip_route_output helpers) for their IPv4 lookups,
    which calls xfrm_lookup_route(). This patch fixes this inconsistent
    behavior by switching the stub to ip6_dst_lookup_flow, which also calls
    xfrm_lookup_route().
    
    This requires some changes in all the callers, as these two functions
    take different arguments and have different return types.
    
    Fixes: 5f81bd2e5d80 ("ipv6: export a stub for IPv6 symbols used by vxlan")
    Reported-by: Xiumei Mu <xmu@redhat.com>
    Signed-off-by: Sabrina Dubroca <sd@queasysnail.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 5a3474f9351b..312c2fc961c0 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -117,10 +117,12 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 	memcpy(&fl6.daddr, daddr, sizeof(*daddr));
 	fl6.flowi6_proto = IPPROTO_UDP;
 
-	if (unlikely(ipv6_stub->ipv6_dst_lookup(sock_net(recv_sockets.sk6->sk),
-						recv_sockets.sk6->sk, &ndst, &fl6))) {
+	ndst = ipv6_stub->ipv6_dst_lookup_flow(sock_net(recv_sockets.sk6->sk),
+					       recv_sockets.sk6->sk, &fl6,
+					       NULL);
+	if (unlikely(IS_ERR(ndst))) {
 		pr_err_ratelimited("no route to %pI6\n", daddr);
-		goto put;
+		return NULL;
 	}
 
 	if (unlikely(ndst->error)) {

commit dab2175800ef0b45d5e70a7337bd6cb311292ebe
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu May 2 10:48:05 2019 +0300

    RDMA/rxe: Use rdma_read_gid_attr_ndev_rcu to access netdev
    
    Use rdma_read_gid_attr_ndev_rcu() to access netdevice attached to GID
    entry under rcu lock.
    
    This ensures that while working on the netdevice of the GID, it doesn't
    get freed.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index c44139788afc..5a3474f9351b 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -458,7 +458,7 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 				int paylen, struct rxe_pkt_info *pkt)
 {
 	unsigned int hdr_len;
-	struct sk_buff *skb;
+	struct sk_buff *skb = NULL;
 	struct net_device *ndev;
 	const struct ib_gid_attr *attr;
 	const int port_num = 1;
@@ -466,7 +466,6 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 	attr = rdma_get_gid_attr(&rxe->ib_dev, port_num, av->grh.sgid_index);
 	if (IS_ERR(attr))
 		return NULL;
-	ndev = attr->ndev;
 
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
@@ -475,16 +474,26 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
 			sizeof(struct ipv6hdr);
 
+	rcu_read_lock();
+	ndev = rdma_read_gid_attr_ndev_rcu(attr);
+	if (IS_ERR(ndev)) {
+		rcu_read_unlock();
+		goto out;
+	}
 	skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(ndev),
 			GFP_ATOMIC);
 
-	if (unlikely(!skb))
+	if (unlikely(!skb)) {
+		rcu_read_unlock();
 		goto out;
+	}
 
 	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(ndev));
 
 	/* FIXME: hold reference to this netdev until life of this skb. */
 	skb->dev	= ndev;
+	rcu_read_unlock();
+
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		skb->protocol = htons(ETH_P_IP);
 	else

commit 3bf3e2b881c1412d0329ce9376dfe1518489b8fc
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu May 2 10:48:01 2019 +0300

    RDMA/rxe: Consider skb reserve space based on netdev of GID
    
    Always consider the skb reserve space based on netdevice of the GID
    attribute, regardless of vlan or non vlan netdevice.
    
    Fixes: 43c9fc509fa5 ("rdma_rxe: make rxe work over 802.1q VLAN devices")
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index f186b92ba45b..c44139788afc 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -481,8 +481,9 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 	if (unlikely(!skb))
 		goto out;
 
-	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(rxe->ndev));
+	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(ndev));
 
+	/* FIXME: hold reference to this netdev until life of this skb. */
 	skb->dev	= ndev;
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		skb->protocol = htons(ETH_P_IP);

commit 08304d714638dff08bf001e92ce56411e2e902c7
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Wed Mar 27 05:50:47 2019 -0400

    IB/rxe: Replace av->network_type with skb->protocol
    
    In the function rxe_init_packet, based on av->network_type, skb->protocol
    is set to ipv4 or ipv6. The functions rxe_prepare and rxe_send are called
    after the functin rxe_init_packet.  So in these functions,
    av->network_type can be replaced with skb->protocol.
    
    The functions are in the xmit fast path. So with skb->protocol, the
    performance will be better.
    
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 753cabcd441c..f186b92ba45b 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -338,13 +338,13 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	ip6h->payload_len = htons(skb->len - sizeof(*ip6h));
 }
 
-static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb,
-		    struct rxe_av *av)
+static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	struct rxe_qp *qp = pkt->qp;
 	struct dst_entry *dst;
 	bool xnet = false;
 	__be16 df = htons(IP_DF);
+	struct rxe_av *av = rxe_get_av(pkt);
 	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
 
@@ -364,11 +364,11 @@ static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb,
 	return 0;
 }
 
-static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb,
-		    struct rxe_av *av)
+static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	struct rxe_qp *qp = pkt->qp;
 	struct dst_entry *dst;
+	struct rxe_av *av = rxe_get_av(pkt);
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
 
@@ -392,16 +392,15 @@ static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb,
 int rxe_prepare(struct rxe_pkt_info *pkt, struct sk_buff *skb, u32 *crc)
 {
 	int err = 0;
-	struct rxe_av *av = rxe_get_av(pkt);
 
-	if (av->network_type == RDMA_NETWORK_IPV4)
-		err = prepare4(pkt, skb, av);
-	else if (av->network_type == RDMA_NETWORK_IPV6)
-		err = prepare6(pkt, skb, av);
+	if (skb->protocol == htons(ETH_P_IP))
+		err = prepare4(pkt, skb);
+	else if (skb->protocol == htons(ETH_P_IPV6))
+		err = prepare6(pkt, skb);
 
 	*crc = rxe_icrc_hdr(pkt, skb);
 
-	if (ether_addr_equal(skb->dev->dev_addr, av->dmac))
+	if (ether_addr_equal(skb->dev->dev_addr, rxe_get_av(pkt)->dmac))
 		pkt->mask |= RXE_LOOPBACK_MASK;
 
 	return err;
@@ -422,23 +421,20 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 
 int rxe_send(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
-	struct rxe_av *av;
 	int err;
 
-	av = rxe_get_av(pkt);
-
 	skb->destructor = rxe_skb_tx_dtor;
 	skb->sk = pkt->qp->sk->sk;
 
 	rxe_add_ref(pkt->qp);
 	atomic_inc(&pkt->qp->skb_out);
 
-	if (av->network_type == RDMA_NETWORK_IPV4) {
+	if (skb->protocol == htons(ETH_P_IP)) {
 		err = ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
-	} else if (av->network_type == RDMA_NETWORK_IPV6) {
+	} else if (skb->protocol == htons(ETH_P_IPV6)) {
 		err = ip6_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
 	} else {
-		pr_err("Unknown layer 3 protocol: %d\n", av->network_type);
+		pr_err("Unknown layer 3 protocol: %d\n", skb->protocol);
 		atomic_dec(&pkt->qp->skb_out);
 		rxe_drop_ref(pkt->qp);
 		kfree_skb(skb);

commit 66920e1b25861d5bd99192489f6da7183529285e
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Fri Feb 15 11:03:57 2019 -0800

    rdma_rxe: Use netlink messages to add/delete links
    
    Add support for the RDMA_NLDEV_CMD_NEWLINK/DELLINK messages which allow
    dynamically adding new RXE links.  Deprecate the old module options for
    now.
    
    Cc: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yanjun Zhu <yanjun.zhu@oracle.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index fb792f5bc0b7..753cabcd441c 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -517,7 +517,7 @@ enum rdma_link_layer rxe_link_layer(struct rxe_dev *rxe, unsigned int port_num)
 	return IB_LINK_LAYER_ETHERNET;
 }
 
-int rxe_net_add(struct net_device *ndev)
+int rxe_net_add(const char *ibdev_name, struct net_device *ndev)
 {
 	int err;
 	struct rxe_dev *rxe = NULL;
@@ -528,7 +528,7 @@ int rxe_net_add(struct net_device *ndev)
 
 	rxe->ndev = ndev;
 
-	err = rxe_add(rxe, ndev->mtu);
+	err = rxe_add(rxe, ndev->mtu, ibdev_name);
 	if (err) {
 		ib_dealloc_device(&rxe->ib_dev);
 		return err;

commit ca22354b140853b8155692d5b2bc0110aa54e937
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:56 2019 -0700

    RDMA/rxe: Close a race after ib_register_device
    
    Since rxe allows unregistration from other threads the rxe pointer can
    become invalid any moment after ib_register_driver returns. This could
    cause a user triggered use after free.
    
    Add another driver callback to be called right after the device becomes
    registered to complete any device setup required post-registration.  This
    callback has enough core locking to prevent the device from becoming
    unregistered.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index d6dfbcf6a47e..fb792f5bc0b7 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -517,24 +517,24 @@ enum rdma_link_layer rxe_link_layer(struct rxe_dev *rxe, unsigned int port_num)
 	return IB_LINK_LAYER_ETHERNET;
 }
 
-struct rxe_dev *rxe_net_add(struct net_device *ndev)
+int rxe_net_add(struct net_device *ndev)
 {
 	int err;
 	struct rxe_dev *rxe = NULL;
 
 	rxe = ib_alloc_device(rxe_dev, ib_dev);
 	if (!rxe)
-		return NULL;
+		return -ENOMEM;
 
 	rxe->ndev = ndev;
 
 	err = rxe_add(rxe, ndev->mtu);
 	if (err) {
 		ib_dealloc_device(&rxe->ib_dev);
-		return NULL;
+		return err;
 	}
 
-	return rxe;
+	return 0;
 }
 
 static void rxe_port_event(struct rxe_dev *rxe,

commit 6cc2c8e535ec19153714cee62e11e4d9ac2ea953
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:55 2019 -0700

    RDMA/rxe: Add ib_device_get_by_name() and use it in rxe
    
    rxe has an open coded version of this that is not as safe as the core
    version. This lets us eliminate the internal device list entirely from
    rxe.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index d56a967ff90b..d6dfbcf6a47e 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -45,25 +45,6 @@
 #include "rxe_net.h"
 #include "rxe_loc.h"
 
-static LIST_HEAD(rxe_dev_list);
-static DEFINE_SPINLOCK(dev_list_lock); /* spinlock for device list */
-
-struct rxe_dev *get_rxe_by_name(const char *name)
-{
-	struct rxe_dev *rxe;
-	struct rxe_dev *found = NULL;
-
-	spin_lock_bh(&dev_list_lock);
-	list_for_each_entry(rxe, &rxe_dev_list, list) {
-		if (!strcmp(name, dev_name(&rxe->ib_dev.dev))) {
-			found = rxe;
-			break;
-		}
-	}
-	spin_unlock_bh(&dev_list_lock);
-	return found;
-}
-
 static struct rxe_recv_sockets recv_sockets;
 
 struct device *rxe_dma_device(struct rxe_dev *rxe)
@@ -553,9 +534,6 @@ struct rxe_dev *rxe_net_add(struct net_device *ndev)
 		return NULL;
 	}
 
-	spin_lock_bh(&dev_list_lock);
-	list_add_tail(&rxe->list, &rxe_dev_list);
-	spin_unlock_bh(&dev_list_lock);
 	return rxe;
 }
 

commit c367074b6c37c2d2d45cb60a15b829eefbcee0f1
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jan 22 16:27:24 2019 -0700

    RDMA/rxe: Use driver_unregister and new unregistration API
    
    rxe does not have correct locking for its registration/unregistration
    paths, use the core code to handle it instead. In this mode
    ib_unregister_device will also do the dealloc, so rxe is required to do
    clean up from a callback.
    
    The core code ensures that unregistration is done only once, and generally
    takes care of locking and concurrency problems for rxe.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 56878453f1ae..d56a967ff90b 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -559,21 +559,6 @@ struct rxe_dev *rxe_net_add(struct net_device *ndev)
 	return rxe;
 }
 
-void rxe_remove_all(void)
-{
-	spin_lock_bh(&dev_list_lock);
-	while (!list_empty(&rxe_dev_list)) {
-		struct rxe_dev *rxe =
-			list_first_entry(&rxe_dev_list, struct rxe_dev, list);
-
-		list_del(&rxe->list);
-		spin_unlock_bh(&dev_list_lock);
-		rxe_remove(rxe);
-		spin_lock_bh(&dev_list_lock);
-	}
-	spin_unlock_bh(&dev_list_lock);
-}
-
 static void rxe_port_event(struct rxe_dev *rxe,
 			   enum ib_event_type event)
 {
@@ -631,10 +616,8 @@ static int rxe_notify(struct notifier_block *not_blk,
 
 	switch (event) {
 	case NETDEV_UNREGISTER:
-		list_del(&rxe->list);
-		ib_device_put(&rxe->ib_dev);
-		rxe_remove(rxe);
-		return NOTIFY_OK;
+		ib_unregister_device_queued(&rxe->ib_dev);
+		break;
 	case NETDEV_UP:
 		rxe_port_up(rxe);
 		break;

commit 4c173f596b3ffe6b967f5818043665c565648809
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:52 2019 -0700

    RDMA/rxe: Use ib_device_get_by_netdev() instead of open coding
    
    The core API handles the locking correctly and is faster if there are
    multiple devices.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 3b162e92e8e8..56878453f1ae 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -48,23 +48,6 @@
 static LIST_HEAD(rxe_dev_list);
 static DEFINE_SPINLOCK(dev_list_lock); /* spinlock for device list */
 
-struct rxe_dev *net_to_rxe(struct net_device *ndev)
-{
-	struct rxe_dev *rxe;
-	struct rxe_dev *found = NULL;
-
-	spin_lock_bh(&dev_list_lock);
-	list_for_each_entry(rxe, &rxe_dev_list, list) {
-		if (rxe->ndev == ndev) {
-			found = rxe;
-			break;
-		}
-	}
-	spin_unlock_bh(&dev_list_lock);
-
-	return found;
-}
-
 struct rxe_dev *get_rxe_by_name(const char *name)
 {
 	struct rxe_dev *rxe;
@@ -81,7 +64,6 @@ struct rxe_dev *get_rxe_by_name(const char *name)
 	return found;
 }
 
-
 static struct rxe_recv_sockets recv_sockets;
 
 struct device *rxe_dma_device(struct rxe_dev *rxe)
@@ -229,18 +211,19 @@ static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 	struct udphdr *udph;
 	struct net_device *ndev = skb->dev;
 	struct net_device *rdev = ndev;
-	struct rxe_dev *rxe = net_to_rxe(ndev);
+	struct rxe_dev *rxe = rxe_get_dev_from_net(ndev);
 	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
 
 	if (!rxe && is_vlan_dev(rdev)) {
 		rdev = vlan_dev_real_dev(ndev);
-		rxe = net_to_rxe(rdev);
+		rxe = rxe_get_dev_from_net(rdev);
 	}
 	if (!rxe)
 		goto drop;
 
 	if (skb_linearize(skb)) {
 		pr_err("skb_linearize failed\n");
+		ib_device_put(&rxe->ib_dev);
 		goto drop;
 	}
 
@@ -253,6 +236,12 @@ static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 
 	rxe_rcv(skb);
 
+	/*
+	 * FIXME: this is in the wrong place, it needs to be done when pkt is
+	 * destroyed
+	 */
+	ib_device_put(&rxe->ib_dev);
+
 	return 0;
 drop:
 	kfree_skb(skb);
@@ -635,16 +624,17 @@ static int rxe_notify(struct notifier_block *not_blk,
 		      void *arg)
 {
 	struct net_device *ndev = netdev_notifier_info_to_dev(arg);
-	struct rxe_dev *rxe = net_to_rxe(ndev);
+	struct rxe_dev *rxe = rxe_get_dev_from_net(ndev);
 
 	if (!rxe)
-		goto out;
+		return NOTIFY_OK;
 
 	switch (event) {
 	case NETDEV_UNREGISTER:
 		list_del(&rxe->list);
+		ib_device_put(&rxe->ib_dev);
 		rxe_remove(rxe);
-		break;
+		return NOTIFY_OK;
 	case NETDEV_UP:
 		rxe_port_up(rxe);
 		break;
@@ -668,7 +658,8 @@ static int rxe_notify(struct notifier_block *not_blk,
 			event, ndev->name);
 		break;
 	}
-out:
+
+	ib_device_put(&rxe->ib_dev);
 	return NOTIFY_OK;
 }
 

commit 668aa15b5bf87f156ec805cb7348c785c56b82ab
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Tue Jan 29 12:08:50 2019 +0200

    RDMA/rxe: Improve loopback marking
    
    Currently a packet is marked for loopback only if the source and
    destination addresses equals. This is not enough when multiple gids are
    present in rxe device's gid table and the traffic is from one gid to
    another. Fix it by marking the packet for loopback if the destination MAC
    address is equal to the source MAC address.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Tested-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 19f3c69916b1..3b162e92e8e8 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -384,9 +384,6 @@ static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb,
 		return -EHOSTUNREACH;
 	}
 
-	if (!memcmp(saddr, daddr, sizeof(*daddr)))
-		pkt->mask |= RXE_LOOPBACK_MASK;
-
 	prepare_udp_hdr(skb, cpu_to_be16(qp->src_port),
 			cpu_to_be16(ROCE_V2_UDP_DPORT));
 
@@ -411,9 +408,6 @@ static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb,
 		return -EHOSTUNREACH;
 	}
 
-	if (!memcmp(saddr, daddr, sizeof(*daddr)))
-		pkt->mask |= RXE_LOOPBACK_MASK;
-
 	prepare_udp_hdr(skb, cpu_to_be16(qp->src_port),
 			cpu_to_be16(ROCE_V2_UDP_DPORT));
 
@@ -437,6 +431,9 @@ int rxe_prepare(struct rxe_pkt_info *pkt, struct sk_buff *skb, u32 *crc)
 
 	*crc = rxe_icrc_hdr(pkt, skb);
 
+	if (ether_addr_equal(skb->dev->dev_addr, av->dmac))
+		pkt->mask |= RXE_LOOPBACK_MASK;
+
 	return err;
 }
 

commit 459cc69fa4c17caf21de596693d8a07170820a58
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Jan 30 12:49:11 2019 +0200

    RDMA: Provide safe ib_alloc_device() function
    
    All callers to ib_alloc_device() provide a larger size than struct
    ib_device and rely on the fact that struct ib_device is embedded in their
    driver specific structure as the first member.
    
    Provide a safer variant of ib_alloc_device() that checks and enforces this
    approach to make sure the drivers are using it right.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 8fd03ae20efc..19f3c69916b1 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -555,7 +555,7 @@ struct rxe_dev *rxe_net_add(struct net_device *ndev)
 	int err;
 	struct rxe_dev *rxe = NULL;
 
-	rxe = (struct rxe_dev *)ib_alloc_device(sizeof(*rxe));
+	rxe = ib_alloc_device(rxe_dev, ib_dev);
 	if (!rxe)
 		return NULL;
 

commit f55c3ec42a97bca408f0208ec08e3382340c7be7
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Fri Dec 14 08:05:49 2018 -0800

    IB/rxe: Reuse code which sets port state
    
    Same code is executed in both rxe_param_set_add and rxe_notify functions.
    Make one function and call it from both places.
    
    Since both callers already have a rxe object use it directly instead of
    deriving it from the net device.
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com> 
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index b26a8141f3ed..8fd03ae20efc 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -625,6 +625,14 @@ void rxe_port_down(struct rxe_dev *rxe)
 	dev_info(&rxe->ib_dev.dev, "set down\n");
 }
 
+void rxe_set_port_state(struct rxe_dev *rxe)
+{
+	if (netif_running(rxe->ndev) && netif_carrier_ok(rxe->ndev))
+		rxe_port_up(rxe);
+	else
+		rxe_port_down(rxe);
+}
+
 static int rxe_notify(struct notifier_block *not_blk,
 		      unsigned long event,
 		      void *arg)
@@ -651,10 +659,7 @@ static int rxe_notify(struct notifier_block *not_blk,
 		rxe_set_mtu(rxe, ndev->mtu);
 		break;
 	case NETDEV_CHANGE:
-		if (netif_running(ndev) && netif_carrier_ok(ndev))
-			rxe_port_up(rxe);
-		else
-			rxe_port_down(rxe);
+		rxe_set_port_state(rxe);
 		break;
 	case NETDEV_REBOOT:
 	case NETDEV_GOING_DOWN:

commit 6e5559b275e75a0039d2e11a68d65c2a86df3278
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Thu Nov 1 09:18:46 2018 -0400

    RDMA/rxe: Add link_down, rdma_sends, rdma_recvs stats counters
    
    link_down is self-explanatory.
    
    rdma_sends and rdma_recvs count the number of RDMA Send and RDMA Receive
    operations completed successfully. This is different from the existing
    sent_pkts and rcvd_pkts counters because the existing counters measure
    packets, not RDMA operations.
    
    ack_deffered is renamed to ack_deferred to fix the spelling.
    
    out_of_sequence is renamed to out_of_seq_request to make clear that it is
    counting only requests and not other packets which can be out of sequence.
    
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index cb028a3d8275..b26a8141f3ed 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -621,6 +621,7 @@ void rxe_port_down(struct rxe_dev *rxe)
 	port->attr.state = IB_PORT_DOWN;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ERR);
+	rxe_counter_inc(rxe, RXE_CNT_LINK_DOWNED);
 	dev_info(&rxe->ib_dev.dev, "set down\n");
 }
 

commit 5736c7c499f142bceec06a2ab3113997d0175f80
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Thu Nov 1 09:18:45 2018 -0400

    RDMA/rxe: Distinguish between down links and disabled links
    
    In ib_query_port(), use the netdev's IFF_UP flag to determine phys_state
    (flag set = down = POLLING, flag clear = disabled = DISABLED).
    
    Callers can then use the phys_state field to distinguish between links
    which have a dead partner, cable missing, etc., from links which are
    turned off on the local node. This is useful for HA and supportability.
    
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 40e82e0f6c2d..cb028a3d8275 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -607,7 +607,6 @@ void rxe_port_up(struct rxe_dev *rxe)
 
 	port = &rxe->port;
 	port->attr.state = IB_PORT_ACTIVE;
-	port->attr.phys_state = IB_PHYS_STATE_LINK_UP;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ACTIVE);
 	dev_info(&rxe->ib_dev.dev, "set active\n");
@@ -620,7 +619,6 @@ void rxe_port_down(struct rxe_dev *rxe)
 
 	port = &rxe->port;
 	port->attr.state = IB_PORT_DOWN;
-	port->attr.phys_state = IB_PHYS_STATE_LINK_DOWN;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ERR);
 	dev_info(&rxe->ib_dev.dev, "set down\n");

commit 03241627b222427917999c533f1a2f9d5beda2be
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Tue Oct 2 11:03:10 2018 +0300

    RDMA/rxe: Remove unused addr_same()
    
    This function is not in use - delete it.
    
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index fb06f94f33d8..40e82e0f6c2d 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -491,11 +491,6 @@ void rxe_loopback(struct sk_buff *skb)
 	rxe_rcv(skb);
 }
 
-static inline int addr_same(struct rxe_dev *rxe, struct rxe_av *av)
-{
-	return rxe->port.port_guid == av->grh.dgid.global.interface_id;
-}
-
 struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 				int paylen, struct rxe_pkt_info *pkt)
 {

commit 9de6986148360bdc954e8621cf024f4c830ddb13
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Sep 20 16:42:26 2018 -0600

    RDMA/drivers: Use dev_name instead of ibdev->name
    
    These return the same thing but dev_name is a more conventional use of the
    kernel API.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 8e658ce439b4..fb06f94f33d8 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -72,7 +72,7 @@ struct rxe_dev *get_rxe_by_name(const char *name)
 
 	spin_lock_bh(&dev_list_lock);
 	list_for_each_entry(rxe, &rxe_dev_list, list) {
-		if (!strcmp(name, rxe->ib_dev.name)) {
+		if (!strcmp(name, dev_name(&rxe->ib_dev.dev))) {
 			found = rxe;
 			break;
 		}

commit 5a738b5d47050b77ac8aa90bd79429940533ef6a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Sep 20 16:42:24 2018 -0600

    RDMA/drivers: Use dev_err/dbg/etc instead of pr_* + ibdev->name
    
    Kernel convention is that a driver for a subsystem will print using
    dev_* on the subsystem's struct device, or with dev_* on the physical
    device. Drivers should rarely use a pr_* function.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index ed823cccca48..8e658ce439b4 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -615,7 +615,7 @@ void rxe_port_up(struct rxe_dev *rxe)
 	port->attr.phys_state = IB_PHYS_STATE_LINK_UP;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ACTIVE);
-	pr_info("set %s active\n", rxe->ib_dev.name);
+	dev_info(&rxe->ib_dev.dev, "set active\n");
 }
 
 /* Caller must hold net_info_lock */
@@ -628,7 +628,7 @@ void rxe_port_down(struct rxe_dev *rxe)
 	port->attr.phys_state = IB_PHYS_STATE_LINK_DOWN;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ERR);
-	pr_info("set %s down\n", rxe->ib_dev.name);
+	dev_info(&rxe->ib_dev.dev, "set down\n");
 }
 
 static int rxe_notify(struct notifier_block *not_blk,

commit 3db2bceb29fd9a2f31d5df573f337dbba577f8e9
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Aug 28 13:51:37 2018 +0300

    IB/rxe: Simplify rxe_find_route() to avoid GID query for netdev
    
    rxe_prepare() is called on an skb which has ndev already initialized by
    rxe_init_packet().
    Therefore avoid querying the GID attribute again and use the available
    netdevice from the skb->dev.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Tested-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 4ddb6b4c6a46..ed823cccca48 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -182,19 +182,11 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 
 #endif
 
-static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
+static struct dst_entry *rxe_find_route(struct net_device *ndev,
 					struct rxe_qp *qp,
 					struct rxe_av *av)
 {
-	const struct ib_gid_attr *attr;
 	struct dst_entry *dst = NULL;
-	struct net_device *ndev;
-
-	attr = rdma_get_gid_attr(&rxe->ib_dev, qp->attr.port_num,
-				 av->grh.sgid_index);
-	if (IS_ERR(attr))
-		return NULL;
-	ndev = attr->ndev;
 
 	if (qp_type(qp) == IB_QPT_RC)
 		dst = sk_dst_get(qp->sk->sk);
@@ -229,7 +221,6 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 			sk_dst_set(qp->sk->sk, dst);
 		}
 	}
-	rdma_put_gid_attr(attr);
 	return dst;
 }
 
@@ -377,8 +368,8 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	ip6h->payload_len = htons(skb->len - sizeof(*ip6h));
 }
 
-static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		    struct sk_buff *skb, struct rxe_av *av)
+static int prepare4(struct rxe_pkt_info *pkt, struct sk_buff *skb,
+		    struct rxe_av *av)
 {
 	struct rxe_qp *qp = pkt->qp;
 	struct dst_entry *dst;
@@ -387,7 +378,7 @@ static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
 
-	dst = rxe_find_route(rxe, qp, av);
+	dst = rxe_find_route(skb->dev, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
 		return -EHOSTUNREACH;
@@ -406,15 +397,15 @@ static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	return 0;
 }
 
-static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		    struct sk_buff *skb, struct rxe_av *av)
+static int prepare6(struct rxe_pkt_info *pkt, struct sk_buff *skb,
+		    struct rxe_av *av)
 {
 	struct rxe_qp *qp = pkt->qp;
 	struct dst_entry *dst;
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
 
-	dst = rxe_find_route(rxe, qp, av);
+	dst = rxe_find_route(skb->dev, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
 		return -EHOSTUNREACH;
@@ -434,16 +425,15 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	return 0;
 }
 
-int rxe_prepare(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		struct sk_buff *skb, u32 *crc)
+int rxe_prepare(struct rxe_pkt_info *pkt, struct sk_buff *skb, u32 *crc)
 {
 	int err = 0;
 	struct rxe_av *av = rxe_get_av(pkt);
 
 	if (av->network_type == RDMA_NETWORK_IPV4)
-		err = prepare4(rxe, pkt, skb, av);
+		err = prepare4(pkt, skb, av);
 	else if (av->network_type == RDMA_NETWORK_IPV6)
-		err = prepare6(rxe, pkt, skb, av);
+		err = prepare6(pkt, skb, av);
 
 	*crc = rxe_icrc_hdr(pkt, skb);
 

commit d3c04a3a6870a4b96ee213d99b4f4ca723e64025
Author: Vijay Immanuel <vijayi@attalasystems.com>
Date:   Thu Jul 5 18:43:47 2018 -0700

    IB/rxe: vary the source udp port for receive scaling
    
    Select the source udp port number for a QP based on the
    source QPN. This provides a better spread of traffic
    across NIC RX queues for RC/UC QPs.
    
    Signed-off-by: Vijay Immanuel <vijayi@attalasystems.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 8094cbaa54a9..4ddb6b4c6a46 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -396,8 +396,8 @@ static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	if (!memcmp(saddr, daddr, sizeof(*daddr)))
 		pkt->mask |= RXE_LOOPBACK_MASK;
 
-	prepare_udp_hdr(skb, htons(RXE_ROCE_V2_SPORT),
-			htons(ROCE_V2_UDP_DPORT));
+	prepare_udp_hdr(skb, cpu_to_be16(qp->src_port),
+			cpu_to_be16(ROCE_V2_UDP_DPORT));
 
 	prepare_ipv4_hdr(dst, skb, saddr->s_addr, daddr->s_addr, IPPROTO_UDP,
 			 av->grh.traffic_class, av->grh.hop_limit, df, xnet);
@@ -423,8 +423,8 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	if (!memcmp(saddr, daddr, sizeof(*daddr)))
 		pkt->mask |= RXE_LOOPBACK_MASK;
 
-	prepare_udp_hdr(skb, htons(RXE_ROCE_V2_SPORT),
-			htons(ROCE_V2_UDP_DPORT));
+	prepare_udp_hdr(skb, cpu_to_be16(qp->src_port),
+			cpu_to_be16(ROCE_V2_UDP_DPORT));
 
 	prepare_ipv6_hdr(dst, skb, saddr, daddr, IPPROTO_UDP,
 			 av->grh.traffic_class,

commit 24c937b39dfb102860e086f47ab9a58a0f6b0deb
Author: Vijay Immanuel <vijayi@attalasystems.com>
Date:   Mon Jun 18 18:48:56 2018 -0700

    IB/rxe: don't clear the tx queue on every transfer
    
    Do not call sk_dst_set() on every packet transfer because
    that calls sk_tx_queue_clear(), which clears the tx queue.
    A QP must stay on the same tx queue to maintain packet order.
    
    Signed-off-by: Vijay Immanuel <vijayi@attalasystems.com>
    Acked-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 79b69943a8af..8094cbaa54a9 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -223,6 +223,11 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 					rt6_get_cookie((struct rt6_info *)dst);
 #endif
 		}
+
+		if (dst && (qp_type(qp) == IB_QPT_RC)) {
+			dst_hold(dst);
+			sk_dst_set(qp->sk->sk, dst);
+		}
 	}
 	rdma_put_gid_attr(attr);
 	return dst;
@@ -397,11 +402,7 @@ static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	prepare_ipv4_hdr(dst, skb, saddr->s_addr, daddr->s_addr, IPPROTO_UDP,
 			 av->grh.traffic_class, av->grh.hop_limit, df, xnet);
 
-	if (qp_type(qp) == IB_QPT_RC)
-		sk_dst_set(qp->sk->sk, dst);
-	else
-		dst_release(dst);
-
+	dst_release(dst);
 	return 0;
 }
 
@@ -429,11 +430,7 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 			 av->grh.traffic_class,
 			 av->grh.hop_limit);
 
-	if (qp_type(qp) == IB_QPT_RC)
-		sk_dst_set(qp->sk->sk, dst);
-	else
-		dst_release(dst);
-
+	dst_release(dst);
 	return 0;
 }
 

commit 82f82ceb8e660322c642d1ea6f835756aa061b34
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jun 5 08:40:23 2018 +0300

    IB/rxe: Use rdma GID API
    
    rxe_netdev_from_av can now be done by the core code directly from the
    gid_attrs, no need for a helper in the driver.
    
    ib_find_cached_gid_by_port can be switched to use the rdma version here as
    well.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 59ec6d918ed4..79b69943a8af 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -182,39 +182,19 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 
 #endif
 
-/*
- * Derive the net_device from the av.
- * For physical devices, this will just return rxe->ndev.
- * But for VLAN devices, it will return the vlan dev.
- * Caller should dev_put() the returned net_device.
- */
-static struct net_device *rxe_netdev_from_av(struct rxe_dev *rxe,
-					     int port_num,
-					     struct rxe_av *av)
-{
-	union ib_gid gid;
-	struct ib_gid_attr attr;
-	struct net_device *ndev = rxe->ndev;
-
-	if (ib_get_cached_gid(&rxe->ib_dev, port_num, av->grh.sgid_index,
-			      &gid, &attr) == 0 &&
-	    attr.ndev && attr.ndev != ndev)
-		ndev = attr.ndev;
-	else
-		/* Only to ensure that caller may call dev_put() */
-		dev_hold(ndev);
-
-	return ndev;
-}
-
 static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 					struct rxe_qp *qp,
 					struct rxe_av *av)
 {
+	const struct ib_gid_attr *attr;
 	struct dst_entry *dst = NULL;
 	struct net_device *ndev;
 
-	ndev = rxe_netdev_from_av(rxe, qp->attr.port_num, av);
+	attr = rdma_get_gid_attr(&rxe->ib_dev, qp->attr.port_num,
+				 av->grh.sgid_index);
+	if (IS_ERR(attr))
+		return NULL;
+	ndev = attr->ndev;
 
 	if (qp_type(qp) == IB_QPT_RC)
 		dst = sk_dst_get(qp->sk->sk);
@@ -244,8 +224,7 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 #endif
 		}
 	}
-
-	dev_put(ndev);
+	rdma_put_gid_attr(attr);
 	return dst;
 }
 
@@ -536,9 +515,13 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 	unsigned int hdr_len;
 	struct sk_buff *skb;
 	struct net_device *ndev;
+	const struct ib_gid_attr *attr;
 	const int port_num = 1;
 
-	ndev = rxe_netdev_from_av(rxe, port_num, av);
+	attr = rdma_get_gid_attr(&rxe->ib_dev, port_num, av->grh.sgid_index);
+	if (IS_ERR(attr))
+		return NULL;
+	ndev = attr->ndev;
 
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
@@ -550,10 +533,8 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 	skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(ndev),
 			GFP_ATOMIC);
 
-	if (unlikely(!skb)) {
-		dev_put(ndev);
-		return NULL;
-	}
+	if (unlikely(!skb))
+		goto out;
 
 	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(rxe->ndev));
 
@@ -568,7 +549,8 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 	pkt->hdr	= skb_put_zero(skb, paylen);
 	pkt->mask	|= RXE_GRH_MASK;
 
-	dev_put(ndev);
+out:
+	rdma_put_gid_attr(attr);
 	return skb;
 }
 

commit bb42f87e2924e3b118aed3b3790ce792ae7bb86c
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Mon May 28 05:03:41 2018 -0400

    IB/rxe: avoid unnecessary export
    
    The function rxe_remove_all is only used in this modules.
    There is no other modules that call this function. So it
    is not necessary to export it.
    
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 95e52b3ec757..59ec6d918ed4 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -623,7 +623,6 @@ void rxe_remove_all(void)
 	}
 	spin_unlock_bh(&dev_list_lock);
 }
-EXPORT_SYMBOL(rxe_remove_all);
 
 static void rxe_port_event(struct rxe_dev *rxe,
 			   enum ib_event_type event)

commit ecb238f6a7f369b5e0eece4e913c9d671208860c
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Sat Apr 28 15:31:06 2018 +0800

    IB/cxgb4: use skb_put_zero()/__skb_put_zero
    
    Use the recently introduced helper to replace the pattern of
    skb_put_zero/__skb_put() && memset().
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index fca13a6281f0..95e52b3ec757 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -565,11 +565,9 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 
 	pkt->rxe	= rxe;
 	pkt->port_num	= port_num;
-	pkt->hdr	= skb_put(skb, paylen);
+	pkt->hdr	= skb_put_zero(skb, paylen);
 	pkt->mask	|= RXE_GRH_MASK;
 
-	memset(pkt->hdr, 0, paylen);
-
 	dev_put(ndev);
 	return skb;
 }

commit 10c47d560603a8ba9d74889028cf6728a2d845ca
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Fri Apr 20 17:05:03 2018 +0300

    IB/rxe: Change rxe_rcv to return void
    
    It always returns 0. Change return type to void.
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 97e128579137..fca13a6281f0 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -276,9 +276,12 @@ static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 	pkt->mask = RXE_GRH_MASK;
 	pkt->paylen = be16_to_cpu(udph->len) - sizeof(*udph);
 
-	return rxe_rcv(skb);
+	rxe_rcv(skb);
+
+	return 0;
 drop:
 	kfree_skb(skb);
+
 	return 0;
 }
 
@@ -517,9 +520,9 @@ int rxe_send(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 	return 0;
 }
 
-int rxe_loopback(struct sk_buff *skb)
+void rxe_loopback(struct sk_buff *skb)
 {
-	return rxe_rcv(skb);
+	rxe_rcv(skb);
 }
 
 static inline int addr_same(struct rxe_dev *rxe, struct rxe_av *av)

commit 8f1a72c815cf121f8a842c60c837f0d7605cdad4
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Mon Apr 9 08:19:18 2018 -0400

    IB/rxe: make rxe_release_udp_tunnel static
    
    The function rxe_release_udp_tunnel is only used in rxe_net.c.
    So it is necessary to make this function as static.
    
    CC: Srinivas Eeda <srinivas.eeda@oracle.com>
    CC: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 241762606a66..97e128579137 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -315,7 +315,7 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 	return sock;
 }
 
-void rxe_release_udp_tunnel(struct socket *sk)
+static void rxe_release_udp_tunnel(struct socket *sk)
 {
 	if (sk)
 		udp_tunnel_sock_release(sk);

commit 0f02ba7ed16acdbc8c4f0b46a6fee81bb94f3407
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Tue Apr 3 20:08:20 2018 -0400

    IB/rxe: make the variable static
    
    The variable rxe_net_notifier is only used in the file rxe_net.c. So
    remove it from rxe_net.h file and make it static in the file rxe_net.c.
    
    CC: Srinivas Eeda <srinivas.eeda@oracle.com>
    CC: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 9da6e37fb70c..241762606a66 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -707,7 +707,7 @@ static int rxe_notify(struct notifier_block *not_blk,
 	return NOTIFY_OK;
 }
 
-struct notifier_block rxe_net_notifier = {
+static struct notifier_block rxe_net_notifier = {
 	.notifier_call = rxe_notify,
 };
 

commit 43c9fc509fa59d602f9c303d02b33db015022881
Author: Martin Wilck <mwilck@suse.com>
Date:   Wed Feb 14 21:45:43 2018 +0100

    rdma_rxe: make rxe work over 802.1q VLAN devices
    
    This patch fixes RDMA/rxe over 802.1q VLAN devices.
    
    Without it, I observed the following behavior:
    
    a) adding a VLAN device to RXE via rxe_net_add() creates a non-functional
       RDMA device. This is caused by the logic in enum_all_gids_of_dev_cb() /
       is_eth_port_of_netdev(), which only considers networks connected to
       "upper devices" of the configured network device, resulting in an empty
       set of gids for a VLAN interface that is an "upper device" itself.
       Later attempts to connect via this rdma device fail in cma_acuire_dev()
       because no gids can be resolved.
    
    b) adding the master device of the VLAN device instead seems to work
       initially, target addresses via VLAN devices are resolved successfully.
       But the connection times out because no 802.1q VLAN headers are
       inserted in the ethernet packets, which are therefore never received.
       This happens because the RXE layer sends the packets via the master
       device rather than the VLAN device.
    
    The problem could be solved by changing either a) or b). My thinking was
    that the logic in a) was created deliberately, thus I decided to work on
    b). It turns out that the information about the VLAN interface for the gid
    at hand is available in the AV information. My patch converts the RXE code
    to use this netdev instead of rxe->ndev. With this change, RXE over vlan
    works on my test system.
    
    Signed-off-by: Martin Wilck <mwilck@suse.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index a7753dc3261e..9da6e37fb70c 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -182,11 +182,39 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 
 #endif
 
+/*
+ * Derive the net_device from the av.
+ * For physical devices, this will just return rxe->ndev.
+ * But for VLAN devices, it will return the vlan dev.
+ * Caller should dev_put() the returned net_device.
+ */
+static struct net_device *rxe_netdev_from_av(struct rxe_dev *rxe,
+					     int port_num,
+					     struct rxe_av *av)
+{
+	union ib_gid gid;
+	struct ib_gid_attr attr;
+	struct net_device *ndev = rxe->ndev;
+
+	if (ib_get_cached_gid(&rxe->ib_dev, port_num, av->grh.sgid_index,
+			      &gid, &attr) == 0 &&
+	    attr.ndev && attr.ndev != ndev)
+		ndev = attr.ndev;
+	else
+		/* Only to ensure that caller may call dev_put() */
+		dev_hold(ndev);
+
+	return ndev;
+}
+
 static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 					struct rxe_qp *qp,
 					struct rxe_av *av)
 {
 	struct dst_entry *dst = NULL;
+	struct net_device *ndev;
+
+	ndev = rxe_netdev_from_av(rxe, qp->attr.port_num, av);
 
 	if (qp_type(qp) == IB_QPT_RC)
 		dst = sk_dst_get(qp->sk->sk);
@@ -201,14 +229,14 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 
 			saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 			daddr = &av->dgid_addr._sockaddr_in.sin_addr;
-			dst = rxe_find_route4(rxe->ndev, saddr, daddr);
+			dst = rxe_find_route4(ndev, saddr, daddr);
 		} else if (av->network_type == RDMA_NETWORK_IPV6) {
 			struct in6_addr *saddr6;
 			struct in6_addr *daddr6;
 
 			saddr6 = &av->sgid_addr._sockaddr_in6.sin6_addr;
 			daddr6 = &av->dgid_addr._sockaddr_in6.sin6_addr;
-			dst = rxe_find_route6(rxe->ndev, saddr6, daddr6);
+			dst = rxe_find_route6(ndev, saddr6, daddr6);
 #if IS_ENABLED(CONFIG_IPV6)
 			if (dst)
 				qp->dst_cookie =
@@ -217,6 +245,7 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 		}
 	}
 
+	dev_put(ndev);
 	return dst;
 }
 
@@ -224,9 +253,14 @@ static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 {
 	struct udphdr *udph;
 	struct net_device *ndev = skb->dev;
+	struct net_device *rdev = ndev;
 	struct rxe_dev *rxe = net_to_rxe(ndev);
 	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
 
+	if (!rxe && is_vlan_dev(rdev)) {
+		rdev = vlan_dev_real_dev(ndev);
+		rxe = net_to_rxe(rdev);
+	}
 	if (!rxe)
 		goto drop;
 
@@ -498,6 +532,10 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 {
 	unsigned int hdr_len;
 	struct sk_buff *skb;
+	struct net_device *ndev;
+	const int port_num = 1;
+
+	ndev = rxe_netdev_from_av(rxe, port_num, av);
 
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
@@ -506,26 +544,30 @@ struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
 		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
 			sizeof(struct ipv6hdr);
 
-	skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(rxe->ndev),
+	skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(ndev),
 			GFP_ATOMIC);
-	if (unlikely(!skb))
+
+	if (unlikely(!skb)) {
+		dev_put(ndev);
 		return NULL;
+	}
 
 	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(rxe->ndev));
 
-	skb->dev	= rxe->ndev;
+	skb->dev	= ndev;
 	if (av->network_type == RDMA_NETWORK_IPV4)
 		skb->protocol = htons(ETH_P_IP);
 	else
 		skb->protocol = htons(ETH_P_IPV6);
 
 	pkt->rxe	= rxe;
-	pkt->port_num	= 1;
+	pkt->port_num	= port_num;
 	pkt->hdr	= skb_put(skb, paylen);
 	pkt->mask	|= RXE_GRH_MASK;
 
 	memset(pkt->hdr, 0, paylen);
 
+	dev_put(ndev);
 	return skb;
 }
 

commit 31f1bd14cbfe4f7a4ea1ada2d4e0dc802a258f5d
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Tue Feb 27 06:04:33 2018 -0500

    IB/rxe: remove unnecessary rxe in rxe_send
    
    In the function rxe_send, the variable rxe is not used in it.
    So it should be removed.
    
    CC: Srinivas Eeda <srinivas.eeda@oracle.com>
    CC: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 159246b03867..a7753dc3261e 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -450,7 +450,7 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	rxe_drop_ref(qp);
 }
 
-int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
+int rxe_send(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	struct rxe_av *av;
 	int err;

commit 5793b46521553774d66653993cfdc132846bb23d
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Mon Jan 8 00:14:25 2018 -0500

    IB/rxe: remove unnecessary skb_clone in xmit
    
    In xmit, there is a skb_clone. This function copies the struct sk_buff.
    And some parameters are changed to the new skb. Then the new skb is sent
    while the old skb is freed.
    
    While the function skb_clone is removed, the parameter changes are made on
    the old skb, then the old skb is sent. It can also work well.
    
    The following tests are made.
    
     server                       client
    ---------                    ---------
    |1.1.1.1|<----rxe-channel--->|1.1.1.2|
    ---------                    ---------
    
    On server: rping -s -a 1.1.1.1 -v -C 1000 -S 512
    On client: rping -c -a 1.1.1.1 -v -C 1000 -S 512
    
    The kernel config CONFIG_DEBUG_KMEMLEAK is enabled on both server
    and client.
    
    This test runs for several hours. There is no memory leak and the whole
    system can work well.
    
    As the above network, the following tests are made.
    
    Server: ibv_rc_pingpong -d rxe0 -g 1
    Client: ibv_rc_pingpong -d rxe0 -g 1 1.1.1.1
    
    The result on Server.
    Before:
    8192000 bytes in 0.88 seconds = 74.36 Mbit/sec
    1000 iters in 0.88 seconds = 881.30 usec/iter
    
    After:
    8192000 bytes in 0.81 seconds = 81.15 Mbit/sec
    1000 iters in 0.81 seconds = 807.62 usec/iter
    
    The throughput is enhanced and the latency is reduced.
    
    CC: Srinivas Eeda <srinivas.eeda@oracle.com>
    CC: Joe Jin <joe.jin@oracle.com>
    CC: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 44d77807a69d..159246b03867 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -452,31 +452,26 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 
 int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
-	struct sk_buff *nskb;
 	struct rxe_av *av;
 	int err;
 
 	av = rxe_get_av(pkt);
 
-	nskb = skb_clone(skb, GFP_ATOMIC);
-	if (!nskb)
-		return -ENOMEM;
-
-	nskb->destructor = rxe_skb_tx_dtor;
-	nskb->sk = pkt->qp->sk->sk;
+	skb->destructor = rxe_skb_tx_dtor;
+	skb->sk = pkt->qp->sk->sk;
 
 	rxe_add_ref(pkt->qp);
 	atomic_inc(&pkt->qp->skb_out);
 
 	if (av->network_type == RDMA_NETWORK_IPV4) {
-		err = ip_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
+		err = ip_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
 	} else if (av->network_type == RDMA_NETWORK_IPV6) {
-		err = ip6_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
+		err = ip6_local_out(dev_net(skb_dst(skb)->dev), skb->sk, skb);
 	} else {
 		pr_err("Unknown layer 3 protocol: %d\n", av->network_type);
 		atomic_dec(&pkt->qp->skb_out);
 		rxe_drop_ref(pkt->qp);
-		kfree_skb(nskb);
+		kfree_skb(skb);
 		return -EINVAL;
 	}
 
@@ -485,7 +480,6 @@ int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 		return -EAGAIN;
 	}
 
-	kfree_skb(skb);
 	return 0;
 }
 

commit 91eab79653b89f92f266b411792f0a0d46857a3a
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Sun Jan 7 07:08:48 2018 -0500

    IB/rxe: add the static type to the variable
    
    The variable recv_sockets is only used in the file rxe_net.c. So
    it is better to add static type to it.
    
    CC: Srinivas Eeda <srinivas.eeda@oracle.com>
    CC: Joe Jin <joe.jin@oracle.com>
    CC: Junxiao Bi <junxiao.bi@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 59dee10bebcb..44d77807a69d 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -82,7 +82,7 @@ struct rxe_dev *get_rxe_by_name(const char *name)
 }
 
 
-struct rxe_recv_sockets recv_sockets;
+static struct rxe_recv_sockets recv_sockets;
 
 struct device *rxe_dma_device(struct rxe_dev *rxe)
 {

commit 5c50f1d18f6bdd8ff8d274479b8bf0d89ab5a47d
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:59 2017 -0400

    IB/rxe: Handle NETDEV_CHANGE events
    
    Without this fix, ports configured on top of ixgbe miss link up
    notifications. ibv_query_port() will continue to return IBV_PORT_DOWN even
    though the port is up and working.
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 3ba76e7c7a38..59dee10bebcb 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -651,8 +651,13 @@ static int rxe_notify(struct notifier_block *not_blk,
 		pr_info("%s changed mtu to %d\n", ndev->name, ndev->mtu);
 		rxe_set_mtu(rxe, ndev->mtu);
 		break;
-	case NETDEV_REBOOT:
 	case NETDEV_CHANGE:
+		if (netif_running(ndev) && netif_carrier_ok(ndev))
+			rxe_port_up(rxe);
+		else
+			rxe_port_down(rxe);
+		break;
+	case NETDEV_REBOOT:
 	case NETDEV_GOING_DOWN:
 	case NETDEV_CHANGEADDR:
 	case NETDEV_CHANGENAME:

commit 2418adaed1ab614438d7995c500b1d219b138add
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:56 2017 -0400

    IB/rxe: Remove unneeded initialization in prepare6()
    
    Fixes: 4ed6ad1eb30e ("IB/rxe: Cache dst in QP instead of getting it...")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 3fbbadc5eb47..3ba76e7c7a38 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -393,7 +393,7 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 		    struct sk_buff *skb, struct rxe_av *av)
 {
 	struct rxe_qp *qp = pkt->qp;
-	struct dst_entry *dst = NULL;
+	struct dst_entry *dst;
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
 

commit 48c22be4abaf3925e8ff74fb3ce5214b082ca989
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:54 2017 -0400

    IB/rxe: Add dst_clone() in prepare_ipv6_hdr()
    
    Otherwise the reference count goes negative as IPv6 packets complete.
    
    Fixes: 4ed6ad1eb30e ("IB/rxe: Cache dst in QP instead of getting it...")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index a93ae3a90ff0..3fbbadc5eb47 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -342,7 +342,7 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	memset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));
 	IPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED
 			    | IPSKB_REROUTED);
-	skb_dst_set(skb, dst);
+	skb_dst_set(skb, dst_clone(dst));
 
 	__skb_push(skb, sizeof(*ip6h));
 	skb_reset_network_header(skb);

commit b9109b7ddb13a52b379c3e57d9b1d74d7445e94d
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:53 2017 -0400

    IB/rxe: Fix destination cache for IPv6
    
    To successfully match an IPv6 path, the path cookie must match. Store it
    in the QP so that the IPv6 path can be reused.
    
    Replace open-coded version of dst_check() with the actual call, fixing the
    logic. The open-coded version skips the check call if dst->obsolete is 0
    (DST_OBSOLETE_NONE), proceeding to replace the route. DST_OBSOLETE_NONE
    means that the route may continue to be used, though.
    
    Fixes: 4ed6ad1eb30e ("IB/rxe: Cache dst in QP instead of getting it...")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 0810f38bb1fa..a93ae3a90ff0 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -191,7 +191,7 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 	if (qp_type(qp) == IB_QPT_RC)
 		dst = sk_dst_get(qp->sk->sk);
 
-	if (!dst || !(dst->obsolete && dst->ops->check(dst, 0))) {
+	if (!dst || !dst_check(dst, qp->dst_cookie)) {
 		if (dst)
 			dst_release(dst);
 
@@ -209,6 +209,11 @@ static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
 			saddr6 = &av->sgid_addr._sockaddr_in6.sin6_addr;
 			daddr6 = &av->dgid_addr._sockaddr_in6.sin6_addr;
 			dst = rxe_find_route6(rxe->ndev, saddr6, daddr6);
+#if IS_ENABLED(CONFIG_IPV6)
+			if (dst)
+				qp->dst_cookie =
+					rt6_get_cookie((struct rt6_info *)dst);
+#endif
 		}
 	}
 

commit 9eb7f8e44d13cd2a565a5f088c8a842810270757
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:49 2017 -0400

    IB/rxe: Move refcounting earlier in rxe_send()
    
    The network stack will call nskb's destructor, rxe_skb_tx_dtor(), if the
    packet gets dropped by ip_local_out()/ip6_local_out(). Thus we need to add
    the QP ref before output to avoid extra dereferences during network
    congestion. This could lead to unwanted destruction of the QP.
    
    Fix up the skb_out accounting, too.
    
    Fixes: fda85ce91240 ("IB/rxe: Fix kernel panic from skb destructor")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Acked-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 08f3f90d2912..0810f38bb1fa 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -460,12 +460,17 @@ int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 	nskb->destructor = rxe_skb_tx_dtor;
 	nskb->sk = pkt->qp->sk->sk;
 
+	rxe_add_ref(pkt->qp);
+	atomic_inc(&pkt->qp->skb_out);
+
 	if (av->network_type == RDMA_NETWORK_IPV4) {
 		err = ip_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
 	} else if (av->network_type == RDMA_NETWORK_IPV6) {
 		err = ip6_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
 	} else {
 		pr_err("Unknown layer 3 protocol: %d\n", av->network_type);
+		atomic_dec(&pkt->qp->skb_out);
+		rxe_drop_ref(pkt->qp);
 		kfree_skb(nskb);
 		return -EINVAL;
 	}
@@ -475,10 +480,7 @@ int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 		return -EAGAIN;
 	}
 
-	rxe_add_ref(pkt->qp);
-	atomic_inc(&pkt->qp->skb_out);
 	kfree_skb(skb);
-
 	return 0;
 }
 

commit fda85ce912401750e1e80757627af2784c7cc5a7
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Thu Jun 22 17:09:59 2017 +0300

    IB/rxe: Fix kernel panic from skb destructor
    
    In the time between rxe_send has finished and skb destructor
    called, the QP's ref count might be 0, leading to a possible
    QP destruction. This will lead to a kernel panic when the destructor
    dereferences the QP.
    
    The operation of incrementing QP ref count at rxe_send and decrementing
    from skb destructor will prevent this crash.
    
    BUG: unable to handle kernel NULL pointer dereference at 000000000000072c
    IP: [<ffffffffa05df765>] rxe_skb_tx_dtor+0x15/0x50 [rdma_rxe]
    PGD 0 [16240.211178]
    Oops: 0002 [#1] SMP
    CPU: 3 PID: 0 Comm: swapper/3 Tainted: G           OE   4.9.0-mlnx #1
    Hardware name: Red Hat KVM, BIOS Bochs 01/01/2011
    task: ffff88042d6b1480 task.stack: ffffc90001904000
    RIP: 0010:[<ffffffffa05df765>]  [<ffffffffa05df765>] rxe_skb_tx_dtor+0x15/0x50 [rdma_rxe]
    RSP: 0018:ffff88043fcc3df0  EFLAGS: 00010246
    RAX: 0000000000000000 RBX: ffff880429684700 RCX: ffff88042d248200
    RDX: 00000000ffffffff RSI: 00000000fffffe01 RDI: ffff880429684700
    RBP: ffff88043fcc3e00 R08: ffff88043fcda240 R09: 00000000ff2d1de6
    R10: 0000000000000000 R11: 00000000f49cf6fe R12: ffff880429684700
    R13: ffffffff81893f96 R14: ffffffff817d66f0 R15: ffff880427f74200
    FS:  0000000000000000(0000) GS:ffff88043fcc0000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 000000000000072c CR3: 000000041d3df000 CR4: 00000000000006e0
    Stack:
     ffffffff817b29cf ffff880429684700 ffff88043fcc3e18 ffffffff817b42c2
     ffff880429684700 ffff88043fcc3e40 ffffffff817b4332 ffff880429684700
     ffff880427f74238 ffff880427f74228 ffff88043fcc3e58 ffffffff81893f96
    Call Trace:
     <IRQ> [16240.336345]  [<ffffffff817b29cf>] ? skb_release_head_state+0x4f/0xb0
     [<ffffffff817b42c2>] skb_release_all+0x12/0x30
     [<ffffffff817b4332>] kfree_skb+0x32/0x90
     [<ffffffff81893f96>] ndisc_error_report+0x36/0x40
     [<ffffffff817d4de1>] neigh_invalidate+0x81/0xf0
     [<ffffffff817d68f7>] neigh_timer_handler+0x207/0x2b0
     [<ffffffff81109295>] call_timer_fn+0x35/0x120
     [<ffffffff81109db7>] run_timer_softirq+0x1d7/0x460
     [<ffffffff8106155e>] ? kvm_sched_clock_read+0x1e/0x30
     [<ffffffff810366b9>] ? sched_clock+0x9/0x10
     [<ffffffff810cfed2>] ? sched_clock_cpu+0x72/0xa0
     [<ffffffff818dd537>] __do_softirq+0xd7/0x289
     [<ffffffff810a6c95>] irq_exit+0xb5/0xc0
     [<ffffffff818dd372>] smp_apic_timer_interrupt+0x42/0x50
     [<ffffffff818dc682>] apic_timer_interrupt+0x82/0x90
     <EOI> [16240.395776]  [<ffffffff818da156>] ? native_safe_halt+0x6/0x10
     [<ffffffff818d9e6e>] default_idle+0x1e/0xd0
     [<ffffffff8103797f>] arch_cpu_idle+0xf/0x20
     [<ffffffff818da2c5>] default_idle_call+0x35/0x40
     [<ffffffff810e3eb5>] cpu_startup_entry+0x185/0x210
     [<ffffffff81050433>] start_secondary+0x103/0x130
    RIP  [<ffffffffa05df765>] rxe_skb_tx_dtor+0x15/0x50 [rdma_rxe]
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index c3a140ed4df2..08f3f90d2912 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -441,6 +441,8 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	if (unlikely(qp->need_req_skb &&
 		     skb_out < RXE_INFLIGHT_SKBS_PER_QP_LOW))
 		rxe_run_task(&qp->req.task, 1);
+
+	rxe_drop_ref(qp);
 }
 
 int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
@@ -473,6 +475,7 @@ int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 		return -EAGAIN;
 	}
 
+	rxe_add_ref(pkt->qp);
 	atomic_inc(&pkt->qp->skb_out);
 	kfree_skb(skb);
 

commit 4d6f28591fe415fc5233e9148c66a1b81ad17425
Author: Yuval Shaia <yuval.shaia@oracle.com>
Date:   Tue Mar 14 16:01:57 2017 +0200

    {net,IB}/{rxe,usnic}: Utilize generic mac to eui32 function
    
    This logic seems to be duplicated in (at least) three separate files.
    Move it to one place so code can be re-use.
    
    Signed-off-by: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index bf2c5324e107..c3a140ed4df2 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -84,34 +84,6 @@ struct rxe_dev *get_rxe_by_name(const char *name)
 
 struct rxe_recv_sockets recv_sockets;
 
-static __be64 rxe_mac_to_eui64(struct net_device *ndev)
-{
-	unsigned char *mac_addr = ndev->dev_addr;
-	__be64 eui64;
-	unsigned char *dst = (unsigned char *)&eui64;
-
-	dst[0] = mac_addr[0] ^ 2;
-	dst[1] = mac_addr[1];
-	dst[2] = mac_addr[2];
-	dst[3] = 0xff;
-	dst[4] = 0xfe;
-	dst[5] = mac_addr[3];
-	dst[6] = mac_addr[4];
-	dst[7] = mac_addr[5];
-
-	return eui64;
-}
-
-__be64 rxe_node_guid(struct rxe_dev *rxe)
-{
-	return rxe_mac_to_eui64(rxe->ndev);
-}
-
-__be64 rxe_port_guid(struct rxe_dev *rxe)
-{
-	return rxe_mac_to_eui64(rxe->ndev);
-}
-
 struct device *rxe_dma_device(struct rxe_dev *rxe)
 {
 	struct net_device *ndev;

commit 4ed6ad1eb30e20baf819ae4090b7978519beb3a9
Author: yonatanc <yonatanc@mellanox.com>
Date:   Thu Apr 20 20:55:56 2017 +0300

    IB/rxe: Cache dst in QP instead of getting it for each send
    
    In RC QP there is no need to resolve the outgoing interface
    for each packet, as this does not change during QP life cycle.
    
    Instead cache the interface on the socket and use that one.
    This improves performance by 12% by sparing redundant
    calls to rxe_find_route.
    
    ib_send_bw -d rxe0  -x 1 -n 9000 -e  -s $((1024 * 1024 )) -l 100
    
    ----------------------------------------------------------------------------------------
    |        | bytes   | iterations | BW peak[MB/sec] | BW average[MB/sec] | MsgRate[Mpps] |
    ----------------------------------------------------------------------------------------
    | before | 1048576 | 9000       | inf             | 551.21             | 0.000551      |
    | after  | 1048576 | 9000       | inf             | 615.54             | 0.000616      |
    ----------------------------------------------------------------------------------------
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index d8610960630a..bf2c5324e107 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -210,6 +210,39 @@ static struct dst_entry *rxe_find_route6(struct net_device *ndev,
 
 #endif
 
+static struct dst_entry *rxe_find_route(struct rxe_dev *rxe,
+					struct rxe_qp *qp,
+					struct rxe_av *av)
+{
+	struct dst_entry *dst = NULL;
+
+	if (qp_type(qp) == IB_QPT_RC)
+		dst = sk_dst_get(qp->sk->sk);
+
+	if (!dst || !(dst->obsolete && dst->ops->check(dst, 0))) {
+		if (dst)
+			dst_release(dst);
+
+		if (av->network_type == RDMA_NETWORK_IPV4) {
+			struct in_addr *saddr;
+			struct in_addr *daddr;
+
+			saddr = &av->sgid_addr._sockaddr_in.sin_addr;
+			daddr = &av->dgid_addr._sockaddr_in.sin_addr;
+			dst = rxe_find_route4(rxe->ndev, saddr, daddr);
+		} else if (av->network_type == RDMA_NETWORK_IPV6) {
+			struct in6_addr *saddr6;
+			struct in6_addr *daddr6;
+
+			saddr6 = &av->sgid_addr._sockaddr_in6.sin6_addr;
+			daddr6 = &av->dgid_addr._sockaddr_in6.sin6_addr;
+			dst = rxe_find_route6(rxe->ndev, saddr6, daddr6);
+		}
+	}
+
+	return dst;
+}
+
 static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 {
 	struct udphdr *udph;
@@ -301,7 +334,7 @@ static void prepare_ipv4_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	skb_scrub_packet(skb, xnet);
 
 	skb_clear_hash(skb);
-	skb_dst_set(skb, dst);
+	skb_dst_set(skb, dst_clone(dst));
 	memset(IPCB(skb), 0, sizeof(*IPCB(skb)));
 
 	skb_push(skb, sizeof(struct iphdr));
@@ -349,13 +382,14 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 		    struct sk_buff *skb, struct rxe_av *av)
 {
+	struct rxe_qp *qp = pkt->qp;
 	struct dst_entry *dst;
 	bool xnet = false;
 	__be16 df = htons(IP_DF);
 	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
 
-	dst = rxe_find_route4(rxe->ndev, saddr, daddr);
+	dst = rxe_find_route(rxe, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
 		return -EHOSTUNREACH;
@@ -369,17 +403,24 @@ static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 
 	prepare_ipv4_hdr(dst, skb, saddr->s_addr, daddr->s_addr, IPPROTO_UDP,
 			 av->grh.traffic_class, av->grh.hop_limit, df, xnet);
+
+	if (qp_type(qp) == IB_QPT_RC)
+		sk_dst_set(qp->sk->sk, dst);
+	else
+		dst_release(dst);
+
 	return 0;
 }
 
 static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 		    struct sk_buff *skb, struct rxe_av *av)
 {
-	struct dst_entry *dst;
+	struct rxe_qp *qp = pkt->qp;
+	struct dst_entry *dst = NULL;
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
 
-	dst = rxe_find_route6(rxe->ndev, saddr, daddr);
+	dst = rxe_find_route(rxe, qp, av);
 	if (!dst) {
 		pr_err("Host not reachable\n");
 		return -EHOSTUNREACH;
@@ -394,6 +435,12 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	prepare_ipv6_hdr(dst, skb, saddr, daddr, IPPROTO_UDP,
 			 av->grh.traffic_class,
 			 av->grh.hop_limit);
+
+	if (qp_type(qp) == IB_QPT_RC)
+		sk_dst_set(qp->sk->sk, dst);
+	else
+		dst_release(dst);
+
 	return 0;
 }
 

commit 4cc4b9323f43458c9277e082f90316570431881e
Merge: a57eaa1f25bb db690328a7df
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 23 08:27:57 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "First set of updates for 4.11 kernel merge window
    
       - Add new Broadcom bnxt_re RoCE driver
       - rxe driver updates
       - ioctl cleanups
       - ETH_P_IBOE declaration cleanup
       - IPoIB changes
       - Add port state cache
       - Allow srpt driver to accept guids as port names in config
       - Update to hfi1 driver
       - Update to srp driver
       - Lots of misc minor changes all over"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (114 commits)
      RDMA/bnxt_re: fix for "bnxt_en: Update to firmware interface spec 1.7.0."
      rdma_cm: fail iwarp accepts w/o connection params
      IB/srp: Drain the send queue before destroying a QP
      IB/core: Add support for draining IB_POLL_DIRECT completion queues
      IB/srp: Improve an error path
      IB/srp: Make a diagnostic message more informative
      IB/srp: Document locking conventions
      IB/srp: Fix race conditions related to task management
      IB/srp: Avoid that duplicate responses trigger a kernel bug
      IB/SRP: Avoid using IB_MR_TYPE_SG_GAPS
      RDMA/qedr: Fix some error handling
      RDMA/bnxt_re: add DCB dependency
      IB/hns: include linux/module.h
      IB/vmw_pvrdma: Expose vendor error to ULPs
      vmw_pvrdma: switch to pci_alloc_irq_vectors
      IB/hfi1: use size_t for passing array length
      IB/ipoib: Remove redudant label
      IB/ipoib: remove the unnecessary memory free
      IB/mthca: switch to pci_alloc_irq_vectors
      IB/hfi1: Code reuse with memdup_copy
      ...

commit 6dd7abae7110da6fa01f048baf5b679b5a4a56dd
Merge: 6df6b4a9ce43 646ebd4166ca
Author: Doug Ledford <dledford@redhat.com>
Date:   Sun Feb 19 09:18:21 2017 -0500

    Merge branch 'k.o/for-4.10-rc' into HEAD

commit d0d7b10b05945f40fefd4e60f457c61aefa3e9a9
Author: Parav Pandit <parav@mellanox.com>
Date:   Sat Feb 4 11:00:49 2017 -0600

    net-next: treewide use is_vlan_dev() helper function.
    
    This patch makes use of is_vlan_dev() function instead of flag
    comparison which is exactly done by is_vlan_dev() helper function.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Acked-by: Stephen Hemminger <stephen@networkplumber.org>
    Acked-by: Jon Maxwell <jmaxwell37@gmail.com>
    Acked-by: Johannes Thumshirn <jth@kernel.org>
    Acked-by: Haiyang Zhang <haiyangz@microsoft.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 4abdeb359fb4..d9d15561eb5d 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -118,7 +118,7 @@ static struct device *dma_device(struct rxe_dev *rxe)
 
 	ndev = rxe->ndev;
 
-	if (ndev->priv_flags & IFF_802_1Q_VLAN)
+	if (is_vlan_dev(ndev))
 		ndev = vlan_dev_real_dev(ndev);
 
 	return ndev->dev.parent;

commit f39f775218a7520e3700de2003c84a042c3b5972
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Jan 19 15:25:58 2017 +0200

    IB/rxe: Fix rxe dev insertion to rxe_dev_list
    
    The first argument of list_add_tail is the new item and the second
    is the head of the list. Fix the code to pass arguments in the
    right order, otherwise not all the rxe devices will be removed
    during teardown.
    
    Fixes: 8700e3e7c4857 ('Soft RoCE driver')
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 342e78163613..4abdeb359fb4 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -555,7 +555,7 @@ struct rxe_dev *rxe_net_add(struct net_device *ndev)
 	}
 
 	spin_lock_bh(&dev_list_lock);
-	list_add_tail(&rxe_dev_list, &rxe->list);
+	list_add_tail(&rxe->list, &rxe_dev_list);
 	spin_unlock_bh(&dev_list_lock);
 	return rxe;
 }

commit 839f5ac0d806970a102117be03ef05272c50d20e
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jan 10 11:15:53 2017 -0800

    IB/rxe: Remove a pointless indirection layer
    
    Neither rxe->ifc_ops nor any of the function pointers in struct
    struct rxe_ifc_ops ever change. Hence remove the rxe->ifc_ops
    indirection mechanism.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Andrew Boyer <andrew.boyer@dell.com>
    Cc: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 151f639abebf..50144c307eb4 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -102,17 +102,17 @@ static __be64 rxe_mac_to_eui64(struct net_device *ndev)
 	return eui64;
 }
 
-static __be64 node_guid(struct rxe_dev *rxe)
+__be64 rxe_node_guid(struct rxe_dev *rxe)
 {
 	return rxe_mac_to_eui64(rxe->ndev);
 }
 
-static __be64 port_guid(struct rxe_dev *rxe)
+__be64 rxe_port_guid(struct rxe_dev *rxe)
 {
 	return rxe_mac_to_eui64(rxe->ndev);
 }
 
-static struct device *dma_device(struct rxe_dev *rxe)
+struct device *rxe_dma_device(struct rxe_dev *rxe)
 {
 	struct net_device *ndev;
 
@@ -124,7 +124,7 @@ static struct device *dma_device(struct rxe_dev *rxe)
 	return ndev->dev.parent;
 }
 
-static int mcast_add(struct rxe_dev *rxe, union ib_gid *mgid)
+int rxe_mcast_add(struct rxe_dev *rxe, union ib_gid *mgid)
 {
 	int err;
 	unsigned char ll_addr[ETH_ALEN];
@@ -135,7 +135,7 @@ static int mcast_add(struct rxe_dev *rxe, union ib_gid *mgid)
 	return err;
 }
 
-static int mcast_delete(struct rxe_dev *rxe, union ib_gid *mgid)
+int rxe_mcast_delete(struct rxe_dev *rxe, union ib_gid *mgid)
 {
 	int err;
 	unsigned char ll_addr[ETH_ALEN];
@@ -397,8 +397,8 @@ static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	return 0;
 }
 
-static int prepare(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		   struct sk_buff *skb, u32 *crc)
+int rxe_prepare(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		struct sk_buff *skb, u32 *crc)
 {
 	int err = 0;
 	struct rxe_av *av = rxe_get_av(pkt);
@@ -424,8 +424,7 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 		rxe_run_task(&qp->req.task, 1);
 }
 
-static int send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		struct sk_buff *skb)
+int rxe_send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	struct sk_buff *nskb;
 	struct rxe_av *av;
@@ -461,7 +460,7 @@ static int send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	return 0;
 }
 
-static int loopback(struct sk_buff *skb)
+int rxe_loopback(struct sk_buff *skb)
 {
 	return rxe_rcv(skb);
 }
@@ -471,8 +470,8 @@ static inline int addr_same(struct rxe_dev *rxe, struct rxe_av *av)
 	return rxe->port.port_guid == av->grh.dgid.global.interface_id;
 }
 
-static struct sk_buff *init_packet(struct rxe_dev *rxe, struct rxe_av *av,
-				   int paylen, struct rxe_pkt_info *pkt)
+struct sk_buff *rxe_init_packet(struct rxe_dev *rxe, struct rxe_av *av,
+				int paylen, struct rxe_pkt_info *pkt)
 {
 	unsigned int hdr_len;
 	struct sk_buff *skb;
@@ -511,31 +510,16 @@ static struct sk_buff *init_packet(struct rxe_dev *rxe, struct rxe_av *av,
  * this is required by rxe_cfg to match rxe devices in
  * /sys/class/infiniband up with their underlying ethernet devices
  */
-static char *parent_name(struct rxe_dev *rxe, unsigned int port_num)
+const char *rxe_parent_name(struct rxe_dev *rxe, unsigned int port_num)
 {
 	return rxe->ndev->name;
 }
 
-static enum rdma_link_layer link_layer(struct rxe_dev *rxe,
-				       unsigned int port_num)
+enum rdma_link_layer rxe_link_layer(struct rxe_dev *rxe, unsigned int port_num)
 {
 	return IB_LINK_LAYER_ETHERNET;
 }
 
-static struct rxe_ifc_ops ifc_ops = {
-	.node_guid	= node_guid,
-	.port_guid	= port_guid,
-	.dma_device	= dma_device,
-	.mcast_add	= mcast_add,
-	.mcast_delete	= mcast_delete,
-	.prepare	= prepare,
-	.send		= send,
-	.loopback	= loopback,
-	.init_packet	= init_packet,
-	.parent_name	= parent_name,
-	.link_layer	= link_layer,
-};
-
 struct rxe_dev *rxe_net_add(struct net_device *ndev)
 {
 	int err;
@@ -545,7 +529,6 @@ struct rxe_dev *rxe_net_add(struct net_device *ndev)
 	if (!rxe)
 		return NULL;
 
-	rxe->ifc_ops = &ifc_ops;
 	rxe->ndev = ndev;
 
 	err = rxe_add(rxe, ndev->mtu);

commit 8d8f08372091ae3ea92930ebd0d2c33e8bc199c5
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jan 10 11:15:40 2017 -0800

    IB/rxe: Suppress sparse warnings
    
    Avoid that sparse complains about using 0 as a pointer, about
    missing function declarations and also avoid that sparse complains
    about endianness.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Andrew Boyer <andrew.boyer@dell.com>
    Cc: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 342e78163613..151f639abebf 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -243,8 +243,8 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 {
 	int err;
 	struct socket *sock;
-	struct udp_port_cfg udp_cfg = {0};
-	struct udp_tunnel_sock_cfg tnl_cfg = {0};
+	struct udp_port_cfg udp_cfg = { };
+	struct udp_tunnel_sock_cfg tnl_cfg = { };
 
 	if (ipv6) {
 		udp_cfg.family = AF_INET6;
@@ -658,7 +658,7 @@ struct notifier_block rxe_net_notifier = {
 	.notifier_call = rxe_notify,
 };
 
-int rxe_net_ipv4_init(void)
+static int rxe_net_ipv4_init(void)
 {
 	recv_sockets.sk4 = rxe_setup_udp_tunnel(&init_net,
 				htons(ROCE_V2_UDP_DPORT), false);
@@ -671,7 +671,7 @@ int rxe_net_ipv4_init(void)
 	return 0;
 }
 
-int rxe_net_ipv6_init(void)
+static int rxe_net_ipv6_init(void)
 {
 #if IS_ENABLED(CONFIG_IPV6)
 

commit 296915912d89d1ed2f47472b67fc594b15383d71
Merge: f290cbacb697 5cc8fabc5e4c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 23 10:38:48 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma fixes from Doug Ledford:
     "First round of -rc fixes for 4.10 kernel:
    
       - a series of qedr fixes
       - a series of rxe fixes
       - one i40iw fix
       - one cma fix
       - one cxgb4 fix"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma:
      IB/rxe: Don't check for null ptr in send()
      IB/rxe: Drop future atomic/read packets rather than retrying
      IB/rxe: Use BTH_PSN_MASK when ACKing duplicate sends
      qedr: Always notify the verb consumer of flushed CQEs
      qedr: clear the vendor error field in the work completion
      qedr: post_send/recv according to QP state
      qedr: ignore inline flag in read verbs
      qedr: modify QP state to error when destroying it
      qedr: return correct value on modify qp
      qedr: return error if destroy CQ failed
      qedr: configure the number of CQEs on CQ creation
      i40iw: Set 128B as the only supported RQ WQE size
      IB/cma: Fix a race condition in iboe_addr_get_sgid()
      IB/rxe: Fix a memory leak in rxe_qp_cleanup()
      iw_cxgb4: set correct FetchBurstMax for QPs

commit 5cc8fabc5e4c588c75a5ec21423e7c3425f69f48
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Thu Dec 22 08:54:38 2016 -0500

    IB/rxe: Don't check for null ptr in send()
    
    pkt->qp was already dereferenced earlier in the function.
    
    Fixes Smatch complaint:
    drivers/infiniband/sw/rxe/rxe_net.c:458 send()
             warn: variable dereferenced before check 'pkt->qp' (see line 441)
    
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index a576603304f7..10ec699834fa 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -459,8 +459,7 @@ static int send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 		return -EAGAIN;
 	}
 
-	if (pkt->qp)
-		atomic_inc(&pkt->qp->skb_out);
+	atomic_inc(&pkt->qp->skb_out);
 	kfree_skb(skb);
 
 	return 0;

commit 4d5b57e05a67c3cfd8e2b2a64ca356245a15b1c6
Merge: 6df8b74b1720 6f94ba20799b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 15 12:03:32 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "This is the complete update for the rdma stack for this release cycle.
    
      Most of it is typical driver and core updates, but there is the
      entirely new VMWare pvrdma driver. You may have noticed that there
      were changes in DaveM's pull request to the bnxt Ethernet driver to
      support a RoCE RDMA driver. The bnxt_re driver was tentatively set to
      be pulled in this release cycle, but it simply wasn't ready in time
      and was dropped (a few review comments still to address, and some
      multi-arch build issues like prefetch() not working across all
      arches).
    
      Summary:
    
       - shared mlx5 updates with net stack (will drop out on merge if
         Dave's tree has already been merged)
    
       - driver updates: cxgb4, hfi1, hns-roce, i40iw, mlx4, mlx5, qedr, rxe
    
       - debug cleanups
    
       - new connection rejection helpers
    
       - SRP updates
    
       - various misc fixes
    
       - new paravirt driver from vmware"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (210 commits)
      IB: Add vmw_pvrdma driver
      IB/mlx4: fix improper return value
      IB/ocrdma: fix bad initialization
      infiniband: nes: return value of skb_linearize should be handled
      MAINTAINERS: Update Intel RDMA RNIC driver maintainers
      MAINTAINERS: Remove Mitesh Ahuja from emulex maintainers
      IB/core: fix unmap_sg argument
      qede: fix general protection fault may occur on probe
      IB/mthca: Replace pci_pool_alloc by pci_pool_zalloc
      mlx5, calc_sq_size(): Make a debug message more informative
      mlx5: Remove a set-but-not-used variable
      mlx5: Use { } instead of { 0 } to init struct
      IB/srp: Make writing the add_target sysfs attr interruptible
      IB/srp: Make mapping failures easier to debug
      IB/srp: Make login failures easier to debug
      IB/srp: Introduce a local variable in srp_add_one()
      IB/srp: Fix CONFIG_DYNAMIC_DEBUG=n build
      IB/multicast: Check ib_find_pkey() return value
      IPoIB: Avoid reading an uninitialized member variable
      IB/mad: Fix an array index check
      ...

commit d38eb801aa145aedf4b97e8e0bb2e65763aa6149
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Wed Nov 23 12:39:20 2016 -0500

    IB/rxe: Unblock loopback by moving skb_out increment
    
    skb_out is decremented in rxe_skb_tx_dtor(), which is not called in the
    loopback() path. Move the increment to the send() path rather than
    rxe_xmit_packet().
    
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Acked-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 4cb63780fa74..a576603304f7 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -459,6 +459,8 @@ static int send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 		return -EAGAIN;
 	}
 
+	if (pkt->qp)
+		atomic_inc(&pkt->qp->skb_out);
 	kfree_skb(skb);
 
 	return 0;

commit 4ac4707102d9ea1ffc9b5735891f6c5ee3d236e5
Author: Wei Yongjun <weiyongjun1@huawei.com>
Date:   Sat Oct 29 16:19:33 2016 +0000

    IB/rxe: Use DEFINE_SPINLOCK() for spinlock
    
    spinlock can be initialized automatically with DEFINE_SPINLOCK()
    rather than explicitly calling spin_lock_init().
    
    Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
    Reviewed-by: Leon Romanosky <leonro@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index b8258e4f0aea..4cb63780fa74 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -46,7 +46,7 @@
 #include "rxe_loc.h"
 
 static LIST_HEAD(rxe_dev_list);
-static spinlock_t dev_list_lock; /* spinlock for device list */
+static DEFINE_SPINLOCK(dev_list_lock); /* spinlock for device list */
 
 struct rxe_dev *net_to_rxe(struct net_device *ndev)
 {
@@ -663,8 +663,6 @@ struct notifier_block rxe_net_notifier = {
 
 int rxe_net_ipv4_init(void)
 {
-	spin_lock_init(&dev_list_lock);
-
 	recv_sockets.sk4 = rxe_setup_udp_tunnel(&init_net,
 				htons(ROCE_V2_UDP_DPORT), false);
 	if (IS_ERR(recv_sockets.sk4)) {
@@ -680,8 +678,6 @@ int rxe_net_ipv6_init(void)
 {
 #if IS_ENABLED(CONFIG_IPV6)
 
-	spin_lock_init(&dev_list_lock);
-
 	recv_sockets.sk6 = rxe_setup_udp_tunnel(&init_net,
 						htons(ROCE_V2_UDP_DPORT), true);
 	if (IS_ERR(recv_sockets.sk6)) {

commit 1454ca3a97e147bb91e98b087446c39cf6692a48
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Wed Nov 16 10:39:14 2016 +0200

    IB/rxe: Fix kernel panic in UDP tunnel with GRO and RX checksum
    
    Missing initialization of udp_tunnel_sock_cfg causes to following
    kernel panic, while kernel tries to execute gro_receive().
    
    While being there, we converted udp_port_cfg to use the same
    initialization scheme as udp_tunnel_sock_cfg.
    
    ------------[ cut here ]------------
    kernel tried to execute NX-protected page - exploit attempt? (uid: 0)
    BUG: unable to handle kernel paging request at ffffffffa0588c50
    IP: [<ffffffffa0588c50>] __this_module+0x50/0xffffffffffff8400 [ib_rxe]
    PGD 1c09067 PUD 1c0a063 PMD bb394067 PTE 80000000ad5e8163
    Oops: 0011 [#1] SMP
    Modules linked in: ib_rxe ip6_udp_tunnel udp_tunnel
    CPU: 5 PID: 0 Comm: swapper/5 Not tainted 4.7.0-rc3+ #2
    Hardware name: Red Hat KVM, BIOS Bochs 01/01/2011
    task: ffff880235e4e680 ti: ffff880235e68000 task.ti: ffff880235e68000
    RIP: 0010:[<ffffffffa0588c50>]
    [<ffffffffa0588c50>] __this_module+0x50/0xffffffffffff8400 [ib_rxe]
    RSP: 0018:ffff880237343c80  EFLAGS: 00010282
    RAX: 00000000dffe482d RBX: ffff8800ae330900 RCX: 000000002001b712
    RDX: ffff8800ae330900 RSI: ffff8800ae102578 RDI: ffff880235589c00
    RBP: ffff880237343cb0 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: ffff8800ae33e262
    R13: ffff880235589c00 R14: 0000000000000014 R15: ffff8800ae102578
    FS:  0000000000000000(0000) GS:ffff880237340000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: ffffffffa0588c50 CR3: 0000000001c06000 CR4: 00000000000006e0
    Stack:
    ffffffff8160860e ffff8800ae330900 ffff8800ae102578 0000000000000014
    000000000000004e ffff8800ae102578 ffff880237343ce0 ffffffff816088fb
    0000000000000000 ffff8800ae330900 0000000000000000 00000000ffad0000
    Call Trace:
    <IRQ>
    [<ffffffff8160860e>] ? udp_gro_receive+0xde/0x130
    [<ffffffff816088fb>] udp4_gro_receive+0x10b/0x2d0
    [<ffffffff81611373>] inet_gro_receive+0x1d3/0x270
    [<ffffffff81594e29>] dev_gro_receive+0x269/0x3b0
    [<ffffffff81595188>] napi_gro_receive+0x38/0x120
    [<ffffffffa011caee>] mlx5e_handle_rx_cqe+0x27e/0x340 [mlx5_core]
    [<ffffffffa011d076>] mlx5e_poll_rx_cq+0x66/0x6d0 [mlx5_core]
    [<ffffffffa011d7ae>] mlx5e_napi_poll+0x8e/0x400 [mlx5_core]
    [<ffffffff815949a0>] net_rx_action+0x160/0x380
    [<ffffffff816a9197>] __do_softirq+0xd7/0x2c5
    [<ffffffff81085c35>] irq_exit+0xf5/0x100
    [<ffffffff816a8f16>] do_IRQ+0x56/0xd0
    [<ffffffff816a6dcc>] common_interrupt+0x8c/0x8c
    <EOI>
    [<ffffffff81061f96>] ? native_safe_halt+0x6/0x10
    [<ffffffff81037ade>] default_idle+0x1e/0xd0
    [<ffffffff8103828f>] arch_cpu_idle+0xf/0x20
    [<ffffffff810c37dc>] default_idle_call+0x3c/0x50
    [<ffffffff810c3b13>] cpu_startup_entry+0x323/0x3c0
    [<ffffffff81050d8c>] start_secondary+0x15c/0x1a0
    RIP  [<ffffffffa0588c50>] __this_module+0x50/0xffffffffffff8400 [ib_rxe]
    RSP <ffff880237343c80>
    CR2: ffffffffa0588c50
    ---[ end trace 489ee31fa7614ac5 ]---
    Kernel panic - not syncing: Fatal exception in interrupt
    Kernel Offset: disabled
    ---[ end Kernel panic - not syncing: Fatal exception in interrupt
    ------------[ cut here ]------------
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index b8258e4f0aea..ffff5a54cb34 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -243,10 +243,8 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 {
 	int err;
 	struct socket *sock;
-	struct udp_port_cfg udp_cfg;
-	struct udp_tunnel_sock_cfg tnl_cfg;
-
-	memset(&udp_cfg, 0, sizeof(udp_cfg));
+	struct udp_port_cfg udp_cfg = {0};
+	struct udp_tunnel_sock_cfg tnl_cfg = {0};
 
 	if (ipv6) {
 		udp_cfg.family = AF_INET6;
@@ -264,10 +262,8 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 		return ERR_PTR(err);
 	}
 
-	tnl_cfg.sk_user_data = NULL;
 	tnl_cfg.encap_type = 1;
 	tnl_cfg.encap_rcv = rxe_udp_encap_recv;
-	tnl_cfg.encap_destroy = NULL;
 
 	/* Setup UDP tunnel */
 	setup_udp_tunnel_sock(net, sock, &tnl_cfg);

commit e404f945a6101a6f0f2a29aade6dc32015471daa
Author: Parav Pandit <pandit.parav@gmail.com>
Date:   Wed Sep 28 20:26:26 2016 +0000

    IB/rxe: improved debug prints & code cleanup
    
    1. Debugging qp state transitions and qp errors in loopback and
    multiple QP tests is difficult without qp numbers in debug logs.
    This patch adds qp number to important debug logs.
    
    2. Instead of having rxe: prefix in few logs and not having in
    few logs, using uniform module name prefix using pr_fmt macro.
    
    3. Code cleanup for various warnings reported by checkpatch for
    incomplete unsigned data type, line over 80 characters, return
    statements.
    
    Signed-off-by: Parav Pandit <pandit.parav@gmail.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 549aa84f9f9f..b8258e4f0aea 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -65,7 +65,7 @@ struct rxe_dev *net_to_rxe(struct net_device *ndev)
 	return found;
 }
 
-struct rxe_dev *get_rxe_by_name(const char* name)
+struct rxe_dev *get_rxe_by_name(const char *name)
 {
 	struct rxe_dev *rxe;
 	struct rxe_dev *found = NULL;
@@ -601,8 +601,7 @@ void rxe_port_up(struct rxe_dev *rxe)
 	port->attr.phys_state = IB_PHYS_STATE_LINK_UP;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ACTIVE);
-	pr_info("rxe: set %s active\n", rxe->ib_dev.name);
-	return;
+	pr_info("set %s active\n", rxe->ib_dev.name);
 }
 
 /* Caller must hold net_info_lock */
@@ -615,8 +614,7 @@ void rxe_port_down(struct rxe_dev *rxe)
 	port->attr.phys_state = IB_PHYS_STATE_LINK_DOWN;
 
 	rxe_port_event(rxe, IB_EVENT_PORT_ERR);
-	pr_info("rxe: set %s down\n", rxe->ib_dev.name);
-	return;
+	pr_info("set %s down\n", rxe->ib_dev.name);
 }
 
 static int rxe_notify(struct notifier_block *not_blk,
@@ -641,7 +639,7 @@ static int rxe_notify(struct notifier_block *not_blk,
 		rxe_port_down(rxe);
 		break;
 	case NETDEV_CHANGEMTU:
-		pr_info("rxe: %s changed mtu to %d\n", ndev->name, ndev->mtu);
+		pr_info("%s changed mtu to %d\n", ndev->name, ndev->mtu);
 		rxe_set_mtu(rxe, ndev->mtu);
 		break;
 	case NETDEV_REBOOT:
@@ -651,7 +649,7 @@ static int rxe_notify(struct notifier_block *not_blk,
 	case NETDEV_CHANGENAME:
 	case NETDEV_FEAT_CHANGE:
 	default:
-		pr_info("rxe: ignoring netdev event = %ld for %s\n",
+		pr_info("ignoring netdev event = %ld for %s\n",
 			event, ndev->name);
 		break;
 	}
@@ -671,7 +669,7 @@ int rxe_net_ipv4_init(void)
 				htons(ROCE_V2_UDP_DPORT), false);
 	if (IS_ERR(recv_sockets.sk4)) {
 		recv_sockets.sk4 = NULL;
-		pr_err("rxe: Failed to create IPv4 UDP tunnel\n");
+		pr_err("Failed to create IPv4 UDP tunnel\n");
 		return -1;
 	}
 
@@ -688,7 +686,7 @@ int rxe_net_ipv6_init(void)
 						htons(ROCE_V2_UDP_DPORT), true);
 	if (IS_ERR(recv_sockets.sk6)) {
 		recv_sockets.sk6 = NULL;
-		pr_err("rxe: Failed to create IPv6 UDP tunnel\n");
+		pr_err("Failed to create IPv6 UDP tunnel\n");
 		return -1;
 	}
 #endif
@@ -701,3 +699,26 @@ void rxe_net_exit(void)
 	rxe_release_udp_tunnel(recv_sockets.sk4);
 	unregister_netdevice_notifier(&rxe_net_notifier);
 }
+
+int rxe_net_init(void)
+{
+	int err;
+
+	recv_sockets.sk6 = NULL;
+
+	err = rxe_net_ipv4_init();
+	if (err)
+		return err;
+	err = rxe_net_ipv6_init();
+	if (err)
+		goto err_out;
+	err = register_netdevice_notifier(&rxe_net_notifier);
+	if (err) {
+		pr_err("Failed to register netdev notifier\n");
+		goto err_out;
+	}
+	return 0;
+err_out:
+	rxe_net_exit();
+	return err;
+}

commit ffae955d49e6cff28589d6e7388bb9275f9ce2d1
Author: Parav Pandit <pandit.parav@gmail.com>
Date:   Wed Sep 28 20:24:42 2016 +0000

    IB/rxe: Fix sending out loopback packet on netdev interface.
    
    Both prepare4 and prepare6 sets loopback mask in pkt_info structure
    instance of skb.  The xmit_packet and other requester side functions
    use a pkt_info struct from the stack without the proper mask.  This
    results in sending out the packet to the actual netdev device and
    loopback functionality is broken.
    
    Modify prepare() to pass its correctly marked pkt_info struct to
    prepare4() and prepare6() instead of them using SKB_TO_PKT(skb) and
    getting an incorrectly set mask.
    
    Verified with perftest applications.
    
    Signed-off-by: Parav Pandit <pandit.parav@gmail.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index eedf2f1cafdf..549aa84f9f9f 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -350,14 +350,14 @@ static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
 	ip6h->payload_len = htons(skb->len - sizeof(*ip6h));
 }
 
-static int prepare4(struct rxe_dev *rxe, struct sk_buff *skb, struct rxe_av *av)
+static int prepare4(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		    struct sk_buff *skb, struct rxe_av *av)
 {
 	struct dst_entry *dst;
 	bool xnet = false;
 	__be16 df = htons(IP_DF);
 	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
 	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
-	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
 
 	dst = rxe_find_route4(rxe->ndev, saddr, daddr);
 	if (!dst) {
@@ -376,12 +376,12 @@ static int prepare4(struct rxe_dev *rxe, struct sk_buff *skb, struct rxe_av *av)
 	return 0;
 }
 
-static int prepare6(struct rxe_dev *rxe, struct sk_buff *skb, struct rxe_av *av)
+static int prepare6(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		    struct sk_buff *skb, struct rxe_av *av)
 {
 	struct dst_entry *dst;
 	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
 	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
-	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
 
 	dst = rxe_find_route6(rxe->ndev, saddr, daddr);
 	if (!dst) {
@@ -408,9 +408,9 @@ static int prepare(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
 	struct rxe_av *av = rxe_get_av(pkt);
 
 	if (av->network_type == RDMA_NETWORK_IPV4)
-		err = prepare4(rxe, skb, av);
+		err = prepare4(rxe, pkt, skb, av);
 	else if (av->network_type == RDMA_NETWORK_IPV6)
-		err = prepare6(rxe, skb, av);
+		err = prepare6(rxe, pkt, skb, av);
 
 	*crc = rxe_icrc_hdr(pkt, skb);
 

commit dfdd6158ca2c8838313470c5ab79243d04da70b2
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Wed Sep 7 14:04:04 2016 +0300

    IB/rxe: Fix kernel panic in udp_setup_tunnel
    
    Disable creation of a UDP socket for ipv6 when
    CONFIG_IPV6 is not enabeld. Since udp_sock_create6()
    returns 0 when CONFIG_IPV6 is not set
    
    [   46.888632] IP: [<c220705a>] setup_udp_tunnel_sock+0x6/0x4f
    [   46.891355] *pdpt = 0000000000000000 *pde = f000ff53f000ff53
    [   46.893918] Oops: 0002 [#1] PREEMPT
    [   46.896014] CPU: 0 PID: 1 Comm: swapper Not tainted 4.7.0-rc4-00001-g8700e3e #1
    [   46.900280] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Debian-1.8.2-1 04/01/2014
    [   46.904905] task: cf06c040 ti: cf05e000 task.ti: cf05e000
    [   46.907854] EIP: 0060:[<c220705a>] EFLAGS: 00210246 CPU: 0
    [   46.911137] EIP is at setup_udp_tunnel_sock+0x6/0x4f
    [   46.914070] EAX: 00000044 EBX: 00000001 ECX: cf05fef0 EDX: ca8142e0
    [   46.917236] ESI: c2c4505b EDI: cf05fef0 EBP: cf05fed0 ESP: cf05fed0
    [   46.919836]  DS: 007b ES: 007b FS: 0000 GS: 00e0 SS: 0068
    [   46.922046] CR0: 80050033 CR2: 000001fc CR3: 02cec000 CR4: 000006b0
    [   46.924550] Stack:
    [   46.926014]  cf05ff10 c1fd4657 ca8142e0 0000000a 00000000 00000000 0000b712 00000008
    [   46.931274]  00000000 6bb5bd01 c1fd48de 00000000 00000000 cf05ff1c 00000000 00000000
    [   46.936122]  cf05ff1c c1fd4bdf 00000000 cf05ff28 c2c4507b ffffffff cf05ff88 c2bf1c74
    [   46.942350] Call Trace:
    [   46.944403]  [<c1fd4657>] rxe_setup_udp_tunnel+0x8f/0x99
    [   46.947689]  [<c1fd48de>] ? net_to_rxe+0x4e/0x4e
    [   46.950567]  [<c1fd4bdf>] rxe_net_init+0xe/0xa4
    [   46.953147]  [<c2c4507b>] rxe_module_init+0x20/0x4c
    [   46.955448]  [<c2bf1c74>] do_one_initcall+0x89/0x113
    [   46.957797]  [<c2bf15eb>] ? set_debug_rodata+0xf/0xf
    [   46.959966]  [<c2bf1dbc>] ? kernel_init_freeable+0xbe/0x15b
    [   46.962262]  [<c2bf1ddc>] kernel_init_freeable+0xde/0x15b
    [   46.964418]  [<c232eb54>] kernel_init+0x8/0xd0
    [   46.966618]  [<c2333122>] ret_from_kernel_thread+0xe/0x24
    [   46.969592]  [<c232eb4c>] ? rest_init+0x6f/0x6f
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 0b8d2ea8b41d..eedf2f1cafdf 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -275,9 +275,10 @@ static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
 	return sock;
 }
 
-static void rxe_release_udp_tunnel(struct socket *sk)
+void rxe_release_udp_tunnel(struct socket *sk)
 {
-	udp_tunnel_sock_release(sk);
+	if (sk)
+		udp_tunnel_sock_release(sk);
 }
 
 static void prepare_udp_hdr(struct sk_buff *skb, __be16 src_port,
@@ -658,51 +659,45 @@ static int rxe_notify(struct notifier_block *not_blk,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block rxe_net_notifier = {
+struct notifier_block rxe_net_notifier = {
 	.notifier_call = rxe_notify,
 };
 
-int rxe_net_init(void)
+int rxe_net_ipv4_init(void)
 {
-	int err;
-
 	spin_lock_init(&dev_list_lock);
 
-	recv_sockets.sk6 = rxe_setup_udp_tunnel(&init_net,
-			htons(ROCE_V2_UDP_DPORT), true);
-	if (IS_ERR(recv_sockets.sk6)) {
-		recv_sockets.sk6 = NULL;
-		pr_err("rxe: Failed to create IPv6 UDP tunnel\n");
-		return -1;
-	}
-
 	recv_sockets.sk4 = rxe_setup_udp_tunnel(&init_net,
-			htons(ROCE_V2_UDP_DPORT), false);
+				htons(ROCE_V2_UDP_DPORT), false);
 	if (IS_ERR(recv_sockets.sk4)) {
-		rxe_release_udp_tunnel(recv_sockets.sk6);
 		recv_sockets.sk4 = NULL;
-		recv_sockets.sk6 = NULL;
 		pr_err("rxe: Failed to create IPv4 UDP tunnel\n");
 		return -1;
 	}
 
-	err = register_netdevice_notifier(&rxe_net_notifier);
-	if (err) {
-		rxe_release_udp_tunnel(recv_sockets.sk6);
-		rxe_release_udp_tunnel(recv_sockets.sk4);
-		pr_err("rxe: Failed to rigister netdev notifier\n");
-	}
-
-	return err;
+	return 0;
 }
 
-void rxe_net_exit(void)
+int rxe_net_ipv6_init(void)
 {
-	if (recv_sockets.sk6)
-		rxe_release_udp_tunnel(recv_sockets.sk6);
+#if IS_ENABLED(CONFIG_IPV6)
 
-	if (recv_sockets.sk4)
-		rxe_release_udp_tunnel(recv_sockets.sk4);
+	spin_lock_init(&dev_list_lock);
 
+	recv_sockets.sk6 = rxe_setup_udp_tunnel(&init_net,
+						htons(ROCE_V2_UDP_DPORT), true);
+	if (IS_ERR(recv_sockets.sk6)) {
+		recv_sockets.sk6 = NULL;
+		pr_err("rxe: Failed to create IPv6 UDP tunnel\n");
+		return -1;
+	}
+#endif
+	return 0;
+}
+
+void rxe_net_exit(void)
+{
+	rxe_release_udp_tunnel(recv_sockets.sk6);
+	rxe_release_udp_tunnel(recv_sockets.sk4);
 	unregister_netdevice_notifier(&rxe_net_notifier);
 }

commit 8700e3e7c4857d28ebaa824509934556da0b3e76
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Jun 16 16:45:23 2016 +0300

    Soft RoCE driver
    
    Soft RoCE (RXE) - The software RoCE driver
    
    ib_rxe implements the RDMA transport and registers to the RDMA core
    device as a kernel verbs provider. It also implements the packet IO
    layer. On the other hand ib_rxe registers to the Linux netdev stack
    as a udp encapsulating protocol, in that case RDMA, for sending and
    receiving packets over any Ethernet device.  This yields a RDMA
    transport over the UDP/Ethernet network layer forming a RoCEv2
    compatible device.
    
    The configuration procedure of the Soft RoCE drivers requires
    binding to any existing Ethernet network device. This is done with
    /sys interface.
    
    A userspace Soft RoCE library (librxe) provides user applications
    the ability to run with Soft RoCE devices.  The use of rxe verbs ins
    user space requires the inclusion of librxe as a device specifics
    plug-in to libibverbs. librxe is packaged separately.
    
    Architecture:
    
         +-----------------------------------------------------------+
         |                          Application                      |
         +-----------------------------------------------------------+
                                +-----------------------------------+
                                |             libibverbs            |
    User                        +-----------------------------------+
                                +----------------+ +----------------+
                                | librxe         | | HW RoCE lib    |
                                +----------------+ +----------------+
    +---------------------------------------------------------------+
         +--------------+                           +------------+
         | Sockets      |                           | RDMA ULP   |
         +--------------+                           +------------+
         +--------------+                  +---------------------+
         | TCP/IP       |                  | ib_core             |
         +--------------+                  +---------------------+
                                 +------------+ +----------------+
    Kernel                       | ib_rxe     | | HW RoCE driver |
                                 +------------+ +----------------+
         +------------------------------------+
         | NIC driver                         |
         +------------------------------------+
    
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         +-----------------------------------------------------------+
         |                          Application                      |
         +-----------------------------------------------------------+
                                +-----------------------------------+
                                |             libibverbs            |
    User                        +-----------------------------------+
                                +----------------+ +----------------+
                                | librxe         | | HW RoCE lib    |
                                +----------------+ +----------------+
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         +--------------+                           +------------+
         | Sockets      |                           | RDMA ULP   |
         +--------------+                           +------------+
         +--------------+                  +---------------------+
         | TCP/IP       |                  | ib_core             |
         +--------------+                  +---------------------+
                                 +------------+ +----------------+
    Kernel                       | ib_rxe     | | HW RoCE driver |
                                 +------------+ +----------------+
         +------------------------------------+
         | NIC driver                         |
         +------------------------------------+
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    Soft RoCE resources:
    
    [1[ https://github.com/SoftRoCE/librxe-dev librxe - source code in
    Github
    [2] https://github.com/SoftRoCE/rxe-dev/wiki/rxe-dev:-Home - Soft RoCE
    Wiki page
    [3] https://github.com/SoftRoCE/librxe-dev - Soft RoCE userspace library
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
new file mode 100644
index 000000000000..0b8d2ea8b41d
--- /dev/null
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -0,0 +1,708 @@
+/*
+ * Copyright (c) 2016 Mellanox Technologies Ltd. All rights reserved.
+ * Copyright (c) 2015 System Fabric Works, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *	- Redistributions of source code must retain the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer.
+ *
+ *	- Redistributions in binary form must reproduce the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer in the documentation and/or other materials
+ *	  provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/skbuff.h>
+#include <linux/if_arp.h>
+#include <linux/netdevice.h>
+#include <linux/if.h>
+#include <linux/if_vlan.h>
+#include <net/udp_tunnel.h>
+#include <net/sch_generic.h>
+#include <linux/netfilter.h>
+#include <rdma/ib_addr.h>
+
+#include "rxe.h"
+#include "rxe_net.h"
+#include "rxe_loc.h"
+
+static LIST_HEAD(rxe_dev_list);
+static spinlock_t dev_list_lock; /* spinlock for device list */
+
+struct rxe_dev *net_to_rxe(struct net_device *ndev)
+{
+	struct rxe_dev *rxe;
+	struct rxe_dev *found = NULL;
+
+	spin_lock_bh(&dev_list_lock);
+	list_for_each_entry(rxe, &rxe_dev_list, list) {
+		if (rxe->ndev == ndev) {
+			found = rxe;
+			break;
+		}
+	}
+	spin_unlock_bh(&dev_list_lock);
+
+	return found;
+}
+
+struct rxe_dev *get_rxe_by_name(const char* name)
+{
+	struct rxe_dev *rxe;
+	struct rxe_dev *found = NULL;
+
+	spin_lock_bh(&dev_list_lock);
+	list_for_each_entry(rxe, &rxe_dev_list, list) {
+		if (!strcmp(name, rxe->ib_dev.name)) {
+			found = rxe;
+			break;
+		}
+	}
+	spin_unlock_bh(&dev_list_lock);
+	return found;
+}
+
+
+struct rxe_recv_sockets recv_sockets;
+
+static __be64 rxe_mac_to_eui64(struct net_device *ndev)
+{
+	unsigned char *mac_addr = ndev->dev_addr;
+	__be64 eui64;
+	unsigned char *dst = (unsigned char *)&eui64;
+
+	dst[0] = mac_addr[0] ^ 2;
+	dst[1] = mac_addr[1];
+	dst[2] = mac_addr[2];
+	dst[3] = 0xff;
+	dst[4] = 0xfe;
+	dst[5] = mac_addr[3];
+	dst[6] = mac_addr[4];
+	dst[7] = mac_addr[5];
+
+	return eui64;
+}
+
+static __be64 node_guid(struct rxe_dev *rxe)
+{
+	return rxe_mac_to_eui64(rxe->ndev);
+}
+
+static __be64 port_guid(struct rxe_dev *rxe)
+{
+	return rxe_mac_to_eui64(rxe->ndev);
+}
+
+static struct device *dma_device(struct rxe_dev *rxe)
+{
+	struct net_device *ndev;
+
+	ndev = rxe->ndev;
+
+	if (ndev->priv_flags & IFF_802_1Q_VLAN)
+		ndev = vlan_dev_real_dev(ndev);
+
+	return ndev->dev.parent;
+}
+
+static int mcast_add(struct rxe_dev *rxe, union ib_gid *mgid)
+{
+	int err;
+	unsigned char ll_addr[ETH_ALEN];
+
+	ipv6_eth_mc_map((struct in6_addr *)mgid->raw, ll_addr);
+	err = dev_mc_add(rxe->ndev, ll_addr);
+
+	return err;
+}
+
+static int mcast_delete(struct rxe_dev *rxe, union ib_gid *mgid)
+{
+	int err;
+	unsigned char ll_addr[ETH_ALEN];
+
+	ipv6_eth_mc_map((struct in6_addr *)mgid->raw, ll_addr);
+	err = dev_mc_del(rxe->ndev, ll_addr);
+
+	return err;
+}
+
+static struct dst_entry *rxe_find_route4(struct net_device *ndev,
+				  struct in_addr *saddr,
+				  struct in_addr *daddr)
+{
+	struct rtable *rt;
+	struct flowi4 fl = { { 0 } };
+
+	memset(&fl, 0, sizeof(fl));
+	fl.flowi4_oif = ndev->ifindex;
+	memcpy(&fl.saddr, saddr, sizeof(*saddr));
+	memcpy(&fl.daddr, daddr, sizeof(*daddr));
+	fl.flowi4_proto = IPPROTO_UDP;
+
+	rt = ip_route_output_key(&init_net, &fl);
+	if (IS_ERR(rt)) {
+		pr_err_ratelimited("no route to %pI4\n", &daddr->s_addr);
+		return NULL;
+	}
+
+	return &rt->dst;
+}
+
+#if IS_ENABLED(CONFIG_IPV6)
+static struct dst_entry *rxe_find_route6(struct net_device *ndev,
+					 struct in6_addr *saddr,
+					 struct in6_addr *daddr)
+{
+	struct dst_entry *ndst;
+	struct flowi6 fl6 = { { 0 } };
+
+	memset(&fl6, 0, sizeof(fl6));
+	fl6.flowi6_oif = ndev->ifindex;
+	memcpy(&fl6.saddr, saddr, sizeof(*saddr));
+	memcpy(&fl6.daddr, daddr, sizeof(*daddr));
+	fl6.flowi6_proto = IPPROTO_UDP;
+
+	if (unlikely(ipv6_stub->ipv6_dst_lookup(sock_net(recv_sockets.sk6->sk),
+						recv_sockets.sk6->sk, &ndst, &fl6))) {
+		pr_err_ratelimited("no route to %pI6\n", daddr);
+		goto put;
+	}
+
+	if (unlikely(ndst->error)) {
+		pr_err("no route to %pI6\n", daddr);
+		goto put;
+	}
+
+	return ndst;
+put:
+	dst_release(ndst);
+	return NULL;
+}
+
+#else
+
+static struct dst_entry *rxe_find_route6(struct net_device *ndev,
+					 struct in6_addr *saddr,
+					 struct in6_addr *daddr)
+{
+	return NULL;
+}
+
+#endif
+
+static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct udphdr *udph;
+	struct net_device *ndev = skb->dev;
+	struct rxe_dev *rxe = net_to_rxe(ndev);
+	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
+
+	if (!rxe)
+		goto drop;
+
+	if (skb_linearize(skb)) {
+		pr_err("skb_linearize failed\n");
+		goto drop;
+	}
+
+	udph = udp_hdr(skb);
+	pkt->rxe = rxe;
+	pkt->port_num = 1;
+	pkt->hdr = (u8 *)(udph + 1);
+	pkt->mask = RXE_GRH_MASK;
+	pkt->paylen = be16_to_cpu(udph->len) - sizeof(*udph);
+
+	return rxe_rcv(skb);
+drop:
+	kfree_skb(skb);
+	return 0;
+}
+
+static struct socket *rxe_setup_udp_tunnel(struct net *net, __be16 port,
+					   bool ipv6)
+{
+	int err;
+	struct socket *sock;
+	struct udp_port_cfg udp_cfg;
+	struct udp_tunnel_sock_cfg tnl_cfg;
+
+	memset(&udp_cfg, 0, sizeof(udp_cfg));
+
+	if (ipv6) {
+		udp_cfg.family = AF_INET6;
+		udp_cfg.ipv6_v6only = 1;
+	} else {
+		udp_cfg.family = AF_INET;
+	}
+
+	udp_cfg.local_udp_port = port;
+
+	/* Create UDP socket */
+	err = udp_sock_create(net, &udp_cfg, &sock);
+	if (err < 0) {
+		pr_err("failed to create udp socket. err = %d\n", err);
+		return ERR_PTR(err);
+	}
+
+	tnl_cfg.sk_user_data = NULL;
+	tnl_cfg.encap_type = 1;
+	tnl_cfg.encap_rcv = rxe_udp_encap_recv;
+	tnl_cfg.encap_destroy = NULL;
+
+	/* Setup UDP tunnel */
+	setup_udp_tunnel_sock(net, sock, &tnl_cfg);
+
+	return sock;
+}
+
+static void rxe_release_udp_tunnel(struct socket *sk)
+{
+	udp_tunnel_sock_release(sk);
+}
+
+static void prepare_udp_hdr(struct sk_buff *skb, __be16 src_port,
+			    __be16 dst_port)
+{
+	struct udphdr *udph;
+
+	__skb_push(skb, sizeof(*udph));
+	skb_reset_transport_header(skb);
+	udph = udp_hdr(skb);
+
+	udph->dest = dst_port;
+	udph->source = src_port;
+	udph->len = htons(skb->len);
+	udph->check = 0;
+}
+
+static void prepare_ipv4_hdr(struct dst_entry *dst, struct sk_buff *skb,
+			     __be32 saddr, __be32 daddr, __u8 proto,
+			     __u8 tos, __u8 ttl, __be16 df, bool xnet)
+{
+	struct iphdr *iph;
+
+	skb_scrub_packet(skb, xnet);
+
+	skb_clear_hash(skb);
+	skb_dst_set(skb, dst);
+	memset(IPCB(skb), 0, sizeof(*IPCB(skb)));
+
+	skb_push(skb, sizeof(struct iphdr));
+	skb_reset_network_header(skb);
+
+	iph = ip_hdr(skb);
+
+	iph->version	=	IPVERSION;
+	iph->ihl	=	sizeof(struct iphdr) >> 2;
+	iph->frag_off	=	df;
+	iph->protocol	=	proto;
+	iph->tos	=	tos;
+	iph->daddr	=	daddr;
+	iph->saddr	=	saddr;
+	iph->ttl	=	ttl;
+	__ip_select_ident(dev_net(dst->dev), iph,
+			  skb_shinfo(skb)->gso_segs ?: 1);
+	iph->tot_len = htons(skb->len);
+	ip_send_check(iph);
+}
+
+static void prepare_ipv6_hdr(struct dst_entry *dst, struct sk_buff *skb,
+			     struct in6_addr *saddr, struct in6_addr *daddr,
+			     __u8 proto, __u8 prio, __u8 ttl)
+{
+	struct ipv6hdr *ip6h;
+
+	memset(&(IPCB(skb)->opt), 0, sizeof(IPCB(skb)->opt));
+	IPCB(skb)->flags &= ~(IPSKB_XFRM_TUNNEL_SIZE | IPSKB_XFRM_TRANSFORMED
+			    | IPSKB_REROUTED);
+	skb_dst_set(skb, dst);
+
+	__skb_push(skb, sizeof(*ip6h));
+	skb_reset_network_header(skb);
+	ip6h		  = ipv6_hdr(skb);
+	ip6_flow_hdr(ip6h, prio, htonl(0));
+	ip6h->payload_len = htons(skb->len);
+	ip6h->nexthdr     = proto;
+	ip6h->hop_limit   = ttl;
+	ip6h->daddr	  = *daddr;
+	ip6h->saddr	  = *saddr;
+	ip6h->payload_len = htons(skb->len - sizeof(*ip6h));
+}
+
+static int prepare4(struct rxe_dev *rxe, struct sk_buff *skb, struct rxe_av *av)
+{
+	struct dst_entry *dst;
+	bool xnet = false;
+	__be16 df = htons(IP_DF);
+	struct in_addr *saddr = &av->sgid_addr._sockaddr_in.sin_addr;
+	struct in_addr *daddr = &av->dgid_addr._sockaddr_in.sin_addr;
+	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
+
+	dst = rxe_find_route4(rxe->ndev, saddr, daddr);
+	if (!dst) {
+		pr_err("Host not reachable\n");
+		return -EHOSTUNREACH;
+	}
+
+	if (!memcmp(saddr, daddr, sizeof(*daddr)))
+		pkt->mask |= RXE_LOOPBACK_MASK;
+
+	prepare_udp_hdr(skb, htons(RXE_ROCE_V2_SPORT),
+			htons(ROCE_V2_UDP_DPORT));
+
+	prepare_ipv4_hdr(dst, skb, saddr->s_addr, daddr->s_addr, IPPROTO_UDP,
+			 av->grh.traffic_class, av->grh.hop_limit, df, xnet);
+	return 0;
+}
+
+static int prepare6(struct rxe_dev *rxe, struct sk_buff *skb, struct rxe_av *av)
+{
+	struct dst_entry *dst;
+	struct in6_addr *saddr = &av->sgid_addr._sockaddr_in6.sin6_addr;
+	struct in6_addr *daddr = &av->dgid_addr._sockaddr_in6.sin6_addr;
+	struct rxe_pkt_info *pkt = SKB_TO_PKT(skb);
+
+	dst = rxe_find_route6(rxe->ndev, saddr, daddr);
+	if (!dst) {
+		pr_err("Host not reachable\n");
+		return -EHOSTUNREACH;
+	}
+
+	if (!memcmp(saddr, daddr, sizeof(*daddr)))
+		pkt->mask |= RXE_LOOPBACK_MASK;
+
+	prepare_udp_hdr(skb, htons(RXE_ROCE_V2_SPORT),
+			htons(ROCE_V2_UDP_DPORT));
+
+	prepare_ipv6_hdr(dst, skb, saddr, daddr, IPPROTO_UDP,
+			 av->grh.traffic_class,
+			 av->grh.hop_limit);
+	return 0;
+}
+
+static int prepare(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		   struct sk_buff *skb, u32 *crc)
+{
+	int err = 0;
+	struct rxe_av *av = rxe_get_av(pkt);
+
+	if (av->network_type == RDMA_NETWORK_IPV4)
+		err = prepare4(rxe, skb, av);
+	else if (av->network_type == RDMA_NETWORK_IPV6)
+		err = prepare6(rxe, skb, av);
+
+	*crc = rxe_icrc_hdr(pkt, skb);
+
+	return err;
+}
+
+static void rxe_skb_tx_dtor(struct sk_buff *skb)
+{
+	struct sock *sk = skb->sk;
+	struct rxe_qp *qp = sk->sk_user_data;
+	int skb_out = atomic_dec_return(&qp->skb_out);
+
+	if (unlikely(qp->need_req_skb &&
+		     skb_out < RXE_INFLIGHT_SKBS_PER_QP_LOW))
+		rxe_run_task(&qp->req.task, 1);
+}
+
+static int send(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		struct sk_buff *skb)
+{
+	struct sk_buff *nskb;
+	struct rxe_av *av;
+	int err;
+
+	av = rxe_get_av(pkt);
+
+	nskb = skb_clone(skb, GFP_ATOMIC);
+	if (!nskb)
+		return -ENOMEM;
+
+	nskb->destructor = rxe_skb_tx_dtor;
+	nskb->sk = pkt->qp->sk->sk;
+
+	if (av->network_type == RDMA_NETWORK_IPV4) {
+		err = ip_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
+	} else if (av->network_type == RDMA_NETWORK_IPV6) {
+		err = ip6_local_out(dev_net(skb_dst(skb)->dev), nskb->sk, nskb);
+	} else {
+		pr_err("Unknown layer 3 protocol: %d\n", av->network_type);
+		kfree_skb(nskb);
+		return -EINVAL;
+	}
+
+	if (unlikely(net_xmit_eval(err))) {
+		pr_debug("error sending packet: %d\n", err);
+		return -EAGAIN;
+	}
+
+	kfree_skb(skb);
+
+	return 0;
+}
+
+static int loopback(struct sk_buff *skb)
+{
+	return rxe_rcv(skb);
+}
+
+static inline int addr_same(struct rxe_dev *rxe, struct rxe_av *av)
+{
+	return rxe->port.port_guid == av->grh.dgid.global.interface_id;
+}
+
+static struct sk_buff *init_packet(struct rxe_dev *rxe, struct rxe_av *av,
+				   int paylen, struct rxe_pkt_info *pkt)
+{
+	unsigned int hdr_len;
+	struct sk_buff *skb;
+
+	if (av->network_type == RDMA_NETWORK_IPV4)
+		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
+			sizeof(struct iphdr);
+	else
+		hdr_len = ETH_HLEN + sizeof(struct udphdr) +
+			sizeof(struct ipv6hdr);
+
+	skb = alloc_skb(paylen + hdr_len + LL_RESERVED_SPACE(rxe->ndev),
+			GFP_ATOMIC);
+	if (unlikely(!skb))
+		return NULL;
+
+	skb_reserve(skb, hdr_len + LL_RESERVED_SPACE(rxe->ndev));
+
+	skb->dev	= rxe->ndev;
+	if (av->network_type == RDMA_NETWORK_IPV4)
+		skb->protocol = htons(ETH_P_IP);
+	else
+		skb->protocol = htons(ETH_P_IPV6);
+
+	pkt->rxe	= rxe;
+	pkt->port_num	= 1;
+	pkt->hdr	= skb_put(skb, paylen);
+	pkt->mask	|= RXE_GRH_MASK;
+
+	memset(pkt->hdr, 0, paylen);
+
+	return skb;
+}
+
+/*
+ * this is required by rxe_cfg to match rxe devices in
+ * /sys/class/infiniband up with their underlying ethernet devices
+ */
+static char *parent_name(struct rxe_dev *rxe, unsigned int port_num)
+{
+	return rxe->ndev->name;
+}
+
+static enum rdma_link_layer link_layer(struct rxe_dev *rxe,
+				       unsigned int port_num)
+{
+	return IB_LINK_LAYER_ETHERNET;
+}
+
+static struct rxe_ifc_ops ifc_ops = {
+	.node_guid	= node_guid,
+	.port_guid	= port_guid,
+	.dma_device	= dma_device,
+	.mcast_add	= mcast_add,
+	.mcast_delete	= mcast_delete,
+	.prepare	= prepare,
+	.send		= send,
+	.loopback	= loopback,
+	.init_packet	= init_packet,
+	.parent_name	= parent_name,
+	.link_layer	= link_layer,
+};
+
+struct rxe_dev *rxe_net_add(struct net_device *ndev)
+{
+	int err;
+	struct rxe_dev *rxe = NULL;
+
+	rxe = (struct rxe_dev *)ib_alloc_device(sizeof(*rxe));
+	if (!rxe)
+		return NULL;
+
+	rxe->ifc_ops = &ifc_ops;
+	rxe->ndev = ndev;
+
+	err = rxe_add(rxe, ndev->mtu);
+	if (err) {
+		ib_dealloc_device(&rxe->ib_dev);
+		return NULL;
+	}
+
+	spin_lock_bh(&dev_list_lock);
+	list_add_tail(&rxe_dev_list, &rxe->list);
+	spin_unlock_bh(&dev_list_lock);
+	return rxe;
+}
+
+void rxe_remove_all(void)
+{
+	spin_lock_bh(&dev_list_lock);
+	while (!list_empty(&rxe_dev_list)) {
+		struct rxe_dev *rxe =
+			list_first_entry(&rxe_dev_list, struct rxe_dev, list);
+
+		list_del(&rxe->list);
+		spin_unlock_bh(&dev_list_lock);
+		rxe_remove(rxe);
+		spin_lock_bh(&dev_list_lock);
+	}
+	spin_unlock_bh(&dev_list_lock);
+}
+EXPORT_SYMBOL(rxe_remove_all);
+
+static void rxe_port_event(struct rxe_dev *rxe,
+			   enum ib_event_type event)
+{
+	struct ib_event ev;
+
+	ev.device = &rxe->ib_dev;
+	ev.element.port_num = 1;
+	ev.event = event;
+
+	ib_dispatch_event(&ev);
+}
+
+/* Caller must hold net_info_lock */
+void rxe_port_up(struct rxe_dev *rxe)
+{
+	struct rxe_port *port;
+
+	port = &rxe->port;
+	port->attr.state = IB_PORT_ACTIVE;
+	port->attr.phys_state = IB_PHYS_STATE_LINK_UP;
+
+	rxe_port_event(rxe, IB_EVENT_PORT_ACTIVE);
+	pr_info("rxe: set %s active\n", rxe->ib_dev.name);
+	return;
+}
+
+/* Caller must hold net_info_lock */
+void rxe_port_down(struct rxe_dev *rxe)
+{
+	struct rxe_port *port;
+
+	port = &rxe->port;
+	port->attr.state = IB_PORT_DOWN;
+	port->attr.phys_state = IB_PHYS_STATE_LINK_DOWN;
+
+	rxe_port_event(rxe, IB_EVENT_PORT_ERR);
+	pr_info("rxe: set %s down\n", rxe->ib_dev.name);
+	return;
+}
+
+static int rxe_notify(struct notifier_block *not_blk,
+		      unsigned long event,
+		      void *arg)
+{
+	struct net_device *ndev = netdev_notifier_info_to_dev(arg);
+	struct rxe_dev *rxe = net_to_rxe(ndev);
+
+	if (!rxe)
+		goto out;
+
+	switch (event) {
+	case NETDEV_UNREGISTER:
+		list_del(&rxe->list);
+		rxe_remove(rxe);
+		break;
+	case NETDEV_UP:
+		rxe_port_up(rxe);
+		break;
+	case NETDEV_DOWN:
+		rxe_port_down(rxe);
+		break;
+	case NETDEV_CHANGEMTU:
+		pr_info("rxe: %s changed mtu to %d\n", ndev->name, ndev->mtu);
+		rxe_set_mtu(rxe, ndev->mtu);
+		break;
+	case NETDEV_REBOOT:
+	case NETDEV_CHANGE:
+	case NETDEV_GOING_DOWN:
+	case NETDEV_CHANGEADDR:
+	case NETDEV_CHANGENAME:
+	case NETDEV_FEAT_CHANGE:
+	default:
+		pr_info("rxe: ignoring netdev event = %ld for %s\n",
+			event, ndev->name);
+		break;
+	}
+out:
+	return NOTIFY_OK;
+}
+
+static struct notifier_block rxe_net_notifier = {
+	.notifier_call = rxe_notify,
+};
+
+int rxe_net_init(void)
+{
+	int err;
+
+	spin_lock_init(&dev_list_lock);
+
+	recv_sockets.sk6 = rxe_setup_udp_tunnel(&init_net,
+			htons(ROCE_V2_UDP_DPORT), true);
+	if (IS_ERR(recv_sockets.sk6)) {
+		recv_sockets.sk6 = NULL;
+		pr_err("rxe: Failed to create IPv6 UDP tunnel\n");
+		return -1;
+	}
+
+	recv_sockets.sk4 = rxe_setup_udp_tunnel(&init_net,
+			htons(ROCE_V2_UDP_DPORT), false);
+	if (IS_ERR(recv_sockets.sk4)) {
+		rxe_release_udp_tunnel(recv_sockets.sk6);
+		recv_sockets.sk4 = NULL;
+		recv_sockets.sk6 = NULL;
+		pr_err("rxe: Failed to create IPv4 UDP tunnel\n");
+		return -1;
+	}
+
+	err = register_netdevice_notifier(&rxe_net_notifier);
+	if (err) {
+		rxe_release_udp_tunnel(recv_sockets.sk6);
+		rxe_release_udp_tunnel(recv_sockets.sk4);
+		pr_err("rxe: Failed to rigister netdev notifier\n");
+	}
+
+	return err;
+}
+
+void rxe_net_exit(void)
+{
+	if (recv_sockets.sk6)
+		rxe_release_udp_tunnel(recv_sockets.sk6);
+
+	if (recv_sockets.sk4)
+		rxe_release_udp_tunnel(recv_sockets.sk4);
+
+	unregister_netdevice_notifier(&rxe_net_notifier);
+}
