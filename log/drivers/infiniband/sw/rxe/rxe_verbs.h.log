commit 6ca18d8927d468c763571f78c9a7387a69ffa020
Author: Jiewei Ke <kejiewei.cn@gmail.com>
Date:   Fri Dec 27 19:36:13 2019 +0800

    RDMA/rxe: Fix error type of mmap_offset
    
    The type of mmap_offset should be u64 instead of int to match the type of
    mminfo.offset. If otherwise, after we create several thousands of CQs, it
    will run into overflow issues.
    
    Link: https://lore.kernel.org/r/20191227113613.5020-1-kejiewei.cn@gmail.com
    Signed-off-by: Jiewei Ke <kejiewei.cn@gmail.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 95834206c80c..92de39c4a7c1 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -408,7 +408,7 @@ struct rxe_dev {
 	struct list_head	pending_mmaps;
 
 	spinlock_t		mmap_offset_lock; /* guard mmap_offset */
-	int			mmap_offset;
+	u64			mmap_offset;
 
 	atomic64_t		stats_counters[RXE_NUM_OF_COUNTERS];
 

commit 97458fd510917f142c417245fa5701a892ce69d7
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Fri Oct 25 15:58:28 2019 -0700

    RDMA/rxe: Increase DMA max_segment_size parameter
    
    Increase the DMA max_segment_size parameter from 64 KB to 2 GB.
    
    Link: https://lore.kernel.org/r/20191025225830.257535-3-bvanassche@acm.org
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 5c4b2239129c..95834206c80c 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -384,6 +384,7 @@ struct rxe_port {
 struct rxe_dev {
 	struct ib_device	ib_dev;
 	struct ib_device_attr	attr;
+	struct device_dma_parameters dma_parms;
 	int			max_ucontext;
 	int			max_inline_data;
 	struct mutex	usdev_lock;

commit bdce1290493caa3f8119f24b5dacc3fb7ca27389
Author: Konstantin Taranov <konstantin.taranov@inf.ethz.ch>
Date:   Thu Jun 27 16:06:43 2019 +0200

    RDMA/rxe: Fill in wc byte_len with IB_WC_RECV_RDMA_WITH_IMM
    
    Calculate the correct byte_len on the receiving side when a work
    completion is generated with IB_WC_RECV_RDMA_WITH_IMM opcode.
    
    According to the IBA byte_len must indicate the number of written bytes,
    whereas it was always equal to zero for the IB_WC_RECV_RDMA_WITH_IMM
    opcode, even though data was transferred.
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Konstantin Taranov <konstantin.taranov@inf.ethz.ch>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 6c997d39a418..5c4b2239129c 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -213,6 +213,7 @@ struct rxe_resp_info {
 	struct rxe_mem		*mr;
 	u32			resid;
 	u32			rkey;
+	u32			length;
 	u64			atomic_orig;
 
 	/* SRQ only */

commit e39afe3d6dbd908d8fd189571a3c1561088a86c2
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 28 14:37:29 2019 +0300

    RDMA: Convert CQ allocations to be under core responsibility
    
    Ensure that CQ is allocated and freed by IB/core and not by drivers.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Tested-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index e8be7f44e3be..6c997d39a418 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -85,8 +85,8 @@ struct rxe_cqe {
 };
 
 struct rxe_cq {
-	struct rxe_pool_entry	pelem;
 	struct ib_cq		ibcq;
+	struct rxe_pool_entry	pelem;
 	struct rxe_queue	*queue;
 	spinlock_t		cq_lock;
 	u8			notify;

commit 68e326dea1dba935f6a5299a24343a58b33eed10
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:43 2019 +0300

    RDMA: Handle SRQ allocations by IB/core
    
    Convert SRQ allocation from drivers to be in the IB/core
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 23c5002b5134..e8be7f44e3be 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -120,8 +120,8 @@ struct rxe_rq {
 };
 
 struct rxe_srq {
-	struct rxe_pool_entry	pelem;
 	struct ib_srq		ibsrq;
+	struct rxe_pool_entry	pelem;
 	struct rxe_pd		*pd;
 	struct rxe_rq		rq;
 	u32			srq_num;

commit d345691471b426e540140a4cc431c69f80abfcb6
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:42 2019 +0300

    RDMA: Handle AH allocations by IB/core
    
    Simplify drivers by ensuring lifetime of ib_ah object. The changes
    in .create_ah() go hand in hand with relevant update in .destroy_ah().
    
    We will use this opportunity and convert .destroy_ah() to don't fail, as
    it was suggested a long time ago, because there is nothing to do in case
    of failure during destroy.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 157e51aeb1e1..23c5002b5134 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -71,8 +71,8 @@ struct rxe_pd {
 };
 
 struct rxe_ah {
-	struct rxe_pool_entry	pelem;
 	struct ib_ah		ibah;
+	struct rxe_pool_entry	pelem;
 	struct rxe_pd		*pd;
 	struct rxe_av		av;
 };

commit a2a074ef396f8738d9ee08ceefa8811381a4fe4f
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Feb 12 20:39:16 2019 +0200

    RDMA: Handle ucontext allocations by IB/core
    
    Following the PD conversion patch, do the same for ucontext allocations.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index d02eb75ef282..157e51aeb1e1 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -61,8 +61,8 @@ static inline int psn_compare(u32 psn_a, u32 psn_b)
 }
 
 struct rxe_ucontext {
+	struct ib_ucontext ibuc;
 	struct rxe_pool_entry	pelem;
-	struct ib_ucontext	ibuc;
 };
 
 struct rxe_pd {

commit 66920e1b25861d5bd99192489f6da7183529285e
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Fri Feb 15 11:03:57 2019 -0800

    rdma_rxe: Use netlink messages to add/delete links
    
    Add support for the RDMA_NLDEV_CMD_NEWLINK/DELLINK messages which allow
    dynamically adding new RXE links.  Deprecate the old module options for
    now.
    
    Cc: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yanjun Zhu <yanjun.zhu@oracle.com>
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index a22822526a62..d02eb75ef282 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -464,7 +464,7 @@ static inline struct rxe_mem *to_rmw(struct ib_mw *mw)
 	return mw ? container_of(mw, struct rxe_mem, ibmw) : NULL;
 }
 
-int rxe_register_device(struct rxe_dev *rxe);
+int rxe_register_device(struct rxe_dev *rxe, const char *ibdev_name);
 
 void rxe_mc_cleanup(struct rxe_pool_entry *arg);
 

commit 6cc2c8e535ec19153714cee62e11e4d9ac2ea953
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Feb 12 21:12:55 2019 -0700

    RDMA/rxe: Add ib_device_get_by_name() and use it in rxe
    
    rxe has an open coded version of this that is not as safe as the core
    version. This lets us eliminate the internal device list entirely from
    rxe.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index ad5574d8e8ba..a22822526a62 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -411,7 +411,6 @@ struct rxe_dev {
 	atomic64_t		stats_counters[RXE_NUM_OF_COUNTERS];
 
 	struct rxe_port		port;
-	struct list_head	list;
 	struct crypto_shash	*tfm;
 };
 

commit c367074b6c37c2d2d45cb60a15b829eefbcee0f1
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jan 22 16:27:24 2019 -0700

    RDMA/rxe: Use driver_unregister and new unregistration API
    
    rxe does not have correct locking for its registration/unregistration
    paths, use the core code to handle it instead. In this mode
    ib_unregister_device will also do the dealloc, so rxe is required to do
    clean up from a callback.
    
    The core code ensures that unregistration is done only once, and generally
    takes care of locking and concurrency problems for rxe.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 70839d3f55d9..ad5574d8e8ba 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -385,7 +385,6 @@ struct rxe_dev {
 	struct ib_device_attr	attr;
 	int			max_ucontext;
 	int			max_inline_data;
-	struct kref		ref_cnt;
 	struct mutex	usdev_lock;
 
 	struct net_device	*ndev;
@@ -467,7 +466,6 @@ static inline struct rxe_mem *to_rmw(struct ib_mw *mw)
 }
 
 int rxe_register_device(struct rxe_dev *rxe);
-void rxe_unregister_device(struct rxe_dev *rxe);
 
 void rxe_mc_cleanup(struct rxe_pool_entry *arg);
 

commit 21a428a019c9a6d133e745b529b9bf18c1187e70
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Feb 3 14:55:51 2019 +0200

    RDMA: Handle PD allocations by IB/core
    
    The PD allocations in IB/core allows us to simplify drivers and their
    error flows in their .alloc_pd() paths. The changes in .alloc_pd() go hand
    in had with relevant update in .dealloc_pd().
    
    We will use this opportunity and convert .dealloc_pd() to don't fail, as
    it was suggested a long time ago, failures are not happening as we have
    never seen a WARN_ON print.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 74e04801d34d..70839d3f55d9 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -66,8 +66,8 @@ struct rxe_ucontext {
 };
 
 struct rxe_pd {
+	struct ib_pd            ibpd;
 	struct rxe_pool_entry	pelem;
-	struct ib_pd		ibpd;
 };
 
 struct rxe_ah {

commit d5108e69fe013ff47ab815b849caba9cc33ca1e5
Author: Parav Pandit <parav@mellanox.com>
Date:   Fri Dec 14 00:32:42 2018 -0600

    IB/rxe: Make counters thread safe
    
    Current rxe device counters are not thread safe.
    When multiple QPs are used, they can be racy.
    Make them thread safe by making it atomic64.
    
    Fixes: 0b1e5b99a48b ("IB/rxe: Add port protocol stats")
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 831381b7788d..74e04801d34d 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -409,16 +409,16 @@ struct rxe_dev {
 	spinlock_t		mmap_offset_lock; /* guard mmap_offset */
 	int			mmap_offset;
 
-	u64			stats_counters[RXE_NUM_OF_COUNTERS];
+	atomic64_t		stats_counters[RXE_NUM_OF_COUNTERS];
 
 	struct rxe_port		port;
 	struct list_head	list;
 	struct crypto_shash	*tfm;
 };
 
-static inline void rxe_counter_inc(struct rxe_dev *rxe, enum rxe_counters cnt)
+static inline void rxe_counter_inc(struct rxe_dev *rxe, enum rxe_counters index)
 {
-	rxe->stats_counters[cnt]++;
+	atomic64_inc(&rxe->stats_counters[index]);
 }
 
 static inline struct rxe_dev *to_rdev(struct ib_device *dev)

commit 8c9959689b203a388e9a78413a6e669049a87bd2
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Sat Nov 3 08:13:19 2018 -0400

    IB/rxe: make rxe_unregister_device void
    
    Since the function rxe_unregister_device always returns 0, it is changed
    to void.
    
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 82e670d6eeea..831381b7788d 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -467,7 +467,7 @@ static inline struct rxe_mem *to_rmw(struct ib_mw *mw)
 }
 
 int rxe_register_device(struct rxe_dev *rxe);
-int rxe_unregister_device(struct rxe_dev *rxe);
+void rxe_unregister_device(struct rxe_dev *rxe);
 
 void rxe_mc_cleanup(struct rxe_pool_entry *arg);
 

commit 4e4c53df567714b3d08b2b5d8ccb1d175fc9be01
Author: Vijay Immanuel <vijayi@attalasystems.com>
Date:   Wed Jun 13 18:47:30 2018 -0700

    IB/rxe: avoid back-to-back retries
    
    Error retries can occur due to timeouts, NAKs or receiving
    packets beyond the current read request. Avoid back-to-back
    retries due to packet processing, by only retrying the initial
    attempt immediately. Subsequent retries must be due to timeouts.
    
    Continue to process completion packets after scheduling a retry.
    
    Signed-off-by: Vijay Immanuel <vijayi@attalasystems.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 979e987e3c46..82e670d6eeea 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -158,6 +158,7 @@ struct rxe_comp_info {
 	int			opcode;
 	int			timeout;
 	int			timeout_retry;
+	int			started_retry;
 	u32			retry_cnt;
 	u32			rnr_retry;
 	struct rxe_task		task;

commit b97db58557f4aa6d9903f8e1deea6b3d1ed0ba43
Author: Vijay Immanuel <vijayi@attalasystems.com>
Date:   Tue Jun 12 18:20:49 2018 -0700

    IB/rxe: fix for duplicate request processing and ack psns
    
    Don't reset the resp opcode for a replayed read response.
    The resp opcode could be in the middle of a write or send
    sequence, when the duplicate read request was received.
    An example sequence is as follows:
    - Receive read request for 12KB PSN 20. Transmit read response
      first, middle and last with PSNs 20,21,22.
    - Receive write first PSN 23.
      At this point the resp psn is 24 and resp opcode is write first.
    - The sender notices that PSN 20 is dropped and retransmits.
      Receive read request for 12KB PSN 20. Transmit read response
      first, middle and last with PSNs 20,21,22. The resp opcode is
      set to -1, the resp psn remains 24.
    - Receive write first PSN 23. This is processed by duplicate_request().
      The resp opcode remains -1 and resp psn remains 24.
    - Receive write middle PSN 24. check_op_seq() reports a missing
      first error since the resp opcode is -1.
    
    When sending an ack for a duplicate send or write request,
    use the psn of the previous ack sent. Do not use the psn
    of a read response for the ack.
    An example sequence is as follows:
    - Receive write PSN 30. Transmit ACK for PSN 30.
    - Receive read request 4KB PSN 31. Transmit read response with
      PSN 31. The resp psn is now 32.
    - The sender notices that PSN 30 is dropped and retransmits.
      Receive write PSN 30. duplicate_request() sends an ACK with
      PSN 31. That is incorrect since PSN 31 was a read request.
    
    Signed-off-by: Vijay Immanuel <vijayi@attalasystems.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 0d920087811a..979e987e3c46 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -171,6 +171,7 @@ enum rdatm_res_state {
 
 struct resp_res {
 	int			type;
+	int			replay;
 	u32			first_psn;
 	u32			last_psn;
 	u32			cur_psn;
@@ -195,6 +196,7 @@ struct rxe_resp_info {
 	enum rxe_qp_state	state;
 	u32			msn;
 	u32			psn;
+	u32			ack_psn;
 	int			opcode;
 	int			drop_msg;
 	int			goto_error;

commit d3c04a3a6870a4b96ee213d99b4f4ca723e64025
Author: Vijay Immanuel <vijayi@attalasystems.com>
Date:   Thu Jul 5 18:43:47 2018 -0700

    IB/rxe: vary the source udp port for receive scaling
    
    Select the source udp port number for a QP based on the
    source QPN. This provides a better spread of traffic
    across NIC RX queues for RC/UC QPs.
    
    Signed-off-by: Vijay Immanuel <vijayi@attalasystems.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index af1470d29391..0d920087811a 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -248,6 +248,7 @@ struct rxe_qp {
 
 	struct socket		*sk;
 	u32			dst_cookie;
+	u16			src_port;
 
 	struct rxe_av		pri_av;
 	struct rxe_av		alt_av;

commit c33bab622d47b7db55b387096c1a5c8e02f5bf37
Author: Hernán Gonzalez <hernan@vanguardiasur.com.ar>
Date:   Tue Feb 27 19:07:58 2018 -0300

    IB/rxe: Remove unused variable (char *rxe_qp_state_name[])
    
    Note: This is compile only tested as I have no access to the hw.  This
    variable was not used anywhere in the code. Removing it saves 24 bytes.
    
    add/remove: 0/1 grow/shrink: 0/0 up/down: 0/-24 (-24)
    Function                                     old     new   delta
    rxe_qp_state_name                             24       -     -24
    Total: Before=3348732, After=3348708, chg -0.00%
    
    Signed-off-by: Hernán Gonzalez <hernan@vanguardiasur.com.ar>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 1019f5e7dbdd..af1470d29391 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -139,8 +139,6 @@ enum rxe_qp_state {
 	QP_STATE_ERROR
 };
 
-extern char *rxe_qp_state_name[];
-
 struct rxe_req_info {
 	enum rxe_qp_state	state;
 	int			wqe_index;

commit bb3ffb7ad48a21e98a5c64eb21103a74fd9f03f6
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jan 12 15:11:59 2018 -0800

    RDMA/rxe: Fix rxe_qp_cleanup()
    
    rxe_qp_cleanup() can sleep so it must be run in thread context and
    not in atomic context. This patch avoids that the following bug is
    triggered:
    
    Kernel BUG at 00000000560033f3 [verbose debug info unavailable]
    BUG: sleeping function called from invalid context at net/core/sock.c:2761
    in_atomic(): 1, irqs_disabled(): 0, pid: 7, name: ksoftirqd/0
    INFO: lockdep is turned off.
    Preemption disabled at:
    [<00000000b6e69628>] __do_softirq+0x4e/0x540
    CPU: 0 PID: 7 Comm: ksoftirqd/0 Not tainted 4.15.0-rc7-dbg+ #4
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.0.0-prebuilt.qemu-project.org 04/01/2014
    Call Trace:
     dump_stack+0x85/0xbf
     ___might_sleep+0x177/0x260
     lock_sock_nested+0x1d/0x90
     inet_shutdown+0x2e/0xd0
     rxe_qp_cleanup+0x107/0x140 [rdma_rxe]
     rxe_elem_release+0x18/0x80 [rdma_rxe]
     rxe_requester+0x1cf/0x11b0 [rdma_rxe]
     rxe_do_task+0x78/0xf0 [rdma_rxe]
     tasklet_action+0x99/0x270
     __do_softirq+0xc0/0x540
     run_ksoftirqd+0x1c/0x70
     smpboot_thread_fn+0x1be/0x270
     kthread+0x117/0x130
     ret_from_fork+0x24/0x30
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Moni Shoua <monis@mellanox.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 0c2dbe45c729..1019f5e7dbdd 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -35,6 +35,7 @@
 #define RXE_VERBS_H
 
 #include <linux/interrupt.h>
+#include <linux/workqueue.h>
 #include <rdma/rdma_user_rxe.h>
 #include "rxe_pool.h"
 #include "rxe_task.h"
@@ -281,6 +282,8 @@ struct rxe_qp {
 	struct timer_list rnr_nak_timer;
 
 	spinlock_t		state_lock; /* guard requester and completer */
+
+	struct execute_work	cleanup_work;
 };
 
 enum rxe_mem_state {

commit b9109b7ddb13a52b379c3e57d9b1d74d7445e94d
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:53 2017 -0400

    IB/rxe: Fix destination cache for IPv6
    
    To successfully match an IPv6 path, the path cookie must match. Store it
    in the QP so that the IPv6 path can be reused.
    
    Replace open-coded version of dst_check() with the actual call, fixing the
    logic. The open-coded version skips the check call if dst->obsolete is 0
    (DST_OBSOLETE_NONE), proceeding to replace the route. DST_OBSOLETE_NONE
    means that the route may continue to be used, though.
    
    Fixes: 4ed6ad1eb30e ("IB/rxe: Cache dst in QP instead of getting it...")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index b09a9e26ca73..0c2dbe45c729 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -248,6 +248,7 @@ struct rxe_qp {
 	struct rxe_rq		rq;
 
 	struct socket		*sk;
+	u32			dst_cookie;
 
 	struct rxe_av		pri_av;
 	struct rxe_av		alt_av;

commit bfc3ae05660e928b34034e668a82c334111c0c22
Author: Andrew Boyer <andrew.boyer@dell.com>
Date:   Mon Aug 28 16:11:50 2017 -0400

    IB/rxe: Disable completion upcalls when a CQ is destroyed
    
    This prevents the stack from accessing userspace objects while they
    are being torn down.
    
    One possible sequence of events:
     - Userspace program exits
     - ib_uverbs_cleanup_ucontext() runs, calling ib_destroy_qp(),
       ib_destroy_cq(), etc. and releasing/freeing the UCQ
       - The QP still has tasklets running, so it isn't destroyed yet
       - The CQ is referenced by the QP, so the CQ isn't destroyed yet
       - The UCQ is kfree()'d anyway
     - A send work request completes
     - rxe_send_complete() calls cq->ibcq.comp_handler()
     - ib_uverbs_comp_handler() runs and crashes; the event queue is checked
       for is_closed, but it has no way to check the ib_ucq_object before
       accessing it
    
    The reference counting on the CQ doesn't protect against this since the CQ
    hasn't been destroyed yet.
    There's no available interface to deregister the UCQ from the CQ, and it
    didn't appear that attempting to add reference counting to the UCQ was
    going to be a good way to go since this solution is much simpler.
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 5a180fbe40d9..b09a9e26ca73 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -89,6 +89,7 @@ struct rxe_cq {
 	struct rxe_queue	*queue;
 	spinlock_t		cq_lock;
 	u8			notify;
+	bool			is_dying;
 	int			is_user;
 	struct tasklet_struct	comp_task;
 };

commit cee2688e3cd60e0d1ab2f049e31cf11fa3f62540
Author: yonatanc <yonatanc@mellanox.com>
Date:   Thu Apr 20 20:55:55 2017 +0300

    IB/rxe: Offload CRC calculation when possible
    
    Use CPU ability to perform CRC calculations, by
    replacing direct calls to crc32_le() with crypto_shash_updata().
    
    The overall performance gain measured with ib_send_bw tool is 10% and it
    was tested on "Intel CPU ES-2660 v2 @ 2.20Ghz" CPU.
    
    ib_send_bw -d rxe0  -x 1 -n 9000 -e  -s $((1024 * 1024 )) -l 100
    
    ---------------------------------------------------------------------------------------------
    |             | bytes   | iterations | BW peak[MB/sec] | BW average[MB/sec] | MsgRate[Mpps] |
    ---------------------------------------------------------------------------------------------
    | crc32_le    | 1048576 | 9000       | inf             | 497.60             | 0.000498      |
    | CRC offload | 1048576 | 9000       | inf             | 546.70             | 0.000547      |
    ---------------------------------------------------------------------------------------------
    
    Fixes: 8700e3e7c485 ("Soft RoCE driver")
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 8e0a093f0472..5a180fbe40d9 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -406,6 +406,7 @@ struct rxe_dev {
 
 	struct rxe_port		port;
 	struct list_head	list;
+	struct crypto_shash	*tfm;
 };
 
 static inline void rxe_counter_inc(struct rxe_dev *rxe, enum rxe_counters cnt)

commit 0b1e5b99a48b5b810e3e38f1d6e0d39306b99ec0
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Fri Mar 10 18:23:56 2017 +0200

    IB/rxe: Add port protocol stats
    
    Expose new counters using the get_hw_stats callback.
    We expose the following counters:
    
    +---------------------+----------------------------------------+
    |      Name           |           Description                  |
    |---------------------+----------------------------------------|
    |sent_pkts            | number of sent pkts                    |
    |---------------------+----------------------------------------|
    |rcvd_pkts            | number of received packets             |
    |---------------------+----------------------------------------|
    |out_of_sequence      | number of errors due to packet         |
    |                     | transport sequence number              |
    |---------------------+----------------------------------------|
    |duplicate_request    | number of received duplicated packets. |
    |                     | A request that previously executed is  |
    |                     | named duplicated.                      |
    |---------------------+----------------------------------------|
    |rcvd_rnr_err         | number of received RNR by completer    |
    |---------------------+----------------------------------------|
    |send_rnr_err         | number of sent RNR by responder        |
    |---------------------+----------------------------------------|
    |rcvd_seq_err         | number of out of sequence packets      |
    |                     | received                               |
    |---------------------+----------------------------------------|
    |ack_deffered         | number of deferred handling of ack     |
    |                     | packets.                               |
    |---------------------+----------------------------------------|
    |retry_exceeded_err   | number of times retry exceeded         |
    |---------------------+----------------------------------------|
    |completer_retry_err  | number of times completer decided to   |
    |                     | retry                                  |
    |---------------------+----------------------------------------|
    |send_err             | number of failed send packet           |
    +---------------------+----------------------------------------+
    
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Andrew Boyer <andrew.boyer@dell.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index e100c500ae85..8e0a093f0472 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -38,6 +38,7 @@
 #include <rdma/rdma_user_rxe.h>
 #include "rxe_pool.h"
 #include "rxe_task.h"
+#include "rxe_hw_counters.h"
 
 static inline int pkey_match(u16 key1, u16 key2)
 {
@@ -401,10 +402,17 @@ struct rxe_dev {
 	spinlock_t		mmap_offset_lock; /* guard mmap_offset */
 	int			mmap_offset;
 
+	u64			stats_counters[RXE_NUM_OF_COUNTERS];
+
 	struct rxe_port		port;
 	struct list_head	list;
 };
 
+static inline void rxe_counter_inc(struct rxe_dev *rxe, enum rxe_counters cnt)
+{
+	rxe->stats_counters[cnt]++;
+}
+
 static inline struct rxe_dev *to_rdev(struct ib_device *dev)
 {
 	return dev ? container_of(dev, struct rxe_dev, ib_dev) : NULL;

commit 839f5ac0d806970a102117be03ef05272c50d20e
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jan 10 11:15:53 2017 -0800

    IB/rxe: Remove a pointless indirection layer
    
    Neither rxe->ifc_ops nor any of the function pointers in struct
    struct rxe_ifc_ops ever change. Hence remove the rxe->ifc_ops
    indirection mechanism.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Andrew Boyer <andrew.boyer@dell.com>
    Cc: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index 536974b69ed9..e100c500ae85 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -372,26 +372,6 @@ struct rxe_port {
 	u32			qp_gsi_index;
 };
 
-/* callbacks from rdma_rxe to network interface layer */
-struct rxe_ifc_ops {
-	void (*release)(struct rxe_dev *rxe);
-	__be64 (*node_guid)(struct rxe_dev *rxe);
-	__be64 (*port_guid)(struct rxe_dev *rxe);
-	struct device *(*dma_device)(struct rxe_dev *rxe);
-	int (*mcast_add)(struct rxe_dev *rxe, union ib_gid *mgid);
-	int (*mcast_delete)(struct rxe_dev *rxe, union ib_gid *mgid);
-	int (*prepare)(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		       struct sk_buff *skb, u32 *crc);
-	int (*send)(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
-		    struct sk_buff *skb);
-	int (*loopback)(struct sk_buff *skb);
-	struct sk_buff *(*init_packet)(struct rxe_dev *rxe, struct rxe_av *av,
-				       int paylen, struct rxe_pkt_info *pkt);
-	char *(*parent_name)(struct rxe_dev *rxe, unsigned int port_num);
-	enum rdma_link_layer (*link_layer)(struct rxe_dev *rxe,
-					   unsigned int port_num);
-};
-
 struct rxe_dev {
 	struct ib_device	ib_dev;
 	struct ib_device_attr	attr;
@@ -400,8 +380,6 @@ struct rxe_dev {
 	struct kref		ref_cnt;
 	struct mutex	usdev_lock;
 
-	struct rxe_ifc_ops	*ifc_ops;
-
 	struct net_device	*ndev;
 
 	int			xmit_errors;

commit 32404fb76408e873d79d0723fda3207b18d235ad
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Tue Jan 10 11:15:46 2017 -0800

    IB/rxe: Let the compiler check the type of the cleanup functions
    
    Change the argument type of these functions from void * into
    struct rxe_pool_entry *.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Andrew Boyer <andrew.boyer@dell.com>
    Cc: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
index cac1d52a08f0..536974b69ed9 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.h
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -475,6 +475,6 @@ static inline struct rxe_mem *to_rmw(struct ib_mw *mw)
 int rxe_register_device(struct rxe_dev *rxe);
 int rxe_unregister_device(struct rxe_dev *rxe);
 
-void rxe_mc_cleanup(void *arg);
+void rxe_mc_cleanup(struct rxe_pool_entry *arg);
 
 #endif /* RXE_VERBS_H */

commit 8700e3e7c4857d28ebaa824509934556da0b3e76
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Jun 16 16:45:23 2016 +0300

    Soft RoCE driver
    
    Soft RoCE (RXE) - The software RoCE driver
    
    ib_rxe implements the RDMA transport and registers to the RDMA core
    device as a kernel verbs provider. It also implements the packet IO
    layer. On the other hand ib_rxe registers to the Linux netdev stack
    as a udp encapsulating protocol, in that case RDMA, for sending and
    receiving packets over any Ethernet device.  This yields a RDMA
    transport over the UDP/Ethernet network layer forming a RoCEv2
    compatible device.
    
    The configuration procedure of the Soft RoCE drivers requires
    binding to any existing Ethernet network device. This is done with
    /sys interface.
    
    A userspace Soft RoCE library (librxe) provides user applications
    the ability to run with Soft RoCE devices.  The use of rxe verbs ins
    user space requires the inclusion of librxe as a device specifics
    plug-in to libibverbs. librxe is packaged separately.
    
    Architecture:
    
         +-----------------------------------------------------------+
         |                          Application                      |
         +-----------------------------------------------------------+
                                +-----------------------------------+
                                |             libibverbs            |
    User                        +-----------------------------------+
                                +----------------+ +----------------+
                                | librxe         | | HW RoCE lib    |
                                +----------------+ +----------------+
    +---------------------------------------------------------------+
         +--------------+                           +------------+
         | Sockets      |                           | RDMA ULP   |
         +--------------+                           +------------+
         +--------------+                  +---------------------+
         | TCP/IP       |                  | ib_core             |
         +--------------+                  +---------------------+
                                 +------------+ +----------------+
    Kernel                       | ib_rxe     | | HW RoCE driver |
                                 +------------+ +----------------+
         +------------------------------------+
         | NIC driver                         |
         +------------------------------------+
    
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         +-----------------------------------------------------------+
         |                          Application                      |
         +-----------------------------------------------------------+
                                +-----------------------------------+
                                |             libibverbs            |
    User                        +-----------------------------------+
                                +----------------+ +----------------+
                                | librxe         | | HW RoCE lib    |
                                +----------------+ +----------------+
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         +--------------+                           +------------+
         | Sockets      |                           | RDMA ULP   |
         +--------------+                           +------------+
         +--------------+                  +---------------------+
         | TCP/IP       |                  | ib_core             |
         +--------------+                  +---------------------+
                                 +------------+ +----------------+
    Kernel                       | ib_rxe     | | HW RoCE driver |
                                 +------------+ +----------------+
         +------------------------------------+
         | NIC driver                         |
         +------------------------------------+
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    Soft RoCE resources:
    
    [1[ https://github.com/SoftRoCE/librxe-dev librxe - source code in
    Github
    [2] https://github.com/SoftRoCE/rxe-dev/wiki/rxe-dev:-Home - Soft RoCE
    Wiki page
    [3] https://github.com/SoftRoCE/librxe-dev - Soft RoCE userspace library
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.h b/drivers/infiniband/sw/rxe/rxe_verbs.h
new file mode 100644
index 000000000000..cac1d52a08f0
--- /dev/null
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.h
@@ -0,0 +1,480 @@
+/*
+ * Copyright (c) 2016 Mellanox Technologies Ltd. All rights reserved.
+ * Copyright (c) 2015 System Fabric Works, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *	   Redistribution and use in source and binary forms, with or
+ *	   without modification, are permitted provided that the following
+ *	   conditions are met:
+ *
+ *	- Redistributions of source code must retain the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer.
+ *
+ *	- Redistributions in binary form must reproduce the above
+ *	  copyright notice, this list of conditions and the following
+ *	  disclaimer in the documentation and/or other materials
+ *	  provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef RXE_VERBS_H
+#define RXE_VERBS_H
+
+#include <linux/interrupt.h>
+#include <rdma/rdma_user_rxe.h>
+#include "rxe_pool.h"
+#include "rxe_task.h"
+
+static inline int pkey_match(u16 key1, u16 key2)
+{
+	return (((key1 & 0x7fff) != 0) &&
+		((key1 & 0x7fff) == (key2 & 0x7fff)) &&
+		((key1 & 0x8000) || (key2 & 0x8000))) ? 1 : 0;
+}
+
+/* Return >0 if psn_a > psn_b
+ *	   0 if psn_a == psn_b
+ *	  <0 if psn_a < psn_b
+ */
+static inline int psn_compare(u32 psn_a, u32 psn_b)
+{
+	s32 diff;
+
+	diff = (psn_a - psn_b) << 8;
+	return diff;
+}
+
+struct rxe_ucontext {
+	struct rxe_pool_entry	pelem;
+	struct ib_ucontext	ibuc;
+};
+
+struct rxe_pd {
+	struct rxe_pool_entry	pelem;
+	struct ib_pd		ibpd;
+};
+
+struct rxe_ah {
+	struct rxe_pool_entry	pelem;
+	struct ib_ah		ibah;
+	struct rxe_pd		*pd;
+	struct rxe_av		av;
+};
+
+struct rxe_cqe {
+	union {
+		struct ib_wc		ibwc;
+		struct ib_uverbs_wc	uibwc;
+	};
+};
+
+struct rxe_cq {
+	struct rxe_pool_entry	pelem;
+	struct ib_cq		ibcq;
+	struct rxe_queue	*queue;
+	spinlock_t		cq_lock;
+	u8			notify;
+	int			is_user;
+	struct tasklet_struct	comp_task;
+};
+
+enum wqe_state {
+	wqe_state_posted,
+	wqe_state_processing,
+	wqe_state_pending,
+	wqe_state_done,
+	wqe_state_error,
+};
+
+struct rxe_sq {
+	int			max_wr;
+	int			max_sge;
+	int			max_inline;
+	spinlock_t		sq_lock; /* guard queue */
+	struct rxe_queue	*queue;
+};
+
+struct rxe_rq {
+	int			max_wr;
+	int			max_sge;
+	spinlock_t		producer_lock; /* guard queue producer */
+	spinlock_t		consumer_lock; /* guard queue consumer */
+	struct rxe_queue	*queue;
+};
+
+struct rxe_srq {
+	struct rxe_pool_entry	pelem;
+	struct ib_srq		ibsrq;
+	struct rxe_pd		*pd;
+	struct rxe_rq		rq;
+	u32			srq_num;
+
+	int			limit;
+	int			error;
+};
+
+enum rxe_qp_state {
+	QP_STATE_RESET,
+	QP_STATE_INIT,
+	QP_STATE_READY,
+	QP_STATE_DRAIN,		/* req only */
+	QP_STATE_DRAINED,	/* req only */
+	QP_STATE_ERROR
+};
+
+extern char *rxe_qp_state_name[];
+
+struct rxe_req_info {
+	enum rxe_qp_state	state;
+	int			wqe_index;
+	u32			psn;
+	int			opcode;
+	atomic_t		rd_atomic;
+	int			wait_fence;
+	int			need_rd_atomic;
+	int			wait_psn;
+	int			need_retry;
+	int			noack_pkts;
+	struct rxe_task		task;
+};
+
+struct rxe_comp_info {
+	u32			psn;
+	int			opcode;
+	int			timeout;
+	int			timeout_retry;
+	u32			retry_cnt;
+	u32			rnr_retry;
+	struct rxe_task		task;
+};
+
+enum rdatm_res_state {
+	rdatm_res_state_next,
+	rdatm_res_state_new,
+	rdatm_res_state_replay,
+};
+
+struct resp_res {
+	int			type;
+	u32			first_psn;
+	u32			last_psn;
+	u32			cur_psn;
+	enum rdatm_res_state	state;
+
+	union {
+		struct {
+			struct sk_buff	*skb;
+		} atomic;
+		struct {
+			struct rxe_mem	*mr;
+			u64		va_org;
+			u32		rkey;
+			u32		length;
+			u64		va;
+			u32		resid;
+		} read;
+	};
+};
+
+struct rxe_resp_info {
+	enum rxe_qp_state	state;
+	u32			msn;
+	u32			psn;
+	int			opcode;
+	int			drop_msg;
+	int			goto_error;
+	int			sent_psn_nak;
+	enum ib_wc_status	status;
+	u8			aeth_syndrome;
+
+	/* Receive only */
+	struct rxe_recv_wqe	*wqe;
+
+	/* RDMA read / atomic only */
+	u64			va;
+	struct rxe_mem		*mr;
+	u32			resid;
+	u32			rkey;
+	u64			atomic_orig;
+
+	/* SRQ only */
+	struct {
+		struct rxe_recv_wqe	wqe;
+		struct ib_sge		sge[RXE_MAX_SGE];
+	} srq_wqe;
+
+	/* Responder resources. It's a circular list where the oldest
+	 * resource is dropped first.
+	 */
+	struct resp_res		*resources;
+	unsigned int		res_head;
+	unsigned int		res_tail;
+	struct resp_res		*res;
+	struct rxe_task		task;
+};
+
+struct rxe_qp {
+	struct rxe_pool_entry	pelem;
+	struct ib_qp		ibqp;
+	struct ib_qp_attr	attr;
+	unsigned int		valid;
+	unsigned int		mtu;
+	int			is_user;
+
+	struct rxe_pd		*pd;
+	struct rxe_srq		*srq;
+	struct rxe_cq		*scq;
+	struct rxe_cq		*rcq;
+
+	enum ib_sig_type	sq_sig_type;
+
+	struct rxe_sq		sq;
+	struct rxe_rq		rq;
+
+	struct socket		*sk;
+
+	struct rxe_av		pri_av;
+	struct rxe_av		alt_av;
+
+	/* list of mcast groups qp has joined (for cleanup) */
+	struct list_head	grp_list;
+	spinlock_t		grp_lock; /* guard grp_list */
+
+	struct sk_buff_head	req_pkts;
+	struct sk_buff_head	resp_pkts;
+	struct sk_buff_head	send_pkts;
+
+	struct rxe_req_info	req;
+	struct rxe_comp_info	comp;
+	struct rxe_resp_info	resp;
+
+	atomic_t		ssn;
+	atomic_t		skb_out;
+	int			need_req_skb;
+
+	/* Timer for retranmitting packet when ACKs have been lost. RC
+	 * only. The requester sets it when it is not already
+	 * started. The responder resets it whenever an ack is
+	 * received.
+	 */
+	struct timer_list retrans_timer;
+	u64 qp_timeout_jiffies;
+
+	/* Timer for handling RNR NAKS. */
+	struct timer_list rnr_nak_timer;
+
+	spinlock_t		state_lock; /* guard requester and completer */
+};
+
+enum rxe_mem_state {
+	RXE_MEM_STATE_ZOMBIE,
+	RXE_MEM_STATE_INVALID,
+	RXE_MEM_STATE_FREE,
+	RXE_MEM_STATE_VALID,
+};
+
+enum rxe_mem_type {
+	RXE_MEM_TYPE_NONE,
+	RXE_MEM_TYPE_DMA,
+	RXE_MEM_TYPE_MR,
+	RXE_MEM_TYPE_FMR,
+	RXE_MEM_TYPE_MW,
+};
+
+#define RXE_BUF_PER_MAP		(PAGE_SIZE / sizeof(struct rxe_phys_buf))
+
+struct rxe_phys_buf {
+	u64      addr;
+	u64      size;
+};
+
+struct rxe_map {
+	struct rxe_phys_buf	buf[RXE_BUF_PER_MAP];
+};
+
+struct rxe_mem {
+	struct rxe_pool_entry	pelem;
+	union {
+		struct ib_mr		ibmr;
+		struct ib_mw		ibmw;
+	};
+
+	struct rxe_pd		*pd;
+	struct ib_umem		*umem;
+
+	u32			lkey;
+	u32			rkey;
+
+	enum rxe_mem_state	state;
+	enum rxe_mem_type	type;
+	u64			va;
+	u64			iova;
+	size_t			length;
+	u32			offset;
+	int			access;
+
+	int			page_shift;
+	int			page_mask;
+	int			map_shift;
+	int			map_mask;
+
+	u32			num_buf;
+	u32			nbuf;
+
+	u32			max_buf;
+	u32			num_map;
+
+	struct rxe_map		**map;
+};
+
+struct rxe_mc_grp {
+	struct rxe_pool_entry	pelem;
+	spinlock_t		mcg_lock; /* guard group */
+	struct rxe_dev		*rxe;
+	struct list_head	qp_list;
+	union ib_gid		mgid;
+	int			num_qp;
+	u32			qkey;
+	u16			pkey;
+};
+
+struct rxe_mc_elem {
+	struct rxe_pool_entry	pelem;
+	struct list_head	qp_list;
+	struct list_head	grp_list;
+	struct rxe_qp		*qp;
+	struct rxe_mc_grp	*grp;
+};
+
+struct rxe_port {
+	struct ib_port_attr	attr;
+	u16			*pkey_tbl;
+	__be64			port_guid;
+	__be64			subnet_prefix;
+	spinlock_t		port_lock; /* guard port */
+	unsigned int		mtu_cap;
+	/* special QPs */
+	u32			qp_smi_index;
+	u32			qp_gsi_index;
+};
+
+/* callbacks from rdma_rxe to network interface layer */
+struct rxe_ifc_ops {
+	void (*release)(struct rxe_dev *rxe);
+	__be64 (*node_guid)(struct rxe_dev *rxe);
+	__be64 (*port_guid)(struct rxe_dev *rxe);
+	struct device *(*dma_device)(struct rxe_dev *rxe);
+	int (*mcast_add)(struct rxe_dev *rxe, union ib_gid *mgid);
+	int (*mcast_delete)(struct rxe_dev *rxe, union ib_gid *mgid);
+	int (*prepare)(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		       struct sk_buff *skb, u32 *crc);
+	int (*send)(struct rxe_dev *rxe, struct rxe_pkt_info *pkt,
+		    struct sk_buff *skb);
+	int (*loopback)(struct sk_buff *skb);
+	struct sk_buff *(*init_packet)(struct rxe_dev *rxe, struct rxe_av *av,
+				       int paylen, struct rxe_pkt_info *pkt);
+	char *(*parent_name)(struct rxe_dev *rxe, unsigned int port_num);
+	enum rdma_link_layer (*link_layer)(struct rxe_dev *rxe,
+					   unsigned int port_num);
+};
+
+struct rxe_dev {
+	struct ib_device	ib_dev;
+	struct ib_device_attr	attr;
+	int			max_ucontext;
+	int			max_inline_data;
+	struct kref		ref_cnt;
+	struct mutex	usdev_lock;
+
+	struct rxe_ifc_ops	*ifc_ops;
+
+	struct net_device	*ndev;
+
+	int			xmit_errors;
+
+	struct rxe_pool		uc_pool;
+	struct rxe_pool		pd_pool;
+	struct rxe_pool		ah_pool;
+	struct rxe_pool		srq_pool;
+	struct rxe_pool		qp_pool;
+	struct rxe_pool		cq_pool;
+	struct rxe_pool		mr_pool;
+	struct rxe_pool		mw_pool;
+	struct rxe_pool		mc_grp_pool;
+	struct rxe_pool		mc_elem_pool;
+
+	spinlock_t		pending_lock; /* guard pending_mmaps */
+	struct list_head	pending_mmaps;
+
+	spinlock_t		mmap_offset_lock; /* guard mmap_offset */
+	int			mmap_offset;
+
+	struct rxe_port		port;
+	struct list_head	list;
+};
+
+static inline struct rxe_dev *to_rdev(struct ib_device *dev)
+{
+	return dev ? container_of(dev, struct rxe_dev, ib_dev) : NULL;
+}
+
+static inline struct rxe_ucontext *to_ruc(struct ib_ucontext *uc)
+{
+	return uc ? container_of(uc, struct rxe_ucontext, ibuc) : NULL;
+}
+
+static inline struct rxe_pd *to_rpd(struct ib_pd *pd)
+{
+	return pd ? container_of(pd, struct rxe_pd, ibpd) : NULL;
+}
+
+static inline struct rxe_ah *to_rah(struct ib_ah *ah)
+{
+	return ah ? container_of(ah, struct rxe_ah, ibah) : NULL;
+}
+
+static inline struct rxe_srq *to_rsrq(struct ib_srq *srq)
+{
+	return srq ? container_of(srq, struct rxe_srq, ibsrq) : NULL;
+}
+
+static inline struct rxe_qp *to_rqp(struct ib_qp *qp)
+{
+	return qp ? container_of(qp, struct rxe_qp, ibqp) : NULL;
+}
+
+static inline struct rxe_cq *to_rcq(struct ib_cq *cq)
+{
+	return cq ? container_of(cq, struct rxe_cq, ibcq) : NULL;
+}
+
+static inline struct rxe_mem *to_rmr(struct ib_mr *mr)
+{
+	return mr ? container_of(mr, struct rxe_mem, ibmr) : NULL;
+}
+
+static inline struct rxe_mem *to_rmw(struct ib_mw *mw)
+{
+	return mw ? container_of(mw, struct rxe_mem, ibmw) : NULL;
+}
+
+int rxe_register_device(struct rxe_dev *rxe);
+int rxe_unregister_device(struct rxe_dev *rxe);
+
+void rxe_mc_cleanup(void *arg);
+
+#endif /* RXE_VERBS_H */
