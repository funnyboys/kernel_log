commit f6f3f532556e4fcaa2d259fd04a800bfb4f9670d
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Mon Mar 18 09:58:30 2019 -0700

    IB/hfi1: Delay the release of destination mr for TID RDMA WRITE DATA
    
    The reference of destination memory region is first obtained when TID RDMA
    WRITE request is first received on the responder side. This reference is
    released once all TID RDMA WRITE RESP packets are sent to the requester
    side, even though not all TID RDMA WRITE DATA packets may have been
    received. This early release will especially be undesired if the software
    needs to access the destination memory before the last data packet is
    received.
    
    This patch delays the release of the MR until all TID RDMA DATA packets
    have been received. A helper function to release the reference is also
    created to simplify the code.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/hfi1/rc.h b/drivers/infiniband/hw/hfi1/rc.h
index 8e0935b9bf2a..5ed5e85d5841 100644
--- a/drivers/infiniband/hw/hfi1/rc.h
+++ b/drivers/infiniband/hw/hfi1/rc.h
@@ -41,6 +41,14 @@ static inline u32 restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe,
 	return rvt_restart_sge(ss, wqe, len);
 }
 
+static inline void release_rdma_sge_mr(struct rvt_ack_entry *e)
+{
+	if (e->rdma_sge.mr) {
+		rvt_put_mr(e->rdma_sge.mr);
+		e->rdma_sge.mr = NULL;
+	}
+}
+
 struct rvt_ack_entry *find_prev_entry(struct rvt_qp *qp, u32 psn, u8 *prev,
 				      u8 *prev_ack, bool *scheduled);
 int do_rc_ack(struct rvt_qp *qp, u32 aeth, u32 psn, int opcode, u64 val,

commit 4f9264d156dc6c154a8a6cfae780730bad45c6f8
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 21:48:48 2019 -0800

    IB/hfi1: Add an s_acked_ack_queue pointer
    
    The s_ack_queue is managed by two pointers into the ring:
    r_head_ack_queue and s_tail_ack_queue. r_head_ack_queue is the index of
    where the next received request is going to be placed and s_tail_ack_queue
    is the entry of the request currently being processed. This works
    perfectly fine for normal Verbs as the requests are processed one at a
    time and the s_tail_ack_queue is not moved until the request that it
    points to is fully completed.
    
    In this fashion, s_tail_ack_queue constantly chases r_head_ack_queue and
    the two pointers can easily be used to determine "queue full" and "queue
    empty" conditions.
    
    The detection of these two conditions are imported in determining when an
    old entry can safely be overwritten with a new received request and the
    resources associated with the old request be safely released.
    
    When pipelined TID RDMA WRITE is introduced into this mix, things look
    very different. r_head_ack_queue is still the point at which a newly
    received request will be inserted, s_tail_ack_queue is still the
    currently processed request. However, with pipelined TID RDMA WRITE
    requests, s_tail_ack_queue moves to the next request once all TID RDMA
    WRITE responses for that request have been sent. The rest of the protocol
    for a particular request is managed by other pointers specific to TID RDMA
    - r_tid_tail and r_tid_ack - which point to the entries for which the next
    TID RDMA DATA packets are going to arrive and the request for which
    the next TID RDMA ACK packets are to be generated, respectively.
    
    What this means is that entries in the ring, which are "behind"
    s_tail_ack_queue (entries which s_tail_ack_queue has gone past) are no
    longer considered complete. This is where the problem is - a newly
    received request could potentially overwrite a still active TID RDMA WRITE
    request.
    
    The reason why the TID RDMA pointers trail s_tail_ack_queue is that the
    normal Verbs send engine uses s_tail_ack_queue as the pointer for the next
    response. Since TID RDMA WRITE responses are processed by the normal Verbs
    send engine, s_tail_ack_queue had to be moved to the next entry once all
    TID RDMA WRITE response packets were sent to get the desired pipelining
    between requests. Doing otherwise would mean that the normal Verbs send
    engine would not be able to send the TID RDMA WRITE responses for the next
    TID RDMA request until the current one is fully completed.
    
    This patch introduces the s_acked_ack_queue index to point to the next
    request to complete on the responder side. For requests other than TID
    RDMA WRITE, s_acked_ack_queue should always be kept in sync with
    s_tail_ack_queue. For TID RDMA WRITE request, it may fall behind
    s_tail_ack_queue.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/hfi1/rc.h b/drivers/infiniband/hw/hfi1/rc.h
index 4329eadcb3df..8e0935b9bf2a 100644
--- a/drivers/infiniband/hw/hfi1/rc.h
+++ b/drivers/infiniband/hw/hfi1/rc.h
@@ -18,6 +18,7 @@ static inline void update_ack_queue(struct rvt_qp *qp, unsigned int n)
 	if (next > rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))
 		next = 0;
 	qp->s_tail_ack_queue = next;
+	qp->s_acked_ack_queue = next;
 	qp->s_ack_state = OP(ACKNOWLEDGE);
 }
 

commit 385156c5f2a61834666f079ee66338f177c65c28
Author: Kaike Wan <kaike.wan@intel.com>
Date:   Wed Jan 23 19:29:44 2019 -0800

    IB/hfi: Move RC functions into a header file
    
    This patch moves some RC helper functions into a header file so that
    they can be called from both RC and  TID RDMA functions. In addition,
    a common function for rewinding a request is created in rdmavt so that
    it can be shared between qib and hfi1 driver.
    
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Mitko Haralanov <mitko.haralanov@intel.com>
    Signed-off-by: Kaike Wan <kaike.wan@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/hfi1/rc.h b/drivers/infiniband/hw/hfi1/rc.h
new file mode 100644
index 000000000000..4329eadcb3df
--- /dev/null
+++ b/drivers/infiniband/hw/hfi1/rc.h
@@ -0,0 +1,50 @@
+/* SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause) */
+/*
+ * Copyright(c) 2018 Intel Corporation.
+ *
+ */
+
+#ifndef HFI1_RC_H
+#define HFI1_RC_H
+
+/* cut down ridiculously long IB macro names */
+#define OP(x) IB_OPCODE_RC_##x
+
+static inline void update_ack_queue(struct rvt_qp *qp, unsigned int n)
+{
+	unsigned int next;
+
+	next = n + 1;
+	if (next > rvt_size_atomic(ib_to_rvt(qp->ibqp.device)))
+		next = 0;
+	qp->s_tail_ack_queue = next;
+	qp->s_ack_state = OP(ACKNOWLEDGE);
+}
+
+static inline void rc_defered_ack(struct hfi1_ctxtdata *rcd,
+				  struct rvt_qp *qp)
+{
+	if (list_empty(&qp->rspwait)) {
+		qp->r_flags |= RVT_R_RSP_NAK;
+		rvt_get_qp(qp);
+		list_add_tail(&qp->rspwait, &rcd->qp_wait_list);
+	}
+}
+
+static inline u32 restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe,
+			      u32 psn, u32 pmtu)
+{
+	u32 len;
+
+	len = delta_psn(psn, wqe->psn) * pmtu;
+	return rvt_restart_sge(ss, wqe, len);
+}
+
+struct rvt_ack_entry *find_prev_entry(struct rvt_qp *qp, u32 psn, u8 *prev,
+				      u8 *prev_ack, bool *scheduled);
+int do_rc_ack(struct rvt_qp *qp, u32 aeth, u32 psn, int opcode, u64 val,
+	      struct hfi1_ctxtdata *rcd);
+struct rvt_swqe *do_rc_completion(struct rvt_qp *qp, struct rvt_swqe *wqe,
+				  struct hfi1_ibport *ibp);
+
+#endif /* HFI1_RC_H */
