commit f1f6a7dd9b53aafd81b696b9017036e7b08e57ea
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Thu Jan 30 22:13:35 2020 -0800

    mm, tree-wide: rename put_user_page*() to unpin_user_page*()
    
    In order to provide a clearer, more symmetric API for pinning and
    unpinning DMA pages.  This way, pin_user_pages*() calls match up with
    unpin_user_pages*() calls, and the API is a lot closer to being
    self-explanatory.
    
    Link: http://lkml.kernel.org/r/20200107224558.2362728-23-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Alex Williamson <alex.williamson@redhat.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Björn Töpel <bjorn.topel@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Hans Verkuil <hverkuil-cisco@xs4all.nl>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Leon Romanovsky <leonro@mellanox.com>
    Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 9a94761765c0..3b505006c0a6 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -118,7 +118,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 			     size_t npages, bool dirty)
 {
-	put_user_pages_dirty_lock(p, npages, dirty);
+	unpin_user_pages_dirty_lock(p, npages, dirty);
 
 	if (mm) { /* during close after signal, mm can be NULL */
 		atomic64_sub(npages, &mm->pinned_vm);

commit dfa0a4fff11b32740c67fb0baf219702b978fc51
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Thu Jan 30 22:13:02 2020 -0800

    IB/{core,hw,umem}: set FOLL_PIN via pin_user_pages*(), fix up ODP
    
    Convert infiniband to use the new pin_user_pages*() calls.
    
    Also, revert earlier changes to Infiniband ODP that had it using
    put_user_page().  ODP is "Case 3" in
    Documentation/core-api/pin_user_pages.rst, which is to say, normal
    get_user_pages() and put_page() is the API to use there.
    
    The new pin_user_pages*() calls replace corresponding get_user_pages*()
    calls, and set the FOLL_PIN flag.  The FOLL_PIN flag requires that the
    caller must return the pages via put_user_page*() calls, but infiniband
    was already doing that as part of an earlier commit.
    
    Link: http://lkml.kernel.org/r/20200107224558.2362728-14-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Alex Williamson <alex.williamson@redhat.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Björn Töpel <bjorn.topel@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Hans Verkuil <hverkuil-cisco@xs4all.nl>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Leon Romanovsky <leonro@mellanox.com>
    Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 469acb961fbd..9a94761765c0 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -106,7 +106,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 	int ret;
 	unsigned int gup_flags = FOLL_LONGTERM | (writable ? FOLL_WRITE : 0);
 
-	ret = get_user_pages_fast(vaddr, npages, gup_flags, pages);
+	ret = pin_user_pages_fast(vaddr, npages, gup_flags, pages);
 	if (ret < 0)
 		return ret;
 

commit 2d15eb31b50a1b93927bdb7cc5d9960b90f7c1e4
Author: akpm@linux-foundation.org <akpm@linux-foundation.org>
Date:   Mon Sep 23 15:35:04 2019 -0700

    mm/gup: add make_dirty arg to put_user_pages_dirty_lock()
    
    [11~From: John Hubbard <jhubbard@nvidia.com>
    Subject: mm/gup: add make_dirty arg to put_user_pages_dirty_lock()
    
    Patch series "mm/gup: add make_dirty arg to put_user_pages_dirty_lock()",
    v3.
    
    There are about 50+ patches in my tree [2], and I'll be sending out the
    remaining ones in a few more groups:
    
    * The block/bio related changes (Jerome mostly wrote those, but I've had
      to move stuff around extensively, and add a little code)
    
    * mm/ changes
    
    * other subsystem patches
    
    * an RFC that shows the current state of the tracking patch set.  That
      can only be applied after all call sites are converted, but it's good to
      get an early look at it.
    
    This is part a tree-wide conversion, as described in fc1d8e7cca2d ("mm:
    introduce put_user_page*(), placeholder versions").
    
    This patch (of 3):
    
    Provide more capable variation of put_user_pages_dirty_lock(), and delete
    put_user_pages_dirty().  This is based on the following:
    
    1.  Lots of call sites become simpler if a bool is passed into
       put_user_page*(), instead of making the call site choose which
       put_user_page*() variant to call.
    
    2.  Christoph Hellwig's observation that set_page_dirty_lock() is
       usually correct, and set_page_dirty() is usually a bug, or at least
       questionable, within a put_user_page*() calling chain.
    
    This leads to the following API choices:
    
        * put_user_pages_dirty_lock(page, npages, make_dirty)
    
        * There is no put_user_pages_dirty(). You have to
          hand code that, in the rare case that it's
          required.
    
    [jhubbard@nvidia.com: remove unused variable in siw_free_plist()]
      Link: http://lkml.kernel.org/r/20190729074306.10368-1-jhubbard@nvidia.com
    Link: http://lkml.kernel.org/r/20190724044537.10458-2-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index b89a9b9aef7a..469acb961fbd 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -118,10 +118,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 			     size_t npages, bool dirty)
 {
-	if (dirty)
-		put_user_pages_dirty_lock(p, npages);
-	else
-		put_user_pages(p, npages);
+	put_user_pages_dirty_lock(p, npages, dirty);
 
 	if (mm) { /* during close after signal, mm can be NULL */
 		atomic64_sub(npages, &mm->pinned_vm);

commit ea996974589e7987eb463d8a7c404358244755ea
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Fri May 24 18:45:22 2019 -0700

    RDMA: Convert put_page() to put_user_page*()
    
    For infiniband code that retains pages via get_user_pages*(), release
    those pages via the new put_user_page(), or put_user_pages*(), instead of
    put_page()
    
    This is a tiny part of the second step of fixing the problem described in
    [1]. The steps are:
    
    1) Provide put_user_page*() routines, intended to be used for releasing
       pages that were pinned via get_user_pages*().
    
    2) Convert all of the call sites for get_user_pages*(), to invoke
       put_user_page*(), instead of put_page(). This involves dozens of call
       sites, and will take some time.
    
    3) After (2) is complete, use get_user_pages*() and put_user_page*() to
       implement tracking of these pages. This tracking will be separate from
       the existing struct page refcounting.
    
    4) Use the tracking and identification of these pages, to implement
       special handling (especially in writeback paths) when the pages are
       backed by a filesystem. Again, [1] provides details as to why that is
       desirable.
    
    [1] https://lwn.net/Articles/753027/ : "The Trouble with get_user_pages()"
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Jérôme Glisse <jglisse@redhat.com>
    Acked-by: Jason Gunthorpe <jgg@mellanox.com>
    Tested-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 02eee8eff1db..b89a9b9aef7a 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -118,13 +118,10 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 			     size_t npages, bool dirty)
 {
-	size_t i;
-
-	for (i = 0; i < npages; i++) {
-		if (dirty)
-			set_page_dirty_lock(p[i]);
-		put_page(p[i]);
-	}
+	if (dirty)
+		put_user_pages_dirty_lock(p, npages);
+	else
+		put_user_pages(p, npages);
 
 	if (mm) { /* during close after signal, mm can be NULL */
 		atomic64_sub(npages, &mm->pinned_vm);

commit 9fdf4aa156733e3f075a9d7d0b026648b3874afe
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Mon May 13 17:17:18 2019 -0700

    IB/hfi1: use the new FOLL_LONGTERM flag to get_user_pages_fast()
    
    Use the new FOLL_LONGTERM to get_user_pages_fast() to protect against FS
    DAX pages being mapped.
    
    [ira.weiny@intel.com: v3]
      Link: http://lkml.kernel.org/r/20190328084422.29911-6-ira.weiny@intel.com
    Link: http://lkml.kernel.org/r/20190328084422.29911-6-ira.weiny@intel.com
    Link: http://lkml.kernel.org/r/20190317183438.2057-6-ira.weiny@intel.com
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Mike Marshall <hubcap@omnibond.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 78ccacaf97d0..02eee8eff1db 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -104,9 +104,9 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 			    bool writable, struct page **pages)
 {
 	int ret;
+	unsigned int gup_flags = FOLL_LONGTERM | (writable ? FOLL_WRITE : 0);
 
-	ret = get_user_pages_fast(vaddr, npages, writable ? FOLL_WRITE : 0,
-				  pages);
+	ret = get_user_pages_fast(vaddr, npages, gup_flags, pages);
 	if (ret < 0)
 		return ret;
 

commit 73b0140bf0fe9df90fb267c00673c4b9bf285430
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Mon May 13 17:17:11 2019 -0700

    mm/gup: change GUP fast to use flags rather than a write 'bool'
    
    To facilitate additional options to get_user_pages_fast() change the
    singular write parameter to be gup_flags.
    
    This patch does not change any functionality.  New functionality will
    follow in subsequent patches.
    
    Some of the get_user_pages_fast() call sites were unchanged because they
    already passed FOLL_WRITE or 0 for the write parameter.
    
    NOTE: It was suggested to change the ordering of the get_user_pages_fast()
    arguments to ensure that callers were converted.  This breaks the current
    GUP call site convention of having the returned pages be the final
    parameter.  So the suggestion was rejected.
    
    Link: http://lkml.kernel.org/r/20190328084422.29911-4-ira.weiny@intel.com
    Link: http://lkml.kernel.org/r/20190317183438.2057-4-ira.weiny@intel.com
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Mike Marshall <hubcap@omnibond.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 24b592c6522e..78ccacaf97d0 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -105,7 +105,8 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 {
 	int ret;
 
-	ret = get_user_pages_fast(vaddr, npages, writable, pages);
+	ret = get_user_pages_fast(vaddr, npages, writable ? FOLL_WRITE : 0,
+				  pages);
 	if (ret < 0)
 		return ret;
 

commit 0e15c253363e9a8824e6b9c9fc17468eb924cebd
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Feb 6 09:59:18 2019 -0800

    drivers/IB,hfi1: do not se mmap_sem
    
    This driver already uses gup_fast() and thus we can just drop the mmap_sem
    protection around the pinned_vm counter. Note that the window between when
    hfi1_can_pin_pages() is called and the actual counter is incremented
    remains the same as mmap_sem was _only_ used for when ->pinned_vm was
    touched.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.det>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 40a6e434190f..24b592c6522e 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -91,9 +91,7 @@ bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,
 	/* Convert to number of pages */
 	size = DIV_ROUND_UP(size, PAGE_SIZE);
 
-	down_read(&mm->mmap_sem);
 	pinned = atomic64_read(&mm->pinned_vm);
-	up_read(&mm->mmap_sem);
 
 	/* First, check the absolute limit against all pinned pages. */
 	if (pinned + npages >= ulimit && !can_lock)
@@ -111,9 +109,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 	if (ret < 0)
 		return ret;
 
-	down_write(&mm->mmap_sem);
 	atomic64_add(ret, &mm->pinned_vm);
-	up_write(&mm->mmap_sem);
 
 	return ret;
 }
@@ -130,8 +126,6 @@ void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 	}
 
 	if (mm) { /* during close after signal, mm can be NULL */
-		down_write(&mm->mmap_sem);
 		atomic64_sub(npages, &mm->pinned_vm);
-		up_write(&mm->mmap_sem);
 	}
 }

commit 70f8a3ca68d3e1f3344d959981ca55d5f6ec77f7
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Feb 6 09:59:15 2019 -0800

    mm: make mm->pinned_vm an atomic64 counter
    
    Taking a sleeping lock to _only_ increment a variable is quite the
    overkill, and pretty much all users do this. Furthermore, some drivers
    (ie: infiniband and scif) that need pinned semantics can go to quite
    some trouble to actually delay via workqueue (un)accounting for pinned
    pages when not possible to acquire it.
    
    By making the counter atomic we no longer need to hold the mmap_sem and
    can simply some code around it for pinned_vm users. The counter is 64-bit
    such that we need not worry about overflows such as rdma user input
    controlled from userspace.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index e341e6dcc388..40a6e434190f 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -92,7 +92,7 @@ bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,
 	size = DIV_ROUND_UP(size, PAGE_SIZE);
 
 	down_read(&mm->mmap_sem);
-	pinned = mm->pinned_vm;
+	pinned = atomic64_read(&mm->pinned_vm);
 	up_read(&mm->mmap_sem);
 
 	/* First, check the absolute limit against all pinned pages. */
@@ -112,7 +112,7 @@ int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t np
 		return ret;
 
 	down_write(&mm->mmap_sem);
-	mm->pinned_vm += ret;
+	atomic64_add(ret, &mm->pinned_vm);
 	up_write(&mm->mmap_sem);
 
 	return ret;
@@ -131,7 +131,7 @@ void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
 
 	if (mm) { /* during close after signal, mm can be NULL */
 		down_write(&mm->mmap_sem);
-		mm->pinned_vm -= npages;
+		atomic64_sub(npages, &mm->pinned_vm);
 		up_write(&mm->mmap_sem);
 	}
 }

commit 2280740f01aee0883a2885f332aee27449390a4b
Author: Vishwanathapura, Niranjana <niranjana.vishwanathapura@intel.com>
Date:   Wed Apr 12 20:29:29 2017 -0700

    IB/hfi1: Virtual Network Interface Controller (VNIC) HW support
    
    HFI1 HW specific support for VNIC functionality.
    Dynamically allocate a set of contexts for VNIC when the first vnic
    port is instantiated. Allocate VNIC contexts from user contexts pool
    and return them back to the same pool while freeing up. Set aside
    enough MSI-X interrupts for VNIC contexts and assign them when the
    contexts are allocated. On the receive side, use an RSM rule to
    spread TCP/UDP streams among VNIC contexts.
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Signed-off-by: Andrzej Kacprowski <andrzej.kacprowski@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 68295a12b771..e341e6dcc388 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -1,5 +1,5 @@
 /*
- * Copyright(c) 2015, 2016 Intel Corporation.
+ * Copyright(c) 2015-2017 Intel Corporation.
  *
  * This file is provided under a dual BSD/GPLv2 license.  When using or
  * redistributing this file, you may do so under either license.
@@ -73,7 +73,8 @@ bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,
 {
 	unsigned long ulimit = rlimit(RLIMIT_MEMLOCK), pinned, cache_limit,
 		size = (cache_size * (1UL << 20)); /* convert to bytes */
-	unsigned usr_ctxts = dd->num_rcv_contexts - dd->first_user_ctxt;
+	unsigned int usr_ctxts =
+			dd->num_rcv_contexts - dd->first_dyn_alloc_ctxt;
 	bool can_lock = capable(CAP_IPC_LOCK);
 
 	/*

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 20f4ddcac3b0..68295a12b771 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -46,7 +46,7 @@
  */
 
 #include <linux/mm.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/device.h>
 #include <linux/module.h>
 

commit 3faa3d9a308e539cc48355b1f419a5ed9f8274a2
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Thu Jul 28 15:21:19 2016 -0400

    IB/hfi1: Make use of mm consistent
    
    The hfi1 driver registers a mmu_notifier callback when /dev/hfi1_* is
    opened, and unregisters it when the device is closed.  The driver
    incorrectly assumes that the close will always happen from the same
    context as the open.  In particular, closes due to SIGKILL or OOM killer
    activity may happen from a different context.  In these cases, the wrong
    mm is passed to mmu_notifier_unregister(), which causes improper reference
    counting for the victim mm, and eventual memory corruption.
    
    Preserve the mm for all open file descriptors and use this mm rather than
    current->mm for memory operations for the lifetime of that fd.  Note: this
    patch leaves 1 use of current->mm in place.  This use is removed in a
    follow on patch because other functional changes were required prior to
    that use being removed.
    
    If registration fails, there is no reason to keep the handler object
    around.  Free the handler object rather than add it to the list to
    prevent any mmu_notifier operations, including unregister, when
    registration fails.
    
    Suggested-by: Jim Foraker <foraker1@llnl.gov>
    Reviewed-by: Dean Luick <dean.luick@intel.com>
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
index 88e10b5f55f1..20f4ddcac3b0 100644
--- a/drivers/infiniband/hw/hfi1/user_pages.c
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -68,7 +68,8 @@ MODULE_PARM_DESC(cache_size, "Send and receive side cache size limit (in MB)");
  * could keeping caching buffers.
  *
  */
-bool hfi1_can_pin_pages(struct hfi1_devdata *dd, u32 nlocked, u32 npages)
+bool hfi1_can_pin_pages(struct hfi1_devdata *dd, struct mm_struct *mm,
+			u32 nlocked, u32 npages)
 {
 	unsigned long ulimit = rlimit(RLIMIT_MEMLOCK), pinned, cache_limit,
 		size = (cache_size * (1UL << 20)); /* convert to bytes */
@@ -89,9 +90,9 @@ bool hfi1_can_pin_pages(struct hfi1_devdata *dd, u32 nlocked, u32 npages)
 	/* Convert to number of pages */
 	size = DIV_ROUND_UP(size, PAGE_SIZE);
 
-	down_read(&current->mm->mmap_sem);
-	pinned = current->mm->pinned_vm;
-	up_read(&current->mm->mmap_sem);
+	down_read(&mm->mmap_sem);
+	pinned = mm->pinned_vm;
+	up_read(&mm->mmap_sem);
 
 	/* First, check the absolute limit against all pinned pages. */
 	if (pinned + npages >= ulimit && !can_lock)
@@ -100,8 +101,8 @@ bool hfi1_can_pin_pages(struct hfi1_devdata *dd, u32 nlocked, u32 npages)
 	return ((nlocked + npages) <= size) || can_lock;
 }
 
-int hfi1_acquire_user_pages(unsigned long vaddr, size_t npages, bool writable,
-			    struct page **pages)
+int hfi1_acquire_user_pages(struct mm_struct *mm, unsigned long vaddr, size_t npages,
+			    bool writable, struct page **pages)
 {
 	int ret;
 
@@ -109,9 +110,9 @@ int hfi1_acquire_user_pages(unsigned long vaddr, size_t npages, bool writable,
 	if (ret < 0)
 		return ret;
 
-	down_write(&current->mm->mmap_sem);
-	current->mm->pinned_vm += ret;
-	up_write(&current->mm->mmap_sem);
+	down_write(&mm->mmap_sem);
+	mm->pinned_vm += ret;
+	up_write(&mm->mmap_sem);
 
 	return ret;
 }

commit f48ad614c100783be1e7e777dc36328001b83999
Author: Dennis Dalessandro <dennis.dalessandro@intel.com>
Date:   Thu May 19 05:26:51 2016 -0700

    IB/hfi1: Move driver out of staging
    
    The TODO list for the hfi1 driver was completed during 4.6. In addition
    other objections raised (which are far beyond what was in the TODO list)
    have been addressed as well. It is now time to remove the driver from
    staging and into the drivers/infiniband sub-tree.
    
    Reviewed-by: Jubin John <jubin.john@intel.com>
    Signed-off-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/hfi1/user_pages.c b/drivers/infiniband/hw/hfi1/user_pages.c
new file mode 100644
index 000000000000..88e10b5f55f1
--- /dev/null
+++ b/drivers/infiniband/hw/hfi1/user_pages.c
@@ -0,0 +1,135 @@
+/*
+ * Copyright(c) 2015, 2016 Intel Corporation.
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * BSD LICENSE
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *  - Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  - Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in
+ *    the documentation and/or other materials provided with the
+ *    distribution.
+ *  - Neither the name of Intel Corporation nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ */
+
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/device.h>
+#include <linux/module.h>
+
+#include "hfi.h"
+
+static unsigned long cache_size = 256;
+module_param(cache_size, ulong, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(cache_size, "Send and receive side cache size limit (in MB)");
+
+/*
+ * Determine whether the caller can pin pages.
+ *
+ * This function should be used in the implementation of buffer caches.
+ * The cache implementation should call this function prior to attempting
+ * to pin buffer pages in order to determine whether they should do so.
+ * The function computes cache limits based on the configured ulimit and
+ * cache size. Use of this function is especially important for caches
+ * which are not limited in any other way (e.g. by HW resources) and, thus,
+ * could keeping caching buffers.
+ *
+ */
+bool hfi1_can_pin_pages(struct hfi1_devdata *dd, u32 nlocked, u32 npages)
+{
+	unsigned long ulimit = rlimit(RLIMIT_MEMLOCK), pinned, cache_limit,
+		size = (cache_size * (1UL << 20)); /* convert to bytes */
+	unsigned usr_ctxts = dd->num_rcv_contexts - dd->first_user_ctxt;
+	bool can_lock = capable(CAP_IPC_LOCK);
+
+	/*
+	 * Calculate per-cache size. The calculation below uses only a quarter
+	 * of the available per-context limit. This leaves space for other
+	 * pinning. Should we worry about shared ctxts?
+	 */
+	cache_limit = (ulimit / usr_ctxts) / 4;
+
+	/* If ulimit isn't set to "unlimited" and is smaller than cache_size. */
+	if (ulimit != (-1UL) && size > cache_limit)
+		size = cache_limit;
+
+	/* Convert to number of pages */
+	size = DIV_ROUND_UP(size, PAGE_SIZE);
+
+	down_read(&current->mm->mmap_sem);
+	pinned = current->mm->pinned_vm;
+	up_read(&current->mm->mmap_sem);
+
+	/* First, check the absolute limit against all pinned pages. */
+	if (pinned + npages >= ulimit && !can_lock)
+		return false;
+
+	return ((nlocked + npages) <= size) || can_lock;
+}
+
+int hfi1_acquire_user_pages(unsigned long vaddr, size_t npages, bool writable,
+			    struct page **pages)
+{
+	int ret;
+
+	ret = get_user_pages_fast(vaddr, npages, writable, pages);
+	if (ret < 0)
+		return ret;
+
+	down_write(&current->mm->mmap_sem);
+	current->mm->pinned_vm += ret;
+	up_write(&current->mm->mmap_sem);
+
+	return ret;
+}
+
+void hfi1_release_user_pages(struct mm_struct *mm, struct page **p,
+			     size_t npages, bool dirty)
+{
+	size_t i;
+
+	for (i = 0; i < npages; i++) {
+		if (dirty)
+			set_page_dirty_lock(p[i]);
+		put_page(p[i]);
+	}
+
+	if (mm) { /* during close after signal, mm can be NULL */
+		down_write(&mm->mmap_sem);
+		mm->pinned_vm -= npages;
+		up_write(&mm->mmap_sem);
+	}
+}
