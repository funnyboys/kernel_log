commit d29d58e772ecde84df15f547dc06f0b7afc7ae5c
Author: Gal Pressman <galpress@amazon.com>
Date:   Thu May 28 16:45:47 2020 -0300

    RDMA/mlx5: Remove FMR leftovers
    
    Remove a few leftovers from FMR functionality which are no longer used.
    
    Link: https://lore.kernel.org/r/5-v3-f58e6669d5d3+2cf-fmr_removal_jgg@mellanox.com
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Acked-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2a702fa9e943..5dbe3eb0d9cb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -675,12 +675,6 @@ struct umr_common {
 	struct semaphore	sem;
 };
 
-enum {
-	MLX5_FMR_INVALID,
-	MLX5_FMR_VALID,
-	MLX5_FMR_BUSY,
-};
-
 struct mlx5_cache_ent {
 	struct list_head	head;
 	/* sync access to the cahce entry
@@ -1253,8 +1247,6 @@ int mlx5_query_mad_ifc_port(struct ib_device *ibdev, u8 port,
 			    struct ib_port_attr *props);
 int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,
 		       struct ib_port_attr *props);
-int mlx5_ib_init_fmr(struct mlx5_ib_dev *dev);
-void mlx5_ib_cleanup_fmr(struct mlx5_ib_dev *dev);
 void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
 			unsigned long max_page_shift,
 			int *count, int *shift,

commit 802dcc7fc5ec0932bea0f33db046cc744aecf233
Author: Mark Zhang <markz@mellanox.com>
Date:   Wed May 27 08:50:14 2020 +0300

    RDMA/mlx5: Support TX port affinity for VF drivers in LAG mode
    
    The mlx5 VF driver doesn't set QP tx port affinity because it doesn't know
    if the lag is active or not, since the "lag_active" works only for PF
    interfaces. In this case for VF interfaces only one lag is used which
    brings performance issue.
    
    Add a lag_tx_port_affinity CAP bit; When it is enabled and
    "num_lag_ports > 1", then driver always set QP tx affinity, regardless
    of lag state.
    
    Link: https://lore.kernel.org/r/20200527055014.355093-1-leon@kernel.org
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 482b54eb9764..2a702fa9e943 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1550,4 +1550,11 @@ static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
 
 int mlx5_ib_enable_driver(struct ib_device *dev);
 int mlx5_ib_test_wc(struct mlx5_ib_dev *dev);
+
+static inline bool mlx5_ib_lag_should_assign_affinity(struct mlx5_ib_dev *dev)
+{
+	return dev->lag_active ||
+		(MLX5_CAP_GEN(dev->mdev, num_lag_ports) > 1 &&
+		 MLX5_CAP_GEN(dev->mdev, lag_tx_port_affinity));
+}
 #endif /* MLX5_IB_H */

commit 029e88fd1e6142ded73f07e2baef3e8a2a87e0ed
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed May 6 09:55:13 2020 +0300

    RDMA/mlx5: Move all WR logic from qp.c to separate file
    
    Split qp.c by removing all WR logic to separate file.
    
    Link: https://lore.kernel.org/r/20200506065513.4668-4-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3041808773e6..482b54eb9764 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1178,10 +1178,6 @@ int mlx5_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr
 int mlx5_ib_destroy_qp(struct ib_qp *qp, struct ib_udata *udata);
 void mlx5_ib_drain_sq(struct ib_qp *qp);
 void mlx5_ib_drain_rq(struct ib_qp *qp);
-int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
-		      const struct ib_send_wr **bad_wr);
-int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
-		      const struct ib_recv_wr **bad_wr);
 int mlx5_ib_read_wqe_sq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
 			size_t buflen, size_t *bc);
 int mlx5_ib_read_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,

commit 5ac55dfc6d92c12d5ef423cd16165eb0350f8f51
Author: Mark Zhang <markz@mellanox.com>
Date:   Mon May 4 08:19:35 2020 +0300

    RDMA/mlx5: Set UDP source port based on the grh.flow_label
    
    Calculate UDP source port based on the grh.flow_label. If grh.flow_label
    is not valid, we will use minimal supported UDP source port.
    
    Link: https://lore.kernel.org/r/20200504051935.269708-6-leon@kernel.org
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f250753319d0..3041808773e6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1356,8 +1356,8 @@ int mlx5_ib_get_vf_guid(struct ib_device *device, int vf, u8 port,
 int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
 			u64 guid, int type);
 
-__be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev,
-			       const struct ib_gid_attr *attr);
+__be16 mlx5_get_roce_udp_sport_min(const struct mlx5_ib_dev *dev,
+				   const struct ib_gid_attr *attr);
 
 void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
 void mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);

commit cfc1a89e449c02207952c72a4c0394691fdedf43
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Apr 30 22:21:46 2020 +0300

    RDMA/mlx5: Set lag tx affinity according to slave
    
    The patch sets the lag tx affinity of the data QPs and the GSI QPs
    according to the LAG xmit slave.
    
    For GSI QPs, in case the link layer is Ethenet (RoCE) we create two GSI
    QPs, one for each physical port. When the driver selects the GSI QP, it
    will consider the port affinity result.  For connected QPs, the driver
    sets the affinity of the xmit slave.
    
    The above, ensures that RC QP and it's corresponding GSI QP will transmit
    from the same physical port.
    
    Link: https://lore.kernel.org/r/20200430192146.12863-17-maorg@mellanox.com
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7dffc87601eb..f250753319d0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -461,6 +461,7 @@ struct mlx5_ib_qp {
 	 * but not take effective
 	 */
 	u32                     counter_pending;
+	u16			gsi_lag_port;
 };
 
 struct mlx5_ib_cq_buf {

commit fa5d010c5630b143b802e0477e87bba0656829cf
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Apr 30 22:21:42 2020 +0300

    RDMA: Group create AH arguments in struct
    
    Following patch adds additional argument to the create AH function, so it
    make sense to group ah_attr and flags arguments in struct.
    
    Link: https://lore.kernel.org/r/20200430192146.12863-13-maorg@mellanox.com
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Acked-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Acked-by: Weihang Li <liweihang@huawei.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index df375cb4efbb..7dffc87601eb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1153,7 +1153,7 @@ void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db)
 void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
-int mlx5_ib_create_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr, u32 flags,
+int mlx5_ib_create_ah(struct ib_ah *ah, struct rdma_ah_init_attr *init_attr,
 		      struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
 void mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags);

commit 03c4077b284056fdb144f84aaa0ac0c80023f597
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:24 2020 +0300

    RDMA/mlx5: Rely on existence of udata to separate kernel/user flows
    
    Instead of keeping special field to separate kernel/user create/destroy
    flows, rely on existence of udata pointer. All allocation flows are
    using kzalloc() and leave uninitialized pointers as NULL which makes
    MLX5_QP_EMPTY and MLX5_QP_KERNEL flows to be the same.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-25-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 82ea01a211dd..df375cb4efbb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -337,7 +337,6 @@ struct mlx5_ib_rwq {
 	struct ib_umem		*umem;
 	size_t			buf_size;
 	unsigned int		page_shift;
-	int			create_type;
 	struct mlx5_db		db;
 	u32			user_index;
 	u32			wqe_count;
@@ -346,17 +345,6 @@ struct mlx5_ib_rwq {
 	u32			create_flags; /* Use enum mlx5_ib_wq_flags */
 };
 
-enum {
-	MLX5_QP_USER,
-	MLX5_QP_KERNEL,
-	MLX5_QP_EMPTY
-};
-
-enum {
-	MLX5_WQ_USER,
-	MLX5_WQ_KERNEL
-};
-
 struct mlx5_ib_rwq_ind_table {
 	struct ib_rwq_ind_table ib_rwq_ind_tbl;
 	u32			rqtn;
@@ -457,8 +445,6 @@ struct mlx5_ib_qp {
 	 */
 	int			bfregn;
 
-	int			create_type;
-
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;

commit 7aede1a25f4b84318e8a266d7b830a5ed554e370
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:20 2020 +0300

    RDMA/mlx5: Store QP type in the vendor QP structure
    
    QP type is stored in the IB/core QP struct, but it doesn't have all the
    needed information, like internal QP type used in the driver itself.
    Update mlx5_ib to have cached QP type which includes both IBTA and
    Mellanox specific one.
    
    Such change allows us to make even further cleanup of QP creation flow.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-21-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 9b2baf119823..82ea01a211dd 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -465,8 +465,12 @@ struct mlx5_ib_qp {
 	struct mlx5_rate_limit	rl;
 	u32                     underlay_qpn;
 	u32			flags_en;
-	/* storage for qp sub type when core qp type is IB_QPT_DRIVER */
-	enum ib_qp_type		qp_sub_type;
+	/*
+	 * IB/core doesn't store low-level QP types, so
+	 * store both MLX and IBTA types in the field below.
+	 * IB_QPT_DRIVER will be break to DCI/DCT subtypes.
+	 */
+	enum ib_qp_type		type;
 	/* A flag to indicate if there's a new counter is configured
 	 * but not take effective
 	 */

commit a8f3ea61e1c826a1f882b3fffbb052390ddee310
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:17 2020 +0300

    RDMA/mlx5: Return all configured create flags through query QP
    
    The "flags" field in struct mlx5_ib_qp contains all UAPI flags
    configured at the create QP stage. Return all the data as is
    without masking.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-18-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b6467cadc384..9b2baf119823 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -443,6 +443,7 @@ struct mlx5_ib_qp {
 	/* serialize qp state modifications
 	 */
 	struct mutex		mutex;
+	/* cached variant of create_flags from struct ib_qp_init_attr */
 	u32			flags;
 	u8			port;
 	u8			state;

commit 90ecb37a751b6923bee846c4e19f73b943c6ffa1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:16 2020 +0300

    RDMA/mlx5: Change scatter CQE flag to be set like other vendor flags
    
    In similar way to wqe_sig, the scat_cqe was treated differently from
    other create QP vendor flags. Change it to be similar to other flags
    and use flags_en mechanism.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-17-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 61a96c1dd125..b6467cadc384 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -446,7 +446,6 @@ struct mlx5_ib_qp {
 	u32			flags;
 	u8			port;
 	u8			state;
-	int			scat_cqe;
 	int			max_inline_data;
 	struct mlx5_bf	        bf;
 	u8			has_rq:1;

commit c95e6d53970254fa04a09c0fd79ae2cfa54cd1f5
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:15 2020 +0300

    RDMA/mlx5: Use flags_en mechanism to mark QP created with WQE signature
    
    MLX5_QP_FLAG_SIGNATURE is exposed to the users but in the kernel
    the create_qp flow treated it differently from other MLX5_QP_FLAG_*s.
    Fix it by ditching wq_sig boolean variable and use general flag_en
    mechanism.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-16-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b144660a47e1..61a96c1dd125 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -446,7 +446,6 @@ struct mlx5_ib_qp {
 	u32			flags;
 	u8			port;
 	u8			state;
-	int			wq_sig;
 	int			scat_cqe;
 	int			max_inline_data;
 	struct mlx5_bf	        bf;

commit 2be08c308f102eeaee7ffc4a0d08ecee82b77f9d
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Mon Apr 27 18:46:13 2020 +0300

    RDMA/mlx5: Delete create QP flags obfuscation
    
    There is no point in redefinition of stable and exposed to users create
    flags. Their values won't be changed and it is equal to used by the
    mlx5. Delete the mlx5 definitions and use IB/core fields.
    
    Link: https://lore.kernel.org/r/20200427154636.381474-14-leon@kernel.org
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index aaabb8a98eed..b144660a47e1 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -450,7 +450,8 @@ struct mlx5_ib_qp {
 	int			scat_cqe;
 	int			max_inline_data;
 	struct mlx5_bf	        bf;
-	int			has_rq;
+	u8			has_rq:1;
+	u8			is_rss:1;
 
 	/* only for user space QPs. For kernel
 	 * we have it from the bf object
@@ -481,24 +482,6 @@ struct mlx5_ib_cq_buf {
 	int			nent;
 };
 
-enum mlx5_ib_qp_flags {
-	MLX5_IB_QP_LSO                          = IB_QP_CREATE_IPOIB_UD_LSO,
-	MLX5_IB_QP_BLOCK_MULTICAST_LOOPBACK     = IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,
-	MLX5_IB_QP_CROSS_CHANNEL            = IB_QP_CREATE_CROSS_CHANNEL,
-	MLX5_IB_QP_MANAGED_SEND             = IB_QP_CREATE_MANAGED_SEND,
-	MLX5_IB_QP_MANAGED_RECV             = IB_QP_CREATE_MANAGED_RECV,
-	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 5,
-	/* QP uses 1 as its source QP number */
-	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
-	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
-	MLX5_IB_QP_RSS				= 1 << 8,
-	MLX5_IB_QP_CVLAN_STRIPPING		= 1 << 9,
-	MLX5_IB_QP_UNDERLAY			= 1 << 10,
-	MLX5_IB_QP_PCI_WRITE_END_PADDING	= 1 << 11,
-	MLX5_IB_QP_TUNNEL_OFFLOAD		= 1 << 12,
-	MLX5_IB_QP_PACKET_BASED_CREDIT		= 1 << 13,
-};
-
 struct mlx5_umr_wr {
 	struct ib_send_wr		wr;
 	u64				virt_addr;

commit 333fbaa0255b8d471fc7ae767ef3a1766c732d6d
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sat Apr 4 10:40:24 2020 +0300

    net/mlx5: Move QP logic to mlx5_ib
    
    The mlx5_core doesn't need any functionality coded in qp.c, so move
    that file to drivers/infiniband/ be under mlx5_ib responsibility.
    
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index cb2a021aa93c..aaabb8a98eed 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -869,6 +869,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_CAPS,
 	MLX5_IB_STAGE_NON_DEFAULT_CB,
 	MLX5_IB_STAGE_ROCE,
+	MLX5_IB_STAGE_QP,
 	MLX5_IB_STAGE_SRQ,
 	MLX5_IB_STAGE_DEVICE_RESOURCES,
 	MLX5_IB_STAGE_DEVICE_NOTIFIER,
@@ -1064,6 +1065,7 @@ struct mlx5_ib_dev {
 	struct mlx5_dm		dm;
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
+	struct mlx5_qp_table    qp_table;
 	struct mlx5_async_ctx   async_ctx;
 	struct mlx5_devx_event_table devx_event_table;
 	struct mlx5_var_table var_table;

commit bfd745f8f327c40d74c8207ca62db05a264b5b7c
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Fri Apr 3 13:31:18 2020 +0300

    RDMA/mlx5: Delete Q counter allocations command
    
    Remove mlx5_ib implementation of Q counter allocation logic
    together with cleaning boolean which controlled validity of the
    counter. It is not needed, because counter_id == 0 means that
    counter is not valid.
    
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a4e522385de0..cb2a021aa93c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -780,7 +780,6 @@ struct mlx5_ib_counters {
 	u32 num_cong_counters;
 	u32 num_ext_ppcnt_counters;
 	u16 set_id;
-	bool set_id_valid;
 };
 
 struct mlx5_ib_multiport_info;

commit 919dce24701f7b34681a6a1d3ef95c9f6c4fb1cc
Merge: 50a5de895dbe b4d8ddf8356d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 1 18:18:18 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "The majority of the patches are cleanups, refactorings and clarity
      improvements.
    
      This cycle saw some more activity from Syzkaller, I think we are now
      clean on all but one of those bugs, including the long standing and
      obnoxious rdma_cm locking design defect. Continue to see many drivers
      getting cleanups, with a few new user visible features.
    
      Summary:
    
       - Various driver updates for siw, bnxt_re, rxe, efa, mlx5, hfi1
    
       - Lots of cleanup patches for hns
    
       - Convert more places to use refcount
    
       - Aggressively lock the RDMA CM code that syzkaller says isn't
         working
    
       - Work to clarify ib_cm
    
       - Use the new ib_device lifecycle model in bnxt_re
    
       - Fix mlx5's MR cache which seems to be failing more often with the
         new ODP code
    
       - mlx5 'dynamic uar' and 'tx steering' user interfaces"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (144 commits)
      RDMA/bnxt_re: make bnxt_re_ib_init static
      IB/qib: Delete struct qib_ivdev.qp_rnd
      RDMA/hns: Fix uninitialized variable bug
      RDMA/hns: Modify the mask of QP number for CQE of hip08
      RDMA/hns: Reduce the maximum number of extend SGE per WQE
      RDMA/hns: Reduce PFC frames in congestion scenarios
      RDMA/mlx5: Add support for RDMA TX flow table
      net/mlx5: Add support for RDMA TX steering
      IB/hfi1: Call kobject_put() when kobject_init_and_add() fails
      IB/hfi1: Fix memory leaks in sysfs registration and unregistration
      IB/mlx5: Move to fully dynamic UAR mode once user space supports it
      IB/mlx5: Limit the scope of struct mlx5_bfreg_info to mlx5_ib
      IB/mlx5: Extend QP creation to get uar page index from user space
      IB/mlx5: Extend CQ creation to get uar page index from user space
      IB/mlx5: Expose UAR object and its alloc/destroy commands
      IB/hfi1: Get rid of a warning
      RDMA/hns: Remove redundant judgment of qp_type
      RDMA/hns: Remove redundant assignment of wc->smac when polling cq
      RDMA/hns: Remove redundant qpc setup operations
      RDMA/hns: Remove meaningless prints
      ...

commit e999a7343da734f24643fcfcfa821e214126480f
Merge: c189b5483c1b 826096d84f50
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sun Mar 29 23:41:50 2020 -0700

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    * 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux:
      mlx5: Remove uninitialized use of key in mlx5_core_create_mkey
      {IB,net}/mlx5: Move asynchronous mkey creation to mlx5_ib
      {IB,net}/mlx5: Assign mkey variant in mlx5_ib only
      {IB,net}/mlx5: Setup mkey variant before mr create command invocation
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit dbdf8909d03d8ab23dc7a6d6edce0791d6a9055c
Merge: dfb5394f804e af9c38411d18
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Mar 27 13:26:59 2020 -0300

    Merge branch 'mlx5_tx_steering' into rdma.git for-next
    
    Leon Romanovsky says:
    
    ====================
    Those two patches from Michael extends mlx5_core and mlx5_ib flow steering
    to support RDMA TX in similar way to already supported RDMA RX.
    ====================
    
    Based on the mlx5-next branch at
     git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    Due to dependencies
    
    * branch 'mlx5_tx_steering':
      RDMA/mlx5: Add support for RDMA TX flow table
      net/mlx5: Add support for RDMA TX steering

commit af9c38411d188021900031d00bd8e8dafd4ad557
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Tue Mar 24 08:14:25 2020 +0200

    RDMA/mlx5: Add support for RDMA TX flow table
    
    Enable user application to add rules for RDMA TX steering table.
    Rules in this steering table will allow to steer transmitted RDMA
    traffic.
    
    Link: https://lore.kernel.org/r/20200324061425.1570190-3-leon@kernel.org
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 89a050e516a8..6fe01d6142aa 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -210,6 +210,7 @@ struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	egress[MLX5_IB_NUM_EGRESS_FTS];
 	struct mlx5_ib_flow_prio	fdb;
 	struct mlx5_ib_flow_prio	rdma_rx[MLX5_IB_NUM_FLOW_FT];
+	struct mlx5_ib_flow_prio	rdma_tx[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_flow_table		*lag_demux_ft;
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.

commit 0a2fd01c28ae490a639a32a52b81fb2df48b92a0
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Mar 24 08:01:43 2020 +0200

    IB/mlx5: Move to fully dynamic UAR mode once user space supports it
    
    Move to fully dynamic UAR mode once user space supports it.  In this case
    we prevent any legacy mode of UARs on the allocated context and prevent
    redundant allocation of the static ones.
    
    Link: https://lore.kernel.org/r/20200324060143.1569116-6-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ae6289133cd8..b88414acb993 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -139,6 +139,7 @@ struct mlx5_bfreg_info {
 	struct mutex lock;
 	u32 ver;
 	u8 lib_uar_4k : 1;
+	u8 lib_uar_dyn : 1;
 	u32 num_sys_pages;
 	u32 num_static_sys_pages;
 	u32 total_num_bfregs;

commit 2152862298fbfd237d37c231dfca8ae8f3ed0e48
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Mar 24 08:01:42 2020 +0200

    IB/mlx5: Limit the scope of struct mlx5_bfreg_info to mlx5_ib
    
    struct mlx5_bfreg_info is used by mlx5_ib only but is exposed to both RDMA
    and netdev parts of mlx5 driver. Move that struct to mlx5_ib namespace,
    clean vertical space alignment and convert lib_uar_4k from bool to
    bitfield.
    
    Link: https://lore.kernel.org/r/20200324060143.1569116-5-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3ba6175f949e..ae6289133cd8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -128,6 +128,23 @@ enum mlx5_ib_mmap_type {
 	MLX5_IB_MMAP_TYPE_UAR_NC = 4,
 };
 
+struct mlx5_bfreg_info {
+	u32 *sys_pages;
+	int num_low_latency_bfregs;
+	unsigned int *count;
+
+	/*
+	 * protect bfreg allocation data structs
+	 */
+	struct mutex lock;
+	u32 ver;
+	u8 lib_uar_4k : 1;
+	u32 num_sys_pages;
+	u32 num_static_sys_pages;
+	u32 total_num_bfregs;
+	u32 num_dyn_bfregs;
+};
+
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;

commit 342ee59de98a2ecdf15a46849a2534e7c808eb1f
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Mar 24 08:01:39 2020 +0200

    IB/mlx5: Expose UAR object and its alloc/destroy commands
    
    Expose UAR object and its alloc/destroy commands to be used over the ioctl
    interface by user space applications.
    
    This API supports both BF & NC modes and enables a dynamic allocation of
    UARs once really needed.
    
    As the number of driver objects were limited by the core ones when the
    merged tree is prepared, had to decrease the number of core objects to
    enable the new UAR object usage.
    
    Link: https://lore.kernel.org/r/20200324060143.1569116-2-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2be773a24dda..3ba6175f949e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -124,6 +124,8 @@ enum {
 enum mlx5_ib_mmap_type {
 	MLX5_IB_MMAP_TYPE_MEMIC = 1,
 	MLX5_IB_MMAP_TYPE_VAR = 2,
+	MLX5_IB_MMAP_TYPE_UAR_WC = 3,
+	MLX5_IB_MMAP_TYPE_UAR_NC = 4,
 };
 
 struct mlx5_ib_ucontext {

commit 950bf4f17725556bbc773a5b71e88a6c14c9ff25
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Mar 18 11:16:40 2020 +0200

    RDMA/mlx5: Fix access to wrong pointer while performing flush due to error
    
    The main difference between send and receive SW completions is related to
    separate treatment of WQ queue. For receive completions, the initial index
    to be flushed is stored in "tail", while for send completions, it is in
    deleted "last_poll".
    
      CPU: 54 PID: 53405 Comm: kworker/u161:0 Kdump: loaded Tainted: G           OE    --------- -t - 4.18.0-147.el8.ppc64le #1
      Workqueue: ib-comp-unb-wq ib_cq_poll_work [ib_core]
      NIP:  c000003c7c00a000 LR: c00800000e586af4 CTR: c000003c7c00a000
      REGS: c0000036cc9db940 TRAP: 0400   Tainted: G           OE    --------- -t -  (4.18.0-147.el8.ppc64le)
      MSR:  9000000010009033 <SF,HV,EE,ME,IR,DR,RI,LE>  CR: 24004488  XER: 20040000
      CFAR: c00800000e586af0 IRQMASK: 0
      GPR00: c00800000e586ab4 c0000036cc9dbbc0 c00800000e5f1a00 c0000037d8433800
      GPR04: c000003895a26800 c0000037293f2000 0000000000000201 0000000000000011
      GPR08: c000003895a26c80 c000003c7c00a000 0000000000000000 c00800000ed30438
      GPR12: c000003c7c00a000 c000003fff684b80 c00000000017c388 c00000396ec4be40
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: c00000000151e498 0000000000000010 c000003895a26848 0000000000000010
      GPR24: 0000000000000010 0000000000010000 c000003895a26800 0000000000000000
      GPR28: 0000000000000010 c0000037d8433800 c000003895a26c80 c000003895a26800
      NIP [c000003c7c00a000] 0xc000003c7c00a000
      LR [c00800000e586af4] __ib_process_cq+0xec/0x1b0 [ib_core]
      Call Trace:
      [c0000036cc9dbbc0] [c00800000e586ab4] __ib_process_cq+0xac/0x1b0 [ib_core] (unreliable)
      [c0000036cc9dbc40] [c00800000e586c88] ib_cq_poll_work+0x40/0xb0 [ib_core]
      [c0000036cc9dbc70] [c000000000171f44] process_one_work+0x2f4/0x5c0
      [c0000036cc9dbd10] [c000000000172a0c] worker_thread+0xcc/0x760
      [c0000036cc9dbdc0] [c00000000017c52c] kthread+0x1ac/0x1c0
      [c0000036cc9dbe30] [c00000000000b75c] ret_from_kernel_thread+0x5c/0x80
    
    Fixes: 8e3b68830186 ("RDMA/mlx5: Delete unreachable handle_atomic code by simplifying SW completion")
    Link: https://lore.kernel.org/r/20200318091640.44069-1-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index bb78142bca5e..f3bdbd5e5096 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -288,6 +288,7 @@ struct mlx5_ib_wq {
 	unsigned		head;
 	unsigned		tail;
 	u16			cur_post;
+	u16			last_poll;
 	void			*cur_edge;
 };
 

commit d613bd64c68bab6712c472281e79559bdc984b62
Merge: a4f994a05926 aad719dcf379
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Mar 13 11:11:07 2020 -0300

    Merge branch 'mlx5_mr_cache' into rdma.git for-next
    
    Leon Romanovsky says:
    
    ====================
    This series fixes various corner cases in the mlx5_ib MR cache
    implementation, see specific commit messages for more information.
    ====================
    
    Based on the mlx5-next branch at
     git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    Due to dependencies
    
    * branch 'mlx5_mr-cache':
      RDMA/mlx5: Allow MRs to be created in the cache synchronously
      RDMA/mlx5: Revise how the hysteresis scheme works for cache filling
      RDMA/mlx5: Fix locking in MR cache work queue
      RDMA/mlx5: Lock access to ent->available_mrs/limit when doing queue_work
      RDMA/mlx5: Fix MR cache size and limit debugfs
      RDMA/mlx5: Always remove MRs from the cache before destroying them
      RDMA/mlx5: Simplify how the MR cache bucket is located
      RDMA/mlx5: Rename the tracking variables for the MR cache
      RDMA/mlx5: Replace spinlock protected write with atomic var
      {IB,net}/mlx5: Move asynchronous mkey creation to mlx5_ib
      {IB,net}/mlx5: Assign mkey variant in mlx5_ib only
      {IB,net}/mlx5: Setup mkey variant before mr create command invocation

commit aad719dcf379f1413dcb168413a53fea66e2ef90
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 10:22:38 2020 +0200

    RDMA/mlx5: Allow MRs to be created in the cache synchronously
    
    If the cache is completely out of MRs, and we are running in cache mode,
    then directly, and synchronously, create an MR that is compatible with the
    cache bucket using a sleeping mailbox command. This ensures that the
    thread that is waiting for the MR absolutely will get one.
    
    When a MR allocated in this way becomes freed then it is compatible with
    the cache bucket and will be recycled back into it.
    
    Deletes the very buggy ent->compl scheme to create a synchronous MR
    allocation.
    
    Link: https://lore.kernel.org/r/20200310082238.239865-13-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1216575292a7..a5da2d5cf659 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -722,7 +722,6 @@ struct mlx5_cache_ent {
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	work;
 	struct delayed_work	dwork;
-	struct completion	compl;
 };
 
 struct mlx5_mr_cache {

commit 1c78a21a0c6f8df98a2281c1c0232f9c85e1c44d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 10:22:37 2020 +0200

    RDMA/mlx5: Revise how the hysteresis scheme works for cache filling
    
    Currently if the work queue is running then it is in 'hysteresis' mode and
    will fill until the cache reaches the high water mark. This implicit state
    is very tricky and doesn't interact with pending very well.
    
    Instead of self re-scheduling the work queue after the add_keys() has
    started to create the new MR, have the queue scheduled from
    reg_mr_callback() only after the requested MR has been added.
    
    This avoids the bad design of an in-rush of queue'd work doing back to
    back add_keys() until EAGAIN then sleeping. The add_keys() will be paced
    one at a time as they complete, slowly filling up the cache.
    
    Also, fix pending to be only manipulated under lock.
    
    Link: https://lore.kernel.org/r/20200310082238.239865-12-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a22932ffb9c8..1216575292a7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -700,6 +700,7 @@ struct mlx5_cache_ent {
 	u32			page;
 
 	u8 disabled:1;
+	u8 fill_to_high_water:1;
 
 	/*
 	 * - available_mrs is the length of list head, ie the number of MRs

commit b9358bdbc713cb64b8701bcbd450edc155d761a1
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 10:22:36 2020 +0200

    RDMA/mlx5: Fix locking in MR cache work queue
    
    All of the members of mlx5_cache_ent must be accessed while holding the
    spinlock, add the missing spinlock in the __cache_work_func().
    
    Using cache->stopped and flush_workqueue() is an inherently racy way to
    shutdown self-scheduling work on a queue. Replace it with ent->disabled
    under lock, and always check disabled before queuing any new work. Use
    cancel_work_sync() to shutdown the queue.
    
    Use READ_ONCE/WRITE_ONCE for dev->last_add to manage concurrency as
    coherency is less important here.
    
    Split fill_delay from the bitfield. C bitfield updates are not atomic and
    this is just a mess. Use READ_ONCE/WRITE_ONCE, but this could also use
    test_bit()/set_bit().
    
    Link: https://lore.kernel.org/r/20200310082238.239865-11-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7208946d2787..a22932ffb9c8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -699,6 +699,8 @@ struct mlx5_cache_ent {
 	u32			access_mode;
 	u32			page;
 
+	u8 disabled:1;
+
 	/*
 	 * - available_mrs is the length of list head, ie the number of MRs
 	 *   available for immediate allocation.
@@ -725,7 +727,6 @@ struct mlx5_cache_ent {
 struct mlx5_mr_cache {
 	struct workqueue_struct *wq;
 	struct mlx5_cache_ent	ent[MAX_MR_CACHE_ENTRIES];
-	int			stopped;
 	struct dentry		*root;
 	unsigned long		last_add;
 };
@@ -995,10 +996,10 @@ struct mlx5_ib_dev {
 	 */
 	struct mutex			cap_mask_mutex;
 	u8				ib_active:1;
-	u8				fill_delay:1;
 	u8				is_rep:1;
 	u8				lag_active:1;
 	u8				wc_support:1;
+	u8				fill_delay;
 	struct umr_common		umrc;
 	/* sync used page count stats
 	 */

commit b91e1751fbcee7692e45308e74d8816c43802ede
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 10:22:32 2020 +0200

    RDMA/mlx5: Simplify how the MR cache bucket is located
    
    There are many bad APIs here that are accepting a cache bucket index
    instead of a bucket pointer. Many of the callers already have a bucket
    pointer, so this results in a lot of confusing uses of order2idx().
    
    Pass the struct mlx5_cache_ent into add_keys(), remove_keys(), and
    alloc_cached_mr().
    
    Once the MR is in the cache, store the cache bucket pointer directly in
    the MR, replacing the 'bool allocated_from cache'.
    
    In the end there is only one place that needs to form index from order,
    alloc_mr_from_cache(). Increase the safety of this function by disallowing
    it from accessing cache entries in the ODP special area.
    
    Link: https://lore.kernel.org/r/20200310082238.239865-7-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 731b0f7bbe5b..7208946d2787 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -617,8 +617,8 @@ struct mlx5_ib_mr {
 	struct ib_umem	       *umem;
 	struct mlx5_shared_mr_info	*smr_info;
 	struct list_head	list;
-	int			order;
-	bool			allocated_from_cache;
+	unsigned int		order;
+	struct mlx5_cache_ent  *cache_ent;
 	int			npages;
 	struct mlx5_ib_dev     *dev;
 	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
@@ -1274,7 +1274,8 @@ int mlx5_ib_get_cqe_size(struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 
-struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry);
+struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev,
+				       unsigned int entry);
 void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
 int mlx5_mr_cache_invalidate(struct mlx5_ib_mr *mr);
 

commit 7c8691a396bd2084c7abc02d7aa34dade597814d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 10:22:31 2020 +0200

    RDMA/mlx5: Rename the tracking variables for the MR cache
    
    The old names do not clearly indicate the intent.
    
    Link: https://lore.kernel.org/r/20200310082238.239865-6-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3445402b23cc..731b0f7bbe5b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -699,15 +699,26 @@ struct mlx5_cache_ent {
 	u32			access_mode;
 	u32			page;
 
-	u32			size;
-	u32                     cur;
+	/*
+	 * - available_mrs is the length of list head, ie the number of MRs
+	 *   available for immediate allocation.
+	 * - total_mrs is available_mrs plus all in use MRs that could be
+	 *   returned to the cache.
+	 * - limit is the low water mark for available_mrs, 2* limit is the
+	 *   upper water mark.
+	 * - pending is the number of MRs currently being created
+	 */
+	u32 total_mrs;
+	u32 available_mrs;
+	u32 limit;
+	u32 pending;
+
+	/* Statistics */
 	u32                     miss;
-	u32			limit;
 
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	work;
 	struct delayed_work	dwork;
-	int			pending;
 	struct completion	compl;
 };
 

commit f743ff3b37dfc8e8cf4a32d4254cddd5f0f1add8
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Mar 10 10:22:29 2020 +0200

    RDMA/mlx5: Replace spinlock protected write with atomic var
    
    mkey variant calculation was spinlock protected to make it atomic, replace
    that with one atomic variable.
    
    Link: https://lore.kernel.org/r/20200310082238.239865-4-leon@kernel.org
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 89a050e516a8..3445402b23cc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -993,10 +993,7 @@ struct mlx5_ib_dev {
 	 */
 	struct mlx5_ib_resources	devr;
 
-	/* protect mkey key part */
-	spinlock_t			mkey_lock;
-	u8				mkey_key;
-
+	atomic_t			mkey_var;
 	struct mlx5_mr_cache		cache;
 	struct timer_list		delay_timer;
 	/* Prevents soft lock on massive reg MRs */

commit fc6a9f86f08acd3665f788619afae0d2b2d5a480
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Mar 10 10:22:28 2020 +0200

    {IB,net}/mlx5: Assign mkey variant in mlx5_ib only
    
    mkey variant is not required for mlx5_core use, move the mkey variant
    counter to mlx5_ib.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d9bffcc93587..89a050e516a8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -992,6 +992,11 @@ struct mlx5_ib_dev {
 	/* sync used page count stats
 	 */
 	struct mlx5_ib_resources	devr;
+
+	/* protect mkey key part */
+	spinlock_t			mkey_lock;
+	u8				mkey_key;
+
 	struct mlx5_mr_cache		cache;
 	struct timer_list		delay_timer;
 	/* Prevents soft lock on massive reg MRs */

commit a762d460a06abc8d462ac513ba57dc3c31dd8c73
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Mar 10 11:14:31 2020 +0200

    RDMA/mlx5: Use offsetofend() instead of duplicated variant
    
    Convert mlx5 driver to use offsetofend() instead of its duplicated
    variant.
    
    Link: https://lore.kernel.org/r/20200310091438.248429-5-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2e42258e6fce..4b7d0dfabea2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -64,8 +64,6 @@
 	dev_warn(&(_dev)->ib_dev.dev, "%s:%d:(pid %d): " format, __func__,     \
 		 __LINE__, current->pid, ##arg)
 
-#define field_avail(type, fld, sz) (offsetof(type, fld) +		\
-				    sizeof(((type *)0)->fld) <= (sz))
 #define MLX5_IB_DEFAULT_UIDX 0xffffff
 #define MLX5_USER_ASSIGNED_UIDX_MASK __mlx5_mask(qpc, user_index)
 
@@ -1475,12 +1473,11 @@ static inline int get_qp_user_index(struct mlx5_ib_ucontext *ucontext,
 {
 	u8 cqe_version = ucontext->cqe_version;
 
-	if (field_avail(struct mlx5_ib_create_qp, uidx, inlen) &&
-	    !cqe_version && (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
+	if ((offsetofend(typeof(*ucmd), uidx) <= inlen) && !cqe_version &&
+	    (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
 		return 0;
 
-	if (!!(field_avail(struct mlx5_ib_create_qp, uidx, inlen) !=
-	       !!cqe_version))
+	if ((offsetofend(typeof(*ucmd), uidx) <= inlen) != !!cqe_version)
 		return -EINVAL;
 
 	return verify_assign_uidx(cqe_version, ucmd->uidx, user_index);
@@ -1493,12 +1490,11 @@ static inline int get_srq_user_index(struct mlx5_ib_ucontext *ucontext,
 {
 	u8 cqe_version = ucontext->cqe_version;
 
-	if (field_avail(struct mlx5_ib_create_srq, uidx, inlen) &&
-	    !cqe_version && (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
+	if ((offsetofend(typeof(*ucmd), uidx) <= inlen) && !cqe_version &&
+	    (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
 		return 0;
 
-	if (!!(field_avail(struct mlx5_ib_create_srq, uidx, inlen) !=
-	       !!cqe_version))
+	if ((offsetofend(typeof(*ucmd), uidx) <= inlen) != !!cqe_version)
 		return -EINVAL;
 
 	return verify_assign_uidx(cqe_version, ucmd->uidx, user_index);

commit 0897f301bc285409ab6453839fa6a43d9ceb26e0
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Tue Mar 10 09:57:06 2020 +0200

    RDMA/mlx5: Remove duplicate definitions of SW_ICM macros
    
    Those macros are already defined in include/linux/mlx5/driver.h, so delete
    their duplicate variants.
    
    Link: https://lore.kernel.org/r/20200310075706.238592-1-leon@kernel.org
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Yevgeny Kliteynik <kliteyn@mellanox.com>
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Reviewed-by: Alex Vesker <valex@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 798366bf6dd5..2e42258e6fce 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -128,10 +128,6 @@ enum mlx5_ib_mmap_type {
 	MLX5_IB_MMAP_TYPE_VAR = 2,
 };
 
-#define MLX5_LOG_SW_ICM_BLOCK_SIZE(dev)                                        \
-	(MLX5_CAP_DEV_MEM(dev, log_sw_icm_alloc_granularity))
-#define MLX5_SW_ICM_BLOCK_SIZE(dev) (1 << MLX5_LOG_SW_ICM_BLOCK_SIZE(dev))
-
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;

commit 6f00a54c2cdc82543d5804b0074821b5a40e577f
Merge: 3e3cf2e82cca 2c523b344dfa
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 12:49:09 2020 -0300

    Merge tag 'v5.6-rc5' into rdma.git for-next
    
    Required due to dependencies in following patches.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 3e3cf2e82cca92ecedba972251a20da4fa4ab1c8
Merge: 0aeb3622ea6f 30f2fe40c72b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Mar 10 11:54:17 2020 -0300

    Merge branch 'mlx5_packet_pacing' into rdma.git for-next
    
    Yishai Hadas Says:
    
    ====================
    Expose raw packet pacing APIs to be used by DEVX based applications.  The
    existing code was refactored to have a single flow with the new raw APIs.
    ====================
    
    Based on the mlx5-next branch at
     git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    Due to dependencies
    
    * branch 'mlx5_packet_pacing':
      IB/mlx5: Introduce UAPIs to manage packet pacing
      net/mlx5: Expose raw packet pacing APIs

commit 30f2fe40c72bfbdde7bc066cb862bd05014be9f1
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Wed Feb 19 21:05:18 2020 +0200

    IB/mlx5: Introduce UAPIs to manage packet pacing
    
    Introduce packet pacing uobject and its alloc and destroy
    methods.
    
    This uobject holds mlx5 packet pacing context according to the device
    specification and enables managing packet pacing device entries that are
    needed by DEVX applications.
    
    Link: https://lore.kernel.org/r/20200219190518.200912-3-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d9bffcc93587..09ce80febec7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -203,6 +203,11 @@ struct mlx5_ib_flow_matcher {
 	u8			match_criteria_enable;
 };
 
+struct mlx5_ib_pp {
+	u16 index;
+	struct mlx5_core_dev *mdev;
+};
+
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	egress_prios[MLX5_IB_NUM_FLOW_FT];
@@ -1381,6 +1386,7 @@ int mlx5_ib_fill_stat_entry(struct sk_buff *msg,
 
 extern const struct uapi_definition mlx5_ib_devx_defs[];
 extern const struct uapi_definition mlx5_ib_flow_defs[];
+extern const struct uapi_definition mlx5_ib_qos_defs[];
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);

commit 9e3aaf6883b3b47d9f14579c1a03c911dd17cf10
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Feb 27 14:52:46 2020 +0200

    IB/mlx5: Add np_min_time_between_cnps and rp_max_rate debug params
    
    Add two debugfs parameters described below.
    
    np_min_time_between_cnps - Minimum time between sending CNPs from the
                               port.
                               Unit = microseconds.
                               Default = 0 (no min wait time; generated
                               based on incoming ECN marked packets).
    
    rp_max_rate - Maximum rate at which reaction point node can transmit.
                  Once this limit is reached, RP is no longer rate limited.
                  Unit = Mbits/sec
                  Default = 0 (full speed)
    
    Link: https://lore.kernel.org/r/20200227125246.99472-1-leon@kernel.org
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f21d446249b8..3976071a5dc9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -792,6 +792,7 @@ enum mlx5_ib_dbg_cc_types {
 	MLX5_IB_DBG_CC_RP_BYTE_RESET,
 	MLX5_IB_DBG_CC_RP_THRESHOLD,
 	MLX5_IB_DBG_CC_RP_AI_RATE,
+	MLX5_IB_DBG_CC_RP_MAX_RATE,
 	MLX5_IB_DBG_CC_RP_HAI_RATE,
 	MLX5_IB_DBG_CC_RP_MIN_DEC_FAC,
 	MLX5_IB_DBG_CC_RP_MIN_RATE,
@@ -801,6 +802,7 @@ enum mlx5_ib_dbg_cc_types {
 	MLX5_IB_DBG_CC_RP_RATE_REDUCE_MONITOR_PERIOD,
 	MLX5_IB_DBG_CC_RP_INITIAL_ALPHA_VALUE,
 	MLX5_IB_DBG_CC_RP_GD,
+	MLX5_IB_DBG_CC_NP_MIN_TIME_BETWEEN_CNPS,
 	MLX5_IB_DBG_CC_NP_CNP_DSCP,
 	MLX5_IB_DBG_CC_NP_CNP_PRIO_MODE,
 	MLX5_IB_DBG_CC_NP_CNP_PRIO,

commit de5ed007a03d71daaa505f5daa4d3666530c7090
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Thu Feb 27 13:39:18 2020 +0200

    IB/mlx5: Fix implicit ODP race
    
    Following race may occur because of the call_srcu and the placement of
    the synchronize_srcu vs the xa_erase.
    
    CPU0                               CPU1
    
    mlx5_ib_free_implicit_mr:          destroy_unused_implicit_child_mr:
     xa_erase(odp_mkeys)
     synchronize_srcu()
                                        xa_lock(implicit_children)
                                        if (still in xarray)
                                           atomic_inc()
                                           call_srcu()
                                        xa_unlock(implicit_children)
     xa_erase(implicit_children):
       xa_lock(implicit_children)
       __xa_erase()
       xa_unlock(implicit_children)
    
     flush_workqueue()
                                       [..]
                                        free_implicit_child_mr_rcu:
                                         (via call_srcu)
                                          queue_work()
    
     WARN_ON(atomic_read())
                                       [..]
                                        free_implicit_child_mr_work:
                                         (via wq)
                                          free_implicit_child_mr()
     mlx5_mr_cache_invalidate()
                                         mlx5_ib_update_xlt() <-- UMR QP fail
                                         atomic_dec()
    
    The wait_event() solves the race because it blocks until
    free_implicit_child_mr_work() completes.
    
    Fixes: 5256edcb98a1 ("RDMA/mlx5: Rework implicit ODP destroy")
    Link: https://lore.kernel.org/r/20200227113918.94432-1-leon@kernel.org
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d9bffcc93587..bb78142bca5e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -636,6 +636,7 @@ struct mlx5_ib_mr {
 
 	/* For ODP and implicit */
 	atomic_t		num_deferred_work;
+	wait_queue_head_t       q_deferred_work;
 	struct xarray		implicit_children;
 	union {
 		struct rcu_head rcu;

commit 5e29d1443c46b6ca70a4c940a67e8c09f05dcb7e
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Thu Feb 27 13:38:34 2020 +0200

    RDMA/mlx5: Prevent UMR usage with RO only when we have RO caps
    
    Relaxed ordering is not supported in UMR so we are disabling UMR usage
    when user passes relaxed ordering access flag.
    
    Enable using UMR when user requested relaxed ordering but there are no
    relaxed ordering capabilities.
    
    This will prevent user from unnecessarily registering a new mkey.
    
    Fixes: d6de0bb1850f ("RDMA/mlx5: Set relaxed ordering when requested")
    Link: https://lore.kernel.org/r/20200227113834.94233-1-leon@kernel.org
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d9bffcc93587..f21d446249b8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1532,7 +1532,9 @@ static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
 		return false;
 
-	if (access_flags & IB_ACCESS_RELAXED_ORDERING)
+	if (access_flags & IB_ACCESS_RELAXED_ORDERING &&
+	    (MLX5_CAP_GEN(dev->mdev, relaxed_ordering_write) ||
+	     MLX5_CAP_GEN(dev->mdev, relaxed_ordering_read)))
 		return false;
 
 	return true;

commit 8889f6fa35884d09f24734e10fea0c9ddcbc6429
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Jan 30 11:21:21 2020 -0400

    RDMA/core: Make the entire API tree static
    
    Compilation of mlx5 driver without CONFIG_INFINIBAND_USER_ACCESS generates
    the following error.
    
    on x86_64:
    
     ld: drivers/infiniband/hw/mlx5/main.o: in function `mlx5_ib_handler_MLX5_IB_METHOD_VAR_OBJ_ALLOC':
     main.c:(.text+0x186d): undefined reference to `ib_uverbs_get_ucontext_file'
     ld: drivers/infiniband/hw/mlx5/main.o:(.rodata+0x2480): undefined reference to `uverbs_idr_class'
     ld: drivers/infiniband/hw/mlx5/main.o:(.rodata+0x24d8): undefined reference to `uverbs_destroy_def_handler'
    
    This is happening because some parts of the UAPI description are not
    static. This is a hold over from earlier code that relied on struct
    pointers to refer to object types, now object types are referenced by
    number. Remove the unused globals and add statics to the remaining UAPI
    description elements.
    
    Remove the redundent #ifdefs around mlx5_ib_*defs and obsolete
    mlx5_ib_get_devx_tree().
    
    The compiler now trims alot more unused code, including the above
    problematic definitions when !CONFIG_INFINIBAND_USER_ACCESS.
    
    Fixes: 7be76bef320b ("IB/mlx5: Introduce VAR object and its alloc/destroy methods")
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7b019bd4de4b..d9bffcc93587 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1379,14 +1379,14 @@ int mlx5_ib_fill_res_entry(struct sk_buff *msg,
 int mlx5_ib_fill_stat_entry(struct sk_buff *msg,
 			    struct rdma_restrack_entry *res);
 
+extern const struct uapi_definition mlx5_ib_devx_defs[];
+extern const struct uapi_definition mlx5_ib_flow_defs[];
+
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
 void mlx5_ib_devx_init_event_table(struct mlx5_ib_dev *dev);
 void mlx5_ib_devx_cleanup_event_table(struct mlx5_ib_dev *dev);
-const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
-extern const struct uapi_definition mlx5_ib_devx_defs[];
-extern const struct uapi_definition mlx5_ib_flow_defs[];
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
 	struct mlx5_flow_context *flow_context,
@@ -1394,7 +1394,6 @@ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
 bool mlx5_ib_devx_is_flow_counter(void *obj, u32 offset, u32 *counter_id);
-int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int

commit e8b3a426fb4a9e2856a69b6e19de044c7416c316
Merge: eaad647e5cc2 b2dfc6765e45
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jan 21 09:55:04 2020 -0400

    Merge tag 'rds-odp-for-5.5' into rdma.git for-next
    
    From https://git.kernel.org/pub/scm/linux/kernel/git/leon/linux-rdma
    
    Leon Romanovsky says:
    
    ====================
    Use ODP MRs for kernel ULPs
    
    The following series extends MR creation routines to allow creation of
    user MRs through kernel ULPs as a proxy. The immediate use case is to
    allow RDS to work over FS-DAX, which requires ODP (on-demand-paging)
    MRs to be created and such MRs were not possible to create prior this
    series.
    
    The first part of this patchset extends RDMA to have special verb
    ib_reg_user_mr(). The common use case that uses this function is a
    userspace application that allocates memory for HCA access but the
    responsibility to register the memory at the HCA is on an kernel ULP.
    This ULP acts as an agent for the userspace application.
    
    The second part provides advise MR functionality for ULPs. This is
    integral part of ODP flows and used to trigger pagefaults in advance
    to prepare memory before running working set.
    
    The third part is actual user of those in-kernel APIs.
    ====================
    
    * tag 'rds-odp-for-5.5':
      net/rds: Use prefetch for On-Demand-Paging MR
      net/rds: Handle ODP mr registration/unregistration
      net/rds: Detect need of On-Demand-Paging memory registration
      RDMA/mlx5: Fix handling of IOVA != user_va in ODP paths
      IB/mlx5: Mask out unsupported ODP capabilities for kernel QPs
      RDMA/mlx5: Don't fake udata for kernel path
      IB/mlx5: Add ODP WQE handlers for kernel QPs
      IB/core: Add interface to advise_mr for kernel users
      IB/core: Introduce ib_reg_user_mr
      IB: Allow calls to ib_umem_get from kernel ULPs
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit d6de0bb1850f6fcefd4f9fed2de69c0915a2c8a9
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Wed Jan 8 20:05:40 2020 +0200

    RDMA/mlx5: Set relaxed ordering when requested
    
    Enable relaxed ordering in the mkey context when requested. As relaxed
    ordering is not currently supported in UMR, disable UMR usage for relaxed
    ordering MRs.
    
    Link: https://lore.kernel.org/r/1578506740-22188-11-git-send-email-yishaih@mellanox.com
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 676462c8909f..aa14d3c8abd9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1523,7 +1523,7 @@ int mlx5_ib_qp_set_counter(struct ib_qp *qp, struct rdma_counter *counter);
 u16 mlx5_ib_get_counters_id(struct mlx5_ib_dev *dev, u8 port_num);
 
 static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
-				       bool do_modify_atomic)
+				       bool do_modify_atomic, int access_flags)
 {
 	if (MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
 		return false;
@@ -1533,6 +1533,9 @@ static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
 	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
 		return false;
 
+	if (access_flags & IB_ACCESS_RELAXED_ORDERING)
+		return false;
+
 	return true;
 }
 

commit da9ee9d8a8745e70e481446e0bfe2d773b1c364b
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Jan 15 14:43:34 2020 +0200

    IB/mlx5: Add ODP WQE handlers for kernel QPs
    
    One of the steps in ODP page fault handler for WQEs is to read a WQE
    from a QP send queue or receive queue buffer at a specific index.
    
    Since the implementation of this buffer is different between kernel and
    user QP the implementation of the handler needs to be aware of that and
    handle it in a different way.
    
    ODP for kernel MRs is currently supported only for RDMA_READ
    and RDMA_WRITE operations so change the handler to
    - read a WQE from a kernel QP send queue
    - fail if access to receive queue or shared receive queue is
      required for a kernel QP
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b06f32ff5748..77d495b2032d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1153,12 +1153,12 @@ int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		      const struct ib_send_wr **bad_wr);
 int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 		      const struct ib_recv_wr **bad_wr);
-int mlx5_ib_read_user_wqe_sq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
-			     int buflen, size_t *bc);
-int mlx5_ib_read_user_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
-			     int buflen, size_t *bc);
-int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
-			      void *buffer, int buflen, size_t *bc);
+int mlx5_ib_read_wqe_sq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+			size_t buflen, size_t *bc);
+int mlx5_ib_read_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+			size_t buflen, size_t *bc);
+int mlx5_ib_read_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index, void *buffer,
+			 size_t buflen, size_t *bc);
 int mlx5_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
 		      struct ib_udata *udata);
 void mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);

commit 7be76bef320b1f1d1b8dc87d3d5a03f3a2421a43
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Dec 12 13:09:27 2019 +0200

    IB/mlx5: Introduce VAR object and its alloc/destroy methods
    
    Introduce VAR object and its alloc/destroy KABI methods. The internal
    implementation uses the IB core API to manage mmap/munamp calls.
    
    Link: https://lore.kernel.org/r/20191212110928.334995-5-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d3ec11fb59a6..676462c8909f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -71,6 +71,11 @@
 
 #define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
 
+enum {
+	MLX5_IB_MMAP_OFFSET_START = 9,
+	MLX5_IB_MMAP_OFFSET_END = 255,
+};
+
 enum {
 	MLX5_IB_MMAP_CMD_SHIFT	= 8,
 	MLX5_IB_MMAP_CMD_MASK	= 0xff,
@@ -120,6 +125,7 @@ enum {
 
 enum mlx5_ib_mmap_type {
 	MLX5_IB_MMAP_TYPE_MEMIC = 1,
+	MLX5_IB_MMAP_TYPE_VAR = 2,
 };
 
 #define MLX5_LOG_SW_ICM_BLOCK_SIZE(dev)                                        \
@@ -563,6 +569,7 @@ struct mlx5_user_mmap_entry {
 	struct rdma_user_mmap_entry rdma_entry;
 	u8 mmap_flag;
 	u64 address;
+	u32 page_idx;
 };
 
 struct mlx5_ib_dm {

commit f164be8c03663034416d019c355fbbd2dbd189d7
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Dec 12 13:09:26 2019 +0200

    IB/mlx5: Extend caps stage to handle VAR capabilities
    
    Extend caps stage to handle VAR capabilities.
    
    Link: https://lore.kernel.org/r/20191212110928.334995-4-leon@kernel.org
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 927948328184..d3ec11fb59a6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -959,6 +959,15 @@ struct mlx5_devx_event_table {
 	struct xarray event_xa;
 };
 
+struct mlx5_var_table {
+	/* serialize updating the bitmap */
+	struct mutex bitmap_lock;
+	unsigned long *bitmap;
+	u64 hw_start_addr;
+	u32 stride_size;
+	u64 num_var_hw_entries;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -1013,6 +1022,7 @@ struct mlx5_ib_dev {
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;
 	struct mlx5_devx_event_table devx_event_table;
+	struct mlx5_var_table var_table;
 
 	struct xarray sig_mrs;
 };

commit cbe4b8f0a5766a40563876932cba6c9bf28eb98a
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Sun Dec 22 14:46:47 2019 +0200

    IB/mlx5: Unify ODP MR code paths to allow extra flexibility
    
    Building MR translation table in the ODP case requires additional
    flexibility, namely random access to DMA addresses. Make both direct and
    indirect ODP MR use same code path, separated from the non-ODP MR code
    path.
    
    With the restructuring the correct page_shift is now used around
    __mlx5_ib_populate_pas().
    
    Fixes: d2183c6f1958 ("RDMA/umem: Move page_shift from ib_umem to ib_odp_umem")
    Link: https://lore.kernel.org/r/20191222124649.52300-2-leon@kernel.org
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b06f32ff5748..927948328184 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1276,8 +1276,8 @@ void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
-void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
-			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
+void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
+			   struct mlx5_ib_mr *mr, int flags);
 
 int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
 			       enum ib_uverbs_advise_mr_advice advice,
@@ -1293,9 +1293,8 @@ static inline void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev) {}
 static inline int mlx5_ib_odp_init(void) { return 0; }
 static inline void mlx5_ib_odp_cleanup(void)				    {}
 static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}
-static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
-					 size_t nentries, struct mlx5_ib_mr *mr,
-					 int flags) {}
+static inline void mlx5_odp_populate_xlt(void *xlt, size_t idx, size_t nentries,
+					 struct mlx5_ib_mr *mr, int flags) {}
 
 static inline int
 mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,

commit dc2316eba73ff03da6dde082a372c6b5209304c5
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Dec 12 12:02:37 2019 +0200

    IB/mlx5: Fix device memory flows
    
    Fix device memory flows so that only once there will be no live mmaped
    VA to a given allocation the matching object will be destroyed.
    
    This prevents a potential scenario that existing VA that was mmaped by
    one process might still be used post its deallocation despite that it's
    owned now by other process.
    
    The above is achieved by integrating with IB core APIs to manage
    mmap/munmap. Only once the refcount will become 0 the DM object and its
    underlay area will be freed.
    
    Fixes: 3b113a1ec3d4 ("IB/mlx5: Support device memory type attribute")
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20191212100237.330654-3-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5986953ec2fa..b06f32ff5748 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -118,6 +118,10 @@ enum {
 	MLX5_MEMIC_BASE_SIZE	= 1 << MLX5_MEMIC_BASE_ALIGN,
 };
 
+enum mlx5_ib_mmap_type {
+	MLX5_IB_MMAP_TYPE_MEMIC = 1,
+};
+
 #define MLX5_LOG_SW_ICM_BLOCK_SIZE(dev)                                        \
 	(MLX5_CAP_DEV_MEM(dev, log_sw_icm_alloc_granularity))
 #define MLX5_SW_ICM_BLOCK_SIZE(dev) (1 << MLX5_LOG_SW_ICM_BLOCK_SIZE(dev))
@@ -135,7 +139,6 @@ struct mlx5_ib_ucontext {
 	u32			tdn;
 
 	u64			lib_caps;
-	DECLARE_BITMAP(dm_pages, MLX5_MAX_MEMIC_PAGES);
 	u16			devx_uid;
 	/* For RoCE LAG TX affinity */
 	atomic_t		tx_port_affinity;
@@ -556,6 +559,12 @@ enum mlx5_ib_mtt_access_flags {
 	MLX5_IB_MTT_WRITE = (1 << 1),
 };
 
+struct mlx5_user_mmap_entry {
+	struct rdma_user_mmap_entry rdma_entry;
+	u8 mmap_flag;
+	u64 address;
+};
+
 struct mlx5_ib_dm {
 	struct ib_dm		ibdm;
 	phys_addr_t		dev_addr;
@@ -567,6 +576,7 @@ struct mlx5_ib_dm {
 		} icm_dm;
 		/* other dm types specific params should be added here */
 	};
+	struct mlx5_user_mmap_entry mentry;
 };
 
 #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
@@ -1101,6 +1111,13 @@ to_mflow_act(struct ib_flow_action *ibact)
 	return container_of(ibact, struct mlx5_ib_flow_action, ib_action);
 }
 
+static inline struct mlx5_user_mmap_entry *
+to_mmmap(struct rdma_user_mmap_entry *rdma_entry)
+{
+	return container_of(rdma_entry,
+		struct mlx5_user_mmap_entry, rdma_entry);
+}
+
 int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context,
 			struct ib_udata *udata, unsigned long virt,
 			struct mlx5_db *db);

commit aa32f1169148beb90d71494e2f2a1999ba7b5366
Merge: d5bb349dbbe2 93f4e735b6d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 10:33:14 2019 -0800

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull hmm updates from Jason Gunthorpe:
     "This is another round of bug fixing and cleanup. This time the focus
      is on the driver pattern to use mmu notifiers to monitor a VA range.
      This code is lifted out of many drivers and hmm_mirror directly into
      the mmu_notifier core and written using the best ideas from all the
      driver implementations.
    
      This removes many bugs from the drivers and has a very pleasing
      diffstat. More drivers can still be converted, but that is for another
      cycle.
    
       - A shared branch with RDMA reworking the RDMA ODP implementation
    
       - New mmu_interval_notifier API. This is focused on the use case of
         monitoring a VA and simplifies the process for drivers
    
       - A common seq-count locking scheme built into the
         mmu_interval_notifier API usable by drivers that call
         get_user_pages() or hmm_range_fault() with the VA range
    
       - Conversion of mlx5 ODP, hfi1, radeon, nouveau, AMD GPU, and Xen
         GntDev drivers to the new API. This deletes a lot of wonky driver
         code.
    
       - Two improvements for hmm_range_fault(), from testing done by Ralph"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:
      mm/hmm: remove hmm_range_dma_map and hmm_range_dma_unmap
      mm/hmm: make full use of walk_page_range()
      xen/gntdev: use mmu_interval_notifier_insert
      mm/hmm: remove hmm_mirror and related
      drm/amdgpu: Use mmu_interval_notifier instead of hmm_mirror
      drm/amdgpu: Use mmu_interval_insert instead of hmm_mirror
      drm/amdgpu: Call find_vma under mmap_sem
      nouveau: use mmu_interval_notifier instead of hmm_mirror
      nouveau: use mmu_notifier directly for invalidate_range_start
      drm/radeon: use mmu_interval_notifier_insert
      RDMA/hfi1: Use mmu_interval_notifier_insert for user_exp_rcv
      RDMA/odp: Use mmu_interval_notifier_insert()
      mm/hmm: define the pre-processor related parts of hmm.h even if disabled
      mm/hmm: allow hmm_range to be used with a mmu_interval_notifier or hmm_mirror
      mm/mmu_notifier: add an interval tree notifier
      mm/mmu_notifier: define the header pre-processor parts even if disabled
      mm/hmm: allow snapshot of the special zero page

commit 9a3d7fd275be4559277667228902824165153c80
Merge: 0dd09bc02c1b 0e4a459f56c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 27 11:06:20 2019 -0800

    Merge tag 'driver-core-5.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull driver core updates from Greg KH:
     "Here is the "big" set of driver core patches for 5.5-rc1
    
      There's a few minor cleanups and fixes in here, but the majority of
      the patches in here fall into two buckets:
    
       - debugfs api cleanups and fixes
    
       - driver core device link support for boot dependancy issues
    
      The debugfs api cleanups are working to slowly refactor the debugfs
      apis so that it is even harder to use incorrectly. That work has been
      happening for the past few kernel releases and will continue over
      time, it's a long-term project/goal
    
      The driver core device link support missed 5.4 by just a bit, so it's
      been sitting and baking for many months now. It's from Saravana Kannan
      to help resolve the problems that DT-based systems have at boot time
      with dependancy graphs and kernel modules. Turns out that no one has
      actually tried to build a generic arm64 kernel with loads of modules
      and have it "just work" for a variety of platforms (like a distro
      kernel). The big problem turned out to be a lack of dependency
      information between different areas of DT entries, and the work here
      resolves that problem and now allows devices to boot properly, and
      quicker than a monolith kernel.
    
      All of these patches have been in linux-next for a long time with no
      reported issues"
    
    * tag 'driver-core-5.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core: (68 commits)
      tracing: Remove unnecessary DEBUG_FS dependency
      of: property: Add device link support for interrupt-parent, dmas and -gpio(s)
      debugfs: Fix !DEBUG_FS debugfs_create_automount
      of: property: Add device link support for "iommu-map"
      of: property: Fix the semantics of of_is_ancestor_of()
      i2c: of: Populate fwnode in of_i2c_get_board_info()
      drivers: base: Fix Kconfig indentation
      firmware_loader: Fix labels with comma for builtin firmware
      driver core: Allow device link operations inside sync_state()
      driver core: platform: Declare ret variable only once
      cpu-topology: declare parse_acpi_topology in <linux/arch_topology.h>
      crypto: hisilicon: no need to check return value of debugfs_create functions
      driver core: platform: use the correct callback type for bus_find_device
      firmware_class: make firmware caching configurable
      driver core: Clarify documentation for fwnode_operations.add_links()
      mailbox: tegra: Fix superfluous IRQ error message
      net: caif: Fix debugfs on 64-bit platforms
      mac80211: Use debugfs_create_xul() helper
      media: c8sectpfe: no need to check return value of debugfs_create functions
      of: property: Add device link support for iommus, mboxes and io-channels
      ...

commit 3694e41e41517994664518ece6265f0bc04a840d
Merge: a25984f3baaa 9c0015ef0928
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Nov 22 16:08:34 2019 -0400

    Merge branch 'ib-guids' into rdma.git for-next
    
    Danit Goldberg says:
    
    ====================
    This series extends RTNETLINK to provide IB port and node GUIDs, which
    were configured for Infiniband VFs.
    
    The functionality to set VF GUIDs already existed for a long time, and
    here we are adding the missing "get" so that netlink will be symmetric and
    various cloud orchestration tools will be able to manage such VFs more
    naturally.
    
    The iproute2 was extended too to present those GUIDs.
    
    - ip link show <device>
    
    For example:
    - ip link set ib4 vf 0 node_guid 22:44:33:00:33:11:00:33
    - ip link set ib4 vf 0 port_guid 10:21:33:12:00:11:22:10
    - ip link show ib4
        ib4: <BROADCAST,MULTICAST> mtu 4092 qdisc noop state DOWN mode DEFAULT group default qlen 256
        link/infiniband 00:00:0a:2d:fe:80:00:00:00:00:00:00:ec:0d:9a:03:00:44:36:8d brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
        vf 0     link/infiniband 00:00:0a:2d:fe:80:00:00:00:00:00:00:ec:0d:9a:03:00:44:36:8d brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff,
        spoof checking off, NODE_GUID 22:44:33:00:33:11:00:33, PORT_GUID 10:21:33:12:00:11:22:10, link-state disable, trust off, query_rss off
    ====================
    
    Based on the mlx5-next branch from
    git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux for
    dependencies
    
    * branch 'ib-guids': (35 commits)
      IB/mlx5: Implement callbacks for getting VFs GUID attributes
      IB/ipoib: Add ndo operation for getting VFs GUID attributes
      IB/core: Add interfaces to get VF node and port GUIDs
      net/core: Add support for getting VF GUIDs
    
      net/mlx5: Add new chain for netfilter flow table offload
      net/mlx5: Refactor creating fast path prio chains
      net/mlx5: Accumulate levels for chains prio namespaces
      net/mlx5: Define fdb tc levels per prio
      net/mlx5: Rename FDB_* tc related defines to FDB_TC_* defines
      net/mlx5: Simplify fdb chain and prio eswitch defines
      IB/mlx5: Load profile according to RoCE enablement state
      IB/mlx5: Rename profile and init methods
      net/mlx5: Handle "enable_roce" devlink param
      net/mlx5: Document flow_steering_mode devlink param
      devlink: Add new "enable_roce" generic device param
      net/mlx5: fix spelling mistake "metdata" -> "metadata"
      net/mlx5: fix kvfree of uninitialized pointer spec
      IB/mlx5: Introduce and use mlx5_core_is_vf()
      net/mlx5: E-switch, Enable metadata on own vport
      net/mlx5: Refactor ingress acl configuration
      ...
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit f25a546e65292b36f15cca0912450c4944fae031
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Nov 12 16:22:22 2019 -0400

    RDMA/odp: Use mmu_interval_notifier_insert()
    
    Replace the internal interval tree based mmu notifier with the new common
    mmu_interval_notifier_insert() API. This removes a lot of code and fixes a
    deadlock that can be triggered in ODP:
    
     zap_page_range()
      mmu_notifier_invalidate_range_start()
       [..]
        ib_umem_notifier_invalidate_range_start()
           down_read(&per_mm->umem_rwsem)
      unmap_single_vma()
        [..]
          __split_huge_page_pmd()
            mmu_notifier_invalidate_range_start()
            [..]
               ib_umem_notifier_invalidate_range_start()
                  down_read(&per_mm->umem_rwsem)   // DEADLOCK
    
            mmu_notifier_invalidate_range_end()
               up_read(&per_mm->umem_rwsem)
      mmu_notifier_invalidate_range_end()
         up_read(&per_mm->umem_rwsem)
    
    The umem_rwsem is held across the range_start/end as the ODP algorithm for
    invalidate_range_end cannot tolerate changes to the interval
    tree. However, due to the nested invalidation regions the second
    down_read() can deadlock if there are competing writers. The new core code
    provides an alternative scheme to solve this problem.
    
    Fixes: ca748c39ea3f ("RDMA/umem: Get rid of per_mm->notifier_count")
    Link: https://lore.kernel.org/r/20191112202231.3856-6-jgg@ziepe.ca
    Tested-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f61d4005c6c3..108cadf9af1f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1263,8 +1263,6 @@ int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
 void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
-void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
-			      unsigned long end);
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
 void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
@@ -1294,11 +1292,10 @@ mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
 {
 	return -EOPNOTSUPP;
 }
-static inline void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp,
-					    unsigned long start,
-					    unsigned long end){};
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
+extern const struct mmu_interval_notifier_ops mlx5_mn_ops;
+
 /* Needed for rep profile */
 void __mlx5_ib_remove(struct mlx5_ib_dev *dev,
 		      const struct mlx5_ib_profile *profile,

commit 9c0015ef092879f61129cdffbbe22fd30201d39b
Author: Danit Goldberg <danitg@mellanox.com>
Date:   Wed Nov 6 15:18:12 2019 +0200

    IB/mlx5: Implement callbacks for getting VFs GUID attributes
    
    Implement the IB defined callback mlx5_ib_get_vf_guid used to query FW
    for VFs attributes and return node and port GUIDs.
    
    Signed-off-by: Danit Goldberg <danitg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2ceaef3ea3fb..96811f6ca15a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1301,6 +1301,9 @@ int mlx5_ib_set_vf_link_state(struct ib_device *device, int vf,
 			      u8 port, int state);
 int mlx5_ib_get_vf_stats(struct ib_device *device, int vf,
 			 u8 port, struct ifla_vf_stats *stats);
+int mlx5_ib_get_vf_guid(struct ib_device *device, int vf, u8 port,
+			struct ifla_vf_guid *node_guid,
+			struct ifla_vf_guid *port_guid);
 int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
 			u64 guid, int type);
 

commit c16339b69c0d58f456154b43fc396b1f1c3dc055
Author: Mark Zhang <markz@mellanox.com>
Date:   Fri Nov 15 17:45:55 2019 +0200

    IB/mlx5: Support extended number of strides for Striding RQ
    
    Extends the minimum single WQE strides from 64 to 8, which is exposed
    by the "min_single_wqe_log_num_of_strides" field of striding_rq_caps.
    Choose right number of strides based on FW capability.
    
    Link: https://lore.kernel.org/r/20191115154555.247856-1-leon@kernel.org
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d14fe43837e7..5e826eb7e4fe 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -291,6 +291,7 @@ enum mlx5_ib_wq_flags {
 #define MLX5_MAX_SINGLE_WQE_LOG_NUM_STRIDES 16
 #define MLX5_MIN_SINGLE_STRIDE_LOG_NUM_BYTES 6
 #define MLX5_MAX_SINGLE_STRIDE_LOG_NUM_BYTES 13
+#define MLX5_EXT_MIN_SINGLE_WQE_LOG_NUM_STRIDES 3
 
 struct mlx5_ib_rwq {
 	struct ib_wq		ibwq;

commit 208d70f562e563226df178ff8f969364972e9e99
Author: Yevgeny Kliteynik <kliteyn@mellanox.com>
Date:   Sun Nov 3 16:07:23 2019 +0200

    IB/mlx5: Support flow counters offset for bulk counters
    
    Add support for flow steering counters action with a non-base counter
    ID (offset) for bulk counters.
    
    When creating a flow counter object, save the bulk value.  This value is
    used when a flow action with a non-base counter ID is requested - to
    validate that the required offset is in the range of the allocated bulk.
    
    Link: https://lore.kernel.org/r/20191103140723.77411-1-leon@kernel.org
    Signed-off-by: Yevgeny Kliteynik <kliteyn@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 933a2059886b..d14fe43837e7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1366,7 +1366,7 @@ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_flow_act *flow_act, u32 counter_id,
 	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
-bool mlx5_ib_devx_is_flow_counter(void *obj, u32 *counter_id);
+bool mlx5_ib_devx_is_flow_counter(void *obj, u32 offset, u32 *counter_id);
 int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else

commit e26e7b88f6b7482cbff633c6fc9eaee3ecbd41b1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Oct 29 08:27:45 2019 +0200

    RDMA: Change MAD processing function to remove extra casting and parameter
    
    All users of process_mad() converts input pointers from ib_mad_hdr to be
    ib_mad, update the function declaration to use ib_mad directly.
    
    Also remove not used input MAD size parameter.
    
    Link: https://lore.kernel.org/r/20191029062745.7932-17-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Tested-By: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0bdb8b45ea15..933a2059886b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1192,9 +1192,8 @@ int mlx5_ib_map_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
 			 unsigned int *meta_sg_offset);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
-			const struct ib_mad_hdr *in, size_t in_mad_size,
-			struct ib_mad_hdr *out, size_t *out_mad_size,
-			u16 *out_mad_pkey_index);
+			const struct ib_mad *in, struct ib_mad *out,
+			size_t *out_mad_size, u16 *out_mad_pkey_index);
 struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 				   struct ib_udata *udata);
 int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);

commit 09b0965ee8cced20a4791ff54506f1a6600996fe
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Mon Nov 4 08:38:07 2019 +0100

    IB: mlx5: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    Cc: Leon Romanovsky <leon@kernel.org>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: linux-rdma@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1a98ee2e01c4..55ce599db803 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -792,13 +792,6 @@ enum {
 	MLX5_MAX_DELAY_DROP_TIMEOUT_MS = 100,
 };
 
-struct mlx5_ib_dbg_delay_drop {
-	struct dentry		*dir_debugfs;
-	struct dentry		*rqs_cnt_debugfs;
-	struct dentry		*events_cnt_debugfs;
-	struct dentry		*timeout_debugfs;
-};
-
 struct mlx5_ib_delay_drop {
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	delay_drop_work;
@@ -808,7 +801,7 @@ struct mlx5_ib_delay_drop {
 	bool			activate;
 	atomic_t		events_cnt;
 	atomic_t		rqs_cnt;
-	struct mlx5_ib_dbg_delay_drop *dbg;
+	struct dentry		*dir_debugfs;
 };
 
 enum mlx5_ib_stages {

commit 11f552e21755cb6f804572243a1502b6bbd008dd
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Mon Jun 10 15:21:24 2019 +0300

    IB/mlx5: Test write combining support
    
    Linux can run in all sorts of physical machines and VMs where write
    combining may or may not be supported. Currently there is no way to
    reliably tell if the system supports WC, or not. The driver uses WC to
    optimize posting work to the HCA, and getting this wrong in either
    direction can cause a significant performance loss.
    
    Add a test in mlx5_ib initialization process to test whether
    write-combining is supported on the machine.  The test will run as part of
    the enable_driver callback to ensure that the test runs after the device
    is setup and can create and modify the QP needed, but runs before the
    device is exposed to the users.
    
    The test opens UD QP and posts NOP WQEs, the WQE written to the BlueFlame
    is different from the WQE in memory, requesting CQE only on the BlueFlame
    WQE. By checking whether we received a completion on one of these WQEs we
    can know if BlueFlame succeeded and this write-combining must be
    supported.
    
    Change reporting of BlueFlame support to be dependent on write-combining
    support instead of the FW's guess as to what the machine can do.
    
    Link: https://lore.kernel.org/r/20191027062234.10993-1-leon@kernel.org
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5b4c5751a98f..0bdb8b45ea15 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -248,6 +248,7 @@ struct mlx5_ib_flow_db {
  * rely on the range reserved for that use in the ib_qp_create_flags enum.
  */
 #define MLX5_IB_QP_CREATE_SQPN_QP1	IB_QP_CREATE_RESERVED_START
+#define MLX5_IB_QP_CREATE_WC_TEST	(IB_QP_CREATE_RESERVED_START << 1)
 
 struct wr_list {
 	u16	opcode;
@@ -966,6 +967,7 @@ struct mlx5_ib_dev {
 	u8				fill_delay:1;
 	u8				is_rep:1;
 	u8				lag_active:1;
+	u8				wc_support:1;
 	struct umr_common		umrc;
 	/* sync used page count stats
 	 */
@@ -993,6 +995,7 @@ struct mlx5_ib_dev {
 	/* Array with num_ports elements */
 	struct mlx5_ib_port	*port;
 	struct mlx5_sq_bfreg	bfreg;
+	struct mlx5_sq_bfreg	wc_bfreg;
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
@@ -1506,4 +1509,7 @@ static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
 
 	return true;
 }
+
+int mlx5_ib_enable_driver(struct ib_device *dev);
+int mlx5_ib_test_wc(struct mlx5_ib_dev *dev);
 #endif /* MLX5_IB_H */

commit bb3dba330006fcf820136992afef64c3d2cdcc55
Merge: 036313316d3a 46870b2391d5
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Oct 28 16:44:35 2019 -0300

    Merge branch 'odp_rework' into rdma.git for-next
    
    Jason Gunthorpe says:
    
    ====================
    In order to hoist the interval tree code out of the drivers and into the
    mmu_notifiers it is necessary for the drivers to not use the interval tree
    for other things.
    
    This series replaces the interval tree with an xarray and along the way
    re-aligns all the locking to use a sensible SRCU model where the 'update'
    step is done by modifying an xarray.
    
    The result is overall much simpler and with less locking in the critical
    path. Many functions were reworked for clarity and small details like
    using 'imr' to refer to the implicit MR make the entire code flow here
    more readable.
    
    This also squashes at least two race bugs on its own, and quite possibily
    more that haven't been identified.
    ====================
    
    Merge conflicts with the odp statistics patch resolved.
    
    * branch 'odp_rework':
      RDMA/odp: Remove broken debugging call to invalidate_range
      RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy
      RDMA/mlx5: Do not store implicit children in the odp_mkeys xarray
      RDMA/mlx5: Rework implicit ODP destroy
      RDMA/mlx5: Avoid double lookups on the pagefault path
      RDMA/mlx5: Reduce locking in implicit_mr_get_data()
      RDMA/mlx5: Use an xarray for the children of an implicit ODP
      RDMA/mlx5: Split implicit handling from pagefault_mr
      RDMA/mlx5: Set the HW IOVA of the child MRs to their place in the tree
      RDMA/mlx5: Lift implicit_mr_alloc() into the two routines that call it
      RDMA/mlx5: Rework implicit_mr_get_data
      RDMA/mlx5: Delete struct mlx5_priv->mkey_table
      RDMA/mlx5: Use a dedicated mkey xarray for ODP
      RDMA/mlx5: Split sig_err MR data into its own xarray
      RDMA/mlx5: Use SRCU properly in ODP prefetch
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 09689703d29a3b75c510c198c3aca85d7d8b50c7
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Oct 9 13:09:34 2019 -0300

    RDMA/mlx5: Do not race with mlx5_ib_invalidate_range during create and destroy
    
    For creation, as soon as the umem_odp is created the notifier can be
    called, however the underlying MR may not have been setup yet. This would
    cause problems if mlx5_ib_invalidate_range() runs. There is some
    confusing/ulocked/racy code that might by trying to solve this, but
    without locks it isn't going to work right.
    
    Instead trivially solve the problem by short-circuiting the invalidation
    if there are not yet any DMA mapped pages. By definition there is nothing
    to invalidate in this case.
    
    The create code will have the umem fully setup before anything is DMA
    mapped, and npages is fully locked by the umem_mutex.
    
    For destroy, invalidate the entire MR at the HW to stop DMA then DMA unmap
    the pages before destroying the MR. This drives npages to zero and
    prevents similar racing with invalidate while the MR is undergoing
    destruction.
    
    Arguably it would be better if the umem was created after the MR and
    destroyed before, but that would require a big rework of the MR code.
    
    Fixes: 6aec21f6a832 ("IB/mlx5: Page faults handling infrastructure")
    Link: https://lore.kernel.org/r/20191009160934.3143-15-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b8c958f62628..f61d4005c6c3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1171,6 +1171,7 @@ struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
 					     struct ib_udata *udata,
 					     int access_flags);
 void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
+void mlx5_ib_fence_odp_mr(struct mlx5_ib_mr *mr);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);
@@ -1232,6 +1233,8 @@ int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 
 struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry);
 void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
+int mlx5_mr_cache_invalidate(struct mlx5_ib_mr *mr);
+
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,

commit 5256edcb98a14b11409a2d323f56a70a8b366363
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Oct 9 13:09:32 2019 -0300

    RDMA/mlx5: Rework implicit ODP destroy
    
    Use SRCU in a sensible way by removing all MRs in the implicit tree from
    the two xarrays (the update operation), then a synchronize, followed by a
    normal single threaded teardown.
    
    This is only a little unusual from the normal pattern as there can still
    be some work pending in the unbound wq that may also require a workqueue
    flush. This is tracked with a single atomic, consolidating the redundant
    existing atomics and wait queue.
    
    For understand-ability the entire ODP implicit create/destroy flow now
    largely exists in a single pair of functions within odp.c, with a few
    support functions for tearing down an unused child.
    
    Link: https://lore.kernel.org/r/20191009160934.3143-13-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 88769fcffb5a..b8c958f62628 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -618,10 +618,13 @@ struct mlx5_ib_mr {
 	u64			pi_iova;
 
 	/* For ODP and implicit */
-	atomic_t		num_leaf_free;
-	wait_queue_head_t       q_leaf_free;
-	atomic_t		num_pending_prefetch;
+	atomic_t		num_deferred_work;
 	struct xarray		implicit_children;
+	union {
+		struct rcu_head rcu;
+		struct list_head elm;
+		struct work_struct work;
+	} odp_destroy;
 
 	struct mlx5_async_work  cb_work;
 };

commit 423f52d65005e8f5067d94bd4f41d8a7d8388135
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Oct 9 13:09:29 2019 -0300

    RDMA/mlx5: Use an xarray for the children of an implicit ODP
    
    Currently the child leaves are stored in the shared interval tree and
    every lookup for a child must be done under the interval tree rwsem.
    
    This is further complicated by dropping the rwsem during iteration (ie the
    odp_lookup(), odp_next() pattern), which requires a very tricky an
    difficult to understand locking scheme with SRCU.
    
    Instead reserve the interval tree for the exclusive use of the mmu
    notifier related code in umem_odp.c and give each implicit MR a xarray
    containing all the child MRs.
    
    Since the size of each child is 1GB of VA, a 1 level xarray will index 64G
    of VA, and a 2 level will index 2TB, making xarray a much better
    data structure choice than an interval tree.
    
    The locking properties of xarray will be used in the next patches to
    rework the implicit ODP locking scheme into something simpler.
    
    At this point, the xarray is locked by the implicit MR's umem_mutex, and
    read can also be locked by the odp_srcu.
    
    Link: https://lore.kernel.org/r/20191009160934.3143-10-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2cf91db6a36f..88769fcffb5a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -617,10 +617,13 @@ struct mlx5_ib_mr {
 	u64			data_iova;
 	u64			pi_iova;
 
+	/* For ODP and implicit */
 	atomic_t		num_leaf_free;
 	wait_queue_head_t       q_leaf_free;
-	struct mlx5_async_work  cb_work;
 	atomic_t		num_pending_prefetch;
+	struct xarray		implicit_children;
+
+	struct mlx5_async_work  cb_work;
 };
 
 static inline bool is_odp_mr(struct mlx5_ib_mr *mr)

commit 806b101b2bfa800a9c779336b750bee39c7fb3b4
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Oct 9 13:09:23 2019 -0300

    RDMA/mlx5: Use a dedicated mkey xarray for ODP
    
    There is a per device xarray storing mkeys that is used to store every
    mkey in the system. However, this xarray is now only read by ODP for
    certain ODP designated MRs (ODP, implicit ODP, MW, DEVX_INDIRECT).
    
    Create an xarray only for use by ODP, that only contains ODP related
    MKeys. This xarray is protected by SRCU and all erases are protected by a
    synchronize.
    
    This improves performance:
    
     - All MRs in the odp_mkeys xarray are ODP MRs, so some tests for is_odp()
       can be deleted. The xarray will also consume fewer nodes.
    
     - normal MR's are never mixed with ODP MRs in a SRCU data structure so
       performance sucking synchronize_srcu() on every MR destruction is not
       needed.
    
     - No smp_load_acquire(live) and xa_load() double barrier on read
    
    Due to the SRCU locking scheme care must be taken with the placement of
    the xa_store(). Once it completes the MR is immediately visible to other
    threads and only through a xa_erase() & synchronize_srcu() cycle could it
    be destroyed.
    
    Link: https://lore.kernel.org/r/20191009160934.3143-4-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f4ce58354544..2cf91db6a36f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -606,7 +606,6 @@ struct mlx5_ib_mr {
 	struct mlx5_ib_dev     *dev;
 	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
 	struct mlx5_core_sig_ctx    *sig;
-	unsigned int		live;
 	void			*descs_alloc;
 	int			access_flags; /* Needed for rereg MR */
 
@@ -975,7 +974,9 @@ struct mlx5_ib_dev {
 	 * Sleepable RCU that prevents destruction of MRs while they are still
 	 * being used by a page fault handler.
 	 */
-	struct srcu_struct      mr_srcu;
+	struct srcu_struct      odp_srcu;
+	struct xarray		odp_mkeys;
+
 	u32			null_mkey;
 	struct mlx5_ib_flow_db	*flow_db;
 	/* protect resources needed as part of reset flow */

commit 50211ec9443ff2e16db43f691dfcc0ef435cf45d
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Oct 9 13:09:22 2019 -0300

    RDMA/mlx5: Split sig_err MR data into its own xarray
    
    The locking model for signature is completely different than ODP, do not
    share the same xarray that relies on SRCU locking to support ODP.
    
    Simply store the active mlx5_core_sig_ctx's in an xarray when signature
    MRs are created and rely on trivial xarray locking to serialize
    everything.
    
    The overhead of storing only a handful of SIG related MRs is going to be
    much less than an xarray full of every mkey.
    
    Link: https://lore.kernel.org/r/20191009160934.3143-3-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1a98ee2e01c4..f4ce58354544 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -999,6 +999,8 @@ struct mlx5_ib_dev {
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;
 	struct mlx5_devx_event_table devx_event_table;
+
+	struct xarray sig_mrs;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 036313316d3a38bfde9ba49b3d00f73b7d8019d2
Merge: a52dc3a10095 d6d5df1db6e9
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Oct 28 16:36:29 2019 -0300

    Merge tag 'v5.4-rc5' into rdma.git for-next
    
    Linux 5.4-rc5
    
    For dependencies in the next patches
    
    Conflict resolved by keeping the delete of the unlock.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 3f89b01f4bbab2dcd1a6e7e31e64faacb448e3be
Author: Michael Guralnik <michaelgur@mellanox.com>
Date:   Sun Oct 20 09:43:59 2019 +0300

    IB/mlx5: Align usage of QP1 create flags with rest of mlx5 defines
    
    There is little value in keeping separate function for one flag, provide
    it directly like any other mlx5 define.
    
    Link: https://lore.kernel.org/r/20191020064400.8344-2-leon@kernel.org
    Signed-off-by: Michael Guralnik <michaelgur@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 64f56926bf6b..d53739e43a93 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -247,12 +247,7 @@ struct mlx5_ib_flow_db {
  * These flags are intended for internal use by the mlx5_ib driver, and they
  * rely on the range reserved for that use in the ib_qp_create_flags enum.
  */
-
-/* Create a UD QP whose source QP number is 1 */
-static inline enum ib_qp_create_flags mlx5_ib_create_qp_sqpn_qp1(void)
-{
-	return IB_QP_CREATE_RESERVED_START;
-}
+#define MLX5_IB_QP_CREATE_SQPN_QP1	IB_QP_CREATE_RESERVED_START
 
 struct wr_list {
 	u16	opcode;

commit 68abaa765e410dc1583de1fa285ec7b0c58c6252
Author: Ran Rozenstein <ranro@mellanox.com>
Date:   Sun Oct 20 09:44:54 2019 +0300

    IB/mlx5: Remove dead code
    
    mlx5_ib_dc_atomic_is_supported function is not used anywhere. Remove the
    dead code.
    
    Fixes: a60109dc9a95 ("IB/mlx5: Add support for extended atomic operations")
    Link: https://lore.kernel.org/r/20191020064454.8551-1-leon@kernel.org
    Signed-off-by: Ran Rozenstein <ranro@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e9bdb48cf1d3..64f56926bf6b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1241,7 +1241,6 @@ struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
 						      struct ib_rwq_ind_table_init_attr *init_attr,
 						      struct ib_udata *udata);
 int mlx5_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
-bool mlx5_ib_dc_atomic_is_supported(struct mlx5_ib_dev *dev);
 struct ib_dm *mlx5_ib_alloc_dm(struct ib_device *ibdev,
 			       struct ib_ucontext *context,
 			       struct ib_dm_alloc_attr *attr,

commit 4061ff7aa379fa770a82da0ed7ec4f9163034518
Author: Erez Alfasi <ereza@mellanox.com>
Date:   Wed Oct 16 09:23:08 2019 +0300

    RDMA/nldev: Provide MR statistics
    
    Add RDMA nldev netlink interface for dumping MR statistics information.
    
    Output example:
    
    $ ./ibv_rc_pingpong -o -P -s 500000000
      local address:  LID 0x0001, QPN 0x00008a, PSN 0xf81096, GID ::
    
    $ rdma stat show mr
    dev mlx5_0 mrn 2 page_faults 122071 page_invalidations 0
    
    Link: https://lore.kernel.org/r/20191016062308.11886-5-leon@kernel.org
    Signed-off-by: Erez Alfasi <ereza@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a0ca1ef16e4e..e9bdb48cf1d3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1342,6 +1342,8 @@ void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 				  u8 port_num);
 int mlx5_ib_fill_res_entry(struct sk_buff *msg,
 			   struct rdma_restrack_entry *res);
+int mlx5_ib_fill_stat_entry(struct sk_buff *msg,
+			    struct rdma_restrack_entry *res);
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);

commit e1b95ae0b0ea4987afca73d1dc71dfc0b8ad4e49
Author: Erez Alfasi <ereza@mellanox.com>
Date:   Wed Oct 16 09:23:07 2019 +0300

    RDMA/mlx5: Return ODP type per MR
    
    Provide an ODP explicit/implicit type as part of 'rdma -dd resource show
    mr' dump.
    
    For example:
    
    $ rdma -dd resource show mr
    dev mlx5_0 mrn 1 rkey 0xa99a lkey 0xa99a mrlen 50000000
    pdn 9 pid 7372 comm ibv_rc_pingpong drv_odp explicit
    
    For non-ODP MRs, we won't print "drv_odp ..." at all.
    
    Link: https://lore.kernel.org/r/20191016062308.11886-4-leon@kernel.org
    Signed-off-by: Erez Alfasi <ereza@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5aae05ebf64b..a0ca1ef16e4e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -626,6 +626,7 @@ struct mlx5_ib_mr {
 	struct mlx5_async_work  cb_work;
 	atomic_t		num_pending_prefetch;
 	struct ib_odp_counters	odp_stats;
+	bool			is_odp_implicit;
 };
 
 static inline bool is_odp_mr(struct mlx5_ib_mr *mr)
@@ -1339,6 +1340,8 @@ struct mlx5_core_dev *mlx5_ib_get_native_port_mdev(struct mlx5_ib_dev *dev,
 						   u8 *native_port_num);
 void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 				  u8 port_num);
+int mlx5_ib_fill_res_entry(struct sk_buff *msg,
+			   struct rdma_restrack_entry *res);
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);

commit a3de94e3d61ec6e6c57ee066ec4d28ebc260dafa
Author: Erez Alfasi <ereza@mellanox.com>
Date:   Wed Oct 16 09:23:05 2019 +0300

    IB/mlx5: Introduce ODP diagnostic counters
    
    Introduce ODP diagnostic counters and count the following
    per MR within IB/mlx5 driver:
     1) Page faults:
            Total number of faulted pages.
     2) Page invalidations:
            Total number of pages invalidated by the OS during all
            invalidation events. The translations can be no longer
            valid due to either non-present pages or mapping changes.
    
    Link: https://lore.kernel.org/r/20191016062308.11886-2-leon@kernel.org
    Signed-off-by: Erez Alfasi <ereza@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index bf30d53d94dc..5aae05ebf64b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -585,6 +585,9 @@ struct mlx5_ib_dm {
 					  IB_ACCESS_REMOTE_READ   |\
 					  IB_ZERO_BASED)
 
+#define mlx5_update_odp_stats(mr, counter_name, value)		\
+	atomic64_add(value, &((mr)->odp_stats.counter_name))
+
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
 	void			*descs;
@@ -622,6 +625,7 @@ struct mlx5_ib_mr {
 	wait_queue_head_t       q_leaf_free;
 	struct mlx5_async_work  cb_work;
 	atomic_t		num_pending_prefetch;
+	struct ib_odp_counters	odp_stats;
 };
 
 static inline bool is_odp_mr(struct mlx5_ib_mr *mr)

commit 0417791536ae1e28d7f0418f1d20048ec4d3c6cf
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Oct 1 12:38:21 2019 -0300

    RDMA/mlx5: Add missing synchronize_srcu() for MW cases
    
    While MR uses live as the SRCU 'update', the MW case uses the xarray
    directly, xa_erase() causes the MW to become inaccessible to the pagefault
    thread.
    
    Thus whenever a MW is removed from the xarray we must synchronize_srcu()
    before freeing it.
    
    This must be done before freeing the mkey as re-use of the mkey while the
    pagefault thread is using the stale mkey is undesirable.
    
    Add the missing synchronizes to MW and DEVX indirect mkey and delete the
    bogus protection against double destroy in mlx5_core_destroy_mkey()
    
    Fixes: 534fd7aac56a ("IB/mlx5: Manage indirection mkey upon DEVX flow for ODP")
    Fixes: 6aec21f6a832 ("IB/mlx5: Page faults handling infrastructure")
    Link: https://lore.kernel.org/r/20191001153821.23621-7-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 15e42825cc97..1a98ee2e01c4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -639,7 +639,6 @@ struct mlx5_ib_mw {
 struct mlx5_ib_devx_mr {
 	struct mlx5_core_mkey	mmkey;
 	int			ndescs;
-	struct rcu_head		rcu;
 };
 
 struct mlx5_ib_umr_context {

commit aa603815c7f8a8f368d6d8ea6ebf601429883439
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Oct 1 12:38:20 2019 -0300

    RDMA/mlx5: Put live in the correct place for ODP MRs
    
    live is used to signal to the pagefault thread that the MR is initialized
    and ready for use. It should be after the umem is assigned and all other
    setup is completed. This prevents races (at least) of the form:
    
        CPU0                                     CPU1
    mlx5_ib_alloc_implicit_mr()
     implicit_mr_alloc()
      live = 1
     imr->umem = umem
                                        num_pending_prefetch_inc()
                                          if (live)
                                            atomic_inc(num_pending_prefetch)
     atomic_set(num_pending_prefetch,0) // Overwrites other thread's store
    
    Further, live is being used with SRCU as the 'update' in an
    acquire/release fashion, so it can not be read and written raw.
    
    Move all live = 1's to after MR initialization is completed and use
    smp_store_release/smp_load_acquire() for manipulating it.
    
    Add a missing live = 0 when an implicit MR child is deleted, before
    queuing work to do synchronize_srcu().
    
    The barriers in update_odp_mr() were some broken attempt to create a
    acquire/release, but were not even applied consistently and missed the
    point, delete it as well.
    
    Fixes: 6aec21f6a832 ("IB/mlx5: Page faults handling infrastructure")
    Link: https://lore.kernel.org/r/20191001153821.23621-6-jgg@ziepe.ca
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2ceaef3ea3fb..15e42825cc97 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -606,7 +606,7 @@ struct mlx5_ib_mr {
 	struct mlx5_ib_dev     *dev;
 	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
 	struct mlx5_core_sig_ctx    *sig;
-	int			live;
+	unsigned int		live;
 	void			*descs_alloc;
 	int			access_flags; /* Needed for rereg MR */
 

commit 4b2a67362e7814641730fa4c561a9ef609fc7a03
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Oct 2 15:25:14 2019 +0300

    RDMA/mlx5: Group boolean parameters to take less space
    
    Clean the code to store all boolean parameters inside one variable.
    
    Link: https://lore.kernel.org/r/20191002122517.17721-2-leon@kernel.org
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2ceaef3ea3fb..bf30d53d94dc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -958,7 +958,10 @@ struct mlx5_ib_dev {
 	/* serialize update of capability mask
 	 */
 	struct mutex			cap_mask_mutex;
-	bool				ib_active;
+	u8				ib_active:1;
+	u8				fill_delay:1;
+	u8				is_rep:1;
+	u8				lag_active:1;
 	struct umr_common		umrc;
 	/* sync used page count stats
 	 */
@@ -967,7 +970,6 @@ struct mlx5_ib_dev {
 	struct timer_list		delay_timer;
 	/* Prevents soft lock on massive reg MRs */
 	struct mutex			slow_path_mutex;
-	int				fill_delay;
 	struct ib_odp_caps	odp_caps;
 	u64			odp_max_size;
 	struct mlx5_ib_pf_eq	odp_pf_eq;
@@ -988,8 +990,6 @@ struct mlx5_ib_dev {
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
-	bool			is_rep;
-	int				lag_active;
 
 	struct mlx5_ib_lb_state		lb;
 	u8			umr_fence;

commit 018c6837f3e63b45163d55a1668d9f8e6fdecf6e
Merge: 84da111de0b4 3eca7fc2d8d1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 21 10:26:24 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull RDMA subsystem updates from Jason Gunthorpe:
     "This cycle mainly saw lots of bug fixes and clean up code across the
      core code and several drivers, few new functional changes were made.
    
       - Many cleanup and bug fixes for hns
    
       - Various small bug fixes and cleanups in hfi1, mlx5, usnic, qed,
         bnxt_re, efa
    
       - Share the query_port code between all the iWarp drivers
    
       - General rework and cleanup of the ODP MR umem code to fit better
         with the mmu notifier get/put scheme
    
       - Support rdma netlink in non init_net name spaces
    
       - mlx5 support for XRC devx and DC ODP"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (99 commits)
      RDMA: Fix double-free in srq creation error flow
      RDMA/efa: Fix incorrect error print
      IB/mlx5: Free mpi in mp_slave mode
      IB/mlx5: Use the original address for the page during free_pages
      RDMA/bnxt_re: Fix spelling mistake "missin_resp" -> "missing_resp"
      RDMA/hns: Package operations of rq inline buffer into separate functions
      RDMA/hns: Optimize cmd init and mode selection for hip08
      IB/hfi1: Define variables as unsigned long to fix KASAN warning
      IB/{rdmavt, hfi1, qib}: Add a counter for credit waits
      IB/hfi1: Add traces for TID RDMA READ
      RDMA/siw: Relax from kmap_atomic() use in TX path
      IB/iser: Support up to 16MB data transfer in a single command
      RDMA/siw: Fix page address mapping in TX path
      RDMA: Fix goto target to release the allocated memory
      RDMA/usnic: Avoid overly large buffers on stack
      RDMA/odp: Add missing cast for 32 bit
      RDMA/hns: Use devm_platform_ioremap_resource() to simplify code
      Documentation/infiniband: update name of some functions
      RDMA/cma: Fix false error message
      RDMA/hns: Fix wrong assignment of qp_access_flags
      ...

commit 75c66515e4fea4e9bb488b6125e014220f846c61
Merge: 1ba7c8f80058 f74c2bb98776
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Sep 12 12:49:56 2019 -0300

    Merge tag 'v5.3-rc8' into rdma.git for-next
    
    To resolve dependencies in following patches
    
    mlx5_ib.h conflict resolved by keeing both hunks
    
    Linux 5.3-rc8
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 2b688ea5efdee2868ed23eddfdbe27dbd232edac
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Aug 15 13:54:17 2019 +0300

    net/mlx5: Add flow steering actions to fs_cmd shim layer
    
    Add flow steering actions: modify header and packet reformat
    to the fs_cmd shim layer. This allows each namespace to define
    possibly different functionality for alloc/dealloc action commands.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a20d2ee08a3b..125a507c10ed 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -868,7 +868,10 @@ struct mlx5_ib_flow_action {
 		struct {
 			struct mlx5_ib_dev *dev;
 			u32 sub_type;
-			u32 action_id;
+			union {
+				struct mlx5_modify_hdr *modify_hdr;
+				struct mlx5_pkt_reformat *pkt_reformat;
+			};
 		} flow_action_raw;
 	};
 };

commit a06ebb8d953b4100236f3057be51d67640e06323
Merge: 4bc61b0b1695 fc603294267f
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sun Sep 1 23:47:09 2019 -0700

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    Merge mlx5-next patches needed for upcoming mlx5 software steering.
    
    1) Alex adds HW bits and definitions required for SW steering
    2) Ariel moves device memory management to mlx5_core (From mlx5_ib)
    3) Maor, Cleanups and fixups for eswitch mode and RoCE
    4) Mark, Set only stag for match untagged packets
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit c9b9dcb430b3cd0ad2b04c360c4e528d73430481
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Aug 29 23:42:30 2019 +0000

    net/mlx5: Move device memory management to mlx5_core
    
    Move the device memory allocation and deallocation commands
    SW ICM memory to mlx5_core to expose this API for all
    mlx5_core users.
    
    This comes as preparation for supporting SW steering in kernel
    where it will be required to allocate and register device
    memory for direct rule insertion.
    
    In addition, an API to register this device memory for future
    remote access operations is introduced using the create_mkey
    commands.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c482f19958b3..afd69ba33b2b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -880,8 +880,6 @@ struct mlx5_dm {
 	 */
 	spinlock_t lock;
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
-	unsigned long *steering_sw_icm_alloc_blocks;
-	unsigned long *header_modify_sw_icm_alloc_blocks;
 };
 
 struct mlx5_read_counters_attr {

commit d8abe88450beb96a66e434323eb6ab737654b840
Author: Mark Zhang <markz@mellanox.com>
Date:   Mon Aug 19 14:36:26 2019 +0300

    RDMA/mlx5: RDMA_RX flow type support for user applications
    
    Currently user applications can only steer TCP/IP(NIC RX/RX) traffic.
    This patch adds RDMA_RX as a new flow type to allow the user to insert
    steering rules to control RDMA traffic.
    Two destinations are supported(but not set at the same time): devx
    flow table object and QP.
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190819113626.20284-4-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index cb41a7e6255a..6abfbf3a69b7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -200,6 +200,7 @@ struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
 	struct mlx5_ib_flow_prio	egress[MLX5_IB_NUM_EGRESS_FTS];
 	struct mlx5_ib_flow_prio	fdb;
+	struct mlx5_ib_flow_prio	rdma_rx[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_flow_table		*lag_demux_ft;
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.

commit 0e6613b41edd2f55a4b33234c5f31410c1ed3783
Author: Moni Shoua <monis@mellanox.com>
Date:   Thu Aug 15 11:38:31 2019 +0300

    IB/mlx5: Consolidate use_umr checks into single function
    
    Introduce helper function to unify various use_umr checks.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190815083834.9245-6-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f6a53455bf8b..9ae587b74b12 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1475,4 +1475,18 @@ int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
 			bool dyn_bfreg);
 
 int mlx5_ib_qp_set_counter(struct ib_qp *qp, struct rdma_counter *counter);
+
+static inline bool mlx5_ib_can_use_umr(struct mlx5_ib_dev *dev,
+				       bool do_modify_atomic)
+{
+	if (MLX5_CAP_GEN(dev->mdev, umr_modify_entity_size_disabled))
+		return false;
+
+	if (do_modify_atomic &&
+	    MLX5_CAP_GEN(dev->mdev, atomic) &&
+	    MLX5_CAP_GEN(dev->mdev, umr_modify_atomic_disabled))
+		return false;
+
+	return true;
+}
 #endif /* MLX5_IB_H */

commit 525a2c651cdd08b19a4c04f63b87e460765220ac
Merge: 3e1f000ff746 708637e65abd
Author: Doug Ledford <dledford@redhat.com>
Date:   Mon Jul 29 13:38:42 2019 -0400

    Merge branch 'wip/dl-for-rc' into wip/dl-for-next
    
    The fix for IB port statistics initialization ("IB/core: Fix querying
    total rdma stats") is needed before we take a follow-on patch to
    for-next.
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 3e1f000ff74627c1adb99ee513f29ec2522ee309
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Jul 23 10:31:17 2019 +0300

    IB/mlx5: Support per device q counters in switchdev mode
    
    When parent mlx5_core_dev is in switchdev mode, q_counters are not
    applicable to multiple non uplink vports.
    Hence, have make them limited to device level.
    
    While at it, correct __mlx5_ib_qp_set_counter() and
    __mlx5_ib_modify_qp() to use u16 set_id as defined by the device.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Link: https://lore.kernel.org/r/20190723073117.7175-3-leon@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c482f19958b3..7a62454ad979 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1474,4 +1474,5 @@ int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
 			bool dyn_bfreg);
 
 int mlx5_ib_qp_set_counter(struct ib_qp *qp, struct rdma_counter *counter);
+u16 mlx5_ib_get_counters_id(struct mlx5_ib_dev *dev, u8 port_num);
 #endif /* MLX5_IB_H */

commit 6a053953739d23694474a5f9c81d1a30093da81a
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Jul 23 09:57:25 2019 +0300

    IB/mlx5: Fix unreg_umr to ignore the mkey state
    
    Fix unreg_umr to ignore the mkey state and do not fail if was freed.  This
    prevents a case that a user space application already changed the mkey
    state to free and then the UMR operation will fail leaving the mkey in an
    inappropriate state.
    
    Link: https://lore.kernel.org/r/20190723065733.4899-3-leon@kernel.org
    Cc: <stable@vger.kernel.org> # 3.19
    Fixes: 968e78dd9644 ("IB/mlx5: Enhance UMR support to allow partial page table update")
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c482f19958b3..f6a53455bf8b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -481,6 +481,7 @@ struct mlx5_umr_wr {
 	u64				length;
 	int				access_flags;
 	u32				mkey;
+	u8				ignore_free_state:1;
 };
 
 static inline const struct mlx5_umr_wr *umr_wr(const struct ib_send_wr *wr)

commit d14133dd41614aaaac1fa0505c7dab01f4211d2c
Author: Mark Zhang <markz@mellanox.com>
Date:   Tue Jul 2 13:02:36 2019 +0300

    IB/mlx5: Support set qp counter
    
    Support bind a qp with counter. If counter is null then bind the qp to the
    default counter. Different QP state has different operation:
    
    - RESET: Set the counter field so that it will take effective during
      RST2INIT change;
    - RTS: Issue an RTS2RTS change to update the QP counter;
    - Other: Set the counter field and mark the counter_pending flag, when QP
      is moved to RTS state and this flag is set, then issue an RTS2RTS
      modification to update the counter.
    
    Signed-off-by: Mark Zhang <markz@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7373e9da0919..c482f19958b3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -439,6 +439,10 @@ struct mlx5_ib_qp {
 	u32			flags_en;
 	/* storage for qp sub type when core qp type is IB_QPT_DRIVER */
 	enum ib_qp_type		qp_sub_type;
+	/* A flag to indicate if there's a new counter is configured
+	 * but not take effective
+	 */
+	u32                     counter_pending;
 };
 
 struct mlx5_ib_cq_buf {
@@ -1468,4 +1472,6 @@ void mlx5_ib_put_xlt_emergency_page(void);
 int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
 			struct mlx5_bfreg_info *bfregi, u32 bfregn,
 			bool dyn_bfreg);
+
+int mlx5_ib_qp_set_counter(struct ib_qp *qp, struct rdma_counter *counter);
 #endif /* MLX5_IB_H */

commit e337dd53ce4cc3db79e52704e554f648c46d5e91
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 30 19:23:30 2019 +0300

    IB/mlx5: Register DEVX with mlx5_core to get async events
    
    Register DEVX with with mlx5_core to get async events.  This will enable
    to dispatch the applicable events to its consumers in down stream patches.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 305d26cdf7f3..7373e9da0919 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -936,6 +936,13 @@ struct mlx5_ib_pf_eq {
 	mempool_t *pool;
 };
 
+struct mlx5_devx_event_table {
+	struct mlx5_nb devx_nb;
+	/* serialize updating the event_xa */
+	struct mutex event_xa_lock;
+	struct xarray event_xa;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -985,6 +992,7 @@ struct mlx5_ib_dev {
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;
+	struct mlx5_devx_event_table devx_event_table;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
@@ -1324,6 +1332,8 @@ void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
+void mlx5_ib_devx_init_event_table(struct mlx5_ib_dev *dev);
+void mlx5_ib_devx_cleanup_event_table(struct mlx5_ib_dev *dev);
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
 extern const struct uapi_definition mlx5_ib_devx_defs[];
 extern const struct uapi_definition mlx5_ib_flow_defs[];
@@ -1341,6 +1351,8 @@ static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 			   bool is_user) { return -EOPNOTSUPP; }
 static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
+static inline void mlx5_ib_devx_init_event_table(struct mlx5_ib_dev *dev) {}
+static inline void mlx5_ib_devx_cleanup_event_table(struct mlx5_ib_dev *dev) {}
 static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
 					     int *dest_type)
 {

commit 69ea0582f3ce7a72d312da7305e455801a8dc5a4
Merge: 2f40cf30c864 e4075c442876
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jul 3 16:43:45 2019 -0300

    Merge mlx5-next into rdma for-next
    
    From git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    Required for dependencies in the next patches.
    
    Resolved the conflicts:
     - esw_destroy_offloads_acl_tables() use the newer mlx5_esw_for_all_vports()
       version
     - esw_offloads_steering_init() drop the cap test
     - esw_offloads_init() drop the extra function arguments
    
    * branch 'mlx5-next': (39 commits)
      net/mlx5: Expose device definitions for object events
      net/mlx5: Report EQE data upon CQ completion
      net/mlx5: Report a CQ error event only when a handler was set
      net/mlx5: mlx5_core_create_cq() enhancements
      net/mlx5: Expose the API to register for ANY event
      net/mlx5: Use event mask based on device capabilities
      net/mlx5: Fix mlx5_core_destroy_cq() error flow
      net/mlx5: E-Switch, Handle UC address change in switchdev mode
      net/mlx5: E-Switch, Consider host PF for inline mode and vlan pop
      net/mlx5: E-Switch, Use iterator for vlan and min-inline setups
      net/mlx5: E-Switch, Reg/unreg function changed event at correct stage
      net/mlx5: E-Switch, Consolidate eswitch function number of VFs
      net/mlx5: E-Switch, Refactor eswitch SR-IOV interface
      net/mlx5: Handle host PF vport mac/guid for ECPF
      net/mlx5: E-Switch, Use correct flags when configuring vlan
      net/mlx5: Reduce dependency on enabled_vfs counter and num_vfs
      net/mlx5: Don't handle VF func change if host PF is disabled
      net/mlx5: Limit scope of mlx5_get_next_phys_dev() to PCI PF devices
      net/mlx5: Move pci status reg access mutex to mlx5_pci_init
      net/mlx5: Rename mlx5_pci_dev_type to mlx5_coredev_type
      ...
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 2f69e591e4531d3192841a4eb2bd9b512f5a8b66
Author: Bodong Wang <bodong@mellanox.com>
Date:   Fri Jun 28 22:35:53 2019 +0000

    {IB, net}/mlx5: E-Switch, Use index of rep for vport to IB port mapping
    
    In the single IB device mode, the mapping between vport number and
    rep relies on a counter. However for dynamic vport allocation, it is
    desired to keep consistent map of eswitch vport and IB port.
    
    Hence, simplify code to remove the free running counter and instead
    use the available vport index during load/unload sequence from the
    eswitch.
    
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Suggested-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1c205c2bd486..ee73dc122d28 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -978,7 +978,6 @@ struct mlx5_ib_dev {
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;
-	int			free_port;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit bb0ee7dcc4ecd6af39823b80ae3995ddc119c373
Author: Jianbo Liu <jianbol@mellanox.com>
Date:   Tue Jun 25 17:47:58 2019 +0000

    net/mlx5: Add flow context for flow tag
    
    Refactor the flow data structures, add new flow_context and move
    flow_tag into it, as flow_tag doesn't belong to the rule action.
    
    Signed-off-by: Jianbo Liu <jianbol@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a043af7ee366..1c205c2bd486 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1317,6 +1317,7 @@ extern const struct uapi_definition mlx5_ib_devx_defs[];
 extern const struct uapi_definition mlx5_ib_flow_defs[];
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+	struct mlx5_flow_context *flow_context,
 	struct mlx5_flow_act *flow_act, u32 counter_id,
 	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);

commit 2563e2f30acb4c914fc475331e476fa920eb4245
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:56 2019 +0300

    RDMA/mlx5: Use PA mapping for PI handover
    
    If possibe, avoid doing a UMR operation to register data and protection
    buffers (via MTT/KLM mkeys). Instead, use the local DMA key and map the
    SG lists using PA access. This is safe, since the internal key for data
    and protection never exposed to the remote server (only signature key
    might be exposed). If PA mappings are not possible, perform mapping
    using MTT/KLM descriptors.
    
    The setup of the tested benchmark (using iSER ULP):
     - 2 servers with 24 cores (1 initiator and 1 target)
     - ConnectX-4/ConnectX-5 adapters
     - 24 target sessions with 1 LUN each
     - ramdisk backstore
     - PI active
    
    Performance results running fio (24 jobs, 128 iodepth) using
    write_generate=1 and read_verify=1 (w/w.o patch):
    
    bs      IOPS(read)        IOPS(write)
    ----    ----------        ----------
    512   1266.4K/1262.4K    1720.1K/1732.1K
    4k    793139/570902      1129.6K/773982
    32k   72660/72086        97229/96164
    
    Using write_generate=0 and read_verify=0 (w/w.o patch):
    bs      IOPS(read)        IOPS(write)
    ----    ----------        ----------
    512   1590.2K/1600.1K    1828.2K/1830.3K
    4k    1078.1K/937272     1142.1K/815304
    32k   77012/77369        98125/97435
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Suggested-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 405059521321..bdb83fc85f94 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -609,6 +609,7 @@ struct mlx5_ib_mr {
 	struct mlx5_ib_mr      *pi_mr;
 	struct mlx5_ib_mr      *klm_mr;
 	struct mlx5_ib_mr      *mtt_mr;
+	u64			data_iova;
 	u64			pi_iova;
 
 	atomic_t		num_leaf_free;

commit de0ae958deb5e6af35c4c6a4679d4fe9896a98ca
Author: Israel Rukshin <israelr@mellanox.com>
Date:   Tue Jun 11 18:52:55 2019 +0300

    RDMA/mlx5: Improve PI handover performance
    
    In some loads, there is performance degradation when using KLM mkey
    instead of MTT mkey. This is because KLM descriptor access is via
    indirection that might require more HW resources and cycles.
    Using KLM descriptor is not necessary when there are no gaps at the
    data/metadata sg lists. As an optimization, use MTT mkey whenever it
    is possible. For that matter, allocate internal MTT mkey and choose the
    effective pi_mr for in transaction according to the required mapping
    scheme.
    
    The setup of the tested benchmark (using iSER ULP):
     - 2 servers with 24 cores (1 initiator and 1 target)
     - ConnectX-4/ConnectX-5 adapters
     - 24 target sessions with 1 LUN each
     - ramdisk backstore
     - PI active
    
    Performance results running fio (24 jobs, 128 iodepth) using
    write_generate=1 and read_verify=1 (w/w.o/baseline):
    
    bs      IOPS(read)                IOPS(write)
    ----    ----------                ----------
    512   1262.4K/1243.3K/1147.1K    1732.1K/1725.1K/1423.8K
    4k    570902/571233/457874       773982/743293/642080
    32k   72086/72388/71933          96164/71789/93249
    
    Using write_generate=0 and read_verify=0 (w/w.o patch):
    bs      IOPS(read)                IOPS(write)
    ----    ----------                ----------
    512   1600.1K/1572.1K/1393.3K    1830.3K/1823.5K/1557.2K
    4k    937272/921992/762934       815304/753772/646071
    32k   77369/75052/72058          97435/73180/94612
    
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Suggested-by: Max Gurtovoy <maxg@mellanox.com>
    Suggested-by: Idan Burstein <idanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d418219e68c6..405059521321 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -605,7 +605,12 @@ struct mlx5_ib_mr {
 	int			access_flags; /* Needed for rereg MR */
 
 	struct mlx5_ib_mr      *parent;
-	struct mlx5_ib_mr      *pi_mr; /* Needed for IB_MR_TYPE_INTEGRITY */
+	/* Needed for IB_MR_TYPE_INTEGRITY */
+	struct mlx5_ib_mr      *pi_mr;
+	struct mlx5_ib_mr      *klm_mr;
+	struct mlx5_ib_mr      *mtt_mr;
+	u64			pi_iova;
+
 	atomic_t		num_leaf_free;
 	wait_queue_head_t       q_leaf_free;
 	struct mlx5_async_work  cb_work;

commit 185eddc45798b9f73e5470964948d79b4c8df4b7
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:51 2019 +0300

    RDMA/core: Validate integrity handover device cap
    
    Protect the case that a ULP tries to allocate a QP with signature
    enabled flag while the LLD doesn't support this feature.
    While we're here, also move integrity_en attribute from mlx5_qp to
    ib_qp as a preparation for adding new integrity API to the rw-API
    (that is part of ib_core module).
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5999792b5698..d418219e68c6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -431,8 +431,6 @@ struct mlx5_ib_qp {
 
 	int			create_type;
 
-	bool			integrity_en;
-
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;

commit c0a6cbb9cbccffc249743afa16e64f16c46c80b2
Author: Israel Rukshin <israelr@mellanox.com>
Date:   Tue Jun 11 18:52:50 2019 +0300

    RDMA/core: Rename signature qp create flag and signature device capability
    
    Rename IB_QP_CREATE_SIGNATURE_EN to IB_QP_CREATE_INTEGRITY_EN
    and IB_DEVICE_SIGNATURE_HANDOVER to IB_DEVICE_INTEGRITY_HANDOVER.
    
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7980814f355d..5999792b5698 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -431,8 +431,7 @@ struct mlx5_ib_qp {
 
 	int			create_type;
 
-	/* Store signature errors */
-	bool			signature_en;
+	bool			integrity_en;
 
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;

commit 6c984472bad12da18b88e9f4345f4970bbec0b3e
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Tue Jun 11 18:52:42 2019 +0300

    RDMA/mlx5: Implement mlx5_ib_map_mr_sg_pi and mlx5_ib_alloc_mr_integrity
    
    mlx5_ib_map_mr_sg_pi() will map the PI and data dma mapped SG lists to the
    mlx5 memory region prior to the registration operation. In the new
    API, the mlx5 driver will allocate an internal memory region for the
    UMR operation to register both PI and data SG lists. The internal MR
    will use KLM mode in order to map 2 (possibly non-contiguous/non-align)
    SG lists using 1 memory key. In the new API, each ULP will use 1 memory
    region for the signature operation (instead of 3 in the old API). This
    memory region will have a key that will be exposed to remote server to
    perform RDMA operation. The internal memory key that will map the SG lists
    will stay private.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Israel Rukshin <israelr@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 82cfe86087b6..7980814f355d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -587,6 +587,9 @@ struct mlx5_ib_mr {
 	void			*descs;
 	dma_addr_t		desc_map;
 	int			ndescs;
+	int			data_length;
+	int			meta_ndescs;
+	int			meta_length;
 	int			max_descs;
 	int			desc_size;
 	int			access_mode;
@@ -605,6 +608,7 @@ struct mlx5_ib_mr {
 	int			access_flags; /* Needed for rereg MR */
 
 	struct mlx5_ib_mr      *parent;
+	struct mlx5_ib_mr      *pi_mr; /* Needed for IB_MR_TYPE_INTEGRITY */
 	atomic_t		num_leaf_free;
 	wait_queue_head_t       q_leaf_free;
 	struct mlx5_async_work  cb_work;
@@ -1148,8 +1152,15 @@ int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 			       u32 max_num_sg, struct ib_udata *udata);
+struct ib_mr *mlx5_ib_alloc_mr_integrity(struct ib_pd *pd,
+					 u32 max_num_sg,
+					 u32 max_num_meta_sg);
 int mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		      unsigned int *sg_offset);
+int mlx5_ib_map_mr_sg_pi(struct ib_mr *ibmr, struct scatterlist *data_sg,
+			 int data_sg_nents, unsigned int *data_sg_offset,
+			 struct scatterlist *meta_sg, int meta_sg_nents,
+			 unsigned int *meta_sg_offset);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 			const struct ib_mad_hdr *in, size_t in_mad_size,

commit a49b1dc7ae447d7085360cd587fc1c8b9ec6c871
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Jun 12 15:27:41 2019 +0300

    RDMA: Convert destroy_wq to be void
    
    All callers of destroy WQ are always success and there is no need
    to check their return value, so convert destroy_wq to be void.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 35e2c8f5ae78..82cfe86087b6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1201,7 +1201,7 @@ int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
 				struct ib_wq_init_attr *init_attr,
 				struct ib_udata *udata);
-int mlx5_ib_destroy_wq(struct ib_wq *wq, struct ib_udata *udata);
+void mlx5_ib_destroy_wq(struct ib_wq *wq, struct ib_udata *udata);
 int mlx5_ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *wq_attr,
 		      u32 wq_attr_mask, struct ib_udata *udata);
 struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,

commit 12dbc04db08dc225b0e06cc5bb99c2dfa8236df9
Merge: 7608bf40cf24 82b11f071936
Author: Doug Ledford <dledford@redhat.com>
Date:   Tue Jun 18 22:44:36 2019 -0400

    Merge remote-tracking branch 'mlx5-next/mlx5-next' into HEAD
    
    Take mlx5-next so we can take a dependent two patch series next.
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit ca390799c2aa03632c294107fa7f647bcbdff428
Author: Yuval Avnery <yuvalav@mellanox.com>
Date:   Mon Jun 10 23:38:23 2019 +0000

    net/mlx5: Change interrupt handler to call chain notifier
    
    Multiple EQs may share the same IRQ in subsequent patches.
    
    Instead of calling the IRQ handler directly, the EQ will register
    to an atomic chain notfier.
    
    The Linux built-in shared IRQ is not used because it forces the caller
    to disable the IRQ and clear affinity before free_irq() can be called.
    
    This patch is the first step in the separation of IRQ and EQ logic.
    
    Signed-off-by: Yuval Avnery <yuvalav@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 40eb8be482e4..a043af7ee366 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -920,6 +920,7 @@ struct mlx5_ib_lb_state {
 };
 
 struct mlx5_ib_pf_eq {
+	struct notifier_block irq_nb;
 	struct mlx5_ib_dev *dev;
 	struct mlx5_eq *core;
 	struct work_struct work;

commit e39afe3d6dbd908d8fd189571a3c1561088a86c2
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 28 14:37:29 2019 +0300

    RDMA: Convert CQ allocations to be under core responsibility
    
    Ensure that CQ is allocated and freed by IB/core and not by drivers.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Tested-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index bf697c5b3e79..f2ad0372d38d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1115,9 +1115,8 @@ int mlx5_ib_read_user_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
 			     int buflen, size_t *bc);
 int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
 			      void *buffer, int buflen, size_t *bc);
-struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
-				const struct ib_cq_init_attr *attr,
-				struct ib_udata *udata);
+int mlx5_ib_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
+		      struct ib_udata *udata);
 void mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
 int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
 int mlx5_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);

commit a52c8e2469c30cf7ac453d624aed9c168b23d1af
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue May 28 14:37:28 2019 +0300

    RDMA: Clean destroy CQ in drivers do not return errors
    
    Like all other destroy commands, .destroy_cq() call is not supposed
    to fail. In all flows, the attempt to return earlier caused to memory
    leaks.
    
    This patch converts .destroy_cq() to do not return any errors.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Gal Pressman <galpress@amazon.com>
    Acked-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 40eb8be482e4..bf697c5b3e79 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1118,7 +1118,7 @@ int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
 struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_udata *udata);
-int mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
+void mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
 int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
 int mlx5_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
 int mlx5_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);

commit 25c13324d03d004f9e8071bf5bf5d5c6fdace71e
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Sun May 5 17:07:13 2019 +0300

    IB/mlx5: Add steering SW ICM device memory type
    
    This patch adds support for allocating, deallocating and registering a new
    device memory type, STEERING_SW_ICM.  This memory can be allocated and
    used by a privileged user for direct rule insertion and management of the
    device's steering tables.
    
    The type is provided by the user via the dedicated attribute in the
    alloc_dm ioctl command.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 714c360dc9fb..40eb8be482e4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -118,6 +118,10 @@ enum {
 	MLX5_MEMIC_BASE_SIZE	= 1 << MLX5_MEMIC_BASE_ALIGN,
 };
 
+#define MLX5_LOG_SW_ICM_BLOCK_SIZE(dev)                                        \
+	(MLX5_CAP_DEV_MEM(dev, log_sw_icm_alloc_granularity))
+#define MLX5_SW_ICM_BLOCK_SIZE(dev) (1 << MLX5_LOG_SW_ICM_BLOCK_SIZE(dev))
+
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;
@@ -557,6 +561,12 @@ struct mlx5_ib_dm {
 	phys_addr_t		dev_addr;
 	u32			type;
 	size_t			size;
+	union {
+		struct {
+			u32	obj_id;
+		} icm_dm;
+		/* other dm types specific params should be added here */
+	};
 };
 
 #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
@@ -567,6 +577,11 @@ struct mlx5_ib_dm {
 					 IB_ACCESS_REMOTE_ATOMIC |\
 					 IB_ZERO_BASED)
 
+#define MLX5_IB_DM_SW_ICM_ALLOWED_ACCESS (IB_ACCESS_LOCAL_WRITE   |\
+					  IB_ACCESS_REMOTE_WRITE  |\
+					  IB_ACCESS_REMOTE_READ   |\
+					  IB_ZERO_BASED)
+
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
 	void			*descs;
@@ -854,6 +869,8 @@ struct mlx5_dm {
 	 */
 	spinlock_t lock;
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
+	unsigned long *steering_sw_icm_alloc_blocks;
+	unsigned long *header_modify_sw_icm_alloc_blocks;
 };
 
 struct mlx5_read_counters_attr {

commit 3b113a1ec3d4ac7e1e621b77650ac05491f5924a
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Sun May 5 17:07:11 2019 +0300

    IB/mlx5: Support device memory type attribute
    
    This patch intoruduces a new mlx5_ib driver attribute to the DM allocation
    method - the DM type.
    
    In order to allow addition of new types in downstream patches this patch
    also refactors the allocation, deallocation and registration handlers to
    consider the requested type and perform the necessary actions according to
    it.
    
    Since not all future device memory types will be such that are mapped to
    user memory, the mandatory page index output attribute is modified to be
    optional.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 55b8bdb402b6..714c360dc9fb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -48,6 +48,7 @@
 #include <rdma/mlx5-abi.h>
 #include <rdma/uverbs_ioctl.h>
 #include <rdma/mlx5_user_ioctl_cmds.h>
+#include <rdma/mlx5_user_ioctl_verbs.h>
 
 #include "srq.h"
 
@@ -554,15 +555,17 @@ enum mlx5_ib_mtt_access_flags {
 struct mlx5_ib_dm {
 	struct ib_dm		ibdm;
 	phys_addr_t		dev_addr;
+	u32			type;
+	size_t			size;
 };
 
 #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
 
-#define MLX5_IB_DM_ALLOWED_ACCESS (IB_ACCESS_LOCAL_WRITE   |\
-				   IB_ACCESS_REMOTE_WRITE  |\
-				   IB_ACCESS_REMOTE_READ   |\
-				   IB_ACCESS_REMOTE_ATOMIC |\
-				   IB_ZERO_BASED)
+#define MLX5_IB_DM_MEMIC_ALLOWED_ACCESS (IB_ACCESS_LOCAL_WRITE   |\
+					 IB_ACCESS_REMOTE_WRITE  |\
+					 IB_ACCESS_REMOTE_READ   |\
+					 IB_ACCESS_REMOTE_ATOMIC |\
+					 IB_ZERO_BASED)
 
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
@@ -843,9 +846,13 @@ struct mlx5_ib_flow_action {
 	};
 };
 
-struct mlx5_memic {
+struct mlx5_dm {
 	struct mlx5_core_dev *dev;
-	spinlock_t		memic_lock;
+	/* This lock is used to protect the access to the shared
+	 * allocation map when concurrent requests by different
+	 * processes are handled.
+	 */
+	spinlock_t lock;
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
 };
 
@@ -949,7 +956,7 @@ struct mlx5_ib_dev {
 	u8			umr_fence;
 	struct list_head	ib_dev_list;
 	u64			sys_image_guid;
-	struct mlx5_memic	memic;
+	struct mlx5_dm		dm;
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;

commit 13a4376568f6e3a6df21e4c116b027101e28f954
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Mar 28 15:46:21 2019 +0200

    RDMA/mlx5: Access the prio bypass inside the FDB flow table namespace
    
    Now that we have a specific prio inside the FDB namespace allow retrieving
    it from the RDMA side.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f5d572d1a492..55b8bdb402b6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -194,6 +194,7 @@ struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	egress_prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
 	struct mlx5_ib_flow_prio	egress[MLX5_IB_NUM_EGRESS_FTS];
+	struct mlx5_ib_flow_prio	fdb;
 	struct mlx5_flow_table		*lag_demux_ft;
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.

commit fb652d3299023c8fd1af13c9f897c3e3d8a424d3
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Mar 28 15:27:42 2019 +0200

    RDMA/mlx5: Remove VF representor profile
    
    Now that we have a single IB device with multiple ports we can remove the
    VF representor profile.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4bab14e699ae..f5d572d1a492 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1236,23 +1236,6 @@ static inline void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp,
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
 /* Needed for rep profile */
-int mlx5_ib_stage_init_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_rep_flow_db_init(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_caps_init(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_rep_non_default_cb(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_rep_roce_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_rep_roce_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_dev_res_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_dev_res_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_counters_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_counters_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_bfrag_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_bfrag_cleanup(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_pre_ib_reg_umr_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_ib_reg_init(struct mlx5_ib_dev *dev);
-void mlx5_ib_stage_ib_reg_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_post_ib_reg_umr_init(struct mlx5_ib_dev *dev);
 void __mlx5_ib_remove(struct mlx5_ib_dev *dev,
 		      const struct mlx5_ib_profile *profile,
 		      int stage);

commit 26628e2d58c910fa724312c6bcc3f4d12c9e805e
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Mar 28 15:27:41 2019 +0200

    RDMA/mlx5: Move to single device multiport ports in switchdev mode
    
    Move from IB device (representor) per virtual function to single IB device
    with port per virtual function (port 1 represents the uplink). As number
    of ports is a static property of an IB device, declare the IB device with
    as many port as the possible according to the PCI bus.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 9445e7f2c8fd..4bab14e699ae 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -952,6 +952,7 @@ struct mlx5_ib_dev {
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
 	struct mlx5_async_ctx   async_ctx;
+	int			free_port;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 6a4d00be08334f15502f2fbec08eabbdddc2e64a
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Mar 28 15:27:37 2019 +0200

    RDMA/mlx5: Move rep into port struct
    
    In preparation of moving into a model of single IB device multiple ports
    move rep to be part of the port structure. We mark a representor device by
    setting is_rep, no functional change with this patch.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ad0effec3d33..9445e7f2c8fd 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -720,6 +720,7 @@ struct mlx5_ib_port {
 	struct mlx5_ib_multiport mp;
 	struct mlx5_ib_dbg_cc_params *dbg_cc_params;
 	struct mlx5_roce roce;
+	struct mlx5_eswitch_rep		*rep;
 };
 
 struct mlx5_ib_dbg_param {
@@ -940,7 +941,7 @@ struct mlx5_ib_dev {
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
-	struct mlx5_eswitch_rep		*rep;
+	bool			is_rep;
 	int				lag_active;
 
 	struct mlx5_ib_lb_state		lb;

commit 95579e785a9ae7d98c199b38c4b79b64a31d90fa
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Mar 28 15:27:33 2019 +0200

    RDMA/mlx5: Move netdev info into the port struct
    
    Netdev info is stored in a separate array and holds data relevant on a per
    port basis, move it to be part of the port struct.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3e8d54618c78..ad0effec3d33 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -702,12 +702,6 @@ struct mlx5_ib_multiport {
 	spinlock_t mpi_lock;
 };
 
-struct mlx5_ib_port {
-	struct mlx5_ib_counters cnts;
-	struct mlx5_ib_multiport mp;
-	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
-};
-
 struct mlx5_roce {
 	/* Protect mlx5_ib_get_netdev from invoking dev_hold() with a NULL
 	 * netdev pointer
@@ -721,6 +715,13 @@ struct mlx5_roce {
 	u8			native_port_num;
 };
 
+struct mlx5_ib_port {
+	struct mlx5_ib_counters cnts;
+	struct mlx5_ib_multiport mp;
+	struct mlx5_ib_dbg_cc_params *dbg_cc_params;
+	struct mlx5_roce roce;
+};
+
 struct mlx5_ib_dbg_param {
 	int			offset;
 	struct mlx5_ib_dev	*dev;
@@ -905,7 +906,6 @@ struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
 	struct notifier_block		mdev_events;
-	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;
 	/* serialize update of capability mask
 	 */

commit 68e326dea1dba935f6a5299a24343a58b33eed10
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:43 2019 +0300

    RDMA: Handle SRQ allocations by IB/core
    
    Convert SRQ allocation from drivers to be in the IB/core
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5b4206653fdb..3e8d54618c78 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1058,13 +1058,12 @@ int mlx5_ib_create_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr, u32 flags,
 		      struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
 void mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags);
-struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
-				  struct ib_srq_init_attr *init_attr,
-				  struct ib_udata *udata);
+int mlx5_ib_create_srq(struct ib_srq *srq, struct ib_srq_init_attr *init_attr,
+		       struct ib_udata *udata);
 int mlx5_ib_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
 		       enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
 int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
-int mlx5_ib_destroy_srq(struct ib_srq *srq, struct ib_udata *udata);
+void mlx5_ib_destroy_srq(struct ib_srq *srq, struct ib_udata *udata);
 int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
 			  const struct ib_recv_wr **bad_wr);
 int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);

commit d345691471b426e540140a4cc431c69f80abfcb6
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Apr 3 16:42:42 2019 +0300

    RDMA: Handle AH allocations by IB/core
    
    Simplify drivers by ensuring lifetime of ib_ah object. The changes
    in .create_ah() go hand in hand with relevant update in .destroy_ah().
    
    We will use this opportunity and convert .destroy_ah() to don't fail, as
    it was suggested a long time ago, because there is nothing to do in case
    of failure during destroy.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e36aa2f79943..5b4206653fdb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1054,10 +1054,10 @@ void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db)
 void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
-struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
-				u32 flags, struct ib_udata *udata);
+int mlx5_ib_create_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr, u32 flags,
+		      struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
-int mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags, struct ib_udata *udata);
+void mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
 				  struct ib_srq_init_attr *init_attr,
 				  struct ib_udata *udata);

commit e79c9c60622a59a814c54a1ee70298afe544441a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Apr 1 17:08:23 2019 -0300

    IB/mlx5: Remove references to uboject->context
    
    These should all go through udata now. Add mlx5_udata_to_mdev to convert
    a udata into the struct mlx5_ib_dev as these call sites require.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f7314d78aafd..e36aa2f79943 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -968,6 +968,14 @@ static inline struct mlx5_ib_dev *to_mdev(struct ib_device *ibdev)
 	return container_of(ibdev, struct mlx5_ib_dev, ib_dev);
 }
 
+static inline struct mlx5_ib_dev *mlx5_udata_to_mdev(struct ib_udata *udata)
+{
+	struct mlx5_ib_ucontext *context = rdma_udata_to_drv_context(
+		udata, struct mlx5_ib_ucontext, ibucontext);
+
+	return to_mdev(context->ibucontext.device);
+}
+
 static inline struct mlx5_ib_cq *to_mcq(struct ib_cq *ibcq)
 {
 	return container_of(ibcq, struct mlx5_ib_cq, ibcq);

commit ff23dfa134576e071ace69e91761d229a0f73139
Author: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
Date:   Sun Mar 31 19:10:07 2019 +0300

    IB: Pass only ib_udata in function prototypes
    
    Now when ib_udata is passed to all the driver's object create/destroy APIs
    the ib_udata will carry the ib_ucontext for every user command. There is
    no need to also pass the ib_ucontext via the functions prototypes.
    
    Make ib_udata the only argument psssed.
    
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e45f59b0cc52..f7314d78aafd 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1083,7 +1083,6 @@ int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
 			      void *buffer, int buflen, size_t *bc);
 struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
-				struct ib_ucontext *context,
 				struct ib_udata *udata);
 int mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
 int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
@@ -1123,8 +1122,7 @@ int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			struct ib_mad_hdr *out, size_t *out_mad_size,
 			u16 *out_mad_pkey_index);
 struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
-					  struct ib_ucontext *context,
-					  struct ib_udata *udata);
+				   struct ib_udata *udata);
 int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);
 int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
 int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);

commit c4367a26357be501338e41ceae7ebb7ce57064e5
Author: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
Date:   Sun Mar 31 19:10:05 2019 +0300

    IB: Pass uverbs_attr_bundle down ib_x destroy path
    
    The uverbs_attr_bundle with the ucontext is sent down to the drivers ib_x
    destroy path as ib_udata. The next patch will use the ib_udata to free the
    drivers destroy path from the dependency in 'uobject->context' as we
    already did for the create path.
    
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4a617d78eae1..e45f59b0cc52 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1049,14 +1049,14 @@ void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
 				u32 flags, struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
-int mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags);
+int mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags, struct ib_udata *udata);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
 				  struct ib_srq_init_attr *init_attr,
 				  struct ib_udata *udata);
 int mlx5_ib_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
 		       enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
 int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
-int mlx5_ib_destroy_srq(struct ib_srq *srq);
+int mlx5_ib_destroy_srq(struct ib_srq *srq, struct ib_udata *udata);
 int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
 			  const struct ib_recv_wr **bad_wr);
 int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);
@@ -1068,7 +1068,7 @@ int mlx5_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 		      int attr_mask, struct ib_udata *udata);
 int mlx5_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr_mask,
 		     struct ib_qp_init_attr *qp_init_attr);
-int mlx5_ib_destroy_qp(struct ib_qp *qp);
+int mlx5_ib_destroy_qp(struct ib_qp *qp, struct ib_udata *udata);
 void mlx5_ib_drain_sq(struct ib_qp *qp);
 void mlx5_ib_drain_rq(struct ib_qp *qp);
 int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
@@ -1085,7 +1085,7 @@ struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_ucontext *context,
 				struct ib_udata *udata);
-int mlx5_ib_destroy_cq(struct ib_cq *cq);
+int mlx5_ib_destroy_cq(struct ib_cq *cq, struct ib_udata *udata);
 int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
 int mlx5_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
 int mlx5_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
@@ -1112,10 +1112,9 @@ void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);
-int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
-struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
-			       enum ib_mr_type mr_type,
-			       u32 max_num_sg);
+int mlx5_ib_dereg_mr(struct ib_mr *ibmr, struct ib_udata *udata);
+struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
+			       u32 max_num_sg, struct ib_udata *udata);
 int mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
 		      unsigned int *sg_offset);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
@@ -1126,7 +1125,7 @@ int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 					  struct ib_ucontext *context,
 					  struct ib_udata *udata);
-int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd);
+int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd, struct ib_udata *udata);
 int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
 int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);
 int mlx5_query_mad_ifc_smp_attr_node_info(struct ib_device *ibdev,
@@ -1170,7 +1169,7 @@ int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
 				struct ib_wq_init_attr *init_attr,
 				struct ib_udata *udata);
-int mlx5_ib_destroy_wq(struct ib_wq *wq);
+int mlx5_ib_destroy_wq(struct ib_wq *wq, struct ib_udata *udata);
 int mlx5_ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *wq_attr,
 		      u32 wq_attr_mask, struct ib_udata *udata);
 struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
@@ -1182,7 +1181,7 @@ struct ib_dm *mlx5_ib_alloc_dm(struct ib_device *ibdev,
 			       struct ib_ucontext *context,
 			       struct ib_dm_alloc_attr *attr,
 			       struct uverbs_attr_bundle *attrs);
-int mlx5_ib_dealloc_dm(struct ib_dm *ibdm);
+int mlx5_ib_dealloc_dm(struct ib_dm *ibdm, struct uverbs_attr_bundle *attrs);
 struct ib_mr *mlx5_ib_reg_dm_mr(struct ib_pd *pd, struct ib_dm *dm,
 				struct ib_dm_mr_attr *attr,
 				struct uverbs_attr_bundle *attrs);

commit a6bc3875f176f52c4a247c341e80d52dd4f5e356
Author: Moni Shoua <monis@mellanox.com>
Date:   Sun Feb 17 16:08:22 2019 +0200

    IB/mlx5: Protect against prefetch of invalid MR
    
    When deferring a prefetch request we need to protect against MR or PD
    being destroyed while the request is still enqueued.
    
    The first step is to validate that PD owns the lkey that describes the MR
    and that the MR that the lkey refers to is owned by that PD.
    
    The second step is to dequeue all requests when MR is destroyed.
    
    Since PD can't be destroyed while it owns MRs it is guaranteed that when a
    worker wakes up the request it refers to is still valid.
    
    Now, it is possible to refrain from taking a reference on the device since
    it is assured to be present as pd.
    
    While that, replace the dedicated ordered workqueue with the system
    unbound workqueue to reuse an existing resource and improve
    performance. This will also fix a bug of queueing to the wrong workqueue.
    
    Fixes: 813e90b1aeaa ("IB/mlx5: Add advise_mr() support")
    Reported-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f9817284c7a3..4a617d78eae1 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -589,6 +589,7 @@ struct mlx5_ib_mr {
 	atomic_t		num_leaf_free;
 	wait_queue_head_t       q_leaf_free;
 	struct mlx5_async_work  cb_work;
+	atomic_t		num_pending_prefetch;
 };
 
 static inline bool is_odp_mr(struct mlx5_ib_mr *mr)
@@ -929,7 +930,6 @@ struct mlx5_ib_dev {
 	 */
 	struct srcu_struct      mr_srcu;
 	u32			null_mkey;
-	struct workqueue_struct *advise_mr_wq;
 	struct mlx5_ib_flow_db	*flow_db;
 	/* protect resources needed as part of reset flow */
 	spinlock_t		reset_flow_resource_lock;

commit fbeb4075c67080869bed6ed973b2e54514e750a2
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 22 08:48:46 2019 +0200

    IB/mlx5: Let read user wqe also from SRQ buffer
    
    Reading a WQE from SRQ is almost identical to reading from regular RQ.
    The differences are the size of the queue, the size of a WQE and buffer
    location.
    
    Make necessary changes to mlx5_ib_read_user_wqe() to let it read a WQE
    from a SRQ or RQ by caller choice.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7fcc3f095371..f9817284c7a3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1075,9 +1075,12 @@ int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		      const struct ib_send_wr **bad_wr);
 int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 		      const struct ib_recv_wr **bad_wr);
-int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
-			  void *buffer, u32 length,
-			  struct mlx5_ib_qp_base *base);
+int mlx5_ib_read_user_wqe_sq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+			     int buflen, size_t *bc);
+int mlx5_ib_read_user_wqe_rq(struct mlx5_ib_qp *qp, int wqe_index, void *buffer,
+			     int buflen, size_t *bc);
+int mlx5_ib_read_user_wqe_srq(struct mlx5_ib_srq *srq, int wqe_index,
+			      void *buffer, int buflen, size_t *bc);
 struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_ucontext *context,

commit 55c293c38efa4408920e3ff8135a85a0dc2e3f56
Merge: b360ce3b2be9 eaebaf77e7cb
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jan 29 13:49:31 2019 -0700

    Merge branch 'devx-async' into k.o/for-next
    
    Yishai Hadas says:
    
    Enable DEVX asynchronous query commands
    
    This series enables querying a DEVX object in an asynchronous mode.
    
    The userspace application won't block when calling the firmware and it will be
    able to get the response back once that it will be ready.
    
    To enable the above functionality:
    
    - DEVX asynchronous command completion FD object was introduced.
    - The applicable file operations were implemented to enable using it by
      the user application.
    - Query asynchronous method was added to the DEVX object, it will call the
      firmware asynchronously and manages the response on the given input FD.
    - Hot unplug support was added for the FD to work properly upon
      unbind/disassociate.
    - mlx5 core fence for asynchronous commands was implemented and used to
      prevent racing upon unbind/disassociate.
    
    This branch is based on mlx5-next & v5.0-rc2 due to dependencies, from
    git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    * branch 'devx-async':
      IB/mlx5: Implement DEVX hot unplug for async command FD
      IB/mlx5: Implement the file ops of DEVX async command FD
      IB/mlx5: Introduce async DEVX obj query API
      IB/mlx5: Introduce MLX5_IB_OBJECT_DEVX_ASYNC_CMD_FD
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 73eb8f03f0ecfcb45ed1995990229f5a81e8e38f
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Tue Jan 22 16:17:57 2019 +0100

    infiniband: mlx5: no need to check return value of debugfs_create functions
    
    When calling debugfs functions, there is no need to ever check the
    return value.  The function can work or not, but the code logic should
    never do something different based on this.
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 819207190343..ea124af56dce 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -636,7 +636,6 @@ struct mlx5_cache_ent {
 	spinlock_t		lock;
 
 
-	struct dentry	       *dir;
 	char                    name[4];
 	u32                     order;
 	u32			xlt;
@@ -648,11 +647,6 @@ struct mlx5_cache_ent {
 	u32                     miss;
 	u32			limit;
 
-	struct dentry          *fsize;
-	struct dentry          *fcur;
-	struct dentry          *fmiss;
-	struct dentry          *flimit;
-
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	work;
 	struct delayed_work	dwork;
@@ -1270,7 +1264,7 @@ __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev,
 			       const struct ib_gid_attr *attr);
 
 void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
-int mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
+void mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
 
 /* GSI QP helper functions */
 struct ib_qp *mlx5_ib_gsi_create_qp(struct ib_pd *pd,

commit e355477ed9e4f401e3931043df97325d38552d54
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jan 18 16:33:10 2019 -0800

    net/mlx5: Make mlx5_cmd_exec_cb() a safe API
    
    APIs that have deferred callbacks should have some kind of cleanup
    function that callers can use to fence the callbacks. Otherwise things
    like module unloading can lead to dangling function pointers, or worse.
    
    The IB MR code is the only place that calls this function and had a
    really poor attempt at creating this fence. Provide a good version in
    the core code as future patches will add more places that need this
    fence.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index efe383c0ac86..eedba0d2ec4b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -587,6 +587,7 @@ struct mlx5_ib_mr {
 	struct mlx5_ib_mr      *parent;
 	atomic_t		num_leaf_free;
 	wait_queue_head_t       q_leaf_free;
+	struct mlx5_async_work  cb_work;
 };
 
 struct mlx5_ib_mw {
@@ -944,6 +945,7 @@ struct mlx5_ib_dev {
 	struct mlx5_memic	memic;
 	u16			devx_whitelist_uid;
 	struct mlx5_srq_table   srq_table;
+	struct mlx5_async_ctx   async_ctx;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 534fd7aac56a7994d16032f32123def9923e339f
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jan 13 16:01:17 2019 +0200

    IB/mlx5: Manage indirection mkey upon DEVX flow for ODP
    
    Manage indirection mkey upon DEVX flow to support ODP.
    
    To support a page fault event on the indirection mkey it needs to be part
    of the device mkey radix tree.
    
    Both the creation and the deletion flows for a DEVX object which is
    indirection mkey were adapted to handle that.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b0a37ca2a714..819207190343 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -602,6 +602,12 @@ struct mlx5_ib_mw {
 	int			ndescs;
 };
 
+struct mlx5_ib_devx_mr {
+	struct mlx5_core_mkey	mmkey;
+	int			ndescs;
+	struct rcu_head		rcu;
+};
+
 struct mlx5_ib_umr_context {
 	struct ib_cqe		cqe;
 	enum ib_wc_status	status;

commit 73f5a82bb3c9fce550da4a74a32b8cb064b50663
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jan 13 15:57:04 2019 +0200

    RDMA/mad: Reduce MAD scope to mlx5_ib only
    
    Management Datagram Interface (MAD) is applicable
    only when physical port is Infiniband. It makes MAD
    command logic to be completely unrelated to eth/core
    parts of mlx5.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Acked-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b06d3b1efea8..efe383c0ac86 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1038,9 +1038,6 @@ void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db)
 void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
-int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
-		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
-		 const void *in_mad, void *response_mad);
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
 				u32 flags, struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);

commit b0ea0fa5435f9df7213a9af098558f7dd584d8e8
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Jan 9 11:15:16 2019 +0200

    IB/{core,hw}: Have ib_umem_get extract the ib_ucontext from ib_udata
    
    ib_umem_get() can only be called in a method callback, which always has a
    udata parameter. This allows ib_umem_get() to derive the ucontext pointer
    directly from the udata without requiring the drivers to find it in some
    way or another.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Shamir Rabinovitch <shamir.rabinovitch@oracle.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 81efa5def8ad..b0a37ca2a714 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1037,7 +1037,8 @@ to_mflow_act(struct ib_flow_action *ibact)
 	return container_of(ibact, struct mlx5_ib_flow_action, ib_action);
 }
 
-int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context, unsigned long virt,
+int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context,
+			struct ib_udata *udata, unsigned long virt,
 			struct mlx5_db *db);
 void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db);
 void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
@@ -1103,6 +1104,7 @@ int mlx5_ib_dealloc_mw(struct ib_mw *mw);
 int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 		       int page_shift, int flags);
 struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
+					     struct ib_udata *udata,
 					     int access_flags);
 void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,

commit 8b4d5bc5cf3f813dc4df5d69c2fcde16c40d8abd
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Jan 8 16:07:25 2019 +0200

    RDMA/mlx5: Introduce and reuse helper to identify ODP MR
    
    Consolidate various checks if MR is ODP backed to one simple helper and
    update call sites to use it.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 93c288a4de2b..81efa5def8ad 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -36,6 +36,7 @@
 #include <linux/kernel.h>
 #include <linux/sched.h>
 #include <rdma/ib_verbs.h>
+#include <rdma/ib_umem.h>
 #include <rdma/ib_smi.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cq.h>
@@ -589,6 +590,12 @@ struct mlx5_ib_mr {
 	wait_queue_head_t       q_leaf_free;
 };
 
+static inline bool is_odp_mr(struct mlx5_ib_mr *mr)
+{
+	return IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) && mr->umem &&
+	       mr->umem->is_odp;
+}
+
 struct mlx5_ib_mw {
 	struct ib_mw		ibmw;
 	struct mlx5_core_mkey	mmkey;
@@ -1213,6 +1220,9 @@ mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
 {
 	return -EOPNOTSUPP;
 }
+static inline void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp,
+					    unsigned long start,
+					    unsigned long end){};
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
 /* Needed for rep profile */

commit 96f87ee1811306d0c8cf94b8c37b0e4f725b01d1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Jan 8 16:07:23 2019 +0200

    RDMA: Clean structures from CONFIG_INFINIBAND_ON_DEMAND_PAGING
    
    CONFIG_INFINIBAND_ON_DEMAND_PAGING is used in general structures to
    micro-optimize the memory footprint. Remove it, so it will allow us to
    simplify various ODP device flows.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b06d3b1efea8..93c288a4de2b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -911,7 +911,6 @@ struct mlx5_ib_dev {
 	/* Prevents soft lock on massive reg MRs */
 	struct mutex			slow_path_mutex;
 	int				fill_delay;
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_odp_caps	odp_caps;
 	u64			odp_max_size;
 	struct mlx5_ib_pf_eq	odp_pf_eq;
@@ -923,7 +922,6 @@ struct mlx5_ib_dev {
 	struct srcu_struct      mr_srcu;
 	u32			null_mkey;
 	struct workqueue_struct *advise_mr_wq;
-#endif
 	struct mlx5_ib_flow_db	*flow_db;
 	/* protect resources needed as part of reset flow */
 	spinlock_t		reset_flow_resource_lock;

commit 5d24ae67a961c51beb255a28c9c417d9710247c2
Merge: 938edb8a31b9 f617e5ffe04f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 28 14:57:10 2018 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "This has been a fairly typical cycle, with the usual sorts of driver
      updates. Several series continue to come through which improve and
      modernize various parts of the core code, and we finally are starting
      to get the uAPI command interface cleaned up.
    
       - Various driver fixes for bnxt_re, cxgb3/4, hfi1, hns, i40iw, mlx4,
         mlx5, qib, rxe, usnic
    
       - Rework the entire syscall flow for uverbs to be able to run over
         ioctl(). Finally getting past the historic bad choice to use
         write() for command execution
    
       - More functional coverage with the mlx5 'devx' user API
    
       - Start of the HFI1 series for 'TID RDMA'
    
       - SRQ support in the hns driver
    
       - Support for new IBTA defined 2x lane widths
    
       - A big series to consolidate all the driver function pointers into a
         big struct and have drivers provide a 'static const' version of the
         struct instead of open coding initialization
    
       - New 'advise_mr' uAPI to control device caching/loading of page
         tables
    
       - Support for inline data in SRPT
    
       - Modernize how umad uses the driver core and creates cdev's and
         sysfs files
    
       - First steps toward removing 'uobject' from the view of the drivers"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (193 commits)
      RDMA/srpt: Use kmem_cache_free() instead of kfree()
      RDMA/mlx5: Signedness bug in UVERBS_HANDLER()
      IB/uverbs: Signedness bug in UVERBS_HANDLER()
      IB/mlx5: Allocate the per-port Q counter shared when DEVX is supported
      IB/umad: Start using dev_groups of class
      IB/umad: Use class_groups and let core create class file
      IB/umad: Refactor code to use cdev_device_add()
      IB/umad: Avoid destroying device while it is accessed
      IB/umad: Simplify and avoid dynamic allocation of class
      IB/mlx5: Fix wrong error unwind
      IB/mlx4: Remove set but not used variable 'pd'
      RDMA/iwcm: Don't copy past the end of dev_name() string
      IB/mlx5: Fix long EEH recover time with NVMe offloads
      IB/mlx5: Simplify netdev unbinding
      IB/core: Move query port to ioctl
      RDMA/nldev: Expose port_cap_flags2
      IB/core: uverbs copy to struct or zero helper
      IB/rxe: Reuse code which sets port state
      IB/rxe: Make counters thread safe
      IB/mlx5: Use the correct commands for UMEM and UCTX allocation
      ...

commit ed50edfb72352c4bee489b5b27418a30177cf38f
Merge: bd1c24ccf9eb 71bef2fd583b
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Dec 20 13:24:50 2018 -0700

    Merge branch 'mlx5-next' into rdma.git
    
    From git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    mlx5 updates taken for dependencies on following patches.
    
    * branche 'mlx5-next': (23 commits)
      IB/mlx5: Introduce uid as part of alloc/dealloc transport domain
      net/mlx5: Add shared Q counter bits
      net/mlx5: Continue driver initialization despite debugfs failure
      net/mlx5: Fold the modify lag code into function
      net/mlx5: Add lag affinity info to log
      net/mlx5: Split the activate lag function into two routines
      net/mlx5: E-Switch, Introduce flow counter affinity
      IB/mlx5: Unify e-switch representors load approach between uplink and VFs
      net/mlx5: Use lowercase 'X' for hex values
      net/mlx5: Remove duplicated include from eswitch.c
      net/mlx5: Remove the get protocol device interface entry
      net/mlx5: Support extended destination format in flow steering command
      net/mlx5: E-Switch, Change vhca id valid bool field to bit flag
      net/mlx5: Introduce extended destination fields
      net/mlx5: Revise gre and nvgre key formats
      net/mlx5: Add monitor commands layout and event data
      net/mlx5: Add support for plugged-disabled cable status in PME
      net/mlx5: Add support for PCIe power slot exceeded error in PME
      net/mlx5: Rework handling of port module events
      net/mlx5: Move flow counters data structures from flow steering header
      ...

commit 2553ba217eea37dc6291635ecddb883fb5c36a8b
Author: Gal Pressman <galpress@amazon.com>
Date:   Wed Dec 12 11:09:06 2018 +0200

    RDMA: Mark if destroy address handle is in a sleepable context
    
    Introduce a 'flags' field to destroy address handle callback and add a
    flag that marks whether the callback is executed in an atomic context or
    not.
    
    This will allow drivers to wait for completion instead of polling for it
    when it is allowed.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ebf700298acb..96e8fa1109f5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1044,7 +1044,7 @@ int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
 				u32 flags, struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
-int mlx5_ib_destroy_ah(struct ib_ah *ah);
+int mlx5_ib_destroy_ah(struct ib_ah *ah, u32 flags);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
 				  struct ib_srq_init_attr *init_attr,
 				  struct ib_udata *udata);

commit b090c4e3a07c33ffdf95fb7601551b38fc2a4bbb
Author: Gal Pressman <galpress@amazon.com>
Date:   Wed Dec 12 11:09:05 2018 +0200

    RDMA: Mark if create address handle is in a sleepable context
    
    Introduce a 'flags' field to create address handle callback and add a flag
    that marks whether the callback is executed in an atomic context or not.
    
    This will allow drivers to wait for completion instead of polling for it
    when it is allowed.
    
    Signed-off-by: Gal Pressman <galpress@amazon.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 9b4e2554889a..ebf700298acb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1042,7 +1042,7 @@ int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
 		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 		 const void *in_mad, void *response_mad);
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
-				struct ib_udata *udata);
+				u32 flags, struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
 int mlx5_ib_destroy_ah(struct ib_ah *ah);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,

commit c9e585ebdc28a334b8100e560c9aaad6d4525a6e
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Dec 19 13:43:17 2018 -0500

    IB/mlx5: Fix compile issue when ODP disabled
    
    When CONFIG_INFINIBAND_ON_DEMAND_PAGING is not enabled, we were getting
    build failures for defined but not used code.  Fix that.
    
    Fixes: 813e90b1aeaa ("IB/mlx5: Add advise_mr() support")
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f245b5d8a3bc..9b4e2554889a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1208,10 +1208,10 @@ static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 					 size_t nentries, struct mlx5_ib_mr *mr,
 					 int flags) {}
 
-static int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
-				      enum ib_uverbs_advise_mr_advice advice,
-				      u32 flags, struct ib_sge *sg_list,
-				      u32 num_sge)
+static inline int
+mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
+			   enum ib_uverbs_advise_mr_advice advice, u32 flags,
+			   struct ib_sge *sg_list, u32 num_sge)
 {
 	return -EOPNOTSUPP;
 }

commit 813e90b1aeaa550641332625174d57edb15bc8bd
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Dec 11 13:37:53 2018 +0200

    IB/mlx5: Add advise_mr() support
    
    The verb advise_mr() is used to give advice to the kernel about an address
    range that belongs to a MR.  Implement the verb and register it on the
    device. The current implementation supports the only known advice to date,
    prefetch.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1285ac11bb70..f245b5d8a3bc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -923,6 +923,7 @@ struct mlx5_ib_dev {
 	 */
 	struct srcu_struct      mr_srcu;
 	u32			null_mkey;
+	struct workqueue_struct *advise_mr_wq;
 #endif
 	struct mlx5_ib_flow_db	*flow_db;
 	/* protect resources needed as part of reset flow */
@@ -1085,6 +1086,12 @@ struct ib_mr *mlx5_ib_get_dma_mr(struct ib_pd *pd, int acc);
 struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 virt_addr, int access_flags,
 				  struct ib_udata *udata);
+int mlx5_ib_advise_mr(struct ib_pd *pd,
+		      enum ib_uverbs_advise_mr_advice advice,
+		      u32 flags,
+		      struct ib_sge *sg_list,
+		      u32 num_sge,
+		      struct uverbs_attr_bundle *attrs);
 struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 			       struct ib_udata *udata);
 int mlx5_ib_dealloc_mw(struct ib_mw *mw);
@@ -1182,6 +1189,10 @@ void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
 void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
+
+int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
+			       enum ib_uverbs_advise_mr_advice advice,
+			       u32 flags, struct ib_sge *sg_list, u32 num_sge);
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 {
@@ -1197,6 +1208,13 @@ static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 					 size_t nentries, struct mlx5_ib_mr *mr,
 					 int flags) {}
 
+static int mlx5_ib_advise_mr_prefetch(struct ib_pd *pd,
+				      enum ib_uverbs_advise_mr_advice advice,
+				      u32 flags, struct ib_sge *sg_list,
+				      u32 num_sge)
+{
+	return -EOPNOTSUPP;
+}
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
 /* Needed for rep profile */

commit 8e3b688301863fba914883e3531b406c68cb7501
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Dec 12 19:45:53 2018 +0200

    RDMA/mlx5: Delete unreachable handle_atomic code by simplifying SW completion
    
    Handle atomic was left as unimplemented from 2013, remove the code till
    this part will be developed.
    
    Remove the dead code by simplifying SW completion logic which is supposed
    to be the same for send and receive paths.
    
    Fixes: e126ba97dba9 ("mlx5: Add driver for Mellanox Connect-IB adapters")
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Tested-by: Stephen Rothwell <sfr@canb.auug.org.au> # compile tested
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8fd3de05cdba..1285ac11bb70 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -275,7 +275,6 @@ struct mlx5_ib_wq {
 	unsigned		head;
 	unsigned		tail;
 	u16			cur_post;
-	u16			last_poll;
 	void			*cur_edge;
 };
 
@@ -1070,7 +1069,6 @@ int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		      const struct ib_send_wr **bad_wr);
 int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 		      const struct ib_recv_wr **bad_wr);
-void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 			  void *buffer, u32 length,
 			  struct mlx5_ib_qp_base *base);

commit 7c34ec19e10c0d13ca2f3435fb85d2dddccad917
Author: Aviv Heller <avivh@mellanox.com>
Date:   Thu Aug 23 13:47:53 2018 +0300

    net/mlx5: Make RoCE and SR-IOV LAG modes explicit
    
    With the introduction of SR-IOV LAG, checking whether LAG is active
    is no longer good enough, since RoCE and SR-IOV LAG each entails
    different behavior by both the core and infiniband drivers.
    
    This patch introduces facilities to discern LAG type, in addition to
    mlx5_lag_is_active(). These are implemented in such a way as to allow
    more complex mode combinations in the future.
    
    Signed-off-by: Aviv Heller <avivh@mellanox.com>
    Reviewed-by: Roi Dayan <roid@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c89b3b44b22e..e507b6eb7c09 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -936,6 +936,7 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
 	struct mlx5_eswitch_rep		*rep;
+	int				lag_active;
 
 	struct mlx5_ib_lb_state		lb;
 	u8			umr_fence;

commit 06cc74af05c33091a9877b54f1821966b446003c
Author: Mark Bloch <markb@mellanox.com>
Date:   Wed Dec 12 19:11:37 2018 -0800

    IB/mlx5: Unify e-switch representors load approach between uplink and VFs
    
    When in switchdev mode and the add function is called by the core
    level driver, make sure we only register the callbacks, but don't
    create the mlx5 IB device or initialize anything. With this change
    all the IB devices in switchdev mode are created only once the load
    callback is invoked by the e-switch core sub-module. This follows
    the design paradigm under which the all the Eth representors must
    be loaded before any of IB reprs is loaded.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Acked-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 861b68f2e330..c89b3b44b22e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -790,7 +790,6 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_POST_IB_REG_UMR,
 	MLX5_IB_STAGE_DELAY_DROP,
 	MLX5_IB_STAGE_CLASS_ATTR,
-	MLX5_IB_STAGE_REP_REG,
 	MLX5_IB_STAGE_MAX,
 };
 

commit f94e02ddfd88a511ed64fb68905e35d727635fea
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Dec 10 15:27:59 2018 +0800

    IB/mlx5: Remove duplicated include from mlx5_ib.h
    
    Remove duplicated include.
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7145f512f948..8fd3de05cdba 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -41,7 +41,6 @@
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/fs.h>
 #include <linux/mlx5/qp.h>
-#include <linux/mlx5/fs.h>
 #include <linux/types.h>
 #include <linux/mlx5/transobj.h>
 #include <rdma/ib_user_verbs.h>

commit fe15bcc6e23f4d5273dfbc056bae8710723f03d0
Merge: e7521d82b335 7e11b911b520
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Dec 7 13:25:12 2018 -0700

    Merge branch 'mlx5-packet-credit-fc' into rdma.git
    
    Danit Goldberg says:
    
    Packet based credit mode
    
    Packet based credit mode is an alternative end-to-end credit mode for QPs
    set during their creation. Credits are transported from the responder to
    the requester to optimize the use of its receive resources.  In
    packet-based credit mode, credits are issued on a per packet basis.
    
    The advantage of this feature comes while sending large RDMA messages
    through switches that are short in memory.
    
    The first commit exposes QP creation flag and the HCA capability. The
    second commit adds support for a new DV QP creation flag. The last commit
    report packet based credit mode capability via the MLX5DV device
    capabilities.
    
    * branch 'mlx5-packet-credit-fc':
      IB/mlx5: Report packet based credit mode device capability
      IB/mlx5: Add packet based credit mode support
      net/mlx5: Expose packet based credit mode
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit 569c665150156e44ecbd92af47a6d3fd4e2e4690
Author: Danit Goldberg <danitg@mellanox.com>
Date:   Fri Nov 30 13:22:05 2018 +0200

    IB/mlx5: Add packet based credit mode support
    
    The device can support two credit modes, message based (default) and
    packet based. In order to enable packet based mode, the QP should be
    created with special flag that indicates this.
    
    This patch adds support for the new DV QP creation flag that can be used
    for RC QPs in order to change the credit mode.
    
    Signed-off-by: Danit Goldberg <danitg@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 861b68f2e330..3e034bc85bde 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -461,6 +461,7 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_UNDERLAY			= 1 << 10,
 	MLX5_IB_QP_PCI_WRITE_END_PADDING	= 1 << 11,
 	MLX5_IB_QP_TUNNEL_OFFLOAD		= 1 << 12,
+	MLX5_IB_QP_PACKET_BASED_CREDIT		= 1 << 13,
 };
 
 struct mlx5_umr_wr {

commit 5aa3771ded54894ce34f4ec6bc2bb403e6771eb2
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Nov 26 08:28:38 2018 +0200

    IB/mlx5: Allow XRC usage via verbs in DEVX context
    
    Allows XRC usage from the verbs flow in a DEVX context.
    As XRCD is some shared kernel resource between processes it should be
    created with UID=0 to point on that.
    
    As a result once XRC QP/SRQ are created they must be used as well with
    UID=0 so that firmware will allow the XRCD usage.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4d33965369cc..24cb2f793210 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -543,7 +543,6 @@ struct mlx5_ib_srq {
 struct mlx5_ib_xrcd {
 	struct ib_xrcd		ibxrcd;
 	u32			xrcdn;
-	u16			uid;
 };
 
 enum mlx5_ib_mtt_access_flags {

commit fb98153bbf28b627fe52f41e658ae39fa67d2684
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Nov 26 08:28:36 2018 +0200

    IB/mlx5: Enforce DEVX privilege by firmware
    
    Enforce DEVX privilege by firmware, this enables future device
    functionality without the need to make driver changes unless a new
    privilege type will be introduced.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 59e1664a107f..4d33965369cc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1268,7 +1268,7 @@ void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 				  u8 port_num);
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
-int mlx5_ib_devx_create(struct mlx5_ib_dev *dev);
+int mlx5_ib_devx_create(struct mlx5_ib_dev *dev, bool is_user);
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
 extern const struct uapi_definition mlx5_ib_devx_defs[];
@@ -1283,7 +1283,8 @@ int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int
-mlx5_ib_devx_create(struct mlx5_ib_dev *dev) { return -EOPNOTSUPP; };
+mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+			   bool is_user) { return -EOPNOTSUPP; }
 static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
 static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
 					     int *dest_type)

commit f33cb7e760dee94dd2b2d7a7f6225578a66a17d5
Merge: ffd321e4b7e4 9d43faac02e3
Author: Doug Ledford <dledford@redhat.com>
Date:   Tue Dec 4 13:36:57 2018 -0500

    Merge 'mlx5-next' into mlx5-devx
    
    The enhanced devx support series needs commit:
    9d43faac02e3 ("net/mlx5: Update mlx5_ifc with DEVX UCTX capabilities bits")
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit f3da6577da67a3cd44610ca54e308c6838c92157
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Nov 28 20:53:41 2018 +0200

    RDMA/mlx5: Initialize SRQ tables on mlx5_ib
    
    Transfer initialization and cleanup from mlx5_priv struct of
    mlx5_core_dev to be part of mlx5_ib_dev. This completes removal
    of SRQ from mlx5_core.
    
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 332d5c4d8ab3..861b68f2e330 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -41,7 +41,6 @@
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/fs.h>
 #include <linux/mlx5/qp.h>
-#include <linux/mlx5/srq.h>
 #include <linux/mlx5/fs.h>
 #include <linux/types.h>
 #include <linux/mlx5/transobj.h>
@@ -50,6 +49,8 @@
 #include <rdma/uverbs_ioctl.h>
 #include <rdma/mlx5_user_ioctl_cmds.h>
 
+#include "srq.h"
+
 #define mlx5_ib_dbg(_dev, format, arg...)                                      \
 	dev_dbg(&(_dev)->ib_dev.dev, "%s:%d:(pid %d): " format, __func__,      \
 		__LINE__, current->pid, ##arg)
@@ -774,6 +775,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_CAPS,
 	MLX5_IB_STAGE_NON_DEFAULT_CB,
 	MLX5_IB_STAGE_ROCE,
+	MLX5_IB_STAGE_SRQ,
 	MLX5_IB_STAGE_DEVICE_RESOURCES,
 	MLX5_IB_STAGE_DEVICE_NOTIFIER,
 	MLX5_IB_STAGE_ODP,
@@ -942,6 +944,7 @@ struct mlx5_ib_dev {
 	u64			sys_image_guid;
 	struct mlx5_memic	memic;
 	u16			devx_whitelist_uid;
+	struct mlx5_srq_table   srq_table;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 81773ce5f07fd4c3acd4bb7e918ce03a4e652a5f
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Wed Nov 28 20:53:39 2018 +0200

    RDMA/mlx5: Use stages for callback to setup and release DEVX
    
    Reuse existing infrastructure to initialize and release DEVX uid.
    The DevX interface is intended for user space access, so it is supposed
    to be initialized before ib_register_device(). Also it isn't supported
    in switchdev mode and don't need to initialize it in that mode.
    
    Fixes: 76dc5a8406bf ("IB/mlx5: Manage device uid for DEVX white list commands")
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7326fb8710e0..332d5c4d8ab3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -783,6 +783,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_BFREG,
 	MLX5_IB_STAGE_PRE_IB_REG_UMR,
 	MLX5_IB_STAGE_SPECS,
+	MLX5_IB_STAGE_WHITELIST_UID,
 	MLX5_IB_STAGE_IB_REG,
 	MLX5_IB_STAGE_POST_IB_REG_UMR,
 	MLX5_IB_STAGE_DELAY_DROP,

commit df097a278c7592873d88571a8c78f987f6ae511b
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Nov 26 14:39:00 2018 -0800

    IB/mlx5: Use the new mlx5 core notifier API
    
    Remove the deprecated mlx5_interface->event mlx5_ib callback and use new
    mlx5 notifier API to subscribe for mlx5 events.
    
    For native mlx5_ib devices profiles pf_profile/nic_rep_profile register
    the notifier callback mlx5_ib_handle_event which treats the notifier
    context as mlx5_ib_dev.
    
    For vport repesentors, don't register any notifier, same as before, they
    didn't receive any mlx5 events.
    
    For slave port (mlx5_ib_multiport_info) register a different notifier
    callback mlx5_ib_event_slave_port, which knows that the event is coming
    for mlx5_ib_multiport_info and prepares the event job accordingly.
    Before this on the event handler work we had to ask mlx5_core if this is
    a slave port mlx5_core_is_mp_slave(work->dev), now it is not needed
    anymore.
    mlx5_ib_multiport_info notifier registration is done on
    mlx5_ib_bind_slave_port and de-registration is done on
    mlx5_ib_unbind_slave_port.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 27999fd32356..7326fb8710e0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -775,6 +775,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_NON_DEFAULT_CB,
 	MLX5_IB_STAGE_ROCE,
 	MLX5_IB_STAGE_DEVICE_RESOURCES,
+	MLX5_IB_STAGE_DEVICE_NOTIFIER,
 	MLX5_IB_STAGE_ODP,
 	MLX5_IB_STAGE_COUNTERS,
 	MLX5_IB_STAGE_CONG_DEBUGFS,
@@ -806,6 +807,7 @@ struct mlx5_ib_multiport_info {
 	struct list_head list;
 	struct mlx5_ib_dev *ibdev;
 	struct mlx5_core_dev *mdev;
+	struct notifier_block mdev_events;
 	struct completion unref_comp;
 	u64 sys_image_guid;
 	u32 mdev_refcnt;
@@ -893,6 +895,7 @@ struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	const struct uverbs_object_tree_def *driver_trees[7];
 	struct mlx5_core_dev		*mdev;
+	struct notifier_block		mdev_events;
 	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;
 	/* serialize update of capability mask

commit 34f4c9554d8b2a7d2deb9503e9373b598ee3279f
Author: Guy Levi <guyle@mellanox.com>
Date:   Mon Nov 26 08:15:50 2018 +0200

    IB/mlx5: Use fragmented QP's buffer for in-kernel users
    
    The current implementation of create QP requires contiguous memory, such a
    requirement is problematic once the memory is fragmented or the system is
    low in memory, it causes failures in dma_zalloc_coherent().
    
    This patch takes advantage of the new mlx5_core API which allocates a
    fragmented buffer. This makes the QP creation much more resilient to
    memory fragmentation. Data-path code was adapted to the fact that WQEs can
    cross buffers.
    
    We also use the opportunity to fix some cosmetic legacy coding convention
    errors which were in the feature scope.
    
    Signed-off-by: Guy Levi <guyle@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index caed1fe76102..93772434b9e3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -257,6 +257,7 @@ enum mlx5_ib_rq_flags {
 };
 
 struct mlx5_ib_wq {
+	struct mlx5_frag_buf_ctrl fbc;
 	u64		       *wrid;
 	u32		       *wr_data;
 	struct wr_list	       *w_list;
@@ -275,7 +276,7 @@ struct mlx5_ib_wq {
 	unsigned		tail;
 	u16			cur_post;
 	u16			last_poll;
-	void		       *qend;
+	void			*cur_edge;
 };
 
 enum mlx5_ib_wq_flags {

commit 20e5a59b2e64a01f8e0957727887564a4d004970
Author: Guy Levi <guyle@mellanox.com>
Date:   Mon Nov 26 08:15:39 2018 +0200

    IB/mlx5: Use fragmented SRQ's buffer for in-kernel users
    
    The current implementation of create SRQ requires contiguous memory, such
    a requirement is problematic once the memory is fragmented or the system
    is low in memory, it causes failures in dma_zalloc_coherent().
    
    This patch takes the advantage of the new mlx5_core API which allocates a
    fragmented buffer, and makes the SRQ creation much more resilient to
    memory fragmentation. Data-path code was adapted to the fact that WQEs can
    cross buffers.
    
    Signed-off-by: Guy Levi <guyle@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a2b35a1a5031..caed1fe76102 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -523,6 +523,7 @@ struct mlx5_ib_srq {
 	struct mlx5_core_srq	msrq;
 	struct mlx5_frag_buf	buf;
 	struct mlx5_db		db;
+	struct mlx5_frag_buf_ctrl fbc;
 	u64		       *wrid;
 	/* protect SRQ hanlding
 	 */

commit bfc5d839184f53cc16d551873f9254f2d4d493be
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Nov 20 20:31:08 2018 +0200

    RDMA/mlx5: Attach a DEVX counter via raw flow creation
    
    Allow a user to attach a DEVX counter via mlx5 raw flow creation. In order
    to attach a counter we introduce a new attribute:
    
    MLX5_IB_ATTR_CREATE_FLOW_ARR_COUNTERS_DEVX
    
    A counter can be attached to multiple flow steering rules.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 9b434246d4e3..a2b35a1a5031 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1266,9 +1266,11 @@ extern const struct uapi_definition mlx5_ib_devx_defs[];
 extern const struct uapi_definition mlx5_ib_flow_defs[];
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
-	struct mlx5_flow_act *flow_act, void *cmd_in, int inlen,
-	int dest_id, int dest_type);
+	struct mlx5_flow_act *flow_act, u32 counter_id,
+	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
+bool mlx5_ib_devx_is_flow_counter(void *obj, u32 *counter_id);
+int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int

commit 36e235c8829935a59d4652c878cffb08229205c2
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Nov 12 22:59:53 2018 +0200

    RDMA/mlx5: Use the uapi disablement APIs instead of code
    
    Rely on UAPI_DEF_IS_OBJ_SUPPORTED instead of manipulating the contents of
    the driver's definition list.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3663664ecee8..9b434246d4e3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -781,7 +781,6 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_UAR,
 	MLX5_IB_STAGE_BFREG,
 	MLX5_IB_STAGE_PRE_IB_REG_UMR,
-	MLX5_IB_STAGE_SPECS,
 	MLX5_IB_STAGE_IB_REG,
 	MLX5_IB_STAGE_POST_IB_REG_UMR,
 	MLX5_IB_STAGE_DELAY_DROP,
@@ -891,7 +890,6 @@ struct mlx5_ib_pf_eq {
 
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
-	struct uapi_definition		driver_defs[7];
 	struct mlx5_core_dev		*mdev;
 	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;

commit 0cbf432db405289216747a8d31d74bab2452337c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Mon Nov 12 22:59:50 2018 +0200

    RDMA/uverbs: Use a linear list to describe the compiled-in uapi
    
    The 'tree' data structure is very hard to build at compile time, and this
    makes it very limited. The new radix tree based compiler can handle a more
    complex input language that does not require the compiler to perfectly
    group everything into a neat tree structure.
    
    Instead use a simple list to describe to input, where the list elements
    can be of various different 'opcodes' instructing the radix compiler what
    to do. Start out with opcodes chaining to other definition lists and
    chaining to the existing 'tree' definition.
    
    Replace the very top level of the 'object tree' with this list type and
    get rid of struct uverbs_object_tree_def and DECLARE_UVERBS_OBJECT_TREE.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 27999fd32356..3663664ecee8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -891,7 +891,7 @@ struct mlx5_ib_pf_eq {
 
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
-	const struct uverbs_object_tree_def *driver_trees[7];
+	struct uapi_definition		driver_defs[7];
 	struct mlx5_core_dev		*mdev;
 	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;
@@ -1264,29 +1264,23 @@ void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 int mlx5_ib_devx_create(struct mlx5_ib_dev *dev);
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
+extern const struct uapi_definition mlx5_ib_devx_defs[];
+extern const struct uapi_definition mlx5_ib_flow_defs[];
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
 	struct mlx5_flow_act *flow_act, void *cmd_in, int inlen,
 	int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
-int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev) { return -EOPNOTSUPP; };
 static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
-static inline const struct uverbs_object_tree_def *
-mlx5_ib_get_devx_tree(void) { return NULL; }
 static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
 					     int *dest_type)
 {
 	return false;
 }
-static inline int
-mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root)
-{
-	return 0;
-}
 static inline void
 mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction)
 {

commit d5d284b829a6eb7127df24d1bd3896a698981e62
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Nov 19 10:52:41 2018 -0800

    {net,IB}/mlx5: Move Page fault EQ and ODP logic to RDMA
    
    Use the new generic EQ API to move all ODP RDMA data structures and logic
    form mlx5 core driver into mlx5_ib driver.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b651a7a6fde9..27999fd32356 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -880,6 +880,15 @@ struct mlx5_ib_lb_state {
 	bool			enabled;
 };
 
+struct mlx5_ib_pf_eq {
+	struct mlx5_ib_dev *dev;
+	struct mlx5_eq *core;
+	struct work_struct work;
+	spinlock_t lock; /* Pagefaults spinlock */
+	struct workqueue_struct *wq;
+	mempool_t *pool;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	const struct uverbs_object_tree_def *driver_trees[7];
@@ -902,6 +911,8 @@ struct mlx5_ib_dev {
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_odp_caps	odp_caps;
 	u64			odp_max_size;
+	struct mlx5_ib_pf_eq	odp_pf_eq;
+
 	/*
 	 * Sleepable RCU that prevents destruction of MRs while they are still
 	 * being used by a page fault handler.
@@ -1158,9 +1169,8 @@ struct ib_mr *mlx5_ib_reg_dm_mr(struct ib_pd *pd, struct ib_dm *dm,
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);
-void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
-		    struct mlx5_pagefault *pfault);
 int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
+void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
 void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
@@ -1175,6 +1185,7 @@ static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 }
 
 static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
+static inline void mlx5_ib_odp_cleanup_one(struct mlx5_ib_dev *ibdev) {}
 static inline int mlx5_ib_odp_init(void) { return 0; }
 static inline void mlx5_ib_odp_cleanup(void)				    {}
 static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}

commit da19a102ce87bf3e0a7fe277a659d1fc35330d6d
Merge: e5f6d9afa341 a60109dc9a95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 07:38:19 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "This has been a smaller cycle with many of the commits being smallish
      code fixes and improvements across the drivers.
    
       - Driver updates for bnxt_re, cxgb4, hfi1, hns, mlx5, nes, qedr, and
         rxe
    
       - Memory window support in hns
    
       - mlx5 user API 'flow mutate/steering' allows accessing the full
         packet mangling and matching machinery from user space
    
       - Support inter-working with verbs API calls in the 'devx' mlx5 user
         API, and provide options to use devx with less privilege
    
       - Modernize the use of syfs and the device interface to use attribute
         groups and cdev properly for uverbs, and clean up some of the core
         code's device list management
    
       - More progress on net namespaces for RDMA devices
    
       - Consolidate driver BAR mmapping support into core code helpers and
         rework how RDMA holds poitners to mm_struct for get_user_pages
         cases
    
       - First pass to use 'dev_name' instead of ib_device->name
    
       - Device renaming for RDMA devices"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (242 commits)
      IB/mlx5: Add support for extended atomic operations
      RDMA/core: Fix comment for hw stats init for port == 0
      RDMA/core: Refactor ib_register_device() function
      RDMA/core: Fix unwinding flow in case of error to register device
      ib_srp: Remove WARN_ON in srp_terminate_io()
      IB/mlx5: Allow scatter to CQE without global signaled WRs
      IB/mlx5: Verify that driver supports user flags
      IB/mlx5: Support scatter to CQE for DC transport type
      RDMA/drivers: Use core provided API for registering device attributes
      RDMA/core: Allow existing drivers to set one sysfs group per device
      IB/rxe: Remove unnecessary enum values
      RDMA/umad: Use kernel API to allocate umad indexes
      RDMA/uverbs: Use kernel API to allocate uverbs indexes
      RDMA/core: Increase total number of RDMA ports across all devices
      IB/mlx4: Add port and TID to MAD debug print
      IB/mlx4: Enable debug print of SMPs
      RDMA/core: Rename ports_parent to ports_kobj
      RDMA/core: Do not expose unsupported counters
      IB/mlx4: Refer to the device kobject instead of ports_parent
      RDMA/nldev: Allow IB device rename through RDMA netlink
      ...

commit 4972e6fa3a04032830bc3d6bb343d08ab3546773
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Sep 12 15:36:41 2018 +0300

    net/mlx5: Refactor fragmented buffer struct fields and init flow
    
    Take struct mlx5_frag_buf out of mlx5_frag_buf_ctrl, as it is not
    needed to manage and control the datapath of the fragmented buffers API.
    
    struct mlx5_frag_buf contains control info to manage the allocation
    and de-allocation of the fragmented buffer.
    Its fields are not relevant for datapath, so here I take them out of the
    struct mlx5_frag_buf_ctrl, except for the fragments array itself.
    
    In addition, modified mlx5_fill_fbc to initialise the frags pointers
    as well. This implies that the buffer must be allocated before the
    function is called.
    
    A set of type-specific *_get_byte_size() functions are replaced by
    a generic one.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 320d4dfe8c2f..289c18db2611 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -435,6 +435,7 @@ struct mlx5_ib_qp {
 
 struct mlx5_ib_cq_buf {
 	struct mlx5_frag_buf_ctrl fbc;
+	struct mlx5_frag_buf    frag_buf;
 	struct ib_umem		*umem;
 	int			cqe_size;
 	int			nent;

commit 5d6ff1babe78034f0cf8e5f7bf312a257e5574cc
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Tue Oct 9 12:05:13 2018 +0300

    IB/mlx5: Support scatter to CQE for DC transport type
    
    Scatter to CQE is a HW offload that saves PCI writes by scattering the
    payload to the CQE.
    This patch extends already existing functionality to support DC
    transport type.
    
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Guy Levi <guyle@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8444ea78229a..9de9397166b8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1127,7 +1127,7 @@ void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 			  int page_shift, __be64 *pas, int access_flags);
 void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
-int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
+int mlx5_ib_get_cqe_size(struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 

commit 508a523f6bc6cdfbf7031d66559d4ad24956b741
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Oct 11 22:31:54 2018 +0300

    RDMA/drivers: Use core provided API for registering device attributes
    
    Use rdma_set_device_sysfs_group() to register device attributes and
    simplify the driver.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e5ec3fdaa4d5..8444ea78229a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1201,7 +1201,6 @@ void mlx5_ib_stage_pre_ib_reg_umr_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_ib_stage_ib_reg_init(struct mlx5_ib_dev *dev);
 void mlx5_ib_stage_ib_reg_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_ib_stage_post_ib_reg_umr_init(struct mlx5_ib_dev *dev);
-int mlx5_ib_stage_class_attr_init(struct mlx5_ib_dev *dev);
 void __mlx5_ib_remove(struct mlx5_ib_dev *dev,
 		      const struct mlx5_ib_profile *profile,
 		      int stage);

commit 76dc5a8406bffabf3f466e331a3e9515ddf93954
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Sep 20 21:45:19 2018 +0300

    IB/mlx5: Manage device uid for DEVX white list commands
    
    Manage device uid for DEVX white list commands.  The created device uid
    will be used on white list commands if the user didn't supply its own uid.
    
    This will enable the firmware to filter out non privileged functionality
    as of the recognition of the uid.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5ada28d6e5e9..e5ec3fdaa4d5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -925,6 +925,7 @@ struct mlx5_ib_dev {
 	struct list_head	ib_dev_list;
 	u64			sys_image_guid;
 	struct mlx5_memic	memic;
+	u16			devx_whitelist_uid;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
@@ -1249,10 +1250,8 @@ void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 				  u8 port_num);
 
 #if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
-int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
-			struct mlx5_ib_ucontext *context);
-void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
-			  struct mlx5_ib_ucontext *context);
+int mlx5_ib_devx_create(struct mlx5_ib_dev *dev);
+void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid);
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
@@ -1263,10 +1262,8 @@ int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int
-mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
-		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
-static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
-					struct mlx5_ib_ucontext *context) {}
+mlx5_ib_devx_create(struct mlx5_ib_dev *dev) { return -EOPNOTSUPP; };
+static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev, u16 uid) {}
 static inline const struct uverbs_object_tree_def *
 mlx5_ib_get_devx_tree(void) { return NULL; }
 static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,

commit 5a738b5d47050b77ac8aa90bd79429940533ef6a
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Sep 20 16:42:24 2018 -0600

    RDMA/drivers: Use dev_err/dbg/etc instead of pr_* + ibdev->name
    
    Kernel convention is that a driver for a subsystem will print using
    dev_* on the subsystem's struct device, or with dev_* on the physical
    device. Drivers should rarely use a pr_* function.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 94cf83bd1716..5ada28d6e5e9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -50,17 +50,17 @@
 #include <rdma/uverbs_ioctl.h>
 #include <rdma/mlx5_user_ioctl_cmds.h>
 
-#define mlx5_ib_dbg(dev, format, arg...)				\
-pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
-	 __LINE__, current->pid, ##arg)
+#define mlx5_ib_dbg(_dev, format, arg...)                                      \
+	dev_dbg(&(_dev)->ib_dev.dev, "%s:%d:(pid %d): " format, __func__,      \
+		__LINE__, current->pid, ##arg)
 
-#define mlx5_ib_err(dev, format, arg...)				\
-pr_err("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
-	__LINE__, current->pid, ##arg)
+#define mlx5_ib_err(_dev, format, arg...)                                      \
+	dev_err(&(_dev)->ib_dev.dev, "%s:%d:(pid %d): " format, __func__,      \
+		__LINE__, current->pid, ##arg)
 
-#define mlx5_ib_warn(dev, format, arg...)				\
-pr_warn("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
-	__LINE__, current->pid, ##arg)
+#define mlx5_ib_warn(_dev, format, arg...)                                     \
+	dev_warn(&(_dev)->ib_dev.dev, "%s:%d:(pid %d): " format, __func__,     \
+		 __LINE__, current->pid, ##arg)
 
 #define field_avail(type, fld, sz) (offsetof(type, fld) +		\
 				    sizeof(((type *)0)->fld) <= (sz))

commit d00614c0570656480d6dea5c84c1caec58f40021
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Sep 20 21:39:31 2018 +0300

    IB/mlx5: Set uid as part of XRCD commands
    
    Set uid as part of XRCD commands so that the firmware can manage the
    XRCD object in a secured way.
    
    That will enable using an XRCD that was created by verbs application
    to be used by the DEVX flow in case the uid is equal.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 43eddd452a68..94cf83bd1716 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -539,6 +539,7 @@ struct mlx5_ib_srq {
 struct mlx5_ib_xrcd {
 	struct ib_xrcd		ibxrcd;
 	u32			xrcdn;
+	u16			uid;
 };
 
 enum mlx5_ib_mtt_access_flags {

commit 5deba86ee2cddaebaa1d37ad71efcda26b626592
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Sep 20 21:39:28 2018 +0300

    IB/mlx5: Set uid as part of RQT commands
    
    Set uid as part of RQT commands so that the firmware can manage the
    RQT object in a secured way.
    
    That will enable using an RQT that was created by verbs application
    to be used by the DEVX flow in case the uid is equal.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6b0f27b05c67..43eddd452a68 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -325,6 +325,7 @@ enum {
 struct mlx5_ib_rwq_ind_table {
 	struct ib_rwq_ind_table ib_rwq_ind_tbl;
 	u32			rqtn;
+	u16			uid;
 };
 
 struct mlx5_ib_ubuffer {

commit a1069c1c75d5be4d7fed354a33e5590de27ae0f3
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Sep 20 21:39:19 2018 +0300

    IB/mlx5: Use uid as part of PD commands
    
    Use uid as part of PD commands so that the firmware can manage the
    PD object in a secured way.
    
    For example when a QP is created its uid must match the CQ uid which it
    uses.
    
    Next patches in this series will use the uid from the PD, then will come
    a patch to set the uid on the PD so that all objects will be properly
    work in one change.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a28d04d4c9df..6b0f27b05c67 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -143,6 +143,7 @@ static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibuconte
 struct mlx5_ib_pd {
 	struct ib_pd		ibpd;
 	u32			pdn;
+	u16			uid;
 };
 
 enum {

commit f9882bb5060b956a22c4ce6d477f9860128c0f08
Merge: 26f91da29650 0042f9e458a5
Author: Doug Ledford <dledford@redhat.com>
Date:   Fri Sep 21 20:41:58 2018 -0400

    Merge branch 'mlx5-vport-loopback' into rdma.get
    
    For dependencies, branch based on 'mlx5-next' of
        git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux.git
    
    mlx5 mcast/ucast loopback control enhancements from Leon Romanovsky:
    
    ====================
    This is short series from Mark which extends handling of loopback
    traffic. Originally mlx5 IB dynamically enabled/disabled both unicast
    and multicast based on number of users. However RAW ethernet QPs need
    more granular access.
    ====================
    
    Fixed failed automerge in mlx5_ib.h (minor context conflict issue)
    
    mlx5-vport-loopback branch:
        RDMA/mlx5: Enable vport loopback when user context or QP mandate
        RDMA/mlx5: Allow creating RAW ethernet QP with loopback support
        RDMA/mlx5: Refactor transport domain bookkeeping logic
        net/mlx5: Rename incorrect naming in IFC file
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 0042f9e458a560e13c1da2211cf6429e0c7dd812
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Sep 17 13:30:49 2018 +0300

    RDMA/mlx5: Enable vport loopback when user context or QP mandate
    
    A user can create a QP which can accept loopback traffic, but that's not
    enough. We need to enable loopback on the vport as well. Currently vport
    loopback is enabled only when more than 1 users are using the IB device,
    update the logic to consider whatever a QP which supports loopback was
    created, if so enable vport loopback even if there is only a single user.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ca435654c30b..8376408e2bc9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -862,6 +862,8 @@ struct mlx5_ib_lb_state {
 	/* protect the user_td */
 	struct mutex		mutex;
 	u32			user_td;
+	int			qps;
+	bool			enabled;
 };
 
 struct mlx5_ib_dev {
@@ -1020,6 +1022,8 @@ int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
 int mlx5_ib_destroy_srq(struct ib_srq *srq);
 int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
 			  const struct ib_recv_wr **bad_wr);
+int mlx5_ib_enable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);
+void mlx5_ib_disable_lb(struct mlx5_ib_dev *dev, bool td, bool qp);
 struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
 				struct ib_qp_init_attr *init_attr,
 				struct ib_udata *udata);

commit 175edba85634a8be0ddab5ee96d0b23d9f17627e
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Sep 17 13:30:48 2018 +0300

    RDMA/mlx5: Allow creating RAW ethernet QP with loopback support
    
    Expose two new flags:
    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC
    MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC
    
    Those flags can be used at creation time in order to allow a QP
    to be able to receive loopback traffic (unicast and multicast).
    We store the state in the QP to be used on the destroy path
    to indicate with which flags the QP was created with.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index fde5a867a7d3..ca435654c30b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -428,7 +428,7 @@ struct mlx5_ib_qp {
 	struct list_head	cq_send_list;
 	struct mlx5_rate_limit	rl;
 	u32                     underlay_qpn;
-	bool			tunnel_offload_en;
+	u32			flags_en;
 	/* storage for qp sub type when core qp type is IB_QPT_DRIVER */
 	enum ib_qp_type		qp_sub_type;
 };

commit a560f1d9af4be84ee91d1a47382cacf620eb4a79
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Sep 17 13:30:47 2018 +0300

    RDMA/mlx5: Refactor transport domain bookkeeping logic
    
    In preparation to enable loopback on a single user context move the logic
    that enables/disables loopback to separate functions and group variables
    under a single struct.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 320d4dfe8c2f..fde5a867a7d3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -858,6 +858,12 @@ to_mcounters(struct ib_counters *ibcntrs)
 	return container_of(ibcntrs, struct mlx5_ib_mcounters, ibcntrs);
 }
 
+struct mlx5_ib_lb_state {
+	/* protect the user_td */
+	struct mutex		mutex;
+	u32			user_td;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	const struct uverbs_object_tree_def *driver_trees[6];
@@ -899,9 +905,7 @@ struct mlx5_ib_dev {
 	const struct mlx5_ib_profile	*profile;
 	struct mlx5_eswitch_rep		*rep;
 
-	/* protect the user_td */
-	struct mutex		lb_mutex;
-	u32			user_td;
+	struct mlx5_ib_lb_state		lb;
 	u8			umr_fence;
 	struct list_head	ib_dev_list;
 	u64			sys_image_guid;

commit b5231b019d76521dd8c59a54c174770ec92c767c
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:48:04 2018 +0300

    RDMA/umem: Use ib_umem_odp in all function signatures connected to ODP
    
    All of these functions already require the ODP version of the umem struct,
    make this very clear by having the signature require it. This paves the
    way to using the container_of() pattern to link umem_odp and umem
    together.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 81154b598266..dc34ffa4c8b3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1150,7 +1150,7 @@ void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
 int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
-void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
+void mlx5_ib_invalidate_range(struct ib_umem_odp *umem_odp, unsigned long start,
 			      unsigned long end);
 void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
 void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,

commit e2cd1d1ad204664f9ce78cb61c9307c149051f8e
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Sun Sep 16 20:43:10 2018 +0300

    RDMA/mlx5: Use rdma_user_mmap_io
    
    Rely on the new core code helper to map BAR memory from the driver.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6c57872fdc4e..81154b598266 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -116,13 +116,6 @@ enum {
 	MLX5_MEMIC_BASE_SIZE	= 1 << MLX5_MEMIC_BASE_ALIGN,
 };
 
-struct mlx5_ib_vma_private_data {
-	struct list_head list;
-	struct vm_area_struct *vma;
-	/* protect vma_private_list add/del */
-	struct mutex *vma_private_list_mutex;
-};
-
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;
@@ -134,9 +127,6 @@ struct mlx5_ib_ucontext {
 	u8			cqe_version;
 	/* Transport Domain number */
 	u32			tdn;
-	struct list_head	vma_private_list;
-	/* protect vma_private_list add/del */
-	struct mutex		vma_private_list_mutex;
 
 	u64			lib_caps;
 	DECLARE_BITMAP(dm_pages, MLX5_MAX_MEMIC_PAGES);

commit b47fd4ffe2d6422a986f19d47563d72c79ebbc21
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Sep 6 17:27:07 2018 +0300

    RDMA/mlx5: Add NIC TX namespace when getting a flow table
    
    Add the ability to get a NIC TX flow table when using _get_flow_table().
    This will allow to create a matcher and a flow rule on the NIC TX path.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c667a6c51c12..6c57872fdc4e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -190,6 +190,7 @@ struct mlx5_ib_flow_matcher {
 	struct mlx5_ib_match_params matcher_mask;
 	int			mask_len;
 	enum mlx5_ib_flow_type	flow_type;
+	enum mlx5_flow_namespace_type ns_type;
 	u16			priority;
 	struct mlx5_core_dev	*mdev;
 	atomic_t		usecnt;

commit b823dd6d86ce6576d229c865895d0ee5285d0363
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Sep 6 17:27:05 2018 +0300

    RDMA/mlx5: Refactor raw flow creation
    
    Move struct mlx5_flow_act to be passed from the method entry point,
    this will allow to add support for flow action for the raw create flow
    path.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0e831e48047a..c667a6c51c12 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -39,6 +39,7 @@
 #include <rdma/ib_smi.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cq.h>
+#include <linux/mlx5/fs.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/srq.h>
 #include <linux/mlx5/fs.h>
@@ -1253,7 +1254,8 @@ void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
 struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
-	void *cmd_in, int inlen, int dest_id, int dest_type);
+	struct mlx5_flow_act *flow_act, void *cmd_in, int inlen,
+	int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
 int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);

commit 2ea262039015ba7e74dcaff91e70c547a45437c7
Author: Mark Bloch <markb@mellanox.com>
Date:   Thu Sep 6 17:27:03 2018 +0300

    RDMA/mlx5: Refactor flow action parsing to be more generic
    
    Make the parsing of flow actions more generic so it could be used by
    mlx5 raw create flow.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a3ade16937b4..0e831e48047a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -41,6 +41,7 @@
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/srq.h>
+#include <linux/mlx5/fs.h>
 #include <linux/types.h>
 #include <linux/mlx5/transobj.h>
 #include <rdma/ib_user_verbs.h>
@@ -872,6 +873,9 @@ to_mcounters(struct ib_counters *ibcntrs)
 	return container_of(ibcntrs, struct mlx5_ib_mcounters, ibcntrs);
 }
 
+int parse_flow_flow_action(struct mlx5_ib_flow_action *maction,
+			   bool is_egress,
+			   struct mlx5_flow_act *action);
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	const struct uverbs_object_tree_def *driver_trees[7];

commit 78dd0c430f116a325eeda61416c0b36a8206fbfc
Author: Mark Bloch <markb@mellanox.com>
Date:   Sun Sep 2 12:51:31 2018 +0300

    RDMA/mlx5: Add NIC TX steering support
    
    Just like ingress steering, allow a user to create steering rules that
    match egress vport traffic. We expose the same number of priorities as
    the bypass (NIC RX) steering.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4e9676dc7531..a3ade16937b4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -196,6 +196,7 @@ struct mlx5_ib_flow_matcher {
 
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
+	struct mlx5_ib_flow_prio	egress_prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
 	struct mlx5_ib_flow_prio	egress[MLX5_IB_NUM_EGRESS_FTS];
 	struct mlx5_flow_table		*lag_demux_ft;

commit af68ccbc1131ddd8dcda65b015cd9919b352485a
Merge: c6a21c3864fc a090d0d859ff
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Sep 5 15:24:58 2018 -0600

    Merge branch 'mlx5-flow-mutate' into rdma.git for-next
    
    For dependencies, branch based on 'mellanox/mlx5-next' of
    git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux.git
    
    Pull Flow actions to mutate packets from Leon Romanovsky:
    
    ====================
    This series exposes the ability to create flow actions which can
    mutate packet headers. We do that by exposing two new verbs:
     * modify header - can change existing packet headers. packet
     * reformat - can encapsulate or decapsulate a packet.
                  Once created a flow action must be attached to a steering
                  rule for it to take effect.
    
    The first 10 patches refactor mlx5_core code, rename internal structures
    to better reflect their operation and export needed functions so the RDMA
    side can allocate the action.
    
    The last 5 patches expose via the IOCTL infrastructure mlx5_ib methods
    which do the actual allocation of resources and return an handle to the
    user. A user of this API is expected to know how to work with the device's
    spec as the input to those function is HW depended.
    
    An example usage of the modify header action is routing, A user can create
    an action which edits the L2 header and decrease the TTL.
    
    An example usage of the packet reformat action is VXLAN encap/decap which
    is done by the HW.
    ====================
    
    * branch 'mlx5-flow-mutate':
      RDMA/mlx5: Extend packet reformat verbs
      RDMA/mlx5: Add new flow action verb - packet reformat
      RDMA/uverbs: Add generic function to fill in flow action object
      RDMA/mlx5: Add a new flow action verb - modify header
      RDMA/uverbs: Add UVERBS_ATTR_CONST_IN to the specs language
      net/mlx5: Export packet reformat alloc/dealloc functions
      net/mlx5: Pass a namespace for packet reformat ID allocation
      net/mlx5: Expose new packet reformat capabilities
      {net, RDMA}/mlx5: Rename encap to reformat packet
      net/mlx5: Move header encap type to IFC header file
      net/mlx5: Break encap/decap into two separated flow table creation flags
      net/mlx5: Add support for more namespaces when allocating modify header
      net/mlx5: Export modify header alloc/dealloc functions
      net/mlx5: Add proper NIC TX steering flow tables support
      net/mlx5: Cleanup flow namespace getter switch logic
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit a090d0d859ff88dd4c34614d01cee9b0603f4313
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Aug 28 14:18:54 2018 +0300

    RDMA/mlx5: Extend packet reformat verbs
    
    We expose new actions:
    
    L2_TO_L2_TUNNEL - A generic encap from L2 to L2, the data passed should
                      be the encapsulating headers.
    
    L3_TUNNEL_TO_L2 - Will do decap where the inner packet starts from L3,
                      the data should be mac or mac + vlan (14 or 18 bytes).
    
    L2_TO_L3_TUNNEL - Will do encap where is L2 of the original packet will
                      not be included, the data should be the encapsulating
                      header.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8ac84cc00fd5..eb6a0ca0247f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -153,6 +153,7 @@ struct mlx5_ib_pd {
 
 enum {
 	MLX5_IB_FLOW_ACTION_MODIFY_HEADER,
+	MLX5_IB_FLOW_ACTION_PACKET_REFORMAT,
 	MLX5_IB_FLOW_ACTION_DECAP,
 };
 

commit 08aeb97cb82483192bd8ad8e60d1b73ce1b75923
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Aug 28 14:18:53 2018 +0300

    RDMA/mlx5: Add new flow action verb - packet reformat
    
    For now, only add L2_TUNNEL_TO_L2 option. This will allow to perform
    generic decap operation if the encapsulating protocol is L2 based, and the
    inner packet is also L2 based. For example this can be used to decap VXLAN
    packets.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c26ea868b4f1..8ac84cc00fd5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -153,6 +153,7 @@ struct mlx5_ib_pd {
 
 enum {
 	MLX5_IB_FLOW_ACTION_MODIFY_HEADER,
+	MLX5_IB_FLOW_ACTION_DECAP,
 };
 
 #define MLX5_IB_FLOW_MCAST_PRIO		(MLX5_BY_PASS_NUM_PRIOS - 1)

commit b4749bf25652689d8e33827460266b78bb2ec42c
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Aug 28 14:18:51 2018 +0300

    RDMA/mlx5: Add a new flow action verb - modify header
    
    Expose the ability to create a flow action which changes packet
    headers. The data passed from userspace should be modify header actions as
    defined by HW specification.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 320d4dfe8c2f..c26ea868b4f1 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -151,6 +151,10 @@ struct mlx5_ib_pd {
 	u32			pdn;
 };
 
+enum {
+	MLX5_IB_FLOW_ACTION_MODIFY_HEADER,
+};
+
 #define MLX5_IB_FLOW_MCAST_PRIO		(MLX5_BY_PASS_NUM_PRIOS - 1)
 #define MLX5_IB_FLOW_LAST_PRIO		(MLX5_BY_PASS_NUM_REGULAR_PRIOS - 1)
 #if (MLX5_IB_FLOW_LAST_PRIO <= 0)
@@ -814,6 +818,11 @@ struct mlx5_ib_flow_action {
 			u64			    ib_flags;
 			struct mlx5_accel_esp_xfrm *ctx;
 		} esp_aes_gcm;
+		struct {
+			struct mlx5_ib_dev *dev;
+			u32 sub_type;
+			u32 action_id;
+		} flow_action_raw;
 	};
 };
 
@@ -860,7 +869,7 @@ to_mcounters(struct ib_counters *ibcntrs)
 
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
-	const struct uverbs_object_tree_def *driver_trees[6];
+	const struct uverbs_object_tree_def *driver_trees[7];
 	struct mlx5_core_dev		*mdev;
 	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;
@@ -1238,6 +1247,7 @@ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
 int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
+void mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction);
 #else
 static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
@@ -1256,6 +1266,11 @@ mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root)
 {
 	return 0;
 }
+static inline void
+mlx5_ib_destroy_flow_action_raw(struct mlx5_ib_flow_action *maction)
+{
+	return;
+};
 #endif
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit c6a21c3864fc7f5febae7d096cd136f397c791f2
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Tue Aug 28 14:29:05 2018 +0300

    IB/mlx5: Change TX affinity assignment in RoCE LAG mode
    
    In the current code, the TX affinity is per RoCE device, which can cause
    unfairness between different contexts. e.g. if we open two contexts, and
    each open 10 QPs concurrently, all of the QPs of the first context might
    end up on the first port instead of distributed on the two ports as
    expected
    
    To overcome this unfairness between processes, we maintain per device TX
    affinity, and per process TX affinity.
    
    The allocation algorithm is as follow:
    
    1. Hold two tx_port_affinity atomic variables, one per RoCE device and one
       per ucontext. Both initialized to 0.
    
    2. In mlx5_ib_alloc_ucontext do:
     2.1. ucontext.tx_port_affinity = device.tx_port_affinity
     2.2. device.tx_port_affinity += 1
    
    3. In modify QP INIT2RST:
     3.1. qp.tx_port_affinity = ucontext.tx_port_affinity % MLX5_PORT_NUM
     3.2. ucontext.tx_port_affinity += 1
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 320d4dfe8c2f..387a6ec1ce3b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -139,6 +139,8 @@ struct mlx5_ib_ucontext {
 	u64			lib_caps;
 	DECLARE_BITMAP(dm_pages, MLX5_MAX_MEMIC_PAGES);
 	u16			devx_uid;
+	/* For RoCE LAG TX affinity */
+	atomic_t		tx_port_affinity;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -699,7 +701,7 @@ struct mlx5_roce {
 	rwlock_t		netdev_lock;
 	struct net_device	*netdev;
 	struct notifier_block	nb;
-	atomic_t		next_port;
+	atomic_t		tx_port_affinity;
 	enum ib_port_state last_port_state;
 	struct mlx5_ib_dev	*dev;
 	u8			native_port_num;

commit 7d96c9b17636b6148534617ddf95dead18617776
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu Aug 9 20:14:35 2018 -0600

    IB/uverbs: Have the core code create the uverbs_root_spec
    
    There is no reason for drivers to do this, the core code should take of
    everything. The drivers will provide their information from rodata to
    describe their modifications to the core's base uapi specification.
    
    The core uses this to build up the runtime uapi for each device.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b75754efc663..320d4dfe8c2f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -860,6 +860,7 @@ to_mcounters(struct ib_counters *ibcntrs)
 
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
+	const struct uverbs_object_tree_def *driver_trees[6];
 	struct mlx5_core_dev		*mdev;
 	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;

commit d34ac5cd3a73aacd11009c4fc3ba15d7ea62c411
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jul 18 09:25:32 2018 -0700

    RDMA, core and ULPs: Declare ib_post_send() and ib_post_recv() arguments const
    
    Since neither ib_post_send() nor ib_post_recv() modify the data structure
    their second argument points at, declare that argument const. This change
    makes it necessary to declare the 'bad_wr' argument const too and also to
    modify all ULPs that call ib_post_send(), ib_post_recv() or
    ib_post_srq_recv(). This patch does not change any functionality but makes
    it possible for the compiler to verify whether the
    ib_post_(send|recv|srq_recv) really do not modify the posted work request.
    
    To make this possible, only one cast had to be introduce that casts away
    constness, namely in rpcrdma_post_recvs(). The only way I can think of to
    avoid that cast is to introduce an additional loop in that function or to
    change the data type of bad_wr from struct ib_recv_wr ** into int
    (an index that refers to an element in the work request list). However,
    both approaches would require even more extensive changes than this
    patch.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8d9eaa31fab2..b75754efc663 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1013,8 +1013,8 @@ int mlx5_ib_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
 		       enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
 int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
 int mlx5_ib_destroy_srq(struct ib_srq *srq);
-int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,
-			  struct ib_recv_wr **bad_wr);
+int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
+			  const struct ib_recv_wr **bad_wr);
 struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
 				struct ib_qp_init_attr *init_attr,
 				struct ib_udata *udata);
@@ -1025,10 +1025,10 @@ int mlx5_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr
 int mlx5_ib_destroy_qp(struct ib_qp *qp);
 void mlx5_ib_drain_sq(struct ib_qp *qp);
 void mlx5_ib_drain_rq(struct ib_qp *qp);
-int mlx5_ib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
-		      struct ib_send_wr **bad_wr);
-int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,
-		      struct ib_recv_wr **bad_wr);
+int mlx5_ib_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
+		      const struct ib_send_wr **bad_wr);
+int mlx5_ib_post_recv(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
+		      const struct ib_recv_wr **bad_wr);
 void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 			  void *buffer, u32 length,
@@ -1209,10 +1209,10 @@ int mlx5_ib_gsi_modify_qp(struct ib_qp *qp, struct ib_qp_attr *attr,
 int mlx5_ib_gsi_query_qp(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
 			 int qp_attr_mask,
 			 struct ib_qp_init_attr *qp_init_attr);
-int mlx5_ib_gsi_post_send(struct ib_qp *qp, struct ib_send_wr *wr,
-			  struct ib_send_wr **bad_wr);
-int mlx5_ib_gsi_post_recv(struct ib_qp *qp, struct ib_recv_wr *wr,
-			  struct ib_recv_wr **bad_wr);
+int mlx5_ib_gsi_post_send(struct ib_qp *qp, const struct ib_send_wr *wr,
+			  const struct ib_send_wr **bad_wr);
+int mlx5_ib_gsi_post_recv(struct ib_qp *qp, const struct ib_recv_wr *wr,
+			  const struct ib_recv_wr **bad_wr);
 void mlx5_ib_gsi_pkey_change(struct mlx5_ib_gsi_qp *gsi);
 
 int mlx5_ib_generate_wc(struct ib_cq *ibcq, struct ib_wc *wc);

commit f696bf6d64b195b83ca1bdb7cd33c999c9dcf514
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Jul 18 09:25:14 2018 -0700

    RDMA: Constify the argument of the work request conversion functions
    
    When posting a send work request, the work request that is posted is not
    modified by any of the RDMA drivers. Make this explicit by constifying
    most ib_send_wr pointers in RDMA transport drivers.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 462505c8fa25..8d9eaa31fab2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -469,7 +469,7 @@ struct mlx5_umr_wr {
 	u32				mkey;
 };
 
-static inline struct mlx5_umr_wr *umr_wr(struct ib_send_wr *wr)
+static inline const struct mlx5_umr_wr *umr_wr(const struct ib_send_wr *wr)
 {
 	return container_of(wr, struct mlx5_umr_wr, wr);
 }

commit cb80fb189270e7b2c32fa470d40e951852614eb2
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:12 2018 +0300

    IB/mlx5: Enable driver uapi commands for flow steering
    
    Expose the mlx5 flow steering parsing trees, exposing the functionality to
    user space.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 33948b547894..462505c8fa25 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1236,6 +1236,7 @@ struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
 	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
 	void *cmd_in, int inlen, int dest_id, int dest_type);
 bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
+int mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root);
 #else
 static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
@@ -1244,17 +1245,16 @@ static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 					struct mlx5_ib_ucontext *context) {}
 static inline const struct uverbs_object_tree_def *
 mlx5_ib_get_devx_tree(void) { return NULL; }
-static inline struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
-	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
-	void *cmd_in, int inlen, int dest_id, int dest_type)
-{
-	return ERR_PTR(-EOPNOTSUPP);
-}
 static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
 					     int *dest_type)
 {
 	return false;
 }
+static inline int
+mlx5_ib_get_flow_trees(const struct uverbs_object_tree_def **root)
+{
+	return 0;
+}
 #endif
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit d4be3f4466b8a770ea2c3b57b942efd057fe1c19
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:10 2018 +0300

    IB/mlx5: Support adding flow steering rule by raw description
    
    Add support to set a public flow steering rule when its destination is a
    TIR by using raw specification data.
    
    The logic follows the verbs API but instead of using ib_spec(s) the raw,
    device specific, description is used.
    
    This allows supporting specialty matchers without having to define new
    matches in the verbs struct based language.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 324f4ea5fce6..33948b547894 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -172,6 +172,8 @@ struct mlx5_ib_flow_handler {
 	struct mlx5_ib_flow_prio	*prio;
 	struct mlx5_flow_handle		*rule;
 	struct ib_counters		*ibcounters;
+	struct mlx5_ib_dev		*dev;
+	struct mlx5_ib_flow_matcher	*flow_matcher;
 };
 
 struct mlx5_ib_flow_matcher {

commit 32269441240064c7475241ae28fee787fcdf55b9
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:09 2018 +0300

    IB/mlx5: Introduce driver create and destroy flow methods
    
    Introduce driver create and destroy flow methods on the uverbs flow
    object.
    
    This allows the driver to get its specific device attributes to match the
    underlay specification while still using the generic ib_flow object for
    cleanup and code sharing.
    
    The IB object's attributes are set via the ib_set_flow() helper function.
    
    The specific implementation for the given specification is added in
    downstream patches.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c556b00bf4f7..324f4ea5fce6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1230,6 +1230,10 @@ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 			  struct mlx5_ib_ucontext *context);
 const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
+struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
+	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+	void *cmd_in, int inlen, int dest_id, int dest_type);
+bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id, int *dest_type);
 #else
 static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
@@ -1238,6 +1242,17 @@ static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 					struct mlx5_ib_ucontext *context) {}
 static inline const struct uverbs_object_tree_def *
 mlx5_ib_get_devx_tree(void) { return NULL; }
+static inline struct mlx5_ib_flow_handler *mlx5_ib_raw_fs_rule_add(
+	struct mlx5_ib_dev *dev, struct mlx5_ib_flow_matcher *fs_matcher,
+	void *cmd_in, int inlen, int dest_id, int dest_type)
+{
+	return ERR_PTR(-EOPNOTSUPP);
+}
+static inline bool mlx5_ib_devx_is_flow_dest(void *obj, int *dest_id,
+					     int *dest_type)
+{
+	return false;
+}
 #endif
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit fd44e3853c0155fa82314f341f476d4793415cd2
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon Jul 23 15:25:07 2018 +0300

    IB/mlx5: Introduce flow steering matcher uapi object
    
    Introduce flow steering matcher object and its create and destroy methods.
    
    This matcher object holds some mlx5 specific driver properties that
    matches the underlay device specification when an mlx5 flow steering group
    is created.
    
    It will be used in downstream patches to be part of mlx5 specific create
    flow method.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 04a5d82c9cf3..c556b00bf4f7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -46,6 +46,7 @@
 #include <rdma/ib_user_verbs.h>
 #include <rdma/mlx5-abi.h>
 #include <rdma/uverbs_ioctl.h>
+#include <rdma/mlx5_user_ioctl_cmds.h>
 
 #define mlx5_ib_dbg(dev, format, arg...)				\
 pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
@@ -173,6 +174,16 @@ struct mlx5_ib_flow_handler {
 	struct ib_counters		*ibcounters;
 };
 
+struct mlx5_ib_flow_matcher {
+	struct mlx5_ib_match_params matcher_mask;
+	int			mask_len;
+	enum mlx5_ib_flow_type	flow_type;
+	u16			priority;
+	struct mlx5_core_dev	*mdev;
+	atomic_t		usecnt;
+	u8			match_criteria_enable;
+};
+
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];

commit 05f58ceba123bdb420cf44c6ea04b6db467edd1c
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jul 8 13:50:21 2018 +0300

    RDMA/mlx5: Check that supplied blue flame index doesn't overflow
    
    User's supplied index is checked again total number of system pages, but
    this number already includes num_static_sys_pages, so addition of that
    value to supplied index causes to below error while trying to access
    sys_pages[].
    
    BUG: KASAN: slab-out-of-bounds in bfregn_to_uar_index+0x34f/0x400
    Read of size 4 at addr ffff880065561904 by task syz-executor446/314
    
    CPU: 0 PID: 314 Comm: syz-executor446 Not tainted 4.18.0-rc1+ #256
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.11.0-0-g63451fca13-prebuilt.qemu-project.org 04/01/2014
    Call Trace:
     dump_stack+0xef/0x17e
     print_address_description+0x83/0x3b0
     kasan_report+0x18d/0x4d0
     bfregn_to_uar_index+0x34f/0x400
     create_user_qp+0x272/0x227d
     create_qp_common+0x32eb/0x43e0
     mlx5_ib_create_qp+0x379/0x1ca0
     create_qp.isra.5+0xc94/0x22d0
     ib_uverbs_create_qp+0x21b/0x2a0
     ib_uverbs_write+0xc2c/0x1010
     vfs_write+0x1b0/0x550
     ksys_write+0xc6/0x1a0
     do_syscall_64+0xa7/0x590
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x433679
    Code: fd ff 48 81 c4 80 00 00 00 e9 f1 fe ff ff 0f 1f 00 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 3b 91 fd ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fff2b3d8e48 EFLAGS: 00000217 ORIG_RAX: 0000000000000001
    RAX: ffffffffffffffda RBX: 00000000004002f8 RCX: 0000000000433679
    RDX: 0000000000000040 RSI: 0000000020000240 RDI: 0000000000000003
    RBP: 00000000006d4018 R08: 00000000004002f8 R09: 00000000004002f8
    R10: 00000000004002f8 R11: 0000000000000217 R12: 0000000000000000
    R13: 000000000040cb00 R14: 000000000040cb90 R15: 0000000000000006
    
    Allocated by task 314:
     kasan_kmalloc+0xa0/0xd0
     __kmalloc+0x1a9/0x510
     mlx5_ib_alloc_ucontext+0x966/0x2620
     ib_uverbs_get_context+0x23f/0xa60
     ib_uverbs_write+0xc2c/0x1010
     __vfs_write+0x10d/0x720
     vfs_write+0x1b0/0x550
     ksys_write+0xc6/0x1a0
     do_syscall_64+0xa7/0x590
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Freed by task 1:
     __kasan_slab_free+0x12e/0x180
     kfree+0x159/0x630
     kvfree+0x37/0x50
     single_release+0x8e/0xf0
     __fput+0x2d8/0x900
     task_work_run+0x102/0x1f0
     exit_to_usermode_loop+0x159/0x1c0
     do_syscall_64+0x408/0x590
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    The buggy address belongs to the object at ffff880065561100
     which belongs to the cache kmalloc-4096 of size 4096
    The buggy address is located 2052 bytes inside of
     4096-byte region [ffff880065561100, ffff880065562100)
    The buggy address belongs to the page:
    page:ffffea0001955800 count:1 mapcount:0 mapping:ffff88006c402480 index:0x0 compound_mapcount: 0
    flags: 0x4000000000008100(slab|head)
    raw: 4000000000008100 ffffea0001a7c000 0000000200000002 ffff88006c402480
    raw: 0000000000000000 0000000080070007 00000001ffffffff 0000000000000000
    page dumped because: kasan: bad access detected
    
    Memory state around the buggy address:
     ffff880065561800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
     ffff880065561880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    >ffff880065561900: 04 fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
                       ^
     ffff880065561980: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
     ffff880065561a00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
    
    Cc: <stable@vger.kernel.org> # 4.15
    Fixes: 1ee47ab3e8d8 ("IB/mlx5: Enable QP creation with a given blue flame index")
    Reported-by: Noa Osherovich <noaos@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 93087409f4b8..04a5d82c9cf3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1330,6 +1330,6 @@ unsigned long mlx5_ib_get_xlt_emergency_page(void);
 void mlx5_ib_put_xlt_emergency_page(void);
 
 int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
-			struct mlx5_bfreg_info *bfregi, int bfregn,
+			struct mlx5_bfreg_info *bfregi, u32 bfregn,
 			bool dyn_bfreg);
 #endif /* MLX5_IB_H */

commit ffaf58def01ebdbf2669204e105c5a4f356ba276
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Jul 8 13:50:20 2018 +0300

    RDMA/mlx5: Melt consecutive calls to alloc_bfreg() in one call
    
    There is no need for three consecutive calls to alloc_bfreg(). It can be
    implemented with one function.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 67e86c8304a2..93087409f4b8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -78,12 +78,6 @@ enum {
 	MLX5_REQ_SCAT_DATA64_CQE	= 0x22,
 };
 
-enum mlx5_ib_latency_class {
-	MLX5_IB_LATENCY_CLASS_LOW,
-	MLX5_IB_LATENCY_CLASS_MEDIUM,
-	MLX5_IB_LATENCY_CLASS_HIGH,
-};
-
 enum mlx5_ib_mad_ifc_flags {
 	MLX5_MAD_IFC_IGNORE_MKEY	= 1,
 	MLX5_MAD_IFC_IGNORE_BKEY	= 2,

commit d0e84c0ad39826c38a9d6881fd8f9af476a5d9a7
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Jun 19 10:43:55 2018 +0300

    IB/mlx5: Add support for drain SQ & RQ
    
    This patch follows the logic from ib_core but considers the internal
    device state upon executing the involved commands.
    
    Specifically,
    Upon internal error state modify QP to an error state can be assumed to
    be success as each in-progress WR going to be flushed in error in any
    case as expected by that modify command.
    
    In addition,
    As the drain should never fail the driver makes sure that post_send/recv
    will succeed even if the device is already in an internal error state.
    As such once the driver will supply the simulated/SW CQEs the CQE for
    the drain WR will be handled as well.
    
    In case of an internal error state the CQE for the drain WR may be
    completed as part of the main task that handled the error state or by
    the task that issued the drain WR.
    
    As the above depends on scheduling the code takes the relevant locks and
    actions to make sure that the completion handler for that WR will always
    be called after that the post_send/recv were issued but not in parallel
    to the other task that handles the error flow.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index df547f060b60..67e86c8304a2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1016,6 +1016,8 @@ int mlx5_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 int mlx5_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr_mask,
 		     struct ib_qp_init_attr *qp_init_attr);
 int mlx5_ib_destroy_qp(struct ib_qp *qp);
+void mlx5_ib_drain_sq(struct ib_qp *qp);
+void mlx5_ib_drain_rq(struct ib_qp *qp);
 int mlx5_ib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 		      struct ib_send_wr **bad_wr);
 int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,

commit 4d7dff2b8b24d0087c75afb8e53140acdd1dc2bd
Merge: cfdeb8934b6f 9f876f3de661
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jun 22 08:53:27 2018 -0600

    Merge branch 'icrc-counter' into rdma.git for-next
    
    For dependencies, branch based on 'mellanox/mlx5-next' of
    git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux.git
    
    Pull RoCE ICRC counters from Leon Romanovsky:
    
    ====================
    This series exposes RoCE ICRC counter through existing RDMA hw_counters
    sysfs interface.
    
    The first patch has all HW definitions in mlx5_ifc.h file and second patch
    is the actual counter implementation.
    ====================
    
    * branch 'icrc-counter':
      IB/mlx5: Support RoCE ICRC encapsulated error counter
      net/mlx5: Add RoCE RX ICRC encapsulated counter

commit 9f876f3de6616f02960d7d88ad52c805946f4b63
Author: Talat Batheesh <talatb@mellanox.com>
Date:   Thu Jun 21 15:37:56 2018 +0300

    IB/mlx5: Support RoCE ICRC encapsulated error counter
    
    This patch adds support to query the counter that counts the
    RoCE packets with corrupted ICRC (Invariant Cyclic Redundancy Code).
    
    This counter will be under
    /sys/class/infiniband/<mlx5-dev>/ports/<port>/hw_counters/
    
    rx_icrc_encapsulated - The number of RoCE packets with ICRC
    error.
    
    Signed-off-by: Talat Batheesh <talatb@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d89c8fe626f6..298d6a341bb2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -665,6 +665,7 @@ struct mlx5_ib_counters {
 	size_t *offsets;
 	u32 num_q_counters;
 	u32 num_cong_counters;
+	u32 num_ext_ppcnt_counters;
 	u16 set_id;
 	bool set_id_valid;
 };

commit c59450c463695a016e823175bac421cff219935d
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 17 13:00:06 2018 +0300

    IB/mlx5: Expose DEVX tree
    
    Expose DEVX tree to be used by upper layers.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e45f364622eb..a72c73c3ed33 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1221,12 +1221,15 @@ int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 			struct mlx5_ib_ucontext *context);
 void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 			  struct mlx5_ib_ucontext *context);
+const struct uverbs_object_tree_def *mlx5_ib_get_devx_tree(void);
 #else
 static inline int
 mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
 		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
 static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
 					struct mlx5_ib_ucontext *context) {}
+static inline const struct uverbs_object_tree_def *
+mlx5_ib_get_devx_tree(void) { return NULL; }
 #endif
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit 7c043e908a74ae0a935037cdd984d0cb89b2b970
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 17 13:00:03 2018 +0300

    IB/mlx5: Add support for DEVX query UAR
    
    Return a device UAR index for a given user index via the DEVX interface.
    
    Security note:
    The hardware protection mechanism works like this: Each device object that
    is subject to UAR doorbells (QP/SQ/CQ) gets a UAR ID (called uar_page in
    the device specification manual) upon its creation. Then upon doorbell,
    hardware fetches the object context for which the doorbell was rang, and
    validates that the UAR through which the DB was rang matches the UAR ID
    of the object.
    
    If no match the doorbell is silently ignored by the hardware.  Of
    course, the user cannot ring a doorbell on a UAR that was not mapped to
    it.
    
    Now in devx, as the devx kernel does not manipulate the QP/SQ/CQ command
    mailboxes (except tagging them with UID), we expose to the user its UAR
    ID, so it can embed it in these objects in the expected specification
    format. So the only thing the user can do is hurt itself by creating a
    QP/SQ/CQ with a UAR ID other than his, and then in this case other users
    may ring a doorbell on its objects.
    
    The consequence of that will be that another user can schedule a QP/SQ
    of the buggy user for execution (just insert it to the hardware schedule
    queue or arm its CQ for event generation), no further harm is expected.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1c857dd3c77f..e45f364622eb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1329,4 +1329,7 @@ static inline int get_num_static_uars(struct mlx5_ib_dev *dev,
 unsigned long mlx5_ib_get_xlt_emergency_page(void);
 void mlx5_ib_put_xlt_emergency_page(void);
 
+int bfregn_to_uar_index(struct mlx5_ib_dev *dev,
+			struct mlx5_bfreg_info *bfregi, int bfregn,
+			bool dyn_bfreg);
 #endif /* MLX5_IB_H */

commit a8b92ca1b0e5ce620e425e9d2f89ce44f1a82a82
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 17 12:59:57 2018 +0300

    IB/mlx5: Introduce DEVX
    
    Introduce DEVX to enable direct device commands in downstream patches
    from this series.
    
    In that mode of work the firmware manages the isolation between
    processes' resources and as such a DEVX user id is created and assigned
    to the given user context upon allocation request.
    
    A capability check is done to make sure that this feature is really
    supported by the firmware prior to creating the DEVX user id.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 615bd6e9db6c..1c857dd3c77f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -143,6 +143,7 @@ struct mlx5_ib_ucontext {
 
 	u64			lib_caps;
 	DECLARE_BITMAP(dm_pages, MLX5_MAX_MEMIC_PAGES);
+	u16			devx_uid;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -1215,6 +1216,18 @@ struct mlx5_core_dev *mlx5_ib_get_native_port_mdev(struct mlx5_ib_dev *dev,
 void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
 				  u8 port_num);
 
+#if IS_ENABLED(CONFIG_INFINIBAND_USER_ACCESS)
+int mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+			struct mlx5_ib_ucontext *context);
+void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
+			  struct mlx5_ib_ucontext *context);
+#else
+static inline int
+mlx5_ib_devx_create(struct mlx5_ib_dev *dev,
+		    struct mlx5_ib_ucontext *context) { return -EOPNOTSUPP; };
+static inline void mlx5_ib_devx_destroy(struct mlx5_ib_dev *dev,
+					struct mlx5_ib_ucontext *context) {}
+#endif
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;

commit 47ec38666210485de860ab24675acb3d2e7d4954
Author: Parav Pandit <parav@mellanox.com>
Date:   Wed Jun 13 10:22:06 2018 +0300

    RDMA: Convert drivers to use sgid_attr instead of sgid_index
    
    The core code now ensures that all driver callbacks that receive an
    rdma_ah_attrs will have a sgid_attr's pointer if there is a GRH present.
    
    Drivers can use this pointer instead of calling a query function with
    sgid_index. This simplifies the drivers and also avoids races where a
    gid_index lookup may return different data if it is changed.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d89c8fe626f6..615bd6e9db6c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1183,10 +1183,8 @@ int mlx5_ib_get_vf_stats(struct ib_device *device, int vf,
 int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
 			u64 guid, int type);
 
-__be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
-			       int index);
-int mlx5_get_roce_gid_type(struct mlx5_ib_dev *dev, u8 port_num,
-			   int index, enum ib_gid_type *gid_type);
+__be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev,
+			       const struct ib_gid_attr *attr);
 
 void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
 int mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);

commit 5e95af5f7b60796ccd890a39c0ed9c5df3537952
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:40 2018 +0300

    IB/mlx5: Add flow counters read support
    
    Implements the flow counters read wrapper.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 155bca627222..d89c8fe626f6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -814,6 +814,12 @@ struct mlx5_memic {
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
 };
 
+struct mlx5_read_counters_attr {
+	struct mlx5_fc *hw_cntrs_hndl;
+	u64 *out;
+	u32 flags;
+};
+
 enum mlx5_ib_counters_type {
 	MLX5_IB_COUNTERS_FLOW,
 };
@@ -821,7 +827,12 @@ enum mlx5_ib_counters_type {
 struct mlx5_ib_mcounters {
 	struct ib_counters ibcntrs;
 	enum mlx5_ib_counters_type type;
-	void *hw_cntrs_hndl;
+	/* number of counters supported for this counters type */
+	u32 counters_num;
+	struct mlx5_fc *hw_cntrs_hndl;
+	/* read function for this counters type */
+	int (*read_counters)(struct ib_device *ibdev,
+			     struct mlx5_read_counters_attr *read_attr);
 	/* max index set as part of create_flow */
 	u32 cntrs_max_index;
 	/* number of counters data entries (<description,index> pair) */

commit 3b3233fbf02ee4c5de4d635ca6c4f2566d9716df
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:39 2018 +0300

    IB/mlx5: Add flow counters binding support
    
    Associates a counters with a flow when IB_FLOW_SPEC_ACTION_COUNT is part
    of the flow specifications.
    
    The counters user space placements of location and description (index,
    description) pairs are passed as private data of the counters flow
    specification.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index fd27ec1aed08..155bca627222 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -175,6 +175,7 @@ struct mlx5_ib_flow_handler {
 	struct ib_flow			ibflow;
 	struct mlx5_ib_flow_prio	*prio;
 	struct mlx5_flow_handle		*rule;
+	struct ib_counters		*ibcounters;
 };
 
 struct mlx5_ib_flow_db {
@@ -813,8 +814,22 @@ struct mlx5_memic {
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
 };
 
+enum mlx5_ib_counters_type {
+	MLX5_IB_COUNTERS_FLOW,
+};
+
 struct mlx5_ib_mcounters {
 	struct ib_counters ibcntrs;
+	enum mlx5_ib_counters_type type;
+	void *hw_cntrs_hndl;
+	/* max index set as part of create_flow */
+	u32 cntrs_max_index;
+	/* number of counters data entries (<description,index> pair) */
+	u32 ncounters;
+	/* counters data array for descriptions and indexes */
+	struct mlx5_ib_flow_counters_desc *counters_data;
+	/* protects access to mcounters internal data */
+	struct mutex mcntrs_mutex;
 };
 
 static inline struct mlx5_ib_mcounters *

commit b29e2a1309e38cd1afa598a54f3ccb4e4d2ee01c
Author: Raed Salem <raeds@mellanox.com>
Date:   Thu May 31 16:43:38 2018 +0300

    IB/mlx5: Add counters create and destroy support
    
    This patch implements the device counters create and destroy APIs and
    introducing some internal management structures.
    
    Downstream patches in this series will add the functionality to support
    flow counters binding and reading.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 49a1aa0ff429..fd27ec1aed08 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -813,6 +813,16 @@ struct mlx5_memic {
 	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
 };
 
+struct mlx5_ib_mcounters {
+	struct ib_counters ibcntrs;
+};
+
+static inline struct mlx5_ib_mcounters *
+to_mcounters(struct ib_counters *ibcntrs)
+{
+	return container_of(ibcntrs, struct mlx5_ib_mcounters, ibcntrs);
+}
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;

commit 6c29f57ea4751c4887627521027cd72aba831a97
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Apr 5 18:53:29 2018 +0300

    IB/mlx5: Device memory mr registration support
    
    Adding mlx5_ib driver implementation for reg_dm_mr callback
    which allows registering device memory (DM) as an MR for
    local and remote access.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3e9b6548a96b..49a1aa0ff429 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -540,6 +540,12 @@ struct mlx5_ib_dm {
 
 #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
 
+#define MLX5_IB_DM_ALLOWED_ACCESS (IB_ACCESS_LOCAL_WRITE   |\
+				   IB_ACCESS_REMOTE_WRITE  |\
+				   IB_ACCESS_REMOTE_READ   |\
+				   IB_ACCESS_REMOTE_ATOMIC |\
+				   IB_ZERO_BASED)
+
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
 	void			*descs;
@@ -1075,6 +1081,9 @@ struct ib_dm *mlx5_ib_alloc_dm(struct ib_device *ibdev,
 			       struct ib_dm_alloc_attr *attr,
 			       struct uverbs_attr_bundle *attrs);
 int mlx5_ib_dealloc_dm(struct ib_dm *ibdm);
+struct ib_mr *mlx5_ib_reg_dm_mr(struct ib_pd *pd, struct ib_dm *dm,
+				struct ib_dm_mr_attr *attr,
+				struct uverbs_attr_bundle *attrs);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);

commit 24da00164f7a9c247d2224a54494d0e955199630
Author: Ariel Levkovich <lariel@mellanox.com>
Date:   Thu Apr 5 18:53:27 2018 +0300

    IB/mlx5: Device memory support in mlx5_ib
    
    This patch adds the mlx5_ib driver implementation for the device
    memory allocation API.
    It implements the ib_device callbacks for allocation and deallocation
    operations as well as a new mmap command support which allows mapping
    an allocated device memory to a VMA.
    
    The change also adds reporting of device memory maximum size and
    alignment parameters reported in device capabilities.
    
    The allocation/deallocation operations are using new firmware
    commands to allocate MEMIC memory on the device.
    
    Signed-off-by: Ariel Levkovich <lariel@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2b27ddafc354..3e9b6548a96b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -45,6 +45,7 @@
 #include <linux/mlx5/transobj.h>
 #include <rdma/ib_user_verbs.h>
 #include <rdma/mlx5-abi.h>
+#include <rdma/uverbs_ioctl.h>
 
 #define mlx5_ib_dbg(dev, format, arg...)				\
 pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
@@ -108,6 +109,16 @@ enum {
 	MLX5_IB_INVALID_BFREG		= BIT(31),
 };
 
+enum {
+	MLX5_MAX_MEMIC_PAGES = 0x100,
+	MLX5_MEMIC_ALLOC_SIZE_MASK = 0x3f,
+};
+
+enum {
+	MLX5_MEMIC_BASE_ALIGN	= 6,
+	MLX5_MEMIC_BASE_SIZE	= 1 << MLX5_MEMIC_BASE_ALIGN,
+};
+
 struct mlx5_ib_vma_private_data {
 	struct list_head list;
 	struct vm_area_struct *vma;
@@ -131,6 +142,7 @@ struct mlx5_ib_ucontext {
 	struct mutex		vma_private_list_mutex;
 
 	u64			lib_caps;
+	DECLARE_BITMAP(dm_pages, MLX5_MAX_MEMIC_PAGES);
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -521,6 +533,11 @@ enum mlx5_ib_mtt_access_flags {
 	MLX5_IB_MTT_WRITE = (1 << 1),
 };
 
+struct mlx5_ib_dm {
+	struct ib_dm		ibdm;
+	phys_addr_t		dev_addr;
+};
+
 #define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
 
 struct mlx5_ib_mr {
@@ -784,6 +801,12 @@ struct mlx5_ib_flow_action {
 	};
 };
 
+struct mlx5_memic {
+	struct mlx5_core_dev *dev;
+	spinlock_t		memic_lock;
+	DECLARE_BITMAP(memic_alloc_pages, MLX5_MAX_MEMIC_PAGES);
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -830,6 +853,7 @@ struct mlx5_ib_dev {
 	u8			umr_fence;
 	struct list_head	ib_dev_list;
 	u64			sys_image_guid;
+	struct mlx5_memic	memic;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
@@ -897,6 +921,11 @@ static inline struct mlx5_ib_srq *to_mibsrq(struct mlx5_core_srq *msrq)
 	return container_of(msrq, struct mlx5_ib_srq, msrq);
 }
 
+static inline struct mlx5_ib_dm *to_mdm(struct ib_dm *ibdm)
+{
+	return container_of(ibdm, struct mlx5_ib_dm, ibdm);
+}
+
 static inline struct mlx5_ib_mr *to_mmr(struct ib_mr *ibmr)
 {
 	return container_of(ibmr, struct mlx5_ib_mr, ibmr);
@@ -1041,7 +1070,11 @@ struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
 						      struct ib_udata *udata);
 int mlx5_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
 bool mlx5_ib_dc_atomic_is_supported(struct mlx5_ib_dev *dev);
-
+struct ib_dm *mlx5_ib_alloc_dm(struct ib_device *ibdev,
+			       struct ib_ucontext *context,
+			       struct ib_dm_alloc_attr *attr,
+			       struct uverbs_attr_bundle *attrs);
+int mlx5_ib_dealloc_dm(struct ib_dm *ibdm);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);

commit 802c2125689d1ceedd9671a8f728e85eacdac077
Author: Aviad Yehezkel <aviadye@mellanox.com>
Date:   Wed Mar 28 09:27:53 2018 +0300

    IB/mlx5: Add IPsec support for egress and ingress
    
    This commit introduces support for the esp_aes_gcm flow
    specification for the Innova device. To that end we add
    support for egress steering and some validations that an
    IPsec rule is indeed valid.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5fe73971425e..2b27ddafc354 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -152,6 +152,7 @@ struct mlx5_ib_pd {
 
 #define MLX5_IB_NUM_FLOW_FT		(MLX5_IB_FLOW_LEFTOVERS_PRIO + 1)
 #define MLX5_IB_NUM_SNIFFER_FTS		2
+#define MLX5_IB_NUM_EGRESS_FTS		1
 struct mlx5_ib_flow_prio {
 	struct mlx5_flow_table		*flow_table;
 	unsigned int			refcount;
@@ -167,6 +168,7 @@ struct mlx5_ib_flow_handler {
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
+	struct mlx5_ib_flow_prio	egress[MLX5_IB_NUM_EGRESS_FTS];
 	struct mlx5_flow_table		*lag_demux_ft;
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.

commit c6475a0bca30fc2f9e5e4c48935f08973c2780ef
Author: Aviad Yehezkel <aviadye@mellanox.com>
Date:   Wed Mar 28 09:27:50 2018 +0300

    IB/mlx5: Add implementation for create and destroy action_xfrm
    
    Adding implementation in mlx5 driver to create and destroy action_xfrm
    object. This merely call the accel layer.
    
    A user may pass MLX5_IB_XFRM_FLAGS_REQUIRE_METADATA flag which states
    that [s]he expects a metadata header to be added to the payload. This
    header represents information regarding the transformation's state.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Aviad Yehezkel <aviadye@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0eda960ab8e0..5fe73971425e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -772,6 +772,16 @@ struct mlx5_ib_multiport_info {
 	bool unaffiliate;
 };
 
+struct mlx5_ib_flow_action {
+	struct ib_flow_action		ib_action;
+	union {
+		struct {
+			u64			    ib_flags;
+			struct mlx5_accel_esp_xfrm *ctx;
+		} esp_aes_gcm;
+	};
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -895,6 +905,12 @@ static inline struct mlx5_ib_mw *to_mmw(struct ib_mw *ibmw)
 	return container_of(ibmw, struct mlx5_ib_mw, ibmw);
 }
 
+static inline struct mlx5_ib_flow_action *
+to_mflow_act(struct ib_flow_action *ibact)
+{
+	return container_of(ibact, struct mlx5_ib_flow_action, ib_action);
+}
+
 int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context, unsigned long virt,
 			struct mlx5_db *db);
 void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db);

commit 8c84660bb437fe8692e6a2b4e85023ccb874a520
Author: Matan Barak <matanb@mellanox.com>
Date:   Wed Mar 28 09:27:41 2018 +0300

    IB/mlx5: Initialize the parsing tree root without the help of uverbs
    
    In order to have a custom parsing tree, a provider driver needs to
    assign its parsing tree to ib_device specs_tree field. Otherwise, the
    uverbs client assigns a common default parsing tree for it.
    In downstream patches, the mlx5_ib driver gains a custom parsing tree,
    which contains both the common objects and a new flags field for the
    UVERBS_FLOW_ACTION_ESP_CREATE command.
    This patch makes mlx5_ib assign its own tree to specs_root, which
    later on will be extended.
    
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index aeea74357cbe..0eda960ab8e0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -740,6 +740,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_UAR,
 	MLX5_IB_STAGE_BFREG,
 	MLX5_IB_STAGE_PRE_IB_REG_UMR,
+	MLX5_IB_STAGE_SPECS,
 	MLX5_IB_STAGE_IB_REG,
 	MLX5_IB_STAGE_POST_IB_REG_UMR,
 	MLX5_IB_STAGE_DELAY_DROP,

commit 61147f391a8b3bdde4c0a631dd132d85d00b90a0
Author: Bodong Wang <bodong@mellanox.com>
Date:   Mon Mar 19 15:10:30 2018 +0200

    IB/mlx5: Packet packing enhancement for RAW QP
    
    Enable RAW QP to be able to configure burst control by modify_qp. By
    using burst control with rate limiting, user can achieve best
    performance and accuracy. The burst control information is passed by
    user through udata.
    
    This patch also reports burst control capability for mlx5 related
    hardwares, burst control is only marked as supported when both
    packet_pacing_burst_bound and packet_pacing_typical_size are
    supported.
    
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f9ba1ea94f0f..aeea74357cbe 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -403,7 +403,7 @@ struct mlx5_ib_qp {
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;
-	u32			rate_limit;
+	struct mlx5_rate_limit	rl;
 	u32                     underlay_qpn;
 	bool			tunnel_offload_en;
 	/* storage for qp sub type when core qp type is IB_QPT_DRIVER */

commit 2d873449a202d02e0c4d90009fb2beb7013ac575
Merge: 06892cc19055 bd8602ca42f6
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Mar 14 18:49:12 2018 -0400

    Merge branch 'k.o/wip/dl-for-rc' into k.o/wip/dl-for-next
    
    Due to bug fixes found by the syzkaller bot and taken into the for-rc
    branch after development for the 4.17 merge window had already started
    being taken into the for-next branch, there were fairly non-trivial
    merge issues that would need to be resolved between the for-rc branch
    and the for-next branch.  This merge resolves those conflicts and
    provides a unified base upon which ongoing development for 4.17 can
    be based.
    
    Conflicts:
            drivers/infiniband/hw/mlx5/main.c - Commit 42cea83f9524
            (IB/mlx5: Fix cleanup order on unload) added to for-rc and
            commit b5ca15ad7e61 (IB/mlx5: Add proper representors support)
            add as part of the devel cycle both needed to modify the
            init/de-init functions used by mlx5.  To support the new
            representors, the new functions added by the cleanup patch
            needed to be made non-static, and the init/de-init list
            added by the representors patch needed to be modified to
            match the init/de-init list changes made by the cleanup
            patch.
    Updates:
            drivers/infiniband/hw/mlx5/mlx5_ib.h - Update function
            prototypes added by representors patch to reflect new function
            names as changed by cleanup patch
            drivers/infiniband/hw/mlx5/ib_rep.c - Update init/de-init
            stage list to match new order from cleanup patch
    
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 42cea83f952499f31e2671c4917be8627617db81
Author: Mark Bloch <markb@mellanox.com>
Date:   Wed Mar 14 09:14:15 2018 +0200

    IB/mlx5: Fix cleanup order on unload
    
    On load we create private CQ/QP/PD in order to be used by UMR, we create
    those resources after we register ourself as an IB device, and we destroy
    them after we unregister as an IB device. This was changed by commit
    16c1975f1032 ("IB/mlx5: Create profile infrastructure to add and remove
    stages") which moved the destruction before we unregistration. This
    allowed to trigger an invalid memory access when unloading mlx5_ib while
    there are open resources:
    
    BUG: unable to handle kernel paging request at 00000001002c012c
    ...
    Call Trace:
     mlx5_ib_post_send_wait+0x75/0x110 [mlx5_ib]
     __slab_free+0x9a/0x2d0
     delay_time_func+0x10/0x10 [mlx5_ib]
     unreg_umr.isra.15+0x4b/0x50 [mlx5_ib]
     mlx5_mr_cache_free+0x46/0x150 [mlx5_ib]
     clean_mr+0xc9/0x190 [mlx5_ib]
     dereg_mr+0xba/0xf0 [mlx5_ib]
     ib_dereg_mr+0x13/0x20 [ib_core]
     remove_commit_idr_uobject+0x16/0x70 [ib_uverbs]
     uverbs_cleanup_ucontext+0xe8/0x1a0 [ib_uverbs]
     ib_uverbs_cleanup_ucontext.isra.9+0x19/0x40 [ib_uverbs]
     ib_uverbs_remove_one+0x162/0x2e0 [ib_uverbs]
     ib_unregister_device+0xd4/0x190 [ib_core]
     __mlx5_ib_remove+0x2e/0x40 [mlx5_ib]
     mlx5_remove_device+0xf5/0x120 [mlx5_core]
     mlx5_unregister_interface+0x37/0x90 [mlx5_core]
     mlx5_ib_cleanup+0xc/0x225 [mlx5_ib]
     SyS_delete_module+0x153/0x230
     do_syscall_64+0x62/0x110
     entry_SYSCALL_64_after_hwframe+0x21/0x86
    ...
    
    We restore the original behavior by breaking the UMR stage into two parts,
    pre and post IB registration stages, this way we can restore the original
    functionality and maintain clean separation of logic between stages.
    
    Fixes: 16c1975f1032 ("IB/mlx5: Create profile infrastructure to add and remove stages")
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 139385129973..a5272499b600 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -739,8 +739,9 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_CONG_DEBUGFS,
 	MLX5_IB_STAGE_UAR,
 	MLX5_IB_STAGE_BFREG,
+	MLX5_IB_STAGE_PRE_IB_REG_UMR,
 	MLX5_IB_STAGE_IB_REG,
-	MLX5_IB_STAGE_UMR_RESOURCES,
+	MLX5_IB_STAGE_POST_IB_REG_UMR,
 	MLX5_IB_STAGE_DELAY_DROP,
 	MLX5_IB_STAGE_CLASS_ATTR,
 	MLX5_IB_STAGE_MAX,

commit c44ef998f25eaddcd78924f98e5baed602d933e6
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Tue Mar 13 15:18:48 2018 +0200

    IB/mlx5: Maintain a single emergency page
    
    The mlx5 driver needs to be able to issue invalidation to ODP MRs
    even if it cannot allocate memory. To this end it preallocates
    emergency pages to use when the situation arises.
    
    This flow should be extremely rare enough, that we don't need
    to worry about contention and therefore a single emergency page
    is good enough.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e0bad28e0f09..d88c240c52ce 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -130,9 +130,6 @@ struct mlx5_ib_ucontext {
 	/* protect vma_private_list add/del */
 	struct mutex		vma_private_list_mutex;
 
-	unsigned long		upd_xlt_page;
-	/* protect ODP/KSM */
-	struct mutex		upd_xlt_page_mutex;
 	u64			lib_caps;
 };
 
@@ -1220,4 +1217,7 @@ static inline int get_num_static_uars(struct mlx5_ib_dev *dev,
 	return get_uars_per_sys_page(dev, bfregi->lib_uar_4k) * bfregi->num_static_sys_pages;
 }
 
+unsigned long mlx5_ib_get_xlt_emergency_page(void);
+void mlx5_ib_put_xlt_emergency_page(void);
+
 #endif /* MLX5_IB_H */

commit b5ca15ad7e6189f39dff73d786e41d2a507f7b8b
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Jan 23 11:16:30 2018 +0000

    IB/mlx5: Add proper representors support
    
    This commit adds full support for IB representor:
    
    1) Representors profile, We add two new profiles:
       nic_rep_profile - This profile will be used to create an IB device that
       represents the PF/UPLINK.
       rep_profile - This profile will be used to create an IB device that
       represents VFs. Each VF will be its own representor.
    2) Proper load/unload callbacks, Those are called by the E-Switch when
       moving to/from switchdev mode.
    3) Different flow DB handling for when we in switchdev mode.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 86d07670bfeb..e0bad28e0f09 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1054,6 +1054,31 @@ static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
 
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
+/* Needed for rep profile */
+int mlx5_ib_stage_init_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_init_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_rep_flow_db_init(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_caps_init(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_rep_non_default_cb(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_rep_roce_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_rep_roce_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_dev_res_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_dev_res_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_counters_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_counters_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_bfrag_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_bfrag_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_ib_reg_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_ib_reg_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_umr_res_init(struct mlx5_ib_dev *dev);
+void mlx5_ib_stage_umr_res_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_ib_stage_class_attr_init(struct mlx5_ib_dev *dev);
+void __mlx5_ib_remove(struct mlx5_ib_dev *dev,
+		      const struct mlx5_ib_profile *profile,
+		      int stage);
+void *__mlx5_ib_add(struct mlx5_ib_dev *dev,
+		    const struct mlx5_ib_profile *profile);
+
 int mlx5_ib_get_vf_config(struct ib_device *device, int vf,
 			  u8 port, struct ifla_vf_info *info);
 int mlx5_ib_set_vf_link_state(struct ib_device *device, int vf,

commit b96c9dde17359520d6a5a8eb6d56d91f22c5a413
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Jan 29 10:40:37 2018 +0000

    IB/mlx5: E-Switch, Add rule to forward traffic to vport
    
    In order to forward traffic from representor's SQ to the right virtual
    function, every time an SQ is created also add the corresponding flow rule
    to the FDB.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4dd98b1e9165..86d07670bfeb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -343,6 +343,7 @@ struct mlx5_ib_sq {
 	struct mlx5_ib_wq	*sq;
 	struct mlx5_ib_ubuffer  ubuffer;
 	struct mlx5_db		*doorbell;
+	struct mlx5_flow_handle	*flow_rule;
 	u32			tisn;
 	u8			state;
 };

commit 8e6efa3a31f4a81a4d8817d68110446df383d049
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Nov 6 12:22:13 2017 +0000

    IB/mlx5: When in switchdev mode, expose only raw packet capabilities
    
    Currently in switchdev mode we allow only for raw packet QPs.
    Expose the right capabilities and set the gid table length to 0, also
    make sure we don't try to enable RoCE, so split the function
    to enable RoCE so representors can enable only the notifier needed for
    net device events.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 659bff5e687d..4dd98b1e9165 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -733,6 +733,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_INIT,
 	MLX5_IB_STAGE_FLOW_DB,
 	MLX5_IB_STAGE_CAPS,
+	MLX5_IB_STAGE_NON_DEFAULT_CB,
 	MLX5_IB_STAGE_ROCE,
 	MLX5_IB_STAGE_DEVICE_RESOURCES,
 	MLX5_IB_STAGE_ODP,

commit 9a4ca38d77054febf35cd568cd86aa41c6477301
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Jan 16 14:42:35 2018 +0000

    IB/mlx5: Allocate flow DB only on PF IB device
    
    A flow DB is a shared resource between PF and representors,
    need to allocate it only when creating the PF IB device.
    Once we add IB representors, they will use the flow db which was
    created by the PF.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ec798c6371be..659bff5e687d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -731,6 +731,7 @@ struct mlx5_ib_delay_drop {
 
 enum mlx5_ib_stages {
 	MLX5_IB_STAGE_INIT,
+	MLX5_IB_STAGE_FLOW_DB,
 	MLX5_IB_STAGE_CAPS,
 	MLX5_IB_STAGE_ROCE,
 	MLX5_IB_STAGE_DEVICE_RESOURCES,
@@ -798,7 +799,7 @@ struct mlx5_ib_dev {
 	struct srcu_struct      mr_srcu;
 	u32			null_mkey;
 #endif
-	struct mlx5_ib_flow_db	flow_db;
+	struct mlx5_ib_flow_db	*flow_db;
 	/* protect resources needed as part of reset flow */
 	spinlock_t		reset_flow_resource_lock;
 	struct list_head	qp_list;

commit fc385b7ac48089ed1c6866cdc0dceb4ae1fa54de
Author: Mark Bloch <markb@mellanox.com>
Date:   Tue Jan 16 14:34:48 2018 +0000

    IB/mlx5: Add basic regiser/unregister representors code
    
    Create the basic infrastructure of registering and unregistering
    IB representors. The load/unload callbacks are left empty and
    proper implementation will be introduced in following patches.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index eafb9751daf6..ec798c6371be 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -743,6 +743,7 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_UMR_RESOURCES,
 	MLX5_IB_STAGE_DELAY_DROP,
 	MLX5_IB_STAGE_CLASS_ATTR,
+	MLX5_IB_STAGE_REP_REG,
 	MLX5_IB_STAGE_MAX,
 };
 
@@ -807,6 +808,7 @@ struct mlx5_ib_dev {
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
 	const struct mlx5_ib_profile	*profile;
+	struct mlx5_eswitch_rep		*rep;
 
 	/* protect the user_td */
 	struct mutex		lb_mutex;

commit 388ca8be00370db132464e27f745b8a0add19fcb
Author: Yonatan Cohen <yonatanc@mellanox.com>
Date:   Tue Jan 2 16:08:06 2018 +0200

    IB/mlx5: Implement fragmented completion queue (CQ)
    
    The current implementation of create CQ requires contiguous
    memory, such requirement is problematic once the memory is
    fragmented or the system is low in memory, it causes for
    failures in dma_zalloc_coherent().
    
    This patch implements new scheme of fragmented CQ to overcome
    this issue by introducing new type: 'struct mlx5_frag_buf_ctrl'
    to allocate fragmented buffers, rather than contiguous ones.
    
    Base the Completion Queues (CQs) on this new fragmented buffer.
    
    It fixes following crashes:
    kworker/29:0: page allocation failure: order:6, mode:0x80d0
    CPU: 29 PID: 8374 Comm: kworker/29:0 Tainted: G OE 3.10.0
    Workqueue: ib_cm cm_work_handler [ib_cm]
    Call Trace:
    [<>] dump_stack+0x19/0x1b
    [<>] warn_alloc_failed+0x110/0x180
    [<>] __alloc_pages_slowpath+0x6b7/0x725
    [<>] __alloc_pages_nodemask+0x405/0x420
    [<>] dma_generic_alloc_coherent+0x8f/0x140
    [<>] x86_swiotlb_alloc_coherent+0x21/0x50
    [<>] mlx5_dma_zalloc_coherent_node+0xad/0x110 [mlx5_core]
    [<>] ? mlx5_db_alloc_node+0x69/0x1b0 [mlx5_core]
    [<>] mlx5_buf_alloc_node+0x3e/0xa0 [mlx5_core]
    [<>] mlx5_buf_alloc+0x14/0x20 [mlx5_core]
    [<>] create_cq_kernel+0x90/0x1f0 [mlx5_ib]
    [<>] mlx5_ib_create_cq+0x3b0/0x4e0 [mlx5_ib]
    
    Signed-off-by: Yonatan Cohen <yonatanc@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 139385129973..eafb9751daf6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -371,7 +371,7 @@ struct mlx5_ib_qp {
 		struct mlx5_ib_rss_qp rss_qp;
 		struct mlx5_ib_dct dct;
 	};
-	struct mlx5_buf		buf;
+	struct mlx5_frag_buf	buf;
 
 	struct mlx5_db		db;
 	struct mlx5_ib_wq	rq;
@@ -413,7 +413,7 @@ struct mlx5_ib_qp {
 };
 
 struct mlx5_ib_cq_buf {
-	struct mlx5_buf		buf;
+	struct mlx5_frag_buf_ctrl fbc;
 	struct ib_umem		*umem;
 	int			cqe_size;
 	int			nent;
@@ -495,7 +495,7 @@ struct mlx5_ib_wc {
 struct mlx5_ib_srq {
 	struct ib_srq		ibsrq;
 	struct mlx5_core_srq	msrq;
-	struct mlx5_buf		buf;
+	struct mlx5_frag_buf	buf;
 	struct mlx5_db		db;
 	u64		       *wrid;
 	/* protect SRQ hanlding

commit beb801ac51be3e024edef435333198d59ccfbb8f
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jan 26 15:16:46 2018 -0700

    RDMA: Move enum ib_cq_creation_flags to uapi headers
    
    The flags field the enum is used with comes directly from the uapi
    so it belongs in the uapi headers for clarity and so userspace can
    use it.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 69a80f7512f0..139385129973 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1123,8 +1123,8 @@ static inline u32 check_cq_create_flags(u32 flags)
 	 * It returns non-zero value for unsupported CQ
 	 * create flags, otherwise it returns zero.
 	 */
-	return (flags & ~(IB_CQ_FLAGS_IGNORE_OVERRUN |
-			  IB_CQ_FLAGS_TIMESTAMP_COMPLETION));
+	return (flags & ~(IB_UVERBS_CQ_FLAGS_IGNORE_OVERRUN |
+			  IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION));
 }
 
 static inline int verify_assign_uidx(u8 cqe_version, u32 cmd_uidx,

commit 5c99eaecb1fce76e86cf74020624e36fbb63c3bf
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Tue Jan 16 20:08:41 2018 +0200

    IB/mlx5: Mmap the HCA's clock info to user-space
    
    This patch maps the new page to user space applications to
    allow converting a user space completion timestamp to system wall
    time at the lowest possible latency cost.
    By using a versioning scheme we allow compatibility between current
    and future userspace libraries.
    The change moves mlx5_ib_mmap_cmd enum from mlx5_ib.h to the
    abi header file mlx5-abi.h.
    
    Reviewed-by: Alex Vesker <valex@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Eitan Rabin <rabin@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 51228dfcfbe7..69a80f7512f0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -70,16 +70,6 @@ enum {
 	MLX5_IB_MMAP_CMD_MASK	= 0xff,
 };
 
-enum mlx5_ib_mmap_cmd {
-	MLX5_IB_MMAP_REGULAR_PAGE		= 0,
-	MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES	= 1,
-	MLX5_IB_MMAP_WC_PAGE			= 2,
-	MLX5_IB_MMAP_NC_PAGE			= 3,
-	/* 5 is chosen in order to be compatible with old versions of libmlx5 */
-	MLX5_IB_MMAP_CORE_CLOCK			= 5,
-	MLX5_IB_MMAP_ALLOC_WC			= 6,
-};
-
 enum {
 	MLX5_RES_SCAT_DATA32_CQE	= 0x1,
 	MLX5_RES_SCAT_DATA64_CQE	= 0x2,

commit aac4492ef23a176b6f1a41aadb99177eceb1fc06
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jan 4 17:25:40 2018 +0200

    IB/mlx5: Update counter implementation for dual port RoCE
    
    Update the counter interface for multiple ports. Some counter sets
    always comes from the primary device.
    
    Port specific counters should be accessed per mlx5_core_dev not always
    through the IB master mdev.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 92faba9a47af..51228dfcfbe7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -652,6 +652,7 @@ struct mlx5_ib_counters {
 	u32 num_q_counters;
 	u32 num_cong_counters;
 	u16 set_id;
+	bool set_id_valid;
 };
 
 struct mlx5_ib_multiport_info;

commit a9e546e73ace1ebfb80dc9b55b46ace306f684cd
Author: Parav Pandit <parav@mellanox.com>
Date:   Thu Jan 4 17:25:39 2018 +0200

    IB/mlx5: Change debugfs to have per port contents
    
    When there are multiple ports for single IB(RoCE) device, support
    debugfs entries to be available for each port.
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a70a4c02e396..92faba9a47af 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -665,6 +665,7 @@ struct mlx5_ib_multiport {
 struct mlx5_ib_port {
 	struct mlx5_ib_counters cnts;
 	struct mlx5_ib_multiport mp;
+	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
 };
 
 struct mlx5_roce {
@@ -684,6 +685,7 @@ struct mlx5_ib_dbg_param {
 	int			offset;
 	struct mlx5_ib_dev	*dev;
 	struct dentry		*dentry;
+	u8			port_num;
 };
 
 enum mlx5_ib_dbg_cc_types {
@@ -813,7 +815,6 @@ struct mlx5_ib_dev {
 	struct mlx5_sq_bfreg	bfreg;
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
-	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
 	const struct mlx5_ib_profile	*profile;
 
 	/* protect the user_td */
@@ -1071,8 +1072,8 @@ __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
 int mlx5_get_roce_gid_type(struct mlx5_ib_dev *dev, u8 port_num,
 			   int index, enum ib_gid_type *gid_type);
 
-void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev);
-int mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev);
+void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
+int mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev, u8 port_num);
 
 /* GSI QP helper functions */
 struct ib_qp *mlx5_ib_gsi_create_qp(struct ib_pd *pd,

commit 32f69e4be269739c3850cd20f1a3322e95c1145f
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jan 4 17:25:36 2018 +0200

    {net, IB}/mlx5: Manage port association for multiport RoCE
    
    When mlx5_ib_add is called determine if the mlx5 core device being
    added is capable of dual port RoCE operation. If it is, determine
    whether it is a master device or a slave device using the
    num_vhca_ports and affiliate_nic_vport_criteria capabilities.
    
    If the device is a slave, attempt to find a master device to affiliate it
    with. Devices that can be affiliated will share a system image guid. If
    none are found place it on a list of unaffiliated ports. If a master is
    found bind the port to it by configuring the port affiliation in the NIC
    vport context.
    
    Similarly when mlx5_ib_remove is called determine the port type. If it's
    a slave port, unaffiliate it from the master device, otherwise just
    remove it from the unaffiliated port list.
    
    The IB device is registered as a multiport device, even if a 2nd port is
    not available for affiliation. When the 2nd port is affiliated later the
    GID cache must be refreshed in order to get the default GIDs for the 2nd
    port in the cache. Export roce_rescan_device to provide a mechanism to
    refresh the cache after a new port is bound.
    
    In a multiport configuration all IB object (QP, MR, PD, etc) related
    commands should flow through the master mlx5_core_dev, other commands
    must be sent to the slave port mlx5_core_mdev, an interface is provide
    to get the correct mdev for non IB object commands.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6106dde35144..a70a4c02e396 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -654,8 +654,17 @@ struct mlx5_ib_counters {
 	u16 set_id;
 };
 
+struct mlx5_ib_multiport_info;
+
+struct mlx5_ib_multiport {
+	struct mlx5_ib_multiport_info *mpi;
+	/* To be held when accessing the multiport info */
+	spinlock_t mpi_lock;
+};
+
 struct mlx5_ib_port {
 	struct mlx5_ib_counters cnts;
+	struct mlx5_ib_multiport mp;
 };
 
 struct mlx5_roce {
@@ -756,6 +765,17 @@ struct mlx5_ib_profile {
 	struct mlx5_ib_stage stage[MLX5_IB_STAGE_MAX];
 };
 
+struct mlx5_ib_multiport_info {
+	struct list_head list;
+	struct mlx5_ib_dev *ibdev;
+	struct mlx5_core_dev *mdev;
+	struct completion unref_comp;
+	u64 sys_image_guid;
+	u32 mdev_refcnt;
+	bool is_master;
+	bool unaffiliate;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -800,6 +820,8 @@ struct mlx5_ib_dev {
 	struct mutex		lb_mutex;
 	u32			user_td;
 	u8			umr_fence;
+	struct list_head	ib_dev_list;
+	u64			sys_image_guid;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
@@ -1071,6 +1093,12 @@ int mlx5_ib_generate_wc(struct ib_cq *ibcq, struct ib_wc *wc);
 
 void mlx5_ib_free_bfreg(struct mlx5_ib_dev *dev, struct mlx5_bfreg_info *bfregi,
 			int bfregn);
+struct mlx5_ib_dev *mlx5_ib_get_ibdev_from_mpi(struct mlx5_ib_multiport_info *mpi);
+struct mlx5_core_dev *mlx5_ib_get_native_port_mdev(struct mlx5_ib_dev *dev,
+						   u8 ib_port_num,
+						   u8 *native_port_num);
+void mlx5_ib_put_native_port_mdev(struct mlx5_ib_dev *dev,
+				  u8 port_num);
 
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit 7fd8aefb7ce202dd9d97f752bf249be6215f1004
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jan 4 17:25:35 2018 +0200

    IB/mlx5: Make netdev notifications multiport capable
    
    When multiple RoCE ports are supported registration for events on
    multiple netdevs is required. Refactor the event registration and
    handling to support multiple ports.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 57405cf8a5c5..6106dde35144 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -667,6 +667,8 @@ struct mlx5_roce {
 	struct notifier_block	nb;
 	atomic_t		next_port;
 	enum ib_port_state last_port_state;
+	struct mlx5_ib_dev	*dev;
+	u8			native_port_num;
 };
 
 struct mlx5_ib_dbg_param {
@@ -757,7 +759,7 @@ struct mlx5_ib_profile {
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
-	struct mlx5_roce		roce;
+	struct mlx5_roce		roce[MLX5_MAX_PORTS];
 	int				num_ports;
 	/* serialize update of capability mask
 	 */

commit 776a3906b692963586ee9952e64ed87fb4b401c6
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 2 16:19:33 2018 +0200

    IB/mlx5: Add support for DC target QP
    
    A DC Target (DCT) QP is represented in the hardware as a unique object.
    This object is created by CREATE_DCT command and destroyed by DESTROY_DCT
    command. However, in the driver we describe it as a QP.
    
    The hardware command that creates a DCT needs parameters that the verb
    create_qp() does not provide. Those remaining parameters are provided
    with the call to the verb modify_qp(). Therefore we delay the actual
    creation of a DCT in the hardware until the stage of modify_qp() to RTR.
    
    A support for query_qp() was added as well. It uses QUERY_DCT command to
    retrieve the applicable fields.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6286992e1d39..57405cf8a5c5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1002,6 +1002,8 @@ struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
 						      struct ib_rwq_ind_table_init_attr *init_attr,
 						      struct ib_udata *udata);
 int mlx5_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
+bool mlx5_ib_dc_atomic_is_supported(struct mlx5_ib_dev *dev);
+
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);

commit b4aaa1f0b415cf8aa79742cbed56a2d75cfc5102
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue Jan 2 16:19:31 2018 +0200

    IB/mlx5: Handle type IB_QPT_DRIVER when creating a QP
    
    The QP type IB_QPT_DRIVER doesn't describe the transport or the service
    that the QP provides but those are known only to the hardware driver.
    The actual type of the QP is stored in the hardware driver context (i.e.
    mlx5_qp) under the field qp_sub_type.
    
    Take the real QP type and any extra data that is required to create the QP
    from the driver channel and modify the QP initial attributes before continuing
    with create_qp().
    
    Downstream patches from this series will add support for both DCI and
    DCT driver QPs.
    
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b3f2f5cae672..6286992e1d39 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -206,6 +206,8 @@ struct mlx5_ib_flow_db {
  * creates the actual hardware QP.
  */
 #define MLX5_IB_QPT_HW_GSI	IB_QPT_RESERVED2
+#define MLX5_IB_QPT_DCI		IB_QPT_RESERVED3
+#define MLX5_IB_QPT_DCT		IB_QPT_RESERVED4
 #define MLX5_IB_WR_UMR		IB_WR_RESERVED1
 
 #define MLX5_IB_UMR_OCTOWORD	       16
@@ -366,12 +368,18 @@ struct mlx5_bf {
 	struct mlx5_sq_bfreg   *bfreg;
 };
 
+struct mlx5_ib_dct {
+	struct mlx5_core_dct    mdct;
+	u32                     *in;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
 	union {
 		struct mlx5_ib_qp_trans trans_qp;
 		struct mlx5_ib_raw_packet_qp raw_packet_qp;
 		struct mlx5_ib_rss_qp rss_qp;
+		struct mlx5_ib_dct dct;
 	};
 	struct mlx5_buf		buf;
 
@@ -410,6 +418,8 @@ struct mlx5_ib_qp {
 	u32			rate_limit;
 	u32                     underlay_qpn;
 	bool			tunnel_offload_en;
+	/* storage for qp sub type when core qp type is IB_QPT_DRIVER */
+	enum ib_qp_type		qp_sub_type;
 };
 
 struct mlx5_ib_cq_buf {

commit 3cc297db970762024109f75ce289078f8479a2f8
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Jan 1 13:07:03 2018 +0200

    IB/mlx5: Move locks initialization to the corresponding stage
    
    Unconditional locks/list and ODP srcu initialization should be done in
    the INIT stage. Remove those from the CAPS stage and move them to the
    proper stage.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 768fa7334100..b3f2f5cae672 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -998,7 +998,6 @@ void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);
 void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
 		    struct mlx5_pagefault *pfault);
 int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
-void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
 void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
@@ -1013,7 +1012,6 @@ static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 }
 
 static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
-static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	    {}
 static inline int mlx5_ib_odp_init(void) { return 0; }
 static inline void mlx5_ib_odp_cleanup(void)				    {}
 static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}

commit c8b8992446a945c103ac74ebd5e05672d9b3c48a
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Jan 1 13:07:02 2018 +0200

    IB/mlx5: Move loopback initialization to the corresponding stage
    
    The loopback stage only initializes a lock, move it to be in
    the CAPS initialization phase and get rid loopback step completely.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0504ab3b9a06..768fa7334100 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -729,7 +729,6 @@ enum mlx5_ib_stages {
 	MLX5_IB_STAGE_UMR_RESOURCES,
 	MLX5_IB_STAGE_DELAY_DROP,
 	MLX5_IB_STAGE_CLASS_ATTR,
-	MLX5_IB_STAGE_LOOPBACK,
 	MLX5_IB_STAGE_MAX,
 };
 

commit 16c1975f1032330d54979fbc83c5f767afce67a9
Author: Mark Bloch <markb@mellanox.com>
Date:   Mon Jan 1 13:06:58 2018 +0200

    IB/mlx5: Create profile infrastructure to add and remove stages
    
    Today we have single function which is used when we add an IB interface,
    break this function into multiple functions.
    
    Create stages and a generic mechanism to execute each stage.
    This is in preparation for RDMA/IB representors which might not need
    all stages or will do things differently in some of the stages.
    
    This patch doesn't change any functionality.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5fabd5807db6..0504ab3b9a06 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -715,6 +715,36 @@ struct mlx5_ib_delay_drop {
 	struct mlx5_ib_dbg_delay_drop *dbg;
 };
 
+enum mlx5_ib_stages {
+	MLX5_IB_STAGE_INIT,
+	MLX5_IB_STAGE_CAPS,
+	MLX5_IB_STAGE_ROCE,
+	MLX5_IB_STAGE_DEVICE_RESOURCES,
+	MLX5_IB_STAGE_ODP,
+	MLX5_IB_STAGE_COUNTERS,
+	MLX5_IB_STAGE_CONG_DEBUGFS,
+	MLX5_IB_STAGE_UAR,
+	MLX5_IB_STAGE_BFREG,
+	MLX5_IB_STAGE_IB_REG,
+	MLX5_IB_STAGE_UMR_RESOURCES,
+	MLX5_IB_STAGE_DELAY_DROP,
+	MLX5_IB_STAGE_CLASS_ATTR,
+	MLX5_IB_STAGE_LOOPBACK,
+	MLX5_IB_STAGE_MAX,
+};
+
+struct mlx5_ib_stage {
+	int (*init)(struct mlx5_ib_dev *dev);
+	void (*cleanup)(struct mlx5_ib_dev *dev);
+};
+
+#define STAGE_CREATE(_stage, _init, _cleanup) \
+	.stage[_stage] = {.init = _init, .cleanup = _cleanup}
+
+struct mlx5_ib_profile {
+	struct mlx5_ib_stage stage[MLX5_IB_STAGE_MAX];
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -753,6 +783,7 @@ struct mlx5_ib_dev {
 	struct mlx5_sq_bfreg	fp_bfreg;
 	struct mlx5_ib_delay_drop	delay_drop;
 	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
+	const struct mlx5_ib_profile	*profile;
 
 	/* protect the user_td */
 	struct mutex		lb_mutex;

commit 1ee47ab3e8d868185ec9a0bfe5da2a7f502c04ab
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Dec 24 16:31:36 2017 +0200

    IB/mlx5: Enable QP creation with a given blue flame index
    
    This patch enables QP creation with a given BF index, this allows the
    user space driver to share same BF between few QPs or alternatively have
    a dedicated BF per QP.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6c0bdc9a92c4..5fabd5807db6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -115,6 +115,7 @@ enum {
 
 enum {
 	MLX5_IB_INVALID_UAR_INDEX	= BIT(31),
+	MLX5_IB_INVALID_BFREG		= BIT(31),
 };
 
 struct mlx5_ib_vma_private_data {

commit 4ed131d0bb1597ce12fff22d9d7fc9720a6e8cf0
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Dec 24 16:31:35 2017 +0200

    IB/mlx5: Expose dynamic mmap allocation
    
    This patch exposes the option to dynamic allocates a UAR, this
    functionality will be used in downstream patch in this series as
    part of QP creation.
    
    Specifically, the user space driver asks for a UAR allocation in a given
    page index, upon success this UAR and its bfregs can be used as part of
    QP creation by the user space driver.
    
    To enable allocating more than 256 UARs the page index is encoded in an
    extra one byte just after the command byte.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 330c69cb87df..6c0bdc9a92c4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -77,6 +77,7 @@ enum mlx5_ib_mmap_cmd {
 	MLX5_IB_MMAP_NC_PAGE			= 3,
 	/* 5 is chosen in order to be compatible with old versions of libmlx5 */
 	MLX5_IB_MMAP_CORE_CLOCK			= 5,
+	MLX5_IB_MMAP_ALLOC_WC			= 6,
 };
 
 enum {
@@ -112,6 +113,10 @@ enum {
 	MLX5_TM_MAX_SGE			= 1,
 };
 
+enum {
+	MLX5_IB_INVALID_UAR_INDEX	= BIT(31),
+};
+
 struct mlx5_ib_vma_private_data {
 	struct list_head list;
 	struct vm_area_struct *vma;
@@ -1021,6 +1026,9 @@ void mlx5_ib_gsi_pkey_change(struct mlx5_ib_gsi_qp *gsi);
 
 int mlx5_ib_generate_wc(struct ib_cq *ibcq, struct ib_wc *wc);
 
+void mlx5_ib_free_bfreg(struct mlx5_ib_dev *dev, struct mlx5_bfreg_info *bfregi,
+			int bfregn);
+
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;

commit 31a78a5a7983141c17852d31eb3a1f70d8161225
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Dec 24 16:31:34 2017 +0200

    IB/mlx5: Extend UAR stuff to support dynamic allocation
    
    This patch extends the alloc context flow to be prepared for working
    with dynamic UAR allocations.
    
    Currently upon alloc context there is some fix size of UARs that are
    allocated (named 'static allocation') and there is no option to user
    application to ask for more or control which UAR will be used by which
    QP.
    
    In this patch the driver prepares its data structures to manage both the
    static and the dynamic allocations and let the user driver knows about
    the max value of dynamic blue-flame registers that are allowed.
    
    Downstream patches from this series will enable the dynamic allocation
    and the association as part of QP creation.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2c5f3533bbc9..330c69cb87df 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1113,10 +1113,10 @@ static inline int get_uars_per_sys_page(struct mlx5_ib_dev *dev, bool lib_suppor
 				MLX5_UARS_IN_PAGE : 1;
 }
 
-static inline int get_num_uars(struct mlx5_ib_dev *dev,
-			       struct mlx5_bfreg_info *bfregi)
+static inline int get_num_static_uars(struct mlx5_ib_dev *dev,
+				      struct mlx5_bfreg_info *bfregi)
 {
-	return get_uars_per_sys_page(dev, bfregi->lib_uar_4k) * bfregi->num_sys_pages;
+	return get_uars_per_sys_page(dev, bfregi->lib_uar_4k) * bfregi->num_static_sys_pages;
 }
 
 #endif /* MLX5_IB_H */

commit ad9a3668a434faca1339789ed2f043d679199309
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Sun Dec 24 13:54:56 2017 +0200

    IB/mlx5: Serialize access to the VMA list
    
    User-space applications can do mmap and munmap directly at
    any time.
    
    Since the VMA list is not protected with a mutex, concurrent
    accesses to the VMA list from the mmap and munmap can cause
    data corruption. Add a mutex around the list.
    
    Cc: <stable@vger.kernel.org> # v4.7
    Fixes: 7c2344c3bbf9 ("IB/mlx5: Implements disassociate_ucontext API")
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6dd8cac78de2..2c5f3533bbc9 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -115,6 +115,8 @@ enum {
 struct mlx5_ib_vma_private_data {
 	struct list_head list;
 	struct vm_area_struct *vma;
+	/* protect vma_private_list add/del */
+	struct mutex *vma_private_list_mutex;
 };
 
 struct mlx5_ib_ucontext {
@@ -129,6 +131,8 @@ struct mlx5_ib_ucontext {
 	/* Transport Domain number */
 	u32			tdn;
 	struct list_head	vma_private_list;
+	/* protect vma_private_list add/del */
+	struct mutex		vma_private_list_mutex;
 
 	unsigned long		upd_xlt_page;
 	/* protect ODP/KSM */

commit b1383aa64121778d9419cc982e387e27d9e96c64
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Sun Oct 29 13:59:45 2017 +0200

    IB/mlx5: Add PCI write end padding support
    
    Add the PCI write end padding flag to device_cap_flags enum and set it
    during mlx5_ib_query_device so it will be reported to user-space.
    
    During WQ/QP creation, set that capability for WQ/QP if user requested
    it and HW supports it.
    
    PCI write end padding modification is not supported for now. There's no
    such flag for a QP but for a WQ, create and modify use the same flag.
    Return an error if PCI write end padding flag is set during modify_wq.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0a328d6c6494..6dd8cac78de2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -228,6 +228,7 @@ struct wr_list {
 
 enum mlx5_ib_rq_flags {
 	MLX5_IB_RQ_CVLAN_STRIPPING	= 1 << 0,
+	MLX5_IB_RQ_PCI_WRITE_END_PADDING	= 1 << 1,
 };
 
 struct mlx5_ib_wq {
@@ -421,7 +422,7 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_RSS				= 1 << 8,
 	MLX5_IB_QP_CVLAN_STRIPPING		= 1 << 9,
 	MLX5_IB_QP_UNDERLAY			= 1 << 10,
-	/* Reserved for PCI_WRITE_PAD 		= 1 << 11, */
+	MLX5_IB_QP_PCI_WRITE_END_PADDING	= 1 << 11,
 	MLX5_IB_QP_TUNNEL_OFFLOAD		= 1 << 12,
 };
 

commit f95ef6cbae61fa1dd563f5c0f6a0e5b512fda5ba
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Oct 19 08:25:55 2017 +0300

    IB/mlx5: Add tunneling offloads support
    
    The device can support receive Stateless Offloads for the inner
    packet's fields only when the packet is processed by TIR which is
    enabled to support tunneling. Otherwise, the device treats the
    packet as an ordinary non-tunneling packet and receive offloads
    can be done only for the outer packet's field.
    In order to enable receive Stateless Offloading support for incoming
    tunneling traffic the TIR should be created with tunneled_offload_en.
    Tunneling offloads is supported only be raw ethernet QP.
    
    This patch includes:
    * New QP creation flag for tunneling offloads.
    * Reports device capabilities.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 137f2116911f..0a328d6c6494 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -398,6 +398,7 @@ struct mlx5_ib_qp {
 	struct list_head	cq_send_list;
 	u32			rate_limit;
 	u32                     underlay_qpn;
+	bool			tunnel_offload_en;
 };
 
 struct mlx5_ib_cq_buf {
@@ -420,6 +421,8 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_RSS				= 1 << 8,
 	MLX5_IB_QP_CVLAN_STRIPPING		= 1 << 9,
 	MLX5_IB_QP_UNDERLAY			= 1 << 10,
+	/* Reserved for PCI_WRITE_PAD 		= 1 << 11, */
+	MLX5_IB_QP_TUNNEL_OFFLOAD		= 1 << 12,
 };
 
 struct mlx5_umr_wr {

commit 7a0c8f4244e9ec7a630563d294b211342b46223d
Author: Guy Levi <guyle@mellanox.com>
Date:   Thu Oct 19 08:25:53 2017 +0300

    IB/mlx5: Support padded 128B CQE feature
    
    In some benchmarks and some CPU architectures, writing the CQE on a full
    cache line size improves performance by saving memory access operations
    (read-modify-write) relative to partial cache line change. This patch
    lets the user to configure the device to pad the CQE up to 128B in case
    its content is less than 128B. Currently the driver supports only padding
    for a CQE size of 128B.
    
    Signed-off-by: Guy Levi <guyle@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e7deaa08535b..137f2116911f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -444,6 +444,10 @@ struct mlx5_shared_mr_info {
 	struct ib_umem		*umem;
 };
 
+enum mlx5_ib_cq_pr_flags {
+	MLX5_IB_CQ_PR_FLAGS_CQE_128_PAD	= 1 << 0,
+};
+
 struct mlx5_ib_cq {
 	struct ib_cq		ibcq;
 	struct mlx5_core_cq	mcq;
@@ -466,6 +470,7 @@ struct mlx5_ib_cq {
 	struct list_head	wc_list;
 	enum ib_cq_notify_flags notify_flags;
 	struct work_struct	notify_work;
+	u16			private_flags; /* Use mlx5_ib_cq_pr_flags */
 };
 
 struct mlx5_ib_wc {

commit ccc8708790273811db24676223b040710793cba7
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Tue Oct 17 18:01:13 2017 +0300

    IB/mlx5: Allow creation of a multi-packet RQ
    
    Allow creation of a multi-packet receive queue.
    
    In order to create a multi-packet RQ, the following fields in
    the mlx5_ib_rwq should be set:
    - log_num_strides: Log of number of strides per WQE
    - single_stride_log_num_of_bytes: Log of a single stride size
    - two_byte_shift_en: When enabled, hardware pads 2 bytes of zeros
      before writing the message to memory (e.g. for the IP alignment).
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8de40852818b..e7deaa08535b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -254,6 +254,7 @@ struct mlx5_ib_wq {
 
 enum mlx5_ib_wq_flags {
 	MLX5_IB_WQ_FLAGS_DELAY_DROP = 0x1,
+	MLX5_IB_WQ_FLAGS_STRIDING_RQ = 0x2,
 };
 
 #define MLX5_MIN_SINGLE_WQE_LOG_NUM_STRIDES 9
@@ -269,6 +270,9 @@ struct mlx5_ib_rwq {
 	u32			log_rq_size;
 	u32			rq_page_offset;
 	u32			log_page_size;
+	u32			log_num_strides;
+	u32			two_byte_shift_en;
+	u32			single_stride_log_num_of_bytes;
 	struct ib_umem		*umem;
 	size_t			buf_size;
 	unsigned int		page_shift;

commit b4f34597a5ce148b88a47da621037537c384d565
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Tue Oct 17 18:01:12 2017 +0300

    IB/mlx5: Expose multi-packet RQ capabilities
    
    This patch reports the device's striding RQ capabilities to
    the user-space:
    - min/max_single_stride_log_num_of_bytes: Log of min/max number of
      bytes in a single stride.
    - min/max_single_wqe_log_num_of_strides: Log of min/max number of
      strides in a single WQE.
    - supported_qpts: A bit mask to know which QP types support multi-
      packet RQ, for now only Raw Packet QPs.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 189e80cd6b2f..8de40852818b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -256,6 +256,11 @@ enum mlx5_ib_wq_flags {
 	MLX5_IB_WQ_FLAGS_DELAY_DROP = 0x1,
 };
 
+#define MLX5_MIN_SINGLE_WQE_LOG_NUM_STRIDES 9
+#define MLX5_MAX_SINGLE_WQE_LOG_NUM_STRIDES 16
+#define MLX5_MIN_SINGLE_STRIDE_LOG_NUM_BYTES 6
+#define MLX5_MAX_SINGLE_STRIDE_LOG_NUM_BYTES 13
+
 struct mlx5_ib_rwq {
 	struct ib_wq		ibwq;
 	struct mlx5_core_qp	core_qp;

commit eb761894351d0372248f2636c213d7b822e8775f
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Thu Aug 17 15:52:09 2017 +0300

    IB/mlx5: Fill XRQ capabilities
    
    Provide driver specific values for XRQ capabilities.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Reviewed-by: Yossi Itigin <yosefe@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b3380e8beacf..189e80cd6b2f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -107,6 +107,11 @@ enum {
 	MLX5_CQE_VERSION_V1,
 };
 
+enum {
+	MLX5_TM_MAX_RNDV_MSG_SIZE	= 64,
+	MLX5_TM_MAX_SGE			= 1,
+};
+
 struct mlx5_ib_vma_private_data {
 	struct list_head list;
 	struct vm_area_struct *vma;

commit 8b7ff7f3b301de52924cb2cf3fed47b181893116
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Thu Aug 17 15:52:29 2017 +0300

    IB/mlx5: Enable UMR for MRs created with reg_create
    
    This patch is the first step in decoupling UMR usage and
    allocation from the MR cache. The only functional change
    in this patch is to enables UMR for MRs created with
    reg_create.
    
    This change fixes a bug where ODP memory regions that
    were not allocated from the MR cache did not have UMR
    enabled.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7ac991070020..b3380e8beacf 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -503,7 +503,7 @@ struct mlx5_ib_mr {
 	struct mlx5_shared_mr_info	*smr_info;
 	struct list_head	list;
 	int			order;
-	int			umred;
+	bool			allocated_from_cache;
 	int			npages;
 	struct mlx5_ib_dev     *dev;
 	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];

commit c2e53b2ce1ba351918ede492c0cb207f42e1228f
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Thu Jun 8 16:15:08 2017 +0300

    IB/mlx5: Add support for QP with a given source QPN
    
    Allow user space applications to accelerate send and receive
    traffic which is typically handled by IPoIB ULP by creating
    a UD QP with a given source QPN of the IPoIB UD QP.
    
    UD QP with a given source QPN should basically be similar to
    RAW QP from point of view of its created resources.
    
    However,
    - Its TIS should point to the source QPN.
    - Modify can be done only on its state as the transport attributes
      are managed by its source QP.
    
    This patch manages below:
    - Creating/destroying/modifying UD QP with a given source QPN.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0316147a39a8..7ac991070020 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -383,6 +383,7 @@ struct mlx5_ib_qp {
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;
 	u32			rate_limit;
+	u32                     underlay_qpn;
 };
 
 struct mlx5_ib_cq_buf {
@@ -404,6 +405,7 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
 	MLX5_IB_QP_RSS				= 1 << 8,
 	MLX5_IB_QP_CVLAN_STRIPPING		= 1 << 9,
+	MLX5_IB_QP_UNDERLAY			= 1 << 10,
 };
 
 struct mlx5_umr_wr {

commit fe248c3a5837848717ed566fb4aefe66f43a5e53
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue May 30 10:29:14 2017 +0300

    IB/mlx5: Add delay drop configuration and statistics
    
    Add debugfs interface for monitor the number of delay drop timeout
    events and the number of existing dropless RQs in the system.
    
    In addition add debugfs interface for configuring the global timeout value
    which is used in the SET_DELAY_DROP command.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 097f12dc65a6..0316147a39a8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -661,6 +661,13 @@ enum {
 	MLX5_MAX_DELAY_DROP_TIMEOUT_MS = 100,
 };
 
+struct mlx5_ib_dbg_delay_drop {
+	struct dentry		*dir_debugfs;
+	struct dentry		*rqs_cnt_debugfs;
+	struct dentry		*events_cnt_debugfs;
+	struct dentry		*timeout_debugfs;
+};
+
 struct mlx5_ib_delay_drop {
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	delay_drop_work;
@@ -668,6 +675,9 @@ struct mlx5_ib_delay_drop {
 	struct mutex		lock;
 	u32			timeout;
 	bool			activate;
+	atomic_t		events_cnt;
+	atomic_t		rqs_cnt;
+	struct mlx5_ib_dbg_delay_drop *dbg;
 };
 
 struct mlx5_ib_dev {

commit 03404e8ae652e02a5e3388224836cef53d7a0988
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Tue May 30 10:29:13 2017 +0300

    IB/mlx5: Add support to dropless RQ
    
    RQs that were configured for "delay drop" will prevent packet drops
    when their WQEs are depleted.
    Marking an RQ to be drop-less is done by setting delay_drop_en in RQ
    context using CREATE_RQ command.
    
    Since this feature is globally activated/deactivated by using the
    SET_DELAY_DROP command on all the marked RQs, we activated/deactivated
    it according to the number of RQs with 'delay_drop' enabled.
    
    When timeout is expired, then the feature is deactivated. Therefore
    the driver handles the delay drop timeout event and reactivate it.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f0682f383b52..097f12dc65a6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -247,6 +247,10 @@ struct mlx5_ib_wq {
 	void		       *qend;
 };
 
+enum mlx5_ib_wq_flags {
+	MLX5_IB_WQ_FLAGS_DELAY_DROP = 0x1,
+};
+
 struct mlx5_ib_rwq {
 	struct ib_wq		ibwq;
 	struct mlx5_core_qp	core_qp;
@@ -264,6 +268,7 @@ struct mlx5_ib_rwq {
 	u32			wqe_count;
 	u32			wqe_shift;
 	int			wq_sig;
+	u32			create_flags; /* Use enum mlx5_ib_wq_flags */
 };
 
 enum {
@@ -652,6 +657,19 @@ struct mlx5_ib_dbg_cc_params {
 	struct mlx5_ib_dbg_param	params[MLX5_IB_DBG_CC_MAX];
 };
 
+enum {
+	MLX5_MAX_DELAY_DROP_TIMEOUT_MS = 100,
+};
+
+struct mlx5_ib_delay_drop {
+	struct mlx5_ib_dev     *dev;
+	struct work_struct	delay_drop_work;
+	/* serialize setting of delay drop */
+	struct mutex		lock;
+	u32			timeout;
+	bool			activate;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -688,6 +706,7 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_port	*port;
 	struct mlx5_sq_bfreg	bfreg;
 	struct mlx5_sq_bfreg	fp_bfreg;
+	struct mlx5_ib_delay_drop	delay_drop;
 	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
 
 	/* protect the user_td */

commit 4a2da0b8c0782816f3ae6846ae7942fcbb0f8172
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue May 30 10:05:15 2017 +0300

    IB/mlx5: Add debug control parameters for congestion control
    
    This patch adds debug control parameters for congestion control which
    can be read or written through debugfs. They are for reaction point and
    notification point nodes.
    
    These control parameters are as below:
     +------------------------------+-----------------------------------------+
     |      Name                    |           Description                   |
     |------------------------------+-----------------------------------------|
     |rp_clamp_tgt_rate             | When set target rate is updated to      |
     |                              | current rate                            |
     |------------------------------+-----------------------------------------|
     |rp_clamp_tgt_rate_ati         | When set update target rate based on    |
     |                              | timer as well                           |
     |------------------------------+-----------------------------------------|
     |rp_time_reset                 | time between rate increase if no        |
     |                              | CNP is received unit in usec            |
     |------------------------------+-----------------------------------------|
     |rp_byte_reset                 | Number of bytes between rate inease if  |
     |                              | no CNP is received                      |
     |------------------------------+-----------------------------------------|
     |rp_threshold                  | Threshold for reaction point rate       |
     |                              | control                                 |
     |------------------------------+-----------------------------------------|
     |rp_ai_rate                    | Rate for target rate, unit in Mbps      |
     |------------------------------+-----------------------------------------|
     |rp_hai_rate                   | Rate for hyper increase state           |
     |                              | unit in Mbps                            |
     |------------------------------+-----------------------------------------|
     |rp_min_dec_fac                | Minimum factor by which the current     |
     |                              | transmit rate can be changed when       |
     |                              | processing a CNP, unit is percerntage   |
     |------------------------------+-----------------------------------------|
     |rp_min_rate                   | Minimum value for rate limit,           |
     |                              | unit in Mbps                            |
     |------------------------------+-----------------------------------------|
     |rp_rate_to_set_on_first_cnp   | Rate that is set when first CNP is      |
     |                              | received, unit is Mbps                  |
     |------------------------------+-----------------------------------------|
     |rp_dce_tcp_g                  | Used to calculate alpha                 |
     |------------------------------+-----------------------------------------|
     |rp_dce_tcp_rtt                | Time between updates of alpha value,    |
     |                              | unit is usec                            |
     |------------------------------+-----------------------------------------|
     |rp_rate_reduce_monitor_period | Minimum time between consecutive rate   |
     |                              | reductions                              |
     |------------------------------+-----------------------------------------|
     |rp_initial_alpha_value        | Initial value of alpha                  |
     |------------------------------+-----------------------------------------|
     |rp_gd                         | When CNP is received, flow rate is      |
     |                              | reduced based on gd, rp_gd is given as  |
     |                              | log2(rp_gd)                             |
     |------------------------------+-----------------------------------------|
     |np_cnp_dscp                   | dscp code point for generated cnp       |
     |------------------------------+-----------------------------------------|
     |np_cnp_prio_mode              | 802.1p priority for generated cnp       |
     |------------------------------+-----------------------------------------|
     |np_cnp_prio                   | cnp priority mode                       |
     +------------------------------+-----------------------------------------+
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5fb4547bbcb8..f0682f383b52 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -619,6 +619,39 @@ struct mlx5_roce {
 	enum ib_port_state last_port_state;
 };
 
+struct mlx5_ib_dbg_param {
+	int			offset;
+	struct mlx5_ib_dev	*dev;
+	struct dentry		*dentry;
+};
+
+enum mlx5_ib_dbg_cc_types {
+	MLX5_IB_DBG_CC_RP_CLAMP_TGT_RATE,
+	MLX5_IB_DBG_CC_RP_CLAMP_TGT_RATE_ATI,
+	MLX5_IB_DBG_CC_RP_TIME_RESET,
+	MLX5_IB_DBG_CC_RP_BYTE_RESET,
+	MLX5_IB_DBG_CC_RP_THRESHOLD,
+	MLX5_IB_DBG_CC_RP_AI_RATE,
+	MLX5_IB_DBG_CC_RP_HAI_RATE,
+	MLX5_IB_DBG_CC_RP_MIN_DEC_FAC,
+	MLX5_IB_DBG_CC_RP_MIN_RATE,
+	MLX5_IB_DBG_CC_RP_RATE_TO_SET_ON_FIRST_CNP,
+	MLX5_IB_DBG_CC_RP_DCE_TCP_G,
+	MLX5_IB_DBG_CC_RP_DCE_TCP_RTT,
+	MLX5_IB_DBG_CC_RP_RATE_REDUCE_MONITOR_PERIOD,
+	MLX5_IB_DBG_CC_RP_INITIAL_ALPHA_VALUE,
+	MLX5_IB_DBG_CC_RP_GD,
+	MLX5_IB_DBG_CC_NP_CNP_DSCP,
+	MLX5_IB_DBG_CC_NP_CNP_PRIO_MODE,
+	MLX5_IB_DBG_CC_NP_CNP_PRIO,
+	MLX5_IB_DBG_CC_MAX,
+};
+
+struct mlx5_ib_dbg_cc_params {
+	struct dentry			*root;
+	struct mlx5_ib_dbg_param	params[MLX5_IB_DBG_CC_MAX];
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
@@ -655,6 +688,7 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_port	*port;
 	struct mlx5_sq_bfreg	bfreg;
 	struct mlx5_sq_bfreg	fp_bfreg;
+	struct mlx5_ib_dbg_cc_params	*dbg_cc_params;
 
 	/* protect the user_td */
 	struct mutex		lb_mutex;
@@ -909,6 +943,9 @@ __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
 int mlx5_get_roce_gid_type(struct mlx5_ib_dev *dev, u8 port_num,
 			   int index, enum ib_gid_type *gid_type);
 
+void mlx5_ib_cleanup_cong_debugfs(struct mlx5_ib_dev *dev);
+int mlx5_ib_init_cong_debugfs(struct mlx5_ib_dev *dev);
+
 /* GSI QP helper functions */
 struct ib_qp *mlx5_ib_gsi_create_qp(struct ib_pd *pd,
 				    struct ib_qp_init_attr *init_attr);

commit fd65f1b8eefdd20bd70cc920a7306e1e1b113d81
Author: Moni Shoua <monis@mellanox.com>
Date:   Tue May 30 09:56:05 2017 +0300

    IB/mlx5: Change logic for dispatching IB events for port state
    
    The old logic ignored link state. This led to missing IB events like
    when link goes down on the switch while admin state is up or to redundant
    events like when admin state goes up while link is down.
    To fix that, probe the port state on NETDEV events and compare to last
    known state to decide if IB events needs to be dispatched.
    
    FIxes: 5ec8c83e3ad3 ("IB/mlx5: Port events in RoCE now rely on netdev events")
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Noa Osherovich <noaos@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d2b27647f939..5fb4547bbcb8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -616,6 +616,7 @@ struct mlx5_roce {
 	struct net_device	*netdev;
 	struct notifier_block	nb;
 	atomic_t		next_port;
+	enum ib_port_state last_port_state;
 };
 
 struct mlx5_ib_dev {

commit c85023e153e3824661d07307138fdeff41f6d86a
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Tue May 30 09:42:54 2017 +0300

    IB/mlx5: Add raw ethernet local loopback support
    
    Currently, unicast/multicast loopback raw ethernet
    (non-RDMA) packets are sent back to the vport.
    A unicast loopback packet is the packet with destination
    MAC address the same as the source MAC address.
    For multicast, the destination MAC address is in the
    vport's multicast filter list.
    
    Moreover, the local loopback is not needed if
    there is one or none user space context.
    
    After this patch, the raw ethernet unicast and multicast
    local loopback are disabled by default. When there is more
    than one user space context, the local loopback is enabled.
    
    Note that when local loopback is disabled, raw ethernet
    packets are not looped back to the vport and are forwarded
    to the next routing level (eswitch, or multihost switch,
    or out to the wire depending on the configuration).
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index bdcf25410c99..d2b27647f939 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -652,9 +652,13 @@ struct mlx5_ib_dev {
 	struct list_head	qp_list;
 	/* Array with num_ports elements */
 	struct mlx5_ib_port	*port;
-	struct mlx5_sq_bfreg     bfreg;
-	struct mlx5_sq_bfreg     fp_bfreg;
-	u8				umr_fence;
+	struct mlx5_sq_bfreg	bfreg;
+	struct mlx5_sq_bfreg	fp_bfreg;
+
+	/* protect the user_td */
+	struct mutex		lb_mutex;
+	u32			user_td;
+	u8			umr_fence;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 6e8484c5cf07c7ee632587e98c1a12d319dacb7c
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Sun May 28 10:53:11 2017 +0300

    RDMA/mlx5: set UMR wqe fence according to HCA cap
    
    Cache the needed umr_fence and set the wqe ctrl segmennt
    accordingly.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Acked-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 38c877bc45e5..bdcf25410c99 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -349,7 +349,7 @@ struct mlx5_ib_qp {
 	struct mlx5_ib_wq	rq;
 
 	u8			sq_signal_bits;
-	u8			fm_cache;
+	u8			next_fence;
 	struct mlx5_ib_wq	sq;
 
 	/* serialize qp state modifications
@@ -654,6 +654,7 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_port	*port;
 	struct mlx5_sq_bfreg     bfreg;
 	struct mlx5_sq_bfreg     fp_bfreg;
+	u8				umr_fence;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 90898850ec4e7b3ba0f9a35cc7169ff19ff367a6
Author: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
Date:   Sat Apr 29 14:41:18 2017 -0400

    IB/core: Rename struct ib_ah_attr to rdma_ah_attr
    
    This patch simply renames struct ib_ah_attr to
    rdma_ah_attr as these fields specify attributes that are
    not necessarily specific to IB.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Don Hiatt <don.hiatt@intel.com>
    Reviewed-by: Niranjana Vishwanathapura <niranjana.vishwanathapura@intel.com>
    Reviewed-by: Sean Hefty <sean.hefty@intel.com>
    Signed-off-by: Dasaratharaman Chandramouli <dasaratharaman.chandramouli@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 93c646691208..38c877bc45e5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -740,9 +740,9 @@ void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
 		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 		 const void *in_mad, void *response_mad);
-struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr,
+struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,
 				struct ib_udata *udata);
-int mlx5_ib_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr);
+int mlx5_ib_query_ah(struct ib_ah *ibah, struct rdma_ah_attr *ah_attr);
 int mlx5_ib_destroy_ah(struct ib_ah *ah);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
 				  struct ib_srq_init_attr *init_attr,

commit db570d7deafb47ee635981f403a6531844c18ba5
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Apr 5 09:23:59 2017 +0300

    IB/mlx5: Add ODP support to MW
    
    Internally MW implemented as KLM MKey and filled by userspace UMR
    postsends.  Handle pagefault trigered by operations on this MKeys.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 191b82b038d7..93c646691208 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -513,6 +513,7 @@ struct mlx5_ib_mr {
 struct mlx5_ib_mw {
 	struct ib_mw		ibmw;
 	struct mlx5_core_mkey	mmkey;
+	int			ndescs;
 };
 
 struct mlx5_ib_umr_context {

commit e1f24a79f424ddb03828de7c0152668c9a30146e
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Apr 16 07:29:29 2017 +0300

    IB/mlx5: Support congestion related counters
    
    This patch adds support to query the congestion related hardware counters
    through new command and links them with other hw counters being available
    in hw_counters sysfs location.
    
    In order to reuse existing infrastructure it renames related q_counter
    data structures to more generic counters to reflect q_counters and
    congestion counters and maybe some other counters in the future.
    
    New hardware counters:
     * rp_cnp_handled - CNP packets handled by the reaction point
     * rp_cnp_ignored - CNP packets ignored by the reaction point
     * np_cnp_sent    - CNP packets sent by notification point to respond to
                         CE marked RoCE packets
     * np_ecn_marked_roce_packets - CE marked RoCE packets received by
                                    notification point
    
    It also avoids returning ENOSYS which is specific for invalid
    system call and produces the following checkpatch.pl warning.
    
    WARNING: ENOSYS means 'invalid syscall nr' and nothing else
    +               return -ENOSYS;
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ce8ba617d46e..191b82b038d7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -595,15 +595,16 @@ struct mlx5_ib_resources {
 	struct mutex	mutex;
 };
 
-struct mlx5_ib_q_counters {
+struct mlx5_ib_counters {
 	const char **names;
 	size_t *offsets;
-	u32 num_counters;
+	u32 num_q_counters;
+	u32 num_cong_counters;
 	u16 set_id;
 };
 
 struct mlx5_ib_port {
-	struct mlx5_ib_q_counters q_cnts;
+	struct mlx5_ib_counters cnts;
 };
 
 struct mlx5_roce {

commit 258545449b7b410727b516b782256f8a3bde8bf2
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:37:02 2017 +0300

    net/mlx5e: IPoIB, Xmit flow
    
    Implement mlx5e's IPoIB SKB transmit using the helper functions provided
    by mlx5e ethernet tx flow, the only difference in the code between
    mlx5e_xmit and mlx5i_xmit is that IPoIB has some extra fields to fill
    (UD datagram segment) in the TX descriptor (WQE) and it doesn't need to
    have any vlan handling.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3cd064b5f0bf..ce8ba617d46e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -729,16 +729,6 @@ static inline struct mlx5_ib_mw *to_mmw(struct ib_mw *ibmw)
 	return container_of(ibmw, struct mlx5_ib_mw, ibmw);
 }
 
-struct mlx5_ib_ah {
-	struct ib_ah		ibah;
-	struct mlx5_av		av;
-};
-
-static inline struct mlx5_ib_ah *to_mah(struct ib_ah *ibah)
-{
-	return container_of(ibah, struct mlx5_ib_ah, ibah);
-}
-
 int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context, unsigned long virt,
 			struct mlx5_db *db);
 void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db);

commit 81713d3788d2e6bc005f15ee1c59d0eb06050a6b
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Jan 18 16:58:11 2017 +0200

    IB/mlx5: Add implicit MR support
    
    Add implicit MR, covering entire user address space.
    The MR is implemented as an indirect KSM MR consisting of
    1GB direct MRs.
    Pages and direct MRs are added/removed to MR by ODP.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index efc44de3c7d7..3cd064b5f0bf 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -202,6 +202,7 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_UPD_XLT_ADDR	      BIT(3)
 #define MLX5_IB_UPD_XLT_PD	      BIT(4)
 #define MLX5_IB_UPD_XLT_ACCESS	      BIT(5)
+#define MLX5_IB_UPD_XLT_INDIRECT      BIT(6)
 
 /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
  *
@@ -503,6 +504,10 @@ struct mlx5_ib_mr {
 	int			live;
 	void			*descs_alloc;
 	int			access_flags; /* Needed for rereg MR */
+
+	struct mlx5_ib_mr      *parent;
+	atomic_t		num_leaf_free;
+	wait_queue_head_t       q_leaf_free;
 };
 
 struct mlx5_ib_mw {
@@ -637,6 +642,7 @@ struct mlx5_ib_dev {
 	 * being used by a page fault handler.
 	 */
 	struct srcu_struct      mr_srcu;
+	u32			null_mkey;
 #endif
 	struct mlx5_ib_flow_db	flow_db;
 	/* protect resources needed as part of reset flow */
@@ -789,6 +795,9 @@ struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 int mlx5_ib_dealloc_mw(struct ib_mw *mw);
 int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
 		       int page_shift, int flags);
+struct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,
+					     int access_flags);
+void mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *mr);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);
@@ -868,6 +877,9 @@ int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
 void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			      unsigned long end);
+void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent);
+void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
+			   size_t nentries, struct mlx5_ib_mr *mr, int flags);
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 {
@@ -875,9 +887,13 @@ static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 }
 
 static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
-static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	{}
+static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	    {}
 static inline int mlx5_ib_odp_init(void) { return 0; }
-static inline void mlx5_ib_odp_cleanup(void)				{}
+static inline void mlx5_ib_odp_cleanup(void)				    {}
+static inline void mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent) {}
+static inline void mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,
+					 size_t nentries, struct mlx5_ib_mr *mr,
+					 int flags) {}
 
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 

commit 49780d42dfc9ec0f4090c32ca59688449da1a1cd
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Wed Jan 18 16:58:10 2017 +0200

    IB/mlx5: Expose MR cache for mlx5_ib
    
    Allow other parts of mlx5_ib to use MR cache mechanism.
    * Add new functions mlx5_mr_cache_alloc and mlx5_mr_cache_free
    * Traditional MTT MKey buckets are limited by MAX_UMR_CACHE_ENTRY
      Additinal buckets may be added above.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 13bef19649f2..efc44de3c7d7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -541,6 +541,10 @@ struct mlx5_cache_ent {
 	struct dentry	       *dir;
 	char                    name[4];
 	u32                     order;
+	u32			xlt;
+	u32			access_mode;
+	u32			page;
+
 	u32			size;
 	u32                     cur;
 	u32                     miss;
@@ -555,6 +559,7 @@ struct mlx5_cache_ent {
 	struct work_struct	work;
 	struct delayed_work	dwork;
 	int			pending;
+	struct completion	compl;
 };
 
 struct mlx5_mr_cache {
@@ -837,7 +842,9 @@ void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
 int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
-int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
+
+struct mlx5_ib_mr *mlx5_mr_cache_alloc(struct mlx5_ib_dev *dev, int entry);
+void mlx5_mr_cache_free(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,

commit e4cc4fa7cca9ac7d7c3abea7d6c90db1c519d6c6
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Wed Jan 18 15:40:03 2017 +0200

    IB/mlx5: Enable QP creation with cvlan offload
    
    Enable creating a RAW Ethernet QP with cvlan stripping offload when
    it's supported by the hardware.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 66090835de27..13bef19649f2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -220,6 +220,10 @@ struct wr_list {
 	u16	next;
 };
 
+enum mlx5_ib_rq_flags {
+	MLX5_IB_RQ_CVLAN_STRIPPING	= 1 << 0,
+};
+
 struct mlx5_ib_wq {
 	u64		       *wrid;
 	u32		       *wr_data;
@@ -308,6 +312,7 @@ struct mlx5_ib_rq {
 	struct mlx5_db		*doorbell;
 	u32			tirn;
 	u8			state;
+	u32			flags;
 };
 
 struct mlx5_ib_sq {
@@ -392,6 +397,7 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
 	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
 	MLX5_IB_QP_RSS				= 1 << 8,
+	MLX5_IB_QP_CVLAN_STRIPPING		= 1 << 9,
 };
 
 struct mlx5_umr_wr {

commit 7c16f47779498650e9f11a395f8d63accedf35a3
Author: Kamal Heib <kamalh@mellanox.com>
Date:   Wed Jan 18 15:25:09 2017 +0200

    IB/mlx5: Expose Q counters groups only if they are supported by FW
    
    This patch modify the Q counters implementation, so each one of the
    three Q counters groups will be exposed by the driver only if they are
    supported by the firmware.
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dda01d7e8847..66090835de27 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -579,8 +579,15 @@ struct mlx5_ib_resources {
 	struct mutex	mutex;
 };
 
+struct mlx5_ib_q_counters {
+	const char **names;
+	size_t *offsets;
+	u32 num_counters;
+	u16 set_id;
+};
+
 struct mlx5_ib_port {
-	u16 q_cnt_id;
+	struct mlx5_ib_q_counters q_cnts;
 };
 
 struct mlx5_roce {

commit ed88451e1f2d400fd6a743d0a481631cf9f97550
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Wed Jan 18 14:10:35 2017 +0200

    IB/mlx5: Assign DSCP for R-RoCE QPs Address Path
    
    For Routable RoCE QPs, the DSCP should be set in the QP's
    address path.
    
    The DSCP's value is derived from the traffic class.
    
    Fixes: 2811ba51b049 ("IB/mlx5: Add RoCE fields to Address Vector")
    Cc: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Reviewed-by: Yuval Shaia <yuval.shaia@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e1a4b93dce6b..dda01d7e8847 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -872,6 +872,8 @@ int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
 
 __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
 			       int index);
+int mlx5_get_roce_gid_type(struct mlx5_ib_dev *dev, u8 port_num,
+			   int index, enum ib_gid_type *gid_type);
 
 /* GSI QP helper functions */
 struct ib_qp *mlx5_ib_gsi_create_qp(struct ib_pd *pd,

commit b037c29a8056b8e896c4e084ba7cc30d6a1f165f
Author: Eli Cohen <eli@mellanox.com>
Date:   Tue Jan 3 23:55:26 2017 +0200

    IB/mlx5: Allow future extension of libmlx5 input data
    
    Current check requests that new fields in struct
    mlx5_ib_alloc_ucontext_req_v2 that are not known to the driver be zero.
    This was introduced so new libraries passing additional information to
    the kernel through struct mlx5_ib_alloc_ucontext_req_v2 will be notified
    by old kernels that do not support their request by failing the
    operation. This schecme is problematic since it requires libmlx5 to issue
    the requests with descending input size for struct
    mlx5_ib_alloc_ucontext_req_v2.
    
    To avoid this, we require that new features that will obey the following
    rules:
    If the feature requires one or more fields in the response and the at
    least one of the fields can be encoded such that a zero value means the
    kernel ignored the request then this field will provide the indication
    to the library. If no response is required or if zero is a valid
    response, a new field should be added that indicates to the library
    whether its request was processed.
    
    Fixes: b368d7cb8ceb ('IB/mlx5: Add hca_core_clock_offset to udata in init_ucontext')
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ae3bc4a1bfed..e1a4b93dce6b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -90,7 +90,6 @@ enum mlx5_ib_latency_class {
 	MLX5_IB_LATENCY_CLASS_LOW,
 	MLX5_IB_LATENCY_CLASS_MEDIUM,
 	MLX5_IB_LATENCY_CLASS_HIGH,
-	MLX5_IB_LATENCY_CLASS_FAST_PATH
 };
 
 enum mlx5_ib_mad_ifc_flags {
@@ -129,6 +128,7 @@ struct mlx5_ib_ucontext {
 	unsigned long		upd_xlt_page;
 	/* protect ODP/KSM */
 	struct mutex		upd_xlt_page_mutex;
+	u64			lib_caps;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -975,4 +975,17 @@ static inline int get_srq_user_index(struct mlx5_ib_ucontext *ucontext,
 
 	return verify_assign_uidx(cqe_version, ucmd->uidx, user_index);
 }
+
+static inline int get_uars_per_sys_page(struct mlx5_ib_dev *dev, bool lib_support)
+{
+	return lib_support && MLX5_CAP_GEN(dev->mdev, uar_4k) ?
+				MLX5_UARS_IN_PAGE : 1;
+}
+
+static inline int get_num_uars(struct mlx5_ib_dev *dev,
+			       struct mlx5_bfreg_info *bfregi)
+{
+	return get_uars_per_sys_page(dev, bfregi->lib_uar_4k) * bfregi->num_sys_pages;
+}
+
 #endif /* MLX5_IB_H */

commit 5fe9dec0d045437e48f112b8fa705197bd7bc3c0
Author: Eli Cohen <eli@mellanox.com>
Date:   Tue Jan 3 23:55:25 2017 +0200

    IB/mlx5: Use blue flame register allocator in mlx5_ib
    
    Make use of the blue flame registers allocator at mlx5_ib. Since blue
    flame was not really supported we remove all the code that is related to
    blue flame and we let all consumers to use the same blue flame register.
    Once blue flame is supported we will add the code. As part of this patch
    we also move the definition of struct mlx5_bf to mlx5_ib.h as it is only
    used by mlx5_ib.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d4d1329df94a..ae3bc4a1bfed 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -324,6 +324,12 @@ struct mlx5_ib_raw_packet_qp {
 	struct mlx5_ib_rq rq;
 };
 
+struct mlx5_bf {
+	int			buf_size;
+	unsigned long		offset;
+	struct mlx5_sq_bfreg   *bfreg;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
 	union {
@@ -349,7 +355,7 @@ struct mlx5_ib_qp {
 	int			wq_sig;
 	int			scat_cqe;
 	int			max_inline_data;
-	struct mlx5_bf	       *bf;
+	struct mlx5_bf	        bf;
 	int			has_rq;
 
 	/* only for user space QPs. For kernel
@@ -591,7 +597,6 @@ struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
 	struct mlx5_roce		roce;
-	MLX5_DECLARE_DOORBELL_LOCK(uar_lock);
 	int				num_ports;
 	/* serialize update of capability mask
 	 */
@@ -621,6 +626,8 @@ struct mlx5_ib_dev {
 	struct list_head	qp_list;
 	/* Array with num_ports elements */
 	struct mlx5_ib_port	*port;
+	struct mlx5_sq_bfreg     bfreg;
+	struct mlx5_sq_bfreg     fp_bfreg;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 2f5ff26478adaff5ed9b7ad4079d6a710b5f27e7
Author: Eli Cohen <eli@mellanox.com>
Date:   Tue Jan 3 23:55:21 2017 +0200

    mlx5: Fix naming convention with respect to UARs
    
    This establishes a solid naming conventions for UARs. A UAR (User Access
    Region) can have size identical to a system page or can be fixed 4KB
    depending on a value queried by firmware. Each UAR always has 4 blue
    flame register which are used to post doorbell to send queue. In
    addition, a UAR has section used for posting doorbells to CQs or EQs. In
    this patch we change names to reflect this conventions.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a51c8051aeb2..d4d1329df94a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -100,7 +100,7 @@ enum mlx5_ib_mad_ifc_flags {
 };
 
 enum {
-	MLX5_CROSS_CHANNEL_UUAR         = 0,
+	MLX5_CROSS_CHANNEL_BFREG         = 0,
 };
 
 enum {
@@ -120,7 +120,7 @@ struct mlx5_ib_ucontext {
 	/* protect doorbell record alloc/free
 	 */
 	struct mutex		db_page_mutex;
-	struct mlx5_uuar_info	uuari;
+	struct mlx5_bfreg_info	bfregi;
 	u8			cqe_version;
 	/* Transport Domain number */
 	u32			tdn;
@@ -355,7 +355,7 @@ struct mlx5_ib_qp {
 	/* only for user space QPs. For kernel
 	 * we have it from the bf object
 	 */
-	int			uuarn;
+	int			bfregn;
 
 	int			create_type;
 

commit d9aaed838765e28234cb700c7d1ac975cadf28c9
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Mon Jan 2 11:37:46 2017 +0200

    {net,IB}/mlx5: Refactor page fault handling
    
    * Update page fault event according to last specification.
    * Separate code path for page fault EQ, completion EQ and async EQ.
    * Move page fault handling work queue from mlx5_ib static variable
      into mlx5_core page fault EQ.
    * Allocate memory to store ODP event dynamically as the
      events arrive, since in atomic context - use mempool.
    * Make mlx5_ib page fault handler run in process context.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 02d925573945..a51c8051aeb2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -277,29 +277,6 @@ struct mlx5_ib_rwq_ind_table {
 	u32			rqtn;
 };
 
-/*
- * Connect-IB can trigger up to four concurrent pagefaults
- * per-QP.
- */
-enum mlx5_ib_pagefault_context {
-	MLX5_IB_PAGEFAULT_RESPONDER_READ,
-	MLX5_IB_PAGEFAULT_REQUESTOR_READ,
-	MLX5_IB_PAGEFAULT_RESPONDER_WRITE,
-	MLX5_IB_PAGEFAULT_REQUESTOR_WRITE,
-	MLX5_IB_PAGEFAULT_CONTEXTS
-};
-
-static inline enum mlx5_ib_pagefault_context
-	mlx5_ib_get_pagefault_context(struct mlx5_pagefault *pagefault)
-{
-	return pagefault->flags & (MLX5_PFAULT_REQUESTOR | MLX5_PFAULT_WRITE);
-}
-
-struct mlx5_ib_pfault {
-	struct work_struct	work;
-	struct mlx5_pagefault	mpfault;
-};
-
 struct mlx5_ib_ubuffer {
 	struct ib_umem	       *umem;
 	int			buf_size;
@@ -385,20 +362,6 @@ struct mlx5_ib_qp {
 	/* Store signature errors */
 	bool			signature_en;
 
-#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-	/*
-	 * A flag that is true for QP's that are in a state that doesn't
-	 * allow page faults, and shouldn't schedule any more faults.
-	 */
-	int                     disable_page_faults;
-	/*
-	 * The disable_page_faults_lock protects a QP's disable_page_faults
-	 * field, allowing for a thread to atomically check whether the QP
-	 * allows page faults, and if so schedule a page fault.
-	 */
-	spinlock_t              disable_page_faults_lock;
-	struct mlx5_ib_pfault	pagefaults[MLX5_IB_PAGEFAULT_CONTEXTS];
-#endif
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;
@@ -869,18 +832,13 @@ struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
 int mlx5_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
-extern struct workqueue_struct *mlx5_ib_page_fault_wq;
-
 void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);
-void mlx5_ib_mr_pfault_handler(struct mlx5_ib_qp *qp,
-			       struct mlx5_ib_pfault *pfault);
-void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp);
+void mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,
+		    struct mlx5_pagefault *pfault);
 int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
 void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev);
 int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
-void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
-void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			      unsigned long end);
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
@@ -889,13 +847,10 @@ static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 	return;
 }
 
-static inline void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp)		{}
 static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
 static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	{}
 static inline int mlx5_ib_odp_init(void) { return 0; }
 static inline void mlx5_ib_odp_cleanup(void)				{}
-static inline void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp) {}
-static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
 
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 

commit 7d0cc6edcc7011133c45f62a7796a98b8cb5da0f
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Mon Jan 2 11:37:44 2017 +0200

    IB/mlx5: Add MR cache for large UMR regions
    
    In this change we turn mlx5_ib_update_mtt() into generic
    mlx5_ib_update_xlt() to perfrom HCA translation table modifiactions
    supporting both atomic and process contexts and not limited by number
    of modified entries.
    Using this function we increase preallocated MRs up to 16GB.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 73bff77ab20c..02d925573945 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -125,6 +125,10 @@ struct mlx5_ib_ucontext {
 	/* Transport Domain number */
 	u32			tdn;
 	struct list_head	vma_private_list;
+
+	unsigned long		upd_xlt_page;
+	/* protect ODP/KSM */
+	struct mutex		upd_xlt_page_mutex;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -192,6 +196,13 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_UMR_OCTOWORD	       16
 #define MLX5_IB_UMR_XLT_ALIGNMENT      64
 
+#define MLX5_IB_UPD_XLT_ZAP	      BIT(0)
+#define MLX5_IB_UPD_XLT_ENABLE	      BIT(1)
+#define MLX5_IB_UPD_XLT_ATOMIC	      BIT(2)
+#define MLX5_IB_UPD_XLT_ADDR	      BIT(3)
+#define MLX5_IB_UPD_XLT_PD	      BIT(4)
+#define MLX5_IB_UPD_XLT_ACCESS	      BIT(5)
+
 /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
  *
  * These flags are intended for internal use by the mlx5_ib driver, and they
@@ -788,8 +799,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
 			       struct ib_udata *udata);
 int mlx5_ib_dealloc_mw(struct ib_mw *mw);
-int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
-		       int npages, int zap);
+int mlx5_ib_update_xlt(struct mlx5_ib_mr *mr, u64 idx, int npages,
+		       int page_shift, int flags);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
 			  u64 length, u64 virt_addr, int access_flags,
 			  struct ib_pd *pd, struct ib_udata *udata);

commit c438fde1c288a754aa5d22e3668f03a1dde18335
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Mon Jan 2 11:37:43 2017 +0200

    IB/mlx5: Add support for big MRs
    
    Make use of extended UMR translation offset.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d79580dcf20f..73bff77ab20c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -634,6 +634,7 @@ struct mlx5_ib_dev {
 	int				fill_delay;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_odp_caps	odp_caps;
+	u64			odp_max_size;
 	/*
 	 * Sleepable RCU that prevents destruction of MRs while they are still
 	 * being used by a page fault handler.

commit 3161625589c1d7c54e949d462f4d0c327664881a
Author: Artemy Kovalyov <artemyko@mellanox.com>
Date:   Mon Jan 2 11:37:42 2017 +0200

    IB/mlx5: Refactor UMR post send format
    
    * Update struct mlx5_wqe_umr_ctrl_seg.
    * Currenlty UMR send_flags aim only certain use cases: enabled/disable
      cached MR, modifying XLT for ODP. By making flags independent make UMR
      more flexible allowing arbitrary manipulations.
    * Since different UMR formats have different entry sizes UMR request
      should receive exact size of translation table update instead of
      number of entries. Rename field npages to xlt_size in struct mlx5_umr_wr
      and update relevant code accordingly.
    * Add support of length64 bit.
    
    Signed-off-by: Artemy Kovalyov <artemyko@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6c6057eb60ea..d79580dcf20f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -174,13 +174,12 @@ struct mlx5_ib_flow_db {
  * enum ib_send_flags and enum ib_qp_type for low-level driver
  */
 
-#define MLX5_IB_SEND_UMR_UNREG	IB_SEND_RESERVED_START
-#define MLX5_IB_SEND_UMR_FAIL_IF_FREE (IB_SEND_RESERVED_START << 1)
-#define MLX5_IB_SEND_UMR_UPDATE_MTT (IB_SEND_RESERVED_START << 2)
-
-#define MLX5_IB_SEND_UMR_UPDATE_TRANSLATION	(IB_SEND_RESERVED_START << 3)
-#define MLX5_IB_SEND_UMR_UPDATE_PD		(IB_SEND_RESERVED_START << 4)
-#define MLX5_IB_SEND_UMR_UPDATE_ACCESS		IB_SEND_RESERVED_END
+#define MLX5_IB_SEND_UMR_ENABLE_MR	       (IB_SEND_RESERVED_START << 0)
+#define MLX5_IB_SEND_UMR_DISABLE_MR	       (IB_SEND_RESERVED_START << 1)
+#define MLX5_IB_SEND_UMR_FAIL_IF_FREE	       (IB_SEND_RESERVED_START << 2)
+#define MLX5_IB_SEND_UMR_UPDATE_XLT	       (IB_SEND_RESERVED_START << 3)
+#define MLX5_IB_SEND_UMR_UPDATE_TRANSLATION    (IB_SEND_RESERVED_START << 4)
+#define MLX5_IB_SEND_UMR_UPDATE_PD_ACCESS       IB_SEND_RESERVED_END
 
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
 /*
@@ -190,6 +189,9 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_QPT_HW_GSI	IB_QPT_RESERVED2
 #define MLX5_IB_WR_UMR		IB_WR_RESERVED1
 
+#define MLX5_IB_UMR_OCTOWORD	       16
+#define MLX5_IB_UMR_XLT_ALIGNMENT      64
+
 /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
  *
  * These flags are intended for internal use by the mlx5_ib driver, and they
@@ -414,13 +416,11 @@ enum mlx5_ib_qp_flags {
 
 struct mlx5_umr_wr {
 	struct ib_send_wr		wr;
-	union {
-		u64			virt_addr;
-		u64			offset;
-	} target;
+	u64				virt_addr;
+	u64				offset;
 	struct ib_pd		       *pd;
 	unsigned int			page_shift;
-	unsigned int			npages;
+	unsigned int			xlt_size;
 	u64				length;
 	int				access_flags;
 	u32				mkey;

commit 4d5b57e05a67c3cfd8e2b2a64ca356245a15b1c6
Merge: 6df8b74b1720 6f94ba20799b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 15 12:03:32 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "This is the complete update for the rdma stack for this release cycle.
    
      Most of it is typical driver and core updates, but there is the
      entirely new VMWare pvrdma driver. You may have noticed that there
      were changes in DaveM's pull request to the bnxt Ethernet driver to
      support a RoCE RDMA driver. The bnxt_re driver was tentatively set to
      be pulled in this release cycle, but it simply wasn't ready in time
      and was dropped (a few review comments still to address, and some
      multi-arch build issues like prefetch() not working across all
      arches).
    
      Summary:
    
       - shared mlx5 updates with net stack (will drop out on merge if
         Dave's tree has already been merged)
    
       - driver updates: cxgb4, hfi1, hns-roce, i40iw, mlx4, mlx5, qedr, rxe
    
       - debug cleanups
    
       - new connection rejection helpers
    
       - SRP updates
    
       - various misc fixes
    
       - new paravirt driver from vmware"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (210 commits)
      IB: Add vmw_pvrdma driver
      IB/mlx4: fix improper return value
      IB/ocrdma: fix bad initialization
      infiniband: nes: return value of skb_linearize should be handled
      MAINTAINERS: Update Intel RDMA RNIC driver maintainers
      MAINTAINERS: Remove Mitesh Ahuja from emulex maintainers
      IB/core: fix unmap_sg argument
      qede: fix general protection fault may occur on probe
      IB/mthca: Replace pci_pool_alloc by pci_pool_zalloc
      mlx5, calc_sq_size(): Make a debug message more informative
      mlx5: Remove a set-but-not-used variable
      mlx5: Use { } instead of { 0 } to init struct
      IB/srp: Make writing the add_target sysfs attr interruptible
      IB/srp: Make mapping failures easier to debug
      IB/srp: Make login failures easier to debug
      IB/srp: Introduce a local variable in srp_add_one()
      IB/srp: Fix CONFIG_DYNAMIC_DEBUG=n build
      IB/multicast: Check ib_find_pkey() return value
      IPoIB: Avoid reading an uninitialized member variable
      IB/mad: Fix an array index check
      ...

commit 7d29f349a4b9dcf5bc9dcc05630d6a7f6b6b3ccd
Author: Bodong Wang <bodong@mellanox.com>
Date:   Thu Dec 1 13:43:16 2016 +0200

    IB/mlx5: Properly adjust rate limit on QP state transitions
    
    - Add MODIFY_QP_EX CMD to extend modify_qp.
    - Rate limit will be updated in the following state transactions: RTR2RTS,
      RTS2RTS. The limit will be removed when SQ is in RST and ERR state.
    
    Signed-off-by: Bodong Wang <bodong@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index df3d6af3f683..ab8961cc8bca 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -389,6 +389,7 @@ struct mlx5_ib_qp {
 	struct list_head	qps_list;
 	struct list_head	cq_recv_list;
 	struct list_head	cq_send_list;
+	u32			rate_limit;
 };
 
 struct mlx5_ib_cq_buf {

commit b216af408c985092a79472ad10e6f216cb2973fc
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Sun Nov 27 15:18:22 2016 +0200

    IB/mlx5: Use u64 for UMR length
    
    The fast_registration length is used to convey length for memory
    registrations through UMR which can be of any size up to 2^64.
    
    Change the length type to be u64.
    
    Fixes: 968e78dd9644 ('IB/mlx5: Enhance UMR support to allow partial page table update')
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index ff05afa864ee..df3d6af3f683 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -420,7 +420,7 @@ struct mlx5_umr_wr {
 	struct ib_pd		       *pd;
 	unsigned int			page_shift;
 	unsigned int			npages;
-	u32				length;
+	u64				length;
 	int				access_flags;
 	u32				mkey;
 };

commit 477864c8fcd953e5a988073ca5be18bb7fd93410
Author: Moni Shoua <monis@mellanox.com>
Date:   Wed Nov 23 08:23:24 2016 +0200

    IB/core: Let create_ah return extended response to user
    
    Add struct ib_udata to the signature of create_ah callback that is
    implemented by IB device drivers. This allows HW drivers to return extra
    data to the userspace library.
    This patch prepares the ground for mlx5 driver to resolve destination
    mac address for a given GID and return it to userspace.
    This patch was previously submitted by Knut Omang as a part of the
    patch set to support Oracle's Infiniband HCA (SIF).
    
    Signed-off-by: Knut Omang <knut.omang@oracle.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 95937e7d2584..ff05afa864ee 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -739,7 +739,8 @@ void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
 		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 		 const void *in_mad, void *response_mad);
-struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
+struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr,
+				struct ib_udata *udata);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr);
 int mlx5_ib_destroy_ah(struct ib_ah *ah);
 struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,

commit f9aa9dc7d2d00e6eb02168ffc64ef614b89d7998
Merge: 06b37b650cf8 3b404a519815
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Nov 22 11:29:28 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    All conflicts were simple overlapping changes except perhaps
    for the Thunder driver.
    
    That driver has a change_mtu method explicitly for sending
    a message to the hardware.  If that fails it returns an
    error.
    
    Normally a driver doesn't need an ndo_change_mtu method becuase those
    are usually just range changes, which are now handled generically.
    But since this extra operation is needed in the Thunder driver, it has
    to stay.
    
    However, if the message send fails we have to restore the original
    MTU before the change because the entire call chain expects that if
    an error is thrown by ndo_change_mtu then the MTU did not change.
    Therefore code is added to nicvf_change_mtu to remember the original
    MTU, and to restore it upon nicvf_update_hw_max_frs() failue.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 762f899ae7875554284af92b821be8c083227092
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Thu Oct 27 16:36:47 2016 +0300

    IB/mlx5: Limit mkey page size to 2GB
    
    The maximum page size in the mkey context is 2GB.
    
    Until today, we didn't enforce this requirement in the code,
    and therefore, if we got a page size larger than 2GB, we
    have passed zeros in the log_page_shift instead of the actual value
    and the registration failed.
    
    This patch limits the driver to use compound pages of 2GB for mkeys.
    
    Fixes: e126ba97dba9 ('mlx5: Add driver for Mellanox Connect-IB adapters')
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d5d007740159..95937e7d2584 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -63,6 +63,8 @@ pr_warn("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
 #define MLX5_IB_DEFAULT_UIDX 0xffffff
 #define MLX5_USER_ASSIGNED_UIDX_MASK __mlx5_mask(qpc, user_index)
 
+#define MLX5_MKEY_PAGE_SHIFT_MASK __mlx5_mask(mkc, log_page_size)
+
 enum {
 	MLX5_IB_MMAP_CMD_SHIFT	= 8,
 	MLX5_IB_MMAP_CMD_MASK	= 0xff,
@@ -823,7 +825,9 @@ int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,
 		       struct ib_port_attr *props);
 int mlx5_ib_init_fmr(struct mlx5_ib_dev *dev);
 void mlx5_ib_cleanup_fmr(struct mlx5_ib_dev *dev);
-void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift,
+void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr,
+			unsigned long max_page_shift,
+			int *count, int *shift,
 			int *ncont, int *order);
 void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 			    int page_shift, size_t offset, size_t num_pages,

commit 6bc1a656ab9f57f0112823b4a36930c9a29d1f89
Author: Moshe Lazer <moshel@mellanox.com>
Date:   Thu Oct 27 16:36:42 2016 +0300

    IB/mlx5: Resolve soft lock on massive reg MRs
    
    When calling reg_mr of large MRs (e.g. 4GB) from multiple processes
    and MR caches can't supply the required amount of MRs the slow-path
    of MR allocation may be used. In this case we need to serialize the
    slow-path between the processes to avoid soft lock.
    
    Fixes: e126ba97dba9 ('mlx5: Add driver for Mellanox Connect-IB adapters')
    Signed-off-by: Moshe Lazer <moshel@mellanox.com>
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dcdcd195fe53..7d689903c87c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -626,6 +626,8 @@ struct mlx5_ib_dev {
 	struct mlx5_ib_resources	devr;
 	struct mlx5_mr_cache		cache;
 	struct timer_list		delay_timer;
+	/* Prevents soft lock on massive reg MRs */
+	struct mutex			slow_path_mutex;
 	int				fill_delay;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_odp_caps	odp_caps;

commit 74491de937125d0c98c9b9c9208b4105717a3caa
Author: Mark Bloch <markb@mellanox.com>
Date:   Wed Aug 31 11:24:25 2016 +0000

    net/mlx5: Add multi dest support
    
    Currently when calling mlx5_add_flow_rule we accept
    only one flow destination, this commit allows to pass
    multiple destinations.
    
    This change forces us to change the return structure to a more
    flexible one. We introduce a flow handle (struct mlx5_flow_handle),
    it holds internally the number for rules created and holds an array
    where each cell points the to a flow rule.
    
    From the consumers (of mlx5_add_flow_rule) point of view this
    change is only cosmetic and requires only to change the type
    of the returned value they store.
    
    From the core point of view, we now need to use a loop when
    allocating and deleting rules (e.g given to us a flow handler).
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dcdcd195fe53..d5d007740159 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -153,7 +153,7 @@ struct mlx5_ib_flow_handler {
 	struct list_head		list;
 	struct ib_flow			ibflow;
 	struct mlx5_ib_flow_prio	*prio;
-	struct mlx5_flow_rule	*rule;
+	struct mlx5_flow_handle		*rule;
 };
 
 struct mlx5_ib_flow_db {

commit b9044ac8292fc94bee33f6f08acaed3ac55f0c75
Merge: 1fde76f173e4 2937f3757519
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:04:33 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull main rdma updates from Doug Ledford:
     "This is the main pull request for the rdma stack this release.  The
      code has been through 0day and I had it tagged for linux-next testing
      for a couple days.
    
      Summary:
    
       - updates to mlx5
    
       - updates to mlx4 (two conflicts, both minor and easily resolved)
    
       - updates to iw_cxgb4 (one conflict, not so obvious to resolve,
         proper resolution is to keep the code in cxgb4_main.c as it is in
         Linus' tree as attach_uld was refactored and moved into
         cxgb4_uld.c)
    
       - improvements to uAPI (moved vendor specific API elements to uAPI
         area)
    
       - add hns-roce driver and hns and hns-roce ACPI reset support
    
       - conversion of all rdma code away from deprecated
         create_singlethread_workqueue
    
       - security improvement: remove unsafe ib_get_dma_mr (breaks lustre in
         staging)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (75 commits)
      staging/lustre: Disable InfiniBand support
      iw_cxgb4: add fast-path for small REG_MR operations
      cxgb4: advertise support for FR_NSMR_TPTE_WR
      IB/core: correctly handle rdma_rw_init_mrs() failure
      IB/srp: Fix infinite loop when FMR sg[0].offset != 0
      IB/srp: Remove an unused argument
      IB/core: Improve ib_map_mr_sg() documentation
      IB/mlx4: Fix possible vl/sl field mismatch in LRH header in QP1 packets
      IB/mthca: Move user vendor structures
      IB/nes: Move user vendor structures
      IB/ocrdma: Move user vendor structures
      IB/mlx4: Move user vendor structures
      IB/cxgb4: Move user vendor structures
      IB/cxgb3: Move user vendor structures
      IB/mlx5: Move and decouple user vendor structures
      IB/{core,hw}: Add constant for node_desc
      ipoib: Make ipoib_warn ratelimited
      IB/mlx4/alias_GUID: Remove deprecated create_singlethread_workqueue
      IB/ipoib_verbs: Remove deprecated create_singlethread_workqueue
      IB/ipoib: Remove deprecated create_singlethread_workqueue
      ...

commit 3085e29e2f832cbf77ddeeffe715809a31254b5f
Author: Leon Romanovsky <leon@kernel.org>
Date:   Thu Sep 22 17:31:11 2016 +0300

    IB/mlx5: Move and decouple user vendor structures
    
    This patch decouples and moves vendors specific structures to
    common UAPI folder which will be visible to all consumers.
    
    These structures are used by user-space library driver
    (libmlx5) and currently manually copied to that library.
    
    This move will allow cross-compile against these files and
    simplify introduction of vendor specific data.
    
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 40fe1a65402b..1df8a67d4f02 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -44,6 +44,7 @@
 #include <linux/types.h>
 #include <linux/mlx5/transobj.h>
 #include <rdma/ib_user_verbs.h>
+#include <rdma/mlx5-abi.h>
 
 #define mlx5_ib_dbg(dev, format, arg...)				\
 pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
@@ -955,4 +956,40 @@ static inline int verify_assign_uidx(u8 cqe_version, u32 cmd_uidx,
 
 	return 0;
 }
+
+static inline int get_qp_user_index(struct mlx5_ib_ucontext *ucontext,
+				    struct mlx5_ib_create_qp *ucmd,
+				    int inlen,
+				    u32 *user_index)
+{
+	u8 cqe_version = ucontext->cqe_version;
+
+	if (field_avail(struct mlx5_ib_create_qp, uidx, inlen) &&
+	    !cqe_version && (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
+		return 0;
+
+	if (!!(field_avail(struct mlx5_ib_create_qp, uidx, inlen) !=
+	       !!cqe_version))
+		return -EINVAL;
+
+	return verify_assign_uidx(cqe_version, ucmd->uidx, user_index);
+}
+
+static inline int get_srq_user_index(struct mlx5_ib_ucontext *ucontext,
+				     struct mlx5_ib_create_srq *ucmd,
+				     int inlen,
+				     u32 *user_index)
+{
+	u8 cqe_version = ucontext->cqe_version;
+
+	if (field_avail(struct mlx5_ib_create_srq, uidx, inlen) &&
+	    !cqe_version && (ucmd->uidx == MLX5_IB_DEFAULT_UIDX))
+		return 0;
+
+	if (!!(field_avail(struct mlx5_ib_create_srq, uidx, inlen) !=
+	       !!cqe_version))
+		return -EINVAL;
+
+	return verify_assign_uidx(cqe_version, ucmd->uidx, user_index);
+}
 #endif /* MLX5_IB_H */

commit 13eab21f92de21f324fd6afe1aeca310446b8731
Author: Aviv Heller <avivh@mellanox.com>
Date:   Sun Sep 18 20:48:04 2016 +0300

    IB/mlx5: LAG QP load balancing
    
    When LAG is active, QP tx affinity (the physical port
    to which a QP is affined, or the TIS in case of raw-eth)
    is set in a round robin fashion during state transition
    from RESET to INIT.
    
    Signed-off-by: Aviv Heller <avivh@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 53e1f1dc0000..40fe1a65402b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -605,6 +605,7 @@ struct mlx5_roce {
 	rwlock_t		netdev_lock;
 	struct net_device	*netdev;
 	struct notifier_block	nb;
+	atomic_t		next_port;
 };
 
 struct mlx5_ib_dev {

commit 9ef9c640f4c5376189d8ca70f51a50a3d0b16914
Author: Aviv Heller <avivh@mellanox.com>
Date:   Sun Sep 18 20:48:01 2016 +0300

    IB/mlx5: Merge vports flow steering during LAG
    
    This is done in two steps:
    1) Issuing CREATE_VPORT_LAG in order to have Ethernet traffic from
    both ports arriving on PF0 root flowtable, so we will be able to catch
    all raw-eth traffic on PF0.
    2) Creation of LAG demux flowtable in order to direct all non-raw-eth
    traffic back to its source port, assuring that normal Ethernet
    traffic "jumps" to the root flowtable of its RX port (non-LAG behavior).
    
    Signed-off-by: Aviv Heller <avivh@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f8a62a643613..53e1f1dc0000 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -158,6 +158,7 @@ struct mlx5_ib_flow_handler {
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
 	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
+	struct mlx5_flow_table		*lag_demux_ft;
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.
 	 * only single add/removal of flow steering rule could be done

commit 350d0e4c7e4b03ed5646ac39ba4aac98bb3d9c56
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Aug 28 14:58:18 2016 +0300

    IB/mlx5: Track asynchronous events on a receive work queue
    
    Track asynchronous events on a receive work queue by using the
    mlx5_core_create_rq_tracked API.
    
    In case a fatal error has occurred letting the IB layer know about by
    using the ib_wq event handler.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e1b120574792..f8a62a643613 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -227,7 +227,7 @@ struct mlx5_ib_wq {
 
 struct mlx5_ib_rwq {
 	struct ib_wq		ibwq;
-	u32			rqn;
+	struct mlx5_core_qp	core_qp;
 	u32			rq_num_pas;
 	u32			log_rq_stride;
 	u32			log_rq_size;
@@ -664,6 +664,11 @@ static inline struct mlx5_ib_qp *to_mibqp(struct mlx5_core_qp *mqp)
 	return container_of(mqp, struct mlx5_ib_qp_base, mqp)->container_mibqp;
 }
 
+static inline struct mlx5_ib_rwq *to_mibrwq(struct mlx5_core_qp *core_qp)
+{
+	return container_of(core_qp, struct mlx5_ib_rwq, core_qp);
+}
+
 static inline struct mlx5_ib_mr *to_mibmr(struct mlx5_core_mkey *mmkey)
 {
 	return container_of(mmkey, struct mlx5_ib_mr, mmkey);

commit cc0e5d42351de1f9233738697718d0eed4396536
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Sun Aug 28 14:16:34 2016 +0300

    IB/mlx5: Add sniffer support to steering
    
    Add support to create sniffer rule. This rule receive all
    incoming and outgoing packets from the port.
    A user could create such rule by using IB_FLOW_ATTR_SNIFFER type.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a24cc20363cb..e1b120574792 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -142,6 +142,7 @@ struct mlx5_ib_pd {
 #define MLX5_IB_FLOW_LEFTOVERS_PRIO	(MLX5_IB_FLOW_MCAST_PRIO + 1)
 
 #define MLX5_IB_NUM_FLOW_FT		(MLX5_IB_FLOW_LEFTOVERS_PRIO + 1)
+#define MLX5_IB_NUM_SNIFFER_FTS		2
 struct mlx5_ib_flow_prio {
 	struct mlx5_flow_table		*flow_table;
 	unsigned int			refcount;
@@ -156,6 +157,7 @@ struct mlx5_ib_flow_handler {
 
 struct mlx5_ib_flow_db {
 	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
+	struct mlx5_ib_flow_prio	sniffer[MLX5_IB_NUM_SNIFFER_FTS];
 	/* Protect flow steering bypass flow tables
 	 * when add/del flow rules.
 	 * only single add/removal of flow steering rule could be done

commit 5497adc63213e2b9b27c23e42a33e259d59bfe28
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Sun Aug 28 14:16:31 2016 +0300

    IB/mlx5: Save flow table priority handler instead of index
    
    Saving the flow table priority object's pointer in the flow handle
    is necessary for downstream patches since the sniffer flow table isn't
    placed at the standard flow_db structure but in a different database.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a59034aaa297..a24cc20363cb 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -150,7 +150,7 @@ struct mlx5_ib_flow_prio {
 struct mlx5_ib_flow_handler {
 	struct list_head		list;
 	struct ib_flow			ibflow;
-	unsigned int			prio;
+	struct mlx5_ib_flow_prio	*prio;
 	struct mlx5_flow_rule	*rule;
 };
 

commit b20b378d49926b82c0a131492fa8842156e0e8a9
Merge: 02154927c115 da499f8f5385
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Sep 12 15:52:44 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/mediatek/mtk_eth_soc.c
            drivers/net/ethernet/qlogic/qed/qed_dcbx.c
            drivers/net/phy/Kconfig
    
    All conflicts were cases of overlapping commits.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d9f88e5ab9a73058ebdde589219c0d37da250f06
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Aug 28 10:58:37 2016 +0300

    IB/mlx5: Use TIR number based on selector
    
    Use TIR number based on selector, it should be done to differentiate
    between RSS QP to RAW one.
    
    Reported-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Tested-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 372385d0f993..95146f4aa3e3 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -402,6 +402,7 @@ enum mlx5_ib_qp_flags {
 	/* QP uses 1 as its source QP number */
 	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
 	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
+	MLX5_IB_QP_RSS				= 1 << 8,
 };
 
 struct mlx5_umr_wr {

commit ec22eb53106be1472ba6573dc900943f52f8fd1e
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Jul 16 06:28:36 2016 +0300

    {net,IB}/mlx5: MKey/PSV commands via mlx5 ifc
    
    Remove old representation of manually created MKey/PSV commands layout,
    and use mlx5_ifc canonical structures and defines.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 372385d0f993..a59034aaa297 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -504,7 +504,7 @@ struct mlx5_ib_mr {
 	int			umred;
 	int			npages;
 	struct mlx5_ib_dev     *dev;
-	struct mlx5_create_mkey_mbox_out out;
+	u32 out[MLX5_ST_SZ_DW(create_mkey_out)];
 	struct mlx5_core_sig_ctx    *sig;
 	int			live;
 	void			*descs_alloc;

commit 0837e86a7a3422b85aa45c6f4631f6a3f74cbd01
Author: Mark Bloch <markb@mellanox.com>
Date:   Fri Jun 17 15:10:55 2016 +0300

    IB/mlx5: Add per port counters
    
    In order to support statistics for ports, we attach
    each QP to a counter set which is dedicate to this port.
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0001ed51e1bd..372385d0f993 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -591,6 +591,10 @@ struct mlx5_ib_resources {
 	struct mutex	mutex;
 };
 
+struct mlx5_ib_port {
+	u16 q_cnt_id;
+};
+
 struct mlx5_roce {
 	/* Protect mlx5_ib_get_netdev from invoking dev_hold() with a NULL
 	 * netdev pointer
@@ -629,6 +633,8 @@ struct mlx5_ib_dev {
 	/* protect resources needed as part of reset flow */
 	spinlock_t		reset_flow_resource_lock;
 	struct list_head	qp_list;
+	/* Array with num_ports elements */
+	struct mlx5_ib_port	*port;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 89ea94a7b6c40eb423c144aef1caceebaff79c8d
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Jun 17 15:01:38 2016 +0300

    IB/mlx5: Reset flow support for IB kernel ULPs
    
    The driver exposes interfaces that directly relate to HW state.
    Upon fatal error, consumers of these interfaces (ULPs) that rely
    on completion of all their posted work-request could hang, thereby
    introducing dependencies in shutdown order. To prevent this from
    happening, we manage the relevant resources (CQs, QPs) that are used
    by the device. Upon a fatal error, we now generate simulated
    completions for outstanding WQEs that were not completed at the
    time the HW was reset.
    
    It includes invoking the completion event handler for all involved
    CQs so that the ULPs will poll those CQs. When polled we return
    simulated CQEs with IB_WC_WR_FLUSH_ERR return code enabling ULPs
    to clean up their  resources and not wait forever for completions
    upon receiving remove_one.
    
    The above change requires an extra check in the data path to make
    sure that when device is in error state, the simulated CQEs will
    be returned and no further WQEs will be posted.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 391588e01317..0001ed51e1bd 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -380,6 +380,9 @@ struct mlx5_ib_qp {
 	spinlock_t              disable_page_faults_lock;
 	struct mlx5_ib_pfault	pagefaults[MLX5_IB_PAGEFAULT_CONTEXTS];
 #endif
+	struct list_head	qps_list;
+	struct list_head	cq_recv_list;
+	struct list_head	cq_send_list;
 };
 
 struct mlx5_ib_cq_buf {
@@ -441,6 +444,8 @@ struct mlx5_ib_cq {
 	struct mlx5_ib_cq_buf  *resize_buf;
 	struct ib_umem	       *resize_umem;
 	int			cqe_size;
+	struct list_head	list_send_qp;
+	struct list_head	list_recv_qp;
 	u32			create_flags;
 	struct list_head	wc_list;
 	enum ib_cq_notify_flags notify_flags;
@@ -621,6 +626,9 @@ struct mlx5_ib_dev {
 	struct srcu_struct      mr_srcu;
 #endif
 	struct mlx5_ib_flow_db	flow_db;
+	/* protect resources needed as part of reset flow */
+	spinlock_t		reset_flow_resource_lock;
+	struct list_head	qp_list;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 7c2344c3bbf97eb5dfa732d5098285d15d3bf9bf
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Jun 17 14:56:44 2016 +0300

    IB/mlx5: Implements disassociate_ucontext API
    
    Implements the IB core disassociate_ucontext API.
    The driver detaches the HW resources for a given user context to
    prevent a dependency between application termination and device
    disconnect. This is done by managing the VMAs that were mapped
    to the HW bars such as doorbell and blueflame. When need to detach,
    remap them to an arbitrary kernel page returned by the zap API.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7ac4647f7382..391588e01317 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -105,6 +105,11 @@ enum {
 	MLX5_CQE_VERSION_V1,
 };
 
+struct mlx5_ib_vma_private_data {
+	struct list_head list;
+	struct vm_area_struct *vma;
+};
+
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;
@@ -116,6 +121,7 @@ struct mlx5_ib_ucontext {
 	u8			cqe_version;
 	/* Transport Domain number */
 	u32			tdn;
+	struct list_head	vma_private_list;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)

commit 28d6137008b2aa09e35750c604394e363dbfca94
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:56 2016 +0300

    IB/mlx5: Add RSS QP support
    
    Add support for Raw Ethernet RX HASH QP. Currently, creation and
    destruction of such a QP are supported. This QP is implemented as
    a simple TIR object which points to the receive RQ indirection table.
    The given hashing configuration is used to configure the TIR and by
    that it chooses the right RQ from the RQ indirection table.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index cd3d6203ca1c..7ac4647f7382 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -295,6 +295,10 @@ struct mlx5_ib_qp_trans {
 	u8			resp_depth;
 };
 
+struct mlx5_ib_rss_qp {
+	u32	tirn;
+};
+
 struct mlx5_ib_rq {
 	struct mlx5_ib_qp_base base;
 	struct mlx5_ib_wq	*rq;
@@ -323,6 +327,7 @@ struct mlx5_ib_qp {
 	union {
 		struct mlx5_ib_qp_trans trans_qp;
 		struct mlx5_ib_raw_packet_qp raw_packet_qp;
+		struct mlx5_ib_rss_qp rss_qp;
 	};
 	struct mlx5_buf		buf;
 

commit c5f9092936fe88b39e2eddccedeb1c51883fcd31
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:53 2016 +0300

    IB/mlx5: Add Receive Work Queue Indirection table operations
    
    Some mlx5 based hardwares support a RQ table object. This RQ table
    points to a few RQ objects. We implement the receive work queue
    indirection table API (create and destroy) by using this hardware
    object.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 62d4e137593a..cd3d6203ca1c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -247,6 +247,11 @@ enum {
 	MLX5_WQ_KERNEL
 };
 
+struct mlx5_ib_rwq_ind_table {
+	struct ib_rwq_ind_table ib_rwq_ind_tbl;
+	u32			rqtn;
+};
+
 /*
  * Connect-IB can trigger up to four concurrent pagefaults
  * per-QP.
@@ -657,6 +662,11 @@ static inline struct mlx5_ib_rwq *to_mrwq(struct ib_wq *ibwq)
 	return container_of(ibwq, struct mlx5_ib_rwq, ibwq);
 }
 
+static inline struct mlx5_ib_rwq_ind_table *to_mrwq_ind_table(struct ib_rwq_ind_table *ib_rwq_ind_tbl)
+{
+	return container_of(ib_rwq_ind_tbl, struct mlx5_ib_rwq_ind_table, ib_rwq_ind_tbl);
+}
+
 static inline struct mlx5_ib_srq *to_mibsrq(struct mlx5_core_srq *msrq)
 {
 	return container_of(msrq, struct mlx5_ib_srq, msrq);
@@ -797,6 +807,10 @@ struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
 int mlx5_ib_destroy_wq(struct ib_wq *wq);
 int mlx5_ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *wq_attr,
 		      u32 wq_attr_mask, struct ib_udata *udata);
+struct ib_rwq_ind_table *mlx5_ib_create_rwq_ind_table(struct ib_device *device,
+						      struct ib_rwq_ind_table_init_attr *init_attr,
+						      struct ib_udata *udata);
+int mlx5_ib_destroy_rwq_ind_table(struct ib_rwq_ind_table *wq_ind_table);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 extern struct workqueue_struct *mlx5_ib_page_fault_wq;

commit 79b20a6c3014c789253fcb1ac4f09f8bdee2e94b
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Mon May 23 15:20:50 2016 +0300

    IB/mlx5: Add receive Work Queue verbs
    
    A QP can be created without internal WQs "packaged" inside it,
    this QP can be configured to use "external" WQ object as its
    receive/send queue.
    
    WQ is a necessary component for RSS technology since RSS mechanism
    is supposed to distribute the traffic between multiple
    Receive Work Queues
    
    Receive WQs are implemented by RQs.
    
    Implement the WQ creation, modification and destruction verbs.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c4a9825828bc..62d4e137593a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -217,12 +217,36 @@ struct mlx5_ib_wq {
 	void		       *qend;
 };
 
+struct mlx5_ib_rwq {
+	struct ib_wq		ibwq;
+	u32			rqn;
+	u32			rq_num_pas;
+	u32			log_rq_stride;
+	u32			log_rq_size;
+	u32			rq_page_offset;
+	u32			log_page_size;
+	struct ib_umem		*umem;
+	size_t			buf_size;
+	unsigned int		page_shift;
+	int			create_type;
+	struct mlx5_db		db;
+	u32			user_index;
+	u32			wqe_count;
+	u32			wqe_shift;
+	int			wq_sig;
+};
+
 enum {
 	MLX5_QP_USER,
 	MLX5_QP_KERNEL,
 	MLX5_QP_EMPTY
 };
 
+enum {
+	MLX5_WQ_USER,
+	MLX5_WQ_KERNEL
+};
+
 /*
  * Connect-IB can trigger up to four concurrent pagefaults
  * per-QP.
@@ -628,6 +652,11 @@ static inline struct mlx5_ib_qp *to_mqp(struct ib_qp *ibqp)
 	return container_of(ibqp, struct mlx5_ib_qp, ibqp);
 }
 
+static inline struct mlx5_ib_rwq *to_mrwq(struct ib_wq *ibwq)
+{
+	return container_of(ibwq, struct mlx5_ib_rwq, ibwq);
+}
+
 static inline struct mlx5_ib_srq *to_mibsrq(struct mlx5_core_srq *msrq)
 {
 	return container_of(msrq, struct mlx5_ib_srq, msrq);
@@ -762,6 +791,12 @@ int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
+struct ib_wq *mlx5_ib_create_wq(struct ib_pd *pd,
+				struct ib_wq_init_attr *init_attr,
+				struct ib_udata *udata);
+int mlx5_ib_destroy_wq(struct ib_wq *wq);
+int mlx5_ib_modify_wq(struct ib_wq *wq, struct ib_wq_attr *wq_attr,
+		      u32 wq_attr_mask, struct ib_udata *udata);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 extern struct workqueue_struct *mlx5_ib_page_fault_wq;

commit 0651ec932afffce6547efb3e0352e5d229273962
Merge: e9bb8af98a98 ba987e51a637 78c49f83ee28 e3614bc9dc44 37aa5c36aa70 cff5a0f3a3cd
Author: Doug Ledford <dledford@redhat.com>
Date:   Fri May 13 19:40:38 2016 -0400

    Merge branches 'cxgb4-2', 'i40iw-2', 'ipoib', 'misc-4.7' and 'mlx5-fcs' into k.o/for-4.7

commit 358e42ea66e26d30a7a3e2c967c78f01ec31fe4f
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Sun Apr 17 17:19:37 2016 +0300

    IB/mlx5: Add Scatter FCS support for Raw Packet QP
    
    Enable Scatter FCS in the RQ context when the user passes
    Scatter FCS create flag.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b46c25542a7c..fb0a110b66c0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -356,6 +356,7 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 5,
 	/* QP uses 1 as its source QP number */
 	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
+	MLX5_IB_QP_CAP_SCATTER_FCS		= 1 << 7,
 };
 
 struct mlx5_umr_wr {

commit 37aa5c36aa70c9fc5f633b89cce990f04aaa3cd4
Author: Guy Levi <guyle@mellanox.com>
Date:   Wed Apr 27 16:49:50 2016 +0300

    IB/mlx5: Add UARs write-combining and non-cached mapping
    
    By this patch, the user space library will be able to improve
    performance using appropriate ringing DoorBell method according to the
    memory type it asked for.
    
    Currently only one mapping command is allowed for UARs:
    MLX5_IB_MMAP_REGULAR_PAGE. Using this mapping, the kernel maps the
    UARs to write-combining (WC) if the system supports it.
    If the system is not supporting WC the UARs are mapped to
    non-cached(NC). In this case the user space library can't tell which
    mapping is applied.
    This patch adds 2 new mapping commands: MLX5_IB_MMAP_WC_PAGE and
    MLX5_IB_MMAP_NC_PAGE. For these commands the kernel maps exactly as
    requested and fails if it can't.
    
    Since there is no generic way to check if the requested memory region
    can be mapped as WC, driver enables conclusive WC mapping only for
    x86, PowerPC and ARM which support WC for the device's memory region.
    
    Signed-off-by: Guy Levy <guyle@mellanox.com>
    Signed-off-by: Moshe Lazer <moshel@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b46c25542a7c..6e81217b7c67 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -70,6 +70,8 @@ enum {
 enum mlx5_ib_mmap_cmd {
 	MLX5_IB_MMAP_REGULAR_PAGE		= 0,
 	MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES	= 1,
+	MLX5_IB_MMAP_WC_PAGE			= 2,
+	MLX5_IB_MMAP_NC_PAGE			= 3,
 	/* 5 is chosen in order to be compatible with old versions of libmlx5 */
 	MLX5_IB_MMAP_CORE_CLOCK			= 5,
 };

commit 9aa8b3217ed3c13d4e3496020b140da0e6f49a08
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Thu May 12 10:49:15 2016 -0700

    IB/core: Enhance ib_map_mr_sg()
    
    The SRP initiator allows to set max_sectors to a value that exceeds
    the largest amount of data that can be mapped at once with an mlx4
    HCA using fast registration and a page size of 4 KB. Hence modify
    ib_map_mr_sg() such that it can map partial sg-elements. If an
    sg-element has been mapped partially, let the caller know
    which fraction has been mapped by adjusting *sg_offset.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Tested-by: Laurence Oberman <loberman@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8c835b2be39e..f05cf57f874c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -713,7 +713,7 @@ struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 			       enum ib_mr_type mr_type,
 			       u32 max_num_sg);
 int mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
-		unsigned int sg_offset);
+		      unsigned int *sg_offset);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 			const struct ib_mad_hdr *in, size_t in_mad_size,

commit ff2ba9936591a1364ae21adf18366dca7608395a
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue May 3 18:01:04 2016 +0200

    IB/core: Add passing an offset into the SG to ib_map_mr_sg
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b46c25542a7c..8c835b2be39e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -712,9 +712,8 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 			       enum ib_mr_type mr_type,
 			       u32 max_num_sg);
-int mlx5_ib_map_mr_sg(struct ib_mr *ibmr,
-		      struct scatterlist *sg,
-		      int sg_nents);
+int mlx5_ib_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg, int sg_nents,
+		unsigned int sg_offset);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 			const struct ib_mad_hdr *in, size_t in_mad_size,

commit 9967c70abc929e9b910be8d419fdf6a85411a066
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Mar 23 11:37:45 2016 +0100

    IB/mlx5: fix VFs callback function prototypes
    
    The previous patch that added a couple of callback functions put
    the declarations inside of an #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING,
    which causes the build to fail if that option is disabled:
    
    drivers/infiniband/hw/mlx5/main.c: In function 'mlx5_ib_add':
    drivers/infiniband/hw/mlx5/main.c:2358:31: error: 'mlx5_ib_get_vf_config' undeclared (first use in this function)
    
    This moves the four declarations below the #ifdef section so they
    are always available.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Fixes: eff901d30e6c ("IB/mlx5: Implement callbacks for manipulating VFs")
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f16c818ad2e6..b46c25542a7c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -776,15 +776,6 @@ void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			      unsigned long end);
-int mlx5_ib_get_vf_config(struct ib_device *device, int vf,
-			  u8 port, struct ifla_vf_info *info);
-int mlx5_ib_set_vf_link_state(struct ib_device *device, int vf,
-			      u8 port, int state);
-int mlx5_ib_get_vf_stats(struct ib_device *device, int vf,
-			 u8 port, struct ifla_vf_stats *stats);
-int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
-			u64 guid, int type);
-
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 {
@@ -801,6 +792,15 @@ static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
 
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
+int mlx5_ib_get_vf_config(struct ib_device *device, int vf,
+			  u8 port, struct ifla_vf_info *info);
+int mlx5_ib_set_vf_link_state(struct ib_device *device, int vf,
+			      u8 port, int state);
+int mlx5_ib_get_vf_stats(struct ib_device *device, int vf,
+			 u8 port, struct ifla_vf_stats *stats);
+int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
+			u64 guid, int type);
+
 __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
 			       int index);
 

commit eff901d30e6cebd940072637f112ce4d0090ac12
Author: Eli Cohen <eli@mellanox.com>
Date:   Fri Mar 11 22:58:42 2016 +0200

    IB/mlx5: Implement callbacks for manipulating VFs
    
    Implement the IB defined callbacks used to manipulate the policy for the
    link state, set GUIDs or get statistics information. This functionality
    is added into a new file that will be used to add any SRIOV related
    functionality to the mlx5 IB layer.
    
    The following callbacks have been added:
    
    mlx5_ib_get_vf_config
    mlx5_ib_set_vf_link_state
    mlx5_ib_get_vf_stats
    mlx5_ib_set_vf_guid
    
    In addition, publish whether this device is based on a virtual function.
    
    In mlx5 supported devices, virtual functions are implemented as vHCAs.
    vHCAs have their own QP number space so it is possible that two vHCAs
    will use a QP with the same number at the same time.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 76b2b42e0535..f16c818ad2e6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -776,6 +776,14 @@ void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			      unsigned long end);
+int mlx5_ib_get_vf_config(struct ib_device *device, int vf,
+			  u8 port, struct ifla_vf_info *info);
+int mlx5_ib_set_vf_link_state(struct ib_device *device, int vf,
+			      u8 port, int state);
+int mlx5_ib_get_vf_stats(struct ib_device *device, int vf,
+			 u8 port, struct ifla_vf_stats *stats);
+int mlx5_ib_set_vf_guid(struct ib_device *device, int vf, u8 port,
+			u64 guid, int type);
 
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)

commit d2ad9cc75963714d04d4596c226a499765950dbf
Merge: 76b06402796c 35d1901134e9 318d311e8f01 95f60bb8118c
Author: Doug Ledford <dledford@redhat.com>
Date:   Wed Mar 16 13:38:28 2016 -0400

    Merge branches 'mlx4', 'mlx5' and 'ocrdma' into k.o/for-4.6

commit 35d1901134e97cf95c0ab6ef70f5aead6cb34e9e
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Mon Mar 7 18:51:47 2016 +0200

    IB/mlx5: Add support for don't trap rules
    
    Each bypass flow steering priority will be split into two priorities:
    1. Priority for don't trap rules.
    2. Priority for normal rules.
    
    When user creates a flow using IB_FLOW_ATTR_FLAGS_DONT_TRAP flag, the
    driver creates two flow rules, one used for receiving the traffic and
    the other one for forwarding the packet to continue matching in lower
    or equal priorities.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d2b9737baa36..bd84b1fbb787 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -126,7 +126,7 @@ struct mlx5_ib_pd {
 };
 
 #define MLX5_IB_FLOW_MCAST_PRIO		(MLX5_BY_PASS_NUM_PRIOS - 1)
-#define MLX5_IB_FLOW_LAST_PRIO		(MLX5_IB_FLOW_MCAST_PRIO - 1)
+#define MLX5_IB_FLOW_LAST_PRIO		(MLX5_BY_PASS_NUM_REGULAR_PRIOS - 1)
 #if (MLX5_IB_FLOW_LAST_PRIO <= 0)
 #error "Invalid number of bypass priorities"
 #endif

commit b005d316471374b1ff26df8c8460cc1ea9186647
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Mon Feb 29 19:07:33 2016 +0200

    mlx5: Add arbitrary sg list support
    
    Allocate proper context for arbitrary scatterlist registration
    If ib_alloc_mr is called with IB_MR_MAP_ARB_SG, the driver
    allocate a private klm list instead of a private page list.
    Set the UMR wqe correctly when posting the fast registration.
    
    Also, expose device cap IB_DEVICE_MAP_ARB_SG according to the
    device id (until we have a FW bit that correctly exposes it).
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 3c02b3ce76ae..60b89629f091 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -447,6 +447,7 @@ struct mlx5_ib_mr {
 	int			ndescs;
 	int			max_descs;
 	int			desc_size;
+	int			access_mode;
 	struct mlx5_core_mkey	mmkey;
 	struct ib_umem	       *umem;
 	struct mlx5_shared_mr_info	*smr_info;

commit add08d765e942eab8eb15a592baeb372a3dd6831
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Mar 3 09:38:22 2016 +0100

    IB/mlx5: Convert UMR CQ to new CQ API
    
    Simplifies the code, and makes it more fair vs other users by using a
    softirq for polling.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Haggai Eran <haggaie@mellanox.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 648d2e2e445b..3c02b3ce76ae 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -468,16 +468,11 @@ struct mlx5_ib_mw {
 };
 
 struct mlx5_ib_umr_context {
+	struct ib_cqe		cqe;
 	enum ib_wc_status	status;
 	struct completion	done;
 };
 
-static inline void mlx5_ib_init_umr_context(struct mlx5_ib_umr_context *context)
-{
-	context->status = -1;
-	init_completion(&context->done);
-}
-
 struct umr_common {
 	struct ib_pd	*pd;
 	struct ib_cq	*cq;
@@ -762,7 +757,6 @@ int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
-void mlx5_umr_cq_handler(struct ib_cq *cq, void *cq_context);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 

commit d2370e0a573e5c5ea9c96373558727abb3ea71f7
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Feb 29 18:05:30 2016 +0200

    IB/mlx5: Add memory windows allocation support
    
    This patch adds user-space support for memory windows allocation and
    deallocation. It also exposes the supported types via
    query_device_caps verb.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Tested-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4167d67179ff..648d2e2e445b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -43,6 +43,7 @@
 #include <linux/mlx5/srq.h>
 #include <linux/types.h>
 #include <linux/mlx5/transobj.h>
+#include <rdma/ib_user_verbs.h>
 
 #define mlx5_ib_dbg(dev, format, arg...)				\
 pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
@@ -461,6 +462,11 @@ struct mlx5_ib_mr {
 	int			access_flags; /* Needed for rereg MR */
 };
 
+struct mlx5_ib_mw {
+	struct ib_mw		ibmw;
+	struct mlx5_core_mkey	mmkey;
+};
+
 struct mlx5_ib_umr_context {
 	enum ib_wc_status	status;
 	struct completion	done;
@@ -633,6 +639,11 @@ static inline struct mlx5_ib_mr *to_mmr(struct ib_mr *ibmr)
 	return container_of(ibmr, struct mlx5_ib_mr, ibmr);
 }
 
+static inline struct mlx5_ib_mw *to_mmw(struct ib_mw *ibmw)
+{
+	return container_of(ibmw, struct mlx5_ib_mw, ibmw);
+}
+
 struct mlx5_ib_ah {
 	struct ib_ah		ibah;
 	struct mlx5_av		av;
@@ -693,6 +704,9 @@ struct ib_mr *mlx5_ib_get_dma_mr(struct ib_pd *pd, int acc);
 struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 virt_addr, int access_flags,
 				  struct ib_udata *udata);
+struct ib_mw *mlx5_ib_alloc_mw(struct ib_pd *pd, enum ib_mw_type type,
+			       struct ib_udata *udata);
+int mlx5_ib_dealloc_mw(struct ib_mw *mw);
 int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
 		       int npages, int zap);
 int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,

commit a606b0f6691daf861482f8b77326f672238ffbfd
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Feb 29 18:05:28 2016 +0200

    net/mlx5: Refactor mlx5_core_mr to mkey
    
    Mlx5's mkey mechanism is also used for memory windows.
    The current code base uses MR (memory region) naming, which is
    inaccurate. Changing MR to mkey in order to represent its different
    usages more accurately.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f84ec2b6425c..4167d67179ff 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -446,7 +446,7 @@ struct mlx5_ib_mr {
 	int			ndescs;
 	int			max_descs;
 	int			desc_size;
-	struct mlx5_core_mr	mmr;
+	struct mlx5_core_mkey	mmkey;
 	struct ib_umem	       *umem;
 	struct mlx5_shared_mr_info	*smr_info;
 	struct list_head	list;
@@ -603,9 +603,9 @@ static inline struct mlx5_ib_qp *to_mibqp(struct mlx5_core_qp *mqp)
 	return container_of(mqp, struct mlx5_ib_qp_base, mqp)->container_mibqp;
 }
 
-static inline struct mlx5_ib_mr *to_mibmr(struct mlx5_core_mr *mmr)
+static inline struct mlx5_ib_mr *to_mibmr(struct mlx5_core_mkey *mmkey)
 {
-	return container_of(mmr, struct mlx5_ib_mr, mmr);
+	return container_of(mmkey, struct mlx5_ib_mr, mmkey);
 }
 
 static inline struct mlx5_ib_pd *to_mpd(struct ib_pd *ibpd)

commit 56e11d628c5d0553d9fc2ca1855144970e6b9eb6
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Mon Feb 29 16:46:51 2016 +0200

    IB/mlx5: Added support for re-registration of MRs
    
    This patch adds support for re-registration of memory regions in MLX5.
    The functionality is basically the same as deregister followed by
    register, but attempts to reuse the existing resources as much as
    possible.
    Original memory keys are kept if possible, saving the need to
    communicate new ones to remote peers.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0142efb5dd9c..f84ec2b6425c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -162,6 +162,11 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_SEND_UMR_UNREG	IB_SEND_RESERVED_START
 #define MLX5_IB_SEND_UMR_FAIL_IF_FREE (IB_SEND_RESERVED_START << 1)
 #define MLX5_IB_SEND_UMR_UPDATE_MTT (IB_SEND_RESERVED_START << 2)
+
+#define MLX5_IB_SEND_UMR_UPDATE_TRANSLATION	(IB_SEND_RESERVED_START << 3)
+#define MLX5_IB_SEND_UMR_UPDATE_PD		(IB_SEND_RESERVED_START << 4)
+#define MLX5_IB_SEND_UMR_UPDATE_ACCESS		IB_SEND_RESERVED_END
+
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
 /*
  * IB_QPT_GSI creates the software wrapper around GSI, and MLX5_IB_QPT_HW_GSI
@@ -453,6 +458,7 @@ struct mlx5_ib_mr {
 	struct mlx5_core_sig_ctx    *sig;
 	int			live;
 	void			*descs_alloc;
+	int			access_flags; /* Needed for rereg MR */
 };
 
 struct mlx5_ib_umr_context {
@@ -689,6 +695,9 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  struct ib_udata *udata);
 int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
 		       int npages, int zap);
+int mlx5_ib_rereg_user_mr(struct ib_mr *ib_mr, int flags, u64 start,
+			  u64 length, u64 virt_addr, int access_flags,
+			  struct ib_pd *pd, struct ib_udata *udata);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 			       enum ib_mr_type mr_type,

commit 25361e02c44873a17e0148d9d5c42fa2e938a019
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Mon Feb 29 15:45:08 2016 +0200

    IB/mlx5: Generate completions in software
    
    The GSI QP emulation requires also emulating completions for transmitted
    MADs. The CQ on which these completions are generated can also be used by
    the hardware, and the MAD layer is free to use any CQ of the device for the
    GSI QP.
    
    Add a method for generating software completions to each mlx5 CQ. Software
    completions are polled first, and generate calls to the completion handler
    callback if necessary.
    
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a8fc345c088a..0142efb5dd9c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -393,6 +393,14 @@ struct mlx5_ib_cq {
 	struct ib_umem	       *resize_umem;
 	int			cqe_size;
 	u32			create_flags;
+	struct list_head	wc_list;
+	enum ib_cq_notify_flags notify_flags;
+	struct work_struct	notify_work;
+};
+
+struct mlx5_ib_wc {
+	struct ib_wc wc;
+	struct list_head list;
 };
 
 struct mlx5_ib_srq {
@@ -785,6 +793,8 @@ int mlx5_ib_gsi_post_recv(struct ib_qp *qp, struct ib_recv_wr *wr,
 			  struct ib_recv_wr **bad_wr);
 void mlx5_ib_gsi_pkey_change(struct mlx5_ib_gsi_qp *gsi);
 
+int mlx5_ib_generate_wc(struct ib_cq *ibcq, struct ib_wc *wc);
+
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;

commit 7722f47e71e58592a2ba4437d27c802ba1c64e08
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Mon Feb 29 15:45:07 2016 +0200

    IB/mlx5: Create GSI transmission QPs when P_Key table is changed
    
    Whenever the P_Key table is changed, we create the required GSI
    transmission QPs on-demand.
    
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c68a9135831f..a8fc345c088a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -510,7 +510,9 @@ struct mlx5_mr_cache {
 struct mlx5_ib_gsi_qp;
 
 struct mlx5_ib_port_resources {
+	struct mlx5_ib_resources *devr;
 	struct mlx5_ib_gsi_qp *gsi;
+	struct work_struct pkey_change_work;
 };
 
 struct mlx5_ib_resources {
@@ -781,6 +783,7 @@ int mlx5_ib_gsi_post_send(struct ib_qp *qp, struct ib_send_wr *wr,
 			  struct ib_send_wr **bad_wr);
 int mlx5_ib_gsi_post_recv(struct ib_qp *qp, struct ib_recv_wr *wr,
 			  struct ib_recv_wr **bad_wr);
+void mlx5_ib_gsi_pkey_change(struct mlx5_ib_gsi_qp *gsi);
 
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit d16e91daf446c605a92112889552f9df757186bc
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Mon Feb 29 15:45:05 2016 +0200

    IB/mlx5: Add GSI QP wrapper
    
    mlx5 creates special GSI QPs that has limited ability to control the P_Key
    of transmitted packets. The sent P_Key is taken from the QP object,
    similarly to what happens with regular UD QPs.
    
    Create a software wrapper around GSI QPs that with the following patches
    will be able to emulate the functionality of a GSI QP including control of
    the P_Key per work request.
    
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 32699f92480a..c68a9135831f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -163,6 +163,11 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_SEND_UMR_FAIL_IF_FREE (IB_SEND_RESERVED_START << 1)
 #define MLX5_IB_SEND_UMR_UPDATE_MTT (IB_SEND_RESERVED_START << 2)
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
+/*
+ * IB_QPT_GSI creates the software wrapper around GSI, and MLX5_IB_QPT_HW_GSI
+ * creates the actual hardware QP.
+ */
+#define MLX5_IB_QPT_HW_GSI	IB_QPT_RESERVED2
 #define MLX5_IB_WR_UMR		IB_WR_RESERVED1
 
 /* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
@@ -502,6 +507,12 @@ struct mlx5_mr_cache {
 	unsigned long		last_add;
 };
 
+struct mlx5_ib_gsi_qp;
+
+struct mlx5_ib_port_resources {
+	struct mlx5_ib_gsi_qp *gsi;
+};
+
 struct mlx5_ib_resources {
 	struct ib_cq	*c0;
 	struct ib_xrcd	*x0;
@@ -509,6 +520,9 @@ struct mlx5_ib_resources {
 	struct ib_pd	*p0;
 	struct ib_srq	*s0;
 	struct ib_srq	*s1;
+	struct mlx5_ib_port_resources ports[2];
+	/* Protects changes to the port resources */
+	struct mutex	mutex;
 };
 
 struct mlx5_roce {
@@ -754,6 +768,20 @@ static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
 __be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
 			       int index);
 
+/* GSI QP helper functions */
+struct ib_qp *mlx5_ib_gsi_create_qp(struct ib_pd *pd,
+				    struct ib_qp_init_attr *init_attr);
+int mlx5_ib_gsi_destroy_qp(struct ib_qp *qp);
+int mlx5_ib_gsi_modify_qp(struct ib_qp *qp, struct ib_qp_attr *attr,
+			  int attr_mask);
+int mlx5_ib_gsi_query_qp(struct ib_qp *qp, struct ib_qp_attr *qp_attr,
+			 int qp_attr_mask,
+			 struct ib_qp_init_attr *qp_init_attr);
+int mlx5_ib_gsi_post_send(struct ib_qp *qp, struct ib_send_wr *wr,
+			  struct ib_send_wr **bad_wr);
+int mlx5_ib_gsi_post_recv(struct ib_qp *qp, struct ib_recv_wr *wr,
+			  struct ib_recv_wr **bad_wr);
+
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;
@@ -773,7 +801,7 @@ static inline u8 convert_access(int acc)
 
 static inline int is_qp1(enum ib_qp_type qp_type)
 {
-	return qp_type == IB_QPT_GSI;
+	return qp_type == MLX5_IB_QPT_HW_GSI;
 }
 
 #define MLX5_MAX_UMR_SHIFT 16

commit b11a4f9cde1c06e0073662882b60c1fb95a1d597
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Mon Feb 29 15:45:03 2016 +0200

    IB/mlx5: Add support for setting source QP number
    
    In order to create multiple GSI QPs, we need to set the source QP number to
    one on all these QPs. Add the necessary definitions and infrastructure to
    do that.
    
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 14396b0eac74..32699f92480a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -165,6 +165,18 @@ struct mlx5_ib_flow_db {
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
 #define MLX5_IB_WR_UMR		IB_WR_RESERVED1
 
+/* Private QP creation flags to be passed in ib_qp_init_attr.create_flags.
+ *
+ * These flags are intended for internal use by the mlx5_ib driver, and they
+ * rely on the range reserved for that use in the ib_qp_create_flags enum.
+ */
+
+/* Create a UD QP whose source QP number is 1 */
+static inline enum ib_qp_create_flags mlx5_ib_create_qp_sqpn_qp1(void)
+{
+	return IB_QP_CREATE_RESERVED_START;
+}
+
 struct wr_list {
 	u16	opcode;
 	u16	next;
@@ -331,6 +343,8 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_MANAGED_SEND             = IB_QP_CREATE_MANAGED_SEND,
 	MLX5_IB_QP_MANAGED_RECV             = IB_QP_CREATE_MANAGED_RECV,
 	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 5,
+	/* QP uses 1 as its source QP number */
+	MLX5_IB_QP_SQPN_QP1			= 1 << 6,
 };
 
 struct mlx5_umr_wr {

commit f031396531fe2b1a6ffb4fa5eceb9c1fa276869a
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Sun Feb 21 16:27:17 2016 +0200

    IB/mlx5: Implement UD QP offloads for IPoIB in the TX flow
    
    In order to support LSO and CSUM in the TX flow the driver does the
    following:
    * LSO bit for the enum mlx5_ib_qp_flags was added, indicates QP that
      supports LSO offloads.
    * Enables the special offload when the QP is created, and enable the
      special work request id (IB_WR_LSO) when comes.
    * Calculates the size of the WQE according to the new WQE format that
      support these offloads.
    * Handles the new WQE format when arrived, sets the relevant
      fields, and copies the needed data.
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d2b9737baa36..14396b0eac74 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -325,11 +325,12 @@ struct mlx5_ib_cq_buf {
 };
 
 enum mlx5_ib_qp_flags {
-	MLX5_IB_QP_BLOCK_MULTICAST_LOOPBACK     = 1 << 0,
-	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 1,
-	MLX5_IB_QP_CROSS_CHANNEL		= 1 << 2,
-	MLX5_IB_QP_MANAGED_SEND			= 1 << 3,
-	MLX5_IB_QP_MANAGED_RECV			= 1 << 4,
+	MLX5_IB_QP_LSO                          = IB_QP_CREATE_IPOIB_UD_LSO,
+	MLX5_IB_QP_BLOCK_MULTICAST_LOOPBACK     = IB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,
+	MLX5_IB_QP_CROSS_CHANNEL            = IB_QP_CREATE_CROSS_CHANNEL,
+	MLX5_IB_QP_MANAGED_SEND             = IB_QP_CREATE_MANAGED_SEND,
+	MLX5_IB_QP_MANAGED_RECV             = IB_QP_CREATE_MANAGED_RECV,
+	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 5,
 };
 
 struct mlx5_umr_wr {

commit 048ccca8c1c8f583deec3367d7df521bb1f542ae
Merge: b3e27d5d4a29 34356f64ac0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 23 18:45:06 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "Initial roundup of 4.5 merge window patches
    
       - Remove usage of ib_query_device and instead store attributes in
         ib_device struct
    
       - Move iopoll out of block and into lib, rename to irqpoll, and use
         in several places in the rdma stack as our new completion queue
         polling library mechanism.  Update the other block drivers that
         already used iopoll to use the new mechanism too.
    
       - Replace the per-entry GID table locks with a single GID table lock
    
       - IPoIB multicast cleanup
    
       - Cleanups to the IB MR facility
    
       - Add support for 64bit extended IB counters
    
       - Fix for netlink oops while parsing RDMA nl messages
    
       - RoCEv2 support for the core IB code
    
       - mlx4 RoCEv2 support
    
       - mlx5 RoCEv2 support
    
       - Cross Channel support for mlx5
    
       - Timestamp support for mlx5
    
       - Atomic support for mlx5
    
       - Raw QP support for mlx5
    
       - MAINTAINERS update for mlx4/mlx5
    
       - Misc ocrdma, qib, nes, usNIC, cxgb3, cxgb4, mlx4, mlx5 updates
    
       - Add support for remote invalidate to the iSER driver (pushed
         through the RDMA tree due to dependencies, acknowledged by nab)
    
       - Update to NFSoRDMA (pushed through the RDMA tree due to
         dependencies, acknowledged by Bruce)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (169 commits)
      IB/mlx5: Unify CQ create flags check
      IB/mlx5: Expose Raw Packet QP to user space consumers
      {IB, net}/mlx5: Move the modify QP operation table to mlx5_ib
      IB/mlx5: Support setting Ethernet priority for Raw Packet QPs
      IB/mlx5: Add Raw Packet QP query functionality
      IB/mlx5: Add create and destroy functionality for Raw Packet QP
      IB/mlx5: Refactor mlx5_ib_qp to accommodate other QP types
      IB/mlx5: Allocate a Transport Domain for each ucontext
      net/mlx5_core: Warn on unsupported events of QP/RQ/SQ
      net/mlx5_core: Add RQ and SQ event handling
      net/mlx5_core: Export transport objects
      IB/mlx5: Expose CQE version to user-space
      IB/mlx5: Add CQE version 1 support to user QPs and SRQs
      IB/mlx5: Fix data validation in mlx5_ib_alloc_ucontext
      IB/sa: Fix netlink local service GFP crash
      IB/srpt: Remove redundant wc array
      IB/qib: Improve ipoib UD performance
      IB/mlx4: Advertise RoCE v2 support
      IB/mlx4: Create and use another QP1 for RoCEv2
      IB/mlx4: Enable send of RoCE QP1 packets with IP/UDP headers
      ...

commit 34356f64ac0df2326fa50e2d4bca6f7c03ed16c1
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Tue Dec 29 17:01:30 2015 +0200

    IB/mlx5: Unify CQ create flags check
    
    The create_cq() can receive creation flags which were used
    differently by two commits which added create_cq extended
    command and cross-channel. The merged code caused to not
    accept any flags at all.
    
    This patch unifies the check into one function and one return
    error code.
    
    Fixes: 972ecb821379 ("IB/mlx5: Add create_cq extended command")
    Fixes: 051f263098a9 ("IB/mlx5: Add driver cross-channel support")
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 726e31de59d5..d475f83c295b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -739,7 +739,8 @@ static inline u32 check_cq_create_flags(u32 flags)
 	 * It returns non-zero value for unsupported CQ
 	 * create flags, otherwise it returns zero.
 	 */
-	return (flags & ~IB_CQ_FLAGS_IGNORE_OVERRUN);
+	return (flags & ~(IB_CQ_FLAGS_IGNORE_OVERRUN |
+			  IB_CQ_FLAGS_TIMESTAMP_COMPLETION));
 }
 
 static inline int verify_assign_uidx(u8 cqe_version, u32 cmd_uidx,

commit 0fb2ed66a14c8c34096d6a8cff5112356c5e9ea2
Author: majd@mellanox.com <majd@mellanox.com>
Date:   Thu Jan 14 19:13:04 2016 +0200

    IB/mlx5: Add create and destroy functionality for Raw Packet QP
    
    This patch adds support for Raw Packet QP for the mlx5 device.
    
    Raw Packet QP, unlike other QP types, has no matching mlx5_core_qp
    object but rather it is built of RQ/SQ/TIR/TIS/TD mlx5_core object.
    
    Since the SQ and RQ work-queue (WQ) buffers are not contiguous like
    other QPs, we allocate separate buffers in the user-space and pass
    the address of each one of them separately to the kernel.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 71e44302cab2..726e31de59d5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -211,9 +211,35 @@ struct mlx5_ib_qp_trans {
 	u8			resp_depth;
 };
 
+struct mlx5_ib_rq {
+	struct mlx5_ib_qp_base base;
+	struct mlx5_ib_wq	*rq;
+	struct mlx5_ib_ubuffer	ubuffer;
+	struct mlx5_db		*doorbell;
+	u32			tirn;
+	u8			state;
+};
+
+struct mlx5_ib_sq {
+	struct mlx5_ib_qp_base base;
+	struct mlx5_ib_wq	*sq;
+	struct mlx5_ib_ubuffer  ubuffer;
+	struct mlx5_db		*doorbell;
+	u32			tisn;
+	u8			state;
+};
+
+struct mlx5_ib_raw_packet_qp {
+	struct mlx5_ib_sq sq;
+	struct mlx5_ib_rq rq;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
-	struct mlx5_ib_qp_trans trans_qp;
+	union {
+		struct mlx5_ib_qp_trans trans_qp;
+		struct mlx5_ib_raw_packet_qp raw_packet_qp;
+	};
 	struct mlx5_buf		buf;
 
 	struct mlx5_db		db;

commit 19098df2da784c851532c78bd570cb1c8f4aff52
Author: majd@mellanox.com <majd@mellanox.com>
Date:   Thu Jan 14 19:13:03 2016 +0200

    IB/mlx5: Refactor mlx5_ib_qp to accommodate other QP types
    
    Extract specific IB QP fields to mlx5_ib_qp_trans structure.
    The mlx5_core QP object resides in mlx5_ib_qp_base, which all QP types
    inherit from. When we need to find mlx5_ib_qp using mlx5_core QP
    (event handling and co), we use a pointer that resides in
    mlx5_ib_qp_base.
    
    In addition, we delete all redundant fields that weren't used anywhere
    in the code:
    -doorbell_qpn
    -sq_max_wqes_per_wr
    -sq_spare_wqes
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2ccc3d95d574..71e44302cab2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -191,35 +191,44 @@ struct mlx5_ib_pfault {
 	struct mlx5_pagefault	mpfault;
 };
 
+struct mlx5_ib_ubuffer {
+	struct ib_umem	       *umem;
+	int			buf_size;
+	u64			buf_addr;
+};
+
+struct mlx5_ib_qp_base {
+	struct mlx5_ib_qp	*container_mibqp;
+	struct mlx5_core_qp	mqp;
+	struct mlx5_ib_ubuffer	ubuffer;
+};
+
+struct mlx5_ib_qp_trans {
+	struct mlx5_ib_qp_base	base;
+	u16			xrcdn;
+	u8			alt_port;
+	u8			atomic_rd_en;
+	u8			resp_depth;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
-	struct mlx5_core_qp	mqp;
+	struct mlx5_ib_qp_trans trans_qp;
 	struct mlx5_buf		buf;
 
 	struct mlx5_db		db;
 	struct mlx5_ib_wq	rq;
 
-	u32			doorbell_qpn;
 	u8			sq_signal_bits;
 	u8			fm_cache;
-	int			sq_max_wqes_per_wr;
-	int			sq_spare_wqes;
 	struct mlx5_ib_wq	sq;
 
-	struct ib_umem	       *umem;
-	int			buf_size;
-
 	/* serialize qp state modifications
 	 */
 	struct mutex		mutex;
-	u16			xrcdn;
 	u32			flags;
 	u8			port;
-	u8			alt_port;
-	u8			atomic_rd_en;
-	u8			resp_depth;
 	u8			state;
-	int			mlx_type;
 	int			wq_sig;
 	int			scat_cqe;
 	int			max_inline_data;
@@ -489,7 +498,7 @@ static inline struct mlx5_ib_cq *to_mcq(struct ib_cq *ibcq)
 
 static inline struct mlx5_ib_qp *to_mibqp(struct mlx5_core_qp *mqp)
 {
-	return container_of(mqp, struct mlx5_ib_qp, mqp);
+	return container_of(mqp, struct mlx5_ib_qp_base, mqp)->container_mibqp;
 }
 
 static inline struct mlx5_ib_mr *to_mibmr(struct mlx5_core_mr *mmr)
@@ -567,7 +576,8 @@ int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 		      struct ib_recv_wr **bad_wr);
 void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
-			  void *buffer, u32 length);
+			  void *buffer, u32 length,
+			  struct mlx5_ib_qp_base *base);
 struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_ucontext *context,

commit 146d2f1af3245a10b13eef263687e54f4e253d1d
Author: majd@mellanox.com <majd@mellanox.com>
Date:   Thu Jan 14 19:13:02 2016 +0200

    IB/mlx5: Allocate a Transport Domain for each ucontext
    
    Transport Domain groups several TIS and TIR object. By grouping
    these object, it defines wheather local loopback packets that
    are sent from the TIS objects in the group are received by the
    TIR objects in the same group.
    
    Allocate a Transport Domain(TD) for each user context to be used
    in the future by Raw Packet QP for Self-Loopback Control.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a6a57de278b4..2ccc3d95d574 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -42,6 +42,7 @@
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/srq.h>
 #include <linux/types.h>
+#include <linux/mlx5/transobj.h>
 
 #define mlx5_ib_dbg(dev, format, arg...)				\
 pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
@@ -110,6 +111,8 @@ struct mlx5_ib_ucontext {
 	struct mutex		db_page_mutex;
 	struct mlx5_uuar_info	uuari;
 	u8			cqe_version;
+	/* Transport Domain number */
+	u32			tdn;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)

commit cfb5e088e26ae0e12064171f645ed022cf6d55b9
Author: Haggai Abramovsky <hagaya@mellanox.com>
Date:   Thu Jan 14 19:12:57 2016 +0200

    IB/mlx5: Add CQE version 1 support to user QPs and SRQs
    
    Enforce working with CQE version 1 when the user supports CQE
    version 1 and asked to work this way.
    
    If the user still works with CQE version 0, then use the default
    CQE version to tell the Firmware that the user still works in the
    older mode.
    
    After this patch, the kernel still reports CQE version 0.
    
    Signed-off-by: Haggai Abramovsky <hagaya@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index d4b227126265..a6a57de278b4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -57,6 +57,8 @@ pr_warn("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
 
 #define field_avail(type, fld, sz) (offsetof(type, fld) +		\
 				    sizeof(((type *)0)->fld) <= (sz))
+#define MLX5_IB_DEFAULT_UIDX 0xffffff
+#define MLX5_USER_ASSIGNED_UIDX_MASK __mlx5_mask(qpc, user_index)
 
 enum {
 	MLX5_IB_MMAP_CMD_SHIFT	= 8,
@@ -94,6 +96,11 @@ enum {
 	MLX5_CROSS_CHANNEL_UUAR         = 0,
 };
 
+enum {
+	MLX5_CQE_VERSION_V0,
+	MLX5_CQE_VERSION_V1,
+};
+
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;
@@ -102,6 +109,7 @@ struct mlx5_ib_ucontext {
 	 */
 	struct mutex		db_page_mutex;
 	struct mlx5_uuar_info	uuari;
+	u8			cqe_version;
 };
 
 static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
@@ -694,4 +702,19 @@ static inline u32 check_cq_create_flags(u32 flags)
 	 */
 	return (flags & ~IB_CQ_FLAGS_IGNORE_OVERRUN);
 }
+
+static inline int verify_assign_uidx(u8 cqe_version, u32 cmd_uidx,
+				     u32 *user_index)
+{
+	if (cqe_version) {
+		if ((cmd_uidx == MLX5_IB_DEFAULT_UIDX) ||
+		    (cmd_uidx & ~MLX5_USER_ASSIGNED_UIDX_MASK))
+			return -EINVAL;
+		*user_index = cmd_uidx;
+	} else {
+		*user_index = MLX5_IB_DEFAULT_UIDX;
+	}
+
+	return 0;
+}
 #endif /* MLX5_IB_H */

commit 038d2ef87572757861a177b19f9d489def2c48b8
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Mon Jan 11 10:26:07 2016 +0200

    IB/mlx5: Add flow steering support
    
    Adding flow steering support by creating a flow-table per
    priority (if rules exist in the priority). mlx5_ib uses
    autogrouping and thus only creates the required destinations.
    
    Also includes adding of these flow steering utilities
    
    1. Parsing verbs flow attributes hardware steering specs.
    
    2. Check if flow is multicast - this is required in order to decide
    to which flow table will we add the steering rule.
    
    3. Set outer headers in flow match criteria to zeros.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 633347260b79..1474cccd1e0f 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -105,6 +105,36 @@ struct mlx5_ib_pd {
 	u32			pdn;
 };
 
+#define MLX5_IB_FLOW_MCAST_PRIO		(MLX5_BY_PASS_NUM_PRIOS - 1)
+#define MLX5_IB_FLOW_LAST_PRIO		(MLX5_IB_FLOW_MCAST_PRIO - 1)
+#if (MLX5_IB_FLOW_LAST_PRIO <= 0)
+#error "Invalid number of bypass priorities"
+#endif
+#define MLX5_IB_FLOW_LEFTOVERS_PRIO	(MLX5_IB_FLOW_MCAST_PRIO + 1)
+
+#define MLX5_IB_NUM_FLOW_FT		(MLX5_IB_FLOW_LEFTOVERS_PRIO + 1)
+struct mlx5_ib_flow_prio {
+	struct mlx5_flow_table		*flow_table;
+	unsigned int			refcount;
+};
+
+struct mlx5_ib_flow_handler {
+	struct list_head		list;
+	struct ib_flow			ibflow;
+	unsigned int			prio;
+	struct mlx5_flow_rule	*rule;
+};
+
+struct mlx5_ib_flow_db {
+	struct mlx5_ib_flow_prio	prios[MLX5_IB_NUM_FLOW_FT];
+	/* Protect flow steering bypass flow tables
+	 * when add/del flow rules.
+	 * only single add/removal of flow steering rule could be done
+	 * simultaneously.
+	 */
+	struct mutex			lock;
+};
+
 /* Use macros here so that don't have to duplicate
  * enum ib_send_flags and enum ib_qp_type for low-level driver
  */
@@ -171,9 +201,21 @@ struct mlx5_ib_pfault {
 	struct mlx5_pagefault	mpfault;
 };
 
+struct mlx5_ib_rq {
+	u32			tirn;
+};
+
+struct mlx5_ib_raw_packet_qp {
+	struct mlx5_ib_rq rq;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
-	struct mlx5_core_qp	mqp;
+	union {
+		struct mlx5_core_qp		mqp;
+		struct mlx5_ib_raw_packet_qp	raw_packet_qp;
+	};
+
 	struct mlx5_buf		buf;
 
 	struct mlx5_db		db;
@@ -431,6 +473,7 @@ struct mlx5_ib_dev {
 	 */
 	struct srcu_struct      mr_srcu;
 #endif
+	struct mlx5_ib_flow_db	flow_db;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit 051f263098a90d208e2d20251bfd4834bc783214
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Sun Dec 20 12:16:11 2015 +0200

    IB/mlx5: Add driver cross-channel support
    
    Add support of cross-channel functionality to mlx5
    driver. This includes ability to ignore overrun for CQ
    which intended for cross-channel, export device capability and
    configure the QP to be sync master/slave queues.
    
    The cross-channel enabled QP supports combination of
    three possible properties:
    * WQE processing on the receive queue of this QP
    * WQE processing on the send queue of this QP
    * WQE are supported on the send queue
    
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1303487ffe6b..d4b227126265 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -90,6 +90,10 @@ enum mlx5_ib_mad_ifc_flags {
 	MLX5_MAD_IFC_NET_VIEW		= 4,
 };
 
+enum {
+	MLX5_CROSS_CHANNEL_UUAR         = 0,
+};
+
 struct mlx5_ib_ucontext {
 	struct ib_ucontext	ibucontext;
 	struct list_head	db_page_list;
@@ -247,6 +251,9 @@ struct mlx5_ib_cq_buf {
 enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_BLOCK_MULTICAST_LOOPBACK     = 1 << 0,
 	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 1,
+	MLX5_IB_QP_CROSS_CHANNEL		= 1 << 2,
+	MLX5_IB_QP_MANAGED_SEND			= 1 << 3,
+	MLX5_IB_QP_MANAGED_RECV			= 1 << 4,
 };
 
 struct mlx5_umr_wr {
@@ -289,6 +296,7 @@ struct mlx5_ib_cq {
 	struct mlx5_ib_cq_buf  *resize_buf;
 	struct ib_umem	       *resize_umem;
 	int			cqe_size;
+	u32			create_flags;
 };
 
 struct mlx5_ib_srq {
@@ -678,4 +686,12 @@ static inline int is_qp1(enum ib_qp_type qp_type)
 #define MLX5_MAX_UMR_SHIFT 16
 #define MLX5_MAX_UMR_PAGES (1 << MLX5_MAX_UMR_SHIFT)
 
+static inline u32 check_cq_create_flags(u32 flags)
+{
+	/*
+	 * It returns non-zero value for unsupported CQ
+	 * create flags, otherwise it returns zero.
+	 */
+	return (flags & ~IB_CQ_FLAGS_IGNORE_OVERRUN);
+}
 #endif /* MLX5_IB_H */

commit d69e3bcf79764ed833e0cc6e0645dc87589cec22
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Dec 15 20:30:13 2015 +0200

    IB/mlx5: Mmap the HCA's core clock register to user-space
    
    In order to read the HCA's current cycles register, we need
    to map it to user-space. Add support to map this register
    via mmap command.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Moshe Lazer <moshel@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b2a66432b865..1303487ffe6b 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -65,7 +65,9 @@ enum {
 
 enum mlx5_ib_mmap_cmd {
 	MLX5_IB_MMAP_REGULAR_PAGE		= 0,
-	MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES	= 1, /* always last */
+	MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES	= 1,
+	/* 5 is chosen in order to be compatible with old versions of libmlx5 */
+	MLX5_IB_MMAP_CORE_CLOCK			= 5,
 };
 
 enum {

commit b368d7cb8ceb77f481b066bd8be5fada82da7301
Author: Matan Barak <matanb@mellanox.com>
Date:   Tue Dec 15 20:30:12 2015 +0200

    IB/mlx5: Add hca_core_clock_offset to udata in init_ucontext
    
    Pass hca_core_clock_offset to user-space is mandatory in order to
    let the user-space read the free-running clock register from the
    right offset in the memory mapped page.
    Passing this value is done by changing the vendor's command
    and response of init_ucontext to be in extensible form.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Moshe Lazer <moshel@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index b0deeb38175d..b2a66432b865 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -55,6 +55,9 @@ pr_err("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
 pr_warn("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
 	__LINE__, current->pid, ##arg)
 
+#define field_avail(type, fld, sz) (offsetof(type, fld) +		\
+				    sizeof(((type *)0)->fld) <= (sz))
+
 enum {
 	MLX5_IB_MMAP_CMD_SHIFT	= 8,
 	MLX5_IB_MMAP_CMD_MASK	= 0xff,

commit 2811ba51b04958cd001b6409c9f70e8563376346
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Wed Dec 23 18:47:24 2015 +0200

    IB/mlx5: Add RoCE fields to Address Vector
    
    Set the address handle and QP address path fields according to the
    link layer type (IB/Eth).
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 1eaa6111dce0..b0deeb38175d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -517,8 +517,6 @@ void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
 		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 		 const void *in_mad, void *response_mad);
-struct ib_ah *create_ib_ah(struct ib_ah_attr *ah_attr,
-			   struct mlx5_ib_ah *ah);
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
 int mlx5_ib_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr);
 int mlx5_ib_destroy_ah(struct ib_ah *ah);
@@ -647,6 +645,9 @@ static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
 
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
+__be16 mlx5_get_roce_udp_sport(struct mlx5_ib_dev *dev, u8 port_num,
+			       int index);
+
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;

commit fc24fc5e9530a4fde7feddaa543f5885af013461
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Wed Dec 23 18:47:17 2015 +0200

    IB/mlx5: Support IB device's callback for getting its netdev
    
    For Eth ports only:
    Maintain a net device pointer in mlx5_ib_device and update it
    upon NETDEV_REGISTER and NETDEV_UNREGISTER events if the
    net-device and IB device have the same PCI parent device.
    Implement the get_netdev callback to return this net device.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 633347260b79..1eaa6111dce0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -407,9 +407,19 @@ struct mlx5_ib_resources {
 	struct ib_srq	*s1;
 };
 
+struct mlx5_roce {
+	/* Protect mlx5_ib_get_netdev from invoking dev_hold() with a NULL
+	 * netdev pointer
+	 */
+	rwlock_t		netdev_lock;
+	struct net_device	*netdev;
+	struct notifier_block	nb;
+};
+
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
+	struct mlx5_roce		roce;
 	MLX5_DECLARE_DOORBELL_LOCK(uar_lock);
 	int				num_ports;
 	/* serialize update of capability mask

commit dd01e66a6c532a8cd183cbc02ebaef99f186345f
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:42 2015 +0300

    IB/mlx5: Remove old FRWR API support
    
    No ULP uses it anymore, go ahead and remove it.
    Keep only the local invalidate part of the handlers.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a29b28c31c44..633347260b79 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -338,12 +338,6 @@ struct mlx5_ib_mr {
 	void			*descs_alloc;
 };
 
-struct mlx5_ib_fast_reg_page_list {
-	struct ib_fast_reg_page_list	ibfrpl;
-	__be64			       *mapped_page_list;
-	dma_addr_t			map;
-};
-
 struct mlx5_ib_umr_context {
 	enum ib_wc_status	status;
 	struct completion	done;
@@ -494,11 +488,6 @@ static inline struct mlx5_ib_mr *to_mmr(struct ib_mr *ibmr)
 	return container_of(ibmr, struct mlx5_ib_mr, ibmr);
 }
 
-static inline struct mlx5_ib_fast_reg_page_list *to_mfrpl(struct ib_fast_reg_page_list *ibfrpl)
-{
-	return container_of(ibfrpl, struct mlx5_ib_fast_reg_page_list, ibfrpl);
-}
-
 struct mlx5_ib_ah {
 	struct ib_ah		ibah;
 	struct mlx5_av		av;
@@ -569,9 +558,6 @@ struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 int mlx5_ib_map_mr_sg(struct ib_mr *ibmr,
 		      struct scatterlist *sg,
 		      int sg_nents);
-struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,
-							       int page_list_len);
-void mlx5_ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 			const struct ib_mad_hdr *in, size_t in_mad_size,

commit 8a187ee52b043f8201e7089e5e538974142722e0
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:26 2015 +0300

    IB/mlx5: Support the new memory registration API
    
    Support the new memory registration API by allocating a
    private page list array in mlx5_ib_mr and populate it when
    mlx5_ib_map_mr_sg is invoked. Also, support IB_WR_REG_MR
    by setting the exact WQE as IB_WR_FAST_REG_MR, just take the
    needed information from different places:
    - page_size, iova, length, access flags (ib_mr)
    - page array (mlx5_ib_mr)
    - key (ib_reg_wr)
    
    The IB_WR_FAST_REG_MR handlers will be removed later when
    all the ULPs will be converted.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f789a3e6c215..a29b28c31c44 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -319,6 +319,11 @@ enum mlx5_ib_mtt_access_flags {
 
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
+	void			*descs;
+	dma_addr_t		desc_map;
+	int			ndescs;
+	int			max_descs;
+	int			desc_size;
 	struct mlx5_core_mr	mmr;
 	struct ib_umem	       *umem;
 	struct mlx5_shared_mr_info	*smr_info;
@@ -330,6 +335,7 @@ struct mlx5_ib_mr {
 	struct mlx5_create_mkey_mbox_out out;
 	struct mlx5_core_sig_ctx    *sig;
 	int			live;
+	void			*descs_alloc;
 };
 
 struct mlx5_ib_fast_reg_page_list {
@@ -560,6 +566,9 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 			       enum ib_mr_type mr_type,
 			       u32 max_num_sg);
+int mlx5_ib_map_mr_sg(struct ib_mr *ibmr,
+		      struct scatterlist *sg,
+		      int sg_nents);
 struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,
 							       int page_list_len);
 void mlx5_ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);

commit a7060009161344637da8cb4a389ead5fc156ea37
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:25 2015 +0300

    IB/mlx5: Remove dead fmr code
    
    Just function declarations - no need for those
    laying arround. If for some reason someone will want
    FMR support in mlx5, it should be easy enough to restore
    a few structs.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 29f3ecdbe790..f789a3e6c215 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -364,20 +364,6 @@ enum {
 	MLX5_FMR_BUSY,
 };
 
-struct mlx5_ib_fmr {
-	struct ib_fmr			ibfmr;
-	struct mlx5_core_mr		mr;
-	int				access_flags;
-	int				state;
-	/* protect fmr state
-	 */
-	spinlock_t			lock;
-	u64				wrid;
-	struct ib_send_wr		wr[2];
-	u8				page_shift;
-	struct ib_fast_reg_page_list	page_list;
-};
-
 struct mlx5_cache_ent {
 	struct list_head	head;
 	/* sync access to the cahce entry
@@ -462,11 +448,6 @@ static inline struct mlx5_ib_dev *to_mdev(struct ib_device *ibdev)
 	return container_of(ibdev, struct mlx5_ib_dev, ib_dev);
 }
 
-static inline struct mlx5_ib_fmr *to_mfmr(struct ib_fmr *ibfmr)
-{
-	return container_of(ibfmr, struct mlx5_ib_fmr, ibfmr);
-}
-
 static inline struct mlx5_ib_cq *to_mcq(struct ib_cq *ibcq)
 {
 	return container_of(ibcq, struct mlx5_ib_cq, ibcq);
@@ -582,12 +563,6 @@ struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,
 							       int page_list_len);
 void mlx5_ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);
-struct ib_fmr *mlx5_ib_fmr_alloc(struct ib_pd *pd, int acc,
-				 struct ib_fmr_attr *fmr_attr);
-int mlx5_ib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,
-		      int npages, u64 iova);
-int mlx5_ib_unmap_fmr(struct list_head *fmr_list);
-int mlx5_ib_fmr_dealloc(struct ib_fmr *ibfmr);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
 			const struct ib_mad_hdr *in, size_t in_mad_size,

commit e622f2f4ad2142d2a613a57fb85f8cf737935ef5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 8 09:16:33 2015 +0100

    IB: split struct ib_send_wr
    
    This patch split up struct ib_send_wr so that all non-trivial verbs
    use their own structure which embedds struct ib_send_wr.  This dramaticly
    shrinks the size of a WR for most common operations:
    
    sizeof(struct ib_send_wr) (old):        96
    
    sizeof(struct ib_send_wr):              48
    sizeof(struct ib_rdma_wr):              64
    sizeof(struct ib_atomic_wr):            96
    sizeof(struct ib_ud_wr):                88
    sizeof(struct ib_fast_reg_wr):          88
    sizeof(struct ib_bind_mw_wr):           96
    sizeof(struct ib_sig_handover_wr):      80
    
    And with Sagi's pending MR rework the fast registration WR will also be
    down to a reasonable size:
    
    sizeof(struct ib_fastreg_wr):           64
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com> [srp, srpt]
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com> [sunrpc]
    Tested-by: Haggai Eran <haggaie@mellanox.com>
    Tested-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 22123b79d550..29f3ecdbe790 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -245,6 +245,7 @@ enum mlx5_ib_qp_flags {
 };
 
 struct mlx5_umr_wr {
+	struct ib_send_wr		wr;
 	union {
 		u64			virt_addr;
 		u64			offset;
@@ -257,6 +258,11 @@ struct mlx5_umr_wr {
 	u32				mkey;
 };
 
+static inline struct mlx5_umr_wr *umr_wr(struct ib_send_wr *wr)
+{
+	return container_of(wr, struct mlx5_umr_wr, wr);
+}
+
 struct mlx5_shared_mr_info {
 	int mr_id;
 	struct ib_umem		*umem;

commit 81fb5e26a9d05674c048803a20cb8f08a1b1c9b8
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Sep 24 10:34:24 2015 +0300

    IB/mlx5: Remove pa_lkey usages
    
    Since mlx5 driver cannot rely on registration using the
    reserved lkey (global_dma_lkey) it used to allocate a private
    physical address lkey for each allocated pd.
    Commit 96249d70dd70 ("IB/core: Guarantee that a local_dma_lkey
    is available") just does it in the core layer so we can go ahead
    and use that.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index bb8cda79e881..22123b79d550 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -103,7 +103,6 @@ static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibuconte
 struct mlx5_ib_pd {
 	struct ib_pd		ibpd;
 	u32			pdn;
-	u32			pa_lkey;
 };
 
 /* Use macros here so that don't have to duplicate
@@ -213,7 +212,6 @@ struct mlx5_ib_qp {
 	int			uuarn;
 
 	int			create_type;
-	u32			pa_lkey;
 
 	/* Store signature errors */
 	bool			signature_en;

commit b636401f0ec9bbf7931774e00f3adf7ee9214cce
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Wed Sep 2 22:23:04 2015 +0300

    mlx5: Fix incorrect wc pkey_index assignment for GSI messages
    
    Since patch series "Demux IB CM requests in the rdma_cm module" the
    P_Key index is taken from the work completion rather than the message
    itself.
    
    The HCA provides us with the message P_Key. In order to provide the
    P_Key index, we need to look it up. Given that this is relevant only
    for GSI messages (session establishments) which is less performance critical,
    micro-optimize against the GSI (is_qp1) branch.
    
    Fixes: 4c21b5bcef73 ("IB/cma: Add net_dev and private data checks to
    RDMA CM")
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a5fa0b9c7580..bb8cda79e881 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -680,6 +680,11 @@ static inline u8 convert_access(int acc)
 	       MLX5_PERM_LOCAL_READ;
 }
 
+static inline int is_qp1(enum ib_qp_type qp_type)
+{
+	return qp_type == IB_QPT_GSI;
+}
+
 #define MLX5_MAX_UMR_SHIFT 16
 #define MLX5_MAX_UMR_PAGES (1 << MLX5_MAX_UMR_SHIFT)
 

commit b37c788f595cd578524fb8f50d3bd2fff8b62bc3
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Thu Jul 30 17:22:19 2015 -0600

    IB/mlx5: Remove ib_get_dma_mr calls
    
    The pd now has a local_dma_lkey member which completely replaces
    ib_get_dma_mr, use it instead.
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 62b06ae2c87d..a5fa0b9c7580 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -349,7 +349,6 @@ struct umr_common {
 	struct ib_pd	*pd;
 	struct ib_cq	*cq;
 	struct ib_qp	*qp;
-	struct ib_mr	*mr;
 	/* control access to UMR QP
 	 */
 	struct semaphore	sem;

commit b3778ba8ded0aafbd820b68be0686ff0c6026eff
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:41 2015 +0300

    mlx5: Drop mlx5_ib_alloc_fast_reg_mr
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 31b50a4aa5d8..62b06ae2c87d 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -576,8 +576,6 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
 			       enum ib_mr_type mr_type,
 			       u32 max_num_sg);
-struct ib_mr *mlx5_ib_alloc_fast_reg_mr(struct ib_pd *pd,
-					int max_page_list_len);
 struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,
 							       int page_list_len);
 void mlx5_ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);

commit 9bee178b4f6b3e122ed8eda990450a638706e271
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:35 2015 +0300

    IB: Modify ib_create_mr API
    
    Use ib_alloc_mr with specific parameters.
    Change the existing callers.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 29c74e9ffc00..31b50a4aa5d8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -573,8 +573,9 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
 		       int npages, int zap);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
-struct ib_mr *mlx5_ib_create_mr(struct ib_pd *pd,
-				struct ib_mr_init_attr *mr_init_attr);
+struct ib_mr *mlx5_ib_alloc_mr(struct ib_pd *pd,
+			       enum ib_mr_type mr_type,
+			       u32 max_num_sg);
 struct ib_mr *mlx5_ib_alloc_fast_reg_mr(struct ib_pd *pd,
 					int max_page_list_len);
 struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,

commit 8b91ffc1cf67d3f0834197c80c5182890c8d508d
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Thu Jul 30 10:32:34 2015 +0300

    IB/core: Get rid of redundant verb ib_destroy_mr
    
    This was added in a thought of uniting all mr allocation
    and deallocation routines but the fact is we have a single
    deallocation routine already, ib_dereg_mr.
    
    And, move mlx5_ib_destroy_mr specific logic into mlx5_ib_dereg_mr
    (includes only signature stuff for now).
    
    And, fixup the only callers (iser/isert) accordingly.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 7cae09836481..29c74e9ffc00 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -573,7 +573,6 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
 		       int npages, int zap);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
-int mlx5_ib_destroy_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_create_mr(struct ib_pd *pd,
 				struct ib_mr_init_attr *mr_init_attr);
 struct ib_mr *mlx5_ib_alloc_fast_reg_mr(struct ib_pd *pd,

commit e0456717e483bb8a9431b80a5bdc99a928b9b003
Merge: 98ec21a01896 1ea2d020ba47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 16:49:49 2015 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add TX fast path in mac80211, from Johannes Berg.
    
     2) Add TSO/GRO support to ibmveth, from Thomas Falcon
    
     3) Move away from cached routes in ipv6, just like ipv4, from Martin
        KaFai Lau.
    
     4) Lots of new rhashtable tests, from Thomas Graf.
    
     5) Run ingress qdisc lockless, from Alexei Starovoitov.
    
     6) Allow servers to fetch TCP packet headers for SYN packets of new
        connections, for fingerprinting.  From Eric Dumazet.
    
     7) Add mode parameter to pktgen, for testing receive.  From Alexei
        Starovoitov.
    
     8) Cache access optimizations via simplifications of build_skb(), from
        Alexander Duyck.
    
     9) Move page frag allocator under mm/, also from Alexander.
    
    10) Add xmit_more support to hv_netvsc, from KY Srinivasan.
    
    11) Add a counter guard in case we try to perform endless reclassify
        loops in the packet scheduler.
    
    12) Extern flow dissector to be programmable and use it in new "Flower"
        classifier.  From Jiri Pirko.
    
    13) AF_PACKET fanout rollover fixes, performance improvements, and new
        statistics.  From Willem de Bruijn.
    
    14) Add netdev driver for GENEVE tunnels, from John W Linville.
    
    15) Add ingress netfilter hooks and filtering, from Pablo Neira Ayuso.
    
    16) Fix handling of epoll edge triggers in TCP, from Eric Dumazet.
    
    17) Add an ECN retry fallback for the initial TCP handshake, from Daniel
        Borkmann.
    
    18) Add tail call support to BPF, from Alexei Starovoitov.
    
    19) Add several pktgen helper scripts, from Jesper Dangaard Brouer.
    
    20) Add zerocopy support to AF_UNIX, from Hannes Frederic Sowa.
    
    21) Favor even port numbers for allocation to connect() requests, and
        odd port numbers for bind(0), in an effort to help avoid
        ip_local_port_range exhaustion.  From Eric Dumazet.
    
    22) Add Cavium ThunderX driver, from Sunil Goutham.
    
    23) Allow bpf programs to access skb_iif and dev->ifindex SKB metadata,
        from Alexei Starovoitov.
    
    24) Add support for T6 chips in cxgb4vf driver, from Hariprasad Shenai.
    
    25) Double TCP Small Queues default to 256K to accomodate situations
        like the XEN driver and wireless aggregation.  From Wei Liu.
    
    26) Add more entropy inputs to flow dissector, from Tom Herbert.
    
    27) Add CDG congestion control algorithm to TCP, from Kenneth Klette
        Jonassen.
    
    28) Convert ipset over to RCU locking, from Jozsef Kadlecsik.
    
    29) Track and act upon link status of ipv4 route nexthops, from Andy
        Gospodarek.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1670 commits)
      bridge: vlan: flush the dynamically learned entries on port vlan delete
      bridge: multicast: add a comment to br_port_state_selection about blocking state
      net: inet_diag: export IPV6_V6ONLY sockopt
      stmmac: troubleshoot unexpected bits in des0 & des1
      net: ipv4 sysctl option to ignore routes when nexthop link is down
      net: track link-status of ipv4 nexthops
      net: switchdev: ignore unsupported bridge flags
      net: Cavium: Fix MAC address setting in shutdown state
      drivers: net: xgene: fix for ACPI support without ACPI
      ip: report the original address of ICMP messages
      net/mlx5e: Prefetch skb data on RX
      net/mlx5e: Pop cq outside mlx5e_get_cqe
      net/mlx5e: Remove mlx5e_cq.sqrq back-pointer
      net/mlx5e: Remove extra spaces
      net/mlx5e: Avoid TX CQE generation if more xmit packets expected
      net/mlx5e: Avoid redundant dev_kfree_skb() upon NOP completion
      net/mlx5e: Remove re-assignment of wq type in mlx5e_enable_rq()
      net/mlx5e: Use skb_shinfo(skb)->gso_segs rather than counting them
      net/mlx5e: Static mapping of netdev priv resources to/from netdev TX queues
      net/mlx4_en: Use HW counters for rx/tx bytes/packets in PF device
      ...

commit 4cd7c9479aff33746af490fa4a5a7dee8654891a
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sat Jun 6 14:38:31 2015 -0400

    IB/mad: Add support for additional MAD info to/from drivers
    
    In order to support alternate sized MADs (and variable sized MADs on OPA
    devices) add in/out MAD size parameters to the process_mad core call.
    
    In addition, add an out_mad_pkey_index to communicate the pkey index the driver
    wishes the MAD stack to use when sending OPA MAD responses.
    
    The out MAD size and the out MAD PKey index are required by the MAD
    stack to generate responses on OPA devices.
    
    Furthermore, the in and out MAD parameters are made generic by specifying them
    as ib_mad_hdr rather than ib_mad.
    
    Drivers are modified as needed and are protected by BUG_ON flags if the MAD
    sizes passed to them is incorrect.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 2f72f326d5c2..178314e764da 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -588,7 +588,9 @@ int mlx5_ib_unmap_fmr(struct list_head *fmr_list);
 int mlx5_ib_fmr_dealloc(struct ib_fmr *ibfmr);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
 			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
-			const struct ib_mad *in_mad, struct ib_mad *out_mad);
+			const struct ib_mad_hdr *in, size_t in_mad_size,
+			struct ib_mad_hdr *out, size_t *out_mad_size,
+			u16 *out_mad_pkey_index);
 struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 					  struct ib_ucontext *context,
 					  struct ib_udata *udata);

commit bcf4c1ea583cd213f0bafdbeb11d80f83c5f10e6
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jun 11 16:35:20 2015 +0300

    IB/core: Change provider's API of create_cq to be extendible
    
    Add a new ib_cq_init_attr structure which contains the
    previous cqe (minimum number of CQ entries) and comp_vector
    (completion vector) in addition to a new flags field.
    All vendors' create_cq callbacks are changed in order
    to work with the new API.
    
    This commit does not change any functionality.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com> to patch #2
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c6219032d00c..2f72f326d5c2 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -556,8 +556,9 @@ int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 			  void *buffer, u32 length);
-struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev, int entries,
-				int vector, struct ib_ucontext *context,
+struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev,
+				const struct ib_cq_init_attr *attr,
+				struct ib_ucontext *context,
 				struct ib_udata *udata);
 int mlx5_ib_destroy_cq(struct ib_cq *cq);
 int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);

commit 4aa17b2879f66e478aa9b81cc3bbade6416126aa
Author: Haggai Abramonvsky <hagaya@mellanox.com>
Date:   Thu Jun 4 19:30:48 2015 +0300

    mlx5: Enable mutual support for IB and Ethernet
    
    Ethernet functionality is only available when working in ISSI > 0 mode.
    
    Previously, the IB driver wasn't ready to work on that mode, and hence
    building both the IB driver and the Ethernet functionality in the core
    driver were disallowed by Kconfigs.
    
    Now, once we have all the pre-steps in place, we can remove this limitation.
    
    The last steps in the IB driver for getting that setup to work are:
    create dummy SRQ for the driver's use (until now we could use XRC_SRQ
    as SRQ and XRC_SRQ, after moving to ISSI > 0, we separate XRC SRQs from
    basic SRQs) and adapt the create QP function to be compatible with ISSI > 0.
    
    Signed-off-by: Haggai Abramovsky <hagaya@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f731b2592a36..873dc354766a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -415,6 +415,7 @@ struct mlx5_ib_resources {
 	struct ib_xrcd	*x1;
 	struct ib_pd	*p0;
 	struct ib_srq	*s0;
+	struct ib_srq	*s1;
 };
 
 struct mlx5_ib_dev {

commit 1b5daf11b015123108686a9060ee6de705a03e76
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Thu Jun 4 19:30:46 2015 +0300

    IB/mlx5: Avoid using the MAD_IFC command under ISSI > 0 mode
    
    In ISSI > 0 mode, most of the MAD_IFC command features are deprecated, and can't
    be used. Therefore, when in that mode, we replace all of them with other commands
    that provide the required functionality.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 0c441add0464..f731b2592a36 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -594,6 +594,22 @@ struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd);
 int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
 int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);
+int mlx5_query_mad_ifc_smp_attr_node_info(struct ib_device *ibdev,
+					  struct ib_smp *out_mad);
+int mlx5_query_mad_ifc_system_image_guid(struct ib_device *ibdev,
+					 __be64 *sys_image_guid);
+int mlx5_query_mad_ifc_max_pkeys(struct ib_device *ibdev,
+				 u16 *max_pkeys);
+int mlx5_query_mad_ifc_vendor_id(struct ib_device *ibdev,
+				 u32 *vendor_id);
+int mlx5_query_mad_ifc_node_desc(struct mlx5_ib_dev *dev, char *node_desc);
+int mlx5_query_mad_ifc_node_guid(struct mlx5_ib_dev *dev, __be64 *node_guid);
+int mlx5_query_mad_ifc_pkey(struct ib_device *ibdev, u8 port, u16 index,
+			    u16 *pkey);
+int mlx5_query_mad_ifc_gids(struct ib_device *ibdev, u8 port, int index,
+			    union ib_gid *gid);
+int mlx5_query_mad_ifc_port(struct ib_device *ibdev, u8 port,
+			    struct ib_port_attr *props);
 int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,
 		       struct ib_port_attr *props);
 int mlx5_ib_init_fmr(struct mlx5_ib_dev *dev);

commit a97e2d86a9b88ea9e9a280b594b80f0eec2c955b
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Sun May 31 17:15:30 2015 -0400

    IB/core cleanup: Add const on args - device->process_mad
    
    The process_mad device function declares some parameters as "in".  Make those
    parameters const and adjust the call tree under process_mad in the various
    drivers accordingly.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Hal Rosenstock <hal@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dff1cfcdf476..c6219032d00c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -525,8 +525,8 @@ void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq)
 void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
-		 u8 port, struct ib_wc *in_wc, struct ib_grh *in_grh,
-		 void *in_mad, void *response_mad);
+		 u8 port, const struct ib_wc *in_wc, const struct ib_grh *in_grh,
+		 const void *in_mad, void *response_mad);
 struct ib_ah *create_ib_ah(struct ib_ah_attr *ah_attr,
 			   struct mlx5_ib_ah *ah);
 struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
@@ -586,8 +586,8 @@ int mlx5_ib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,
 int mlx5_ib_unmap_fmr(struct list_head *fmr_list);
 int mlx5_ib_fmr_dealloc(struct ib_fmr *ibfmr);
 int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
-			struct ib_wc *in_wc, struct ib_grh *in_grh,
-			struct ib_mad *in_mad, struct ib_mad *out_mad);
+			const struct ib_wc *in_wc, const struct ib_grh *in_grh,
+			const struct ib_mad *in_mad, struct ib_mad *out_mad);
 struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 					  struct ib_ucontext *context,
 					  struct ib_udata *udata);

commit 938fe83c8dcbbf294d167e6163200a8540ae43c4
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu May 28 22:28:41 2015 +0300

    net/mlx5_core: New device capabilities handling
    
    - Query all supported types of dev caps on driver load.
    - Store the Cap data outbox per cap type into driver private data.
    - Introduce new Macros to access/dump stored caps (using the auto
      generated data types).
    - Obsolete SW representation of dev caps (no need for SW copy for each
      cap).
    - Modify IB driver to use new macros for checking caps.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index dff1cfcdf476..0c441add0464 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -617,7 +617,7 @@ int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 extern struct workqueue_struct *mlx5_ib_page_fault_wq;
 
-int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev);
+void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev);
 void mlx5_ib_mr_pfault_handler(struct mlx5_ib_qp *qp,
 			       struct mlx5_ib_pfault *pfault);
 void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp);
@@ -631,9 +631,9 @@ void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
 			      unsigned long end);
 
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
-static inline int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev)
+static inline void mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)
 {
-	return 0;
+	return;
 }
 
 static inline void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp)		{}

commit 233d05d28ad942929b6b4fbc48aa8dd083c16484
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 2 17:07:32 2015 +0300

    net/mlx5_core: Move completion eqs from mlx5_ib to mlx5_core
    
    Preparation for ethernet driver.
    These functions will be used in drivers other than mlx5_ib.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 8f94f69fbea5..dff1cfcdf476 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -421,9 +421,7 @@ struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
 	struct mlx5_core_dev		*mdev;
 	MLX5_DECLARE_DOORBELL_LOCK(uar_lock);
-	struct list_head		eqs_list;
 	int				num_ports;
-	int				num_comp_vectors;
 	/* serialize update of capability mask
 	 */
 	struct mutex			cap_mask_mutex;
@@ -594,7 +592,6 @@ struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
 					  struct ib_ucontext *context,
 					  struct ib_udata *udata);
 int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd);
-int mlx5_vector2eqn(struct mlx5_ib_dev *dev, int vector, int *eqn, int *irqn);
 int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
 int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);
 int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,

commit 6cf0a15f07a5a4600beb72f3a246fcd9325e86aa
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 2 17:07:30 2015 +0300

    IB/mlx5: Fix Mellanox copyright note
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 83f22fe297c8..8f94f69fbea5 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2013, Mellanox Technologies inc.  All rights reserved.
+ * Copyright (c) 2013-2015, Mellanox Technologies. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU

commit b4cfe447d47b5763f630412fd5dc5fbe66e991d1
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:26 2014 +0200

    IB/mlx5: Implement on demand paging by adding support for MMU notifiers
    
    * Implement the relevant invalidation functions (zap MTTs as needed)
    * Implement interlocking (and rollback in the page fault handlers) for
      cases of a racing notifier and fault.
    * With this patch we can now enable the capability bits for supporting RC
      send/receive/RDMA read/RDMA write, and UD send.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index c6ceec3e3d6a..83f22fe297c8 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -325,6 +325,7 @@ struct mlx5_ib_mr {
 	struct mlx5_ib_dev     *dev;
 	struct mlx5_create_mkey_mbox_out out;
 	struct mlx5_core_sig_ctx    *sig;
+	int			live;
 };
 
 struct mlx5_ib_fast_reg_page_list {
@@ -629,6 +630,8 @@ int __init mlx5_ib_odp_init(void);
 void mlx5_ib_odp_cleanup(void);
 void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
 void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
+void mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,
+			      unsigned long end);
 
 #else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev)

commit 6aec21f6a8322fa8d43df3ea7f051dfd8967f1b9
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:23 2014 +0200

    IB/mlx5: Page faults handling infrastructure
    
    * Refactor MR registration and cleanup, and fix reg_pages accounting.
    * Create a work queue to handle page fault events in a kthread context.
    * Register a fault handler to get events from the core for each QP.
    
    The registered fault handler is empty in this patch, and only a later
    patch implements it.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 6856e27bfb6a..c6ceec3e3d6a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -149,6 +149,29 @@ enum {
 	MLX5_QP_EMPTY
 };
 
+/*
+ * Connect-IB can trigger up to four concurrent pagefaults
+ * per-QP.
+ */
+enum mlx5_ib_pagefault_context {
+	MLX5_IB_PAGEFAULT_RESPONDER_READ,
+	MLX5_IB_PAGEFAULT_REQUESTOR_READ,
+	MLX5_IB_PAGEFAULT_RESPONDER_WRITE,
+	MLX5_IB_PAGEFAULT_REQUESTOR_WRITE,
+	MLX5_IB_PAGEFAULT_CONTEXTS
+};
+
+static inline enum mlx5_ib_pagefault_context
+	mlx5_ib_get_pagefault_context(struct mlx5_pagefault *pagefault)
+{
+	return pagefault->flags & (MLX5_PFAULT_REQUESTOR | MLX5_PFAULT_WRITE);
+}
+
+struct mlx5_ib_pfault {
+	struct work_struct	work;
+	struct mlx5_pagefault	mpfault;
+};
+
 struct mlx5_ib_qp {
 	struct ib_qp		ibqp;
 	struct mlx5_core_qp	mqp;
@@ -194,6 +217,21 @@ struct mlx5_ib_qp {
 
 	/* Store signature errors */
 	bool			signature_en;
+
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+	/*
+	 * A flag that is true for QP's that are in a state that doesn't
+	 * allow page faults, and shouldn't schedule any more faults.
+	 */
+	int                     disable_page_faults;
+	/*
+	 * The disable_page_faults_lock protects a QP's disable_page_faults
+	 * field, allowing for a thread to atomically check whether the QP
+	 * allows page faults, and if so schedule a page fault.
+	 */
+	spinlock_t              disable_page_faults_lock;
+	struct mlx5_ib_pfault	pagefaults[MLX5_IB_PAGEFAULT_CONTEXTS];
+#endif
 };
 
 struct mlx5_ib_cq_buf {
@@ -392,13 +430,17 @@ struct mlx5_ib_dev {
 	struct umr_common		umrc;
 	/* sync used page count stats
 	 */
-	spinlock_t			mr_lock;
 	struct mlx5_ib_resources	devr;
 	struct mlx5_mr_cache		cache;
 	struct timer_list		delay_timer;
 	int				fill_delay;
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	struct ib_odp_caps	odp_caps;
+	/*
+	 * Sleepable RCU that prevents destruction of MRs while they are still
+	 * being used by a page fault handler.
+	 */
+	struct srcu_struct      mr_srcu;
 #endif
 };
 
@@ -575,12 +617,33 @@ int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+extern struct workqueue_struct *mlx5_ib_page_fault_wq;
+
 int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev);
-#else
+void mlx5_ib_mr_pfault_handler(struct mlx5_ib_qp *qp,
+			       struct mlx5_ib_pfault *pfault);
+void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp);
+int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev);
+void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev);
+int __init mlx5_ib_odp_init(void);
+void mlx5_ib_odp_cleanup(void);
+void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp);
+void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp);
+
+#else /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 static inline int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev)
 {
 	return 0;
 }
+
+static inline void mlx5_ib_odp_create_qp(struct mlx5_ib_qp *qp)		{}
+static inline int mlx5_ib_odp_init_one(struct mlx5_ib_dev *ibdev) { return 0; }
+static inline void mlx5_ib_odp_remove_one(struct mlx5_ib_dev *ibdev)	{}
+static inline int mlx5_ib_odp_init(void) { return 0; }
+static inline void mlx5_ib_odp_cleanup(void)				{}
+static inline void mlx5_ib_qp_disable_pagefaults(struct mlx5_ib_qp *qp) {}
+static inline void mlx5_ib_qp_enable_pagefaults(struct mlx5_ib_qp *qp)  {}
+
 #endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
 
 static inline void init_query_mad(struct ib_smp *mad)

commit 832a6b06ab5e13c228fc27e333ad360aa03ace6f
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:22 2014 +0200

    IB/mlx5: Add mlx5_ib_update_mtt to update page tables after creation
    
    The new function allows updating the page tables of a memory region
    after it was created. This can be used to handle page faults and page
    invalidations.
    
    Since mlx5_ib_update_mtt will need to work from within page invalidation,
    so it must not block on memory allocation. It employs an atomic memory
    allocation mechanism that is used as a fallback when kmalloc(GFP_ATOMIC) fails.
    
    In order to reuse code from mlx5_ib_populate_pas, the patch splits
    this function and add the needed parameters.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 83c1690e9dd0..6856e27bfb6a 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -527,6 +527,8 @@ struct ib_mr *mlx5_ib_get_dma_mr(struct ib_pd *pd, int acc);
 struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 virt_addr, int access_flags,
 				  struct ib_udata *udata);
+int mlx5_ib_update_mtt(struct mlx5_ib_mr *mr, u64 start_page_index,
+		       int npages, int zap);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
 int mlx5_ib_destroy_mr(struct ib_mr *ibmr);
 struct ib_mr *mlx5_ib_create_mr(struct ib_pd *pd,
@@ -558,6 +560,9 @@ int mlx5_ib_init_fmr(struct mlx5_ib_dev *dev);
 void mlx5_ib_cleanup_fmr(struct mlx5_ib_dev *dev);
 void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift,
 			int *ncont, int *order);
+void __mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
+			    int page_shift, size_t offset, size_t num_pages,
+			    __be64 *pas, int access_flags);
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
 			  int page_shift, __be64 *pas, int access_flags);
 void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);

commit cc149f751b75211df8c41fcd60bd0006e6143ed6
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:21 2014 +0200

    IB/mlx5: Changes in memory region creation to support on-demand paging
    
    This patch wraps together several changes needed for on-demand paging support
    in the mlx5_ib_populate_pas function, and when registering memory regions.
    
    * Instead of accepting a UMR bit telling the function to enable all
      access flags, the function now accepts the access flags themselves.
    * For on-demand paging memory regions, fill the memory tables from the
      correct list, and enable/disable the access flags per-page according
      to whether the page is present.
    * A new bit is set to enable writing of access flags when using the
      firmware create_mkey command.
    * Disable contig pages when on-demand paging is enabled.
    
    In addition the patch changes the UMR code to use PTR_ALIGN instead of
    our own macro.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index cc50fce8cca7..83c1690e9dd0 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -268,6 +268,13 @@ struct mlx5_ib_xrcd {
 	u32			xrcdn;
 };
 
+enum mlx5_ib_mtt_access_flags {
+	MLX5_IB_MTT_READ  = (1 << 0),
+	MLX5_IB_MTT_WRITE = (1 << 1),
+};
+
+#define MLX5_IB_MTT_PRESENT (MLX5_IB_MTT_READ | MLX5_IB_MTT_WRITE)
+
 struct mlx5_ib_mr {
 	struct ib_mr		ibmr;
 	struct mlx5_core_mr	mmr;
@@ -552,7 +559,7 @@ void mlx5_ib_cleanup_fmr(struct mlx5_ib_dev *dev);
 void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift,
 			int *ncont, int *order);
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
-			  int page_shift, __be64 *pas, int umr);
+			  int page_shift, __be64 *pas, int access_flags);
 void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
 int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
 int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
@@ -588,4 +595,7 @@ static inline u8 convert_access(int acc)
 	       MLX5_PERM_LOCAL_READ;
 }
 
+#define MLX5_MAX_UMR_SHIFT 16
+#define MLX5_MAX_UMR_PAGES (1 << MLX5_MAX_UMR_SHIFT)
+
 #endif /* MLX5_IB_H */

commit 8cdd312cfed706b067d7ea952603e28cc33c40cc
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:20 2014 +0200

    IB/mlx5: Implement the ODP capability query verb
    
    The patch adds infrastructure to query ODP capabilities in the mlx5
    driver. The code will read the capabilities from the device, and
    enable only those capabilities that both the driver and the device
    supports.  At this point ODP is not supported, so no capability is
    copied from the device, but the patch exposes the global ODP device
    capability bit.
    
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 14a0311eaa1c..cc50fce8cca7 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -390,6 +390,9 @@ struct mlx5_ib_dev {
 	struct mlx5_mr_cache		cache;
 	struct timer_list		delay_timer;
 	int				fill_delay;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+	struct ib_odp_caps	odp_caps;
+#endif
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
@@ -559,6 +562,15 @@ void mlx5_umr_cq_handler(struct ib_cq *cq, void *cq_context);
 int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
 			    struct ib_mr_status *mr_status);
 
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
+int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev);
+#else
+static inline int mlx5_ib_internal_query_odp_caps(struct mlx5_ib_dev *dev)
+{
+	return 0;
+}
+#endif /* CONFIG_INFINIBAND_ON_DEMAND_PAGING */
+
 static inline void init_query_mad(struct ib_smp *mad)
 {
 	mad->base_version  = 1;

commit c1395a2a8c01e8a919e47d64eb3d23d00e824b8b
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:14 2014 +0200

    IB/mlx5: Add function to read WQE from user-space
    
    Add a helper function mlx5_ib_read_user_wqe to read information from
    user-space owned work queues.  The function will be used in a later
    patch by the page-fault handling code in mlx5_ib.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    
    [ Add stub for ib_umem_copy_from() for CONFIG_INFINIBAND_USER_MEM=n
      - Roland ]
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 53d19e6e69a4..14a0311eaa1c 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -503,6 +503,8 @@ int mlx5_ib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
 int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,
 		      struct ib_recv_wr **bad_wr);
 void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
+int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
+			  void *buffer, u32 length);
 struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev, int entries,
 				int vector, struct ib_ucontext *context,
 				struct ib_udata *udata);

commit 968e78dd96443e2cc963c493070574778805e76a
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:11 2014 +0200

    IB/mlx5: Enhance UMR support to allow partial page table update
    
    The current UMR interface doesn't allow partial updates to a memory
    region's page tables. This patch changes the interface to allow that.
    
    It also changes the way the UMR operation validates the memory
    region's state.  When set, IB_SEND_UMR_FAIL_IF_FREE will cause the UMR
    operation to fail if the MKEY is in the free state. When it is
    unchecked the operation will check that it isn't in the free state.
    
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 29da55222070..53d19e6e69a4 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -111,6 +111,8 @@ struct mlx5_ib_pd {
  */
 
 #define MLX5_IB_SEND_UMR_UNREG	IB_SEND_RESERVED_START
+#define MLX5_IB_SEND_UMR_FAIL_IF_FREE (IB_SEND_RESERVED_START << 1)
+#define MLX5_IB_SEND_UMR_UPDATE_MTT (IB_SEND_RESERVED_START << 2)
 #define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
 #define MLX5_IB_WR_UMR		IB_WR_RESERVED1
 
@@ -206,6 +208,19 @@ enum mlx5_ib_qp_flags {
 	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 1,
 };
 
+struct mlx5_umr_wr {
+	union {
+		u64			virt_addr;
+		u64			offset;
+	} target;
+	struct ib_pd		       *pd;
+	unsigned int			page_shift;
+	unsigned int			npages;
+	u32				length;
+	int				access_flags;
+	u32				mkey;
+};
+
 struct mlx5_shared_mr_info {
 	int mr_id;
 	struct ib_umem		*umem;

commit 21af2c3ebfd551660ae0016ecc5bc9afcc24f116
Author: Haggai Eran <haggaie@mellanox.com>
Date:   Thu Dec 11 17:04:10 2014 +0200

    IB/mlx5: Remove per-MR pas and dma pointers
    
    Since UMR code now uses its own context struct on the stack, the pas
    and dma pointers for the UMR operation that remained in the mlx5_ib_mr
    struct are not necessary.  This patch removes them.
    
    Fixes: a74d24168d2d ("IB/mlx5: Refactor UMR to have its own context struct")
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 386780f0d1e1..29da55222070 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -261,8 +261,6 @@ struct mlx5_ib_mr {
 	struct list_head	list;
 	int			order;
 	int			umred;
-	__be64			*pas;
-	dma_addr_t		dma;
 	int			npages;
 	struct mlx5_ib_dev     *dev;
 	struct mlx5_create_mkey_mbox_out out;

commit f241e7497ec2d22b83002b17ae91a851d4034cb7
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Mon Jul 28 23:30:23 2014 +0300

    mlx5: minor fixes (mainly avoidance of hidden casts)
    
    There were many places where parameters which should be u8/u16 were
    integer type.
    
    Additionally, in 2 places, a check for a non-null pointer was added
    before dereferencing the pointer (this is actually a bug fix).
    
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index a0e204ffe367..386780f0d1e1 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -461,7 +461,7 @@ void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq)
 void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
 void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
 int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
-		 int port, struct ib_wc *in_wc, struct ib_grh *in_grh,
+		 u8 port, struct ib_wc *in_wc, struct ib_grh *in_grh,
 		 void *in_mad, void *response_mad);
 struct ib_ah *create_ib_ah(struct ib_ah_attr *ah_attr,
 			   struct mlx5_ib_ah *ah);

commit 9603b61de1eee92977d74ff42541be20c0c5b1a7
Author: Jack Morgenstein <jackm@dev.mellanox.co.il>
Date:   Mon Jul 28 23:30:22 2014 +0300

    mlx5: Move pci device handling from mlx5_ib to mlx5_core
    
    In preparation for a new mlx5 device which is VPI (i.e., ports can be
    either IB or ETH), move the pci device functionality from mlx5_ib
    to mlx5_core.
    
    This involves the following changes:
    1. Move mlx5_core_dev struct out of mlx5_ib_dev. mlx5_core_dev
       is now an independent structure maintained by mlx5_core.
       mlx5_ib_dev now has a pointer to that struct.
       This requires changing a lot of places where the core_dev
       struct was accessed via mlx5_ib_dev (now, this needs to
       be a pointer dereference).
    2. All PCI initializations are now done in mlx5_core. Thus,
       it is now mlx5_core which does pci_register_device (and not
       mlx5_ib, as was previously).
    3. mlx5_ib now registers itself with mlx5_core as an "interface"
       driver. This is very similar to the mechanism employed for
       the mlx4 (ConnectX) driver. Once the HCA is initialized
       (by mlx5_core), it invokes the interface drivers to do
       their initializations.
    4. There is a new event handler which the core registers:
       mlx5_core_event(). This event handler invokes the
       event handlers registered by the interfaces.
    
    Based on a patch by Eli Cohen <eli@mellanox.com>
    
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index f2ccf1a5a291..a0e204ffe367 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -360,7 +360,7 @@ struct mlx5_ib_resources {
 
 struct mlx5_ib_dev {
 	struct ib_device		ib_dev;
-	struct mlx5_core_dev		mdev;
+	struct mlx5_core_dev		*mdev;
 	MLX5_DECLARE_DOORBELL_LOCK(uar_lock);
 	struct list_head		eqs_list;
 	int				num_ports;
@@ -454,16 +454,6 @@ static inline struct mlx5_ib_ah *to_mah(struct ib_ah *ibah)
 	return container_of(ibah, struct mlx5_ib_ah, ibah);
 }
 
-static inline struct mlx5_ib_dev *mlx5_core2ibdev(struct mlx5_core_dev *dev)
-{
-	return container_of(dev, struct mlx5_ib_dev, mdev);
-}
-
-static inline struct mlx5_ib_dev *mlx5_pci2ibdev(struct pci_dev *pdev)
-{
-	return mlx5_core2ibdev(pci2mlx5_core_dev(pdev));
-}
-
 int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context, unsigned long virt,
 			struct mlx5_db *db);
 void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db);

commit a74d24168d2df78e7a532567eb0e7538e6b09568
Author: Shachar Raindel <raindel@mellanox.com>
Date:   Thu May 22 14:50:12 2014 +0300

    IB/mlx5: Refactor UMR to have its own context struct
    
    Instead of having the UMR context part of each memory region, allocate
    a struct on the stack.  This allows queuing multiple UMRs that access
    the same memory region.
    
    Signed-off-by: Shachar Raindel <raindel@mellanox.com>
    Signed-off-by: Haggai Eran <haggaie@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 50541586e0a6..f2ccf1a5a291 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -264,8 +264,6 @@ struct mlx5_ib_mr {
 	__be64			*pas;
 	dma_addr_t		dma;
 	int			npages;
-	struct completion	done;
-	enum ib_wc_status	status;
 	struct mlx5_ib_dev     *dev;
 	struct mlx5_create_mkey_mbox_out out;
 	struct mlx5_core_sig_ctx    *sig;
@@ -277,6 +275,17 @@ struct mlx5_ib_fast_reg_page_list {
 	dma_addr_t			map;
 };
 
+struct mlx5_ib_umr_context {
+	enum ib_wc_status	status;
+	struct completion	done;
+};
+
+static inline void mlx5_ib_init_umr_context(struct mlx5_ib_umr_context *context)
+{
+	context->status = -1;
+	init_completion(&context->done);
+}
+
 struct umr_common {
 	struct ib_pd	*pd;
 	struct ib_cq	*cq;

commit d5436ba01075ef4629015f7a00914d64ffd795d6
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Sun Feb 23 14:19:12 2014 +0200

    IB/mlx5: Collect signature error completion
    
    This commit takes care of the generated signature error CQE generated
    by the HW (if happened).  The underlying mlx5 driver will handle
    signature error completions and will mark the relevant memory region
    as dirty.
    
    Once the consumer gets the completion for the transaction, it must
    check for signature errors on signature memory region using a new
    lightweight verb ib_check_mr_status().
    
    In case the user doesn't check for signature error (i.e. doesn't call
    ib_check_mr_status() with status check IB_MR_CHECK_SIG_STATUS), the
    memory region cannot be used for another signature operation
    (REG_SIG_MR work request will fail).
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index e438f08899ae..50541586e0a6 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -400,6 +400,11 @@ static inline struct mlx5_ib_qp *to_mibqp(struct mlx5_core_qp *mqp)
 	return container_of(mqp, struct mlx5_ib_qp, mqp);
 }
 
+static inline struct mlx5_ib_mr *to_mibmr(struct mlx5_core_mr *mmr)
+{
+	return container_of(mmr, struct mlx5_ib_mr, mmr);
+}
+
 static inline struct mlx5_ib_pd *to_mpd(struct ib_pd *ibpd)
 {
 	return container_of(ibpd, struct mlx5_ib_pd, ibpd);
@@ -537,6 +542,8 @@ int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
 int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
 int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
 void mlx5_umr_cq_handler(struct ib_cq *cq, void *cq_context);
+int mlx5_ib_check_mr_status(struct ib_mr *ibmr, u32 check_mask,
+			    struct ib_mr_status *mr_status);
 
 static inline void init_query_mad(struct ib_smp *mad)
 {

commit e1e66cc26457c2e9412f67618646ec2a441fc409
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Sun Feb 23 14:19:07 2014 +0200

    IB/mlx5: Initialize mlx5_ib_qp signature-related members
    
    If user requested signature enable we initialize relevant mlx5_ib_qp
    members.  We mark the qp as sig_enable and we increase the effective
    SQ size, but still limit the user max_send_wr to original size
    computed.  We also allow the create_qp routine to accept sig_enable
    create flag.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 79c4f14c4d3e..e438f08899ae 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -189,6 +189,9 @@ struct mlx5_ib_qp {
 
 	int			create_type;
 	u32			pa_lkey;
+
+	/* Store signature errors */
+	bool			signature_en;
 };
 
 struct mlx5_ib_cq_buf {

commit 3121e3c441b5eccdd15e6c320ec32215b334b9ec
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Sun Feb 23 14:19:06 2014 +0200

    mlx5: Implement create_mr and destroy_mr
    
    Support create_mr and destroy_mr verbs.  Creating ib_mr may be done
    for either ib_mr that will register regular page lists like
    alloc_fast_reg_mr routine, or indirect ib_mrs that can register other
    (pre-registered) ib_mrs in an indirect manner.
    
    In addition user may request signature enable, that will mean that the
    created ib_mr may be attached with signature attributes (BSF, PSVs).
    
    Currently we only allow direct/indirect registration modes.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 389e31965773..79c4f14c4d3e 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -265,6 +265,7 @@ struct mlx5_ib_mr {
 	enum ib_wc_status	status;
 	struct mlx5_ib_dev     *dev;
 	struct mlx5_create_mkey_mbox_out out;
+	struct mlx5_core_sig_ctx    *sig;
 };
 
 struct mlx5_ib_fast_reg_page_list {
@@ -495,6 +496,9 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
 				  u64 virt_addr, int access_flags,
 				  struct ib_udata *udata);
 int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
+int mlx5_ib_destroy_mr(struct ib_mr *ibmr);
+struct ib_mr *mlx5_ib_create_mr(struct ib_pd *pd,
+				struct ib_mr_init_attr *mr_init_attr);
 struct ib_mr *mlx5_ib_alloc_fast_reg_mr(struct ib_pd *pd,
 					int max_page_list_len);
 struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,

commit bde51583f49bd87e452e9504d489926638046b11
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Tue Jan 14 17:45:18 2014 +0200

    IB/mlx5: Add support for resize CQ
    
    Implement resize CQ which is a mandatory verb in mlx5.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 5acef30a4998..389e31965773 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -195,6 +195,7 @@ struct mlx5_ib_cq_buf {
 	struct mlx5_buf		buf;
 	struct ib_umem		*umem;
 	int			cqe_size;
+	int			nent;
 };
 
 enum mlx5_ib_qp_flags {
@@ -220,7 +221,7 @@ struct mlx5_ib_cq {
 	/* protect resize cq
 	 */
 	struct mutex		resize_mutex;
-	struct mlx5_ib_cq_resize *resize_buf;
+	struct mlx5_ib_cq_buf  *resize_buf;
 	struct ib_umem	       *resize_umem;
 	int			cqe_size;
 };

commit d9fe40916387bab884e458c99399c149b033506c
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Tue Jan 14 17:45:10 2014 +0200

    IB/mlx5: Remove unused code in mr.c
    
    The variable start in struct mlx5_ib_mr is never used. Remove it.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 4c134d93d4fc..5acef30a4998 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -264,7 +264,6 @@ struct mlx5_ib_mr {
 	enum ib_wc_status	status;
 	struct mlx5_ib_dev     *dev;
 	struct mlx5_create_mkey_mbox_out out;
-	unsigned long		start;
 };
 
 struct mlx5_ib_fast_reg_page_list {

commit 746b5583c1a48a837f4891adaff5e09d61b204a6
Author: Eli Cohen <eli@dev.mellanox.co.il>
Date:   Wed Oct 23 09:53:14 2013 +0300

    IB/mlx5: Multithreaded create MR
    
    Use asynchronous commands to execute up to eight concurrent create MR
    commands. This is to fill memory caches faster so we keep consuming
    from there.  Also, increase timeout for shrinking caches to five
    minutes.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
index 836be9157242..4c134d93d4fc 100644
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -262,6 +262,9 @@ struct mlx5_ib_mr {
 	int			npages;
 	struct completion	done;
 	enum ib_wc_status	status;
+	struct mlx5_ib_dev     *dev;
+	struct mlx5_create_mkey_mbox_out out;
+	unsigned long		start;
 };
 
 struct mlx5_ib_fast_reg_page_list {
@@ -323,6 +326,7 @@ struct mlx5_cache_ent {
 	struct mlx5_ib_dev     *dev;
 	struct work_struct	work;
 	struct delayed_work	dwork;
+	int			pending;
 };
 
 struct mlx5_mr_cache {
@@ -358,6 +362,8 @@ struct mlx5_ib_dev {
 	spinlock_t			mr_lock;
 	struct mlx5_ib_resources	devr;
 	struct mlx5_mr_cache		cache;
+	struct timer_list		delay_timer;
+	int				fill_delay;
 };
 
 static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)

commit e126ba97dba9edeb6fafa3665b5f8497fc9cdf8c
Author: Eli Cohen <eli@mellanox.com>
Date:   Sun Jul 7 17:25:49 2013 +0300

    mlx5: Add driver for Mellanox Connect-IB adapters
    
    The driver is comprised of two kernel modules: mlx5_ib and mlx5_core.
    This partitioning resembles what we have for mlx4, except that mlx5_ib
    is the pci device driver and not mlx5_core.
    
    mlx5_core is essentially a library that provides general functionality
    that is intended to be used by other Mellanox devices that will be
    introduced in the future.  mlx5_ib has a similar role as any hardware
    device under drivers/infiniband/hw.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Jack Morgenstein <jackm@dev.mellanox.co.il>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    
    [ Merge in coccinelle fixes from Fengguang Wu <fengguang.wu@intel.com>.
      - Roland ]
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>

diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h
new file mode 100644
index 000000000000..836be9157242
--- /dev/null
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -0,0 +1,545 @@
+/*
+ * Copyright (c) 2013, Mellanox Technologies inc.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef MLX5_IB_H
+#define MLX5_IB_H
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <rdma/ib_verbs.h>
+#include <rdma/ib_smi.h>
+#include <linux/mlx5/driver.h>
+#include <linux/mlx5/cq.h>
+#include <linux/mlx5/qp.h>
+#include <linux/mlx5/srq.h>
+#include <linux/types.h>
+
+#define mlx5_ib_dbg(dev, format, arg...)				\
+pr_debug("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
+	 __LINE__, current->pid, ##arg)
+
+#define mlx5_ib_err(dev, format, arg...)				\
+pr_err("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
+	__LINE__, current->pid, ##arg)
+
+#define mlx5_ib_warn(dev, format, arg...)				\
+pr_warn("%s:%s:%d:(pid %d): " format, (dev)->ib_dev.name, __func__,	\
+	__LINE__, current->pid, ##arg)
+
+enum {
+	MLX5_IB_MMAP_CMD_SHIFT	= 8,
+	MLX5_IB_MMAP_CMD_MASK	= 0xff,
+};
+
+enum mlx5_ib_mmap_cmd {
+	MLX5_IB_MMAP_REGULAR_PAGE		= 0,
+	MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES	= 1, /* always last */
+};
+
+enum {
+	MLX5_RES_SCAT_DATA32_CQE	= 0x1,
+	MLX5_RES_SCAT_DATA64_CQE	= 0x2,
+	MLX5_REQ_SCAT_DATA32_CQE	= 0x11,
+	MLX5_REQ_SCAT_DATA64_CQE	= 0x22,
+};
+
+enum mlx5_ib_latency_class {
+	MLX5_IB_LATENCY_CLASS_LOW,
+	MLX5_IB_LATENCY_CLASS_MEDIUM,
+	MLX5_IB_LATENCY_CLASS_HIGH,
+	MLX5_IB_LATENCY_CLASS_FAST_PATH
+};
+
+enum mlx5_ib_mad_ifc_flags {
+	MLX5_MAD_IFC_IGNORE_MKEY	= 1,
+	MLX5_MAD_IFC_IGNORE_BKEY	= 2,
+	MLX5_MAD_IFC_NET_VIEW		= 4,
+};
+
+struct mlx5_ib_ucontext {
+	struct ib_ucontext	ibucontext;
+	struct list_head	db_page_list;
+
+	/* protect doorbell record alloc/free
+	 */
+	struct mutex		db_page_mutex;
+	struct mlx5_uuar_info	uuari;
+};
+
+static inline struct mlx5_ib_ucontext *to_mucontext(struct ib_ucontext *ibucontext)
+{
+	return container_of(ibucontext, struct mlx5_ib_ucontext, ibucontext);
+}
+
+struct mlx5_ib_pd {
+	struct ib_pd		ibpd;
+	u32			pdn;
+	u32			pa_lkey;
+};
+
+/* Use macros here so that don't have to duplicate
+ * enum ib_send_flags and enum ib_qp_type for low-level driver
+ */
+
+#define MLX5_IB_SEND_UMR_UNREG	IB_SEND_RESERVED_START
+#define MLX5_IB_QPT_REG_UMR	IB_QPT_RESERVED1
+#define MLX5_IB_WR_UMR		IB_WR_RESERVED1
+
+struct wr_list {
+	u16	opcode;
+	u16	next;
+};
+
+struct mlx5_ib_wq {
+	u64		       *wrid;
+	u32		       *wr_data;
+	struct wr_list	       *w_list;
+	unsigned	       *wqe_head;
+	u16		        unsig_count;
+
+	/* serialize post to the work queue
+	 */
+	spinlock_t		lock;
+	int			wqe_cnt;
+	int			max_post;
+	int			max_gs;
+	int			offset;
+	int			wqe_shift;
+	unsigned		head;
+	unsigned		tail;
+	u16			cur_post;
+	u16			last_poll;
+	void		       *qend;
+};
+
+enum {
+	MLX5_QP_USER,
+	MLX5_QP_KERNEL,
+	MLX5_QP_EMPTY
+};
+
+struct mlx5_ib_qp {
+	struct ib_qp		ibqp;
+	struct mlx5_core_qp	mqp;
+	struct mlx5_buf		buf;
+
+	struct mlx5_db		db;
+	struct mlx5_ib_wq	rq;
+
+	u32			doorbell_qpn;
+	u8			sq_signal_bits;
+	u8			fm_cache;
+	int			sq_max_wqes_per_wr;
+	int			sq_spare_wqes;
+	struct mlx5_ib_wq	sq;
+
+	struct ib_umem	       *umem;
+	int			buf_size;
+
+	/* serialize qp state modifications
+	 */
+	struct mutex		mutex;
+	u16			xrcdn;
+	u32			flags;
+	u8			port;
+	u8			alt_port;
+	u8			atomic_rd_en;
+	u8			resp_depth;
+	u8			state;
+	int			mlx_type;
+	int			wq_sig;
+	int			scat_cqe;
+	int			max_inline_data;
+	struct mlx5_bf	       *bf;
+	int			has_rq;
+
+	/* only for user space QPs. For kernel
+	 * we have it from the bf object
+	 */
+	int			uuarn;
+
+	int			create_type;
+	u32			pa_lkey;
+};
+
+struct mlx5_ib_cq_buf {
+	struct mlx5_buf		buf;
+	struct ib_umem		*umem;
+	int			cqe_size;
+};
+
+enum mlx5_ib_qp_flags {
+	MLX5_IB_QP_BLOCK_MULTICAST_LOOPBACK     = 1 << 0,
+	MLX5_IB_QP_SIGNATURE_HANDLING           = 1 << 1,
+};
+
+struct mlx5_shared_mr_info {
+	int mr_id;
+	struct ib_umem		*umem;
+};
+
+struct mlx5_ib_cq {
+	struct ib_cq		ibcq;
+	struct mlx5_core_cq	mcq;
+	struct mlx5_ib_cq_buf	buf;
+	struct mlx5_db		db;
+
+	/* serialize access to the CQ
+	 */
+	spinlock_t		lock;
+
+	/* protect resize cq
+	 */
+	struct mutex		resize_mutex;
+	struct mlx5_ib_cq_resize *resize_buf;
+	struct ib_umem	       *resize_umem;
+	int			cqe_size;
+};
+
+struct mlx5_ib_srq {
+	struct ib_srq		ibsrq;
+	struct mlx5_core_srq	msrq;
+	struct mlx5_buf		buf;
+	struct mlx5_db		db;
+	u64		       *wrid;
+	/* protect SRQ hanlding
+	 */
+	spinlock_t		lock;
+	int			head;
+	int			tail;
+	u16			wqe_ctr;
+	struct ib_umem	       *umem;
+	/* serialize arming a SRQ
+	 */
+	struct mutex		mutex;
+	int			wq_sig;
+};
+
+struct mlx5_ib_xrcd {
+	struct ib_xrcd		ibxrcd;
+	u32			xrcdn;
+};
+
+struct mlx5_ib_mr {
+	struct ib_mr		ibmr;
+	struct mlx5_core_mr	mmr;
+	struct ib_umem	       *umem;
+	struct mlx5_shared_mr_info	*smr_info;
+	struct list_head	list;
+	int			order;
+	int			umred;
+	__be64			*pas;
+	dma_addr_t		dma;
+	int			npages;
+	struct completion	done;
+	enum ib_wc_status	status;
+};
+
+struct mlx5_ib_fast_reg_page_list {
+	struct ib_fast_reg_page_list	ibfrpl;
+	__be64			       *mapped_page_list;
+	dma_addr_t			map;
+};
+
+struct umr_common {
+	struct ib_pd	*pd;
+	struct ib_cq	*cq;
+	struct ib_qp	*qp;
+	struct ib_mr	*mr;
+	/* control access to UMR QP
+	 */
+	struct semaphore	sem;
+};
+
+enum {
+	MLX5_FMR_INVALID,
+	MLX5_FMR_VALID,
+	MLX5_FMR_BUSY,
+};
+
+struct mlx5_ib_fmr {
+	struct ib_fmr			ibfmr;
+	struct mlx5_core_mr		mr;
+	int				access_flags;
+	int				state;
+	/* protect fmr state
+	 */
+	spinlock_t			lock;
+	u64				wrid;
+	struct ib_send_wr		wr[2];
+	u8				page_shift;
+	struct ib_fast_reg_page_list	page_list;
+};
+
+struct mlx5_cache_ent {
+	struct list_head	head;
+	/* sync access to the cahce entry
+	 */
+	spinlock_t		lock;
+
+
+	struct dentry	       *dir;
+	char                    name[4];
+	u32                     order;
+	u32			size;
+	u32                     cur;
+	u32                     miss;
+	u32			limit;
+
+	struct dentry          *fsize;
+	struct dentry          *fcur;
+	struct dentry          *fmiss;
+	struct dentry          *flimit;
+
+	struct mlx5_ib_dev     *dev;
+	struct work_struct	work;
+	struct delayed_work	dwork;
+};
+
+struct mlx5_mr_cache {
+	struct workqueue_struct *wq;
+	struct mlx5_cache_ent	ent[MAX_MR_CACHE_ENTRIES];
+	int			stopped;
+	struct dentry		*root;
+	unsigned long		last_add;
+};
+
+struct mlx5_ib_resources {
+	struct ib_cq	*c0;
+	struct ib_xrcd	*x0;
+	struct ib_xrcd	*x1;
+	struct ib_pd	*p0;
+	struct ib_srq	*s0;
+};
+
+struct mlx5_ib_dev {
+	struct ib_device		ib_dev;
+	struct mlx5_core_dev		mdev;
+	MLX5_DECLARE_DOORBELL_LOCK(uar_lock);
+	struct list_head		eqs_list;
+	int				num_ports;
+	int				num_comp_vectors;
+	/* serialize update of capability mask
+	 */
+	struct mutex			cap_mask_mutex;
+	bool				ib_active;
+	struct umr_common		umrc;
+	/* sync used page count stats
+	 */
+	spinlock_t			mr_lock;
+	struct mlx5_ib_resources	devr;
+	struct mlx5_mr_cache		cache;
+};
+
+static inline struct mlx5_ib_cq *to_mibcq(struct mlx5_core_cq *mcq)
+{
+	return container_of(mcq, struct mlx5_ib_cq, mcq);
+}
+
+static inline struct mlx5_ib_xrcd *to_mxrcd(struct ib_xrcd *ibxrcd)
+{
+	return container_of(ibxrcd, struct mlx5_ib_xrcd, ibxrcd);
+}
+
+static inline struct mlx5_ib_dev *to_mdev(struct ib_device *ibdev)
+{
+	return container_of(ibdev, struct mlx5_ib_dev, ib_dev);
+}
+
+static inline struct mlx5_ib_fmr *to_mfmr(struct ib_fmr *ibfmr)
+{
+	return container_of(ibfmr, struct mlx5_ib_fmr, ibfmr);
+}
+
+static inline struct mlx5_ib_cq *to_mcq(struct ib_cq *ibcq)
+{
+	return container_of(ibcq, struct mlx5_ib_cq, ibcq);
+}
+
+static inline struct mlx5_ib_qp *to_mibqp(struct mlx5_core_qp *mqp)
+{
+	return container_of(mqp, struct mlx5_ib_qp, mqp);
+}
+
+static inline struct mlx5_ib_pd *to_mpd(struct ib_pd *ibpd)
+{
+	return container_of(ibpd, struct mlx5_ib_pd, ibpd);
+}
+
+static inline struct mlx5_ib_srq *to_msrq(struct ib_srq *ibsrq)
+{
+	return container_of(ibsrq, struct mlx5_ib_srq, ibsrq);
+}
+
+static inline struct mlx5_ib_qp *to_mqp(struct ib_qp *ibqp)
+{
+	return container_of(ibqp, struct mlx5_ib_qp, ibqp);
+}
+
+static inline struct mlx5_ib_srq *to_mibsrq(struct mlx5_core_srq *msrq)
+{
+	return container_of(msrq, struct mlx5_ib_srq, msrq);
+}
+
+static inline struct mlx5_ib_mr *to_mmr(struct ib_mr *ibmr)
+{
+	return container_of(ibmr, struct mlx5_ib_mr, ibmr);
+}
+
+static inline struct mlx5_ib_fast_reg_page_list *to_mfrpl(struct ib_fast_reg_page_list *ibfrpl)
+{
+	return container_of(ibfrpl, struct mlx5_ib_fast_reg_page_list, ibfrpl);
+}
+
+struct mlx5_ib_ah {
+	struct ib_ah		ibah;
+	struct mlx5_av		av;
+};
+
+static inline struct mlx5_ib_ah *to_mah(struct ib_ah *ibah)
+{
+	return container_of(ibah, struct mlx5_ib_ah, ibah);
+}
+
+static inline struct mlx5_ib_dev *mlx5_core2ibdev(struct mlx5_core_dev *dev)
+{
+	return container_of(dev, struct mlx5_ib_dev, mdev);
+}
+
+static inline struct mlx5_ib_dev *mlx5_pci2ibdev(struct pci_dev *pdev)
+{
+	return mlx5_core2ibdev(pci2mlx5_core_dev(pdev));
+}
+
+int mlx5_ib_db_map_user(struct mlx5_ib_ucontext *context, unsigned long virt,
+			struct mlx5_db *db);
+void mlx5_ib_db_unmap_user(struct mlx5_ib_ucontext *context, struct mlx5_db *db);
+void __mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
+void mlx5_ib_cq_clean(struct mlx5_ib_cq *cq, u32 qpn, struct mlx5_ib_srq *srq);
+void mlx5_ib_free_srq_wqe(struct mlx5_ib_srq *srq, int wqe_index);
+int mlx5_MAD_IFC(struct mlx5_ib_dev *dev, int ignore_mkey, int ignore_bkey,
+		 int port, struct ib_wc *in_wc, struct ib_grh *in_grh,
+		 void *in_mad, void *response_mad);
+struct ib_ah *create_ib_ah(struct ib_ah_attr *ah_attr,
+			   struct mlx5_ib_ah *ah);
+struct ib_ah *mlx5_ib_create_ah(struct ib_pd *pd, struct ib_ah_attr *ah_attr);
+int mlx5_ib_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr);
+int mlx5_ib_destroy_ah(struct ib_ah *ah);
+struct ib_srq *mlx5_ib_create_srq(struct ib_pd *pd,
+				  struct ib_srq_init_attr *init_attr,
+				  struct ib_udata *udata);
+int mlx5_ib_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,
+		       enum ib_srq_attr_mask attr_mask, struct ib_udata *udata);
+int mlx5_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr);
+int mlx5_ib_destroy_srq(struct ib_srq *srq);
+int mlx5_ib_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,
+			  struct ib_recv_wr **bad_wr);
+struct ib_qp *mlx5_ib_create_qp(struct ib_pd *pd,
+				struct ib_qp_init_attr *init_attr,
+				struct ib_udata *udata);
+int mlx5_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
+		      int attr_mask, struct ib_udata *udata);
+int mlx5_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *qp_attr, int qp_attr_mask,
+		     struct ib_qp_init_attr *qp_init_attr);
+int mlx5_ib_destroy_qp(struct ib_qp *qp);
+int mlx5_ib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,
+		      struct ib_send_wr **bad_wr);
+int mlx5_ib_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,
+		      struct ib_recv_wr **bad_wr);
+void *mlx5_get_send_wqe(struct mlx5_ib_qp *qp, int n);
+struct ib_cq *mlx5_ib_create_cq(struct ib_device *ibdev, int entries,
+				int vector, struct ib_ucontext *context,
+				struct ib_udata *udata);
+int mlx5_ib_destroy_cq(struct ib_cq *cq);
+int mlx5_ib_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
+int mlx5_ib_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
+int mlx5_ib_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period);
+int mlx5_ib_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata);
+struct ib_mr *mlx5_ib_get_dma_mr(struct ib_pd *pd, int acc);
+struct ib_mr *mlx5_ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,
+				  u64 virt_addr, int access_flags,
+				  struct ib_udata *udata);
+int mlx5_ib_dereg_mr(struct ib_mr *ibmr);
+struct ib_mr *mlx5_ib_alloc_fast_reg_mr(struct ib_pd *pd,
+					int max_page_list_len);
+struct ib_fast_reg_page_list *mlx5_ib_alloc_fast_reg_page_list(struct ib_device *ibdev,
+							       int page_list_len);
+void mlx5_ib_free_fast_reg_page_list(struct ib_fast_reg_page_list *page_list);
+struct ib_fmr *mlx5_ib_fmr_alloc(struct ib_pd *pd, int acc,
+				 struct ib_fmr_attr *fmr_attr);
+int mlx5_ib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,
+		      int npages, u64 iova);
+int mlx5_ib_unmap_fmr(struct list_head *fmr_list);
+int mlx5_ib_fmr_dealloc(struct ib_fmr *ibfmr);
+int mlx5_ib_process_mad(struct ib_device *ibdev, int mad_flags, u8 port_num,
+			struct ib_wc *in_wc, struct ib_grh *in_grh,
+			struct ib_mad *in_mad, struct ib_mad *out_mad);
+struct ib_xrcd *mlx5_ib_alloc_xrcd(struct ib_device *ibdev,
+					  struct ib_ucontext *context,
+					  struct ib_udata *udata);
+int mlx5_ib_dealloc_xrcd(struct ib_xrcd *xrcd);
+int mlx5_vector2eqn(struct mlx5_ib_dev *dev, int vector, int *eqn, int *irqn);
+int mlx5_ib_get_buf_offset(u64 addr, int page_shift, u32 *offset);
+int mlx5_query_ext_port_caps(struct mlx5_ib_dev *dev, u8 port);
+int mlx5_ib_query_port(struct ib_device *ibdev, u8 port,
+		       struct ib_port_attr *props);
+int mlx5_ib_init_fmr(struct mlx5_ib_dev *dev);
+void mlx5_ib_cleanup_fmr(struct mlx5_ib_dev *dev);
+void mlx5_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift,
+			int *ncont, int *order);
+void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
+			  int page_shift, __be64 *pas, int umr);
+void mlx5_ib_copy_pas(u64 *old, u64 *new, int step, int num);
+int mlx5_ib_get_cqe_size(struct mlx5_ib_dev *dev, struct ib_cq *ibcq);
+int mlx5_mr_cache_init(struct mlx5_ib_dev *dev);
+int mlx5_mr_cache_cleanup(struct mlx5_ib_dev *dev);
+int mlx5_mr_ib_cont_pages(struct ib_umem *umem, u64 addr, int *count, int *shift);
+void mlx5_umr_cq_handler(struct ib_cq *cq, void *cq_context);
+
+static inline void init_query_mad(struct ib_smp *mad)
+{
+	mad->base_version  = 1;
+	mad->mgmt_class    = IB_MGMT_CLASS_SUBN_LID_ROUTED;
+	mad->class_version = 1;
+	mad->method	   = IB_MGMT_METHOD_GET;
+}
+
+static inline u8 convert_access(int acc)
+{
+	return (acc & IB_ACCESS_REMOTE_ATOMIC ? MLX5_PERM_ATOMIC       : 0) |
+	       (acc & IB_ACCESS_REMOTE_WRITE  ? MLX5_PERM_REMOTE_WRITE : 0) |
+	       (acc & IB_ACCESS_REMOTE_READ   ? MLX5_PERM_REMOTE_READ  : 0) |
+	       (acc & IB_ACCESS_LOCAL_WRITE   ? MLX5_PERM_LOCAL_WRITE  : 0) |
+	       MLX5_PERM_LOCAL_READ;
+}
+
+#endif /* MLX5_IB_H */
