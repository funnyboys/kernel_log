commit be9c52ed84eb0949fed3d4140e35ea70eecb02a2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:08 2019 +0200

    dm persistent data: Simplify stack trace handling
    
    Replace the indirection through struct stack_trace with an invocation of
    the storage array based interface. This results in less storage space and
    indirection.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094802.533968922@linutronix.de

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 3972232b8037..749ec268d957 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -35,7 +35,10 @@
 #define MAX_HOLDERS 4
 #define MAX_STACK 10
 
-typedef unsigned long stack_entries[MAX_STACK];
+struct stack_store {
+	unsigned int	nr_entries;
+	unsigned long	entries[MAX_STACK];
+};
 
 struct block_lock {
 	spinlock_t lock;
@@ -44,8 +47,7 @@ struct block_lock {
 	struct task_struct *holders[MAX_HOLDERS];
 
 #ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
-	struct stack_trace traces[MAX_HOLDERS];
-	stack_entries entries[MAX_HOLDERS];
+	struct stack_store traces[MAX_HOLDERS];
 #endif
 };
 
@@ -73,7 +75,7 @@ static void __add_holder(struct block_lock *lock, struct task_struct *task)
 {
 	unsigned h = __find_holder(lock, NULL);
 #ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
-	struct stack_trace *t;
+	struct stack_store *t;
 #endif
 
 	get_task_struct(task);
@@ -81,11 +83,7 @@ static void __add_holder(struct block_lock *lock, struct task_struct *task)
 
 #ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
 	t = lock->traces + h;
-	t->nr_entries = 0;
-	t->max_entries = MAX_STACK;
-	t->entries = lock->entries[h];
-	t->skip = 2;
-	save_stack_trace(t);
+	t->nr_entries = stack_trace_save(t->entries, MAX_STACK, 2);
 #endif
 }
 
@@ -106,7 +104,8 @@ static int __check_holder(struct block_lock *lock)
 			DMERR("recursive lock detected in metadata");
 #ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
 			DMERR("previously held here:");
-			print_stack_trace(lock->traces + i, 4);
+			stack_trace_print(lock->traces[i].entries,
+					  lock->traces[i].nr_entries, 4);
 
 			DMERR("subsequent acquisition attempted here:");
 			dump_stack();

commit 5941c621dc9eb61e3b22f934675577d559d1fa27
Author: Chengguang Xu <cgxu519@gmx.com>
Date:   Wed Feb 13 13:46:58 2019 +0800

    dm block manager: remove redundant unlikely annotation
    
    unlikely has already included in IS_ERR(),
    so just remove redundant unlikely annotation.
    
    Signed-off-by: Chengguang Xu <cgxu519@gmx.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 492a3f8ac119..3972232b8037 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -462,7 +462,7 @@ int dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,
 	int r;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -498,7 +498,7 @@ int dm_bm_write_lock(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -531,7 +531,7 @@ int dm_bm_read_try_lock(struct dm_block_manager *bm,
 	int r;
 
 	p = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 	if (unlikely(!p))
 		return -EWOULDBLOCK;
@@ -567,7 +567,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	memset(p, 0, dm_bm_block_size(bm));

commit afa53df869121fd4f6f1265cbe794d64387890ae
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Thu Mar 15 16:02:31 2018 -0400

    dm bufio: move dm-bufio.h to include/linux/
    
    Move dm-bufio.h to include/linux/ so that external GPL'd DM target
    modules can use it.
    
    It is better to allow the use of dm-bufio than force external modules
    to implement the equivalent buffered IO mechanism in some new way.  The
    hope is this will encourage the use of dm-bufio; which will then make it
    easier for a GPL'd external DM target module to be included upstream.
    
    A couple dm-bufio EXPORT_SYMBOL exports have also been updated to use
    EXPORT_SYMBOL_GPL.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index ea15d220ced7..492a3f8ac119 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -5,8 +5,8 @@
  */
 #include "dm-block-manager.h"
 #include "dm-persistent-data-internal.h"
-#include "../dm-bufio.h"
 
+#include <linux/dm-bufio.h>
 #include <linux/crc32c.h>
 #include <linux/module.h>
 #include <linux/slab.h>

commit 73cbca6a637eb88738ea5a5cd6a611bbbca8ac19
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Thu Apr 27 10:11:20 2017 -0700

    dm block manager: remove an unused argument from dm_block_manager_create()
    
    The 'cache_size' argument of dm_block_manager_create() has never been
    used.  Remove it along with the definitions of the constants passed as
    the 'cache_size' argument.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 8589e0a14068..ea15d220ced7 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -378,7 +378,6 @@ struct dm_block_manager {
 
 struct dm_block_manager *dm_block_manager_create(struct block_device *bdev,
 						 unsigned block_size,
-						 unsigned cache_size,
 						 unsigned max_held_per_thread)
 {
 	int r;

commit 0881e7bd341e2158b314596bcf2059e88e68f04e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 15:30:50 2017 +0100

    sched/headers: Prepare to move the get_task_struct()/put_task_struct() and related APIs from <linux/sched.h> to <linux/sched/task.h>
    
    But first update usage sites with the new header dependency.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 0863905dee02..8589e0a14068 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -13,6 +13,7 @@
 #include <linux/rwsem.h>
 #include <linux/device-mapper.h>
 #include <linux/stacktrace.h>
+#include <linux/sched/task.h>
 
 #define DM_MSG_PREFIX "block manager"
 

commit 7a771ceac771d009f7203c40b256b0608d7ea2f8
Merge: e67bd12d6036 d67a5f4b5947
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 12:11:41 2017 -0800

    Merge tag 'dm-4.11-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm
    
    Pull device mapper updates from Mike Snitzer:
    
     - Fix dm-raid transient device failure processing and other smaller
       tweaks.
    
     - Add journal support to the DM raid target to close the 'write hole'
       on raid 4/5/6.
    
     - Fix dm-cache corruption, due to rounding bug, when cache exceeds 2TB.
    
     - Add 'metadata2' feature to dm-cache to separate the dirty bitset out
       from other cache metadata. This improves speed of shutting down a
       large cache device (which implies writing out dirty bits).
    
     - Fix a memory leak during dm-stats data structure destruction.
    
     - Fix a DM multipath round-robin path selector performance regression
       that was caused by less precise balancing across all paths.
    
     - Lastly, introduce a DM core fix for a long-standing DM snapshot
       deadlock that is rooted in the complexity of the device stack used in
       conjunction with block core maintaining bios on current->bio_list to
       manage recursion in generic_make_request(). A more comprehensive fix
       to block core (and its hook in the cpu scheduler) would be wonderful
       but this DM-specific fix is pragmatic considering how difficult it
       has been to make progress on a generic fix.
    
    * tag 'dm-4.11-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm: (22 commits)
      dm: flush queued bios when process blocks to avoid deadlock
      dm round robin: revert "use percpu 'repeat_count' and 'current_path'"
      dm stats: fix a leaked s->histogram_boundaries array
      dm space map metadata: constify dm_space_map structures
      dm cache metadata: use cursor api in blocks_are_clean_separate_dirty()
      dm persistent data: add cursor skip functions to the cursor APIs
      dm cache metadata: use dm_bitset_new() to create the dirty bitset in format 2
      dm bitset: add dm_bitset_new()
      dm cache metadata: name the cache block that couldn't be loaded
      dm cache metadata: add "metadata2" feature
      dm cache metadata: use bitset cursor api to load discard bitset
      dm bitset: introduce cursor api
      dm btree: use GFP_NOFS in dm_btree_del()
      dm space map common: memcpy the disk root to ensure it's arch aligned
      dm block manager: add unlikely() annotations on dm_bufio error paths
      dm cache: fix corruption seen when using cache > 2TB
      dm raid: cleanup awkward branching in raid_message() option processing
      dm raid: use mddev rather than rdev->mddev
      dm raid: use read_disk_sb() throughout
      dm raid: add raid4/5/6 journaling support
      ...

commit 602548bdd5ac4ed7025d992e3ad61a628af4c500
Author: Joe Thornber <ejt@redhat.com>
Date:   Thu Nov 19 12:55:58 2015 +0000

    dm block manager: add unlikely() annotations on dm_bufio error paths
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index a6dde7cab458..8212f14214f1 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -462,7 +462,7 @@ int dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,
 	int r;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (IS_ERR(p))
+	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -498,7 +498,7 @@ int dm_bm_write_lock(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (IS_ERR(p))
+	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -531,7 +531,7 @@ int dm_bm_read_try_lock(struct dm_block_manager *bm,
 	int r;
 
 	p = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);
-	if (IS_ERR(p))
+	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 	if (unlikely(!p))
 		return -EWOULDBLOCK;
@@ -567,7 +567,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);
-	if (IS_ERR(p))
+	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
 	memset(p, 0, dm_bm_block_size(bm));

commit 642fa448ae6b3a4e5e8737054a094173405b7643
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue Jan 3 13:43:14 2017 -0800

    sched/core: Remove set_task_state()
    
    This is a nasty interface and setting the state of a foreign task must
    not be done. As of the following commit:
    
      be628be0956 ("bcache: Make gc wakeup sane, remove set_task_state()")
    
    ... everyone in the kernel calls set_task_state() with current, allowing
    the helper to be removed.
    
    However, as the comment indicates, it is still around for those archs
    where computing current is more expensive than using a pointer, at least
    in theory. An important arch that is affected is arm64, however this has
    been addressed now [1] and performance is up to par making no difference
    with either calls.
    
    Of all the callers, if any, it's the locking bits that would care most
    about this -- ie: we end up passing a tsk pointer to a lot of the lock
    slowpath, and setting ->state on that. The following numbers are based
    on two tests: a custom ad-hoc microbenchmark that just measures
    latencies (for ~65 million calls) between get_task_state() vs
    get_current_state().
    
    Secondly for a higher overview, an unlink microbenchmark was used,
    which pounds on a single file with open, close,unlink combos with
    increasing thread counts (up to 4x ncpus). While the workload is quite
    unrealistic, it does contend a lot on the inode mutex or now rwsem.
    
    [1] https://lkml.kernel.org/r/1483468021-8237-1-git-send-email-mark.rutland@arm.com
    
    == 1. x86-64 ==
    
    Avg runtime set_task_state():    601 msecs
    Avg runtime set_current_state(): 552 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      36089.26 (  0.00%)    38977.33 (  8.00%)
    Hmean    unlink1-processes-5      28555.01 (  0.00%)    29832.55 (  4.28%)
    Hmean    unlink1-processes-8      37323.75 (  0.00%)    44974.57 ( 20.50%)
    Hmean    unlink1-processes-12     43571.88 (  0.00%)    44283.01 (  1.63%)
    Hmean    unlink1-processes-21     34431.52 (  0.00%)    38284.45 ( 11.19%)
    Hmean    unlink1-processes-30     34813.26 (  0.00%)    37975.17 (  9.08%)
    Hmean    unlink1-processes-48     37048.90 (  0.00%)    39862.78 (  7.59%)
    Hmean    unlink1-processes-79     35630.01 (  0.00%)    36855.30 (  3.44%)
    Hmean    unlink1-processes-110    36115.85 (  0.00%)    39843.91 ( 10.32%)
    Hmean    unlink1-processes-141    32546.96 (  0.00%)    35418.52 (  8.82%)
    Hmean    unlink1-processes-172    34674.79 (  0.00%)    36899.21 (  6.42%)
    Hmean    unlink1-processes-203    37303.11 (  0.00%)    36393.04 ( -2.44%)
    Hmean    unlink1-processes-224    35712.13 (  0.00%)    36685.96 (  2.73%)
    
    == 2. ppc64le ==
    
    Avg runtime set_task_state():  938 msecs
    Avg runtime set_current_state: 940 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      19269.19 (  0.00%)    30704.50 ( 59.35%)
    Hmean    unlink1-processes-5      20106.15 (  0.00%)    21804.15 (  8.45%)
    Hmean    unlink1-processes-8      17496.97 (  0.00%)    17243.28 ( -1.45%)
    Hmean    unlink1-processes-12     14224.15 (  0.00%)    17240.21 ( 21.20%)
    Hmean    unlink1-processes-21     14155.66 (  0.00%)    15681.23 ( 10.78%)
    Hmean    unlink1-processes-30     14450.70 (  0.00%)    15995.83 ( 10.69%)
    Hmean    unlink1-processes-48     16945.57 (  0.00%)    16370.42 ( -3.39%)
    Hmean    unlink1-processes-79     15788.39 (  0.00%)    14639.27 ( -7.28%)
    Hmean    unlink1-processes-110    14268.48 (  0.00%)    14377.40 (  0.76%)
    Hmean    unlink1-processes-141    14023.65 (  0.00%)    16271.69 ( 16.03%)
    Hmean    unlink1-processes-172    13417.62 (  0.00%)    16067.55 ( 19.75%)
    Hmean    unlink1-processes-203    15293.08 (  0.00%)    15440.40 (  0.96%)
    Hmean    unlink1-processes-234    13719.32 (  0.00%)    16190.74 ( 18.01%)
    Hmean    unlink1-processes-265    16400.97 (  0.00%)    16115.22 ( -1.74%)
    Hmean    unlink1-processes-296    14388.60 (  0.00%)    16216.13 ( 12.70%)
    Hmean    unlink1-processes-320    15771.85 (  0.00%)    15905.96 (  0.85%)
    
    x86-64 (known to be fast for get_current()/this_cpu_read_stable() caching)
    and ppc64 (with paca) show similar improvements in the unlink microbenches.
    The small delta for ppc64 (2ms), does not represent the gains on the unlink
    runs. In the case of x86, there was a decent amount of variation in the
    latency runs, but always within a 20 to 50ms increase), ppc was more constant.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: mark.rutland@arm.com
    Link: http://lkml.kernel.org/r/1483479794-14013-5-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index a6dde7cab458..758d90cc2733 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -120,7 +120,7 @@ static int __check_holder(struct block_lock *lock)
 static void __wait(struct waiter *w)
 {
 	for (;;) {
-		set_task_state(current, TASK_UNINTERRUPTIBLE);
+		set_current_state(TASK_UNINTERRUPTIBLE);
 
 		if (!w->task)
 			break;
@@ -128,7 +128,7 @@ static void __wait(struct waiter *w)
 		schedule();
 	}
 
-	set_task_state(current, TASK_RUNNING);
+	set_current_state(TASK_RUNNING);
 }
 
 static void __wake_waiter(struct waiter *w)

commit 2e8ed71102ff8fe3919dd3a2d73ac4da72686efc
Author: Joe Thornber <ejt@redhat.com>
Date:   Thu Nov 19 13:50:32 2015 +0000

    dm block manager: make block locking optional
    
    The block manager's locking is useful for catching cycles that may
    result from certain btree metadata corruption.  But in general it serves
    as a developer tool to catch bugs in code.  Unless you're finding that
    DM thin provisioning is hanging due to infinite loops within the block
    manager's access to btree nodes you can safely disable this feature.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de> # do/while(0) macro fix
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 1e33dd51c21f..a6dde7cab458 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -18,6 +18,8 @@
 
 /*----------------------------------------------------------------*/
 
+#ifdef CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING
+
 /*
  * This is a read/write semaphore with a couple of differences.
  *
@@ -302,6 +304,18 @@ static void report_recursive_bug(dm_block_t b, int r)
 		      (unsigned long long) b);
 }
 
+#else  /* !CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING */
+
+#define bl_init(x) do { } while (0)
+#define bl_down_read(x) 0
+#define bl_down_read_nonblock(x) 0
+#define bl_up_read(x) do { } while (0)
+#define bl_down_write(x) 0
+#define bl_up_write(x) do { } while (0)
+#define report_recursive_bug(x, y) do { } while (0)
+
+#endif /* CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING */
+
 /*----------------------------------------------------------------*/
 
 /*
@@ -330,8 +344,11 @@ EXPORT_SYMBOL_GPL(dm_block_data);
 
 struct buffer_aux {
 	struct dm_block_validator *validator;
-	struct block_lock lock;
 	int write_locked;
+
+#ifdef CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING
+	struct block_lock lock;
+#endif
 };
 
 static void dm_block_manager_alloc_callback(struct dm_buffer *buf)

commit 313c9b97361fca3a606fc74de86bee6dc7988bca
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Mon Nov 23 19:12:05 2015 -0500

    dm block manager: cleanup code that prints stacktrace
    
    There is no need to record stack trace and immediately print it.  Just
    use dump_stack() to print the current stack.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index f2393ba838eb..1e33dd51c21f 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -97,10 +97,6 @@ static void __del_holder(struct block_lock *lock, struct task_struct *task)
 static int __check_holder(struct block_lock *lock)
 {
 	unsigned i;
-#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
-	static struct stack_trace t;
-	static stack_entries entries;
-#endif
 
 	for (i = 0; i < MAX_HOLDERS; i++) {
 		if (lock->holders[i] == current) {
@@ -110,12 +106,7 @@ static int __check_holder(struct block_lock *lock)
 			print_stack_trace(lock->traces + i, 4);
 
 			DMERR("subsequent acquisition attempted here:");
-			t.nr_entries = 0;
-			t.max_entries = MAX_STACK;
-			t.entries = entries;
-			t.skip = 3;
-			save_stack_trace(&t);
-			print_stack_trace(&t, 4);
+			dump_stack();
 #endif
 			return -EINVAL;
 		}

commit 4c7da06f5a780bbf44ebd7547789e48536d0a823
Author: Mikulas Patocka <mpatocka@redhat.com>
Date:   Thu Oct 22 16:46:59 2015 -0400

    dm persistent data: eliminate unnecessary return values
    
    dm_bm_unlock and dm_tm_unlock return an integer value but the returned
    value is always 0.  The calling code sometimes checks the return value
    and sometimes doesn't.
    
    Eliminate these unnecessary return values and also the checks for them.
    
    Signed-off-by: Mikulas Patocka <mpatocka@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 88dbe7b97c2c..f2393ba838eb 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -578,7 +578,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 }
 EXPORT_SYMBOL_GPL(dm_bm_write_lock_zero);
 
-int dm_bm_unlock(struct dm_block *b)
+void dm_bm_unlock(struct dm_block *b)
 {
 	struct buffer_aux *aux;
 	aux = dm_bufio_get_aux_data(to_buffer(b));
@@ -590,8 +590,6 @@ int dm_bm_unlock(struct dm_block *b)
 		bl_up_read(&aux->lock);
 
 	dm_bufio_release(to_buffer(b));
-
-	return 0;
 }
 EXPORT_SYMBOL_GPL(dm_bm_unlock);
 

commit fc0a446152877d3a4e40166ddb19b6e0cb5f6567
Author: viresh kumar <viresh.kumar@linaro.org>
Date:   Mon Aug 10 11:42:26 2015 +0530

    dm: remove unlikely() before IS_ERR()
    
    IS_ERR() already contains an 'unlikely' compiler flag so there is no
    need to do that again from IS_ERR() callers.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 4d6c9b689eaa..88dbe7b97c2c 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -454,7 +454,7 @@ int dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,
 	int r;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -490,7 +490,7 @@ int dm_bm_write_lock(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	aux = dm_bufio_get_aux_data(to_buffer(*result));
@@ -523,7 +523,7 @@ int dm_bm_read_try_lock(struct dm_block_manager *bm,
 	int r;
 
 	p = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 	if (unlikely(!p))
 		return -EWOULDBLOCK;
@@ -559,7 +559,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 		return -EPERM;
 
 	p = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);
-	if (unlikely(IS_ERR(p)))
+	if (IS_ERR(p))
 		return PTR_ERR(p);
 
 	memset(p, 0, dm_bm_block_size(bm));

commit 49f154c7327139c275c9392b5fd69cad82f106bc
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Thu Apr 23 15:06:27 2015 -0400

    dm thin metadata: remove in-core 'read_only' flag
    
    Leverage the block manager's read_only flag instead of duplicating it;
    access with new dm_bm_is_read_only() method.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 087411c95ffc..4d6c9b689eaa 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -609,6 +609,12 @@ void dm_bm_prefetch(struct dm_block_manager *bm, dm_block_t b)
 	dm_bufio_prefetch(bm->bufio, b, 1);
 }
 
+bool dm_bm_is_read_only(struct dm_block_manager *bm)
+{
+	return bm->read_only;
+}
+EXPORT_SYMBOL_GPL(dm_bm_is_read_only);
+
 void dm_bm_set_read_only(struct dm_block_manager *bm)
 {
 	bm->read_only = true;

commit a9d45396f5956d0b615c7ae3b936afd888351a47
Author: Joe Thornber <ejt@redhat.com>
Date:   Thu Mar 27 14:13:20 2014 +0000

    dm transaction manager: fix corruption due to non-atomic transaction commit
    
    The persistent-data library used by dm-thin, dm-cache, etc is
    transactional.  If anything goes wrong, such as an io error when writing
    new metadata or a power failure, then we roll back to the last
    transaction.
    
    Atomicity when committing a transaction is achieved by:
    
    a) Never overwriting data from the previous transaction.
    b) Writing the superblock last, after all other metadata has hit the
       disk.
    
    This commit and the following commit ("dm: take care to copy the space
    map roots before locking the superblock") fix a bug associated with (b).
    When committing it was possible for the superblock to still be written
    in spite of an io error occurring during the preceeding metadata flush.
    With these commits we're careful not to take the write lock out on the
    superblock until after the metadata flush has completed.
    
    Change the transaction manager's semantics for dm_tm_commit() to assume
    all data has been flushed _before_ the single superblock that is passed
    in.
    
    As a prerequisite, split the block manager's block unlocking and
    flushing by simplifying dm_bm_flush_and_unlock() to dm_bm_flush().  Now
    the unlocking must be done separately.
    
    This issue was discovered by forcing io errors at the crucial time
    using dm-flakey.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@vger.kernel.org

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 455f79279a16..087411c95ffc 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -595,25 +595,14 @@ int dm_bm_unlock(struct dm_block *b)
 }
 EXPORT_SYMBOL_GPL(dm_bm_unlock);
 
-int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
-			   struct dm_block *superblock)
+int dm_bm_flush(struct dm_block_manager *bm)
 {
-	int r;
-
 	if (bm->read_only)
 		return -EPERM;
 
-	r = dm_bufio_write_dirty_buffers(bm->bufio);
-	if (unlikely(r)) {
-		dm_bm_unlock(superblock);
-		return r;
-	}
-
-	dm_bm_unlock(superblock);
-
 	return dm_bufio_write_dirty_buffers(bm->bufio);
 }
-EXPORT_SYMBOL_GPL(dm_bm_flush_and_unlock);
+EXPORT_SYMBOL_GPL(dm_bm_flush);
 
 void dm_bm_prefetch(struct dm_block_manager *bm, dm_block_t b)
 {

commit 10343180f5c4023043e82d46e71048e68f975f50
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Dec 13 08:24:44 2013 -0500

    dm persistent data: cleanup dm-thin specific references in text
    
    DM's persistent-data library is now used my multiple targets so
    exclusive references to "pool" or "thin provisioning" need to be
    cleaned up.  Adjust Kconfig's DM_DEBUG_BLOCK_STACK_TRACING text
    and remove "pool" from a block manager error message.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Acked-by: Joe Thornber <ejt@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 064a3c271baa..455f79279a16 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -104,7 +104,7 @@ static int __check_holder(struct block_lock *lock)
 
 	for (i = 0; i < MAX_HOLDERS; i++) {
 		if (lock->holders[i] == current) {
-			DMERR("recursive lock detected in pool metadata");
+			DMERR("recursive lock detected in metadata");
 #ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
 			DMERR("previously held here:");
 			print_stack_trace(lock->traces + i, 4);

commit 9b7aaa64f96f7ca280d75326fca42f42017b89ef
Author: Joe Thornber <ejt@redhat.com>
Date:   Wed Dec 4 16:58:19 2013 -0500

    dm thin: allow pool in read-only mode to transition to read-write mode
    
    A thin-pool may be in read-only mode because the pool's data or metadata
    space was exhausted.  To allow for recovery, by adding more space to the
    pool, we must allow a pool to transition from PM_READ_ONLY to PM_WRITE
    mode.  Otherwise, running out of space will render the pool permanently
    read-only.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Cc: stable@vger.kernel.org

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index a7e8bf296388..064a3c271baa 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -626,6 +626,12 @@ void dm_bm_set_read_only(struct dm_block_manager *bm)
 }
 EXPORT_SYMBOL_GPL(dm_bm_set_read_only);
 
+void dm_bm_set_read_write(struct dm_block_manager *bm)
+{
+	bm->read_only = false;
+}
+EXPORT_SYMBOL_GPL(dm_bm_set_read_write);
+
 u32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)
 {
 	return crc32c(~(u32) 0, data, len) ^ init_xor;

commit 04f17c802f447e76d13ade5bb78fbbf34baef0f8
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Aug 9 12:59:30 2013 +0100

    dm btree: prefetch child nodes when walking tree for a dm_btree_del
    
    dm-btree now takes advantage of dm-bufio's ability to prefetch data via
    dm_bm_prefetch().  Prior to this change many btree node visits were
    causing a synchronous read.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 81b513890e2b..a7e8bf296388 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -615,6 +615,11 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 }
 EXPORT_SYMBOL_GPL(dm_bm_flush_and_unlock);
 
+void dm_bm_prefetch(struct dm_block_manager *bm, dm_block_t b)
+{
+	dm_bufio_prefetch(bm->bufio, b, 1);
+}
+
 void dm_bm_set_read_only(struct dm_block_manager *bm)
 {
 	bm->read_only = true;

commit c6b4fcbad044e6fffcc75bba160e720eb8d67d17
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Mar 1 22:45:51 2013 +0000

    dm: add cache target
    
    Add a target that allows a fast device such as an SSD to be used as a
    cache for a slower device such as a disk.
    
    A plug-in architecture was chosen so that the decisions about which data
    to migrate and when are delegated to interchangeable tunable policy
    modules.  The first general purpose module we have developed, called
    "mq" (multiqueue), follows in the next patch.  Other modules are
    under development.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Heinz Mauelshagen <mauelshagen@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 28c3ed072a79..81b513890e2b 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -613,6 +613,7 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 
 	return dm_bufio_write_dirty_buffers(bm->bufio);
 }
+EXPORT_SYMBOL_GPL(dm_bm_flush_and_unlock);
 
 void dm_bm_set_read_only(struct dm_block_manager *bm)
 {

commit b49249d10324d0fd6fb29725c2807dfd80d0edbc
Merge: 10532b560bac 45e621d45e24
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 21 17:08:06 2012 -0800

    Merge tag 'dm-3.8-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/agk/linux-dm
    
    Pull dm update from Alasdair G Kergon:
     "Miscellaneous device-mapper fixes, cleanups and performance
      improvements.
    
      Of particular note:
       - Disable broken WRITE SAME support in all targets except linear and
         striped.  Use it when kcopyd is zeroing blocks.
       - Remove several mempools from targets by moving the data into the
         bio's new front_pad area(which dm calls 'per_bio_data').
       - Fix a race in thin provisioning if discards are misused.
       - Prevent userspace from interfering with the ioctl parameters and
         use kmalloc for the data buffer if it's small instead of vmalloc.
       - Throttle some annoying error messages when I/O fails."
    
    * tag 'dm-3.8-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/agk/linux-dm: (36 commits)
      dm stripe: add WRITE SAME support
      dm: remove map_info
      dm snapshot: do not use map_context
      dm thin: dont use map_context
      dm raid1: dont use map_context
      dm flakey: dont use map_context
      dm raid1: rename read_record to bio_record
      dm: move target request nr to dm_target_io
      dm snapshot: use per_bio_data
      dm verity: use per_bio_data
      dm raid1: use per_bio_data
      dm: introduce per_bio_data
      dm kcopyd: add WRITE SAME support to dm_kcopyd_zero
      dm linear: add WRITE SAME support
      dm: add WRITE SAME support
      dm: prepare to support WRITE SAME
      dm ioctl: use kmalloc if possible
      dm ioctl: remove PF_MEMALLOC
      dm persistent data: improve improve space map block alloc failure message
      dm thin: use DMERR_LIMIT for errors
      ...

commit 89ddeb8cb1383bfdfaaeb458f1c4a91e890cc60b
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Dec 21 20:23:34 2012 +0000

    dm persistent data: use DMERR_LIMIT for errors
    
    Nearly all of persistent-data is in the IO path so throttle error
    messages with DMERR_LIMIT to limit the amount logged when
    something has gone wrong.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 47b683e67a7c..ec4cb3c58a0c 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -429,17 +429,16 @@ static int dm_bm_validate_buffer(struct dm_block_manager *bm,
 			return 0;
 		r = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(bm->bufio));
 		if (unlikely(r)) {
-			DMERR("%s validator check failed for block %llu", v->name,
-			      (unsigned long long) dm_bufio_get_block_number(buf));
+			DMERR_LIMIT("%s validator check failed for block %llu", v->name,
+				    (unsigned long long) dm_bufio_get_block_number(buf));
 			return r;
 		}
 		aux->validator = v;
 	} else {
 		if (unlikely(aux->validator != v)) {
-			DMERR("validator mismatch (old=%s vs new=%s) for block %llu",
-				aux->validator->name, v ? v->name : "NULL",
-				(unsigned long long)
-					dm_bufio_get_block_number(buf));
+			DMERR_LIMIT("validator mismatch (old=%s vs new=%s) for block %llu",
+				    aux->validator->name, v ? v->name : "NULL",
+				    (unsigned long long) dm_bufio_get_block_number(buf));
 			return -EINVAL;
 		}
 	}

commit a5bd968aeb87af4a3a96ecdd78912832997e8a9d
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Fri Dec 21 20:23:34 2012 +0000

    dm block manager: reinstate message when validator fails
    
    Reinstate a useful error message when the block manager buffer validator fails.
    This was mistakenly eliminated when the block manager was converted to use
    dm-bufio.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 5ba277768d99..47b683e67a7c 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -428,8 +428,11 @@ static int dm_bm_validate_buffer(struct dm_block_manager *bm,
 		if (!v)
 			return 0;
 		r = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(bm->bufio));
-		if (unlikely(r))
+		if (unlikely(r)) {
+			DMERR("%s validator check failed for block %llu", v->name,
+			      (unsigned long long) dm_bufio_get_block_number(buf));
 			return r;
+		}
 		aux->validator = v;
 	} else {
 		if (unlikely(aux->validator != v)) {

commit 83f0d77a7f336130a0ae1290d6bcfa606abaad66
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Tue Oct 30 00:18:08 2012 +0900

    md: Fix typo in drivers/md
    
    Correct spelling typo in drivers/md.
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 5ba277768d99..a3ae09124a67 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -25,7 +25,7 @@
  * may be held at once.  This is just an implementation detail.
  *
  * ii) Recursive locking attempts are detected and return EINVAL.  A stack
- * trace is also emitted for the previous lock aquisition.
+ * trace is also emitted for the previous lock acquisition.
  *
  * iii) Priority is given to write locks.
  */
@@ -109,7 +109,7 @@ static int __check_holder(struct block_lock *lock)
 			DMERR("previously held here:");
 			print_stack_trace(lock->traces + i, 4);
 
-			DMERR("subsequent aquisition attempted here:");
+			DMERR("subsequent acquisition attempted here:");
 			t.nr_entries = 0;
 			t.max_entries = MAX_STACK;
 			t.entries = entries;

commit 310975573ba40ab526b0d76d28225d8d0999d72b
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:15 2012 +0100

    dm persistent data: introduce dm_bm_set_read_only
    
    Introduce dm_bm_set_read_only to switch the block manager into a
    read-only mode.  To be used when dm-thin degrades due to io errors on
    the metadata device.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index ad1712e802f6..5ba277768d99 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -364,6 +364,7 @@ static void dm_block_manager_write_callback(struct dm_buffer *buf)
  *--------------------------------------------------------------*/
 struct dm_block_manager {
 	struct dm_bufio_client *bufio;
+	bool read_only:1;
 };
 
 struct dm_block_manager *dm_block_manager_create(struct block_device *bdev,
@@ -390,6 +391,8 @@ struct dm_block_manager *dm_block_manager_create(struct block_device *bdev,
 		goto bad;
 	}
 
+	bm->read_only = false;
+
 	return bm;
 
 bad:
@@ -481,6 +484,9 @@ int dm_bm_write_lock(struct dm_block_manager *bm,
 	void *p;
 	int r;
 
+	if (bm->read_only)
+		return -EPERM;
+
 	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
@@ -547,6 +553,9 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 	struct buffer_aux *aux;
 	void *p;
 
+	if (bm->read_only)
+		return -EPERM;
+
 	p = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
@@ -589,6 +598,9 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 {
 	int r;
 
+	if (bm->read_only)
+		return -EPERM;
+
 	r = dm_bufio_write_dirty_buffers(bm->bufio);
 	if (unlikely(r)) {
 		dm_bm_unlock(superblock);
@@ -600,6 +612,12 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 	return dm_bufio_write_dirty_buffers(bm->bufio);
 }
 
+void dm_bm_set_read_only(struct dm_block_manager *bm)
+{
+	bm->read_only = true;
+}
+EXPORT_SYMBOL_GPL(dm_bm_set_read_only);
+
 u32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)
 {
 	return crc32c(~(u32) 0, data, len) ^ init_xor;

commit 3c9ad9bd87b03032999ddbeb44bdf7938f7dbd57
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:09 2012 +0100

    dm persistent data: stop using dm_bm_unlock_move when shadowing blocks in tm
    
    Stop using dm_bm_unlock_move when shadowing blocks in the transaction
    manager as an optimisation and remove the function as it is then no
    longer used.
    
    Some code, such as the space maps, keeps using on-disk data structures
    from the previous transaction.  It can do this because blocks won't
    be reallocated until the subsequent transaction.  Using
    dm_bm_unlock_move to copy blocks sounds like a win, but it forces a
    synchronous read should the old block be accessed.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 4b5c504f47af..ad1712e802f6 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -584,22 +584,6 @@ int dm_bm_unlock(struct dm_block *b)
 }
 EXPORT_SYMBOL_GPL(dm_bm_unlock);
 
-int dm_bm_unlock_move(struct dm_block *b, dm_block_t n)
-{
-	struct buffer_aux *aux;
-
-	aux = dm_bufio_get_aux_data(to_buffer(b));
-
-	if (aux->write_locked) {
-		dm_bufio_mark_buffer_dirty(to_buffer(b));
-		bl_up_write(&aux->lock);
-	} else
-		bl_up_read(&aux->lock);
-
-	dm_bufio_release_move(to_buffer(b), n);
-	return 0;
-}
-
 int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 			   struct dm_block *superblock)
 {

commit 384ef0e62e409e52c80adef5b1ff83075377c19e
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:09 2012 +0100

    dm persistent data: tidy transaction manager creation fns
    
    Tidy the transaction manager creation functions.
    
    They no longer lock the superblock.  Superblock locking is pulled out to
    the caller.
    
    Also export dm_bm_write_lock_zero.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index c70ad6e303d3..4b5c504f47af 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -565,6 +565,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(dm_bm_write_lock_zero);
 
 int dm_bm_unlock(struct dm_block *b)
 {

commit 51a0f659c03ccc8ec407c7a9f9701d1b4618d515
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:08 2012 +0100

    dm persistent data: create new dm_block_manager struct
    
    This patch introduces a separate struct for the block_manager.
    It also uses IS_ERR to check the return value of dm_bufio_client_create
    instead of testing incorrectly for NULL.
    
    Prior to this patch a struct dm_block_manager was really an alias for
    a struct dm_bufio_client.  We want to add some functionality to the
    block manager that will require extra fields, so this one to one
    mapping is no longer valid.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 8ec46dd511f9..c70ad6e303d3 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -325,11 +325,6 @@ static struct dm_buffer *to_buffer(struct dm_block *b)
 	return (struct dm_buffer *) b;
 }
 
-static struct dm_bufio_client *to_bufio(struct dm_block_manager *bm)
-{
-	return (struct dm_bufio_client *) bm;
-}
-
 dm_block_t dm_block_location(struct dm_block *b)
 {
 	return dm_bufio_get_block_number(to_buffer(b));
@@ -367,34 +362,57 @@ static void dm_block_manager_write_callback(struct dm_buffer *buf)
 /*----------------------------------------------------------------
  * Public interface
  *--------------------------------------------------------------*/
+struct dm_block_manager {
+	struct dm_bufio_client *bufio;
+};
+
 struct dm_block_manager *dm_block_manager_create(struct block_device *bdev,
 						 unsigned block_size,
 						 unsigned cache_size,
 						 unsigned max_held_per_thread)
 {
-	return (struct dm_block_manager *)
-		dm_bufio_client_create(bdev, block_size, max_held_per_thread,
-				       sizeof(struct buffer_aux),
-				       dm_block_manager_alloc_callback,
-				       dm_block_manager_write_callback);
+	int r;
+	struct dm_block_manager *bm;
+
+	bm = kmalloc(sizeof(*bm), GFP_KERNEL);
+	if (!bm) {
+		r = -ENOMEM;
+		goto bad;
+	}
+
+	bm->bufio = dm_bufio_client_create(bdev, block_size, max_held_per_thread,
+					   sizeof(struct buffer_aux),
+					   dm_block_manager_alloc_callback,
+					   dm_block_manager_write_callback);
+	if (IS_ERR(bm->bufio)) {
+		r = PTR_ERR(bm->bufio);
+		kfree(bm);
+		goto bad;
+	}
+
+	return bm;
+
+bad:
+	return ERR_PTR(r);
 }
 EXPORT_SYMBOL_GPL(dm_block_manager_create);
 
 void dm_block_manager_destroy(struct dm_block_manager *bm)
 {
-	return dm_bufio_client_destroy(to_bufio(bm));
+	dm_bufio_client_destroy(bm->bufio);
+	kfree(bm);
 }
 EXPORT_SYMBOL_GPL(dm_block_manager_destroy);
 
 unsigned dm_bm_block_size(struct dm_block_manager *bm)
 {
-	return dm_bufio_get_block_size(to_bufio(bm));
+	return dm_bufio_get_block_size(bm->bufio);
 }
 EXPORT_SYMBOL_GPL(dm_bm_block_size);
 
 dm_block_t dm_bm_nr_blocks(struct dm_block_manager *bm)
 {
-	return dm_bufio_get_device_size(to_bufio(bm));
+	return dm_bufio_get_device_size(bm->bufio);
 }
 
 static int dm_bm_validate_buffer(struct dm_block_manager *bm,
@@ -406,7 +424,7 @@ static int dm_bm_validate_buffer(struct dm_block_manager *bm,
 		int r;
 		if (!v)
 			return 0;
-		r = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(to_bufio(bm)));
+		r = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(bm->bufio));
 		if (unlikely(r))
 			return r;
 		aux->validator = v;
@@ -430,7 +448,7 @@ int dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,
 	void *p;
 	int r;
 
-	p = dm_bufio_read(to_bufio(bm), b, (struct dm_buffer **) result);
+	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
@@ -463,7 +481,7 @@ int dm_bm_write_lock(struct dm_block_manager *bm,
 	void *p;
 	int r;
 
-	p = dm_bufio_read(to_bufio(bm), b, (struct dm_buffer **) result);
+	p = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
@@ -496,7 +514,7 @@ int dm_bm_read_try_lock(struct dm_block_manager *bm,
 	void *p;
 	int r;
 
-	p = dm_bufio_get(to_bufio(bm), b, (struct dm_buffer **) result);
+	p = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 	if (unlikely(!p))
@@ -529,7 +547,7 @@ int dm_bm_write_lock_zero(struct dm_block_manager *bm,
 	struct buffer_aux *aux;
 	void *p;
 
-	p = dm_bufio_new(to_bufio(bm), b, (struct dm_buffer **) result);
+	p = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);
 	if (unlikely(IS_ERR(p)))
 		return PTR_ERR(p);
 
@@ -586,7 +604,7 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 {
 	int r;
 
-	r = dm_bufio_write_dirty_buffers(to_bufio(bm));
+	r = dm_bufio_write_dirty_buffers(bm->bufio);
 	if (unlikely(r)) {
 		dm_bm_unlock(superblock);
 		return r;
@@ -594,7 +612,7 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 
 	dm_bm_unlock(superblock);
 
-	return dm_bufio_write_dirty_buffers(to_bufio(bm));
+	return dm_bufio_write_dirty_buffers(bm->bufio);
 }
 
 u32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)

commit 8d44c98aac540cdf3cb5385bc6ef8d56930c7d70
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:06 2012 +0100

    dm persistent data: always unlock superblock in dm_bm_flush_and_unlock
    
    Unlock the superblock even if initial dm_bufio_write_dirty_buffers fails.
    
    Also, remove redundant flush calls.  dm_bm_flush_and_unlock's calls to
    dm_bufio_write_dirty_buffers already result in dm_bufio_issue_flush
    being called.
    
    This avoids warnings about unflushed dirty buffers from bufio.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
index 0317ecdc6e53..8ec46dd511f9 100644
--- a/drivers/md/persistent-data/dm-block-manager.c
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -587,22 +587,14 @@ int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
 	int r;
 
 	r = dm_bufio_write_dirty_buffers(to_bufio(bm));
-	if (unlikely(r))
-		return r;
-	r = dm_bufio_issue_flush(to_bufio(bm));
-	if (unlikely(r))
+	if (unlikely(r)) {
+		dm_bm_unlock(superblock);
 		return r;
+	}
 
 	dm_bm_unlock(superblock);
 
-	r = dm_bufio_write_dirty_buffers(to_bufio(bm));
-	if (unlikely(r))
-		return r;
-	r = dm_bufio_issue_flush(to_bufio(bm));
-	if (unlikely(r))
-		return r;
-
-	return 0;
+	return dm_bufio_write_dirty_buffers(to_bufio(bm));
 }
 
 u32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)

commit 3241b1d3e0aaafbfcd320f4d71ade629728cc4f4
Author: Joe Thornber <thornber@redhat.com>
Date:   Mon Oct 31 20:19:11 2011 +0000

    dm: add persistent data library
    
    The persistent-data library offers a re-usable framework for the storage
    and management of on-disk metadata in device-mapper targets.
    
    It's used by the thin-provisioning target in the next patch and in an
    upcoming hierarchical storage target.
    
    For further information, please read
    Documentation/device-mapper/persistent-data.txt
    
    Signed-off-by: Joe Thornber <thornber@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

diff --git a/drivers/md/persistent-data/dm-block-manager.c b/drivers/md/persistent-data/dm-block-manager.c
new file mode 100644
index 000000000000..0317ecdc6e53
--- /dev/null
+++ b/drivers/md/persistent-data/dm-block-manager.c
@@ -0,0 +1,620 @@
+/*
+ * Copyright (C) 2011 Red Hat, Inc.
+ *
+ * This file is released under the GPL.
+ */
+#include "dm-block-manager.h"
+#include "dm-persistent-data-internal.h"
+#include "../dm-bufio.h"
+
+#include <linux/crc32c.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/rwsem.h>
+#include <linux/device-mapper.h>
+#include <linux/stacktrace.h>
+
+#define DM_MSG_PREFIX "block manager"
+
+/*----------------------------------------------------------------*/
+
+/*
+ * This is a read/write semaphore with a couple of differences.
+ *
+ * i) There is a restriction on the number of concurrent read locks that
+ * may be held at once.  This is just an implementation detail.
+ *
+ * ii) Recursive locking attempts are detected and return EINVAL.  A stack
+ * trace is also emitted for the previous lock aquisition.
+ *
+ * iii) Priority is given to write locks.
+ */
+#define MAX_HOLDERS 4
+#define MAX_STACK 10
+
+typedef unsigned long stack_entries[MAX_STACK];
+
+struct block_lock {
+	spinlock_t lock;
+	__s32 count;
+	struct list_head waiters;
+	struct task_struct *holders[MAX_HOLDERS];
+
+#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
+	struct stack_trace traces[MAX_HOLDERS];
+	stack_entries entries[MAX_HOLDERS];
+#endif
+};
+
+struct waiter {
+	struct list_head list;
+	struct task_struct *task;
+	int wants_write;
+};
+
+static unsigned __find_holder(struct block_lock *lock,
+			      struct task_struct *task)
+{
+	unsigned i;
+
+	for (i = 0; i < MAX_HOLDERS; i++)
+		if (lock->holders[i] == task)
+			break;
+
+	BUG_ON(i == MAX_HOLDERS);
+	return i;
+}
+
+/* call this *after* you increment lock->count */
+static void __add_holder(struct block_lock *lock, struct task_struct *task)
+{
+	unsigned h = __find_holder(lock, NULL);
+#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
+	struct stack_trace *t;
+#endif
+
+	get_task_struct(task);
+	lock->holders[h] = task;
+
+#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
+	t = lock->traces + h;
+	t->nr_entries = 0;
+	t->max_entries = MAX_STACK;
+	t->entries = lock->entries[h];
+	t->skip = 2;
+	save_stack_trace(t);
+#endif
+}
+
+/* call this *before* you decrement lock->count */
+static void __del_holder(struct block_lock *lock, struct task_struct *task)
+{
+	unsigned h = __find_holder(lock, task);
+	lock->holders[h] = NULL;
+	put_task_struct(task);
+}
+
+static int __check_holder(struct block_lock *lock)
+{
+	unsigned i;
+#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
+	static struct stack_trace t;
+	static stack_entries entries;
+#endif
+
+	for (i = 0; i < MAX_HOLDERS; i++) {
+		if (lock->holders[i] == current) {
+			DMERR("recursive lock detected in pool metadata");
+#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING
+			DMERR("previously held here:");
+			print_stack_trace(lock->traces + i, 4);
+
+			DMERR("subsequent aquisition attempted here:");
+			t.nr_entries = 0;
+			t.max_entries = MAX_STACK;
+			t.entries = entries;
+			t.skip = 3;
+			save_stack_trace(&t);
+			print_stack_trace(&t, 4);
+#endif
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static void __wait(struct waiter *w)
+{
+	for (;;) {
+		set_task_state(current, TASK_UNINTERRUPTIBLE);
+
+		if (!w->task)
+			break;
+
+		schedule();
+	}
+
+	set_task_state(current, TASK_RUNNING);
+}
+
+static void __wake_waiter(struct waiter *w)
+{
+	struct task_struct *task;
+
+	list_del(&w->list);
+	task = w->task;
+	smp_mb();
+	w->task = NULL;
+	wake_up_process(task);
+}
+
+/*
+ * We either wake a few readers or a single writer.
+ */
+static void __wake_many(struct block_lock *lock)
+{
+	struct waiter *w, *tmp;
+
+	BUG_ON(lock->count < 0);
+	list_for_each_entry_safe(w, tmp, &lock->waiters, list) {
+		if (lock->count >= MAX_HOLDERS)
+			return;
+
+		if (w->wants_write) {
+			if (lock->count > 0)
+				return; /* still read locked */
+
+			lock->count = -1;
+			__add_holder(lock, w->task);
+			__wake_waiter(w);
+			return;
+		}
+
+		lock->count++;
+		__add_holder(lock, w->task);
+		__wake_waiter(w);
+	}
+}
+
+static void bl_init(struct block_lock *lock)
+{
+	int i;
+
+	spin_lock_init(&lock->lock);
+	lock->count = 0;
+	INIT_LIST_HEAD(&lock->waiters);
+	for (i = 0; i < MAX_HOLDERS; i++)
+		lock->holders[i] = NULL;
+}
+
+static int __available_for_read(struct block_lock *lock)
+{
+	return lock->count >= 0 &&
+		lock->count < MAX_HOLDERS &&
+		list_empty(&lock->waiters);
+}
+
+static int bl_down_read(struct block_lock *lock)
+{
+	int r;
+	struct waiter w;
+
+	spin_lock(&lock->lock);
+	r = __check_holder(lock);
+	if (r) {
+		spin_unlock(&lock->lock);
+		return r;
+	}
+
+	if (__available_for_read(lock)) {
+		lock->count++;
+		__add_holder(lock, current);
+		spin_unlock(&lock->lock);
+		return 0;
+	}
+
+	get_task_struct(current);
+
+	w.task = current;
+	w.wants_write = 0;
+	list_add_tail(&w.list, &lock->waiters);
+	spin_unlock(&lock->lock);
+
+	__wait(&w);
+	put_task_struct(current);
+	return 0;
+}
+
+static int bl_down_read_nonblock(struct block_lock *lock)
+{
+	int r;
+
+	spin_lock(&lock->lock);
+	r = __check_holder(lock);
+	if (r)
+		goto out;
+
+	if (__available_for_read(lock)) {
+		lock->count++;
+		__add_holder(lock, current);
+		r = 0;
+	} else
+		r = -EWOULDBLOCK;
+
+out:
+	spin_unlock(&lock->lock);
+	return r;
+}
+
+static void bl_up_read(struct block_lock *lock)
+{
+	spin_lock(&lock->lock);
+	BUG_ON(lock->count <= 0);
+	__del_holder(lock, current);
+	--lock->count;
+	if (!list_empty(&lock->waiters))
+		__wake_many(lock);
+	spin_unlock(&lock->lock);
+}
+
+static int bl_down_write(struct block_lock *lock)
+{
+	int r;
+	struct waiter w;
+
+	spin_lock(&lock->lock);
+	r = __check_holder(lock);
+	if (r) {
+		spin_unlock(&lock->lock);
+		return r;
+	}
+
+	if (lock->count == 0 && list_empty(&lock->waiters)) {
+		lock->count = -1;
+		__add_holder(lock, current);
+		spin_unlock(&lock->lock);
+		return 0;
+	}
+
+	get_task_struct(current);
+	w.task = current;
+	w.wants_write = 1;
+
+	/*
+	 * Writers given priority. We know there's only one mutator in the
+	 * system, so ignoring the ordering reversal.
+	 */
+	list_add(&w.list, &lock->waiters);
+	spin_unlock(&lock->lock);
+
+	__wait(&w);
+	put_task_struct(current);
+
+	return 0;
+}
+
+static void bl_up_write(struct block_lock *lock)
+{
+	spin_lock(&lock->lock);
+	__del_holder(lock, current);
+	lock->count = 0;
+	if (!list_empty(&lock->waiters))
+		__wake_many(lock);
+	spin_unlock(&lock->lock);
+}
+
+static void report_recursive_bug(dm_block_t b, int r)
+{
+	if (r == -EINVAL)
+		DMERR("recursive acquisition of block %llu requested.",
+		      (unsigned long long) b);
+}
+
+/*----------------------------------------------------------------*/
+
+/*
+ * Block manager is currently implemented using dm-bufio.  struct
+ * dm_block_manager and struct dm_block map directly onto a couple of
+ * structs in the bufio interface.  I want to retain the freedom to move
+ * away from bufio in the future.  So these structs are just cast within
+ * this .c file, rather than making it through to the public interface.
+ */
+static struct dm_buffer *to_buffer(struct dm_block *b)
+{
+	return (struct dm_buffer *) b;
+}
+
+static struct dm_bufio_client *to_bufio(struct dm_block_manager *bm)
+{
+	return (struct dm_bufio_client *) bm;
+}
+
+dm_block_t dm_block_location(struct dm_block *b)
+{
+	return dm_bufio_get_block_number(to_buffer(b));
+}
+EXPORT_SYMBOL_GPL(dm_block_location);
+
+void *dm_block_data(struct dm_block *b)
+{
+	return dm_bufio_get_block_data(to_buffer(b));
+}
+EXPORT_SYMBOL_GPL(dm_block_data);
+
+struct buffer_aux {
+	struct dm_block_validator *validator;
+	struct block_lock lock;
+	int write_locked;
+};
+
+static void dm_block_manager_alloc_callback(struct dm_buffer *buf)
+{
+	struct buffer_aux *aux = dm_bufio_get_aux_data(buf);
+	aux->validator = NULL;
+	bl_init(&aux->lock);
+}
+
+static void dm_block_manager_write_callback(struct dm_buffer *buf)
+{
+	struct buffer_aux *aux = dm_bufio_get_aux_data(buf);
+	if (aux->validator) {
+		aux->validator->prepare_for_write(aux->validator, (struct dm_block *) buf,
+			 dm_bufio_get_block_size(dm_bufio_get_client(buf)));
+	}
+}
+
+/*----------------------------------------------------------------
+ * Public interface
+ *--------------------------------------------------------------*/
+struct dm_block_manager *dm_block_manager_create(struct block_device *bdev,
+						 unsigned block_size,
+						 unsigned cache_size,
+						 unsigned max_held_per_thread)
+{
+	return (struct dm_block_manager *)
+		dm_bufio_client_create(bdev, block_size, max_held_per_thread,
+				       sizeof(struct buffer_aux),
+				       dm_block_manager_alloc_callback,
+				       dm_block_manager_write_callback);
+}
+EXPORT_SYMBOL_GPL(dm_block_manager_create);
+
+void dm_block_manager_destroy(struct dm_block_manager *bm)
+{
+	return dm_bufio_client_destroy(to_bufio(bm));
+}
+EXPORT_SYMBOL_GPL(dm_block_manager_destroy);
+
+unsigned dm_bm_block_size(struct dm_block_manager *bm)
+{
+	return dm_bufio_get_block_size(to_bufio(bm));
+}
+EXPORT_SYMBOL_GPL(dm_bm_block_size);
+
+dm_block_t dm_bm_nr_blocks(struct dm_block_manager *bm)
+{
+	return dm_bufio_get_device_size(to_bufio(bm));
+}
+
+static int dm_bm_validate_buffer(struct dm_block_manager *bm,
+				 struct dm_buffer *buf,
+				 struct buffer_aux *aux,
+				 struct dm_block_validator *v)
+{
+	if (unlikely(!aux->validator)) {
+		int r;
+		if (!v)
+			return 0;
+		r = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(to_bufio(bm)));
+		if (unlikely(r))
+			return r;
+		aux->validator = v;
+	} else {
+		if (unlikely(aux->validator != v)) {
+			DMERR("validator mismatch (old=%s vs new=%s) for block %llu",
+				aux->validator->name, v ? v->name : "NULL",
+				(unsigned long long)
+					dm_bufio_get_block_number(buf));
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+int dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,
+		    struct dm_block_validator *v,
+		    struct dm_block **result)
+{
+	struct buffer_aux *aux;
+	void *p;
+	int r;
+
+	p = dm_bufio_read(to_bufio(bm), b, (struct dm_buffer **) result);
+	if (unlikely(IS_ERR(p)))
+		return PTR_ERR(p);
+
+	aux = dm_bufio_get_aux_data(to_buffer(*result));
+	r = bl_down_read(&aux->lock);
+	if (unlikely(r)) {
+		dm_bufio_release(to_buffer(*result));
+		report_recursive_bug(b, r);
+		return r;
+	}
+
+	aux->write_locked = 0;
+
+	r = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);
+	if (unlikely(r)) {
+		bl_up_read(&aux->lock);
+		dm_bufio_release(to_buffer(*result));
+		return r;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dm_bm_read_lock);
+
+int dm_bm_write_lock(struct dm_block_manager *bm,
+		     dm_block_t b, struct dm_block_validator *v,
+		     struct dm_block **result)
+{
+	struct buffer_aux *aux;
+	void *p;
+	int r;
+
+	p = dm_bufio_read(to_bufio(bm), b, (struct dm_buffer **) result);
+	if (unlikely(IS_ERR(p)))
+		return PTR_ERR(p);
+
+	aux = dm_bufio_get_aux_data(to_buffer(*result));
+	r = bl_down_write(&aux->lock);
+	if (r) {
+		dm_bufio_release(to_buffer(*result));
+		report_recursive_bug(b, r);
+		return r;
+	}
+
+	aux->write_locked = 1;
+
+	r = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);
+	if (unlikely(r)) {
+		bl_up_write(&aux->lock);
+		dm_bufio_release(to_buffer(*result));
+		return r;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dm_bm_write_lock);
+
+int dm_bm_read_try_lock(struct dm_block_manager *bm,
+			dm_block_t b, struct dm_block_validator *v,
+			struct dm_block **result)
+{
+	struct buffer_aux *aux;
+	void *p;
+	int r;
+
+	p = dm_bufio_get(to_bufio(bm), b, (struct dm_buffer **) result);
+	if (unlikely(IS_ERR(p)))
+		return PTR_ERR(p);
+	if (unlikely(!p))
+		return -EWOULDBLOCK;
+
+	aux = dm_bufio_get_aux_data(to_buffer(*result));
+	r = bl_down_read_nonblock(&aux->lock);
+	if (r < 0) {
+		dm_bufio_release(to_buffer(*result));
+		report_recursive_bug(b, r);
+		return r;
+	}
+	aux->write_locked = 0;
+
+	r = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);
+	if (unlikely(r)) {
+		bl_up_read(&aux->lock);
+		dm_bufio_release(to_buffer(*result));
+		return r;
+	}
+
+	return 0;
+}
+
+int dm_bm_write_lock_zero(struct dm_block_manager *bm,
+			  dm_block_t b, struct dm_block_validator *v,
+			  struct dm_block **result)
+{
+	int r;
+	struct buffer_aux *aux;
+	void *p;
+
+	p = dm_bufio_new(to_bufio(bm), b, (struct dm_buffer **) result);
+	if (unlikely(IS_ERR(p)))
+		return PTR_ERR(p);
+
+	memset(p, 0, dm_bm_block_size(bm));
+
+	aux = dm_bufio_get_aux_data(to_buffer(*result));
+	r = bl_down_write(&aux->lock);
+	if (r) {
+		dm_bufio_release(to_buffer(*result));
+		return r;
+	}
+
+	aux->write_locked = 1;
+	aux->validator = v;
+
+	return 0;
+}
+
+int dm_bm_unlock(struct dm_block *b)
+{
+	struct buffer_aux *aux;
+	aux = dm_bufio_get_aux_data(to_buffer(b));
+
+	if (aux->write_locked) {
+		dm_bufio_mark_buffer_dirty(to_buffer(b));
+		bl_up_write(&aux->lock);
+	} else
+		bl_up_read(&aux->lock);
+
+	dm_bufio_release(to_buffer(b));
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dm_bm_unlock);
+
+int dm_bm_unlock_move(struct dm_block *b, dm_block_t n)
+{
+	struct buffer_aux *aux;
+
+	aux = dm_bufio_get_aux_data(to_buffer(b));
+
+	if (aux->write_locked) {
+		dm_bufio_mark_buffer_dirty(to_buffer(b));
+		bl_up_write(&aux->lock);
+	} else
+		bl_up_read(&aux->lock);
+
+	dm_bufio_release_move(to_buffer(b), n);
+	return 0;
+}
+
+int dm_bm_flush_and_unlock(struct dm_block_manager *bm,
+			   struct dm_block *superblock)
+{
+	int r;
+
+	r = dm_bufio_write_dirty_buffers(to_bufio(bm));
+	if (unlikely(r))
+		return r;
+	r = dm_bufio_issue_flush(to_bufio(bm));
+	if (unlikely(r))
+		return r;
+
+	dm_bm_unlock(superblock);
+
+	r = dm_bufio_write_dirty_buffers(to_bufio(bm));
+	if (unlikely(r))
+		return r;
+	r = dm_bufio_issue_flush(to_bufio(bm));
+	if (unlikely(r))
+		return r;
+
+	return 0;
+}
+
+u32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)
+{
+	return crc32c(~(u32) 0, data, len) ^ init_xor;
+}
+EXPORT_SYMBOL_GPL(dm_bm_checksum);
+
+/*----------------------------------------------------------------*/
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Joe Thornber <dm-devel@redhat.com>");
+MODULE_DESCRIPTION("Immutable metadata library for dm");
+
+/*----------------------------------------------------------------*/
