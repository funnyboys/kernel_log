commit 4b25bbf52a55b58f03473333eafe61c0d46125aa
Author: Coly Li <colyli@suse.de>
Date:   Mon Jun 15 00:53:33 2020 +0800

    bcache: pr_info() format clean up in bcache_device_init()
    
    scripts/checkpatch.pl reports following warning for patch
    ("bcache: check and adjust logical block size for backing devices"),
        WARNING: quoted string split across lines
        #146: FILE: drivers/md/bcache/super.c:896:
        +  pr_info("%s: sb/logical block size (%u) greater than page size "
        +          "(%lu) falling back to device logical block size (%u)",
    
    There are two things to fix up,
    - The kernel message print should be in a single line.
    - pr_info() won't automatically add new line since v5.8, a '\n' should
      be added.
    
    This patch just does the above cleanup in bcache_device_init().
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a6e79083010a..2014016f9a60 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -893,8 +893,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 		 * This should only happen with BCACHE_SB_VERSION_BDEV.
 		 * Block/page size is checked for BCACHE_SB_VERSION_CDEV.
 		 */
-		pr_info("%s: sb/logical block size (%u) greater than page size "
-			"(%lu) falling back to device logical block size (%u)",
+		pr_info("%s: sb/logical block size (%u) greater than page size (%lu) falling back to device logical block size (%u)\n",
 			d->disk->disk_name, q->limits.logical_block_size,
 			PAGE_SIZE, bdev_logical_block_size(cached_bdev));
 

commit ee4a36f414617aaae01f93133e828363e7e9dd17
Author: Coly Li <colyli@suse.de>
Date:   Mon Jun 15 00:53:32 2020 +0800

    bcache: use delayed kworker fo asynchronous devices registration
    
    This patch changes the asynchronous registration kworker to a delayed
    kworker. There is probability queue_work() queues the async registration
    kworker to the same CPU (even though very little), then the process
    which writing sysfs interface to reigster bcache device may won't return
    immeidately. queue_delayed_work() in this patch will delay 10 jiffies
    before insert the kworker to run queue, which makes sure the registering
    process may always returns to user space in time.
    
    Fixes: 9e23ccf8f0a22 ("bcache: asynchronous devices registration")
    Signed-off-by: Coly Li <colyli@suse.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 904ea9bbb5c4..a6e79083010a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -19,6 +19,7 @@
 #include <linux/genhd.h>
 #include <linux/idr.h>
 #include <linux/kthread.h>
+#include <linux/workqueue.h>
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/reboot.h>
@@ -2380,7 +2381,7 @@ static bool bch_is_open(struct block_device *bdev)
 }
 
 struct async_reg_args {
-	struct work_struct reg_work;
+	struct delayed_work reg_work;
 	char *path;
 	struct cache_sb *sb;
 	struct cache_sb_disk *sb_disk;
@@ -2391,7 +2392,7 @@ static void register_bdev_worker(struct work_struct *work)
 {
 	int fail = false;
 	struct async_reg_args *args =
-		container_of(work, struct async_reg_args, reg_work);
+		container_of(work, struct async_reg_args, reg_work.work);
 	struct cached_dev *dc;
 
 	dc = kzalloc(sizeof(*dc), GFP_KERNEL);
@@ -2421,7 +2422,7 @@ static void register_cache_worker(struct work_struct *work)
 {
 	int fail = false;
 	struct async_reg_args *args =
-		container_of(work, struct async_reg_args, reg_work);
+		container_of(work, struct async_reg_args, reg_work.work);
 	struct cache *ca;
 
 	ca = kzalloc(sizeof(*ca), GFP_KERNEL);
@@ -2449,11 +2450,12 @@ static void register_cache_worker(struct work_struct *work)
 static void register_device_aync(struct async_reg_args *args)
 {
 	if (SB_IS_BDEV(args->sb))
-		INIT_WORK(&args->reg_work, register_bdev_worker);
+		INIT_DELAYED_WORK(&args->reg_work, register_bdev_worker);
 	else
-		INIT_WORK(&args->reg_work, register_cache_worker);
+		INIT_DELAYED_WORK(&args->reg_work, register_cache_worker);
 
-	queue_work(system_wq, &args->reg_work);
+	/* 10 jiffies is enough for a delay */
+	queue_delayed_work(system_wq, &args->reg_work, 10);
 }
 
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,

commit dcacbc1242c71e18fa9d2eadc5647e115c9c627d
Author: Mauricio Faria de Oliveira <mfo@canonical.com>
Date:   Mon Jun 15 00:53:31 2020 +0800

    bcache: check and adjust logical block size for backing devices
    
    It's possible for a block driver to set logical block size to
    a value greater than page size incorrectly; e.g. bcache takes
    the value from the superblock, set by the user w/ make-bcache.
    
    This causes a BUG/NULL pointer dereference in the path:
    
      __blkdev_get()
      -> set_init_blocksize() // set i_blkbits based on ...
         -> bdev_logical_block_size()
            -> queue_logical_block_size() // ... this value
      -> bdev_disk_changed()
         ...
         -> blkdev_readpage()
            -> block_read_full_page()
               -> create_page_buffers() // size = 1 << i_blkbits
                  -> create_empty_buffers() // give size/take pointer
                     -> alloc_page_buffers() // return NULL
                     .. BUG!
    
    Because alloc_page_buffers() is called with size > PAGE_SIZE,
    thus it initializes head = NULL, skips the loop, return head;
    then create_empty_buffers() gets (and uses) the NULL pointer.
    
    This has been around longer than commit ad6bf88a6c19 ("block:
    fix an integer overflow in logical block size"); however, it
    increased the range of values that can trigger the issue.
    
    Previously only 8k/16k/32k (on x86/4k page size) would do it,
    as greater values overflow unsigned short to zero, and queue_
    logical_block_size() would then use the default of 512.
    
    Now the range with unsigned int is much larger, and users w/
    the 512k value, which happened to be zero'ed previously and
    work fine, started to hit this issue -- as the zero is gone,
    and queue_logical_block_size() does return 512k (>PAGE_SIZE.)
    
    Fix this by checking the bcache device's logical block size,
    and if it's greater than page size, fallback to the backing/
    cached device's logical page size.
    
    This doesn't affect cache devices as those are still checked
    for block/page size in read_super(); only the backing/cached
    devices are not.
    
    Apparently it's a regression from commit 2903381fce71 ("bcache:
    Take data offset from the bdev superblock."), moving the check
    into BCACHE_SB_VERSION_CDEV only. Now that we have superblocks
    of backing devices out there with this larger value, we cannot
    refuse to load them (i.e., have a similar check in _BDEV.)
    
    Ideally perhaps bcache should use all values from the backing
    device (physical/logical/io_min block size)? But for now just
    fix the problematic case.
    
    Test-case:
    
        # IMG=/root/disk.img
        # dd if=/dev/zero of=$IMG bs=1 count=0 seek=1G
        # DEV=$(losetup --find --show $IMG)
        # make-bcache --bdev $DEV --block 8k
          < see dmesg >
    
    Before:
    
        # uname -r
        5.7.0-rc7
    
        [   55.944046] BUG: kernel NULL pointer dereference, address: 0000000000000000
        ...
        [   55.949742] CPU: 3 PID: 610 Comm: bcache-register Not tainted 5.7.0-rc7 #4
        ...
        [   55.952281] RIP: 0010:create_empty_buffers+0x1a/0x100
        ...
        [   55.966434] Call Trace:
        [   55.967021]  create_page_buffers+0x48/0x50
        [   55.967834]  block_read_full_page+0x49/0x380
        [   55.972181]  do_read_cache_page+0x494/0x610
        [   55.974780]  read_part_sector+0x2d/0xaa
        [   55.975558]  read_lba+0x10e/0x1e0
        [   55.977904]  efi_partition+0x120/0x5a6
        [   55.980227]  blk_add_partitions+0x161/0x390
        [   55.982177]  bdev_disk_changed+0x61/0xd0
        [   55.982961]  __blkdev_get+0x350/0x490
        [   55.983715]  __device_add_disk+0x318/0x480
        [   55.984539]  bch_cached_dev_run+0xc5/0x270
        [   55.986010]  register_bcache.cold+0x122/0x179
        [   55.987628]  kernfs_fop_write+0xbc/0x1a0
        [   55.988416]  vfs_write+0xb1/0x1a0
        [   55.989134]  ksys_write+0x5a/0xd0
        [   55.989825]  do_syscall_64+0x43/0x140
        [   55.990563]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
        [   55.991519] RIP: 0033:0x7f7d60ba3154
        ...
    
    After:
    
        # uname -r
        5.7.0.bcachelbspgsz
    
        [   31.672460] bcache: bcache_device_init() bcache0: sb/logical block size (8192) greater than page size (4096) falling back to device logical block size (512)
        [   31.675133] bcache: register_bdev() registered backing device loop0
    
        # grep ^ /sys/block/bcache0/queue/*_block_size
        /sys/block/bcache0/queue/logical_block_size:512
        /sys/block/bcache0/queue/physical_block_size:8192
    
    Reported-by: Ryan Finnie <ryan@finnie.org>
    Reported-by: Sebastian Marsching <sebastian@marsching.com>
    Signed-off-by: Mauricio Faria de Oliveira <mfo@canonical.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f9975c22bf7e..904ea9bbb5c4 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -819,7 +819,8 @@ static void bcache_device_free(struct bcache_device *d)
 }
 
 static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
-			      sector_t sectors, make_request_fn make_request_fn)
+			      sector_t sectors, make_request_fn make_request_fn,
+			      struct block_device *cached_bdev)
 {
 	struct request_queue *q;
 	const size_t max_stripes = min_t(size_t, INT_MAX,
@@ -885,6 +886,21 @@ static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 	q->limits.io_min		= block_size;
 	q->limits.logical_block_size	= block_size;
 	q->limits.physical_block_size	= block_size;
+
+	if (q->limits.logical_block_size > PAGE_SIZE && cached_bdev) {
+		/*
+		 * This should only happen with BCACHE_SB_VERSION_BDEV.
+		 * Block/page size is checked for BCACHE_SB_VERSION_CDEV.
+		 */
+		pr_info("%s: sb/logical block size (%u) greater than page size "
+			"(%lu) falling back to device logical block size (%u)",
+			d->disk->disk_name, q->limits.logical_block_size,
+			PAGE_SIZE, bdev_logical_block_size(cached_bdev));
+
+		/* This also adjusts physical block size/min io size if needed */
+		blk_queue_logical_block_size(q, bdev_logical_block_size(cached_bdev));
+	}
+
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, d->disk->queue);
 	blk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, d->disk->queue);
 	blk_queue_flag_set(QUEUE_FLAG_DISCARD, d->disk->queue);
@@ -1340,7 +1356,7 @@ static int cached_dev_init(struct cached_dev *dc, unsigned int block_size)
 
 	ret = bcache_device_init(&dc->disk, block_size,
 			 dc->bdev->bd_part->nr_sects - dc->sb.data_offset,
-			 cached_dev_make_request);
+			 cached_dev_make_request, dc->bdev);
 	if (ret)
 		return ret;
 
@@ -1453,7 +1469,7 @@ static int flash_dev_run(struct cache_set *c, struct uuid_entry *u)
 	kobject_init(&d->kobj, &bch_flash_dev_ktype);
 
 	if (bcache_device_init(d, block_bytes(c), u->sectors,
-			flash_dev_make_request))
+			flash_dev_make_request, NULL))
 		goto err;
 
 	bcache_device_attach(d, c, u - c->uuids);

commit 0c8d3fceade2ab1bbac68bca013e62bfdb851d19
Author: Coly Li <colyli@suse.de>
Date:   Wed May 27 12:01:55 2020 +0800

    bcache: configure the asynchronous registertion to be experimental
    
    In order to avoid the experimental async registration interface to
    be treated as new kernel ABI for common users, this patch makes it
    as an experimental kernel configure BCACHE_ASYNC_REGISTRAION.
    
    This interface is for extreme large cached data situation, to make sure
    the bcache device can always created without the udev timeout issue. For
    normal users the async or sync registration does not make difference.
    
    In future when we decide to use the asynchronous registration as default
    behavior, this experimental interface may be removed.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b971d8e916d5..f9975c22bf7e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2765,7 +2765,9 @@ static int __init bcache_init(void)
 	static const struct attribute *files[] = {
 		&ksysfs_register.attr,
 		&ksysfs_register_quiet.attr,
+#ifdef CONFIG_BCACHE_ASYNC_REGISTRAION
 		&ksysfs_register_async.attr,
+#endif
 		&ksysfs_pendings_cleanup.attr,
 		NULL
 	};

commit 9e23ccf8f0a22e5b86a9e0d8ecbb49fe2fa73ae9
Author: Coly Li <colyli@suse.de>
Date:   Wed May 27 12:01:54 2020 +0800

    bcache: asynchronous devices registration
    
    When there is a lot of data cached on cache device, the bcach internal
    btree can take a very long to validate during the backing device and
    cache device registration. In my test, it may takes 55+ minutes to check
    all the internal btree nodes.
    
    The problem is that the registration is invoked by udev rules and the
    udevd has 180 seconds timeout by default. If the btree node checking
    time is longer than udevd timeout, the registering  process will be
    killed by udevd with SIGKILL. If the registering process has pending
    sigal, creating kthread for bcache will fail and the device registration
    will fail. The result is, for bcache device which cached a lot of data
    on cache device, the bcache device node like /dev/bcache<N> won't create
    always due to the very long btree checking time.
    
    A solution to avoid the udevd 180 seconds timeout is to register devices
    in an asynchronous way. Which is, after writing cache or backing device
    path into /sys/fs/bcache/register_async, the kernel code will create a
    kworker and move all the btree node checking (for cache device) or dirty
    data counting (for cached device) in the kwork context. Then the kworder
    is scheduled on system_wq and the registration code just returned to
    user space udev rule task. By this asynchronous way, the udev task for
    bcache rule will complete in seconds, no matter how long time spent in
    the kworker context, it won't be killed by udevd for a timeout.
    
    After all the checking and counting are done asynchronously in the
    kworker, the bcache device will eventually be created successfully.
    
    This patch does the above chagne and add a register sysfs file
    /sys/fs/bcache/register_async. Writing the registering device path into
    this sysfs file will do the asynchronous registration.
    
    The register_async interface is for very rare condition and won't be
    used for common users. In future I plan to make the asynchronous
    registration as default behavior, which depends on feedback for this
    patch.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a10a3c78f4ff..b971d8e916d5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2327,6 +2327,7 @@ static ssize_t bch_pending_bdevs_cleanup(struct kobject *k,
 
 kobj_attribute_write(register,		register_bcache);
 kobj_attribute_write(register_quiet,	register_bcache);
+kobj_attribute_write(register_async,	register_bcache);
 kobj_attribute_write(pendings_cleanup,	bch_pending_bdevs_cleanup);
 
 static bool bch_is_open_backing(struct block_device *bdev)
@@ -2362,6 +2363,83 @@ static bool bch_is_open(struct block_device *bdev)
 	return bch_is_open_cache(bdev) || bch_is_open_backing(bdev);
 }
 
+struct async_reg_args {
+	struct work_struct reg_work;
+	char *path;
+	struct cache_sb *sb;
+	struct cache_sb_disk *sb_disk;
+	struct block_device *bdev;
+};
+
+static void register_bdev_worker(struct work_struct *work)
+{
+	int fail = false;
+	struct async_reg_args *args =
+		container_of(work, struct async_reg_args, reg_work);
+	struct cached_dev *dc;
+
+	dc = kzalloc(sizeof(*dc), GFP_KERNEL);
+	if (!dc) {
+		fail = true;
+		put_page(virt_to_page(args->sb_disk));
+		blkdev_put(args->bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
+		goto out;
+	}
+
+	mutex_lock(&bch_register_lock);
+	if (register_bdev(args->sb, args->sb_disk, args->bdev, dc) < 0)
+		fail = true;
+	mutex_unlock(&bch_register_lock);
+
+out:
+	if (fail)
+		pr_info("error %s: fail to register backing device\n",
+			args->path);
+	kfree(args->sb);
+	kfree(args->path);
+	kfree(args);
+	module_put(THIS_MODULE);
+}
+
+static void register_cache_worker(struct work_struct *work)
+{
+	int fail = false;
+	struct async_reg_args *args =
+		container_of(work, struct async_reg_args, reg_work);
+	struct cache *ca;
+
+	ca = kzalloc(sizeof(*ca), GFP_KERNEL);
+	if (!ca) {
+		fail = true;
+		put_page(virt_to_page(args->sb_disk));
+		blkdev_put(args->bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
+		goto out;
+	}
+
+	/* blkdev_put() will be called in bch_cache_release() */
+	if (register_cache(args->sb, args->sb_disk, args->bdev, ca) != 0)
+		fail = true;
+
+out:
+	if (fail)
+		pr_info("error %s: fail to register cache device\n",
+			args->path);
+	kfree(args->sb);
+	kfree(args->path);
+	kfree(args);
+	module_put(THIS_MODULE);
+}
+
+static void register_device_aync(struct async_reg_args *args)
+{
+	if (SB_IS_BDEV(args->sb))
+		INIT_WORK(&args->reg_work, register_bdev_worker);
+	else
+		INIT_WORK(&args->reg_work, register_cache_worker);
+
+	queue_work(system_wq, &args->reg_work);
+}
+
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size)
 {
@@ -2424,6 +2502,26 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		goto out_blkdev_put;
 
 	err = "failed to register device";
+	if (attr == &ksysfs_register_async) {
+		/* register in asynchronous way */
+		struct async_reg_args *args =
+			kzalloc(sizeof(struct async_reg_args), GFP_KERNEL);
+
+		if (!args) {
+			ret = -ENOMEM;
+			err = "cannot allocate memory";
+			goto out_put_sb_page;
+		}
+
+		args->path	= path;
+		args->sb	= sb;
+		args->sb_disk	= sb_disk;
+		args->bdev	= bdev;
+		register_device_aync(args);
+		/* No wait and returns to user space */
+		goto async_done;
+	}
+
 	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
 
@@ -2451,6 +2549,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	kfree(sb);
 	kfree(path);
 	module_put(THIS_MODULE);
+async_done:
 	return size;
 
 out_put_sb_page:
@@ -2666,6 +2765,7 @@ static int __init bcache_init(void)
 	static const struct attribute *files[] = {
 		&ksysfs_register.attr,
 		&ksysfs_register_quiet.attr,
+		&ksysfs_register_async.attr,
 		&ksysfs_pendings_cleanup.attr,
 		NULL
 	};

commit 86da9f736740eba602389908574dfbb0f517baa5
Author: Coly Li <colyli@suse.de>
Date:   Wed May 27 12:01:53 2020 +0800

    bcache: fix refcount underflow in bcache_device_free()
    
    The problematic code piece in bcache_device_free() is,
    
     785 static void bcache_device_free(struct bcache_device *d)
     786 {
     787     struct gendisk *disk = d->disk;
     [snipped]
     799     if (disk) {
     800             if (disk->flags & GENHD_FL_UP)
     801                     del_gendisk(disk);
     802
     803             if (disk->queue)
     804                     blk_cleanup_queue(disk->queue);
     805
     806             ida_simple_remove(&bcache_device_idx,
     807                               first_minor_to_idx(disk->first_minor));
     808             put_disk(disk);
     809         }
     [snipped]
     816 }
    
    At line 808, put_disk(disk) may encounter kobject refcount of 'disk'
    being underflow.
    
    Here is how to reproduce the issue,
    - Attche the backing device to a cache device and do random write to
      make the cache being dirty.
    - Stop the bcache device while the cache device has dirty data of the
      backing device.
    - Only register the backing device back, NOT register cache device.
    - The bcache device node /dev/bcache0 won't show up, because backing
      device waits for the cache device shows up for the missing dirty
      data.
    - Now echo 1 into /sys/fs/bcache/pendings_cleanup, to stop the pending
      backing device.
    - After the pending backing device stopped, use 'dmesg' to check kernel
      message, a use-after-free warning from KASA reported the refcount of
      kobject linked to the 'disk' is underflow.
    
    The dropping refcount at line 808 in the above code piece is added by
    add_disk(d->disk) in bch_cached_dev_run(). But in the above condition
    the cache device is not registered, bch_cached_dev_run() has no chance
    to be called and the refcount is not added. The put_disk() for a non-
    added refcount of gendisk kobject triggers a underflow warning.
    
    This patch checks whether GENHD_FL_UP is set in disk->flags, if it is
    not set then the bcache device was not added, don't call put_disk()
    and the the underflow issue can be avoided.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 467149f3bcc5..a10a3c78f4ff 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -797,7 +797,9 @@ static void bcache_device_free(struct bcache_device *d)
 		bcache_device_detach(d);
 
 	if (disk) {
-		if (disk->flags & GENHD_FL_UP)
+		bool disk_added = (disk->flags & GENHD_FL_UP) != 0;
+
+		if (disk_added)
 			del_gendisk(disk);
 
 		if (disk->queue)
@@ -805,7 +807,8 @@ static void bcache_device_free(struct bcache_device *d)
 
 		ida_simple_remove(&bcache_device_idx,
 				  first_minor_to_idx(disk->first_minor));
-		put_disk(disk);
+		if (disk_added)
+			put_disk(disk);
 	}
 
 	bioset_exit(&d->bio_split);

commit 46f5aa8806e34f2e48de852cc7db2c74c3a5cd8d
Author: Joe Perches <joe@perches.com>
Date:   Wed May 27 12:01:52 2020 +0800

    bcache: Convert pr_<level> uses to a more typical style
    
    Remove the trailing newline from the define of pr_fmt and add newlines
    to the uses.
    
    Miscellanea:
    
    o Convert bch_bkey_dump from multiple uses of pr_err to pr_cont
      as the earlier conversion was inappropriate done causing multiple
      lines to be emitted where only a single output line was desired
    o Use vsprintf extension %pV in bch_cache_set_error to avoid multiple
      line output where only a single line output was desired
    o Coalesce formats
    
    Fixes: 6ae63e3501c4 ("bcache: replace printk() by pr_*() routines")
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d98354fa28e3..467149f3bcc5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -89,7 +89,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	for (i = 0; i < SB_JOURNAL_BUCKETS; i++)
 		sb->d[i] = le64_to_cpu(s->d[i]);
 
-	pr_debug("read sb version %llu, flags %llu, seq %llu, journal size %u",
+	pr_debug("read sb version %llu, flags %llu, seq %llu, journal size %u\n",
 		 sb->version, sb->flags, sb->seq, sb->keys);
 
 	err = "Not a bcache superblock (bad offset)";
@@ -234,7 +234,7 @@ static void __write_super(struct cache_sb *sb, struct cache_sb_disk *out,
 
 	out->csum = csum_set(out);
 
-	pr_debug("ver %llu, flags %llu, seq %llu",
+	pr_debug("ver %llu, flags %llu, seq %llu\n",
 		 sb->version, sb->flags, sb->seq);
 
 	submit_bio(bio);
@@ -365,11 +365,11 @@ static void uuid_io(struct cache_set *c, int op, unsigned long op_flags,
 	}
 
 	bch_extent_to_text(buf, sizeof(buf), k);
-	pr_debug("%s UUIDs at %s", op == REQ_OP_WRITE ? "wrote" : "read", buf);
+	pr_debug("%s UUIDs at %s\n", op == REQ_OP_WRITE ? "wrote" : "read", buf);
 
 	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)
 		if (!bch_is_zero(u->uuid, 16))
-			pr_debug("Slot %zi: %pU: %s: 1st: %u last: %u inv: %u",
+			pr_debug("Slot %zi: %pU: %s: 1st: %u last: %u inv: %u\n",
 				 u - c->uuids, u->uuid, u->label,
 				 u->first_reg, u->last_reg, u->invalidated);
 
@@ -534,7 +534,7 @@ int bch_prio_write(struct cache *ca, bool wait)
 	struct bucket *b;
 	struct closure cl;
 
-	pr_debug("free_prio=%zu, free_none=%zu, free_inc=%zu",
+	pr_debug("free_prio=%zu, free_none=%zu, free_inc=%zu\n",
 		 fifo_used(&ca->free[RESERVE_PRIO]),
 		 fifo_used(&ca->free[RESERVE_NONE]),
 		 fifo_used(&ca->free_inc));
@@ -629,12 +629,12 @@ static int prio_read(struct cache *ca, uint64_t bucket)
 
 			if (p->csum !=
 			    bch_crc64(&p->magic, bucket_bytes(ca) - 8)) {
-				pr_warn("bad csum reading priorities");
+				pr_warn("bad csum reading priorities\n");
 				goto out;
 			}
 
 			if (p->magic != pset_magic(&ca->sb)) {
-				pr_warn("bad magic reading priorities");
+				pr_warn("bad magic reading priorities\n");
 				goto out;
 			}
 
@@ -728,11 +728,11 @@ static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
 
 	ret = sysfs_create_link(&d->kobj, &c->kobj, "cache");
 	if (ret < 0)
-		pr_err("Couldn't create device -> cache set symlink");
+		pr_err("Couldn't create device -> cache set symlink\n");
 
 	ret = sysfs_create_link(&c->kobj, &d->kobj, d->name);
 	if (ret < 0)
-		pr_err("Couldn't create cache set -> device symlink");
+		pr_err("Couldn't create cache set -> device symlink\n");
 
 	clear_bit(BCACHE_DEV_UNLINK_DONE, &d->flags);
 }
@@ -789,9 +789,9 @@ static void bcache_device_free(struct bcache_device *d)
 	lockdep_assert_held(&bch_register_lock);
 
 	if (disk)
-		pr_info("%s stopped", disk->disk_name);
+		pr_info("%s stopped\n", disk->disk_name);
 	else
-		pr_err("bcache device (NULL gendisk) stopped");
+		pr_err("bcache device (NULL gendisk) stopped\n");
 
 	if (d->c)
 		bcache_device_detach(d);
@@ -830,7 +830,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 	d->nr_stripes = DIV_ROUND_UP_ULL(sectors, d->stripe_size);
 
 	if (!d->nr_stripes || d->nr_stripes > max_stripes) {
-		pr_err("nr_stripes too large or invalid: %u (start sector beyond end of disk?)",
+		pr_err("nr_stripes too large or invalid: %u (start sector beyond end of disk?)\n",
 			(unsigned int)d->nr_stripes);
 		return -ENOMEM;
 	}
@@ -928,11 +928,11 @@ static int cached_dev_status_update(void *arg)
 			dc->offline_seconds = 0;
 
 		if (dc->offline_seconds >= BACKING_DEV_OFFLINE_TIMEOUT) {
-			pr_err("%s: device offline for %d seconds",
+			pr_err("%s: device offline for %d seconds\n",
 			       dc->backing_dev_name,
 			       BACKING_DEV_OFFLINE_TIMEOUT);
-			pr_err("%s: disable I/O request due to backing "
-			       "device offline", dc->disk.name);
+			pr_err("%s: disable I/O request due to backing device offline\n",
+			       dc->disk.name);
 			dc->io_disable = true;
 			/* let others know earlier that io_disable is true */
 			smp_mb();
@@ -959,7 +959,7 @@ int bch_cached_dev_run(struct cached_dev *dc)
 	};
 
 	if (dc->io_disable) {
-		pr_err("I/O disabled on cached dev %s",
+		pr_err("I/O disabled on cached dev %s\n",
 		       dc->backing_dev_name);
 		kfree(env[1]);
 		kfree(env[2]);
@@ -971,7 +971,7 @@ int bch_cached_dev_run(struct cached_dev *dc)
 		kfree(env[1]);
 		kfree(env[2]);
 		kfree(buf);
-		pr_info("cached dev %s is running already",
+		pr_info("cached dev %s is running already\n",
 		       dc->backing_dev_name);
 		return -EBUSY;
 	}
@@ -1001,16 +1001,14 @@ int bch_cached_dev_run(struct cached_dev *dc)
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj,
 			      &d->kobj, "bcache")) {
-		pr_err("Couldn't create bcache dev <-> disk sysfs symlinks");
+		pr_err("Couldn't create bcache dev <-> disk sysfs symlinks\n");
 		return -ENOMEM;
 	}
 
 	dc->status_update_thread = kthread_run(cached_dev_status_update,
 					       dc, "bcache_status_update");
 	if (IS_ERR(dc->status_update_thread)) {
-		pr_warn("failed to create bcache_status_update kthread, "
-			"continue to run without monitoring backing "
-			"device status");
+		pr_warn("failed to create bcache_status_update kthread, continue to run without monitoring backing device status\n");
 	}
 
 	return 0;
@@ -1036,7 +1034,7 @@ static void cancel_writeback_rate_update_dwork(struct cached_dev *dc)
 	} while (time_out > 0);
 
 	if (time_out == 0)
-		pr_warn("give up waiting for dc->writeback_write_update to quit");
+		pr_warn("give up waiting for dc->writeback_write_update to quit\n");
 
 	cancel_delayed_work_sync(&dc->writeback_rate_update);
 }
@@ -1077,7 +1075,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 
 	mutex_unlock(&bch_register_lock);
 
-	pr_info("Caching disabled for %s", dc->backing_dev_name);
+	pr_info("Caching disabled for %s\n", dc->backing_dev_name);
 
 	/* Drop ref we took in cached_dev_detach() */
 	closure_put(&dc->disk.cl);
@@ -1117,20 +1115,20 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 		return -ENOENT;
 
 	if (dc->disk.c) {
-		pr_err("Can't attach %s: already attached",
+		pr_err("Can't attach %s: already attached\n",
 		       dc->backing_dev_name);
 		return -EINVAL;
 	}
 
 	if (test_bit(CACHE_SET_STOPPING, &c->flags)) {
-		pr_err("Can't attach %s: shutting down",
+		pr_err("Can't attach %s: shutting down\n",
 		       dc->backing_dev_name);
 		return -EINVAL;
 	}
 
 	if (dc->sb.block_size < c->sb.block_size) {
 		/* Will die */
-		pr_err("Couldn't attach %s: block size less than set's block size",
+		pr_err("Couldn't attach %s: block size less than set's block size\n",
 		       dc->backing_dev_name);
 		return -EINVAL;
 	}
@@ -1138,7 +1136,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	/* Check whether already attached */
 	list_for_each_entry_safe(exist_dc, t, &c->cached_devs, list) {
 		if (!memcmp(dc->sb.uuid, exist_dc->sb.uuid, 16)) {
-			pr_err("Tried to attach %s but duplicate UUID already attached",
+			pr_err("Tried to attach %s but duplicate UUID already attached\n",
 				dc->backing_dev_name);
 
 			return -EINVAL;
@@ -1157,14 +1155,14 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 
 	if (!u) {
 		if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
-			pr_err("Couldn't find uuid for %s in set",
+			pr_err("Couldn't find uuid for %s in set\n",
 			       dc->backing_dev_name);
 			return -ENOENT;
 		}
 
 		u = uuid_find_empty(c);
 		if (!u) {
-			pr_err("Not caching %s, no room for UUID",
+			pr_err("Not caching %s, no room for UUID\n",
 			       dc->backing_dev_name);
 			return -EINVAL;
 		}
@@ -1210,7 +1208,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	down_write(&dc->writeback_lock);
 	if (bch_cached_dev_writeback_start(dc)) {
 		up_write(&dc->writeback_lock);
-		pr_err("Couldn't start writeback facilities for %s",
+		pr_err("Couldn't start writeback facilities for %s\n",
 		       dc->disk.disk->disk_name);
 		return -ENOMEM;
 	}
@@ -1233,7 +1231,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 		 */
 		kthread_stop(dc->writeback_thread);
 		cancel_writeback_rate_update_dwork(dc);
-		pr_err("Couldn't run cached device %s",
+		pr_err("Couldn't run cached device %s\n",
 		       dc->backing_dev_name);
 		return ret;
 	}
@@ -1244,7 +1242,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	/* Allow the writeback thread to proceed */
 	up_write(&dc->writeback_lock);
 
-	pr_info("Caching %s as %s on set %pU",
+	pr_info("Caching %s as %s on set %pU\n",
 		dc->backing_dev_name,
 		dc->disk.disk->disk_name,
 		dc->disk.c->sb.set_uuid);
@@ -1384,7 +1382,7 @@ static int register_bdev(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 	if (bch_cache_accounting_add_kobjs(&dc->accounting, &dc->disk.kobj))
 		goto err;
 
-	pr_info("registered backing device %s", dc->backing_dev_name);
+	pr_info("registered backing device %s\n", dc->backing_dev_name);
 
 	list_add(&dc->list, &uncached_devices);
 	/* attach to a matched cache set if it exists */
@@ -1401,7 +1399,7 @@ static int register_bdev(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 
 	return 0;
 err:
-	pr_notice("error %s: %s", dc->backing_dev_name, err);
+	pr_notice("error %s: %s\n", dc->backing_dev_name, err);
 	bcache_device_stop(&dc->disk);
 	return ret;
 }
@@ -1497,7 +1495,7 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 
 	u = uuid_find_empty(c);
 	if (!u) {
-		pr_err("Can't create volume, no room for UUID");
+		pr_err("Can't create volume, no room for UUID\n");
 		return -EINVAL;
 	}
 
@@ -1523,7 +1521,7 @@ bool bch_cached_dev_error(struct cached_dev *dc)
 	smp_mb();
 
 	pr_err("stop %s: too many IO errors on backing device %s\n",
-		dc->disk.disk->disk_name, dc->backing_dev_name);
+	       dc->disk.disk->disk_name, dc->backing_dev_name);
 
 	bcache_device_stop(&dc->disk);
 	return true;
@@ -1534,6 +1532,7 @@ bool bch_cached_dev_error(struct cached_dev *dc)
 __printf(2, 3)
 bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 {
+	struct va_format vaf;
 	va_list args;
 
 	if (c->on_error != ON_ERROR_PANIC &&
@@ -1541,20 +1540,22 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 		return false;
 
 	if (test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
-		pr_info("CACHE_SET_IO_DISABLE already set");
+		pr_info("CACHE_SET_IO_DISABLE already set\n");
 
 	/*
 	 * XXX: we can be called from atomic context
 	 * acquire_console_sem();
 	 */
 
-	pr_err("bcache: error on %pU: ", c->sb.set_uuid);
-
 	va_start(args, fmt);
-	vprintk(fmt, args);
-	va_end(args);
 
-	pr_err(", disabling caching\n");
+	vaf.fmt = fmt;
+	vaf.va = &args;
+
+	pr_err("error on %pU: %pV, disabling caching\n",
+	       c->sb.set_uuid, &vaf);
+
+	va_end(args);
 
 	if (c->on_error == ON_ERROR_PANIC)
 		panic("panic forced after error\n");
@@ -1606,7 +1607,7 @@ static void cache_set_free(struct closure *cl)
 	list_del(&c->list);
 	mutex_unlock(&bch_register_lock);
 
-	pr_info("Cache set %pU unregistered", c->sb.set_uuid);
+	pr_info("Cache set %pU unregistered\n", c->sb.set_uuid);
 	wake_up(&unregister_wait);
 
 	closure_debug_destroy(&c->cl);
@@ -1677,7 +1678,7 @@ static void conditional_stop_bcache_device(struct cache_set *c,
 					   struct cached_dev *dc)
 {
 	if (dc->stop_when_cache_set_failed == BCH_CACHED_DEV_STOP_ALWAYS) {
-		pr_warn("stop_when_cache_set_failed of %s is \"always\", stop it for failed cache set %pU.",
+		pr_warn("stop_when_cache_set_failed of %s is \"always\", stop it for failed cache set %pU.\n",
 			d->disk->disk_name, c->sb.set_uuid);
 		bcache_device_stop(d);
 	} else if (atomic_read(&dc->has_dirty)) {
@@ -1685,7 +1686,7 @@ static void conditional_stop_bcache_device(struct cache_set *c,
 		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO
 		 * and dc->has_dirty == 1
 		 */
-		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is dirty, stop it to avoid potential data corruption.",
+		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is dirty, stop it to avoid potential data corruption.\n",
 			d->disk->disk_name);
 		/*
 		 * There might be a small time gap that cache set is
@@ -1707,7 +1708,7 @@ static void conditional_stop_bcache_device(struct cache_set *c,
 		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO
 		 * and dc->has_dirty == 0
 		 */
-		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is clean, keep it alive.",
+		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is clean, keep it alive.\n",
 			d->disk->disk_name);
 	}
 }
@@ -1874,7 +1875,7 @@ static int run_cache_set(struct cache_set *c)
 		if (bch_journal_read(c, &journal))
 			goto err;
 
-		pr_debug("btree_journal_read() done");
+		pr_debug("btree_journal_read() done\n");
 
 		err = "no journal entries found";
 		if (list_empty(&journal))
@@ -1920,7 +1921,7 @@ static int run_cache_set(struct cache_set *c)
 
 		bch_journal_mark(c, &journal);
 		bch_initial_gc_finish(c);
-		pr_debug("btree_check() done");
+		pr_debug("btree_check() done\n");
 
 		/*
 		 * bcache_journal_next() can't happen sooner, or
@@ -1951,7 +1952,7 @@ static int run_cache_set(struct cache_set *c)
 		if (bch_journal_replay(c, &journal))
 			goto err;
 	} else {
-		pr_notice("invalidating existing data");
+		pr_notice("invalidating existing data\n");
 
 		for_each_cache(ca, c, i) {
 			unsigned int j;
@@ -2085,7 +2086,7 @@ static const char *register_cache_set(struct cache *ca)
 		memcpy(c->sb.set_uuid, ca->sb.set_uuid, 16);
 		c->sb.flags             = ca->sb.flags;
 		c->sb.seq		= ca->sb.seq;
-		pr_debug("set version = %llu", c->sb.version);
+		pr_debug("set version = %llu\n", c->sb.version);
 	}
 
 	kobject_get(&ca->kobj);
@@ -2247,7 +2248,7 @@ static int cache_alloc(struct cache *ca)
 err_free:
 	module_put(THIS_MODULE);
 	if (err)
-		pr_notice("error %s: %s", ca->cache_dev_name, err);
+		pr_notice("error %s: %s\n", ca->cache_dev_name, err);
 	return ret;
 }
 
@@ -2301,14 +2302,14 @@ static int register_cache(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 		goto out;
 	}
 
-	pr_info("registered cache device %s", ca->cache_dev_name);
+	pr_info("registered cache device %s\n", ca->cache_dev_name);
 
 out:
 	kobject_put(&ca->kobj);
 
 err:
 	if (err)
-		pr_notice("error %s: %s", ca->cache_dev_name, err);
+		pr_notice("error %s: %s\n", ca->cache_dev_name, err);
 
 	return ret;
 }
@@ -2461,7 +2462,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 out_module_put:
 	module_put(THIS_MODULE);
 out:
-	pr_info("error %s: %s", path?path:"", err);
+	pr_info("error %s: %s\n", path?path:"", err);
 	return ret;
 }
 
@@ -2506,7 +2507,7 @@ static ssize_t bch_pending_bdevs_cleanup(struct kobject *k,
 	mutex_unlock(&bch_register_lock);
 
 	list_for_each_entry_safe(pdev, tpdev, &pending_devs, list) {
-		pr_info("delete pdev %p", pdev);
+		pr_info("delete pdev %p\n", pdev);
 		list_del(&pdev->list);
 		bcache_device_stop(&pdev->dc->disk);
 		kfree(pdev);
@@ -2549,7 +2550,7 @@ static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 
 		mutex_unlock(&bch_register_lock);
 
-		pr_info("Stopping all devices:");
+		pr_info("Stopping all devices:\n");
 
 		/*
 		 * The reason bch_register_lock is not held to call
@@ -2599,9 +2600,9 @@ static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 		finish_wait(&unregister_wait, &wait);
 
 		if (stopped)
-			pr_info("All devices stopped");
+			pr_info("All devices stopped\n");
 		else
-			pr_notice("Timeout waiting for devices to be closed");
+			pr_notice("Timeout waiting for devices to be closed\n");
 out:
 		mutex_unlock(&bch_register_lock);
 	}
@@ -2637,7 +2638,7 @@ static void check_module_parameters(void)
 	if (bch_cutoff_writeback_sync == 0)
 		bch_cutoff_writeback_sync = CUTOFF_WRITEBACK_SYNC;
 	else if (bch_cutoff_writeback_sync > CUTOFF_WRITEBACK_SYNC_MAX) {
-		pr_warn("set bch_cutoff_writeback_sync (%u) to max value %u",
+		pr_warn("set bch_cutoff_writeback_sync (%u) to max value %u\n",
 			bch_cutoff_writeback_sync, CUTOFF_WRITEBACK_SYNC_MAX);
 		bch_cutoff_writeback_sync = CUTOFF_WRITEBACK_SYNC_MAX;
 	}
@@ -2645,13 +2646,13 @@ static void check_module_parameters(void)
 	if (bch_cutoff_writeback == 0)
 		bch_cutoff_writeback = CUTOFF_WRITEBACK;
 	else if (bch_cutoff_writeback > CUTOFF_WRITEBACK_MAX) {
-		pr_warn("set bch_cutoff_writeback (%u) to max value %u",
+		pr_warn("set bch_cutoff_writeback (%u) to max value %u\n",
 			bch_cutoff_writeback, CUTOFF_WRITEBACK_MAX);
 		bch_cutoff_writeback = CUTOFF_WRITEBACK_MAX;
 	}
 
 	if (bch_cutoff_writeback > bch_cutoff_writeback_sync) {
-		pr_warn("set bch_cutoff_writeback (%u) to %u",
+		pr_warn("set bch_cutoff_writeback (%u) to %u\n",
 			bch_cutoff_writeback, bch_cutoff_writeback_sync);
 		bch_cutoff_writeback = bch_cutoff_writeback_sync;
 	}

commit 3d745ea5b095a3985129e162900b7e6c22518a9d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:30:11 2020 +0100

    block: simplify queue allocation
    
    Current make_request based drivers use either blk_alloc_queue_node or
    blk_alloc_queue to allocate a queue, and then set up the make_request_fn
    function pointer and a few parameters using the blk_queue_make_request
    helper.  Simplify this by passing the make_request pointer to
    blk_alloc_queue, and while at it merge the _node variant into the main
    helper by always passing a node_id, and remove the superfluous gfp_mask
    parameter.  A lower-level __blk_alloc_queue is kept for the blk-mq case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5e38a167c85e..d98354fa28e3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -866,11 +866,10 @@ static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 	d->disk->fops		= &bcache_ops;
 	d->disk->private_data	= d;
 
-	q = blk_alloc_queue(GFP_KERNEL);
+	q = blk_alloc_queue(make_request_fn, NUMA_NO_NODE);
 	if (!q)
 		return -ENOMEM;
 
-	blk_queue_make_request(q, make_request_fn);
 	d->disk->queue			= q;
 	q->queuedata			= d;
 	q->backing_dev_info->congested_data = d;

commit ff27668ce8092c74965f21b5c02ebc6b6764db95
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:30:10 2020 +0100

    bcache: pass the make_request methods to blk_queue_make_request
    
    bcache is the only driver not actually passing its make_request
    methods to blk_queue_make_request, but instead just sets them up
    manually a little later.  Make bcache follow the common way of
    setting up make_request based queues.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 0c3c5419c52b..5e38a167c85e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -816,7 +816,7 @@ static void bcache_device_free(struct bcache_device *d)
 }
 
 static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
-			      sector_t sectors)
+			      sector_t sectors, make_request_fn make_request_fn)
 {
 	struct request_queue *q;
 	const size_t max_stripes = min_t(size_t, INT_MAX,
@@ -870,7 +870,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 	if (!q)
 		return -ENOMEM;
 
-	blk_queue_make_request(q, NULL);
+	blk_queue_make_request(q, make_request_fn);
 	d->disk->queue			= q;
 	q->queuedata			= d;
 	q->backing_dev_info->congested_data = d;
@@ -1339,7 +1339,8 @@ static int cached_dev_init(struct cached_dev *dc, unsigned int block_size)
 			q->limits.raid_partial_stripes_expensive;
 
 	ret = bcache_device_init(&dc->disk, block_size,
-			 dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
+			 dc->bdev->bd_part->nr_sects - dc->sb.data_offset,
+			 cached_dev_make_request);
 	if (ret)
 		return ret;
 
@@ -1451,7 +1452,8 @@ static int flash_dev_run(struct cache_set *c, struct uuid_entry *u)
 
 	kobject_init(&d->kobj, &bch_flash_dev_ktype);
 
-	if (bcache_device_init(d, block_bytes(c), u->sectors))
+	if (bcache_device_init(d, block_bytes(c), u->sectors,
+			flash_dev_make_request))
 		goto err;
 
 	bcache_device_attach(d, c, u - c->uuids);

commit 309cc719a2c869b71a7388209a0a80d4284d98fd
Author: Coly Li <colyli@suse.de>
Date:   Thu Feb 13 22:12:06 2020 +0800

    bcache: Revert "bcache: shrink btree node cache after bch_btree_check()"
    
    This reverts commit 1df3877ff6a4810054237c3259d900ded4468969.
    
    In my testing, sometimes even all the cached btree nodes are freed,
    creating gc and allocator kernel threads may still fail. Finally it
    turns out that kthread_run() may fail if there is pending signal for
    current task. And the pending signal is sent from OOM killer which
    is triggered by memory consuption in bch_btree_check().
    
    Therefore explicitly shrinking bcache btree node here does not help,
    and after the shrinker callback is improved, as well as pending signals
    are ignored before creating kernel threads, now such operation is
    unncessary anymore.
    
    This patch reverts the commit 1df3877ff6a4 ("bcache: shrink btree node
    cache after bch_btree_check()") because we have better improvement now.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2749daf09724..0c3c5419c52b 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1917,23 +1917,6 @@ static int run_cache_set(struct cache_set *c)
 		if (bch_btree_check(c))
 			goto err;
 
-		/*
-		 * bch_btree_check() may occupy too much system memory which
-		 * has negative effects to user space application (e.g. data
-		 * base) performance. Shrink the mca cache memory proactively
-		 * here to avoid competing memory with user space workloads..
-		 */
-		if (!c->shrinker_disabled) {
-			struct shrink_control sc;
-
-			sc.gfp_mask = GFP_KERNEL;
-			sc.nr_to_scan = c->btree_cache_used * c->btree_pages;
-			/* first run to clear b->accessed tag */
-			c->shrink.scan_objects(&c->shrink, &sc);
-			/* second run to reap non-accessed nodes */
-			c->shrink.scan_objects(&c->shrink, &sc);
-		}
-
 		bch_journal_mark(c, &journal);
 		bch_initial_gc_finish(c);
 		pr_debug("btree_check() done");

commit 49d08d596e85f39ded48e85df362827cbab1f5ae
Author: Coly Li <colyli@suse.de>
Date:   Sat Feb 1 22:42:35 2020 +0800

    bcache: check return value of prio_read()
    
    Now if prio_read() failed during starting a cache set, we can print
    out error message in run_cache_set() and handle the failure properly.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3dea1d5acd5c..2749daf09724 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -609,12 +609,13 @@ int bch_prio_write(struct cache *ca, bool wait)
 	return 0;
 }
 
-static void prio_read(struct cache *ca, uint64_t bucket)
+static int prio_read(struct cache *ca, uint64_t bucket)
 {
 	struct prio_set *p = ca->disk_buckets;
 	struct bucket_disk *d = p->data + prios_per_bucket(ca), *end = d;
 	struct bucket *b;
 	unsigned int bucket_nr = 0;
+	int ret = -EIO;
 
 	for (b = ca->buckets;
 	     b < ca->buckets + ca->sb.nbuckets;
@@ -627,11 +628,15 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 			prio_io(ca, bucket, REQ_OP_READ, 0);
 
 			if (p->csum !=
-			    bch_crc64(&p->magic, bucket_bytes(ca) - 8))
+			    bch_crc64(&p->magic, bucket_bytes(ca) - 8)) {
 				pr_warn("bad csum reading priorities");
+				goto out;
+			}
 
-			if (p->magic != pset_magic(&ca->sb))
+			if (p->magic != pset_magic(&ca->sb)) {
 				pr_warn("bad magic reading priorities");
+				goto out;
+			}
 
 			bucket = p->next_bucket;
 			d = p->data;
@@ -640,6 +645,10 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 		b->prio = le16_to_cpu(d->prio);
 		b->gen = b->last_gc = d->gen;
 	}
+
+	ret = 0;
+out:
+	return ret;
 }
 
 /* Bcache device */
@@ -1873,8 +1882,10 @@ static int run_cache_set(struct cache_set *c)
 		j = &list_entry(journal.prev, struct journal_replay, list)->j;
 
 		err = "IO error reading priorities";
-		for_each_cache(ca, c, i)
-			prio_read(ca, j->prio_bucket[ca->sb.nr_this_dev]);
+		for_each_cache(ca, c, i) {
+			if (prio_read(ca, j->prio_bucket[ca->sb.nr_this_dev]))
+				goto err;
+		}
 
 		/*
 		 * If prio_read() fails it'll call cache_set_error and we'll

commit 6321bef028de43724c47cfa7f9dee69ecb783796
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:34 2020 +0800

    bcache: use read_cache_page_gfp to read the superblock
    
    Avoid a pointless dependency on buffer heads in bcache by simply open
    coding reading a single page.  Also add a SB_OFFSET define for the
    byte offset of the superblock instead of using magic numbers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5797c03f993e..3dea1d5acd5c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -15,7 +15,6 @@
 #include "writeback.h"
 
 #include <linux/blkdev.h>
-#include <linux/buffer_head.h>
 #include <linux/debugfs.h>
 #include <linux/genhd.h>
 #include <linux/idr.h>
@@ -64,13 +63,14 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 {
 	const char *err;
 	struct cache_sb_disk *s;
-	struct buffer_head *bh = __bread(bdev, 1, SB_SIZE);
+	struct page *page;
 	unsigned int i;
 
-	if (!bh)
+	page = read_cache_page_gfp(bdev->bd_inode->i_mapping,
+				   SB_OFFSET >> PAGE_SHIFT, GFP_KERNEL);
+	if (IS_ERR(page))
 		return "IO error";
-
-	s = (struct cache_sb_disk *)bh->b_data;
+	s = page_address(page) + offset_in_page(SB_OFFSET);
 
 	sb->offset		= le64_to_cpu(s->offset);
 	sb->version		= le64_to_cpu(s->version);
@@ -188,12 +188,10 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	}
 
 	sb->last_mount = (u32)ktime_get_real_seconds();
-	err = NULL;
-
-	get_page(bh->b_page);
 	*res = s;
+	return NULL;
 err:
-	put_bh(bh);
+	put_page(page);
 	return err;
 }
 

commit 475389ae5c08656f8bb5cc3adc88a531c7c4877f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:33 2020 +0800

    bcache: store a pointer to the on-disk sb in the cache and cached_dev structures
    
    This allows to properly build the superblock bio including the offset in
    the page using the normal bio helpers.  This fixes writing the superblock
    for page sizes larger than 4k where the sb write bio would need an offset
    in the bio_vec.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b30e4c6011d2..5797c03f993e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -207,15 +207,15 @@ static void write_bdev_super_endio(struct bio *bio)
 	closure_put(&dc->sb_write);
 }
 
-static void __write_super(struct cache_sb *sb, struct bio *bio)
+static void __write_super(struct cache_sb *sb, struct cache_sb_disk *out,
+		struct bio *bio)
 {
-	struct cache_sb_disk *out = page_address(bio_first_page_all(bio));
 	unsigned int i;
 
+	bio->bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_META;
 	bio->bi_iter.bi_sector	= SB_SECTOR;
-	bio->bi_iter.bi_size	= SB_SIZE;
-	bio_set_op_attrs(bio, REQ_OP_WRITE, REQ_SYNC|REQ_META);
-	bch_bio_map(bio, NULL);
+	__bio_add_page(bio, virt_to_page(out), SB_SIZE,
+			offset_in_page(out));
 
 	out->offset		= cpu_to_le64(sb->offset);
 	out->version		= cpu_to_le64(sb->version);
@@ -257,14 +257,14 @@ void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 	down(&dc->sb_write_mutex);
 	closure_init(cl, parent);
 
-	bio_reset(bio);
+	bio_init(bio, dc->sb_bv, 1);
 	bio_set_dev(bio, dc->bdev);
 	bio->bi_end_io	= write_bdev_super_endio;
 	bio->bi_private = dc;
 
 	closure_get(cl);
 	/* I/O request sent to backing device */
-	__write_super(&dc->sb, bio);
+	__write_super(&dc->sb, dc->sb_disk, bio);
 
 	closure_return_with_destructor(cl, bch_write_bdev_super_unlock);
 }
@@ -306,13 +306,13 @@ void bcache_write_super(struct cache_set *c)
 
 		SET_CACHE_SYNC(&ca->sb, CACHE_SYNC(&c->sb));
 
-		bio_reset(bio);
+		bio_init(bio, ca->sb_bv, 1);
 		bio_set_dev(bio, ca->bdev);
 		bio->bi_end_io	= write_super_endio;
 		bio->bi_private = ca;
 
 		closure_get(cl);
-		__write_super(&ca->sb, bio);
+		__write_super(&ca->sb, ca->sb_disk, bio);
 	}
 
 	closure_return_with_destructor(cl, bcache_write_super_unlock);
@@ -1275,8 +1275,8 @@ static void cached_dev_free(struct closure *cl)
 
 	mutex_unlock(&bch_register_lock);
 
-	if (dc->sb_bio.bi_inline_vecs[0].bv_page)
-		put_page(bio_first_page_all(&dc->sb_bio));
+	if (dc->sb_disk)
+		put_page(virt_to_page(dc->sb_disk));
 
 	if (!IS_ERR_OR_NULL(dc->bdev))
 		blkdev_put(dc->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
@@ -1365,9 +1365,7 @@ static int register_bdev(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 	memcpy(&dc->sb, sb, sizeof(struct cache_sb));
 	dc->bdev = bdev;
 	dc->bdev->bd_holder = dc;
-
-	bio_init(&dc->sb_bio, dc->sb_bio.bi_inline_vecs, 1);
-	bio_first_bvec_all(&dc->sb_bio)->bv_page = virt_to_page(sb_disk);
+	dc->sb_disk = sb_disk;
 
 	if (cached_dev_init(dc, sb->block_size << 9))
 		goto err;
@@ -2137,8 +2135,8 @@ void bch_cache_release(struct kobject *kobj)
 	for (i = 0; i < RESERVE_NR; i++)
 		free_fifo(&ca->free[i]);
 
-	if (ca->sb_bio.bi_inline_vecs[0].bv_page)
-		put_page(bio_first_page_all(&ca->sb_bio));
+	if (ca->sb_disk)
+		put_page(virt_to_page(ca->sb_disk));
 
 	if (!IS_ERR_OR_NULL(ca->bdev))
 		blkdev_put(ca->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
@@ -2270,9 +2268,7 @@ static int register_cache(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
 	ca->bdev = bdev;
 	ca->bdev->bd_holder = ca;
-
-	bio_init(&ca->sb_bio, ca->sb_bio.bi_inline_vecs, 1);
-	bio_first_bvec_all(&ca->sb_bio)->bv_page = virt_to_page(sb_disk);
+	ca->sb_disk = sb_disk;
 
 	if (blk_queue_discard(bdev_get_queue(bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);

commit cfa0c56db9c087227988f6af2c7b9888d24a3b16
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:32 2020 +0800

    bcache: return a pointer to the on-disk sb from read_super
    
    Returning the properly typed actual data structure insteaf of the
    containing struct page will save the callers some work going
    forward.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index cf2cafef381f..b30e4c6011d2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -60,7 +60,7 @@ struct workqueue_struct *bch_journal_wq;
 /* Superblock */
 
 static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
-			      struct page **res)
+			      struct cache_sb_disk **res)
 {
 	const char *err;
 	struct cache_sb_disk *s;
@@ -191,7 +191,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	err = NULL;
 
 	get_page(bh->b_page);
-	*res = bh->b_page;
+	*res = s;
 err:
 	put_bh(bh);
 	return err;
@@ -1353,7 +1353,7 @@ static int cached_dev_init(struct cached_dev *dc, unsigned int block_size)
 
 /* Cached device - bcache superblock */
 
-static int register_bdev(struct cache_sb *sb, struct page *sb_page,
+static int register_bdev(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 				 struct block_device *bdev,
 				 struct cached_dev *dc)
 {
@@ -1367,7 +1367,7 @@ static int register_bdev(struct cache_sb *sb, struct page *sb_page,
 	dc->bdev->bd_holder = dc;
 
 	bio_init(&dc->sb_bio, dc->sb_bio.bi_inline_vecs, 1);
-	bio_first_bvec_all(&dc->sb_bio)->bv_page = sb_page;
+	bio_first_bvec_all(&dc->sb_bio)->bv_page = virt_to_page(sb_disk);
 
 	if (cached_dev_init(dc, sb->block_size << 9))
 		goto err;
@@ -2260,7 +2260,7 @@ static int cache_alloc(struct cache *ca)
 	return ret;
 }
 
-static int register_cache(struct cache_sb *sb, struct page *sb_page,
+static int register_cache(struct cache_sb *sb, struct cache_sb_disk *sb_disk,
 				struct block_device *bdev, struct cache *ca)
 {
 	const char *err = NULL; /* must be set for any error case */
@@ -2272,7 +2272,7 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	ca->bdev->bd_holder = ca;
 
 	bio_init(&ca->sb_bio, ca->sb_bio.bi_inline_vecs, 1);
-	bio_first_bvec_all(&ca->sb_bio)->bv_page = sb_page;
+	bio_first_bvec_all(&ca->sb_bio)->bv_page = virt_to_page(sb_disk);
 
 	if (blk_queue_discard(bdev_get_queue(bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
@@ -2375,8 +2375,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	const char *err;
 	char *path = NULL;
 	struct cache_sb *sb;
+	struct cache_sb_disk *sb_disk;
 	struct block_device *bdev;
-	struct page *sb_page;
 	ssize_t ret;
 
 	ret = -EBUSY;
@@ -2426,7 +2426,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (set_blocksize(bdev, 4096))
 		goto out_blkdev_put;
 
-	err = read_super(sb, bdev, &sb_page);
+	err = read_super(sb, bdev, &sb_disk);
 	if (err)
 		goto out_blkdev_put;
 
@@ -2438,7 +2438,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			goto out_put_sb_page;
 
 		mutex_lock(&bch_register_lock);
-		ret = register_bdev(sb, sb_page, bdev, dc);
+		ret = register_bdev(sb, sb_disk, bdev, dc);
 		mutex_unlock(&bch_register_lock);
 		/* blkdev_put() will be called in cached_dev_free() */
 		if (ret < 0)
@@ -2450,7 +2450,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			goto out_put_sb_page;
 
 		/* blkdev_put() will be called in bch_cache_release() */
-		if (register_cache(sb, sb_page, bdev, ca) != 0)
+		if (register_cache(sb, sb_disk, bdev, ca) != 0)
 			goto out_free_sb;
 	}
 
@@ -2461,7 +2461,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	return size;
 
 out_put_sb_page:
-	put_page(sb_page);
+	put_page(virt_to_page(sb_disk));
 out_blkdev_put:
 	blkdev_put(bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
 out_free_sb:

commit fc8f19cc5dce183cde4fecb9a1da07d81fa6ea8d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:31 2020 +0800

    bcache: transfer the sb_page reference to register_{bdev,cache}
    
    Avoid an extra reference count roundtrip by transferring the sb_page
    ownership to the lower level register helpers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fad9c6cbee5e..cf2cafef381f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1368,8 +1368,6 @@ static int register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	bio_init(&dc->sb_bio, dc->sb_bio.bi_inline_vecs, 1);
 	bio_first_bvec_all(&dc->sb_bio)->bv_page = sb_page;
-	get_page(sb_page);
-
 
 	if (cached_dev_init(dc, sb->block_size << 9))
 		goto err;
@@ -2275,7 +2273,6 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 
 	bio_init(&ca->sb_bio, ca->sb_bio.bi_inline_vecs, 1);
 	bio_first_bvec_all(&ca->sb_bio)->bv_page = sb_page;
-	get_page(sb_page);
 
 	if (blk_queue_discard(bdev_get_queue(bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
@@ -2378,7 +2375,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	const char *err;
 	char *path = NULL;
 	struct cache_sb *sb;
-	struct block_device *bdev = NULL;
+	struct block_device *bdev;
 	struct page *sb_page;
 	ssize_t ret;
 
@@ -2444,10 +2441,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		ret = register_bdev(sb, sb_page, bdev, dc);
 		mutex_unlock(&bch_register_lock);
 		/* blkdev_put() will be called in cached_dev_free() */
-		if (ret < 0) {
-			bdev = NULL;
-			goto out_put_sb_page;
-		}
+		if (ret < 0)
+			goto out_free_sb;
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
 
@@ -2455,13 +2450,10 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			goto out_put_sb_page;
 
 		/* blkdev_put() will be called in bch_cache_release() */
-		if (register_cache(sb, sb_page, bdev, ca) != 0) {
-			bdev = NULL;
-			goto out_put_sb_page;
-		}
+		if (register_cache(sb, sb_page, bdev, ca) != 0)
+			goto out_free_sb;
 	}
 
-	put_page(sb_page);
 done:
 	kfree(sb);
 	kfree(path);
@@ -2471,8 +2463,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 out_put_sb_page:
 	put_page(sb_page);
 out_blkdev_put:
-	if (bdev)
-		blkdev_put(bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
+	blkdev_put(bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
 out_free_sb:
 	kfree(sb);
 out_free_path:

commit ae3cd299919af6eb670d5af0bc9d7ba14086bd8e
Author: Coly Li <colyli@suse.de>
Date:   Fri Jan 24 01:01:30 2020 +0800

    bcache: fix use-after-free in register_bcache()
    
    The patch "bcache: rework error unwinding in register_bcache" introduces
    a use-after-free regression in register_bcache(). Here are current code,
            2510 out_free_path:
            2511         kfree(path);
            2512 out_module_put:
            2513         module_put(THIS_MODULE);
            2514 out:
            2515         pr_info("error %s: %s", path, err);
            2516         return ret;
    If some error happens and the above code path is executed, at line 2511
    path is released, but referenced at line 2515. Then KASAN reports a use-
    after-free error message.
    
    This patch changes line 2515 in the following way to fix the problem,
            2515         pr_info("error %s: %s", path?path:"", err);
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index ee7c87f38d0d..fad9c6cbee5e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2477,10 +2477,11 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	kfree(sb);
 out_free_path:
 	kfree(path);
+	path = NULL;
 out_module_put:
 	module_put(THIS_MODULE);
 out:
-	pr_info("error %s: %s", path, err);
+	pr_info("error %s: %s", path?path:"", err);
 	return ret;
 }
 

commit 29cda393bcaad160c4bf3676ddd99855adafc72f
Author: Coly Li <colyli@suse.de>
Date:   Fri Jan 24 01:01:29 2020 +0800

    bcache: properly initialize 'path' and 'err' in register_bcache()
    
    Patch "bcache: rework error unwinding in register_bcache" from
    Christoph Hellwig changes the local variables 'path' and 'err'
    in undefined initial state. If the code in register_bcache() jumps
    to label 'out:' or 'out_module_put:' by goto, these two variables
    might be reference with undefined value by the following line,
    
            out_module_put:
                    module_put(THIS_MODULE);
            out:
                    pr_info("error %s: %s", path, err);
                    return ret;
    
    Therefore this patch initializes these two local variables properly
    in register_bcache() to avoid such issue.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e8013e1b0a14..ee7c87f38d0d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2376,18 +2376,20 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size)
 {
 	const char *err;
-	char *path;
+	char *path = NULL;
 	struct cache_sb *sb;
 	struct block_device *bdev = NULL;
 	struct page *sb_page;
 	ssize_t ret;
 
 	ret = -EBUSY;
+	err = "failed to reference bcache module";
 	if (!try_module_get(THIS_MODULE))
 		goto out;
 
 	/* For latest state of bcache_is_reboot */
 	smp_mb();
+	err = "bcache is in reboot";
 	if (bcache_is_reboot)
 		goto out_module_put;
 

commit 50246693f81fe887f4db78bf7089051d7f1894cc
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:28 2020 +0800

    bcache: rework error unwinding in register_bcache
    
    Split the successful and error return path, and use one goto label for each
    resource to unwind.  This also fixes some small errors like leaking the
    module reference count in the reboot case (which seems entirely harmless)
    or printing the wrong warning messages for early failures.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3045f27e0d67..e8013e1b0a14 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2375,29 +2375,33 @@ static bool bch_is_open(struct block_device *bdev)
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size)
 {
-	ssize_t ret = -EINVAL;
-	const char *err = "cannot allocate memory";
-	char *path = NULL;
-	struct cache_sb *sb = NULL;
+	const char *err;
+	char *path;
+	struct cache_sb *sb;
 	struct block_device *bdev = NULL;
-	struct page *sb_page = NULL;
+	struct page *sb_page;
+	ssize_t ret;
 
+	ret = -EBUSY;
 	if (!try_module_get(THIS_MODULE))
-		return -EBUSY;
+		goto out;
 
 	/* For latest state of bcache_is_reboot */
 	smp_mb();
 	if (bcache_is_reboot)
-		return -EBUSY;
+		goto out_module_put;
 
+	ret = -ENOMEM;
+	err = "cannot allocate memory";
 	path = kstrndup(buffer, size, GFP_KERNEL);
 	if (!path)
-		goto err;
+		goto out_module_put;
 
 	sb = kmalloc(sizeof(struct cache_sb), GFP_KERNEL);
 	if (!sb)
-		goto err;
+		goto out_free_path;
 
+	ret = -EINVAL;
 	err = "failed to open device";
 	bdev = blkdev_get_by_path(strim(path),
 				  FMODE_READ|FMODE_WRITE|FMODE_EXCL,
@@ -2414,57 +2418,68 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			if (!IS_ERR(bdev))
 				bdput(bdev);
 			if (attr == &ksysfs_register_quiet)
-				goto quiet_out;
+				goto done;
 		}
-		goto err;
+		goto out_free_sb;
 	}
 
 	err = "failed to set blocksize";
 	if (set_blocksize(bdev, 4096))
-		goto err_close;
+		goto out_blkdev_put;
 
 	err = read_super(sb, bdev, &sb_page);
 	if (err)
-		goto err_close;
+		goto out_blkdev_put;
 
 	err = "failed to register device";
 	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
 
 		if (!dc)
-			goto err_close;
+			goto out_put_sb_page;
 
 		mutex_lock(&bch_register_lock);
 		ret = register_bdev(sb, sb_page, bdev, dc);
 		mutex_unlock(&bch_register_lock);
 		/* blkdev_put() will be called in cached_dev_free() */
-		if (ret < 0)
-			goto err;
+		if (ret < 0) {
+			bdev = NULL;
+			goto out_put_sb_page;
+		}
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
 
 		if (!ca)
-			goto err_close;
+			goto out_put_sb_page;
 
 		/* blkdev_put() will be called in bch_cache_release() */
-		if (register_cache(sb, sb_page, bdev, ca) != 0)
-			goto err;
+		if (register_cache(sb, sb_page, bdev, ca) != 0) {
+			bdev = NULL;
+			goto out_put_sb_page;
+		}
 	}
-quiet_out:
-	ret = size;
-out:
-	if (sb_page)
-		put_page(sb_page);
+
+	put_page(sb_page);
+done:
 	kfree(sb);
 	kfree(path);
 	module_put(THIS_MODULE);
-	return ret;
-
-err_close:
-	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
-err:
+	return size;
+
+out_put_sb_page:
+	put_page(sb_page);
+out_blkdev_put:
+	if (bdev)
+		blkdev_put(bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);
+out_free_sb:
+	kfree(sb);
+out_free_path:
+	kfree(path);
+out_module_put:
+	module_put(THIS_MODULE);
+out:
 	pr_info("error %s: %s", path, err);
-	goto out;
+	return ret;
 }
 
 

commit a702a692cd7559053ea573f4e2c84828f0e62824
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:27 2020 +0800

    bcache: use a separate data structure for the on-disk super block
    
    Split out an on-disk version struct cache_sb with the proper endianness
    annotations.  This fixes a fair chunk of sparse warnings, but there are
    some left due to the way the checksum is defined.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a573ce1d85aa..3045f27e0d67 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -63,14 +63,14 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 			      struct page **res)
 {
 	const char *err;
-	struct cache_sb *s;
+	struct cache_sb_disk *s;
 	struct buffer_head *bh = __bread(bdev, 1, SB_SIZE);
 	unsigned int i;
 
 	if (!bh)
 		return "IO error";
 
-	s = (struct cache_sb *) bh->b_data;
+	s = (struct cache_sb_disk *)bh->b_data;
 
 	sb->offset		= le64_to_cpu(s->offset);
 	sb->version		= le64_to_cpu(s->version);
@@ -209,7 +209,7 @@ static void write_bdev_super_endio(struct bio *bio)
 
 static void __write_super(struct cache_sb *sb, struct bio *bio)
 {
-	struct cache_sb *out = page_address(bio_first_page_all(bio));
+	struct cache_sb_disk *out = page_address(bio_first_page_all(bio));
 	unsigned int i;
 
 	bio->bi_iter.bi_sector	= SB_SECTOR;

commit e8547d42095e58bee658f00fef8e33d2a185c927
Author: Liang Chen <liangchen.linux@gmail.com>
Date:   Fri Jan 24 01:01:26 2020 +0800

    bcache: cached_dev_free needs to put the sb page
    
    Same as cache device, the buffer page needs to be put while
    freeing cached_dev.  Otherwise a page would be leaked every
    time a cached_dev is stopped.
    
    Signed-off-by: Liang Chen <liangchen.linux@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 77e9869345e7..a573ce1d85aa 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1275,6 +1275,9 @@ static void cached_dev_free(struct closure *cl)
 
 	mutex_unlock(&bch_register_lock);
 
+	if (dc->sb_bio.bi_inline_vecs[0].bv_page)
+		put_page(bio_first_page_all(&dc->sb_bio));
+
 	if (!IS_ERR_OR_NULL(dc->bdev))
 		blkdev_put(dc->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 

commit c5fcdedcee4e6ae15c0eb5e0fbe25467e57d2963
Author: Coly Li <colyli@suse.de>
Date:   Wed Nov 13 16:03:23 2019 +0800

    bcache: add idle_max_writeback_rate sysfs interface
    
    For writeback mode, if there is no regular I/O request for a while,
    the writeback rate will be set to the maximum value (1TB/s for now).
    This is good for most of the storage workload, but there are still
    people don't what the maximum writeback rate in I/O idle time.
    
    This patch adds a sysfs interface file idle_max_writeback_rate to
    permit people to disable maximum writeback rate. Then the minimum
    writeback rate can be advised by writeback_rate_minimum in the
    bcache device's sysfs interface.
    
    Reported-by: Christian Balzer <chibi@gol.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d1352fcc6ff2..77e9869345e7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1834,6 +1834,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	c->congested_read_threshold_us	= 2000;
 	c->congested_write_threshold_us	= 20000;
 	c->error_limit	= DEFAULT_IO_ERROR_LIMIT;
+	c->idle_max_writeback_rate_enabled = 1;
 	WARN_ON(test_and_clear_bit(CACHE_SET_IO_DISABLE, &c->flags));
 
 	return c;

commit 84c529aea182939e68f618ed9813740c9165c7eb
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 13 16:03:21 2019 +0800

    bcache: fix deadlock in bcache_allocator
    
    bcache_allocator can call the following:
    
     bch_allocator_thread()
      -> bch_prio_write()
         -> bch_bucket_alloc()
            -> wait on &ca->set->bucket_wait
    
    But the wake up event on bucket_wait is supposed to come from
    bch_allocator_thread() itself => deadlock:
    
    [ 1158.490744] INFO: task bcache_allocato:15861 blocked for more than 10 seconds.
    [ 1158.495929]       Not tainted 5.3.0-050300rc3-generic #201908042232
    [ 1158.500653] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [ 1158.504413] bcache_allocato D    0 15861      2 0x80004000
    [ 1158.504419] Call Trace:
    [ 1158.504429]  __schedule+0x2a8/0x670
    [ 1158.504432]  schedule+0x2d/0x90
    [ 1158.504448]  bch_bucket_alloc+0xe5/0x370 [bcache]
    [ 1158.504453]  ? wait_woken+0x80/0x80
    [ 1158.504466]  bch_prio_write+0x1dc/0x390 [bcache]
    [ 1158.504476]  bch_allocator_thread+0x233/0x490 [bcache]
    [ 1158.504491]  kthread+0x121/0x140
    [ 1158.504503]  ? invalidate_buckets+0x890/0x890 [bcache]
    [ 1158.504506]  ? kthread_park+0xb0/0xb0
    [ 1158.504510]  ret_from_fork+0x35/0x40
    
    Fix by making the call to bch_prio_write() non-blocking, so that
    bch_allocator_thread() never waits on itself.
    
    Moreover, make sure to wake up the garbage collector thread when
    bch_prio_write() is failing to allocate buckets.
    
    BugLink: https://bugs.launchpad.net/bugs/1784665
    BugLink: https://bugs.launchpad.net/bugs/1796292
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 623fdaf10c4c..d1352fcc6ff2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -530,12 +530,29 @@ static void prio_io(struct cache *ca, uint64_t bucket, int op,
 	closure_sync(cl);
 }
 
-void bch_prio_write(struct cache *ca)
+int bch_prio_write(struct cache *ca, bool wait)
 {
 	int i;
 	struct bucket *b;
 	struct closure cl;
 
+	pr_debug("free_prio=%zu, free_none=%zu, free_inc=%zu",
+		 fifo_used(&ca->free[RESERVE_PRIO]),
+		 fifo_used(&ca->free[RESERVE_NONE]),
+		 fifo_used(&ca->free_inc));
+
+	/*
+	 * Pre-check if there are enough free buckets. In the non-blocking
+	 * scenario it's better to fail early rather than starting to allocate
+	 * buckets and do a cleanup later in case of failure.
+	 */
+	if (!wait) {
+		size_t avail = fifo_used(&ca->free[RESERVE_PRIO]) +
+			       fifo_used(&ca->free[RESERVE_NONE]);
+		if (prio_buckets(ca) > avail)
+			return -ENOMEM;
+	}
+
 	closure_init_stack(&cl);
 
 	lockdep_assert_held(&ca->set->bucket_lock);
@@ -545,9 +562,6 @@ void bch_prio_write(struct cache *ca)
 	atomic_long_add(ca->sb.bucket_size * prio_buckets(ca),
 			&ca->meta_sectors_written);
 
-	//pr_debug("free %zu, free_inc %zu, unused %zu", fifo_used(&ca->free),
-	//	 fifo_used(&ca->free_inc), fifo_used(&ca->unused));
-
 	for (i = prio_buckets(ca) - 1; i >= 0; --i) {
 		long bucket;
 		struct prio_set *p = ca->disk_buckets;
@@ -565,7 +579,7 @@ void bch_prio_write(struct cache *ca)
 		p->magic	= pset_magic(&ca->sb);
 		p->csum		= bch_crc64(&p->magic, bucket_bytes(ca) - 8);
 
-		bucket = bch_bucket_alloc(ca, RESERVE_PRIO, true);
+		bucket = bch_bucket_alloc(ca, RESERVE_PRIO, wait);
 		BUG_ON(bucket == -1);
 
 		mutex_unlock(&ca->set->bucket_lock);
@@ -594,6 +608,7 @@ void bch_prio_write(struct cache *ca)
 
 		ca->prio_last_buckets[i] = ca->prio_buckets[i];
 	}
+	return 0;
 }
 
 static void prio_read(struct cache *ca, uint64_t bucket)
@@ -1964,7 +1979,7 @@ static int run_cache_set(struct cache_set *c)
 
 		mutex_lock(&c->bucket_lock);
 		for_each_cache(ca, c, i)
-			bch_prio_write(ca);
+			bch_prio_write(ca, true);
 		mutex_unlock(&c->bucket_lock);
 
 		err = "cannot allocate new UUID bucket";

commit aaf8dbeab5865720c66db60ae8329309e81a0c9c
Author: Coly Li <colyli@suse.de>
Date:   Wed Nov 13 16:03:18 2019 +0800

    bcache: add more accurate error messages in read_super()
    
    Previous code only returns "Not a bcache superblock" for both bcache
    super block offset and magic error. This patch addss more accurate error
    messages,
    - for super block unmatched offset:
      "Not a bcache superblock (bad offset)"
    - for super block unmatched magic number:
      "Not a bcache superblock (bad magic)"
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7beccede5360..623fdaf10c4c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -92,10 +92,11 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	pr_debug("read sb version %llu, flags %llu, seq %llu, journal size %u",
 		 sb->version, sb->flags, sb->seq, sb->keys);
 
-	err = "Not a bcache superblock";
+	err = "Not a bcache superblock (bad offset)";
 	if (sb->offset != SB_SECTOR)
 		goto err;
 
+	err = "Not a bcache superblock (bad magic)";
 	if (memcmp(sb->magic, bcache_magic, 16))
 		goto err;
 

commit 2d8869518a525c9bce5f5268419df9dfbe3dfdeb
Author: Coly Li <colyli@suse.de>
Date:   Wed Nov 13 16:03:17 2019 +0800

    bcache: fix static checker warning in bcache_device_free()
    
    Commit cafe56359144 ("bcache: A block layer cache") leads to the
    following static checker warning:
    
        ./drivers/md/bcache/super.c:770 bcache_device_free()
        warn: variable dereferenced before check 'd->disk' (see line 766)
    
    drivers/md/bcache/super.c
       762  static void bcache_device_free(struct bcache_device *d)
       763  {
       764          lockdep_assert_held(&bch_register_lock);
       765
       766          pr_info("%s stopped", d->disk->disk_name);
                                          ^^^^^^^^^
    Unchecked dereference.
    
       767
       768          if (d->c)
       769                  bcache_device_detach(d);
       770          if (d->disk && d->disk->flags & GENHD_FL_UP)
                        ^^^^^^^
    Check too late.
    
       771                  del_gendisk(d->disk);
       772          if (d->disk && d->disk->queue)
       773                  blk_cleanup_queue(d->disk->queue);
       774          if (d->disk) {
       775                  ida_simple_remove(&bcache_device_idx,
       776                                    first_minor_to_idx(d->disk->first_minor));
       777                  put_disk(d->disk);
       778          }
       779
    
    It is not 100% sure that the gendisk struct of bcache device will always
    be there, the warning makes sense when there is problem in block core.
    
    This patch tries to remove the static checking warning by checking
    d->disk to avoid NULL pointer deferences.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index ebb854ed05a4..7beccede5360 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -761,20 +761,28 @@ static inline int idx_to_first_minor(int idx)
 
 static void bcache_device_free(struct bcache_device *d)
 {
+	struct gendisk *disk = d->disk;
+
 	lockdep_assert_held(&bch_register_lock);
 
-	pr_info("%s stopped", d->disk->disk_name);
+	if (disk)
+		pr_info("%s stopped", disk->disk_name);
+	else
+		pr_err("bcache device (NULL gendisk) stopped");
 
 	if (d->c)
 		bcache_device_detach(d);
-	if (d->disk && d->disk->flags & GENHD_FL_UP)
-		del_gendisk(d->disk);
-	if (d->disk && d->disk->queue)
-		blk_cleanup_queue(d->disk->queue);
-	if (d->disk) {
+
+	if (disk) {
+		if (disk->flags & GENHD_FL_UP)
+			del_gendisk(disk);
+
+		if (disk->queue)
+			blk_cleanup_queue(disk->queue);
+
 		ida_simple_remove(&bcache_device_idx,
-				  first_minor_to_idx(d->disk->first_minor));
-		put_disk(d->disk);
+				  first_minor_to_idx(disk->first_minor));
+		put_disk(disk);
 	}
 
 	bioset_exit(&d->bio_split);

commit 34cf78bf34d48dddddfeeadb44f9841d7864997a
Author: Guoju Fang <fangguoju@gmail.com>
Date:   Wed Nov 13 16:03:16 2019 +0800

    bcache: fix a lost wake-up problem caused by mca_cannibalize_lock
    
    This patch fix a lost wake-up problem caused by the race between
    mca_cannibalize_lock and bch_cannibalize_unlock.
    
    Consider two processes, A and B. Process A is executing
    mca_cannibalize_lock, while process B takes c->btree_cache_alloc_lock
    and is executing bch_cannibalize_unlock. The problem happens that after
    process A executes cmpxchg and will execute prepare_to_wait. In this
    timeslice process B executes wake_up, but after that process A executes
    prepare_to_wait and set the state to TASK_INTERRUPTIBLE. Then process A
    goes to sleep but no one will wake up it. This problem may cause bcache
    device to dead.
    
    Signed-off-by: Guoju Fang <fangguoju@gmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 20ed838e9413..ebb854ed05a4 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1769,6 +1769,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	sema_init(&c->sb_write_mutex, 1);
 	mutex_init(&c->bucket_lock);
 	init_waitqueue_head(&c->btree_cache_wait);
+	spin_lock_init(&c->btree_cannibalize_lock);
 	init_waitqueue_head(&c->bucket_wait);
 	init_waitqueue_head(&c->gc_wait);
 	sema_init(&c->uuid_write_mutex, 1);

commit 5d9e06d60eee95e021ffccf0d2c7ed800ae9dc14
Author: Wei Yongjun <weiyongjun1@huawei.com>
Date:   Mon Jul 22 22:12:36 2019 +0800

    bcache: fix possible memory leak in bch_cached_dev_run()
    
    memory malloced in bch_cached_dev_run() and should be freed before
    leaving from the error handling cases, otherwise it will cause
    memory leak.
    
    Fixes: 0b13efecf5f2 ("bcache: add return value check to bch_cached_dev_run()")
    Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 26e374fbf57c..20ed838e9413 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -931,6 +931,9 @@ int bch_cached_dev_run(struct cached_dev *dc)
 	if (dc->io_disable) {
 		pr_err("I/O disabled on cached dev %s",
 		       dc->backing_dev_name);
+		kfree(env[1]);
+		kfree(env[2]);
+		kfree(buf);
 		return -EIO;
 	}
 

commit 1df3877ff6a4810054237c3259d900ded4468969
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:52 2019 +0800

    bcache: shrink btree node cache after bch_btree_check()
    
    When cache set starts, bch_btree_check() will check all bkeys on cache
    device by calculating the checksum. This operation will consume a huge
    number of system memory if there are a lot of data cached. Since bcache
    uses its own mca cache to maintain all its read-in btree nodes, and only
    releases the cache space when system memory manage code starts to shrink
    caches. Then before memory manager code to call the mca cache shrinker
    callback, bcache mca cache will compete memory resource with user space
    application, which may have nagive effect to performance of user space
    workloads (e.g. data base, or I/O service of distributed storage node).
    
    This patch tries to call bcache mca shrinker routine to proactively
    release mca cache memory, to decrease the memory pressure of system and
    avoid negative effort of the overall system I/O performance.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a8ea4e2086a9..26e374fbf57c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1880,6 +1880,23 @@ static int run_cache_set(struct cache_set *c)
 		if (bch_btree_check(c))
 			goto err;
 
+		/*
+		 * bch_btree_check() may occupy too much system memory which
+		 * has negative effects to user space application (e.g. data
+		 * base) performance. Shrink the mca cache memory proactively
+		 * here to avoid competing memory with user space workloads..
+		 */
+		if (!c->shrinker_disabled) {
+			struct shrink_control sc;
+
+			sc.gfp_mask = GFP_KERNEL;
+			sc.nr_to_scan = c->btree_cache_used * c->btree_pages;
+			/* first run to clear b->accessed tag */
+			c->shrink.scan_objects(&c->shrink, &sc);
+			/* second run to reap non-accessed nodes */
+			c->shrink.scan_objects(&c->shrink, &sc);
+		}
+
 		bch_journal_mark(c, &journal);
 		bch_initial_gc_finish(c);
 		pr_debug("btree_check() done");

commit 7e865eba00a3df2dc8c4746173a8ca1c1c7f042e
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:49 2019 +0800

    bcache: fix potential deadlock in cached_def_free()
    
    When enable lockdep and reboot system with a writeback mode bcache
    device, the following potential deadlock warning is reported by lockdep
    engine.
    
    [  101.536569][  T401] kworker/2:2/401 is trying to acquire lock:
    [  101.538575][  T401] 00000000bbf6e6c7 ((wq_completion)bcache_writeback_wq){+.+.}, at: flush_workqueue+0x87/0x4c0
    [  101.542054][  T401]
    [  101.542054][  T401] but task is already holding lock:
    [  101.544587][  T401] 00000000f5f305b3 ((work_completion)(&cl->work)#2){+.+.}, at: process_one_work+0x21e/0x640
    [  101.548386][  T401]
    [  101.548386][  T401] which lock already depends on the new lock.
    [  101.548386][  T401]
    [  101.551874][  T401]
    [  101.551874][  T401] the existing dependency chain (in reverse order) is:
    [  101.555000][  T401]
    [  101.555000][  T401] -> #1 ((work_completion)(&cl->work)#2){+.+.}:
    [  101.557860][  T401]        process_one_work+0x277/0x640
    [  101.559661][  T401]        worker_thread+0x39/0x3f0
    [  101.561340][  T401]        kthread+0x125/0x140
    [  101.562963][  T401]        ret_from_fork+0x3a/0x50
    [  101.564718][  T401]
    [  101.564718][  T401] -> #0 ((wq_completion)bcache_writeback_wq){+.+.}:
    [  101.567701][  T401]        lock_acquire+0xb4/0x1c0
    [  101.569651][  T401]        flush_workqueue+0xae/0x4c0
    [  101.571494][  T401]        drain_workqueue+0xa9/0x180
    [  101.573234][  T401]        destroy_workqueue+0x17/0x250
    [  101.575109][  T401]        cached_dev_free+0x44/0x120 [bcache]
    [  101.577304][  T401]        process_one_work+0x2a4/0x640
    [  101.579357][  T401]        worker_thread+0x39/0x3f0
    [  101.581055][  T401]        kthread+0x125/0x140
    [  101.582709][  T401]        ret_from_fork+0x3a/0x50
    [  101.584592][  T401]
    [  101.584592][  T401] other info that might help us debug this:
    [  101.584592][  T401]
    [  101.588355][  T401]  Possible unsafe locking scenario:
    [  101.588355][  T401]
    [  101.590974][  T401]        CPU0                    CPU1
    [  101.592889][  T401]        ----                    ----
    [  101.594743][  T401]   lock((work_completion)(&cl->work)#2);
    [  101.596785][  T401]                                lock((wq_completion)bcache_writeback_wq);
    [  101.600072][  T401]                                lock((work_completion)(&cl->work)#2);
    [  101.602971][  T401]   lock((wq_completion)bcache_writeback_wq);
    [  101.605255][  T401]
    [  101.605255][  T401]  *** DEADLOCK ***
    [  101.605255][  T401]
    [  101.608310][  T401] 2 locks held by kworker/2:2/401:
    [  101.610208][  T401]  #0: 00000000cf2c7d17 ((wq_completion)events){+.+.}, at: process_one_work+0x21e/0x640
    [  101.613709][  T401]  #1: 00000000f5f305b3 ((work_completion)(&cl->work)#2){+.+.}, at: process_one_work+0x21e/0x640
    [  101.617480][  T401]
    [  101.617480][  T401] stack backtrace:
    [  101.619539][  T401] CPU: 2 PID: 401 Comm: kworker/2:2 Tainted: G        W         5.2.0-rc4-lp151.20-default+ #1
    [  101.623225][  T401] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 04/13/2018
    [  101.627210][  T401] Workqueue: events cached_dev_free [bcache]
    [  101.629239][  T401] Call Trace:
    [  101.630360][  T401]  dump_stack+0x85/0xcb
    [  101.631777][  T401]  print_circular_bug+0x19a/0x1f0
    [  101.633485][  T401]  __lock_acquire+0x16cd/0x1850
    [  101.635184][  T401]  ? __lock_acquire+0x6a8/0x1850
    [  101.636863][  T401]  ? lock_acquire+0xb4/0x1c0
    [  101.638421][  T401]  ? find_held_lock+0x34/0xa0
    [  101.640015][  T401]  lock_acquire+0xb4/0x1c0
    [  101.641513][  T401]  ? flush_workqueue+0x87/0x4c0
    [  101.643248][  T401]  flush_workqueue+0xae/0x4c0
    [  101.644832][  T401]  ? flush_workqueue+0x87/0x4c0
    [  101.646476][  T401]  ? drain_workqueue+0xa9/0x180
    [  101.648303][  T401]  drain_workqueue+0xa9/0x180
    [  101.649867][  T401]  destroy_workqueue+0x17/0x250
    [  101.651503][  T401]  cached_dev_free+0x44/0x120 [bcache]
    [  101.653328][  T401]  process_one_work+0x2a4/0x640
    [  101.655029][  T401]  worker_thread+0x39/0x3f0
    [  101.656693][  T401]  ? process_one_work+0x640/0x640
    [  101.658501][  T401]  kthread+0x125/0x140
    [  101.660012][  T401]  ? kthread_create_worker_on_cpu+0x70/0x70
    [  101.661985][  T401]  ret_from_fork+0x3a/0x50
    [  101.691318][  T401] bcache: bcache_device_free() bcache0 stopped
    
    Here is how the above potential deadlock may happen in reboot/shutdown
    code path,
    1) bcache_reboot() is called firstly in the reboot/shutdown code path,
       then in bcache_reboot(), bcache_device_stop() is called.
    2) bcache_device_stop() sets BCACHE_DEV_CLOSING on d->falgs, then call
       closure_queue(&d->cl) to invoke cached_dev_flush(). And in turn
       cached_dev_flush() calls cached_dev_free() via closure_at()
    3) In cached_dev_free(), after stopped writebach kthread
       dc->writeback_thread, the kwork dc->writeback_write_wq is stopping by
       destroy_workqueue().
    4) Inside destroy_workqueue(), drain_workqueue() is called. Inside
       drain_workqueue(), flush_workqueue() is called. Then wq->lockdep_map
       is acquired by lock_map_acquire() in flush_workqueue(). After the
       lock acquired the rest part of flush_workqueue() just wait for the
       workqueue to complete.
    5) Now we look back at writeback thread routine bch_writeback_thread(),
       in the main while-loop, write_dirty() is called via continue_at() in
       read_dirty_submit(), which is called via continue_at() in while-loop
       level called function read_dirty(). Inside write_dirty() it may be
       re-called on workqueeu dc->writeback_write_wq via continue_at().
       It means when the writeback kthread is stopped in cached_dev_free()
       there might be still one kworker queued on dc->writeback_write_wq
       to execute write_dirty() again.
    6) Now this kworker is scheduled on dc->writeback_write_wq to run by
       process_one_work() (which is called by worker_thread()). Before
       calling the kwork routine, wq->lockdep_map is acquired.
    7) But wq->lockdep_map is acquired already in step 4), so a A-A lock
       (lockdep terminology) scenario happens.
    
    Indeed on multiple cores syatem, the above deadlock is very rare to
    happen, just as the code comments in process_one_work() says,
    2263     * AFAICT there is no possible deadlock scenario between the
    2264     * flush_work() and complete() primitives (except for
               single-threaded
    2265     * workqueues), so hiding them isn't a problem.
    
    But it is still good to fix such lockdep warning, even no one running
    bcache on single core system.
    
    The fix is simple. This patch solves the above potential deadlock by,
    - Do not destroy workqueue dc->writeback_write_wq in cached_dev_free().
    - Flush and destroy dc->writeback_write_wq in writebach kthread routine
      bch_writeback_thread(), where after quit the thread main while-loop
      and before cached_dev_put() is called.
    
    By this fix, dc->writeback_write_wq will be stopped and destroy before
    the writeback kthread stopped, so the chance for a A-A locking on
    wq->lockdep_map is disappeared, such A-A deadlock won't happen
    any more.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8a12a8313367..a8ea4e2086a9 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1236,8 +1236,6 @@ static void cached_dev_free(struct closure *cl)
 
 	if (!IS_ERR_OR_NULL(dc->writeback_thread))
 		kthread_stop(dc->writeback_thread);
-	if (dc->writeback_write_wq)
-		destroy_workqueue(dc->writeback_write_wq);
 	if (!IS_ERR_OR_NULL(dc->status_update_thread))
 		kthread_stop(dc->status_update_thread);
 

commit 80265d8dfd77792e133793cef44a21323aac2908
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:48 2019 +0800

    bcache: acquire bch_register_lock later in cached_dev_free()
    
    When enable lockdep engine, a lockdep warning can be observed when
    reboot or shutdown system,
    
    [ 3142.764557][    T1] bcache: bcache_reboot() Stopping all devices:
    [ 3142.776265][ T2649]
    [ 3142.777159][ T2649] ======================================================
    [ 3142.780039][ T2649] WARNING: possible circular locking dependency detected
    [ 3142.782869][ T2649] 5.2.0-rc4-lp151.20-default+ #1 Tainted: G        W
    [ 3142.785684][ T2649] ------------------------------------------------------
    [ 3142.788479][ T2649] kworker/3:67/2649 is trying to acquire lock:
    [ 3142.790738][ T2649] 00000000aaf02291 ((wq_completion)bcache_writeback_wq){+.+.}, at: flush_workqueue+0x87/0x4c0
    [ 3142.794678][ T2649]
    [ 3142.794678][ T2649] but task is already holding lock:
    [ 3142.797402][ T2649] 000000004fcf89c5 (&bch_register_lock){+.+.}, at: cached_dev_free+0x17/0x120 [bcache]
    [ 3142.801462][ T2649]
    [ 3142.801462][ T2649] which lock already depends on the new lock.
    [ 3142.801462][ T2649]
    [ 3142.805277][ T2649]
    [ 3142.805277][ T2649] the existing dependency chain (in reverse order) is:
    [ 3142.808902][ T2649]
    [ 3142.808902][ T2649] -> #2 (&bch_register_lock){+.+.}:
    [ 3142.812396][ T2649]        __mutex_lock+0x7a/0x9d0
    [ 3142.814184][ T2649]        cached_dev_free+0x17/0x120 [bcache]
    [ 3142.816415][ T2649]        process_one_work+0x2a4/0x640
    [ 3142.818413][ T2649]        worker_thread+0x39/0x3f0
    [ 3142.820276][ T2649]        kthread+0x125/0x140
    [ 3142.822061][ T2649]        ret_from_fork+0x3a/0x50
    [ 3142.823965][ T2649]
    [ 3142.823965][ T2649] -> #1 ((work_completion)(&cl->work)#2){+.+.}:
    [ 3142.827244][ T2649]        process_one_work+0x277/0x640
    [ 3142.829160][ T2649]        worker_thread+0x39/0x3f0
    [ 3142.830958][ T2649]        kthread+0x125/0x140
    [ 3142.832674][ T2649]        ret_from_fork+0x3a/0x50
    [ 3142.834915][ T2649]
    [ 3142.834915][ T2649] -> #0 ((wq_completion)bcache_writeback_wq){+.+.}:
    [ 3142.838121][ T2649]        lock_acquire+0xb4/0x1c0
    [ 3142.840025][ T2649]        flush_workqueue+0xae/0x4c0
    [ 3142.842035][ T2649]        drain_workqueue+0xa9/0x180
    [ 3142.844042][ T2649]        destroy_workqueue+0x17/0x250
    [ 3142.846142][ T2649]        cached_dev_free+0x52/0x120 [bcache]
    [ 3142.848530][ T2649]        process_one_work+0x2a4/0x640
    [ 3142.850663][ T2649]        worker_thread+0x39/0x3f0
    [ 3142.852464][ T2649]        kthread+0x125/0x140
    [ 3142.854106][ T2649]        ret_from_fork+0x3a/0x50
    [ 3142.855880][ T2649]
    [ 3142.855880][ T2649] other info that might help us debug this:
    [ 3142.855880][ T2649]
    [ 3142.859663][ T2649] Chain exists of:
    [ 3142.859663][ T2649]   (wq_completion)bcache_writeback_wq --> (work_completion)(&cl->work)#2 --> &bch_register_lock
    [ 3142.859663][ T2649]
    [ 3142.865424][ T2649]  Possible unsafe locking scenario:
    [ 3142.865424][ T2649]
    [ 3142.868022][ T2649]        CPU0                    CPU1
    [ 3142.869885][ T2649]        ----                    ----
    [ 3142.871751][ T2649]   lock(&bch_register_lock);
    [ 3142.873379][ T2649]                                lock((work_completion)(&cl->work)#2);
    [ 3142.876399][ T2649]                                lock(&bch_register_lock);
    [ 3142.879727][ T2649]   lock((wq_completion)bcache_writeback_wq);
    [ 3142.882064][ T2649]
    [ 3142.882064][ T2649]  *** DEADLOCK ***
    [ 3142.882064][ T2649]
    [ 3142.885060][ T2649] 3 locks held by kworker/3:67/2649:
    [ 3142.887245][ T2649]  #0: 00000000e774cdd0 ((wq_completion)events){+.+.}, at: process_one_work+0x21e/0x640
    [ 3142.890815][ T2649]  #1: 00000000f7df89da ((work_completion)(&cl->work)#2){+.+.}, at: process_one_work+0x21e/0x640
    [ 3142.894884][ T2649]  #2: 000000004fcf89c5 (&bch_register_lock){+.+.}, at: cached_dev_free+0x17/0x120 [bcache]
    [ 3142.898797][ T2649]
    [ 3142.898797][ T2649] stack backtrace:
    [ 3142.900961][ T2649] CPU: 3 PID: 2649 Comm: kworker/3:67 Tainted: G        W         5.2.0-rc4-lp151.20-default+ #1
    [ 3142.904789][ T2649] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 04/13/2018
    [ 3142.909168][ T2649] Workqueue: events cached_dev_free [bcache]
    [ 3142.911422][ T2649] Call Trace:
    [ 3142.912656][ T2649]  dump_stack+0x85/0xcb
    [ 3142.914181][ T2649]  print_circular_bug+0x19a/0x1f0
    [ 3142.916193][ T2649]  __lock_acquire+0x16cd/0x1850
    [ 3142.917936][ T2649]  ? __lock_acquire+0x6a8/0x1850
    [ 3142.919704][ T2649]  ? lock_acquire+0xb4/0x1c0
    [ 3142.921335][ T2649]  ? find_held_lock+0x34/0xa0
    [ 3142.923052][ T2649]  lock_acquire+0xb4/0x1c0
    [ 3142.924635][ T2649]  ? flush_workqueue+0x87/0x4c0
    [ 3142.926375][ T2649]  flush_workqueue+0xae/0x4c0
    [ 3142.928047][ T2649]  ? flush_workqueue+0x87/0x4c0
    [ 3142.929824][ T2649]  ? drain_workqueue+0xa9/0x180
    [ 3142.931686][ T2649]  drain_workqueue+0xa9/0x180
    [ 3142.933534][ T2649]  destroy_workqueue+0x17/0x250
    [ 3142.935787][ T2649]  cached_dev_free+0x52/0x120 [bcache]
    [ 3142.937795][ T2649]  process_one_work+0x2a4/0x640
    [ 3142.939803][ T2649]  worker_thread+0x39/0x3f0
    [ 3142.941487][ T2649]  ? process_one_work+0x640/0x640
    [ 3142.943389][ T2649]  kthread+0x125/0x140
    [ 3142.944894][ T2649]  ? kthread_create_worker_on_cpu+0x70/0x70
    [ 3142.947744][ T2649]  ret_from_fork+0x3a/0x50
    [ 3142.970358][ T2649] bcache: bcache_device_free() bcache0 stopped
    
    Here is how the deadlock happens.
    1) bcache_reboot() calls bcache_device_stop(), then inside
       bcache_device_stop() BCACHE_DEV_CLOSING bit is set on d->flags.
       Then closure_queue(&d->cl) is called to invoke cached_dev_flush().
    2) In cached_dev_flush(), cached_dev_free() is called by continu_at().
    3) In cached_dev_free(), when stopping the writeback kthread of the
       cached device by kthread_stop(), dc->writeback_thread will be waken
       up to quite the kthread while-loop, then cached_dev_put() is called
       in bch_writeback_thread().
    4) Calling cached_dev_put() in writeback kthread may drop dc->count to
       0, then dc->detach kworker is scheduled, which is initialized as
       cached_dev_detach_finish().
    5) Inside cached_dev_detach_finish(), the last line of code is to call
       closure_put(&dc->disk.cl), which drops the last reference counter of
       closrure dc->disk.cl, then the callback cached_dev_flush() gets
       called.
    Now cached_dev_flush() is called for second time in the code path, the
    first time is in step 2). And again bch_register_lock will be acquired
    again, and a A-A lock (lockdep terminology) is happening.
    
    The root cause of the above A-A lock is in cached_dev_free(), mutex
    bch_register_lock is held before stopping writeback kthread and other
    kworkers. Fortunately now we have variable 'bcache_is_reboot', which may
    prevent device registration or unregistration during reboot/shutdown
    time, so it is unncessary to hold bch_register_lock such early now.
    
    This is how this patch fixes the reboot/shutdown time A-A lock issue:
    After moving mutex_lock(&bch_register_lock) to a later location where
    before atomic_read(&dc->running) in cached_dev_free(), such A-A lock
    problem can be solved without any reboot time registration race.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 40d857e690f9..8a12a8313367 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1231,8 +1231,6 @@ static void cached_dev_free(struct closure *cl)
 {
 	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
 
-	mutex_lock(&bch_register_lock);
-
 	if (test_and_clear_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags))
 		cancel_writeback_rate_update_dwork(dc);
 
@@ -1243,6 +1241,8 @@ static void cached_dev_free(struct closure *cl)
 	if (!IS_ERR_OR_NULL(dc->status_update_thread))
 		kthread_stop(dc->status_update_thread);
 
+	mutex_lock(&bch_register_lock);
+
 	if (atomic_read(&dc->running))
 		bd_unlink_disk_holder(dc->bdev, dc->disk.disk);
 	bcache_device_free(&dc->disk);

commit 97ba3b816e2cdea798398bc8486125f79f2453c1
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:47 2019 +0800

    bcache: acquire bch_register_lock later in cached_dev_detach_finish()
    
    Now there is variable bcache_is_reboot to prevent device register or
    unregister during reboot, it is unncessary to still hold mutex lock
    bch_register_lock before stopping writeback_rate_update kworker and
    writeback kthread. And if the stopping kworker or kthread holding
    bch_register_lock inside their routine (we used to have such problem
    in writeback thread, thanks to Junhui Wang fixed it), it is very easy
    to introduce deadlock during reboot/shutdown procedure.
    
    Therefore in this patch, the location to acquire bch_register_lock is
    moved to the location before calling calc_cached_dev_sectors(). Which
    is later then original location in cached_dev_detach_finish().
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a88238ad5da1..40d857e690f9 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1018,7 +1018,6 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	BUG_ON(!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags));
 	BUG_ON(refcount_read(&dc->count));
 
-	mutex_lock(&bch_register_lock);
 
 	if (test_and_clear_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags))
 		cancel_writeback_rate_update_dwork(dc);
@@ -1034,6 +1033,8 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	bch_write_bdev_super(dc, &cl);
 	closure_sync(&cl);
 
+	mutex_lock(&bch_register_lock);
+
 	calc_cached_dev_sectors(dc->disk.c);
 	bcache_device_detach(&dc->disk);
 	list_move(&dc->list, &uncached_devices);

commit a59ff6ccc2bf2e2934b31bbf734f0bc04b5ec78a
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:46 2019 +0800

    bcache: avoid a deadlock in bcache_reboot()
    
    It is quite frequently to observe deadlock in bcache_reboot() happens
    and hang the system reboot process. The reason is, in bcache_reboot()
    when calling bch_cache_set_stop() and bcache_device_stop() the mutex
    bch_register_lock is held. But in the process to stop cache set and
    bcache device, bch_register_lock will be acquired again. If this mutex
    is held here, deadlock will happen inside the stopping process. The
    aftermath of the deadlock is, whole system reboot gets hung.
    
    The fix is to avoid holding bch_register_lock for the following loops
    in bcache_reboot(),
           list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
                    bch_cache_set_stop(c);
    
            list_for_each_entry_safe(dc, tdc, &uncached_devices, list)
                    bcache_device_stop(&dc->disk);
    
    A module range variable 'bcache_is_reboot' is added, it sets to true
    in bcache_reboot(). In register_bcache(), if bcache_is_reboot is checked
    to be true, reject the registration by returning -EBUSY immediately.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 791cb930b353..a88238ad5da1 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -40,6 +40,7 @@ static const char invalid_uuid[] = {
 
 static struct kobject *bcache_kobj;
 struct mutex bch_register_lock;
+bool bcache_is_reboot;
 LIST_HEAD(bch_cache_sets);
 static LIST_HEAD(uncached_devices);
 
@@ -49,6 +50,7 @@ static wait_queue_head_t unregister_wait;
 struct workqueue_struct *bcache_wq;
 struct workqueue_struct *bch_journal_wq;
 
+
 #define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
 /* limitation of partitions number on single bcache device */
 #define BCACHE_MINORS		128
@@ -2335,6 +2337,11 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (!try_module_get(THIS_MODULE))
 		return -EBUSY;
 
+	/* For latest state of bcache_is_reboot */
+	smp_mb();
+	if (bcache_is_reboot)
+		return -EBUSY;
+
 	path = kstrndup(buffer, size, GFP_KERNEL);
 	if (!path)
 		goto err;
@@ -2464,6 +2471,9 @@ static ssize_t bch_pending_bdevs_cleanup(struct kobject *k,
 
 static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 {
+	if (bcache_is_reboot)
+		return NOTIFY_DONE;
+
 	if (code == SYS_DOWN ||
 	    code == SYS_HALT ||
 	    code == SYS_POWER_OFF) {
@@ -2476,19 +2486,45 @@ static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 
 		mutex_lock(&bch_register_lock);
 
+		if (bcache_is_reboot)
+			goto out;
+
+		/* New registration is rejected since now */
+		bcache_is_reboot = true;
+		/*
+		 * Make registering caller (if there is) on other CPU
+		 * core know bcache_is_reboot set to true earlier
+		 */
+		smp_mb();
+
 		if (list_empty(&bch_cache_sets) &&
 		    list_empty(&uncached_devices))
 			goto out;
 
+		mutex_unlock(&bch_register_lock);
+
 		pr_info("Stopping all devices:");
 
+		/*
+		 * The reason bch_register_lock is not held to call
+		 * bch_cache_set_stop() and bcache_device_stop() is to
+		 * avoid potential deadlock during reboot, because cache
+		 * set or bcache device stopping process will acqurie
+		 * bch_register_lock too.
+		 *
+		 * We are safe here because bcache_is_reboot sets to
+		 * true already, register_bcache() will reject new
+		 * registration now. bcache_is_reboot also makes sure
+		 * bcache_reboot() won't be re-entered on by other thread,
+		 * so there is no race in following list iteration by
+		 * list_for_each_entry_safe().
+		 */
 		list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
 			bch_cache_set_stop(c);
 
 		list_for_each_entry_safe(dc, tdc, &uncached_devices, list)
 			bcache_device_stop(&dc->disk);
 
-		mutex_unlock(&bch_register_lock);
 
 		/*
 		 * Give an early chance for other kthreads and
@@ -2616,6 +2652,8 @@ static int __init bcache_init(void)
 	bch_debug_init();
 	closure_debug_init();
 
+	bcache_is_reboot = false;
+
 	return 0;
 err:
 	bcache_exit();

commit 5c2a634cbfaf1971cb6453fe5f86d83585257790
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:45 2019 +0800

    bcache: stop writeback kthread and kworker when bch_cached_dev_run() failed
    
    In bch_cached_dev_attach() after bch_cached_dev_writeback_start()
    called, the wrireback kthread and writeback rate update kworker of the
    cached device are created, if the following bch_cached_dev_run()
    failed, bch_cached_dev_attach() will return with -ENOMEM without
    stopping the writeback related kthread and kworker.
    
    This patch stops writeback kthread and writeback rate update kworker
    before returning -ENOMEM if bch_cached_dev_run() returns error.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c4c4b2d99dc2..791cb930b353 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1189,6 +1189,14 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	ret = bch_cached_dev_run(dc);
 	if (ret && (ret != -EBUSY)) {
 		up_write(&dc->writeback_lock);
+		/*
+		 * bch_register_lock is held, bcache_device_stop() is not
+		 * able to be directly called. The kthread and kworker
+		 * created previously in bch_cached_dev_writeback_start()
+		 * have to be stopped manually here.
+		 */
+		kthread_stop(dc->writeback_thread);
+		cancel_writeback_rate_update_dwork(dc);
 		pr_err("Couldn't run cached device %s",
 		       dc->backing_dev_name);
 		return ret;

commit 0c277e211aae056b26513358fc060291d8523747
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:42 2019 +0800

    bcache: add pendings_cleanup to stop pending bcache device
    
    If a bcache device is in dirty state and its cache set is not
    registered, this bcache device will not appear in /dev/bcache<N>,
    and there is no way to stop it or remove the bcache kernel module.
    
    This is an as-designed behavior, but sometimes people has to reboot
    whole system to release or stop the pending backing device.
    
    This sysfs interface may remove such pending bcache devices when
    write anything into the sysfs file manually.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c53fe0f1629f..c4c4b2d99dc2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2273,9 +2273,13 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size);
+static ssize_t bch_pending_bdevs_cleanup(struct kobject *k,
+					 struct kobj_attribute *attr,
+					 const char *buffer, size_t size);
 
 kobj_attribute_write(register,		register_bcache);
 kobj_attribute_write(register_quiet,	register_bcache);
+kobj_attribute_write(pendings_cleanup,	bch_pending_bdevs_cleanup);
 
 static bool bch_is_open_backing(struct block_device *bdev)
 {
@@ -2400,6 +2404,56 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	goto out;
 }
 
+
+struct pdev {
+	struct list_head list;
+	struct cached_dev *dc;
+};
+
+static ssize_t bch_pending_bdevs_cleanup(struct kobject *k,
+					 struct kobj_attribute *attr,
+					 const char *buffer,
+					 size_t size)
+{
+	LIST_HEAD(pending_devs);
+	ssize_t ret = size;
+	struct cached_dev *dc, *tdc;
+	struct pdev *pdev, *tpdev;
+	struct cache_set *c, *tc;
+
+	mutex_lock(&bch_register_lock);
+	list_for_each_entry_safe(dc, tdc, &uncached_devices, list) {
+		pdev = kmalloc(sizeof(struct pdev), GFP_KERNEL);
+		if (!pdev)
+			break;
+		pdev->dc = dc;
+		list_add(&pdev->list, &pending_devs);
+	}
+
+	list_for_each_entry_safe(pdev, tpdev, &pending_devs, list) {
+		list_for_each_entry_safe(c, tc, &bch_cache_sets, list) {
+			char *pdev_set_uuid = pdev->dc->sb.set_uuid;
+			char *set_uuid = c->sb.uuid;
+
+			if (!memcmp(pdev_set_uuid, set_uuid, 16)) {
+				list_del(&pdev->list);
+				kfree(pdev);
+				break;
+			}
+		}
+	}
+	mutex_unlock(&bch_register_lock);
+
+	list_for_each_entry_safe(pdev, tpdev, &pending_devs, list) {
+		pr_info("delete pdev %p", pdev);
+		list_del(&pdev->list);
+		bcache_device_stop(&pdev->dc->disk);
+		kfree(pdev);
+	}
+
+	return ret;
+}
+
 static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 {
 	if (code == SYS_DOWN ||
@@ -2518,6 +2572,7 @@ static int __init bcache_init(void)
 	static const struct attribute *files[] = {
 		&ksysfs_register.attr,
 		&ksysfs_register_quiet.attr,
+		&ksysfs_pendings_cleanup.attr,
 		NULL
 	};
 

commit 68a53c95a0fce541321fbca74a7f72c71361f496
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:40 2019 +0800

    bcache: remove "XXX:" comment line from run_cache_set()
    
    In previous bcache patches for Linux v5.2, the failure code path of
    run_cache_set() is tested and fixed. So now the following comment
    line can be removed from run_cache_set(),
            /* XXX: test this, it's broken */
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e9e6d653bf70..c53fe0f1629f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1979,7 +1979,7 @@ static int run_cache_set(struct cache_set *c)
 	}
 
 	closure_sync(&cl);
-	/* XXX: test this, it's broken */
+
 	bch_cache_set_error(c, "%s", err);
 
 	return -EIO;

commit e0faa3d7f79f7e1abb43de168e88c76061518ea4
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:39 2019 +0800

    bcache: improve error message in bch_cached_dev_run()
    
    This patch adds more error message in bch_cached_dev_run() to indicate
    the exact reason why an error value is returned. Please notice when
    printing out the "is running already" message, pr_info() is used here,
    because in this case also -EBUSY is returned, the bcache device can
    continue to attach to the cache devince and run, so it won't be an
    error level message in kernel message.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a836910ef368..e9e6d653bf70 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -926,13 +926,18 @@ int bch_cached_dev_run(struct cached_dev *dc)
 		NULL,
 	};
 
-	if (dc->io_disable)
+	if (dc->io_disable) {
+		pr_err("I/O disabled on cached dev %s",
+		       dc->backing_dev_name);
 		return -EIO;
+	}
 
 	if (atomic_xchg(&dc->running, 1)) {
 		kfree(env[1]);
 		kfree(env[2]);
 		kfree(buf);
+		pr_info("cached dev %s is running already",
+		       dc->backing_dev_name);
 		return -EBUSY;
 	}
 
@@ -961,7 +966,7 @@ int bch_cached_dev_run(struct cached_dev *dc)
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj,
 			      &d->kobj, "bcache")) {
-		pr_debug("error creating sysfs link");
+		pr_err("Couldn't create bcache dev <-> disk sysfs symlinks");
 		return -ENOMEM;
 	}
 

commit 633bb2ce60b949e2990c15324be162c54788c027
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:38 2019 +0800

    bcache: add more error message in bch_cached_dev_attach()
    
    This patch adds more error message for attaching cached device, this is
    helpful to debug code failure during bache device start up.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d4d8d1300faf..a836910ef368 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1169,6 +1169,8 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	down_write(&dc->writeback_lock);
 	if (bch_cached_dev_writeback_start(dc)) {
 		up_write(&dc->writeback_lock);
+		pr_err("Couldn't start writeback facilities for %s",
+		       dc->disk.disk->disk_name);
 		return -ENOMEM;
 	}
 
@@ -1182,6 +1184,8 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	ret = bch_cached_dev_run(dc);
 	if (ret && (ret != -EBUSY)) {
 		up_write(&dc->writeback_lock);
+		pr_err("Couldn't run cached device %s",
+		       dc->backing_dev_name);
 		return ret;
 	}
 

commit 4b6efb4bdbce25097f1a6329e18c2b77c4f27722
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:37 2019 +0800

    bcache: more detailed error message to bcache_device_link()
    
    This patch adds more accurate error message for specific
    ssyfs_create_link() call, to help debugging failure during
    bcache device start tup.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 0abee44092bf..d4d8d1300faf 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -693,6 +693,7 @@ static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
 {
 	unsigned int i;
 	struct cache *ca;
+	int ret;
 
 	for_each_cache(ca, d->c, i)
 		bd_link_disk_holder(ca->bdev, d->disk);
@@ -700,9 +701,13 @@ static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
 	snprintf(d->name, BCACHEDEVNAME_SIZE,
 		 "%s%u", name, d->id);
 
-	WARN(sysfs_create_link(&d->kobj, &c->kobj, "cache") ||
-	     sysfs_create_link(&c->kobj, &d->kobj, d->name),
-	     "Couldn't create device <-> cache set symlinks");
+	ret = sysfs_create_link(&d->kobj, &c->kobj, "cache");
+	if (ret < 0)
+		pr_err("Couldn't create device -> cache set symlink");
+
+	ret = sysfs_create_link(&c->kobj, &d->kobj, d->name);
+	if (ret < 0)
+		pr_err("Couldn't create cache set -> device symlink");
 
 	clear_bit(BCACHE_DEV_UNLINK_DONE, &d->flags);
 }

commit 0b13efecf5f25ce5e31f2ab3930335015cb65a7d
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:33 2019 +0800

    bcache: add return value check to bch_cached_dev_run()
    
    This patch adds return value check to bch_cached_dev_run(), now if there
    is error happens inside bch_cached_dev_run(), it can be catched.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 73466bda12a7..0abee44092bf 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -910,7 +910,7 @@ static int cached_dev_status_update(void *arg)
 }
 
 
-void bch_cached_dev_run(struct cached_dev *dc)
+int bch_cached_dev_run(struct cached_dev *dc)
 {
 	struct bcache_device *d = &dc->disk;
 	char *buf = kmemdup_nul(dc->sb.label, SB_LABEL_SIZE, GFP_KERNEL);
@@ -921,11 +921,14 @@ void bch_cached_dev_run(struct cached_dev *dc)
 		NULL,
 	};
 
+	if (dc->io_disable)
+		return -EIO;
+
 	if (atomic_xchg(&dc->running, 1)) {
 		kfree(env[1]);
 		kfree(env[2]);
 		kfree(buf);
-		return;
+		return -EBUSY;
 	}
 
 	if (!d->c &&
@@ -951,8 +954,11 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	kfree(buf);
 
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
-	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))
+	    sysfs_create_link(&disk_to_dev(d->disk)->kobj,
+			      &d->kobj, "bcache")) {
 		pr_debug("error creating sysfs link");
+		return -ENOMEM;
+	}
 
 	dc->status_update_thread = kthread_run(cached_dev_status_update,
 					       dc, "bcache_status_update");
@@ -961,6 +967,8 @@ void bch_cached_dev_run(struct cached_dev *dc)
 			"continue to run without monitoring backing "
 			"device status");
 	}
+
+	return 0;
 }
 
 /*
@@ -1056,6 +1064,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	uint32_t rtime = cpu_to_le32((u32)ktime_get_real_seconds());
 	struct uuid_entry *u;
 	struct cached_dev *exist_dc, *t;
+	int ret = 0;
 
 	if ((set_uuid && memcmp(set_uuid, c->sb.set_uuid, 16)) ||
 	    (!set_uuid && memcmp(dc->sb.set_uuid, c->sb.set_uuid, 16)))
@@ -1165,7 +1174,12 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 
 	bch_sectors_dirty_init(&dc->disk);
 
-	bch_cached_dev_run(dc);
+	ret = bch_cached_dev_run(dc);
+	if (ret && (ret != -EBUSY)) {
+		up_write(&dc->writeback_lock);
+		return ret;
+	}
+
 	bcache_device_link(&dc->disk, c, "bdev");
 	atomic_inc(&c->attached_dev_nr);
 
@@ -1292,6 +1306,7 @@ static int register_bdev(struct cache_sb *sb, struct page *sb_page,
 {
 	const char *err = "cannot allocate memory";
 	struct cache_set *c;
+	int ret = -ENOMEM;
 
 	bdevname(bdev, dc->backing_dev_name);
 	memcpy(&dc->sb, sb, sizeof(struct cache_sb));
@@ -1321,14 +1336,18 @@ static int register_bdev(struct cache_sb *sb, struct page *sb_page,
 		bch_cached_dev_attach(dc, c, NULL);
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_NONE ||
-	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE)
-		bch_cached_dev_run(dc);
+	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE) {
+		err = "failed to run cached device";
+		ret = bch_cached_dev_run(dc);
+		if (ret)
+			goto err;
+	}
 
 	return 0;
 err:
 	pr_notice("error %s: %s", dc->backing_dev_name, err);
 	bcache_device_stop(&dc->disk);
-	return -EIO;
+	return ret;
 }
 
 /* Flash only volumes */

commit 08ec1e6282f271698f0053983fab89de6e1a8217
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:30 2019 +0800

    bcache: add io error counting in write_bdev_super_endio()
    
    When backing device super block is written by bch_write_bdev_super(),
    the bio complete callback write_bdev_super_endio() simply ignores I/O
    status. Indeed such write request also contribute to backing device
    health status if the request failed.
    
    This patch checkes bio->bi_status in write_bdev_super_endio(), if there
    is error, bch_count_backing_io_errors() will be called to count an I/O
    error to dc->io_errors.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index dc6702c2c4b6..73466bda12a7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -197,7 +197,9 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 static void write_bdev_super_endio(struct bio *bio)
 {
 	struct cached_dev *dc = bio->bi_private;
-	/* XXX: error checking */
+
+	if (bio->bi_status)
+		bch_count_backing_io_errors(dc, bio);
 
 	closure_put(&dc->sb_write);
 }

commit e6dcbd3e6c91b7828cb305ec324eb7fd9bdea8a0
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:28 2019 +0800

    bcache: avoid flushing btree node in cache_set_flush() if io disabled
    
    When cache_set_flush() is called for too many I/O errors detected on
    cache device and the cache set is retiring, inside the function it
    doesn't make sense to flushing cached btree nodes from c->btree_cache
    because CACHE_SET_IO_DISABLE is set on c->flags already and all I/Os
    onto cache device will be rejected.
    
    This patch checks in cache_set_flush() that whether CACHE_SET_IO_DISABLE
    is set. If yes, then avoids to flush the cached btree nodes to reduce
    more time and make cache set retiring more faster.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index ba2ad093bc80..dc6702c2c4b6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1553,13 +1553,17 @@ static void cache_set_flush(struct closure *cl)
 	if (!IS_ERR_OR_NULL(c->root))
 		list_add(&c->root->list, &c->btree_cache);
 
-	/* Should skip this if we're unregistering because of an error */
-	list_for_each_entry(b, &c->btree_cache, list) {
-		mutex_lock(&b->write_lock);
-		if (btree_node_dirty(b))
-			__bch_btree_node_write(b, NULL);
-		mutex_unlock(&b->write_lock);
-	}
+	/*
+	 * Avoid flushing cached nodes if cache set is retiring
+	 * due to too many I/O errors detected.
+	 */
+	if (!test_bit(CACHE_SET_IO_DISABLE, &c->flags))
+		list_for_each_entry(b, &c->btree_cache, list) {
+			mutex_lock(&b->write_lock);
+			if (btree_node_dirty(b))
+				__bch_btree_node_write(b, NULL);
+			mutex_unlock(&b->write_lock);
+		}
 
 	for_each_cache(ca, c, i)
 		if (ca->alloc_thread)

commit 695277f16b3a102fcc22c97fdf2de77c7b19f0b3
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:27 2019 +0800

    Revert "bcache: set CACHE_SET_IO_DISABLE in bch_cached_dev_error()"
    
    This reverts commit 6147305c73e4511ca1a975b766b97a779d442567.
    
    Although this patch helps the failed bcache device to stop faster when
    too many I/O errors detected on corresponding cached device, setting
    CACHE_SET_IO_DISABLE bit to cache set c->flags was not a good idea. This
    operation will disable all I/Os on cache set, which means other attached
    bcache devices won't work neither.
    
    Without this patch, the failed bcache device can also be stopped
    eventually if internal I/O accomplished (e.g. writeback). Therefore here
    I revert it.
    
    Fixes: 6147305c73e4 ("bcache: set CACHE_SET_IO_DISABLE in bch_cached_dev_error()")
    Reported-by: Yong Li <mr.liyong@qq.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 64d9de89a63f..ba2ad093bc80 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1437,8 +1437,6 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 
 bool bch_cached_dev_error(struct cached_dev *dc)
 {
-	struct cache_set *c;
-
 	if (!dc || test_bit(BCACHE_DEV_CLOSING, &dc->disk.flags))
 		return false;
 
@@ -1449,21 +1447,6 @@ bool bch_cached_dev_error(struct cached_dev *dc)
 	pr_err("stop %s: too many IO errors on backing device %s\n",
 		dc->disk.disk->disk_name, dc->backing_dev_name);
 
-	/*
-	 * If the cached device is still attached to a cache set,
-	 * even dc->io_disable is true and no more I/O requests
-	 * accepted, cache device internal I/O (writeback scan or
-	 * garbage collection) may still prevent bcache device from
-	 * being stopped. So here CACHE_SET_IO_DISABLE should be
-	 * set to c->flags too, to make the internal I/O to cache
-	 * device rejected and stopped immediately.
-	 * If c is NULL, that means the bcache device is not attached
-	 * to any cache set, then no CACHE_SET_IO_DISABLE bit to set.
-	 */
-	c = dc->disk.c;
-	if (c && test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
-		pr_info("CACHE_SET_IO_DISABLE already set");
-
 	bcache_device_stop(&dc->disk);
 	return true;
 }

commit b387e9b58679c60f5b1e4313939bd4878204fc37
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:25 2019 +0800

    bcache: check c->gc_thread by IS_ERR_OR_NULL in cache_set_flush()
    
    When system memory is in heavy pressure, bch_gc_thread_start() from
    run_cache_set() may fail due to out of memory. In such condition,
    c->gc_thread is assigned to -ENOMEM, not NULL pointer. Then in following
    failure code path bch_cache_set_error(), when cache_set_flush() gets
    called, the code piece to stop c->gc_thread is broken,
             if (!IS_ERR_OR_NULL(c->gc_thread))
                     kthread_stop(c->gc_thread);
    
    And KASAN catches such NULL pointer deference problem, with the warning
    information:
    
    [  561.207881] ==================================================================
    [  561.207900] BUG: KASAN: null-ptr-deref in kthread_stop+0x3b/0x440
    [  561.207904] Write of size 4 at addr 000000000000001c by task kworker/15:1/313
    
    [  561.207913] CPU: 15 PID: 313 Comm: kworker/15:1 Tainted: G        W         5.0.0-vanilla+ #3
    [  561.207916] Hardware name: Lenovo ThinkSystem SR650 -[7X05CTO1WW]-/-[7X05CTO1WW]-, BIOS -[IVE136T-2.10]- 03/22/2019
    [  561.207935] Workqueue: events cache_set_flush [bcache]
    [  561.207940] Call Trace:
    [  561.207948]  dump_stack+0x9a/0xeb
    [  561.207955]  ? kthread_stop+0x3b/0x440
    [  561.207960]  ? kthread_stop+0x3b/0x440
    [  561.207965]  kasan_report+0x176/0x192
    [  561.207973]  ? kthread_stop+0x3b/0x440
    [  561.207981]  kthread_stop+0x3b/0x440
    [  561.207995]  cache_set_flush+0xd4/0x6d0 [bcache]
    [  561.208008]  process_one_work+0x856/0x1620
    [  561.208015]  ? find_held_lock+0x39/0x1d0
    [  561.208028]  ? drain_workqueue+0x380/0x380
    [  561.208048]  worker_thread+0x87/0xb80
    [  561.208058]  ? __kthread_parkme+0xb6/0x180
    [  561.208067]  ? process_one_work+0x1620/0x1620
    [  561.208072]  kthread+0x326/0x3e0
    [  561.208079]  ? kthread_create_worker_on_cpu+0xc0/0xc0
    [  561.208090]  ret_from_fork+0x3a/0x50
    [  561.208110] ==================================================================
    [  561.208113] Disabling lock debugging due to kernel taint
    [  561.208115] irq event stamp: 11800231
    [  561.208126] hardirqs last  enabled at (11800231): [<ffffffff83008538>] do_syscall_64+0x18/0x410
    [  561.208127] BUG: unable to handle kernel NULL pointer dereference at 000000000000001c
    [  561.208129] #PF error: [WRITE]
    [  561.312253] hardirqs last disabled at (11800230): [<ffffffff830052ff>] trace_hardirqs_off_thunk+0x1a/0x1c
    [  561.312259] softirqs last  enabled at (11799832): [<ffffffff850005c7>] __do_softirq+0x5c7/0x8c3
    [  561.405975] PGD 0 P4D 0
    [  561.442494] softirqs last disabled at (11799821): [<ffffffff831add2c>] irq_exit+0x1ac/0x1e0
    [  561.791359] Oops: 0002 [#1] SMP KASAN NOPTI
    [  561.791362] CPU: 15 PID: 313 Comm: kworker/15:1 Tainted: G    B   W         5.0.0-vanilla+ #3
    [  561.791363] Hardware name: Lenovo ThinkSystem SR650 -[7X05CTO1WW]-/-[7X05CTO1WW]-, BIOS -[IVE136T-2.10]- 03/22/2019
    [  561.791371] Workqueue: events cache_set_flush [bcache]
    [  561.791374] RIP: 0010:kthread_stop+0x3b/0x440
    [  561.791376] Code: 00 00 65 8b 05 26 d5 e0 7c 89 c0 48 0f a3 05 ec aa df 02 0f 82 dc 02 00 00 4c 8d 63 20 be 04 00 00 00 4c 89 e7 e8 65 c5 53 00 <f0> ff 43 20 48 8d 7b 24 48 b8 00 00 00 00 00 fc ff df 48 89 fa 48
    [  561.791377] RSP: 0018:ffff88872fc8fd10 EFLAGS: 00010286
    [  561.838895] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838916] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838934] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838948] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838966] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838979] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  561.838996] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  563.067028] RAX: 0000000000000000 RBX: fffffffffffffffc RCX: ffffffff832dd314
    [  563.067030] RDX: 0000000000000000 RSI: 0000000000000004 RDI: 0000000000000297
    [  563.067032] RBP: ffff88872fc8fe88 R08: fffffbfff0b8213d R09: fffffbfff0b8213d
    [  563.067034] R10: 0000000000000001 R11: fffffbfff0b8213c R12: 000000000000001c
    [  563.408618] R13: ffff88dc61cc0f68 R14: ffff888102b94900 R15: ffff88dc61cc0f68
    [  563.408620] FS:  0000000000000000(0000) GS:ffff888f7dc00000(0000) knlGS:0000000000000000
    [  563.408622] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  563.408623] CR2: 000000000000001c CR3: 0000000f48a1a004 CR4: 00000000007606e0
    [  563.408625] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [  563.408627] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [  563.904795] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  563.915796] PKRU: 55555554
    [  563.915797] Call Trace:
    [  563.915807]  cache_set_flush+0xd4/0x6d0 [bcache]
    [  563.915812]  process_one_work+0x856/0x1620
    [  564.001226] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  564.033563]  ? find_held_lock+0x39/0x1d0
    [  564.033567]  ? drain_workqueue+0x380/0x380
    [  564.033574]  worker_thread+0x87/0xb80
    [  564.062823] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  564.118042]  ? __kthread_parkme+0xb6/0x180
    [  564.118046]  ? process_one_work+0x1620/0x1620
    [  564.118048]  kthread+0x326/0x3e0
    [  564.118050]  ? kthread_create_worker_on_cpu+0xc0/0xc0
    [  564.167066] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  564.252441]  ret_from_fork+0x3a/0x50
    [  564.252447] Modules linked in: msr rpcrdma sunrpc rdma_ucm ib_iser ib_umad rdma_cm ib_ipoib i40iw configfs iw_cm ib_cm libiscsi scsi_transport_iscsi mlx4_ib ib_uverbs mlx4_en ib_core nls_iso8859_1 nls_cp437 vfat fat intel_rapl skx_edac x86_pkg_temp_thermal coretemp iTCO_wdt iTCO_vendor_support crct10dif_pclmul crc32_pclmul crc32c_intel ghash_clmulni_intel ses raid0 aesni_intel cdc_ether enclosure usbnet ipmi_ssif joydev aes_x86_64 i40e scsi_transport_sas mii bcache md_mod crypto_simd mei_me ioatdma crc64 ptp cryptd pcspkr i2c_i801 mlx4_core glue_helper pps_core mei lpc_ich dca wmi ipmi_si ipmi_devintf nd_pmem dax_pmem nd_btt ipmi_msghandler device_dax pcc_cpufreq button hid_generic usbhid mgag200 i2c_algo_bit drm_kms_helper syscopyarea sysfillrect xhci_pci sysimgblt fb_sys_fops xhci_hcd ttm megaraid_sas drm usbcore nfit libnvdimm sg dm_multipath dm_mod scsi_dh_rdac scsi_dh_emc scsi_dh_alua efivarfs
    [  564.299390] bcache: bch_count_io_errors() nvme0n1: IO error on writing btree.
    [  564.348360] CR2: 000000000000001c
    [  564.348362] ---[ end trace b7f0e5cc7b2103b0 ]---
    
    Therefore, it is not enough to only check whether c->gc_thread is NULL,
    we should use IS_ERR_OR_NULL() to check both NULL pointer and error
    value.
    
    This patch changes the above buggy code piece in this way,
             if (!IS_ERR_OR_NULL(c->gc_thread))
                     kthread_stop(c->gc_thread);
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1b63ac876169..64d9de89a63f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1564,7 +1564,7 @@ static void cache_set_flush(struct closure *cl)
 	kobject_put(&c->internal);
 	kobject_del(&c->kobj);
 
-	if (c->gc_thread)
+	if (!IS_ERR_OR_NULL(c->gc_thread))
 		kthread_stop(c->gc_thread);
 
 	if (!IS_ERR_OR_NULL(c->root))

commit cdca22bcbc64fc83dadb8d927df400a8d86ddabb
Author: Coly Li <colyli@suse.de>
Date:   Tue Apr 30 22:02:25 2019 +0800

    bcache: remove redundant LIST_HEAD(journal) from run_cache_set()
    
    Commit 95f18c9d1310 ("bcache: avoid potential memleak of list of
    journal_replay(s) in the CACHE_SYNC branch of run_cache_set") forgets
    to remove the original define of LIST_HEAD(journal), which makes
    the change no take effect. This patch removes redundant variable
    LIST_HEAD(journal) from run_cache_set(), to make Shenghui's fix
    working.
    
    Fixes: 95f18c9d1310 ("bcache: avoid potential memleak of list of journal_replay(s) in the CACHE_SYNC branch of run_cache_set")
    Reported-by: Juha Aatrokoski <juha.aatrokoski@aalto.fi>
    Cc: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 0ffe9acee9d8..1b63ac876169 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1800,7 +1800,6 @@ static int run_cache_set(struct cache_set *c)
 	set_gc_sectors(c);
 
 	if (CACHE_SYNC(&c->sb)) {
-		LIST_HEAD(journal);
 		struct bkey *k;
 		struct jset *j;
 

commit 95f18c9d1310730d075499a75aaf13bcd60405a7
Author: Shenghui Wang <shhuiw@foxmail.com>
Date:   Thu Apr 25 00:48:43 2019 +0800

    bcache: avoid potential memleak of list of journal_replay(s) in the CACHE_SYNC branch of run_cache_set
    
    In the CACHE_SYNC branch of run_cache_set(), LIST_HEAD(journal) is used
    to collect journal_replay(s) and filled by bch_journal_read().
    
    If all goes well, bch_journal_replay() will release the list of
    jounal_replay(s) at the end of the branch.
    
    If something goes wrong, code flow will jump to the label "err:" and leave
    the list unreleased.
    
    This patch will release the list of journal_replay(s) in the case of
    error detected.
    
    v1 -> v2:
    * Move the release code to the location after label 'err:' to
      simply the change.
    
    Signed-off-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3f34b96ebbc3..0ffe9acee9d8 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1790,6 +1790,8 @@ static int run_cache_set(struct cache_set *c)
 	struct cache *ca;
 	struct closure cl;
 	unsigned int i;
+	LIST_HEAD(journal);
+	struct journal_replay *l;
 
 	closure_init_stack(&cl);
 
@@ -1949,6 +1951,12 @@ static int run_cache_set(struct cache_set *c)
 	set_bit(CACHE_SET_RUNNING, &c->flags);
 	return 0;
 err:
+	while (!list_empty(&journal)) {
+		l = list_first_entry(&journal, struct journal_replay, list);
+		list_del(&l->list);
+		kfree(l);
+	}
+
 	closure_sync(&cl);
 	/* XXX: test this, it's broken */
 	bch_cache_set_error(c, "%s", err);

commit eb8cbb6df38f6e5124a3d5f1f8a3dbf519537c60
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:40 2019 +0800

    bcache: improve bcache_reboot()
    
    This patch tries to release mutex bch_register_lock early, to give
    chance to stop cache set and bcache device early.
    
    This patch also expends time out of stopping all bcache device from
    2 seconds to 10 seconds, because stopping writeback rate update worker
    may delay for 5 seconds, 2 seconds is not enough.
    
    After this patch applied, stopping bcache devices during system reboot
    or shutdown is very hard to be observed any more.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 0363ab534c8e..3f34b96ebbc3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2397,10 +2397,19 @@ static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 		list_for_each_entry_safe(dc, tdc, &uncached_devices, list)
 			bcache_device_stop(&dc->disk);
 
+		mutex_unlock(&bch_register_lock);
+
+		/*
+		 * Give an early chance for other kthreads and
+		 * kworkers to stop themselves
+		 */
+		schedule();
+
 		/* What's a condition variable? */
 		while (1) {
-			long timeout = start + 2 * HZ - jiffies;
+			long timeout = start + 10 * HZ - jiffies;
 
+			mutex_lock(&bch_register_lock);
 			stopped = list_empty(&bch_cache_sets) &&
 				list_empty(&uncached_devices);
 
@@ -2412,7 +2421,6 @@ static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
 
 			mutex_unlock(&bch_register_lock);
 			schedule_timeout(timeout);
-			mutex_lock(&bch_register_lock);
 		}
 
 		finish_wait(&unregister_wait, &wait);

commit 63d63b51d70fb5155754dcf0baa2c1700bcafcb0
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:39 2019 +0800

    bcache: add comments for closure_fn to be called in closure_queue()
    
    Add code comments to explain which call back function might be called
    for the closure_queue(). This is an effort to make code to be more
    understandable for readers.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fa856b2ca7af..0363ab534c8e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -662,6 +662,11 @@ static const struct block_device_operations bcache_ops = {
 void bcache_device_stop(struct bcache_device *d)
 {
 	if (!test_and_set_bit(BCACHE_DEV_CLOSING, &d->flags))
+		/*
+		 * closure_fn set to
+		 * - cached device: cached_dev_flush()
+		 * - flash dev: flash_dev_flush()
+		 */
 		closure_queue(&d->cl);
 }
 
@@ -1675,6 +1680,7 @@ static void __cache_set_unregister(struct closure *cl)
 void bch_cache_set_stop(struct cache_set *c)
 {
 	if (!test_and_set_bit(CACHE_SET_STOPPING, &c->flags))
+		/* closure_fn set to __cache_set_unregister() */
 		closure_queue(&c->caching);
 }
 

commit bb6d355c2aff42d4075a8e7428dd72cb009d6143
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:38 2019 +0800

    bcache: Add comments for blkdev_put() in registration code path
    
    Add comments to explain why in register_bcache() blkdev_put() won't
    be called in two location. Add comments to explain why blkdev_put()
    must be called in register_cache() when cache_alloc() failed.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fde334939545..fa856b2ca7af 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2189,6 +2189,12 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 
 	ret = cache_alloc(ca);
 	if (ret != 0) {
+		/*
+		 * If we failed here, it means ca->kobj is not initialized yet,
+		 * kobject_put() won't be called and there is no chance to
+		 * call blkdev_put() to bdev in bch_cache_release(). So we
+		 * explicitly call blkdev_put() here.
+		 */
 		blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 		if (ret == -ENOMEM)
 			err = "cache_alloc(): -ENOMEM";
@@ -2329,6 +2335,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		mutex_lock(&bch_register_lock);
 		ret = register_bdev(sb, sb_page, bdev, dc);
 		mutex_unlock(&bch_register_lock);
+		/* blkdev_put() will be called in cached_dev_free() */
 		if (ret < 0)
 			goto err;
 	} else {
@@ -2337,6 +2344,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		if (!ca)
 			goto err_close;
 
+		/* blkdev_put() will be called in bch_cache_release() */
 		if (register_cache(sb, sb_page, bdev, ca) != 0)
 			goto err;
 	}

commit 88c12d42d2bb6e05deb3cfd24d12f6fe80544575
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:37 2019 +0800

    bcache: add error check for calling register_bdev()
    
    This patch adds return value to register_bdev(). Then if failure happens
    inside register_bdev(), its caller register_bcache() may detect and
    handle the failure more properly.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f8d80adcafec..fde334939545 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1279,7 +1279,7 @@ static int cached_dev_init(struct cached_dev *dc, unsigned int block_size)
 
 /* Cached device - bcache superblock */
 
-static void register_bdev(struct cache_sb *sb, struct page *sb_page,
+static int register_bdev(struct cache_sb *sb, struct page *sb_page,
 				 struct block_device *bdev,
 				 struct cached_dev *dc)
 {
@@ -1317,10 +1317,11 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE)
 		bch_cached_dev_run(dc);
 
-	return;
+	return 0;
 err:
 	pr_notice("error %s: %s", dc->backing_dev_name, err);
 	bcache_device_stop(&dc->disk);
+	return -EIO;
 }
 
 /* Flash only volumes */
@@ -2271,7 +2272,7 @@ static bool bch_is_open(struct block_device *bdev)
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size)
 {
-	ssize_t ret = size;
+	ssize_t ret = -EINVAL;
 	const char *err = "cannot allocate memory";
 	char *path = NULL;
 	struct cache_sb *sb = NULL;
@@ -2305,7 +2306,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			if (!IS_ERR(bdev))
 				bdput(bdev);
 			if (attr == &ksysfs_register_quiet)
-				goto out;
+				goto quiet_out;
 		}
 		goto err;
 	}
@@ -2326,8 +2327,10 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			goto err_close;
 
 		mutex_lock(&bch_register_lock);
-		register_bdev(sb, sb_page, bdev, dc);
+		ret = register_bdev(sb, sb_page, bdev, dc);
 		mutex_unlock(&bch_register_lock);
+		if (ret < 0)
+			goto err;
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
 
@@ -2337,6 +2340,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		if (register_cache(sb, sb_page, bdev, ca) != 0)
 			goto err;
 	}
+quiet_out:
+	ret = size;
 out:
 	if (sb_page)
 		put_page(sb_page);
@@ -2349,7 +2354,6 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 err:
 	pr_info("error %s: %s", path, err);
-	ret = -EINVAL;
 	goto out;
 }
 

commit 2d17456eb1cc78803b999fdd503c2dbd42a7d3da
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:35 2019 +0800

    bcache: add comments for kobj release callback routine
    
    Bcache has several routines to release resources in implicit way, they
    are called when the associated kobj released. This patch adds code
    comments to notice when and which release callback will be called,
    - When dc->disk.kobj released:
      void bch_cached_dev_release(struct kobject *kobj)
    - When d->kobj released:
      void bch_flash_dev_release(struct kobject *kobj)
    - When c->kobj released:
      void bch_cache_set_release(struct kobject *kobj)
    - When ca->kobj released
      void bch_cache_release(struct kobject *kobj)
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8c7fdada0acf..f8d80adcafec 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1172,6 +1172,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	return 0;
 }
 
+/* when dc->disk.kobj released */
 void bch_cached_dev_release(struct kobject *kobj)
 {
 	struct cached_dev *dc = container_of(kobj, struct cached_dev,
@@ -1324,6 +1325,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 /* Flash only volumes */
 
+/* When d->kobj released */
 void bch_flash_dev_release(struct kobject *kobj)
 {
 	struct bcache_device *d = container_of(kobj, struct bcache_device,
@@ -1494,6 +1496,7 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 	return true;
 }
 
+/* When c->kobj released */
 void bch_cache_set_release(struct kobject *kobj)
 {
 	struct cache_set *c = container_of(kobj, struct cache_set, kobj);
@@ -2021,6 +2024,7 @@ static const char *register_cache_set(struct cache *ca)
 
 /* Cache device */
 
+/* When ca->kobj released */
 void bch_cache_release(struct kobject *kobj)
 {
 	struct cache *ca = container_of(kobj, struct cache, kobj);

commit ce3e4cfb59cb382f8e5ce359238aa580d4ae7778
Author: Coly Li <colyli@suse.de>
Date:   Thu Apr 25 00:48:34 2019 +0800

    bcache: add failure check to run_cache_set() for journal replay
    
    Currently run_cache_set() has no return value, if there is failure in
    bch_journal_replay(), the caller of run_cache_set() has no idea about
    such failure and just continue to execute following code after
    run_cache_set().  The internal failure is triggered inside
    bch_journal_replay() and being handled in async way. This behavior is
    inefficient, while failure handling inside bch_journal_replay(), cache
    register code is still running to start the cache set. Registering and
    unregistering code running as same time may introduce some rare race
    condition, and make the code to be more hard to be understood.
    
    This patch adds return value to run_cache_set(), and returns -EIO if
    bch_journal_rreplay() fails. Then caller of run_cache_set() may detect
    such failure and stop registering code flow immedidately inside
    register_cache_set().
    
    If journal replay fails, run_cache_set() can report error immediately
    to register_cache_set(). This patch makes the failure handling for
    bch_journal_replay() be in synchronized way, easier to understand and
    debug, and avoid poetential race condition for register-and-unregister
    in same time.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 53c5e3e0ac22..8c7fdada0acf 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1773,7 +1773,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	return NULL;
 }
 
-static void run_cache_set(struct cache_set *c)
+static int run_cache_set(struct cache_set *c)
 {
 	const char *err = "cannot allocate memory";
 	struct cached_dev *dc, *t;
@@ -1867,7 +1867,9 @@ static void run_cache_set(struct cache_set *c)
 		if (j->version < BCACHE_JSET_VERSION_UUID)
 			__uuid_write(c);
 
-		bch_journal_replay(c, &journal);
+		err = "bcache: replay journal failed";
+		if (bch_journal_replay(c, &journal))
+			goto err;
 	} else {
 		pr_notice("invalidating existing data");
 
@@ -1935,11 +1937,13 @@ static void run_cache_set(struct cache_set *c)
 	flash_devs_run(c);
 
 	set_bit(CACHE_SET_RUNNING, &c->flags);
-	return;
+	return 0;
 err:
 	closure_sync(&cl);
 	/* XXX: test this, it's broken */
 	bch_cache_set_error(c, "%s", err);
+
+	return -EIO;
 }
 
 static bool can_attach_cache(struct cache *ca, struct cache_set *c)
@@ -2003,8 +2007,11 @@ static const char *register_cache_set(struct cache *ca)
 	ca->set->cache[ca->sb.nr_this_dev] = ca;
 	c->cache_by_alloc[c->caches_loaded++] = ca;
 
-	if (c->caches_loaded == c->sb.nr_in_set)
-		run_cache_set(c);
+	if (c->caches_loaded == c->sb.nr_in_set) {
+		err = "failed to run cache set";
+		if (run_cache_set(c) < 0)
+			goto err;
+	}
 
 	return NULL;
 err:

commit a4b732a248d12cbdb46999daf0bf288c011335eb
Author: Liang Chen <liangchen.linux@gmail.com>
Date:   Thu Apr 25 00:48:31 2019 +0800

    bcache: fix a race between cache register and cacheset unregister
    
    There is a race between cache device register and cache set unregister.
    For an already registered cache device, register_bcache will call
    bch_is_open to iterate through all cachesets and check every cache
    there. The race occurs if cache_set_free executes at the same time and
    clears the caches right before ca is dereferenced in bch_is_open_cache.
    To close the race, let's make sure the clean up work is protected by
    the bch_register_lock as well.
    
    This issue can be reproduced as follows,
    while true; do echo /dev/XXX> /sys/fs/bcache/register ; done&
    while true; do echo 1> /sys/block/XXX/bcache/set/unregister ; done &
    
    and results in the following oops,
    
    [  +0.000053] BUG: unable to handle kernel NULL pointer dereference at 0000000000000998
    [  +0.000457] #PF error: [normal kernel read fault]
    [  +0.000464] PGD 800000003ca9d067 P4D 800000003ca9d067 PUD 3ca9c067 PMD 0
    [  +0.000388] Oops: 0000 [#1] SMP PTI
    [  +0.000269] CPU: 1 PID: 3266 Comm: bash Not tainted 5.0.0+ #6
    [  +0.000346] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.11.0-2.fc28 04/01/2014
    [  +0.000472] RIP: 0010:register_bcache+0x1829/0x1990 [bcache]
    [  +0.000344] Code: b0 48 83 e8 50 48 81 fa e0 e1 10 c0 0f 84 a9 00 00 00 48 89 c6 48 89 ca 0f b7 ba 54 04 00 00 4c 8b 82 60 0c 00 00 85 ff 74 2f <49> 3b a8 98 09 00 00 74 4e 44 8d 47 ff 31 ff 49 c1 e0 03 eb 0d
    [  +0.000839] RSP: 0018:ffff92ee804cbd88 EFLAGS: 00010202
    [  +0.000328] RAX: ffffffffc010e190 RBX: ffff918b5c6b5000 RCX: ffff918b7d8e0000
    [  +0.000399] RDX: ffff918b7d8e0000 RSI: ffffffffc010e190 RDI: 0000000000000001
    [  +0.000398] RBP: ffff918b7d318340 R08: 0000000000000000 R09: ffffffffb9bd2d7a
    [  +0.000385] R10: ffff918b7eb253c0 R11: ffffb95980f51200 R12: ffffffffc010e1a0
    [  +0.000411] R13: fffffffffffffff2 R14: 000000000000000b R15: ffff918b7e232620
    [  +0.000384] FS:  00007f955bec2740(0000) GS:ffff918b7eb00000(0000) knlGS:0000000000000000
    [  +0.000420] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  +0.000801] CR2: 0000000000000998 CR3: 000000003cad6000 CR4: 00000000001406e0
    [  +0.000837] Call Trace:
    [  +0.000682]  ? _cond_resched+0x10/0x20
    [  +0.000691]  ? __kmalloc+0x131/0x1b0
    [  +0.000710]  kernfs_fop_write+0xfa/0x170
    [  +0.000733]  __vfs_write+0x2e/0x190
    [  +0.000688]  ? inode_security+0x10/0x30
    [  +0.000698]  ? selinux_file_permission+0xd2/0x120
    [  +0.000752]  ? security_file_permission+0x2b/0x100
    [  +0.000753]  vfs_write+0xa8/0x1a0
    [  +0.000676]  ksys_write+0x4d/0xb0
    [  +0.000699]  do_syscall_64+0x3a/0xf0
    [  +0.000692]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Signed-off-by: Liang Chen <liangchen.linux@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6e618cb6126c..53c5e3e0ac22 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1514,6 +1514,7 @@ static void cache_set_free(struct closure *cl)
 	bch_btree_cache_free(c);
 	bch_journal_free(c);
 
+	mutex_lock(&bch_register_lock);
 	for_each_cache(ca, c, i)
 		if (ca) {
 			ca->set = NULL;
@@ -1532,7 +1533,6 @@ static void cache_set_free(struct closure *cl)
 	mempool_exit(&c->search);
 	kfree(c->devices);
 
-	mutex_lock(&bch_register_lock);
 	list_del(&c->list);
 	mutex_unlock(&bch_register_lock);
 

commit 792732d9852c0e4505aceff4631ea2168fd02480
Author: Geliang Tang <geliangtang@gmail.com>
Date:   Thu Apr 25 00:48:29 2019 +0800

    bcache: use kmemdup_nul for CACHED_LABEL buffer
    
    This patch uses kmemdup_nul to create a NUL-terminated string from
    dc->sb.label. This is better than open coding it.
    
    With this, we can move env[2] initialization into env[] array to make
    code more elegant.
    
    Signed-off-by: Geliang Tang <geliangtang@gmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a697a3a923cd..6e618cb6126c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -906,21 +906,18 @@ static int cached_dev_status_update(void *arg)
 void bch_cached_dev_run(struct cached_dev *dc)
 {
 	struct bcache_device *d = &dc->disk;
-	char buf[SB_LABEL_SIZE + 1];
+	char *buf = kmemdup_nul(dc->sb.label, SB_LABEL_SIZE, GFP_KERNEL);
 	char *env[] = {
 		"DRIVER=bcache",
 		kasprintf(GFP_KERNEL, "CACHED_UUID=%pU", dc->sb.uuid),
-		NULL,
+		kasprintf(GFP_KERNEL, "CACHED_LABEL=%s", buf ? : ""),
 		NULL,
 	};
 
-	memcpy(buf, dc->sb.label, SB_LABEL_SIZE);
-	buf[SB_LABEL_SIZE] = '\0';
-	env[2] = kasprintf(GFP_KERNEL, "CACHED_LABEL=%s", buf);
-
 	if (atomic_xchg(&dc->running, 1)) {
 		kfree(env[1]);
 		kfree(env[2]);
+		kfree(buf);
 		return;
 	}
 
@@ -944,6 +941,7 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
 	kfree(env[1]);
 	kfree(env[2]);
+	kfree(buf);
 
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))

commit e8cf978dffb2c603340d4615eec2e5358c9df06d
Author: Colin Ian King <colin.king@canonical.com>
Date:   Sat Feb 9 12:52:57 2019 +0800

    bcache: fix indentation issue, remove tabs on a hunk of code
    
    There is a hunk of code that is indented one level too deep, fix this
    by removing the extra tabs.
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4dee119c3664..a697a3a923cd 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1615,21 +1615,21 @@ static void conditional_stop_bcache_device(struct cache_set *c,
 		 */
 		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is dirty, stop it to avoid potential data corruption.",
 			d->disk->disk_name);
-			/*
-			 * There might be a small time gap that cache set is
-			 * released but bcache device is not. Inside this time
-			 * gap, regular I/O requests will directly go into
-			 * backing device as no cache set attached to. This
-			 * behavior may also introduce potential inconsistence
-			 * data in writeback mode while cache is dirty.
-			 * Therefore before calling bcache_device_stop() due
-			 * to a broken cache device, dc->io_disable should be
-			 * explicitly set to true.
-			 */
-			dc->io_disable = true;
-			/* make others know io_disable is true earlier */
-			smp_mb();
-			bcache_device_stop(d);
+		/*
+		 * There might be a small time gap that cache set is
+		 * released but bcache device is not. Inside this time
+		 * gap, regular I/O requests will directly go into
+		 * backing device as no cache set attached to. This
+		 * behavior may also introduce potential inconsistence
+		 * data in writeback mode while cache is dirty.
+		 * Therefore before calling bcache_device_stop() due
+		 * to a broken cache device, dc->io_disable should be
+		 * explicitly set to true.
+		 */
+		dc->io_disable = true;
+		/* make others know io_disable is true earlier */
+		smp_mb();
+		bcache_device_stop(d);
 	} else {
 		/*
 		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO

commit 9aaf51654672b16566c5fe787da3ca41ebf6d297
Author: Coly Li <colyli@suse.de>
Date:   Thu Dec 13 22:53:55 2018 +0800

    bcache: make cutoff_writeback and cutoff_writeback_sync tunable
    
    Currently the cutoff writeback and cutoff writeback sync thresholds are
    defined by CUTOFF_WRITEBACK (40) and CUTOFF_WRITEBACK_SYNC (70) as
    static values. Most of time these they work fine, but when people want
    to do research on bcache writeback mode performance tuning, there is no
    chance to modify the soft and hard cutoff writeback values.
    
    This patch introduces two module parameters bch_cutoff_writeback_sync
    and bch_cutoff_writeback which permit people to tune the values when
    loading bcache.ko. If they are not specified by module loading, current
    values CUTOFF_WRITEBACK_SYNC and CUTOFF_WRITEBACK will be used as
    default and nothing changes.
    
    When people want to tune this two values,
    - cutoff_writeback can be set in range [1, 70]
    - cutoff_writeback_sync can be set in range [1, 90]
    - cutoff_writeback always <= cutoff_writeback_sync
    
    The default values are strongly recommended to most of users for most of
    workloads. Anyway, if people wants to take their own risk to do research
    on new writeback cutoff tuning for their own workload, now they can make
    it.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 61d3b63fa617..4dee119c3664 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -25,6 +25,9 @@
 #include <linux/reboot.h>
 #include <linux/sysfs.h>
 
+unsigned int bch_cutoff_writeback;
+unsigned int bch_cutoff_writeback_sync;
+
 static const char bcache_magic[] = {
 	0xc6, 0x85, 0x73, 0xf6, 0x4e, 0x1a, 0x45, 0xca,
 	0x82, 0x65, 0xf5, 0x7f, 0x48, 0xba, 0x6d, 0x81
@@ -2420,6 +2423,32 @@ static void bcache_exit(void)
 	mutex_destroy(&bch_register_lock);
 }
 
+/* Check and fixup module parameters */
+static void check_module_parameters(void)
+{
+	if (bch_cutoff_writeback_sync == 0)
+		bch_cutoff_writeback_sync = CUTOFF_WRITEBACK_SYNC;
+	else if (bch_cutoff_writeback_sync > CUTOFF_WRITEBACK_SYNC_MAX) {
+		pr_warn("set bch_cutoff_writeback_sync (%u) to max value %u",
+			bch_cutoff_writeback_sync, CUTOFF_WRITEBACK_SYNC_MAX);
+		bch_cutoff_writeback_sync = CUTOFF_WRITEBACK_SYNC_MAX;
+	}
+
+	if (bch_cutoff_writeback == 0)
+		bch_cutoff_writeback = CUTOFF_WRITEBACK;
+	else if (bch_cutoff_writeback > CUTOFF_WRITEBACK_MAX) {
+		pr_warn("set bch_cutoff_writeback (%u) to max value %u",
+			bch_cutoff_writeback, CUTOFF_WRITEBACK_MAX);
+		bch_cutoff_writeback = CUTOFF_WRITEBACK_MAX;
+	}
+
+	if (bch_cutoff_writeback > bch_cutoff_writeback_sync) {
+		pr_warn("set bch_cutoff_writeback (%u) to %u",
+			bch_cutoff_writeback, bch_cutoff_writeback_sync);
+		bch_cutoff_writeback = bch_cutoff_writeback_sync;
+	}
+}
+
 static int __init bcache_init(void)
 {
 	static const struct attribute *files[] = {
@@ -2428,6 +2457,8 @@ static int __init bcache_init(void)
 		NULL
 	};
 
+	check_module_parameters();
+
 	mutex_init(&bch_register_lock);
 	init_waitqueue_head(&unregister_wait);
 	register_reboot_notifier(&reboot);
@@ -2464,9 +2495,18 @@ static int __init bcache_init(void)
 	return -ENOMEM;
 }
 
+/*
+ * Module hooks
+ */
 module_exit(bcache_exit);
 module_init(bcache_init);
 
+module_param(bch_cutoff_writeback, uint, 0);
+MODULE_PARM_DESC(bch_cutoff_writeback, "threshold to cutoff writeback");
+
+module_param(bch_cutoff_writeback_sync, uint, 0);
+MODULE_PARM_DESC(bch_cutoff_writeback_sync, "hard threshold to cutoff writeback");
+
 MODULE_DESCRIPTION("Bcache: a Linux block layer cache");
 MODULE_AUTHOR("Kent Overstreet <kent.overstreet@gmail.com>");
 MODULE_LICENSE("GPL");

commit 009673d02fa92acaa7ed0b1e1389610e4390ba49
Author: Coly Li <colyli@suse.de>
Date:   Thu Dec 13 22:53:54 2018 +0800

    bcache: add MODULE_DESCRIPTION information
    
    This patch moves MODULE_AUTHOR and MODULE_LICENSE to end of super.c, and
    add MODULE_DESCRIPTION("Bcache: a Linux block layer cache").
    
    This is preparation for adding module parameters.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5b59d44656c0..61d3b63fa617 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -25,9 +25,6 @@
 #include <linux/reboot.h>
 #include <linux/sysfs.h>
 
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Kent Overstreet <kent.overstreet@gmail.com>");
-
 static const char bcache_magic[] = {
 	0xc6, 0x85, 0x73, 0xf6, 0x4e, 0x1a, 0x45, 0xca,
 	0x82, 0x65, 0xf5, 0x7f, 0x48, 0xba, 0x6d, 0x81
@@ -2469,3 +2466,7 @@ static int __init bcache_init(void)
 
 module_exit(bcache_exit);
 module_init(bcache_init);
+
+MODULE_DESCRIPTION("Bcache: a Linux block layer cache");
+MODULE_AUTHOR("Kent Overstreet <kent.overstreet@gmail.com>");
+MODULE_LICENSE("GPL");

commit ae17102316550b4b230a283febe31b2a9ff30084
Author: Shenghui Wang <shhuiw@foxmail.com>
Date:   Thu Dec 13 22:53:47 2018 +0800

    bcache: do not check if debug dentry is ERR or NULL explicitly on remove
    
    debugfs_remove and debugfs_remove_recursive will check if the dentry
    pointer is NULL or ERR, and will do nothing in that case.
    
    Remove the check in cache_set_free and bch_debug_init.
    
    Signed-off-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7bbd670a5a84..5b59d44656c0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1510,8 +1510,7 @@ static void cache_set_free(struct closure *cl)
 	struct cache *ca;
 	unsigned int i;
 
-	if (!IS_ERR_OR_NULL(c->debug))
-		debugfs_remove(c->debug);
+	debugfs_remove(c->debug);
 
 	bch_open_buckets_free(c);
 	bch_btree_cache_free(c);

commit 3a646fd77684dd5fbe20748bb04e12077bbecddc
Author: Dongbo Cao <cdbdyx@163.com>
Date:   Mon Oct 8 20:41:21 2018 +0800

    bcache: panic fix for making cache device
    
    when the nbuckets of cache device is smaller than 1024, making cache
    device will trigger BUG_ON in kernel, add a condition to avoid this.
    
    Reported-by: nitroxis <n@nxs.re>
    Signed-off-by: Dongbo Cao <cdbdyx@163.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4e89d6d4bbd6..7bbd670a5a84 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2071,6 +2071,11 @@ static int cache_alloc(struct cache *ca)
 	 */
 	btree_buckets = ca->sb.njournal_buckets ?: 8;
 	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
+	if (!free) {
+		ret = -EPERM;
+		err = "ca->sb.nbuckets is too small";
+		goto err_free;
+	}
 
 	if (!init_fifo(&ca->free[RESERVE_BTREE], btree_buckets,
 						GFP_KERNEL)) {
@@ -2148,6 +2153,7 @@ static int cache_alloc(struct cache *ca)
 err_prio_alloc:
 	free_fifo(&ca->free[RESERVE_BTREE]);
 err_btree_alloc:
+err_free:
 	module_put(THIS_MODULE);
 	if (err)
 		pr_notice("error %s: %s", ca->cache_dev_name, err);
@@ -2177,6 +2183,8 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 		blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 		if (ret == -ENOMEM)
 			err = "cache_alloc(): -ENOMEM";
+		else if (ret == -EPERM)
+			err = "cache_alloc(): cache device is too small";
 		else
 			err = "cache_alloc(): unknown error";
 		goto err;

commit f6027bca9e382efc4d4f28a2d9678e0a07428363
Author: Dongbo Cao <cdbdyx@163.com>
Date:   Mon Oct 8 20:41:20 2018 +0800

    bcache: split combined if-condition code into separate ones
    
    Split the combined '||' statements in if() check, to make the code easier
    for debug.
    
    Signed-off-by: Dongbo Cao <cdbdyx@163.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2ce3fe9a1efe..4e89d6d4bbd6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2052,6 +2052,8 @@ static int cache_alloc(struct cache *ca)
 	size_t free;
 	size_t btree_buckets;
 	struct bucket *b;
+	int ret = -ENOMEM;
+	const char *err = NULL;
 
 	__module_get(THIS_MODULE);
 	kobject_init(&ca->kobj, &bch_cache_ktype);
@@ -2070,26 +2072,86 @@ static int cache_alloc(struct cache *ca)
 	btree_buckets = ca->sb.njournal_buckets ?: 8;
 	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
 
-	if (!init_fifo(&ca->free[RESERVE_BTREE], btree_buckets, GFP_KERNEL) ||
-	    !init_fifo_exact(&ca->free[RESERVE_PRIO], prio_buckets(ca), GFP_KERNEL) ||
-	    !init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL) ||
-	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||
-	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
-	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
-	    !(ca->buckets	= vzalloc(array_size(sizeof(struct bucket),
-						     ca->sb.nbuckets))) ||
-	    !(ca->prio_buckets	= kzalloc(array3_size(sizeof(uint64_t),
-						      prio_buckets(ca), 2),
-					  GFP_KERNEL)) ||
-	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)))
-		return -ENOMEM;
+	if (!init_fifo(&ca->free[RESERVE_BTREE], btree_buckets,
+						GFP_KERNEL)) {
+		err = "ca->free[RESERVE_BTREE] alloc failed";
+		goto err_btree_alloc;
+	}
+
+	if (!init_fifo_exact(&ca->free[RESERVE_PRIO], prio_buckets(ca),
+							GFP_KERNEL)) {
+		err = "ca->free[RESERVE_PRIO] alloc failed";
+		goto err_prio_alloc;
+	}
+
+	if (!init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL)) {
+		err = "ca->free[RESERVE_MOVINGGC] alloc failed";
+		goto err_movinggc_alloc;
+	}
+
+	if (!init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL)) {
+		err = "ca->free[RESERVE_NONE] alloc failed";
+		goto err_none_alloc;
+	}
+
+	if (!init_fifo(&ca->free_inc, free << 2, GFP_KERNEL)) {
+		err = "ca->free_inc alloc failed";
+		goto err_free_inc_alloc;
+	}
+
+	if (!init_heap(&ca->heap, free << 3, GFP_KERNEL)) {
+		err = "ca->heap alloc failed";
+		goto err_heap_alloc;
+	}
+
+	ca->buckets = vzalloc(array_size(sizeof(struct bucket),
+			      ca->sb.nbuckets));
+	if (!ca->buckets) {
+		err = "ca->buckets alloc failed";
+		goto err_buckets_alloc;
+	}
+
+	ca->prio_buckets = kzalloc(array3_size(sizeof(uint64_t),
+				   prio_buckets(ca), 2),
+				   GFP_KERNEL);
+	if (!ca->prio_buckets) {
+		err = "ca->prio_buckets alloc failed";
+		goto err_prio_buckets_alloc;
+	}
+
+	ca->disk_buckets = alloc_bucket_pages(GFP_KERNEL, ca);
+	if (!ca->disk_buckets) {
+		err = "ca->disk_buckets alloc failed";
+		goto err_disk_buckets_alloc;
+	}
 
 	ca->prio_last_buckets = ca->prio_buckets + prio_buckets(ca);
 
 	for_each_bucket(b, ca)
 		atomic_set(&b->pin, 0);
-
 	return 0;
+
+err_disk_buckets_alloc:
+	kfree(ca->prio_buckets);
+err_prio_buckets_alloc:
+	vfree(ca->buckets);
+err_buckets_alloc:
+	free_heap(&ca->heap);
+err_heap_alloc:
+	free_fifo(&ca->free_inc);
+err_free_inc_alloc:
+	free_fifo(&ca->free[RESERVE_NONE]);
+err_none_alloc:
+	free_fifo(&ca->free[RESERVE_MOVINGGC]);
+err_movinggc_alloc:
+	free_fifo(&ca->free[RESERVE_PRIO]);
+err_prio_alloc:
+	free_fifo(&ca->free[RESERVE_BTREE]);
+err_btree_alloc:
+	module_put(THIS_MODULE);
+	if (err)
+		pr_notice("error %s: %s", ca->cache_dev_name, err);
+	return ret;
 }
 
 static int register_cache(struct cache_sb *sb, struct page *sb_page,

commit 91bafdf081b8ad8ab4977918ee45dffe3d744060
Author: Dongbo Cao <cdbdyx@163.com>
Date:   Mon Oct 8 20:41:17 2018 +0800

    bcache: remove useless parameter of bch_debug_init()
    
    Parameter "struct kobject *kobj" in bch_debug_init() is useless,
    remove it in this patch.
    
    Signed-off-by: Dongbo Cao <cdbdyx@163.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1155bd823b6f..2ce3fe9a1efe 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2389,7 +2389,7 @@ static int __init bcache_init(void)
 	    sysfs_create_files(bcache_kobj, files))
 		goto err;
 
-	bch_debug_init(bcache_kobj);
+	bch_debug_init();
 	closure_debug_init();
 
 	return 0;

commit 46010141da6677b81cc77f9b47f8ac62bd1cbfd3
Author: Shenghui Wang <shhuiw@foxmail.com>
Date:   Mon Oct 8 20:41:15 2018 +0800

    bcache: recal cached_dev_sectors on detach
    
    Recal cached_dev_sectors on cached_dev detached, as recal done on
    cached_dev attached.
    
    Update the cached_dev_sectors before bcache_device_detach called
    as bcache_device_detach will set bcache_device->c to NULL.
    
    Signed-off-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c116b6e0d85a..1155bd823b6f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1009,6 +1009,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	bch_write_bdev_super(dc, &cl);
 	closure_sync(&cl);
 
+	calc_cached_dev_sectors(dc->disk.c);
 	bcache_device_detach(&dc->disk);
 	list_move(&dc->list, &uncached_devices);
 

commit 2e17a262a2371d38d2ec03614a2675a32cef9912
Author: Tang Junhui <tang.junhui.linux@gmail.com>
Date:   Mon Oct 8 20:41:12 2018 +0800

    bcache: correct dirty data statistics
    
    When bcache device is clean, dirty keys may still exist after
    journal replay, so we need to count these dirty keys even
    device in clean status, otherwise after writeback, the amount
    of dirty data would be incorrect.
    
    Signed-off-by: Tang Junhui <tang.junhui.linux@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 9de6695195bf..c116b6e0d85a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1153,11 +1153,12 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	}
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
-		bch_sectors_dirty_init(&dc->disk);
 		atomic_set(&dc->has_dirty, 1);
 		bch_writeback_queue(dc);
 	}
 
+	bch_sectors_dirty_init(&dc->disk);
+
 	bch_cached_dev_run(dc);
 	bcache_device_link(&dc->disk, c, "bdev");
 	atomic_inc(&c->attached_dev_nr);

commit dd0c91793b7c2658ea32c6b3a2247a8ceca45dc0
Author: Tang Junhui <tang.junhui.linux@gmail.com>
Date:   Mon Oct 8 20:41:10 2018 +0800

    bcache: fix ioctl in flash device
    
    When doing ioctl in flash device, it will call ioctl_dev() in super.c,
    then we should not to get cached device since flash only device has
    no backend device. This patch just move the jugement dc->io_disable
    to cached_dev_ioctl() to make ioctl in flash device correctly.
    
    Fixes: 0f0709e6bfc3c ("bcache: stop bcache device when backing device is offline")
    Signed-off-by: Tang Junhui <tang.junhui.linux@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5fd1466b34e4..9de6695195bf 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -648,10 +648,6 @@ static int ioctl_dev(struct block_device *b, fmode_t mode,
 		     unsigned int cmd, unsigned long arg)
 {
 	struct bcache_device *d = b->bd_disk->private_data;
-	struct cached_dev *dc = container_of(d, struct cached_dev, disk);
-
-	if (dc->io_disable)
-		return -EIO;
 
 	return d->ioctl(d, mode, cmd, arg);
 }

commit 7a55948d38eb9b274cbbdd56dc1dd4b96ebfbe04
Author: Shenghui Wang <shhuiw@foxmail.com>
Date:   Mon Oct 8 20:41:07 2018 +0800

    bcache: account size of buckets used in uuid write to ca->meta_sectors_written
    
    UUIDs are considered as metadata. __uuid_write should add the number
    of buckets (in sectors) written to disk to ca->meta_sectors_written.
    Currently only 1 bucket is used in uuid write.
    
    Steps to test:
    1) create a fresh backing device and a fresh cache device separately.
       The backing device didn't attach to any cache set.
    2) cd /sys/block/<cache device>/bcache
       cat metadata_written      // record the output value
       cat bucket_size
    3) attach the backing device to cache set
    4) cat metadata_written
       The output value is almost the same as the value in step 2
       before the change.
       After the change, the value is bigger about 1 bucket size.
    
    Signed-off-by: Shenghui Wang <shhuiw@foxmail.com>
    Reviewed-by: Tang Junhui <tang.junhui.linux@gmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 30ba9aeb5ee8..5fd1466b34e4 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -418,6 +418,7 @@ static int __uuid_write(struct cache_set *c)
 {
 	BKEY_PADDED(key) k;
 	struct closure cl;
+	struct cache *ca;
 
 	closure_init_stack(&cl);
 	lockdep_assert_held(&bch_register_lock);
@@ -429,6 +430,10 @@ static int __uuid_write(struct cache_set *c)
 	uuid_io(c, REQ_OP_WRITE, 0, &k.key, &cl);
 	closure_sync(&cl);
 
+	/* Only one bucket used for uuid write */
+	ca = PTR_CACHE(c, &k.key, 0);
+	atomic_long_add(ca->sb.bucket_size, &ca->meta_sectors_written);
+
 	bkey_copy(&c->uuid_bucket, &k.key);
 	bkey_put(c, &k.key);
 	return 0;

commit 0f843e65d9eef4936929bb036c5f771fb261eea4
Author: Guoju Fang <fangguoju@gmail.com>
Date:   Thu Sep 27 23:41:46 2018 +0800

    bcache: add separate workqueue for journal_write to avoid deadlock
    
    After write SSD completed, bcache schedules journal_write work to
    system_wq, which is a public workqueue in system, without WQ_MEM_RECLAIM
    flag. system_wq is also a bound wq, and there may be no idle kworker on
    current processor. Creating a new kworker may unfortunately need to
    reclaim memory first, by shrinking cache and slab used by vfs, which
    depends on bcache device. That's a deadlock.
    
    This patch create a new workqueue for journal_write with WQ_MEM_RECLAIM
    flag. It's rescuer thread will work to avoid the deadlock.
    
    Signed-off-by: Guoju Fang <fangguoju@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 94c756c66bd7..30ba9aeb5ee8 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -47,6 +47,7 @@ static int bcache_major;
 static DEFINE_IDA(bcache_device_idx);
 static wait_queue_head_t unregister_wait;
 struct workqueue_struct *bcache_wq;
+struct workqueue_struct *bch_journal_wq;
 
 #define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
 /* limitation of partitions number on single bcache device */
@@ -2341,6 +2342,9 @@ static void bcache_exit(void)
 		kobject_put(bcache_kobj);
 	if (bcache_wq)
 		destroy_workqueue(bcache_wq);
+	if (bch_journal_wq)
+		destroy_workqueue(bch_journal_wq);
+
 	if (bcache_major)
 		unregister_blkdev(bcache_major, "bcache");
 	unregister_reboot_notifier(&reboot);
@@ -2370,6 +2374,10 @@ static int __init bcache_init(void)
 	if (!bcache_wq)
 		goto err;
 
+	bch_journal_wq = alloc_workqueue("bch_journal", WQ_MEM_RECLAIM, 0);
+	if (!bch_journal_wq)
+		goto err;
+
 	bcache_kobj = kobject_create_and_add("bcache", fs_kobj);
 	if (!bcache_kobj)
 		goto err;

commit eb2b3d034511f7f5f5fa781b9354ea4d5dbd4fc5
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:20:00 2018 +0800

    bcache: add the missing comments for smp_mb()/smp_wmb()
    
    Checkpatch.pl warns there are 2 locations of smp_mb() and smp_wmb()
    without code comment. This patch adds the missing code comments for
    these memory barrier calls.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e637d74e2908..94c756c66bd7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1136,11 +1136,11 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	list_move(&dc->list, &c->cached_devs);
 	calc_cached_dev_sectors(c);
 
-	smp_wmb();
 	/*
 	 * dc->c must be set before dc->count != 0 - paired with the mb in
 	 * cached_dev_get()
 	 */
+	smp_wmb();
 	refcount_set(&dc->count, 1);
 
 	/* Block writeback thread, but spawn it */

commit 87418ef9f07ac6bc00af7992dc1ccd96da46cd68
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:58 2018 +0800

    bcache: add missing SPDX header
    
    The SPDX header is missing fro closure.c, super.c and util.c, this
    patch adds SPDX header for GPL-2.0 into these files.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e3ecf08a10fc..e637d74e2908 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * bcache setup/teardown code, and some metadata io - read a superblock and
  * figure out what to do with it.

commit b3cf37bfa1a48acd5620d7be3dea05a4f76d87fd
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:57 2018 +0800

    bcache: move open brace at end of function definitions to next line
    
    This is not a preferred style to place open brace '{' at the end of
    function definition, checkpatch.pl reports error for such coding
    style. This patch moves them into the start of the next new line.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 01fc3c015a58..e3ecf08a10fc 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2152,7 +2152,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 kobj_attribute_write(register,		register_bcache);
 kobj_attribute_write(register_quiet,	register_bcache);
 
-static bool bch_is_open_backing(struct block_device *bdev) {
+static bool bch_is_open_backing(struct block_device *bdev)
+{
 	struct cache_set *c, *tc;
 	struct cached_dev *dc, *t;
 
@@ -2166,7 +2167,8 @@ static bool bch_is_open_backing(struct block_device *bdev) {
 	return false;
 }
 
-static bool bch_is_open_cache(struct block_device *bdev) {
+static bool bch_is_open_cache(struct block_device *bdev)
+{
 	struct cache_set *c, *tc;
 	struct cache *ca;
 	unsigned int i;
@@ -2178,7 +2180,8 @@ static bool bch_is_open_cache(struct block_device *bdev) {
 	return false;
 }
 
-static bool bch_is_open(struct block_device *bdev) {
+static bool bch_is_open(struct block_device *bdev)
+{
 	return bch_is_open_cache(bdev) || bch_is_open_backing(bdev);
 }
 

commit 3be11dbab67a3ed28358a950671de9b8e7fb5a02
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:55 2018 +0800

    bcache: fix code comments style
    
    This patch fixes 3 style issues warned by checkpatch.pl,
    - Comment lines are not aligned
    - Comments use "/*" on subsequent lines
    - Comment lines use a trailing "*/"
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2a7be104557e..01fc3c015a58 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -465,8 +465,8 @@ static struct uuid_entry *uuid_find_empty(struct cache_set *c)
  * Bucket priorities/gens:
  *
  * For each bucket, we store on disk its
-   * 8 bit gen
-   * 16 bit priority
+ *   8 bit gen
+ *  16 bit priority
  *
  * See alloc.c for an explanation of the gen. The priority is used to implement
  * lru (and in the future other) cache replacement policies; for most purposes
@@ -934,8 +934,10 @@ void bch_cached_dev_run(struct cached_dev *dc)
 
 	add_disk(d->disk);
 	bd_link_disk_holder(dc->bdev, dc->disk.disk);
-	/* won't show up in the uevent file, use udevadm monitor -e instead
-	 * only class / kset properties are persistent */
+	/*
+	 * won't show up in the uevent file, use udevadm monitor -e instead
+	 * only class / kset properties are persistent
+	 */
 	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
 	kfree(env[1]);
 	kfree(env[2]);
@@ -1104,8 +1106,9 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 		}
 	}
 
-	/* Deadlocks since we're called via sysfs...
-	sysfs_remove_file(&dc->kobj, &sysfs_attach);
+	/*
+	 * Deadlocks since we're called via sysfs...
+	 * sysfs_remove_file(&dc->kobj, &sysfs_attach);
 	 */
 
 	if (bch_is_zero(u->uuid, 16)) {
@@ -1468,9 +1471,10 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 	if (test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
 		pr_info("CACHE_SET_IO_DISABLE already set");
 
-	/* XXX: we can be called from atomic context
-	acquire_console_sem();
-	*/
+	/*
+	 * XXX: we can be called from atomic context
+	 * acquire_console_sem();
+	 */
 
 	pr_err("bcache: error on %pU: ", c->sb.set_uuid);
 

commit 6ae63e3501c493616612b90be26b35a6084fc75d
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:49 2018 +0800

    bcache: replace printk() by pr_*() routines
    
    There are still many places in bcache use printk to display kernel
    message, which are suggested to be preplaced by pr_*() routines like
    pr_err(), pr_info(), or pr_notice().
    
    This patch replaces all printk() with a proper pr_*() routine for
    bcache code.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 296fc8c31c6c..2a7be104557e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1472,13 +1472,13 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 	acquire_console_sem();
 	*/
 
-	printk(KERN_ERR "bcache: error on %pU: ", c->sb.set_uuid);
+	pr_err("bcache: error on %pU: ", c->sb.set_uuid);
 
 	va_start(args, fmt);
 	vprintk(fmt, args);
 	va_end(args);
 
-	printk(", disabling caching\n");
+	pr_err(", disabling caching\n");
 
 	if (c->on_error == ON_ERROR_PANIC)
 		panic("panic forced after error\n");

commit b0d30981c05f32d8cc032b209408ca3224f05f36
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:47 2018 +0800

    bcache: style fixes for lines over 80 characters
    
    This patch fixes the lines over 80 characters into more lines, to minimize
    warnings by checkpatch.pl. There are still some lines exceed 80 characters,
    but it is better to be a single line and I don't change them.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6a80069650c7..296fc8c31c6c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -149,7 +149,8 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 			goto err;
 
 		err = "Invalid superblock: device too small";
-		if (get_capacity(bdev->bd_disk) < sb->bucket_size * sb->nbuckets)
+		if (get_capacity(bdev->bd_disk) <
+		    sb->bucket_size * sb->nbuckets)
 			goto err;
 
 		err = "Bad UUID";
@@ -600,7 +601,8 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 
 			prio_io(ca, bucket, REQ_OP_READ, 0);
 
-			if (p->csum != bch_crc64(&p->magic, bucket_bytes(ca) - 8))
+			if (p->csum !=
+			    bch_crc64(&p->magic, bucket_bytes(ca) - 8))
 				pr_warn("bad csum reading priorities");
 
 			if (p->magic != pset_magic(&ca->sb))
@@ -1740,8 +1742,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	if (!(c->devices = kcalloc(c->nr_uuids, sizeof(void *), GFP_KERNEL)) ||
 	    mempool_init_slab_pool(&c->search, 32, bch_search_cache) ||
 	    mempool_init_kmalloc_pool(&c->bio_meta, 2,
-				      sizeof(struct bbio) + sizeof(struct bio_vec) *
-				      bucket_pages(c)) ||
+				sizeof(struct bbio) + sizeof(struct bio_vec) *
+				bucket_pages(c)) ||
 	    mempool_init_kmalloc_pool(&c->fill_iter, 1, iter_size) ||
 	    bioset_init(&c->bio_split, 4, offsetof(struct bbio, bio),
 			BIOSET_NEED_BVECS|BIOSET_NEED_RESCUER) ||
@@ -1813,7 +1815,9 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "error reading btree root";
-		c->root = bch_btree_node_get(c, NULL, k, j->btree_level, true, NULL);
+		c->root = bch_btree_node_get(c, NULL, k,
+					     j->btree_level,
+					     true, NULL);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 
@@ -2107,7 +2111,9 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 		goto err;
 	}
 
-	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache")) {
+	if (kobject_add(&ca->kobj,
+			&part_to_dev(bdev->bd_part)->kobj,
+			"bcache")) {
 		err = "error calling kobject_add";
 		ret = -ENOMEM;
 		goto out;

commit fc2d5988b5972bced859944986fb36d902ac3698
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:46 2018 +0800

    bcache: add identifier names to arguments of function definitions
    
    There are many function definitions do not have identifier argument names,
    scripts/checkpatch.pl complains warnings like this,
    
     WARNING: function definition argument 'struct bcache_device *' should
      also have an identifier name
      #16735: FILE: writeback.h:120:
      +void bch_sectors_dirty_init(struct bcache_device *);
    
    This patch adds identifier argument names to all bcache function
    definitions to fix such warnings.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c11cf852715c..6a80069650c7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2136,8 +2136,8 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 
 /* Global interfaces/init */
 
-static ssize_t register_bcache(struct kobject *, struct kobj_attribute *,
-			       const char *, size_t);
+static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
+			       const char *buffer, size_t size);
 
 kobj_attribute_write(register,		register_bcache);
 kobj_attribute_write(register_quiet,	register_bcache);

commit 1fae7cf05293d3a2c9e59c1bc59372322386467c
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:45 2018 +0800

    bcache: style fix to add a blank line after declarations
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4ab1b1968d9a..c11cf852715c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -415,8 +415,8 @@ static int __uuid_write(struct cache_set *c)
 {
 	BKEY_PADDED(key) k;
 	struct closure cl;
-	closure_init_stack(&cl);
 
+	closure_init_stack(&cl);
 	lockdep_assert_held(&bch_register_lock);
 
 	if (bch_bucket_alloc_set(c, RESERVE_BTREE, &k.key, 1, true))
@@ -456,6 +456,7 @@ static struct uuid_entry *uuid_find(struct cache_set *c, const char *uuid)
 static struct uuid_entry *uuid_find_empty(struct cache_set *c)
 {
 	static const char zero_uuid[16] = "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0";
+
 	return uuid_find(c, zero_uuid);
 }
 
@@ -619,6 +620,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 static int open_dev(struct block_device *b, fmode_t mode)
 {
 	struct bcache_device *d = b->bd_disk->private_data;
+
 	if (test_bit(BCACHE_DEV_CLOSING, &d->flags))
 		return -ENXIO;
 
@@ -629,6 +631,7 @@ static int open_dev(struct block_device *b, fmode_t mode)
 static void release_dev(struct gendisk *b, fmode_t mode)
 {
 	struct bcache_device *d = b->private_data;
+
 	closure_put(&d->cl);
 }
 
@@ -919,6 +922,7 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	if (!d->c &&
 	    BDEV_STATE(&dc->sb) != BDEV_STATE_NONE) {
 		struct closure cl;
+
 		closure_init_stack(&cl);
 
 		SET_BDEV_STATE(&dc->sb, BDEV_STATE_STALE);
@@ -976,6 +980,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 {
 	struct cached_dev *dc = container_of(w, struct cached_dev, detach);
 	struct closure cl;
+
 	closure_init_stack(&cl);
 
 	BUG_ON(!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags));
@@ -1103,6 +1108,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 
 	if (bch_is_zero(u->uuid, 16)) {
 		struct closure cl;
+
 		closure_init_stack(&cl);
 
 		memcpy(u->uuid, dc->sb.uuid, 16);
@@ -1320,6 +1326,7 @@ void bch_flash_dev_release(struct kobject *kobj)
 static void flash_dev_free(struct closure *cl)
 {
 	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
+
 	mutex_lock(&bch_register_lock);
 	atomic_long_sub(bcache_dev_sectors_dirty(d),
 			&d->c->flash_dev_dirty_sectors);
@@ -1481,6 +1488,7 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 void bch_cache_set_release(struct kobject *kobj)
 {
 	struct cache_set *c = container_of(kobj, struct cache_set, kobj);
+
 	kfree(c);
 	module_put(THIS_MODULE);
 }
@@ -1671,6 +1679,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 {
 	int iter_size;
 	struct cache_set *c = kzalloc(sizeof(struct cache_set), GFP_KERNEL);
+
 	if (!c)
 		return NULL;
 
@@ -2216,6 +2225,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	err = "failed to register device";
 	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
+
 		if (!dc)
 			goto err_close;
 
@@ -2224,6 +2234,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		mutex_unlock(&bch_register_lock);
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
+
 		if (!ca)
 			goto err_close;
 

commit 6f10f7d1b02b1bbc305f88d7696445dd38b13881
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:44 2018 +0800

    bcache: style fix to replace 'unsigned' by 'unsigned int'
    
    This patch fixes warning reported by checkpatch.pl by replacing 'unsigned'
    with 'unsigned int'.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 55a37641aa95..4ab1b1968d9a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -61,7 +61,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	const char *err;
 	struct cache_sb *s;
 	struct buffer_head *bh = __bread(bdev, 1, SB_SIZE);
-	unsigned i;
+	unsigned int i;
 
 	if (!bh)
 		return "IO error";
@@ -202,7 +202,7 @@ static void write_bdev_super_endio(struct bio *bio)
 static void __write_super(struct cache_sb *sb, struct bio *bio)
 {
 	struct cache_sb *out = page_address(bio_first_page_all(bio));
-	unsigned i;
+	unsigned int i;
 
 	bio->bi_iter.bi_sector	= SB_SECTOR;
 	bio->bi_iter.bi_size	= SB_SIZE;
@@ -282,7 +282,7 @@ void bcache_write_super(struct cache_set *c)
 {
 	struct closure *cl = &c->sb_write;
 	struct cache *ca;
-	unsigned i;
+	unsigned int i;
 
 	down(&c->sb_write_mutex);
 	closure_init(cl, &c->cl);
@@ -334,7 +334,7 @@ static void uuid_io(struct cache_set *c, int op, unsigned long op_flags,
 {
 	struct closure *cl = &c->uuid_write;
 	struct uuid_entry *u;
-	unsigned i;
+	unsigned int i;
 	char buf[80];
 
 	BUG_ON(!parent);
@@ -587,7 +587,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 	struct prio_set *p = ca->disk_buckets;
 	struct bucket_disk *d = p->data + prios_per_bucket(ca), *end = d;
 	struct bucket *b;
-	unsigned bucket_nr = 0;
+	unsigned int bucket_nr = 0;
 
 	for (b = ca->buckets;
 	     b < ca->buckets + ca->sb.nbuckets;
@@ -662,7 +662,7 @@ static void bcache_device_unlink(struct bcache_device *d)
 	lockdep_assert_held(&bch_register_lock);
 
 	if (d->c && !test_and_set_bit(BCACHE_DEV_UNLINK_DONE, &d->flags)) {
-		unsigned i;
+		unsigned int i;
 		struct cache *ca;
 
 		sysfs_remove_link(&d->c->kobj, d->name);
@@ -676,7 +676,7 @@ static void bcache_device_unlink(struct bcache_device *d)
 static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
 			       const char *name)
 {
-	unsigned i;
+	unsigned int i;
 	struct cache *ca;
 
 	for_each_cache(ca, d->c, i)
@@ -715,7 +715,7 @@ static void bcache_device_detach(struct bcache_device *d)
 }
 
 static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
-				 unsigned id)
+				 unsigned int id)
 {
 	d->id = id;
 	d->c = c;
@@ -762,7 +762,7 @@ static void bcache_device_free(struct bcache_device *d)
 	closure_debug_destroy(&d->cl);
 }
 
-static int bcache_device_init(struct bcache_device *d, unsigned block_size,
+static int bcache_device_init(struct bcache_device *d, unsigned int block_size,
 			      sector_t sectors)
 {
 	struct request_queue *q;
@@ -778,7 +778,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 
 	if (!d->nr_stripes || d->nr_stripes > max_stripes) {
 		pr_err("nr_stripes too large or invalid: %u (start sector beyond end of disk?)",
-			(unsigned)d->nr_stripes);
+			(unsigned int)d->nr_stripes);
 		return -ENOMEM;
 	}
 
@@ -1212,7 +1212,7 @@ static void cached_dev_flush(struct closure *cl)
 	continue_at(cl, cached_dev_free, system_wq);
 }
 
-static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
+static int cached_dev_init(struct cached_dev *dc, unsigned int block_size)
 {
 	int ret;
 	struct io *io;
@@ -1489,7 +1489,7 @@ static void cache_set_free(struct closure *cl)
 {
 	struct cache_set *c = container_of(cl, struct cache_set, cl);
 	struct cache *ca;
-	unsigned i;
+	unsigned int i;
 
 	if (!IS_ERR_OR_NULL(c->debug))
 		debugfs_remove(c->debug);
@@ -1532,7 +1532,7 @@ static void cache_set_flush(struct closure *cl)
 	struct cache_set *c = container_of(cl, struct cache_set, caching);
 	struct cache *ca;
 	struct btree *b;
-	unsigned i;
+	unsigned int i;
 
 	bch_cache_accounting_destroy(&c->accounting);
 
@@ -1762,7 +1762,7 @@ static void run_cache_set(struct cache_set *c)
 	struct cached_dev *dc, *t;
 	struct cache *ca;
 	struct closure cl;
-	unsigned i;
+	unsigned int i;
 
 	closure_init_stack(&cl);
 
@@ -1853,7 +1853,7 @@ static void run_cache_set(struct cache_set *c)
 		pr_notice("invalidating existing data");
 
 		for_each_cache(ca, c, i) {
-			unsigned j;
+			unsigned int j;
 
 			ca->sb.keys = clamp_t(int, ca->sb.nbuckets >> 7,
 					      2, SB_JOURNAL_BUCKETS);
@@ -1998,7 +1998,7 @@ static const char *register_cache_set(struct cache *ca)
 void bch_cache_release(struct kobject *kobj)
 {
 	struct cache *ca = container_of(kobj, struct cache, kobj);
-	unsigned i;
+	unsigned int i;
 
 	if (ca->set) {
 		BUG_ON(ca->set->cache[ca->sb.nr_this_dev] != ca);
@@ -2150,7 +2150,7 @@ static bool bch_is_open_backing(struct block_device *bdev) {
 static bool bch_is_open_cache(struct block_device *bdev) {
 	struct cache_set *c, *tc;
 	struct cache *ca;
-	unsigned i;
+	unsigned int i;
 
 	list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
 		for_each_cache(ca, c, i)

commit ea8c5356d39048bc94bae068228f51ddbecc6b89
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:49 2018 +0800

    bcache: set max writeback rate when I/O request is idle
    
    Commit b1092c9af9ed ("bcache: allow quick writeback when backing idle")
    allows the writeback rate to be faster if there is no I/O request on a
    bcache device. It works well if there is only one bcache device attached
    to the cache set. If there are many bcache devices attached to a cache
    set, it may introduce performance regression because multiple faster
    writeback threads of the idle bcache devices will compete the btree level
    locks with the bcache device who have I/O requests coming.
    
    This patch fixes the above issue by only permitting fast writebac when
    all bcache devices attached on the cache set are idle. And if one of the
    bcache devices has new I/O request coming, minimized all writeback
    throughput immediately and let PI controller __update_writeback_rate()
    to decide the upcoming writeback rate for each bcache device.
    
    Also when all bcache devices are idle, limited wrieback rate to a small
    number is wast of thoughput, especially when backing devices are slower
    non-rotation devices (e.g. SATA SSD). This patch sets a max writeback
    rate for each backing device if the whole cache set is idle. A faster
    writeback rate in idle time means new I/Os may have more available space
    for dirty data, and people may observe a better write performance then.
    
    Please note bcache may change its cache mode in run time, and this patch
    still works if the cache mode is switched from writeback mode and there
    is still dirty data on cache.
    
    Fixes: Commit b1092c9af9ed ("bcache: allow quick writeback when backing idle")
    Cc: stable@vger.kernel.org #4.16+
    Signed-off-by: Coly Li <colyli@suse.de>
    Tested-by: Kai Krakow <kai@kaishome.de>
    Tested-by: Stefan Priebe <s.priebe@profihost.ag>
    Cc: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1e85cbb4c159..55a37641aa95 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -696,6 +696,8 @@ static void bcache_device_detach(struct bcache_device *d)
 {
 	lockdep_assert_held(&bch_register_lock);
 
+	atomic_dec(&d->c->attached_dev_nr);
+
 	if (test_bit(BCACHE_DEV_DETACHING, &d->flags)) {
 		struct uuid_entry *u = d->c->uuids + d->id;
 
@@ -1144,6 +1146,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 
 	bch_cached_dev_run(dc);
 	bcache_device_link(&dc->disk, c, "bdev");
+	atomic_inc(&c->attached_dev_nr);
 
 	/* Allow the writeback thread to proceed */
 	up_write(&dc->writeback_lock);
@@ -1696,6 +1699,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	c->block_bits		= ilog2(sb->block_size);
 	c->nr_uuids		= bucket_bytes(c) / sizeof(struct uuid_entry);
 	c->devices_max_used	= 0;
+	atomic_set(&c->attached_dev_nr, 0);
 	c->btree_pages		= bucket_pages(c);
 	if (c->btree_pages > BTREE_MAX_PAGES)
 		c->btree_pages = max_t(int, c->btree_pages / 4,

commit e57fd746848597e9c701751e26416d4b2e3b27ea
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:45 2018 +0800

    bcache: add a comment in super.c
    
    This patch adds a line of code comment in super.c:register_bdev(), to
    make code to be more comprehensible.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c7ffa6ef3f82..1e85cbb4c159 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1291,6 +1291,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	pr_info("registered backing device %s", dc->backing_dev_name);
 
 	list_add(&dc->list, &uncached_devices);
+	/* attach to a matched cache set if it exists */
 	list_for_each_entry(c, &bch_cache_sets, list)
 		bch_cached_dev_attach(dc, c, NULL);
 

commit 78ac2107176baa0daf65b0fb8e561d2ed14c83ca
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:42 2018 +0800

    bcache: do not check return value of debugfs_create_dir()
    
    Greg KH suggests that normal code should not care about debugfs. Therefore
    no matter successful or failed of debugfs_create_dir() execution, it is
    unncessary to check its return value.
    
    There are two functions called debugfs_create_dir() and check the return
    value, which are bch_debug_init() and closure_debug_init(). This patch
    changes these two functions from int to void type, and ignore return values
    of debugfs_create_dir().
    
    This patch does not fix exact bug, just makes things work as they should.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Suggested-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org
    Cc: Kai Krakow <kai@kaishome.de>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e0a92104ca23..c7ffa6ef3f82 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2345,10 +2345,12 @@ static int __init bcache_init(void)
 		goto err;
 
 	if (bch_request_init() ||
-	    bch_debug_init(bcache_kobj) || closure_debug_init() ||
 	    sysfs_create_files(bcache_kobj, files))
 		goto err;
 
+	bch_debug_init(bcache_kobj);
+	closure_debug_init();
+
 	return 0;
 err:
 	bcache_exit();

commit 75cbb3f1d840429e6aa67b351332f38b29e6292c
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Jul 26 12:17:41 2018 +0800

    bcache: stop using the deprecated get_seconds()
    
    The get_seconds function is deprecated now since it returns a 32-bit
    value that will eventually overflow, and we are replacing it throughout
    the kernel with ktime_get_seconds() or ktime_get_real_seconds() that
    return a time64_t.
    
    bcache uses get_seconds() to read the current system time and store it in
    the superblock as well as in uuid_entry structures that are user visible.
    
    Unfortunately, the two structures in are still limited to 32 bits, so this
    won't fix any real problems but will still overflow in year 2106. Let's
    at least document that properly, in case we get an updated format in the
    future it can be fixed. We still have a long time before the overflow
    and checking the tools at https://github.com/koverstreet/bcache-tools
    reveals no access to any of them.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 40fe26fef00f..e0a92104ca23 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -181,7 +181,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 		goto err;
 	}
 
-	sb->last_mount = get_seconds();
+	sb->last_mount = (u32)ktime_get_real_seconds();
 	err = NULL;
 
 	get_page(bh->b_page);
@@ -701,7 +701,7 @@ static void bcache_device_detach(struct bcache_device *d)
 
 		SET_UUID_FLASH_ONLY(u, 0);
 		memcpy(u->uuid, invalid_uuid, 16);
-		u->invalidated = cpu_to_le32(get_seconds());
+		u->invalidated = cpu_to_le32((u32)ktime_get_real_seconds());
 		bch_uuid_write(d->c);
 	}
 
@@ -1033,7 +1033,7 @@ void bch_cached_dev_detach(struct cached_dev *dc)
 int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 			  uint8_t *set_uuid)
 {
-	uint32_t rtime = cpu_to_le32(get_seconds());
+	uint32_t rtime = cpu_to_le32((u32)ktime_get_real_seconds());
 	struct uuid_entry *u;
 	struct cached_dev *exist_dc, *t;
 
@@ -1076,7 +1076,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	    (BDEV_STATE(&dc->sb) == BDEV_STATE_STALE ||
 	     BDEV_STATE(&dc->sb) == BDEV_STATE_NONE)) {
 		memcpy(u->uuid, invalid_uuid, 16);
-		u->invalidated = cpu_to_le32(get_seconds());
+		u->invalidated = cpu_to_le32((u32)ktime_get_real_seconds());
 		u = NULL;
 	}
 
@@ -1398,7 +1398,7 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 
 	get_random_bytes(u->uuid, 16);
 	memset(u->label, 0, 32);
-	u->first_reg = u->last_reg = cpu_to_le32(get_seconds());
+	u->first_reg = u->last_reg = cpu_to_le32((u32)ktime_get_real_seconds());
 
 	SET_UUID_FLASH_ONLY(u, 1);
 	u->sectors = size >> 9;
@@ -1902,7 +1902,7 @@ static void run_cache_set(struct cache_set *c)
 		goto err;
 
 	closure_sync(&cl);
-	c->sb.last_mount = get_seconds();
+	c->sb.last_mount = (u32)ktime_get_real_seconds();
 	bcache_write_super(c);
 
 	list_for_each_entry_safe(dc, t, &uncached_devices, list)

commit 9b4e9f5abb94b671f998ffc0efebd1582852fee3
Author: Florian Schmaus <flo@geekplace.eu>
Date:   Thu Jul 26 12:17:40 2018 +0800

    bcache: do not assign in if condition in bcache_device_init()
    
    Fixes an error condition reported by checkpatch.pl which is caused by
    assigning a variable in an if condition.
    
    Signed-off-by: Florian Schmaus <flo@geekplace.eu>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a852018964ad..40fe26fef00f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -796,11 +796,12 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 		return idx;
 
 	if (bioset_init(&d->bio_split, 4, offsetof(struct bbio, bio),
-			BIOSET_NEED_BVECS|BIOSET_NEED_RESCUER) ||
-	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
-		ida_simple_remove(&bcache_device_idx, idx);
-		return -ENOMEM;
-	}
+			BIOSET_NEED_BVECS|BIOSET_NEED_RESCUER))
+		goto err;
+
+	d->disk = alloc_disk(BCACHE_MINORS);
+	if (!d->disk)
+		goto err;
 
 	set_capacity(d->disk, sectors);
 	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", idx);
@@ -834,6 +835,11 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	blk_queue_write_cache(q, true, true);
 
 	return 0;
+
+err:
+	ida_simple_remove(&bcache_device_idx, idx);
+	return -ENOMEM;
+
 }
 
 /* Cached device */

commit 16c1fdf4cfd6c0091e59b93ec2cb7e99973f8244
Author: Florian Schmaus <flo@geekplace.eu>
Date:   Thu Jul 26 12:17:39 2018 +0800

    bcache: do not assign in if condition in bcache_init()
    
    Fixes an error condition reported by checkpatch.pl which is caused by
    assigning a variable in an if condition.
    
    Signed-off-by: Florian Schmaus <flo@geekplace.eu>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 093b3789ce05..a852018964ad 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2330,9 +2330,15 @@ static int __init bcache_init(void)
 		return bcache_major;
 	}
 
-	if (!(bcache_wq = alloc_workqueue("bcache", WQ_MEM_RECLAIM, 0)) ||
-	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
-	    bch_request_init() ||
+	bcache_wq = alloc_workqueue("bcache", WQ_MEM_RECLAIM, 0);
+	if (!bcache_wq)
+		goto err;
+
+	bcache_kobj = kobject_create_and_add("bcache", fs_kobj);
+	if (!bcache_kobj)
+		goto err;
+
+	if (bch_request_init() ||
 	    bch_debug_init(bcache_kobj) || closure_debug_init() ||
 	    sysfs_create_files(bcache_kobj, files))
 		goto err;

commit a56489d4b3c914eb30b724ff25debc2e59c7950e
Author: Florian Schmaus <flo@geekplace.eu>
Date:   Thu Jul 26 12:17:37 2018 +0800

    bcache: do not assign in if condition register_bcache()
    
    Fixes an error condition reported by checkpatch.pl which is caused by
    assigning a variable in an if condition.
    
    Signed-off-by: Florian Schmaus <flo@geekplace.eu>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index cea2a42ea276..093b3789ce05 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2165,8 +2165,12 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (!try_module_get(THIS_MODULE))
 		return -EBUSY;
 
-	if (!(path = kstrndup(buffer, size, GFP_KERNEL)) ||
-	    !(sb = kmalloc(sizeof(struct cache_sb), GFP_KERNEL)))
+	path = kstrndup(buffer, size, GFP_KERNEL);
+	if (!path)
+		goto err;
+
+	sb = kmalloc(sizeof(struct cache_sb), GFP_KERNEL);
+	if (!sb)
 		goto err;
 
 	err = "failed to open device";

commit 99a27d59bd7b2ce1a82a4e826e8e7881f4d4954d
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Thu Jul 26 12:17:33 2018 +0800

    bcache: simplify the calculation of the total amount of flash dirty data
    
    Currently we calculate the total amount of flash only devices dirty data
    by adding the dirty data of each flash only device under registering
    locker. It is very inefficient.
    
    In this patch, we add a member flash_dev_dirty_sectors in struct cache_set
    to record the total amount of flash only devices dirty data in real time,
    so we didn't need to calculate the total amount of dirty data any more.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fa4058e43202..cea2a42ea276 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1311,6 +1311,8 @@ static void flash_dev_free(struct closure *cl)
 {
 	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
 	mutex_lock(&bch_register_lock);
+	atomic_long_sub(bcache_dev_sectors_dirty(d),
+			&d->c->flash_dev_dirty_sectors);
 	bcache_device_free(d);
 	mutex_unlock(&bch_register_lock);
 	kobject_put(&d->kobj);

commit fad953ce0b22cfd352a9a90b070c34b8791e6868
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 12 14:27:37 2018 -0700

    treewide: Use array_size() in vzalloc()
    
    The vzalloc() function has no 2-factor argument form, so multiplication
    factors need to be wrapped in array_size(). This patch replaces cases of:
    
            vzalloc(a * b)
    
    with:
            vzalloc(array_size(a, b))
    
    as well as handling cases of:
    
            vzalloc(a * b * c)
    
    with:
    
            vzalloc(array3_size(a, b, c))
    
    This does, however, attempt to ignore constant size factors like:
    
            vzalloc(4 * 1024)
    
    though any constants defined via macros get caught up in the conversion.
    
    Any factors with a sizeof() of "unsigned char", "char", and "u8" were
    dropped, since they're redundant.
    
    The Coccinelle script used for this was:
    
    // Fix redundant parens around sizeof().
    @@
    type TYPE;
    expression THING, E;
    @@
    
    (
      vzalloc(
    -       (sizeof(TYPE)) * E
    +       sizeof(TYPE) * E
      , ...)
    |
      vzalloc(
    -       (sizeof(THING)) * E
    +       sizeof(THING) * E
      , ...)
    )
    
    // Drop single-byte sizes and redundant parens.
    @@
    expression COUNT;
    typedef u8;
    typedef __u8;
    @@
    
    (
      vzalloc(
    -       sizeof(u8) * (COUNT)
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(__u8) * (COUNT)
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(char) * (COUNT)
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(unsigned char) * (COUNT)
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(u8) * COUNT
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(__u8) * COUNT
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(char) * COUNT
    +       COUNT
      , ...)
    |
      vzalloc(
    -       sizeof(unsigned char) * COUNT
    +       COUNT
      , ...)
    )
    
    // 2-factor product with sizeof(type/expression) and identifier or constant.
    @@
    type TYPE;
    expression THING;
    identifier COUNT_ID;
    constant COUNT_CONST;
    @@
    
    (
      vzalloc(
    -       sizeof(TYPE) * (COUNT_ID)
    +       array_size(COUNT_ID, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * COUNT_ID
    +       array_size(COUNT_ID, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * (COUNT_CONST)
    +       array_size(COUNT_CONST, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * COUNT_CONST
    +       array_size(COUNT_CONST, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * (COUNT_ID)
    +       array_size(COUNT_ID, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * COUNT_ID
    +       array_size(COUNT_ID, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * (COUNT_CONST)
    +       array_size(COUNT_CONST, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * COUNT_CONST
    +       array_size(COUNT_CONST, sizeof(THING))
      , ...)
    )
    
    // 2-factor product, only identifiers.
    @@
    identifier SIZE, COUNT;
    @@
    
      vzalloc(
    -       SIZE * COUNT
    +       array_size(COUNT, SIZE)
      , ...)
    
    // 3-factor product with 1 sizeof(type) or sizeof(expression), with
    // redundant parens removed.
    @@
    expression THING;
    identifier STRIDE, COUNT;
    type TYPE;
    @@
    
    (
      vzalloc(
    -       sizeof(TYPE) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      vzalloc(
    -       sizeof(THING) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    )
    
    // 3-factor product with 2 sizeof(variable), with redundant parens removed.
    @@
    expression THING1, THING2;
    identifier COUNT;
    type TYPE1, TYPE2;
    @@
    
    (
      vzalloc(
    -       sizeof(TYPE1) * sizeof(TYPE2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      vzalloc(
    -       sizeof(THING1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      vzalloc(
    -       sizeof(THING1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    |
      vzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    )
    
    // 3-factor product, only identifiers, with redundant parens removed.
    @@
    identifier STRIDE, SIZE, COUNT;
    @@
    
    (
      vzalloc(
    -       (COUNT) * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       COUNT * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       COUNT * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       (COUNT) * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       COUNT * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       (COUNT) * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       (COUNT) * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      vzalloc(
    -       COUNT * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    )
    
    // Any remaining multi-factor products, first at least 3-factor products
    // when they're not all constants...
    @@
    expression E1, E2, E3;
    constant C1, C2, C3;
    @@
    
    (
      vzalloc(C1 * C2 * C3, ...)
    |
      vzalloc(
    -       E1 * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    )
    
    // And then all remaining 2 factors products when they're not all constants.
    @@
    expression E1, E2;
    constant C1, C2;
    @@
    
    (
      vzalloc(C1 * C2, ...)
    |
      vzalloc(
    -       E1 * E2
    +       array_size(E1, E2)
      , ...)
    )
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index ec5f70d021de..fa4058e43202 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2041,8 +2041,8 @@ static int cache_alloc(struct cache *ca)
 	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
 	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
-	    !(ca->buckets	= vzalloc(sizeof(struct bucket) *
-					  ca->sb.nbuckets)) ||
+	    !(ca->buckets	= vzalloc(array_size(sizeof(struct bucket),
+						     ca->sb.nbuckets))) ||
 	    !(ca->prio_buckets	= kzalloc(array3_size(sizeof(uint64_t),
 						      prio_buckets(ca), 2),
 					  GFP_KERNEL)) ||

commit 6396bb221514d2876fd6dc0aa2a1f240d99b37bb
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 12 14:03:40 2018 -0700

    treewide: kzalloc() -> kcalloc()
    
    The kzalloc() function has a 2-factor argument form, kcalloc(). This
    patch replaces cases of:
    
            kzalloc(a * b, gfp)
    
    with:
            kcalloc(a * b, gfp)
    
    as well as handling cases of:
    
            kzalloc(a * b * c, gfp)
    
    with:
    
            kzalloc(array3_size(a, b, c), gfp)
    
    as it's slightly less ugly than:
    
            kzalloc_array(array_size(a, b), c, gfp)
    
    This does, however, attempt to ignore constant size factors like:
    
            kzalloc(4 * 1024, gfp)
    
    though any constants defined via macros get caught up in the conversion.
    
    Any factors with a sizeof() of "unsigned char", "char", and "u8" were
    dropped, since they're redundant.
    
    The Coccinelle script used for this was:
    
    // Fix redundant parens around sizeof().
    @@
    type TYPE;
    expression THING, E;
    @@
    
    (
      kzalloc(
    -       (sizeof(TYPE)) * E
    +       sizeof(TYPE) * E
      , ...)
    |
      kzalloc(
    -       (sizeof(THING)) * E
    +       sizeof(THING) * E
      , ...)
    )
    
    // Drop single-byte sizes and redundant parens.
    @@
    expression COUNT;
    typedef u8;
    typedef __u8;
    @@
    
    (
      kzalloc(
    -       sizeof(u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(__u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(char) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(unsigned char) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(u8) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(__u8) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(char) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc(
    -       sizeof(unsigned char) * COUNT
    +       COUNT
      , ...)
    )
    
    // 2-factor product with sizeof(type/expression) and identifier or constant.
    @@
    type TYPE;
    expression THING;
    identifier COUNT_ID;
    constant COUNT_CONST;
    @@
    
    (
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * (COUNT_ID)
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * COUNT_ID
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * COUNT_CONST
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * (COUNT_ID)
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * COUNT_ID
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(THING)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * COUNT_CONST
    +       COUNT_CONST, sizeof(THING)
      , ...)
    )
    
    // 2-factor product, only identifiers.
    @@
    identifier SIZE, COUNT;
    @@
    
    - kzalloc
    + kcalloc
      (
    -       SIZE * COUNT
    +       COUNT, SIZE
      , ...)
    
    // 3-factor product with 1 sizeof(type) or sizeof(expression), with
    // redundant parens removed.
    @@
    expression THING;
    identifier STRIDE, COUNT;
    type TYPE;
    @@
    
    (
      kzalloc(
    -       sizeof(TYPE) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc(
    -       sizeof(THING) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc(
    -       sizeof(THING) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc(
    -       sizeof(THING) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc(
    -       sizeof(THING) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    )
    
    // 3-factor product with 2 sizeof(variable), with redundant parens removed.
    @@
    expression THING1, THING2;
    identifier COUNT;
    type TYPE1, TYPE2;
    @@
    
    (
      kzalloc(
    -       sizeof(TYPE1) * sizeof(TYPE2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kzalloc(
    -       sizeof(THING1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kzalloc(
    -       sizeof(THING1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    |
      kzalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    )
    
    // 3-factor product, only identifiers, with redundant parens removed.
    @@
    identifier STRIDE, SIZE, COUNT;
    @@
    
    (
      kzalloc(
    -       (COUNT) * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       COUNT * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       COUNT * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       (COUNT) * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       COUNT * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       (COUNT) * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       (COUNT) * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc(
    -       COUNT * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    )
    
    // Any remaining multi-factor products, first at least 3-factor products,
    // when they're not all constants...
    @@
    expression E1, E2, E3;
    constant C1, C2, C3;
    @@
    
    (
      kzalloc(C1 * C2 * C3, ...)
    |
      kzalloc(
    -       (E1) * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc(
    -       (E1) * (E2) * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc(
    -       (E1) * (E2) * (E3)
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc(
    -       E1 * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    )
    
    // And then all remaining 2 factors products when they're not all constants,
    // keeping sizeof() as the second factor argument.
    @@
    expression THING, E1, E2;
    type TYPE;
    constant C1, C2, C3;
    @@
    
    (
      kzalloc(sizeof(THING) * C2, ...)
    |
      kzalloc(sizeof(TYPE) * C2, ...)
    |
      kzalloc(C1 * C2 * C3, ...)
    |
      kzalloc(C1 * C2, ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * (E2)
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(TYPE) * E2
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * (E2)
    +       E2, sizeof(THING)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       sizeof(THING) * E2
    +       E2, sizeof(THING)
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       (E1) * E2
    +       E1, E2
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       (E1) * (E2)
    +       E1, E2
      , ...)
    |
    - kzalloc
    + kcalloc
      (
    -       E1 * E2
    +       E1, E2
      , ...)
    )
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a31e55bcc4e5..ec5f70d021de 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1715,7 +1715,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	iter_size = (sb->bucket_size / sb->block_size + 1) *
 		sizeof(struct btree_iter_set);
 
-	if (!(c->devices = kzalloc(c->nr_uuids * sizeof(void *), GFP_KERNEL)) ||
+	if (!(c->devices = kcalloc(c->nr_uuids, sizeof(void *), GFP_KERNEL)) ||
 	    mempool_init_slab_pool(&c->search, 32, bch_search_cache) ||
 	    mempool_init_kmalloc_pool(&c->bio_meta, 2,
 				      sizeof(struct bbio) + sizeof(struct bio_vec) *
@@ -2043,8 +2043,9 @@ static int cache_alloc(struct cache *ca)
 	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
 	    !(ca->buckets	= vzalloc(sizeof(struct bucket) *
 					  ca->sb.nbuckets)) ||
-	    !(ca->prio_buckets	= kzalloc(sizeof(uint64_t) * prio_buckets(ca) *
-					  2, GFP_KERNEL)) ||
+	    !(ca->prio_buckets	= kzalloc(array3_size(sizeof(uint64_t),
+						      prio_buckets(ca), 2),
+					  GFP_KERNEL)) ||
 	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)))
 		return -ENOMEM;
 

commit d19936a26658a7a53edd5619d631ee2c2c3151a2
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sun May 20 18:25:51 2018 -0400

    bcache: convert to bioset_init()/mempool_init()
    
    Convert bcache to embedded bio sets.
    
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index bd438e96b742..a31e55bcc4e5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -753,8 +753,7 @@ static void bcache_device_free(struct bcache_device *d)
 		put_disk(d->disk);
 	}
 
-	if (d->bio_split)
-		bioset_free(d->bio_split);
+	bioset_exit(&d->bio_split);
 	kvfree(d->full_dirty_stripes);
 	kvfree(d->stripe_sectors_dirty);
 
@@ -796,9 +795,8 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (idx < 0)
 		return idx;
 
-	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio),
-					   BIOSET_NEED_BVECS |
-					   BIOSET_NEED_RESCUER)) ||
+	if (bioset_init(&d->bio_split, 4, offsetof(struct bbio, bio),
+			BIOSET_NEED_BVECS|BIOSET_NEED_RESCUER) ||
 	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
 		ida_simple_remove(&bcache_device_idx, idx);
 		return -ENOMEM;
@@ -1500,14 +1498,10 @@ static void cache_set_free(struct closure *cl)
 
 	if (c->moving_gc_wq)
 		destroy_workqueue(c->moving_gc_wq);
-	if (c->bio_split)
-		bioset_free(c->bio_split);
-	if (c->fill_iter)
-		mempool_destroy(c->fill_iter);
-	if (c->bio_meta)
-		mempool_destroy(c->bio_meta);
-	if (c->search)
-		mempool_destroy(c->search);
+	bioset_exit(&c->bio_split);
+	mempool_exit(&c->fill_iter);
+	mempool_exit(&c->bio_meta);
+	mempool_exit(&c->search);
 	kfree(c->devices);
 
 	mutex_lock(&bch_register_lock);
@@ -1718,21 +1712,17 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	INIT_LIST_HEAD(&c->btree_cache_freed);
 	INIT_LIST_HEAD(&c->data_buckets);
 
-	c->search = mempool_create_slab_pool(32, bch_search_cache);
-	if (!c->search)
-		goto err;
-
 	iter_size = (sb->bucket_size / sb->block_size + 1) *
 		sizeof(struct btree_iter_set);
 
 	if (!(c->devices = kzalloc(c->nr_uuids * sizeof(void *), GFP_KERNEL)) ||
-	    !(c->bio_meta = mempool_create_kmalloc_pool(2,
-				sizeof(struct bbio) + sizeof(struct bio_vec) *
-				bucket_pages(c))) ||
-	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
-	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio),
-					   BIOSET_NEED_BVECS |
-					   BIOSET_NEED_RESCUER)) ||
+	    mempool_init_slab_pool(&c->search, 32, bch_search_cache) ||
+	    mempool_init_kmalloc_pool(&c->bio_meta, 2,
+				      sizeof(struct bbio) + sizeof(struct bio_vec) *
+				      bucket_pages(c)) ||
+	    mempool_init_kmalloc_pool(&c->fill_iter, 1, iter_size) ||
+	    bioset_init(&c->bio_split, 4, offsetof(struct bbio, bio),
+			BIOSET_NEED_BVECS|BIOSET_NEED_RESCUER) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    !(c->moving_gc_wq = alloc_workqueue("bcache_gc",
 						WQ_MEM_RECLAIM, 0)) ||

commit 04cbc21137bfa4d7b8771a5b14f3d6c9b2aee671
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Mon May 28 15:37:42 2018 +0800

    bcache: Move couple of string arrays to sysfs.c
    
    There is couple of string arrays that are used exclusively in sysfs.c.
    Move it to there and make them static.
    
    Besides above, it will allow further clean up.
    
    No functional change intended.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a16b5b1df62a..bd438e96b742 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -37,24 +37,6 @@ static const char invalid_uuid[] = {
 	0xc8, 0x50, 0xfc, 0x5e, 0xcb, 0x16, 0xcd, 0x99
 };
 
-/* Default is -1; we skip past it for struct cached_dev's cache mode */
-const char * const bch_cache_modes[] = {
-	"default",
-	"writethrough",
-	"writeback",
-	"writearound",
-	"none",
-	NULL
-};
-
-/* Default is -1; we skip past it for stop_when_cache_set_failed */
-const char * const bch_stop_on_failure_modes[] = {
-	"default",
-	"auto",
-	"always",
-	NULL
-};
-
 static struct kobject *bcache_kobj;
 struct mutex bch_register_lock;
 LIST_HEAD(bch_cache_sets);

commit 0f0709e6bfc3ce4e8e1c0e8573490c45f76cfeee
Author: Coly Li <colyli@suse.de>
Date:   Mon May 28 15:37:41 2018 +0800

    bcache: stop bcache device when backing device is offline
    
    Currently bcache does not handle backing device failure, if backing
    device is offline and disconnected from system, its bcache device can still
    be accessible. If the bcache device is in writeback mode, I/O requests even
    can success if the requests hit on cache device. That is to say, when and
    how bcache handles offline backing device is undefined.
    
    This patch tries to handle backing device offline in a rather simple way,
    - Add cached_dev->status_update_thread kernel thread to update backing
      device status in every 1 second.
    - Add cached_dev->offline_seconds to record how many seconds the backing
      device is observed to be offline. If the backing device is offline for
      BACKING_DEV_OFFLINE_TIMEOUT (30) seconds, set dc->io_disable to 1 and
      call bcache_device_stop() to stop the bache device which linked to the
      offline backing device.
    
    Now if a backing device is offline for BACKING_DEV_OFFLINE_TIMEOUT seconds,
    its bcache device will be removed, then user space application writing on
    it will get error immediately, and handler the device failure in time.
    
    This patch is quite simple, does not handle more complicated situations.
    Once the bcache device is stopped, users need to recovery the backing
    device, register and attach it manually.
    
    Changelog:
    v3: call wait_for_kthread_stop() before exits kernel thread.
    v2: remove "bcache: " prefix when calling pr_warn().
    v1: initial version.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3dea06b41d43..a16b5b1df62a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -654,6 +654,11 @@ static int ioctl_dev(struct block_device *b, fmode_t mode,
 		     unsigned int cmd, unsigned long arg)
 {
 	struct bcache_device *d = b->bd_disk->private_data;
+	struct cached_dev *dc = container_of(d, struct cached_dev, disk);
+
+	if (dc->io_disable)
+		return -EIO;
+
 	return d->ioctl(d, mode, cmd, arg);
 }
 
@@ -864,6 +869,44 @@ static void calc_cached_dev_sectors(struct cache_set *c)
 	c->cached_dev_sectors = sectors;
 }
 
+#define BACKING_DEV_OFFLINE_TIMEOUT 5
+static int cached_dev_status_update(void *arg)
+{
+	struct cached_dev *dc = arg;
+	struct request_queue *q;
+
+	/*
+	 * If this delayed worker is stopping outside, directly quit here.
+	 * dc->io_disable might be set via sysfs interface, so check it
+	 * here too.
+	 */
+	while (!kthread_should_stop() && !dc->io_disable) {
+		q = bdev_get_queue(dc->bdev);
+		if (blk_queue_dying(q))
+			dc->offline_seconds++;
+		else
+			dc->offline_seconds = 0;
+
+		if (dc->offline_seconds >= BACKING_DEV_OFFLINE_TIMEOUT) {
+			pr_err("%s: device offline for %d seconds",
+			       dc->backing_dev_name,
+			       BACKING_DEV_OFFLINE_TIMEOUT);
+			pr_err("%s: disable I/O request due to backing "
+			       "device offline", dc->disk.name);
+			dc->io_disable = true;
+			/* let others know earlier that io_disable is true */
+			smp_mb();
+			bcache_device_stop(&dc->disk);
+			break;
+		}
+		schedule_timeout_interruptible(HZ);
+	}
+
+	wait_for_kthread_stop();
+	return 0;
+}
+
+
 void bch_cached_dev_run(struct cached_dev *dc)
 {
 	struct bcache_device *d = &dc->disk;
@@ -906,6 +949,14 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))
 		pr_debug("error creating sysfs link");
+
+	dc->status_update_thread = kthread_run(cached_dev_status_update,
+					       dc, "bcache_status_update");
+	if (IS_ERR(dc->status_update_thread)) {
+		pr_warn("failed to create bcache_status_update kthread, "
+			"continue to run without monitoring backing "
+			"device status");
+	}
 }
 
 /*
@@ -1139,6 +1190,8 @@ static void cached_dev_free(struct closure *cl)
 		kthread_stop(dc->writeback_thread);
 	if (dc->writeback_write_wq)
 		destroy_workqueue(dc->writeback_write_wq);
+	if (!IS_ERR_OR_NULL(dc->status_update_thread))
+		kthread_stop(dc->status_update_thread);
 
 	if (atomic_read(&dc->running))
 		bd_unlink_disk_holder(dc->bdev, dc->disk.disk);

commit 09a44ca2114737e0932257619c16a2b50c7807f1
Author: Coly Li <colyli@suse.de>
Date:   Thu May 3 18:51:37 2018 +0800

    bcache: use pr_info() to inform duplicated CACHE_SET_IO_DISABLE set
    
    It is possible that multiple I/O requests hits on failed cache device or
    backing device, therefore it is quite common that CACHE_SET_IO_DISABLE is
    set already when a task tries to set the bit from bch_cache_set_error().
    Currently the message "CACHE_SET_IO_DISABLE already set" is printed by
    pr_warn(), which might mislead users to think a serious fault happens in
    source code.
    
    This patch uses pr_info() to print the information in such situation,
    avoid extra worries. This information is helpful to understand bcache
    behavior in cache device failures, so I still keep them in source code.
    
    Fixes: 771f393e8ffc9 ("bcache: add CACHE_SET_IO_DISABLE to struct cache_set flags")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index cedbb41533c2..3dea06b41d43 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1412,7 +1412,7 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 		return false;
 
 	if (test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
-		pr_warn("CACHE_SET_IO_DISABLE already set");
+		pr_info("CACHE_SET_IO_DISABLE already set");
 
 	/* XXX: we can be called from atomic context
 	acquire_console_sem();

commit 4fd8e13843978cbba48b8c21119da60c7fd5910d
Author: Coly Li <colyli@suse.de>
Date:   Thu May 3 18:51:36 2018 +0800

    bcache: set dc->io_disable to true in conditional_stop_bcache_device()
    
    Commit 7e027ca4b534b ("bcache: add stop_when_cache_set_failed option to
    backing device") adds stop_when_cache_set_failed option and stops bcache
    device if stop_when_cache_set_failed is auto and there is dirty data on
    broken cache device. There might exists a small time gap that the cache
    set is released and set to NULL but bcache device is not released yet
    (because they are released in parallel). During this time gap, dc->c is
    NULL so CACHE_SET_IO_DISABLE won't be checked, and dc->io_disable is still
    false, so new coming I/O requests will be accepted and directly go into
    backing device as no cache set attached to. If there is dirty data on
    cache device, this behavior may introduce potential inconsistent data.
    
    This patch sets dc->io_disable to true before calling bcache_device_stop()
    to make sure the backing device will reject new coming I/O request as
    well, so even in the small time gap no I/O will directly go into backing
    device to corrupt data consistency.
    
    Fixes: 7e027ca4b534b ("bcache: add stop_when_cache_set_failed option to backing device")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c017cd444c66..cedbb41533c2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1556,6 +1556,20 @@ static void conditional_stop_bcache_device(struct cache_set *c,
 		 */
 		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is dirty, stop it to avoid potential data corruption.",
 			d->disk->disk_name);
+			/*
+			 * There might be a small time gap that cache set is
+			 * released but bcache device is not. Inside this time
+			 * gap, regular I/O requests will directly go into
+			 * backing device as no cache set attached to. This
+			 * behavior may also introduce potential inconsistence
+			 * data in writeback mode while cache is dirty.
+			 * Therefore before calling bcache_device_stop() due
+			 * to a broken cache device, dc->io_disable should be
+			 * explicitly set to true.
+			 */
+			dc->io_disable = true;
+			/* make others know io_disable is true earlier */
+			smp_mb();
 			bcache_device_stop(d);
 	} else {
 		/*

commit 6147305c73e4511ca1a975b766b97a779d442567
Author: Coly Li <colyli@suse.de>
Date:   Thu May 3 18:51:33 2018 +0800

    bcache: set CACHE_SET_IO_DISABLE in bch_cached_dev_error()
    
    Commit c7b7bd07404c5 ("bcache: add io_disable to struct cached_dev") tries
    to stop bcache device by calling bcache_device_stop() when too many I/O
    errors happened on backing device. But if there is internal I/O happening
    on cache device (writeback scan, garbage collection, etc), a regular I/O
    request triggers the internal I/Os may still holds a refcount of dc->count,
    and the refcount may only be dropped after the internal I/O stopped.
    
    By this patch, bch_cached_dev_error() will check if the backing device is
    attached to a cache set, if yes that CACHE_SET_IO_DISABLE will be set to
    flags of this cache set. Then internal I/Os on cache device will be
    rejected and stopped immediately, and the bcache device can be stopped.
    
    For people who are not familiar with the interesting refcount dependance,
    let me explain a bit more how the fix works. Example the writeback thread
    will scan cache device for dirty data writeback purpose. Before it stopps,
    it holds a refcount of dc->count. When CACHE_SET_IO_DISABLE bit is set,
    the internal I/O will stopped and the while-loop in bch_writeback_thread()
    quits and calls cached_dev_put() to drop dc->count. If this is the last
    refcount to drop, then cached_dev_detach_finish() will be called. In this
    call back function, in turn closure_put(dc->disk.cl) is called to drop a
    refcount of closure dc->disk.cl. If this is the last refcount of this
    closure to drop, then cached_dev_flush() will be called. Then the cached
    device is freed. So if CACHE_SET_IO_DISABLE is not set, the bache device
    can not be stopped until all inernal cache device I/O stopped. For large
    size cache device, and writeback thread competes locks with gc thread,
    there might be a quite long time to wait.
    
    Fixes: c7b7bd07404c5 ("bcache: add io_disable to struct cached_dev")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8196b19fada2..c017cd444c66 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1369,6 +1369,8 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 
 bool bch_cached_dev_error(struct cached_dev *dc)
 {
+	struct cache_set *c;
+
 	if (!dc || test_bit(BCACHE_DEV_CLOSING, &dc->disk.flags))
 		return false;
 
@@ -1379,6 +1381,21 @@ bool bch_cached_dev_error(struct cached_dev *dc)
 	pr_err("stop %s: too many IO errors on backing device %s\n",
 		dc->disk.disk->disk_name, dc->backing_dev_name);
 
+	/*
+	 * If the cached device is still attached to a cache set,
+	 * even dc->io_disable is true and no more I/O requests
+	 * accepted, cache device internal I/O (writeback scan or
+	 * garbage collection) may still prevent bcache device from
+	 * being stopped. So here CACHE_SET_IO_DISABLE should be
+	 * set to c->flags too, to make the internal I/O to cache
+	 * device rejected and stopped immediately.
+	 * If c is NULL, that means the bcache device is not attached
+	 * to any cache set, then no CACHE_SET_IO_DISABLE bit to set.
+	 */
+	c = dc->disk.c;
+	if (c && test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
+		pr_info("CACHE_SET_IO_DISABLE already set");
+
 	bcache_device_stop(&dc->disk);
 	return true;
 }

commit 6e916a7eb1bc045f4e27355632ee7692014e6e60
Author: Coly Li <colyli@suse.de>
Date:   Thu May 3 18:51:32 2018 +0800

    bcache: store disk name in struct cache and struct cached_dev
    
    Current code uses bdevname() or bio_devname() to reference gendisk
    disk name when bcache needs to display the disk names in kernel message.
    It was safe before bcache device failure handling patch set merged in,
    because when devices are failed, there was deadlock to prevent bcache
    printing error messages with gendisk disk name. But after the failure
    handling patch set merged, the deadlock is fixed, so it is possible
    that the gendisk structure bdev->hd_disk is released when bdevname() is
    called to reference bdev->bd_disk->disk_name[]. This is why I receive
    bug report of NULL pointers deference panic.
    
    This patch stores gendisk disk name in a buffer inside struct cache and
    struct cached_dev, then print out the offline device name won't reference
    bdev->hd_disk anymore. And this patch also avoids extra function calls
    of bdevname() and bio_devnmae().
    
    Changelog:
    v3, add Reviewed-by from Hannes.
    v2, call bdevname() earlier in register_bdev()
    v1, first version with segguestion from Junhui Tang.
    
    Fixes: c7b7bd07404c5 ("bcache: add io_disable to struct cached_dev")
    Fixes: 5138ac6748e38 ("bcache: fix misleading error message in bch_count_io_errors()")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d90d9e59ca00..8196b19fada2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -936,7 +936,6 @@ static void cancel_writeback_rate_update_dwork(struct cached_dev *dc)
 static void cached_dev_detach_finish(struct work_struct *w)
 {
 	struct cached_dev *dc = container_of(w, struct cached_dev, detach);
-	char buf[BDEVNAME_SIZE];
 	struct closure cl;
 	closure_init_stack(&cl);
 
@@ -967,7 +966,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 
 	mutex_unlock(&bch_register_lock);
 
-	pr_info("Caching disabled for %s", bdevname(dc->bdev, buf));
+	pr_info("Caching disabled for %s", dc->backing_dev_name);
 
 	/* Drop ref we took in cached_dev_detach() */
 	closure_put(&dc->disk.cl);
@@ -999,29 +998,28 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 {
 	uint32_t rtime = cpu_to_le32(get_seconds());
 	struct uuid_entry *u;
-	char buf[BDEVNAME_SIZE];
 	struct cached_dev *exist_dc, *t;
 
-	bdevname(dc->bdev, buf);
-
 	if ((set_uuid && memcmp(set_uuid, c->sb.set_uuid, 16)) ||
 	    (!set_uuid && memcmp(dc->sb.set_uuid, c->sb.set_uuid, 16)))
 		return -ENOENT;
 
 	if (dc->disk.c) {
-		pr_err("Can't attach %s: already attached", buf);
+		pr_err("Can't attach %s: already attached",
+		       dc->backing_dev_name);
 		return -EINVAL;
 	}
 
 	if (test_bit(CACHE_SET_STOPPING, &c->flags)) {
-		pr_err("Can't attach %s: shutting down", buf);
+		pr_err("Can't attach %s: shutting down",
+		       dc->backing_dev_name);
 		return -EINVAL;
 	}
 
 	if (dc->sb.block_size < c->sb.block_size) {
 		/* Will die */
 		pr_err("Couldn't attach %s: block size less than set's block size",
-		       buf);
+		       dc->backing_dev_name);
 		return -EINVAL;
 	}
 
@@ -1029,7 +1027,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	list_for_each_entry_safe(exist_dc, t, &c->cached_devs, list) {
 		if (!memcmp(dc->sb.uuid, exist_dc->sb.uuid, 16)) {
 			pr_err("Tried to attach %s but duplicate UUID already attached",
-				buf);
+				dc->backing_dev_name);
 
 			return -EINVAL;
 		}
@@ -1047,13 +1045,15 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 
 	if (!u) {
 		if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
-			pr_err("Couldn't find uuid for %s in set", buf);
+			pr_err("Couldn't find uuid for %s in set",
+			       dc->backing_dev_name);
 			return -ENOENT;
 		}
 
 		u = uuid_find_empty(c);
 		if (!u) {
-			pr_err("Not caching %s, no room for UUID", buf);
+			pr_err("Not caching %s, no room for UUID",
+			       dc->backing_dev_name);
 			return -EINVAL;
 		}
 	}
@@ -1112,7 +1112,8 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	up_write(&dc->writeback_lock);
 
 	pr_info("Caching %s as %s on set %pU",
-		bdevname(dc->bdev, buf), dc->disk.disk->disk_name,
+		dc->backing_dev_name,
+		dc->disk.disk->disk_name,
 		dc->disk.c->sb.set_uuid);
 	return 0;
 }
@@ -1225,10 +1226,10 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 				 struct block_device *bdev,
 				 struct cached_dev *dc)
 {
-	char name[BDEVNAME_SIZE];
 	const char *err = "cannot allocate memory";
 	struct cache_set *c;
 
+	bdevname(bdev, dc->backing_dev_name);
 	memcpy(&dc->sb, sb, sizeof(struct cache_sb));
 	dc->bdev = bdev;
 	dc->bdev->bd_holder = dc;
@@ -1237,6 +1238,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	bio_first_bvec_all(&dc->sb_bio)->bv_page = sb_page;
 	get_page(sb_page);
 
+
 	if (cached_dev_init(dc, sb->block_size << 9))
 		goto err;
 
@@ -1247,7 +1249,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	if (bch_cache_accounting_add_kobjs(&dc->accounting, &dc->disk.kobj))
 		goto err;
 
-	pr_info("registered backing device %s", bdevname(bdev, name));
+	pr_info("registered backing device %s", dc->backing_dev_name);
 
 	list_add(&dc->list, &uncached_devices);
 	list_for_each_entry(c, &bch_cache_sets, list)
@@ -1259,7 +1261,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	return;
 err:
-	pr_notice("error %s: %s", bdevname(bdev, name), err);
+	pr_notice("error %s: %s", dc->backing_dev_name, err);
 	bcache_device_stop(&dc->disk);
 }
 
@@ -1367,8 +1369,6 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 
 bool bch_cached_dev_error(struct cached_dev *dc)
 {
-	char name[BDEVNAME_SIZE];
-
 	if (!dc || test_bit(BCACHE_DEV_CLOSING, &dc->disk.flags))
 		return false;
 
@@ -1377,7 +1377,7 @@ bool bch_cached_dev_error(struct cached_dev *dc)
 	smp_mb();
 
 	pr_err("stop %s: too many IO errors on backing device %s\n",
-		dc->disk.disk->disk_name, bdevname(dc->bdev, name));
+		dc->disk.disk->disk_name, dc->backing_dev_name);
 
 	bcache_device_stop(&dc->disk);
 	return true;
@@ -2003,12 +2003,10 @@ static int cache_alloc(struct cache *ca)
 static int register_cache(struct cache_sb *sb, struct page *sb_page,
 				struct block_device *bdev, struct cache *ca)
 {
-	char name[BDEVNAME_SIZE];
 	const char *err = NULL; /* must be set for any error case */
 	int ret = 0;
 
-	bdevname(bdev, name);
-
+	bdevname(bdev, ca->cache_dev_name);
 	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
 	ca->bdev = bdev;
 	ca->bdev->bd_holder = ca;
@@ -2045,14 +2043,14 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 		goto out;
 	}
 
-	pr_info("registered cache device %s", name);
+	pr_info("registered cache device %s", ca->cache_dev_name);
 
 out:
 	kobject_put(&ca->kobj);
 
 err:
 	if (err)
-		pr_notice("error %s: %s", name, err);
+		pr_notice("error %s: %s", ca->cache_dev_name, err);
 
 	return ret;
 }

commit 3526dd0c7832f1011a0477cc6d903662bae05ea8
Merge: dd972f924df6 bc6d65e6dc89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 5 14:27:02 2018 -0700

    Merge tag 'for-4.17/block-20180402' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
     "It's a pretty quiet round this time, which is nice. This contains:
    
       - series from Bart, cleaning up the way we set/test/clear atomic
         queue flags.
    
       - series from Bart, fixing races between gendisk and queue
         registration and removal.
    
       - set of bcache fixes and improvements from various folks, by way of
         Michael Lyle.
    
       - set of lightnvm updates from Matias, most of it being the 1.2 to
         2.0 transition.
    
       - removal of unused DIO flags from Nikolay.
    
       - blk-mq/sbitmap memory ordering fixes from Omar.
    
       - divide-by-zero fix for BFQ from Paolo.
    
       - minor documentation patches from Randy.
    
       - timeout fix from Tejun.
    
       - Alpha "can't write a char atomically" fix from Mikulas.
    
       - set of NVMe fixes by way of Keith.
    
       - bsg and bsg-lib improvements from Christoph.
    
       - a few sed-opal fixes from Jonas.
    
       - cdrom check-disk-change deadlock fix from Maurizio.
    
       - various little fixes, comment fixes, etc from various folks"
    
    * tag 'for-4.17/block-20180402' of git://git.kernel.dk/linux-block: (139 commits)
      blk-mq: Directly schedule q->timeout_work when aborting a request
      blktrace: fix comment in blktrace_api.h
      lightnvm: remove function name in strings
      lightnvm: pblk: remove some unnecessary NULL checks
      lightnvm: pblk: don't recover unwritten lines
      lightnvm: pblk: implement 2.0 support
      lightnvm: pblk: implement get log report chunk
      lightnvm: pblk: rename ppaf* to addrf*
      lightnvm: pblk: check for supported version
      lightnvm: implement get log report chunk helpers
      lightnvm: make address conversions depend on generic device
      lightnvm: add support for 2.0 address format
      lightnvm: normalize geometry nomenclature
      lightnvm: complete geo structure with maxoc*
      lightnvm: add shorten OCSSD version in geo
      lightnvm: add minor version to generic geometry
      lightnvm: simplify geometry structure
      lightnvm: pblk: refactor init/exit sequences
      lightnvm: Avoid validation of default op value
      lightnvm: centralize permission check for lightnvm ioctl
      ...

commit 5f2b18ec8e1643410a2369f06888951cdedea0bf
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Sun Mar 18 17:36:33 2018 -0700

    bcache: Fix a compiler warning in bcache_device_init()
    
    Avoid that building with W=1 triggers the following compiler warning:
    
    drivers/md/bcache/super.c:776:20: warning: comparison is always false due to limited range of data type [-Wtype-limits]
          d->nr_stripes > SIZE_MAX / sizeof(atomic_t)) {
                        ^
    
    Reviewed-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 06f4b4833755..a21694788619 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -778,6 +778,8 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 			      sector_t sectors)
 {
 	struct request_queue *q;
+	const size_t max_stripes = min_t(size_t, INT_MAX,
+					 SIZE_MAX / sizeof(atomic_t));
 	size_t n;
 	int idx;
 
@@ -786,9 +788,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 
 	d->nr_stripes = DIV_ROUND_UP_ULL(sectors, d->stripe_size);
 
-	if (!d->nr_stripes ||
-	    d->nr_stripes > INT_MAX ||
-	    d->nr_stripes > SIZE_MAX / sizeof(atomic_t)) {
+	if (!d->nr_stripes || d->nr_stripes > max_stripes) {
 		pr_err("nr_stripes too large or invalid: %u (start sector beyond end of disk?)",
 			(unsigned)d->nr_stripes);
 		return -ENOMEM;

commit c7b7bd07404c52d8b9c6fd2fe794052ac367a818
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:25 2018 -0700

    bcache: add io_disable to struct cached_dev
    
    If a bcache device is configured to writeback mode, current code does not
    handle write I/O errors on backing devices properly.
    
    In writeback mode, write request is written to cache device, and
    latter being flushed to backing device. If I/O failed when writing from
    cache device to the backing device, bcache code just ignores the error and
    upper layer code is NOT noticed that the backing device is broken.
    
    This patch tries to handle backing device failure like how the cache device
    failure is handled,
    - Add a error counter 'io_errors' and error limit 'error_limit' in struct
      cached_dev. Add another io_disable to struct cached_dev to disable I/Os
      on the problematic backing device.
    - When I/O error happens on backing device, increase io_errors counter. And
      if io_errors reaches error_limit, set cache_dev->io_disable to true, and
      stop the bcache device.
    
    The result is, if backing device is broken of disconnected, and I/O errors
    reach its error limit, backing device will be disabled and the associated
    bcache device will be removed from system.
    
    Changelog:
    v2: remove "bcache: " prefix in pr_error(), and use correct name string to
        print out bcache device gendisk name.
    v1: indeed this is new added in v2 patch set.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2f8e70aefc90..06f4b4833755 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1197,6 +1197,9 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 		max(dc->disk.disk->queue->backing_dev_info->ra_pages,
 		    q->backing_dev_info->ra_pages);
 
+	atomic_set(&dc->io_errors, 0);
+	dc->io_disable = false;
+	dc->error_limit = DEFAULT_CACHED_DEV_ERROR_LIMIT;
 	/* default to auto */
 	dc->stop_when_cache_set_failed = BCH_CACHED_DEV_STOP_AUTO;
 
@@ -1351,6 +1354,24 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 	return flash_dev_run(c, u);
 }
 
+bool bch_cached_dev_error(struct cached_dev *dc)
+{
+	char name[BDEVNAME_SIZE];
+
+	if (!dc || test_bit(BCACHE_DEV_CLOSING, &dc->disk.flags))
+		return false;
+
+	dc->io_disable = true;
+	/* make others know io_disable is true earlier */
+	smp_mb();
+
+	pr_err("stop %s: too many IO errors on backing device %s\n",
+		dc->disk.disk->disk_name, bdevname(dc->bdev, name));
+
+	bcache_device_stop(&dc->disk);
+	return true;
+}
+
 /* Cache set */
 
 __printf(2, 3)

commit 27a40ab9269e79b55672312b324f8f29d94463d4
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:24 2018 -0700

    bcache: add backing_request_endio() for bi_end_io
    
    In order to catch I/O error of backing device, a separate bi_end_io
    call back is required. Then a per backing device counter can record I/O
    errors number and retire the backing device if the counter reaches a
    per backing device I/O error limit.
    
    This patch adds backing_request_endio() to bcache backing device I/O code
    path, this is a preparation for further complicated backing device failure
    handling. So far there is no real code logic change, I make this change a
    separate patch to make sure it is stable and reliable for further work.
    
    Changelog:
    v2: Fix code comments typo, remove a redundant bch_writeback_add() line
        added in v4 patch set.
    v1: indeed this is new added in this patch set.
    
    [mlyle: truncated commit subject]
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Cc: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f1f64853114b..2f8e70aefc90 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -273,6 +273,7 @@ void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 	bio->bi_private = dc;
 
 	closure_get(cl);
+	/* I/O request sent to backing device */
 	__write_super(&dc->sb, bio);
 
 	closure_return_with_destructor(cl, bch_write_bdev_super_unlock);

commit df2b94313ae5b4f60d49e01d4dff5acb4c2757cf
Author: Chengguang Xu <cgxu519@gmx.com>
Date:   Sun Mar 18 17:36:23 2018 -0700

    bcache: move closure debug file into debug directory
    
    In current code closure debug file is outside of debug directory
    and when unloading module there is lack of removing operation
    for closure debug file, so it will cause creating error when trying
    to reload  module.
    
    This patch move closure debug file into "bcache" debug direcory
    so that the file can get deleted properly.
    
    Signed-off-by: Chengguang Xu <cgxu519@gmx.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7b45160e9a22..f1f64853114b 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2224,7 +2224,6 @@ static int __init bcache_init(void)
 	mutex_init(&bch_register_lock);
 	init_waitqueue_head(&unregister_wait);
 	register_reboot_notifier(&reboot);
-	closure_debug_init();
 
 	bcache_major = register_blkdev(0, "bcache");
 	if (bcache_major < 0) {
@@ -2236,7 +2235,7 @@ static int __init bcache_init(void)
 	if (!(bcache_wq = alloc_workqueue("bcache", WQ_MEM_RECLAIM, 0)) ||
 	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
 	    bch_request_init() ||
-	    bch_debug_init(bcache_kobj) ||
+	    bch_debug_init(bcache_kobj) || closure_debug_init() ||
 	    sysfs_create_files(bcache_kobj, files))
 		goto err;
 

commit 7e027ca4b534b6b99a7c0471e13ba075ffa3f482
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:18 2018 -0700

    bcache: add stop_when_cache_set_failed option to backing device
    
    When there are too many I/O errors on cache device, current bcache code
    will retire the whole cache set, and detach all bcache devices. But the
    detached bcache devices are not stopped, which is problematic when bcache
    is in writeback mode.
    
    If the retired cache set has dirty data of backing devices, continue
    writing to bcache device will write to backing device directly. If the
    LBA of write request has a dirty version cached on cache device, next time
    when the cache device is re-registered and backing device re-attached to
    it again, the stale dirty data on cache device will be written to backing
    device, and overwrite latest directly written data. This situation causes
    a quite data corruption.
    
    But we cannot simply stop all attached bcache devices when the cache set is
    broken or disconnected. For example, use bcache to accelerate performance
    of an email service. In such workload, if cache device is broken but no
    dirty data lost, keep the bcache device alive and permit email service
    continue to access user data might be a better solution for the cache
    device failure.
    
    Nix <nix@esperi.org.uk> points out the issue and provides the above example
    to explain why it might be necessary to not stop bcache device for broken
    cache device. Pavel Goran <via-bcache@pvgoran.name> provides a brilliant
    suggestion to provide "always" and "auto" options to per-cached device
    sysfs file stop_when_cache_set_failed. If cache set is retiring and the
    backing device has no dirty data on cache, it should be safe to keep the
    bcache device alive. In this case, if stop_when_cache_set_failed is set to
    "auto", the device failure handling code will not stop this bcache device
    and permit application to access the backing device with a unattached
    bcache device.
    
    Changelog:
    [mlyle: edited to not break string constants across lines]
    v3: fix typos pointed out by Nix.
    v2: change option values of stop_when_cache_set_failed from 1/0 to
        "auto"/"always".
    v1: initial version, stop_when_cache_set_failed can be 0 (not stop) or 1
        (always stop).
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Cc: Nix <nix@esperi.org.uk>
    Cc: Pavel Goran <via-bcache@pvgoran.name>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index dda4ccdd1360..7b45160e9a22 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -47,6 +47,14 @@ const char * const bch_cache_modes[] = {
 	NULL
 };
 
+/* Default is -1; we skip past it for stop_when_cache_set_failed */
+const char * const bch_stop_on_failure_modes[] = {
+	"default",
+	"auto",
+	"always",
+	NULL
+};
+
 static struct kobject *bcache_kobj;
 struct mutex bch_register_lock;
 LIST_HEAD(bch_cache_sets);
@@ -1188,6 +1196,9 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 		max(dc->disk.disk->queue->backing_dev_info->ra_pages,
 		    q->backing_dev_info->ra_pages);
 
+	/* default to auto */
+	dc->stop_when_cache_set_failed = BCH_CACHED_DEV_STOP_AUTO;
+
 	bch_cached_dev_request_init(dc);
 	bch_cached_dev_writeback_init(dc);
 	return 0;
@@ -1464,25 +1475,72 @@ static void cache_set_flush(struct closure *cl)
 	closure_return(cl);
 }
 
+/*
+ * This function is only called when CACHE_SET_IO_DISABLE is set, which means
+ * cache set is unregistering due to too many I/O errors. In this condition,
+ * the bcache device might be stopped, it depends on stop_when_cache_set_failed
+ * value and whether the broken cache has dirty data:
+ *
+ * dc->stop_when_cache_set_failed    dc->has_dirty   stop bcache device
+ *  BCH_CACHED_STOP_AUTO               0               NO
+ *  BCH_CACHED_STOP_AUTO               1               YES
+ *  BCH_CACHED_DEV_STOP_ALWAYS         0               YES
+ *  BCH_CACHED_DEV_STOP_ALWAYS         1               YES
+ *
+ * The expected behavior is, if stop_when_cache_set_failed is configured to
+ * "auto" via sysfs interface, the bcache device will not be stopped if the
+ * backing device is clean on the broken cache device.
+ */
+static void conditional_stop_bcache_device(struct cache_set *c,
+					   struct bcache_device *d,
+					   struct cached_dev *dc)
+{
+	if (dc->stop_when_cache_set_failed == BCH_CACHED_DEV_STOP_ALWAYS) {
+		pr_warn("stop_when_cache_set_failed of %s is \"always\", stop it for failed cache set %pU.",
+			d->disk->disk_name, c->sb.set_uuid);
+		bcache_device_stop(d);
+	} else if (atomic_read(&dc->has_dirty)) {
+		/*
+		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO
+		 * and dc->has_dirty == 1
+		 */
+		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is dirty, stop it to avoid potential data corruption.",
+			d->disk->disk_name);
+			bcache_device_stop(d);
+	} else {
+		/*
+		 * dc->stop_when_cache_set_failed == BCH_CACHED_STOP_AUTO
+		 * and dc->has_dirty == 0
+		 */
+		pr_warn("stop_when_cache_set_failed of %s is \"auto\" and cache is clean, keep it alive.",
+			d->disk->disk_name);
+	}
+}
+
 static void __cache_set_unregister(struct closure *cl)
 {
 	struct cache_set *c = container_of(cl, struct cache_set, caching);
 	struct cached_dev *dc;
+	struct bcache_device *d;
 	size_t i;
 
 	mutex_lock(&bch_register_lock);
 
-	for (i = 0; i < c->devices_max_used; i++)
-		if (c->devices[i]) {
-			if (!UUID_FLASH_ONLY(&c->uuids[i]) &&
-			    test_bit(CACHE_SET_UNREGISTERING, &c->flags)) {
-				dc = container_of(c->devices[i],
-						  struct cached_dev, disk);
-				bch_cached_dev_detach(dc);
-			} else {
-				bcache_device_stop(c->devices[i]);
-			}
+	for (i = 0; i < c->devices_max_used; i++) {
+		d = c->devices[i];
+		if (!d)
+			continue;
+
+		if (!UUID_FLASH_ONLY(&c->uuids[i]) &&
+		    test_bit(CACHE_SET_UNREGISTERING, &c->flags)) {
+			dc = container_of(d, struct cached_dev, disk);
+			bch_cached_dev_detach(dc);
+			if (test_bit(CACHE_SET_IO_DISABLE, &c->flags))
+				conditional_stop_bcache_device(c, d, dc);
+		} else {
+			bcache_device_stop(d);
 		}
+	}
 
 	mutex_unlock(&bch_register_lock);
 

commit 771f393e8ffc9b3066e4830ee5f7391b8e8874f1
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:17 2018 -0700

    bcache: add CACHE_SET_IO_DISABLE to struct cache_set flags
    
    When too many I/Os failed on cache device, bch_cache_set_error() is called
    in the error handling code path to retire whole problematic cache set. If
    new I/O requests continue to come and take refcount dc->count, the cache
    set won't be retired immediately, this is a problem.
    
    Further more, there are several kernel thread and self-armed kernel work
    may still running after bch_cache_set_error() is called. It needs to wait
    quite a while for them to stop, or they won't stop at all. They also
    prevent the cache set from being retired.
    
    The solution in this patch is, to add per cache set flag to disable I/O
    request on this cache and all attached backing devices. Then new coming I/O
    requests can be rejected in *_make_request() before taking refcount, kernel
    threads and self-armed kernel worker can stop very fast when flags bit
    CACHE_SET_IO_DISABLE is set.
    
    Because bcache also do internal I/Os for writeback, garbage collection,
    bucket allocation, journaling, this kind of I/O should be disabled after
    bch_cache_set_error() is called. So closure_bio_submit() is modified to
    check whether CACHE_SET_IO_DISABLE is set on cache_set->flags. If set,
    closure_bio_submit() will set bio->bi_status to BLK_STS_IOERR and
    return, generic_make_request() won't be called.
    
    A sysfs interface is also added to set or clear CACHE_SET_IO_DISABLE bit
    from cache_set->flags, to disable or enable cache set I/O for debugging. It
    is helpful to trigger more corner case issues for failed cache device.
    
    Changelog
    v4, add wait_for_kthread_stop(), and call it before exits writeback and gc
        kernel threads.
    v3, change CACHE_SET_IO_DISABLE from 4 to 3, since it is bit index.
        remove "bcache: " prefix when printing out kernel message.
    v2, more changes by previous review,
    - Use CACHE_SET_IO_DISABLE of cache_set->flags, suggested by Junhui.
    - Check CACHE_SET_IO_DISABLE in bch_btree_gc() to stop a while-loop, this
      is reported and inspired from origal patch of Pavel Vazharov.
    v1, initial version.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Pavel Vazharov <freakpv@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e5be599338c5..dda4ccdd1360 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -521,7 +521,7 @@ static void prio_io(struct cache *ca, uint64_t bucket, int op,
 	bio_set_op_attrs(bio, op, REQ_SYNC|REQ_META|op_flags);
 	bch_bio_map(bio, ca->disk_buckets);
 
-	closure_bio_submit(bio, &ca->prio);
+	closure_bio_submit(ca->set, bio, &ca->prio);
 	closure_sync(cl);
 }
 
@@ -1350,6 +1350,9 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 	    test_bit(CACHE_SET_STOPPING, &c->flags))
 		return false;
 
+	if (test_and_set_bit(CACHE_SET_IO_DISABLE, &c->flags))
+		pr_warn("CACHE_SET_IO_DISABLE already set");
+
 	/* XXX: we can be called from atomic context
 	acquire_console_sem();
 	*/
@@ -1585,6 +1588,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	c->congested_read_threshold_us	= 2000;
 	c->congested_write_threshold_us	= 20000;
 	c->error_limit	= DEFAULT_IO_ERROR_LIMIT;
+	WARN_ON(test_and_clear_bit(CACHE_SET_IO_DISABLE, &c->flags));
 
 	return c;
 err:

commit 3fd47bfe55b00d5ac7b0a44c9301c07be39b1082
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:16 2018 -0700

    bcache: stop dc->writeback_rate_update properly
    
    struct delayed_work writeback_rate_update in struct cache_dev is a delayed
    worker to call function update_writeback_rate() in period (the interval is
    defined by dc->writeback_rate_update_seconds).
    
    When a metadate I/O error happens on cache device, bcache error handling
    routine bch_cache_set_error() will call bch_cache_set_unregister() to
    retire whole cache set. On the unregister code path, this delayed work is
    stopped by calling cancel_delayed_work_sync(&dc->writeback_rate_update).
    
    dc->writeback_rate_update is a special delayed work from others in bcache.
    In its routine update_writeback_rate(), this delayed work is re-armed
    itself. That means when cancel_delayed_work_sync() returns, this delayed
    work can still be executed after several seconds defined by
    dc->writeback_rate_update_seconds.
    
    The problem is, after cancel_delayed_work_sync() returns, the cache set
    unregister code path will continue and release memory of struct cache set.
    Then the delayed work is scheduled to run, __update_writeback_rate()
    will reference the already released cache_set memory, and trigger a NULL
    pointer deference fault.
    
    This patch introduces two more bcache device flags,
    - BCACHE_DEV_WB_RUNNING
      bit set:  bcache device is in writeback mode and running, it is OK for
                dc->writeback_rate_update to re-arm itself.
      bit clear:bcache device is trying to stop dc->writeback_rate_update,
                this delayed work should not re-arm itself and quit.
    - BCACHE_DEV_RATE_DW_RUNNING
      bit set:  routine update_writeback_rate() is executing.
      bit clear: routine update_writeback_rate() quits.
    
    This patch also adds a function cancel_writeback_rate_update_dwork() to
    wait for dc->writeback_rate_update quits before cancel it by calling
    cancel_delayed_work_sync(). In order to avoid a deadlock by unexpected
    quit dc->writeback_rate_update, after time_out seconds this function will
    give up and continue to call cancel_delayed_work_sync().
    
    And here I explain how this patch stops self re-armed delayed work properly
    with the above stuffs.
    
    update_writeback_rate() sets BCACHE_DEV_RATE_DW_RUNNING at its beginning
    and clears BCACHE_DEV_RATE_DW_RUNNING at its end. Before calling
    cancel_writeback_rate_update_dwork() clear flag BCACHE_DEV_WB_RUNNING.
    
    Before calling cancel_delayed_work_sync() wait utill flag
    BCACHE_DEV_RATE_DW_RUNNING is clear. So when calling
    cancel_delayed_work_sync(), dc->writeback_rate_update must be already re-
    armed, or quite by seeing BCACHE_DEV_WB_RUNNING cleared. In both cases
    delayed work routine update_writeback_rate() won't be executed after
    cancel_delayed_work_sync() returns.
    
    Inside update_writeback_rate() before calling schedule_delayed_work(), flag
    BCACHE_DEV_WB_RUNNING is checked before. If this flag is cleared, it means
    someone is about to stop the delayed work. Because flag
    BCACHE_DEV_RATE_DW_RUNNING is set already and cancel_delayed_work_sync()
    has to wait for this flag to be cleared, we don't need to worry about race
    condition here.
    
    If update_writeback_rate() is scheduled to run after checking
    BCACHE_DEV_RATE_DW_RUNNING and before calling cancel_delayed_work_sync()
    in cancel_writeback_rate_update_dwork(), it is also safe. Because at this
    moment BCACHE_DEV_WB_RUNNING is cleared with memory barrier. As I mentioned
    previously, update_writeback_rate() will see BCACHE_DEV_WB_RUNNING is clear
    and quit immediately.
    
    Because there are more dependences inside update_writeback_rate() to struct
    cache_set memory, dc->writeback_rate_update is not a simple self re-arm
    delayed work. After trying many different methods (e.g. hold dc->count, or
    use locks), this is the only way I can find which works to properly stop
    dc->writeback_rate_update delayed work.
    
    Changelog:
    v3: change values of BCACHE_DEV_WB_RUNNING and BCACHE_DEV_RATE_DW_RUNNING
        to bit index, for test_bit().
    v2: Try to fix the race issue which is pointed out by Junhui.
    v1: The initial version for review
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Junhui Tang <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 020be4f1cd8b..e5be599338c5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -899,6 +899,31 @@ void bch_cached_dev_run(struct cached_dev *dc)
 		pr_debug("error creating sysfs link");
 }
 
+/*
+ * If BCACHE_DEV_RATE_DW_RUNNING is set, it means routine of the delayed
+ * work dc->writeback_rate_update is running. Wait until the routine
+ * quits (BCACHE_DEV_RATE_DW_RUNNING is clear), then continue to
+ * cancel it. If BCACHE_DEV_RATE_DW_RUNNING is not clear after time_out
+ * seconds, give up waiting here and continue to cancel it too.
+ */
+static void cancel_writeback_rate_update_dwork(struct cached_dev *dc)
+{
+	int time_out = WRITEBACK_RATE_UPDATE_SECS_MAX * HZ;
+
+	do {
+		if (!test_bit(BCACHE_DEV_RATE_DW_RUNNING,
+			      &dc->disk.flags))
+			break;
+		time_out--;
+		schedule_timeout_interruptible(1);
+	} while (time_out > 0);
+
+	if (time_out == 0)
+		pr_warn("give up waiting for dc->writeback_write_update to quit");
+
+	cancel_delayed_work_sync(&dc->writeback_rate_update);
+}
+
 static void cached_dev_detach_finish(struct work_struct *w)
 {
 	struct cached_dev *dc = container_of(w, struct cached_dev, detach);
@@ -911,7 +936,9 @@ static void cached_dev_detach_finish(struct work_struct *w)
 
 	mutex_lock(&bch_register_lock);
 
-	cancel_delayed_work_sync(&dc->writeback_rate_update);
+	if (test_and_clear_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags))
+		cancel_writeback_rate_update_dwork(dc);
+
 	if (!IS_ERR_OR_NULL(dc->writeback_thread)) {
 		kthread_stop(dc->writeback_thread);
 		dc->writeback_thread = NULL;
@@ -954,6 +981,7 @@ void bch_cached_dev_detach(struct cached_dev *dc)
 	closure_get(&dc->disk.cl);
 
 	bch_writeback_queue(dc);
+
 	cached_dev_put(dc);
 }
 
@@ -1081,14 +1109,16 @@ static void cached_dev_free(struct closure *cl)
 {
 	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
 
-	cancel_delayed_work_sync(&dc->writeback_rate_update);
+	mutex_lock(&bch_register_lock);
+
+	if (test_and_clear_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags))
+		cancel_writeback_rate_update_dwork(dc);
+
 	if (!IS_ERR_OR_NULL(dc->writeback_thread))
 		kthread_stop(dc->writeback_thread);
 	if (dc->writeback_write_wq)
 		destroy_workqueue(dc->writeback_write_wq);
 
-	mutex_lock(&bch_register_lock);
-
 	if (atomic_read(&dc->running))
 		bd_unlink_disk_holder(dc->bdev, dc->disk.disk);
 	bcache_device_free(&dc->disk);

commit 804f3c6981f5e4a506a8f14dc284cb218d0659ae
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:14 2018 -0700

    bcache: fix cached_dev->count usage for bch_cache_set_error()
    
    When bcache metadata I/O fails, bcache will call bch_cache_set_error()
    to retire the whole cache set. The expected behavior to retire a cache
    set is to unregister the cache set, and unregister all backing device
    attached to this cache set, then remove sysfs entries of the cache set
    and all attached backing devices, finally release memory of structs
    cache_set, cache, cached_dev and bcache_device.
    
    In my testing when journal I/O failure triggered by disconnected cache
    device, sometimes the cache set cannot be retired, and its sysfs
    entry /sys/fs/bcache/<uuid> still exits and the backing device also
    references it. This is not expected behavior.
    
    When metadata I/O failes, the call senquence to retire whole cache set is,
            bch_cache_set_error()
            bch_cache_set_unregister()
            bch_cache_set_stop()
            __cache_set_unregister()     <- called as callback by calling
                                            clousre_queue(&c->caching)
            cache_set_flush()            <- called as a callback when refcount
                                            of cache_set->caching is 0
            cache_set_free()             <- called as a callback when refcount
                                            of catch_set->cl is 0
            bch_cache_set_release()      <- called as a callback when refcount
                                            of catch_set->kobj is 0
    
    I find if kernel thread bch_writeback_thread() quits while-loop when
    kthread_should_stop() is true and searched_full_index is false, clousre
    callback cache_set_flush() set by continue_at() will never be called. The
    result is, bcache fails to retire whole cache set.
    
    cache_set_flush() will be called when refcount of closure c->caching is 0,
    and in function bcache_device_detach() refcount of closure c->caching is
    released to 0 by clousre_put(). In metadata error code path, function
    bcache_device_detach() is called by cached_dev_detach_finish(). This is a
    callback routine being called when cached_dev->count is 0. This refcount
    is decreased by cached_dev_put().
    
    The above dependence indicates, cache_set_flush() will be called when
    refcount of cache_set->cl is 0, and refcount of cache_set->cl to be 0
    when refcount of cache_dev->count is 0.
    
    The reason why sometimes cache_dev->count is not 0 (when metadata I/O fails
    and bch_cache_set_error() called) is, in bch_writeback_thread(), refcount
    of cache_dev is not decreased properly.
    
    In bch_writeback_thread(), cached_dev_put() is called only when
    searched_full_index is true and cached_dev->writeback_keys is empty, a.k.a
    there is no dirty data on cache. In most of run time it is correct, but
    when bch_writeback_thread() quits the while-loop while cache is still
    dirty, current code forget to call cached_dev_put() before this kernel
    thread exits. This is why sometimes cache_set_flush() is not executed and
    cache set fails to be retired.
    
    The reason to call cached_dev_put() in bch_writeback_rate() is, when the
    cache device changes from clean to dirty, cached_dev_get() is called, to
    make sure during writeback operatiions both backing and cache devices
    won't be released.
    
    Adding following code in bch_writeback_thread() does not work,
       static int bch_writeback_thread(void *arg)
            }
    
    +       if (atomic_read(&dc->has_dirty))
    +               cached_dev_put()
    +
            return 0;
     }
    because writeback kernel thread can be waken up and start via sysfs entry:
            echo 1 > /sys/block/bcache<N>/bcache/writeback_running
    It is difficult to check whether backing device is dirty without race and
    extra lock. So the above modification will introduce potential refcount
    underflow in some conditions.
    
    The correct fix is, to take cached dev refcount when creating the kernel
    thread, and put it before the kernel thread exits. Then bcache does not
    need to take a cached dev refcount when cache turns from clean to dirty,
    or to put a cached dev refcount when cache turns from ditry to clean. The
    writeback kernel thread is alwasy safe to reference data structure from
    cache set, cache and cached device (because a refcount of cache device is
    taken for it already), and no matter the kernel thread is stopped by I/O
    errors or system reboot, cached_dev->count can always be used correctly.
    
    The patch is simple, but understanding how it works is quite complicated.
    
    Changelog:
    v2: set dc->writeback_thread to NULL in this patch, as suggested by Hannes.
    v1: initial version for review.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e8dfa804bd98..020be4f1cd8b 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1054,7 +1054,6 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
 		bch_sectors_dirty_init(&dc->disk);
 		atomic_set(&dc->has_dirty, 1);
-		refcount_inc(&dc->count);
 		bch_writeback_queue(dc);
 	}
 

commit 44e1ebe2a33b6cf70d6bee6beb1d5a198a841380
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:07 2018 -0800

    bcache: Use the blk_queue_flag_{set,clear}() functions
    
    Use the blk_queue_flag_{set,clear}() functions instead of open-coding
    these.
    
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4d1d8dfb2d2a..e8dfa804bd98 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -833,9 +833,9 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	q->limits.io_min		= block_size;
 	q->limits.logical_block_size	= block_size;
 	q->limits.physical_block_size	= block_size;
-	set_bit(QUEUE_FLAG_NONROT,	&d->disk->queue->queue_flags);
-	clear_bit(QUEUE_FLAG_ADD_RANDOM, &d->disk->queue->queue_flags);
-	set_bit(QUEUE_FLAG_DISCARD,	&d->disk->queue->queue_flags);
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, d->disk->queue);
+	blk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, d->disk->queue);
+	blk_queue_flag_set(QUEUE_FLAG_DISCARD, d->disk->queue);
 
 	blk_queue_write_cache(q, true, true);
 

commit 86755b7a96faed57f910f9e6b8061e019ac1ec08
Author: Michael Lyle <mlyle@lyle.org>
Date:   Mon Mar 5 13:41:55 2018 -0800

    bcache: don't attach backing with duplicate UUID
    
    This can happen e.g. during disk cloning.
    
    This is an incomplete fix: it does not catch duplicate UUIDs earlier
    when things are still unattached.  It does not unregister the device.
    Further changes to cope better with this are planned but conflict with
    Coly's ongoing improvements to handling device errors.  In the meantime,
    one can manually stop the device after this has happened.
    
    Attempts to attach a duplicate device result in:
    
    [  136.372404] loop: module loaded
    [  136.424461] bcache: register_bdev() registered backing device loop0
    [  136.424464] bcache: bch_cached_dev_attach() Tried to attach loop0 but duplicate UUID already attached
    
    My test procedure is:
    
      dd if=/dev/sdb1 of=imgfile bs=1024 count=262144
      losetup -f imgfile
    
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 58d8998529de..f2273143b3cb 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -963,6 +963,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 	uint32_t rtime = cpu_to_le32(get_seconds());
 	struct uuid_entry *u;
 	char buf[BDEVNAME_SIZE];
+	struct cached_dev *exist_dc, *t;
 
 	bdevname(dc->bdev, buf);
 
@@ -987,6 +988,16 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 		return -EINVAL;
 	}
 
+	/* Check whether already attached */
+	list_for_each_entry_safe(exist_dc, t, &c->cached_devs, list) {
+		if (!memcmp(dc->sb.uuid, exist_dc->sb.uuid, 16)) {
+			pr_err("Tried to attach %s but duplicate UUID already attached",
+				buf);
+
+			return -EINVAL;
+		}
+	}
+
 	u = uuid_find(c, dc->sb.uuid);
 
 	if (u &&

commit cc40daf91bdddbba72a4a8cd0860640e06668309
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Mon Mar 5 13:41:54 2018 -0800

    bcache: fix crashes in duplicate cache device register
    
    Kernel crashed when register a duplicate cache device, the call trace is
    bellow:
    [  417.643790] CPU: 1 PID: 16886 Comm: bcache-register Tainted: G
       W  OE    4.15.5-amd64-preempt-sysrq-20171018 #2
    [  417.643861] Hardware name: LENOVO 20ERCTO1WW/20ERCTO1WW, BIOS
    N1DET41W (1.15 ) 12/31/2015
    [  417.643870] RIP: 0010:bdevname+0x13/0x1e
    [  417.643876] RSP: 0018:ffffa3aa9138fd38 EFLAGS: 00010282
    [  417.643884] RAX: 0000000000000000 RBX: ffff8c8f2f2f8000 RCX: ffffd6701f8
    c7edf
    [  417.643890] RDX: ffffa3aa9138fd88 RSI: ffffa3aa9138fd88 RDI: 00000000000
    00000
    [  417.643895] RBP: ffffa3aa9138fde0 R08: ffffa3aa9138fae8 R09: 00000000000
    1850e
    [  417.643901] R10: ffff8c8eed34b271 R11: ffff8c8eed34b250 R12: 00000000000
    00000
    [  417.643906] R13: ffffd6701f78f940 R14: ffff8c8f38f80000 R15: ffff8c8ea7d
    90000
    [  417.643913] FS:  00007fde7e66f500(0000) GS:ffff8c8f61440000(0000) knlGS:
    0000000000000000
    [  417.643919] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  417.643925] CR2: 0000000000000314 CR3: 00000007e6fa0001 CR4: 00000000003
    606e0
    [  417.643931] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 00000000000
    00000
    [  417.643938] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 00000000000
    00400
    [  417.643946] Call Trace:
    [  417.643978]  register_bcache+0x1117/0x1270 [bcache]
    [  417.643994]  ? slab_pre_alloc_hook+0x15/0x3c
    [  417.644001]  ? slab_post_alloc_hook.isra.44+0xa/0x1a
    [  417.644013]  ? kernfs_fop_write+0xf6/0x138
    [  417.644020]  kernfs_fop_write+0xf6/0x138
    [  417.644031]  __vfs_write+0x31/0xcc
    [  417.644043]  ? current_kernel_time64+0x10/0x36
    [  417.644115]  ? __audit_syscall_entry+0xbf/0xe3
    [  417.644124]  vfs_write+0xa5/0xe2
    [  417.644133]  SyS_write+0x5c/0x9f
    [  417.644144]  do_syscall_64+0x72/0x81
    [  417.644161]  entry_SYSCALL_64_after_hwframe+0x3d/0xa2
    [  417.644169] RIP: 0033:0x7fde7e1c1974
    [  417.644175] RSP: 002b:00007fff13009a38 EFLAGS: 00000246 ORIG_RAX: 0000000
    000000001
    [  417.644183] RAX: ffffffffffffffda RBX: 0000000001658280 RCX: 00007fde7e1c
    1974
    [  417.644188] RDX: 000000000000000a RSI: 0000000001658280 RDI: 000000000000
    0001
    [  417.644193] RBP: 000000000000000a R08: 0000000000000003 R09: 000000000000
    0077
    [  417.644198] R10: 000000000000089e R11: 0000000000000246 R12: 000000000000
    0001
    [  417.644203] R13: 000000000000000a R14: 7fffffffffffffff R15: 000000000000
    0000
    [  417.644213] Code: c7 c2 83 6f ee 98 be 20 00 00 00 48 89 df e8 6c 27 3b 0
    0 48 89 d8 5b c3 0f 1f 44 00 00 48 8b 47 70 48 89 f2 48 8b bf 80 00 00 00 <8
    b> b0 14 03 00 00 e9 73 ff ff ff 0f 1f 44 00 00 48 8b 47 40 39
    [  417.644302] RIP: bdevname+0x13/0x1e RSP: ffffa3aa9138fd38
    [  417.644306] CR2: 0000000000000314
    
    When registering duplicate cache device in register_cache(), after failure
    on calling register_cache_set(), bch_cache_release() will be called, then
    bdev will be freed, so bdevname(bdev, name) caused kernel crash.
    
    Since bch_cache_release() will free bdev, so in this patch we make sure
    bdev being freed if register_cache() fail, and do not free bdev again in
    register_bcache() when register_cache() fail.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reported-by: Marc MERLIN <marc@merlins.org>
    Tested-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4d1d8dfb2d2a..58d8998529de 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1204,7 +1204,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	return;
 err:
-	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
+	pr_notice("error %s: %s", bdevname(bdev, name), err);
 	bcache_device_stop(&dc->disk);
 }
 
@@ -1883,6 +1883,8 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	const char *err = NULL; /* must be set for any error case */
 	int ret = 0;
 
+	bdevname(bdev, name);
+
 	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
 	ca->bdev = bdev;
 	ca->bdev->bd_holder = ca;
@@ -1891,11 +1893,12 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	bio_first_bvec_all(&ca->sb_bio)->bv_page = sb_page;
 	get_page(sb_page);
 
-	if (blk_queue_discard(bdev_get_queue(ca->bdev)))
+	if (blk_queue_discard(bdev_get_queue(bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
 
 	ret = cache_alloc(ca);
 	if (ret != 0) {
+		blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 		if (ret == -ENOMEM)
 			err = "cache_alloc(): -ENOMEM";
 		else
@@ -1918,14 +1921,14 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 		goto out;
 	}
 
-	pr_info("registered cache device %s", bdevname(bdev, name));
+	pr_info("registered cache device %s", name);
 
 out:
 	kobject_put(&ca->kobj);
 
 err:
 	if (err)
-		pr_notice("error opening %s: %s", bdevname(bdev, name), err);
+		pr_notice("error %s: %s", name, err);
 
 	return ret;
 }
@@ -2014,6 +2017,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (err)
 		goto err_close;
 
+	err = "failed to register device";
 	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
 		if (!dc)
@@ -2028,7 +2032,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			goto err_close;
 
 		if (register_cache(sb, sb_page, bdev, ca) != 0)
-			goto err_close;
+			goto err;
 	}
 out:
 	if (sb_page)
@@ -2041,7 +2045,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 err_close:
 	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 err:
-	pr_info("error opening %s: %s", path, err);
+	pr_info("error %s: %s", path, err);
 	ret = -EINVAL;
 	goto out;
 }

commit 02aa8a8b2b84531fa78b9a486d9b2a0700f7bc08
Author: Coly Li <colyli@suse.de>
Date:   Tue Feb 27 09:49:29 2018 -0800

    bcache: correct flash only vols (check all uuids)
    
    Commit 2831231d4c3f ("bcache: reduce cache_set devices iteration by
    devices_max_used") adds c->devices_max_used to reduce iteration of
    c->uuids elements, this value is updated in bcache_device_attach().
    
    But for flash only volume, when calling flash_devs_run(), the function
    bcache_device_attach() is not called yet and c->devices_max_used is not
    updated. The unexpected result is, the flash only volume won't be run
    by flash_devs_run().
    
    This patch fixes the issue by iterate all c->uuids elements in
    flash_devs_run(). c->devices_max_used will be updated properly when
    bcache_device_attach() gets called.
    
    [mlyle: commit subject edited for character limit]
    
    Fixes: 2831231d4c3f ("bcache: reduce cache_set devices iteration by devices_max_used")
    Reported-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 312895788036..4d1d8dfb2d2a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1274,7 +1274,7 @@ static int flash_devs_run(struct cache_set *c)
 	struct uuid_entry *u;
 
 	for (u = c->uuids;
-	     u < c->uuids + c->devices_max_used && !ret;
+	     u < c->uuids + c->nr_uuids && !ret;
 	     u++)
 		if (UUID_FLASH_ONLY(u))
 			ret = flash_dev_run(c, u);

commit 73ac105be390c1de42a2f21643c9778a5e002930
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Feb 7 11:41:46 2018 -0800

    bcache: fix for data collapse after re-attaching an attached device
    
    back-end device sdm has already attached a cache_set with ID
    f67ebe1f-f8bc-4d73-bfe5-9dc88607f119, then try to attach with
    another cache set, and it returns with an error:
    [root]# cd /sys/block/sdm/bcache
    [root]# echo 5ccd0a63-148e-48b8-afa2-aca9cbd6279f > attach
    -bash: echo: write error: Invalid argument
    
    After that, execute a command to modify the label of bcache
    device:
    [root]# echo data_disk1 > label
    
    Then we reboot the system, when the system power on, the back-end
    device can not attach to cache_set, a messages show in the log:
    Feb  5 12:05:52 ceph152 kernel: [922385.508498] bcache:
    bch_cached_dev_attach() couldn't find uuid for sdm in set
    
    In sysfs_attach(), dc->sb.set_uuid was assigned to the value
    which input through sysfs, no matter whether it is success
    or not in bch_cached_dev_attach(). For example, If the back-end
    device has already attached to an cache set, bch_cached_dev_attach()
    would fail, but dc->sb.set_uuid was changed. Then modify the
    label of bcache device, it will call bch_write_bdev_super(),
    which would write the dc->sb.set_uuid to the super block, so we
    record a wrong cache set ID in the super block, after the system
    reboot, the cache set couldn't find the uuid of the back-end
    device, so the bcache device couldn't exist and use any more.
    
    In this patch, we don't assigned cache set ID to dc->sb.set_uuid
    in sysfs_attach() directly, but input it into bch_cached_dev_attach(),
    and assigned dc->sb.set_uuid to the cache set ID after the back-end
    device attached to the cache set successful.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a2ad37a8afc0..312895788036 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -957,7 +957,8 @@ void bch_cached_dev_detach(struct cached_dev *dc)
 	cached_dev_put(dc);
 }
 
-int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
+int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
+			  uint8_t *set_uuid)
 {
 	uint32_t rtime = cpu_to_le32(get_seconds());
 	struct uuid_entry *u;
@@ -965,7 +966,8 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 
 	bdevname(dc->bdev, buf);
 
-	if (memcmp(dc->sb.set_uuid, c->sb.set_uuid, 16))
+	if ((set_uuid && memcmp(set_uuid, c->sb.set_uuid, 16)) ||
+	    (!set_uuid && memcmp(dc->sb.set_uuid, c->sb.set_uuid, 16)))
 		return -ENOENT;
 
 	if (dc->disk.c) {
@@ -1194,7 +1196,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	list_add(&dc->list, &uncached_devices);
 	list_for_each_entry(c, &bch_cache_sets, list)
-		bch_cached_dev_attach(dc, c);
+		bch_cached_dev_attach(dc, c, NULL);
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_NONE ||
 	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE)
@@ -1716,7 +1718,7 @@ static void run_cache_set(struct cache_set *c)
 	bcache_write_super(c);
 
 	list_for_each_entry_safe(dc, t, &uncached_devices, list)
-		bch_cached_dev_attach(dc, c);
+		bch_cached_dev_attach(dc, c, NULL);
 
 	flash_devs_run(c);
 

commit 682811b3ce1a5a4e20d700939a9042f01dbc66c4
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Feb 7 11:41:43 2018 -0800

    bcache: fix for allocator and register thread race
    
    After long time running of random small IO writing,
    I reboot the machine, and after the machine power on,
    I found bcache got stuck, the stack is:
    [root@ceph153 ~]# cat /proc/2510/task/*/stack
    [<ffffffffa06b2455>] closure_sync+0x25/0x90 [bcache]
    [<ffffffffa06b6be8>] bch_journal+0x118/0x2b0 [bcache]
    [<ffffffffa06b6dc7>] bch_journal_meta+0x47/0x70 [bcache]
    [<ffffffffa06be8f7>] bch_prio_write+0x237/0x340 [bcache]
    [<ffffffffa06a8018>] bch_allocator_thread+0x3c8/0x3d0 [bcache]
    [<ffffffff810a631f>] kthread+0xcf/0xe0
    [<ffffffff8164c318>] ret_from_fork+0x58/0x90
    [<ffffffffffffffff>] 0xffffffffffffffff
    [root@ceph153 ~]# cat /proc/2038/task/*/stack
    [<ffffffffa06b1abd>] __bch_btree_map_nodes+0x12d/0x150 [bcache]
    [<ffffffffa06b1bd1>] bch_btree_insert+0xf1/0x170 [bcache]
    [<ffffffffa06b637f>] bch_journal_replay+0x13f/0x230 [bcache]
    [<ffffffffa06c75fe>] run_cache_set+0x79a/0x7c2 [bcache]
    [<ffffffffa06c0cf8>] register_bcache+0xd48/0x1310 [bcache]
    [<ffffffff812f702f>] kobj_attr_store+0xf/0x20
    [<ffffffff8125b216>] sysfs_write_file+0xc6/0x140
    [<ffffffff811dfbfd>] vfs_write+0xbd/0x1e0
    [<ffffffff811e069f>] SyS_write+0x7f/0xe0
    [<ffffffff8164c3c9>] system_call_fastpath+0x16/0x1
    The stack shows the register thread and allocator thread
    were getting stuck when registering cache device.
    
    I reboot the machine several times, the issue always
    exsit in this machine.
    
    I debug the code, and found the call trace as bellow:
    register_bcache()
       ==>run_cache_set()
          ==>bch_journal_replay()
             ==>bch_btree_insert()
                ==>__bch_btree_map_nodes()
                   ==>btree_insert_fn()
                      ==>btree_split() //node need split
                         ==>btree_check_reserve()
    In btree_check_reserve(), It will check if there is enough buckets
    of RESERVE_BTREE type, since allocator thread did not work yet, so
    no buckets of RESERVE_BTREE type allocated, so the register thread
    waits on c->btree_cache_wait, and goes to sleep.
    
    Then the allocator thread initialized, the call trace is bellow:
    bch_allocator_thread()
    ==>bch_prio_write()
       ==>bch_journal_meta()
          ==>bch_journal()
             ==>journal_wait_for_write()
    In journal_wait_for_write(), It will check if journal is full by
    journal_full(), but the long time random small IO writing
    causes the exhaustion of journal buckets(journal.blocks_free=0),
    In order to release the journal buckets,
    the allocator calls btree_flush_write() to flush keys to
    btree nodes, and waits on c->journal.wait until btree nodes writing
    over or there has already some journal buckets space, then the
    allocator thread goes to sleep. but in btree_flush_write(), since
    bch_journal_replay() is not finished, so no btree nodes have journal
    (condition "if (btree_current_write(b)->journal)" never satisfied),
    so we got no btree node to flush, no journal bucket released,
    and allocator sleep all the times.
    
    Through the above analysis, we can see that:
    1) Register thread wait for allocator thread to allocate buckets of
       RESERVE_BTREE type;
    2) Alloctor thread wait for register thread to replay journal, so it
       can flush btree nodes and get journal bucket.
       then they are all got stuck by waiting for each other.
    
    Hua Rui provided a patch for me, by allocating some buckets of
    RESERVE_BTREE type in advance, so the register thread can get bucket
    when btree node splitting and no need to waiting for the allocator
    thread. I tested it, it has effect, and register thread run a step
    forward, but finally are still got stuck, the reason is only 8 bucket
    of RESERVE_BTREE type were allocated, and in bch_journal_replay(),
    after 2 btree nodes splitting, only 4 bucket of RESERVE_BTREE type left,
    then btree_check_reserve() is not satisfied anymore, so it goes to sleep
    again, and in the same time, alloctor thread did not flush enough btree
    nodes to release a journal bucket, so they all got stuck again.
    
    So we need to allocate more buckets of RESERVE_BTREE type in advance,
    but how much is enough?  By experience and test, I think it should be
    as much as journal buckets. Then I modify the code as this patch,
    and test in the machine, and it works.
    
    This patch modified base on Hua Ruis patch, and allocate more buckets
    of RESERVE_BTREE type in advance to avoid register thread and allocate
    thread going to wait for each other.
    
    [patch v2] ca->sb.njournal_buckets would be 0 in the first time after
    cache creation, and no journal exists, so just 8 btree buckets is OK.
    
    Signed-off-by: Hua Rui <huarui.dev@gmail.com>
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e29150ec17e7..a2ad37a8afc0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1833,6 +1833,7 @@ void bch_cache_release(struct kobject *kobj)
 static int cache_alloc(struct cache *ca)
 {
 	size_t free;
+	size_t btree_buckets;
 	struct bucket *b;
 
 	__module_get(THIS_MODULE);
@@ -1840,9 +1841,19 @@ static int cache_alloc(struct cache *ca)
 
 	bio_init(&ca->journal.bio, ca->journal.bio.bi_inline_vecs, 8);
 
+	/*
+	 * when ca->sb.njournal_buckets is not zero, journal exists,
+	 * and in bch_journal_replay(), tree node may split,
+	 * so bucket of RESERVE_BTREE type is needed,
+	 * the worst situation is all journal buckets are valid journal,
+	 * and all the keys need to replay,
+	 * so the number of  RESERVE_BTREE type buckets should be as much
+	 * as journal buckets
+	 */
+	btree_buckets = ca->sb.njournal_buckets ?: 8;
 	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
 
-	if (!init_fifo(&ca->free[RESERVE_BTREE], 8, GFP_KERNEL) ||
+	if (!init_fifo(&ca->free[RESERVE_BTREE], btree_buckets, GFP_KERNEL) ||
 	    !init_fifo_exact(&ca->free[RESERVE_PRIO], prio_buckets(ca), GFP_KERNEL) ||
 	    !init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||

commit 7ba0d830dc0e4c7e88602c91656029b6ae8a1766
Author: Coly Li <colyli@suse.de>
Date:   Wed Feb 7 11:41:42 2018 -0800

    bcache: set error_limit correctly
    
    Struct cache uses io_errors for two purposes,
    - Error decay: when cache set error_decay is set, io_errors is used to
      generate a small piece of delay when I/O error happens.
    - I/O errors counter: in order to generate big enough value for error
      decay, I/O errors counter value is stored by left shifting 20 bits (a.k.a
      IO_ERROR_SHIFT).
    
    In function bch_count_io_errors(), if I/O errors counter reaches cache set
    error limit, bch_cache_set_error() will be called to retire the whold cache
    set. But current code is problematic when checking the error limit, see the
    following code piece from bch_count_io_errors(),
    
     90     if (error) {
     91             char buf[BDEVNAME_SIZE];
     92             unsigned errors = atomic_add_return(1 << IO_ERROR_SHIFT,
     93                                                 &ca->io_errors);
     94             errors >>= IO_ERROR_SHIFT;
     95
     96             if (errors < ca->set->error_limit)
     97                     pr_err("%s: IO error on %s, recovering",
     98                            bdevname(ca->bdev, buf), m);
     99             else
    100                     bch_cache_set_error(ca->set,
    101                                         "%s: too many IO errors %s",
    102                                         bdevname(ca->bdev, buf), m);
    103     }
    
    At line 94, errors is right shifting IO_ERROR_SHIFT bits, now it is real
    errors counter to compare at line 96. But ca->set->error_limit is initia-
    lized with an amplified value in bch_cache_set_alloc(),
    1545         c->error_limit  = 8 << IO_ERROR_SHIFT;
    
    It means by default, in bch_count_io_errors(), before 8<<20 errors happened
    bch_cache_set_error() won't be called to retire the problematic cache
    device. If the average request size is 64KB, it means bcache won't handle
    failed device until 512GB data is requested. This is too large to be an I/O
    threashold. So I believe the correct error limit should be much less.
    
    This patch sets default cache set error limit to 8, then in
    bch_count_io_errors() when errors counter reaches 8 (if it is default
    value), function bch_cache_set_error() will be called to retire the whole
    cache set. This patch also removes bits shifting when store or show
    io_error_limit value via sysfs interface.
    
    Nowadays most of SSDs handle internal flash failure automatically by LBA
    address re-indirect mapping. If an I/O error can be observed by upper layer
    code, it will be a notable error because that SSD can not re-indirect
    map the problematic LBA address to an available flash block. This situation
    indicates the whole SSD will be failed very soon. Therefore setting 8 as
    the default io error limit value makes sense, it is enough for most of
    cache devices.
    
    Changelog:
    v2: add reviewed-by from Hannes.
    v1: initial version for review.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 133b81225ea9..e29150ec17e7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1553,7 +1553,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 
 	c->congested_read_threshold_us	= 2000;
 	c->congested_write_threshold_us	= 20000;
-	c->error_limit	= 8 << IO_ERROR_SHIFT;
+	c->error_limit	= DEFAULT_IO_ERROR_LIMIT;
 
 	return c;
 err:

commit 5138ac6748e381501894976f995fb7d1a63f80f4
Author: Coly Li <colyli@suse.de>
Date:   Mon Jan 8 12:21:29 2018 -0800

    bcache: fix misleading error message in bch_count_io_errors()
    
    Bcache only does recoverable I/O for read operations by calling
    cached_dev_read_error(). For write opertions there is no I/O recovery for
    failed requests.
    
    But in bch_count_io_errors() no matter read or write I/Os, before errors
    counter reaches io error limit, pr_err() always prints "IO error on %,
    recoverying". For write requests this information is misleading, because
    there is no I/O recovery at all.
    
    This patch adds a parameter 'is_read' to bch_count_io_errors(), and only
    prints "recovering" by pr_err() when the bio direction is READ.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d13e4ccb30a0..133b81225ea9 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -274,7 +274,9 @@ static void write_super_endio(struct bio *bio)
 {
 	struct cache *ca = bio->bi_private;
 
-	bch_count_io_errors(ca, bio->bi_status, "writing superblock");
+	/* is_read = 0 */
+	bch_count_io_errors(ca, bio->bi_status, 0,
+			    "writing superblock");
 	closure_put(&ca->set->sb_write);
 }
 

commit 2831231d4c3f999d2d062b23dfbc8b0faa4bc6e0
Author: Coly Li <colyli@suse.de>
Date:   Mon Jan 8 12:21:28 2018 -0800

    bcache: reduce cache_set devices iteration by devices_max_used
    
    Member devices of struct cache_set is used to reference all attached
    bcache devices to this cache set. If it is treated as array of pointers,
    size of devices[] is indicated by member nr_uuids of struct cache_set.
    
    nr_uuids is calculated in drivers/md/super.c:bch_cache_set_alloc(),
            bucket_bytes(c) / sizeof(struct uuid_entry)
    Bucket size is determined by user space tool "make-bcache", by default it
    is 1024 sectors (defined in bcache-tools/make-bcache.c:main()). So default
    nr_uuids value is 4096 from the above calculation.
    
    Every time when bcache code iterates bcache devices of a cache set, all
    the 4096 pointers are checked even only 1 bcache device is attached to the
    cache set, that's a wast of time and unncessary.
    
    This patch adds a member devices_max_used to struct cache_set. Its value
    is 1 + the maximum used index of devices[] in a cache set. When iterating
    all valid bcache devices of a cache set, use c->devices_max_used in
    for-loop may reduce a lot of useless checking.
    
    Personally, my motivation of this patch is not for performance, I use it
    in bcache debugging, which helps me to narrow down the scape to check
    valid bcached devices of a cache set.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 553e841e897d..d13e4ccb30a0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -721,6 +721,9 @@ static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
 	d->c = c;
 	c->devices[id] = d;
 
+	if (id >= c->devices_max_used)
+		c->devices_max_used = id + 1;
+
 	closure_get(&c->caching);
 }
 
@@ -1267,7 +1270,7 @@ static int flash_devs_run(struct cache_set *c)
 	struct uuid_entry *u;
 
 	for (u = c->uuids;
-	     u < c->uuids + c->nr_uuids && !ret;
+	     u < c->uuids + c->devices_max_used && !ret;
 	     u++)
 		if (UUID_FLASH_ONLY(u))
 			ret = flash_dev_run(c, u);
@@ -1433,7 +1436,7 @@ static void __cache_set_unregister(struct closure *cl)
 
 	mutex_lock(&bch_register_lock);
 
-	for (i = 0; i < c->nr_uuids; i++)
+	for (i = 0; i < c->devices_max_used; i++)
 		if (c->devices[i]) {
 			if (!UUID_FLASH_ONLY(&c->uuids[i]) &&
 			    test_bit(CACHE_SET_UNREGISTERING, &c->flags)) {
@@ -1496,7 +1499,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	c->bucket_bits		= ilog2(sb->bucket_size);
 	c->block_bits		= ilog2(sb->block_size);
 	c->nr_uuids		= bucket_bytes(c) / sizeof(struct uuid_entry);
-
+	c->devices_max_used	= 0;
 	c->btree_pages		= bucket_pages(c);
 	if (c->btree_pages > BTREE_MAX_PAGES)
 		c->btree_pages = max_t(int, c->btree_pages / 4,

commit 8d29c4426b9f8afaccf28de414fde8a722b35fdf
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Mon Jan 8 12:21:19 2018 -0800

    bcache: stop writeback thread after detaching
    
    Currently, when a cached device detaching from cache, writeback thread is
    not stopped, and writeback_rate_update work is not canceled. For example,
    after the following command:
    echo 1 >/sys/block/sdb/bcache/detach
    you can still see the writeback thread. Then you attach the device to the
    cache again, bcache will create another writeback thread, for example,
    after below command:
    echo  ba0fb5cd-658a-4533-9806-6ce166d883b9 > /sys/block/sdb/bcache/attach
    then you will see 2 writeback threads.
    This patch stops writeback thread and cancels writeback_rate_update work
    when cached device detaching from cache.
    
    Compare with patch v1, this v2 patch moves code down into the register
    lock for safety in case of any future changes as Coly and Mike suggested.
    
    [edit by mlyle: commit log spelling/formatting]
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8399fe0651f2..553e841e897d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -906,6 +906,12 @@ static void cached_dev_detach_finish(struct work_struct *w)
 
 	mutex_lock(&bch_register_lock);
 
+	cancel_delayed_work_sync(&dc->writeback_rate_update);
+	if (!IS_ERR_OR_NULL(dc->writeback_thread)) {
+		kthread_stop(dc->writeback_thread);
+		dc->writeback_thread = NULL;
+	}
+
 	memset(&dc->sb.set_uuid, 0, 16);
 	SET_BDEV_STATE(&dc->sb, BDEV_STATE_NONE);
 

commit 263663cd3c4fbfc40cb7504c4be2dadbc0992cc1
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Dec 18 20:22:04 2017 +0800

    block: convert to bio_first_bvec_all & bio_first_page_all
    
    This patch converts to bio_first_bvec_all() & bio_first_page_all() for
    retrieving the 1st bvec/page, and prepares for supporting multipage bvec.
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b4d28928dec5..8399fe0651f2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -211,7 +211,7 @@ static void write_bdev_super_endio(struct bio *bio)
 
 static void __write_super(struct cache_sb *sb, struct bio *bio)
 {
-	struct cache_sb *out = page_address(bio->bi_io_vec[0].bv_page);
+	struct cache_sb *out = page_address(bio_first_page_all(bio));
 	unsigned i;
 
 	bio->bi_iter.bi_sector	= SB_SECTOR;
@@ -1166,7 +1166,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	dc->bdev->bd_holder = dc;
 
 	bio_init(&dc->sb_bio, dc->sb_bio.bi_inline_vecs, 1);
-	dc->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	bio_first_bvec_all(&dc->sb_bio)->bv_page = sb_page;
 	get_page(sb_page);
 
 	if (cached_dev_init(dc, sb->block_size << 9))
@@ -1810,7 +1810,7 @@ void bch_cache_release(struct kobject *kobj)
 		free_fifo(&ca->free[i]);
 
 	if (ca->sb_bio.bi_inline_vecs[0].bv_page)
-		put_page(ca->sb_bio.bi_io_vec[0].bv_page);
+		put_page(bio_first_page_all(&ca->sb_bio));
 
 	if (!IS_ERR_OR_NULL(ca->bdev))
 		blkdev_put(ca->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
@@ -1864,7 +1864,7 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	ca->bdev->bd_holder = ca;
 
 	bio_init(&ca->sb_bio, ca->sb_bio.bi_inline_vecs, 1);
-	ca->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	bio_first_bvec_all(&ca->sb_bio)->bv_page = sb_page;
 	get_page(sb_page);
 
 	if (blk_queue_discard(bdev_get_queue(ca->bdev)))

commit 330a4db89d39a6b43f36da16824eaa7a7509d34d
Author: Liang Chen <liangchen.linux@gmail.com>
Date:   Mon Oct 30 14:46:35 2017 -0700

    bcache: explicitly destroy mutex while exiting
    
    mutex_destroy does nothing most of time, but it's better to call
    it to make the code future proof and it also has some meaning
    for like mutex debug.
    
    As Coly pointed out in a previous review, bcache_exit() may not be
    able to handle all the references properly if userspace registers
    cache and backing devices right before bch_debug_init runs and
    bch_debug_init failes later. So not exposing userspace interface
    until everything is ready to avoid that issue.
    
    Signed-off-by: Liang Chen <liangchen.linux@gmail.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Reviewed-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 46134c45c6f6..b4d28928dec5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2095,6 +2095,7 @@ static void bcache_exit(void)
 	if (bcache_major)
 		unregister_blkdev(bcache_major, "bcache");
 	unregister_reboot_notifier(&reboot);
+	mutex_destroy(&bch_register_lock);
 }
 
 static int __init bcache_init(void)
@@ -2113,14 +2114,15 @@ static int __init bcache_init(void)
 	bcache_major = register_blkdev(0, "bcache");
 	if (bcache_major < 0) {
 		unregister_reboot_notifier(&reboot);
+		mutex_destroy(&bch_register_lock);
 		return bcache_major;
 	}
 
 	if (!(bcache_wq = alloc_workqueue("bcache", WQ_MEM_RECLAIM, 0)) ||
 	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
-	    sysfs_create_files(bcache_kobj, files) ||
 	    bch_request_init() ||
-	    bch_debug_init(bcache_kobj))
+	    bch_debug_init(bcache_kobj) ||
+	    sysfs_create_files(bcache_kobj, files))
 		goto err;
 
 	return 0;

commit 3b304d24a718ae779ee9c7f2014dd3b2d0893b70
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Mon Oct 30 14:46:32 2017 -0700

    bcache: convert cached_dev.count from atomic_t to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable cached_dev.count is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 72c3b2929ef0..46134c45c6f6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -902,7 +902,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	closure_init_stack(&cl);
 
 	BUG_ON(!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags));
-	BUG_ON(atomic_read(&dc->count));
+	BUG_ON(refcount_read(&dc->count));
 
 	mutex_lock(&bch_register_lock);
 
@@ -1029,7 +1029,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	 * dc->c must be set before dc->count != 0 - paired with the mb in
 	 * cached_dev_get()
 	 */
-	atomic_set(&dc->count, 1);
+	refcount_set(&dc->count, 1);
 
 	/* Block writeback thread, but spawn it */
 	down_write(&dc->writeback_lock);
@@ -1041,7 +1041,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
 		bch_sectors_dirty_init(&dc->disk);
 		atomic_set(&dc->has_dirty, 1);
-		atomic_inc(&dc->count);
+		refcount_inc(&dc->count);
 		bch_writeback_queue(dc);
 	}
 

commit e89d67596e202119ea846cc997f4cf75cb284490
Author: Yijing Wang <wangyijing@huawei.com>
Date:   Fri Oct 13 16:35:32 2017 -0700

    bcache: Remove redundant set_capacity
    
    set_capacity() has been called in bcache_device_init(),
    remove the redundant one.
    
    Signed-off-by: Yijing Wang <wangyijing@huawei.com>
    Reviewed-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Acked-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a478d1ac0480..72c3b2929ef0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1142,9 +1142,6 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 	if (ret)
 		return ret;
 
-	set_capacity(dc->disk.disk,
-		     dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
-
 	dc->disk.disk->queue->backing_dev_info->ra_pages =
 		max(dc->disk.disk->queue->backing_dev_info->ra_pages,
 		    q->backing_dev_info->ra_pages);

commit 1dbe32ad0a82f39c6dfb7667c5da5c23b9333664
Author: Coly Li <colyli@suse.de>
Date:   Fri Oct 13 16:35:31 2017 -0700

    bcache: rewrite multiple partitions support
    
    Current partition support of bcache is confusing and buggy. It tries to
    trace non-continuous device minor numbers by an ida bit string, and
    mistakenly mixed bcache device index with minor numbers. This design
    generates several negative results,
    - Index of bcache device name is not consecutive under /dev/. If there are
      3 bcache devices, they name will be,
      /dev/bcache0, /dev/bcache16, /dev/bcache32
      Only bcache code indexes bcache device name is such an interesting way.
    - First minor number of each bcache device is traced by ida bit string.
      One bcache device will occupy 16 bits, this is not a good idea. Indeed
      only one bit is enough.
    - Because minor number and bcache device index are mixed, a device index
      is allocated by ida_simple_get(), but an first minor number is sent into
      ida_simple_remove() to release the device. It confused original author
      too.
    
    Root cause of the above errors is, bcache code should not handle device
    minor numbers at all! A standard process to support multiple partitions in
    Linux kernel is,
    - Device driver provides major device number, and indexes multiple device
      instances.
    - Device driver does not allocat nor trace device minor number, only
      provides a first minor number of a given device instance, and sets how
      many minor numbers (paritions) the device instance may have.
    All rested stuffs are handled by block layer code, most of the details can
    be found from block/{genhd, partition-generic}.c files.
    
    This patch re-writes multiple partitions support for bcache. It makes
    whole things to be more clear, and uses ida bit string in a more efficeint
    way.
    - Ida bit string only traces bcache device index, not minor number. For a
      bcache device with 128 partitions, only one bit in ida bit string is
      enough.
    - Device minor number and device index are separated in concept. Device
      index is used for /dev node naming, and ida bit string trace. Minor
      number is calculated from device index and only used to initialize
      first_minor of a bcache device.
    - It does not follow any standard for 16 partitions on a bcache device.
      This patch sets 128 partitions on single bcache device at max, this is
      the limitation from GPT (GUID Partition Table) and supported by fdisk.
    
    Considering a typical device minor number is 20 bits width, each bcache
    device may have 128 partitions (7 bits), there can be 8192 bcache devices
    existing on system. For most common deployment for a single server in
    now days, it should be enough.
    
    [minor spelling fixes in commit message by Michael Lyle]
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Cc: Eric Wheeler <bcache@lists.ewheeler.net>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fc0a31b13ac4..a478d1ac0480 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -53,12 +53,15 @@ LIST_HEAD(bch_cache_sets);
 static LIST_HEAD(uncached_devices);
 
 static int bcache_major;
-static DEFINE_IDA(bcache_minor);
+static DEFINE_IDA(bcache_device_idx);
 static wait_queue_head_t unregister_wait;
 struct workqueue_struct *bcache_wq;
 
 #define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
-#define BCACHE_MINORS		16 /* partition support */
+/* limitation of partitions number on single bcache device */
+#define BCACHE_MINORS		128
+/* limitation of bcache devices number on single system */
+#define BCACHE_DEVICE_IDX_MAX	((1U << MINORBITS)/BCACHE_MINORS)
 
 /* Superblock */
 
@@ -721,6 +724,16 @@ static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
 	closure_get(&c->caching);
 }
 
+static inline int first_minor_to_idx(int first_minor)
+{
+	return (first_minor/BCACHE_MINORS);
+}
+
+static inline int idx_to_first_minor(int idx)
+{
+	return (idx * BCACHE_MINORS);
+}
+
 static void bcache_device_free(struct bcache_device *d)
 {
 	lockdep_assert_held(&bch_register_lock);
@@ -734,7 +747,8 @@ static void bcache_device_free(struct bcache_device *d)
 	if (d->disk && d->disk->queue)
 		blk_cleanup_queue(d->disk->queue);
 	if (d->disk) {
-		ida_simple_remove(&bcache_minor, d->disk->first_minor);
+		ida_simple_remove(&bcache_device_idx,
+				  first_minor_to_idx(d->disk->first_minor));
 		put_disk(d->disk);
 	}
 
@@ -751,7 +765,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 {
 	struct request_queue *q;
 	size_t n;
-	int minor;
+	int idx;
 
 	if (!d->stripe_size)
 		d->stripe_size = 1 << 31;
@@ -776,25 +790,24 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (!d->full_dirty_stripes)
 		return -ENOMEM;
 
-	minor = ida_simple_get(&bcache_minor, 0, MINORMASK + 1, GFP_KERNEL);
-	if (minor < 0)
-		return minor;
-
-	minor *= BCACHE_MINORS;
+	idx = ida_simple_get(&bcache_device_idx, 0,
+				BCACHE_DEVICE_IDX_MAX, GFP_KERNEL);
+	if (idx < 0)
+		return idx;
 
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio),
 					   BIOSET_NEED_BVECS |
 					   BIOSET_NEED_RESCUER)) ||
 	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
-		ida_simple_remove(&bcache_minor, minor);
+		ida_simple_remove(&bcache_device_idx, idx);
 		return -ENOMEM;
 	}
 
 	set_capacity(d->disk, sectors);
-	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", minor);
+	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", idx);
 
 	d->disk->major		= bcache_major;
-	d->disk->first_minor	= minor;
+	d->disk->first_minor	= idx_to_first_minor(idx);
 	d->disk->fops		= &bcache_ops;
 	d->disk->private_data	= d;
 

commit 175206cf9ab63161dec74d9cd7f9992e062491f5
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Thu Sep 7 01:28:53 2017 +0800

    bcache: initialize dirty stripes in flash_dev_run()
    
    bcache uses a Proportion-Differentiation Controller algorithm to control
    writeback rate to cached devices. In the PD controller algorithm, dirty
    stripes of thin flash device should not be counted in, because flash only
    volumes never write back dirty data.
    
    Currently dirty stripe counter for thin flash device is not initialized
    when the thin flash device starts. Which means the following calculation
    in PD controller will reference an undefined dirty stripes number, and
    all cached devices attached to the same cache set where the thin flash
    device lies on may have an inaccurate writeback rate.
    
    This patch calles bch_sectors_dirty_init() in flash_dev_run(), to
    correctly initialize dirty stripe counter when the thin flash device
    starts to run. This patch also does following parameter data type change,
     -void bch_sectors_dirty_init(struct cached_dev *dc);
     +void bch_sectors_dirty_init(struct bcache_device *);
    to call this function conveniently in flash_dev_run().
    
    (Commit log is composed by Coly Li)
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1ae63374366c..fc0a31b13ac4 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1026,7 +1026,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	}
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
-		bch_sectors_dirty_init(dc);
+		bch_sectors_dirty_init(&dc->disk);
 		atomic_set(&dc->has_dirty, 1);
 		atomic_inc(&dc->count);
 		bch_writeback_queue(dc);
@@ -1230,6 +1230,7 @@ static int flash_dev_run(struct cache_set *c, struct uuid_entry *u)
 		goto err;
 
 	bcache_device_attach(d, c, u - c->uuids);
+	bch_sectors_dirty_init(d);
 	bch_flash_dev_request_init(d);
 	add_disk(d->disk);
 

commit da22f0eea555baf9b0a84b52afe56db2052cfe8d
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed Sep 6 14:26:00 2017 +0800

    bcache: silence static checker warning
    
    In olden times, closure_return() used to have a hidden return built in.
    We removed the hidden return but forgot to add a new return here.  If
    "c" were NULL we would oops on the next line, but fortunately "c" is
    never NULL.  Let's just remove the if statement.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3b724fa2b68d..1ae63374366c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1376,9 +1376,6 @@ static void cache_set_flush(struct closure *cl)
 	struct btree *b;
 	unsigned i;
 
-	if (!c)
-		closure_return(cl);
-
 	bch_cache_accounting_destroy(&c->accounting);
 
 	kobject_put(&c->internal);

commit 9baf30972b5568d8b5bc8b3c46a6ec5b58100463
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Sep 6 14:25:59 2017 +0800

    bcache: fix for gc and write-back race
    
    gc and write-back get raced (see the email "bcache get stucked" I sended
    before):
    gc thread                               write-back thread
    |                                       |bch_writeback_thread()
    |bch_gc_thread()                        |
    |                                       |==>read_dirty()
    |==>bch_btree_gc()                      |
    |==>btree_root() //get btree root       |
    |                //node write locker    |
    |==>bch_btree_gc_root()                 |
    |                                       |==>read_dirty_submit()
    |                                       |==>write_dirty()
    |                                       |==>continue_at(cl,
    |                                       |               write_dirty_finish,
    |                                       |               system_wq);
    |                                       |==>write_dirty_finish()//excute
    |                                       |               //in system_wq
    |                                       |==>bch_btree_insert()
    |                                       |==>bch_btree_map_leaf_nodes()
    |                                       |==>__bch_btree_map_nodes()
    |                                       |==>btree_root //try to get btree
    |                                       |              //root node read
    |                                       |              //lock
    |                                       |-----stuck here
    |==>bch_btree_set_root()
    |==>bch_journal_meta()
    |==>bch_journal()
    |==>journal_try_write()
    |==>journal_write_unlocked() //journal_full(&c->journal)
    |                            //condition satisfied
    |==>continue_at(cl, journal_write, system_wq); //try to excute
    |                               //journal_write in system_wq
    |                               //but work queue is excuting
    |                               //write_dirty_finish()
    |==>closure_sync(); //wait journal_write execute
    |                   //over and wake up gc,
    |-------------stuck here
    |==>release root node write locker
    
    This patch alloc a separate work-queue for write-back thread to avoid such
    race.
    
    (Commit log re-organized by Coly Li to pass checkpatch.pl checking)
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Acked-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c3fedd265d18..3b724fa2b68d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1059,6 +1059,8 @@ static void cached_dev_free(struct closure *cl)
 	cancel_delayed_work_sync(&dc->writeback_rate_update);
 	if (!IS_ERR_OR_NULL(dc->writeback_thread))
 		kthread_stop(dc->writeback_thread);
+	if (dc->writeback_write_wq)
+		destroy_workqueue(dc->writeback_write_wq);
 
 	mutex_lock(&bch_register_lock);
 

commit 4b758df21ee7081ab41448d21d60367efaa625b3
Author: Jan Kara <jack@suse.cz>
Date:   Wed Sep 6 14:25:51 2017 +0800

    bcache: Fix leak of bdev reference
    
    If blkdev_get_by_path() in register_bcache() fails, we try to lookup the
    block device using lookup_bdev() to detect which situation we are in to
    properly report error. However we never drop the reference returned to
    us from lookup_bdev(). Fix that.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 974d832e54a6..c3fedd265d18 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1964,6 +1964,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			else
 				err = "device busy";
 			mutex_unlock(&bch_register_lock);
+			if (!IS_ERR(bdev))
+				bdput(bdev);
 			if (attr == &ksysfs_register_quiet)
 				goto out;
 		}

commit 74d46992e0d9dee7f1f376de0d56d31614c8a17a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 23 19:10:32 2017 +0200

    block: replace bi_bdev with a gendisk pointer and partitions index
    
    This way we don't need a block_device structure to submit I/O.  The
    block_device has different life time rules from the gendisk and
    request_queue and is usually only available when the block device node
    is open.  Other callers need to explicitly create one (e.g. the lightnvm
    passthrough code, or the new nvme multipathing code).
    
    For the actual I/O path all that we need is the gendisk, which exists
    once per block device.  But given that the block layer also does
    partition remapping we additionally need a partition index, which is
    used for said remapping in generic_make_request.
    
    Note that all the block drivers generally want request_queue or
    sometimes the gendisk, so this removes a layer of indirection all
    over the stack.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8352fad765f6..974d832e54a6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -257,7 +257,7 @@ void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 	closure_init(cl, parent);
 
 	bio_reset(bio);
-	bio->bi_bdev	= dc->bdev;
+	bio_set_dev(bio, dc->bdev);
 	bio->bi_end_io	= write_bdev_super_endio;
 	bio->bi_private = dc;
 
@@ -303,7 +303,7 @@ void bcache_write_super(struct cache_set *c)
 		SET_CACHE_SYNC(&ca->sb, CACHE_SYNC(&c->sb));
 
 		bio_reset(bio);
-		bio->bi_bdev	= ca->bdev;
+		bio_set_dev(bio, ca->bdev);
 		bio->bi_end_io	= write_super_endio;
 		bio->bi_private = ca;
 
@@ -508,7 +508,7 @@ static void prio_io(struct cache *ca, uint64_t bucket, int op,
 	closure_init_stack(cl);
 
 	bio->bi_iter.bi_sector	= bucket * ca->sb.bucket_size;
-	bio->bi_bdev		= ca->bdev;
+	bio_set_dev(bio, ca->bdev);
 	bio->bi_iter.bi_size	= bucket_bytes(ca);
 
 	bio->bi_end_io	= prio_endio;

commit 47e0fb461fca1a68a566c82fcc006cc787312d8c
Author: NeilBrown <neilb@suse.com>
Date:   Sun Jun 18 14:38:57 2017 +1000

    blk: make the bioset rescue_workqueue optional.
    
    This patch converts bioset_create() to not create a workqueue by
    default, so alloctions will never trigger punt_bios_to_rescuer().  It
    also introduces a new flag BIOSET_NEED_RESCUER which tells
    bioset_create() to preserve the old behavior.
    
    All callers of bioset_create() that are inside block device drivers,
    are given the BIOSET_NEED_RESCUER flag.
    
    biosets used by filesystems or other top-level users do not
    need rescuing as the bio can never be queued behind other
    bios.  This includes fs_bio_set, blkdev_dio_pool,
    btrfs_bioset, xfs_ioend_bioset, and one allocated by
    target_core_iblock.c.
    
    biosets used by md/raid do not need rescuing as
    their usage was recently audited and revised to never
    risk deadlock.
    
    It is hoped that most, if not all, of the remaining biosets
    can end up being the non-rescued version.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Credit-to: Ming Lei <ming.lei@redhat.com> (minor fixes)
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index abd6e825b39b..8352fad765f6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -782,7 +782,9 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 
 	minor *= BCACHE_MINORS;
 
-	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio), BIOSET_NEED_BVECS)) ||
+	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio),
+					   BIOSET_NEED_BVECS |
+					   BIOSET_NEED_RESCUER)) ||
 	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
 		ida_simple_remove(&bcache_minor, minor);
 		return -ENOMEM;
@@ -1516,7 +1518,9 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 				sizeof(struct bbio) + sizeof(struct bio_vec) *
 				bucket_pages(c))) ||
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
-	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio), BIOSET_NEED_BVECS)) ||
+	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio),
+					   BIOSET_NEED_BVECS |
+					   BIOSET_NEED_RESCUER)) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    !(c->moving_gc_wq = alloc_workqueue("bcache_gc",
 						WQ_MEM_RECLAIM, 0)) ||

commit 011067b05668b05aae88e5a24cff0ca0a67ca0b0
Author: NeilBrown <neilb@suse.com>
Date:   Sun Jun 18 14:38:57 2017 +1000

    blk: replace bioset_create_nobvec() with a flags arg to bioset_create()
    
    "flags" arguments are often seen as good API design as they allow
    easy extensibility.
    bioset_create_nobvec() is implemented internally as a variation in
    flags passed to __bioset_create().
    
    To support future extension, make the internal structure part of the
    API.
    i.e. add a 'flags' argument to bioset_create() and discard
    bioset_create_nobvec().
    
    Note that the bio_split allocations in drivers/md/raid* do not need
    the bvec mempool - they should have used bioset_create_nobvec().
    
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Reviewed-by: Christoph Hellwig <hch@infradead.org>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fbc4f5412dec..abd6e825b39b 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -782,7 +782,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 
 	minor *= BCACHE_MINORS;
 
-	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
+	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio), BIOSET_NEED_BVECS)) ||
 	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
 		ida_simple_remove(&bcache_minor, minor);
 		return -ENOMEM;
@@ -1516,7 +1516,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 				sizeof(struct bbio) + sizeof(struct bio_vec) *
 				bucket_pages(c))) ||
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
-	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
+	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio), BIOSET_NEED_BVECS)) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    !(c->moving_gc_wq = alloc_workqueue("bcache_gc",
 						WQ_MEM_RECLAIM, 0)) ||

commit 4e4cbee93d56137ebff722be022cae5f70ef84fb
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:06 2017 +0200

    block: switch bios to blk_status_t
    
    Replace bi_error with a new bi_status to allow for a clear conversion.
    Note that device mapper overloaded bi_error with a private value, which
    we'll have to keep arround at least for now and thus propagate to a
    proper blk_status_t value.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e57353e39168..fbc4f5412dec 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -271,7 +271,7 @@ static void write_super_endio(struct bio *bio)
 {
 	struct cache *ca = bio->bi_private;
 
-	bch_count_io_errors(ca, bio->bi_error, "writing superblock");
+	bch_count_io_errors(ca, bio->bi_status, "writing superblock");
 	closure_put(&ca->set->sb_write);
 }
 
@@ -321,7 +321,7 @@ static void uuid_endio(struct bio *bio)
 	struct closure *cl = bio->bi_private;
 	struct cache_set *c = container_of(cl, struct cache_set, uuid_write);
 
-	cache_set_err_on(bio->bi_error, c, "accessing uuids");
+	cache_set_err_on(bio->bi_status, c, "accessing uuids");
 	bch_bbio_free(bio, c);
 	closure_put(cl);
 }
@@ -494,7 +494,7 @@ static void prio_endio(struct bio *bio)
 {
 	struct cache *ca = bio->bi_private;
 
-	cache_set_err_on(bio->bi_error, ca->set, "accessing priorities");
+	cache_set_err_on(bio->bi_status, ca->set, "accessing priorities");
 	bch_bbio_free(bio, ca->set);
 	closure_put(&ca->prio);
 }

commit bc4e54f6e966e9ca35064cd60f91b1478c07a1b2
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:37 2017 -0700

    drivers/md/bcache/super.c: use kvmalloc
    
    bcache_device_init uses kmalloc for small requests and vmalloc for those
    which are larger than 64 pages.  This alone is a strange criterion.
    Moreover kmalloc can fallback to vmalloc on the failure.  Let's simply
    use kvmalloc instead as it knows how to handle the fallback properly
    
    Link: http://lkml.kernel.org/r/20170306103327.2766-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 85e3f21c2514..e57353e39168 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -767,16 +767,12 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	}
 
 	n = d->nr_stripes * sizeof(atomic_t);
-	d->stripe_sectors_dirty = n < PAGE_SIZE << 6
-		? kzalloc(n, GFP_KERNEL)
-		: vzalloc(n);
+	d->stripe_sectors_dirty = kvzalloc(n, GFP_KERNEL);
 	if (!d->stripe_sectors_dirty)
 		return -ENOMEM;
 
 	n = BITS_TO_LONGS(d->nr_stripes) * sizeof(unsigned long);
-	d->full_dirty_stripes = n < PAGE_SIZE << 6
-		? kzalloc(n, GFP_KERNEL)
-		: vzalloc(n);
+	d->full_dirty_stripes = kvzalloc(n, GFP_KERNEL);
 	if (!d->full_dirty_stripes)
 		return -ENOMEM;
 

commit dc3b17cc8bf21307c7e076e7c778d5db756f7871
Author: Jan Kara <jack@suse.cz>
Date:   Thu Feb 2 15:56:50 2017 +0100

    block: Use pointer to backing_dev_info from request_queue
    
    We will want to have struct backing_dev_info allocated separately from
    struct request_queue. As the first step add pointer to backing_dev_info
    to request_queue and convert all users touching it. No functional
    changes in this patch.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3a19cbc8b230..85e3f21c2514 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -807,7 +807,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	blk_queue_make_request(q, NULL);
 	d->disk->queue			= q;
 	q->queuedata			= d;
-	q->backing_dev_info.congested_data = d;
+	q->backing_dev_info->congested_data = d;
 	q->limits.max_hw_sectors	= UINT_MAX;
 	q->limits.max_sectors		= UINT_MAX;
 	q->limits.max_segment_size	= UINT_MAX;
@@ -1132,9 +1132,9 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 	set_capacity(dc->disk.disk,
 		     dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
 
-	dc->disk.disk->queue->backing_dev_info.ra_pages =
-		max(dc->disk.disk->queue->backing_dev_info.ra_pages,
-		    q->backing_dev_info.ra_pages);
+	dc->disk.disk->queue->backing_dev_info->ra_pages =
+		max(dc->disk.disk->queue->backing_dev_info->ra_pages,
+		    q->backing_dev_info->ra_pages);
 
 	bch_cached_dev_request_init(dc);
 	bch_cached_dev_writeback_init(dc);

commit b8c0d911ac5285e6be8967713271a51bdc5a936a
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Sun Oct 23 18:19:20 2016 -0700

    bcache: partition support: add 16 minors per bcacheN device
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Tested-by: Wido den Hollander <wido@widodh.nl>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b33dd3bd104f..3a19cbc8b230 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -58,6 +58,7 @@ static wait_queue_head_t unregister_wait;
 struct workqueue_struct *bcache_wq;
 
 #define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
+#define BCACHE_MINORS		16 /* partition support */
 
 /* Superblock */
 
@@ -783,8 +784,10 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (minor < 0)
 		return minor;
 
+	minor *= BCACHE_MINORS;
+
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    !(d->disk = alloc_disk(1))) {
+	    !(d->disk = alloc_disk(BCACHE_MINORS))) {
 		ida_simple_remove(&bcache_minor, minor);
 		return -ENOMEM;
 	}

commit be628be09563f8f6e81929efbd7cf3f45c344416
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Wed Oct 26 20:31:17 2016 -0700

    bcache: Make gc wakeup sane, remove set_task_state()
    
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2fb5bfeb43e2..b33dd3bd104f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1489,6 +1489,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	mutex_init(&c->bucket_lock);
 	init_waitqueue_head(&c->btree_cache_wait);
 	init_waitqueue_head(&c->bucket_wait);
+	init_waitqueue_head(&c->gc_wait);
 	sema_init(&c->uuid_write_mutex, 1);
 
 	spin_lock_init(&c->btree_gc_time.lock);
@@ -1548,6 +1549,7 @@ static void run_cache_set(struct cache_set *c)
 
 	for_each_cache(ca, c, i)
 		c->nbuckets += ca->sb.nbuckets;
+	set_gc_sectors(c);
 
 	if (CACHE_SYNC(&c->sb)) {
 		LIST_HEAD(journal);

commit 3a83f4677539bce8eaa2bca9ee9c20e172d7ab04
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Tue Nov 22 08:57:21 2016 -0700

    block: bio: pass bvec table to bio_init()
    
    Some drivers often use external bvec table, so introduce
    this helper for this case. It is always safe to access the
    bio->bi_io_vec in this way for this case.
    
    After converting to this usage, it will becomes a bit easier
    to evaluate the remaining direct access to bio->bi_io_vec,
    so it can help to prepare for the following multipage bvec
    support.
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    
    Fixed up the new O_DIRECT cases.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 988edf928466..2fb5bfeb43e2 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1152,9 +1152,7 @@ static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 	dc->bdev = bdev;
 	dc->bdev->bd_holder = dc;
 
-	bio_init(&dc->sb_bio);
-	dc->sb_bio.bi_max_vecs	= 1;
-	dc->sb_bio.bi_io_vec	= dc->sb_bio.bi_inline_vecs;
+	bio_init(&dc->sb_bio, dc->sb_bio.bi_inline_vecs, 1);
 	dc->sb_bio.bi_io_vec[0].bv_page = sb_page;
 	get_page(sb_page);
 
@@ -1814,9 +1812,7 @@ static int cache_alloc(struct cache *ca)
 	__module_get(THIS_MODULE);
 	kobject_init(&ca->kobj, &bch_cache_ktype);
 
-	bio_init(&ca->journal.bio);
-	ca->journal.bio.bi_max_vecs = 8;
-	ca->journal.bio.bi_io_vec = ca->journal.bio.bi_inline_vecs;
+	bio_init(&ca->journal.bio, ca->journal.bio.bi_inline_vecs, 8);
 
 	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
 
@@ -1852,9 +1848,7 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	ca->bdev = bdev;
 	ca->bdev->bd_holder = ca;
 
-	bio_init(&ca->sb_bio);
-	ca->sb_bio.bi_max_vecs	= 1;
-	ca->sb_bio.bi_io_vec	= ca->sb_bio.bi_inline_vecs;
+	bio_init(&ca->sb_bio, ca->sb_bio.bi_inline_vecs, 1);
 	ca->sb_bio.bi_io_vec[0].bv_page = sb_page;
 	get_page(sb_page);
 

commit 70fd76140a6cb63262bd47b68d57b42e889c10ee
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 07:40:10 2016 -0600

    block,fs: use REQ_* flags directly
    
    Remove the WRITE_* and READ_SYNC wrappers, and just use the flags
    directly.  Where applicable this also drops usage of the
    bio_set_op_attrs wrapper.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 849ad441cd76..988edf928466 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -381,7 +381,7 @@ static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
 		return "bad uuid pointer";
 
 	bkey_copy(&c->uuid_bucket, k);
-	uuid_io(c, REQ_OP_READ, READ_SYNC, k, cl);
+	uuid_io(c, REQ_OP_READ, 0, k, cl);
 
 	if (j->version < BCACHE_JSET_VERSION_UUIDv1) {
 		struct uuid_entry_v0	*u0 = (void *) c->uuids;
@@ -600,7 +600,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 			ca->prio_last_buckets[bucket_nr] = bucket;
 			bucket_nr++;
 
-			prio_io(ca, bucket, REQ_OP_READ, READ_SYNC);
+			prio_io(ca, bucket, REQ_OP_READ, 0);
 
 			if (p->csum != bch_crc64(&p->magic, bucket_bytes(ca) - 8))
 				pr_warn("bad csum reading priorities");

commit 90706094d5be614ae7285b3c96c3125bb198618c
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Thu Aug 18 20:15:26 2016 -0700

    bcache: pr_err: more meaningful error message when nr_stripes is invalid
    
    The original error was thought to be corruption, but was actually caused by:
            make-bcache --data-offset N
    where N was in bytes and should have been in sectors.  While userspace
    tools should be updated to check --data-offset beyond end of volume,
    hopefully this will help others that might not have noticed the units.
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6b93e1b77767..849ad441cd76 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -760,7 +760,8 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (!d->nr_stripes ||
 	    d->nr_stripes > INT_MAX ||
 	    d->nr_stripes > SIZE_MAX / sizeof(atomic_t)) {
-		pr_err("nr_stripes too large");
+		pr_err("nr_stripes too large or invalid: %u (start sector beyond end of disk?)",
+			(unsigned)d->nr_stripes);
 		return -ENOMEM;
 	}
 

commit acc9cf8c66c66b2cbbdb4a375537edee72be64df
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Wed Aug 17 18:21:24 2016 -0700

    bcache: RESERVE_PRIO is too small by one when prio_buckets() is a power of two.
    
    This patch fixes a cachedev registration-time allocation deadlock.
    This can deadlock on boot if your initrd auto-registeres bcache devices:
    
    Allocator thread:
    [  720.727614] INFO: task bcache_allocato:3833 blocked for more than 120 seconds.
    [  720.732361]  [<ffffffff816eeac7>] schedule+0x37/0x90
    [  720.732963]  [<ffffffffa05192b8>] bch_bucket_alloc+0x188/0x360 [bcache]
    [  720.733538]  [<ffffffff810e6950>] ? prepare_to_wait_event+0xf0/0xf0
    [  720.734137]  [<ffffffffa05302bd>] bch_prio_write+0x19d/0x340 [bcache]
    [  720.734715]  [<ffffffffa05190bf>] bch_allocator_thread+0x3ff/0x470 [bcache]
    [  720.735311]  [<ffffffff816ee41c>] ? __schedule+0x2dc/0x950
    [  720.735884]  [<ffffffffa0518cc0>] ? invalidate_buckets+0x980/0x980 [bcache]
    
    Registration thread:
    [  720.710403] INFO: task bash:3531 blocked for more than 120 seconds.
    [  720.715226]  [<ffffffff816eeac7>] schedule+0x37/0x90
    [  720.715805]  [<ffffffffa05235cd>] __bch_btree_map_nodes+0x12d/0x150 [bcache]
    [  720.716409]  [<ffffffffa0522d30>] ? bch_btree_insert_check_key+0x1c0/0x1c0 [bcache]
    [  720.717008]  [<ffffffffa05236e4>] bch_btree_insert+0xf4/0x170 [bcache]
    [  720.717586]  [<ffffffff810e6950>] ? prepare_to_wait_event+0xf0/0xf0
    [  720.718191]  [<ffffffffa0527d9a>] bch_journal_replay+0x14a/0x290 [bcache]
    [  720.718766]  [<ffffffff810cc90d>] ? ttwu_do_activate.constprop.94+0x5d/0x70
    [  720.719369]  [<ffffffff810cf684>] ? try_to_wake_up+0x1d4/0x350
    [  720.719968]  [<ffffffffa05317d0>] run_cache_set+0x580/0x8e0 [bcache]
    [  720.720553]  [<ffffffffa053302e>] register_bcache+0xe2e/0x13b0 [bcache]
    [  720.721153]  [<ffffffff81354cef>] kobj_attr_store+0xf/0x20
    [  720.721730]  [<ffffffff812a2dad>] sysfs_kf_write+0x3d/0x50
    [  720.722327]  [<ffffffff812a225a>] kernfs_fop_write+0x12a/0x180
    [  720.722904]  [<ffffffff81225177>] __vfs_write+0x37/0x110
    [  720.723503]  [<ffffffff81228048>] ? __sb_start_write+0x58/0x110
    [  720.724100]  [<ffffffff812cedb3>] ? security_file_permission+0x23/0xa0
    [  720.724675]  [<ffffffff812258a9>] vfs_write+0xa9/0x1b0
    [  720.725275]  [<ffffffff8102479c>] ? do_audit_syscall_entry+0x6c/0x70
    [  720.725849]  [<ffffffff81226755>] SyS_write+0x55/0xd0
    [  720.726451]  [<ffffffff8106a390>] ? do_page_fault+0x30/0x80
    [  720.727045]  [<ffffffff816f2cae>] system_call_fastpath+0x12/0x71
    
    The fifo code in upstream bcache can't use the last element in the buffer,
    which was the cause of the bug: if you asked for a power of two size,
    it'd give you a fifo that could hold one less than what you asked for
    rather than allocating a buffer twice as big.
    
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Tested-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: stable@vger.kernel.org

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6ada14b9a157..6b93e1b77767 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1820,7 +1820,7 @@ static int cache_alloc(struct cache *ca)
 	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
 
 	if (!init_fifo(&ca->free[RESERVE_BTREE], 8, GFP_KERNEL) ||
-	    !init_fifo(&ca->free[RESERVE_PRIO], prio_buckets(ca), GFP_KERNEL) ||
+	    !init_fifo_exact(&ca->free[RESERVE_PRIO], prio_buckets(ca), GFP_KERNEL) ||
 	    !init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||

commit d9dc1702b297ec4a6bb9c0326a70641b322ba886
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Fri Jun 17 15:01:54 2016 -0700

    bcache: register_bcache(): call blkdev_put() when cache_alloc() fails
    
    register_cache() is supposed to return an error string on error so that
    register_bcache() will will blkdev_put and cleanup other user counters,
    but it does not set 'char *err' when cache_alloc() fails (eg, due to
    memory pressure) and thus register_bcache() performs no cleanup.
    
    register_bcache() <----------\  <- no jump to err_close, no blkdev_put()
       |                         |
       +->register_cache()       |  <- fails to set char *err
             |                   |
             +->cache_alloc() ---/  <- returns error
    
    This patch sets `char *err` for this failure case so that register_cache()
    will cause register_bcache() to correctly jump to err_close and do
    cleanup.  This was tested under OOM conditions that triggered the bug.
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Cc: stable@vger.kernel.org

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 95a4ca6ce6ff..6ada14b9a157 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1844,7 +1844,7 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 				struct block_device *bdev, struct cache *ca)
 {
 	char name[BDEVNAME_SIZE];
-	const char *err = NULL;
+	const char *err = NULL; /* must be set for any error case */
 	int ret = 0;
 
 	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
@@ -1861,8 +1861,13 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 		ca->discard = CACHE_DISCARD(&ca->sb);
 
 	ret = cache_alloc(ca);
-	if (ret != 0)
+	if (ret != 0) {
+		if (ret == -ENOMEM)
+			err = "cache_alloc(): -ENOMEM";
+		else
+			err = "cache_alloc(): unknown error";
 		goto err;
+	}
 
 	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache")) {
 		err = "error calling kobject_add";

commit 1eff9d322a444245c67515edb52bc0eb68374aa8
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Aug 5 15:35:16 2016 -0600

    block: rename bio bi_rw to bi_opf
    
    Since commit 63a4cc24867d, bio->bi_rw contains flags in the lower
    portion and the op code in the higher portions. This means that
    old code that relies on manually setting bi_rw is most likely
    going to be broken. Instead of letting that brokeness linger,
    rename the member, to force old and out-of-tree code to break
    at compile time instead of at runtime.
    
    No intended functional changes in this commit.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 88ef6d14cce3..95a4ca6ce6ff 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -347,7 +347,7 @@ static void uuid_io(struct cache_set *c, int op, unsigned long op_flags,
 	for (i = 0; i < KEY_PTRS(k); i++) {
 		struct bio *bio = bch_bbio_alloc(c);
 
-		bio->bi_rw	= REQ_SYNC|REQ_META|op_flags;
+		bio->bi_opf = REQ_SYNC | REQ_META | op_flags;
 		bio->bi_iter.bi_size = KEY_SIZE(k) << 9;
 
 		bio->bi_end_io	= uuid_endio;

commit 89b920e0031673882e2217189c655dd89d5c1553
Author: Yijing Wang <wangyijing@huawei.com>
Date:   Mon Jul 4 09:23:41 2016 +0800

    bcache: Remove redundant block_size assignment
    
    We have assigned sb->block_size before the switch,
    so remove the redundant one.
    
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Yijing Wang <wangyijing@huawei.com>
    Acked-by: Eric Wheeler <bcache@lists.ewheeler.net>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c3138f6fc783..88ef6d14cce3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -134,7 +134,6 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	case BCACHE_SB_VERSION_CDEV:
 	case BCACHE_SB_VERSION_CDEV_WITH_UUID:
 		sb->nbuckets	= le64_to_cpu(s->nbuckets);
-		sb->block_size	= le16_to_cpu(s->block_size);
 		sb->bucket_size	= le16_to_cpu(s->bucket_size);
 
 		sb->nr_in_set	= le16_to_cpu(s->nr_in_set);

commit c50d4d5dd367f996393f1d33f5a3a3048bb414f8
Author: Yijing Wang <wangyijing@huawei.com>
Date:   Mon Jul 4 09:23:25 2016 +0800

    bcache: Remove redundant parameter for cache_alloc()
    
    Cache_sb is not used in cache_alloc, and we have copied
    sb info to cache->sb already, remove it.
    
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Yijing Wang <wangyijing@huawei.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d542438b18df..c3138f6fc783 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1806,7 +1806,7 @@ void bch_cache_release(struct kobject *kobj)
 	module_put(THIS_MODULE);
 }
 
-static int cache_alloc(struct cache_sb *sb, struct cache *ca)
+static int cache_alloc(struct cache *ca)
 {
 	size_t free;
 	struct bucket *b;
@@ -1861,7 +1861,7 @@ static int register_cache(struct cache_sb *sb, struct page *sb_page,
 	if (blk_queue_discard(bdev_get_queue(ca->bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
 
-	ret = cache_alloc(sb, ca);
+	ret = cache_alloc(ca);
 	if (ret != 0)
 		goto err;
 

commit 81baf90af2dcc8259e99e2f236024524b55313fc
Author: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
Date:   Wed Jun 8 01:57:19 2016 +0530

    bcache: Remove deprecated create_workqueue
    
    alloc_workqueue replaces deprecated create_workqueue().
    
    Dedicated workqueues have been used since bcache_wq and moving_gc_wq
    are workqueues for writes and are being used on a memory reclaim path.
    WQ_MEM_RECLAIM has been set to ensure forward progress under memory
    pressure.
    Since there are only a fixed number of work items, explicit concurrency
    limit is unnecessary here.
    
    Signed-off-by: Bhaktipriya Shridhar <bhaktipriya96@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c944daf75dd0..d542438b18df 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1520,7 +1520,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
 	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
-	    !(c->moving_gc_wq = create_workqueue("bcache_gc")) ||
+	    !(c->moving_gc_wq = alloc_workqueue("bcache_gc",
+						WQ_MEM_RECLAIM, 0)) ||
 	    bch_journal_alloc(c) ||
 	    bch_btree_cache_alloc(c) ||
 	    bch_open_buckets_alloc(c) ||
@@ -2099,7 +2100,7 @@ static int __init bcache_init(void)
 		return bcache_major;
 	}
 
-	if (!(bcache_wq = create_workqueue("bcache")) ||
+	if (!(bcache_wq = alloc_workqueue("bcache", WQ_MEM_RECLAIM, 0)) ||
 	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
 	    sysfs_create_files(bcache_kobj, files) ||
 	    bch_request_init() ||

commit ad0d9e76a4124708dddd00c04fc4b56fc86c02d6
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:32:05 2016 -0500

    bcache: use bio op accessors
    
    Separate the op from the rq_flag_bits and have bcache
    set/get the bio using bio_set_op_attrs/bio_op.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1eb526a7b8b3..c944daf75dd0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -212,8 +212,8 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	unsigned i;
 
 	bio->bi_iter.bi_sector	= SB_SECTOR;
-	bio->bi_rw		= REQ_WRITE|REQ_SYNC|REQ_META;
 	bio->bi_iter.bi_size	= SB_SIZE;
+	bio_set_op_attrs(bio, REQ_OP_WRITE, REQ_SYNC|REQ_META);
 	bch_bio_map(bio, NULL);
 
 	out->offset		= cpu_to_le64(sb->offset);
@@ -333,7 +333,7 @@ static void uuid_io_unlock(struct closure *cl)
 	up(&c->uuid_write_mutex);
 }
 
-static void uuid_io(struct cache_set *c, unsigned long rw,
+static void uuid_io(struct cache_set *c, int op, unsigned long op_flags,
 		    struct bkey *k, struct closure *parent)
 {
 	struct closure *cl = &c->uuid_write;
@@ -348,21 +348,22 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 	for (i = 0; i < KEY_PTRS(k); i++) {
 		struct bio *bio = bch_bbio_alloc(c);
 
-		bio->bi_rw	= REQ_SYNC|REQ_META|rw;
+		bio->bi_rw	= REQ_SYNC|REQ_META|op_flags;
 		bio->bi_iter.bi_size = KEY_SIZE(k) << 9;
 
 		bio->bi_end_io	= uuid_endio;
 		bio->bi_private = cl;
+		bio_set_op_attrs(bio, op, REQ_SYNC|REQ_META|op_flags);
 		bch_bio_map(bio, c->uuids);
 
 		bch_submit_bbio(bio, c, k, i);
 
-		if (!(rw & WRITE))
+		if (op != REQ_OP_WRITE)
 			break;
 	}
 
 	bch_extent_to_text(buf, sizeof(buf), k);
-	pr_debug("%s UUIDs at %s", rw & REQ_WRITE ? "wrote" : "read", buf);
+	pr_debug("%s UUIDs at %s", op == REQ_OP_WRITE ? "wrote" : "read", buf);
 
 	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)
 		if (!bch_is_zero(u->uuid, 16))
@@ -381,7 +382,7 @@ static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
 		return "bad uuid pointer";
 
 	bkey_copy(&c->uuid_bucket, k);
-	uuid_io(c, READ_SYNC, k, cl);
+	uuid_io(c, REQ_OP_READ, READ_SYNC, k, cl);
 
 	if (j->version < BCACHE_JSET_VERSION_UUIDv1) {
 		struct uuid_entry_v0	*u0 = (void *) c->uuids;
@@ -426,7 +427,7 @@ static int __uuid_write(struct cache_set *c)
 		return 1;
 
 	SET_KEY_SIZE(&k.key, c->sb.bucket_size);
-	uuid_io(c, REQ_WRITE, &k.key, &cl);
+	uuid_io(c, REQ_OP_WRITE, 0, &k.key, &cl);
 	closure_sync(&cl);
 
 	bkey_copy(&c->uuid_bucket, &k.key);
@@ -498,7 +499,8 @@ static void prio_endio(struct bio *bio)
 	closure_put(&ca->prio);
 }
 
-static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
+static void prio_io(struct cache *ca, uint64_t bucket, int op,
+		    unsigned long op_flags)
 {
 	struct closure *cl = &ca->prio;
 	struct bio *bio = bch_bbio_alloc(ca->set);
@@ -507,11 +509,11 @@ static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
 
 	bio->bi_iter.bi_sector	= bucket * ca->sb.bucket_size;
 	bio->bi_bdev		= ca->bdev;
-	bio->bi_rw		= REQ_SYNC|REQ_META|rw;
 	bio->bi_iter.bi_size	= bucket_bytes(ca);
 
 	bio->bi_end_io	= prio_endio;
 	bio->bi_private = ca;
+	bio_set_op_attrs(bio, op, REQ_SYNC|REQ_META|op_flags);
 	bch_bio_map(bio, ca->disk_buckets);
 
 	closure_bio_submit(bio, &ca->prio);
@@ -557,7 +559,7 @@ void bch_prio_write(struct cache *ca)
 		BUG_ON(bucket == -1);
 
 		mutex_unlock(&ca->set->bucket_lock);
-		prio_io(ca, bucket, REQ_WRITE);
+		prio_io(ca, bucket, REQ_OP_WRITE, 0);
 		mutex_lock(&ca->set->bucket_lock);
 
 		ca->prio_buckets[i] = bucket;
@@ -599,7 +601,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 			ca->prio_last_buckets[bucket_nr] = bucket;
 			bucket_nr++;
 
-			prio_io(ca, bucket, READ_SYNC);
+			prio_io(ca, bucket, REQ_OP_READ, READ_SYNC);
 
 			if (p->csum != bch_crc64(&p->magic, bucket_bytes(ca) - 8))
 				pr_warn("bad csum reading priorities");

commit 4e49ea4a3d276365bf7396c9b77b4d1d5923835a
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:41 2016 -0500

    block/fs/drivers: remove rw argument from submit_bio
    
    This has callers of submit_bio/submit_bio_wait set the bio->bi_rw
    instead of passing it in. This makes that use the same as
    generic_make_request and how we set the other bio fields.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    
    Fixed up fs/ext4/crypto.c
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f5dbb4e884d8..1eb526a7b8b3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -212,7 +212,7 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	unsigned i;
 
 	bio->bi_iter.bi_sector	= SB_SECTOR;
-	bio->bi_rw		= REQ_SYNC|REQ_META;
+	bio->bi_rw		= REQ_WRITE|REQ_SYNC|REQ_META;
 	bio->bi_iter.bi_size	= SB_SIZE;
 	bch_bio_map(bio, NULL);
 
@@ -238,7 +238,7 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	pr_debug("ver %llu, flags %llu, seq %llu",
 		 sb->version, sb->flags, sb->seq);
 
-	submit_bio(REQ_WRITE, bio);
+	submit_bio(bio);
 }
 
 static void bch_write_bdev_super_unlock(struct closure *cl)

commit 84b4ff9ef22a97231e5d6aeca544a243d0ac5d81
Author: Jens Axboe <axboe@fb.com>
Date:   Wed Mar 30 10:13:22 2016 -0600

    bcache: switch to using blk_queue_write_cache()
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a296425a7270..f5dbb4e884d8 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -816,7 +816,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	clear_bit(QUEUE_FLAG_ADD_RANDOM, &d->disk->queue->queue_flags);
 	set_bit(QUEUE_FLAG_DISCARD,	&d->disk->queue->queue_flags);
 
-	blk_queue_flush(q, REQ_FLUSH|REQ_FUA);
+	blk_queue_write_cache(q, true, true);
 
 	return 0;
 }

commit f8b11260a445169989d01df75d35af0f56178f95
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Mon Mar 7 15:17:50 2016 -0800

    bcache: fix cache_set_flush() NULL pointer dereference on OOM
    
    When bch_cache_set_alloc() fails to kzalloc the cache_set, the
    asyncronous closure handling tries to dereference a cache_set that
    hadn't yet been allocated inside of cache_set_flush() which is called
    by __cache_set_unregister() during cleanup.  This appears to happen only
    during an OOM condition on bcache_register.
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: stable@vger.kernel.org

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6b07a0c8c729..a296425a7270 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1373,6 +1373,9 @@ static void cache_set_flush(struct closure *cl)
 	struct btree *b;
 	unsigned i;
 
+	if (!c)
+		closure_return(cl);
+
 	bch_cache_accounting_destroy(&c->accounting);
 
 	kobject_put(&c->internal);

commit 9b299728ed777428b3908ac72ace5f8f84b97789
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Fri Feb 26 14:33:56 2016 -0800

    bcache: cleaned up error handling around register_cache()
    
    Fix null pointer dereference by changing register_cache() to return an int
    instead of being void.  This allows it to return -ENOMEM or -ENODEV and
    enables upper layers to handle the OOM case without NULL pointer issues.
    
    See this thread:
      http://thread.gmane.org/gmane.linux.kernel.bcache.devel/3521
    
    Fixes this error:
      gargamel:/sys/block/md5/bcache# echo /dev/sdh2 > /sys/fs/bcache/register
    
      bcache: register_cache() error opening sdh2: cannot allocate memory
      BUG: unable to handle kernel NULL pointer dereference at 00000000000009b8
      IP: [<ffffffffc05a7e8d>] cache_set_flush+0x102/0x15c [bcache]
      PGD 120dff067 PUD 1119a3067 PMD 0
      Oops: 0000 [#1] SMP
      Modules linked in: veth ip6table_filter ip6_tables
      (...)
      CPU: 4 PID: 3371 Comm: kworker/4:3 Not tainted 4.4.2-amd64-i915-volpreempt-20160213bc1 #3
      Hardware name: System manufacturer System Product Name/P8H67-M PRO, BIOS 3904 04/27/2013
      Workqueue: events cache_set_flush [bcache]
      task: ffff88020d5dc280 ti: ffff88020b6f8000 task.ti: ffff88020b6f8000
      RIP: 0010:[<ffffffffc05a7e8d>]  [<ffffffffc05a7e8d>] cache_set_flush+0x102/0x15c [bcache]
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Tested-by: Marc MERLIN <marc@merlins.org>
    Cc: <stable@vger.kernel.org>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b411c73bfeb3..6b07a0c8c729 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1835,11 +1835,12 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	return 0;
 }
 
-static void register_cache(struct cache_sb *sb, struct page *sb_page,
+static int register_cache(struct cache_sb *sb, struct page *sb_page,
 				struct block_device *bdev, struct cache *ca)
 {
 	char name[BDEVNAME_SIZE];
-	const char *err = "cannot allocate memory";
+	const char *err = NULL;
+	int ret = 0;
 
 	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
 	ca->bdev = bdev;
@@ -1854,27 +1855,35 @@ static void register_cache(struct cache_sb *sb, struct page *sb_page,
 	if (blk_queue_discard(bdev_get_queue(ca->bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
 
-	if (cache_alloc(sb, ca) != 0)
+	ret = cache_alloc(sb, ca);
+	if (ret != 0)
 		goto err;
 
-	err = "error creating kobject";
-	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache"))
-		goto err;
+	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache")) {
+		err = "error calling kobject_add";
+		ret = -ENOMEM;
+		goto out;
+	}
 
 	mutex_lock(&bch_register_lock);
 	err = register_cache_set(ca);
 	mutex_unlock(&bch_register_lock);
 
-	if (err)
-		goto err;
+	if (err) {
+		ret = -ENODEV;
+		goto out;
+	}
 
 	pr_info("registered cache device %s", bdevname(bdev, name));
+
 out:
 	kobject_put(&ca->kobj);
-	return;
+
 err:
-	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
-	goto out;
+	if (err)
+		pr_notice("error opening %s: %s", bdevname(bdev, name), err);
+
+	return ret;
 }
 
 /* Global interfaces/init */
@@ -1972,7 +1981,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		if (!ca)
 			goto err_close;
 
-		register_cache(sb, sb_page, bdev, ca);
+		if (register_cache(sb, sb_page, bdev, ca) != 0)
+			goto err_close;
 	}
 out:
 	if (sb_page)

commit 07cc6ef8edc47f8b4fc1e276d31127a0a5863d4d
Author: Eric Wheeler <git@linux.ewheeler.net>
Date:   Fri Feb 26 14:39:06 2016 -0800

    bcache: fix race of writeback thread starting before complete initialization
    
    The bch_writeback_thread might BUG_ON in read_dirty() if
    dc->sb==BDEV_STATE_DIRTY and bch_sectors_dirty_init has not yet completed
    its related initialization.  This patch downs the dc->writeback_lock until
    after initialization is complete, thus preventing bch_writeback_thread
    from proceeding prematurely.
    
    See this thread:
      http://thread.gmane.org/gmane.linux.kernel.bcache.devel/3453
    
    Signed-off-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Tested-by: Marc MERLIN <marc@merlins.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8d0ead98eb6e..b411c73bfeb3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1015,8 +1015,12 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	 */
 	atomic_set(&dc->count, 1);
 
-	if (bch_cached_dev_writeback_start(dc))
+	/* Block writeback thread, but spawn it */
+	down_write(&dc->writeback_lock);
+	if (bch_cached_dev_writeback_start(dc)) {
+		up_write(&dc->writeback_lock);
 		return -ENOMEM;
+	}
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
 		bch_sectors_dirty_init(dc);
@@ -1028,6 +1032,9 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	bch_cached_dev_run(dc);
 	bcache_device_link(&dc->disk, c, "bdev");
 
+	/* Allow the writeback thread to proceed */
+	up_write(&dc->writeback_lock);
+
 	pr_info("Caching %s as %s on set %pU",
 		bdevname(dc->bdev, buf), dc->disk.disk->disk_name,
 		dc->disk.c->sb.set_uuid);

commit d7076f21629f8f329bca4a44dc408d94670f49e2
Author: Gabriel de Perthuis <g2p.code@gmail.com>
Date:   Sun Nov 29 18:40:23 2015 -0800

    bcache: allows use of register in udev to avoid "device_busy" error.
    
    Allows to use register, not register_quiet in udev to avoid "device_busy" error.
    The initial patch proposed at https://lkml.org/lkml/2013/8/26/549 by Gabriel de Perthuis
    <g2p.code@gmail.com> does not unlock the mutex and hangs the kernel.
    
    See http://thread.gmane.org/gmane.linux.kernel.bcache.devel/2594 for the discussion.
    
    Cc: Denis Bychkov <manover@gmail.com>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Cc: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Gabriel de Perthuis <g2p.code@gmail.com>
    Cc: stable@vger.kernel.org
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 18f14a246d70..8d0ead98eb6e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1938,6 +1938,8 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			else
 				err = "device busy";
 			mutex_unlock(&bch_register_lock);
+			if (attr == &ksysfs_register_quiet)
+				goto out;
 		}
 		goto err;
 	}
@@ -1976,8 +1978,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 err_close:
 	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 err:
-	if (attr != &ksysfs_register_quiet)
-		pr_info("error opening %s: %s", path, err);
+	pr_info("error opening %s: %s", path, err);
 	ret = -EINVAL;
 	goto out;
 }

commit 2ecf0cdb2b437402110ab57546e02abfa68a716b
Author: Zheng Liu <wenqing.lz@taobao.com>
Date:   Sun Nov 29 17:21:57 2015 -0800

    bcache: unregister reboot notifier if bcache fails to unregister device
    
    In bcache_init() function it forgot to unregister reboot notifier if
    bcache fails to unregister a block device.  This commit fixes this.
    
    Signed-off-by: Zheng Liu <wenqing.lz@taobao.com>
    Tested-by: Joshua Schmid <jschmid@suse.com>
    Tested-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Kent Overstreet <kmo@daterainc.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 43e911e4e8d0..18f14a246d70 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2071,8 +2071,10 @@ static int __init bcache_init(void)
 	closure_debug_init();
 
 	bcache_major = register_blkdev(0, "bcache");
-	if (bcache_major < 0)
+	if (bcache_major < 0) {
+		unregister_reboot_notifier(&reboot);
 		return bcache_major;
+	}
 
 	if (!(bcache_wq = create_workqueue("bcache")) ||
 	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||

commit 4d4d8573a8451acc9f01cbea24b7e55f04a252fe
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Sun Nov 29 17:20:59 2015 -0800

    bcache: fix a leak in bch_cached_dev_run()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Tested-by: Joshua Schmid <jschmid@suse.com>
    Tested-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Kent Overstreet <kmo@daterainc.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 383f06028a0a..43e911e4e8d0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -849,8 +849,11 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	buf[SB_LABEL_SIZE] = '\0';
 	env[2] = kasprintf(GFP_KERNEL, "CACHED_LABEL=%s", buf);
 
-	if (atomic_xchg(&dc->running, 1))
+	if (atomic_xchg(&dc->running, 1)) {
+		kfree(env[1]);
+		kfree(env[2]);
 		return;
+	}
 
 	if (!d->c &&
 	    BDEV_STATE(&dc->sb) != BDEV_STATE_NONE) {

commit fecaee6f20ee122ad75402c53d8278f9bb142ddc
Author: Zheng Liu <wenqing.lz@taobao.com>
Date:   Sun Nov 29 17:19:32 2015 -0800

    bcache: clear BCACHE_DEV_UNLINK_DONE flag when attaching a backing device
    
    This bug can be reproduced by the following script:
    
      #!/bin/bash
    
      bcache_sysfs="/sys/fs/bcache"
    
      function clear_cache()
      {
            if [ ! -e $bcache_sysfs ]; then
                    echo "no bcache sysfs"
                    exit
            fi
    
            cset_uuid=$(ls -l $bcache_sysfs|head -n 2|tail -n 1|awk '{print $9}')
            sudo sh -c "echo $cset_uuid > /sys/block/sdb/sdb1/bcache/detach"
            sleep 5
            sudo sh -c "echo $cset_uuid > /sys/block/sdb/sdb1/bcache/attach"
      }
    
      for ((i=0;i<10;i++)); do
            clear_cache
      done
    
    The warning messages look like below:
    [  275.948611] ------------[ cut here ]------------
    [  275.963840] WARNING: at fs/sysfs/dir.c:512 sysfs_add_one+0xb8/0xd0() (Tainted: P        W
    ---------------   )
    [  275.979253] Hardware name: Tecal RH2285
    [  275.994106] sysfs: cannot create duplicate filename '/devices/pci0000:00/0000:00:09.0/0000:08:00.0/host4/target4:2:1/4:2:1:0/block/sdb/sdb1/bcache/cache'
    [  276.024105] Modules linked in: bcache tcp_diag inet_diag ipmi_devintf ipmi_si ipmi_msghandler
    bonding 8021q garp stp llc ipv6 ext3 jbd loop sg iomemory_vsl(P) bnx2 microcode serio_raw i2c_i801
    i2c_core iTCO_wdt iTCO_vendor_support i7core_edac edac_core shpchp ext4 jbd2 mbcache megaraid_sas
    pata_acpi ata_generic ata_piix dm_mod [last unloaded: scsi_wait_scan]
    [  276.072643] Pid: 2765, comm: sh Tainted: P        W  ---------------    2.6.32 #1
    [  276.089315] Call Trace:
    [  276.105801]  [<ffffffff81070fe7>] ? warn_slowpath_common+0x87/0xc0
    [  276.122650]  [<ffffffff810710d6>] ? warn_slowpath_fmt+0x46/0x50
    [  276.139361]  [<ffffffff81205c08>] ? sysfs_add_one+0xb8/0xd0
    [  276.156012]  [<ffffffff8120609b>] ? sysfs_do_create_link+0x12b/0x170
    [  276.172682]  [<ffffffff81206113>] ? sysfs_create_link+0x13/0x20
    [  276.189282]  [<ffffffffa03bda21>] ? bcache_device_link+0xc1/0x110 [bcache]
    [  276.205993]  [<ffffffffa03bfa08>] ? bch_cached_dev_attach+0x478/0x4f0 [bcache]
    [  276.222794]  [<ffffffffa03c4a17>] ? bch_cached_dev_store+0x627/0x780 [bcache]
    [  276.239680]  [<ffffffff8116783a>] ? alloc_pages_current+0xaa/0x110
    [  276.256594]  [<ffffffff81203b15>] ? sysfs_write_file+0xe5/0x170
    [  276.273364]  [<ffffffff811887b8>] ? vfs_write+0xb8/0x1a0
    [  276.290133]  [<ffffffff811890b1>] ? sys_write+0x51/0x90
    [  276.306368]  [<ffffffff8100c072>] ? system_call_fastpath+0x16/0x1b
    [  276.322301] ---[ end trace 9f5d4fcdd0c3edfb ]---
    [  276.338241] ------------[ cut here ]------------
    [  276.354109] WARNING: at /home/wenqing.lz/bcache/bcache/super.c:720
    bcache_device_link+0xdf/0x110 [bcache]() (Tainted: P        W  ---------------   )
    [  276.386017] Hardware name: Tecal RH2285
    [  276.401430] Couldn't create device <-> cache set symlinks
    [  276.401759] Modules linked in: bcache tcp_diag inet_diag ipmi_devintf ipmi_si ipmi_msghandler
    bonding 8021q garp stp llc ipv6 ext3 jbd loop sg iomemory_vsl(P) bnx2 microcode serio_raw i2c_i801
    i2c_core iTCO_wdt iTCO_vendor_support i7core_edac edac_core shpchp ext4 jbd2 mbcache megaraid_sas
    pata_acpi ata_generic ata_piix dm_mod [last unloaded: scsi_wait_scan]
    [  276.465477] Pid: 2765, comm: sh Tainted: P        W  ---------------    2.6.32 #1
    [  276.482169] Call Trace:
    [  276.498610]  [<ffffffff81070fe7>] ? warn_slowpath_common+0x87/0xc0
    [  276.515405]  [<ffffffff810710d6>] ? warn_slowpath_fmt+0x46/0x50
    [  276.532059]  [<ffffffffa03bda3f>] ? bcache_device_link+0xdf/0x110 [bcache]
    [  276.548808]  [<ffffffffa03bfa08>] ? bch_cached_dev_attach+0x478/0x4f0 [bcache]
    [  276.565569]  [<ffffffffa03c4a17>] ? bch_cached_dev_store+0x627/0x780 [bcache]
    [  276.582418]  [<ffffffff8116783a>] ? alloc_pages_current+0xaa/0x110
    [  276.599341]  [<ffffffff81203b15>] ? sysfs_write_file+0xe5/0x170
    [  276.616142]  [<ffffffff811887b8>] ? vfs_write+0xb8/0x1a0
    [  276.632607]  [<ffffffff811890b1>] ? sys_write+0x51/0x90
    [  276.648671]  [<ffffffff8100c072>] ? system_call_fastpath+0x16/0x1b
    [  276.664756] ---[ end trace 9f5d4fcdd0c3edfc ]---
    
    We forget to clear BCACHE_DEV_UNLINK_DONE flag in bcache_device_attach()
    function when we attach a backing device first time.  After detaching this
    backing device, this flag will be true and sysfs_remove_link() isn't called in
    bcache_device_unlink().  Then when we attach this backing device again,
    sysfs_create_link() will return EEXIST error in bcache_device_link().
    
    So the fix is trival and we clear this flag in bcache_device_link().
    
    Signed-off-by: Zheng Liu <wenqing.lz@taobao.com>
    Tested-by: Joshua Schmid <jschmid@suse.com>
    Tested-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Kent Overstreet <kmo@daterainc.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 679a093a3bf6..383f06028a0a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -685,6 +685,8 @@ static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
 	WARN(sysfs_create_link(&d->kobj, &c->kobj, "cache") ||
 	     sysfs_create_link(&c->kobj, &d->kobj, d->name),
 	     "Couldn't create device <-> cache set symlinks");
+
+	clear_bit(BCACHE_DEV_UNLINK_DONE, &d->flags);
 }
 
 static void bcache_device_detach(struct bcache_device *d)

commit 749b61dab30736eb95b1ee23738cae90973d4fc3
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sat Nov 23 23:11:25 2013 -0800

    bcache: remove driver private bio splitting code
    
    The bcache driver has always accepted arbitrarily large bios and split
    them internally.  Now that every driver must accept arbitrarily large
    bios this code isn't nessecary anymore.
    
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    [dpark: add more description in commit message]
    Signed-off-by: Dongsu Park <dpark@posteo.net>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index be01fd3c87f1..679a093a3bf6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -59,29 +59,6 @@ struct workqueue_struct *bcache_wq;
 
 #define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
 
-static void bio_split_pool_free(struct bio_split_pool *p)
-{
-	if (p->bio_split_hook)
-		mempool_destroy(p->bio_split_hook);
-
-	if (p->bio_split)
-		bioset_free(p->bio_split);
-}
-
-static int bio_split_pool_init(struct bio_split_pool *p)
-{
-	p->bio_split = bioset_create(4, 0);
-	if (!p->bio_split)
-		return -ENOMEM;
-
-	p->bio_split_hook = mempool_create_kmalloc_pool(4,
-				sizeof(struct bio_split_hook));
-	if (!p->bio_split_hook)
-		return -ENOMEM;
-
-	return 0;
-}
-
 /* Superblock */
 
 static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
@@ -537,7 +514,7 @@ static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
 	bio->bi_private = ca;
 	bch_bio_map(bio, ca->disk_buckets);
 
-	closure_bio_submit(bio, &ca->prio, ca);
+	closure_bio_submit(bio, &ca->prio);
 	closure_sync(cl);
 }
 
@@ -757,7 +734,6 @@ static void bcache_device_free(struct bcache_device *d)
 		put_disk(d->disk);
 	}
 
-	bio_split_pool_free(&d->bio_split_hook);
 	if (d->bio_split)
 		bioset_free(d->bio_split);
 	kvfree(d->full_dirty_stripes);
@@ -804,7 +780,6 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 		return minor;
 
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    bio_split_pool_init(&d->bio_split_hook) ||
 	    !(d->disk = alloc_disk(1))) {
 		ida_simple_remove(&bcache_minor, minor);
 		return -ENOMEM;
@@ -1793,8 +1768,6 @@ void bch_cache_release(struct kobject *kobj)
 		ca->set->cache[ca->sb.nr_this_dev] = NULL;
 	}
 
-	bio_split_pool_free(&ca->bio_split_hook);
-
 	free_pages((unsigned long) ca->disk_buckets, ilog2(bucket_pages(ca)));
 	kfree(ca->prio_buckets);
 	vfree(ca->buckets);
@@ -1839,8 +1812,7 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 					  ca->sb.nbuckets)) ||
 	    !(ca->prio_buckets	= kzalloc(sizeof(uint64_t) * prio_buckets(ca) *
 					  2, GFP_KERNEL)) ||
-	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)) ||
-	    bio_split_pool_init(&ca->bio_split_hook))
+	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)))
 		return -ENOMEM;
 
 	ca->prio_last_buckets = ca->prio_buckets + prio_buckets(ca);

commit 4246a0b63bd8f56a1469b12eafeb875b1041a451
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 20 15:29:37 2015 +0200

    block: add a bi_error field to struct bio
    
    Currently we have two different ways to signal an I/O error on a BIO:
    
     (1) by clearing the BIO_UPTODATE flag
     (2) by returning a Linux errno value to the bi_end_io callback
    
    The first one has the drawback of only communicating a single possible
    error (-EIO), and the second one has the drawback of not beeing persistent
    when bios are queued up, and are not passed along from child to parent
    bio in the ever more popular chaining scenario.  Having both mechanisms
    available has the additional drawback of utterly confusing driver authors
    and introducing bugs where various I/O submitters only deal with one of
    them, and the others have to add boilerplate code to deal with both kinds
    of error returns.
    
    So add a new bi_error field to store an errno value directly in struct
    bio and remove the existing mechanisms to clean all this up.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fc8e545ced18..be01fd3c87f1 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -221,7 +221,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	return err;
 }
 
-static void write_bdev_super_endio(struct bio *bio, int error)
+static void write_bdev_super_endio(struct bio *bio)
 {
 	struct cached_dev *dc = bio->bi_private;
 	/* XXX: error checking */
@@ -290,11 +290,11 @@ void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 	closure_return_with_destructor(cl, bch_write_bdev_super_unlock);
 }
 
-static void write_super_endio(struct bio *bio, int error)
+static void write_super_endio(struct bio *bio)
 {
 	struct cache *ca = bio->bi_private;
 
-	bch_count_io_errors(ca, error, "writing superblock");
+	bch_count_io_errors(ca, bio->bi_error, "writing superblock");
 	closure_put(&ca->set->sb_write);
 }
 
@@ -339,12 +339,12 @@ void bcache_write_super(struct cache_set *c)
 
 /* UUID io */
 
-static void uuid_endio(struct bio *bio, int error)
+static void uuid_endio(struct bio *bio)
 {
 	struct closure *cl = bio->bi_private;
 	struct cache_set *c = container_of(cl, struct cache_set, uuid_write);
 
-	cache_set_err_on(error, c, "accessing uuids");
+	cache_set_err_on(bio->bi_error, c, "accessing uuids");
 	bch_bbio_free(bio, c);
 	closure_put(cl);
 }
@@ -512,11 +512,11 @@ static struct uuid_entry *uuid_find_empty(struct cache_set *c)
  * disk.
  */
 
-static void prio_endio(struct bio *bio, int error)
+static void prio_endio(struct bio *bio)
 {
 	struct cache *ca = bio->bi_private;
 
-	cache_set_err_on(error, ca->set, "accessing priorities");
+	cache_set_err_on(bio->bi_error, ca->set, "accessing priorities");
 	bch_bbio_free(bio, ca->set);
 	closure_put(&ca->prio);
 }

commit 2bb4cd5cc472b191a46938becb7dafdd44644329
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Jul 14 08:15:12 2015 -0600

    block: have drivers use blk_queue_max_discard_sectors()
    
    Some drivers use it now, others just set the limits field manually.
    But in preparation for splitting this into a hard and soft limit,
    ensure that they all call the proper function for setting the hw
    limit for discards.
    
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 94980bfca434..fc8e545ced18 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -830,7 +830,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	q->limits.max_sectors		= UINT_MAX;
 	q->limits.max_segment_size	= UINT_MAX;
 	q->limits.max_segments		= BIO_MAX_PAGES;
-	q->limits.max_discard_sectors	= UINT_MAX;
+	blk_queue_max_discard_sectors(q, UINT_MAX);
 	q->limits.discard_granularity	= 512;
 	q->limits.io_min		= block_size;
 	q->limits.logical_block_size	= block_size;

commit 958b43384e41c129117284f48ba3fb9c11ebac75
Author: Pekka Enberg <penberg@kernel.org>
Date:   Tue Jun 30 14:59:30 2015 -0700

    bcache: use kvfree() in various places
    
    Use kvfree() instead of open-coding it.
    
    Signed-off-by: Pekka Enberg <penberg@kernel.org>
    Cc: Kent Overstreet <kmo@daterainc.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4dd2bb7167f0..94980bfca434 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -760,14 +760,8 @@ static void bcache_device_free(struct bcache_device *d)
 	bio_split_pool_free(&d->bio_split_hook);
 	if (d->bio_split)
 		bioset_free(d->bio_split);
-	if (is_vmalloc_addr(d->full_dirty_stripes))
-		vfree(d->full_dirty_stripes);
-	else
-		kfree(d->full_dirty_stripes);
-	if (is_vmalloc_addr(d->stripe_sectors_dirty))
-		vfree(d->stripe_sectors_dirty);
-	else
-		kfree(d->stripe_sectors_dirty);
+	kvfree(d->full_dirty_stripes);
+	kvfree(d->stripe_sectors_dirty);
 
 	closure_debug_destroy(&d->cl);
 }

commit b277da0a8a594308e17881f4926879bd5fca2a2d
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Sat Oct 4 10:55:32 2014 -0600

    block: disable entropy contributions for nonrot devices
    
    Clear QUEUE_FLAG_ADD_RANDOM in all block drivers that set
    QUEUE_FLAG_NONROT.
    
    Historically, all block devices have automatically made entropy
    contributions.  But as previously stated in commit e2e1a148 ("block: add
    sysfs knob for turning off disk entropy contributions"):
        - On SSD disks, the completion times aren't as random as they
          are for rotational drives. So it's questionable whether they
          should contribute to the random pool in the first place.
        - Calling add_disk_randomness() has a lot of overhead.
    
    There are more reliable sources for randomness than non-rotational block
    devices.  From a security perspective it is better to err on the side of
    caution than to allow entropy contributions from unreliable "random"
    sources.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d4713d098a39..4dd2bb7167f0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -842,6 +842,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	q->limits.logical_block_size	= block_size;
 	q->limits.physical_block_size	= block_size;
 	set_bit(QUEUE_FLAG_NONROT,	&d->disk->queue->queue_flags);
+	clear_bit(QUEUE_FLAG_ADD_RANDOM, &d->disk->queue->queue_flags);
 	set_bit(QUEUE_FLAG_DISCARD,	&d->disk->queue->queue_flags);
 
 	blk_queue_flush(q, REQ_FLUSH|REQ_FUA);

commit 0781c8748cf1ea2b0dcd966571103909528c4efa
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Jul 7 13:03:36 2014 -0700

    bcache: Drop unneeded blk_sync_queue() calls
    
    this is needed for the queue/block device we created (it's done by
    blk_cleanup_queue() which we do call) - but calling it for the block devices we
    only opened is pointless.
    
    Change-Id: I53dfded14ed15b9581d10ca8399d5e1b3abbf9f2

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 60e75130a44c..d4713d098a39 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1084,12 +1084,8 @@ static void cached_dev_free(struct closure *cl)
 
 	mutex_unlock(&bch_register_lock);
 
-	if (!IS_ERR_OR_NULL(dc->bdev)) {
-		if (dc->bdev->bd_disk)
-			blk_sync_queue(bdev_get_queue(dc->bdev));
-
+	if (!IS_ERR_OR_NULL(dc->bdev))
 		blkdev_put(dc->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
-	}
 
 	wake_up(&unregister_wait);
 
@@ -1817,10 +1813,8 @@ void bch_cache_release(struct kobject *kobj)
 	if (ca->sb_bio.bi_inline_vecs[0].bv_page)
 		put_page(ca->sb_bio.bi_io_vec[0].bv_page);
 
-	if (!IS_ERR_OR_NULL(ca->bdev)) {
-		blk_sync_queue(bdev_get_queue(ca->bdev));
+	if (!IS_ERR_OR_NULL(ca->bdev))
 		blkdev_put(ca->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
-	}
 
 	kfree(ca);
 	module_put(THIS_MODULE);

commit 789d21dbd9d8889e62c79ec19585fcc97e42ef07
Author: Jianjian Huo <samuel.huo@gmail.com>
Date:   Sun Jul 13 09:08:59 2014 -0700

    bcache: add mutex lock for bch_is_open
    
    Since bch_is_open will iterate linked list bch_cache_sets and
    uncached_devices, it needs bch_register_lock.
    
    Signed-off-by: Jianjian Huo <samuel.huo@gmail.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b6114d672413..60e75130a44c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1966,10 +1966,12 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (IS_ERR(bdev)) {
 		if (bdev == ERR_PTR(-EBUSY)) {
 			bdev = lookup_bdev(strim(path));
+			mutex_lock(&bch_register_lock);
 			if (!IS_ERR(bdev) && bch_is_open(bdev))
 				err = "device already registered";
 			else
 				err = "device busy";
+			mutex_unlock(&bch_register_lock);
 		}
 		goto err;
 	}

commit 2452cc89063a2a6890368f185c4b6d7d8802179e
Author: Slava Pestov <sp@daterainc.com>
Date:   Sat Jul 12 00:22:53 2014 -0700

    bcache: try to set b->parent properly
    
    bcache_flash_dev.ktest would reliably crash with 8k and 16k bucket size
    before; now it passes.
    
    Change-Id: Ib542232235e39298c3a7548fe52b645cabb823d1

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 12ad381bdbab..b6114d672413 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1601,7 +1601,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "error reading btree root";
-		c->root = bch_btree_node_get(c, NULL, k, j->btree_level, true);
+		c->root = bch_btree_node_get(c, NULL, k, j->btree_level, true, NULL);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 
@@ -1676,7 +1676,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "cannot allocate new btree root";
-		c->root = __bch_btree_node_alloc(c, NULL, 0, true);
+		c->root = __bch_btree_node_alloc(c, NULL, 0, true, NULL);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 

commit c9a78332b42cbdcdd386a95192a716b67d1711a4
Author: Slava Pestov <sp@daterainc.com>
Date:   Thu Jun 19 15:05:59 2014 -0700

    bcache: fix memory corruption in init error path
    
    If register_cache_set() failed, we would touch ca->set after
    it had already been freed. Also, fix an assertion to catch
    this.
    
    Change-Id: I748e5f5b223e2d9b2602075dec2f997cced2394d

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 72fbaf79b0c8..12ad381bdbab 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1356,8 +1356,11 @@ static void cache_set_free(struct closure *cl)
 	bch_journal_free(c);
 
 	for_each_cache(ca, c, i)
-		if (ca)
+		if (ca) {
+			ca->set = NULL;
+			c->cache[ca->sb.nr_this_dev] = NULL;
 			kobject_put(&ca->kobj);
+		}
 
 	bch_bset_sort_state_free(&c->sort);
 	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
@@ -1794,8 +1797,10 @@ void bch_cache_release(struct kobject *kobj)
 	struct cache *ca = container_of(kobj, struct cache, kobj);
 	unsigned i;
 
-	if (ca->set)
+	if (ca->set) {
+		BUG_ON(ca->set->cache[ca->sb.nr_this_dev] != ca);
 		ca->set->cache[ca->sb.nr_this_dev] = NULL;
+	}
 
 	bio_split_pool_free(&ca->bio_split_hook);
 
@@ -1858,7 +1863,7 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 }
 
 static void register_cache(struct cache_sb *sb, struct page *sb_page,
-				  struct block_device *bdev, struct cache *ca)
+				struct block_device *bdev, struct cache *ca)
 {
 	char name[BDEVNAME_SIZE];
 	const char *err = "cannot allocate memory";

commit bf0c55c986540483c34ca640f2eef4c3314388b1
Author: Slava Pestov <sp@daterainc.com>
Date:   Fri Jul 11 12:17:41 2014 -0700

    bcache: fix crash with incomplete cache set
    
    Change-Id: I6abde52afe917633480caaf4e2518f42a816d886

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 29dd1e8ae19f..72fbaf79b0c8 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1284,6 +1284,9 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size)
 	if (test_bit(CACHE_SET_STOPPING, &c->flags))
 		return -EINTR;
 
+	if (!test_bit(CACHE_SET_RUNNING, &c->flags))
+		return -EPERM;
+
 	u = uuid_find_empty(c);
 	if (!u) {
 		pr_err("Can't create volume, no room for UUID");
@@ -1706,6 +1709,7 @@ static void run_cache_set(struct cache_set *c)
 
 	flash_devs_run(c);
 
+	set_bit(CACHE_SET_RUNNING, &c->flags);
 	return;
 err:
 	closure_sync(&cl);

commit d83353b319d47ef8cce82467da6a25c2d558253f
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jun 11 19:44:49 2014 -0700

    bcache: Fix more early shutdown bugs
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 00cc42550f34..29dd1e8ae19f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -733,8 +733,6 @@ static void bcache_device_detach(struct bcache_device *d)
 static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
 				 unsigned id)
 {
-	BUG_ON(test_bit(CACHE_SET_STOPPING, &c->flags));
-
 	d->id = id;
 	d->c = c;
 	c->devices[id] = d;
@@ -1771,6 +1769,7 @@ static const char *register_cache_set(struct cache *ca)
 		pr_debug("set version = %llu", c->sb.version);
 	}
 
+	kobject_get(&ca->kobj);
 	ca->set = c;
 	ca->set->cache[ca->sb.nr_this_dev] = ca;
 	c->cache_by_alloc[c->caches_loaded++] = ca;
@@ -1888,10 +1887,12 @@ static void register_cache(struct cache_sb *sb, struct page *sb_page,
 		goto err;
 
 	pr_info("registered cache device %s", bdevname(bdev, name));
+out:
+	kobject_put(&ca->kobj);
 	return;
 err:
 	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
-	kobject_put(&ca->kobj);
+	goto out;
 }
 
 /* Global interfaces/init */

commit 9e5c353510b26500bd6b8309823ac9ef2837b761
Author: Slava Pestov <sp@daterainc.com>
Date:   Thu May 1 13:48:57 2014 -0700

    bcache: fix uninterruptible sleep in writeback thread
    
    There were two issues here:
    
    - writeback thread did not start until the device first became dirty
    - writeback thread used uninterruptible sleep once running
    
    Without this patch I see kernel warnings printed and a load average of
    1.52 after booting my test VM. With this patch the warnings are gone and
    the load average is near 0.00 as expected.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3b043a04d9bd..00cc42550f34 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1042,6 +1042,9 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	 */
 	atomic_set(&dc->count, 1);
 
+	if (bch_cached_dev_writeback_start(dc))
+		return -ENOMEM;
+
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
 		bch_sectors_dirty_init(dc);
 		atomic_set(&dc->has_dirty, 1);

commit c5aa4a3157b55bdca18dd2a9d9f43314470b6d32
Author: Slava Pestov <sp@daterainc.com>
Date:   Mon Apr 21 18:23:12 2014 -0700

    bcache: wait for buckets when allocating new btree root
    
    Tested:
    - sometimes bcache_tier test would hang on startup with a failure
      to allocate the btree root -- no longer seeing this
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6ceaec35b871..3b043a04d9bd 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1669,7 +1669,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "cannot allocate new btree root";
-		c->root = bch_btree_node_alloc(c, NULL, 0);
+		c->root = __bch_btree_node_alloc(c, NULL, 0, true);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 

commit a664d0f05a2ec02c8f042db536d84d15d6e19e81
Author: Slava Pestov <sp@daterainc.com>
Date:   Tue May 20 12:20:28 2014 -0700

    bcache: fix crash on shutdown in passthrough mode
    
    We never started the writeback thread in this case, so don't stop it.

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 09573c2bcb5d..6ceaec35b871 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1071,7 +1071,8 @@ static void cached_dev_free(struct closure *cl)
 	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
 
 	cancel_delayed_work_sync(&dc->writeback_rate_update);
-	kthread_stop(dc->writeback_thread);
+	if (!IS_ERR_OR_NULL(dc->writeback_thread))
+		kthread_stop(dc->writeback_thread);
 
 	mutex_lock(&bch_register_lock);
 

commit e5112201c1285841f8b565ece5d6ae7e0d7947a2
Author: Slava Pestov <sp@daterainc.com>
Date:   Tue Apr 29 15:39:27 2014 -0700

    bcache: fix lockdep warnings on shutdown

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1ea9fa27ee3c..09573c2bcb5d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1214,7 +1214,9 @@ void bch_flash_dev_release(struct kobject *kobj)
 static void flash_dev_free(struct closure *cl)
 {
 	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
+	mutex_lock(&bch_register_lock);
 	bcache_device_free(d);
+	mutex_unlock(&bch_register_lock);
 	kobject_put(&d->kobj);
 }
 
@@ -1222,7 +1224,9 @@ static void flash_dev_flush(struct closure *cl)
 {
 	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
 
+	mutex_lock(&bch_register_lock);
 	bcache_device_unlink(d);
+	mutex_unlock(&bch_register_lock);
 	kobject_del(&d->kobj);
 	continue_at(cl, flash_dev_free, system_wq);
 }

commit 5b1016e62f74c53e0330403025954c8d95384c03
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Mar 19 17:49:37 2014 -0700

    bcache: Fix a bug when detaching
    
    After detaching a backing device from a cache set, a bit wasn't getting
    reset meaning the second detach wouldn't work correctly.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 926ded8ccbf5..1ea9fa27ee3c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -927,6 +927,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	list_move(&dc->list, &uncached_devices);
 
 	clear_bit(BCACHE_DEV_DETACHING, &dc->disk.flags);
+	clear_bit(BCACHE_DEV_UNLINK_DONE, &dc->disk.flags);
 
 	mutex_unlock(&bch_register_lock);
 
@@ -1405,9 +1406,11 @@ static void cache_set_flush(struct closure *cl)
 		if (ca->alloc_thread)
 			kthread_stop(ca->alloc_thread);
 
-	cancel_delayed_work_sync(&c->journal.work);
-	/* flush last journal entry if needed */
-	c->journal.work.work.func(&c->journal.work.work);
+	if (c->journal.cur) {
+		cancel_delayed_work_sync(&c->journal.work);
+		/* flush last journal entry if needed */
+		c->journal.work.work.func(&c->journal.work.work);
+	}
 
 	closure_return(cl);
 }

commit 3a2fd9d5090b83aab85378a846fa10f39b0b5aa7
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Feb 27 17:51:12 2014 -0800

    bcache: Kill bucket->gc_gen
    
    gc_gen was a temporary used to recalculate last_gc, but since we only need
    bucket->last_gc when gc isn't running (gc_mark_valid = 1), we can just update
    last_gc directly.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a8c57d59a726..926ded8ccbf5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -635,7 +635,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 		}
 
 		b->prio = le16_to_cpu(d->prio);
-		b->gen = b->last_gc = b->gc_gen = d->gen;
+		b->gen = b->last_gc = d->gen;
 	}
 }
 

commit 2531d9ee61fa08a5a9ab8f002c50779888d232c7
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Mar 17 16:55:55 2014 -0700

    bcache: Kill unused freelist
    
    This was originally added as at optimization that for various reasons isn't
    needed anymore, but it does add a lot of nasty corner cases (and it was
    responsible for some recently fixed bugs). Just get rid of it now.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 2d4a56219ec7..a8c57d59a726 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -541,9 +541,6 @@ static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
 	closure_sync(cl);
 }
 
-#define buckets_free(c)	"free %zu, free_inc %zu, unused %zu",		\
-	fifo_used(&c->free), fifo_used(&c->free_inc), fifo_used(&c->unused)
-
 void bch_prio_write(struct cache *ca)
 {
 	int i;
@@ -554,10 +551,6 @@ void bch_prio_write(struct cache *ca)
 
 	lockdep_assert_held(&ca->set->bucket_lock);
 
-	for (b = ca->buckets;
-	     b < ca->buckets + ca->sb.nbuckets; b++)
-		b->disk_gen = b->gen;
-
 	ca->disk_buckets->seq++;
 
 	atomic_long_add(ca->sb.bucket_size * prio_buckets(ca),
@@ -601,14 +594,17 @@ void bch_prio_write(struct cache *ca)
 
 	mutex_lock(&ca->set->bucket_lock);
 
-	ca->need_save_prio = 0;
-
 	/*
 	 * Don't want the old priorities to get garbage collected until after we
 	 * finish writing the new ones, and they're journalled
 	 */
-	for (i = 0; i < prio_buckets(ca); i++)
+	for (i = 0; i < prio_buckets(ca); i++) {
+		if (ca->prio_last_buckets[i])
+			__bch_bucket_free(ca,
+				&ca->buckets[ca->prio_last_buckets[i]]);
+
 		ca->prio_last_buckets[i] = ca->prio_buckets[i];
+	}
 }
 
 static void prio_read(struct cache *ca, uint64_t bucket)
@@ -639,7 +635,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 		}
 
 		b->prio = le16_to_cpu(d->prio);
-		b->gen = b->disk_gen = b->last_gc = b->gc_gen = d->gen;
+		b->gen = b->last_gc = b->gc_gen = d->gen;
 	}
 }
 
@@ -1606,7 +1602,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		bch_journal_mark(c, &journal);
-		bch_btree_gc_finish(c);
+		bch_initial_gc_finish(c);
 		pr_debug("btree_check() done");
 
 		/*
@@ -1648,7 +1644,7 @@ static void run_cache_set(struct cache_set *c)
 				ca->sb.d[j] = ca->sb.first_bucket + j;
 		}
 
-		bch_btree_gc_finish(c);
+		bch_initial_gc_finish(c);
 
 		err = "error starting allocator thread";
 		for_each_cache(ca, c, i)
@@ -1794,7 +1790,6 @@ void bch_cache_release(struct kobject *kobj)
 	vfree(ca->buckets);
 
 	free_heap(&ca->heap);
-	free_fifo(&ca->unused);
 	free_fifo(&ca->free_inc);
 
 	for (i = 0; i < RESERVE_NR; i++)
@@ -1831,7 +1826,6 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	    !init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
-	    !init_fifo(&ca->unused,	free << 2, GFP_KERNEL) ||
 	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
 	    !(ca->buckets	= vzalloc(sizeof(struct bucket) *
 					  ca->sb.nbuckets)) ||

commit 0a63b66db566cffdf90182eb6e66fdd4d0479e63
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Mar 17 17:15:53 2014 -0700

    bcache: Rework btree cache reserve handling
    
    This changes the bucket allocation reserves to use _real_ reserves - separate
    freelists - instead of watermarks, which if nothing else makes the current code
    saner to reason about and is going to be important in the future when we add
    support for multiple btrees.
    
    It also adds btree_check_reserve(), which checks (and locks) the reserves for
    both bucket allocation and memory allocation for btree nodes; the old code just
    kinda sorta assumed that since (e.g. for btree node splits) it had the root
    locked and that meant no other threads could try to make use of the same
    reserve; this technically should have been ok for memory allocation (we should
    always have a reserve for memory allocation (the btree node cache is used as a
    reserve and we preallocate it)), but multiple btrees will mean that locking the
    root won't be sufficient anymore, and for the bucket allocation reserve it was
    technically possible for the old code to deadlock.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 307fe378ea43..2d4a56219ec7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1495,14 +1495,13 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 
 	sema_init(&c->sb_write_mutex, 1);
 	mutex_init(&c->bucket_lock);
-	init_waitqueue_head(&c->try_wait);
+	init_waitqueue_head(&c->btree_cache_wait);
 	init_waitqueue_head(&c->bucket_wait);
 	sema_init(&c->uuid_write_mutex, 1);
 
 	spin_lock_init(&c->btree_gc_time.lock);
 	spin_lock_init(&c->btree_split_time.lock);
 	spin_lock_init(&c->btree_read_time.lock);
-	spin_lock_init(&c->try_harder_time.lock);
 
 	bch_moving_init_cache_set(c);
 
@@ -1591,7 +1590,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "error reading btree root";
-		c->root = bch_btree_node_get(c, k, j->btree_level, true);
+		c->root = bch_btree_node_get(c, NULL, k, j->btree_level, true);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 
@@ -1666,7 +1665,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "cannot allocate new btree root";
-		c->root = bch_btree_node_alloc(c, 0, true);
+		c->root = bch_btree_node_alloc(c, NULL, 0);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 
@@ -1847,13 +1846,7 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	for_each_bucket(b, ca)
 		atomic_set(&b->pin, 0);
 
-	if (bch_cache_allocator_init(ca))
-		goto err;
-
 	return 0;
-err:
-	kobject_put(&ca->kobj);
-	return -ENOMEM;
 }
 
 static void register_cache(struct cache_sb *sb, struct page *sb_page,

commit 56b30770b27d54d68ad51eccc6d888282b568cee
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jan 23 01:44:55 2014 -0800

    bcache: Kill btree_io_wq
    
    With the locking rework in the last patch, this shouldn't be needed anymore -
    btree_node_write_work() only takes b->write_lock which is never held for very
    long.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 9ded06434e11..307fe378ea43 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2072,7 +2072,6 @@ static void bcache_exit(void)
 {
 	bch_debug_exit();
 	bch_request_exit();
-	bch_btree_exit();
 	if (bcache_kobj)
 		kobject_put(bcache_kobj);
 	if (bcache_wq)
@@ -2102,7 +2101,6 @@ static int __init bcache_init(void)
 	if (!(bcache_wq = create_workqueue("bcache")) ||
 	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
 	    sysfs_create_files(bcache_kobj, files) ||
-	    bch_btree_init() ||
 	    bch_request_init() ||
 	    bch_debug_init(bcache_kobj))
 		goto err;

commit 2a285686c109816ba71a00b9278262cf02648258
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Mar 4 16:42:42 2014 -0800

    bcache: btree locking rework
    
    Add a new lock, b->write_lock, which is required to actually modify - or write -
    a btree node; this lock is only held for short durations.
    
    This means we can write out a btree node without taking b->lock, which _is_ held
    for long durations - solving a deadlock when btree_flush_write() (from the
    journalling code) is called with a btree node locked.
    
    Right now just occurs in bch_btree_set_root(), but with an upcoming journalling
    rework is going to happen a lot more.
    
    This also turns b->lock is now more of a read/intent lock instead of a
    read/write lock - but not completely, since it still blocks readers. May turn it
    into a real intent lock at some point in the future.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index ddfde380b49f..9ded06434e11 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1398,9 +1398,12 @@ static void cache_set_flush(struct closure *cl)
 		list_add(&c->root->list, &c->btree_cache);
 
 	/* Should skip this if we're unregistering because of an error */
-	list_for_each_entry(b, &c->btree_cache, list)
+	list_for_each_entry(b, &c->btree_cache, list) {
+		mutex_lock(&b->write_lock);
 		if (btree_node_dirty(b))
-			bch_btree_node_write(b, NULL);
+			__bch_btree_node_write(b, NULL);
+		mutex_unlock(&b->write_lock);
+	}
 
 	for_each_cache(ca, c, i)
 		if (ca->alloc_thread)
@@ -1667,8 +1670,10 @@ static void run_cache_set(struct cache_set *c)
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 
+		mutex_lock(&c->root->write_lock);
 		bkey_copy_key(&c->root->key, &MAX_KEY);
 		bch_btree_node_write(c->root, &cl);
+		mutex_unlock(&c->root->write_lock);
 
 		bch_btree_set_root(c->root);
 		rw_unlock(true, c->root);

commit da415a096fc06e49d1a15f7a06bcfe6ad44c5d38
Author: Nicholas Swenson <nks@daterainc.com>
Date:   Thu Jan 9 16:03:04 2014 -0800

    bcache: Fix moving_gc deadlocking with a foreground write
    
    Deadlock happened because a foreground write slept, waiting for a bucket
    to be allocated. Normally the gc would mark buckets available for invalidation.
    But the moving_gc was stuck waiting for outstanding writes to complete.
    These writes used the bcache_wq, the same queue foreground writes used.
    
    This fix gives moving_gc its own work queue, so it was still finish moving
    even if foreground writes are stuck waiting for allocation. It also makes
    work queue a parameter to the data_insert path, so moving_gc can use its
    workqueue for writes.
    
    Signed-off-by: Nicholas Swenson <nks@daterainc.com>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fb343276beef..ddfde380b49f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1356,6 +1356,8 @@ static void cache_set_free(struct closure *cl)
 	bch_bset_sort_state_free(&c->sort);
 	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
 
+	if (c->moving_gc_wq)
+		destroy_workqueue(c->moving_gc_wq);
 	if (c->bio_split)
 		bioset_free(c->bio_split);
 	if (c->fill_iter)
@@ -1522,6 +1524,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
 	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
+	    !(c->moving_gc_wq = create_workqueue("bcache_gc")) ||
 	    bch_journal_alloc(c) ||
 	    bch_btree_cache_alloc(c) ||
 	    bch_open_buckets_alloc(c) ||

commit 90db6919f5f1614d1b7a92052445506bc6c564d2
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Feb 10 17:26:40 2014 -0800

    bcache: Fix discard granularity
    
    blk_stack_limits() doesn't like a discard granularity of 0.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5136e11eadb0..fb343276beef 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -843,6 +843,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	q->limits.max_segment_size	= UINT_MAX;
 	q->limits.max_segments		= BIO_MAX_PAGES;
 	q->limits.max_discard_sectors	= UINT_MAX;
+	q->limits.discard_granularity	= 512;
 	q->limits.io_min		= block_size;
 	q->limits.logical_block_size	= block_size;
 	q->limits.physical_block_size	= block_size;

commit 4fa03402cda2fac1a54248c7578b939d95931dc0
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Mar 17 18:58:55 2014 -0700

    bcache: Fix a lockdep splat in an error path
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c70521fe57a6..5136e11eadb0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1873,7 +1873,10 @@ static void register_cache(struct cache_sb *sb, struct page *sb_page,
 	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache"))
 		goto err;
 
+	mutex_lock(&bch_register_lock);
 	err = register_cache_set(ca);
+	mutex_unlock(&bch_register_lock);
+
 	if (err)
 		goto err;
 
@@ -1935,8 +1938,6 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (!try_module_get(THIS_MODULE))
 		return -EBUSY;
 
-	mutex_lock(&bch_register_lock);
-
 	if (!(path = kstrndup(buffer, size, GFP_KERNEL)) ||
 	    !(sb = kmalloc(sizeof(struct cache_sb), GFP_KERNEL)))
 		goto err;
@@ -1969,7 +1970,9 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		if (!dc)
 			goto err_close;
 
+		mutex_lock(&bch_register_lock);
 		register_bdev(sb, sb_page, bdev, dc);
+		mutex_unlock(&bch_register_lock);
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
 		if (!ca)
@@ -1982,7 +1985,6 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 		put_page(sb_page);
 	kfree(sb);
 	kfree(path);
-	mutex_unlock(&bch_register_lock);
 	module_put(THIS_MODULE);
 	return ret;
 

commit dabb44334060b4b84051b34c58573e57cc7432b2
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Feb 19 19:48:26 2014 -0800

    bcache: Fix a shutdown bug
    
    Shutdown wasn't cancelling/waiting on journal_write_work()
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 24a3a1546caa..c70521fe57a6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1403,6 +1403,10 @@ static void cache_set_flush(struct closure *cl)
 		if (ca->alloc_thread)
 			kthread_stop(ca->alloc_thread);
 
+	cancel_delayed_work_sync(&c->journal.work);
+	/* flush last journal entry if needed */
+	c->journal.work.work.func(&c->journal.work.work);
+
 	closure_return(cl);
 }
 

commit dc9d98d621bdce0552997200ce855659875a5c9f
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 23:47:33 2013 -0800

    bcache: Convert debug code to btree_keys
    
    More work to disentangle various code from struct btree
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6d6a7a15043e..24a3a1546caa 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -384,7 +384,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 			break;
 	}
 
-	bch_bkey_to_text(buf, sizeof(buf), k);
+	bch_extent_to_text(buf, sizeof(buf), k);
 	pr_debug("%s UUIDs at %s", rw & REQ_WRITE ? "wrote" : "read", buf);
 
 	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)

commit 65d45231b56efb3db51eb441e2c68f8252ecdd12
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Dec 20 17:22:05 2013 -0800

    bcache: Abstract out stuff needed for sorting
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 12057a43e01f..6d6a7a15043e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -9,6 +9,7 @@
 #include "bcache.h"
 #include "btree.h"
 #include "debug.h"
+#include "extents.h"
 #include "request.h"
 #include "writeback.h"
 
@@ -399,7 +400,7 @@ static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
 {
 	struct bkey *k = &j->uuid_bucket;
 
-	if (bch_btree_ptr_invalid(c, k))
+	if (__bch_btree_ptr_invalid(c, k))
 		return "bad uuid pointer";
 
 	bkey_copy(&c->uuid_bucket, k);
@@ -1575,7 +1576,7 @@ static void run_cache_set(struct cache_set *c)
 		k = &j->btree_root;
 
 		err = "bad btree root";
-		if (bch_btree_ptr_invalid(c, k))
+		if (__bch_btree_ptr_invalid(c, k))
 			goto err;
 
 		err = "error reading btree root";

commit ee811287c9f241641899788cbfc9d70ed96ba3a5
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 23:49:49 2013 -0800

    bcache: Rename/shuffle various code around
    
    More work to disentangle bset.c from the rest of the code:
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1fc8165a5c01..12057a43e01f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1477,7 +1477,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	c->block_bits		= ilog2(sb->block_size);
 	c->nr_uuids		= bucket_bytes(c) / sizeof(struct uuid_entry);
 
-	c->btree_pages		= c->sb.bucket_size / PAGE_SECTORS;
+	c->btree_pages		= bucket_pages(c);
 	if (c->btree_pages > BTREE_MAX_PAGES)
 		c->btree_pages = max_t(int, c->btree_pages / 4,
 				       BTREE_MAX_PAGES);

commit 67539e85289c14a76a1c4162613d14a5f05a0027
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 22:53:34 2013 -0700

    bcache: Add struct bset_sort_state
    
    More disentangling bset.c from the rest of the bcache code - soon, the
    sorting routines won't have any dependencies on any outside structs.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d05e75627714..1fc8165a5c01 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1351,6 +1351,7 @@ static void cache_set_free(struct closure *cl)
 		if (ca)
 			kobject_put(&ca->kobj);
 
+	bch_bset_sort_state_free(&c->sort);
 	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
 
 	if (c->bio_split)
@@ -1481,15 +1482,12 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 		c->btree_pages = max_t(int, c->btree_pages / 4,
 				       BTREE_MAX_PAGES);
 
-	c->sort_crit_factor = int_sqrt(c->btree_pages);
-
 	sema_init(&c->sb_write_mutex, 1);
 	mutex_init(&c->bucket_lock);
 	init_waitqueue_head(&c->try_wait);
 	init_waitqueue_head(&c->bucket_wait);
 	sema_init(&c->uuid_write_mutex, 1);
 
-	spin_lock_init(&c->sort_time.lock);
 	spin_lock_init(&c->btree_gc_time.lock);
 	spin_lock_init(&c->btree_split_time.lock);
 	spin_lock_init(&c->btree_read_time.lock);
@@ -1517,12 +1515,11 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 				bucket_pages(c))) ||
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
 	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    !(c->sort_pool = mempool_create_page_pool(1,
-				ilog2(bucket_pages(c)))) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    bch_journal_alloc(c) ||
 	    bch_btree_cache_alloc(c) ||
-	    bch_open_buckets_alloc(c))
+	    bch_open_buckets_alloc(c) ||
+	    bch_bset_sort_state_init(&c->sort, ilog2(c->btree_pages)))
 		goto err;
 
 	c->congested_read_threshold_us	= 2000;

commit 0a45114534766058193eb2605c136562a4f7bcc8
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Dec 18 00:01:06 2013 -0800

    bcache: Use a mempool for mergesort temporary space
    
    It was a single element mempool before, it's slightly cleaner to just use a real
    mempool.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 677a604e7f3f..d05e75627714 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1352,7 +1352,6 @@ static void cache_set_free(struct closure *cl)
 			kobject_put(&ca->kobj);
 
 	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
-	free_pages((unsigned long) c->sort, ilog2(bucket_pages(c)));
 
 	if (c->bio_split)
 		bioset_free(c->bio_split);
@@ -1489,7 +1488,6 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	init_waitqueue_head(&c->try_wait);
 	init_waitqueue_head(&c->bucket_wait);
 	sema_init(&c->uuid_write_mutex, 1);
-	mutex_init(&c->sort_lock);
 
 	spin_lock_init(&c->sort_time.lock);
 	spin_lock_init(&c->btree_gc_time.lock);
@@ -1519,7 +1517,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 				bucket_pages(c))) ||
 	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
 	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    !(c->sort = alloc_bucket_pages(GFP_KERNEL, c)) ||
+	    !(c->sort_pool = mempool_create_page_pool(1,
+				ilog2(bucket_pages(c)))) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    bch_journal_alloc(c) ||
 	    bch_btree_cache_alloc(c) ||

commit 5c41c8a713da7874bd79196696a275beeb11821e
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Jul 8 17:53:26 2013 -0700

    bcache: Trivial error handling fix
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e363efcf2b76..677a604e7f3f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -2065,7 +2065,8 @@ static void bcache_exit(void)
 		kobject_put(bcache_kobj);
 	if (bcache_wq)
 		destroy_workqueue(bcache_wq);
-	unregister_blkdev(bcache_major, "bcache");
+	if (bcache_major)
+		unregister_blkdev(bcache_major, "bcache");
 	unregister_reboot_notifier(&reboot);
 }
 

commit c78afc6261b09f74abff8c0719b80692a4959768
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jul 11 22:39:53 2013 -0700

    bcache/md: Use raid stripe size
    
    Now that we've got code for raid5/6 stripe awareness, bcache just needs
    to know about the stripes and when writing partial stripes is expensive
    - we probably don't want to enable this optimization for raid1 or 10,
    even though they have stripes. So add a flag to queue_limits.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 63ebef78df4a..e363efcf2b76 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1134,6 +1134,12 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 		hlist_add_head(&io->hash, dc->io_hash + RECENT_IO);
 	}
 
+	dc->disk.stripe_size = q->limits.io_opt >> 9;
+
+	if (dc->disk.stripe_size)
+		dc->partial_stripes_expensive =
+			q->limits.raid_partial_stripes_expensive;
+
 	ret = bcache_device_init(&dc->disk, block_size,
 			 dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
 	if (ret)

commit 78365411b344df35a198b119133e6515c2dcfb9f
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 01:29:34 2013 -0800

    bcache: Rework allocator reserves
    
    We need a reserve for allocating buckets for new btree nodes - and now that
    we've got multiple btrees, it really needs to be per btree.
    
    This reworks the reserves so we've got separate freelists for each reserve
    instead of watermarks, which seems to make things a bit cleaner, and it adds
    some code so that btree_split() can make sure the reserve is available before it
    starts.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b057676fc67d..63ebef78df4a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -444,7 +444,7 @@ static int __uuid_write(struct cache_set *c)
 
 	lockdep_assert_held(&bch_register_lock);
 
-	if (bch_bucket_alloc_set(c, WATERMARK_METADATA, &k.key, 1, true))
+	if (bch_bucket_alloc_set(c, RESERVE_BTREE, &k.key, 1, true))
 		return 1;
 
 	SET_KEY_SIZE(&k.key, c->sb.bucket_size);
@@ -562,8 +562,8 @@ void bch_prio_write(struct cache *ca)
 	atomic_long_add(ca->sb.bucket_size * prio_buckets(ca),
 			&ca->meta_sectors_written);
 
-	pr_debug("free %zu, free_inc %zu, unused %zu", fifo_used(&ca->free),
-		 fifo_used(&ca->free_inc), fifo_used(&ca->unused));
+	//pr_debug("free %zu, free_inc %zu, unused %zu", fifo_used(&ca->free),
+	//	 fifo_used(&ca->free_inc), fifo_used(&ca->unused));
 
 	for (i = prio_buckets(ca) - 1; i >= 0; --i) {
 		long bucket;
@@ -582,7 +582,7 @@ void bch_prio_write(struct cache *ca)
 		p->magic	= pset_magic(&ca->sb);
 		p->csum		= bch_crc64(&p->magic, bucket_bytes(ca) - 8);
 
-		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, true);
+		bucket = bch_bucket_alloc(ca, RESERVE_PRIO, true);
 		BUG_ON(bucket == -1);
 
 		mutex_unlock(&ca->set->bucket_lock);
@@ -1767,6 +1767,7 @@ static const char *register_cache_set(struct cache *ca)
 void bch_cache_release(struct kobject *kobj)
 {
 	struct cache *ca = container_of(kobj, struct cache, kobj);
+	unsigned i;
 
 	if (ca->set)
 		ca->set->cache[ca->sb.nr_this_dev] = NULL;
@@ -1780,7 +1781,9 @@ void bch_cache_release(struct kobject *kobj)
 	free_heap(&ca->heap);
 	free_fifo(&ca->unused);
 	free_fifo(&ca->free_inc);
-	free_fifo(&ca->free);
+
+	for (i = 0; i < RESERVE_NR; i++)
+		free_fifo(&ca->free[i]);
 
 	if (ca->sb_bio.bi_inline_vecs[0].bv_page)
 		put_page(ca->sb_bio.bi_io_vec[0].bv_page);
@@ -1806,10 +1809,12 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	ca->journal.bio.bi_max_vecs = 8;
 	ca->journal.bio.bi_io_vec = ca->journal.bio.bi_inline_vecs;
 
-	free = roundup_pow_of_two(ca->sb.nbuckets) >> 9;
-	free = max_t(size_t, free, (prio_buckets(ca) + 8) * 2);
+	free = roundup_pow_of_two(ca->sb.nbuckets) >> 10;
 
-	if (!init_fifo(&ca->free,	free, GFP_KERNEL) ||
+	if (!init_fifo(&ca->free[RESERVE_BTREE], 8, GFP_KERNEL) ||
+	    !init_fifo(&ca->free[RESERVE_PRIO], prio_buckets(ca), GFP_KERNEL) ||
+	    !init_fifo(&ca->free[RESERVE_MOVINGGC], free, GFP_KERNEL) ||
+	    !init_fifo(&ca->free[RESERVE_NONE], free, GFP_KERNEL) ||
 	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
 	    !init_fifo(&ca->unused,	free << 2, GFP_KERNEL) ||
 	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||

commit cb7a583e6a6ace661a5890803e115d2292a293df
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Dec 16 15:27:25 2013 -0800

    bcache: kill closure locking usage
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 93d593f957f6..b057676fc67d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -225,7 +225,7 @@ static void write_bdev_super_endio(struct bio *bio, int error)
 	struct cached_dev *dc = bio->bi_private;
 	/* XXX: error checking */
 
-	closure_put(&dc->sb_write.cl);
+	closure_put(&dc->sb_write);
 }
 
 static void __write_super(struct cache_sb *sb, struct bio *bio)
@@ -263,12 +263,20 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	submit_bio(REQ_WRITE, bio);
 }
 
+static void bch_write_bdev_super_unlock(struct closure *cl)
+{
+	struct cached_dev *dc = container_of(cl, struct cached_dev, sb_write);
+
+	up(&dc->sb_write_mutex);
+}
+
 void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 {
-	struct closure *cl = &dc->sb_write.cl;
+	struct closure *cl = &dc->sb_write;
 	struct bio *bio = &dc->sb_bio;
 
-	closure_lock(&dc->sb_write, parent);
+	down(&dc->sb_write_mutex);
+	closure_init(cl, parent);
 
 	bio_reset(bio);
 	bio->bi_bdev	= dc->bdev;
@@ -278,7 +286,7 @@ void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
 	closure_get(cl);
 	__write_super(&dc->sb, bio);
 
-	closure_return(cl);
+	closure_return_with_destructor(cl, bch_write_bdev_super_unlock);
 }
 
 static void write_super_endio(struct bio *bio, int error)
@@ -286,16 +294,24 @@ static void write_super_endio(struct bio *bio, int error)
 	struct cache *ca = bio->bi_private;
 
 	bch_count_io_errors(ca, error, "writing superblock");
-	closure_put(&ca->set->sb_write.cl);
+	closure_put(&ca->set->sb_write);
+}
+
+static void bcache_write_super_unlock(struct closure *cl)
+{
+	struct cache_set *c = container_of(cl, struct cache_set, sb_write);
+
+	up(&c->sb_write_mutex);
 }
 
 void bcache_write_super(struct cache_set *c)
 {
-	struct closure *cl = &c->sb_write.cl;
+	struct closure *cl = &c->sb_write;
 	struct cache *ca;
 	unsigned i;
 
-	closure_lock(&c->sb_write, &c->cl);
+	down(&c->sb_write_mutex);
+	closure_init(cl, &c->cl);
 
 	c->sb.seq++;
 
@@ -317,7 +333,7 @@ void bcache_write_super(struct cache_set *c)
 		__write_super(&ca->sb, bio);
 	}
 
-	closure_return(cl);
+	closure_return_with_destructor(cl, bcache_write_super_unlock);
 }
 
 /* UUID io */
@@ -325,23 +341,31 @@ void bcache_write_super(struct cache_set *c)
 static void uuid_endio(struct bio *bio, int error)
 {
 	struct closure *cl = bio->bi_private;
-	struct cache_set *c = container_of(cl, struct cache_set, uuid_write.cl);
+	struct cache_set *c = container_of(cl, struct cache_set, uuid_write);
 
 	cache_set_err_on(error, c, "accessing uuids");
 	bch_bbio_free(bio, c);
 	closure_put(cl);
 }
 
+static void uuid_io_unlock(struct closure *cl)
+{
+	struct cache_set *c = container_of(cl, struct cache_set, uuid_write);
+
+	up(&c->uuid_write_mutex);
+}
+
 static void uuid_io(struct cache_set *c, unsigned long rw,
 		    struct bkey *k, struct closure *parent)
 {
-	struct closure *cl = &c->uuid_write.cl;
+	struct closure *cl = &c->uuid_write;
 	struct uuid_entry *u;
 	unsigned i;
 	char buf[80];
 
 	BUG_ON(!parent);
-	closure_lock(&c->uuid_write, parent);
+	down(&c->uuid_write_mutex);
+	closure_init(cl, parent);
 
 	for (i = 0; i < KEY_PTRS(k); i++) {
 		struct bio *bio = bch_bbio_alloc(c);
@@ -368,7 +392,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 				 u - c->uuids, u->uuid, u->label,
 				 u->first_reg, u->last_reg, u->invalidated);
 
-	closure_return(cl);
+	closure_return_with_destructor(cl, uuid_io_unlock);
 }
 
 static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
@@ -1098,7 +1122,7 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 	set_closure_fn(&dc->disk.cl, cached_dev_flush, system_wq);
 	kobject_init(&dc->disk.kobj, &bch_cached_dev_ktype);
 	INIT_WORK(&dc->detach, cached_dev_detach_finish);
-	closure_init_unlocked(&dc->sb_write);
+	sema_init(&dc->sb_write_mutex, 1);
 	INIT_LIST_HEAD(&dc->io_lru);
 	spin_lock_init(&dc->io_lock);
 	bch_cache_accounting_init(&dc->accounting, &dc->disk.cl);
@@ -1454,11 +1478,11 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 
 	c->sort_crit_factor = int_sqrt(c->btree_pages);
 
-	closure_init_unlocked(&c->sb_write);
+	sema_init(&c->sb_write_mutex, 1);
 	mutex_init(&c->bucket_lock);
 	init_waitqueue_head(&c->try_wait);
 	init_waitqueue_head(&c->bucket_wait);
-	closure_init_unlocked(&c->uuid_write);
+	sema_init(&c->uuid_write_mutex, 1);
 	mutex_init(&c->sort_lock);
 
 	spin_lock_init(&c->sort_time.lock);

commit b28bc9b38c52f63f43e3fd875af982f2240a2859
Merge: 8d30726912cb 802eee95bde7
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 31 09:51:02 2013 -0700

    Merge tag 'v3.13-rc6' into for-3.14/core
    
    Needed to bring blk-mq uptodate, since changes have been going in
    since for-3.14/core was established.
    
    Fixup merge issues related to the immutable biovec changes.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
            block/blk-flush.c
            fs/btrfs/check-integrity.c
            fs/btrfs/extent_io.c
            fs/btrfs/scrub.c
            fs/logfs/dev_bdev.c

commit 9eb8ebeb2471b8cbaa078066aeb85fe5d46f5304
Author: Nicholas Swenson <nks@daterainc.com>
Date:   Tue Oct 22 13:19:23 2013 -0700

    bcache: Fix for can_attach_cache()
    
    Signed-off-by: Nicholas Swenson <nks@daterainc.com>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index dec15cd2d797..c57bfa071a57 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1676,7 +1676,7 @@ static void run_cache_set(struct cache_set *c)
 static bool can_attach_cache(struct cache *ca, struct cache_set *c)
 {
 	return ca->sb.block_size	== c->sb.block_size &&
-		ca->sb.bucket_size	== c->sb.block_size &&
+		ca->sb.bucket_size	== c->sb.bucket_size &&
 		ca->sb.nr_in_set	== c->sb.nr_in_set;
 }
 

commit 4f024f3797c43cb4b73cd2c50cec728842d0e49e
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Oct 11 15:44:27 2013 -0700

    block: Abstract out bvec iterator
    
    Immutable biovecs are going to require an explicit iterator. To
    implement immutable bvecs, a later patch is going to add a bi_bvec_done
    member to this struct; for now, this patch effectively just renames
    things.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Ed L. Cashin" <ecashin@coraid.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Lars Ellenberg <drbd-dev@lists.linbit.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Yehuda Sadeh <yehuda@inktank.com>
    Cc: Sage Weil <sage@inktank.com>
    Cc: Alex Elder <elder@inktank.com>
    Cc: ceph-devel@vger.kernel.org
    Cc: Joshua Morris <josh.h.morris@us.ibm.com>
    Cc: Philip Kelleher <pjk1939@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: dm-devel@redhat.com
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: linux390@de.ibm.com
    Cc: Boaz Harrosh <bharrosh@panasas.com>
    Cc: Benny Halevy <bhalevy@tonian.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Nicholas A. Bellinger" <nab@linux-iscsi.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Chris Mason <chris.mason@fusionio.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Andreas Dilger <adilger.kernel@dilger.ca>
    Cc: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Dave Kleikamp <shaggy@kernel.org>
    Cc: Joern Engel <joern@logfs.org>
    Cc: Prasad Joshi <prasadjoshi.linux@gmail.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: KONISHI Ryusuke <konishi.ryusuke@lab.ntt.co.jp>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Ben Myers <bpm@sgi.com>
    Cc: xfs@oss.sgi.com
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Guo Chao <yan@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Cc: "Roger Pau Monn" <roger.pau@citrix.com>
    Cc: Jan Beulich <jbeulich@suse.com>
    Cc: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Cc: Ian Campbell <Ian.Campbell@citrix.com>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchand@redhat.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Peng Tao <tao.peng@emc.com>
    Cc: Andy Adamson <andros@netapp.com>
    Cc: fanchaoting <fanchaoting@cn.fujitsu.com>
    Cc: Jie Liu <jeff.liu@oracle.com>
    Cc: Sunil Mushran <sunil.mushran@gmail.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Namjae Jeon <namjae.jeon@samsung.com>
    Cc: Pankaj Kumar <pankaj.km@samsung.com>
    Cc: Dan Magenheimer <dan.magenheimer@oracle.com>
    Cc: Mel Gorman <mgorman@suse.de>6

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 1d9ee67d14ec..60fb6044b953 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -233,9 +233,9 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	struct cache_sb *out = page_address(bio->bi_io_vec[0].bv_page);
 	unsigned i;
 
-	bio->bi_sector	= SB_SECTOR;
-	bio->bi_rw	= REQ_SYNC|REQ_META;
-	bio->bi_size	= SB_SIZE;
+	bio->bi_iter.bi_sector	= SB_SECTOR;
+	bio->bi_rw		= REQ_SYNC|REQ_META;
+	bio->bi_iter.bi_size	= SB_SIZE;
 	bch_bio_map(bio, NULL);
 
 	out->offset		= cpu_to_le64(sb->offset);
@@ -347,7 +347,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 		struct bio *bio = bch_bbio_alloc(c);
 
 		bio->bi_rw	= REQ_SYNC|REQ_META|rw;
-		bio->bi_size	= KEY_SIZE(k) << 9;
+		bio->bi_iter.bi_size = KEY_SIZE(k) << 9;
 
 		bio->bi_end_io	= uuid_endio;
 		bio->bi_private = cl;
@@ -503,10 +503,10 @@ static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
 
 	closure_init_stack(cl);
 
-	bio->bi_sector	= bucket * ca->sb.bucket_size;
-	bio->bi_bdev	= ca->bdev;
-	bio->bi_rw	= REQ_SYNC|REQ_META|rw;
-	bio->bi_size	= bucket_bytes(ca);
+	bio->bi_iter.bi_sector	= bucket * ca->sb.bucket_size;
+	bio->bi_bdev		= ca->bdev;
+	bio->bi_rw		= REQ_SYNC|REQ_META|rw;
+	bio->bi_iter.bi_size	= bucket_bytes(ca);
 
 	bio->bi_end_io	= prio_endio;
 	bio->bi_private = ca;

commit ed9c47bebeeea4a468b07cfd745c690190f8014c
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Nov 22 19:37:48 2013 -0800

    bcache: Kill unaligned bvec hack
    
    Bcache has a hack to avoid cloning the biovec if it's all full pages -
    but with immutable biovecs coming this won't be necessary anymore.
    
    For now, we remove the special case and always clone the bvec array so
    that the immutable biovec patches are simpler.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index dec15cd2d797..1d9ee67d14ec 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -739,8 +739,6 @@ static void bcache_device_free(struct bcache_device *d)
 	}
 
 	bio_split_pool_free(&d->bio_split_hook);
-	if (d->unaligned_bvec)
-		mempool_destroy(d->unaligned_bvec);
 	if (d->bio_split)
 		bioset_free(d->bio_split);
 	if (is_vmalloc_addr(d->full_dirty_stripes))
@@ -793,8 +791,6 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 		return minor;
 
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
-				sizeof(struct bio_vec) * BIO_MAX_PAGES)) ||
 	    bio_split_pool_init(&d->bio_split_hook) ||
 	    !(d->disk = alloc_disk(1))) {
 		ida_simple_remove(&bcache_minor, minor);

commit c86949486d41d9e7d7681fc72923555114fd702f
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Sep 10 21:41:34 2013 -0700

    bcache: defensively handle format strings
    
    Just to be safe, call the error reporting function with "%s" to avoid
    any possible future format string leak.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 70708ab0b8f9..dec15cd2d797 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1670,7 +1670,7 @@ static void run_cache_set(struct cache_set *c)
 err:
 	closure_sync(&cl);
 	/* XXX: test this, it's broken */
-	bch_cache_set_error(c, err);
+	bch_cache_set_error(c, "%s", err);
 }
 
 static bool can_attach_cache(struct cache *ca, struct cache_set *c)

commit 28935ab5163c49ca5c199d67335e5e3c72c50853
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 31 01:12:02 2013 -0700

    bcache: Use ida for bcache block dev minor
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fa1d53087f88..70708ab0b8f9 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -16,6 +16,7 @@
 #include <linux/buffer_head.h>
 #include <linux/debugfs.h>
 #include <linux/genhd.h>
+#include <linux/idr.h>
 #include <linux/kthread.h>
 #include <linux/module.h>
 #include <linux/random.h>
@@ -50,7 +51,8 @@ struct mutex bch_register_lock;
 LIST_HEAD(bch_cache_sets);
 static LIST_HEAD(uncached_devices);
 
-static int bcache_major, bcache_minor;
+static int bcache_major;
+static DEFINE_IDA(bcache_minor);
 static wait_queue_head_t unregister_wait;
 struct workqueue_struct *bcache_wq;
 
@@ -731,8 +733,10 @@ static void bcache_device_free(struct bcache_device *d)
 		del_gendisk(d->disk);
 	if (d->disk && d->disk->queue)
 		blk_cleanup_queue(d->disk->queue);
-	if (d->disk)
+	if (d->disk) {
+		ida_simple_remove(&bcache_minor, d->disk->first_minor);
 		put_disk(d->disk);
+	}
 
 	bio_split_pool_free(&d->bio_split_hook);
 	if (d->unaligned_bvec)
@@ -756,6 +760,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 {
 	struct request_queue *q;
 	size_t n;
+	int minor;
 
 	if (!d->stripe_size)
 		d->stripe_size = 1 << 31;
@@ -783,22 +788,31 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (!d->full_dirty_stripes)
 		return -ENOMEM;
 
+	minor = ida_simple_get(&bcache_minor, 0, MINORMASK + 1, GFP_KERNEL);
+	if (minor < 0)
+		return minor;
+
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
 				sizeof(struct bio_vec) * BIO_MAX_PAGES)) ||
 	    bio_split_pool_init(&d->bio_split_hook) ||
-	    !(d->disk = alloc_disk(1)) ||
-	    !(q = blk_alloc_queue(GFP_KERNEL)))
+	    !(d->disk = alloc_disk(1))) {
+		ida_simple_remove(&bcache_minor, minor);
 		return -ENOMEM;
+	}
 
 	set_capacity(d->disk, sectors);
-	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", bcache_minor);
+	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", minor);
 
 	d->disk->major		= bcache_major;
-	d->disk->first_minor	= bcache_minor++;
+	d->disk->first_minor	= minor;
 	d->disk->fops		= &bcache_ops;
 	d->disk->private_data	= d;
 
+	q = blk_alloc_queue(GFP_KERNEL);
+	if (!q)
+		return -ENOMEM;
+
 	blk_queue_make_request(q, NULL);
 	d->disk->queue			= q;
 	q->queuedata			= d;

commit c4d951ddb66fe1d087447b0ba65c4fa4446f1083
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Aug 21 17:49:09 2013 -0700

    bcache: Fix sysfs splat on shutdown with flash only devs
    
    Whoops.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 43fcfe38be11..fa1d53087f88 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -621,7 +621,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 static int open_dev(struct block_device *b, fmode_t mode)
 {
 	struct bcache_device *d = b->bd_disk->private_data;
-	if (atomic_read(&d->closing))
+	if (test_bit(BCACHE_DEV_CLOSING, &d->flags))
 		return -ENXIO;
 
 	closure_get(&d->cl);
@@ -650,20 +650,24 @@ static const struct block_device_operations bcache_ops = {
 
 void bcache_device_stop(struct bcache_device *d)
 {
-	if (!atomic_xchg(&d->closing, 1))
+	if (!test_and_set_bit(BCACHE_DEV_CLOSING, &d->flags))
 		closure_queue(&d->cl);
 }
 
 static void bcache_device_unlink(struct bcache_device *d)
 {
-	unsigned i;
-	struct cache *ca;
+	lockdep_assert_held(&bch_register_lock);
 
-	sysfs_remove_link(&d->c->kobj, d->name);
-	sysfs_remove_link(&d->kobj, "cache");
+	if (d->c && !test_and_set_bit(BCACHE_DEV_UNLINK_DONE, &d->flags)) {
+		unsigned i;
+		struct cache *ca;
 
-	for_each_cache(ca, d->c, i)
-		bd_unlink_disk_holder(ca->bdev, d->disk);
+		sysfs_remove_link(&d->c->kobj, d->name);
+		sysfs_remove_link(&d->kobj, "cache");
+
+		for_each_cache(ca, d->c, i)
+			bd_unlink_disk_holder(ca->bdev, d->disk);
+	}
 }
 
 static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
@@ -687,19 +691,16 @@ static void bcache_device_detach(struct bcache_device *d)
 {
 	lockdep_assert_held(&bch_register_lock);
 
-	if (atomic_read(&d->detaching)) {
+	if (test_bit(BCACHE_DEV_DETACHING, &d->flags)) {
 		struct uuid_entry *u = d->c->uuids + d->id;
 
 		SET_UUID_FLASH_ONLY(u, 0);
 		memcpy(u->uuid, invalid_uuid, 16);
 		u->invalidated = cpu_to_le32(get_seconds());
 		bch_uuid_write(d->c);
-
-		atomic_set(&d->detaching, 0);
 	}
 
-	if (!d->flush_done)
-		bcache_device_unlink(d);
+	bcache_device_unlink(d);
 
 	d->c->devices[d->id] = NULL;
 	closure_put(&d->c->caching);
@@ -879,7 +880,7 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	struct closure cl;
 	closure_init_stack(&cl);
 
-	BUG_ON(!atomic_read(&dc->disk.detaching));
+	BUG_ON(!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags));
 	BUG_ON(atomic_read(&dc->count));
 
 	mutex_lock(&bch_register_lock);
@@ -893,6 +894,8 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	bcache_device_detach(&dc->disk);
 	list_move(&dc->list, &uncached_devices);
 
+	clear_bit(BCACHE_DEV_DETACHING, &dc->disk.flags);
+
 	mutex_unlock(&bch_register_lock);
 
 	pr_info("Caching disabled for %s", bdevname(dc->bdev, buf));
@@ -905,10 +908,10 @@ void bch_cached_dev_detach(struct cached_dev *dc)
 {
 	lockdep_assert_held(&bch_register_lock);
 
-	if (atomic_read(&dc->disk.closing))
+	if (test_bit(BCACHE_DEV_CLOSING, &dc->disk.flags))
 		return;
 
-	if (atomic_xchg(&dc->disk.detaching, 1))
+	if (test_and_set_bit(BCACHE_DEV_DETACHING, &dc->disk.flags))
 		return;
 
 	/*
@@ -1064,11 +1067,7 @@ static void cached_dev_flush(struct closure *cl)
 	struct bcache_device *d = &dc->disk;
 
 	mutex_lock(&bch_register_lock);
-	d->flush_done = 1;
-
-	if (d->c)
-		bcache_device_unlink(d);
-
+	bcache_device_unlink(d);
 	mutex_unlock(&bch_register_lock);
 
 	bch_cache_accounting_destroy(&dc->accounting);

commit 48a915a87f0bd98c3d68d029acf223a2e5116f07
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 31 15:43:22 2013 -0700

    bcache: Better full stripe scanning
    
    The old scanning-by-stripe code burned too much CPU, this should be
    better.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 4813ef67cef5..43fcfe38be11 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -738,6 +738,10 @@ static void bcache_device_free(struct bcache_device *d)
 		mempool_destroy(d->unaligned_bvec);
 	if (d->bio_split)
 		bioset_free(d->bio_split);
+	if (is_vmalloc_addr(d->full_dirty_stripes))
+		vfree(d->full_dirty_stripes);
+	else
+		kfree(d->full_dirty_stripes);
 	if (is_vmalloc_addr(d->stripe_sectors_dirty))
 		vfree(d->stripe_sectors_dirty);
 	else
@@ -757,8 +761,12 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 
 	d->nr_stripes = DIV_ROUND_UP_ULL(sectors, d->stripe_size);
 
-	if (!d->nr_stripes || d->nr_stripes > SIZE_MAX / sizeof(atomic_t))
+	if (!d->nr_stripes ||
+	    d->nr_stripes > INT_MAX ||
+	    d->nr_stripes > SIZE_MAX / sizeof(atomic_t)) {
+		pr_err("nr_stripes too large");
 		return -ENOMEM;
+	}
 
 	n = d->nr_stripes * sizeof(atomic_t);
 	d->stripe_sectors_dirty = n < PAGE_SIZE << 6
@@ -767,6 +775,13 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	if (!d->stripe_sectors_dirty)
 		return -ENOMEM;
 
+	n = BITS_TO_LONGS(d->nr_stripes) * sizeof(unsigned long);
+	d->full_dirty_stripes = n < PAGE_SIZE << 6
+		? kzalloc(n, GFP_KERNEL)
+		: vzalloc(n);
+	if (!d->full_dirty_stripes)
+		return -ENOMEM;
+
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
 				sizeof(struct bio_vec) * BIO_MAX_PAGES)) ||

commit 65d22e911bfc4f46cda4751f1b1926b43c316c14
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 31 00:03:54 2013 -0700

    bcache: Move spinlock into struct time_stats
    
    Minor cleanup.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 041dd9d1d882..4813ef67cef5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1435,9 +1435,13 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	init_waitqueue_head(&c->try_wait);
 	init_waitqueue_head(&c->bucket_wait);
 	closure_init_unlocked(&c->uuid_write);
-	spin_lock_init(&c->sort_time_lock);
 	mutex_init(&c->sort_lock);
-	spin_lock_init(&c->btree_read_time_lock);
+
+	spin_lock_init(&c->sort_time.lock);
+	spin_lock_init(&c->btree_gc_time.lock);
+	spin_lock_init(&c->btree_split_time.lock);
+	spin_lock_init(&c->btree_read_time.lock);
+	spin_lock_init(&c->try_harder_time.lock);
 
 	bch_moving_init_cache_set(c);
 

commit 8aee122071a69ca6fa3314da7713bdf0b61dc07c
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Jul 30 22:34:40 2013 -0700

    bcache: Kill sequential_merge option
    
    It never really made sense to expose this, so just kill it.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e21200e98da6..041dd9d1d882 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1079,7 +1079,6 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 	spin_lock_init(&dc->io_lock);
 	bch_cache_accounting_init(&dc->accounting, &dc->disk.cl);
 
-	dc->sequential_merge		= true;
 	dc->sequential_cutoff		= 4 << 20;
 
 	for (io = dc->io; io < dc->io + RECENT_IO; io++) {

commit bc9389eefe479b7b7b323c2729b61a7155d2d0ea
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 19:07:35 2013 -0700

    bcache: Avoid deadlocking in garbage collection
    
    Not a complete fix - we could still deadlock if btree_insert_node() has
    to split...
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7ab926d94d81..e21200e98da6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1601,7 +1601,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "cannot allocate new btree root";
-		c->root = bch_btree_node_alloc(c, 0);
+		c->root = bch_btree_node_alloc(c, 0, true);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 

commit d5cc66e95744065f96024add4bf7d7e019be54ac
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 23:06:40 2013 -0700

    bcache: bch_(btree|extent)_ptr_invalid()
    
    Trying to treat btree pointers and leaf node pointers the same way was a
    mistake - going to start being more explicit about the type of
    key/pointer we're dealing with. This is the first part of that
    refactoring; this patch shouldn't change any actual behaviour.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 05f8ccb9f8ca..7ab926d94d81 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -373,7 +373,7 @@ static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
 {
 	struct bkey *k = &j->uuid_bucket;
 
-	if (__bch_ptr_invalid(c, 1, k))
+	if (bch_btree_ptr_invalid(c, k))
 		return "bad uuid pointer";
 
 	bkey_copy(&c->uuid_bucket, k);
@@ -1522,7 +1522,7 @@ static void run_cache_set(struct cache_set *c)
 		k = &j->btree_root;
 
 		err = "bad btree root";
-		if (__bch_ptr_invalid(c, j->btree_level + 1, k))
+		if (bch_btree_ptr_invalid(c, k))
 			goto err;
 
 		err = "error reading btree root";

commit 3a3b6a4e075188342b58d4b6560f5540af64cac0
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 16:46:42 2013 -0700

    bcache: Don't bother with bucket refcount for btree node allocations
    
    The bucket refcount (dropped with bkey_put()) is only needed to prevent
    the newly allocated bucket from being garbage collected until we've
    added a pointer to it somewhere. But for btree node allocations, the
    fact that we have btree nodes locked is enough to guard against races
    with garbage collection.
    
    Eventually the per bucket refcount is going to be replaced with
    something specific to bch_alloc_sectors().
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c67d19a8913d..05f8ccb9f8ca 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -426,7 +426,7 @@ static int __uuid_write(struct cache_set *c)
 	closure_sync(&cl);
 
 	bkey_copy(&c->uuid_bucket, &k.key);
-	__bkey_put(c, &k.key);
+	bkey_put(c, &k.key);
 	return 0;
 }
 

commit 81ab4190ac17df41686a37c97f701623276b652a
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 31 15:46:42 2013 -0700

    bcache: Pull on disk data structures out into a separate header
    
    Now, the on disk data structures are in a header that can be exported to
    userspace - and having them all centralized is nice too.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a314c771263f..c67d19a8913d 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -45,15 +45,6 @@ const char * const bch_cache_modes[] = {
 	NULL
 };
 
-struct uuid_entry_v0 {
-	uint8_t		uuid[16];
-	uint8_t		label[32];
-	uint32_t	first_reg;
-	uint32_t	last_reg;
-	uint32_t	invalidated;
-	uint32_t	pad;
-};
-
 static struct kobject *bcache_kobj;
 struct mutex bch_register_lock;
 LIST_HEAD(bch_cache_sets);
@@ -562,7 +553,7 @@ void bch_prio_write(struct cache *ca)
 		}
 
 		p->next_bucket	= ca->prio_buckets[i + 1];
-		p->magic	= pset_magic(ca);
+		p->magic	= pset_magic(&ca->sb);
 		p->csum		= bch_crc64(&p->magic, bucket_bytes(ca) - 8);
 
 		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, true);
@@ -613,7 +604,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 			if (p->csum != bch_crc64(&p->magic, bucket_bytes(ca) - 8))
 				pr_warn("bad csum reading priorities");
 
-			if (p->magic != pset_magic(ca))
+			if (p->magic != pset_magic(&ca->sb))
 				pr_warn("bad magic reading priorities");
 
 			bucket = p->next_bucket;

commit c18536a72ddd7fe30d63e6c1500b5c930ac14594
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:44:17 2013 -0700

    bcache: Prune struct btree_op
    
    Eventual goal is for struct btree_op to contain only what is necessary
    for traversing the btree.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b79dd5a6679e..a314c771263f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1493,11 +1493,10 @@ static void run_cache_set(struct cache_set *c)
 	const char *err = "cannot allocate memory";
 	struct cached_dev *dc, *t;
 	struct cache *ca;
+	struct closure cl;
 	unsigned i;
 
-	struct btree_op op;
-	bch_btree_op_init_stack(&op);
-	op.lock = SHRT_MAX;
+	closure_init_stack(&cl);
 
 	for_each_cache(ca, c, i)
 		c->nbuckets += ca->sb.nbuckets;
@@ -1508,7 +1507,7 @@ static void run_cache_set(struct cache_set *c)
 		struct jset *j;
 
 		err = "cannot allocate memory for journal";
-		if (bch_journal_read(c, &journal, &op))
+		if (bch_journal_read(c, &journal))
 			goto err;
 
 		pr_debug("btree_journal_read() done");
@@ -1543,12 +1542,12 @@ static void run_cache_set(struct cache_set *c)
 		list_del_init(&c->root->list);
 		rw_unlock(true, c->root);
 
-		err = uuid_read(c, j, &op.cl);
+		err = uuid_read(c, j, &cl);
 		if (err)
 			goto err;
 
 		err = "error in recovery";
-		if (bch_btree_check(c, &op))
+		if (bch_btree_check(c))
 			goto err;
 
 		bch_journal_mark(c, &journal);
@@ -1580,7 +1579,7 @@ static void run_cache_set(struct cache_set *c)
 		if (j->version < BCACHE_JSET_VERSION_UUID)
 			__uuid_write(c);
 
-		bch_journal_replay(c, &journal, &op);
+		bch_journal_replay(c, &journal);
 	} else {
 		pr_notice("invalidating existing data");
 
@@ -1616,7 +1615,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		bkey_copy_key(&c->root->key, &MAX_KEY);
-		bch_btree_node_write(c->root, &op.cl);
+		bch_btree_node_write(c->root, &cl);
 
 		bch_btree_set_root(c->root);
 		rw_unlock(true, c->root);
@@ -1629,14 +1628,14 @@ static void run_cache_set(struct cache_set *c)
 		SET_CACHE_SYNC(&c->sb, true);
 
 		bch_journal_next(&c->journal);
-		bch_journal_meta(c, &op.cl);
+		bch_journal_meta(c, &cl);
 	}
 
 	err = "error starting gc thread";
 	if (bch_gc_thread_start(c))
 		goto err;
 
-	closure_sync(&op.cl);
+	closure_sync(&cl);
 	c->sb.last_mount = get_seconds();
 	bcache_write_super(c);
 
@@ -1647,7 +1646,7 @@ static void run_cache_set(struct cache_set *c)
 
 	return;
 err:
-	closure_sync(&op.cl);
+	closure_sync(&cl);
 	/* XXX: test this, it's broken */
 	bch_cache_set_error(c, err);
 }

commit 5e6926daac267dd99552ae613f041a9e88bbf258
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:50:06 2013 -0700

    bcache: Convert writeback to a kthread
    
    This simplifies the writeback flow control quite a bit - previously, it
    was conceptually two coroutines, refill_dirty() and read_dirty(). This
    makes the code quite a bit more straightforward.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f89e2296bde1..b79dd5a6679e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1029,6 +1029,7 @@ static void cached_dev_free(struct closure *cl)
 	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
 
 	cancel_delayed_work_sync(&dc->writeback_rate_update);
+	kthread_stop(dc->writeback_thread);
 
 	mutex_lock(&bch_register_lock);
 
@@ -2006,7 +2007,6 @@ static struct notifier_block reboot = {
 static void bcache_exit(void)
 {
 	bch_debug_exit();
-	bch_writeback_exit();
 	bch_request_exit();
 	bch_btree_exit();
 	if (bcache_kobj)
@@ -2039,7 +2039,6 @@ static int __init bcache_init(void)
 	    sysfs_create_files(bcache_kobj, files) ||
 	    bch_btree_init() ||
 	    bch_request_init() ||
-	    bch_writeback_init() ||
 	    bch_debug_init(bcache_kobj))
 		goto err;
 

commit 72a44517f3ca3725dc86081d105457df46448679
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 24 17:19:26 2013 -0700

    bcache: Convert gc to a kthread
    
    We needed a dedicated rescuer workqueue for gc anyways... and gc was
    conceptually a dedicated thread, just one that wasn't running all the
    time. Switch it to a dedicated thread to make the code a bit more
    straightforward.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 84398a82fbe3..f89e2296bde1 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1342,6 +1342,9 @@ static void cache_set_flush(struct closure *cl)
 	kobject_put(&c->internal);
 	kobject_del(&c->kobj);
 
+	if (c->gc_thread)
+		kthread_stop(c->gc_thread);
+
 	if (!IS_ERR_OR_NULL(c->root))
 		list_add(&c->root->list, &c->btree_cache);
 
@@ -1579,8 +1582,6 @@ static void run_cache_set(struct cache_set *c)
 		bch_journal_replay(c, &journal, &op);
 	} else {
 		pr_notice("invalidating existing data");
-		/* Don't want invalidate_buckets() to queue a gc yet */
-		closure_lock(&c->gc, NULL);
 
 		for_each_cache(ca, c, i) {
 			unsigned j;
@@ -1606,12 +1607,12 @@ static void run_cache_set(struct cache_set *c)
 
 		err = "cannot allocate new UUID bucket";
 		if (__uuid_write(c))
-			goto err_unlock_gc;
+			goto err;
 
 		err = "cannot allocate new btree root";
 		c->root = bch_btree_node_alloc(c, 0);
 		if (IS_ERR_OR_NULL(c->root))
-			goto err_unlock_gc;
+			goto err;
 
 		bkey_copy_key(&c->root->key, &MAX_KEY);
 		bch_btree_node_write(c->root, &op.cl);
@@ -1628,12 +1629,12 @@ static void run_cache_set(struct cache_set *c)
 
 		bch_journal_next(&c->journal);
 		bch_journal_meta(c, &op.cl);
-
-		/* Unlock */
-		closure_set_stopped(&c->gc.cl);
-		closure_put(&c->gc.cl);
 	}
 
+	err = "error starting gc thread";
+	if (bch_gc_thread_start(c))
+		goto err;
+
 	closure_sync(&op.cl);
 	c->sb.last_mount = get_seconds();
 	bcache_write_super(c);
@@ -1644,9 +1645,6 @@ static void run_cache_set(struct cache_set *c)
 	flash_devs_run(c);
 
 	return;
-err_unlock_gc:
-	closure_set_stopped(&c->gc.cl);
-	closure_put(&c->gc.cl);
 err:
 	closure_sync(&op.cl);
 	/* XXX: test this, it's broken */

commit 35fcd848d72683141052aa9880542461577f2dbe
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:29:09 2013 -0700

    bcache: Convert bucket_wait to wait_queue_head_t
    
    At one point we did do fancy asynchronous waiting stuff with
    bucket_wait, but that's all gone (and bucket_wait is used a lot less
    than it used to be). So use the standard primitives.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 9a164cd4058c..84398a82fbe3 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -427,7 +427,7 @@ static int __uuid_write(struct cache_set *c)
 
 	lockdep_assert_held(&bch_register_lock);
 
-	if (bch_bucket_alloc_set(c, WATERMARK_METADATA, &k.key, 1, &cl))
+	if (bch_bucket_alloc_set(c, WATERMARK_METADATA, &k.key, 1, true))
 		return 1;
 
 	SET_KEY_SIZE(&k.key, c->sb.bucket_size);
@@ -565,7 +565,7 @@ void bch_prio_write(struct cache *ca)
 		p->magic	= pset_magic(ca);
 		p->csum		= bch_crc64(&p->magic, bucket_bytes(ca) - 8);
 
-		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, &cl);
+		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, true);
 		BUG_ON(bucket == -1);
 
 		mutex_unlock(&ca->set->bucket_lock);
@@ -1439,6 +1439,7 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	closure_init_unlocked(&c->sb_write);
 	mutex_init(&c->bucket_lock);
 	init_waitqueue_head(&c->try_wait);
+	init_waitqueue_head(&c->bucket_wait);
 	closure_init_unlocked(&c->uuid_write);
 	spin_lock_init(&c->sort_time_lock);
 	mutex_init(&c->sort_lock);
@@ -1608,7 +1609,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err_unlock_gc;
 
 		err = "cannot allocate new btree root";
-		c->root = bch_btree_node_alloc(c, 0, &op.cl);
+		c->root = bch_btree_node_alloc(c, 0);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err_unlock_gc;
 

commit e8e1d4682c8cb06dbcb5ef7bb851bf9bcb889c84
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:27:07 2013 -0700

    bcache: Convert try_wait to wait_queue_head_t
    
    We never waited on c->try_wait asynchronously, so just use the standard
    primitives.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index d3169c0652f8..9a164cd4058c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1436,12 +1436,14 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 
 	c->sort_crit_factor = int_sqrt(c->btree_pages);
 
-	mutex_init(&c->bucket_lock);
-	mutex_init(&c->sort_lock);
-	spin_lock_init(&c->sort_time_lock);
 	closure_init_unlocked(&c->sb_write);
+	mutex_init(&c->bucket_lock);
+	init_waitqueue_head(&c->try_wait);
 	closure_init_unlocked(&c->uuid_write);
+	spin_lock_init(&c->sort_time_lock);
+	mutex_init(&c->sort_lock);
 	spin_lock_init(&c->btree_read_time_lock);
+
 	bch_moving_init_cache_set(c);
 
 	INIT_LIST_HEAD(&c->list);
@@ -1529,7 +1531,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err;
 
 		err = "error reading btree root";
-		c->root = bch_btree_node_get(c, k, j->btree_level, &op);
+		c->root = bch_btree_node_get(c, k, j->btree_level, true);
 		if (IS_ERR_OR_NULL(c->root))
 			goto err;
 

commit 2d679fc75678551485df62274edaed452becd16d
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Sat Aug 17 02:13:15 2013 -0700

    bcache: Stripe size isn't necessarily a power of two
    
    Originally I got this right... except that the divides didn't use
    do_div(), which broke 32 bit kernels. When I went to fix that, I forgot
    that the raid stripe size usually isn't a power of two... doh
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 74f2e902d876..d3169c0652f8 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -761,11 +761,10 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	struct request_queue *q;
 	size_t n;
 
-	if (!d->stripe_size_bits)
-		d->stripe_size_bits = 31;
+	if (!d->stripe_size)
+		d->stripe_size = 1 << 31;
 
-	d->nr_stripes = round_up(sectors, 1 << d->stripe_size_bits) >>
-		d->stripe_size_bits;
+	d->nr_stripes = DIV_ROUND_UP_ULL(sectors, d->stripe_size);
 
 	if (!d->nr_stripes || d->nr_stripes > SIZE_MAX / sizeof(atomic_t))
 		return -ENOMEM;

commit 77c320eb46e216c17aee5c943949229ccfed6904
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jul 11 19:42:51 2013 -0700

    bcache: Add on error panic/unregister setting
    
    Works kind of like the ext4 setting, to panic or remount read only on
    errors.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index fd37342a7eb5..74f2e902d876 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1260,7 +1260,8 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 {
 	va_list args;
 
-	if (test_bit(CACHE_SET_STOPPING, &c->flags))
+	if (c->on_error != ON_ERROR_PANIC &&
+	    test_bit(CACHE_SET_STOPPING, &c->flags))
 		return false;
 
 	/* XXX: we can be called from atomic context
@@ -1275,6 +1276,9 @@ bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
 
 	printk(", disabling caching\n");
 
+	if (c->on_error == ON_ERROR_PANIC)
+		panic("panic forced after error\n");
+
 	bch_cache_set_unregister(c);
 	return true;
 }

commit 49b1212dfacfe51f951442563d1617bb06aac575
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:16:09 2013 -0700

    bcache: Use blkdev_issue_discard()
    
    The old asynchronous discard code was really a relic from when all the
    allocation code was asynchronous - now that allocation runs out of a
    dedicated thread there's no point in keeping around all that complicated
    machinery.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 547c4c57b052..fd37342a7eb5 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1725,8 +1725,6 @@ void bch_cache_release(struct kobject *kobj)
 	if (ca->set)
 		ca->set->cache[ca->sb.nr_this_dev] = NULL;
 
-	bch_cache_allocator_exit(ca);
-
 	bio_split_pool_free(&ca->bio_split_hook);
 
 	free_pages((unsigned long) ca->disk_buckets, ilog2(bucket_pages(ca)));
@@ -1758,8 +1756,6 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	__module_get(THIS_MODULE);
 	kobject_init(&ca->kobj, &bch_cache_ktype);
 
-	INIT_LIST_HEAD(&ca->discards);
-
 	bio_init(&ca->journal.bio);
 	ca->journal.bio.bi_max_vecs = 8;
 	ca->journal.bio.bi_io_vec = ca->journal.bio.bi_inline_vecs;

commit 79826c35eb99cd3c0873b8396f45fa26c87fb0b0
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 18:31:58 2013 -0700

    bcache: Allocation kthread fixes
    
    The alloc kthread should've been using try_to_freeze() - and also there
    was the potential for the alloc kthread to get woken up after it had
    shut down, which would have been bad.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f6a62174e8f6..547c4c57b052 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -16,6 +16,7 @@
 #include <linux/buffer_head.h>
 #include <linux/debugfs.h>
 #include <linux/genhd.h>
+#include <linux/kthread.h>
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/reboot.h>
@@ -1329,11 +1330,9 @@ static void cache_set_free(struct closure *cl)
 static void cache_set_flush(struct closure *cl)
 {
 	struct cache_set *c = container_of(cl, struct cache_set, caching);
+	struct cache *ca;
 	struct btree *b;
-
-	/* Shut down allocator threads */
-	set_bit(CACHE_SET_STOPPING_2, &c->flags);
-	wake_up_allocators(c);
+	unsigned i;
 
 	bch_cache_accounting_destroy(&c->accounting);
 
@@ -1348,6 +1347,10 @@ static void cache_set_flush(struct closure *cl)
 		if (btree_node_dirty(b))
 			bch_btree_node_write(b, NULL);
 
+	for_each_cache(ca, c, i)
+		if (ca->alloc_thread)
+			kthread_stop(ca->alloc_thread);
+
 	closure_return(cl);
 }
 

commit 5caa52afc5abd1396e4af720469abb5843a71eb8
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 21:03:25 2013 -0700

    bcache: Shutdown fix
    
    Stopping a cache set is supposed to make it stop attached backing
    devices, but somewhere along the way that code got lost. Fixing this
    mainly has the effect of fixing our reboot notifier.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: linux-stable <stable@vger.kernel.org> # >= v3.10

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7a1dcdb2536e..f6a62174e8f6 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1354,18 +1354,22 @@ static void cache_set_flush(struct closure *cl)
 static void __cache_set_unregister(struct closure *cl)
 {
 	struct cache_set *c = container_of(cl, struct cache_set, caching);
-	struct cached_dev *dc, *t;
+	struct cached_dev *dc;
 	size_t i;
 
 	mutex_lock(&bch_register_lock);
 
-	if (test_bit(CACHE_SET_UNREGISTERING, &c->flags))
-		list_for_each_entry_safe(dc, t, &c->cached_devs, list)
-			bch_cached_dev_detach(dc);
-
 	for (i = 0; i < c->nr_uuids; i++)
-		if (c->devices[i] && UUID_FLASH_ONLY(&c->uuids[i]))
-			bcache_device_stop(c->devices[i]);
+		if (c->devices[i]) {
+			if (!UUID_FLASH_ONLY(&c->uuids[i]) &&
+			    test_bit(CACHE_SET_UNREGISTERING, &c->flags)) {
+				dc = container_of(c->devices[i],
+						  struct cached_dev, disk);
+				bch_cached_dev_detach(dc);
+			} else {
+				bcache_device_stop(c->devices[i]);
+			}
+		}
 
 	mutex_unlock(&bch_register_lock);
 

commit c9502ea4424b31728703d113fc6b30bfead14633
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 21:25:02 2013 -0700

    bcache: Fix a sysfs splat on shutdown
    
    If we stopped a bcache device when we were already detaching (or
    something like that), bcache_device_unlink() would try to remove a
    symlink from sysfs that was already gone because the bcache dev kobject
    had already been removed from sysfs.
    
    So keep track of whether we've removed stuff from sysfs.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: linux-stable <stable@vger.kernel.org> # >= v3.10

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 728fdc673f31..7a1dcdb2536e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -706,7 +706,8 @@ static void bcache_device_detach(struct bcache_device *d)
 		atomic_set(&d->detaching, 0);
 	}
 
-	bcache_device_unlink(d);
+	if (!d->flush_done)
+		bcache_device_unlink(d);
 
 	d->c->devices[d->id] = NULL;
 	closure_put(&d->c->caching);
@@ -1055,6 +1056,14 @@ static void cached_dev_flush(struct closure *cl)
 	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
 	struct bcache_device *d = &dc->disk;
 
+	mutex_lock(&bch_register_lock);
+	d->flush_done = 1;
+
+	if (d->c)
+		bcache_device_unlink(d);
+
+	mutex_unlock(&bch_register_lock);
+
 	bch_cache_accounting_destroy(&dc->accounting);
 	kobject_del(&d->kobj);
 

commit 54d12f2b4fd0f218590d1490b41a18d0e2328a9a
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 18:44:40 2013 -0700

    bcache: Advertise that flushes are supported
    
    Whoops - bcache's flush/FUA was mostly correct, but flushes get filtered
    out unless we say we support them...
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: linux-stable <stable@vger.kernel.org> # >= v3.10

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index cff2d182dfb0..728fdc673f31 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -806,6 +806,8 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size,
 	set_bit(QUEUE_FLAG_NONROT,	&d->disk->queue->queue_flags);
 	set_bit(QUEUE_FLAG_DISCARD,	&d->disk->queue->queue_flags);
 
+	blk_queue_flush(q, REQ_FLUSH|REQ_FUA);
+
 	return 0;
 }
 

commit ab9e14002e271eba41f7f9ab7e9b03cac4adc22d
Author: Gabriel de Perthuis <g2p.code@gmail.com>
Date:   Sun Jun 9 00:54:48 2013 +0200

    bcache: Send label uevents
    
    Signed-off-by: Gabriel de Perthuis <g2p.code@gmail.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index a104dad2c7fa..cff2d182dfb0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -825,12 +825,18 @@ static void calc_cached_dev_sectors(struct cache_set *c)
 void bch_cached_dev_run(struct cached_dev *dc)
 {
 	struct bcache_device *d = &dc->disk;
+	char buf[SB_LABEL_SIZE + 1];
 	char *env[] = {
 		"DRIVER=bcache",
 		kasprintf(GFP_KERNEL, "CACHED_UUID=%pU", dc->sb.uuid),
-		NULL
+		NULL,
+		NULL,
 	};
 
+	memcpy(buf, dc->sb.label, SB_LABEL_SIZE);
+	buf[SB_LABEL_SIZE] = '\0';
+	env[2] = kasprintf(GFP_KERNEL, "CACHED_LABEL=%s", buf);
+
 	if (atomic_xchg(&dc->running, 1))
 		return;
 
@@ -850,6 +856,7 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	 * only class / kset properties are persistent */
 	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
 	kfree(env[1]);
+	kfree(env[2]);
 
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))

commit a25c32bedeff3573b53572b87bcafe10ec5b75a9
Author: Gabriel de Perthuis <g2p.code@gmail.com>
Date:   Fri Jun 7 23:27:01 2013 +0200

    bcache: Send a uevent with a cached device's UUID
    
    Signed-off-by: Gabriel de Perthuis <g2p.code@gmail.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 8c73f0c7f28a..a104dad2c7fa 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -825,6 +825,11 @@ static void calc_cached_dev_sectors(struct cache_set *c)
 void bch_cached_dev_run(struct cached_dev *dc)
 {
 	struct bcache_device *d = &dc->disk;
+	char *env[] = {
+		"DRIVER=bcache",
+		kasprintf(GFP_KERNEL, "CACHED_UUID=%pU", dc->sb.uuid),
+		NULL
+	};
 
 	if (atomic_xchg(&dc->running, 1))
 		return;
@@ -841,10 +846,11 @@ void bch_cached_dev_run(struct cached_dev *dc)
 
 	add_disk(d->disk);
 	bd_link_disk_holder(dc->bdev, dc->disk.disk);
-#if 0
-	char *env[] = { "SYMLINK=label" , NULL };
+	/* won't show up in the uevent file, use udevadm monitor -e instead
+	 * only class / kset properties are persistent */
 	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
-#endif
+	kfree(env[1]);
+
 	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
 	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))
 		pr_debug("error creating sysfs link");

commit 279afbad4e54acbd61bf88a54a73af3bbfdeb5dd
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Jun 5 06:21:07 2013 -0700

    bcache: Track dirty data by stripe
    
    To make background writeback aware of raid5/6 stripes, we first need to
    track the amount of dirty data within each stripe - we do this by
    breaking up the existing sectors_dirty into per stripe atomic_ts
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index dbfa1c38e85e..8c73f0c7f28a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -10,6 +10,7 @@
 #include "btree.h"
 #include "debug.h"
 #include "request.h"
+#include "writeback.h"
 
 #include <linux/blkdev.h>
 #include <linux/buffer_head.h>
@@ -744,13 +745,35 @@ static void bcache_device_free(struct bcache_device *d)
 		mempool_destroy(d->unaligned_bvec);
 	if (d->bio_split)
 		bioset_free(d->bio_split);
+	if (is_vmalloc_addr(d->stripe_sectors_dirty))
+		vfree(d->stripe_sectors_dirty);
+	else
+		kfree(d->stripe_sectors_dirty);
 
 	closure_debug_destroy(&d->cl);
 }
 
-static int bcache_device_init(struct bcache_device *d, unsigned block_size)
+static int bcache_device_init(struct bcache_device *d, unsigned block_size,
+			      sector_t sectors)
 {
 	struct request_queue *q;
+	size_t n;
+
+	if (!d->stripe_size_bits)
+		d->stripe_size_bits = 31;
+
+	d->nr_stripes = round_up(sectors, 1 << d->stripe_size_bits) >>
+		d->stripe_size_bits;
+
+	if (!d->nr_stripes || d->nr_stripes > SIZE_MAX / sizeof(atomic_t))
+		return -ENOMEM;
+
+	n = d->nr_stripes * sizeof(atomic_t);
+	d->stripe_sectors_dirty = n < PAGE_SIZE << 6
+		? kzalloc(n, GFP_KERNEL)
+		: vzalloc(n);
+	if (!d->stripe_sectors_dirty)
+		return -ENOMEM;
 
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
@@ -760,6 +783,7 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size)
 	    !(q = blk_alloc_queue(GFP_KERNEL)))
 		return -ENOMEM;
 
+	set_capacity(d->disk, sectors);
 	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", bcache_minor);
 
 	d->disk->major		= bcache_major;
@@ -1047,7 +1071,8 @@ static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 		hlist_add_head(&io->hash, dc->io_hash + RECENT_IO);
 	}
 
-	ret = bcache_device_init(&dc->disk, block_size);
+	ret = bcache_device_init(&dc->disk, block_size,
+			 dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
 	if (ret)
 		return ret;
 
@@ -1146,11 +1171,10 @@ static int flash_dev_run(struct cache_set *c, struct uuid_entry *u)
 
 	kobject_init(&d->kobj, &bch_flash_dev_ktype);
 
-	if (bcache_device_init(d, block_bytes(c)))
+	if (bcache_device_init(d, block_bytes(c), u->sectors))
 		goto err;
 
 	bcache_device_attach(d, c, u - c->uuids);
-	set_capacity(d->disk, u->sectors);
 	bch_flash_dev_request_init(d);
 	add_disk(d->disk);
 

commit 444fc0b6b167ed164e7436621a9d095e042644dd
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat May 11 17:07:26 2013 -0700

    bcache: Initialize sectors_dirty when attaching
    
    Previously, dirty_data wouldn't get initialized until the first garbage
    collection... which was a bit of a problem for background writeback (as
    the PD controller keys off of it) and also confusing for users.
    
    This is also prep work for making background writeback aware of raid5/6
    stripes.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3c8474161e8e..dbfa1c38e85e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -961,6 +961,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	atomic_set(&dc->count, 1);
 
 	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
+		bch_sectors_dirty_init(dc);
 		atomic_set(&dc->has_dirty, 1);
 		atomic_inc(&dc->count);
 		bch_writeback_queue(dc);

commit 6ded34d1a54c046a45db071d3cb7b37bd0a4a31f
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat May 11 15:59:37 2013 -0700

    bcache: Improve lazy sorting
    
    The old lazy sorting code was kind of hacky - rewrite in a way that
    mathematically makes more sense; the idea is that the size of the sets
    of keys in a btree node should increase by a more or less fixed ratio
    from smallest to biggest.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f24c2e0cbb1c..3c8474161e8e 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1375,6 +1375,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 		c->btree_pages = max_t(int, c->btree_pages / 4,
 				       BTREE_MAX_PAGES);
 
+	c->sort_crit_factor = int_sqrt(c->btree_pages);
+
 	mutex_init(&c->bucket_lock);
 	mutex_init(&c->sort_lock);
 	spin_lock_init(&c->sort_time_lock);

commit 85b1492ee113486d871de7676a61f506a43ca475
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 14 20:33:16 2013 -0700

    bcache: Rip out pkey()/pbtree()
    
    Old gcc doesnt like the struct hack, and it is kind of ugly. So finish
    off the work to convert pr_debug() statements to tracepoints, and delete
    pkey()/pbtree().
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 47bc13745068..f24c2e0cbb1c 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -343,6 +343,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 	struct closure *cl = &c->uuid_write.cl;
 	struct uuid_entry *u;
 	unsigned i;
+	char buf[80];
 
 	BUG_ON(!parent);
 	closure_lock(&c->uuid_write, parent);
@@ -363,8 +364,8 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 			break;
 	}
 
-	pr_debug("%s UUIDs at %s", rw & REQ_WRITE ? "wrote" : "read",
-		 pkey(&c->uuid_bucket));
+	bch_bkey_to_text(buf, sizeof(buf), k);
+	pr_debug("%s UUIDs at %s", rw & REQ_WRITE ? "wrote" : "read", buf);
 
 	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)
 		if (!bch_is_zero(u->uuid, 16))

commit c37511b863f36c1cc6e18440717fd4cc0e881b8a
Author: Kent Overstreet <koverstreet@google.com>
Date:   Fri Apr 26 15:39:55 2013 -0700

    bcache: Fix/revamp tracepoints
    
    The tracepoints were reworked to be more sensible, and fixed a null
    pointer deref in one of the tracepoints.
    
    Converted some of the pr_debug()s to tracepoints - this is partly a
    performance optimization; it used to be that with DEBUG or
    CONFIG_DYNAMIC_DEBUG pr_debug() was an empty macro; but at some point it
    was changed to an empty inline function.
    
    Some of the pr_debug() statements had rather expensive function calls as
    part of the arguments, so this code was getting run unnecessarily even
    on non debug kernels - in some fast paths, too.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index e53f89988b08..47bc13745068 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -11,6 +11,7 @@
 #include "debug.h"
 #include "request.h"
 
+#include <linux/blkdev.h>
 #include <linux/buffer_head.h>
 #include <linux/debugfs.h>
 #include <linux/genhd.h>
@@ -543,7 +544,6 @@ void bch_prio_write(struct cache *ca)
 
 	pr_debug("free %zu, free_inc %zu, unused %zu", fifo_used(&ca->free),
 		 fifo_used(&ca->free_inc), fifo_used(&ca->unused));
-	blktrace_msg(ca, "Starting priorities: " buckets_free(ca));
 
 	for (i = prio_buckets(ca) - 1; i >= 0; --i) {
 		long bucket;

commit 5794351146199b9ac67a5ab1beab82be8bfd7b5d
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Apr 25 13:58:35 2013 -0700

    bcache: Refactor btree io
    
    The most significant change is that btree reads are now done
    synchronously, instead of asynchronously and doing the post read stuff
    from a workqueue.
    
    This was originally done because we can't block on IO under
    generic_make_request(). But - we already have a mechanism to punt cache
    lookups to workqueue if needed, so if we just use that we don't have to
    deal with the complexity of doing things asynchronously.
    
    The main benefit is this makes the locking situation saner; we can hold
    our write lock on the btree node until we're finished reading it, and we
    don't need that btree_node_read_done() flag anymore.
    
    Also, for writes, btree_write() was broken out into btree_node_write()
    and btree_leaf_dirty() - the old code with the boolean argument was dumb
    and confusing.
    
    The prio_blocked mechanism was improved a bit too, now the only counter
    is in struct btree_write, we don't mess with transfering a count from
    struct btree anymore.
    
    This required changing garbage collection to block prios at the start
    and unblock when it finishes, which is cleaner than what it was doing
    anyways (the old code had mostly the same effect, but was doing it in a
    convoluted way)
    
    And the btree iter btree_node_read_done() uses was converted to a real
    mempool.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index aaeda235fc75..e53f89988b08 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1255,9 +1255,10 @@ static void cache_set_free(struct closure *cl)
 	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
 	free_pages((unsigned long) c->sort, ilog2(bucket_pages(c)));
 
-	kfree(c->fill_iter);
 	if (c->bio_split)
 		bioset_free(c->bio_split);
+	if (c->fill_iter)
+		mempool_destroy(c->fill_iter);
 	if (c->bio_meta)
 		mempool_destroy(c->bio_meta);
 	if (c->search)
@@ -1295,7 +1296,7 @@ static void cache_set_flush(struct closure *cl)
 	/* Should skip this if we're unregistering because of an error */
 	list_for_each_entry(b, &c->btree_cache, list)
 		if (btree_node_dirty(b))
-			bch_btree_write(b, true, NULL);
+			bch_btree_node_write(b, NULL);
 
 	closure_return(cl);
 }
@@ -1374,7 +1375,6 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 				       BTREE_MAX_PAGES);
 
 	mutex_init(&c->bucket_lock);
-	mutex_init(&c->fill_lock);
 	mutex_init(&c->sort_lock);
 	spin_lock_init(&c->sort_time_lock);
 	closure_init_unlocked(&c->sb_write);
@@ -1400,8 +1400,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	    !(c->bio_meta = mempool_create_kmalloc_pool(2,
 				sizeof(struct bbio) + sizeof(struct bio_vec) *
 				bucket_pages(c))) ||
+	    !(c->fill_iter = mempool_create_kmalloc_pool(1, iter_size)) ||
 	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
-	    !(c->fill_iter = kmalloc(iter_size, GFP_KERNEL)) ||
 	    !(c->sort = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
 	    bch_journal_alloc(c) ||
@@ -1409,8 +1409,6 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 	    bch_open_buckets_alloc(c))
 		goto err;
 
-	c->fill_iter->size = sb->bucket_size / sb->block_size;
-
 	c->congested_read_threshold_us	= 2000;
 	c->congested_write_threshold_us	= 20000;
 	c->error_limit	= 8 << IO_ERROR_SHIFT;
@@ -1551,7 +1549,7 @@ static void run_cache_set(struct cache_set *c)
 			goto err_unlock_gc;
 
 		bkey_copy_key(&c->root->key, &MAX_KEY);
-		bch_btree_write(c->root, true, &op);
+		bch_btree_node_write(c->root, &op.cl);
 
 		bch_btree_set_root(c->root);
 		rw_unlock(true, c->root);

commit 119ba0f82839cd80eaef3e6991988f1403965d5b
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Apr 24 19:01:12 2013 -0700

    bcache: Convert allocator thread to kthread
    
    Using a workqueue when we just want a single thread is a bit silly.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 3de5626919ef..aaeda235fc75 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1282,7 +1282,7 @@ static void cache_set_flush(struct closure *cl)
 
 	/* Shut down allocator threads */
 	set_bit(CACHE_SET_STOPPING_2, &c->flags);
-	wake_up(&c->alloc_wait);
+	wake_up_allocators(c);
 
 	bch_cache_accounting_destroy(&c->accounting);
 
@@ -1373,7 +1373,6 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
 		c->btree_pages = max_t(int, c->btree_pages / 4,
 				       BTREE_MAX_PAGES);
 
-	init_waitqueue_head(&c->alloc_wait);
 	mutex_init(&c->bucket_lock);
 	mutex_init(&c->fill_lock);
 	mutex_init(&c->sort_lock);
@@ -1496,9 +1495,10 @@ static void run_cache_set(struct cache_set *c)
 		 */
 		bch_journal_next(&c->journal);
 
+		err = "error starting allocator thread";
 		for_each_cache(ca, c, i)
-			closure_call(&ca->alloc, bch_allocator_thread,
-				     system_wq, &c->cl);
+			if (bch_cache_allocator_start(ca))
+				goto err;
 
 		/*
 		 * First place it's safe to allocate: btree_check() and
@@ -1531,17 +1531,16 @@ static void run_cache_set(struct cache_set *c)
 
 		bch_btree_gc_finish(c);
 
+		err = "error starting allocator thread";
 		for_each_cache(ca, c, i)
-			closure_call(&ca->alloc, bch_allocator_thread,
-				     ca->alloc_workqueue, &c->cl);
+			if (bch_cache_allocator_start(ca))
+				goto err;
 
 		mutex_lock(&c->bucket_lock);
 		for_each_cache(ca, c, i)
 			bch_prio_write(ca);
 		mutex_unlock(&c->bucket_lock);
 
-		wake_up(&c->alloc_wait);
-
 		err = "cannot allocate new UUID bucket";
 		if (__uuid_write(c))
 			goto err_unlock_gc;
@@ -1673,9 +1672,6 @@ void bch_cache_release(struct kobject *kobj)
 
 	bio_split_pool_free(&ca->bio_split_hook);
 
-	if (ca->alloc_workqueue)
-		destroy_workqueue(ca->alloc_workqueue);
-
 	free_pages((unsigned long) ca->disk_buckets, ilog2(bucket_pages(ca)));
 	kfree(ca->prio_buckets);
 	vfree(ca->buckets);
@@ -1723,7 +1719,6 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	    !(ca->prio_buckets	= kzalloc(sizeof(uint64_t) * prio_buckets(ca) *
 					  2, GFP_KERNEL)) ||
 	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)) ||
-	    !(ca->alloc_workqueue = alloc_workqueue("bch_allocator", 0, 1)) ||
 	    bio_split_pool_init(&ca->bio_split_hook))
 		return -ENOMEM;
 

commit a9dd53adbb84c12f769a862ba2c80404873c2c99
Author: Gabriel de Perthuis <g2p.code@gmail.com>
Date:   Sat May 4 12:19:41 2013 +0200

    bcache: Warn when a device is already registered.
    
    Signed-off-by: Gabriel de Perthuis <g2p.code+bcache@gmail.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f88e2b653a3f..3de5626919ef 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1786,6 +1786,36 @@ static ssize_t register_bcache(struct kobject *, struct kobj_attribute *,
 kobj_attribute_write(register,		register_bcache);
 kobj_attribute_write(register_quiet,	register_bcache);
 
+static bool bch_is_open_backing(struct block_device *bdev) {
+	struct cache_set *c, *tc;
+	struct cached_dev *dc, *t;
+
+	list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
+		list_for_each_entry_safe(dc, t, &c->cached_devs, list)
+			if (dc->bdev == bdev)
+				return true;
+	list_for_each_entry_safe(dc, t, &uncached_devices, list)
+		if (dc->bdev == bdev)
+			return true;
+	return false;
+}
+
+static bool bch_is_open_cache(struct block_device *bdev) {
+	struct cache_set *c, *tc;
+	struct cache *ca;
+	unsigned i;
+
+	list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
+		for_each_cache(ca, c, i)
+			if (ca->bdev == bdev)
+				return true;
+	return false;
+}
+
+static bool bch_is_open(struct block_device *bdev) {
+	return bch_is_open_cache(bdev) || bch_is_open_backing(bdev);
+}
+
 static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 			       const char *buffer, size_t size)
 {
@@ -1810,8 +1840,13 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 				  FMODE_READ|FMODE_WRITE|FMODE_EXCL,
 				  sb);
 	if (IS_ERR(bdev)) {
-		if (bdev == ERR_PTR(-EBUSY))
-			err = "device busy";
+		if (bdev == ERR_PTR(-EBUSY)) {
+			bdev = lookup_bdev(strim(path));
+			if (!IS_ERR(bdev) && bch_is_open(bdev))
+				err = "device already registered";
+			else
+				err = "device busy";
+		}
 		goto err;
 	}
 

commit f59fce847fc8483508b5028c24e2b1e00523dd88
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed May 15 00:11:26 2013 -0700

    bcache: Fix error handling in init code
    
    This code appears to have rotted... fix various bugs and do some
    refactoring.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b09beb2b52c7..f88e2b653a3f 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -731,8 +731,7 @@ static void bcache_device_free(struct bcache_device *d)
 
 	if (d->c)
 		bcache_device_detach(d);
-
-	if (d->disk)
+	if (d->disk && d->disk->flags & GENHD_FL_UP)
 		del_gendisk(d->disk);
 	if (d->disk && d->disk->queue)
 		blk_cleanup_queue(d->disk->queue);
@@ -755,12 +754,9 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size)
 	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
 	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
 				sizeof(struct bio_vec) * BIO_MAX_PAGES)) ||
-	    bio_split_pool_init(&d->bio_split_hook))
-
-		return -ENOMEM;
-
-	d->disk = alloc_disk(1);
-	if (!d->disk)
+	    bio_split_pool_init(&d->bio_split_hook) ||
+	    !(d->disk = alloc_disk(1)) ||
+	    !(q = blk_alloc_queue(GFP_KERNEL)))
 		return -ENOMEM;
 
 	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", bcache_minor);
@@ -770,10 +766,6 @@ static int bcache_device_init(struct bcache_device *d, unsigned block_size)
 	d->disk->fops		= &bcache_ops;
 	d->disk->private_data	= d;
 
-	q = blk_alloc_queue(GFP_KERNEL);
-	if (!q)
-		return -ENOMEM;
-
 	blk_queue_make_request(q, NULL);
 	d->disk->queue			= q;
 	q->queuedata			= d;
@@ -998,14 +990,17 @@ static void cached_dev_free(struct closure *cl)
 
 	mutex_lock(&bch_register_lock);
 
-	bd_unlink_disk_holder(dc->bdev, dc->disk.disk);
+	if (atomic_read(&dc->running))
+		bd_unlink_disk_holder(dc->bdev, dc->disk.disk);
 	bcache_device_free(&dc->disk);
 	list_del(&dc->list);
 
 	mutex_unlock(&bch_register_lock);
 
 	if (!IS_ERR_OR_NULL(dc->bdev)) {
-		blk_sync_queue(bdev_get_queue(dc->bdev));
+		if (dc->bdev->bd_disk)
+			blk_sync_queue(bdev_get_queue(dc->bdev));
+
 		blkdev_put(dc->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
 	}
 
@@ -1027,73 +1022,67 @@ static void cached_dev_flush(struct closure *cl)
 
 static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
 {
-	int err;
+	int ret;
 	struct io *io;
-
-	closure_init(&dc->disk.cl, NULL);
-	set_closure_fn(&dc->disk.cl, cached_dev_flush, system_wq);
+	struct request_queue *q = bdev_get_queue(dc->bdev);
 
 	__module_get(THIS_MODULE);
 	INIT_LIST_HEAD(&dc->list);
+	closure_init(&dc->disk.cl, NULL);
+	set_closure_fn(&dc->disk.cl, cached_dev_flush, system_wq);
 	kobject_init(&dc->disk.kobj, &bch_cached_dev_ktype);
-
-	bch_cache_accounting_init(&dc->accounting, &dc->disk.cl);
-
-	err = bcache_device_init(&dc->disk, block_size);
-	if (err)
-		goto err;
-
-	spin_lock_init(&dc->io_lock);
-	closure_init_unlocked(&dc->sb_write);
 	INIT_WORK(&dc->detach, cached_dev_detach_finish);
+	closure_init_unlocked(&dc->sb_write);
+	INIT_LIST_HEAD(&dc->io_lru);
+	spin_lock_init(&dc->io_lock);
+	bch_cache_accounting_init(&dc->accounting, &dc->disk.cl);
 
 	dc->sequential_merge		= true;
 	dc->sequential_cutoff		= 4 << 20;
 
-	INIT_LIST_HEAD(&dc->io_lru);
-	dc->sb_bio.bi_max_vecs	= 1;
-	dc->sb_bio.bi_io_vec	= dc->sb_bio.bi_inline_vecs;
-
 	for (io = dc->io; io < dc->io + RECENT_IO; io++) {
 		list_add(&io->lru, &dc->io_lru);
 		hlist_add_head(&io->hash, dc->io_hash + RECENT_IO);
 	}
 
-	bch_writeback_init_cached_dev(dc);
+	ret = bcache_device_init(&dc->disk, block_size);
+	if (ret)
+		return ret;
+
+	set_capacity(dc->disk.disk,
+		     dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
+
+	dc->disk.disk->queue->backing_dev_info.ra_pages =
+		max(dc->disk.disk->queue->backing_dev_info.ra_pages,
+		    q->backing_dev_info.ra_pages);
+
+	bch_cached_dev_request_init(dc);
+	bch_cached_dev_writeback_init(dc);
 	return 0;
-err:
-	bcache_device_stop(&dc->disk);
-	return err;
 }
 
 /* Cached device - bcache superblock */
 
-static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
+static void register_bdev(struct cache_sb *sb, struct page *sb_page,
 				 struct block_device *bdev,
 				 struct cached_dev *dc)
 {
 	char name[BDEVNAME_SIZE];
 	const char *err = "cannot allocate memory";
-	struct gendisk *g;
 	struct cache_set *c;
 
-	if (!dc || cached_dev_init(dc, sb->block_size << 9) != 0)
-		return err;
-
 	memcpy(&dc->sb, sb, sizeof(struct cache_sb));
-	dc->sb_bio.bi_io_vec[0].bv_page = sb_page;
 	dc->bdev = bdev;
 	dc->bdev->bd_holder = dc;
 
-	g = dc->disk.disk;
-
-	set_capacity(g, dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
-
-	g->queue->backing_dev_info.ra_pages =
-		max(g->queue->backing_dev_info.ra_pages,
-		    bdev->bd_queue->backing_dev_info.ra_pages);
+	bio_init(&dc->sb_bio);
+	dc->sb_bio.bi_max_vecs	= 1;
+	dc->sb_bio.bi_io_vec	= dc->sb_bio.bi_inline_vecs;
+	dc->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	get_page(sb_page);
 
-	bch_cached_dev_request_init(dc);
+	if (cached_dev_init(dc, sb->block_size << 9))
+		goto err;
 
 	err = "error creating kobject";
 	if (kobject_add(&dc->disk.kobj, &part_to_dev(bdev->bd_part)->kobj,
@@ -1102,6 +1091,8 @@ static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
 	if (bch_cache_accounting_add_kobjs(&dc->accounting, &dc->disk.kobj))
 		goto err;
 
+	pr_info("registered backing device %s", bdevname(bdev, name));
+
 	list_add(&dc->list, &uncached_devices);
 	list_for_each_entry(c, &bch_cache_sets, list)
 		bch_cached_dev_attach(dc, c);
@@ -1110,15 +1101,10 @@ static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
 	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE)
 		bch_cached_dev_run(dc);
 
-	return NULL;
+	return;
 err:
-	kobject_put(&dc->disk.kobj);
 	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
-	/*
-	 * Return NULL instead of an error because kobject_put() cleans
-	 * everything up
-	 */
-	return NULL;
+	bcache_device_stop(&dc->disk);
 }
 
 /* Flash only volumes */
@@ -1716,20 +1702,11 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	size_t free;
 	struct bucket *b;
 
-	if (!ca)
-		return -ENOMEM;
-
 	__module_get(THIS_MODULE);
 	kobject_init(&ca->kobj, &bch_cache_ktype);
 
-	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
-
 	INIT_LIST_HEAD(&ca->discards);
 
-	bio_init(&ca->sb_bio);
-	ca->sb_bio.bi_max_vecs	= 1;
-	ca->sb_bio.bi_io_vec	= ca->sb_bio.bi_inline_vecs;
-
 	bio_init(&ca->journal.bio);
 	ca->journal.bio.bi_max_vecs = 8;
 	ca->journal.bio.bi_io_vec = ca->journal.bio.bi_inline_vecs;
@@ -1741,18 +1718,17 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
 	    !init_fifo(&ca->unused,	free << 2, GFP_KERNEL) ||
 	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
-	    !(ca->buckets	= vmalloc(sizeof(struct bucket) *
+	    !(ca->buckets	= vzalloc(sizeof(struct bucket) *
 					  ca->sb.nbuckets)) ||
 	    !(ca->prio_buckets	= kzalloc(sizeof(uint64_t) * prio_buckets(ca) *
 					  2, GFP_KERNEL)) ||
 	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)) ||
 	    !(ca->alloc_workqueue = alloc_workqueue("bch_allocator", 0, 1)) ||
 	    bio_split_pool_init(&ca->bio_split_hook))
-		goto err;
+		return -ENOMEM;
 
 	ca->prio_last_buckets = ca->prio_buckets + prio_buckets(ca);
 
-	memset(ca->buckets, 0, ca->sb.nbuckets * sizeof(struct bucket));
 	for_each_bucket(b, ca)
 		atomic_set(&b->pin, 0);
 
@@ -1765,22 +1741,28 @@ static int cache_alloc(struct cache_sb *sb, struct cache *ca)
 	return -ENOMEM;
 }
 
-static const char *register_cache(struct cache_sb *sb, struct page *sb_page,
+static void register_cache(struct cache_sb *sb, struct page *sb_page,
 				  struct block_device *bdev, struct cache *ca)
 {
 	char name[BDEVNAME_SIZE];
 	const char *err = "cannot allocate memory";
 
-	if (cache_alloc(sb, ca) != 0)
-		return err;
-
-	ca->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
 	ca->bdev = bdev;
 	ca->bdev->bd_holder = ca;
 
+	bio_init(&ca->sb_bio);
+	ca->sb_bio.bi_max_vecs	= 1;
+	ca->sb_bio.bi_io_vec	= ca->sb_bio.bi_inline_vecs;
+	ca->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	get_page(sb_page);
+
 	if (blk_queue_discard(bdev_get_queue(ca->bdev)))
 		ca->discard = CACHE_DISCARD(&ca->sb);
 
+	if (cache_alloc(sb, ca) != 0)
+		goto err;
+
 	err = "error creating kobject";
 	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache"))
 		goto err;
@@ -1790,15 +1772,10 @@ static const char *register_cache(struct cache_sb *sb, struct page *sb_page,
 		goto err;
 
 	pr_info("registered cache device %s", bdevname(bdev, name));
-
-	return NULL;
+	return;
 err:
+	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
 	kobject_put(&ca->kobj);
-	pr_info("error opening %s: %s", bdevname(bdev, name), err);
-	/* Return NULL instead of an error because kobject_put() cleans
-	 * everything up
-	 */
-	return NULL;
 }
 
 /* Global interfaces/init */
@@ -1832,12 +1809,15 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	bdev = blkdev_get_by_path(strim(path),
 				  FMODE_READ|FMODE_WRITE|FMODE_EXCL,
 				  sb);
-	if (bdev == ERR_PTR(-EBUSY))
-		err = "device busy";
-
-	if (IS_ERR(bdev) ||
-	    set_blocksize(bdev, 4096))
+	if (IS_ERR(bdev)) {
+		if (bdev == ERR_PTR(-EBUSY))
+			err = "device busy";
 		goto err;
+	}
+
+	err = "failed to set blocksize";
+	if (set_blocksize(bdev, 4096))
+		goto err_close;
 
 	err = read_super(sb, bdev, &sb_page);
 	if (err)
@@ -1845,33 +1825,33 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 
 	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
+		if (!dc)
+			goto err_close;
 
-		err = register_bdev(sb, sb_page, bdev, dc);
+		register_bdev(sb, sb_page, bdev, dc);
 	} else {
 		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
+		if (!ca)
+			goto err_close;
 
-		err = register_cache(sb, sb_page, bdev, ca);
+		register_cache(sb, sb_page, bdev, ca);
 	}
-
-	if (err) {
-		/* register_(bdev|cache) will only return an error if they
-		 * didn't get far enough to create the kobject - if they did,
-		 * the kobject destructor will do this cleanup.
-		 */
+out:
+	if (sb_page)
 		put_page(sb_page);
-err_close:
-		blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
-err:
-		if (attr != &ksysfs_register_quiet)
-			pr_info("error opening %s: %s", path, err);
-		ret = -EINVAL;
-	}
-
 	kfree(sb);
 	kfree(path);
 	mutex_unlock(&bch_register_lock);
 	module_put(THIS_MODULE);
 	return ret;
+
+err_close:
+	blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
+err:
+	if (attr != &ksysfs_register_quiet)
+		pr_info("error opening %s: %s", path, err);
+	ret = -EINVAL;
+	goto out;
 }
 
 static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)

commit 867e1162068eb5632c829d453fd65d6089564f55
Author: Emil Goode <emilgoode@gmail.com>
Date:   Thu May 9 22:39:26 2013 +0200

    bcache: Fix incompatible pointer type warning
    
    The function pointer release in struct block_device_operations
    should point to functions declared as void.
    
    Sparse warnings:
    
    drivers/md/bcache/super.c:656:27: warning:
            incorrect type in initializer (different base types)
            drivers/md/bcache/super.c:656:27:
            expected void ( *release )( ... )
            drivers/md/bcache/super.c:656:27:
            got int ( static [toplevel] *<noident> )( ... )
    
    drivers/md/bcache/super.c:656:2: warning:
            initialization from incompatible pointer type [enabled by default]
    
    drivers/md/bcache/super.c:656:2: warning:
            (near initialization for bcache_ops.release) [enabled by default]
    
    Signed-off-by: Emil Goode <emilgoode@gmail.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index c8046bc4aa57..b09beb2b52c7 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -634,11 +634,10 @@ static int open_dev(struct block_device *b, fmode_t mode)
 	return 0;
 }
 
-static int release_dev(struct gendisk *b, fmode_t mode)
+static void release_dev(struct gendisk *b, fmode_t mode)
 {
 	struct bcache_device *d = b->private_data;
 	closure_put(&d->cl);
-	return 0;
 }
 
 static int ioctl_dev(struct block_device *b, fmode_t mode,

commit ee66850642efda91d04179cae2414310675a1f73
Author: Kent Overstreet <koverstreet@google.com>
Date:   Fri Feb 1 07:29:41 2013 -0800

    bcache: Use bd_link_disk_holder()
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index b3601476f7dd..c8046bc4aa57 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -661,6 +661,35 @@ void bcache_device_stop(struct bcache_device *d)
 		closure_queue(&d->cl);
 }
 
+static void bcache_device_unlink(struct bcache_device *d)
+{
+	unsigned i;
+	struct cache *ca;
+
+	sysfs_remove_link(&d->c->kobj, d->name);
+	sysfs_remove_link(&d->kobj, "cache");
+
+	for_each_cache(ca, d->c, i)
+		bd_unlink_disk_holder(ca->bdev, d->disk);
+}
+
+static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
+			       const char *name)
+{
+	unsigned i;
+	struct cache *ca;
+
+	for_each_cache(ca, d->c, i)
+		bd_link_disk_holder(ca->bdev, d->disk);
+
+	snprintf(d->name, BCACHEDEVNAME_SIZE,
+		 "%s%u", name, d->id);
+
+	WARN(sysfs_create_link(&d->kobj, &c->kobj, "cache") ||
+	     sysfs_create_link(&c->kobj, &d->kobj, d->name),
+	     "Couldn't create device <-> cache set symlinks");
+}
+
 static void bcache_device_detach(struct bcache_device *d)
 {
 	lockdep_assert_held(&bch_register_lock);
@@ -676,6 +705,8 @@ static void bcache_device_detach(struct bcache_device *d)
 		atomic_set(&d->detaching, 0);
 	}
 
+	bcache_device_unlink(d);
+
 	d->c->devices[d->id] = NULL;
 	closure_put(&d->c->caching);
 	d->c = NULL;
@@ -693,17 +724,6 @@ static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
 	closure_get(&c->caching);
 }
 
-static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
-			       const char *name)
-{
-	snprintf(d->name, BCACHEDEVNAME_SIZE,
-		 "%s%u", name, d->id);
-
-	WARN(sysfs_create_link(&d->kobj, &c->kobj, "cache") ||
-	     sysfs_create_link(&c->kobj, &d->kobj, d->name),
-	     "Couldn't create device <-> cache set symlinks");
-}
-
 static void bcache_device_free(struct bcache_device *d)
 {
 	lockdep_assert_held(&bch_register_lock);
@@ -804,6 +824,7 @@ void bch_cached_dev_run(struct cached_dev *dc)
 	}
 
 	add_disk(d->disk);
+	bd_link_disk_holder(dc->bdev, dc->disk.disk);
 #if 0
 	char *env[] = { "SYMLINK=label" , NULL };
 	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
@@ -823,9 +844,6 @@ static void cached_dev_detach_finish(struct work_struct *w)
 	BUG_ON(!atomic_read(&dc->disk.detaching));
 	BUG_ON(atomic_read(&dc->count));
 
-	sysfs_remove_link(&dc->disk.c->kobj, dc->disk.name);
-	sysfs_remove_link(&dc->disk.kobj, "cache");
-
 	mutex_lock(&bch_register_lock);
 
 	memset(&dc->sb.set_uuid, 0, 16);
@@ -940,7 +958,6 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	}
 
 	bcache_device_attach(&dc->disk, c, u - c->uuids);
-	bcache_device_link(&dc->disk, c, "bdev");
 	list_move(&dc->list, &c->cached_devs);
 	calc_cached_dev_sectors(c);
 
@@ -958,6 +975,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	}
 
 	bch_cached_dev_run(dc);
+	bcache_device_link(&dc->disk, c, "bdev");
 
 	pr_info("Caching %s as %s on set %pU",
 		bdevname(dc->bdev, buf), dc->disk.disk->disk_name,
@@ -981,6 +999,7 @@ static void cached_dev_free(struct closure *cl)
 
 	mutex_lock(&bch_register_lock);
 
+	bd_unlink_disk_holder(dc->bdev, dc->disk.disk);
 	bcache_device_free(&dc->disk);
 	list_del(&dc->list);
 
@@ -1123,8 +1142,7 @@ static void flash_dev_flush(struct closure *cl)
 {
 	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
 
-	sysfs_remove_link(&d->c->kobj, d->name);
-	sysfs_remove_link(&d->kobj, "cache");
+	bcache_device_unlink(d);
 	kobject_del(&d->kobj);
 	continue_at(cl, flash_dev_free, system_wq);
 }

commit 8abb2a5dbadab91f9cae22270054e9ee3b3a1d2f
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue Apr 23 21:51:48 2013 -0700

    bcache: Make sure blocksize isn't smaller than device blocksize
    
    Sanity check to make sure we don't end up doing IO the device doesn't
    support.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 6817ea46cf0a..b3601476f7dd 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -139,13 +139,17 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	if (bch_is_zero(sb->uuid, 16))
 		goto err;
 
+	sb->block_size	= le16_to_cpu(s->block_size);
+
+	err = "Superblock block size smaller than device block size";
+	if (sb->block_size << 9 < bdev_logical_block_size(bdev))
+		goto err;
+
 	switch (sb->version) {
 	case BCACHE_SB_VERSION_BDEV:
-		sb->block_size	= le16_to_cpu(s->block_size);
 		sb->data_offset	= BDEV_DATA_START_DEFAULT;
 		break;
 	case BCACHE_SB_VERSION_BDEV_WITH_OFFSET:
-		sb->block_size	= le16_to_cpu(s->block_size);
 		sb->data_offset	= le64_to_cpu(s->data_offset);
 
 		err = "Bad data offset";

commit 4f0fd955cd13bd94166099db7b31065479d884ed
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Mar 27 11:09:23 2013 -0700

    bcache: Set ra_pages based on backing device's ra_pages
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f1e69f2fad37..6817ea46cf0a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1067,6 +1067,10 @@ static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	set_capacity(g, dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
 
+	g->queue->backing_dev_info.ra_pages =
+		max(g->queue->backing_dev_info.ra_pages,
+		    bdev->bd_queue->backing_dev_info.ra_pages);
+
 	bch_cached_dev_request_init(dc);
 
 	err = "error creating kobject";

commit 2903381fce71004a7ce24d40fad53ba8236a3921
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Apr 11 15:14:35 2013 -0700

    bcache: Take data offset from the bdev superblock.
    
    Add a new superblock version, and consolidate related defines.
    
    Signed-off-by: Gabriel de Perthuis <g2p.code+bcache@gmail.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 5fa3cd2d9ff0..f1e69f2fad37 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -110,15 +110,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 
 	sb->flags		= le64_to_cpu(s->flags);
 	sb->seq			= le64_to_cpu(s->seq);
-
-	sb->nbuckets		= le64_to_cpu(s->nbuckets);
-	sb->block_size		= le16_to_cpu(s->block_size);
-	sb->bucket_size		= le16_to_cpu(s->bucket_size);
-
-	sb->nr_in_set		= le16_to_cpu(s->nr_in_set);
-	sb->nr_this_dev		= le16_to_cpu(s->nr_this_dev);
 	sb->last_mount		= le32_to_cpu(s->last_mount);
-
 	sb->first_bucket	= le16_to_cpu(s->first_bucket);
 	sb->keys		= le16_to_cpu(s->keys);
 
@@ -147,53 +139,77 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 	if (bch_is_zero(sb->uuid, 16))
 		goto err;
 
-	err = "Unsupported superblock version";
-	if (sb->version > BCACHE_SB_VERSION)
-		goto err;
+	switch (sb->version) {
+	case BCACHE_SB_VERSION_BDEV:
+		sb->block_size	= le16_to_cpu(s->block_size);
+		sb->data_offset	= BDEV_DATA_START_DEFAULT;
+		break;
+	case BCACHE_SB_VERSION_BDEV_WITH_OFFSET:
+		sb->block_size	= le16_to_cpu(s->block_size);
+		sb->data_offset	= le64_to_cpu(s->data_offset);
+
+		err = "Bad data offset";
+		if (sb->data_offset < BDEV_DATA_START_DEFAULT)
+			goto err;
 
-	err = "Bad block/bucket size";
-	if (!is_power_of_2(sb->block_size) || sb->block_size > PAGE_SECTORS ||
-	    !is_power_of_2(sb->bucket_size) || sb->bucket_size < PAGE_SECTORS)
-		goto err;
+		break;
+	case BCACHE_SB_VERSION_CDEV:
+	case BCACHE_SB_VERSION_CDEV_WITH_UUID:
+		sb->nbuckets	= le64_to_cpu(s->nbuckets);
+		sb->block_size	= le16_to_cpu(s->block_size);
+		sb->bucket_size	= le16_to_cpu(s->bucket_size);
 
-	err = "Too many buckets";
-	if (sb->nbuckets > LONG_MAX)
-		goto err;
+		sb->nr_in_set	= le16_to_cpu(s->nr_in_set);
+		sb->nr_this_dev	= le16_to_cpu(s->nr_this_dev);
 
-	err = "Not enough buckets";
-	if (sb->nbuckets < 1 << 7)
-		goto err;
+		err = "Too many buckets";
+		if (sb->nbuckets > LONG_MAX)
+			goto err;
 
-	err = "Invalid superblock: device too small";
-	if (get_capacity(bdev->bd_disk) < sb->bucket_size * sb->nbuckets)
-		goto err;
+		err = "Not enough buckets";
+		if (sb->nbuckets < 1 << 7)
+			goto err;
 
-	if (sb->version == CACHE_BACKING_DEV)
-		goto out;
+		err = "Bad block/bucket size";
+		if (!is_power_of_2(sb->block_size) ||
+		    sb->block_size > PAGE_SECTORS ||
+		    !is_power_of_2(sb->bucket_size) ||
+		    sb->bucket_size < PAGE_SECTORS)
+			goto err;
 
-	err = "Bad UUID";
-	if (bch_is_zero(sb->set_uuid, 16))
-		goto err;
+		err = "Invalid superblock: device too small";
+		if (get_capacity(bdev->bd_disk) < sb->bucket_size * sb->nbuckets)
+			goto err;
 
-	err = "Bad cache device number in set";
-	if (!sb->nr_in_set ||
-	    sb->nr_in_set <= sb->nr_this_dev ||
-	    sb->nr_in_set > MAX_CACHES_PER_SET)
-		goto err;
+		err = "Bad UUID";
+		if (bch_is_zero(sb->set_uuid, 16))
+			goto err;
 
-	err = "Journal buckets not sequential";
-	for (i = 0; i < sb->keys; i++)
-		if (sb->d[i] != sb->first_bucket + i)
+		err = "Bad cache device number in set";
+		if (!sb->nr_in_set ||
+		    sb->nr_in_set <= sb->nr_this_dev ||
+		    sb->nr_in_set > MAX_CACHES_PER_SET)
 			goto err;
 
-	err = "Too many journal buckets";
-	if (sb->first_bucket + sb->keys > sb->nbuckets)
-		goto err;
+		err = "Journal buckets not sequential";
+		for (i = 0; i < sb->keys; i++)
+			if (sb->d[i] != sb->first_bucket + i)
+				goto err;
 
-	err = "Invalid superblock: first bucket comes before end of super";
-	if (sb->first_bucket * sb->bucket_size < 16)
+		err = "Too many journal buckets";
+		if (sb->first_bucket + sb->keys > sb->nbuckets)
+			goto err;
+
+		err = "Invalid superblock: first bucket comes before end of super";
+		if (sb->first_bucket * sb->bucket_size < 16)
+			goto err;
+
+		break;
+	default:
+		err = "Unsupported superblock version";
 		goto err;
-out:
+	}
+
 	sb->last_mount = get_seconds();
 	err = NULL;
 
@@ -286,7 +302,7 @@ void bcache_write_super(struct cache_set *c)
 	for_each_cache(ca, c, i) {
 		struct bio *bio = &ca->sb_bio;
 
-		ca->sb.version		= BCACHE_SB_VERSION;
+		ca->sb.version		= BCACHE_SB_VERSION_CDEV_WITH_UUID;
 		ca->sb.seq		= c->sb.seq;
 		ca->sb.last_mount	= c->sb.last_mount;
 
@@ -1049,7 +1065,7 @@ static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
 
 	g = dc->disk.disk;
 
-	set_capacity(g, dc->bdev->bd_part->nr_sects - 16);
+	set_capacity(g, dc->bdev->bd_part->nr_sects - dc->sb.data_offset);
 
 	bch_cached_dev_request_init(dc);
 
@@ -1802,7 +1818,7 @@ static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
 	if (err)
 		goto err_close;
 
-	if (sb->version == CACHE_BACKING_DEV) {
+	if (SB_IS_BDEV(sb)) {
 		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
 
 		err = register_bdev(sb, sb_page, bdev, dc);

commit 8ef747909cef0dd1ae018beef4bb2e75f26e3f98
Author: Kent Overstreet <koverstreet@google.com>
Date:   Fri Apr 5 13:46:13 2013 -0700

    bcache: Fix a minor memory leak on device teardown
    
    Reported-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index f47ecb5cb318..5fa3cd2d9ff0 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -64,9 +64,11 @@ struct workqueue_struct *bcache_wq;
 
 static void bio_split_pool_free(struct bio_split_pool *p)
 {
+	if (p->bio_split_hook)
+		mempool_destroy(p->bio_split_hook);
+
 	if (p->bio_split)
 		bioset_free(p->bio_split);
-
 }
 
 static int bio_split_pool_init(struct bio_split_pool *p)

commit 169ef1cf6171d35550fef85645b83b960e241cff
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Mar 28 12:50:55 2013 -0600

    bcache: Don't export utility code, prefix with bch_
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 484ae6c8f43a..f47ecb5cb318 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -142,7 +142,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 		goto err;
 
 	err = "Bad UUID";
-	if (is_zero(sb->uuid, 16))
+	if (bch_is_zero(sb->uuid, 16))
 		goto err;
 
 	err = "Unsupported superblock version";
@@ -170,7 +170,7 @@ static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
 		goto out;
 
 	err = "Bad UUID";
-	if (is_zero(sb->set_uuid, 16))
+	if (bch_is_zero(sb->set_uuid, 16))
 		goto err;
 
 	err = "Bad cache device number in set";
@@ -218,7 +218,7 @@ static void __write_super(struct cache_sb *sb, struct bio *bio)
 	bio->bi_sector	= SB_SECTOR;
 	bio->bi_rw	= REQ_SYNC|REQ_META;
 	bio->bi_size	= SB_SIZE;
-	bio_map(bio, NULL);
+	bch_bio_map(bio, NULL);
 
 	out->offset		= cpu_to_le64(sb->offset);
 	out->version		= cpu_to_le64(sb->version);
@@ -332,7 +332,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 
 		bio->bi_end_io	= uuid_endio;
 		bio->bi_private = cl;
-		bio_map(bio, c->uuids);
+		bch_bio_map(bio, c->uuids);
 
 		bch_submit_bbio(bio, c, k, i);
 
@@ -344,7 +344,7 @@ static void uuid_io(struct cache_set *c, unsigned long rw,
 		 pkey(&c->uuid_bucket));
 
 	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)
-		if (!is_zero(u->uuid, 16))
+		if (!bch_is_zero(u->uuid, 16))
 			pr_debug("Slot %zi: %pU: %s: 1st: %u last: %u inv: %u",
 				 u - c->uuids, u->uuid, u->label,
 				 u->first_reg, u->last_reg, u->invalidated);
@@ -491,7 +491,7 @@ static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
 
 	bio->bi_end_io	= prio_endio;
 	bio->bi_private = ca;
-	bio_map(bio, ca->disk_buckets);
+	bch_bio_map(bio, ca->disk_buckets);
 
 	closure_bio_submit(bio, &ca->prio, ca);
 	closure_sync(cl);
@@ -538,7 +538,7 @@ void bch_prio_write(struct cache *ca)
 
 		p->next_bucket	= ca->prio_buckets[i + 1];
 		p->magic	= pset_magic(ca);
-		p->csum		= crc64(&p->magic, bucket_bytes(ca) - 8);
+		p->csum		= bch_crc64(&p->magic, bucket_bytes(ca) - 8);
 
 		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, &cl);
 		BUG_ON(bucket == -1);
@@ -585,7 +585,7 @@ static void prio_read(struct cache *ca, uint64_t bucket)
 
 			prio_io(ca, bucket, READ_SYNC);
 
-			if (p->csum != crc64(&p->magic, bucket_bytes(ca) - 8))
+			if (p->csum != bch_crc64(&p->magic, bucket_bytes(ca) - 8))
 				pr_warn("bad csum reading priorities");
 
 			if (p->magic != pset_magic(ca))
@@ -898,7 +898,7 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 	sysfs_remove_file(&dc->kobj, &sysfs_attach);
 	 */
 
-	if (is_zero(u->uuid, 16)) {
+	if (bch_is_zero(u->uuid, 16)) {
 		struct closure cl;
 		closure_init_stack(&cl);
 

commit b1a67b0f4c747ca10c96ebb24f04e2a74b3c298d
Author: Kent Overstreet <koverstreet@google.com>
Date:   Mon Mar 25 11:46:44 2013 -0700

    bcache: Style/checkpatch fixes
    
    Took out some nested functions, and fixed some more checkpatch
    complaints.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 7b8efc770871..484ae6c8f43a 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -526,7 +526,8 @@ void bch_prio_write(struct cache *ca)
 	for (i = prio_buckets(ca) - 1; i >= 0; --i) {
 		long bucket;
 		struct prio_set *p = ca->disk_buckets;
-		struct bucket_disk *d = p->data, *end = d + prios_per_bucket(ca);
+		struct bucket_disk *d = p->data;
+		struct bucket_disk *end = d + prios_per_bucket(ca);
 
 		for (b = ca->buckets + i * prios_per_bucket(ca);
 		     b < ca->buckets + ca->sb.nbuckets && d < end;
@@ -865,8 +866,8 @@ int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
 
 	if (dc->sb.block_size < c->sb.block_size) {
 		/* Will die */
-		pr_err("Couldn't attach %s: block size "
-		       "less than set's block size", buf);
+		pr_err("Couldn't attach %s: block size less than set's block size",
+		       buf);
 		return -EINVAL;
 	}
 

commit 07e86ccb543bb1e748f32d6f0f18913d3f58d988
Author: Kent Overstreet <koverstreet@google.com>
Date:   Mon Mar 25 11:46:43 2013 -0700

    bcache: Build fixes from test robot
    
    config: make ARCH=i386 allmodconfig
    
    All error/warnings:
    
       drivers/md/bcache/bset.c: In function 'bch_ptr_bad':
    >> drivers/md/bcache/bset.c:164:2: warning: format '%li' expects argument of type 'long int', but argument 4 has type 'size_t' [-Wformat]
    --
       drivers/md/bcache/debug.c: In function 'bch_pbtree':
    >> drivers/md/bcache/debug.c:86:4: warning: format '%li' expects argument of type 'long int', but argument 4 has type 'size_t' [-Wformat]
    --
       drivers/md/bcache/btree.c: In function 'bch_btree_read_done':
    >> drivers/md/bcache/btree.c:245:8: warning: format '%lu' expects argument of type 'long unsigned int', but argument 4 has type 'size_t' [-Wformat]
    --
       drivers/md/bcache/closure.o: In function `closure_debug_init':
    >> (.init.text+0x0): multiple definition of `init_module'
    >> drivers/md/bcache/super.o:super.c:(.init.text+0x0): first defined here
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
index 31ef47f1f3b6..7b8efc770871 100644
--- a/drivers/md/bcache/super.c
+++ b/drivers/md/bcache/super.c
@@ -1917,6 +1917,7 @@ static int __init bcache_init(void)
 	mutex_init(&bch_register_lock);
 	init_waitqueue_head(&unregister_wait);
 	register_reboot_notifier(&reboot);
+	closure_debug_init();
 
 	bcache_major = register_blkdev(0, "bcache");
 	if (bcache_major < 0)

commit cafe563591446cf80bfbc2fe3bc72a2e36cf1060
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat Mar 23 16:11:31 2013 -0700

    bcache: A block layer cache
    
    Does writethrough and writeback caching, handles unclean shutdown, and
    has a bunch of other nifty features motivated by real world usage.
    
    See the wiki at http://bcache.evilpiepirate.org for more.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/super.c b/drivers/md/bcache/super.c
new file mode 100644
index 000000000000..31ef47f1f3b6
--- /dev/null
+++ b/drivers/md/bcache/super.c
@@ -0,0 +1,1941 @@
+/*
+ * bcache setup/teardown code, and some metadata io - read a superblock and
+ * figure out what to do with it.
+ *
+ * Copyright 2010, 2011 Kent Overstreet <kent.overstreet@gmail.com>
+ * Copyright 2012 Google, Inc.
+ */
+
+#include "bcache.h"
+#include "btree.h"
+#include "debug.h"
+#include "request.h"
+
+#include <linux/buffer_head.h>
+#include <linux/debugfs.h>
+#include <linux/genhd.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/reboot.h>
+#include <linux/sysfs.h>
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Kent Overstreet <kent.overstreet@gmail.com>");
+
+static const char bcache_magic[] = {
+	0xc6, 0x85, 0x73, 0xf6, 0x4e, 0x1a, 0x45, 0xca,
+	0x82, 0x65, 0xf5, 0x7f, 0x48, 0xba, 0x6d, 0x81
+};
+
+static const char invalid_uuid[] = {
+	0xa0, 0x3e, 0xf8, 0xed, 0x3e, 0xe1, 0xb8, 0x78,
+	0xc8, 0x50, 0xfc, 0x5e, 0xcb, 0x16, 0xcd, 0x99
+};
+
+/* Default is -1; we skip past it for struct cached_dev's cache mode */
+const char * const bch_cache_modes[] = {
+	"default",
+	"writethrough",
+	"writeback",
+	"writearound",
+	"none",
+	NULL
+};
+
+struct uuid_entry_v0 {
+	uint8_t		uuid[16];
+	uint8_t		label[32];
+	uint32_t	first_reg;
+	uint32_t	last_reg;
+	uint32_t	invalidated;
+	uint32_t	pad;
+};
+
+static struct kobject *bcache_kobj;
+struct mutex bch_register_lock;
+LIST_HEAD(bch_cache_sets);
+static LIST_HEAD(uncached_devices);
+
+static int bcache_major, bcache_minor;
+static wait_queue_head_t unregister_wait;
+struct workqueue_struct *bcache_wq;
+
+#define BTREE_MAX_PAGES		(256 * 1024 / PAGE_SIZE)
+
+static void bio_split_pool_free(struct bio_split_pool *p)
+{
+	if (p->bio_split)
+		bioset_free(p->bio_split);
+
+}
+
+static int bio_split_pool_init(struct bio_split_pool *p)
+{
+	p->bio_split = bioset_create(4, 0);
+	if (!p->bio_split)
+		return -ENOMEM;
+
+	p->bio_split_hook = mempool_create_kmalloc_pool(4,
+				sizeof(struct bio_split_hook));
+	if (!p->bio_split_hook)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/* Superblock */
+
+static const char *read_super(struct cache_sb *sb, struct block_device *bdev,
+			      struct page **res)
+{
+	const char *err;
+	struct cache_sb *s;
+	struct buffer_head *bh = __bread(bdev, 1, SB_SIZE);
+	unsigned i;
+
+	if (!bh)
+		return "IO error";
+
+	s = (struct cache_sb *) bh->b_data;
+
+	sb->offset		= le64_to_cpu(s->offset);
+	sb->version		= le64_to_cpu(s->version);
+
+	memcpy(sb->magic,	s->magic, 16);
+	memcpy(sb->uuid,	s->uuid, 16);
+	memcpy(sb->set_uuid,	s->set_uuid, 16);
+	memcpy(sb->label,	s->label, SB_LABEL_SIZE);
+
+	sb->flags		= le64_to_cpu(s->flags);
+	sb->seq			= le64_to_cpu(s->seq);
+
+	sb->nbuckets		= le64_to_cpu(s->nbuckets);
+	sb->block_size		= le16_to_cpu(s->block_size);
+	sb->bucket_size		= le16_to_cpu(s->bucket_size);
+
+	sb->nr_in_set		= le16_to_cpu(s->nr_in_set);
+	sb->nr_this_dev		= le16_to_cpu(s->nr_this_dev);
+	sb->last_mount		= le32_to_cpu(s->last_mount);
+
+	sb->first_bucket	= le16_to_cpu(s->first_bucket);
+	sb->keys		= le16_to_cpu(s->keys);
+
+	for (i = 0; i < SB_JOURNAL_BUCKETS; i++)
+		sb->d[i] = le64_to_cpu(s->d[i]);
+
+	pr_debug("read sb version %llu, flags %llu, seq %llu, journal size %u",
+		 sb->version, sb->flags, sb->seq, sb->keys);
+
+	err = "Not a bcache superblock";
+	if (sb->offset != SB_SECTOR)
+		goto err;
+
+	if (memcmp(sb->magic, bcache_magic, 16))
+		goto err;
+
+	err = "Too many journal buckets";
+	if (sb->keys > SB_JOURNAL_BUCKETS)
+		goto err;
+
+	err = "Bad checksum";
+	if (s->csum != csum_set(s))
+		goto err;
+
+	err = "Bad UUID";
+	if (is_zero(sb->uuid, 16))
+		goto err;
+
+	err = "Unsupported superblock version";
+	if (sb->version > BCACHE_SB_VERSION)
+		goto err;
+
+	err = "Bad block/bucket size";
+	if (!is_power_of_2(sb->block_size) || sb->block_size > PAGE_SECTORS ||
+	    !is_power_of_2(sb->bucket_size) || sb->bucket_size < PAGE_SECTORS)
+		goto err;
+
+	err = "Too many buckets";
+	if (sb->nbuckets > LONG_MAX)
+		goto err;
+
+	err = "Not enough buckets";
+	if (sb->nbuckets < 1 << 7)
+		goto err;
+
+	err = "Invalid superblock: device too small";
+	if (get_capacity(bdev->bd_disk) < sb->bucket_size * sb->nbuckets)
+		goto err;
+
+	if (sb->version == CACHE_BACKING_DEV)
+		goto out;
+
+	err = "Bad UUID";
+	if (is_zero(sb->set_uuid, 16))
+		goto err;
+
+	err = "Bad cache device number in set";
+	if (!sb->nr_in_set ||
+	    sb->nr_in_set <= sb->nr_this_dev ||
+	    sb->nr_in_set > MAX_CACHES_PER_SET)
+		goto err;
+
+	err = "Journal buckets not sequential";
+	for (i = 0; i < sb->keys; i++)
+		if (sb->d[i] != sb->first_bucket + i)
+			goto err;
+
+	err = "Too many journal buckets";
+	if (sb->first_bucket + sb->keys > sb->nbuckets)
+		goto err;
+
+	err = "Invalid superblock: first bucket comes before end of super";
+	if (sb->first_bucket * sb->bucket_size < 16)
+		goto err;
+out:
+	sb->last_mount = get_seconds();
+	err = NULL;
+
+	get_page(bh->b_page);
+	*res = bh->b_page;
+err:
+	put_bh(bh);
+	return err;
+}
+
+static void write_bdev_super_endio(struct bio *bio, int error)
+{
+	struct cached_dev *dc = bio->bi_private;
+	/* XXX: error checking */
+
+	closure_put(&dc->sb_write.cl);
+}
+
+static void __write_super(struct cache_sb *sb, struct bio *bio)
+{
+	struct cache_sb *out = page_address(bio->bi_io_vec[0].bv_page);
+	unsigned i;
+
+	bio->bi_sector	= SB_SECTOR;
+	bio->bi_rw	= REQ_SYNC|REQ_META;
+	bio->bi_size	= SB_SIZE;
+	bio_map(bio, NULL);
+
+	out->offset		= cpu_to_le64(sb->offset);
+	out->version		= cpu_to_le64(sb->version);
+
+	memcpy(out->uuid,	sb->uuid, 16);
+	memcpy(out->set_uuid,	sb->set_uuid, 16);
+	memcpy(out->label,	sb->label, SB_LABEL_SIZE);
+
+	out->flags		= cpu_to_le64(sb->flags);
+	out->seq		= cpu_to_le64(sb->seq);
+
+	out->last_mount		= cpu_to_le32(sb->last_mount);
+	out->first_bucket	= cpu_to_le16(sb->first_bucket);
+	out->keys		= cpu_to_le16(sb->keys);
+
+	for (i = 0; i < sb->keys; i++)
+		out->d[i] = cpu_to_le64(sb->d[i]);
+
+	out->csum = csum_set(out);
+
+	pr_debug("ver %llu, flags %llu, seq %llu",
+		 sb->version, sb->flags, sb->seq);
+
+	submit_bio(REQ_WRITE, bio);
+}
+
+void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent)
+{
+	struct closure *cl = &dc->sb_write.cl;
+	struct bio *bio = &dc->sb_bio;
+
+	closure_lock(&dc->sb_write, parent);
+
+	bio_reset(bio);
+	bio->bi_bdev	= dc->bdev;
+	bio->bi_end_io	= write_bdev_super_endio;
+	bio->bi_private = dc;
+
+	closure_get(cl);
+	__write_super(&dc->sb, bio);
+
+	closure_return(cl);
+}
+
+static void write_super_endio(struct bio *bio, int error)
+{
+	struct cache *ca = bio->bi_private;
+
+	bch_count_io_errors(ca, error, "writing superblock");
+	closure_put(&ca->set->sb_write.cl);
+}
+
+void bcache_write_super(struct cache_set *c)
+{
+	struct closure *cl = &c->sb_write.cl;
+	struct cache *ca;
+	unsigned i;
+
+	closure_lock(&c->sb_write, &c->cl);
+
+	c->sb.seq++;
+
+	for_each_cache(ca, c, i) {
+		struct bio *bio = &ca->sb_bio;
+
+		ca->sb.version		= BCACHE_SB_VERSION;
+		ca->sb.seq		= c->sb.seq;
+		ca->sb.last_mount	= c->sb.last_mount;
+
+		SET_CACHE_SYNC(&ca->sb, CACHE_SYNC(&c->sb));
+
+		bio_reset(bio);
+		bio->bi_bdev	= ca->bdev;
+		bio->bi_end_io	= write_super_endio;
+		bio->bi_private = ca;
+
+		closure_get(cl);
+		__write_super(&ca->sb, bio);
+	}
+
+	closure_return(cl);
+}
+
+/* UUID io */
+
+static void uuid_endio(struct bio *bio, int error)
+{
+	struct closure *cl = bio->bi_private;
+	struct cache_set *c = container_of(cl, struct cache_set, uuid_write.cl);
+
+	cache_set_err_on(error, c, "accessing uuids");
+	bch_bbio_free(bio, c);
+	closure_put(cl);
+}
+
+static void uuid_io(struct cache_set *c, unsigned long rw,
+		    struct bkey *k, struct closure *parent)
+{
+	struct closure *cl = &c->uuid_write.cl;
+	struct uuid_entry *u;
+	unsigned i;
+
+	BUG_ON(!parent);
+	closure_lock(&c->uuid_write, parent);
+
+	for (i = 0; i < KEY_PTRS(k); i++) {
+		struct bio *bio = bch_bbio_alloc(c);
+
+		bio->bi_rw	= REQ_SYNC|REQ_META|rw;
+		bio->bi_size	= KEY_SIZE(k) << 9;
+
+		bio->bi_end_io	= uuid_endio;
+		bio->bi_private = cl;
+		bio_map(bio, c->uuids);
+
+		bch_submit_bbio(bio, c, k, i);
+
+		if (!(rw & WRITE))
+			break;
+	}
+
+	pr_debug("%s UUIDs at %s", rw & REQ_WRITE ? "wrote" : "read",
+		 pkey(&c->uuid_bucket));
+
+	for (u = c->uuids; u < c->uuids + c->nr_uuids; u++)
+		if (!is_zero(u->uuid, 16))
+			pr_debug("Slot %zi: %pU: %s: 1st: %u last: %u inv: %u",
+				 u - c->uuids, u->uuid, u->label,
+				 u->first_reg, u->last_reg, u->invalidated);
+
+	closure_return(cl);
+}
+
+static char *uuid_read(struct cache_set *c, struct jset *j, struct closure *cl)
+{
+	struct bkey *k = &j->uuid_bucket;
+
+	if (__bch_ptr_invalid(c, 1, k))
+		return "bad uuid pointer";
+
+	bkey_copy(&c->uuid_bucket, k);
+	uuid_io(c, READ_SYNC, k, cl);
+
+	if (j->version < BCACHE_JSET_VERSION_UUIDv1) {
+		struct uuid_entry_v0	*u0 = (void *) c->uuids;
+		struct uuid_entry	*u1 = (void *) c->uuids;
+		int i;
+
+		closure_sync(cl);
+
+		/*
+		 * Since the new uuid entry is bigger than the old, we have to
+		 * convert starting at the highest memory address and work down
+		 * in order to do it in place
+		 */
+
+		for (i = c->nr_uuids - 1;
+		     i >= 0;
+		     --i) {
+			memcpy(u1[i].uuid,	u0[i].uuid, 16);
+			memcpy(u1[i].label,	u0[i].label, 32);
+
+			u1[i].first_reg		= u0[i].first_reg;
+			u1[i].last_reg		= u0[i].last_reg;
+			u1[i].invalidated	= u0[i].invalidated;
+
+			u1[i].flags	= 0;
+			u1[i].sectors	= 0;
+		}
+	}
+
+	return NULL;
+}
+
+static int __uuid_write(struct cache_set *c)
+{
+	BKEY_PADDED(key) k;
+	struct closure cl;
+	closure_init_stack(&cl);
+
+	lockdep_assert_held(&bch_register_lock);
+
+	if (bch_bucket_alloc_set(c, WATERMARK_METADATA, &k.key, 1, &cl))
+		return 1;
+
+	SET_KEY_SIZE(&k.key, c->sb.bucket_size);
+	uuid_io(c, REQ_WRITE, &k.key, &cl);
+	closure_sync(&cl);
+
+	bkey_copy(&c->uuid_bucket, &k.key);
+	__bkey_put(c, &k.key);
+	return 0;
+}
+
+int bch_uuid_write(struct cache_set *c)
+{
+	int ret = __uuid_write(c);
+
+	if (!ret)
+		bch_journal_meta(c, NULL);
+
+	return ret;
+}
+
+static struct uuid_entry *uuid_find(struct cache_set *c, const char *uuid)
+{
+	struct uuid_entry *u;
+
+	for (u = c->uuids;
+	     u < c->uuids + c->nr_uuids; u++)
+		if (!memcmp(u->uuid, uuid, 16))
+			return u;
+
+	return NULL;
+}
+
+static struct uuid_entry *uuid_find_empty(struct cache_set *c)
+{
+	static const char zero_uuid[16] = "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0";
+	return uuid_find(c, zero_uuid);
+}
+
+/*
+ * Bucket priorities/gens:
+ *
+ * For each bucket, we store on disk its
+   * 8 bit gen
+   * 16 bit priority
+ *
+ * See alloc.c for an explanation of the gen. The priority is used to implement
+ * lru (and in the future other) cache replacement policies; for most purposes
+ * it's just an opaque integer.
+ *
+ * The gens and the priorities don't have a whole lot to do with each other, and
+ * it's actually the gens that must be written out at specific times - it's no
+ * big deal if the priorities don't get written, if we lose them we just reuse
+ * buckets in suboptimal order.
+ *
+ * On disk they're stored in a packed array, and in as many buckets are required
+ * to fit them all. The buckets we use to store them form a list; the journal
+ * header points to the first bucket, the first bucket points to the second
+ * bucket, et cetera.
+ *
+ * This code is used by the allocation code; periodically (whenever it runs out
+ * of buckets to allocate from) the allocation code will invalidate some
+ * buckets, but it can't use those buckets until their new gens are safely on
+ * disk.
+ */
+
+static void prio_endio(struct bio *bio, int error)
+{
+	struct cache *ca = bio->bi_private;
+
+	cache_set_err_on(error, ca->set, "accessing priorities");
+	bch_bbio_free(bio, ca->set);
+	closure_put(&ca->prio);
+}
+
+static void prio_io(struct cache *ca, uint64_t bucket, unsigned long rw)
+{
+	struct closure *cl = &ca->prio;
+	struct bio *bio = bch_bbio_alloc(ca->set);
+
+	closure_init_stack(cl);
+
+	bio->bi_sector	= bucket * ca->sb.bucket_size;
+	bio->bi_bdev	= ca->bdev;
+	bio->bi_rw	= REQ_SYNC|REQ_META|rw;
+	bio->bi_size	= bucket_bytes(ca);
+
+	bio->bi_end_io	= prio_endio;
+	bio->bi_private = ca;
+	bio_map(bio, ca->disk_buckets);
+
+	closure_bio_submit(bio, &ca->prio, ca);
+	closure_sync(cl);
+}
+
+#define buckets_free(c)	"free %zu, free_inc %zu, unused %zu",		\
+	fifo_used(&c->free), fifo_used(&c->free_inc), fifo_used(&c->unused)
+
+void bch_prio_write(struct cache *ca)
+{
+	int i;
+	struct bucket *b;
+	struct closure cl;
+
+	closure_init_stack(&cl);
+
+	lockdep_assert_held(&ca->set->bucket_lock);
+
+	for (b = ca->buckets;
+	     b < ca->buckets + ca->sb.nbuckets; b++)
+		b->disk_gen = b->gen;
+
+	ca->disk_buckets->seq++;
+
+	atomic_long_add(ca->sb.bucket_size * prio_buckets(ca),
+			&ca->meta_sectors_written);
+
+	pr_debug("free %zu, free_inc %zu, unused %zu", fifo_used(&ca->free),
+		 fifo_used(&ca->free_inc), fifo_used(&ca->unused));
+	blktrace_msg(ca, "Starting priorities: " buckets_free(ca));
+
+	for (i = prio_buckets(ca) - 1; i >= 0; --i) {
+		long bucket;
+		struct prio_set *p = ca->disk_buckets;
+		struct bucket_disk *d = p->data, *end = d + prios_per_bucket(ca);
+
+		for (b = ca->buckets + i * prios_per_bucket(ca);
+		     b < ca->buckets + ca->sb.nbuckets && d < end;
+		     b++, d++) {
+			d->prio = cpu_to_le16(b->prio);
+			d->gen = b->gen;
+		}
+
+		p->next_bucket	= ca->prio_buckets[i + 1];
+		p->magic	= pset_magic(ca);
+		p->csum		= crc64(&p->magic, bucket_bytes(ca) - 8);
+
+		bucket = bch_bucket_alloc(ca, WATERMARK_PRIO, &cl);
+		BUG_ON(bucket == -1);
+
+		mutex_unlock(&ca->set->bucket_lock);
+		prio_io(ca, bucket, REQ_WRITE);
+		mutex_lock(&ca->set->bucket_lock);
+
+		ca->prio_buckets[i] = bucket;
+		atomic_dec_bug(&ca->buckets[bucket].pin);
+	}
+
+	mutex_unlock(&ca->set->bucket_lock);
+
+	bch_journal_meta(ca->set, &cl);
+	closure_sync(&cl);
+
+	mutex_lock(&ca->set->bucket_lock);
+
+	ca->need_save_prio = 0;
+
+	/*
+	 * Don't want the old priorities to get garbage collected until after we
+	 * finish writing the new ones, and they're journalled
+	 */
+	for (i = 0; i < prio_buckets(ca); i++)
+		ca->prio_last_buckets[i] = ca->prio_buckets[i];
+}
+
+static void prio_read(struct cache *ca, uint64_t bucket)
+{
+	struct prio_set *p = ca->disk_buckets;
+	struct bucket_disk *d = p->data + prios_per_bucket(ca), *end = d;
+	struct bucket *b;
+	unsigned bucket_nr = 0;
+
+	for (b = ca->buckets;
+	     b < ca->buckets + ca->sb.nbuckets;
+	     b++, d++) {
+		if (d == end) {
+			ca->prio_buckets[bucket_nr] = bucket;
+			ca->prio_last_buckets[bucket_nr] = bucket;
+			bucket_nr++;
+
+			prio_io(ca, bucket, READ_SYNC);
+
+			if (p->csum != crc64(&p->magic, bucket_bytes(ca) - 8))
+				pr_warn("bad csum reading priorities");
+
+			if (p->magic != pset_magic(ca))
+				pr_warn("bad magic reading priorities");
+
+			bucket = p->next_bucket;
+			d = p->data;
+		}
+
+		b->prio = le16_to_cpu(d->prio);
+		b->gen = b->disk_gen = b->last_gc = b->gc_gen = d->gen;
+	}
+}
+
+/* Bcache device */
+
+static int open_dev(struct block_device *b, fmode_t mode)
+{
+	struct bcache_device *d = b->bd_disk->private_data;
+	if (atomic_read(&d->closing))
+		return -ENXIO;
+
+	closure_get(&d->cl);
+	return 0;
+}
+
+static int release_dev(struct gendisk *b, fmode_t mode)
+{
+	struct bcache_device *d = b->private_data;
+	closure_put(&d->cl);
+	return 0;
+}
+
+static int ioctl_dev(struct block_device *b, fmode_t mode,
+		     unsigned int cmd, unsigned long arg)
+{
+	struct bcache_device *d = b->bd_disk->private_data;
+	return d->ioctl(d, mode, cmd, arg);
+}
+
+static const struct block_device_operations bcache_ops = {
+	.open		= open_dev,
+	.release	= release_dev,
+	.ioctl		= ioctl_dev,
+	.owner		= THIS_MODULE,
+};
+
+void bcache_device_stop(struct bcache_device *d)
+{
+	if (!atomic_xchg(&d->closing, 1))
+		closure_queue(&d->cl);
+}
+
+static void bcache_device_detach(struct bcache_device *d)
+{
+	lockdep_assert_held(&bch_register_lock);
+
+	if (atomic_read(&d->detaching)) {
+		struct uuid_entry *u = d->c->uuids + d->id;
+
+		SET_UUID_FLASH_ONLY(u, 0);
+		memcpy(u->uuid, invalid_uuid, 16);
+		u->invalidated = cpu_to_le32(get_seconds());
+		bch_uuid_write(d->c);
+
+		atomic_set(&d->detaching, 0);
+	}
+
+	d->c->devices[d->id] = NULL;
+	closure_put(&d->c->caching);
+	d->c = NULL;
+}
+
+static void bcache_device_attach(struct bcache_device *d, struct cache_set *c,
+				 unsigned id)
+{
+	BUG_ON(test_bit(CACHE_SET_STOPPING, &c->flags));
+
+	d->id = id;
+	d->c = c;
+	c->devices[id] = d;
+
+	closure_get(&c->caching);
+}
+
+static void bcache_device_link(struct bcache_device *d, struct cache_set *c,
+			       const char *name)
+{
+	snprintf(d->name, BCACHEDEVNAME_SIZE,
+		 "%s%u", name, d->id);
+
+	WARN(sysfs_create_link(&d->kobj, &c->kobj, "cache") ||
+	     sysfs_create_link(&c->kobj, &d->kobj, d->name),
+	     "Couldn't create device <-> cache set symlinks");
+}
+
+static void bcache_device_free(struct bcache_device *d)
+{
+	lockdep_assert_held(&bch_register_lock);
+
+	pr_info("%s stopped", d->disk->disk_name);
+
+	if (d->c)
+		bcache_device_detach(d);
+
+	if (d->disk)
+		del_gendisk(d->disk);
+	if (d->disk && d->disk->queue)
+		blk_cleanup_queue(d->disk->queue);
+	if (d->disk)
+		put_disk(d->disk);
+
+	bio_split_pool_free(&d->bio_split_hook);
+	if (d->unaligned_bvec)
+		mempool_destroy(d->unaligned_bvec);
+	if (d->bio_split)
+		bioset_free(d->bio_split);
+
+	closure_debug_destroy(&d->cl);
+}
+
+static int bcache_device_init(struct bcache_device *d, unsigned block_size)
+{
+	struct request_queue *q;
+
+	if (!(d->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
+	    !(d->unaligned_bvec = mempool_create_kmalloc_pool(1,
+				sizeof(struct bio_vec) * BIO_MAX_PAGES)) ||
+	    bio_split_pool_init(&d->bio_split_hook))
+
+		return -ENOMEM;
+
+	d->disk = alloc_disk(1);
+	if (!d->disk)
+		return -ENOMEM;
+
+	snprintf(d->disk->disk_name, DISK_NAME_LEN, "bcache%i", bcache_minor);
+
+	d->disk->major		= bcache_major;
+	d->disk->first_minor	= bcache_minor++;
+	d->disk->fops		= &bcache_ops;
+	d->disk->private_data	= d;
+
+	q = blk_alloc_queue(GFP_KERNEL);
+	if (!q)
+		return -ENOMEM;
+
+	blk_queue_make_request(q, NULL);
+	d->disk->queue			= q;
+	q->queuedata			= d;
+	q->backing_dev_info.congested_data = d;
+	q->limits.max_hw_sectors	= UINT_MAX;
+	q->limits.max_sectors		= UINT_MAX;
+	q->limits.max_segment_size	= UINT_MAX;
+	q->limits.max_segments		= BIO_MAX_PAGES;
+	q->limits.max_discard_sectors	= UINT_MAX;
+	q->limits.io_min		= block_size;
+	q->limits.logical_block_size	= block_size;
+	q->limits.physical_block_size	= block_size;
+	set_bit(QUEUE_FLAG_NONROT,	&d->disk->queue->queue_flags);
+	set_bit(QUEUE_FLAG_DISCARD,	&d->disk->queue->queue_flags);
+
+	return 0;
+}
+
+/* Cached device */
+
+static void calc_cached_dev_sectors(struct cache_set *c)
+{
+	uint64_t sectors = 0;
+	struct cached_dev *dc;
+
+	list_for_each_entry(dc, &c->cached_devs, list)
+		sectors += bdev_sectors(dc->bdev);
+
+	c->cached_dev_sectors = sectors;
+}
+
+void bch_cached_dev_run(struct cached_dev *dc)
+{
+	struct bcache_device *d = &dc->disk;
+
+	if (atomic_xchg(&dc->running, 1))
+		return;
+
+	if (!d->c &&
+	    BDEV_STATE(&dc->sb) != BDEV_STATE_NONE) {
+		struct closure cl;
+		closure_init_stack(&cl);
+
+		SET_BDEV_STATE(&dc->sb, BDEV_STATE_STALE);
+		bch_write_bdev_super(dc, &cl);
+		closure_sync(&cl);
+	}
+
+	add_disk(d->disk);
+#if 0
+	char *env[] = { "SYMLINK=label" , NULL };
+	kobject_uevent_env(&disk_to_dev(d->disk)->kobj, KOBJ_CHANGE, env);
+#endif
+	if (sysfs_create_link(&d->kobj, &disk_to_dev(d->disk)->kobj, "dev") ||
+	    sysfs_create_link(&disk_to_dev(d->disk)->kobj, &d->kobj, "bcache"))
+		pr_debug("error creating sysfs link");
+}
+
+static void cached_dev_detach_finish(struct work_struct *w)
+{
+	struct cached_dev *dc = container_of(w, struct cached_dev, detach);
+	char buf[BDEVNAME_SIZE];
+	struct closure cl;
+	closure_init_stack(&cl);
+
+	BUG_ON(!atomic_read(&dc->disk.detaching));
+	BUG_ON(atomic_read(&dc->count));
+
+	sysfs_remove_link(&dc->disk.c->kobj, dc->disk.name);
+	sysfs_remove_link(&dc->disk.kobj, "cache");
+
+	mutex_lock(&bch_register_lock);
+
+	memset(&dc->sb.set_uuid, 0, 16);
+	SET_BDEV_STATE(&dc->sb, BDEV_STATE_NONE);
+
+	bch_write_bdev_super(dc, &cl);
+	closure_sync(&cl);
+
+	bcache_device_detach(&dc->disk);
+	list_move(&dc->list, &uncached_devices);
+
+	mutex_unlock(&bch_register_lock);
+
+	pr_info("Caching disabled for %s", bdevname(dc->bdev, buf));
+
+	/* Drop ref we took in cached_dev_detach() */
+	closure_put(&dc->disk.cl);
+}
+
+void bch_cached_dev_detach(struct cached_dev *dc)
+{
+	lockdep_assert_held(&bch_register_lock);
+
+	if (atomic_read(&dc->disk.closing))
+		return;
+
+	if (atomic_xchg(&dc->disk.detaching, 1))
+		return;
+
+	/*
+	 * Block the device from being closed and freed until we're finished
+	 * detaching
+	 */
+	closure_get(&dc->disk.cl);
+
+	bch_writeback_queue(dc);
+	cached_dev_put(dc);
+}
+
+int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c)
+{
+	uint32_t rtime = cpu_to_le32(get_seconds());
+	struct uuid_entry *u;
+	char buf[BDEVNAME_SIZE];
+
+	bdevname(dc->bdev, buf);
+
+	if (memcmp(dc->sb.set_uuid, c->sb.set_uuid, 16))
+		return -ENOENT;
+
+	if (dc->disk.c) {
+		pr_err("Can't attach %s: already attached", buf);
+		return -EINVAL;
+	}
+
+	if (test_bit(CACHE_SET_STOPPING, &c->flags)) {
+		pr_err("Can't attach %s: shutting down", buf);
+		return -EINVAL;
+	}
+
+	if (dc->sb.block_size < c->sb.block_size) {
+		/* Will die */
+		pr_err("Couldn't attach %s: block size "
+		       "less than set's block size", buf);
+		return -EINVAL;
+	}
+
+	u = uuid_find(c, dc->sb.uuid);
+
+	if (u &&
+	    (BDEV_STATE(&dc->sb) == BDEV_STATE_STALE ||
+	     BDEV_STATE(&dc->sb) == BDEV_STATE_NONE)) {
+		memcpy(u->uuid, invalid_uuid, 16);
+		u->invalidated = cpu_to_le32(get_seconds());
+		u = NULL;
+	}
+
+	if (!u) {
+		if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
+			pr_err("Couldn't find uuid for %s in set", buf);
+			return -ENOENT;
+		}
+
+		u = uuid_find_empty(c);
+		if (!u) {
+			pr_err("Not caching %s, no room for UUID", buf);
+			return -EINVAL;
+		}
+	}
+
+	/* Deadlocks since we're called via sysfs...
+	sysfs_remove_file(&dc->kobj, &sysfs_attach);
+	 */
+
+	if (is_zero(u->uuid, 16)) {
+		struct closure cl;
+		closure_init_stack(&cl);
+
+		memcpy(u->uuid, dc->sb.uuid, 16);
+		memcpy(u->label, dc->sb.label, SB_LABEL_SIZE);
+		u->first_reg = u->last_reg = rtime;
+		bch_uuid_write(c);
+
+		memcpy(dc->sb.set_uuid, c->sb.set_uuid, 16);
+		SET_BDEV_STATE(&dc->sb, BDEV_STATE_CLEAN);
+
+		bch_write_bdev_super(dc, &cl);
+		closure_sync(&cl);
+	} else {
+		u->last_reg = rtime;
+		bch_uuid_write(c);
+	}
+
+	bcache_device_attach(&dc->disk, c, u - c->uuids);
+	bcache_device_link(&dc->disk, c, "bdev");
+	list_move(&dc->list, &c->cached_devs);
+	calc_cached_dev_sectors(c);
+
+	smp_wmb();
+	/*
+	 * dc->c must be set before dc->count != 0 - paired with the mb in
+	 * cached_dev_get()
+	 */
+	atomic_set(&dc->count, 1);
+
+	if (BDEV_STATE(&dc->sb) == BDEV_STATE_DIRTY) {
+		atomic_set(&dc->has_dirty, 1);
+		atomic_inc(&dc->count);
+		bch_writeback_queue(dc);
+	}
+
+	bch_cached_dev_run(dc);
+
+	pr_info("Caching %s as %s on set %pU",
+		bdevname(dc->bdev, buf), dc->disk.disk->disk_name,
+		dc->disk.c->sb.set_uuid);
+	return 0;
+}
+
+void bch_cached_dev_release(struct kobject *kobj)
+{
+	struct cached_dev *dc = container_of(kobj, struct cached_dev,
+					     disk.kobj);
+	kfree(dc);
+	module_put(THIS_MODULE);
+}
+
+static void cached_dev_free(struct closure *cl)
+{
+	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
+
+	cancel_delayed_work_sync(&dc->writeback_rate_update);
+
+	mutex_lock(&bch_register_lock);
+
+	bcache_device_free(&dc->disk);
+	list_del(&dc->list);
+
+	mutex_unlock(&bch_register_lock);
+
+	if (!IS_ERR_OR_NULL(dc->bdev)) {
+		blk_sync_queue(bdev_get_queue(dc->bdev));
+		blkdev_put(dc->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
+	}
+
+	wake_up(&unregister_wait);
+
+	kobject_put(&dc->disk.kobj);
+}
+
+static void cached_dev_flush(struct closure *cl)
+{
+	struct cached_dev *dc = container_of(cl, struct cached_dev, disk.cl);
+	struct bcache_device *d = &dc->disk;
+
+	bch_cache_accounting_destroy(&dc->accounting);
+	kobject_del(&d->kobj);
+
+	continue_at(cl, cached_dev_free, system_wq);
+}
+
+static int cached_dev_init(struct cached_dev *dc, unsigned block_size)
+{
+	int err;
+	struct io *io;
+
+	closure_init(&dc->disk.cl, NULL);
+	set_closure_fn(&dc->disk.cl, cached_dev_flush, system_wq);
+
+	__module_get(THIS_MODULE);
+	INIT_LIST_HEAD(&dc->list);
+	kobject_init(&dc->disk.kobj, &bch_cached_dev_ktype);
+
+	bch_cache_accounting_init(&dc->accounting, &dc->disk.cl);
+
+	err = bcache_device_init(&dc->disk, block_size);
+	if (err)
+		goto err;
+
+	spin_lock_init(&dc->io_lock);
+	closure_init_unlocked(&dc->sb_write);
+	INIT_WORK(&dc->detach, cached_dev_detach_finish);
+
+	dc->sequential_merge		= true;
+	dc->sequential_cutoff		= 4 << 20;
+
+	INIT_LIST_HEAD(&dc->io_lru);
+	dc->sb_bio.bi_max_vecs	= 1;
+	dc->sb_bio.bi_io_vec	= dc->sb_bio.bi_inline_vecs;
+
+	for (io = dc->io; io < dc->io + RECENT_IO; io++) {
+		list_add(&io->lru, &dc->io_lru);
+		hlist_add_head(&io->hash, dc->io_hash + RECENT_IO);
+	}
+
+	bch_writeback_init_cached_dev(dc);
+	return 0;
+err:
+	bcache_device_stop(&dc->disk);
+	return err;
+}
+
+/* Cached device - bcache superblock */
+
+static const char *register_bdev(struct cache_sb *sb, struct page *sb_page,
+				 struct block_device *bdev,
+				 struct cached_dev *dc)
+{
+	char name[BDEVNAME_SIZE];
+	const char *err = "cannot allocate memory";
+	struct gendisk *g;
+	struct cache_set *c;
+
+	if (!dc || cached_dev_init(dc, sb->block_size << 9) != 0)
+		return err;
+
+	memcpy(&dc->sb, sb, sizeof(struct cache_sb));
+	dc->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	dc->bdev = bdev;
+	dc->bdev->bd_holder = dc;
+
+	g = dc->disk.disk;
+
+	set_capacity(g, dc->bdev->bd_part->nr_sects - 16);
+
+	bch_cached_dev_request_init(dc);
+
+	err = "error creating kobject";
+	if (kobject_add(&dc->disk.kobj, &part_to_dev(bdev->bd_part)->kobj,
+			"bcache"))
+		goto err;
+	if (bch_cache_accounting_add_kobjs(&dc->accounting, &dc->disk.kobj))
+		goto err;
+
+	list_add(&dc->list, &uncached_devices);
+	list_for_each_entry(c, &bch_cache_sets, list)
+		bch_cached_dev_attach(dc, c);
+
+	if (BDEV_STATE(&dc->sb) == BDEV_STATE_NONE ||
+	    BDEV_STATE(&dc->sb) == BDEV_STATE_STALE)
+		bch_cached_dev_run(dc);
+
+	return NULL;
+err:
+	kobject_put(&dc->disk.kobj);
+	pr_notice("error opening %s: %s", bdevname(bdev, name), err);
+	/*
+	 * Return NULL instead of an error because kobject_put() cleans
+	 * everything up
+	 */
+	return NULL;
+}
+
+/* Flash only volumes */
+
+void bch_flash_dev_release(struct kobject *kobj)
+{
+	struct bcache_device *d = container_of(kobj, struct bcache_device,
+					       kobj);
+	kfree(d);
+}
+
+static void flash_dev_free(struct closure *cl)
+{
+	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
+	bcache_device_free(d);
+	kobject_put(&d->kobj);
+}
+
+static void flash_dev_flush(struct closure *cl)
+{
+	struct bcache_device *d = container_of(cl, struct bcache_device, cl);
+
+	sysfs_remove_link(&d->c->kobj, d->name);
+	sysfs_remove_link(&d->kobj, "cache");
+	kobject_del(&d->kobj);
+	continue_at(cl, flash_dev_free, system_wq);
+}
+
+static int flash_dev_run(struct cache_set *c, struct uuid_entry *u)
+{
+	struct bcache_device *d = kzalloc(sizeof(struct bcache_device),
+					  GFP_KERNEL);
+	if (!d)
+		return -ENOMEM;
+
+	closure_init(&d->cl, NULL);
+	set_closure_fn(&d->cl, flash_dev_flush, system_wq);
+
+	kobject_init(&d->kobj, &bch_flash_dev_ktype);
+
+	if (bcache_device_init(d, block_bytes(c)))
+		goto err;
+
+	bcache_device_attach(d, c, u - c->uuids);
+	set_capacity(d->disk, u->sectors);
+	bch_flash_dev_request_init(d);
+	add_disk(d->disk);
+
+	if (kobject_add(&d->kobj, &disk_to_dev(d->disk)->kobj, "bcache"))
+		goto err;
+
+	bcache_device_link(d, c, "volume");
+
+	return 0;
+err:
+	kobject_put(&d->kobj);
+	return -ENOMEM;
+}
+
+static int flash_devs_run(struct cache_set *c)
+{
+	int ret = 0;
+	struct uuid_entry *u;
+
+	for (u = c->uuids;
+	     u < c->uuids + c->nr_uuids && !ret;
+	     u++)
+		if (UUID_FLASH_ONLY(u))
+			ret = flash_dev_run(c, u);
+
+	return ret;
+}
+
+int bch_flash_dev_create(struct cache_set *c, uint64_t size)
+{
+	struct uuid_entry *u;
+
+	if (test_bit(CACHE_SET_STOPPING, &c->flags))
+		return -EINTR;
+
+	u = uuid_find_empty(c);
+	if (!u) {
+		pr_err("Can't create volume, no room for UUID");
+		return -EINVAL;
+	}
+
+	get_random_bytes(u->uuid, 16);
+	memset(u->label, 0, 32);
+	u->first_reg = u->last_reg = cpu_to_le32(get_seconds());
+
+	SET_UUID_FLASH_ONLY(u, 1);
+	u->sectors = size >> 9;
+
+	bch_uuid_write(c);
+
+	return flash_dev_run(c, u);
+}
+
+/* Cache set */
+
+__printf(2, 3)
+bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...)
+{
+	va_list args;
+
+	if (test_bit(CACHE_SET_STOPPING, &c->flags))
+		return false;
+
+	/* XXX: we can be called from atomic context
+	acquire_console_sem();
+	*/
+
+	printk(KERN_ERR "bcache: error on %pU: ", c->sb.set_uuid);
+
+	va_start(args, fmt);
+	vprintk(fmt, args);
+	va_end(args);
+
+	printk(", disabling caching\n");
+
+	bch_cache_set_unregister(c);
+	return true;
+}
+
+void bch_cache_set_release(struct kobject *kobj)
+{
+	struct cache_set *c = container_of(kobj, struct cache_set, kobj);
+	kfree(c);
+	module_put(THIS_MODULE);
+}
+
+static void cache_set_free(struct closure *cl)
+{
+	struct cache_set *c = container_of(cl, struct cache_set, cl);
+	struct cache *ca;
+	unsigned i;
+
+	if (!IS_ERR_OR_NULL(c->debug))
+		debugfs_remove(c->debug);
+
+	bch_open_buckets_free(c);
+	bch_btree_cache_free(c);
+	bch_journal_free(c);
+
+	for_each_cache(ca, c, i)
+		if (ca)
+			kobject_put(&ca->kobj);
+
+	free_pages((unsigned long) c->uuids, ilog2(bucket_pages(c)));
+	free_pages((unsigned long) c->sort, ilog2(bucket_pages(c)));
+
+	kfree(c->fill_iter);
+	if (c->bio_split)
+		bioset_free(c->bio_split);
+	if (c->bio_meta)
+		mempool_destroy(c->bio_meta);
+	if (c->search)
+		mempool_destroy(c->search);
+	kfree(c->devices);
+
+	mutex_lock(&bch_register_lock);
+	list_del(&c->list);
+	mutex_unlock(&bch_register_lock);
+
+	pr_info("Cache set %pU unregistered", c->sb.set_uuid);
+	wake_up(&unregister_wait);
+
+	closure_debug_destroy(&c->cl);
+	kobject_put(&c->kobj);
+}
+
+static void cache_set_flush(struct closure *cl)
+{
+	struct cache_set *c = container_of(cl, struct cache_set, caching);
+	struct btree *b;
+
+	/* Shut down allocator threads */
+	set_bit(CACHE_SET_STOPPING_2, &c->flags);
+	wake_up(&c->alloc_wait);
+
+	bch_cache_accounting_destroy(&c->accounting);
+
+	kobject_put(&c->internal);
+	kobject_del(&c->kobj);
+
+	if (!IS_ERR_OR_NULL(c->root))
+		list_add(&c->root->list, &c->btree_cache);
+
+	/* Should skip this if we're unregistering because of an error */
+	list_for_each_entry(b, &c->btree_cache, list)
+		if (btree_node_dirty(b))
+			bch_btree_write(b, true, NULL);
+
+	closure_return(cl);
+}
+
+static void __cache_set_unregister(struct closure *cl)
+{
+	struct cache_set *c = container_of(cl, struct cache_set, caching);
+	struct cached_dev *dc, *t;
+	size_t i;
+
+	mutex_lock(&bch_register_lock);
+
+	if (test_bit(CACHE_SET_UNREGISTERING, &c->flags))
+		list_for_each_entry_safe(dc, t, &c->cached_devs, list)
+			bch_cached_dev_detach(dc);
+
+	for (i = 0; i < c->nr_uuids; i++)
+		if (c->devices[i] && UUID_FLASH_ONLY(&c->uuids[i]))
+			bcache_device_stop(c->devices[i]);
+
+	mutex_unlock(&bch_register_lock);
+
+	continue_at(cl, cache_set_flush, system_wq);
+}
+
+void bch_cache_set_stop(struct cache_set *c)
+{
+	if (!test_and_set_bit(CACHE_SET_STOPPING, &c->flags))
+		closure_queue(&c->caching);
+}
+
+void bch_cache_set_unregister(struct cache_set *c)
+{
+	set_bit(CACHE_SET_UNREGISTERING, &c->flags);
+	bch_cache_set_stop(c);
+}
+
+#define alloc_bucket_pages(gfp, c)			\
+	((void *) __get_free_pages(__GFP_ZERO|gfp, ilog2(bucket_pages(c))))
+
+struct cache_set *bch_cache_set_alloc(struct cache_sb *sb)
+{
+	int iter_size;
+	struct cache_set *c = kzalloc(sizeof(struct cache_set), GFP_KERNEL);
+	if (!c)
+		return NULL;
+
+	__module_get(THIS_MODULE);
+	closure_init(&c->cl, NULL);
+	set_closure_fn(&c->cl, cache_set_free, system_wq);
+
+	closure_init(&c->caching, &c->cl);
+	set_closure_fn(&c->caching, __cache_set_unregister, system_wq);
+
+	/* Maybe create continue_at_noreturn() and use it here? */
+	closure_set_stopped(&c->cl);
+	closure_put(&c->cl);
+
+	kobject_init(&c->kobj, &bch_cache_set_ktype);
+	kobject_init(&c->internal, &bch_cache_set_internal_ktype);
+
+	bch_cache_accounting_init(&c->accounting, &c->cl);
+
+	memcpy(c->sb.set_uuid, sb->set_uuid, 16);
+	c->sb.block_size	= sb->block_size;
+	c->sb.bucket_size	= sb->bucket_size;
+	c->sb.nr_in_set		= sb->nr_in_set;
+	c->sb.last_mount	= sb->last_mount;
+	c->bucket_bits		= ilog2(sb->bucket_size);
+	c->block_bits		= ilog2(sb->block_size);
+	c->nr_uuids		= bucket_bytes(c) / sizeof(struct uuid_entry);
+
+	c->btree_pages		= c->sb.bucket_size / PAGE_SECTORS;
+	if (c->btree_pages > BTREE_MAX_PAGES)
+		c->btree_pages = max_t(int, c->btree_pages / 4,
+				       BTREE_MAX_PAGES);
+
+	init_waitqueue_head(&c->alloc_wait);
+	mutex_init(&c->bucket_lock);
+	mutex_init(&c->fill_lock);
+	mutex_init(&c->sort_lock);
+	spin_lock_init(&c->sort_time_lock);
+	closure_init_unlocked(&c->sb_write);
+	closure_init_unlocked(&c->uuid_write);
+	spin_lock_init(&c->btree_read_time_lock);
+	bch_moving_init_cache_set(c);
+
+	INIT_LIST_HEAD(&c->list);
+	INIT_LIST_HEAD(&c->cached_devs);
+	INIT_LIST_HEAD(&c->btree_cache);
+	INIT_LIST_HEAD(&c->btree_cache_freeable);
+	INIT_LIST_HEAD(&c->btree_cache_freed);
+	INIT_LIST_HEAD(&c->data_buckets);
+
+	c->search = mempool_create_slab_pool(32, bch_search_cache);
+	if (!c->search)
+		goto err;
+
+	iter_size = (sb->bucket_size / sb->block_size + 1) *
+		sizeof(struct btree_iter_set);
+
+	if (!(c->devices = kzalloc(c->nr_uuids * sizeof(void *), GFP_KERNEL)) ||
+	    !(c->bio_meta = mempool_create_kmalloc_pool(2,
+				sizeof(struct bbio) + sizeof(struct bio_vec) *
+				bucket_pages(c))) ||
+	    !(c->bio_split = bioset_create(4, offsetof(struct bbio, bio))) ||
+	    !(c->fill_iter = kmalloc(iter_size, GFP_KERNEL)) ||
+	    !(c->sort = alloc_bucket_pages(GFP_KERNEL, c)) ||
+	    !(c->uuids = alloc_bucket_pages(GFP_KERNEL, c)) ||
+	    bch_journal_alloc(c) ||
+	    bch_btree_cache_alloc(c) ||
+	    bch_open_buckets_alloc(c))
+		goto err;
+
+	c->fill_iter->size = sb->bucket_size / sb->block_size;
+
+	c->congested_read_threshold_us	= 2000;
+	c->congested_write_threshold_us	= 20000;
+	c->error_limit	= 8 << IO_ERROR_SHIFT;
+
+	return c;
+err:
+	bch_cache_set_unregister(c);
+	return NULL;
+}
+
+static void run_cache_set(struct cache_set *c)
+{
+	const char *err = "cannot allocate memory";
+	struct cached_dev *dc, *t;
+	struct cache *ca;
+	unsigned i;
+
+	struct btree_op op;
+	bch_btree_op_init_stack(&op);
+	op.lock = SHRT_MAX;
+
+	for_each_cache(ca, c, i)
+		c->nbuckets += ca->sb.nbuckets;
+
+	if (CACHE_SYNC(&c->sb)) {
+		LIST_HEAD(journal);
+		struct bkey *k;
+		struct jset *j;
+
+		err = "cannot allocate memory for journal";
+		if (bch_journal_read(c, &journal, &op))
+			goto err;
+
+		pr_debug("btree_journal_read() done");
+
+		err = "no journal entries found";
+		if (list_empty(&journal))
+			goto err;
+
+		j = &list_entry(journal.prev, struct journal_replay, list)->j;
+
+		err = "IO error reading priorities";
+		for_each_cache(ca, c, i)
+			prio_read(ca, j->prio_bucket[ca->sb.nr_this_dev]);
+
+		/*
+		 * If prio_read() fails it'll call cache_set_error and we'll
+		 * tear everything down right away, but if we perhaps checked
+		 * sooner we could avoid journal replay.
+		 */
+
+		k = &j->btree_root;
+
+		err = "bad btree root";
+		if (__bch_ptr_invalid(c, j->btree_level + 1, k))
+			goto err;
+
+		err = "error reading btree root";
+		c->root = bch_btree_node_get(c, k, j->btree_level, &op);
+		if (IS_ERR_OR_NULL(c->root))
+			goto err;
+
+		list_del_init(&c->root->list);
+		rw_unlock(true, c->root);
+
+		err = uuid_read(c, j, &op.cl);
+		if (err)
+			goto err;
+
+		err = "error in recovery";
+		if (bch_btree_check(c, &op))
+			goto err;
+
+		bch_journal_mark(c, &journal);
+		bch_btree_gc_finish(c);
+		pr_debug("btree_check() done");
+
+		/*
+		 * bcache_journal_next() can't happen sooner, or
+		 * btree_gc_finish() will give spurious errors about last_gc >
+		 * gc_gen - this is a hack but oh well.
+		 */
+		bch_journal_next(&c->journal);
+
+		for_each_cache(ca, c, i)
+			closure_call(&ca->alloc, bch_allocator_thread,
+				     system_wq, &c->cl);
+
+		/*
+		 * First place it's safe to allocate: btree_check() and
+		 * btree_gc_finish() have to run before we have buckets to
+		 * allocate, and bch_bucket_alloc_set() might cause a journal
+		 * entry to be written so bcache_journal_next() has to be called
+		 * first.
+		 *
+		 * If the uuids were in the old format we have to rewrite them
+		 * before the next journal entry is written:
+		 */
+		if (j->version < BCACHE_JSET_VERSION_UUID)
+			__uuid_write(c);
+
+		bch_journal_replay(c, &journal, &op);
+	} else {
+		pr_notice("invalidating existing data");
+		/* Don't want invalidate_buckets() to queue a gc yet */
+		closure_lock(&c->gc, NULL);
+
+		for_each_cache(ca, c, i) {
+			unsigned j;
+
+			ca->sb.keys = clamp_t(int, ca->sb.nbuckets >> 7,
+					      2, SB_JOURNAL_BUCKETS);
+
+			for (j = 0; j < ca->sb.keys; j++)
+				ca->sb.d[j] = ca->sb.first_bucket + j;
+		}
+
+		bch_btree_gc_finish(c);
+
+		for_each_cache(ca, c, i)
+			closure_call(&ca->alloc, bch_allocator_thread,
+				     ca->alloc_workqueue, &c->cl);
+
+		mutex_lock(&c->bucket_lock);
+		for_each_cache(ca, c, i)
+			bch_prio_write(ca);
+		mutex_unlock(&c->bucket_lock);
+
+		wake_up(&c->alloc_wait);
+
+		err = "cannot allocate new UUID bucket";
+		if (__uuid_write(c))
+			goto err_unlock_gc;
+
+		err = "cannot allocate new btree root";
+		c->root = bch_btree_node_alloc(c, 0, &op.cl);
+		if (IS_ERR_OR_NULL(c->root))
+			goto err_unlock_gc;
+
+		bkey_copy_key(&c->root->key, &MAX_KEY);
+		bch_btree_write(c->root, true, &op);
+
+		bch_btree_set_root(c->root);
+		rw_unlock(true, c->root);
+
+		/*
+		 * We don't want to write the first journal entry until
+		 * everything is set up - fortunately journal entries won't be
+		 * written until the SET_CACHE_SYNC() here:
+		 */
+		SET_CACHE_SYNC(&c->sb, true);
+
+		bch_journal_next(&c->journal);
+		bch_journal_meta(c, &op.cl);
+
+		/* Unlock */
+		closure_set_stopped(&c->gc.cl);
+		closure_put(&c->gc.cl);
+	}
+
+	closure_sync(&op.cl);
+	c->sb.last_mount = get_seconds();
+	bcache_write_super(c);
+
+	list_for_each_entry_safe(dc, t, &uncached_devices, list)
+		bch_cached_dev_attach(dc, c);
+
+	flash_devs_run(c);
+
+	return;
+err_unlock_gc:
+	closure_set_stopped(&c->gc.cl);
+	closure_put(&c->gc.cl);
+err:
+	closure_sync(&op.cl);
+	/* XXX: test this, it's broken */
+	bch_cache_set_error(c, err);
+}
+
+static bool can_attach_cache(struct cache *ca, struct cache_set *c)
+{
+	return ca->sb.block_size	== c->sb.block_size &&
+		ca->sb.bucket_size	== c->sb.block_size &&
+		ca->sb.nr_in_set	== c->sb.nr_in_set;
+}
+
+static const char *register_cache_set(struct cache *ca)
+{
+	char buf[12];
+	const char *err = "cannot allocate memory";
+	struct cache_set *c;
+
+	list_for_each_entry(c, &bch_cache_sets, list)
+		if (!memcmp(c->sb.set_uuid, ca->sb.set_uuid, 16)) {
+			if (c->cache[ca->sb.nr_this_dev])
+				return "duplicate cache set member";
+
+			if (!can_attach_cache(ca, c))
+				return "cache sb does not match set";
+
+			if (!CACHE_SYNC(&ca->sb))
+				SET_CACHE_SYNC(&c->sb, false);
+
+			goto found;
+		}
+
+	c = bch_cache_set_alloc(&ca->sb);
+	if (!c)
+		return err;
+
+	err = "error creating kobject";
+	if (kobject_add(&c->kobj, bcache_kobj, "%pU", c->sb.set_uuid) ||
+	    kobject_add(&c->internal, &c->kobj, "internal"))
+		goto err;
+
+	if (bch_cache_accounting_add_kobjs(&c->accounting, &c->kobj))
+		goto err;
+
+	bch_debug_init_cache_set(c);
+
+	list_add(&c->list, &bch_cache_sets);
+found:
+	sprintf(buf, "cache%i", ca->sb.nr_this_dev);
+	if (sysfs_create_link(&ca->kobj, &c->kobj, "set") ||
+	    sysfs_create_link(&c->kobj, &ca->kobj, buf))
+		goto err;
+
+	if (ca->sb.seq > c->sb.seq) {
+		c->sb.version		= ca->sb.version;
+		memcpy(c->sb.set_uuid, ca->sb.set_uuid, 16);
+		c->sb.flags             = ca->sb.flags;
+		c->sb.seq		= ca->sb.seq;
+		pr_debug("set version = %llu", c->sb.version);
+	}
+
+	ca->set = c;
+	ca->set->cache[ca->sb.nr_this_dev] = ca;
+	c->cache_by_alloc[c->caches_loaded++] = ca;
+
+	if (c->caches_loaded == c->sb.nr_in_set)
+		run_cache_set(c);
+
+	return NULL;
+err:
+	bch_cache_set_unregister(c);
+	return err;
+}
+
+/* Cache device */
+
+void bch_cache_release(struct kobject *kobj)
+{
+	struct cache *ca = container_of(kobj, struct cache, kobj);
+
+	if (ca->set)
+		ca->set->cache[ca->sb.nr_this_dev] = NULL;
+
+	bch_cache_allocator_exit(ca);
+
+	bio_split_pool_free(&ca->bio_split_hook);
+
+	if (ca->alloc_workqueue)
+		destroy_workqueue(ca->alloc_workqueue);
+
+	free_pages((unsigned long) ca->disk_buckets, ilog2(bucket_pages(ca)));
+	kfree(ca->prio_buckets);
+	vfree(ca->buckets);
+
+	free_heap(&ca->heap);
+	free_fifo(&ca->unused);
+	free_fifo(&ca->free_inc);
+	free_fifo(&ca->free);
+
+	if (ca->sb_bio.bi_inline_vecs[0].bv_page)
+		put_page(ca->sb_bio.bi_io_vec[0].bv_page);
+
+	if (!IS_ERR_OR_NULL(ca->bdev)) {
+		blk_sync_queue(bdev_get_queue(ca->bdev));
+		blkdev_put(ca->bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
+	}
+
+	kfree(ca);
+	module_put(THIS_MODULE);
+}
+
+static int cache_alloc(struct cache_sb *sb, struct cache *ca)
+{
+	size_t free;
+	struct bucket *b;
+
+	if (!ca)
+		return -ENOMEM;
+
+	__module_get(THIS_MODULE);
+	kobject_init(&ca->kobj, &bch_cache_ktype);
+
+	memcpy(&ca->sb, sb, sizeof(struct cache_sb));
+
+	INIT_LIST_HEAD(&ca->discards);
+
+	bio_init(&ca->sb_bio);
+	ca->sb_bio.bi_max_vecs	= 1;
+	ca->sb_bio.bi_io_vec	= ca->sb_bio.bi_inline_vecs;
+
+	bio_init(&ca->journal.bio);
+	ca->journal.bio.bi_max_vecs = 8;
+	ca->journal.bio.bi_io_vec = ca->journal.bio.bi_inline_vecs;
+
+	free = roundup_pow_of_two(ca->sb.nbuckets) >> 9;
+	free = max_t(size_t, free, (prio_buckets(ca) + 8) * 2);
+
+	if (!init_fifo(&ca->free,	free, GFP_KERNEL) ||
+	    !init_fifo(&ca->free_inc,	free << 2, GFP_KERNEL) ||
+	    !init_fifo(&ca->unused,	free << 2, GFP_KERNEL) ||
+	    !init_heap(&ca->heap,	free << 3, GFP_KERNEL) ||
+	    !(ca->buckets	= vmalloc(sizeof(struct bucket) *
+					  ca->sb.nbuckets)) ||
+	    !(ca->prio_buckets	= kzalloc(sizeof(uint64_t) * prio_buckets(ca) *
+					  2, GFP_KERNEL)) ||
+	    !(ca->disk_buckets	= alloc_bucket_pages(GFP_KERNEL, ca)) ||
+	    !(ca->alloc_workqueue = alloc_workqueue("bch_allocator", 0, 1)) ||
+	    bio_split_pool_init(&ca->bio_split_hook))
+		goto err;
+
+	ca->prio_last_buckets = ca->prio_buckets + prio_buckets(ca);
+
+	memset(ca->buckets, 0, ca->sb.nbuckets * sizeof(struct bucket));
+	for_each_bucket(b, ca)
+		atomic_set(&b->pin, 0);
+
+	if (bch_cache_allocator_init(ca))
+		goto err;
+
+	return 0;
+err:
+	kobject_put(&ca->kobj);
+	return -ENOMEM;
+}
+
+static const char *register_cache(struct cache_sb *sb, struct page *sb_page,
+				  struct block_device *bdev, struct cache *ca)
+{
+	char name[BDEVNAME_SIZE];
+	const char *err = "cannot allocate memory";
+
+	if (cache_alloc(sb, ca) != 0)
+		return err;
+
+	ca->sb_bio.bi_io_vec[0].bv_page = sb_page;
+	ca->bdev = bdev;
+	ca->bdev->bd_holder = ca;
+
+	if (blk_queue_discard(bdev_get_queue(ca->bdev)))
+		ca->discard = CACHE_DISCARD(&ca->sb);
+
+	err = "error creating kobject";
+	if (kobject_add(&ca->kobj, &part_to_dev(bdev->bd_part)->kobj, "bcache"))
+		goto err;
+
+	err = register_cache_set(ca);
+	if (err)
+		goto err;
+
+	pr_info("registered cache device %s", bdevname(bdev, name));
+
+	return NULL;
+err:
+	kobject_put(&ca->kobj);
+	pr_info("error opening %s: %s", bdevname(bdev, name), err);
+	/* Return NULL instead of an error because kobject_put() cleans
+	 * everything up
+	 */
+	return NULL;
+}
+
+/* Global interfaces/init */
+
+static ssize_t register_bcache(struct kobject *, struct kobj_attribute *,
+			       const char *, size_t);
+
+kobj_attribute_write(register,		register_bcache);
+kobj_attribute_write(register_quiet,	register_bcache);
+
+static ssize_t register_bcache(struct kobject *k, struct kobj_attribute *attr,
+			       const char *buffer, size_t size)
+{
+	ssize_t ret = size;
+	const char *err = "cannot allocate memory";
+	char *path = NULL;
+	struct cache_sb *sb = NULL;
+	struct block_device *bdev = NULL;
+	struct page *sb_page = NULL;
+
+	if (!try_module_get(THIS_MODULE))
+		return -EBUSY;
+
+	mutex_lock(&bch_register_lock);
+
+	if (!(path = kstrndup(buffer, size, GFP_KERNEL)) ||
+	    !(sb = kmalloc(sizeof(struct cache_sb), GFP_KERNEL)))
+		goto err;
+
+	err = "failed to open device";
+	bdev = blkdev_get_by_path(strim(path),
+				  FMODE_READ|FMODE_WRITE|FMODE_EXCL,
+				  sb);
+	if (bdev == ERR_PTR(-EBUSY))
+		err = "device busy";
+
+	if (IS_ERR(bdev) ||
+	    set_blocksize(bdev, 4096))
+		goto err;
+
+	err = read_super(sb, bdev, &sb_page);
+	if (err)
+		goto err_close;
+
+	if (sb->version == CACHE_BACKING_DEV) {
+		struct cached_dev *dc = kzalloc(sizeof(*dc), GFP_KERNEL);
+
+		err = register_bdev(sb, sb_page, bdev, dc);
+	} else {
+		struct cache *ca = kzalloc(sizeof(*ca), GFP_KERNEL);
+
+		err = register_cache(sb, sb_page, bdev, ca);
+	}
+
+	if (err) {
+		/* register_(bdev|cache) will only return an error if they
+		 * didn't get far enough to create the kobject - if they did,
+		 * the kobject destructor will do this cleanup.
+		 */
+		put_page(sb_page);
+err_close:
+		blkdev_put(bdev, FMODE_READ|FMODE_WRITE|FMODE_EXCL);
+err:
+		if (attr != &ksysfs_register_quiet)
+			pr_info("error opening %s: %s", path, err);
+		ret = -EINVAL;
+	}
+
+	kfree(sb);
+	kfree(path);
+	mutex_unlock(&bch_register_lock);
+	module_put(THIS_MODULE);
+	return ret;
+}
+
+static int bcache_reboot(struct notifier_block *n, unsigned long code, void *x)
+{
+	if (code == SYS_DOWN ||
+	    code == SYS_HALT ||
+	    code == SYS_POWER_OFF) {
+		DEFINE_WAIT(wait);
+		unsigned long start = jiffies;
+		bool stopped = false;
+
+		struct cache_set *c, *tc;
+		struct cached_dev *dc, *tdc;
+
+		mutex_lock(&bch_register_lock);
+
+		if (list_empty(&bch_cache_sets) &&
+		    list_empty(&uncached_devices))
+			goto out;
+
+		pr_info("Stopping all devices:");
+
+		list_for_each_entry_safe(c, tc, &bch_cache_sets, list)
+			bch_cache_set_stop(c);
+
+		list_for_each_entry_safe(dc, tdc, &uncached_devices, list)
+			bcache_device_stop(&dc->disk);
+
+		/* What's a condition variable? */
+		while (1) {
+			long timeout = start + 2 * HZ - jiffies;
+
+			stopped = list_empty(&bch_cache_sets) &&
+				list_empty(&uncached_devices);
+
+			if (timeout < 0 || stopped)
+				break;
+
+			prepare_to_wait(&unregister_wait, &wait,
+					TASK_UNINTERRUPTIBLE);
+
+			mutex_unlock(&bch_register_lock);
+			schedule_timeout(timeout);
+			mutex_lock(&bch_register_lock);
+		}
+
+		finish_wait(&unregister_wait, &wait);
+
+		if (stopped)
+			pr_info("All devices stopped");
+		else
+			pr_notice("Timeout waiting for devices to be closed");
+out:
+		mutex_unlock(&bch_register_lock);
+	}
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block reboot = {
+	.notifier_call	= bcache_reboot,
+	.priority	= INT_MAX, /* before any real devices */
+};
+
+static void bcache_exit(void)
+{
+	bch_debug_exit();
+	bch_writeback_exit();
+	bch_request_exit();
+	bch_btree_exit();
+	if (bcache_kobj)
+		kobject_put(bcache_kobj);
+	if (bcache_wq)
+		destroy_workqueue(bcache_wq);
+	unregister_blkdev(bcache_major, "bcache");
+	unregister_reboot_notifier(&reboot);
+}
+
+static int __init bcache_init(void)
+{
+	static const struct attribute *files[] = {
+		&ksysfs_register.attr,
+		&ksysfs_register_quiet.attr,
+		NULL
+	};
+
+	mutex_init(&bch_register_lock);
+	init_waitqueue_head(&unregister_wait);
+	register_reboot_notifier(&reboot);
+
+	bcache_major = register_blkdev(0, "bcache");
+	if (bcache_major < 0)
+		return bcache_major;
+
+	if (!(bcache_wq = create_workqueue("bcache")) ||
+	    !(bcache_kobj = kobject_create_and_add("bcache", fs_kobj)) ||
+	    sysfs_create_files(bcache_kobj, files) ||
+	    bch_btree_init() ||
+	    bch_request_init() ||
+	    bch_writeback_init() ||
+	    bch_debug_init(bcache_kobj))
+		goto err;
+
+	return 0;
+err:
+	bcache_exit();
+	return -ENOMEM;
+}
+
+module_exit(bcache_exit);
+module_init(bcache_init);
