commit 46f5aa8806e34f2e48de852cc7db2c74c3a5cd8d
Author: Joe Perches <joe@perches.com>
Date:   Wed May 27 12:01:52 2020 +0800

    bcache: Convert pr_<level> uses to a more typical style
    
    Remove the trailing newline from the define of pr_fmt and add newlines
    to the uses.
    
    Miscellanea:
    
    o Convert bch_bkey_dump from multiple uses of pr_err to pr_cont
      as the earlier conversion was inappropriate done causing multiple
      lines to be emitted where only a single output line was desired
    o Use vsprintf extension %pV in bch_cache_set_error to avoid multiple
      line output where only a single line output was desired
    o Coalesce formats
    
    Fixes: 6ae63e3501c4 ("bcache: replace printk() by pr_*() routines")
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 74a9849ea164..221e0191b687 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -176,7 +176,7 @@
  * - updates to non leaf nodes just happen synchronously (see btree_split()).
  */
 
-#define pr_fmt(fmt) "bcache: %s() " fmt "\n", __func__
+#define pr_fmt(fmt) "bcache: %s() " fmt, __func__
 
 #include <linux/bcache.h>
 #include <linux/bio.h>

commit 038ba8cc1bffc51250add4a9b9249d4331576d8f
Author: Coly Li <colyli@suse.de>
Date:   Sat Feb 1 22:42:33 2020 +0800

    bcache: add readahead cache policy options via sysfs interface
    
    In year 2007 high performance SSD was still expensive, in order to
    save more space for real workload or meta data, the readahead I/Os
    for non-meta data was bypassed and not cached on SSD.
    
    In now days, SSD price drops a lot and people can find larger size
    SSD with more comfortable price. It is unncessary to alway bypass
    normal readahead I/Os to save SSD space for now.
    
    This patch adds options for readahead data cache policies via sysfs
    file /sys/block/bcache<N>/readahead_cache_policy, the options are,
    - "all": cache all readahead data I/Os.
    - "meta-only": only cache meta data, and bypass other regular I/Os.
    
    If users want to make bcache continue to only cache readahead request
    for metadata and bypass regular data readahead, please set "meta-only"
    to this sysfs file. By default, bcache will back to cache all read-
    ahead requests now.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Acked-by: Eric Wheeler <bcache@linux.ewheeler.net>
    Cc: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index adf26a21fcd1..74a9849ea164 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -330,6 +330,9 @@ struct cached_dev {
 	 */
 	atomic_t		has_dirty;
 
+#define BCH_CACHE_READA_ALL		0
+#define BCH_CACHE_READA_META_ONLY	1
+	unsigned int		cache_readahead_policy;
 	struct bch_ratelimit	writeback_rate;
 	struct delayed_work	writeback_rate_update;
 

commit 475389ae5c08656f8bb5cc3adc88a531c7c4877f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 24 01:01:33 2020 +0800

    bcache: store a pointer to the on-disk sb in the cache and cached_dev structures
    
    This allows to properly build the superblock bio including the offset in
    the page using the normal bio helpers.  This fixes writing the superblock
    for page sizes larger than 4k where the sb write bio would need an offset
    in the bio_vec.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 9198c1b480d9..adf26a21fcd1 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -301,6 +301,7 @@ struct cached_dev {
 	struct block_device	*bdev;
 
 	struct cache_sb		sb;
+	struct cache_sb_disk	*sb_disk;
 	struct bio		sb_bio;
 	struct bio_vec		sb_bv[1];
 	struct closure		sb_write;
@@ -403,6 +404,7 @@ enum alloc_reserve {
 struct cache {
 	struct cache_set	*set;
 	struct cache_sb		sb;
+	struct cache_sb_disk	*sb_disk;
 	struct bio		sb_bio;
 	struct bio_vec		sb_bv[1];
 

commit c5fcdedcee4e6ae15c0eb5e0fbe25467e57d2963
Author: Coly Li <colyli@suse.de>
Date:   Wed Nov 13 16:03:23 2019 +0800

    bcache: add idle_max_writeback_rate sysfs interface
    
    For writeback mode, if there is no regular I/O request for a while,
    the writeback rate will be set to the maximum value (1TB/s for now).
    This is good for most of the storage workload, but there are still
    people don't what the maximum writeback rate in I/O idle time.
    
    This patch adds a sysfs interface file idle_max_writeback_rate to
    permit people to disable maximum writeback rate. Then the minimum
    writeback rate can be advised by writeback_rate_minimum in the
    bcache device's sysfs interface.
    
    Reported-by: Christian Balzer <chibi@gol.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 50241e045c70..9198c1b480d9 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -724,6 +724,7 @@ struct cache_set {
 	unsigned int		gc_always_rewrite:1;
 	unsigned int		shrinker_disabled:1;
 	unsigned int		copy_gc_enabled:1;
+	unsigned int		idle_max_writeback_rate_enabled:1;
 
 #define BUCKET_HASH_BITS	12
 	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];

commit 84c529aea182939e68f618ed9813740c9165c7eb
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 13 16:03:21 2019 +0800

    bcache: fix deadlock in bcache_allocator
    
    bcache_allocator can call the following:
    
     bch_allocator_thread()
      -> bch_prio_write()
         -> bch_bucket_alloc()
            -> wait on &ca->set->bucket_wait
    
    But the wake up event on bucket_wait is supposed to come from
    bch_allocator_thread() itself => deadlock:
    
    [ 1158.490744] INFO: task bcache_allocato:15861 blocked for more than 10 seconds.
    [ 1158.495929]       Not tainted 5.3.0-050300rc3-generic #201908042232
    [ 1158.500653] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [ 1158.504413] bcache_allocato D    0 15861      2 0x80004000
    [ 1158.504419] Call Trace:
    [ 1158.504429]  __schedule+0x2a8/0x670
    [ 1158.504432]  schedule+0x2d/0x90
    [ 1158.504448]  bch_bucket_alloc+0xe5/0x370 [bcache]
    [ 1158.504453]  ? wait_woken+0x80/0x80
    [ 1158.504466]  bch_prio_write+0x1dc/0x390 [bcache]
    [ 1158.504476]  bch_allocator_thread+0x233/0x490 [bcache]
    [ 1158.504491]  kthread+0x121/0x140
    [ 1158.504503]  ? invalidate_buckets+0x890/0x890 [bcache]
    [ 1158.504506]  ? kthread_park+0xb0/0xb0
    [ 1158.504510]  ret_from_fork+0x35/0x40
    
    Fix by making the call to bch_prio_write() non-blocking, so that
    bch_allocator_thread() never waits on itself.
    
    Moreover, make sure to wake up the garbage collector thread when
    bch_prio_write() is failing to allocate buckets.
    
    BugLink: https://bugs.launchpad.net/bugs/1784665
    BugLink: https://bugs.launchpad.net/bugs/1796292
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 3653faf3bf48..50241e045c70 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -978,7 +978,7 @@ bool bch_cached_dev_error(struct cached_dev *dc);
 __printf(2, 3)
 bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...);
 
-void bch_prio_write(struct cache *ca);
+int bch_prio_write(struct cache *ca, bool wait);
 void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent);
 
 extern struct workqueue_struct *bcache_wq;

commit 34cf78bf34d48dddddfeeadb44f9841d7864997a
Author: Guoju Fang <fangguoju@gmail.com>
Date:   Wed Nov 13 16:03:16 2019 +0800

    bcache: fix a lost wake-up problem caused by mca_cannibalize_lock
    
    This patch fix a lost wake-up problem caused by the race between
    mca_cannibalize_lock and bch_cannibalize_unlock.
    
    Consider two processes, A and B. Process A is executing
    mca_cannibalize_lock, while process B takes c->btree_cache_alloc_lock
    and is executing bch_cannibalize_unlock. The problem happens that after
    process A executes cmpxchg and will execute prepare_to_wait. In this
    timeslice process B executes wake_up, but after that process A executes
    prepare_to_wait and set the state to TASK_INTERRUPTIBLE. Then process A
    goes to sleep but no one will wake up it. This problem may cause bcache
    device to dead.
    
    Signed-off-by: Guoju Fang <fangguoju@gmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 013e35a9e317..3653faf3bf48 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -582,6 +582,7 @@ struct cache_set {
 	 */
 	wait_queue_head_t	btree_cache_wait;
 	struct task_struct	*btree_cache_alloc_lock;
+	spinlock_t		btree_cannibalize_lock;
 
 	/*
 	 * When we free a btree node, we increment the gen of the bucket the

commit dff90d58a1c815b87b2603295382c97e78064349
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 20:00:00 2019 +0800

    bcache: add reclaimed_journal_buckets to struct cache_set
    
    Now we have counters for how many times jouranl is reclaimed, how many
    times cached dirty btree nodes are flushed, but we don't know how many
    jouranl buckets are really reclaimed.
    
    This patch adds reclaimed_journal_buckets into struct cache_set, this
    is an increasing only counter, to tell how many journal buckets are
    reclaimed since cache set runs. From all these three counters (reclaim,
    reclaimed_journal_buckets, flush_write), we can have idea how well
    current journal space reclaim code works.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 35396248a7d5..013e35a9e317 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -705,6 +705,7 @@ struct cache_set {
 	atomic_long_t		writeback_keys_failed;
 
 	atomic_long_t		reclaim;
+	atomic_long_t		reclaimed_journal_buckets;
 	atomic_long_t		flush_write;
 
 	enum			{

commit d91ce7574daf48a4567ba62733d43284f5d2a3f4
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:57 2019 +0800

    bcache: remove retry_flush_write from struct cache_set
    
    In struct cache_set, retry_flush_write is added for commit c4dc2497d50d
    ("bcache: fix high CPU occupancy during journal") which is reverted in
    previous patch.
    
    Now it is useless anymore, and this patch removes it from bcache code.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index cb268d7c6cea..35396248a7d5 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -706,7 +706,6 @@ struct cache_set {
 
 	atomic_long_t		reclaim;
 	atomic_long_t		flush_write;
-	atomic_long_t		retry_flush_write;
 
 	enum			{
 		ON_ERROR_UNREGISTER,

commit 249a5f6da57c28a903c75d81505d58ec8c10030d
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:54 2019 +0800

    bcache: Revert "bcache: fix high CPU occupancy during journal"
    
    This reverts commit c4dc2497d50d9c6fb16aa0d07b6a14f3b2adb1e0.
    
    This patch enlarges a race between normal btree flush code path and
    flush_btree_write(), which causes deadlock when journal space is
    exhausted. Reverts this patch makes the race window from 128 btree
    nodes to only 1 btree nodes.
    
    Fixes: c4dc2497d50d ("bcache: fix high CPU occupancy during journal")
    Signed-off-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Cc: Tang Junhui <tang.junhui.linux@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 73a97586a2ef..cb268d7c6cea 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -726,8 +726,6 @@ struct cache_set {
 
 #define BUCKET_HASH_BITS	12
 	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];
-
-	DECLARE_HEAP(struct btree *, flush_btree);
 };
 
 struct bbio {

commit 0b13efecf5f25ce5e31f2ab3930335015cb65a7d
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:33 2019 +0800

    bcache: add return value check to bch_cached_dev_run()
    
    This patch adds return value check to bch_cached_dev_run(), now if there
    is error happens inside bch_cached_dev_run(), it can be catched.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index fdf75352e16a..73a97586a2ef 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1006,7 +1006,7 @@ int bch_flash_dev_create(struct cache_set *c, uint64_t size);
 int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
 			  uint8_t *set_uuid);
 void bch_cached_dev_detach(struct cached_dev *dc);
-void bch_cached_dev_run(struct cached_dev *dc);
+int bch_cached_dev_run(struct cached_dev *dc);
 void bcache_device_stop(struct bcache_device *d);
 
 void bch_cache_set_unregister(struct cache_set *c);

commit 7a671d8ef821bf5743fdff17fae0600648345b03
Author: Coly Li <colyli@suse.de>
Date:   Thu Dec 13 22:53:53 2018 +0800

    bcache: option to automatically run gc thread after writeback
    
    The option gc_after_writeback is disabled by default, because garbage
    collection will discard SSD data which drops cached data.
    
    Echo 1 into /sys/fs/bcache/<UUID>/internal/gc_after_writeback will
    enable this option, which wakes up gc thread when writeback accomplished
    and all cached data is clean.
    
    This option is helpful for people who cares writing performance more. In
    heavy writing workload, all cached data can be clean only happens when
    writeback thread cleans all cached data in I/O idle time. In such
    situation a following gc running may help to shrink bcache B+ tree and
    discard more clean data, which may be helpful for future writing
    requests.
    
    If you are not sure whether this is helpful for your own workload,
    please leave it as disabled by default.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 96d2213f279e..fdf75352e16a 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -626,6 +626,20 @@ struct cache_set {
 	/* Where in the btree gc currently is */
 	struct bkey		gc_done;
 
+	/*
+	 * For automatical garbage collection after writeback completed, this
+	 * varialbe is used as bit fields,
+	 * - 0000 0001b (BCH_ENABLE_AUTO_GC): enable gc after writeback
+	 * - 0000 0010b (BCH_DO_AUTO_GC):     do gc after writeback
+	 * This is an optimization for following write request after writeback
+	 * finished, but read hit rate dropped due to clean data on cache is
+	 * discarded. Unless user explicitly sets it via sysfs, it won't be
+	 * enabled.
+	 */
+#define BCH_ENABLE_AUTO_GC	1
+#define BCH_DO_AUTO_GC		2
+	uint8_t			gc_after_writeback;
+
 	/*
 	 * The allocation code needs gc_mark in struct bucket to be correct, but
 	 * it's not while a gc is in progress. Protected by bucket_lock.

commit d2f96f487f471b4c718324ecd1540c89d2be52ac
Author: Shenghui Wang <shhuiw@foxmail.com>
Date:   Thu Dec 13 22:53:46 2018 +0800

    bcache: add comment for cache_set->fill_iter
    
    We have the following define for btree iterator:
            struct btree_iter {
                    size_t size, used;
            #ifdef CONFIG_BCACHE_DEBUG
                    struct btree_keys *b;
            #endif
                    struct btree_iter_set {
                            struct bkey *k, *end;
                    } data[MAX_BSETS];
            };
    
    We can see that the length of data[] field is static MAX_BSETS, which is
    defined as 4 currently.
    
    But a btree node on disk could have too many bsets for an iterator to fit
    on the stack - maybe far more that MAX_BSETS. Have to dynamically allocate
    space to host more btree_iter_sets.
    
    bch_cache_set_alloc() will make sure the pool cache_set->fill_iter can
    allocate an iterator equipped with enough room that can host
            (sb.bucket_size / sb.block_size)
    btree_iter_sets, which is more than static MAX_BSETS.
    
    bch_btree_node_read_done() will use that pool to allocate one iterator, to
    host many bsets in one btree node.
    
    Add more comment around cache_set->fill_iter to make code less confusing.
    
    Signed-off-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b61b83bbcfff..96d2213f279e 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -658,7 +658,11 @@ struct cache_set {
 
 	/*
 	 * A btree node on disk could have too many bsets for an iterator to fit
-	 * on the stack - have to dynamically allocate them
+	 * on the stack - have to dynamically allocate them.
+	 * bch_cache_set_alloc() will make sure the pool can allocate iterators
+	 * equipped with enough room that can host
+	 *     (sb.bucket_size / sb.block_size)
+	 * btree_iter_sets, which is more than static MAX_BSETS.
 	 */
 	mempool_t		fill_iter;
 

commit 91bafdf081b8ad8ab4977918ee45dffe3d744060
Author: Dongbo Cao <cdbdyx@163.com>
Date:   Mon Oct 8 20:41:17 2018 +0800

    bcache: remove useless parameter of bch_debug_init()
    
    Parameter "struct kobject *kobj" in bch_debug_init() is useless,
    remove it in this patch.
    
    Signed-off-by: Dongbo Cao <cdbdyx@163.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 954dad29e6e8..b61b83bbcfff 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1004,7 +1004,7 @@ void bch_open_buckets_free(struct cache_set *c);
 int bch_cache_allocator_start(struct cache *ca);
 
 void bch_debug_exit(void);
-void bch_debug_init(struct kobject *kobj);
+void bch_debug_init(void);
 void bch_request_exit(void);
 int bch_request_init(void);
 

commit 0f843e65d9eef4936929bb036c5f771fb261eea4
Author: Guoju Fang <fangguoju@gmail.com>
Date:   Thu Sep 27 23:41:46 2018 +0800

    bcache: add separate workqueue for journal_write to avoid deadlock
    
    After write SSD completed, bcache schedules journal_write work to
    system_wq, which is a public workqueue in system, without WQ_MEM_RECLAIM
    flag. system_wq is also a bound wq, and there may be no idle kworker on
    current processor. Creating a new kworker may unfortunately need to
    reclaim memory first, by shrinking cache and slab used by vfs, which
    depends on bcache device. That's a deadlock.
    
    This patch create a new workqueue for journal_write with WQ_MEM_RECLAIM
    flag. It's rescuer thread will work to avoid the deadlock.
    
    Signed-off-by: Guoju Fang <fangguoju@gmail.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 83504dd8100a..954dad29e6e8 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -965,6 +965,7 @@ void bch_prio_write(struct cache *ca);
 void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent);
 
 extern struct workqueue_struct *bcache_wq;
+extern struct workqueue_struct *bch_journal_wq;
 extern struct mutex bch_register_lock;
 extern struct list_head bch_cache_sets;
 

commit d0c1b89a409fb8f55783e95effeb60d703f81061
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:59 2018 +0800

    bcache: remove unnecessary space before ioctl function pointer arguments
    
    This is warned by checkpatch.pl, this patch removes the extra space.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 66e6d5639b38..83504dd8100a 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -275,8 +275,8 @@ struct bcache_device {
 
 	int (*cache_miss)(struct btree *b, struct search *s,
 			  struct bio *bio, unsigned int sectors);
-	int (*ioctl) (struct bcache_device *d, fmode_t mode,
-		      unsigned int cmd, unsigned long arg);
+	int (*ioctl)(struct bcache_device *d, fmode_t mode,
+		     unsigned int cmd, unsigned long arg);
 };
 
 struct io {

commit 958bf494ecf0a6755309569a9083bf123d6e3754
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:48 2018 +0800

    bcache: replace Symbolic permissions by octal permission numbers
    
    Symbolic permission names are used in bcache, for now octal permission
    numbers are encouraged to use for readability. This patch replaces
    all symbolic permissions by octal permission numbers.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 031a75a25d3e..66e6d5639b38 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -881,11 +881,11 @@ static inline uint8_t bucket_gc_gen(struct bucket *b)
 #define BUCKET_GC_GEN_MAX	96U
 
 #define kobj_attribute_write(n, fn)					\
-	static struct kobj_attribute ksysfs_##n = __ATTR(n, S_IWUSR, NULL, fn)
+	static struct kobj_attribute ksysfs_##n = __ATTR(n, 0200, NULL, fn)
 
 #define kobj_attribute_rw(n, show, store)				\
 	static struct kobj_attribute ksysfs_##n =			\
-		__ATTR(n, S_IWUSR|S_IRUSR, show, store)
+		__ATTR(n, 0600, show, store)
 
 static inline void wake_up_allocators(struct cache_set *c)
 {

commit b0d30981c05f32d8cc032b209408ca3224f05f36
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:47 2018 +0800

    bcache: style fixes for lines over 80 characters
    
    This patch fixes the lines over 80 characters into more lines, to minimize
    warnings by checkpatch.pl. There are still some lines exceed 80 characters,
    but it is better to be a single line and I don't change them.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 0bd505c61943..031a75a25d3e 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -614,8 +614,8 @@ struct cache_set {
 	uint16_t		min_prio;
 
 	/*
-	 * max(gen - last_gc) for all buckets. When it gets too big we have to gc
-	 * to keep gens from wrapping around.
+	 * max(gen - last_gc) for all buckets. When it gets too big we have to
+	 * gc to keep gens from wrapping around.
 	 */
 	uint8_t			need_gc;
 	struct gc_stat		gc_stats;

commit fc2d5988b5972bced859944986fb36d902ac3698
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:46 2018 +0800

    bcache: add identifier names to arguments of function definitions
    
    There are many function definitions do not have identifier argument names,
    scripts/checkpatch.pl complains warnings like this,
    
     WARNING: function definition argument 'struct bcache_device *' should
      also have an identifier name
      #16735: FILE: writeback.h:120:
      +void bch_sectors_dirty_init(struct bcache_device *);
    
    This patch adds identifier argument names to all bcache function
    definitions to fix such warnings.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index fd74dd075951..0bd505c61943 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -273,9 +273,10 @@ struct bcache_device {
 
 	unsigned int		data_csum:1;
 
-	int (*cache_miss)(struct btree *, struct search *,
-			  struct bio *, unsigned int);
-	int (*ioctl) (struct bcache_device *, fmode_t, unsigned int, unsigned long);
+	int (*cache_miss)(struct btree *b, struct search *s,
+			  struct bio *bio, unsigned int sectors);
+	int (*ioctl) (struct bcache_device *d, fmode_t mode,
+		      unsigned int cmd, unsigned long arg);
 };
 
 struct io {
@@ -925,41 +926,43 @@ static inline void wait_for_kthread_stop(void)
 /* Forward declarations */
 
 void bch_count_backing_io_errors(struct cached_dev *dc, struct bio *bio);
-void bch_count_io_errors(struct cache *, blk_status_t, int, const char *);
-void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
-			      blk_status_t, const char *);
-void bch_bbio_endio(struct cache_set *, struct bio *, blk_status_t,
-		const char *);
-void bch_bbio_free(struct bio *, struct cache_set *);
-struct bio *bch_bbio_alloc(struct cache_set *);
-
-void __bch_submit_bbio(struct bio *, struct cache_set *);
-void bch_submit_bbio(struct bio *, struct cache_set *,
-		     struct bkey *, unsigned int);
-
-uint8_t bch_inc_gen(struct cache *, struct bucket *);
-void bch_rescale_priorities(struct cache_set *, int);
-
-bool bch_can_invalidate_bucket(struct cache *, struct bucket *);
-void __bch_invalidate_one_bucket(struct cache *, struct bucket *);
-
-void __bch_bucket_free(struct cache *, struct bucket *);
-void bch_bucket_free(struct cache_set *, struct bkey *);
-
-long bch_bucket_alloc(struct cache *, unsigned int, bool);
-int __bch_bucket_alloc_set(struct cache_set *, unsigned int,
-			   struct bkey *, int, bool);
-int bch_bucket_alloc_set(struct cache_set *, unsigned int,
-			 struct bkey *, int, bool);
-bool bch_alloc_sectors(struct cache_set *, struct bkey *, unsigned int,
-		       unsigned int, unsigned int, bool);
+void bch_count_io_errors(struct cache *ca, blk_status_t error,
+			 int is_read, const char *m);
+void bch_bbio_count_io_errors(struct cache_set *c, struct bio *bio,
+			      blk_status_t error, const char *m);
+void bch_bbio_endio(struct cache_set *c, struct bio *bio,
+		    blk_status_t error, const char *m);
+void bch_bbio_free(struct bio *bio, struct cache_set *c);
+struct bio *bch_bbio_alloc(struct cache_set *c);
+
+void __bch_submit_bbio(struct bio *bio, struct cache_set *c);
+void bch_submit_bbio(struct bio *bio, struct cache_set *c,
+		     struct bkey *k, unsigned int ptr);
+
+uint8_t bch_inc_gen(struct cache *ca, struct bucket *b);
+void bch_rescale_priorities(struct cache_set *c, int sectors);
+
+bool bch_can_invalidate_bucket(struct cache *ca, struct bucket *b);
+void __bch_invalidate_one_bucket(struct cache *ca, struct bucket *b);
+
+void __bch_bucket_free(struct cache *ca, struct bucket *b);
+void bch_bucket_free(struct cache_set *c, struct bkey *k);
+
+long bch_bucket_alloc(struct cache *ca, unsigned int reserve, bool wait);
+int __bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,
+			   struct bkey *k, int n, bool wait);
+int bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,
+			 struct bkey *k, int n, bool wait);
+bool bch_alloc_sectors(struct cache_set *c, struct bkey *k,
+		       unsigned int sectors, unsigned int write_point,
+		       unsigned int write_prio, bool wait);
 bool bch_cached_dev_error(struct cached_dev *dc);
 
 __printf(2, 3)
-bool bch_cache_set_error(struct cache_set *, const char *, ...);
+bool bch_cache_set_error(struct cache_set *c, const char *fmt, ...);
 
-void bch_prio_write(struct cache *);
-void bch_write_bdev_super(struct cached_dev *, struct closure *);
+void bch_prio_write(struct cache *ca);
+void bch_write_bdev_super(struct cached_dev *dc, struct closure *parent);
 
 extern struct workqueue_struct *bcache_wq;
 extern struct mutex bch_register_lock;
@@ -971,30 +974,31 @@ extern struct kobj_type bch_cache_set_ktype;
 extern struct kobj_type bch_cache_set_internal_ktype;
 extern struct kobj_type bch_cache_ktype;
 
-void bch_cached_dev_release(struct kobject *);
-void bch_flash_dev_release(struct kobject *);
-void bch_cache_set_release(struct kobject *);
-void bch_cache_release(struct kobject *);
+void bch_cached_dev_release(struct kobject *kobj);
+void bch_flash_dev_release(struct kobject *kobj);
+void bch_cache_set_release(struct kobject *kobj);
+void bch_cache_release(struct kobject *kobj);
 
-int bch_uuid_write(struct cache_set *);
-void bcache_write_super(struct cache_set *);
+int bch_uuid_write(struct cache_set *c);
+void bcache_write_super(struct cache_set *c);
 
 int bch_flash_dev_create(struct cache_set *c, uint64_t size);
 
-int bch_cached_dev_attach(struct cached_dev *, struct cache_set *, uint8_t *);
-void bch_cached_dev_detach(struct cached_dev *);
-void bch_cached_dev_run(struct cached_dev *);
-void bcache_device_stop(struct bcache_device *);
-
-void bch_cache_set_unregister(struct cache_set *);
-void bch_cache_set_stop(struct cache_set *);
-
-struct cache_set *bch_cache_set_alloc(struct cache_sb *);
-void bch_btree_cache_free(struct cache_set *);
-int bch_btree_cache_alloc(struct cache_set *);
-void bch_moving_init_cache_set(struct cache_set *);
-int bch_open_buckets_alloc(struct cache_set *);
-void bch_open_buckets_free(struct cache_set *);
+int bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,
+			  uint8_t *set_uuid);
+void bch_cached_dev_detach(struct cached_dev *dc);
+void bch_cached_dev_run(struct cached_dev *dc);
+void bcache_device_stop(struct bcache_device *d);
+
+void bch_cache_set_unregister(struct cache_set *c);
+void bch_cache_set_stop(struct cache_set *c);
+
+struct cache_set *bch_cache_set_alloc(struct cache_sb *sb);
+void bch_btree_cache_free(struct cache_set *c);
+int bch_btree_cache_alloc(struct cache_set *c);
+void bch_moving_init_cache_set(struct cache_set *c);
+int bch_open_buckets_alloc(struct cache_set *c);
+void bch_open_buckets_free(struct cache_set *c);
 
 int bch_cache_allocator_start(struct cache *ca);
 

commit 1fae7cf05293d3a2c9e59c1bc59372322386467c
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:45 2018 +0800

    bcache: style fix to add a blank line after declarations
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 1ebd2d9d90d5..fd74dd075951 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -783,6 +783,7 @@ static inline struct bucket *PTR_BUCKET(struct cache_set *c,
 static inline uint8_t gen_after(uint8_t a, uint8_t b)
 {
 	uint8_t r = a - b;
+
 	return r > 128U ? 0 : r;
 }
 

commit 6f10f7d1b02b1bbc305f88d7696445dd38b13881
Author: Coly Li <colyli@suse.de>
Date:   Sat Aug 11 13:19:44 2018 +0800

    bcache: style fix to replace 'unsigned' by 'unsigned int'
    
    This patch fixes warning reported by checkpatch.pl by replacing 'unsigned'
    with 'unsigned int'.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Shenghui Wang <shhuiw@foxmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 05f82ff6f016..1ebd2d9d90d5 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -252,7 +252,7 @@ struct bcache_device {
 	struct kobject		kobj;
 
 	struct cache_set	*c;
-	unsigned		id;
+	unsigned int		id;
 #define BCACHEDEVNAME_SIZE	12
 	char			name[BCACHEDEVNAME_SIZE];
 
@@ -264,18 +264,18 @@ struct bcache_device {
 #define BCACHE_DEV_UNLINK_DONE		2
 #define BCACHE_DEV_WB_RUNNING		3
 #define BCACHE_DEV_RATE_DW_RUNNING	4
-	unsigned		nr_stripes;
-	unsigned		stripe_size;
+	unsigned int		nr_stripes;
+	unsigned int		stripe_size;
 	atomic_t		*stripe_sectors_dirty;
 	unsigned long		*full_dirty_stripes;
 
 	struct bio_set		bio_split;
 
-	unsigned		data_csum:1;
+	unsigned int		data_csum:1;
 
 	int (*cache_miss)(struct btree *, struct search *,
-			  struct bio *, unsigned);
-	int (*ioctl) (struct bcache_device *, fmode_t, unsigned, unsigned long);
+			  struct bio *, unsigned int);
+	int (*ioctl) (struct bcache_device *, fmode_t, unsigned int, unsigned long);
 };
 
 struct io {
@@ -284,7 +284,7 @@ struct io {
 	struct list_head	lru;
 
 	unsigned long		jiffies;
-	unsigned		sequential;
+	unsigned int		sequential;
 	sector_t		last;
 };
 
@@ -358,18 +358,18 @@ struct cached_dev {
 	struct cache_accounting	accounting;
 
 	/* The rest of this all shows up in sysfs */
-	unsigned		sequential_cutoff;
-	unsigned		readahead;
+	unsigned int		sequential_cutoff;
+	unsigned int		readahead;
 
-	unsigned		io_disable:1;
-	unsigned		verify:1;
-	unsigned		bypass_torture_test:1;
+	unsigned int		io_disable:1;
+	unsigned int		verify:1;
+	unsigned int		bypass_torture_test:1;
 
-	unsigned		partial_stripes_expensive:1;
-	unsigned		writeback_metadata:1;
-	unsigned		writeback_running:1;
+	unsigned int		partial_stripes_expensive:1;
+	unsigned int		writeback_metadata:1;
+	unsigned int		writeback_running:1;
 	unsigned char		writeback_percent;
-	unsigned		writeback_delay;
+	unsigned int		writeback_delay;
 
 	uint64_t		writeback_rate_target;
 	int64_t			writeback_rate_proportional;
@@ -377,16 +377,16 @@ struct cached_dev {
 	int64_t			writeback_rate_integral_scaled;
 	int32_t			writeback_rate_change;
 
-	unsigned		writeback_rate_update_seconds;
-	unsigned		writeback_rate_i_term_inverse;
-	unsigned		writeback_rate_p_term_inverse;
-	unsigned		writeback_rate_minimum;
+	unsigned int		writeback_rate_update_seconds;
+	unsigned int		writeback_rate_i_term_inverse;
+	unsigned int		writeback_rate_p_term_inverse;
+	unsigned int		writeback_rate_minimum;
 
 	enum stop_on_failure	stop_when_cache_set_failed;
 #define DEFAULT_CACHED_DEV_ERROR_LIMIT	64
 	atomic_t		io_errors;
-	unsigned		error_limit;
-	unsigned		offline_seconds;
+	unsigned int		error_limit;
+	unsigned int		offline_seconds;
 
 	char			backing_dev_name[BDEVNAME_SIZE];
 };
@@ -447,7 +447,7 @@ struct cache {
 	 * until a gc finishes - otherwise we could pointlessly burn a ton of
 	 * cpu
 	 */
-	unsigned		invalidate_needs_gc;
+	unsigned int		invalidate_needs_gc;
 
 	bool			discard; /* Get rid of? */
 
@@ -472,7 +472,7 @@ struct gc_stat {
 
 	size_t			nkeys;
 	uint64_t		data;	/* sectors */
-	unsigned		in_use; /* percent */
+	unsigned int		in_use; /* percent */
 };
 
 /*
@@ -518,7 +518,7 @@ struct cache_set {
 	int			caches_loaded;
 
 	struct bcache_device	**devices;
-	unsigned		devices_max_used;
+	unsigned int		devices_max_used;
 	atomic_t		attached_dev_nr;
 	struct list_head	cached_devs;
 	uint64_t		cached_dev_sectors;
@@ -548,7 +548,7 @@ struct cache_set {
 	 * Default number of pages for a new btree node - may be less than a
 	 * full bucket
 	 */
-	unsigned		btree_pages;
+	unsigned int		btree_pages;
 
 	/*
 	 * Lists of struct btrees; lru is the list for structs that have memory
@@ -571,7 +571,7 @@ struct cache_set {
 	struct list_head	btree_cache_freed;
 
 	/* Number of elements in btree_cache + btree_cache_freeable lists */
-	unsigned		btree_cache_used;
+	unsigned int		btree_cache_used;
 
 	/*
 	 * If we need to allocate memory for a new btree node and that
@@ -649,7 +649,7 @@ struct cache_set {
 	struct mutex		verify_lock;
 #endif
 
-	unsigned		nr_uuids;
+	unsigned int		nr_uuids;
 	struct uuid_entry	*uuids;
 	BKEY_PADDED(uuid_bucket);
 	struct closure		uuid_write;
@@ -670,12 +670,12 @@ struct cache_set {
 	struct journal		journal;
 
 #define CONGESTED_MAX		1024
-	unsigned		congested_last_us;
+	unsigned int		congested_last_us;
 	atomic_t		congested;
 
 	/* The rest of this all shows up in sysfs */
-	unsigned		congested_read_threshold_us;
-	unsigned		congested_write_threshold_us;
+	unsigned int		congested_read_threshold_us;
+	unsigned int		congested_write_threshold_us;
 
 	struct time_stats	btree_gc_time;
 	struct time_stats	btree_split_time;
@@ -694,16 +694,16 @@ struct cache_set {
 		ON_ERROR_PANIC,
 	}			on_error;
 #define DEFAULT_IO_ERROR_LIMIT 8
-	unsigned		error_limit;
-	unsigned		error_decay;
+	unsigned int		error_limit;
+	unsigned int		error_decay;
 
 	unsigned short		journal_delay_ms;
 	bool			expensive_debug_checks;
-	unsigned		verify:1;
-	unsigned		key_merging_disabled:1;
-	unsigned		gc_always_rewrite:1;
-	unsigned		shrinker_disabled:1;
-	unsigned		copy_gc_enabled:1;
+	unsigned int		verify:1;
+	unsigned int		key_merging_disabled:1;
+	unsigned int		gc_always_rewrite:1;
+	unsigned int		shrinker_disabled:1;
+	unsigned int		copy_gc_enabled:1;
 
 #define BUCKET_HASH_BITS	12
 	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];
@@ -712,7 +712,7 @@ struct cache_set {
 };
 
 struct bbio {
-	unsigned		submit_time_us;
+	unsigned int		submit_time_us;
 	union {
 		struct bkey	key;
 		uint64_t	_pad[3];
@@ -729,10 +729,10 @@ struct bbio {
 
 #define btree_bytes(c)		((c)->btree_pages * PAGE_SIZE)
 #define btree_blocks(b)							\
-	((unsigned) (KEY_SIZE(&b->key) >> (b)->c->block_bits))
+	((unsigned int) (KEY_SIZE(&b->key) >> (b)->c->block_bits))
 
 #define btree_default_blocks(c)						\
-	((unsigned) ((PAGE_SECTORS * (c)->btree_pages) >> (c)->block_bits))
+	((unsigned int) ((PAGE_SECTORS * (c)->btree_pages) >> (c)->block_bits))
 
 #define bucket_pages(c)		((c)->sb.bucket_size / PAGE_SECTORS)
 #define bucket_bytes(c)		((c)->sb.bucket_size << 9)
@@ -761,21 +761,21 @@ static inline sector_t bucket_remainder(struct cache_set *c, sector_t s)
 
 static inline struct cache *PTR_CACHE(struct cache_set *c,
 				      const struct bkey *k,
-				      unsigned ptr)
+				      unsigned int ptr)
 {
 	return c->cache[PTR_DEV(k, ptr)];
 }
 
 static inline size_t PTR_BUCKET_NR(struct cache_set *c,
 				   const struct bkey *k,
-				   unsigned ptr)
+				   unsigned int ptr)
 {
 	return sector_to_bucket(c, PTR_OFFSET(k, ptr));
 }
 
 static inline struct bucket *PTR_BUCKET(struct cache_set *c,
 					const struct bkey *k,
-					unsigned ptr)
+					unsigned int ptr)
 {
 	return PTR_CACHE(c, k, ptr)->buckets + PTR_BUCKET_NR(c, k, ptr);
 }
@@ -787,13 +787,13 @@ static inline uint8_t gen_after(uint8_t a, uint8_t b)
 }
 
 static inline uint8_t ptr_stale(struct cache_set *c, const struct bkey *k,
-				unsigned i)
+				unsigned int i)
 {
 	return gen_after(PTR_BUCKET(c, k, i)->gen, PTR_GEN(k, i));
 }
 
 static inline bool ptr_available(struct cache_set *c, const struct bkey *k,
-				 unsigned i)
+				 unsigned int i)
 {
 	return (PTR_DEV(k, i) < MAX_CACHES_PER_SET) && PTR_CACHE(c, k, i);
 }
@@ -888,7 +888,7 @@ static inline uint8_t bucket_gc_gen(struct bucket *b)
 static inline void wake_up_allocators(struct cache_set *c)
 {
 	struct cache *ca;
-	unsigned i;
+	unsigned int i;
 
 	for_each_cache(ca, c, i)
 		wake_up_process(ca->alloc_thread);
@@ -933,7 +933,8 @@ void bch_bbio_free(struct bio *, struct cache_set *);
 struct bio *bch_bbio_alloc(struct cache_set *);
 
 void __bch_submit_bbio(struct bio *, struct cache_set *);
-void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);
+void bch_submit_bbio(struct bio *, struct cache_set *,
+		     struct bkey *, unsigned int);
 
 uint8_t bch_inc_gen(struct cache *, struct bucket *);
 void bch_rescale_priorities(struct cache_set *, int);
@@ -944,13 +945,13 @@ void __bch_invalidate_one_bucket(struct cache *, struct bucket *);
 void __bch_bucket_free(struct cache *, struct bucket *);
 void bch_bucket_free(struct cache_set *, struct bkey *);
 
-long bch_bucket_alloc(struct cache *, unsigned, bool);
-int __bch_bucket_alloc_set(struct cache_set *, unsigned,
+long bch_bucket_alloc(struct cache *, unsigned int, bool);
+int __bch_bucket_alloc_set(struct cache_set *, unsigned int,
 			   struct bkey *, int, bool);
-int bch_bucket_alloc_set(struct cache_set *, unsigned,
+int bch_bucket_alloc_set(struct cache_set *, unsigned int,
 			 struct bkey *, int, bool);
-bool bch_alloc_sectors(struct cache_set *, struct bkey *, unsigned,
-		       unsigned, unsigned, bool);
+bool bch_alloc_sectors(struct cache_set *, struct bkey *, unsigned int,
+		       unsigned int, unsigned int, bool);
 bool bch_cached_dev_error(struct cached_dev *dc);
 
 __printf(2, 3)

commit ea8c5356d39048bc94bae068228f51ddbecc6b89
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:49 2018 +0800

    bcache: set max writeback rate when I/O request is idle
    
    Commit b1092c9af9ed ("bcache: allow quick writeback when backing idle")
    allows the writeback rate to be faster if there is no I/O request on a
    bcache device. It works well if there is only one bcache device attached
    to the cache set. If there are many bcache devices attached to a cache
    set, it may introduce performance regression because multiple faster
    writeback threads of the idle bcache devices will compete the btree level
    locks with the bcache device who have I/O requests coming.
    
    This patch fixes the above issue by only permitting fast writebac when
    all bcache devices attached on the cache set are idle. And if one of the
    bcache devices has new I/O request coming, minimized all writeback
    throughput immediately and let PI controller __update_writeback_rate()
    to decide the upcoming writeback rate for each bcache device.
    
    Also when all bcache devices are idle, limited wrieback rate to a small
    number is wast of thoughput, especially when backing devices are slower
    non-rotation devices (e.g. SATA SSD). This patch sets a max writeback
    rate for each backing device if the whole cache set is idle. A faster
    writeback rate in idle time means new I/Os may have more available space
    for dirty data, and people may observe a better write performance then.
    
    Please note bcache may change its cache mode in run time, and this patch
    still works if the cache mode is switched from writeback mode and there
    is still dirty data on cache.
    
    Fixes: Commit b1092c9af9ed ("bcache: allow quick writeback when backing idle")
    Cc: stable@vger.kernel.org #4.16+
    Signed-off-by: Coly Li <colyli@suse.de>
    Tested-by: Kai Krakow <kai@kaishome.de>
    Tested-by: Stefan Priebe <s.priebe@profihost.ag>
    Cc: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b393b3fd06b6..05f82ff6f016 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -328,13 +328,6 @@ struct cached_dev {
 	 */
 	atomic_t		has_dirty;
 
-	/*
-	 * Set to zero by things that touch the backing volume-- except
-	 * writeback.  Incremented by writeback.  Used to determine when to
-	 * accelerate idle writeback.
-	 */
-	atomic_t		backing_idle;
-
 	struct bch_ratelimit	writeback_rate;
 	struct delayed_work	writeback_rate_update;
 
@@ -515,6 +508,8 @@ struct cache_set {
 	struct cache_accounting accounting;
 
 	unsigned long		flags;
+	atomic_t		idle_counter;
+	atomic_t		at_max_writeback_rate;
 
 	struct cache_sb		sb;
 
@@ -524,6 +519,7 @@ struct cache_set {
 
 	struct bcache_device	**devices;
 	unsigned		devices_max_used;
+	atomic_t		attached_dev_nr;
 	struct list_head	cached_devs;
 	uint64_t		cached_dev_sectors;
 	atomic_long_t		flash_dev_dirty_sectors;

commit cb329dec11822f84f9d7309766a1f3a35d3d182a
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:46 2018 +0800

    bcache: fix mistaken code comments in bcache.h
    
    This patch updates the code comment in struct cache with correct array
    names, to make the code to be more comprehensible.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 0a3e82b0876d..b393b3fd06b6 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -423,9 +423,9 @@ struct cache {
 	/*
 	 * When allocating new buckets, prio_write() gets first dibs - since we
 	 * may not be allocate at all without writing priorities and gens.
-	 * prio_buckets[] contains the last buckets we wrote priorities to (so
-	 * gc can mark them as metadata), prio_next[] contains the buckets
-	 * allocated for the next prio write.
+	 * prio_last_buckets[] contains the last buckets we wrote priorities to
+	 * (so gc can mark them as metadata), prio_buckets[] contains the
+	 * buckets allocated for the next prio write.
 	 */
 	uint64_t		*prio_buckets;
 	uint64_t		*prio_last_buckets;

commit 78ac2107176baa0daf65b0fb8e561d2ed14c83ca
Author: Coly Li <colyli@suse.de>
Date:   Thu Aug 9 15:48:42 2018 +0800

    bcache: do not check return value of debugfs_create_dir()
    
    Greg KH suggests that normal code should not care about debugfs. Therefore
    no matter successful or failed of debugfs_create_dir() execution, it is
    unncessary to check its return value.
    
    There are two functions called debugfs_create_dir() and check the return
    value, which are bch_debug_init() and closure_debug_init(). This patch
    changes these two functions from int to void type, and ignore return values
    of debugfs_create_dir().
    
    This patch does not fix exact bug, just makes things work as they should.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Suggested-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org
    Cc: Kai Krakow <kai@kaishome.de>
    Cc: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 872ef4d67711..0a3e82b0876d 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1001,7 +1001,7 @@ void bch_open_buckets_free(struct cache_set *);
 int bch_cache_allocator_start(struct cache *ca);
 
 void bch_debug_exit(void);
-int bch_debug_init(struct kobject *);
+void bch_debug_init(struct kobject *kobj);
 void bch_request_exit(void);
 int bch_request_init(void);
 

commit 5c25c4fc74af40657606dd01df27cc5eb9efb26c
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Thu Jul 26 12:17:34 2018 +0800

    bcache: finish incremental GC
    
    In GC thread, we record the latest GC key in gc_done, which is expected
    to be used for incremental GC, but in currently code, we didn't realize
    it. When GC runs, front side IO would be blocked until the GC over, it
    would be a long time if there is a lot of btree nodes.
    
    This patch realizes incremental GC, the main ideal is that, when there
    are front side I/Os, after GC some nodes (100), we stop GC, release locker
    of the btree node, and go to process the front side I/Os for some times
    (100 ms), then go back to GC again.
    
    By this patch, when we doing GC, I/Os are not blocked all the time, and
    there is no obvious I/Os zero jump problem any more.
    
    Patch v2: Rename some variables and macros name as Coly suggested.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 3226d38bf859..872ef4d67711 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -474,6 +474,7 @@ struct cache {
 
 struct gc_stat {
 	size_t			nodes;
+	size_t			nodes_pre;
 	size_t			key_bytes;
 
 	size_t			nkeys;
@@ -603,6 +604,10 @@ struct cache_set {
 	 * rescale; when it hits 0 we rescale all the bucket priorities.
 	 */
 	atomic_t		rescale;
+	/*
+	 * used for GC, identify if any front side I/Os is inflight
+	 */
+	atomic_t		search_inflight;
 	/*
 	 * When we invalidate buckets, we use both the priority and the amount
 	 * of good data to determine which buckets to reuse first - to weight

commit 99a27d59bd7b2ce1a82a4e826e8e7881f4d4954d
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Thu Jul 26 12:17:33 2018 +0800

    bcache: simplify the calculation of the total amount of flash dirty data
    
    Currently we calculate the total amount of flash only devices dirty data
    by adding the dirty data of each flash only device under registering
    locker. It is very inefficient.
    
    In this patch, we add a member flash_dev_dirty_sectors in struct cache_set
    to record the total amount of flash only devices dirty data in real time,
    so we didn't need to calculate the total amount of dirty data any more.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d6bf294f3907..3226d38bf859 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -525,6 +525,7 @@ struct cache_set {
 	unsigned		devices_max_used;
 	struct list_head	cached_devs;
 	uint64_t		cached_dev_sectors;
+	atomic_long_t		flash_dev_dirty_sectors;
 	struct closure		caching;
 
 	struct closure		sb_write;

commit d19936a26658a7a53edd5619d631ee2c2c3151a2
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sun May 20 18:25:51 2018 -0400

    bcache: convert to bioset_init()/mempool_init()
    
    Convert bcache to embedded bio sets.
    
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6220cbda55c1..d6bf294f3907 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -269,7 +269,7 @@ struct bcache_device {
 	atomic_t		*stripe_sectors_dirty;
 	unsigned long		*full_dirty_stripes;
 
-	struct bio_set		*bio_split;
+	struct bio_set		bio_split;
 
 	unsigned		data_csum:1;
 
@@ -530,9 +530,9 @@ struct cache_set {
 	struct closure		sb_write;
 	struct semaphore	sb_write_mutex;
 
-	mempool_t		*search;
-	mempool_t		*bio_meta;
-	struct bio_set		*bio_split;
+	mempool_t		search;
+	mempool_t		bio_meta;
+	struct bio_set		bio_split;
 
 	/* For the btree cache */
 	struct shrinker		shrink;
@@ -657,7 +657,7 @@ struct cache_set {
 	 * A btree node on disk could have too many bsets for an iterator to fit
 	 * on the stack - have to dynamically allocate them
 	 */
-	mempool_t		*fill_iter;
+	mempool_t		fill_iter;
 
 	struct bset_sort_state	sort;
 

commit 04cbc21137bfa4d7b8771a5b14f3d6c9b2aee671
Author: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
Date:   Mon May 28 15:37:42 2018 +0800

    bcache: Move couple of string arrays to sysfs.c
    
    There is couple of string arrays that are used exclusively in sysfs.c.
    Move it to there and make them static.
    
    Besides above, it will allow further clean up.
    
    No functional change intended.
    
    Signed-off-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index ba98547c4ca0..6220cbda55c1 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -958,8 +958,6 @@ void bch_prio_write(struct cache *);
 void bch_write_bdev_super(struct cached_dev *, struct closure *);
 
 extern struct workqueue_struct *bcache_wq;
-extern const char * const bch_cache_modes[];
-extern const char * const bch_stop_on_failure_modes[];
 extern struct mutex bch_register_lock;
 extern struct list_head bch_cache_sets;
 

commit 0f0709e6bfc3ce4e8e1c0e8573490c45f76cfeee
Author: Coly Li <colyli@suse.de>
Date:   Mon May 28 15:37:41 2018 +0800

    bcache: stop bcache device when backing device is offline
    
    Currently bcache does not handle backing device failure, if backing
    device is offline and disconnected from system, its bcache device can still
    be accessible. If the bcache device is in writeback mode, I/O requests even
    can success if the requests hit on cache device. That is to say, when and
    how bcache handles offline backing device is undefined.
    
    This patch tries to handle backing device offline in a rather simple way,
    - Add cached_dev->status_update_thread kernel thread to update backing
      device status in every 1 second.
    - Add cached_dev->offline_seconds to record how many seconds the backing
      device is observed to be offline. If the backing device is offline for
      BACKING_DEV_OFFLINE_TIMEOUT (30) seconds, set dc->io_disable to 1 and
      call bcache_device_stop() to stop the bache device which linked to the
      offline backing device.
    
    Now if a backing device is offline for BACKING_DEV_OFFLINE_TIMEOUT seconds,
    its bcache device will be removed, then user space application writing on
    it will get error immediately, and handler the device failure in time.
    
    This patch is quite simple, does not handle more complicated situations.
    Once the bcache device is stopped, users need to recovery the backing
    device, register and attach it manually.
    
    Changelog:
    v3: call wait_for_kthread_stop() before exits kernel thread.
    v2: remove "bcache: " prefix when calling pr_warn().
    v1: initial version.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 3a0cfb237af9..ba98547c4ca0 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -345,6 +345,7 @@ struct cached_dev {
 
 	struct keybuf		writeback_keys;
 
+	struct task_struct	*status_update_thread;
 	/*
 	 * Order the write-half of writeback operations strongly in dispatch
 	 * order.  (Maintain LBA order; don't allow reads completing out of
@@ -392,6 +393,7 @@ struct cached_dev {
 #define DEFAULT_CACHED_DEV_ERROR_LIMIT	64
 	atomic_t		io_errors;
 	unsigned		error_limit;
+	unsigned		offline_seconds;
 
 	char			backing_dev_name[BDEVNAME_SIZE];
 };

commit 6e916a7eb1bc045f4e27355632ee7692014e6e60
Author: Coly Li <colyli@suse.de>
Date:   Thu May 3 18:51:32 2018 +0800

    bcache: store disk name in struct cache and struct cached_dev
    
    Current code uses bdevname() or bio_devname() to reference gendisk
    disk name when bcache needs to display the disk names in kernel message.
    It was safe before bcache device failure handling patch set merged in,
    because when devices are failed, there was deadlock to prevent bcache
    printing error messages with gendisk disk name. But after the failure
    handling patch set merged, the deadlock is fixed, so it is possible
    that the gendisk structure bdev->hd_disk is released when bdevname() is
    called to reference bdev->bd_disk->disk_name[]. This is why I receive
    bug report of NULL pointers deference panic.
    
    This patch stores gendisk disk name in a buffer inside struct cache and
    struct cached_dev, then print out the offline device name won't reference
    bdev->hd_disk anymore. And this patch also avoids extra function calls
    of bdevname() and bio_devnmae().
    
    Changelog:
    v3, add Reviewed-by from Hannes.
    v2, call bdevname() earlier in register_bdev()
    v1, first version with segguestion from Junhui Tang.
    
    Fixes: c7b7bd07404c5 ("bcache: add io_disable to struct cached_dev")
    Fixes: 5138ac6748e38 ("bcache: fix misleading error message in bch_count_io_errors()")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d338b7086013..3a0cfb237af9 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -392,6 +392,8 @@ struct cached_dev {
 #define DEFAULT_CACHED_DEV_ERROR_LIMIT	64
 	atomic_t		io_errors;
 	unsigned		error_limit;
+
+	char			backing_dev_name[BDEVNAME_SIZE];
 };
 
 enum alloc_reserve {
@@ -464,6 +466,8 @@ struct cache {
 	atomic_long_t		meta_sectors_written;
 	atomic_long_t		btree_sectors_written;
 	atomic_long_t		sectors_written;
+
+	char			cache_dev_name[BDEVNAME_SIZE];
 };
 
 struct gc_stat {

commit c7b7bd07404c52d8b9c6fd2fe794052ac367a818
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:25 2018 -0700

    bcache: add io_disable to struct cached_dev
    
    If a bcache device is configured to writeback mode, current code does not
    handle write I/O errors on backing devices properly.
    
    In writeback mode, write request is written to cache device, and
    latter being flushed to backing device. If I/O failed when writing from
    cache device to the backing device, bcache code just ignores the error and
    upper layer code is NOT noticed that the backing device is broken.
    
    This patch tries to handle backing device failure like how the cache device
    failure is handled,
    - Add a error counter 'io_errors' and error limit 'error_limit' in struct
      cached_dev. Add another io_disable to struct cached_dev to disable I/Os
      on the problematic backing device.
    - When I/O error happens on backing device, increase io_errors counter. And
      if io_errors reaches error_limit, set cache_dev->io_disable to true, and
      stop the bcache device.
    
    The result is, if backing device is broken of disconnected, and I/O errors
    reach its error limit, backing device will be disabled and the associated
    bcache device will be removed from system.
    
    Changelog:
    v2: remove "bcache: " prefix in pr_error(), and use correct name string to
        print out bcache device gendisk name.
    v1: indeed this is new added in v2 patch set.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 5e9f3610c6fd..d338b7086013 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -367,6 +367,7 @@ struct cached_dev {
 	unsigned		sequential_cutoff;
 	unsigned		readahead;
 
+	unsigned		io_disable:1;
 	unsigned		verify:1;
 	unsigned		bypass_torture_test:1;
 
@@ -388,6 +389,9 @@ struct cached_dev {
 	unsigned		writeback_rate_minimum;
 
 	enum stop_on_failure	stop_when_cache_set_failed;
+#define DEFAULT_CACHED_DEV_ERROR_LIMIT	64
+	atomic_t		io_errors;
+	unsigned		error_limit;
 };
 
 enum alloc_reserve {
@@ -911,6 +915,7 @@ static inline void wait_for_kthread_stop(void)
 
 /* Forward declarations */
 
+void bch_count_backing_io_errors(struct cached_dev *dc, struct bio *bio);
 void bch_count_io_errors(struct cache *, blk_status_t, int, const char *);
 void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
 			      blk_status_t, const char *);
@@ -938,6 +943,7 @@ int bch_bucket_alloc_set(struct cache_set *, unsigned,
 			 struct bkey *, int, bool);
 bool bch_alloc_sectors(struct cache_set *, struct bkey *, unsigned,
 		       unsigned, unsigned, bool);
+bool bch_cached_dev_error(struct cached_dev *dc);
 
 __printf(2, 3)
 bool bch_cache_set_error(struct cache_set *, const char *, ...);

commit 7e027ca4b534b6b99a7c0471e13ba075ffa3f482
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:18 2018 -0700

    bcache: add stop_when_cache_set_failed option to backing device
    
    When there are too many I/O errors on cache device, current bcache code
    will retire the whole cache set, and detach all bcache devices. But the
    detached bcache devices are not stopped, which is problematic when bcache
    is in writeback mode.
    
    If the retired cache set has dirty data of backing devices, continue
    writing to bcache device will write to backing device directly. If the
    LBA of write request has a dirty version cached on cache device, next time
    when the cache device is re-registered and backing device re-attached to
    it again, the stale dirty data on cache device will be written to backing
    device, and overwrite latest directly written data. This situation causes
    a quite data corruption.
    
    But we cannot simply stop all attached bcache devices when the cache set is
    broken or disconnected. For example, use bcache to accelerate performance
    of an email service. In such workload, if cache device is broken but no
    dirty data lost, keep the bcache device alive and permit email service
    continue to access user data might be a better solution for the cache
    device failure.
    
    Nix <nix@esperi.org.uk> points out the issue and provides the above example
    to explain why it might be necessary to not stop bcache device for broken
    cache device. Pavel Goran <via-bcache@pvgoran.name> provides a brilliant
    suggestion to provide "always" and "auto" options to per-cached device
    sysfs file stop_when_cache_set_failed. If cache set is retiring and the
    backing device has no dirty data on cache, it should be safe to keep the
    bcache device alive. In this case, if stop_when_cache_set_failed is set to
    "auto", the device failure handling code will not stop this bcache device
    and permit application to access the backing device with a unattached
    bcache device.
    
    Changelog:
    [mlyle: edited to not break string constants across lines]
    v3: fix typos pointed out by Nix.
    v2: change option values of stop_when_cache_set_failed from 1/0 to
        "auto"/"always".
    v1: initial version, stop_when_cache_set_failed can be 0 (not stop) or 1
        (always stop).
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Cc: Nix <nix@esperi.org.uk>
    Cc: Pavel Goran <via-bcache@pvgoran.name>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 8a0327581d62..5e9f3610c6fd 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -288,6 +288,12 @@ struct io {
 	sector_t		last;
 };
 
+enum stop_on_failure {
+	BCH_CACHED_DEV_STOP_AUTO = 0,
+	BCH_CACHED_DEV_STOP_ALWAYS,
+	BCH_CACHED_DEV_STOP_MODE_MAX,
+};
+
 struct cached_dev {
 	struct list_head	list;
 	struct bcache_device	disk;
@@ -380,6 +386,8 @@ struct cached_dev {
 	unsigned		writeback_rate_i_term_inverse;
 	unsigned		writeback_rate_p_term_inverse;
 	unsigned		writeback_rate_minimum;
+
+	enum stop_on_failure	stop_when_cache_set_failed;
 };
 
 enum alloc_reserve {
@@ -939,6 +947,7 @@ void bch_write_bdev_super(struct cached_dev *, struct closure *);
 
 extern struct workqueue_struct *bcache_wq;
 extern const char * const bch_cache_modes[];
+extern const char * const bch_stop_on_failure_modes[];
 extern struct mutex bch_register_lock;
 extern struct list_head bch_cache_sets;
 

commit 771f393e8ffc9b3066e4830ee5f7391b8e8874f1
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:17 2018 -0700

    bcache: add CACHE_SET_IO_DISABLE to struct cache_set flags
    
    When too many I/Os failed on cache device, bch_cache_set_error() is called
    in the error handling code path to retire whole problematic cache set. If
    new I/O requests continue to come and take refcount dc->count, the cache
    set won't be retired immediately, this is a problem.
    
    Further more, there are several kernel thread and self-armed kernel work
    may still running after bch_cache_set_error() is called. It needs to wait
    quite a while for them to stop, or they won't stop at all. They also
    prevent the cache set from being retired.
    
    The solution in this patch is, to add per cache set flag to disable I/O
    request on this cache and all attached backing devices. Then new coming I/O
    requests can be rejected in *_make_request() before taking refcount, kernel
    threads and self-armed kernel worker can stop very fast when flags bit
    CACHE_SET_IO_DISABLE is set.
    
    Because bcache also do internal I/Os for writeback, garbage collection,
    bucket allocation, journaling, this kind of I/O should be disabled after
    bch_cache_set_error() is called. So closure_bio_submit() is modified to
    check whether CACHE_SET_IO_DISABLE is set on cache_set->flags. If set,
    closure_bio_submit() will set bio->bi_status to BLK_STS_IOERR and
    return, generic_make_request() won't be called.
    
    A sysfs interface is also added to set or clear CACHE_SET_IO_DISABLE bit
    from cache_set->flags, to disable or enable cache set I/O for debugging. It
    is helpful to trigger more corner case issues for failed cache device.
    
    Changelog
    v4, add wait_for_kthread_stop(), and call it before exits writeback and gc
        kernel threads.
    v3, change CACHE_SET_IO_DISABLE from 4 to 3, since it is bit index.
        remove "bcache: " prefix when printing out kernel message.
    v2, more changes by previous review,
    - Use CACHE_SET_IO_DISABLE of cache_set->flags, suggested by Junhui.
    - Check CACHE_SET_IO_DISABLE in bch_btree_gc() to stop a while-loop, this
      is reported and inspired from origal patch of Pavel Vazharov.
    v1, initial version.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Pavel Vazharov <freakpv@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b5ddb848cd31..8a0327581d62 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -188,6 +188,7 @@
 #include <linux/refcount.h>
 #include <linux/types.h>
 #include <linux/workqueue.h>
+#include <linux/kthread.h>
 
 #include "bset.h"
 #include "util.h"
@@ -475,10 +476,15 @@ struct gc_stat {
  *
  * CACHE_SET_RUNNING means all cache devices have been registered and journal
  * replay is complete.
+ *
+ * CACHE_SET_IO_DISABLE is set when bcache is stopping the whold cache set, all
+ * external and internal I/O should be denied when this flag is set.
+ *
  */
 #define CACHE_SET_UNREGISTERING		0
 #define	CACHE_SET_STOPPING		1
 #define	CACHE_SET_RUNNING		2
+#define CACHE_SET_IO_DISABLE		3
 
 struct cache_set {
 	struct closure		cl;
@@ -868,6 +874,33 @@ static inline void wake_up_allocators(struct cache_set *c)
 		wake_up_process(ca->alloc_thread);
 }
 
+static inline void closure_bio_submit(struct cache_set *c,
+				      struct bio *bio,
+				      struct closure *cl)
+{
+	closure_get(cl);
+	if (unlikely(test_bit(CACHE_SET_IO_DISABLE, &c->flags))) {
+		bio->bi_status = BLK_STS_IOERR;
+		bio_endio(bio);
+		return;
+	}
+	generic_make_request(bio);
+}
+
+/*
+ * Prevent the kthread exits directly, and make sure when kthread_stop()
+ * is called to stop a kthread, it is still alive. If a kthread might be
+ * stopped by CACHE_SET_IO_DISABLE bit set, wait_for_kthread_stop() is
+ * necessary before the kthread returns.
+ */
+static inline void wait_for_kthread_stop(void)
+{
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule();
+	}
+}
+
 /* Forward declarations */
 
 void bch_count_io_errors(struct cache *, blk_status_t, int, const char *);

commit 3fd47bfe55b00d5ac7b0a44c9301c07be39b1082
Author: Coly Li <colyli@suse.de>
Date:   Sun Mar 18 17:36:16 2018 -0700

    bcache: stop dc->writeback_rate_update properly
    
    struct delayed_work writeback_rate_update in struct cache_dev is a delayed
    worker to call function update_writeback_rate() in period (the interval is
    defined by dc->writeback_rate_update_seconds).
    
    When a metadate I/O error happens on cache device, bcache error handling
    routine bch_cache_set_error() will call bch_cache_set_unregister() to
    retire whole cache set. On the unregister code path, this delayed work is
    stopped by calling cancel_delayed_work_sync(&dc->writeback_rate_update).
    
    dc->writeback_rate_update is a special delayed work from others in bcache.
    In its routine update_writeback_rate(), this delayed work is re-armed
    itself. That means when cancel_delayed_work_sync() returns, this delayed
    work can still be executed after several seconds defined by
    dc->writeback_rate_update_seconds.
    
    The problem is, after cancel_delayed_work_sync() returns, the cache set
    unregister code path will continue and release memory of struct cache set.
    Then the delayed work is scheduled to run, __update_writeback_rate()
    will reference the already released cache_set memory, and trigger a NULL
    pointer deference fault.
    
    This patch introduces two more bcache device flags,
    - BCACHE_DEV_WB_RUNNING
      bit set:  bcache device is in writeback mode and running, it is OK for
                dc->writeback_rate_update to re-arm itself.
      bit clear:bcache device is trying to stop dc->writeback_rate_update,
                this delayed work should not re-arm itself and quit.
    - BCACHE_DEV_RATE_DW_RUNNING
      bit set:  routine update_writeback_rate() is executing.
      bit clear: routine update_writeback_rate() quits.
    
    This patch also adds a function cancel_writeback_rate_update_dwork() to
    wait for dc->writeback_rate_update quits before cancel it by calling
    cancel_delayed_work_sync(). In order to avoid a deadlock by unexpected
    quit dc->writeback_rate_update, after time_out seconds this function will
    give up and continue to call cancel_delayed_work_sync().
    
    And here I explain how this patch stops self re-armed delayed work properly
    with the above stuffs.
    
    update_writeback_rate() sets BCACHE_DEV_RATE_DW_RUNNING at its beginning
    and clears BCACHE_DEV_RATE_DW_RUNNING at its end. Before calling
    cancel_writeback_rate_update_dwork() clear flag BCACHE_DEV_WB_RUNNING.
    
    Before calling cancel_delayed_work_sync() wait utill flag
    BCACHE_DEV_RATE_DW_RUNNING is clear. So when calling
    cancel_delayed_work_sync(), dc->writeback_rate_update must be already re-
    armed, or quite by seeing BCACHE_DEV_WB_RUNNING cleared. In both cases
    delayed work routine update_writeback_rate() won't be executed after
    cancel_delayed_work_sync() returns.
    
    Inside update_writeback_rate() before calling schedule_delayed_work(), flag
    BCACHE_DEV_WB_RUNNING is checked before. If this flag is cleared, it means
    someone is about to stop the delayed work. Because flag
    BCACHE_DEV_RATE_DW_RUNNING is set already and cancel_delayed_work_sync()
    has to wait for this flag to be cleared, we don't need to worry about race
    condition here.
    
    If update_writeback_rate() is scheduled to run after checking
    BCACHE_DEV_RATE_DW_RUNNING and before calling cancel_delayed_work_sync()
    in cancel_writeback_rate_update_dwork(), it is also safe. Because at this
    moment BCACHE_DEV_WB_RUNNING is cleared with memory barrier. As I mentioned
    previously, update_writeback_rate() will see BCACHE_DEV_WB_RUNNING is clear
    and quit immediately.
    
    Because there are more dependences inside update_writeback_rate() to struct
    cache_set memory, dc->writeback_rate_update is not a simple self re-arm
    delayed work. After trying many different methods (e.g. hold dc->count, or
    use locks), this is the only way I can find which works to properly stop
    dc->writeback_rate_update delayed work.
    
    Changelog:
    v3: change values of BCACHE_DEV_WB_RUNNING and BCACHE_DEV_RATE_DW_RUNNING
        to bit index, for test_bit().
    v2: Try to fix the race issue which is pointed out by Junhui.
    v1: The initial version for review
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Junhui Tang <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Michael Lyle <mlyle@lyle.org>
    Cc: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 12e5197f186c..b5ddb848cd31 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -258,10 +258,11 @@ struct bcache_device {
 	struct gendisk		*disk;
 
 	unsigned long		flags;
-#define BCACHE_DEV_CLOSING	0
-#define BCACHE_DEV_DETACHING	1
-#define BCACHE_DEV_UNLINK_DONE	2
-
+#define BCACHE_DEV_CLOSING		0
+#define BCACHE_DEV_DETACHING		1
+#define BCACHE_DEV_UNLINK_DONE		2
+#define BCACHE_DEV_WB_RUNNING		3
+#define BCACHE_DEV_RATE_DW_RUNNING	4
 	unsigned		nr_stripes;
 	unsigned		stripe_size;
 	atomic_t		*stripe_sectors_dirty;

commit 73ac105be390c1de42a2f21643c9778a5e002930
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Feb 7 11:41:46 2018 -0800

    bcache: fix for data collapse after re-attaching an attached device
    
    back-end device sdm has already attached a cache_set with ID
    f67ebe1f-f8bc-4d73-bfe5-9dc88607f119, then try to attach with
    another cache set, and it returns with an error:
    [root]# cd /sys/block/sdm/bcache
    [root]# echo 5ccd0a63-148e-48b8-afa2-aca9cbd6279f > attach
    -bash: echo: write error: Invalid argument
    
    After that, execute a command to modify the label of bcache
    device:
    [root]# echo data_disk1 > label
    
    Then we reboot the system, when the system power on, the back-end
    device can not attach to cache_set, a messages show in the log:
    Feb  5 12:05:52 ceph152 kernel: [922385.508498] bcache:
    bch_cached_dev_attach() couldn't find uuid for sdm in set
    
    In sysfs_attach(), dc->sb.set_uuid was assigned to the value
    which input through sysfs, no matter whether it is success
    or not in bch_cached_dev_attach(). For example, If the back-end
    device has already attached to an cache set, bch_cached_dev_attach()
    would fail, but dc->sb.set_uuid was changed. Then modify the
    label of bcache device, it will call bch_write_bdev_super(),
    which would write the dc->sb.set_uuid to the super block, so we
    record a wrong cache set ID in the super block, after the system
    reboot, the cache set couldn't find the uuid of the back-end
    device, so the bcache device couldn't exist and use any more.
    
    In this patch, we don't assigned cache set ID to dc->sb.set_uuid
    in sysfs_attach() directly, but input it into bch_cached_dev_attach(),
    and assigned dc->sb.set_uuid to the cache set ID after the back-end
    device attached to the cache set successful.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b8c2e1bef1f1..12e5197f186c 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -924,7 +924,7 @@ void bcache_write_super(struct cache_set *);
 
 int bch_flash_dev_create(struct cache_set *c, uint64_t size);
 
-int bch_cached_dev_attach(struct cached_dev *, struct cache_set *);
+int bch_cached_dev_attach(struct cached_dev *, struct cache_set *, uint8_t *);
 void bch_cached_dev_detach(struct cached_dev *);
 void bch_cached_dev_run(struct cached_dev *);
 void bcache_device_stop(struct bcache_device *);

commit 7ba0d830dc0e4c7e88602c91656029b6ae8a1766
Author: Coly Li <colyli@suse.de>
Date:   Wed Feb 7 11:41:42 2018 -0800

    bcache: set error_limit correctly
    
    Struct cache uses io_errors for two purposes,
    - Error decay: when cache set error_decay is set, io_errors is used to
      generate a small piece of delay when I/O error happens.
    - I/O errors counter: in order to generate big enough value for error
      decay, I/O errors counter value is stored by left shifting 20 bits (a.k.a
      IO_ERROR_SHIFT).
    
    In function bch_count_io_errors(), if I/O errors counter reaches cache set
    error limit, bch_cache_set_error() will be called to retire the whold cache
    set. But current code is problematic when checking the error limit, see the
    following code piece from bch_count_io_errors(),
    
     90     if (error) {
     91             char buf[BDEVNAME_SIZE];
     92             unsigned errors = atomic_add_return(1 << IO_ERROR_SHIFT,
     93                                                 &ca->io_errors);
     94             errors >>= IO_ERROR_SHIFT;
     95
     96             if (errors < ca->set->error_limit)
     97                     pr_err("%s: IO error on %s, recovering",
     98                            bdevname(ca->bdev, buf), m);
     99             else
    100                     bch_cache_set_error(ca->set,
    101                                         "%s: too many IO errors %s",
    102                                         bdevname(ca->bdev, buf), m);
    103     }
    
    At line 94, errors is right shifting IO_ERROR_SHIFT bits, now it is real
    errors counter to compare at line 96. But ca->set->error_limit is initia-
    lized with an amplified value in bch_cache_set_alloc(),
    1545         c->error_limit  = 8 << IO_ERROR_SHIFT;
    
    It means by default, in bch_count_io_errors(), before 8<<20 errors happened
    bch_cache_set_error() won't be called to retire the problematic cache
    device. If the average request size is 64KB, it means bcache won't handle
    failed device until 512GB data is requested. This is too large to be an I/O
    threashold. So I believe the correct error limit should be much less.
    
    This patch sets default cache set error limit to 8, then in
    bch_count_io_errors() when errors counter reaches 8 (if it is default
    value), function bch_cache_set_error() will be called to retire the whole
    cache set. This patch also removes bits shifting when store or show
    io_error_limit value via sysfs interface.
    
    Nowadays most of SSDs handle internal flash failure automatically by LBA
    address re-indirect mapping. If an I/O error can be observed by upper layer
    code, it will be a notable error because that SSD can not re-indirect
    map the problematic LBA address to an available flash block. This situation
    indicates the whole SSD will be failed very soon. Therefore setting 8 as
    the default io error limit value makes sense, it is enough for most of
    cache devices.
    
    Changelog:
    v2: add reviewed-by from Hannes.
    v1: initial version for review.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Cc: Junhui Tang <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index a857eb3c10de..b8c2e1bef1f1 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -666,6 +666,7 @@ struct cache_set {
 		ON_ERROR_UNREGISTER,
 		ON_ERROR_PANIC,
 	}			on_error;
+#define DEFAULT_IO_ERROR_LIMIT 8
 	unsigned		error_limit;
 	unsigned		error_decay;
 

commit c4dc2497d50d9c6fb16aa0d07b6a14f3b2adb1e0
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Feb 7 11:41:40 2018 -0800

    bcache: fix high CPU occupancy during journal
    
    After long time small writing I/O running, we found the occupancy of CPU
    is very high and I/O performance has been reduced by about half:
    
    [root@ceph151 internal]# top
    top - 15:51:05 up 1 day,2:43,  4 users,  load average: 16.89, 15.15, 16.53
    Tasks: 2063 total,   4 running, 2059 sleeping,   0 stopped,   0 zombie
    %Cpu(s):4.3 us, 17.1 sy 0.0 ni, 66.1 id, 12.0 wa,  0.0 hi,  0.5 si,  0.0 st
    KiB Mem : 65450044 total, 24586420 free, 38909008 used,  1954616 buff/cache
    KiB Swap: 65667068 total, 65667068 free,        0 used. 25136812 avail Mem
    
      PID USER PR NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
     2023 root 20  0       0      0      0 S 55.1  0.0   0:04.42 kworker/11:191
    14126 root 20  0       0      0      0 S 42.9  0.0   0:08.72 kworker/10:3
     9292 root 20  0       0      0      0 S 30.4  0.0   1:10.99 kworker/6:1
     8553 ceph 20  0 4242492 1.805g  18804 S 30.0  2.9 410:07.04 ceph-osd
    12287 root 20  0       0      0      0 S 26.7  0.0   0:28.13 kworker/7:85
    31019 root 20  0       0      0      0 S 26.1  0.0   1:30.79 kworker/22:1
     1787 root 20  0       0      0      0 R 25.7  0.0   5:18.45 kworker/8:7
    32169 root 20  0       0      0      0 S 14.5  0.0   1:01.92 kworker/23:1
    21476 root 20  0       0      0      0 S 13.9  0.0   0:05.09 kworker/1:54
     2204 root 20  0       0      0      0 S 12.5  0.0   1:25.17 kworker/9:10
    16994 root 20  0       0      0      0 S 12.2  0.0   0:06.27 kworker/5:106
    15714 root 20  0       0      0      0 R 10.9  0.0   0:01.85 kworker/19:2
     9661 ceph 20  0 4246876 1.731g  18800 S 10.6  2.8 403:00.80 ceph-osd
    11460 ceph 20  0 4164692 2.206g  18876 S 10.6  3.5 360:27.19 ceph-osd
     9960 root 20  0       0      0      0 S 10.2  0.0   0:02.75 kworker/2:139
    11699 ceph 20  0 4169244 1.920g  18920 S 10.2  3.1 355:23.67 ceph-osd
     6843 ceph 20  0 4197632 1.810g  18900 S  9.6  2.9 380:08.30 ceph-osd
    
    The kernel work consumed a lot of CPU, and I found they are running journal
    work, The journal is reclaiming source and flush btree node with surprising
    frequency.
    
    Through further analysis, we found that in btree_flush_write(), we try to
    get a btree node with the smallest fifo idex to flush by traverse all the
    btree nodein c->bucket_hash, after we getting it, since no locker protects
    it, this btree node may have been written to cache device by other works,
    and if this occurred, we retry to traverse in c->bucket_hash and get
    another btree node. When the problem occurrd, the retry times is very high,
    and we consume a lot of CPU in looking for a appropriate btree node.
    
    In this patch, we try to record 128 btree nodes with the smallest fifo idex
    in heap, and pop one by one when we need to flush btree node. It greatly
    reduces the time for the loop to find the appropriate BTREE node, and also
    reduce the occupancy of CPU.
    
    [note by mpl: this triggers a checkpatch error because of adjacent,
    pre-existing style violations]
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b98d7705acb6..a857eb3c10de 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -679,6 +679,8 @@ struct cache_set {
 
 #define BUCKET_HASH_BITS	12
 	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];
+
+	DECLARE_HEAP(struct btree *, flush_btree);
 };
 
 struct bbio {

commit a728eacbbdd229d1d903e46261c57d5206f87a4a
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Feb 7 11:41:39 2018 -0800

    bcache: add journal statistic
    
    Sometimes, Journal takes up a lot of CPU, we need statistics
    to know what's the journal is doing. So this patch provide
    some journal statistics:
    1) reclaim: how many times the journal try to reclaim resource,
       usually the journal bucket or/and the pin are exhausted.
    2) flush_write: how many times the journal try to flush btree node
       to cache device, usually the journal bucket are exhausted.
    3) retry_flush_write: how many times the journal retry to flush
       the next btree node, usually the previous tree node have been
       flushed by other thread.
    we show these statistic by sysfs interface. Through these statistics
    We can totally see the status of journal module when the CPU is too
    high.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 5e2d4e80198e..b98d7705acb6 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -658,6 +658,10 @@ struct cache_set {
 	atomic_long_t		writeback_keys_done;
 	atomic_long_t		writeback_keys_failed;
 
+	atomic_long_t		reclaim;
+	atomic_long_t		flush_write;
+	atomic_long_t		retry_flush_write;
+
 	enum			{
 		ON_ERROR_UNREGISTER,
 		ON_ERROR_PANIC,

commit 5138ac6748e381501894976f995fb7d1a63f80f4
Author: Coly Li <colyli@suse.de>
Date:   Mon Jan 8 12:21:29 2018 -0800

    bcache: fix misleading error message in bch_count_io_errors()
    
    Bcache only does recoverable I/O for read operations by calling
    cached_dev_read_error(). For write opertions there is no I/O recovery for
    failed requests.
    
    But in bch_count_io_errors() no matter read or write I/Os, before errors
    counter reaches io error limit, pr_err() always prints "IO error on %,
    recoverying". For write requests this information is misleading, because
    there is no I/O recovery at all.
    
    This patch adds a parameter 'is_read' to bch_count_io_errors(), and only
    prints "recovering" by pr_err() when the bio direction is READ.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 9117da5f494b..5e2d4e80198e 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -862,7 +862,7 @@ static inline void wake_up_allocators(struct cache_set *c)
 
 /* Forward declarations */
 
-void bch_count_io_errors(struct cache *, blk_status_t, const char *);
+void bch_count_io_errors(struct cache *, blk_status_t, int, const char *);
 void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
 			      blk_status_t, const char *);
 void bch_bbio_endio(struct cache_set *, struct bio *, blk_status_t,

commit 2831231d4c3f999d2d062b23dfbc8b0faa4bc6e0
Author: Coly Li <colyli@suse.de>
Date:   Mon Jan 8 12:21:28 2018 -0800

    bcache: reduce cache_set devices iteration by devices_max_used
    
    Member devices of struct cache_set is used to reference all attached
    bcache devices to this cache set. If it is treated as array of pointers,
    size of devices[] is indicated by member nr_uuids of struct cache_set.
    
    nr_uuids is calculated in drivers/md/super.c:bch_cache_set_alloc(),
            bucket_bytes(c) / sizeof(struct uuid_entry)
    Bucket size is determined by user space tool "make-bcache", by default it
    is 1024 sectors (defined in bcache-tools/make-bcache.c:main()). So default
    nr_uuids value is 4096 from the above calculation.
    
    Every time when bcache code iterates bcache devices of a cache set, all
    the 4096 pointers are checked even only 1 bcache device is attached to the
    cache set, that's a wast of time and unncessary.
    
    This patch adds a member devices_max_used to struct cache_set. Its value
    is 1 + the maximum used index of devices[] in a cache set. When iterating
    all valid bcache devices of a cache set, use c->devices_max_used in
    for-loop may reduce a lot of useless checking.
    
    Personally, my motivation of this patch is not for performance, I use it
    in bcache debugging, which helps me to narrow down the scape to check
    valid bcached devices of a cache set.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 5f7b0b2513cc..9117da5f494b 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -497,6 +497,7 @@ struct cache_set {
 	int			caches_loaded;
 
 	struct bcache_device	**devices;
+	unsigned		devices_max_used;
 	struct list_head	cached_devs;
 	uint64_t		cached_dev_sectors;
 	struct closure		caching;

commit b1092c9af9ed88dd2fc8345d987dfb7efe7be8f0
Author: Michael Lyle <mlyle@lyle.org>
Date:   Mon Jan 8 12:21:24 2018 -0800

    bcache: allow quick writeback when backing idle
    
    If the control system would wait for at least half a second, and there's
    been no reqs hitting the backing disk for awhile: use an alternate mode
    where we have at most one contiguous set of writebacks in flight at a
    time. (But don't otherwise delay).  If front-end IO appears, it will
    still be quick, as it will only have to contend with one real operation
    in flight.  But otherwise, we'll be sending data to the backing disk as
    quickly as it can accept it (with one op at a time).
    
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Acked-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 3be0fcc19b1f..5f7b0b2513cc 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -320,6 +320,13 @@ struct cached_dev {
 	 */
 	atomic_t		has_dirty;
 
+	/*
+	 * Set to zero by things that touch the backing volume-- except
+	 * writeback.  Incremented by writeback.  Used to determine when to
+	 * accelerate idle writeback.
+	 */
+	atomic_t		backing_idle;
+
 	struct bch_ratelimit	writeback_rate;
 	struct delayed_work	writeback_rate_update;
 

commit 6e6ccc67b9c7a682d717feedb887cb630a984317
Author: Michael Lyle <mlyle@lyle.org>
Date:   Mon Jan 8 12:21:23 2018 -0800

    bcache: writeback: properly order backing device IO
    
    Writeback keys are presently iterated and dispatched for writeback in
    order of the logical block address on the backing device.  Multiple may
    be, in parallel, read from the cache device and then written back
    (especially when there are contiguous I/O).
    
    However-- there was no guarantee with the existing code that the writes
    would be issued in LBA order, as the reads from the cache device are
    often re-ordered.  In turn, when writing back quickly, the backing disk
    often has to seek backwards-- this slows writeback and increases
    utilization.
    
    This patch introduces an ordering mechanism that guarantees that the
    original order of issue is maintained for the write portion of the I/O.
    Performance for writeback is significantly improved when there are
    multiple contiguous keys or high writeback rates.
    
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Tang Junhui <tang.junhui@zte.com.cn>
    Tested-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 1784e50eb857..3be0fcc19b1f 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -330,6 +330,14 @@ struct cached_dev {
 
 	struct keybuf		writeback_keys;
 
+	/*
+	 * Order the write-half of writeback operations strongly in dispatch
+	 * order.  (Maintain LBA order; don't allow reads completing out of
+	 * order to re-order the writes...)
+	 */
+	struct closure_waitlist writeback_ordering_wait;
+	atomic_t		writeback_sequence_next;
+
 	/* For tracking sequential IO */
 #define RECENT_IO_BITS	7
 #define RECENT_IO	(1 << RECENT_IO_BITS)

commit 539d39eb27083405b82b9e604e88af01a9a46c63
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Mon Jan 8 12:21:22 2018 -0800

    bcache: fix wrong return value in bch_debug_init()
    
    in bch_debug_init(), ret is always 0, and the return value is useless,
    change it to return 0 if be success after calling debugfs_create_dir(),
    else return a non-zero value.
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 843877e017e1..1784e50eb857 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -323,12 +323,6 @@ struct cached_dev {
 	struct bch_ratelimit	writeback_rate;
 	struct delayed_work	writeback_rate_update;
 
-	/*
-	 * Internal to the writeback code, so read_dirty() can keep track of
-	 * where it's at.
-	 */
-	sector_t		last_read;
-
 	/* Limit number of writeback bios in flight */
 	struct semaphore	in_flight;
 	struct task_struct	*writeback_thread;

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 2ed9bd231d84..abd31e847f96 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _BCACHE_H
 #define _BCACHE_H
 

commit d44c2f9e7cc0041f0cd88df1fe7a1fceb713ab14
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Mon Oct 30 14:46:33 2017 -0700

    bcache: update bucket_in_use in real time
    
    bucket_in_use is updated in gc thread which triggered by invalidating or
    writing sectors_to_gc dirty data, It's a long interval. Therefore, when we
    use it to compare with the threshold, it is often not timely, which leads
    to inaccurate judgment and often results in bucket depletion.
    
    We have send a patch before, by the means of updating bucket_in_use
    periodically In gc thread, which Coly thought that would lead high
    latency, In this patch, we add avail_nbuckets to record the count of
    available buckets, and we calculate bucket_in_use when alloc or free
    bucket in real time.
    
    [edited by ML: eliminated some whitespace errors]
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 363ea6256b39..e274082330dc 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -581,6 +581,7 @@ struct cache_set {
 	uint8_t			need_gc;
 	struct gc_stat		gc_stats;
 	size_t			nbuckets;
+	size_t			avail_nbuckets;
 
 	struct task_struct	*gc_thread;
 	/* Where in the btree gc currently is */

commit 3b304d24a718ae779ee9c7f2014dd3b2d0893b70
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Mon Oct 30 14:46:32 2017 -0700

    bcache: convert cached_dev.count from atomic_t to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable cached_dev.count is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Michael Lyle <mlyle@lyle.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d77c4829c497..363ea6256b39 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -184,6 +184,7 @@
 #include <linux/mutex.h>
 #include <linux/rbtree.h>
 #include <linux/rwsem.h>
+#include <linux/refcount.h>
 #include <linux/types.h>
 #include <linux/workqueue.h>
 
@@ -296,7 +297,7 @@ struct cached_dev {
 	struct semaphore	sb_write_mutex;
 
 	/* Refcount on the cache set. Always nonzero when we're caching. */
-	atomic_t		count;
+	refcount_t		count;
 	struct work_struct	detach;
 
 	/*
@@ -805,13 +806,13 @@ do {									\
 
 static inline void cached_dev_put(struct cached_dev *dc)
 {
-	if (atomic_dec_and_test(&dc->count))
+	if (refcount_dec_and_test(&dc->count))
 		schedule_work(&dc->detach);
 }
 
 static inline bool cached_dev_get(struct cached_dev *dc)
 {
-	if (!atomic_inc_not_zero(&dc->count))
+	if (!refcount_inc_not_zero(&dc->count))
 		return false;
 
 	/* Paired with the mb in cached_dev_attach */

commit e41166c5c44e30dbd620f7c77a27efe5d5cc551a
Author: Michael Lyle <mlyle@lyle.org>
Date:   Fri Oct 13 16:35:38 2017 -0700

    bcache: writeback rate shouldn't artifically clamp
    
    The previous code artificially limited writeback rate to 1000000
    blocks/second (NSEC_PER_MSEC), which is a rate that can be met on fast
    hardware.  The rate limiting code works fine (though with decreased
    precision) up to 3 orders of magnitude faster, so use NSEC_PER_SEC.
    
    Additionally, ensure that uint32_t is used as a type for rate throughout
    the rate management so that type checking/clamp_t can work properly.
    
    bch_next_delay should be rewritten for increased precision and better
    handling of high rates and long sleep periods, but this is adequate for
    now.
    
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reported-by: Coly Li <colyli@suse.de>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index eb83be693d60..d77c4829c497 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -361,7 +361,7 @@ struct cached_dev {
 	int64_t			writeback_rate_proportional;
 	int64_t			writeback_rate_integral;
 	int64_t			writeback_rate_integral_scaled;
-	int64_t			writeback_rate_change;
+	int32_t			writeback_rate_change;
 
 	unsigned		writeback_rate_update_seconds;
 	unsigned		writeback_rate_i_term_inverse;

commit 1d316e658374f700fdfff9299c70ce65d8d145e6
Author: Michael Lyle <mlyle@lyle.org>
Date:   Fri Oct 13 16:35:36 2017 -0700

    bcache: implement PI controller for writeback rate
    
    bcache uses a control system to attempt to keep the amount of dirty data
    in cache at a user-configured level, while not responding excessively to
    transients and variations in write rate.  Previously, the system was a
    PD controller; but the output from it was integrated, turning the
    Proportional term into an Integral term, and turning the Derivative term
    into a crude Proportional term.  Performance of the controller has been
    uneven in production, and it has tended to respond slowly, oscillate,
    and overshoot.
    
    This patch set replaces the current control system with an explicit PI
    controller and tuning that should be correct for most hardware.  By
    default, it attempts to write at a rate that would retire 1/40th of the
    current excess blocks per second.  An integral term in turn works to
    remove steady state errors.
    
    IMO, this yields benefits in simplicity (removing weighted average
    filtering, etc) and system performance.
    
    Another small change is a tunable parameter is introduced to allow the
    user to specify a minimum rate at which dirty blocks are retired.
    
    There is a slight difference from earlier versions of the patch in
    integral handling to prevent excessive negative integral windup.
    
    Signed-off-by: Michael Lyle <mlyle@lyle.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 2ed9bd231d84..eb83be693d60 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -265,9 +265,6 @@ struct bcache_device {
 	atomic_t		*stripe_sectors_dirty;
 	unsigned long		*full_dirty_stripes;
 
-	unsigned long		sectors_dirty_last;
-	long			sectors_dirty_derivative;
-
 	struct bio_set		*bio_split;
 
 	unsigned		data_csum:1;
@@ -362,12 +359,14 @@ struct cached_dev {
 
 	uint64_t		writeback_rate_target;
 	int64_t			writeback_rate_proportional;
-	int64_t			writeback_rate_derivative;
+	int64_t			writeback_rate_integral;
+	int64_t			writeback_rate_integral_scaled;
 	int64_t			writeback_rate_change;
 
 	unsigned		writeback_rate_update_seconds;
-	unsigned		writeback_rate_d_term;
+	unsigned		writeback_rate_i_term_inverse;
 	unsigned		writeback_rate_p_term_inverse;
+	unsigned		writeback_rate_minimum;
 };
 
 enum alloc_reserve {

commit 9baf30972b5568d8b5bc8b3c46a6ec5b58100463
Author: Tang Junhui <tang.junhui@zte.com.cn>
Date:   Wed Sep 6 14:25:59 2017 +0800

    bcache: fix for gc and write-back race
    
    gc and write-back get raced (see the email "bcache get stucked" I sended
    before):
    gc thread                               write-back thread
    |                                       |bch_writeback_thread()
    |bch_gc_thread()                        |
    |                                       |==>read_dirty()
    |==>bch_btree_gc()                      |
    |==>btree_root() //get btree root       |
    |                //node write locker    |
    |==>bch_btree_gc_root()                 |
    |                                       |==>read_dirty_submit()
    |                                       |==>write_dirty()
    |                                       |==>continue_at(cl,
    |                                       |               write_dirty_finish,
    |                                       |               system_wq);
    |                                       |==>write_dirty_finish()//excute
    |                                       |               //in system_wq
    |                                       |==>bch_btree_insert()
    |                                       |==>bch_btree_map_leaf_nodes()
    |                                       |==>__bch_btree_map_nodes()
    |                                       |==>btree_root //try to get btree
    |                                       |              //root node read
    |                                       |              //lock
    |                                       |-----stuck here
    |==>bch_btree_set_root()
    |==>bch_journal_meta()
    |==>bch_journal()
    |==>journal_try_write()
    |==>journal_write_unlocked() //journal_full(&c->journal)
    |                            //condition satisfied
    |==>continue_at(cl, journal_write, system_wq); //try to excute
    |                               //journal_write in system_wq
    |                               //but work queue is excuting
    |                               //write_dirty_finish()
    |==>closure_sync(); //wait journal_write execute
    |                   //over and wake up gc,
    |-------------stuck here
    |==>release root node write locker
    
    This patch alloc a separate work-queue for write-back thread to avoid such
    race.
    
    (Commit log re-organized by Coly Li to pass checkpatch.pl checking)
    
    Signed-off-by: Tang Junhui <tang.junhui@zte.com.cn>
    Acked-by: Coly Li <colyli@suse.de>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index dee542fff68e..2ed9bd231d84 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -333,6 +333,7 @@ struct cached_dev {
 	/* Limit number of writeback bios in flight */
 	struct semaphore	in_flight;
 	struct task_struct	*writeback_thread;
+	struct workqueue_struct	*writeback_write_wq;
 
 	struct keybuf		writeback_keys;
 

commit 4e4cbee93d56137ebff722be022cae5f70ef84fb
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Jun 3 09:38:06 2017 +0200

    block: switch bios to blk_status_t
    
    Replace bi_error with a new bi_status to allow for a clear conversion.
    Note that device mapper overloaded bi_error with a private value, which
    we'll have to keep arround at least for now and thus propagate to a
    proper blk_status_t value.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index c3ea03c9a1a8..dee542fff68e 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -849,10 +849,11 @@ static inline void wake_up_allocators(struct cache_set *c)
 
 /* Forward declarations */
 
-void bch_count_io_errors(struct cache *, int, const char *);
+void bch_count_io_errors(struct cache *, blk_status_t, const char *);
 void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
-			      int, const char *);
-void bch_bbio_endio(struct cache_set *, struct bio *, int, const char *);
+			      blk_status_t, const char *);
+void bch_bbio_endio(struct cache_set *, struct bio *, blk_status_t,
+		const char *);
 void bch_bbio_free(struct bio *, struct cache_set *);
 struct bio *bch_bbio_alloc(struct cache_set *);
 

commit be628be09563f8f6e81929efbd7cf3f45c344416
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Wed Oct 26 20:31:17 2016 -0700

    bcache: Make gc wakeup sane, remove set_task_state()
    
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6b420a55c745..c3ea03c9a1a8 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -425,7 +425,7 @@ struct cache {
 	 * until a gc finishes - otherwise we could pointlessly burn a ton of
 	 * cpu
 	 */
-	unsigned		invalidate_needs_gc:1;
+	unsigned		invalidate_needs_gc;
 
 	bool			discard; /* Get rid of? */
 
@@ -593,8 +593,8 @@ struct cache_set {
 
 	/* Counts how many sectors bio_insert has added to the cache */
 	atomic_t		sectors_to_gc;
+	wait_queue_head_t	gc_wait;
 
-	wait_queue_head_t	moving_gc_wait;
 	struct keybuf		moving_gc_keys;
 	/* Number of moving GC bios in flight */
 	struct semaphore	moving_in_flight;

commit 749b61dab30736eb95b1ee23738cae90973d4fc3
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sat Nov 23 23:11:25 2013 -0800

    bcache: remove driver private bio splitting code
    
    The bcache driver has always accepted arbitrarily large bios and split
    them internally.  Now that every driver must accept arbitrarily large
    bios this code isn't nessecary anymore.
    
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    [dpark: add more description in commit message]
    Signed-off-by: Dongsu Park <dpark@posteo.net>
    Signed-off-by: Ming Lin <ming.l@ssi.samsung.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 04f7bc28ef83..6b420a55c745 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -243,19 +243,6 @@ struct keybuf {
 	DECLARE_ARRAY_ALLOCATOR(struct keybuf_key, freelist, KEYBUF_NR);
 };
 
-struct bio_split_pool {
-	struct bio_set		*bio_split;
-	mempool_t		*bio_split_hook;
-};
-
-struct bio_split_hook {
-	struct closure		cl;
-	struct bio_split_pool	*p;
-	struct bio		*bio;
-	bio_end_io_t		*bi_end_io;
-	void			*bi_private;
-};
-
 struct bcache_device {
 	struct closure		cl;
 
@@ -288,8 +275,6 @@ struct bcache_device {
 	int (*cache_miss)(struct btree *, struct search *,
 			  struct bio *, unsigned);
 	int (*ioctl) (struct bcache_device *, fmode_t, unsigned, unsigned long);
-
-	struct bio_split_pool	bio_split_hook;
 };
 
 struct io {
@@ -454,8 +439,6 @@ struct cache {
 	atomic_long_t		meta_sectors_written;
 	atomic_long_t		btree_sectors_written;
 	atomic_long_t		sectors_written;
-
-	struct bio_split_pool	bio_split_hook;
 };
 
 struct gc_stat {
@@ -873,7 +856,6 @@ void bch_bbio_endio(struct cache_set *, struct bio *, int, const char *);
 void bch_bbio_free(struct bio *, struct cache_set *);
 struct bio *bch_bbio_alloc(struct cache_set *);
 
-void bch_generic_make_request(struct bio *, struct bio_split_pool *);
 void __bch_submit_bbio(struct bio *, struct cache_set *);
 void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);
 

commit bf0c55c986540483c34ca640f2eef4c3314388b1
Author: Slava Pestov <sp@daterainc.com>
Date:   Fri Jul 11 12:17:41 2014 -0700

    bcache: fix crash with incomplete cache set
    
    Change-Id: I6abde52afe917633480caaf4e2518f42a816d886

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d2ebcf323094..04f7bc28ef83 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -477,9 +477,13 @@ struct gc_stat {
  * CACHE_SET_STOPPING always gets set first when we're closing down a cache set;
  * we'll continue to run normally for awhile with CACHE_SET_STOPPING set (i.e.
  * flushing dirty data).
+ *
+ * CACHE_SET_RUNNING means all cache devices have been registered and journal
+ * replay is complete.
  */
 #define CACHE_SET_UNREGISTERING		0
 #define	CACHE_SET_STOPPING		1
+#define	CACHE_SET_RUNNING		2
 
 struct cache_set {
 	struct closure		cl;

commit 4e857c58efeb99393cba5a5d0d8ec7117183137c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 17 18:06:10 2014 +0100

    arch: Mass conversion of smp_mb__*()
    
    Mostly scripted conversion of the smp_mb__* barriers.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-55dhyhocezdw1dg7u19hmh1u@git.kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 82c9c5d35251..d2ebcf323094 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -828,7 +828,7 @@ static inline bool cached_dev_get(struct cached_dev *dc)
 		return false;
 
 	/* Paired with the mb in cached_dev_attach */
-	smp_mb__after_atomic_inc();
+	smp_mb__after_atomic();
 	return true;
 }
 

commit 3a2fd9d5090b83aab85378a846fa10f39b0b5aa7
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Feb 27 17:51:12 2014 -0800

    bcache: Kill bucket->gc_gen
    
    gc_gen was a temporary used to recalculate last_gc, but since we only need
    bucket->last_gc when gc isn't running (gc_mark_valid = 1), we can just update
    last_gc directly.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 200efc1b5cf8..82c9c5d35251 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -196,7 +196,6 @@ struct bucket {
 	uint16_t	prio;
 	uint8_t		gen;
 	uint8_t		last_gc; /* Most out of date gen in the btree */
-	uint8_t		gc_gen;
 	uint16_t	gc_mark; /* Bitfield used by GC. See below for field */
 };
 
@@ -588,7 +587,7 @@ struct cache_set {
 	uint16_t		min_prio;
 
 	/*
-	 * max(gen - gc_gen) for all buckets. When it gets too big we have to gc
+	 * max(gen - last_gc) for all buckets. When it gets too big we have to gc
 	 * to keep gens from wrapping around.
 	 */
 	uint8_t			need_gc;

commit 2531d9ee61fa08a5a9ab8f002c50779888d232c7
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Mar 17 16:55:55 2014 -0700

    bcache: Kill unused freelist
    
    This was originally added as at optimization that for various reasons isn't
    needed anymore, but it does add a lot of nasty corner cases (and it was
    responsible for some recently fixed bugs). Just get rid of it now.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 171cda89cb6b..200efc1b5cf8 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -195,7 +195,6 @@ struct bucket {
 	atomic_t	pin;
 	uint16_t	prio;
 	uint8_t		gen;
-	uint8_t		disk_gen;
 	uint8_t		last_gc; /* Most out of date gen in the btree */
 	uint8_t		gc_gen;
 	uint16_t	gc_mark; /* Bitfield used by GC. See below for field */
@@ -426,14 +425,9 @@ struct cache {
 	 * their new gen to disk. After prio_write() finishes writing the new
 	 * gens/prios, they'll be moved to the free list (and possibly discarded
 	 * in the process)
-	 *
-	 * unused: GC found nothing pointing into these buckets (possibly
-	 * because all the data they contained was overwritten), so we only
-	 * need to discard them before they can be moved to the free list.
 	 */
 	DECLARE_FIFO(long, free)[RESERVE_NR];
 	DECLARE_FIFO(long, free_inc);
-	DECLARE_FIFO(long, unused);
 
 	size_t			fifo_last_bucket;
 
@@ -442,12 +436,6 @@ struct cache {
 
 	DECLARE_HEAP(struct bucket *, heap);
 
-	/*
-	 * max(gen - disk_gen) for all buckets. When it gets too big we have to
-	 * call prio_write() to keep gens from wrapping.
-	 */
-	uint8_t			need_save_prio;
-
 	/*
 	 * If nonzero, we know we aren't going to find any buckets to invalidate
 	 * until a gc finishes - otherwise we could pointlessly burn a ton of
@@ -848,9 +836,6 @@ static inline bool cached_dev_get(struct cached_dev *dc)
 /*
  * bucket_gc_gen() returns the difference between the bucket's current gen and
  * the oldest gen of any pointer into that bucket in the btree (last_gc).
- *
- * bucket_disk_gen() returns the difference between the current gen and the gen
- * on disk; they're both used to make sure gens don't wrap around.
  */
 
 static inline uint8_t bucket_gc_gen(struct bucket *b)
@@ -858,13 +843,7 @@ static inline uint8_t bucket_gc_gen(struct bucket *b)
 	return b->gen - b->last_gc;
 }
 
-static inline uint8_t bucket_disk_gen(struct bucket *b)
-{
-	return b->gen - b->disk_gen;
-}
-
 #define BUCKET_GC_GEN_MAX	96U
-#define BUCKET_DISK_GEN_MAX	64U
 
 #define kobj_attribute_write(n, fn)					\
 	static struct kobj_attribute ksysfs_##n = __ATTR(n, S_IWUSR, NULL, fn)
@@ -897,11 +876,14 @@ void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);
 
 uint8_t bch_inc_gen(struct cache *, struct bucket *);
 void bch_rescale_priorities(struct cache_set *, int);
-bool bch_bucket_add_unused(struct cache *, struct bucket *);
 
-long bch_bucket_alloc(struct cache *, unsigned, bool);
+bool bch_can_invalidate_bucket(struct cache *, struct bucket *);
+void __bch_invalidate_one_bucket(struct cache *, struct bucket *);
+
+void __bch_bucket_free(struct cache *, struct bucket *);
 void bch_bucket_free(struct cache_set *, struct bkey *);
 
+long bch_bucket_alloc(struct cache *, unsigned, bool);
 int __bch_bucket_alloc_set(struct cache_set *, unsigned,
 			   struct bkey *, int, bool);
 int bch_bucket_alloc_set(struct cache_set *, unsigned,

commit 0a63b66db566cffdf90182eb6e66fdd4d0479e63
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Mar 17 17:15:53 2014 -0700

    bcache: Rework btree cache reserve handling
    
    This changes the bucket allocation reserves to use _real_ reserves - separate
    freelists - instead of watermarks, which if nothing else makes the current code
    saner to reason about and is going to be important in the future when we add
    support for multiple btrees.
    
    It also adds btree_check_reserve(), which checks (and locks) the reserves for
    both bucket allocation and memory allocation for btree nodes; the old code just
    kinda sorta assumed that since (e.g. for btree node splits) it had the root
    locked and that meant no other threads could try to make use of the same
    reserve; this technically should have been ok for memory allocation (we should
    always have a reserve for memory allocation (the btree node cache is used as a
    reserve and we preallocate it)), but multiple btrees will mean that locking the
    root won't be sufficient anymore, and for the bucket allocation reserve it was
    technically possible for the old code to deadlock.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 15d26236caf9..171cda89cb6b 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -562,19 +562,16 @@ struct cache_set {
 	struct list_head	btree_cache_freed;
 
 	/* Number of elements in btree_cache + btree_cache_freeable lists */
-	unsigned		bucket_cache_used;
+	unsigned		btree_cache_used;
 
 	/*
 	 * If we need to allocate memory for a new btree node and that
 	 * allocation fails, we can cannibalize another node in the btree cache
-	 * to satisfy the allocation. However, only one thread can be doing this
-	 * at a time, for obvious reasons - try_harder and try_wait are
-	 * basically a lock for this that we can wait on asynchronously. The
-	 * btree_root() macro releases the lock when it returns.
+	 * to satisfy the allocation - lock to guarantee only one thread does
+	 * this at a time:
 	 */
-	struct task_struct	*try_harder;
-	wait_queue_head_t	try_wait;
-	uint64_t		try_harder_start;
+	wait_queue_head_t	btree_cache_wait;
+	struct task_struct	*btree_cache_alloc_lock;
 
 	/*
 	 * When we free a btree node, we increment the gen of the bucket the
@@ -669,7 +666,6 @@ struct cache_set {
 	struct time_stats	btree_gc_time;
 	struct time_stats	btree_split_time;
 	struct time_stats	btree_read_time;
-	struct time_stats	try_harder_time;
 
 	atomic_long_t		cache_read_races;
 	atomic_long_t		writeback_keys_done;
@@ -956,7 +952,6 @@ int bch_open_buckets_alloc(struct cache_set *);
 void bch_open_buckets_free(struct cache_set *);
 
 int bch_cache_allocator_start(struct cache *ca);
-int bch_cache_allocator_init(struct cache *ca);
 
 void bch_debug_exit(void);
 int bch_debug_init(struct kobject *);

commit 56b30770b27d54d68ad51eccc6d888282b568cee
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jan 23 01:44:55 2014 -0800

    bcache: Kill btree_io_wq
    
    With the locking rework in the last patch, this shouldn't be needed anymore -
    btree_node_write_work() only takes b->write_lock which is never held for very
    long.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 014236e411d8..15d26236caf9 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -962,7 +962,5 @@ void bch_debug_exit(void);
 int bch_debug_init(struct kobject *);
 void bch_request_exit(void);
 int bch_request_init(void);
-void bch_btree_exit(void);
-int bch_btree_init(void);
 
 #endif /* _BCACHE_H */

commit 4fe6a816707aace9e8e297b708411c5930537793
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Mar 13 13:46:29 2014 -0700

    bcache: Add a real GC_MARK_RECLAIMABLE
    
    This means the garbage collection code can better check for data and metadata
    pointers to the same buckets.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6d814f463d9e..014236e411d8 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -207,9 +207,9 @@ struct bucket {
  */
 
 BITMASK(GC_MARK,	 struct bucket, gc_mark, 0, 2);
-#define GC_MARK_RECLAIMABLE	0
-#define GC_MARK_DIRTY		1
-#define GC_MARK_METADATA	2
+#define GC_MARK_RECLAIMABLE	1
+#define GC_MARK_DIRTY		2
+#define GC_MARK_METADATA	3
 #define GC_SECTORS_USED_SIZE	13
 #define MAX_GC_SECTORS_USED	(~(~0ULL << GC_SECTORS_USED_SIZE))
 BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, GC_SECTORS_USED_SIZE);

commit da415a096fc06e49d1a15f7a06bcfe6ad44c5d38
Author: Nicholas Swenson <nks@daterainc.com>
Date:   Thu Jan 9 16:03:04 2014 -0800

    bcache: Fix moving_gc deadlocking with a foreground write
    
    Deadlock happened because a foreground write slept, waiting for a bucket
    to be allocated. Normally the gc would mark buckets available for invalidation.
    But the moving_gc was stuck waiting for outstanding writes to complete.
    These writes used the bcache_wq, the same queue foreground writes used.
    
    This fix gives moving_gc its own work queue, so it was still finish moving
    even if foreground writes are stuck waiting for allocation. It also makes
    work queue a parameter to the data_insert path, so moving_gc can use its
    workqueue for writes.
    
    Signed-off-by: Nicholas Swenson <nks@daterainc.com>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index a4c7306ff43d..6d814f463d9e 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -628,6 +628,8 @@ struct cache_set {
 	/* Number of moving GC bios in flight */
 	struct semaphore	moving_in_flight;
 
+	struct workqueue_struct	*moving_gc_wq;
+
 	struct btree		*root;
 
 #ifdef CONFIG_BCACHE_DEBUG

commit 947174476701fbc84ea8c7ec9664270f9d80b076
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Tue Jan 28 16:57:39 2014 -0800

    bcache: fix BUG_ON due to integer overflow with GC_SECTORS_USED
    
    The BUG_ON at the end of __bch_btree_mark_key can be triggered due to
    an integer overflow error:
    
    BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 13);
    ...
    SET_GC_SECTORS_USED(g, min_t(unsigned,
                 GC_SECTORS_USED(g) + KEY_SIZE(k),
                 (1 << 14) - 1));
    BUG_ON(!GC_SECTORS_USED(g));
    
    In bcache.h, the SECTORS_USED bitfield is defined to be 13 bits wide.
    While the SET_ code tries to ensure that the field doesn't overflow by
    clamping it to (1<<14)-1 == 16383, this is incorrect because 16383
    requires 14 bits.  Therefore, if GC_SECTORS_USED() + KEY_SIZE() =
    8192, the SET_ statement tries to store 8192 into a 13-bit field.  In
    a 13-bit field, 8192 becomes zero, thus triggering the BUG_ON.
    
    Therefore, create a field width constant and a max value constant, and
    use those to create the bitfield and check the inputs to
    SET_GC_SECTORS_USED.  Arguably the BITMASK() template ought to have
    BUG_ON checks for too-large values, but that's a separate patch.
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 0c707e4f4eaf..a4c7306ff43d 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -210,7 +210,9 @@ BITMASK(GC_MARK,	 struct bucket, gc_mark, 0, 2);
 #define GC_MARK_RECLAIMABLE	0
 #define GC_MARK_DIRTY		1
 #define GC_MARK_METADATA	2
-BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 13);
+#define GC_SECTORS_USED_SIZE	13
+#define MAX_GC_SECTORS_USED	(~(~0ULL << GC_SECTORS_USED_SIZE))
+BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, GC_SECTORS_USED_SIZE);
 BITMASK(GC_MOVE, struct bucket, gc_mark, 15, 1);
 
 #include "journal.h"

commit e0a985a4b1b533311ec88c85177c45d036313f75
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Nov 12 13:49:10 2013 -0800

    bcache: Improve bucket_prio() calculation
    
    When deciding what order to reuse buckets we take into account both the bucket's
    priority (which indicates lru order) and also the amount of live data in that
    bucket. The way they were scaled together wasn't as correct as it could be...
    this patch improves and documents it.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 93b848419665..0c707e4f4eaf 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -704,7 +704,7 @@ struct bbio {
 };
 
 #define BTREE_PRIO		USHRT_MAX
-#define INITIAL_PRIO		32768
+#define INITIAL_PRIO		32768U
 
 #define btree_bytes(c)		((c)->btree_pages * PAGE_SIZE)
 #define btree_blocks(b)							\

commit a85e968e66a175c86d0410719ea84a5bd0f1d070
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Dec 20 17:28:16 2013 -0800

    bcache: Add struct btree_keys
    
    Soon, bset.c won't need to depend on struct btree.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 5c74d55cea7f..93b848419665 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -679,9 +679,9 @@ struct cache_set {
 	unsigned		error_decay;
 
 	unsigned short		journal_delay_ms;
+	bool			expensive_debug_checks;
 	unsigned		verify:1;
 	unsigned		key_merging_disabled:1;
-	unsigned		expensive_debug_checks:1;
 	unsigned		gc_always_rewrite:1;
 	unsigned		shrinker_disabled:1;
 	unsigned		copy_gc_enabled:1;

commit ee811287c9f241641899788cbfc9d70ed96ba3a5
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 23:49:49 2013 -0800

    bcache: Rename/shuffle various code around
    
    More work to disentangle bset.c from the rest of the code:
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 7bd4c93475e7..5c74d55cea7f 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -717,14 +717,6 @@ struct bbio {
 #define bucket_bytes(c)		((c)->sb.bucket_size << 9)
 #define block_bytes(c)		((c)->sb.block_size << 9)
 
-#define __set_bytes(i, k)	(sizeof(*(i)) + (k) * sizeof(uint64_t))
-#define set_bytes(i)		__set_bytes(i, i->keys)
-
-#define __set_blocks(i, k, c)	DIV_ROUND_UP(__set_bytes(i, k), block_bytes(c))
-#define set_blocks(i, c)	__set_blocks(i, (i)->keys, c)
-
-#define btree_data_space(b)	(PAGE_SIZE << (b)->page_order)
-
 #define prios_per_bucket(c)				\
 	((bucket_bytes(c) - sizeof(struct prio_set)) /	\
 	 sizeof(struct bucket_disk))

commit 67539e85289c14a76a1c4162613d14a5f05a0027
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 22:53:34 2013 -0700

    bcache: Add struct bset_sort_state
    
    More disentangling bset.c from the rest of the bcache code - soon, the
    sorting routines won't have any dependencies on any outside structs.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 2b46c86ac440..7bd4c93475e7 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -187,6 +187,7 @@
 #include <linux/types.h>
 #include <linux/workqueue.h>
 
+#include "bset.h"
 #include "util.h"
 #include "closure.h"
 
@@ -645,8 +646,7 @@ struct cache_set {
 	 */
 	mempool_t		*fill_iter;
 
-	mempool_t		*sort_pool;
-	unsigned		sort_crit_factor;
+	struct bset_sort_state	sort;
 
 	/* List of buckets we're currently writing data to */
 	struct list_head	data_buckets;
@@ -662,7 +662,6 @@ struct cache_set {
 	unsigned		congested_read_threshold_us;
 	unsigned		congested_write_threshold_us;
 
-	struct time_stats	sort_time;
 	struct time_stats	btree_gc_time;
 	struct time_stats	btree_split_time;
 	struct time_stats	btree_read_time;

commit fafff81cead78157099df1ee10af16cc51893ddc
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 21:56:21 2013 -0800

    bcache: Bkey indexing renaming
    
    More refactoring:
    
    node() -> bset_bkey_idx()
    end() -> bset_bkey_last()
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 3fd87323368c..2b46c86ac440 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -724,9 +724,6 @@ struct bbio {
 #define __set_blocks(i, k, c)	DIV_ROUND_UP(__set_bytes(i, k), block_bytes(c))
 #define set_blocks(i, c)	__set_blocks(i, (i)->keys, c)
 
-#define node(i, j)		((struct bkey *) ((i)->d + (j)))
-#define end(i)			node(i, (i)->keys)
-
 #define btree_data_space(b)	(PAGE_SIZE << (b)->page_order)
 
 #define prios_per_bucket(c)				\
@@ -791,18 +788,14 @@ static inline bool ptr_available(struct cache_set *c, const struct bkey *k,
 
 /* Btree key macros */
 
-static inline void bkey_init(struct bkey *k)
-{
-	*k = ZERO_KEY;
-}
-
 /*
  * This is used for various on disk data structures - cache_sb, prio_set, bset,
  * jset: The checksum is _always_ the first 8 bytes of these structs
  */
 #define csum_set(i)							\
 	bch_crc64(((void *) (i)) + sizeof(uint64_t),			\
-	      ((void *) end(i)) - (((void *) (i)) + sizeof(uint64_t)))
+		  ((void *) bset_bkey_last(i)) -			\
+		  (((void *) (i)) + sizeof(uint64_t)))
 
 /* Error handling macros */
 

commit 9a02b7eeeb446a0418ec83afc80eb38bc188f5c8
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Dec 20 17:24:46 2013 -0800

    bcache: Remove/fix some header dependencies
    
    In the process of disentagling/libraryizing bset.c from the rest of the
    bcache code.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 25f017248a81..3fd87323368c 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -704,11 +704,6 @@ struct bbio {
 	struct bio		bio;
 };
 
-static inline unsigned local_clock_us(void)
-{
-	return local_clock() >> 10;
-}
-
 #define BTREE_PRIO		USHRT_MAX
 #define INITIAL_PRIO		32768
 
@@ -776,6 +771,24 @@ static inline struct bucket *PTR_BUCKET(struct cache_set *c,
 	return PTR_CACHE(c, k, ptr)->buckets + PTR_BUCKET_NR(c, k, ptr);
 }
 
+static inline uint8_t gen_after(uint8_t a, uint8_t b)
+{
+	uint8_t r = a - b;
+	return r > 128U ? 0 : r;
+}
+
+static inline uint8_t ptr_stale(struct cache_set *c, const struct bkey *k,
+				unsigned i)
+{
+	return gen_after(PTR_BUCKET(c, k, i)->gen, PTR_GEN(k, i));
+}
+
+static inline bool ptr_available(struct cache_set *c, const struct bkey *k,
+				 unsigned i)
+{
+	return (PTR_DEV(k, i) < MAX_CACHES_PER_SET) && PTR_CACHE(c, k, i);
+}
+
 /* Btree key macros */
 
 static inline void bkey_init(struct bkey *k)

commit 0a45114534766058193eb2605c136562a4f7bcc8
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Dec 18 00:01:06 2013 -0800

    bcache: Use a mempool for mergesort temporary space
    
    It was a single element mempool before, it's slightly cleaner to just use a real
    mempool.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index eb6f2e6927ad..25f017248a81 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -645,12 +645,7 @@ struct cache_set {
 	 */
 	mempool_t		*fill_iter;
 
-	/*
-	 * btree_sort() is a merge sort and requires temporary space - single
-	 * element mempool
-	 */
-	struct mutex		sort_lock;
-	struct bset		*sort;
+	mempool_t		*sort_pool;
 	unsigned		sort_crit_factor;
 
 	/* List of buckets we're currently writing data to */

commit 78b77bf8b20431f8ad8a4db7e3120103bd922337
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 22:49:08 2013 -0800

    bcache: Btree verify code improvements
    
    Used this fixed code to find and fix the bug fixed by
    a4d885097b0ac0cd1337f171f2d4b83e946094d4.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d955a4934616..eb6f2e6927ad 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -629,6 +629,7 @@ struct cache_set {
 
 #ifdef CONFIG_BCACHE_DEBUG
 	struct btree		*verify_data;
+	struct bset		*verify_ondisk;
 	struct mutex		verify_lock;
 #endif
 

commit 88b9f8c426f35e04738220c1bc05dd1ea1b513a3
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 21:46:35 2013 -0800

    bcache: kill index()
    
    That was a terrible name for a macro, add some better helpers to replace it.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 94d346e2ea17..d955a4934616 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -736,10 +736,6 @@ static inline unsigned local_clock_us(void)
 #define node(i, j)		((struct bkey *) ((i)->d + (j)))
 #define end(i)			node(i, (i)->keys)
 
-#define index(i, b)							\
-	((size_t) (((void *) i - (void *) (b)->sets[0].data) /		\
-		   block_bytes(b->c)))
-
 #define btree_data_space(b)	(PAGE_SIZE << (b)->page_order)
 
 #define prios_per_bucket(c)				\

commit 78365411b344df35a198b119133e6515c2dcfb9f
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Dec 17 01:29:34 2013 -0800

    bcache: Rework allocator reserves
    
    We need a reserve for allocating buckets for new btree nodes - and now that
    we've got multiple btrees, it really needs to be per btree.
    
    This reworks the reserves so we've got separate freelists for each reserve
    instead of watermarks, which seems to make things a bit cleaner, and it adds
    some code so that btree_split() can make sure the reserve is available before it
    starts.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 9d062bc56261..94d346e2ea17 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -383,12 +383,12 @@ struct cached_dev {
 	unsigned		writeback_rate_p_term_inverse;
 };
 
-enum alloc_watermarks {
-	WATERMARK_PRIO,
-	WATERMARK_METADATA,
-	WATERMARK_MOVINGGC,
-	WATERMARK_NONE,
-	WATERMARK_MAX
+enum alloc_reserve {
+	RESERVE_BTREE,
+	RESERVE_PRIO,
+	RESERVE_MOVINGGC,
+	RESERVE_NONE,
+	RESERVE_NR,
 };
 
 struct cache {
@@ -400,8 +400,6 @@ struct cache {
 	struct kobject		kobj;
 	struct block_device	*bdev;
 
-	unsigned		watermark[WATERMARK_MAX];
-
 	struct task_struct	*alloc_thread;
 
 	struct closure		prio;
@@ -430,7 +428,7 @@ struct cache {
 	 * because all the data they contained was overwritten), so we only
 	 * need to discard them before they can be moved to the free list.
 	 */
-	DECLARE_FIFO(long, free);
+	DECLARE_FIFO(long, free)[RESERVE_NR];
 	DECLARE_FIFO(long, free_inc);
 	DECLARE_FIFO(long, unused);
 

commit cb7a583e6a6ace661a5890803e115d2292a293df
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Dec 16 15:27:25 2013 -0800

    bcache: kill closure locking usage
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index dbdbca5a9591..9d062bc56261 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -309,7 +309,8 @@ struct cached_dev {
 	struct cache_sb		sb;
 	struct bio		sb_bio;
 	struct bio_vec		sb_bv[1];
-	struct closure_with_waitlist sb_write;
+	struct closure		sb_write;
+	struct semaphore	sb_write_mutex;
 
 	/* Refcount on the cache set. Always nonzero when we're caching. */
 	atomic_t		count;
@@ -514,7 +515,8 @@ struct cache_set {
 	uint64_t		cached_dev_sectors;
 	struct closure		caching;
 
-	struct closure_with_waitlist sb_write;
+	struct closure		sb_write;
+	struct semaphore	sb_write_mutex;
 
 	mempool_t		*search;
 	mempool_t		*bio_meta;
@@ -635,7 +637,8 @@ struct cache_set {
 	unsigned		nr_uuids;
 	struct uuid_entry	*uuids;
 	BKEY_PADDED(uuid_bucket);
-	struct closure_with_waitlist uuid_write;
+	struct closure		uuid_write;
+	struct semaphore	uuid_write_mutex;
 
 	/*
 	 * A btree node on disk could have too many bsets for an iterator to fit

commit b28bc9b38c52f63f43e3fd875af982f2240a2859
Merge: 8d30726912cb 802eee95bde7
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 31 09:51:02 2013 -0700

    Merge tag 'v3.13-rc6' into for-3.14/core
    
    Needed to bring blk-mq uptodate, since changes have been going in
    since for-3.14/core was established.
    
    Fixup merge issues related to the immutable biovec changes.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    Conflicts:
            block/blk-flush.c
            fs/btrfs/check-integrity.c
            fs/btrfs/extent_io.c
            fs/btrfs/scrub.c
            fs/logfs/dev_bdev.c

commit 16749c23c00c686ed168471963e3ddb0f3fcd855
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Nov 11 13:58:34 2013 -0800

    bcache: New writeback PD controller
    
    The old writeback PD controller could get into states where it had throttled all
    the way down and take way too long to recover - it was too complicated to really
    understand what it was doing.
    
    This rewrites a good chunk of it to hopefully be simpler and make more sense,
    and it also pays more attention to units which should make the behaviour a bit
    easier to understand.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index a7b1a7631ed2..754f43177483 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -373,14 +373,14 @@ struct cached_dev {
 	unsigned char		writeback_percent;
 	unsigned		writeback_delay;
 
-	int			writeback_rate_change;
-	int64_t			writeback_rate_derivative;
 	uint64_t		writeback_rate_target;
+	int64_t			writeback_rate_proportional;
+	int64_t			writeback_rate_derivative;
+	int64_t			writeback_rate_change;
 
 	unsigned		writeback_rate_update_seconds;
 	unsigned		writeback_rate_d_term;
 	unsigned		writeback_rate_p_term_inverse;
-	unsigned		writeback_rate_d_smooth;
 };
 
 enum alloc_watermarks {

commit 981aa8c091e164ea51dd1e81b71a1f3852bbcceb
Author: Nicholas Swenson <nks@daterainc.com>
Date:   Thu Nov 7 17:53:19 2013 -0800

    bcache: bugfix - moving_gc now moves only correct buckets
    
    Removed gc_move_threshold because picking buckets only by
    threshold could lead moving extra buckets (ei. if there are
    buckets at the threshold that aren't supposed to be moved
    do to space considerations).
    
    This is replaced by a GC_MOVE bit in the gc_mark bitmask.
    Now only marked buckets get moved.
    
    Signed-off-by: Nicholas Swenson <nks@daterainc.com>
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 4beb55a0ff30..a7b1a7631ed2 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -197,7 +197,7 @@ struct bucket {
 	uint8_t		disk_gen;
 	uint8_t		last_gc; /* Most out of date gen in the btree */
 	uint8_t		gc_gen;
-	uint16_t	gc_mark;
+	uint16_t	gc_mark; /* Bitfield used by GC. See below for field */
 };
 
 /*
@@ -209,7 +209,8 @@ BITMASK(GC_MARK,	 struct bucket, gc_mark, 0, 2);
 #define GC_MARK_RECLAIMABLE	0
 #define GC_MARK_DIRTY		1
 #define GC_MARK_METADATA	2
-BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 14);
+BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 13);
+BITMASK(GC_MOVE, struct bucket, gc_mark, 15, 1);
 
 #include "journal.h"
 #include "stats.h"
@@ -445,7 +446,6 @@ struct cache {
 	 * call prio_write() to keep gens from wrapping.
 	 */
 	uint8_t			need_save_prio;
-	unsigned		gc_move_threshold;
 
 	/*
 	 * If nonzero, we know we aren't going to find any buckets to invalidate

commit 20d0189b1012a37d2533a87fb451f7852f2418d1
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Sat Nov 23 18:21:01 2013 -0800

    block: Introduce new bio_split()
    
    The new bio_split() can split arbitrary bios - it's not restricted to
    single page bios, like the old bio_split() (previously renamed to
    bio_pair_split()). It also has different semantics - it doesn't allocate
    a struct bio_pair, leaving it up to the caller to handle completions.
    
    Then convert the existing bio_pair_split() users to the new bio_split()
    - and also nvme, which was open coding bio splitting.
    
    (We have to take that BUG_ON() out of bio_integrity_trim() because this
    bio_split() needs to use it, and there's no reason it has to be used on
    bios marked as cloned; BIO_CLONED doesn't seem to have clearly
    documented semantics anyways.)
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: Matthew Wilcox <matthew.r.wilcox@intel.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Vishal Verma <vishal.l.verma@intel.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Neil Brown <neilb@suse.de>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6b6fe935be73..964353c5329d 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -901,7 +901,6 @@ void bch_bbio_endio(struct cache_set *, struct bio *, int, const char *);
 void bch_bbio_free(struct bio *, struct cache_set *);
 struct bio *bch_bbio_alloc(struct cache_set *);
 
-struct bio *bch_bio_split(struct bio *, int, gfp_t, struct bio_set *);
 void bch_generic_make_request(struct bio *, struct bio_split_pool *);
 void __bch_submit_bbio(struct bio *, struct cache_set *);
 void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);

commit ed9c47bebeeea4a468b07cfd745c690190f8014c
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Fri Nov 22 19:37:48 2013 -0800

    bcache: Kill unaligned bvec hack
    
    Bcache has a hack to avoid cloning the biovec if it's all full pages -
    but with immutable biovecs coming this won't be necessary anymore.
    
    For now, we remove the special case and always clone the bvec array so
    that the immutable biovec patches are simpler.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 4beb55a0ff30..6b6fe935be73 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -279,7 +279,6 @@ struct bcache_device {
 	unsigned long		sectors_dirty_last;
 	long			sectors_dirty_derivative;
 
-	mempool_t		*unaligned_bvec;
 	struct bio_set		*bio_split;
 
 	unsigned		data_csum:1;

commit 5ceaaad7047745c1c02150c39d3fb623b7948d48
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 14:27:42 2013 -0700

    bcache: Bypass torture test
    
    More testing ftw! Also, now verify mode doesn't break if you read dirty
    data.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 97ef126b68bb..4beb55a0ff30 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -364,6 +364,7 @@ struct cached_dev {
 	unsigned		readahead;
 
 	unsigned		verify:1;
+	unsigned		bypass_torture_test:1;
 
 	unsigned		partial_stripes_expensive:1;
 	unsigned		writeback_metadata:1;

commit c4d951ddb66fe1d087447b0ba65c4fa4446f1083
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Aug 21 17:49:09 2013 -0700

    bcache: Fix sysfs splat on shutdown with flash only devs
    
    Whoops.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index ab0b2150fed6..97ef126b68bb 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -266,12 +266,10 @@ struct bcache_device {
 
 	struct gendisk		*disk;
 
-	/* If nonzero, we're closing */
-	atomic_t		closing;
-
-	/* If nonzero, we're detaching/unregistering from cache set */
-	atomic_t		detaching;
-	int			flush_done;
+	unsigned long		flags;
+#define BCACHE_DEV_CLOSING	0
+#define BCACHE_DEV_DETACHING	1
+#define BCACHE_DEV_UNLINK_DONE	2
 
 	unsigned		nr_stripes;
 	unsigned		stripe_size;

commit 48a915a87f0bd98c3d68d029acf223a2e5116f07
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 31 15:43:22 2013 -0700

    bcache: Better full stripe scanning
    
    The old scanning-by-stripe code burned too much CPU, this should be
    better.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 816d07958fac..ab0b2150fed6 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -237,7 +237,7 @@ struct keybuf {
 
 	struct rb_root		keys;
 
-#define KEYBUF_NR		100
+#define KEYBUF_NR		500
 	DECLARE_ARRAY_ALLOCATOR(struct keybuf_key, freelist, KEYBUF_NR);
 };
 
@@ -273,9 +273,10 @@ struct bcache_device {
 	atomic_t		detaching;
 	int			flush_done;
 
-	uint64_t		nr_stripes;
+	unsigned		nr_stripes;
 	unsigned		stripe_size;
 	atomic_t		*stripe_sectors_dirty;
+	unsigned long		*full_dirty_stripes;
 
 	unsigned long		sectors_dirty_last;
 	long			sectors_dirty_derivative;

commit 65d22e911bfc4f46cda4751f1b1926b43c316c14
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 31 00:03:54 2013 -0700

    bcache: Move spinlock into struct time_stats
    
    Minor cleanup.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 322735547eab..816d07958fac 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -666,11 +666,9 @@ struct cache_set {
 	unsigned		congested_read_threshold_us;
 	unsigned		congested_write_threshold_us;
 
-	spinlock_t		sort_time_lock;
 	struct time_stats	sort_time;
 	struct time_stats	btree_gc_time;
 	struct time_stats	btree_split_time;
-	spinlock_t		btree_read_time_lock;
 	struct time_stats	btree_read_time;
 	struct time_stats	try_harder_time;
 

commit 8aee122071a69ca6fa3314da7713bdf0b61dc07c
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Jul 30 22:34:40 2013 -0700

    bcache: Kill sequential_merge option
    
    It never really made sense to expose this, so just kill it.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d6970a651e42..322735547eab 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -364,7 +364,6 @@ struct cached_dev {
 	unsigned		sequential_cutoff;
 	unsigned		readahead;
 
-	unsigned		sequential_merge:1;
 	unsigned		verify:1;
 
 	unsigned		partial_stripes_expensive:1;

commit a1f0358b2bf69be216cb6e4ea40fe7ae4d38b8a6
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 19:07:00 2013 -0700

    bcache: Incremental gc
    
    Big garbage collection rewrite; now, garbage collection uses the same
    mechanisms as used elsewhere for inserting/updating btree node pointers,
    instead of rewriting interior btree nodes in place.
    
    This makes the code significantly cleaner and less fragile, and means we
    can now make garbage collection incremental - it doesn't have to hold a
    write lock on the root of the btree for the entire duration of garbage
    collection.
    
    This means that there's less of a latency hit for doing garbage
    collection, which means we can gc more frequently (and do a better job
    of reclaiming from the cache), and we can coalesce across more btree
    nodes (improving our space efficiency).
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d03bc6f66493..d6970a651e42 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -477,7 +477,6 @@ struct gc_stat {
 
 	size_t			nkeys;
 	uint64_t		data;	/* sectors */
-	uint64_t		dirty;	/* sectors */
 	unsigned		in_use; /* percent */
 };
 

commit 280481d06c8a683d9aaa26125476222e76b733c5
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 24 16:36:03 2013 -0700

    bcache: Debug code improvements
    
    Couple changes:
     * Consolidate bch_check_keys() and bch_check_key_order(), and move the
       checks that only check_key_order() could do to bch_btree_iter_next().
    
     * Get rid of CONFIG_BCACHE_EDEBUG - now, all that code is compiled in
       when CONFIG_BCACHE_DEBUG is enabled, and there's now a sysfs file to
       flip on the EDEBUG checks at runtime.
    
     * Dropped an old not terribly useful check in rw_unlock(), and
       refactored/improved a some of the other debug code.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 045cb99f1ca6..d03bc6f66493 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -690,6 +690,7 @@ struct cache_set {
 	unsigned short		journal_delay_ms;
 	unsigned		verify:1;
 	unsigned		key_merging_disabled:1;
+	unsigned		expensive_debug_checks:1;
 	unsigned		gc_always_rewrite:1;
 	unsigned		shrinker_disabled:1;
 	unsigned		copy_gc_enabled:1;
@@ -698,15 +699,6 @@ struct cache_set {
 	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];
 };
 
-static inline bool key_merging_disabled(struct cache_set *c)
-{
-#ifdef CONFIG_BCACHE_DEBUG
-	return c->key_merging_disabled;
-#else
-	return 0;
-#endif
-}
-
 struct bbio {
 	unsigned		submit_time_us;
 	union {

commit 81ab4190ac17df41686a37c97f701623276b652a
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 31 15:46:42 2013 -0700

    bcache: Pull on disk data structures out into a separate header
    
    Now, the on disk data structures are in a header that can be exported to
    userspace - and having them all centralized is nice too.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index e32f6fd91755..045cb99f1ca6 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -177,6 +177,7 @@
 
 #define pr_fmt(fmt) "bcache: %s() " fmt "\n", __func__
 
+#include <linux/bcache.h>
 #include <linux/bio.h>
 #include <linux/kobject.h>
 #include <linux/list.h>
@@ -210,168 +211,6 @@ BITMASK(GC_MARK,	 struct bucket, gc_mark, 0, 2);
 #define GC_MARK_METADATA	2
 BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 14);
 
-struct bkey {
-	uint64_t	high;
-	uint64_t	low;
-	uint64_t	ptr[];
-};
-
-/* Enough for a key with 6 pointers */
-#define BKEY_PAD		8
-
-#define BKEY_PADDED(key)					\
-	union { struct bkey key; uint64_t key ## _pad[BKEY_PAD]; }
-
-/* Version 0: Cache device
- * Version 1: Backing device
- * Version 2: Seed pointer into btree node checksum
- * Version 3: Cache device with new UUID format
- * Version 4: Backing device with data offset
- */
-#define BCACHE_SB_VERSION_CDEV			0
-#define BCACHE_SB_VERSION_BDEV			1
-#define BCACHE_SB_VERSION_CDEV_WITH_UUID	3
-#define BCACHE_SB_VERSION_BDEV_WITH_OFFSET	4
-#define BCACHE_SB_MAX_VERSION			4
-
-#define SB_SECTOR		8
-#define SB_SIZE			4096
-#define SB_LABEL_SIZE		32
-#define SB_JOURNAL_BUCKETS	256U
-/* SB_JOURNAL_BUCKETS must be divisible by BITS_PER_LONG */
-#define MAX_CACHES_PER_SET	8
-
-#define BDEV_DATA_START_DEFAULT	16	/* sectors */
-
-struct cache_sb {
-	uint64_t		csum;
-	uint64_t		offset;	/* sector where this sb was written */
-	uint64_t		version;
-
-	uint8_t			magic[16];
-
-	uint8_t			uuid[16];
-	union {
-		uint8_t		set_uuid[16];
-		uint64_t	set_magic;
-	};
-	uint8_t			label[SB_LABEL_SIZE];
-
-	uint64_t		flags;
-	uint64_t		seq;
-	uint64_t		pad[8];
-
-	union {
-	struct {
-		/* Cache devices */
-		uint64_t	nbuckets;	/* device size */
-
-		uint16_t	block_size;	/* sectors */
-		uint16_t	bucket_size;	/* sectors */
-
-		uint16_t	nr_in_set;
-		uint16_t	nr_this_dev;
-	};
-	struct {
-		/* Backing devices */
-		uint64_t	data_offset;
-
-		/*
-		 * block_size from the cache device section is still used by
-		 * backing devices, so don't add anything here until we fix
-		 * things to not need it for backing devices anymore
-		 */
-	};
-	};
-
-	uint32_t		last_mount;	/* time_t */
-
-	uint16_t		first_bucket;
-	union {
-		uint16_t	njournal_buckets;
-		uint16_t	keys;
-	};
-	uint64_t		d[SB_JOURNAL_BUCKETS];	/* journal buckets */
-};
-
-BITMASK(CACHE_SYNC,		struct cache_sb, flags, 0, 1);
-BITMASK(CACHE_DISCARD,		struct cache_sb, flags, 1, 1);
-BITMASK(CACHE_REPLACEMENT,	struct cache_sb, flags, 2, 3);
-#define CACHE_REPLACEMENT_LRU	0U
-#define CACHE_REPLACEMENT_FIFO	1U
-#define CACHE_REPLACEMENT_RANDOM 2U
-
-BITMASK(BDEV_CACHE_MODE,	struct cache_sb, flags, 0, 4);
-#define CACHE_MODE_WRITETHROUGH	0U
-#define CACHE_MODE_WRITEBACK	1U
-#define CACHE_MODE_WRITEAROUND	2U
-#define CACHE_MODE_NONE		3U
-BITMASK(BDEV_STATE,		struct cache_sb, flags, 61, 2);
-#define BDEV_STATE_NONE		0U
-#define BDEV_STATE_CLEAN	1U
-#define BDEV_STATE_DIRTY	2U
-#define BDEV_STATE_STALE	3U
-
-/* Version 1: Seed pointer into btree node checksum
- */
-#define BCACHE_BSET_VERSION	1
-
-/*
- * This is the on disk format for btree nodes - a btree node on disk is a list
- * of these; within each set the keys are sorted
- */
-struct bset {
-	uint64_t		csum;
-	uint64_t		magic;
-	uint64_t		seq;
-	uint32_t		version;
-	uint32_t		keys;
-
-	union {
-		struct bkey	start[0];
-		uint64_t	d[0];
-	};
-};
-
-/*
- * On disk format for priorities and gens - see super.c near prio_write() for
- * more.
- */
-struct prio_set {
-	uint64_t		csum;
-	uint64_t		magic;
-	uint64_t		seq;
-	uint32_t		version;
-	uint32_t		pad;
-
-	uint64_t		next_bucket;
-
-	struct bucket_disk {
-		uint16_t	prio;
-		uint8_t		gen;
-	} __attribute((packed)) data[];
-};
-
-struct uuid_entry {
-	union {
-		struct {
-			uint8_t		uuid[16];
-			uint8_t		label[32];
-			uint32_t	first_reg;
-			uint32_t	last_reg;
-			uint32_t	invalidated;
-
-			uint32_t	flags;
-			/* Size of flash only volumes */
-			uint64_t	sectors;
-		};
-
-		uint8_t	pad[128];
-	};
-};
-
-BITMASK(UUID_FLASH_ONLY,	struct uuid_entry, flags, 0, 1);
-
 #include "journal.h"
 #include "stats.h"
 struct search;
@@ -868,12 +707,6 @@ static inline bool key_merging_disabled(struct cache_set *c)
 #endif
 }
 
-static inline bool SB_IS_BDEV(const struct cache_sb *sb)
-{
-	return sb->version == BCACHE_SB_VERSION_BDEV
-		|| sb->version == BCACHE_SB_VERSION_BDEV_WITH_OFFSET;
-}
-
 struct bbio {
 	unsigned		submit_time_us;
 	union {
@@ -927,59 +760,6 @@ static inline unsigned local_clock_us(void)
 #define prio_buckets(c)					\
 	DIV_ROUND_UP((size_t) (c)->sb.nbuckets, prios_per_bucket(c))
 
-#define JSET_MAGIC		0x245235c1a3625032ULL
-#define PSET_MAGIC		0x6750e15f87337f91ULL
-#define BSET_MAGIC		0x90135c78b99e07f5ULL
-
-#define jset_magic(c)		((c)->sb.set_magic ^ JSET_MAGIC)
-#define pset_magic(c)		((c)->sb.set_magic ^ PSET_MAGIC)
-#define bset_magic(c)		((c)->sb.set_magic ^ BSET_MAGIC)
-
-/* Bkey fields: all units are in sectors */
-
-#define KEY_FIELD(name, field, offset, size)				\
-	BITMASK(name, struct bkey, field, offset, size)
-
-#define PTR_FIELD(name, offset, size)					\
-	static inline uint64_t name(const struct bkey *k, unsigned i)	\
-	{ return (k->ptr[i] >> offset) & ~(((uint64_t) ~0) << size); }	\
-									\
-	static inline void SET_##name(struct bkey *k, unsigned i, uint64_t v)\
-	{								\
-		k->ptr[i] &= ~(~((uint64_t) ~0 << size) << offset);	\
-		k->ptr[i] |= v << offset;				\
-	}
-
-KEY_FIELD(KEY_PTRS,	high, 60, 3)
-KEY_FIELD(HEADER_SIZE,	high, 58, 2)
-KEY_FIELD(KEY_CSUM,	high, 56, 2)
-KEY_FIELD(KEY_PINNED,	high, 55, 1)
-KEY_FIELD(KEY_DIRTY,	high, 36, 1)
-
-KEY_FIELD(KEY_SIZE,	high, 20, 16)
-KEY_FIELD(KEY_INODE,	high, 0,  20)
-
-/* Next time I change the on disk format, KEY_OFFSET() won't be 64 bits */
-
-static inline uint64_t KEY_OFFSET(const struct bkey *k)
-{
-	return k->low;
-}
-
-static inline void SET_KEY_OFFSET(struct bkey *k, uint64_t v)
-{
-	k->low = v;
-}
-
-PTR_FIELD(PTR_DEV,		51, 12)
-PTR_FIELD(PTR_OFFSET,		8,  43)
-PTR_FIELD(PTR_GEN,		0,  8)
-
-#define PTR_CHECK_DEV		((1 << 12) - 1)
-
-#define PTR(gen, offset, dev)						\
-	((((uint64_t) dev) << 51) | ((uint64_t) offset) << 8 | gen)
-
 static inline size_t sector_to_bucket(struct cache_set *c, sector_t s)
 {
 	return s >> c->bucket_bits;
@@ -1018,31 +798,11 @@ static inline struct bucket *PTR_BUCKET(struct cache_set *c,
 
 /* Btree key macros */
 
-/*
- * The high bit being set is a relic from when we used it to do binary
- * searches - it told you where a key started. It's not used anymore,
- * and can probably be safely dropped.
- */
-#define KEY(dev, sector, len)						\
-((struct bkey) {							\
-	.high = (1ULL << 63) | ((uint64_t) (len) << 20) | (dev),	\
-	.low = (sector)							\
-})
-
 static inline void bkey_init(struct bkey *k)
 {
-	*k = KEY(0, 0, 0);
+	*k = ZERO_KEY;
 }
 
-#define KEY_START(k)		(KEY_OFFSET(k) - KEY_SIZE(k))
-#define START_KEY(k)		KEY(KEY_INODE(k), KEY_START(k), 0)
-
-#define MAX_KEY_INODE		(~(~0 << 20))
-#define MAX_KEY_OFFSET		(((uint64_t) ~0) >> 1)
-#define MAX_KEY			KEY(MAX_KEY_INODE, MAX_KEY_OFFSET, 0)
-
-#define ZERO_KEY		KEY(0, 0, 0)
-
 /*
  * This is used for various on disk data structures - cache_sb, prio_set, bset,
  * jset: The checksum is _always_ the first 8 bytes of these structs

commit 2599b53b7b0ea6103d1661dca74d35480cb8fa1f
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 18:11:11 2013 -0700

    bcache: Move sector allocator to alloc.c
    
    Just reorganizing things a bit.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 20fe96c121d9..e32f6fd91755 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1170,6 +1170,8 @@ int __bch_bucket_alloc_set(struct cache_set *, unsigned,
 			   struct bkey *, int, bool);
 int bch_bucket_alloc_set(struct cache_set *, unsigned,
 			 struct bkey *, int, bool);
+bool bch_alloc_sectors(struct cache_set *, struct bkey *, unsigned,
+		       unsigned, unsigned, bool);
 
 __printf(2, 3)
 bool bch_cache_set_error(struct cache_set *, const char *, ...);
@@ -1210,6 +1212,8 @@ struct cache_set *bch_cache_set_alloc(struct cache_sb *);
 void bch_btree_cache_free(struct cache_set *);
 int bch_btree_cache_alloc(struct cache_set *);
 void bch_moving_init_cache_set(struct cache_set *);
+int bch_open_buckets_alloc(struct cache_set *);
+void bch_open_buckets_free(struct cache_set *);
 
 int bch_cache_allocator_start(struct cache *ca);
 int bch_cache_allocator_init(struct cache *ca);

commit 48dad8baf92fe8967d9e1358af1cfdda1d2d3298
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 18:48:51 2013 -0700

    bcache: Add btree_map() functions
    
    Lots of stuff has been open coding its own btree traversal - which is
    generally pretty simple code, but there are a few subtleties.
    
    This adds new new functions, bch_btree_map_nodes() and
    bch_btree_map_keys(), which do the traversal for you. Everything that's
    open coding btree traversal now (with the exception of garbage
    collection) is slowly going to be converted to these two functions;
    being able to write other code at a higher level of abstraction  is a
    big improvement w.r.t. overall code quality.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 674e2f42e778..20fe96c121d9 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -384,8 +384,6 @@ struct keybuf_key {
 	void			*private;
 };
 
-typedef bool (keybuf_pred_fn)(struct keybuf *, struct bkey *);
-
 struct keybuf {
 	struct bkey		last_scanned;
 	spinlock_t		lock;

commit 5e6926daac267dd99552ae613f041a9e88bbf258
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:50:06 2013 -0700

    bcache: Convert writeback to a kthread
    
    This simplifies the writeback flow control quite a bit - previously, it
    was conceptually two coroutines, refill_dirty() and read_dirty(). This
    makes the code quite a bit more straightforward.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 09410eb07d82..674e2f42e778 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -509,7 +509,7 @@ struct cached_dev {
 
 	/* Limit number of writeback bios in flight */
 	struct semaphore	in_flight;
-	struct closure_with_timer writeback;
+	struct task_struct	*writeback_thread;
 
 	struct keybuf		writeback_keys;
 
@@ -1038,7 +1038,11 @@ static inline void bkey_init(struct bkey *k)
 
 #define KEY_START(k)		(KEY_OFFSET(k) - KEY_SIZE(k))
 #define START_KEY(k)		KEY(KEY_INODE(k), KEY_START(k), 0)
-#define MAX_KEY			KEY(~(~0 << 20), ((uint64_t) ~0) >> 1, 0)
+
+#define MAX_KEY_INODE		(~(~0 << 20))
+#define MAX_KEY_OFFSET		(((uint64_t) ~0) >> 1)
+#define MAX_KEY			KEY(MAX_KEY_INODE, MAX_KEY_OFFSET, 0)
+
 #define ZERO_KEY		KEY(0, 0, 0)
 
 /*
@@ -1214,8 +1218,6 @@ int bch_cache_allocator_init(struct cache *ca);
 
 void bch_debug_exit(void);
 int bch_debug_init(struct kobject *);
-void bch_writeback_exit(void);
-int bch_writeback_init(void);
 void bch_request_exit(void);
 int bch_request_init(void);
 void bch_btree_exit(void);

commit 72a44517f3ca3725dc86081d105457df46448679
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Oct 24 17:19:26 2013 -0700

    bcache: Convert gc to a kthread
    
    We needed a dedicated rescuer workqueue for gc anyways... and gc was
    conceptually a dedicated thread, just one that wasn't running all the
    time. Switch it to a dedicated thread to make the code a bit more
    straightforward.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d3520748bc27..09410eb07d82 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -773,7 +773,7 @@ struct cache_set {
 	struct gc_stat		gc_stats;
 	size_t			nbuckets;
 
-	struct closure_with_waitlist gc;
+	struct task_struct	*gc_thread;
 	/* Where in the btree gc currently is */
 	struct bkey		gc_done;
 
@@ -786,11 +786,10 @@ struct cache_set {
 	/* Counts how many sectors bio_insert has added to the cache */
 	atomic_t		sectors_to_gc;
 
-	struct closure		moving_gc;
-	struct closure_waitlist	moving_gc_wait;
+	wait_queue_head_t	moving_gc_wait;
 	struct keybuf		moving_gc_keys;
 	/* Number of moving GC bios in flight */
-	atomic_t		in_flight;
+	struct semaphore	moving_in_flight;
 
 	struct btree		*root;
 
@@ -1176,7 +1175,7 @@ bool bch_cache_set_error(struct cache_set *, const char *, ...);
 void bch_prio_write(struct cache *);
 void bch_write_bdev_super(struct cached_dev *, struct closure *);
 
-extern struct workqueue_struct *bcache_wq, *bch_gc_wq;
+extern struct workqueue_struct *bcache_wq;
 extern const char * const bch_cache_modes[];
 extern struct mutex bch_register_lock;
 extern struct list_head bch_cache_sets;

commit 35fcd848d72683141052aa9880542461577f2dbe
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:29:09 2013 -0700

    bcache: Convert bucket_wait to wait_queue_head_t
    
    At one point we did do fancy asynchronous waiting stuff with
    bucket_wait, but that's all gone (and bucket_wait is used a lot less
    than it used to be). So use the standard primitives.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index c1c44191afb1..d3520748bc27 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -750,7 +750,7 @@ struct cache_set {
 	 * written.
 	 */
 	atomic_t		prio_blocked;
-	struct closure_waitlist	bucket_wait;
+	wait_queue_head_t	bucket_wait;
 
 	/*
 	 * For any bio we don't skip we subtract the number of sectors from
@@ -1162,13 +1162,13 @@ uint8_t bch_inc_gen(struct cache *, struct bucket *);
 void bch_rescale_priorities(struct cache_set *, int);
 bool bch_bucket_add_unused(struct cache *, struct bucket *);
 
-long bch_bucket_alloc(struct cache *, unsigned, struct closure *);
+long bch_bucket_alloc(struct cache *, unsigned, bool);
 void bch_bucket_free(struct cache_set *, struct bkey *);
 
 int __bch_bucket_alloc_set(struct cache_set *, unsigned,
-			   struct bkey *, int, struct closure *);
+			   struct bkey *, int, bool);
 int bch_bucket_alloc_set(struct cache_set *, unsigned,
-			 struct bkey *, int, struct closure *);
+			 struct bkey *, int, bool);
 
 __printf(2, 3)
 bool bch_cache_set_error(struct cache_set *, const char *, ...);

commit e8e1d4682c8cb06dbcb5ef7bb851bf9bcb889c84
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:27:07 2013 -0700

    bcache: Convert try_wait to wait_queue_head_t
    
    We never waited on c->try_wait asynchronously, so just use the standard
    primitives.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 10ce0c825fce..c1c44191afb1 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -735,8 +735,8 @@ struct cache_set {
 	 * basically a lock for this that we can wait on asynchronously. The
 	 * btree_root() macro releases the lock when it returns.
 	 */
-	struct closure		*try_harder;
-	struct closure_waitlist	try_wait;
+	struct task_struct	*try_harder;
+	wait_queue_head_t	try_wait;
 	uint64_t		try_harder_start;
 
 	/*

commit e7c590eb63509c5d5f48a390d23aa25f4417ac96
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Sep 10 18:39:16 2013 -0700

    bcache: Convert btree_insert_check_key() to btree_insert_node()
    
    This was the main point of all this refactoring - now,
    btree_insert_check_key() won't fail just because the leaf node happened
    to be full.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6e836f22f276..10ce0c825fce 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1091,14 +1091,6 @@ do {									\
 	for (b = (ca)->buckets + (ca)->sb.first_bucket;			\
 	     b < (ca)->buckets + (ca)->sb.nbuckets; b++)
 
-static inline void __bkey_put(struct cache_set *c, struct bkey *k)
-{
-	unsigned i;
-
-	for (i = 0; i < KEY_PTRS(k); i++)
-		atomic_dec_bug(&PTR_BUCKET(c, k, i)->pin);
-}
-
 static inline void cached_dev_put(struct cached_dev *dc)
 {
 	if (atomic_dec_and_test(&dc->count))

commit 2d679fc75678551485df62274edaed452becd16d
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Sat Aug 17 02:13:15 2013 -0700

    bcache: Stripe size isn't necessarily a power of two
    
    Originally I got this right... except that the divides didn't use
    do_div(), which broke 32 bit kernels. When I went to fix that, I forgot
    that the raid stripe size usually isn't a power of two... doh
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 578615604be5..6e836f22f276 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -437,7 +437,7 @@ struct bcache_device {
 	int			flush_done;
 
 	uint64_t		nr_stripes;
-	unsigned		stripe_size_bits;
+	unsigned		stripe_size;
 	atomic_t		*stripe_sectors_dirty;
 
 	unsigned long		sectors_dirty_last;

commit 77c320eb46e216c17aee5c943949229ccfed6904
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jul 11 19:42:51 2013 -0700

    bcache: Add on error panic/unregister setting
    
    Works kind of like the ext4 setting, to panic or remount read only on
    errors.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index bf37474a6888..578615604be5 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -843,8 +843,14 @@ struct cache_set {
 	atomic_long_t		cache_read_races;
 	atomic_long_t		writeback_keys_done;
 	atomic_long_t		writeback_keys_failed;
+
+	enum			{
+		ON_ERROR_UNREGISTER,
+		ON_ERROR_PANIC,
+	}			on_error;
 	unsigned		error_limit;
 	unsigned		error_decay;
+
 	unsigned short		journal_delay_ms;
 	unsigned		verify:1;
 	unsigned		key_merging_disabled:1;

commit 49b1212dfacfe51f951442563d1617bb06aac575
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 24 17:16:09 2013 -0700

    bcache: Use blkdev_issue_discard()
    
    The old asynchronous discard code was really a relic from when all the
    allocation code was asynchronous - now that allocation runs out of a
    dedicated thread there's no point in keeping around all that complicated
    machinery.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 0f12382aa35d..bf37474a6888 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -620,15 +620,6 @@ struct cache {
 
 	bool			discard; /* Get rid of? */
 
-	/*
-	 * We preallocate structs for issuing discards to buckets, and keep them
-	 * on this list when they're not in use; do_discard() issues discards
-	 * whenever there's work to do and is called by free_some_buckets() and
-	 * when a discard finishes.
-	 */
-	atomic_t		discards_in_flight;
-	struct list_head	discards;
-
 	struct journal_device	journal;
 
 	/* The rest of this all shows up in sysfs */
@@ -1222,7 +1213,6 @@ int bch_btree_cache_alloc(struct cache_set *);
 void bch_moving_init_cache_set(struct cache_set *);
 
 int bch_cache_allocator_start(struct cache *ca);
-void bch_cache_allocator_exit(struct cache *ca);
 int bch_cache_allocator_init(struct cache *ca);
 
 void bch_debug_exit(void);

commit c2a4f3183a1248f615a695fbd8905da55ad11bba
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Mon Sep 23 23:17:31 2013 -0700

    bcache: Fix a writeback performance regression
    
    Background writeback works by scanning the btree for dirty data and
    adding those keys into a fixed size buffer, then for each dirty key in
    the keybuf writing it to the backing device.
    
    When read_dirty() finishes and it's time to scan for more dirty data, we
    need to wait for the outstanding writeback IO to finish - they still
    take up slots in the keybuf (so that foreground writes can check for
    them to avoid races) - without that wait, we'll continually rescan when
    we'll be able to add at most a key or two to the keybuf, and that takes
    locks that starves foreground IO.  Doh.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: linux-stable <stable@vger.kernel.org> # >= v3.10
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b39f6f0b45f2..0f12382aa35d 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -498,7 +498,7 @@ struct cached_dev {
 	 */
 	atomic_t		has_dirty;
 
-	struct ratelimit	writeback_rate;
+	struct bch_ratelimit	writeback_rate;
 	struct delayed_work	writeback_rate_update;
 
 	/*
@@ -507,10 +507,9 @@ struct cached_dev {
 	 */
 	sector_t		last_read;
 
-	/* Number of writeback bios in flight */
-	atomic_t		in_flight;
+	/* Limit number of writeback bios in flight */
+	struct semaphore	in_flight;
 	struct closure_with_timer writeback;
-	struct closure_waitlist	writeback_wait;
 
 	struct keybuf		writeback_keys;
 

commit 79826c35eb99cd3c0873b8396f45fa26c87fb0b0
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 18:31:58 2013 -0700

    bcache: Allocation kthread fixes
    
    The alloc kthread should've been using try_to_freeze() - and also there
    was the potential for the alloc kthread to get woken up after it had
    shut down, which would have been bad.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 68f1ded81ae0..b39f6f0b45f2 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -664,13 +664,9 @@ struct gc_stat {
  * CACHE_SET_STOPPING always gets set first when we're closing down a cache set;
  * we'll continue to run normally for awhile with CACHE_SET_STOPPING set (i.e.
  * flushing dirty data).
- *
- * CACHE_SET_STOPPING_2 gets set at the last phase, when it's time to shut down
- * the allocation thread.
  */
 #define CACHE_SET_UNREGISTERING		0
 #define	CACHE_SET_STOPPING		1
-#define	CACHE_SET_STOPPING_2		2
 
 struct cache_set {
 	struct closure		cl;

commit c9502ea4424b31728703d113fc6b30bfead14633
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Wed Jul 10 21:25:02 2013 -0700

    bcache: Fix a sysfs splat on shutdown
    
    If we stopped a bcache device when we were already detaching (or
    something like that), bcache_device_unlink() would try to remove a
    symlink from sysfs that was already gone because the bcache dev kobject
    had already been removed from sysfs.
    
    So keep track of whether we've removed stuff from sysfs.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>
    Cc: linux-stable <stable@vger.kernel.org> # >= v3.10

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 342ba86c6e4f..68f1ded81ae0 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -434,6 +434,7 @@ struct bcache_device {
 
 	/* If nonzero, we're detaching/unregistering from cache set */
 	atomic_t		detaching;
+	int			flush_done;
 
 	uint64_t		nr_stripes;
 	unsigned		stripe_size_bits;

commit 72c270612bd33192fa836ad0f2939af1ca218292
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Jun 5 06:24:39 2013 -0700

    bcache: Write out full stripes
    
    Now that we're tracking dirty data per stripe, we can add two
    optimizations for raid5/6:
    
     * If a stripe is already dirty, force writes to that stripe to
       writeback mode - to help build up full stripes of dirty data
    
     * When flushing dirty data, preferentially write out full stripes first
       if there are any.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index dbddef0cdb59..342ba86c6e4f 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -387,8 +387,6 @@ struct keybuf_key {
 typedef bool (keybuf_pred_fn)(struct keybuf *, struct bkey *);
 
 struct keybuf {
-	keybuf_pred_fn		*key_predicate;
-
 	struct bkey		last_scanned;
 	spinlock_t		lock;
 
@@ -532,6 +530,7 @@ struct cached_dev {
 	unsigned		sequential_merge:1;
 	unsigned		verify:1;
 
+	unsigned		partial_stripes_expensive:1;
 	unsigned		writeback_metadata:1;
 	unsigned		writeback_running:1;
 	unsigned char		writeback_percent;

commit 279afbad4e54acbd61bf88a54a73af3bbfdeb5dd
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Jun 5 06:21:07 2013 -0700

    bcache: Track dirty data by stripe
    
    To make background writeback aware of raid5/6 stripes, we first need to
    track the amount of dirty data within each stripe - we do this by
    breaking up the existing sectors_dirty into per stripe atomic_ts
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d099d8894c2f..dbddef0cdb59 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -437,7 +437,10 @@ struct bcache_device {
 	/* If nonzero, we're detaching/unregistering from cache set */
 	atomic_t		detaching;
 
-	atomic_long_t		sectors_dirty;
+	uint64_t		nr_stripes;
+	unsigned		stripe_size_bits;
+	atomic_t		*stripe_sectors_dirty;
+
 	unsigned long		sectors_dirty_last;
 	long			sectors_dirty_derivative;
 
@@ -1159,9 +1162,6 @@ static inline void wake_up_allocators(struct cache_set *c)
 
 /* Forward declarations */
 
-void bch_writeback_queue(struct cached_dev *);
-void bch_writeback_add(struct cached_dev *, unsigned);
-
 void bch_count_io_errors(struct cache *, int, const char *);
 void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
 			      int, const char *);
@@ -1224,8 +1224,6 @@ void bch_cache_set_stop(struct cache_set *);
 struct cache_set *bch_cache_set_alloc(struct cache_sb *);
 void bch_btree_cache_free(struct cache_set *);
 int bch_btree_cache_alloc(struct cache_set *);
-void bch_sectors_dirty_init(struct cached_dev *);
-void bch_cached_dev_writeback_init(struct cached_dev *);
 void bch_moving_init_cache_set(struct cache_set *);
 
 int bch_cache_allocator_start(struct cache *ca);

commit 444fc0b6b167ed164e7436621a9d095e042644dd
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat May 11 17:07:26 2013 -0700

    bcache: Initialize sectors_dirty when attaching
    
    Previously, dirty_data wouldn't get initialized until the first garbage
    collection... which was a bit of a problem for background writeback (as
    the PD controller keys off of it) and also confusing for users.
    
    This is also prep work for making background writeback aware of raid5/6
    stripes.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 6fa5a1e33c49..d099d8894c2f 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -438,7 +438,6 @@ struct bcache_device {
 	atomic_t		detaching;
 
 	atomic_long_t		sectors_dirty;
-	unsigned long		sectors_dirty_gc;
 	unsigned long		sectors_dirty_last;
 	long			sectors_dirty_derivative;
 
@@ -1225,6 +1224,7 @@ void bch_cache_set_stop(struct cache_set *);
 struct cache_set *bch_cache_set_alloc(struct cache_sb *);
 void bch_btree_cache_free(struct cache_set *);
 int bch_btree_cache_alloc(struct cache_set *);
+void bch_sectors_dirty_init(struct cached_dev *);
 void bch_cached_dev_writeback_init(struct cached_dev *);
 void bch_moving_init_cache_set(struct cache_set *);
 

commit 6ded34d1a54c046a45db071d3cb7b37bd0a4a31f
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat May 11 15:59:37 2013 -0700

    bcache: Improve lazy sorting
    
    The old lazy sorting code was kind of hacky - rewrite in a way that
    mathematically makes more sense; the idea is that the size of the sets
    of keys in a btree node should increase by a more or less fixed ratio
    from smallest to biggest.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 59c15e09e4dd..6fa5a1e33c49 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -828,6 +828,7 @@ struct cache_set {
 	 */
 	struct mutex		sort_lock;
 	struct bset		*sort;
+	unsigned		sort_crit_factor;
 
 	/* List of buckets we're currently writing data to */
 	struct list_head	data_buckets;

commit c37511b863f36c1cc6e18440717fd4cc0e881b8a
Author: Kent Overstreet <koverstreet@google.com>
Date:   Fri Apr 26 15:39:55 2013 -0700

    bcache: Fix/revamp tracepoints
    
    The tracepoints were reworked to be more sensible, and fixed a null
    pointer deref in one of the tracepoints.
    
    Converted some of the pr_debug()s to tracepoints - this is partly a
    performance optimization; it used to be that with DEBUG or
    CONFIG_DYNAMIC_DEBUG pr_debug() was an empty macro; but at some point it
    was changed to an empty inline function.
    
    Some of the pr_debug() statements had rather expensive function calls as
    part of the arguments, so this code was getting run unnecessarily even
    on non debug kernels - in some fast paths, too.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index ad4957b52f10..59c15e09e4dd 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -178,7 +178,6 @@
 #define pr_fmt(fmt) "bcache: %s() " fmt "\n", __func__
 
 #include <linux/bio.h>
-#include <linux/blktrace_api.h>
 #include <linux/kobject.h>
 #include <linux/list.h>
 #include <linux/mutex.h>
@@ -901,8 +900,6 @@ static inline unsigned local_clock_us(void)
 	return local_clock() >> 10;
 }
 
-#define MAX_BSETS		4U
-
 #define BTREE_PRIO		USHRT_MAX
 #define INITIAL_PRIO		32768
 
@@ -1107,23 +1104,6 @@ static inline void __bkey_put(struct cache_set *c, struct bkey *k)
 		atomic_dec_bug(&PTR_BUCKET(c, k, i)->pin);
 }
 
-/* Blktrace macros */
-
-#define blktrace_msg(c, fmt, ...)					\
-do {									\
-	struct request_queue *q = bdev_get_queue(c->bdev);		\
-	if (q)								\
-		blk_add_trace_msg(q, fmt, ##__VA_ARGS__);		\
-} while (0)
-
-#define blktrace_msg_all(s, fmt, ...)					\
-do {									\
-	struct cache *_c;						\
-	unsigned i;							\
-	for_each_cache(_c, (s), i)					\
-		blktrace_msg(_c, fmt, ##__VA_ARGS__);			\
-} while (0)
-
 static inline void cached_dev_put(struct cached_dev *dc)
 {
 	if (atomic_dec_and_test(&dc->count))

commit 5794351146199b9ac67a5ab1beab82be8bfd7b5d
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Apr 25 13:58:35 2013 -0700

    bcache: Refactor btree io
    
    The most significant change is that btree reads are now done
    synchronously, instead of asynchronously and doing the post read stuff
    from a workqueue.
    
    This was originally done because we can't block on IO under
    generic_make_request(). But - we already have a mechanism to punt cache
    lookups to workqueue if needed, so if we just use that we don't have to
    deal with the complexity of doing things asynchronously.
    
    The main benefit is this makes the locking situation saner; we can hold
    our write lock on the btree node until we're finished reading it, and we
    don't need that btree_node_read_done() flag anymore.
    
    Also, for writes, btree_write() was broken out into btree_node_write()
    and btree_leaf_dirty() - the old code with the boolean argument was dumb
    and confusing.
    
    The prio_blocked mechanism was improved a bit too, now the only counter
    is in struct btree_write, we don't mess with transfering a count from
    struct btree anymore.
    
    This required changing garbage collection to block prios at the start
    and unblock when it finishes, which is cleaner than what it was doing
    anyways (the old code had mostly the same effect, but was doing it in a
    convoluted way)
    
    And the btree iter btree_node_read_done() uses was converted to a real
    mempool.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 166c8ddc0be4..ad4957b52f10 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -819,10 +819,9 @@ struct cache_set {
 
 	/*
 	 * A btree node on disk could have too many bsets for an iterator to fit
-	 * on the stack - this is a single element mempool for btree_read_work()
+	 * on the stack - have to dynamically allocate them
 	 */
-	struct mutex		fill_lock;
-	struct btree_iter	*fill_iter;
+	mempool_t		*fill_iter;
 
 	/*
 	 * btree_sort() is a merge sort and requires temporary space - single

commit 119ba0f82839cd80eaef3e6991988f1403965d5b
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed Apr 24 19:01:12 2013 -0700

    bcache: Convert allocator thread to kthread
    
    Using a workqueue when we just want a single thread is a bit silly.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d3e15b42a4ab..166c8ddc0be4 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -565,8 +565,7 @@ struct cache {
 
 	unsigned		watermark[WATERMARK_MAX];
 
-	struct closure		alloc;
-	struct workqueue_struct	*alloc_workqueue;
+	struct task_struct	*alloc_thread;
 
 	struct closure		prio;
 	struct prio_set		*disk_buckets;
@@ -703,9 +702,6 @@ struct cache_set {
 	/* For the btree cache */
 	struct shrinker		shrink;
 
-	/* For the allocator itself */
-	wait_queue_head_t	alloc_wait;
-
 	/* For the btree cache and anything allocation related */
 	struct mutex		bucket_lock;
 
@@ -1173,6 +1169,15 @@ static inline uint8_t bucket_disk_gen(struct bucket *b)
 	static struct kobj_attribute ksysfs_##n =			\
 		__ATTR(n, S_IWUSR|S_IRUSR, show, store)
 
+static inline void wake_up_allocators(struct cache_set *c)
+{
+	struct cache *ca;
+	unsigned i;
+
+	for_each_cache(ca, c, i)
+		wake_up_process(ca->alloc_thread);
+}
+
 /* Forward declarations */
 
 void bch_writeback_queue(struct cached_dev *);
@@ -1193,7 +1198,6 @@ void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);
 uint8_t bch_inc_gen(struct cache *, struct bucket *);
 void bch_rescale_priorities(struct cache_set *, int);
 bool bch_bucket_add_unused(struct cache *, struct bucket *);
-void bch_allocator_thread(struct closure *);
 
 long bch_bucket_alloc(struct cache *, unsigned, struct closure *);
 void bch_bucket_free(struct cache_set *, struct bkey *);
@@ -1244,6 +1248,7 @@ int bch_btree_cache_alloc(struct cache_set *);
 void bch_cached_dev_writeback_init(struct cached_dev *);
 void bch_moving_init_cache_set(struct cache_set *);
 
+int bch_cache_allocator_start(struct cache *ca);
 void bch_cache_allocator_exit(struct cache *ca);
 int bch_cache_allocator_init(struct cache *ca);
 

commit f59fce847fc8483508b5028c24e2b1e00523dd88
Author: Kent Overstreet <koverstreet@google.com>
Date:   Wed May 15 00:11:26 2013 -0700

    bcache: Fix error handling in init code
    
    This code appears to have rotted... fix various bugs and do some
    refactoring.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index 340146d7c17f..d3e15b42a4ab 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1241,7 +1241,7 @@ void bch_cache_set_stop(struct cache_set *);
 struct cache_set *bch_cache_set_alloc(struct cache_sb *);
 void bch_btree_cache_free(struct cache_set *);
 int bch_btree_cache_alloc(struct cache_set *);
-void bch_writeback_init_cached_dev(struct cached_dev *);
+void bch_cached_dev_writeback_init(struct cached_dev *);
 void bch_moving_init_cache_set(struct cache_set *);
 
 void bch_cache_allocator_exit(struct cache *ca);

commit 2903381fce71004a7ce24d40fad53ba8236a3921
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Apr 11 15:14:35 2013 -0700

    bcache: Take data offset from the bdev superblock.
    
    Add a new superblock version, and consolidate related defines.
    
    Signed-off-by: Gabriel de Perthuis <g2p.code+bcache@gmail.com>
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index f05723565f17..340146d7c17f 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -223,11 +223,17 @@ struct bkey {
 #define BKEY_PADDED(key)					\
 	union { struct bkey key; uint64_t key ## _pad[BKEY_PAD]; }
 
-/* Version 1: Backing device
+/* Version 0: Cache device
+ * Version 1: Backing device
  * Version 2: Seed pointer into btree node checksum
- * Version 3: New UUID format
+ * Version 3: Cache device with new UUID format
+ * Version 4: Backing device with data offset
  */
-#define BCACHE_SB_VERSION	3
+#define BCACHE_SB_VERSION_CDEV			0
+#define BCACHE_SB_VERSION_BDEV			1
+#define BCACHE_SB_VERSION_CDEV_WITH_UUID	3
+#define BCACHE_SB_VERSION_BDEV_WITH_OFFSET	4
+#define BCACHE_SB_MAX_VERSION			4
 
 #define SB_SECTOR		8
 #define SB_SIZE			4096
@@ -236,13 +242,12 @@ struct bkey {
 /* SB_JOURNAL_BUCKETS must be divisible by BITS_PER_LONG */
 #define MAX_CACHES_PER_SET	8
 
-#define BDEV_DATA_START		16	/* sectors */
+#define BDEV_DATA_START_DEFAULT	16	/* sectors */
 
 struct cache_sb {
 	uint64_t		csum;
 	uint64_t		offset;	/* sector where this sb was written */
 	uint64_t		version;
-#define CACHE_BACKING_DEV	1
 
 	uint8_t			magic[16];
 
@@ -257,12 +262,28 @@ struct cache_sb {
 	uint64_t		seq;
 	uint64_t		pad[8];
 
-	uint64_t		nbuckets;	/* device size */
-	uint16_t		block_size;	/* sectors */
-	uint16_t		bucket_size;	/* sectors */
+	union {
+	struct {
+		/* Cache devices */
+		uint64_t	nbuckets;	/* device size */
+
+		uint16_t	block_size;	/* sectors */
+		uint16_t	bucket_size;	/* sectors */
 
-	uint16_t		nr_in_set;
-	uint16_t		nr_this_dev;
+		uint16_t	nr_in_set;
+		uint16_t	nr_this_dev;
+	};
+	struct {
+		/* Backing devices */
+		uint64_t	data_offset;
+
+		/*
+		 * block_size from the cache device section is still used by
+		 * backing devices, so don't add anything here until we fix
+		 * things to not need it for backing devices anymore
+		 */
+	};
+	};
 
 	uint32_t		last_mount;	/* time_t */
 
@@ -861,6 +882,12 @@ static inline bool key_merging_disabled(struct cache_set *c)
 #endif
 }
 
+static inline bool SB_IS_BDEV(const struct cache_sb *sb)
+{
+	return sb->version == BCACHE_SB_VERSION_BDEV
+		|| sb->version == BCACHE_SB_VERSION_BDEV_WITH_OFFSET;
+}
+
 struct bbio {
 	unsigned		submit_time_us;
 	union {

commit 169ef1cf6171d35550fef85645b83b960e241cff
Author: Kent Overstreet <koverstreet@google.com>
Date:   Thu Mar 28 12:50:55 2013 -0600

    bcache: Don't export utility code, prefix with bch_
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index b2846e70149b..f05723565f17 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -1033,7 +1033,7 @@ static inline void bkey_init(struct bkey *k)
  * jset: The checksum is _always_ the first 8 bytes of these structs
  */
 #define csum_set(i)							\
-	crc64(((void *) (i)) + sizeof(uint64_t),			\
+	bch_crc64(((void *) (i)) + sizeof(uint64_t),			\
 	      ((void *) end(i)) - (((void *) (i)) + sizeof(uint64_t)))
 
 /* Error handling macros */

commit b1a67b0f4c747ca10c96ebb24f04e2a74b3c298d
Author: Kent Overstreet <koverstreet@google.com>
Date:   Mon Mar 25 11:46:44 2013 -0700

    bcache: Style/checkpatch fixes
    
    Took out some nested functions, and fixed some more checkpatch
    complaints.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: linux-bcache@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
index d01a553f63f3..b2846e70149b 100644
--- a/drivers/md/bcache/bcache.h
+++ b/drivers/md/bcache/bcache.h
@@ -644,8 +644,8 @@ struct gc_stat {
  * we'll continue to run normally for awhile with CACHE_SET_STOPPING set (i.e.
  * flushing dirty data).
  *
- * CACHE_SET_STOPPING_2 gets set at the last phase, when it's time to shut down the
- * allocation thread.
+ * CACHE_SET_STOPPING_2 gets set at the last phase, when it's time to shut down
+ * the allocation thread.
  */
 #define CACHE_SET_UNREGISTERING		0
 #define	CACHE_SET_STOPPING		1
@@ -1012,11 +1012,11 @@ static inline struct bucket *PTR_BUCKET(struct cache_set *c,
  * searches - it told you where a key started. It's not used anymore,
  * and can probably be safely dropped.
  */
-#define KEY(dev, sector, len)	(struct bkey)				\
-{									\
+#define KEY(dev, sector, len)						\
+((struct bkey) {							\
 	.high = (1ULL << 63) | ((uint64_t) (len) << 20) | (dev),	\
 	.low = (sector)							\
-}
+})
 
 static inline void bkey_init(struct bkey *k)
 {

commit cafe563591446cf80bfbc2fe3bc72a2e36cf1060
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat Mar 23 16:11:31 2013 -0700

    bcache: A block layer cache
    
    Does writethrough and writeback caching, handles unclean shutdown, and
    has a bunch of other nifty features motivated by real world usage.
    
    See the wiki at http://bcache.evilpiepirate.org for more.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/drivers/md/bcache/bcache.h b/drivers/md/bcache/bcache.h
new file mode 100644
index 000000000000..d01a553f63f3
--- /dev/null
+++ b/drivers/md/bcache/bcache.h
@@ -0,0 +1,1232 @@
+#ifndef _BCACHE_H
+#define _BCACHE_H
+
+/*
+ * SOME HIGH LEVEL CODE DOCUMENTATION:
+ *
+ * Bcache mostly works with cache sets, cache devices, and backing devices.
+ *
+ * Support for multiple cache devices hasn't quite been finished off yet, but
+ * it's about 95% plumbed through. A cache set and its cache devices is sort of
+ * like a md raid array and its component devices. Most of the code doesn't care
+ * about individual cache devices, the main abstraction is the cache set.
+ *
+ * Multiple cache devices is intended to give us the ability to mirror dirty
+ * cached data and metadata, without mirroring clean cached data.
+ *
+ * Backing devices are different, in that they have a lifetime independent of a
+ * cache set. When you register a newly formatted backing device it'll come up
+ * in passthrough mode, and then you can attach and detach a backing device from
+ * a cache set at runtime - while it's mounted and in use. Detaching implicitly
+ * invalidates any cached data for that backing device.
+ *
+ * A cache set can have multiple (many) backing devices attached to it.
+ *
+ * There's also flash only volumes - this is the reason for the distinction
+ * between struct cached_dev and struct bcache_device. A flash only volume
+ * works much like a bcache device that has a backing device, except the
+ * "cached" data is always dirty. The end result is that we get thin
+ * provisioning with very little additional code.
+ *
+ * Flash only volumes work but they're not production ready because the moving
+ * garbage collector needs more work. More on that later.
+ *
+ * BUCKETS/ALLOCATION:
+ *
+ * Bcache is primarily designed for caching, which means that in normal
+ * operation all of our available space will be allocated. Thus, we need an
+ * efficient way of deleting things from the cache so we can write new things to
+ * it.
+ *
+ * To do this, we first divide the cache device up into buckets. A bucket is the
+ * unit of allocation; they're typically around 1 mb - anywhere from 128k to 2M+
+ * works efficiently.
+ *
+ * Each bucket has a 16 bit priority, and an 8 bit generation associated with
+ * it. The gens and priorities for all the buckets are stored contiguously and
+ * packed on disk (in a linked list of buckets - aside from the superblock, all
+ * of bcache's metadata is stored in buckets).
+ *
+ * The priority is used to implement an LRU. We reset a bucket's priority when
+ * we allocate it or on cache it, and every so often we decrement the priority
+ * of each bucket. It could be used to implement something more sophisticated,
+ * if anyone ever gets around to it.
+ *
+ * The generation is used for invalidating buckets. Each pointer also has an 8
+ * bit generation embedded in it; for a pointer to be considered valid, its gen
+ * must match the gen of the bucket it points into.  Thus, to reuse a bucket all
+ * we have to do is increment its gen (and write its new gen to disk; we batch
+ * this up).
+ *
+ * Bcache is entirely COW - we never write twice to a bucket, even buckets that
+ * contain metadata (including btree nodes).
+ *
+ * THE BTREE:
+ *
+ * Bcache is in large part design around the btree.
+ *
+ * At a high level, the btree is just an index of key -> ptr tuples.
+ *
+ * Keys represent extents, and thus have a size field. Keys also have a variable
+ * number of pointers attached to them (potentially zero, which is handy for
+ * invalidating the cache).
+ *
+ * The key itself is an inode:offset pair. The inode number corresponds to a
+ * backing device or a flash only volume. The offset is the ending offset of the
+ * extent within the inode - not the starting offset; this makes lookups
+ * slightly more convenient.
+ *
+ * Pointers contain the cache device id, the offset on that device, and an 8 bit
+ * generation number. More on the gen later.
+ *
+ * Index lookups are not fully abstracted - cache lookups in particular are
+ * still somewhat mixed in with the btree code, but things are headed in that
+ * direction.
+ *
+ * Updates are fairly well abstracted, though. There are two different ways of
+ * updating the btree; insert and replace.
+ *
+ * BTREE_INSERT will just take a list of keys and insert them into the btree -
+ * overwriting (possibly only partially) any extents they overlap with. This is
+ * used to update the index after a write.
+ *
+ * BTREE_REPLACE is really cmpxchg(); it inserts a key into the btree iff it is
+ * overwriting a key that matches another given key. This is used for inserting
+ * data into the cache after a cache miss, and for background writeback, and for
+ * the moving garbage collector.
+ *
+ * There is no "delete" operation; deleting things from the index is
+ * accomplished by either by invalidating pointers (by incrementing a bucket's
+ * gen) or by inserting a key with 0 pointers - which will overwrite anything
+ * previously present at that location in the index.
+ *
+ * This means that there are always stale/invalid keys in the btree. They're
+ * filtered out by the code that iterates through a btree node, and removed when
+ * a btree node is rewritten.
+ *
+ * BTREE NODES:
+ *
+ * Our unit of allocation is a bucket, and we we can't arbitrarily allocate and
+ * free smaller than a bucket - so, that's how big our btree nodes are.
+ *
+ * (If buckets are really big we'll only use part of the bucket for a btree node
+ * - no less than 1/4th - but a bucket still contains no more than a single
+ * btree node. I'd actually like to change this, but for now we rely on the
+ * bucket's gen for deleting btree nodes when we rewrite/split a node.)
+ *
+ * Anyways, btree nodes are big - big enough to be inefficient with a textbook
+ * btree implementation.
+ *
+ * The way this is solved is that btree nodes are internally log structured; we
+ * can append new keys to an existing btree node without rewriting it. This
+ * means each set of keys we write is sorted, but the node is not.
+ *
+ * We maintain this log structure in memory - keeping 1Mb of keys sorted would
+ * be expensive, and we have to distinguish between the keys we have written and
+ * the keys we haven't. So to do a lookup in a btree node, we have to search
+ * each sorted set. But we do merge written sets together lazily, so the cost of
+ * these extra searches is quite low (normally most of the keys in a btree node
+ * will be in one big set, and then there'll be one or two sets that are much
+ * smaller).
+ *
+ * This log structure makes bcache's btree more of a hybrid between a
+ * conventional btree and a compacting data structure, with some of the
+ * advantages of both.
+ *
+ * GARBAGE COLLECTION:
+ *
+ * We can't just invalidate any bucket - it might contain dirty data or
+ * metadata. If it once contained dirty data, other writes might overwrite it
+ * later, leaving no valid pointers into that bucket in the index.
+ *
+ * Thus, the primary purpose of garbage collection is to find buckets to reuse.
+ * It also counts how much valid data it each bucket currently contains, so that
+ * allocation can reuse buckets sooner when they've been mostly overwritten.
+ *
+ * It also does some things that are really internal to the btree
+ * implementation. If a btree node contains pointers that are stale by more than
+ * some threshold, it rewrites the btree node to avoid the bucket's generation
+ * wrapping around. It also merges adjacent btree nodes if they're empty enough.
+ *
+ * THE JOURNAL:
+ *
+ * Bcache's journal is not necessary for consistency; we always strictly
+ * order metadata writes so that the btree and everything else is consistent on
+ * disk in the event of an unclean shutdown, and in fact bcache had writeback
+ * caching (with recovery from unclean shutdown) before journalling was
+ * implemented.
+ *
+ * Rather, the journal is purely a performance optimization; we can't complete a
+ * write until we've updated the index on disk, otherwise the cache would be
+ * inconsistent in the event of an unclean shutdown. This means that without the
+ * journal, on random write workloads we constantly have to update all the leaf
+ * nodes in the btree, and those writes will be mostly empty (appending at most
+ * a few keys each) - highly inefficient in terms of amount of metadata writes,
+ * and it puts more strain on the various btree resorting/compacting code.
+ *
+ * The journal is just a log of keys we've inserted; on startup we just reinsert
+ * all the keys in the open journal entries. That means that when we're updating
+ * a node in the btree, we can wait until a 4k block of keys fills up before
+ * writing them out.
+ *
+ * For simplicity, we only journal updates to leaf nodes; updates to parent
+ * nodes are rare enough (since our leaf nodes are huge) that it wasn't worth
+ * the complexity to deal with journalling them (in particular, journal replay)
+ * - updates to non leaf nodes just happen synchronously (see btree_split()).
+ */
+
+#define pr_fmt(fmt) "bcache: %s() " fmt "\n", __func__
+
+#include <linux/bio.h>
+#include <linux/blktrace_api.h>
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/rbtree.h>
+#include <linux/rwsem.h>
+#include <linux/types.h>
+#include <linux/workqueue.h>
+
+#include "util.h"
+#include "closure.h"
+
+struct bucket {
+	atomic_t	pin;
+	uint16_t	prio;
+	uint8_t		gen;
+	uint8_t		disk_gen;
+	uint8_t		last_gc; /* Most out of date gen in the btree */
+	uint8_t		gc_gen;
+	uint16_t	gc_mark;
+};
+
+/*
+ * I'd use bitfields for these, but I don't trust the compiler not to screw me
+ * as multiple threads touch struct bucket without locking
+ */
+
+BITMASK(GC_MARK,	 struct bucket, gc_mark, 0, 2);
+#define GC_MARK_RECLAIMABLE	0
+#define GC_MARK_DIRTY		1
+#define GC_MARK_METADATA	2
+BITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, 14);
+
+struct bkey {
+	uint64_t	high;
+	uint64_t	low;
+	uint64_t	ptr[];
+};
+
+/* Enough for a key with 6 pointers */
+#define BKEY_PAD		8
+
+#define BKEY_PADDED(key)					\
+	union { struct bkey key; uint64_t key ## _pad[BKEY_PAD]; }
+
+/* Version 1: Backing device
+ * Version 2: Seed pointer into btree node checksum
+ * Version 3: New UUID format
+ */
+#define BCACHE_SB_VERSION	3
+
+#define SB_SECTOR		8
+#define SB_SIZE			4096
+#define SB_LABEL_SIZE		32
+#define SB_JOURNAL_BUCKETS	256U
+/* SB_JOURNAL_BUCKETS must be divisible by BITS_PER_LONG */
+#define MAX_CACHES_PER_SET	8
+
+#define BDEV_DATA_START		16	/* sectors */
+
+struct cache_sb {
+	uint64_t		csum;
+	uint64_t		offset;	/* sector where this sb was written */
+	uint64_t		version;
+#define CACHE_BACKING_DEV	1
+
+	uint8_t			magic[16];
+
+	uint8_t			uuid[16];
+	union {
+		uint8_t		set_uuid[16];
+		uint64_t	set_magic;
+	};
+	uint8_t			label[SB_LABEL_SIZE];
+
+	uint64_t		flags;
+	uint64_t		seq;
+	uint64_t		pad[8];
+
+	uint64_t		nbuckets;	/* device size */
+	uint16_t		block_size;	/* sectors */
+	uint16_t		bucket_size;	/* sectors */
+
+	uint16_t		nr_in_set;
+	uint16_t		nr_this_dev;
+
+	uint32_t		last_mount;	/* time_t */
+
+	uint16_t		first_bucket;
+	union {
+		uint16_t	njournal_buckets;
+		uint16_t	keys;
+	};
+	uint64_t		d[SB_JOURNAL_BUCKETS];	/* journal buckets */
+};
+
+BITMASK(CACHE_SYNC,		struct cache_sb, flags, 0, 1);
+BITMASK(CACHE_DISCARD,		struct cache_sb, flags, 1, 1);
+BITMASK(CACHE_REPLACEMENT,	struct cache_sb, flags, 2, 3);
+#define CACHE_REPLACEMENT_LRU	0U
+#define CACHE_REPLACEMENT_FIFO	1U
+#define CACHE_REPLACEMENT_RANDOM 2U
+
+BITMASK(BDEV_CACHE_MODE,	struct cache_sb, flags, 0, 4);
+#define CACHE_MODE_WRITETHROUGH	0U
+#define CACHE_MODE_WRITEBACK	1U
+#define CACHE_MODE_WRITEAROUND	2U
+#define CACHE_MODE_NONE		3U
+BITMASK(BDEV_STATE,		struct cache_sb, flags, 61, 2);
+#define BDEV_STATE_NONE		0U
+#define BDEV_STATE_CLEAN	1U
+#define BDEV_STATE_DIRTY	2U
+#define BDEV_STATE_STALE	3U
+
+/* Version 1: Seed pointer into btree node checksum
+ */
+#define BCACHE_BSET_VERSION	1
+
+/*
+ * This is the on disk format for btree nodes - a btree node on disk is a list
+ * of these; within each set the keys are sorted
+ */
+struct bset {
+	uint64_t		csum;
+	uint64_t		magic;
+	uint64_t		seq;
+	uint32_t		version;
+	uint32_t		keys;
+
+	union {
+		struct bkey	start[0];
+		uint64_t	d[0];
+	};
+};
+
+/*
+ * On disk format for priorities and gens - see super.c near prio_write() for
+ * more.
+ */
+struct prio_set {
+	uint64_t		csum;
+	uint64_t		magic;
+	uint64_t		seq;
+	uint32_t		version;
+	uint32_t		pad;
+
+	uint64_t		next_bucket;
+
+	struct bucket_disk {
+		uint16_t	prio;
+		uint8_t		gen;
+	} __attribute((packed)) data[];
+};
+
+struct uuid_entry {
+	union {
+		struct {
+			uint8_t		uuid[16];
+			uint8_t		label[32];
+			uint32_t	first_reg;
+			uint32_t	last_reg;
+			uint32_t	invalidated;
+
+			uint32_t	flags;
+			/* Size of flash only volumes */
+			uint64_t	sectors;
+		};
+
+		uint8_t	pad[128];
+	};
+};
+
+BITMASK(UUID_FLASH_ONLY,	struct uuid_entry, flags, 0, 1);
+
+#include "journal.h"
+#include "stats.h"
+struct search;
+struct btree;
+struct keybuf;
+
+struct keybuf_key {
+	struct rb_node		node;
+	BKEY_PADDED(key);
+	void			*private;
+};
+
+typedef bool (keybuf_pred_fn)(struct keybuf *, struct bkey *);
+
+struct keybuf {
+	keybuf_pred_fn		*key_predicate;
+
+	struct bkey		last_scanned;
+	spinlock_t		lock;
+
+	/*
+	 * Beginning and end of range in rb tree - so that we can skip taking
+	 * lock and checking the rb tree when we need to check for overlapping
+	 * keys.
+	 */
+	struct bkey		start;
+	struct bkey		end;
+
+	struct rb_root		keys;
+
+#define KEYBUF_NR		100
+	DECLARE_ARRAY_ALLOCATOR(struct keybuf_key, freelist, KEYBUF_NR);
+};
+
+struct bio_split_pool {
+	struct bio_set		*bio_split;
+	mempool_t		*bio_split_hook;
+};
+
+struct bio_split_hook {
+	struct closure		cl;
+	struct bio_split_pool	*p;
+	struct bio		*bio;
+	bio_end_io_t		*bi_end_io;
+	void			*bi_private;
+};
+
+struct bcache_device {
+	struct closure		cl;
+
+	struct kobject		kobj;
+
+	struct cache_set	*c;
+	unsigned		id;
+#define BCACHEDEVNAME_SIZE	12
+	char			name[BCACHEDEVNAME_SIZE];
+
+	struct gendisk		*disk;
+
+	/* If nonzero, we're closing */
+	atomic_t		closing;
+
+	/* If nonzero, we're detaching/unregistering from cache set */
+	atomic_t		detaching;
+
+	atomic_long_t		sectors_dirty;
+	unsigned long		sectors_dirty_gc;
+	unsigned long		sectors_dirty_last;
+	long			sectors_dirty_derivative;
+
+	mempool_t		*unaligned_bvec;
+	struct bio_set		*bio_split;
+
+	unsigned		data_csum:1;
+
+	int (*cache_miss)(struct btree *, struct search *,
+			  struct bio *, unsigned);
+	int (*ioctl) (struct bcache_device *, fmode_t, unsigned, unsigned long);
+
+	struct bio_split_pool	bio_split_hook;
+};
+
+struct io {
+	/* Used to track sequential IO so it can be skipped */
+	struct hlist_node	hash;
+	struct list_head	lru;
+
+	unsigned long		jiffies;
+	unsigned		sequential;
+	sector_t		last;
+};
+
+struct cached_dev {
+	struct list_head	list;
+	struct bcache_device	disk;
+	struct block_device	*bdev;
+
+	struct cache_sb		sb;
+	struct bio		sb_bio;
+	struct bio_vec		sb_bv[1];
+	struct closure_with_waitlist sb_write;
+
+	/* Refcount on the cache set. Always nonzero when we're caching. */
+	atomic_t		count;
+	struct work_struct	detach;
+
+	/*
+	 * Device might not be running if it's dirty and the cache set hasn't
+	 * showed up yet.
+	 */
+	atomic_t		running;
+
+	/*
+	 * Writes take a shared lock from start to finish; scanning for dirty
+	 * data to refill the rb tree requires an exclusive lock.
+	 */
+	struct rw_semaphore	writeback_lock;
+
+	/*
+	 * Nonzero, and writeback has a refcount (d->count), iff there is dirty
+	 * data in the cache. Protected by writeback_lock; must have an
+	 * shared lock to set and exclusive lock to clear.
+	 */
+	atomic_t		has_dirty;
+
+	struct ratelimit	writeback_rate;
+	struct delayed_work	writeback_rate_update;
+
+	/*
+	 * Internal to the writeback code, so read_dirty() can keep track of
+	 * where it's at.
+	 */
+	sector_t		last_read;
+
+	/* Number of writeback bios in flight */
+	atomic_t		in_flight;
+	struct closure_with_timer writeback;
+	struct closure_waitlist	writeback_wait;
+
+	struct keybuf		writeback_keys;
+
+	/* For tracking sequential IO */
+#define RECENT_IO_BITS	7
+#define RECENT_IO	(1 << RECENT_IO_BITS)
+	struct io		io[RECENT_IO];
+	struct hlist_head	io_hash[RECENT_IO + 1];
+	struct list_head	io_lru;
+	spinlock_t		io_lock;
+
+	struct cache_accounting	accounting;
+
+	/* The rest of this all shows up in sysfs */
+	unsigned		sequential_cutoff;
+	unsigned		readahead;
+
+	unsigned		sequential_merge:1;
+	unsigned		verify:1;
+
+	unsigned		writeback_metadata:1;
+	unsigned		writeback_running:1;
+	unsigned char		writeback_percent;
+	unsigned		writeback_delay;
+
+	int			writeback_rate_change;
+	int64_t			writeback_rate_derivative;
+	uint64_t		writeback_rate_target;
+
+	unsigned		writeback_rate_update_seconds;
+	unsigned		writeback_rate_d_term;
+	unsigned		writeback_rate_p_term_inverse;
+	unsigned		writeback_rate_d_smooth;
+};
+
+enum alloc_watermarks {
+	WATERMARK_PRIO,
+	WATERMARK_METADATA,
+	WATERMARK_MOVINGGC,
+	WATERMARK_NONE,
+	WATERMARK_MAX
+};
+
+struct cache {
+	struct cache_set	*set;
+	struct cache_sb		sb;
+	struct bio		sb_bio;
+	struct bio_vec		sb_bv[1];
+
+	struct kobject		kobj;
+	struct block_device	*bdev;
+
+	unsigned		watermark[WATERMARK_MAX];
+
+	struct closure		alloc;
+	struct workqueue_struct	*alloc_workqueue;
+
+	struct closure		prio;
+	struct prio_set		*disk_buckets;
+
+	/*
+	 * When allocating new buckets, prio_write() gets first dibs - since we
+	 * may not be allocate at all without writing priorities and gens.
+	 * prio_buckets[] contains the last buckets we wrote priorities to (so
+	 * gc can mark them as metadata), prio_next[] contains the buckets
+	 * allocated for the next prio write.
+	 */
+	uint64_t		*prio_buckets;
+	uint64_t		*prio_last_buckets;
+
+	/*
+	 * free: Buckets that are ready to be used
+	 *
+	 * free_inc: Incoming buckets - these are buckets that currently have
+	 * cached data in them, and we can't reuse them until after we write
+	 * their new gen to disk. After prio_write() finishes writing the new
+	 * gens/prios, they'll be moved to the free list (and possibly discarded
+	 * in the process)
+	 *
+	 * unused: GC found nothing pointing into these buckets (possibly
+	 * because all the data they contained was overwritten), so we only
+	 * need to discard them before they can be moved to the free list.
+	 */
+	DECLARE_FIFO(long, free);
+	DECLARE_FIFO(long, free_inc);
+	DECLARE_FIFO(long, unused);
+
+	size_t			fifo_last_bucket;
+
+	/* Allocation stuff: */
+	struct bucket		*buckets;
+
+	DECLARE_HEAP(struct bucket *, heap);
+
+	/*
+	 * max(gen - disk_gen) for all buckets. When it gets too big we have to
+	 * call prio_write() to keep gens from wrapping.
+	 */
+	uint8_t			need_save_prio;
+	unsigned		gc_move_threshold;
+
+	/*
+	 * If nonzero, we know we aren't going to find any buckets to invalidate
+	 * until a gc finishes - otherwise we could pointlessly burn a ton of
+	 * cpu
+	 */
+	unsigned		invalidate_needs_gc:1;
+
+	bool			discard; /* Get rid of? */
+
+	/*
+	 * We preallocate structs for issuing discards to buckets, and keep them
+	 * on this list when they're not in use; do_discard() issues discards
+	 * whenever there's work to do and is called by free_some_buckets() and
+	 * when a discard finishes.
+	 */
+	atomic_t		discards_in_flight;
+	struct list_head	discards;
+
+	struct journal_device	journal;
+
+	/* The rest of this all shows up in sysfs */
+#define IO_ERROR_SHIFT		20
+	atomic_t		io_errors;
+	atomic_t		io_count;
+
+	atomic_long_t		meta_sectors_written;
+	atomic_long_t		btree_sectors_written;
+	atomic_long_t		sectors_written;
+
+	struct bio_split_pool	bio_split_hook;
+};
+
+struct gc_stat {
+	size_t			nodes;
+	size_t			key_bytes;
+
+	size_t			nkeys;
+	uint64_t		data;	/* sectors */
+	uint64_t		dirty;	/* sectors */
+	unsigned		in_use; /* percent */
+};
+
+/*
+ * Flag bits, for how the cache set is shutting down, and what phase it's at:
+ *
+ * CACHE_SET_UNREGISTERING means we're not just shutting down, we're detaching
+ * all the backing devices first (their cached data gets invalidated, and they
+ * won't automatically reattach).
+ *
+ * CACHE_SET_STOPPING always gets set first when we're closing down a cache set;
+ * we'll continue to run normally for awhile with CACHE_SET_STOPPING set (i.e.
+ * flushing dirty data).
+ *
+ * CACHE_SET_STOPPING_2 gets set at the last phase, when it's time to shut down the
+ * allocation thread.
+ */
+#define CACHE_SET_UNREGISTERING		0
+#define	CACHE_SET_STOPPING		1
+#define	CACHE_SET_STOPPING_2		2
+
+struct cache_set {
+	struct closure		cl;
+
+	struct list_head	list;
+	struct kobject		kobj;
+	struct kobject		internal;
+	struct dentry		*debug;
+	struct cache_accounting accounting;
+
+	unsigned long		flags;
+
+	struct cache_sb		sb;
+
+	struct cache		*cache[MAX_CACHES_PER_SET];
+	struct cache		*cache_by_alloc[MAX_CACHES_PER_SET];
+	int			caches_loaded;
+
+	struct bcache_device	**devices;
+	struct list_head	cached_devs;
+	uint64_t		cached_dev_sectors;
+	struct closure		caching;
+
+	struct closure_with_waitlist sb_write;
+
+	mempool_t		*search;
+	mempool_t		*bio_meta;
+	struct bio_set		*bio_split;
+
+	/* For the btree cache */
+	struct shrinker		shrink;
+
+	/* For the allocator itself */
+	wait_queue_head_t	alloc_wait;
+
+	/* For the btree cache and anything allocation related */
+	struct mutex		bucket_lock;
+
+	/* log2(bucket_size), in sectors */
+	unsigned short		bucket_bits;
+
+	/* log2(block_size), in sectors */
+	unsigned short		block_bits;
+
+	/*
+	 * Default number of pages for a new btree node - may be less than a
+	 * full bucket
+	 */
+	unsigned		btree_pages;
+
+	/*
+	 * Lists of struct btrees; lru is the list for structs that have memory
+	 * allocated for actual btree node, freed is for structs that do not.
+	 *
+	 * We never free a struct btree, except on shutdown - we just put it on
+	 * the btree_cache_freed list and reuse it later. This simplifies the
+	 * code, and it doesn't cost us much memory as the memory usage is
+	 * dominated by buffers that hold the actual btree node data and those
+	 * can be freed - and the number of struct btrees allocated is
+	 * effectively bounded.
+	 *
+	 * btree_cache_freeable effectively is a small cache - we use it because
+	 * high order page allocations can be rather expensive, and it's quite
+	 * common to delete and allocate btree nodes in quick succession. It
+	 * should never grow past ~2-3 nodes in practice.
+	 */
+	struct list_head	btree_cache;
+	struct list_head	btree_cache_freeable;
+	struct list_head	btree_cache_freed;
+
+	/* Number of elements in btree_cache + btree_cache_freeable lists */
+	unsigned		bucket_cache_used;
+
+	/*
+	 * If we need to allocate memory for a new btree node and that
+	 * allocation fails, we can cannibalize another node in the btree cache
+	 * to satisfy the allocation. However, only one thread can be doing this
+	 * at a time, for obvious reasons - try_harder and try_wait are
+	 * basically a lock for this that we can wait on asynchronously. The
+	 * btree_root() macro releases the lock when it returns.
+	 */
+	struct closure		*try_harder;
+	struct closure_waitlist	try_wait;
+	uint64_t		try_harder_start;
+
+	/*
+	 * When we free a btree node, we increment the gen of the bucket the
+	 * node is in - but we can't rewrite the prios and gens until we
+	 * finished whatever it is we were doing, otherwise after a crash the
+	 * btree node would be freed but for say a split, we might not have the
+	 * pointers to the new nodes inserted into the btree yet.
+	 *
+	 * This is a refcount that blocks prio_write() until the new keys are
+	 * written.
+	 */
+	atomic_t		prio_blocked;
+	struct closure_waitlist	bucket_wait;
+
+	/*
+	 * For any bio we don't skip we subtract the number of sectors from
+	 * rescale; when it hits 0 we rescale all the bucket priorities.
+	 */
+	atomic_t		rescale;
+	/*
+	 * When we invalidate buckets, we use both the priority and the amount
+	 * of good data to determine which buckets to reuse first - to weight
+	 * those together consistently we keep track of the smallest nonzero
+	 * priority of any bucket.
+	 */
+	uint16_t		min_prio;
+
+	/*
+	 * max(gen - gc_gen) for all buckets. When it gets too big we have to gc
+	 * to keep gens from wrapping around.
+	 */
+	uint8_t			need_gc;
+	struct gc_stat		gc_stats;
+	size_t			nbuckets;
+
+	struct closure_with_waitlist gc;
+	/* Where in the btree gc currently is */
+	struct bkey		gc_done;
+
+	/*
+	 * The allocation code needs gc_mark in struct bucket to be correct, but
+	 * it's not while a gc is in progress. Protected by bucket_lock.
+	 */
+	int			gc_mark_valid;
+
+	/* Counts how many sectors bio_insert has added to the cache */
+	atomic_t		sectors_to_gc;
+
+	struct closure		moving_gc;
+	struct closure_waitlist	moving_gc_wait;
+	struct keybuf		moving_gc_keys;
+	/* Number of moving GC bios in flight */
+	atomic_t		in_flight;
+
+	struct btree		*root;
+
+#ifdef CONFIG_BCACHE_DEBUG
+	struct btree		*verify_data;
+	struct mutex		verify_lock;
+#endif
+
+	unsigned		nr_uuids;
+	struct uuid_entry	*uuids;
+	BKEY_PADDED(uuid_bucket);
+	struct closure_with_waitlist uuid_write;
+
+	/*
+	 * A btree node on disk could have too many bsets for an iterator to fit
+	 * on the stack - this is a single element mempool for btree_read_work()
+	 */
+	struct mutex		fill_lock;
+	struct btree_iter	*fill_iter;
+
+	/*
+	 * btree_sort() is a merge sort and requires temporary space - single
+	 * element mempool
+	 */
+	struct mutex		sort_lock;
+	struct bset		*sort;
+
+	/* List of buckets we're currently writing data to */
+	struct list_head	data_buckets;
+	spinlock_t		data_bucket_lock;
+
+	struct journal		journal;
+
+#define CONGESTED_MAX		1024
+	unsigned		congested_last_us;
+	atomic_t		congested;
+
+	/* The rest of this all shows up in sysfs */
+	unsigned		congested_read_threshold_us;
+	unsigned		congested_write_threshold_us;
+
+	spinlock_t		sort_time_lock;
+	struct time_stats	sort_time;
+	struct time_stats	btree_gc_time;
+	struct time_stats	btree_split_time;
+	spinlock_t		btree_read_time_lock;
+	struct time_stats	btree_read_time;
+	struct time_stats	try_harder_time;
+
+	atomic_long_t		cache_read_races;
+	atomic_long_t		writeback_keys_done;
+	atomic_long_t		writeback_keys_failed;
+	unsigned		error_limit;
+	unsigned		error_decay;
+	unsigned short		journal_delay_ms;
+	unsigned		verify:1;
+	unsigned		key_merging_disabled:1;
+	unsigned		gc_always_rewrite:1;
+	unsigned		shrinker_disabled:1;
+	unsigned		copy_gc_enabled:1;
+
+#define BUCKET_HASH_BITS	12
+	struct hlist_head	bucket_hash[1 << BUCKET_HASH_BITS];
+};
+
+static inline bool key_merging_disabled(struct cache_set *c)
+{
+#ifdef CONFIG_BCACHE_DEBUG
+	return c->key_merging_disabled;
+#else
+	return 0;
+#endif
+}
+
+struct bbio {
+	unsigned		submit_time_us;
+	union {
+		struct bkey	key;
+		uint64_t	_pad[3];
+		/*
+		 * We only need pad = 3 here because we only ever carry around a
+		 * single pointer - i.e. the pointer we're doing io to/from.
+		 */
+	};
+	struct bio		bio;
+};
+
+static inline unsigned local_clock_us(void)
+{
+	return local_clock() >> 10;
+}
+
+#define MAX_BSETS		4U
+
+#define BTREE_PRIO		USHRT_MAX
+#define INITIAL_PRIO		32768
+
+#define btree_bytes(c)		((c)->btree_pages * PAGE_SIZE)
+#define btree_blocks(b)							\
+	((unsigned) (KEY_SIZE(&b->key) >> (b)->c->block_bits))
+
+#define btree_default_blocks(c)						\
+	((unsigned) ((PAGE_SECTORS * (c)->btree_pages) >> (c)->block_bits))
+
+#define bucket_pages(c)		((c)->sb.bucket_size / PAGE_SECTORS)
+#define bucket_bytes(c)		((c)->sb.bucket_size << 9)
+#define block_bytes(c)		((c)->sb.block_size << 9)
+
+#define __set_bytes(i, k)	(sizeof(*(i)) + (k) * sizeof(uint64_t))
+#define set_bytes(i)		__set_bytes(i, i->keys)
+
+#define __set_blocks(i, k, c)	DIV_ROUND_UP(__set_bytes(i, k), block_bytes(c))
+#define set_blocks(i, c)	__set_blocks(i, (i)->keys, c)
+
+#define node(i, j)		((struct bkey *) ((i)->d + (j)))
+#define end(i)			node(i, (i)->keys)
+
+#define index(i, b)							\
+	((size_t) (((void *) i - (void *) (b)->sets[0].data) /		\
+		   block_bytes(b->c)))
+
+#define btree_data_space(b)	(PAGE_SIZE << (b)->page_order)
+
+#define prios_per_bucket(c)				\
+	((bucket_bytes(c) - sizeof(struct prio_set)) /	\
+	 sizeof(struct bucket_disk))
+#define prio_buckets(c)					\
+	DIV_ROUND_UP((size_t) (c)->sb.nbuckets, prios_per_bucket(c))
+
+#define JSET_MAGIC		0x245235c1a3625032ULL
+#define PSET_MAGIC		0x6750e15f87337f91ULL
+#define BSET_MAGIC		0x90135c78b99e07f5ULL
+
+#define jset_magic(c)		((c)->sb.set_magic ^ JSET_MAGIC)
+#define pset_magic(c)		((c)->sb.set_magic ^ PSET_MAGIC)
+#define bset_magic(c)		((c)->sb.set_magic ^ BSET_MAGIC)
+
+/* Bkey fields: all units are in sectors */
+
+#define KEY_FIELD(name, field, offset, size)				\
+	BITMASK(name, struct bkey, field, offset, size)
+
+#define PTR_FIELD(name, offset, size)					\
+	static inline uint64_t name(const struct bkey *k, unsigned i)	\
+	{ return (k->ptr[i] >> offset) & ~(((uint64_t) ~0) << size); }	\
+									\
+	static inline void SET_##name(struct bkey *k, unsigned i, uint64_t v)\
+	{								\
+		k->ptr[i] &= ~(~((uint64_t) ~0 << size) << offset);	\
+		k->ptr[i] |= v << offset;				\
+	}
+
+KEY_FIELD(KEY_PTRS,	high, 60, 3)
+KEY_FIELD(HEADER_SIZE,	high, 58, 2)
+KEY_FIELD(KEY_CSUM,	high, 56, 2)
+KEY_FIELD(KEY_PINNED,	high, 55, 1)
+KEY_FIELD(KEY_DIRTY,	high, 36, 1)
+
+KEY_FIELD(KEY_SIZE,	high, 20, 16)
+KEY_FIELD(KEY_INODE,	high, 0,  20)
+
+/* Next time I change the on disk format, KEY_OFFSET() won't be 64 bits */
+
+static inline uint64_t KEY_OFFSET(const struct bkey *k)
+{
+	return k->low;
+}
+
+static inline void SET_KEY_OFFSET(struct bkey *k, uint64_t v)
+{
+	k->low = v;
+}
+
+PTR_FIELD(PTR_DEV,		51, 12)
+PTR_FIELD(PTR_OFFSET,		8,  43)
+PTR_FIELD(PTR_GEN,		0,  8)
+
+#define PTR_CHECK_DEV		((1 << 12) - 1)
+
+#define PTR(gen, offset, dev)						\
+	((((uint64_t) dev) << 51) | ((uint64_t) offset) << 8 | gen)
+
+static inline size_t sector_to_bucket(struct cache_set *c, sector_t s)
+{
+	return s >> c->bucket_bits;
+}
+
+static inline sector_t bucket_to_sector(struct cache_set *c, size_t b)
+{
+	return ((sector_t) b) << c->bucket_bits;
+}
+
+static inline sector_t bucket_remainder(struct cache_set *c, sector_t s)
+{
+	return s & (c->sb.bucket_size - 1);
+}
+
+static inline struct cache *PTR_CACHE(struct cache_set *c,
+				      const struct bkey *k,
+				      unsigned ptr)
+{
+	return c->cache[PTR_DEV(k, ptr)];
+}
+
+static inline size_t PTR_BUCKET_NR(struct cache_set *c,
+				   const struct bkey *k,
+				   unsigned ptr)
+{
+	return sector_to_bucket(c, PTR_OFFSET(k, ptr));
+}
+
+static inline struct bucket *PTR_BUCKET(struct cache_set *c,
+					const struct bkey *k,
+					unsigned ptr)
+{
+	return PTR_CACHE(c, k, ptr)->buckets + PTR_BUCKET_NR(c, k, ptr);
+}
+
+/* Btree key macros */
+
+/*
+ * The high bit being set is a relic from when we used it to do binary
+ * searches - it told you where a key started. It's not used anymore,
+ * and can probably be safely dropped.
+ */
+#define KEY(dev, sector, len)	(struct bkey)				\
+{									\
+	.high = (1ULL << 63) | ((uint64_t) (len) << 20) | (dev),	\
+	.low = (sector)							\
+}
+
+static inline void bkey_init(struct bkey *k)
+{
+	*k = KEY(0, 0, 0);
+}
+
+#define KEY_START(k)		(KEY_OFFSET(k) - KEY_SIZE(k))
+#define START_KEY(k)		KEY(KEY_INODE(k), KEY_START(k), 0)
+#define MAX_KEY			KEY(~(~0 << 20), ((uint64_t) ~0) >> 1, 0)
+#define ZERO_KEY		KEY(0, 0, 0)
+
+/*
+ * This is used for various on disk data structures - cache_sb, prio_set, bset,
+ * jset: The checksum is _always_ the first 8 bytes of these structs
+ */
+#define csum_set(i)							\
+	crc64(((void *) (i)) + sizeof(uint64_t),			\
+	      ((void *) end(i)) - (((void *) (i)) + sizeof(uint64_t)))
+
+/* Error handling macros */
+
+#define btree_bug(b, ...)						\
+do {									\
+	if (bch_cache_set_error((b)->c, __VA_ARGS__))			\
+		dump_stack();						\
+} while (0)
+
+#define cache_bug(c, ...)						\
+do {									\
+	if (bch_cache_set_error(c, __VA_ARGS__))			\
+		dump_stack();						\
+} while (0)
+
+#define btree_bug_on(cond, b, ...)					\
+do {									\
+	if (cond)							\
+		btree_bug(b, __VA_ARGS__);				\
+} while (0)
+
+#define cache_bug_on(cond, c, ...)					\
+do {									\
+	if (cond)							\
+		cache_bug(c, __VA_ARGS__);				\
+} while (0)
+
+#define cache_set_err_on(cond, c, ...)					\
+do {									\
+	if (cond)							\
+		bch_cache_set_error(c, __VA_ARGS__);			\
+} while (0)
+
+/* Looping macros */
+
+#define for_each_cache(ca, cs, iter)					\
+	for (iter = 0; ca = cs->cache[iter], iter < (cs)->sb.nr_in_set; iter++)
+
+#define for_each_bucket(b, ca)						\
+	for (b = (ca)->buckets + (ca)->sb.first_bucket;			\
+	     b < (ca)->buckets + (ca)->sb.nbuckets; b++)
+
+static inline void __bkey_put(struct cache_set *c, struct bkey *k)
+{
+	unsigned i;
+
+	for (i = 0; i < KEY_PTRS(k); i++)
+		atomic_dec_bug(&PTR_BUCKET(c, k, i)->pin);
+}
+
+/* Blktrace macros */
+
+#define blktrace_msg(c, fmt, ...)					\
+do {									\
+	struct request_queue *q = bdev_get_queue(c->bdev);		\
+	if (q)								\
+		blk_add_trace_msg(q, fmt, ##__VA_ARGS__);		\
+} while (0)
+
+#define blktrace_msg_all(s, fmt, ...)					\
+do {									\
+	struct cache *_c;						\
+	unsigned i;							\
+	for_each_cache(_c, (s), i)					\
+		blktrace_msg(_c, fmt, ##__VA_ARGS__);			\
+} while (0)
+
+static inline void cached_dev_put(struct cached_dev *dc)
+{
+	if (atomic_dec_and_test(&dc->count))
+		schedule_work(&dc->detach);
+}
+
+static inline bool cached_dev_get(struct cached_dev *dc)
+{
+	if (!atomic_inc_not_zero(&dc->count))
+		return false;
+
+	/* Paired with the mb in cached_dev_attach */
+	smp_mb__after_atomic_inc();
+	return true;
+}
+
+/*
+ * bucket_gc_gen() returns the difference between the bucket's current gen and
+ * the oldest gen of any pointer into that bucket in the btree (last_gc).
+ *
+ * bucket_disk_gen() returns the difference between the current gen and the gen
+ * on disk; they're both used to make sure gens don't wrap around.
+ */
+
+static inline uint8_t bucket_gc_gen(struct bucket *b)
+{
+	return b->gen - b->last_gc;
+}
+
+static inline uint8_t bucket_disk_gen(struct bucket *b)
+{
+	return b->gen - b->disk_gen;
+}
+
+#define BUCKET_GC_GEN_MAX	96U
+#define BUCKET_DISK_GEN_MAX	64U
+
+#define kobj_attribute_write(n, fn)					\
+	static struct kobj_attribute ksysfs_##n = __ATTR(n, S_IWUSR, NULL, fn)
+
+#define kobj_attribute_rw(n, show, store)				\
+	static struct kobj_attribute ksysfs_##n =			\
+		__ATTR(n, S_IWUSR|S_IRUSR, show, store)
+
+/* Forward declarations */
+
+void bch_writeback_queue(struct cached_dev *);
+void bch_writeback_add(struct cached_dev *, unsigned);
+
+void bch_count_io_errors(struct cache *, int, const char *);
+void bch_bbio_count_io_errors(struct cache_set *, struct bio *,
+			      int, const char *);
+void bch_bbio_endio(struct cache_set *, struct bio *, int, const char *);
+void bch_bbio_free(struct bio *, struct cache_set *);
+struct bio *bch_bbio_alloc(struct cache_set *);
+
+struct bio *bch_bio_split(struct bio *, int, gfp_t, struct bio_set *);
+void bch_generic_make_request(struct bio *, struct bio_split_pool *);
+void __bch_submit_bbio(struct bio *, struct cache_set *);
+void bch_submit_bbio(struct bio *, struct cache_set *, struct bkey *, unsigned);
+
+uint8_t bch_inc_gen(struct cache *, struct bucket *);
+void bch_rescale_priorities(struct cache_set *, int);
+bool bch_bucket_add_unused(struct cache *, struct bucket *);
+void bch_allocator_thread(struct closure *);
+
+long bch_bucket_alloc(struct cache *, unsigned, struct closure *);
+void bch_bucket_free(struct cache_set *, struct bkey *);
+
+int __bch_bucket_alloc_set(struct cache_set *, unsigned,
+			   struct bkey *, int, struct closure *);
+int bch_bucket_alloc_set(struct cache_set *, unsigned,
+			 struct bkey *, int, struct closure *);
+
+__printf(2, 3)
+bool bch_cache_set_error(struct cache_set *, const char *, ...);
+
+void bch_prio_write(struct cache *);
+void bch_write_bdev_super(struct cached_dev *, struct closure *);
+
+extern struct workqueue_struct *bcache_wq, *bch_gc_wq;
+extern const char * const bch_cache_modes[];
+extern struct mutex bch_register_lock;
+extern struct list_head bch_cache_sets;
+
+extern struct kobj_type bch_cached_dev_ktype;
+extern struct kobj_type bch_flash_dev_ktype;
+extern struct kobj_type bch_cache_set_ktype;
+extern struct kobj_type bch_cache_set_internal_ktype;
+extern struct kobj_type bch_cache_ktype;
+
+void bch_cached_dev_release(struct kobject *);
+void bch_flash_dev_release(struct kobject *);
+void bch_cache_set_release(struct kobject *);
+void bch_cache_release(struct kobject *);
+
+int bch_uuid_write(struct cache_set *);
+void bcache_write_super(struct cache_set *);
+
+int bch_flash_dev_create(struct cache_set *c, uint64_t size);
+
+int bch_cached_dev_attach(struct cached_dev *, struct cache_set *);
+void bch_cached_dev_detach(struct cached_dev *);
+void bch_cached_dev_run(struct cached_dev *);
+void bcache_device_stop(struct bcache_device *);
+
+void bch_cache_set_unregister(struct cache_set *);
+void bch_cache_set_stop(struct cache_set *);
+
+struct cache_set *bch_cache_set_alloc(struct cache_sb *);
+void bch_btree_cache_free(struct cache_set *);
+int bch_btree_cache_alloc(struct cache_set *);
+void bch_writeback_init_cached_dev(struct cached_dev *);
+void bch_moving_init_cache_set(struct cache_set *);
+
+void bch_cache_allocator_exit(struct cache *ca);
+int bch_cache_allocator_init(struct cache *ca);
+
+void bch_debug_exit(void);
+int bch_debug_init(struct kobject *);
+void bch_writeback_exit(void);
+int bch_writeback_init(void);
+void bch_request_exit(void);
+int bch_request_init(void);
+void bch_btree_exit(void);
+int bch_btree_init(void);
+
+#endif /* _BCACHE_H */
