commit 067df25c83902e2950ef24fed713f0fa38282f34
Author: Guoqing Jiang <guoqing.jiang@cloud.ionos.com>
Date:   Thu Sep 12 12:10:16 2019 +0200

    raid5: use bio_end_sector in r5_next_bio
    
    Actually, we calculate bio's end sector here, so use the common
    way for the purpose.
    
    Signed-off-by: Guoqing Jiang <guoqing.jiang@cloud.ionos.com>
    Signed-off-by: Song Liu <songliubraving@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 877e7d3f4bd1..f90e0704bed9 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -492,9 +492,7 @@ struct disk_info {
  */
 static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
 {
-	int sectors = bio_sectors(bio);
-
-	if (bio->bi_iter.bi_sector + sectors < sector + STRIPE_SECTORS)
+	if (bio_end_sector(bio) < sector + STRIPE_SECTORS)
 		return bio->bi_next;
 	else
 		return NULL;

commit feb9bf9849e2aa0dd2833285af7c25aee07df7bb
Author: Guoqing Jiang <guoqing.jiang@cloud.ionos.com>
Date:   Thu Sep 12 12:10:15 2019 +0200

    raid5: remove STRIPE_OPS_REQ_PENDING
    
    This stripe state is not used anymore after commit 51acbcec6c42b24
    ("md: remove CONFIG_MULTICORE_RAID456"), so remove the obsoleted
    state.
    
    gjiang@nb01257:~/md$ grep STRIPE_OPS_REQ_PENDING drivers/md/ -r
    drivers/md/raid5.c:                                       (1 << STRIPE_OPS_REQ_PENDING) |
    drivers/md/raid5.h:     STRIPE_OPS_REQ_PENDING,
    
    Signed-off-by: Guoqing Jiang <guoqing.jiang@cloud.ionos.com>
    Signed-off-by: Song Liu <songliubraving@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cf991f13403e..877e7d3f4bd1 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -357,7 +357,6 @@ enum {
 	STRIPE_FULL_WRITE,	/* all blocks are set to be overwritten */
 	STRIPE_BIOFILL_RUN,
 	STRIPE_COMPUTE_RUN,
-	STRIPE_OPS_REQ_PENDING,
 	STRIPE_ON_UNPLUG_LIST,
 	STRIPE_DISCARD,
 	STRIPE_ON_RELEASE_LIST,

commit b330e6a49dc3e9145de5c986b29bbbb884351e92
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Mon Mar 11 23:31:06 2019 -0700

    md: convert to kvmalloc
    
    The code really just wants a big flat buffer, so just do that.
    
    Link: http://lkml.kernel.org/r/20181217131929.11727-3-kent.overstreet@gmail.com
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Reviewed-by: Matthew Wilcox <willy@infradead.org>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Eric Paris <eparis@parisplace.org>
    Cc: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Paul Moore <paul@paul-moore.com>
    Cc: Pravin B Shelar <pshelar@ovn.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Vlad Yasevich <vyasevich@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 8474c224127b..cf991f13403e 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -638,10 +638,11 @@ struct r5conf {
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
-		struct flex_array *scribble;   /* space for constructing buffer
-					      * lists and performing address
-					      * conversions
-					      */
+		void		*scribble;  /* space for constructing buffer
+					     * lists and performing address
+					     * conversions
+					     */
+		int scribble_obj_size;
 	} __percpu *percpu;
 	int scribble_disks;
 	int scribble_sectors;

commit d60dafdca4b463405e5586df923f05b10e9ac2f9
Merge: 1329c20433fb 5a409b4f56d5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 9 12:01:36 2018 -0700

    Merge branch 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/shli/md
    
    Pull MD updates from Shaohua Li:
     "A few fixes of MD for this merge window. Mostly bug fixes:
    
       - raid5 stripe batch fix from Amy
    
       - Read error handling for raid1 FailFast device from Gioh
    
       - raid10 recovery NULL pointer dereference fix from Guoqing
    
       - Support write hint for raid5 stripe cache from Mariusz
    
       - Fixes for device hot add/remove from Neil and Yufen
    
       - Improve flush bio scalability from Xiao"
    
    * 'for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/shli/md:
      MD: fix lock contention for flush bios
      md/raid5: Assigning NULL to sh->batch_head before testing bit R5_Overlap of a stripe
      md/raid1: add error handling of read error from FailFast device
      md: fix NULL dereference of mddev->pers in remove_and_add_spares()
      raid5: copy write hint from origin bio to stripe
      md: fix two problems with setting the "re-add" device state.
      raid10: check bio in r10buf_pool_free to void NULL pointer dereference
      md: fix an error code format and remove unsed bio_sector

commit afeee514ce7f4cab605beedd03be71ebaf0c5fc8
Author: Kent Overstreet <kent.overstreet@gmail.com>
Date:   Sun May 20 18:25:52 2018 -0400

    md: convert to bioset_init()/mempool_init()
    
    Convert md to embedded bio sets.
    
    Signed-off-by: Kent Overstreet <kent.overstreet@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 3f8da26032ac..72e75ba6abf0 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -669,7 +669,7 @@ struct r5conf {
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
 	struct disk_info	*disks;
-	struct bio_set		*bio_split;
+	struct bio_set		bio_split;
 
 	/* When taking over an array from a different personality, we store
 	 * the new thread here until we fully activate the array.

commit 2cd259a77de561d49c1ff939a239095749e65ee7
Author: Mariusz Dabrowski <mariusz.dabrowski@intel.com>
Date:   Thu Apr 19 19:28:10 2018 +0200

    raid5: copy write hint from origin bio to stripe
    
    Store write hint from original bio in stripe head so it can be assigned
    to bio sent to each RAID device.
    
    Signed-off-by: Mariusz Dabrowski <mariusz.dabrowski@intel.com>
    Reviewed-by: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
    Reviewed-by: Pawel Baldysiak <pawel.baldysiak@intel.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 3f8da26032ac..aea2447b0ea7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -257,6 +257,7 @@ struct stripe_head {
 		sector_t	sector;			/* sector of this page */
 		unsigned long	flags;
 		u32		log_checksum;
+		unsigned short	write_hint;
 	} dev[1]; /* allocated with extra space depending of RAID geometry */
 };
 

commit f2785b527cda46314805123ddcbc871655b7c4c4
Author: NeilBrown <neilb@suse.com>
Date:   Sat Feb 3 09:19:30 2018 +1100

    md: document lifetime of internal rdev pointer.
    
    The rdev pointer kept in the local 'config' for each for
    raid1, raid10, raid4/5/6 has non-obvious lifetime rules.
    Sometimes RCU is needed, sometimes a lock, something nothing.
    
    Add documentation to explain this.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <sh.li@alibaba-inc.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2e6123825095..3f8da26032ac 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -450,6 +450,18 @@ enum {
  * HANDLE gets cleared if stripe_handle leaves nothing locked.
  */
 
+/* Note: disk_info.rdev can be set to NULL asynchronously by raid5_remove_disk.
+ * There are three safe ways to access disk_info.rdev.
+ * 1/ when holding mddev->reconfig_mutex
+ * 2/ when resync/recovery/reshape is known to be happening - i.e. in code that
+ *    is called as part of performing resync/recovery/reshape.
+ * 3/ while holding rcu_read_lock(), use rcu_dereference to get the pointer
+ *    and if it is non-NULL, increment rdev->nr_pending before dropping the RCU
+ *    lock.
+ * When .rdev is set to NULL, the nr_pending count checked again and if
+ * it has been incremented, the pointer is put back in .rdev.
+ */
+
 struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 	struct page	*extra_page; /* extra page to use in prexor */

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index f6536399677a..2e6123825095 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _RAID5_H
 #define _RAID5_H
 

commit d35a878ae1c50977b55e352fd46e36e35add72a0
Merge: e5021876c91d 390020ad2af9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 10:31:20 2017 -0700

    Merge tag 'for-4.12/dm-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm
    
    Pull device mapper updates from Mike Snitzer:
    
     - A major update for DM cache that reduces the latency for deciding
       whether blocks should migrate to/from the cache. The bio-prison-v2
       interface supports this improvement by enabling direct dispatch of
       work to workqueues rather than having to delay the actual work
       dispatch to the DM cache core. So the dm-cache policies are much more
       nimble by being able to drive IO as they see fit. One immediate
       benefit from the improved latency is a cache that should be much more
       adaptive to changing workloads.
    
     - Add a new DM integrity target that emulates a block device that has
       additional per-sector tags that can be used for storing integrity
       information.
    
     - Add a new authenticated encryption feature to the DM crypt target
       that builds on the capabilities provided by the DM integrity target.
    
     - Add MD interface for switching the raid4/5/6 journal mode and update
       the DM raid target to use it to enable aid4/5/6 journal write-back
       support.
    
     - Switch the DM verity target over to using the asynchronous hash
       crypto API (this helps work better with architectures that have
       access to off-CPU algorithm providers, which should reduce CPU
       utilization).
    
     - Various request-based DM and DM multipath fixes and improvements from
       Bart and Christoph.
    
     - A DM thinp target fix for a bio structure leak that occurs for each
       discard IFF discard passdown is enabled.
    
     - A fix for a possible deadlock in DM bufio and a fix to re-check the
       new buffer allocation watermark in the face of competing admin
       changes to the 'max_cache_size_bytes' tunable.
    
     - A couple DM core cleanups.
    
    * tag 'for-4.12/dm-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/device-mapper/linux-dm: (50 commits)
      dm bufio: check new buffer allocation watermark every 30 seconds
      dm bufio: avoid a possible ABBA deadlock
      dm mpath: make it easier to detect unintended I/O request flushes
      dm mpath: cleanup QUEUE_IF_NO_PATH bit manipulation by introducing assign_bit()
      dm mpath: micro-optimize the hot path relative to MPATHF_QUEUE_IF_NO_PATH
      dm: introduce enum dm_queue_mode to cleanup related code
      dm mpath: verify __pg_init_all_paths locking assumptions at runtime
      dm: verify suspend_locking assumptions at runtime
      dm block manager: remove an unused argument from dm_block_manager_create()
      dm rq: check blk_mq_register_dev() return value in dm_mq_init_request_queue()
      dm mpath: delay requeuing while path initialization is in progress
      dm mpath: avoid that path removal can trigger an infinite loop
      dm mpath: split and rename activate_path() to prepare for its expanded use
      dm ioctl: prevent stack leak in dm ioctl call
      dm integrity: use previously calculated log2 of sectors_per_block
      dm integrity: use hex2bin instead of open-coded variant
      dm crypt: replace custom implementation of hex2bin()
      dm crypt: remove obsolete references to per-CPU state
      dm verity: switch to using asynchronous hash crypto API
      dm crypt: use WQ_HIGHPRI for the IO and crypt workqueues
      ...

commit dd7a8f5dee81ffb1794df1103f07c63fd4f1d766
Author: NeilBrown <neilb@suse.com>
Date:   Wed Apr 5 14:05:51 2017 +1000

    md/raid5: make chunk_aligned_read() split bios more cleanly.
    
    chunk_aligned_read() currently uses fs_bio_set - which is meant for
    filesystems to use - and loops if multiple splits are needed, which is
    not best practice.
    As this is only used for READ requests, not writes, it is unlikely
    to cause a problem.  However it is best to be consistent in how
    we split bios, and to follow the pattern used in raid1/raid10.
    
    So create a private bioset, bio_split, and use it to perform a single
    split, submitting the remainder to generic_make_request() for later
    processing.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cdc7f92e1806..625c7f16fd6b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -646,6 +646,7 @@ struct r5conf {
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
 	struct disk_info	*disks;
+	struct bio_set		*bio_split;
 
 	/* When taking over an array from a different personality, we store
 	 * the new thread here until we fully activate the array.

commit 78e470c26f524f4706c2555613b9641d85190cbe
Author: Heinz Mauelshagen <heinzm@redhat.com>
Date:   Wed Mar 22 17:44:37 2017 +0100

    md: add raid4/5/6 journal mode switching API
    
    Commit 2ded370373a4 ("md/r5cache: State machine for raid5-cache write
    back mode") added support for "write-back" caching on the raid journal
    device.
    
    In order to allow the dm-raid target to switch between the available
    "write-through" and "write-back" modes, provide a new
    r5c_journal_mode_set() API.
    
    Use the new API in existing r5c_journal_mode_store()
    
    Signed-off-by: Heinz Mauelshagen <heinzm@redhat.com>
    Acked-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 4bb27b97bf6b..ec8ca15774d7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -547,6 +547,16 @@ struct r5worker_group {
 	int stripes_cnt;
 };
 
+/*
+ * r5c journal modes of the array: write-back or write-through.
+ * write-through mode has identical behavior as existing log only
+ * implementation.
+ */
+enum r5c_journal_mode {
+	R5C_JOURNAL_MODE_WRITE_THROUGH = 0,
+	R5C_JOURNAL_MODE_WRITE_BACK = 1,
+};
+
 enum r5_cache_state {
 	R5_INACTIVE_BLOCKED,	/* release of inactive stripes blocked,
 				 * waiting for 25% to be free
@@ -795,4 +805,5 @@ extern void r5c_check_cached_full_stripe(struct r5conf *conf);
 extern struct md_sysfs_entry r5c_journal_mode;
 extern void r5c_update_on_rdev_error(struct mddev *mddev);
 extern bool r5c_big_stripe_cached(struct r5conf *conf, sector_t sect);
+extern int r5c_journal_mode_set(struct mddev *mddev, int journal_mode);
 #endif

commit 0472a42ba1f89ec85f070c731f4440d7cc38c44c
Author: NeilBrown <neilb@suse.com>
Date:   Wed Mar 15 14:05:13 2017 +1100

    md/raid5: remove over-loading of ->bi_phys_segments.
    
    When a read request, which bypassed the cache, fails, we need to retry
    it through the cache.
    This involves attaching it to a sequence of stripe_heads, and it may not
    be possible to get all the stripe_heads we need at once.
    We do what we can, and record how far we got in ->bi_phys_segments so
    we can pick up again later.
    
    There is only ever one bio which may have a non-zero offset stored in
    ->bi_phys_segments, the one that is either active in the single thread
    which calls retry_aligned_read(), or is in conf->retry_read_aligned
    waiting for retry_aligned_read() to be called again.
    
    So we only need to store one offset value.  This can be in a local
    variable passed between remove_bio_from_retry() and
    retry_aligned_read(), or in the r5conf structure next to the
    ->retry_read_aligned pointer.
    
    Storing it there allows the last usage of ->bi_phys_segments to be
    removed from md/raid5.c.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 7d74fb3f2ec6..cdc7f92e1806 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -487,35 +487,6 @@ static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
 		return NULL;
 }
 
-/*
- * We maintain a count of processed stripes in the upper 16 bits
- */
-static inline int raid5_bi_processed_stripes(struct bio *bio)
-{
-	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
-
-	return (atomic_read(segments) >> 16) & 0xffff;
-}
-
-static inline void raid5_set_bi_processed_stripes(struct bio *bio,
-	unsigned int cnt)
-{
-	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
-	int old, new;
-
-	do {
-		old = atomic_read(segments);
-		new = (old & 0xffff) | (cnt << 16);
-	} while (atomic_cmpxchg(segments, old, new) != old);
-}
-
-static inline void raid5_set_bi_stripes(struct bio *bio, unsigned int cnt)
-{
-	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
-
-	atomic_set(segments, cnt);
-}
-
 /* NOTE NR_STRIPE_HASH_LOCKS must remain below 64.
  * This is because we sometimes take all the spinlocks
  * and creating that much locking depth can cause
@@ -613,6 +584,7 @@ struct r5conf {
 	struct list_head	delayed_list; /* stripes that have plugged requests */
 	struct list_head	bitmap_list; /* stripes delaying awaiting bitmap update */
 	struct bio		*retry_read_aligned; /* currently retrying aligned bios   */
+	unsigned int		retry_read_offset; /* sector offset into retry_read_aligned */
 	struct bio		*retry_read_aligned_list; /* aligned bios retry list  */
 	atomic_t		preread_active_stripes; /* stripes with scheduled io */
 	atomic_t		active_aligned_reads;

commit 016c76ac76e4c678b01a75a602dc6be0282f5b29
Author: NeilBrown <neilb@suse.com>
Date:   Wed Mar 15 14:05:13 2017 +1100

    md/raid5: use bio_inc_remaining() instead of repurposing bi_phys_segments as a counter
    
    md/raid5 needs to keep track of how many stripe_heads are processing a
    bio so that it can delay calling bio_endio() until all stripe_heads
    have completed.  It currently uses 16 bits of ->bi_phys_segments for
    this purpose.
    
    16 bits is only enough for 256M requests, and it is possible for a
    single bio to be larger than this, which causes problems.  Also, the
    bio struct contains a larger counter, __bi_remaining, which has a
    purpose very similar to the purpose of our counter.  So stop using
    ->bi_phys_segments, and instead use __bi_remaining.
    
    This means we don't need to initialize the counter, as our caller
    initializes it to '1'.  It also means we can call bio_endio() directly
    as it tests this counter internally.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index fd5c21cde77f..7d74fb3f2ec6 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -488,8 +488,7 @@ static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
 }
 
 /*
- * We maintain a biased count of active stripes in the bottom 16 bits of
- * bi_phys_segments, and a count of processed stripes in the upper 16 bits
+ * We maintain a count of processed stripes in the upper 16 bits
  */
 static inline int raid5_bi_processed_stripes(struct bio *bio)
 {
@@ -498,20 +497,6 @@ static inline int raid5_bi_processed_stripes(struct bio *bio)
 	return (atomic_read(segments) >> 16) & 0xffff;
 }
 
-static inline int raid5_dec_bi_active_stripes(struct bio *bio)
-{
-	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
-
-	return atomic_sub_return(1, segments) & 0xffff;
-}
-
-static inline void raid5_inc_bi_active_stripes(struct bio *bio)
-{
-	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
-
-	atomic_inc(segments);
-}
-
 static inline void raid5_set_bi_processed_stripes(struct bio *bio,
 	unsigned int cnt)
 {

commit bd83d0a28c68bacba88a3193a1bd6a083bb8d9f5
Author: NeilBrown <neilb@suse.com>
Date:   Wed Mar 15 14:05:12 2017 +1100

    md/raid5: call bio_endio() directly rather than queueing for later.
    
    We currently gather bios that need to be returned into a bio_list
    and call bio_endio() on them all together.
    The original reason for this was to avoid making the calls while
    holding a spinlock.
    Locking has changed a lot since then, and that reason is no longer
    valid.
    
    So discard return_io() and various return_bi lists, and just call
    bio_endio() directly as needed.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 13800dc9dd88..fd5c21cde77f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -278,7 +278,6 @@ struct stripe_head_state {
 	int dec_preread_active;
 	unsigned long ops_request;
 
-	struct bio_list return_bi;
 	struct md_rdev *blocked_rdev;
 	int handle_bad_blocks;
 	int log_failed;

commit 16d997b78b157315f5c90fcbc2f9ce575cb3879f
Author: NeilBrown <neilb@suse.com>
Date:   Wed Mar 15 14:05:12 2017 +1100

    md/raid5: simplfy delaying of writes while metadata is updated.
    
    If a device fails during a write, we must ensure the failure is
    recorded in the metadata before the completion of the write is
    acknowleged.
    
    Commit c3cce6cda162 ("md/raid5: ensure device failure recorded before
    write request returns.")  added code for this, but it was
    unnecessarily complicated.  We already had similar functionality for
    handling updates to the bad-block-list, thanks to Commit de393cdea66c
    ("md: make it easier to wait for bad blocks to be acknowledged.")
    
    So revert most of the former commit, and instead avoid collecting
    completed writes if MD_CHANGE_PENDING is set.  raid5d() will then flush
    the metadata and retry the stripe_head.
    As this change can leave a stripe_head ready for handling immediately
    after handle_active_stripes() returns, we change raid5_do_work() to
    pause when MD_CHANGE_PENDING is set, so that it doesn't spin.
    
    We check MD_CHANGE_PENDING *after* analyse_stripe() as it could be set
    asynchronously.  After analyse_stripe(), we have collected stable data
    about the state of devices, which will be used to make decisions.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ba5b7a3790af..13800dc9dd88 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -638,9 +638,6 @@ struct r5conf {
 	int			skip_copy; /* Don't copy data from bio to stripe cache */
 	struct list_head	*last_hold; /* detect hold_list promotions */
 
-	/* bios to have bi_end_io called after metadata is synced */
-	struct bio_list		return_bi;
-
 	atomic_t		reshape_stripes; /* stripes with pending writes for reshape */
 	/* unfortunately we need two cache names as we temporarily have
 	 * two caches.

commit 3418d036c81dcb604b7c7c71b209d5890a8418aa
Author: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
Date:   Thu Mar 9 09:59:59 2017 +0100

    raid5-ppl: Partial Parity Log write logging implementation
    
    Implement the calculation of partial parity for a stripe and PPL write
    logging functionality. The description of PPL is added to the
    documentation. More details can be found in the comments in raid5-ppl.c.
    
    Attach a page for holding the partial parity data to stripe_head.
    Allocate it only if mddev has the MD_HAS_PPL flag set.
    
    Partial parity is the xor of not modified data chunks of a stripe and is
    calculated as follows:
    
    - reconstruct-write case:
      xor data from all not updated disks in a stripe
    
    - read-modify-write case:
      xor old data and parity from all updated disks in a stripe
    
    Implement it using the async_tx API and integrate into raid_run_ops().
    It must be called when we still have access to old data, so do it when
    STRIPE_OP_BIODRAIN is set, but before ops_run_prexor5(). The result is
    stored into sh->ppl_page.
    
    Partial parity is not meaningful for full stripe write and is not stored
    in the log or used for recovery, so don't attempt to calculate it when
    stripe has STRIPE_FULL_WRITE.
    
    Put the PPL metadata structures to md_p.h because userspace tools
    (mdadm) will also need to read/write PPL.
    
    Warn about using PPL with enabled disk volatile write-back cache for
    now. It can be removed once disk cache flushing before writing PPL is
    implemented.
    
    Signed-off-by: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 6dd295a80ee1..ba5b7a3790af 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -224,10 +224,16 @@ struct stripe_head {
 	spinlock_t		batch_lock; /* only header's lock is useful */
 	struct list_head	batch_list; /* protected by head's batch lock*/
 
-	struct r5l_io_unit	*log_io;
+	union {
+		struct r5l_io_unit	*log_io;
+		struct ppl_io_unit	*ppl_io;
+	};
+
 	struct list_head	log_list;
 	sector_t		log_start; /* first meta block on the journal */
 	struct list_head	r5c; /* for r5c_cache->stripe_in_journal */
+
+	struct page		*ppl_page; /* partial parity of this stripe */
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
@@ -400,6 +406,7 @@ enum {
 	STRIPE_OP_BIODRAIN,
 	STRIPE_OP_RECONSTRUCT,
 	STRIPE_OP_CHECK,
+	STRIPE_OP_PARTIAL_PARITY,
 };
 
 /*
@@ -696,6 +703,7 @@ struct r5conf {
 	int			group_cnt;
 	int			worker_cnt_per_group;
 	struct r5l_log		*log;
+	void			*log_private;
 
 	spinlock_t		pending_bios_lock;
 	bool			batch_bio_dispatch;

commit ff875738edd44e3bc892d378deacc50bccc9d70c
Author: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
Date:   Thu Mar 9 09:59:58 2017 +0100

    raid5: separate header for log functions
    
    Move raid5-cache declarations from raid5.h to raid5-log.h, add inline
    wrappers for functions which will be shared with ppl and use them in
    raid5 core instead of direct calls to raid5-cache.
    
    Remove unused parameter from r5c_cache_data(), move two duplicated
    pr_debug() calls to r5l_init_log().
    
    Signed-off-by: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 985cdc4850c2..6dd295a80ee1 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -779,34 +779,4 @@ extern struct stripe_head *
 raid5_get_active_stripe(struct r5conf *conf, sector_t sector,
 			int previous, int noblock, int noquiesce);
 extern int raid5_calc_degraded(struct r5conf *conf);
-extern int r5l_init_log(struct r5conf *conf, struct md_rdev *rdev);
-extern void r5l_exit_log(struct r5l_log *log);
-extern int r5l_write_stripe(struct r5l_log *log, struct stripe_head *head_sh);
-extern void r5l_write_stripe_run(struct r5l_log *log);
-extern void r5l_flush_stripe_to_raid(struct r5l_log *log);
-extern void r5l_stripe_write_finished(struct stripe_head *sh);
-extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
-extern void r5l_quiesce(struct r5l_log *log, int state);
-extern bool r5l_log_disk_error(struct r5conf *conf);
-extern bool r5c_is_writeback(struct r5l_log *log);
-extern int
-r5c_try_caching_write(struct r5conf *conf, struct stripe_head *sh,
-		      struct stripe_head_state *s, int disks);
-extern void
-r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
-			    struct stripe_head_state *s);
-extern void r5c_release_extra_page(struct stripe_head *sh);
-extern void r5c_use_extra_page(struct stripe_head *sh);
-extern void r5l_wake_reclaim(struct r5l_log *log, sector_t space);
-extern void r5c_handle_cached_data_endio(struct r5conf *conf,
-	struct stripe_head *sh, int disks, struct bio_list *return_bi);
-extern int r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
-			  struct stripe_head_state *s);
-extern void r5c_make_stripe_write_out(struct stripe_head *sh);
-extern void r5c_flush_cache(struct r5conf *conf, int num);
-extern void r5c_check_stripe_cache_usage(struct r5conf *conf);
-extern void r5c_check_cached_full_stripe(struct r5conf *conf);
-extern struct md_sysfs_entry r5c_journal_mode;
-extern void r5c_update_on_rdev_error(struct mddev *mddev);
-extern bool r5c_big_stripe_cached(struct r5conf *conf, sector_t sect);
 #endif

commit aaf9f12ebfafd1ea603d61ead6dbcf456a86e0f3
Author: Shaohua Li <shli@fb.com>
Date:   Fri Mar 3 22:06:12 2017 -0800

    md/raid5: sort bios
    
    Previous patch (raid5: only dispatch IO from raid5d for harddisk raid)
    defers IO dispatching. The goal is to create better IO pattern. At that
    time, we don't sort the deffered IO and hope the block layer can do IO
    merge and sort. Now the raid5-cache writeback could create large amount
    of bios. And if we enable muti-thread for stripe handling, we can't
    control when to dispatch IO to raid disks. In a lot of time, we are
    dispatching IO which block layer can't do merge effectively.
    
    This patch moves further for the IO dispatching defer. We accumulate
    bios, but we don't dispatch all the bios after a threshold is met. This
    'dispatch partial portion of bios' stragety allows bios coming in a
    large time window are sent to disks together. At the dispatching time,
    there is large chance the block layer can merge the bios. To make this
    more effective, we dispatch IO in ascending order. This increases
    request merge chance and reduces disk seek.
    
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 6b9d2e839e6d..985cdc4850c2 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -572,6 +572,14 @@ enum r5_cache_state {
 				 */
 };
 
+#define PENDING_IO_MAX 512
+#define PENDING_IO_ONE_FLUSH 128
+struct r5pending_data {
+	struct list_head sibling;
+	sector_t sector; /* stripe sector */
+	struct bio_list bios;
+};
+
 struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
 	/* only protect corresponding hash list and inactive_list */
@@ -689,9 +697,13 @@ struct r5conf {
 	int			worker_cnt_per_group;
 	struct r5l_log		*log;
 
-	struct bio_list		pending_bios;
 	spinlock_t		pending_bios_lock;
 	bool			batch_bio_dispatch;
+	struct r5pending_data	*pending_data;
+	struct list_head	free_list;
+	struct list_head	pending_list;
+	int			pending_data_cnt;
+	struct r5pending_data	*next_pending_data;
 };
 
 

commit 535ae4eb1225f19e1d1848c65eafea8b7e9112f4
Author: Shaohua Li <shli@fb.com>
Date:   Wed Feb 15 19:37:32 2017 -0800

    md/raid5: prioritize stripes for writeback
    
    In raid5-cache writeback mode, we have two types of stripes to handle.
    - stripes which aren't cached yet
    - stripes which are cached and flushing out to raid disks
    
    Upperlayer is more sensistive to latency of the first type of stripes
    generally. But we only one handle list for all these stripes, where the
    two types of stripes are mixed together. When reclaim flushes a lot of
    stripes, the first type of stripes could be noticeably delayed. On the
    other hand, if the log space is tight, we'd like to handle the second
    type of stripes faster and free log space.
    
    This patch destinguishes the two types stripes. They are added into
    different handle list. When we try to get a stripe to handl, we prefer
    the first type of stripes unless log space is tight.
    
    This should have no impact for !writeback case.
    
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 4bb27b97bf6b..6b9d2e839e6d 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -542,6 +542,7 @@ struct r5worker {
 
 struct r5worker_group {
 	struct list_head handle_list;
+	struct list_head loprio_list;
 	struct r5conf *conf;
 	struct r5worker *workers;
 	int stripes_cnt;
@@ -608,6 +609,7 @@ struct r5conf {
 						  */
 
 	struct list_head	handle_list; /* stripes needing handling */
+	struct list_head	loprio_list; /* low priority stripes */
 	struct list_head	hold_list; /* preread ready stripes */
 	struct list_head	delayed_list; /* stripes that have plugged requests */
 	struct list_head	bitmap_list; /* stripes delaying awaiting bitmap update */

commit e33fbb9cc73d6502e69eaf1c178e0c39059763ea
Author: Shaohua Li <shli@fb.com>
Date:   Fri Feb 10 16:18:09 2017 -0800

    md/raid5-cache: exclude reclaiming stripes in reclaim check
    
    stripes which are being reclaimed are still accounted into cached
    stripes. The reclaim takes time. r5c_do_reclaim isn't aware of the
    stripes and does unnecessary stripe reclaim. In practice, I saw one
    stripe is reclaimed one time. This will cause bad IO pattern. Fixing
    this by excluding the reclaing stripes in the check.
    
    Cc: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c0687df5ba06..4bb27b97bf6b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -663,6 +663,8 @@ struct r5conf {
 	struct list_head	r5c_full_stripe_list;
 	atomic_t		r5c_cached_partial_stripes;
 	struct list_head	r5c_partial_stripe_list;
+	atomic_t		r5c_flushing_full_stripes;
+	atomic_t		r5c_flushing_partial_stripes;
 
 	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;

commit 03b047f45c29dff02f913a0234ca0cc1ca51966f
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Jan 11 13:39:14 2017 -0800

    md/r5cache: enable chunk_aligned_read with write back cache
    
    Chunk aligned read significantly reduces CPU usage of raid456.
    However, it is not safe to fully bypass the write back cache.
    This patch enables chunk aligned read with write back cache.
    
    For chunk aligned read, we track stripes in write back cache at
    a bigger granularity, "big_stripe". Each chunk may contain more
    than one stripe (for example, a 256kB chunk contains 64 4kB-page,
    so this chunk contain 64 stripes). For chunk_aligned_read, these
    stripes are grouped into one big_stripe, so we only need one lookup
    for the whole chunk.
    
    For each big_stripe, struct big_stripe_info tracks how many stripes
    of this big_stripe are in the write back cache. We count how many
    stripes of this big_stripe are in the write back cache. These
    counters are tracked in a radix tree (big_stripe_tree).
    r5c_tree_index() is used to calculate keys for the radix tree.
    
    chunk_aligned_read() calls r5c_big_stripe_cached() to look up
    big_stripe of each chunk in the tree. If this big_stripe is in the
    tree, chunk_aligned_read() aborts. This look up is protected by
    rcu_read_lock().
    
    It is necessary to remember whether a stripe is counted in
    big_stripe_tree. Instead of adding new flag, we reuses existing flags:
    STRIPE_R5C_PARTIAL_STRIPE and STRIPE_R5C_FULL_STRIPE. If either of these
    two flags are set, the stripe is counted in big_stripe_tree. This
    requires moving set_bit(STRIPE_R5C_PARTIAL_STRIPE) to
    r5c_try_caching_write(); and moving clear_bit of
    STRIPE_R5C_PARTIAL_STRIPE and STRIPE_R5C_FULL_STRIPE to
    r5c_finish_stripe_write_out().
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ebb89bda88f1..c0687df5ba06 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -792,4 +792,5 @@ extern void r5c_check_stripe_cache_usage(struct r5conf *conf);
 extern void r5c_check_cached_full_stripe(struct r5conf *conf);
 extern struct md_sysfs_entry r5c_journal_mode;
 extern void r5c_update_on_rdev_error(struct mddev *mddev);
+extern bool r5c_big_stripe_cached(struct r5conf *conf, sector_t sect);
 #endif

commit 765d704db1f583630d52dc14c1ea573db6783459
Author: Shaohua Li <shli@fb.com>
Date:   Wed Jan 4 09:33:23 2017 -0800

    raid5: only dispatch IO from raid5d for harddisk raid
    
    We made raid5 stripe handling multi-thread before. It works well for
    SSD. But for harddisk, the multi-threading creates more disk seek, so
    not always improve performance. For several hard disks based raid5,
    multi-threading is required as raid5d becames a bottleneck especially
    for sequential write.
    
    To overcome the disk seek issue, we only dispatch IO from raid5d if the
    array is harddisk based. Other threads can still handle stripes, but
    can't dispatch IO.
    
    Idealy, we should control IO dispatching order according to IO position
    interrnally. Right now we still depend on block layer, which isn't very
    efficient sometimes though.
    
    My setup has 9 harddisks, each disk can do around 180M/s sequential
    write. So in theory, the raid5 can do 180 * 8 = 1440M/s sequential
    write. The test machine uses an ATOM CPU. I measure sequential write
    with large iodepth bandwidth to raid array:
    
    without patch: ~600M/s
    without patch and group_thread_cnt=4: 750M/s
    with patch and group_thread_cnt=4: 950M/s
    with patch, group_thread_cnt=4, skip_copy=1: 1150M/s
    
    We are pretty close to the maximum bandwidth in the large iodepth
    iodepth case. The performance gap of small iodepth sequential write
    between software raid and theory value is still very big though, because
    we don't have an efficient pipeline.
    
    Cc: NeilBrown <neilb@suse.com>
    Cc: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1440fa26e296..ebb89bda88f1 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -684,6 +684,10 @@ struct r5conf {
 	int			group_cnt;
 	int			worker_cnt_per_group;
 	struct r5l_log		*log;
+
+	struct bio_list		pending_bios;
+	spinlock_t		pending_bios_lock;
+	bool			batch_bio_dispatch;
 };
 
 

commit 2e38a37f23c98d7fad87ff022670060b8a0e2bf5
Author: Song Liu <songliubraving@fb.com>
Date:   Tue Jan 24 10:45:30 2017 -0800

    md/r5cache: disable write back for degraded array
    
    write-back cache in degraded mode introduces corner cases to the array.
    Although we try to cover all these corner cases, it is safer to just
    disable write-back cache when the array is in degraded mode.
    
    In this patch, we disable writeback cache for degraded mode:
    1. On device failure, if the array enters degraded mode, raid5_error()
       will submit async job r5c_disable_writeback_async to disable
       writeback;
    2. In r5c_journal_mode_store(), it is invalid to enable writeback in
       degraded mode;
    3. In r5c_try_caching_write(), stripes with s->failed>0 will be handled
       in write-through mode.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 461df197d157..1440fa26e296 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -758,6 +758,7 @@ extern sector_t raid5_compute_sector(struct r5conf *conf, sector_t r_sector,
 extern struct stripe_head *
 raid5_get_active_stripe(struct r5conf *conf, sector_t sector,
 			int previous, int noblock, int noquiesce);
+extern int raid5_calc_degraded(struct r5conf *conf);
 extern int r5l_init_log(struct r5conf *conf, struct md_rdev *rdev);
 extern void r5l_exit_log(struct r5l_log *log);
 extern int r5l_write_stripe(struct r5l_log *log, struct stripe_head *head_sh);
@@ -786,4 +787,5 @@ extern void r5c_flush_cache(struct r5conf *conf, int num);
 extern void r5c_check_stripe_cache_usage(struct r5conf *conf);
 extern void r5c_check_cached_full_stripe(struct r5conf *conf);
 extern struct md_sysfs_entry r5c_journal_mode;
+extern void r5c_update_on_rdev_error(struct mddev *mddev);
 #endif

commit 86aa1397ddfde563b3692adadb8b8e32e97b4e5e
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 12 17:22:41 2017 -0800

    md/r5cache: read data into orig_page for prexor of cached data
    
    With write back cache, we use orig_page to do prexor. This patch
    makes sure we read data into orig_page for it.
    
    Flag R5_OrigPageUPTDODATE is added to show whether orig_page
    has the latest data from raid disk.
    
    We introduce a helper function uptodate_for_rmw() to simplify
    the a couple conditions in handle_stripe_dirtying().
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ed8e1362ab36..461df197d157 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -322,6 +322,11 @@ enum r5dev_flags {
 			 * data and parity being written are in the journal
 			 * device
 			 */
+	R5_OrigPageUPTDODATE,	/* with write back cache, we read old data into
+				 * dev->orig_page for prexor. When this flag is
+				 * set, orig_page contains latest data in the
+				 * raid disk.
+				 */
 };
 
 /*

commit d7bd398e97f236a2353689eca5e8950f67cd34d5
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Nov 23 22:50:39 2016 -0800

    md/r5cache: handle alloc_page failure
    
    RMW of r5c write back cache uses an extra page to store old data for
    prexor. handle_stripe_dirtying() allocates this page by calling
    alloc_page(). However, alloc_page() may fail.
    
    To handle alloc_page() failures, this patch adds an extra page to
    disk_info. When alloc_page fails, handle_stripe() trys to use these
    pages. When these pages are used by other stripe (R5C_EXTRA_PAGE_IN_USE),
    the stripe is added to delayed_list.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index d13fe45d6960..ed8e1362ab36 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -276,6 +276,7 @@ struct stripe_head_state {
 	struct md_rdev *blocked_rdev;
 	int handle_bad_blocks;
 	int log_failed;
+	int waiting_extra_page;
 };
 
 /* Flags for struct r5dev.flags */
@@ -439,6 +440,7 @@ enum {
 
 struct disk_info {
 	struct md_rdev	*rdev, *replacement;
+	struct page	*extra_page; /* extra page to use in prexor */
 };
 
 /*
@@ -559,6 +561,9 @@ enum r5_cache_state {
 				 * only process stripes that are already
 				 * occupying the log
 				 */
+	R5C_EXTRA_PAGE_IN_USE,	/* a stripe is using disk_info.extra_page
+				 * for prexor
+				 */
 };
 
 struct r5conf {
@@ -765,6 +770,7 @@ extern void
 r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
 			    struct stripe_head_state *s);
 extern void r5c_release_extra_page(struct stripe_head *sh);
+extern void r5c_use_extra_page(struct stripe_head *sh);
 extern void r5l_wake_reclaim(struct r5l_log *log, sector_t space);
 extern void r5c_handle_cached_data_endio(struct r5conf *conf,
 	struct stripe_head *sh, int disks, struct bio_list *return_bi);

commit 3bddb7f8f264ec58dc86e11ca97341c24f9d38f6
Author: Song Liu <songliubraving@fb.com>
Date:   Fri Nov 18 16:46:50 2016 -0800

    md/r5cache: handle FLUSH and FUA
    
    With raid5 cache, we committing data from journal device. When
    there is flush request, we need to flush journal device's cache.
    This was not needed in raid5 journal, because we will flush the
    journal before committing data to raid disks.
    
    This is similar to FUA, except that we also need flush journal for
    FUA. Otherwise, corruptions in earlier meta data will stop recovery
    from reaching FUA data.
    
    slightly changed the code by Shaohua
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a698113c6188..d13fe45d6960 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -376,6 +376,7 @@ enum {
 	STRIPE_R5C_FULL_STRIPE,	/* in r5c cache (to-be/being handled or
 				 * in conf->r5c_full_stripe_list)
 				 */
+	STRIPE_R5C_PREFLUSH,	/* need to flush journal device */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAGS \

commit 2c7da14b90a01e48b17a028de6050a796cfd6d8d
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Nov 17 15:24:41 2016 -0800

    md/r5cache: sysfs entry journal_mode
    
    With write cache, journal_mode is the knob to switch between
    write-back and write-through.
    
    Below is an example:
    
    root@virt-test:~/# cat /sys/block/md0/md/journal_mode
    [write-through] write-back
    root@virt-test:~/# echo write-back > /sys/block/md0/md/journal_mode
    root@virt-test:~/# cat /sys/block/md0/md/journal_mode
    write-through [write-back]
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 35b4c0f0a850..a698113c6188 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -773,4 +773,5 @@ extern void r5c_make_stripe_write_out(struct stripe_head *sh);
 extern void r5c_flush_cache(struct r5conf *conf, int num);
 extern void r5c_check_stripe_cache_usage(struct r5conf *conf);
 extern void r5c_check_cached_full_stripe(struct r5conf *conf);
+extern struct md_sysfs_entry r5c_journal_mode;
 #endif

commit a39f7afde358ca89e9fc09a5525d3f8631a98a3a
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Nov 17 15:24:40 2016 -0800

    md/r5cache: write-out phase and reclaim support
    
    There are two limited resources, stripe cache and journal disk space.
    For better performance, we priotize reclaim of full stripe writes.
    To free up more journal space, we free earliest data on the journal.
    
    In current implementation, reclaim happens when:
    1. Periodically (every R5C_RECLAIM_WAKEUP_INTERVAL, 30 seconds) reclaim
       if there is no reclaim in the past 5 seconds.
    2. when there are R5C_FULL_STRIPE_FLUSH_BATCH (256) cached full stripes,
       or cached stripes is enough for a full stripe (chunk size / 4k)
       (r5c_check_cached_full_stripe)
    3. when there is pressure on stripe cache (r5c_check_stripe_cache_usage)
    4. when there is pressure on journal space (r5l_write_stripe, r5c_cache_data)
    
    r5c_do_reclaim() contains new logic of reclaim.
    
    For stripe cache:
    
    When stripe cache pressure is high (more than 3/4 stripes are cached,
    or there is empty inactive lists), flush all full stripe. If fewer
    than R5C_RECLAIM_STRIPE_GROUP (NR_STRIPE_HASH_LOCKS * 2) full stripes
    are flushed, flush some paritial stripes. When stripe cache pressure
    is moderate (1/2 to 3/4 of stripes are cached), flush all full stripes.
    
    For log space:
    
    To avoid deadlock due to log space, we need to reserve enough space
    to flush cached data. The size of required log space depends on total
    number of cached stripes (stripe_in_journal_count). In current
    implementation, the writing-out phase automatically include pending
    data writes with parity writes (similar to write through case).
    Therefore, we need up to (conf->raid_disks + 1) pages for each cached
    stripe (1 page for meta data, raid_disks pages for all data and
    parity). r5c_log_required_to_flush_cache() calculates log space
    required to flush cache. In the following, we refer to the space
    calculated by r5c_log_required_to_flush_cache() as
    reclaim_required_space.
    
    Two flags are added to r5conf->cache_state: R5C_LOG_TIGHT and
    R5C_LOG_CRITICAL. R5C_LOG_TIGHT is set when free space on the log
    device is less than 3x of reclaim_required_space. R5C_LOG_CRITICAL
    is set when free space on the log device is less than 2x of
    reclaim_required_space.
    
    r5c_cache keeps all data in cache (not fully committed to RAID) in
    a list (stripe_in_journal_list). These stripes are in the order of their
    first appearance on the journal. So the log tail (last_checkpoint)
    should point to the journal_start of the first item in the list.
    
    When R5C_LOG_TIGHT is set, r5l_reclaim_thread starts flushing out
    stripes at the head of stripe_in_journal. When R5C_LOG_CRITICAL is
    set, the state machine only writes data that are already in the
    log device (in stripe_in_journal_list).
    
    This patch includes a fix to improve performance by
    Shaohua Li <shli@fb.com>.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 73c183398e38..35b4c0f0a850 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -226,6 +226,8 @@ struct stripe_head {
 
 	struct r5l_io_unit	*log_io;
 	struct list_head	log_list;
+	sector_t		log_start; /* first meta block on the journal */
+	struct list_head	r5c; /* for r5c_cache->stripe_in_journal */
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
@@ -537,6 +539,27 @@ struct r5worker_group {
 	int stripes_cnt;
 };
 
+enum r5_cache_state {
+	R5_INACTIVE_BLOCKED,	/* release of inactive stripes blocked,
+				 * waiting for 25% to be free
+				 */
+	R5_ALLOC_MORE,		/* It might help to allocate another
+				 * stripe.
+				 */
+	R5_DID_ALLOC,		/* A stripe was allocated, don't allocate
+				 * more until at least one has been
+				 * released.  This avoids flooding
+				 * the cache.
+				 */
+	R5C_LOG_TIGHT,		/* log device space tight, need to
+				 * prioritize stripes at last_checkpoint
+				 */
+	R5C_LOG_CRITICAL,	/* log device is running out of space,
+				 * only process stripes that are already
+				 * occupying the log
+				 */
+};
+
 struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
 	/* only protect corresponding hash list and inactive_list */
@@ -636,17 +659,6 @@ struct r5conf {
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
 	unsigned long		cache_state;
-#define R5_INACTIVE_BLOCKED	1	/* release of inactive stripes blocked,
-					 * waiting for 25% to be free
-					 */
-#define R5_ALLOC_MORE		2	/* It might help to allocate another
-					 * stripe.
-					 */
-#define R5_DID_ALLOC		4	/* A stripe was allocated, don't allocate
-					 * more until at least one has been
-					 * released.  This avoids flooding
-					 * the cache.
-					 */
 	struct shrinker		shrinker;
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
@@ -752,8 +764,13 @@ extern void
 r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
 			    struct stripe_head_state *s);
 extern void r5c_release_extra_page(struct stripe_head *sh);
+extern void r5l_wake_reclaim(struct r5l_log *log, sector_t space);
 extern void r5c_handle_cached_data_endio(struct r5conf *conf,
 	struct stripe_head *sh, int disks, struct bio_list *return_bi);
 extern int r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
 			  struct stripe_head_state *s);
+extern void r5c_make_stripe_write_out(struct stripe_head *sh);
+extern void r5c_flush_cache(struct r5conf *conf, int num);
+extern void r5c_check_stripe_cache_usage(struct r5conf *conf);
+extern void r5c_check_cached_full_stripe(struct r5conf *conf);
 #endif

commit 1e6d690b9334b7e1b31d25fd8d93e980e449a5f9
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Nov 17 15:24:39 2016 -0800

    md/r5cache: caching phase of r5cache
    
    As described in previous patch, write back cache operates in two
    phases: caching and writing-out. The caching phase works as:
    1. write data to journal
       (r5c_handle_stripe_dirtying, r5c_cache_data)
    2. call bio_endio
       (r5c_handle_data_cached, r5c_return_dev_pending_writes).
    
    Then the writing-out phase is as:
    1. Mark the stripe as write-out (r5c_make_stripe_write_out)
    2. Calcualte parity (reconstruct or RMW)
    3. Write parity (and maybe some other data) to journal device
    4. Write data and parity to RAID disks
    
    This patch implements caching phase. The cache is integrated with
    stripe cache of raid456. It leverages code of r5l_log to write
    data to journal device.
    
    Writing-out phase of the cache is implemented in the next patch.
    
    With r5cache, write operation does not wait for parity calculation
    and write out, so the write latency is lower (1 write to journal
    device vs. read and then write to raid disks). Also, r5cache will
    reduce RAID overhead (multipile IO due to read-modify-write of
    parity) and provide more opportunities of full stripe writes.
    
    This patch adds 2 flags to stripe_head.state:
     - STRIPE_R5C_PARTIAL_STRIPE,
     - STRIPE_R5C_FULL_STRIPE,
    
    Instead of inactive_list, stripes with cached data are tracked in
    r5conf->r5c_full_stripe_list and r5conf->r5c_partial_stripe_list.
    STRIPE_R5C_FULL_STRIPE and STRIPE_R5C_PARTIAL_STRIPE are flags for
    stripes in these lists. Note: stripes in r5c_full/partial_stripe_list
    are not considered as "active".
    
    For RMW, the code allocates an extra page for each data block
    being updated.  This is stored in r5dev->orig_page and the old data
    is read into it.  Then the prexor calculation subtracts ->orig_page
    from the parity block, and the reconstruct calculation adds the
    ->page data back into the parity block.
    
    r5cache naturally excludes SkipCopy. When the array has write back
    cache, async_copy_data() will not skip copy.
    
    There are some known limitations of the cache implementation:
    
    1. Write cache only covers full page writes (R5_OVERWRITE). Writes
       of smaller granularity are write through.
    2. Only one log io (sh->log_io) for each stripe at anytime. Later
       writes for the same stripe have to wait. This can be improved by
       moving log_io to r5dev.
    3. With writeback cache, read path must enter state machine, which
       is a significant bottleneck for some workloads.
    4. There is no per stripe checkpoint (with r5l_payload_flush) in
       the log, so recovery code has to replay more than necessary data
       (sometimes all the log from last_checkpoint). This reduces
       availability of the array.
    
    This patch includes a fix proposed by ZhengYuan Liu
    <liuzhengyuan@kylinos.cn>
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c9590a8e1425..73c183398e38 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -264,7 +264,7 @@ struct stripe_head_state {
 	int syncing, expanding, expanded, replacing;
 	int locked, uptodate, to_read, to_write, failed, written;
 	int to_fill, compute, req_compute, non_overwrite;
-	int injournal;
+	int injournal, just_cached;
 	int failed_num[2];
 	int p_failed, q_failed;
 	int dec_preread_active;
@@ -368,6 +368,12 @@ enum {
 	STRIPE_R5C_CACHING,	/* the stripe is in caching phase
 				 * see more detail in the raid5-cache.c
 				 */
+	STRIPE_R5C_PARTIAL_STRIPE,	/* in r5c cache (to-be/being handled or
+					 * in conf->r5c_partial_stripe_list)
+					 */
+	STRIPE_R5C_FULL_STRIPE,	/* in r5c cache (to-be/being handled or
+				 * in conf->r5c_full_stripe_list)
+				 */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAGS \
@@ -618,6 +624,12 @@ struct r5conf {
 	 */
 	atomic_t		active_stripes;
 	struct list_head	inactive_list[NR_STRIPE_HASH_LOCKS];
+
+	atomic_t		r5c_cached_full_stripes;
+	struct list_head	r5c_full_stripe_list;
+	atomic_t		r5c_cached_partial_stripes;
+	struct list_head	r5c_partial_stripe_list;
+
 	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_quiescent;
@@ -739,4 +751,9 @@ r5c_try_caching_write(struct r5conf *conf, struct stripe_head *sh,
 extern void
 r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
 			    struct stripe_head_state *s);
+extern void r5c_release_extra_page(struct stripe_head *sh);
+extern void r5c_handle_cached_data_endio(struct r5conf *conf,
+	struct stripe_head *sh, int disks, struct bio_list *return_bi);
+extern int r5c_cache_data(struct r5l_log *log, struct stripe_head *sh,
+			  struct stripe_head_state *s);
 #endif

commit 2ded370373a400c20cf0c6e941e724e61582a867
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Nov 17 15:24:38 2016 -0800

    md/r5cache: State machine for raid5-cache write back mode
    
    This patch adds state machine for raid5-cache. With log device, the
    raid456 array could operate in two different modes (r5c_journal_mode):
      - write-back (R5C_MODE_WRITE_BACK)
      - write-through (R5C_MODE_WRITE_THROUGH)
    
    Existing code of raid5-cache only has write-through mode. For write-back
    cache, it is necessary to extend the state machine.
    
    With write-back cache, every stripe could operate in two different
    phases:
      - caching
      - writing-out
    
    In caching phase, the stripe handles writes as:
      - write to journal
      - return IO
    
    In writing-out phase, the stripe behaviors as a stripe in write through
    mode R5C_MODE_WRITE_THROUGH.
    
    STRIPE_R5C_CACHING is added to sh->state to differentiate caching and
    writing-out phase.
    
    Please note: this is a "no-op" patch for raid5-cache write-through
    mode.
    
    The following detailed explanation is copied from the raid5-cache.c:
    
    /*
     * raid5 cache state machine
     *
     * With rhe RAID cache, each stripe works in two phases:
     *      - caching phase
     *      - writing-out phase
     *
     * These two phases are controlled by bit STRIPE_R5C_CACHING:
     *   if STRIPE_R5C_CACHING == 0, the stripe is in writing-out phase
     *   if STRIPE_R5C_CACHING == 1, the stripe is in caching phase
     *
     * When there is no journal, or the journal is in write-through mode,
     * the stripe is always in writing-out phase.
     *
     * For write-back journal, the stripe is sent to caching phase on write
     * (r5c_handle_stripe_dirtying). r5c_make_stripe_write_out() kicks off
     * the write-out phase by clearing STRIPE_R5C_CACHING.
     *
     * Stripes in caching phase do not write the raid disks. Instead, all
     * writes are committed from the log device. Therefore, a stripe in
     * caching phase handles writes as:
     *      - write to log device
     *      - return IO
     *
     * Stripes in writing-out phase handle writes as:
     *      - calculate parity
     *      - write pending data and parity to journal
     *      - write data and parity to raid disks
     *      - return IO for pending writes
     */
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ffc13c4d7e63..c9590a8e1425 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -264,6 +264,7 @@ struct stripe_head_state {
 	int syncing, expanding, expanded, replacing;
 	int locked, uptodate, to_read, to_write, failed, written;
 	int to_fill, compute, req_compute, non_overwrite;
+	int injournal;
 	int failed_num[2];
 	int p_failed, q_failed;
 	int dec_preread_active;
@@ -313,6 +314,11 @@ enum r5dev_flags {
 			 */
 	R5_Discard,	/* Discard the stripe */
 	R5_SkipCopy,	/* Don't copy data from bio to stripe cache */
+	R5_InJournal,	/* data being written is in the journal device.
+			 * if R5_InJournal is set for parity pd_idx, all the
+			 * data and parity being written are in the journal
+			 * device
+			 */
 };
 
 /*
@@ -345,7 +351,23 @@ enum {
 	STRIPE_BITMAP_PENDING,	/* Being added to bitmap, don't add
 				 * to batch yet.
 				 */
-	STRIPE_LOG_TRAPPED, /* trapped into log */
+	STRIPE_LOG_TRAPPED,	/* trapped into log (see raid5-cache.c)
+				 * this bit is used in two scenarios:
+				 *
+				 * 1. write-out phase
+				 *  set in first entry of r5l_write_stripe
+				 *  clear in second entry of r5l_write_stripe
+				 *  used to bypass logic in handle_stripe
+				 *
+				 * 2. caching phase
+				 *  set in r5c_try_caching_write()
+				 *  clear when journal write is done
+				 *  used to initiate r5c_cache_data()
+				 *  also used to bypass logic in handle_stripe
+				 */
+	STRIPE_R5C_CACHING,	/* the stripe is in caching phase
+				 * see more detail in the raid5-cache.c
+				 */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAGS \
@@ -710,4 +732,11 @@ extern void r5l_stripe_write_finished(struct stripe_head *sh);
 extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
 extern void r5l_quiesce(struct r5l_log *log, int state);
 extern bool r5l_log_disk_error(struct r5conf *conf);
+extern bool r5c_is_writeback(struct r5l_log *log);
+extern int
+r5c_try_caching_write(struct r5conf *conf, struct stripe_head *sh,
+		      struct stripe_head_state *s, int disks);
+extern void
+r5c_finish_stripe_write_out(struct r5conf *conf, struct stripe_head *sh,
+			    struct stripe_head_state *s);
 #endif

commit 937621c36e0ea1af2aceeaea412ba3bd80247199
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Nov 17 15:24:37 2016 -0800

    md/r5cache: move some code to raid5.h
    
    Move some define and inline functions to raid5.h, so they can be
    used in raid5-cache.c
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 57ec49f0839e..ffc13c4d7e63 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -410,6 +410,83 @@ struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 };
 
+/*
+ * Stripe cache
+ */
+
+#define NR_STRIPES		256
+#define STRIPE_SIZE		PAGE_SIZE
+#define STRIPE_SHIFT		(PAGE_SHIFT - 9)
+#define STRIPE_SECTORS		(STRIPE_SIZE>>9)
+#define	IO_THRESHOLD		1
+#define BYPASS_THRESHOLD	1
+#define NR_HASH			(PAGE_SIZE / sizeof(struct hlist_head))
+#define HASH_MASK		(NR_HASH - 1)
+#define MAX_STRIPE_BATCH	8
+
+/* bio's attached to a stripe+device for I/O are linked together in bi_sector
+ * order without overlap.  There may be several bio's per stripe+device, and
+ * a bio could span several devices.
+ * When walking this list for a particular stripe+device, we must never proceed
+ * beyond a bio that extends past this device, as the next bio might no longer
+ * be valid.
+ * This function is used to determine the 'next' bio in the list, given the
+ * sector of the current stripe+device
+ */
+static inline struct bio *r5_next_bio(struct bio *bio, sector_t sector)
+{
+	int sectors = bio_sectors(bio);
+
+	if (bio->bi_iter.bi_sector + sectors < sector + STRIPE_SECTORS)
+		return bio->bi_next;
+	else
+		return NULL;
+}
+
+/*
+ * We maintain a biased count of active stripes in the bottom 16 bits of
+ * bi_phys_segments, and a count of processed stripes in the upper 16 bits
+ */
+static inline int raid5_bi_processed_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	return (atomic_read(segments) >> 16) & 0xffff;
+}
+
+static inline int raid5_dec_bi_active_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	return atomic_sub_return(1, segments) & 0xffff;
+}
+
+static inline void raid5_inc_bi_active_stripes(struct bio *bio)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	atomic_inc(segments);
+}
+
+static inline void raid5_set_bi_processed_stripes(struct bio *bio,
+	unsigned int cnt)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+	int old, new;
+
+	do {
+		old = atomic_read(segments);
+		new = (old & 0xffff) | (cnt << 16);
+	} while (atomic_cmpxchg(segments, old, new) != old);
+}
+
+static inline void raid5_set_bi_stripes(struct bio *bio, unsigned int cnt)
+{
+	atomic_t *segments = (atomic_t *)&bio->bi_phys_segments;
+
+	atomic_set(segments, cnt);
+}
+
 /* NOTE NR_STRIPE_HASH_LOCKS must remain below 64.
  * This is because we sometimes take all the spinlocks
  * and creating that much locking depth can cause

commit 29c6d1bbd7a2cd88a197ea7cef171f616e198526
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Aug 18 14:57:24 2016 +0200

    md/raid5: Convert to hotplug state machine
    
    Install the callbacks via the state machine and let the core invoke
    the callbacks on the already online CPUs.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Neil Brown <neilb@suse.com>
    Cc: linux-raid@vger.kernel.org
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160818125731.27256-10-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 517d4b68a1be..57ec49f0839e 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -512,9 +512,7 @@ struct r5conf {
 	} __percpu *percpu;
 	int scribble_disks;
 	int scribble_sectors;
-#ifdef CONFIG_HOTPLUG_CPU
-	struct notifier_block	cpu_notify;
-#endif
+	struct hlist_node node;
 
 	/*
 	 * Free stripes pool

commit 6ab2a4b806ae21b6c3e47c5ff1285ec06d505325
Author: Shaohua Li <shli@fb.com>
Date:   Thu Feb 25 16:24:42 2016 -0800

    RAID5: revert e9e4c377e2f563 to fix a livelock
    
    Revert commit
    e9e4c377e2f563(md/raid5: per hash value and exclusive wait_for_stripe)
    
    The problem is raid5_get_active_stripe waits on
    conf->wait_for_stripe[hash]. Assume hash is 0. My test release stripes
    in this order:
    - release all stripes with hash 0
    - raid5_get_active_stripe still sleeps since active_stripes >
      max_nr_stripes * 3 / 4
    - release all stripes with hash other than 0. active_stripes becomes 0
    - raid5_get_active_stripe still sleeps, since nobody wakes up
      wait_for_stripe[0]
    The system live locks. The problem is active_stripes isn't a per-hash
    count. Revert the patch makes the live lock go away.
    
    Cc: stable@vger.kernel.org (v4.2+)
    Cc: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Cc: NeilBrown <neilb@suse.de>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ae6068deefdf..517d4b68a1be 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -524,7 +524,7 @@ struct r5conf {
 	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_quiescent;
-	wait_queue_head_t	wait_for_stripe[NR_STRIPE_HASH_LOCKS];
+	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
 	unsigned long		cache_state;
 #define R5_INACTIVE_BLOCKED	1	/* release of inactive stripes blocked,

commit 27a353c026a879a1001e5eac4bda75b16262c44a
Author: Shaohua Li <shli@fb.com>
Date:   Wed Feb 24 17:38:28 2016 -0800

    RAID5: check_reshape() shouldn't call mddev_suspend
    
    check_reshape() is called from raid5d thread. raid5d thread shouldn't
    call mddev_suspend(), because mddev_suspend() waits for all IO finish
    but IO is handled in raid5d thread, we could easily deadlock here.
    
    This issue is introduced by
    738a273 ("md/raid5: fix allocation of 'scribble' array.")
    
    Cc: stable@vger.kernel.org (v4.1+)
    Reported-and-tested-by: Artur Paszkiewicz <artur.paszkiewicz@intel.com>
    Reviewed-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Shaohua Li <shli@fb.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a415e1cd39b8..ae6068deefdf 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -510,6 +510,8 @@ struct r5conf {
 					      * conversions
 					      */
 	} __percpu *percpu;
+	int scribble_disks;
+	int scribble_sectors;
 #ifdef CONFIG_HOTPLUG_CPU
 	struct notifier_block	cpu_notify;
 #endif

commit 6e74a9cfb5a55b0a4214809321b67d7065e55555
Author: Shaohua Li <shli@fb.com>
Date:   Thu Oct 8 21:54:08 2015 -0700

    raid5-cache: IO error handling
    
    There are 3 places the raid5-cache dispatches IO. The discard IO error
    doesn't matter, so we ignore it. The superblock write IO error can be
    handled in MD core. The remaining are log write and flush. When the IO
    error happens, we mark log disk faulty and fail all write IO. Read IO is
    still allowed to run. Userspace will get a notification too and
    corresponding daemon can choose setting raid array readonly for example.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1ab534c909fe..a415e1cd39b8 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -272,6 +272,7 @@ struct stripe_head_state {
 	struct bio_list return_bi;
 	struct md_rdev *blocked_rdev;
 	int handle_bad_blocks;
+	int log_failed;
 };
 
 /* Flags for struct r5dev.flags */
@@ -631,4 +632,5 @@ extern void r5l_flush_stripe_to_raid(struct r5l_log *log);
 extern void r5l_stripe_write_finished(struct stripe_head *sh);
 extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
 extern void r5l_quiesce(struct r5l_log *log, int state);
+extern bool r5l_log_disk_error(struct r5conf *conf);
 #endif

commit e6c033f79a0a1e9ca850575dcfa51bb583b592fa
Author: Shaohua Li <shli@fb.com>
Date:   Sun Oct 4 09:20:12 2015 -0700

    raid5-cache: move reclaim stop to quiesce
    
    Move reclaim stop to quiesce handling, where is safer for this stuff.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 32c8ce81248b..1ab534c909fe 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -630,4 +630,5 @@ extern void r5l_write_stripe_run(struct r5l_log *log);
 extern void r5l_flush_stripe_to_raid(struct r5l_log *log);
 extern void r5l_stripe_write_finished(struct stripe_head *sh);
 extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
+extern void r5l_quiesce(struct r5l_log *log, int state);
 #endif

commit 828cbe989e4f5c8666cb3d99918b03666ccde0a0
Author: Shaohua Li <shli@fb.com>
Date:   Wed Sep 2 13:49:49 2015 -0700

    raid5-cache: optimize FLUSH IO with log enabled
    
    With log enabled, bio is written to raid disks after the bio is settled
    down in log disk. The recovery guarantees we can recovery the bio data
    from log disk, so we we skip FLUSH IO.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1f16d437bfda..32c8ce81248b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -629,4 +629,5 @@ extern int r5l_write_stripe(struct r5l_log *log, struct stripe_head *head_sh);
 extern void r5l_write_stripe_run(struct r5l_log *log);
 extern void r5l_flush_stripe_to_raid(struct r5l_log *log);
 extern void r5l_stripe_write_finished(struct stripe_head *sh);
+extern int r5l_handle_flush_request(struct r5l_log *log, struct bio *bio);
 #endif

commit 0576b1c618ef220051a8555f2aa7dd316e88f330
Author: Shaohua Li <shli@fb.com>
Date:   Thu Aug 13 14:32:00 2015 -0700

    raid5: log reclaim support
    
    This is the reclaim support for raid5 log. A stripe write will have
    following steps:
    
    1. reconstruct the stripe, read data/calculate parity. ops_run_io
    prepares to write data/parity to raid disks
    2. hijack ops_run_io. stripe data/parity is appending to log disk
    3. flush log disk cache
    4. ops_run_io run again and do normal operation. stripe data/parity is
    written in raid array disks. raid core can return io to upper layer.
    5. flush cache of all raid array disks
    6. update super block
    7. log disk space used by the stripe can be reused
    
    In practice, several stripes consist of an io_unit and we will batch
    several io_unit in different steps, but the whole process doesn't
    change.
    
    It's possible io return just after data/parity hit log disk, but then
    read IO will need read from log disk. For simplicity, IO return happens
    at step 4, where read IO can directly read from raid disks.
    
    Currently reclaim run if there is specific reclaimable space (1/4 disk
    size or 10G) or we are out of space. Reclaim is just to free log disk
    spaces, it doesn't impact data consistency. The size based force reclaim
    is to make sure log isn't too big, so recovery doesn't scan log too
    much.
    
    Recovery make sure raid disks and log disk have the same data of a
    stripe. If crash happens before 4, recovery might/might not recovery
    stripe's data/parity depending on if data/parity and its checksum
    matches. In either case, this doesn't change the syntax of an IO write.
    After step 3, stripe is guaranteed recoverable, because stripe's
    data/parity is persistent in log disk. In some cases, log disk content
    and raid disks content of a stripe are the same, but recovery will still
    copy log disk content to raid disks, this doesn't impact data
    consistency. space reuse happens after superblock update and cache
    flush.
    
    There is one situation we want to avoid. A broken meta in the middle of
    a log causes recovery can't find meta at the head of log. If operations
    require meta at the head persistent in log, we must make sure meta
    before it persistent in log too. The case is stripe data/parity is in
    log and we start write stripe to raid disks (before step 4). stripe
    data/parity must be persistent in log before we do the write to raid
    disks. The solution is we restrictly maintain io_unit list order. In
    this case, we only write stripes of an io_unit to raid disks till the
    io_unit is the first one whose data/parity is in log.
    
    The io_unit list order is important for other cases too. For example,
    some io_unit are reclaimable and others not. They can be mixed in the
    list, we shouldn't reuse space of an unreclaimable io_unit.
    
    Includes fixes to problems which were...
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 87fae2b50d87..1f16d437bfda 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -627,4 +627,6 @@ extern int r5l_init_log(struct r5conf *conf, struct md_rdev *rdev);
 extern void r5l_exit_log(struct r5l_log *log);
 extern int r5l_write_stripe(struct r5l_log *log, struct stripe_head *head_sh);
 extern void r5l_write_stripe_run(struct r5l_log *log);
+extern void r5l_flush_stripe_to_raid(struct r5l_log *log);
+extern void r5l_stripe_write_finished(struct stripe_head *sh);
 #endif

commit f6bed0ef0a808164f51197de062e0450ce6c1f96
Author: Shaohua Li <shli@fb.com>
Date:   Thu Aug 13 14:31:59 2015 -0700

    raid5: add basic stripe log
    
    This introduces a simple log for raid5. Data/parity writing to raid
    array first writes to the log, then write to raid array disks. If
    crash happens, we can recovery data from the log. This can speed up
    raid resync and fix write hole issue.
    
    The log structure is pretty simple. Data/meta data is stored in block
    unit, which is 4k generally. It has only one type of meta data block.
    The meta data block can track 3 types of data, stripe data, stripe
    parity and flush block. MD superblock will point to the last valid
    meta data block. Each meta data block has checksum/seq number, so
    recovery can scan the log correctly. We store a checksum of stripe
    data/parity to the metadata block, so meta data and stripe data/parity
    can be written to log disk together. otherwise, meta data write must
    wait till stripe data/parity is finished.
    
    For stripe data, meta data block will record stripe data sector and
    size. Currently the size is always 4k. This meta data record can be made
    simpler if we just fix write hole (eg, we can record data of a stripe's
    different disks together), but this format can be extended to support
    caching in the future, which must record data address/size.
    
    For stripe parity, meta data block will record stripe sector. It's
    size should be 4k (for raid5) or 8k (for raid6). We always store p
    parity first. This format should work for caching too.
    
    flush block indicates a stripe is in raid array disks. Fixing write
    hole doesn't need this type of meta data, it's for caching extension.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a42c123d15d2..87fae2b50d87 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -223,6 +223,9 @@ struct stripe_head {
 	struct stripe_head	*batch_head; /* protected by stripe lock */
 	spinlock_t		batch_lock; /* only header's lock is useful */
 	struct list_head	batch_list; /* protected by head's batch lock*/
+
+	struct r5l_io_unit	*log_io;
+	struct list_head	log_list;
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
@@ -244,6 +247,7 @@ struct stripe_head {
 		struct bio	*toread, *read, *towrite, *written;
 		sector_t	sector;			/* sector of this page */
 		unsigned long	flags;
+		u32		log_checksum;
 	} dev[1]; /* allocated with extra space depending of RAID geometry */
 };
 
@@ -544,6 +548,7 @@ struct r5conf {
 	struct r5worker_group	*worker_groups;
 	int			group_cnt;
 	int			worker_cnt_per_group;
+	struct r5l_log		*log;
 };
 
 
@@ -618,4 +623,8 @@ extern sector_t raid5_compute_sector(struct r5conf *conf, sector_t r_sector,
 extern struct stripe_head *
 raid5_get_active_stripe(struct r5conf *conf, sector_t sector,
 			int previous, int noblock, int noquiesce);
+extern int r5l_init_log(struct r5conf *conf, struct md_rdev *rdev);
+extern void r5l_exit_log(struct r5l_log *log);
+extern int r5l_write_stripe(struct r5l_log *log, struct stripe_head *head_sh);
+extern void r5l_write_stripe_run(struct r5l_log *log);
 #endif

commit b70abcb24711d1327a8a505ab3e931c24cbab0a7
Author: Shaohua Li <shli@fb.com>
Date:   Thu Aug 13 14:31:58 2015 -0700

    raid5: add a new state for stripe log handling
    
    When a stripe finishes construction, we write the stripe to raid in
    ops_run_io normally. With log, we do a bunch of other operations before
    the stripe is written to raid. Mainly write the stripe to log disk,
    flush disk cache and so on. The operations are still driven by raid5d
    and run in the stripe state machine. We introduce a new state for such
    stripe (trapped into log). The stripe is in this state from the time it
    first enters ops_run_io (finish construction) to the time it is written
    to raid. Since we know the state is only for log, we bypass other
    check/operation in handle_stripe.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 7686fcb62157..a42c123d15d2 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -340,6 +340,7 @@ enum {
 	STRIPE_BITMAP_PENDING,	/* Being added to bitmap, don't add
 				 * to batch yet.
 				 */
+	STRIPE_LOG_TRAPPED, /* trapped into log */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAGS \

commit 6d036f7d52e5a9c3b2ff77883db4c34620681804
Author: Shaohua Li <shli@fb.com>
Date:   Thu Aug 13 14:31:57 2015 -0700

    raid5: export some functions
    
    Next several patches use some raid5 functions, rename them with raid5
    prefix and export out.
    
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 828c2925e68f..7686fcb62157 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -609,4 +609,12 @@ static inline int algorithm_is_DDF(int layout)
 
 extern void md_raid5_kick_device(struct r5conf *conf);
 extern int raid5_set_cache_size(struct mddev *mddev, int size);
+extern sector_t raid5_compute_blocknr(struct stripe_head *sh, int i, int previous);
+extern void raid5_release_stripe(struct stripe_head *sh);
+extern sector_t raid5_compute_sector(struct r5conf *conf, sector_t r_sector,
+				     int previous, int *dd_idx,
+				     struct stripe_head *sh);
+extern struct stripe_head *
+raid5_get_active_stripe(struct r5conf *conf, sector_t sector,
+			int previous, int noblock, int noquiesce);
 #endif

commit c3cce6cda162eb2b2960a85d9c8992f4f3be85d0
Author: NeilBrown <neilb@suse.com>
Date:   Fri Aug 14 12:47:33 2015 +1000

    md/raid5: ensure device failure recorded before write request returns.
    
    When a write to one of the devices of a RAID5/6 fails, the failure is
    recorded in the metadata of the other devices so that after a restart
    the data on the failed drive wont be trusted even if that drive seems
    to be working again (maybe a cable was unplugged).
    
    Similarly when we record a bad-block in response to a write failure,
    we must not let the write complete until the bad-block update is safe.
    
    Currently there is no interlock between the write request completing
    and the metadata update.  So it is possible that the write will
    complete, the app will confirm success in some way, and then the
    machine will crash before the metadata update completes.
    
    This is an extremely small hole for a racy to fit in, but it is
    theoretically possible and so should be closed.
    
    So:
     - set MD_CHANGE_PENDING when requesting a metadata update for a
       failed device, so we can know with certainty when it completes
     - queue requests that completed when MD_CHANGE_PENDING is set to
       only be processed after the metadata update completes
     - call raid_end_bio_io() on bios in that queue when the time comes.
    
    
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1de82a6e4c23..828c2925e68f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -476,6 +476,9 @@ struct r5conf {
 	int			skip_copy; /* Don't copy data from bio to stripe cache */
 	struct list_head	*last_hold; /* detect hold_list promotions */
 
+	/* bios to have bi_end_io called after metadata is synced */
+	struct bio_list		return_bi;
+
 	atomic_t		reshape_stripes; /* stripes with pending writes for reshape */
 	/* unfortunately we need two cache names as we temporarily have
 	 * two caches.

commit 34a6f80e1639b124f24b5fadc1d45d69417cbace
Author: NeilBrown <neilb@suse.com>
Date:   Fri Aug 14 12:07:57 2015 +1000

    md/raid5: use bio_list for the list of bios to return.
    
    This will make it easier to splice two lists together which will
    be needed in future patch.
    
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index d05144278690..1de82a6e4c23 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -265,7 +265,7 @@ struct stripe_head_state {
 	int dec_preread_active;
 	unsigned long ops_request;
 
-	struct bio *return_bi;
+	struct bio_list return_bi;
 	struct md_rdev *blocked_rdev;
 	int handle_bad_blocks;
 };

commit 2d5b569b665ea6d0b15c52529ff06300de81a7ce
Author: NeilBrown <neilb@suse.com>
Date:   Mon Jul 6 12:49:23 2015 +1000

    md/raid5: avoid races when changing cache size.
    
    Cache size can grow or shrink due to various pressures at
    any time.  So when we resize the cache as part of a 'grow'
    operation (i.e. change the size to allow more devices) we need
    to blocks that automatic growing/shrinking.
    
    So introduce a mutex.  auto grow/shrink uses mutex_trylock()
    and just doesn't bother if there is a blockage.
    Resizing the whole cache holds the mutex to ensure that
    the correct number of new stripes is allocated.
    
    This bug can result in some stripes not being freed when an
    array is stopped.  This leads to the kmem_cache not being
    freed and a subsequent array can try to use the same kmem_cache
    and get confused.
    
    Fixes: edbe83ab4c27 ("md/raid5: allow the stripe_cache to grow and shrink.")
    Cc: stable@vger.kernel.org (4.1 - please delay until 2 weeks after release of 4.2)
    Signed-off-by: NeilBrown <neilb@suse.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 02c3bf8fbfe7..d05144278690 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -482,7 +482,8 @@ struct r5conf {
 	 */
 	int			active_name;
 	char			cache_name[2][32];
-	struct kmem_cache		*slab_cache; /* for allocating stripes */
+	struct kmem_cache	*slab_cache; /* for allocating stripes */
+	struct mutex		cache_size_mutex; /* Protect changes to cache size */
 
 	int			seq_flush, seq_write;
 	int			quiesce;

commit e9e4c377e2f563892c50d1d093dd55c7d518fc3d
Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>
Date:   Fri May 8 18:19:07 2015 +1000

    md/raid5: per hash value and exclusive wait_for_stripe
    
    I noticed heavy spin lock contention at get_active_stripe() with fsmark
    multiple thread write workloads.
    
    Here is how this hot contention comes from. We have limited stripes, and
    it's a multiple thread write workload. Hence, those stripes will be taken
    soon, which puts later processes to sleep for waiting free stripes. When
    enough stripes(>= 1/4 total stripes) are released, all process are woken,
    trying to get the lock. But there is one only being able to get this lock
    for each hash lock, making other processes spinning out there for acquiring
    the lock.
    
    Thus, it's effectiveless to wakeup all processes and let them battle for
    a lock that permits one to access only each time. Instead, we could make
    it be a exclusive wake up: wake up one process only. That avoids the heavy
    spin lock contention naturally.
    
    To do the exclusive wake up, we've to split wait_for_stripe into multiple
    wait queues, to make it per hash value, just like the hash lock.
    
    Here are some test results I have got with this patch applied(all test run
    3 times):
    
    `fsmark.files_per_sec'
    =====================
    
    next-20150317                 this patch
    -------------------------     -------------------------
    metric_value     stddev      metric_value     stddev     change      testbox/benchmark/testcase-params
    -------------------------     -------------------------   --------     ------------------------------
          25.600     0.0              92.700     2.5          262.1%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-btrfs-4M-30G-fsyncBeforeClose
          25.600     0.0              77.800     0.6          203.9%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-btrfs-4M-30G-fsyncBeforeClose
          32.000     0.0              93.800     1.7          193.1%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-ext4-4M-30G-fsyncBeforeClose
          32.000     0.0              81.233     1.7          153.9%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-ext4-4M-30G-fsyncBeforeClose
          48.800     14.5             99.667     2.0          104.2%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-xfs-4M-30G-fsyncBeforeClose
           6.400     0.0              12.800     0.0          100.0%     ivb44/fsmark/1x-64t-3HDD-RAID5-btrfs-4M-40G-fsyncBeforeClose
          63.133     8.2              82.800     0.7           31.2%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-xfs-4M-30G-fsyncBeforeClose
         245.067     0.7             306.567     7.9           25.1%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-f2fs-4M-30G-fsyncBeforeClose
          17.533     0.3              21.000     0.8           19.8%     ivb44/fsmark/1x-1t-3HDD-RAID5-xfs-4M-40G-fsyncBeforeClose
         188.167     1.9             215.033     3.1           14.3%     ivb44/fsmark/1x-1t-4BRD_12G-RAID5-btrfs-4M-30G-NoSync
         254.500     1.8             290.733     2.4           14.2%     ivb44/fsmark/1x-1t-9BRD_6G-RAID5-btrfs-4M-30G-NoSync
    
    `time.system_time'
    =====================
    
    next-20150317                 this patch
    -------------------------    -------------------------
    metric_value     stddev     metric_value     stddev     change       testbox/benchmark/testcase-params
    -------------------------    -------------------------    --------     ------------------------------
        7235.603     1.2             185.163     1.9          -97.4%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-btrfs-4M-30G-fsyncBeforeClose
        7666.883     2.9             202.750     1.0          -97.4%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-btrfs-4M-30G-fsyncBeforeClose
       14567.893     0.7             421.230     0.4          -97.1%     ivb44/fsmark/1x-64t-3HDD-RAID5-btrfs-4M-40G-fsyncBeforeClose
        3697.667     14.0            148.190     1.7          -96.0%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-xfs-4M-30G-fsyncBeforeClose
        5572.867     3.8             310.717     1.4          -94.4%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-ext4-4M-30G-fsyncBeforeClose
        5565.050     0.5             313.277     1.5          -94.4%     ivb44/fsmark/1x-64t-4BRD_12G-RAID5-ext4-4M-30G-fsyncBeforeClose
        2420.707     17.1            171.043     2.7          -92.9%     ivb44/fsmark/1x-64t-9BRD_6G-RAID5-xfs-4M-30G-fsyncBeforeClose
        3743.300     4.6             379.827     3.5          -89.9%     ivb44/fsmark/1x-64t-3HDD-RAID5-ext4-4M-40G-fsyncBeforeClose
        3308.687     6.3             363.050     2.0          -89.0%     ivb44/fsmark/1x-64t-3HDD-RAID5-xfs-4M-40G-fsyncBeforeClose
    
    Where,
    
         1x: where 'x' means iterations or loop, corresponding to the 'L' option of fsmark
    
         1t, 64t: where 't' means thread
    
         4M: means the single file size, corresponding to the '-s' option of fsmark
         40G, 30G, 120G: means the total test size
    
         4BRD_12G: BRD is the ramdisk, where '4' means 4 ramdisk, and where '12G' means
                   the size of one ramdisk. So, it would be 48G in total. And we made a
                   raid on those ramdisk
    
    As you can see, though there are no much performance gain for hard disk
    workload, the system time is dropped heavily, up to 97%. And as expected,
    the performance increased a lot, up to 260%, for fast device(ram disk).
    
    v2: use bits instead of array to note down wait queue need to wake up.
    
    Signed-off-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 9b84b8820fc5..02c3bf8fbfe7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -512,7 +512,7 @@ struct r5conf {
 	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_quiescent;
-	wait_queue_head_t	wait_for_stripe;
+	wait_queue_head_t	wait_for_stripe[NR_STRIPE_HASH_LOCKS];
 	wait_queue_head_t	wait_for_overlap;
 	unsigned long		cache_state;
 #define R5_INACTIVE_BLOCKED	1	/* release of inactive stripes blocked,

commit b1b4648648e18775082858eca2517322f63e57a1
Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>
Date:   Fri May 8 18:19:06 2015 +1000

    md/raid5: split wait_for_stripe and introduce wait_for_quiescent
    
    I noticed heavy spin lock contention at get_active_stripe(), introduced
    at being wake up stage, where a bunch of processes try to re-hold the
    spin lock again.
    
    After giving some thoughts on this issue, I found the lock could be
    relieved(and even avoided) if we turn the wait_for_stripe to per
    waitqueue for each lock hash and make the wake up exclusive: wake up
    one process each time, which avoids the lock contention naturally.
    
    Before go hacking with wait_for_stripe, I found it actually has 2
    usages: for the array to enter or leave the quiescent state, and also
    to wait for an available stripe in each of the hash lists.
    
    So this patch splits the first usage off into a separate wait_queue,
    wait_for_quiescent, and the next patch will turn the second usage into
    one waitqueue for each hash value, and make it exclusive, to relieve
    the lock contention.
    
    v2: wake_up(wait_for_quiescent) when (active_stripes == 0)
        Commit log refactor suggestion from Neil.
    
    Signed-off-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 896d603ad0da..9b84b8820fc5 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -511,6 +511,7 @@ struct r5conf {
 	struct list_head	inactive_list[NR_STRIPE_HASH_LOCKS];
 	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;
+	wait_queue_head_t	wait_for_quiescent;
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
 	unsigned long		cache_state;

commit 1b956f7a8f9aa63ea9644ab8c3374cf381993363
Author: NeilBrown <neilb@suse.de>
Date:   Thu May 21 12:40:26 2015 +1000

    md/raid5: be more selective about distributing flags across batch.
    
    When a batch of stripes is broken up, we keep some of the flags
    that were per-stripe, and copy other flags from the head to all
    others.
    
    This only happens while a stripe is being handled, so many of the
    flags are irrelevant.
    
    The "SYNC_FLAGS" (which I've renamed to make it clear there are
    several) and STRIPE_DEGRADED are set per-stripe and so need to be
    preserved.  STRIPE_INSYNC is the only flag that is set on the head
    that needs to be propagated to all others.
    
    For safety, add a WARN_ON if others are set, except:
     STRIPE_HANDLE - this is safe and per-stripe and we are going to set
          in several cases anyway
     STRIPE_INSYNC
     STRIPE_IO_STARTED - this is just a hint and doesn't hurt.
     STRIPE_ON_PLUG_LIST
     STRIPE_ON_RELEASE_LIST - It is a point pointless for a batched
               stripe to be on one of these lists, but it can happen
               as can be safely ignored.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 01cdb9f3a0c4..896d603ad0da 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -342,7 +342,7 @@ enum {
 				 */
 };
 
-#define STRIPE_EXPAND_SYNC_FLAG \
+#define STRIPE_EXPAND_SYNC_FLAGS \
 	((1 << STRIPE_EXPAND_SOURCE) |\
 	(1 << STRIPE_EXPAND_READY) |\
 	(1 << STRIPE_EXPANDING) |\

commit d0852df543e5aa7db34c1ad26d053782bcbf48f1
Author: NeilBrown <neilb@suse.de>
Date:   Wed May 27 08:43:45 2015 +1000

    md/raid5: close race between STRIPE_BIT_DELAY and batching.
    
    When we add a write to a stripe we need to make sure the bitmap
    bit is set.  While doing that the stripe is not locked so it could
    be added to a batch after which further changes to STRIPE_BIT_DELAY
    and ->bm_seq are ineffective.
    
    So we need to hold off adding to a stripe until bitmap_startwrite has
    completed at least once, and we need to avoid further changes to
    STRIPE_BIT_DELAY once the stripe has been added to a batch.
    
    If a bitmap_startwrite() completes after the stripe was added to a
    batch, it will not have set the bit, only incremented a counter, so no
    extra delay of the stripe is needed.
    
    Reported-by: Shaohua Li <shli@kernel.org>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 7dc0dd86074b..01cdb9f3a0c4 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -337,6 +337,9 @@ enum {
 	STRIPE_ON_RELEASE_LIST,
 	STRIPE_BATCH_READY,
 	STRIPE_BATCH_ERR,
+	STRIPE_BITMAP_PENDING,	/* Being added to bitmap, don't add
+				 * to batch yet.
+				 */
 };
 
 #define STRIPE_EXPAND_SYNC_FLAG \

commit edbe83ab4c27ea6669eb57adb5ed7eaec1118ceb
Author: NeilBrown <neilb@suse.de>
Date:   Thu Feb 26 12:47:56 2015 +1100

    md/raid5: allow the stripe_cache to grow and shrink.
    
    The default setting of 256 stripe_heads is probably
    much too small for many configurations.  So it is best to make it
    auto-configure.
    
    Shrinking the cache under memory pressure is easy.  The only
    interesting part here is that we put a fairly high cost
    ('seeks') on shrinking the cache as the cost is greater than
    just having to read more data, it reduces parallelism.
    
    Growing the cache on demand needs to be done carefully.  If we allow
    fast growth, that can upset memory balance as lots of dirty memory can
    quickly turn into lots of memory queued in the stripe_cache.
    It is important for the raid5 block device to appear congested to
    allow write-throttling to work.
    
    So we only add stripes slowly. We set a flag when an allocation
    fails because all stripes are in use, allocate at a convenient
    time when that flag is set, and don't allow it to be set again
    until at least one stripe_head has been released for re-use.
    
    This means that a spurt of requests will only cause one stripe_head
    to be allocated, but a steady stream of requests will slowly
    increase the cache size - until memory pressure puts it back again.
    
    It could take hours to reach a steady state.
    
    The value written to, and displayed in, stripe_cache_size is
    used as a minimum.  The cache can grow above this and shrink back
    down to it.  The actual size is not directly visible, though it can
    be deduced to some extent by watching stripe_cache_active.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ebe4e24bc14d..7dc0dd86074b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -433,6 +433,7 @@ struct r5conf {
 	int			max_degraded;
 	int			raid_disks;
 	int			max_nr_stripes;
+	int			min_nr_stripes;
 
 	/* reshape_progress is the leading edge of a 'reshape'
 	 * It has value MaxSector when no reshape is happening
@@ -513,7 +514,15 @@ struct r5conf {
 #define R5_INACTIVE_BLOCKED	1	/* release of inactive stripes blocked,
 					 * waiting for 25% to be free
 					 */
-
+#define R5_ALLOC_MORE		2	/* It might help to allocate another
+					 * stripe.
+					 */
+#define R5_DID_ALLOC		4	/* A stripe was allocated, don't allocate
+					 * more until at least one has been
+					 * released.  This avoids flooding
+					 * the cache.
+					 */
+	struct shrinker		shrinker;
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
 	struct disk_info	*disks;

commit 5423399a84ee1d92d29d763029ed40e4905cf50f
Author: NeilBrown <neilb@suse.de>
Date:   Thu Feb 26 12:21:04 2015 +1100

    md/raid5: change ->inactive_blocked to a bit-flag.
    
    This allows us to easily add more (atomic) flags.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 6614ac5ffc0e..ebe4e24bc14d 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -509,9 +509,11 @@ struct r5conf {
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
-	int			inactive_blocked;	/* release of inactive stripes blocked,
-							 * waiting for 25% to be free
-							 */
+	unsigned long		cache_state;
+#define R5_INACTIVE_BLOCKED	1	/* release of inactive stripes blocked,
+					 * waiting for 25% to be free
+					 */
+
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
 	struct disk_info	*disks;
@@ -526,6 +528,7 @@ struct r5conf {
 	int			worker_cnt_per_group;
 };
 
+
 /*
  * Our supported algorithms
  */

commit d06f191f8ecaef4d524e765fdb455f96392fbd42
Author: Markus Stockhausen <stockhausen@collogia.de>
Date:   Mon Dec 15 12:57:05 2014 +1100

    md/raid5: introduce configuration option rmw_level
    
    Depending on the available coding we allow optimized rmw logic for write
    operations. To support easier testing this patch allows manual control
    of the rmw/rcw descision through the interface /sys/block/mdX/md/rmw_level.
    
    The configuration can handle three levels of control.
    
    rmw_level=0: Disable rmw for all RAID types. Hardware assisted P/Q
    calculation has no implementation path yet to factor in/out chunks of
    a syndrome. Enforcing this level can be benefical for slow CPUs with
    hardware syndrome support and fast SSDs.
    
    rmw_level=1: Estimate rmw IOs and rcw IOs. Execute rmw only if we will
    save IOs. This equals the "old" unpatched behaviour and will be the
    default.
    
    rmw_level=2: Execute rmw even if calculated IOs for rmw and rcw are
    equal. We might have higher CPU consumption because of calculating the
    parity twice but it can be benefical otherwise. E.g. RAID4 with fast
    dedicated parity disk/SSD. The option is implemented just to be
    forward-looking and will ONLY work with this patch!
    
    Signed-off-by: Markus Stockhausen <stockhausen@collogia.de>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 57fef9ba36fa..6614ac5ffc0e 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -362,6 +362,7 @@ enum {
 enum {
 	PARITY_DISABLE_RMW = 0,
 	PARITY_ENABLE_RMW,
+	PARITY_PREFER_RMW,
 };
 
 /*

commit 584acdd49cd2472ca0f5a06adbe979db82d0b4af
Author: Markus Stockhausen <stockhausen@collogia.de>
Date:   Mon Dec 15 12:57:05 2014 +1100

    md/raid5: activate raid6 rmw feature
    
    Glue it altogehter. The raid6 rmw path should work the same as the
    already existing raid5 logic. So emulate the prexor handling/flags
    and split functions as needed.
    
    1) Enable xor_syndrome() in the async layer.
    
    2) Split ops_run_prexor() into RAID4/5 and RAID6 logic. Xor the syndrome
    at the start of a rmw run as we did it before for the single parity.
    
    3) Take care of rmw run in ops_run_reconstruct6(). Again process only
    the changed pages to get syndrome back into sync.
    
    4) Enhance set_syndrome_sources() to fill NULL pages if we are in a rmw
    run. The lower layers will calculate start & end pages from that and
    call the xor_syndrome() correspondingly.
    
    5) Adapt the several places where we ignored Q handling up to now.
    
    Performance numbers for a single E5630 system with a mix of 10 7200k
    desktop/server disks. 300 seconds random write with 8 threads onto a
    3,2TB (10*400GB) RAID6 64K chunk without spare (group_thread_cnt=4)
    
    bsize   rmw_level=1   rmw_level=0   rmw_level=1   rmw_level=0
            skip_copy=1   skip_copy=1   skip_copy=0   skip_copy=0
       4K      115 KB/s      141 KB/s      165 KB/s      140 KB/s
       8K      225 KB/s      275 KB/s      324 KB/s      274 KB/s
      16K      434 KB/s      536 KB/s      640 KB/s      534 KB/s
      32K      751 KB/s    1,051 KB/s    1,234 KB/s    1,045 KB/s
      64K    1,339 KB/s    1,958 KB/s    2,282 KB/s    1,962 KB/s
     128K    2,673 KB/s    3,862 KB/s    4,113 KB/s    3,898 KB/s
     256K    7,685 KB/s    7,539 KB/s    7,557 KB/s    7,638 KB/s
     512K   19,556 KB/s   19,558 KB/s   19,652 KB/s   19,688 Kb/s
    
    Signed-off-by: Markus Stockhausen <stockhausen@collogia.de>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index ee65ed844d3f..57fef9ba36fa 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -355,6 +355,23 @@ enum {
 	STRIPE_OP_RECONSTRUCT,
 	STRIPE_OP_CHECK,
 };
+
+/*
+ * RAID parity calculation preferences
+ */
+enum {
+	PARITY_DISABLE_RMW = 0,
+	PARITY_ENABLE_RMW,
+};
+
+/*
+ * Pages requested from set_syndrome_sources()
+ */
+enum {
+	SYNDROME_SRC_ALL,
+	SYNDROME_SRC_WANT_DRAIN,
+	SYNDROME_SRC_WRITTEN,
+};
 /*
  * Plugging:
  *
@@ -411,7 +428,7 @@ struct r5conf {
 	spinlock_t		hash_locks[NR_STRIPE_HASH_LOCKS];
 	struct mddev		*mddev;
 	int			chunk_sectors;
-	int			level, algorithm;
+	int			level, algorithm, rmw_level;
 	int			max_degraded;
 	int			raid_disks;
 	int			max_nr_stripes;

commit dabc4ec6ba72418ebca6bf1884f344bba40c8709
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:04 2014 +1100

    raid5: handle expansion/resync case with stripe batching
    
    expansion/resync can grab a stripe when the stripe is in batch list. Since all
    stripes in batch list must be in the same state, we can't allow some stripes
    run into expansion/resync. So we delay expansion/resync for stripe in batch
    list.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cf3562e99440..ee65ed844d3f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -339,6 +339,11 @@ enum {
 	STRIPE_BATCH_ERR,
 };
 
+#define STRIPE_EXPAND_SYNC_FLAG \
+	((1 << STRIPE_EXPAND_SOURCE) |\
+	(1 << STRIPE_EXPAND_READY) |\
+	(1 << STRIPE_EXPANDING) |\
+	(1 << STRIPE_SYNC_REQUESTED))
 /*
  * Operation request flags
  */

commit 72ac733015bbdc0356ba3e92c52137a265910a91
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:03 2014 +1100

    raid5: handle io error of batch list
    
    If io error happens in any stripe of a batch list, the batch list will be
    split, then normal process will run for the stripes in the list.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c8d0004dca8f..cf3562e99440 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -336,6 +336,7 @@ enum {
 	STRIPE_DISCARD,
 	STRIPE_ON_RELEASE_LIST,
 	STRIPE_BATCH_READY,
+	STRIPE_BATCH_ERR,
 };
 
 /*

commit 59fc630b8b5f9f21c8ce3ba153341c107dce1b0c
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:03 2014 +1100

    RAID5: batch adjacent full stripe write
    
    stripe cache is 4k size. Even adjacent full stripe writes are handled in 4k
    unit. Idealy we should use big size for adjacent full stripe writes. Bigger
    stripe cache size means less stripes runing in the state machine so can reduce
    cpu overhead. And also bigger size can cause bigger IO size dispatched to under
    layer disks.
    
    With below patch, we will automatically batch adjacent full stripe write
    together. Such stripes will be added to the batch list. Only the first stripe
    of the list will be put to handle_list and so run handle_stripe(). Some steps
    of handle_stripe() are extended to cover all stripes of the list, including
    ops_run_io, ops_run_biodrain and so on. With this patch, we have less stripes
    running in handle_stripe() and we send IO of whole stripe list together to
    increase IO size.
    
    Stripes added to a batch list have some limitations. A batch list can only
    include full stripe write and can't cross chunk boundary to make sure stripes
    have the same parity disks. Stripes in a batch list must be in the same state
    (no written, toread and so on). If a stripe is in a batch list, all new
    read/write to add_stripe_bio will be blocked to overlap conflict till the batch
    list is handled. The limitations will make sure stripes in a batch list be in
    exactly the same state in the life circly.
    
    I did test running 160k randwrite in a RAID5 array with 32k chunk size and 6
    PCIe SSD. This patch improves around 30% performance and IO size to under layer
    disk is exactly 32k. I also run a 4k randwrite test in the same array to make
    sure the performance isn't changed with the patch.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 4cc1a48127c7..c8d0004dca8f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -219,6 +219,10 @@ struct stripe_head {
 	spinlock_t		stripe_lock;
 	int			cpu;
 	struct r5worker_group	*group;
+
+	struct stripe_head	*batch_head; /* protected by stripe lock */
+	spinlock_t		batch_lock; /* only header's lock is useful */
+	struct list_head	batch_list; /* protected by head's batch lock*/
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target

commit 7a87f43405e91ca12b8770eb689dd9886f217091
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:03 2014 +1100

    raid5: track overwrite disk count
    
    Track overwrite disk count, so we can know if a stripe is a full stripe write.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 37644e3d5293..4cc1a48127c7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -210,6 +210,10 @@ struct stripe_head {
 	atomic_t		count;	      /* nr of active thread/requests */
 	int			bm_seq;	/* sequence number for bitmap flushes */
 	int			disks;		/* disks in stripe */
+	int			overwrite_disks; /* total overwrite disks in stripe,
+						  * this is only checked when stripe
+						  * has STRIPE_BATCH_READY
+						  */
 	enum check_states	check_state;
 	enum reconstruct_states reconstruct_state;
 	spinlock_t		stripe_lock;

commit da41ba65972532a04f73927c903029a7ec3bc2ed
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:03 2014 +1100

    raid5: add a new flag to track if a stripe can be batched
    
    A freshly new stripe with write request can be batched. Any time the stripe is
    handled or new read is queued, the flag will be cleared.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1d0f241d7d3b..37644e3d5293 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -327,6 +327,7 @@ enum {
 	STRIPE_ON_UNPLUG_LIST,
 	STRIPE_DISCARD,
 	STRIPE_ON_RELEASE_LIST,
+	STRIPE_BATCH_READY,
 };
 
 /*

commit 46d5b785621ad10a373e292f9101ccfc626466e0
Author: shli@kernel.org <shli@kernel.org>
Date:   Mon Dec 15 12:57:02 2014 +1100

    raid5: use flex_array for scribble data
    
    Use flex_array for scribble data. Next patch will batch several stripes
    together, so scribble data should be able to cover several stripes, so this
    patch also allocates scribble data for stripes across a chunk.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 983e18a83db1..1d0f241d7d3b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -458,15 +458,11 @@ struct r5conf {
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
-		void		*scribble;   /* space for constructing buffer
+		struct flex_array *scribble;   /* space for constructing buffer
 					      * lists and performing address
 					      * conversions
 					      */
 	} __percpu *percpu;
-	size_t			scribble_len; /* size of scribble region must be
-					       * associated with conf to handle
-					       * cpu hotplug while reshaping
-					       */
 #ifdef CONFIG_HOTPLUG_CPU
 	struct notifier_block	cpu_notify;
 #endif

commit 5c675f83c68fbdf9c0e103c1090b06be747fa62c
Author: NeilBrown <neilb@suse.de>
Date:   Mon Dec 15 12:56:56 2014 +1100

    md: make ->congested robust against personality changes.
    
    There is currently no locking around calls to the 'congested'
    bdi function.  If called at an awkward time while an array is
    being converted from one level (or personality) to another, there
    is a tiny chance of running code in an unreferenced module etc.
    
    So add a 'congested' function to the md_personality operations
    structure, and call it with appropriate locking from a central
    'mddev_congested'.
    
    When the array personality is changing the array will be 'suspended'
    so no IO is processed.
    If mddev_congested detects this, it simply reports that the
    array is congested, which is a safe guess.
    As mddev_suspend calls synchronize_rcu(), mddev_congested can
    avoid races by included the whole call inside an rcu_read_lock()
    region.
    This require that the congested functions for all subordinate devices
    can be run under rcu_lock.  Fortunately this is the case.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index d59f5ca743cd..983e18a83db1 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -558,7 +558,6 @@ static inline int algorithm_is_DDF(int layout)
 	return layout >= 8 && layout <= 10;
 }
 
-extern int md_raid5_congested(struct mddev *mddev, int bits);
 extern void md_raid5_kick_device(struct r5conf *conf);
 extern int raid5_set_cache_size(struct mddev *mddev, int size);
 #endif

commit f72ffdd68616e3697bc782b21c82197aeb480fd5
Author: NeilBrown <neilb@suse.de>
Date:   Tue Sep 30 14:23:59 2014 +1000

    md: remove unwanted white space from md.c
    
    My editor shows much of this is RED.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index bc72cd4be5f8..d59f5ca743cd 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -155,7 +155,7 @@
  */
 
 /*
- * Operations state - intermediate states that are visible outside of 
+ * Operations state - intermediate states that are visible outside of
  *   STRIPE_ACTIVE.
  * In general _idle indicates nothing is running, _run indicates a data
  * processing operation is active, and _result means the data processing result
@@ -364,7 +364,6 @@ enum {
  * HANDLE gets cleared if stripe_handle leaves nothing locked.
  */
 
-
 struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 };
@@ -528,7 +527,6 @@ struct r5conf {
 #define ALGORITHM_ROTATING_N_RESTART	9 /* DDF PRL=6 RLQ=2 */
 #define ALGORITHM_ROTATING_N_CONTINUE	10 /*DDF PRL=6 RLQ=3 */
 
-
 /* For every RAID5 algorithm we define a RAID6 algorithm
  * with exactly the same layout for data and parity, and
  * with the Q block always on the last device (N-1).

commit d592a9969141e67a3874c808999a4db4bf82ed83
Author: Shaohua Li <shli@kernel.org>
Date:   Wed May 21 17:57:44 2014 +0800

    raid5: add an option to avoid copy data from bio to stripe cache
    
    The stripe cache has two goals:
    1. cache data, so next time if data can be found in stripe cache, disk access
    can be avoided.
    2. stable data. data is copied from bio to stripe cache and calculated parity.
    data written to disk is from stripe cache, so if upper layer changes bio data,
    data written to disk isn't impacted.
    
    In my environment, I can guarantee 2 will not happen. And BDI_CAP_STABLE_WRITES
    can guarantee 2 too. For 1, it's not common too. block plug mechanism will
    dispatch a bunch of sequentail small requests together. And since I'm using
    SSD, I'm using small chunk size. It's rare case stripe cache is really useful.
    
    So I'd like to avoid the copy from bio to stripe cache and it's very helpful
    for performance. In my 1M randwrite tests, avoid the copy can increase the
    performance more than 30%.
    
    Of course, this shouldn't be enabled by default. It's reported enabling
    BDI_CAP_STABLE_WRITES can harm some workloads before, so I added an option to
    control it.
    
    Neilb:
      changed BUG_ON to WARN_ON
      Removed some assignments from raid5_build_block which are now not needed.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 01ad8ae8f578..bc72cd4be5f8 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -232,7 +232,7 @@ struct stripe_head {
 		 */
 		struct bio	req, rreq;
 		struct bio_vec	vec, rvec;
-		struct page	*page;
+		struct page	*page, *orig_page;
 		struct bio	*toread, *read, *towrite, *written;
 		sector_t	sector;			/* sector of this page */
 		unsigned long	flags;
@@ -299,6 +299,7 @@ enum r5dev_flags {
 			 * data in, and now is a good time to write it out.
 			 */
 	R5_Discard,	/* Discard the stripe */
+	R5_SkipCopy,	/* Don't copy data from bio to stripe cache */
 };
 
 /*
@@ -436,6 +437,7 @@ struct r5conf {
 	atomic_t		pending_full_writes; /* full write backlog */
 	int			bypass_count; /* bypassed prereads */
 	int			bypass_threshold; /* preread nice */
+	int			skip_copy; /* Don't copy data from bio to stripe cache */
 	struct list_head	*last_hold; /* detect hold_list promotions */
 
 	atomic_t		reshape_stripes; /* stripes with pending writes for reshape */

commit 6d6e352c80f22c446d933ca8103e02bac1f09129
Merge: b4789b8e6be3 60aaf9338545
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 20 13:05:25 2013 -0800

    Merge tag 'md/3.13' of git://neil.brown.name/md
    
    Pull md update from Neil Brown:
     "Mostly optimisations and obscure bug fixes.
       - raid5 gets less lock contention
       - raid1 gets less contention between normal-io and resync-io during
         resync"
    
    * tag 'md/3.13' of git://neil.brown.name/md:
      md/raid5: Use conf->device_lock protect changing of multi-thread resources.
      md/raid5: Before freeing old multi-thread worker, it should flush them.
      md/raid5: For stripe with R5_ReadNoMerge, we replace REQ_FLUSH with REQ_NOMERGE.
      UAPI: include <asm/byteorder.h> in linux/raid/md_p.h
      raid1: Rewrite the implementation of iobarrier.
      raid1: Add some macros to make code clearly.
      raid1: Replace raise_barrier/lower_barrier with freeze_array/unfreeze_array when reconfiguring the array.
      raid1: Add a field array_frozen to indicate whether raid in freeze state.
      md: Convert use of typedef ctl_table to struct ctl_table
      md/raid5: avoid deadlock when raid5 array has unack badblocks during md_stop_writes.
      md: use MD_RECOVERY_INTR instead of kthread_should_stop in resync thread.
      md: fix some places where mddev_lock return value is not checked.
      raid5: Retry R5_ReadNoMerge flag when hit a read error.
      raid5: relieve lock contention in get_active_stripe()
      raid5: relieve lock contention in get_active_stripe()
      wait: add wait_event_cmd()
      md/raid5.c: add proper locking to error path of raid5_start_reshape.
      md: fix calculation of stacking limits on level change.
      raid5: Use slow_path to release stripe when mddev->thread is null

commit 4bda556aea1d2916260326c6afa77b84a1f1345a
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Nov 14 15:16:17 2013 +1100

    raid5: relieve lock contention in get_active_stripe()
    
    track empty inactive list count, so md_raid5_congested() can use it to make
    decision.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a9e443a1116f..e4407388670a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -475,6 +475,7 @@ struct r5conf {
 	 */
 	atomic_t		active_stripes;
 	struct list_head	inactive_list[NR_STRIPE_HASH_LOCKS];
+	atomic_t		empty_inactive_list_nr;
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;

commit 9073e1a804c3096eda84ee7cbf11d1f174236c75
Merge: 4937e2a6f939 2bb9936beac2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 16:47:22 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree updates from Jiri Kosina:
     "Usual earth-shaking, news-breaking, rocket science pile from
      trivial.git"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (23 commits)
      doc: usb: Fix typo in Documentation/usb/gadget_configs.txt
      doc: add missing files to timers/00-INDEX
      timekeeping: Fix some trivial typos in comments
      mm: Fix some trivial typos in comments
      irq: Fix some trivial typos in comments
      NUMA: fix typos in Kconfig help text
      mm: update 00-INDEX
      doc: Documentation/DMA-attributes.txt fix typo
      DRM: comment: `halve' -> `half'
      Docs: Kconfig: `devlopers' -> `developers'
      doc: typo on word accounting in kprobes.c in mutliple architectures
      treewide: fix "usefull" typo
      treewide: fix "distingush" typo
      mm/Kconfig: Grammar s/an/a/
      kexec: Typo s/the/then/
      Documentation/kvm: Update cpuid documentation for steal time and pv eoi
      treewide: Fix common typo in "identify"
      __page_to_pfn: Fix typo in comment
      Correct some typos for word frequency
      clk: fixed-factor: Fix a trivial typo
      ...

commit 566c09c53455d7c4f1130928ef8071da1a24ea65
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Nov 14 15:16:17 2013 +1100

    raid5: relieve lock contention in get_active_stripe()
    
    get_active_stripe() is the last place we have lock contention. It has two
    paths. One is stripe isn't found and new stripe is allocated, the other is
    stripe is found.
    
    The first path basically calls __find_stripe and init_stripe. It accesses
    conf->generation, conf->previous_raid_disks, conf->raid_disks,
    conf->prev_chunk_sectors, conf->chunk_sectors, conf->max_degraded,
    conf->prev_algo, conf->algorithm, the stripe_hashtbl and inactive_list. Except
    stripe_hashtbl and inactive_list, other fields are changed very rarely.
    
    With this patch, we split inactive_list and add new hash locks. Each free
    stripe belongs to a specific inactive list. Which inactive list is determined
    by stripe's lock_hash. Note, even a stripe hasn't a sector assigned, it has a
    lock_hash assigned. Stripe's inactive list is protected by a hash lock, which
    is determined by it's lock_hash too. The lock_hash is derivied from current
    stripe_hashtbl hash, which guarantees any stripe_hashtbl list will be assigned
    to a specific lock_hash, so we can use new hash lock to protect stripe_hashtbl
    list too. The goal of the new hash locks introduced is we can only use the new
    locks in the first path of get_active_stripe(). Since we have several hash
    locks, lock contention is relieved significantly.
    
    The first path of get_active_stripe() accesses other fields, since they are
    changed rarely, changing them now need take conf->device_lock and all hash
    locks. For a slow path, this isn't a problem.
    
    If we need lock device_lock and hash lock, we always lock hash lock first. The
    tricky part is release_stripe and friends. We need take device_lock first.
    Neil's suggestion is we put inactive stripes to a temporary list and readd it
    to inactive_list after device_lock is released. In this way, we add stripes to
    temporary list with device_lock hold and remove stripes from the list with hash
    lock hold. So we don't allow concurrent access to the temporary list, which
    means we need allocate temporary list for all participants of release_stripe.
    
    One downside is free stripes are maintained in their inactive list, they can't
    across between the lists. By default, we have total 256 stripes and 8 lists, so
    each list will have 32 stripes. It's possible one list has free stripe but
    other list hasn't. The chance should be rare because stripes allocation are
    even distributed. And we can always allocate more stripes for cache, several
    mega bytes memory isn't a big deal.
    
    This completely removes the lock contention of the first path of
    get_active_stripe(). It slows down the second code path a little bit though
    because we now need takes two locks, but since the hash lock isn't contended,
    the overhead should be quite small (several atomic instructions). The second
    path of get_active_stripe() (basically sequential write or big request size
    randwrite) still has lock contentions.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2113ffa82c7a..a9e443a1116f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -205,6 +205,7 @@ struct stripe_head {
 	short			pd_idx;		/* parity disk index */
 	short			qd_idx;		/* 'Q' disk index for raid6 */
 	short			ddf_layout;/* use DDF ordering to calculate Q */
+	short			hash_lock_index;
 	unsigned long		state;		/* state flags */
 	atomic_t		count;	      /* nr of active thread/requests */
 	int			bm_seq;	/* sequence number for bitmap flushes */
@@ -367,9 +368,18 @@ struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 };
 
+/* NOTE NR_STRIPE_HASH_LOCKS must remain below 64.
+ * This is because we sometimes take all the spinlocks
+ * and creating that much locking depth can cause
+ * problems.
+ */
+#define NR_STRIPE_HASH_LOCKS 8
+#define STRIPE_HASH_LOCKS_MASK (NR_STRIPE_HASH_LOCKS - 1)
+
 struct r5worker {
 	struct work_struct work;
 	struct r5worker_group *group;
+	struct list_head temp_inactive_list[NR_STRIPE_HASH_LOCKS];
 	bool working;
 };
 
@@ -382,6 +392,8 @@ struct r5worker_group {
 
 struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
+	/* only protect corresponding hash list and inactive_list */
+	spinlock_t		hash_locks[NR_STRIPE_HASH_LOCKS];
 	struct mddev		*mddev;
 	int			chunk_sectors;
 	int			level, algorithm;
@@ -462,7 +474,7 @@ struct r5conf {
 	 * Free stripes pool
 	 */
 	atomic_t		active_stripes;
-	struct list_head	inactive_list;
+	struct list_head	inactive_list[NR_STRIPE_HASH_LOCKS];
 	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
@@ -477,6 +489,7 @@ struct r5conf {
 	 * the new thread here until we fully activate the array.
 	 */
 	struct md_thread	*thread;
+	struct list_head	temp_inactive_list[NR_STRIPE_HASH_LOCKS];
 	struct r5worker_group	*worker_groups;
 	int			group_cnt;
 	int			worker_cnt_per_group;

commit aa5e5dc2a8878ecf1a94819d889939023fd576c9
Author: Michael Opdenacker <michael.opdenacker@free-electrons.com>
Date:   Wed Sep 18 06:00:43 2013 +0200

    treewide: fix "distingush" typo
    
    Signed-off-by: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 70c49329ca9a..5c9797c7bbe0 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -49,7 +49,7 @@
  * can't distinguish between a clean block that has been generated
  * from parity calculations, and a clean block that has been
  * successfully written to the spare ( or to parity when resyncing).
- * To distingush these states we have a stripe bit STRIPE_INSYNC that
+ * To distinguish these states we have a stripe bit STRIPE_INSYNC that
  * is set whenever a write is scheduled to the spare, or to the parity
  * disc if there is no spare.  A sync request clears this bit, and
  * when we find it set with no buffers locked, we know the sync is

commit bfc90cb0936f5b972706625f38f72c7cb726c20a
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Aug 29 15:40:32 2013 +0800

    raid5: only wakeup necessary threads
    
    If there are not enough stripes to handle, we'd better not always
    queue all available work_structs. If one worker can only handle small
    or even none stripes, it will impact request merge and create lock
    contention.
    
    With this patch, the number of work_struct running will depend on
    pending stripes number. Note: some statistics info used in the patch
    are accessed without locking protection. This should doesn't matter,
    we just try best to avoid queue unnecessary work_struct.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 435b12d58165..2113ffa82c7a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -213,6 +213,7 @@ struct stripe_head {
 	enum reconstruct_states reconstruct_state;
 	spinlock_t		stripe_lock;
 	int			cpu;
+	struct r5worker_group	*group;
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
@@ -369,12 +370,14 @@ struct disk_info {
 struct r5worker {
 	struct work_struct work;
 	struct r5worker_group *group;
+	bool working;
 };
 
 struct r5worker_group {
 	struct list_head handle_list;
 	struct r5conf *conf;
 	struct r5worker *workers;
+	int stripes_cnt;
 };
 
 struct r5conf {

commit c46501b2deaa06efcaaf82917281941f02c6b307
Author: NeilBrown <neilb@suse.de>
Date:   Tue Aug 27 15:52:13 2013 +1000

    md/raid5: use seqcount to protect access to shape in make_request.
    
    make_request() access various shape parameters (raid_disks, chunk_size
    etc) which might be changed by raid5_start_reshape().
    
    If the later is called at and awkward time during the form, the wrong
    stripe_head might be used.
    
    So introduce a 'seqcount' and after finding a stripe_head make sure
    there is no reason to expect that we got the wrong one.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 105366371fbf..435b12d58165 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -400,6 +400,7 @@ struct r5conf {
 	int			prev_chunk_sectors;
 	int			prev_algo;
 	short			generation; /* increments with every reshape */
+	seqcount_t		gen_lock;	/* lock against generation changes */
 	unsigned long		reshape_checkpoint; /* Time we last updated
 						     * metadata */
 	long long		min_offset_diff; /* minimum difference between

commit 851c30c9badfc6b294c98e887624bff53644ad21
Author: Shaohua Li <shli@kernel.org>
Date:   Wed Aug 28 14:30:16 2013 +0800

    raid5: offload stripe handle to workqueue
    
    This is another attempt to create multiple threads to handle raid5 stripes.
    This time I use workqueue.
    
    raid5 handles request (especially write) in stripe unit. A stripe is page size
    aligned/long and acrosses all disks. Writing to any disk sector, raid5 runs a
    state machine for the corresponding stripe, which includes reading some disks
    of the stripe, calculating parity, and writing some disks of the stripe. The
    state machine is running in raid5d thread currently. Since there is only one
    thread, it doesn't scale well for high speed storage. An obvious solution is
    multi-threading.
    
    To get better performance, we have some requirements:
    a. locality. stripe corresponding to request submitted from one cpu is better
    handled in thread in local cpu or local node. local cpu is preferred but some
    times could be a bottleneck, for example, parity calculation is too heavy.
    local node running has wide adaptability.
    b. configurablity. Different setup of raid5 array might need diffent
    configuration. Especially the thread number. More threads don't always mean
    better performance because of lock contentions.
    
    My original implementation is creating some kernel threads. There are
    interfaces to control which cpu's stripe each thread should handle. And
    userspace can set affinity of the threads. This provides biggest flexibility
    and configurability. But it's hard to use and apparently a new thread pool
    implementation is disfavor.
    
    Recent workqueue improvement is quite promising. unbound workqueue will be
    bound to numa node. If WQ_SYSFS is set in workqueue, there are sysfs option to
    do affinity setting. For example, we can only include one HT sibling in
    affinity. Since work is non-reentrant by default, and we can control running
    thread number by limiting dispatched work_struct number.
    
    In this patch, I created several stripe worker group. A group is a numa node.
    stripes from cpus of one node will be added to a group list. Workqueue thread
    of one node will only handle stripes of worker group of the node. In this way,
    stripe handling has numa node locality. And as I said, we can control thread
    number by limiting dispatched work_struct number.
    
    The work_struct callback function handles several stripes in one run. A typical
    work queue usage is to run one unit in each work_struct. In raid5 case, the
    unit is a stripe. But we can't do that:
    a. Though handling a stripe doesn't need lock because of reference accounting
    and stripe isn't in any list, queuing a work_struct for each stripe will make
    workqueue lock contended very heavily.
    b. blk_start_plug()/blk_finish_plug() should surround stripe handle, as we
    might dispatch request. If each work_struct only handles one stripe, such block
    plug is meaningless.
    
    This implementation can't do very fine grained configuration. But the numa
    binding is most popular usage model, should be enough for most workloads.
    
    Note: since we have only one stripe queue, switching to multi-thread might
    decrease request size dispatching down to low level layer. The impact depends
    on thread number, raid configuration and workload. So multi-thread raid5 might
    not be proper for all setups.
    
    Changes V1 -> V2:
    1. remove WQ_NON_REENTRANT
    2. disabling multi-threading by default
    3. Add more descriptions in changelog
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a98f99d2a58f..105366371fbf 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -212,6 +212,7 @@ struct stripe_head {
 	enum check_states	check_state;
 	enum reconstruct_states reconstruct_state;
 	spinlock_t		stripe_lock;
+	int			cpu;
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
@@ -365,6 +366,17 @@ struct disk_info {
 	struct md_rdev	*rdev, *replacement;
 };
 
+struct r5worker {
+	struct work_struct work;
+	struct r5worker_group *group;
+};
+
+struct r5worker_group {
+	struct list_head handle_list;
+	struct r5conf *conf;
+	struct r5worker *workers;
+};
+
 struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
 	struct mddev		*mddev;
@@ -461,6 +473,9 @@ struct r5conf {
 	 * the new thread here until we fully activate the array.
 	 */
 	struct md_thread	*thread;
+	struct r5worker_group	*worker_groups;
+	int			group_cnt;
+	int			worker_cnt_per_group;
 };
 
 /*

commit 773ca82fa1ee58dd1bf88b6a5ca385ec83a2cac6
Author: Shaohua Li <shli@kernel.org>
Date:   Tue Aug 27 17:50:39 2013 +0800

    raid5: make release_stripe lockless
    
    release_stripe still has big lock contention. We just add the stripe to a llist
    without taking device_lock. We let the raid5d thread to do the real stripe
    release, which must hold device_lock anyway. In this way, release_stripe
    doesn't hold any locks.
    
    The side effect is the released stripes order is changed. But sounds not a big
    deal, stripes are never handled in order. And I thought block layer can already
    do nice request merge, which means order isn't that important.
    
    I kept the unplug release batch, which is unnecessary with this patch from lock
    contention avoid point of view, and actually if we delete it, the stripe_head
    release_list and lru can share storage. But the unplug release batch is also
    helpful for request merge. We probably can delay wakeup raid5d till unplug, but
    I'm still afraid of the case which raid5d is running.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 70c49329ca9a..a98f99d2a58f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -197,6 +197,7 @@ enum reconstruct_states {
 struct stripe_head {
 	struct hlist_node	hash;
 	struct list_head	lru;	      /* inactive_list or handle_list */
+	struct llist_node	release_list;
 	struct r5conf		*raid_conf;
 	short			generation;	/* increments with every
 						 * reshape */
@@ -321,6 +322,7 @@ enum {
 	STRIPE_OPS_REQ_PENDING,
 	STRIPE_ON_UNPLUG_LIST,
 	STRIPE_DISCARD,
+	STRIPE_ON_RELEASE_LIST,
 };
 
 /*
@@ -445,6 +447,7 @@ struct r5conf {
 	 */
 	atomic_t		active_stripes;
 	struct list_head	inactive_list;
+	struct llist_head	released_stripes;
 	wait_queue_head_t	wait_for_stripe;
 	wait_queue_head_t	wait_for_overlap;
 	int			inactive_blocked;	/* release of inactive stripes blocked,

commit f94c0b6658c7edea8bc19d13be321e3860a3fa54
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jul 22 12:57:21 2013 +1000

    md/raid5: fix interaction of 'replace' and 'recovery'.
    
    If a device in a RAID4/5/6 is being replaced while another is being
    recovered, then the writes to the replacement device currently don't
    happen, resulting in corruption when the replacement completes and the
    new drive takes over.
    
    This is because the replacement writes are only triggered when
    's.replacing' is set and not when the similar 's.sync' is set (which
    is the case during resync and recovery - it means all devices need to
    be read).
    
    So schedule those writes when s.replacing is set as well.
    
    In this case we cannot use "STRIPE_INSYNC" to record that the
    replacement has happened as that is needed for recording that any
    parity calculation is complete.  So introduce STRIPE_REPLACED to
    record if the replacement has happened.
    
    For safety we should also check that STRIPE_COMPUTE_RUN is not set.
    This has a similar effect to the "s.locked == 0" test.  The latter
    ensure that now IO has been flagged but not started.  The former
    checks if any parity calculation has been flagged by not started.
    We must wait for both of these to complete before triggering the
    'replace'.
    
    Add a similar test to the subsequent check for "are we finished yet".
    This possibly isn't needed (is subsumed in the STRIPE_INSYNC test),
    but it makes it more obvious that the REPLACE will happen before we
    think we are finished.
    
    Finally if a NeedReplace device is not UPTODATE then that is an
    error.  We really must trigger a warning.
    
    This bug was introduced in commit 9a3e1101b827a59ac9036a672f5fa8d5279d0fe2
    (md/raid5:  detect and handle replacements during recovery.)
    which introduced replacement for raid5.
    That was in 3.3-rc3, so any stable kernel since then would benefit
    from this fix.
    
    Cc: stable@vger.kernel.org (3.3+)
    Reported-by: qindehua <13691222965@163.com>
    Tested-by: qindehua <qindehua@163.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index b0b663b119a8..70c49329ca9a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -306,6 +306,7 @@ enum {
 	STRIPE_SYNC_REQUESTED,
 	STRIPE_SYNCING,
 	STRIPE_INSYNC,
+	STRIPE_REPLACED,
 	STRIPE_PREREAD_ACTIVE,
 	STRIPE_DELAYED,
 	STRIPE_DEGRADED,

commit 238f5908bd48f9e2f4668e0289e88cba969d710c
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Mon Mar 11 11:27:44 2013 +0100

    md: remove CONFIG_MULTICORE_RAID456 entirely
    
    Once instance of this Kconfig macro remained after commit
    51acbcec6c42b24482bac18e42befc822524535d ("md: remove
    CONFIG_MULTICORE_RAID456"). Remove that one too. And, while we're at it,
    also remove it from the defconfig files that carry it.
    
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 050a334e89c1..b0b663b119a8 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -221,10 +221,6 @@ struct stripe_head {
 	struct stripe_operations {
 		int 		     target, target2;
 		enum sum_check_flags zero_sum_result;
-		#ifdef CONFIG_MULTICORE_RAID456
-		unsigned long	     request;
-		wait_queue_head_t    wait_for_ops;
-		#endif
 	} ops;
 	struct r5dev {
 		/* rreq and rvec are used for the replacement device when

commit f8dfcffd0472a0f353f34a567ad3f53568914d04
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 12 12:18:06 2013 +1100

    md/raid5: ensure sync and DISCARD don't happen at the same time.
    
    A number of problems can occur due to races between
    resync/recovery and discard.
    
    - if sync_request calls handle_stripe() while a discard is
      happening on the stripe, it might call handle_stripe_clean_event
      before all of the individual discard requests have completed
      (so some devices are still locked, but not all).
      Since commit ca64cae96037de16e4af92678814f5d4bf0c1c65
         md/raid5: Make sure we clear R5_Discard when discard is finished.
      this will cause R5_Discard to be cleared for the parity device,
      so handle_stripe_clean_event() will not be called when the other
      devices do become unlocked, so their ->written will not be cleared.
      This ultimately leads to a WARN_ON in init_stripe and a lock-up.
    
    - If handle_stripe_clean_event() does clear R5_UPTODATE at an awkward
      time for resync, it can lead to s->uptodate being less than disks
      in handle_parity_checks5(), which triggers a BUG (because it is
      one).
    
    So:
     - keep R5_Discard on the parity device until all other devices have
       completed their discard request
     - make sure we don't try to have a 'discard' and a 'sync' action at
       the same time.
       This involves a new stripe flag to we know when a 'discard' is
       happening, and the use of R5_Overlap on the parity disk so when a
       discard is wanted while a sync is active, so we know to wake up
       the discard at the appropriate time.
    
    Discard support for RAID5 was added in 3.7, so this is suitable for
    any -stable kernel since 3.7.
    
    Cc: stable@vger.kernel.org (v3.7+)
    Reported-by: Jes Sorensen <Jes.Sorensen@redhat.com>
    Tested-by: Jes Sorensen <Jes.Sorensen@redhat.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 18b2c4a8a1fd..050a334e89c1 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -323,6 +323,7 @@ enum {
 	STRIPE_COMPUTE_RUN,
 	STRIPE_OPS_REQ_PENDING,
 	STRIPE_ON_UNPLUG_LIST,
+	STRIPE_DISCARD,
 };
 
 /*

commit 620125f2bf8ff0c4969b79653b54d7bcc9d40637
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Oct 11 13:49:05 2012 +1100

    MD: raid5 trim support
    
    
    Discard for raid4/5/6 has limitation. If discard request size is
    small, we do discard for one disk, but we need calculate parity and
    write parity disk.  To correctly calculate parity, zero_after_discard
    must be guaranteed. Even it's true, we need do discard for one disk
    but write another disks, which makes the parity disks wear out
    fast. This doesn't make sense. So an efficient discard for raid4/5/6
    should discard all data disks and parity disks, which requires the
    write pattern to be (A, A+chunk_size, A+chunk_size*2...). If A's size
    is smaller than chunk_size, such pattern is almost impossible in
    practice. So in this patch, I only handle the case that A's size
    equals to chunk_size. That is discard request should be aligned to
    stripe size and its size is multiple of stripe size.
    
    Since we can only handle request with specific alignment and size (or
    part of the request fitting stripes), we can't guarantee
    zero_after_discard even zero_after_discard is true in low level
    drives.
    
    The block layer doesn't send down correctly aligned requests even
    correct discard alignment is set, so I must filter out.
    
    For raid4/5/6 parity calculation, if data is 0, parity is 0. So if
    zero_after_discard is true for all disks, data is consistent after
    discard.  Otherwise, data might be lost. Let's consider a scenario:
    discard a stripe, write data to one disk and write parity disk. The
    stripe could be still inconsistent till then depending on using data
    from other data disks or parity disks to calculate new parity. If the
    disk is broken, we can't restore it. So in this patch, we only enable
    discard support if all disks have zero_after_discard.
    
    If discard fails in one disk, we face the similar inconsistent issue
    above. The patch will make discard follow the same path as normal
    write request. If discard fails, a resync will be scheduled to make
    the data consistent. This isn't good to have extra writes, but data
    consistency is important.
    
    If a subsequent read/write request hits raid5 cache of a discarded
    stripe, the discarded dev page should have zero filled, so the data is
    consistent. This patch will always zero dev page for discarded request
    stripe. This isn't optimal because discard request doesn't need such
    payload. Next patch will avoid it.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a9fc24901eda..18b2c4a8a1fd 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -298,6 +298,7 @@ enum r5dev_flags {
 	R5_WantReplace, /* We need to update the replacement, we have read
 			 * data in, and now is a good time to write it out.
 			 */
+	R5_Discard,	/* Discard the stripe */
 };
 
 /*

commit 25aa6a7ae46c6a041c46a2d314b9ab7c4f2baa41
Merge: c8924234bd9c d9f691c365a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 2 11:34:40 2012 -0700

    Merge tag 'md-3.6' of git://neil.brown.name/md
    
    Pull additional md update from NeilBrown:
     "This contains a few patches that depend on plugging changes in the
      block layer so needed to wait for those.
    
      It also contains a Kconfig fix for the new RAID10 support in dm-raid."
    
    * tag 'md-3.6' of git://neil.brown.name/md:
      md/dm-raid: DM_RAID should select MD_RAID10
      md/raid1: submit IO from originating thread instead of md thread.
      raid5: raid5d handle stripe in batch way
      raid5: make_request use batch stripe release

commit 8811b5968f6216e97ccb9fe7b9883af39e339921
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Aug 2 08:33:00 2012 +1000

    raid5: make_request use batch stripe release
    
    make_request() does stripe release for every stripe and the stripe usually has
    count 1, which makes previous release_stripe() optimization not work. In my
    test, this release_stripe() becomes the heaviest pleace to take
    conf->device_lock after previous patches applied.
    
    Below patch makes stripe release batch. All the stripes will be released in
    unplug. The STRIPE_ON_UNPLUG_LIST bit is to protect concurrent access stripe
    lru.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2164021f3b5f..9a7b36f0a425 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -319,6 +319,7 @@ enum {
 	STRIPE_BIOFILL_RUN,
 	STRIPE_COMPUTE_RUN,
 	STRIPE_OPS_REQ_PENDING,
+	STRIPE_ON_UNPLUG_LIST,
 };
 
 /*

commit 3f9e7c140e4c4e75bdeeb8df46dd40e49386c978
Author: majianpeng <majianpeng@gmail.com>
Date:   Tue Jul 31 10:04:21 2012 +1000

    raid5: Add R5_ReadNoMerge flag which prevent bio from merging at block layer
    
    Because bios will merge at block-layer,so bios-error may caused by other
    bio which be merged into to the same request.
    Using this flag,it will find exactly error-sector and not do redundant
    operation like re-write and re-read.
    
    V0->V1:Using REQ_FLUSH instead REQ_NOMERGE avoid bio merging at block
    layer.
    
    Signed-off-by: Jianpeng Ma <majianpeng@gmail.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index f03fb3395183..61dbb615c30b 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -274,6 +274,7 @@ enum r5dev_flags {
 	R5_Wantwrite,
 	R5_Overlap,	/* There is a pending overlapping request
 			 * on this block */
+	R5_ReadNoMerge, /* prevent bio from merging in block-layer */
 	R5_ReadError,	/* seen a read error here recently */
 	R5_ReWrite,	/* have tried to over-write the readerror */
 

commit b17459c05000fdbe8d10946570a26510f86ec0f6
Author: Shaohua Li <shli@kernel.org>
Date:   Thu Jul 19 16:01:31 2012 +1000

    raid5: add a per-stripe lock
    
    Add a per-stripe lock to protect stripe specific data. The purpose is to reduce
    lock contention of conf->device_lock.
    
    stripe ->toread, ->towrite are protected by per-stripe lock.  Accessing bio
    list of the stripe is always serialized by this lock, so adding bio to the
    lists (add_stripe_bio()) and removing bio from the lists (like
    ops_run_biofill()) not race.
    
    If bio in ->read, ->written ... list are not shared by multiple stripes, we
    don't need any lock to protect ->read, ->written, because STRIPE_ACTIVE will
    protect them. If the bio are shared,  there are two protections:
    1. bi_phys_segments acts as a reference count
    2. traverse the list uses r5_next_bio, which makes traverse never access bio
    not belonging to the stripe
    
    Let's have an example:
    |  stripe1 |  stripe2    |  stripe3  |
    ...bio1......|bio2|bio3|....bio4.....
    
    stripe2 has 4 bios, when it's finished, it will decrement bi_phys_segments for
    all bios, but only end_bio for bio2 and bio3. bio1->bi_next still points to
    bio2, but this doesn't matter. When stripe1 is finished, it will not touch bio2
    because of r5_next_bio check. Next time stripe1 will end_bio for bio1 and
    stripe3 will end_bio bio4.
    
    before add_stripe_bio() addes a bio to a stripe, we already increament the bio
    bi_phys_segments, so don't worry other stripes release the bio.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2164021f3b5f..f03fb3395183 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -210,6 +210,7 @@ struct stripe_head {
 	int			disks;		/* disks in stripe */
 	enum check_states	check_state;
 	enum reconstruct_states reconstruct_state;
+	spinlock_t		stripe_lock;
 	/**
 	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target

commit bc0934f0477d0a2350a478004799d9c064923b7b
Author: Shaohua Li <shli@kernel.org>
Date:   Tue May 22 13:55:05 2012 +1000

    raid5: support sync request
    
    REQ_SYNC is ignored in current raid5 code. Block layer does use it to do
    policy,
    for example ioscheduler. This patch adds it.
    
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c6bdfa01d987..2164021f3b5f 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -285,6 +285,7 @@ enum r5dev_flags {
 			 */
 	R5_Wantdrain,	/* dev->towrite needs to be drained */
 	R5_WantFUA,	/* Write should be FUA */
+	R5_SyncIO,	/* The IO is sync */
 	R5_WriteError,	/* got a write error - need to record it */
 	R5_MadeGood,	/* A bad block has been fixed by writing to it */
 	R5_ReadRepl,	/* Will/did read from replacement rather than orig */

commit b5254dd5fdd9abcacadb5101beb35df9ae8cc564
Author: NeilBrown <neilb@suse.de>
Date:   Mon May 21 09:27:01 2012 +1000

    md/raid5: allow for change in data_offset while managing a reshape.
    
    The important issue here is incorporating the different in data_offset
    into calculations concerning when we might need to over-write data
    that is still thought to be valid.
    
    To this end we find the minimum offset difference across all devices
    and add that where appropriate.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 8d8e13934a48..c6bdfa01d987 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -385,6 +385,12 @@ struct r5conf {
 	short			generation; /* increments with every reshape */
 	unsigned long		reshape_checkpoint; /* Time we last updated
 						     * metadata */
+	long long		min_offset_diff; /* minimum difference between
+						  * data_offset and
+						  * new_data_offset across all
+						  * devices.  May be negative,
+						  * but is closest to zero.
+						  */
 
 	struct list_head	handle_list; /* stripes needing handling */
 	struct list_head	hold_list; /* preread ready stripes */

commit 9a3e1101b827a59ac9036a672f5fa8d5279d0fe2
Author: NeilBrown <neilb@suse.de>
Date:   Fri Dec 23 10:17:53 2011 +1100

    md/raid5:  detect and handle replacements during recovery.
    
    During recovery we want to write to the replacement but not
    the original.  So we have two new flags
     - R5_NeedReplace if this stripe has a replacement that needs to
       be written at some stage
     - R5_WantReplace if NeedReplace, and the data is available, and
       a 'sync' has been requested on this stripe.
    
    We also distinguish between 'sync and replace' which need to read
    all other devices, and 'replace' which only needs to read the
    devices being replaced.
    
    Note that during resync we always write to any replacement device.
    It might not need to be written to, but as we don't read to compare,
    we have to write to be sure.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index f6faaa16a565..8d8e13934a48 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -242,7 +242,13 @@ struct stripe_head {
  *     for handle_stripe.
  */
 struct stripe_head_state {
-	int syncing, expanding, expanded;
+	/* 'syncing' means that we need to read all devices, either
+	 * to check/correct parity, or to reconstruct a missing device.
+	 * 'replacing' means we are replacing one or more drives and
+	 * the source is valid at this point so we don't need to
+	 * read all devices, just the replacement targets.
+	 */
+	int syncing, expanding, expanded, replacing;
 	int locked, uptodate, to_read, to_write, failed, written;
 	int to_fill, compute, req_compute, non_overwrite;
 	int failed_num[2];
@@ -284,6 +290,11 @@ enum r5dev_flags {
 	R5_ReadRepl,	/* Will/did read from replacement rather than orig */
 	R5_MadeGoodRepl,/* A bad block on the replacement device has been
 			 * fixed by writing to it */
+	R5_NeedReplace,	/* This device has a replacement which is not
+			 * up-to-date at this stripe. */
+	R5_WantReplace, /* We need to update the replacement, we have read
+			 * data in, and now is a good time to write it out.
+			 */
 };
 
 /*

commit 977df36255ab0ea78b048cbc9055300c586dcc91
Author: NeilBrown <neilb@suse.de>
Date:   Fri Dec 23 10:17:53 2011 +1100

    md/raid5: writes should get directed to replacement as well as original.
    
    When writing, we need to submit two writes, one to the original, and
    one to the replacement - if there is a replacement.
    
    If the write to the replacement results in a write error, we just fail
    the device.  We only try to record write errors to the original.
    
    When writing for recovery, we shouldn't write to the original.  This
    will be addressed in a subsequent patch that generally addresses
    recovery.
    
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 4cfd8016010e..f6faaa16a565 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -259,6 +259,7 @@ struct stripe_head_state {
 enum r5dev_flags {
 	R5_UPTODATE,	/* page contains current data */
 	R5_LOCKED,	/* IO has been submitted on "req" */
+	R5_DOUBLE_LOCKED,/* Cannot clear R5_LOCKED until 2 writes complete */
 	R5_OVERWRITE,	/* towrite covers whole page */
 /* and some that are internal to handle_stripe */
 	R5_Insync,	/* rdev && rdev->in_sync at start */

commit ede7ee8b4d007f308aa033be676b1a048f99e9db
Author: NeilBrown <neilb@suse.de>
Date:   Fri Dec 23 10:17:52 2011 +1100

    md/raid5: raid5.h cleanup
    
    Remove some #defines that are no longer used, and replace some
    others with an enum.
    And remove an unused field.
    
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 43106f01862d..4cfd8016010e 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -27,7 +27,7 @@
  * The possible state transitions are:
  *
  *  Empty -> Want   - on read or write to get old data for  parity calc
- *  Empty -> Dirty  - on compute_parity to satisfy write/sync request.(RECONSTRUCT_WRITE)
+ *  Empty -> Dirty  - on compute_parity to satisfy write/sync request.
  *  Empty -> Clean  - on compute_block when computing a block for failed drive
  *  Want  -> Empty  - on failed read
  *  Want  -> Clean  - on successful completion of read request
@@ -284,15 +284,6 @@ enum r5dev_flags {
 	R5_MadeGoodRepl,/* A bad block on the replacement device has been
 			 * fixed by writing to it */
 };
-/*
- * Write method
- */
-#define RECONSTRUCT_WRITE	1
-#define READ_MODIFY_WRITE	2
-/* not a write method, but a compute_parity mode */
-#define	CHECK_PARITY		3
-/* Additional compute_parity mode -- updates the parity w/o LOCKING */
-#define UPDATE_PARITY		4
 
 /*
  * Stripe state
@@ -320,13 +311,14 @@ enum {
 /*
  * Operation request flags
  */
-#define STRIPE_OP_BIOFILL	0
-#define STRIPE_OP_COMPUTE_BLK	1
-#define STRIPE_OP_PREXOR	2
-#define STRIPE_OP_BIODRAIN	3
-#define STRIPE_OP_RECONSTRUCT	4
-#define STRIPE_OP_CHECK	5
-
+enum {
+	STRIPE_OP_BIOFILL,
+	STRIPE_OP_COMPUTE_BLK,
+	STRIPE_OP_PREXOR,
+	STRIPE_OP_BIODRAIN,
+	STRIPE_OP_RECONSTRUCT,
+	STRIPE_OP_CHECK,
+};
 /*
  * Plugging:
  *
@@ -359,7 +351,6 @@ struct disk_info {
 struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
 	struct mddev		*mddev;
-	struct disk_info	*spare;
 	int			chunk_sectors;
 	int			level, algorithm;
 	int			max_degraded;

commit 671488cc25f7c194c7c7a9f258bab1df17a6ff69
Author: NeilBrown <neilb@suse.de>
Date:   Fri Dec 23 10:17:52 2011 +1100

    md/raid5: allow each slot to have an extra replacement device
    
    Just enhance data structures to record a second device per slot to be
    used as a 'replacement' device, replacing the original.
    We also have a second bio in each slot in each stripe_head.  This will
    only be used when writing to the array - we need to write to both the
    original and the replacement at the same time, so will need two bios.
    
    For now, only try using the replacement drive for aligned-reads.
    In this case, we prefer the replacement if it has been recovered far
    enough, otherwise use the original.
    
    This includes a small enhancement.  Previously we would only do
    aligned reads if the target device was fully recovered.  Now we also
    do them if it has recovered far enough.
    
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index e10c5531f9c5..43106f01862d 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -226,8 +226,11 @@ struct stripe_head {
 		#endif
 	} ops;
 	struct r5dev {
-		struct bio	req;
-		struct bio_vec	vec;
+		/* rreq and rvec are used for the replacement device when
+		 * writing data to both devices.
+		 */
+		struct bio	req, rreq;
+		struct bio_vec	vec, rvec;
 		struct page	*page;
 		struct bio	*toread, *read, *towrite, *written;
 		sector_t	sector;			/* sector of this page */
@@ -252,29 +255,35 @@ struct stripe_head_state {
 	int handle_bad_blocks;
 };
 
-/* Flags */
-#define	R5_UPTODATE	0	/* page contains current data */
-#define	R5_LOCKED	1	/* IO has been submitted on "req" */
-#define	R5_OVERWRITE	2	/* towrite covers whole page */
+/* Flags for struct r5dev.flags */
+enum r5dev_flags {
+	R5_UPTODATE,	/* page contains current data */
+	R5_LOCKED,	/* IO has been submitted on "req" */
+	R5_OVERWRITE,	/* towrite covers whole page */
 /* and some that are internal to handle_stripe */
-#define	R5_Insync	3	/* rdev && rdev->in_sync at start */
-#define	R5_Wantread	4	/* want to schedule a read */
-#define	R5_Wantwrite	5
-#define	R5_Overlap	7	/* There is a pending overlapping request on this block */
-#define	R5_ReadError	8	/* seen a read error here recently */
-#define	R5_ReWrite	9	/* have tried to over-write the readerror */
+	R5_Insync,	/* rdev && rdev->in_sync at start */
+	R5_Wantread,	/* want to schedule a read */
+	R5_Wantwrite,
+	R5_Overlap,	/* There is a pending overlapping request
+			 * on this block */
+	R5_ReadError,	/* seen a read error here recently */
+	R5_ReWrite,	/* have tried to over-write the readerror */
 
-#define	R5_Expanded	10	/* This block now has post-expand data */
-#define	R5_Wantcompute	11	/* compute_block in progress treat as
-				 * uptodate
-				 */
-#define	R5_Wantfill	12	/* dev->toread contains a bio that needs
-				 * filling
-				 */
-#define	R5_Wantdrain	13	/* dev->towrite needs to be drained */
-#define	R5_WantFUA	14	/* Write should be FUA */
-#define	R5_WriteError	15	/* got a write error - need to record it */
-#define	R5_MadeGood	16	/* A bad block has been fixed by writing to it*/
+	R5_Expanded,	/* This block now has post-expand data */
+	R5_Wantcompute,	/* compute_block in progress treat as
+			 * uptodate
+			 */
+	R5_Wantfill,	/* dev->toread contains a bio that needs
+			 * filling
+			 */
+	R5_Wantdrain,	/* dev->towrite needs to be drained */
+	R5_WantFUA,	/* Write should be FUA */
+	R5_WriteError,	/* got a write error - need to record it */
+	R5_MadeGood,	/* A bad block has been fixed by writing to it */
+	R5_ReadRepl,	/* Will/did read from replacement rather than orig */
+	R5_MadeGoodRepl,/* A bad block on the replacement device has been
+			 * fixed by writing to it */
+};
 /*
  * Write method
  */
@@ -344,7 +353,7 @@ enum {
 
 
 struct disk_info {
-	struct md_rdev	*rdev;
+	struct md_rdev	*rdev, *replacement;
 };
 
 struct r5conf {

commit d1688a6d5515f1900af76a963b4bb6d9a6554cfa
Author: NeilBrown <neilb@suse.de>
Date:   Tue Oct 11 16:49:52 2011 +1100

    md/raid5: typedef removal: raid5_conf_t -> struct r5conf
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cf4702ccf73a..e10c5531f9c5 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -197,7 +197,7 @@ enum reconstruct_states {
 struct stripe_head {
 	struct hlist_node	hash;
 	struct list_head	lru;	      /* inactive_list or handle_list */
-	struct raid5_private_data *raid_conf;
+	struct r5conf		*raid_conf;
 	short			generation;	/* increments with every
 						 * reshape */
 	sector_t		sector;		/* sector of this row */
@@ -347,7 +347,7 @@ struct disk_info {
 	struct md_rdev	*rdev;
 };
 
-struct raid5_private_data {
+struct r5conf {
 	struct hlist_head	*stripe_hashtbl;
 	struct mddev		*mddev;
 	struct disk_info	*spare;
@@ -439,8 +439,6 @@ struct raid5_private_data {
 	struct md_thread	*thread;
 };
 
-typedef struct raid5_private_data raid5_conf_t;
-
 /*
  * Our supported algorithms
  */
@@ -504,6 +502,6 @@ static inline int algorithm_is_DDF(int layout)
 }
 
 extern int md_raid5_congested(struct mddev *mddev, int bits);
-extern void md_raid5_kick_device(raid5_conf_t *conf);
+extern void md_raid5_kick_device(struct r5conf *conf);
 extern int raid5_set_cache_size(struct mddev *mddev, int size);
 #endif

commit 2b8bf3451d1e3133ebc3998721d14013a6c27114
Author: NeilBrown <neilb@suse.de>
Date:   Tue Oct 11 16:48:23 2011 +1100

    md: remove typedefs: mdk_thread_t -> struct md_thread
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 0d222de5e5be..cf4702ccf73a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -436,7 +436,7 @@ struct raid5_private_data {
 	/* When taking over an array from a different personality, we store
 	 * the new thread here until we fully activate the array.
 	 */
-	struct mdk_thread_s	*thread;
+	struct md_thread	*thread;
 };
 
 typedef struct raid5_private_data raid5_conf_t;

commit fd01b88c75a718020ff77e7f560d33835e9b58de
Author: NeilBrown <neilb@suse.de>
Date:   Tue Oct 11 16:47:53 2011 +1100

    md: remove typedefs: mddev_t -> struct mddev
    
    Having mddev_t and 'struct mddev_s' is ugly and not preferred
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 6b234af7bf17..0d222de5e5be 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -349,7 +349,7 @@ struct disk_info {
 
 struct raid5_private_data {
 	struct hlist_head	*stripe_hashtbl;
-	mddev_t			*mddev;
+	struct mddev		*mddev;
 	struct disk_info	*spare;
 	int			chunk_sectors;
 	int			level, algorithm;
@@ -503,7 +503,7 @@ static inline int algorithm_is_DDF(int layout)
 	return layout >= 8 && layout <= 10;
 }
 
-extern int md_raid5_congested(mddev_t *mddev, int bits);
+extern int md_raid5_congested(struct mddev *mddev, int bits);
 extern void md_raid5_kick_device(raid5_conf_t *conf);
-extern int raid5_set_cache_size(mddev_t *mddev, int size);
+extern int raid5_set_cache_size(struct mddev *mddev, int size);
 #endif

commit 3cb03002000f133f9f97269edefd73611eafc873
Author: NeilBrown <neilb@suse.de>
Date:   Tue Oct 11 16:45:26 2011 +1100

    md: removing typedefs:  mdk_rdev_t -> struct md_rdev
    
    The typedefs are just annoying. 'mdk' probably refers to 'md_k.h'
    which used to be an include file that defined this thing.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 11b9566184b2..6b234af7bf17 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -248,7 +248,7 @@ struct stripe_head_state {
 	unsigned long ops_request;
 
 	struct bio *return_bi;
-	mdk_rdev_t *blocked_rdev;
+	struct md_rdev *blocked_rdev;
 	int handle_bad_blocks;
 };
 
@@ -344,7 +344,7 @@ enum {
 
 
 struct disk_info {
-	mdk_rdev_t	*rdev;
+	struct md_rdev	*rdev;
 };
 
 struct raid5_private_data {

commit b84db560ead5417b5594349512baf8837959df4f
Author: NeilBrown <neilb@suse.de>
Date:   Thu Jul 28 11:39:23 2011 +1000

    md/raid5: Clear bad blocks on successful write.
    
    On a successful write to a known bad block, flag the sh
    so that raid5d can remove the known bad block from the list.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 8620cb67ae39..11b9566184b2 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -274,6 +274,7 @@ struct stripe_head_state {
 #define	R5_Wantdrain	13	/* dev->towrite needs to be drained */
 #define	R5_WantFUA	14	/* Write should be FUA */
 #define	R5_WriteError	15	/* got a write error - need to record it */
+#define	R5_MadeGood	16	/* A bad block has been fixed by writing to it*/
 /*
  * Write method
  */

commit bc2607f393bd4fb844c1886a02af929ca0372056
Author: NeilBrown <neilb@suse.de>
Date:   Thu Jul 28 11:39:22 2011 +1000

    md/raid5: write errors should be recorded as bad blocks if possible.
    
    When a write error is detected, don't mark the device as failed
    immediately but rather record the fact for handle_stripe to deal with.
    
    Handle_stripe then attempts to record a bad block.  Only if that fails
    does the device get marked as faulty.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c5429d123636..8620cb67ae39 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -249,6 +249,7 @@ struct stripe_head_state {
 
 	struct bio *return_bi;
 	mdk_rdev_t *blocked_rdev;
+	int handle_bad_blocks;
 };
 
 /* Flags */
@@ -264,14 +265,15 @@ struct stripe_head_state {
 #define	R5_ReWrite	9	/* have tried to over-write the readerror */
 
 #define	R5_Expanded	10	/* This block now has post-expand data */
-#define	R5_Wantcompute	11 /* compute_block in progress treat as
-				    * uptodate
-				    */
-#define	R5_Wantfill	12 /* dev->toread contains a bio that needs
-				    * filling
-				    */
-#define R5_Wantdrain	13 /* dev->towrite needs to be drained */
-#define R5_WantFUA	14	/* Write should be FUA */
+#define	R5_Wantcompute	11	/* compute_block in progress treat as
+				 * uptodate
+				 */
+#define	R5_Wantfill	12	/* dev->toread contains a bio that needs
+				 * filling
+				 */
+#define	R5_Wantdrain	13	/* dev->towrite needs to be drained */
+#define	R5_WantFUA	14	/* Write should be FUA */
+#define	R5_WriteError	15	/* got a write error - need to record it */
 /*
  * Write method
  */

commit 7f0da59bdc2f65795a57009d78f7753d3aea1de3
Author: NeilBrown <neilb@suse.de>
Date:   Thu Jul 28 11:39:22 2011 +1000

    md/raid5: use bad-block log to improve handling of uncorrectable read errors.
    
    If we get an uncorrectable read error - record a bad block rather than
    failing the device.
    And if these errors (which may be due to known bad blocks) cause
    recovery to be impossible, record a bad block on the recovering
    devices, or abort the recovery.
    
    As we might abort a recovery without failing a device we need to teach
    RAID5 about recovery_disabled handling.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 68c500af1108..c5429d123636 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -399,7 +399,7 @@ struct raid5_private_data {
 					    * (fresh device added).
 					    * Cleared when a sync completes.
 					    */
-
+	int			recovery_disabled;
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */

commit c5709ef6a094c72b56355590bfa55cc107e98376
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 26 11:35:20 2011 +1000

    md/raid5:  add some more fields to stripe_head_state
    
    Adding these three fields will allow more common code to be moved
    to handle_stripe()
    
    struct field rearrangement by Namhyung Kim.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Reviewed-by: Namhyung Kim <namhyung@gmail.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 05ac5cde3707..68c500af1108 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -243,8 +243,12 @@ struct stripe_head_state {
 	int locked, uptodate, to_read, to_write, failed, written;
 	int to_fill, compute, req_compute, non_overwrite;
 	int failed_num[2];
-	unsigned long ops_request;
 	int p_failed, q_failed;
+	int dec_preread_active;
+	unsigned long ops_request;
+
+	struct bio *return_bi;
+	mdk_rdev_t *blocked_rdev;
 };
 
 /* Flags */

commit f2b3b44deee1524ca4f006048e0569f47eefdb74
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 26 11:35:19 2011 +1000

    md/raid5: unify stripe_head_state and r6_state
    
    'struct stripe_head_state' stores state about the 'current' stripe
    that is passed around while handling the stripe.
    For RAID6 there is an extension structure: r6_state, which is also
    passed around.
    There is no value in keeping these separate, so move the fields from
    the latter into the former.
    
    This means that all code now needs to treat s->failed_num as an small
    array, but this is a small cost.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Reviewed-by: Namhyung Kim <namhyung@gmail.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index bb246d9e0547..05ac5cde3707 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -242,13 +242,9 @@ struct stripe_head_state {
 	int syncing, expanding, expanded;
 	int locked, uptodate, to_read, to_write, failed, written;
 	int to_fill, compute, req_compute, non_overwrite;
-	int failed_num;
+	int failed_num[2];
 	unsigned long ops_request;
-};
-
-/* r6_state - extra state data only relevant to r6 */
-struct r6_state {
-	int p_failed, q_failed, failed_num[2];
+	int p_failed, q_failed;
 };
 
 /* Flags */

commit c4c1663be46b2ab94e59d3e0c583a8f6b188ff0c
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 26 11:34:20 2011 +1000

    md/raid5: replace sh->lock with an 'active' flag.
    
    sh->lock is now mainly used to ensure that two threads aren't running
    in the locked part of handle_stripe[56] at the same time.
    
    That can more neatly be achieved with an 'active' flag which we set
    while running handle_stripe.  If we find the flag is set, we simply
    requeue the stripe for later by setting STRIPE_HANDLE.
    
    For safety we take ->device_lock while examining the state of the
    stripe and creating a summary in 'stripe_head_state / r6_state'.
    This possibly isn't needed but as shared fields like ->toread,
    ->towrite are checked it is safer for now at least.
    
    We leave the label after the old 'unlock' called "unlock" because it
    will disappear in a few patches, so renaming seems pointless.
    
    This leaves the stripe 'locked' for longer as we clear STRIPE_ACTIVE
    later, but that is not a problem.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Reviewed-by: Namhyung Kim <namhyung@gmail.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a33001137bf8..bb246d9e0547 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -6,11 +6,11 @@
 
 /*
  *
- * Each stripe contains one buffer per disc.  Each buffer can be in
+ * Each stripe contains one buffer per device.  Each buffer can be in
  * one of a number of states stored in "flags".  Changes between
- * these states happen *almost* exclusively under a per-stripe
- * spinlock.  Some very specific changes can happen in bi_end_io, and
- * these are not protected by the spin lock.
+ * these states happen *almost* exclusively under the protection of the
+ * STRIPE_ACTIVE flag.  Some very specific changes can happen in bi_end_io, and
+ * these are not protected by STRIPE_ACTIVE.
  *
  * The flag bits that are used to represent these states are:
  *   R5_UPTODATE and R5_LOCKED
@@ -76,12 +76,10 @@
  * block and the cached buffer are successfully written, any buffer on
  * a written list can be returned with b_end_io.
  *
- * The write list and read list both act as fifos.  The read list is
- * protected by the device_lock.  The write and written lists are
- * protected by the stripe lock.  The device_lock, which can be
- * claimed while the stipe lock is held, is only for list
- * manipulations and will only be held for a very short time.  It can
- * be claimed from interrupts.
+ * The write list and read list both act as fifos.  The read list,
+ * write list and written list are protected by the device_lock.
+ * The device_lock is only for list manipulations and will only be
+ * held for a very short time.  It can be claimed from interrupts.
  *
  *
  * Stripes in the stripe cache can be on one of two lists (or on
@@ -96,7 +94,6 @@
  *
  * The inactive_list, handle_list and hash bucket lists are all protected by the
  * device_lock.
- *  - stripes on the inactive_list never have their stripe_lock held.
  *  - stripes have a reference counter. If count==0, they are on a list.
  *  - If a stripe might need handling, STRIPE_HANDLE is set.
  *  - When refcount reaches zero, then if STRIPE_HANDLE it is put on
@@ -116,10 +113,10 @@
  *  attach a request to an active stripe (add_stripe_bh())
  *     lockdev attach-buffer unlockdev
  *  handle a stripe (handle_stripe())
- *     lockstripe clrSTRIPE_HANDLE ...
+ *     setSTRIPE_ACTIVE,  clrSTRIPE_HANDLE ...
  *		(lockdev check-buffers unlockdev) ..
  *		change-state ..
- *		record io/ops needed unlockstripe schedule io/ops
+ *		record io/ops needed clearSTRIPE_ACTIVE schedule io/ops
  *  release an active stripe (release_stripe())
  *     lockdev if (!--cnt) { if  STRIPE_HANDLE, add to handle_list else add to inactive-list } unlockdev
  *
@@ -128,8 +125,7 @@
  * on a cached buffer, and plus one if the stripe is undergoing stripe
  * operations.
  *
- * Stripe operations are performed outside the stripe lock,
- * the stripe operations are:
+ * The stripe operations are:
  * -copying data between the stripe cache and user application buffers
  * -computing blocks to save a disk access, or to recover a missing block
  * -updating the parity on a write operation (reconstruct write and
@@ -159,7 +155,8 @@
  */
 
 /*
- * Operations state - intermediate states that are visible outside of sh->lock
+ * Operations state - intermediate states that are visible outside of 
+ *   STRIPE_ACTIVE.
  * In general _idle indicates nothing is running, _run indicates a data
  * processing operation is active, and _result means the data processing result
  * is stable and can be acted upon.  For simple operations like biofill and
@@ -209,7 +206,6 @@ struct stripe_head {
 	short			ddf_layout;/* use DDF ordering to calculate Q */
 	unsigned long		state;		/* state flags */
 	atomic_t		count;	      /* nr of active thread/requests */
-	spinlock_t		lock;
 	int			bm_seq;	/* sequence number for bitmap flushes */
 	int			disks;		/* disks in stripe */
 	enum check_states	check_state;
@@ -240,7 +236,7 @@ struct stripe_head {
 };
 
 /* stripe_head_state - collects and tracks the dynamic state of a stripe_head
- *     for handle_stripe.  It is only valid under spin_lock(sh->lock);
+ *     for handle_stripe.
  */
 struct stripe_head_state {
 	int syncing, expanding, expanded;
@@ -290,6 +286,7 @@ struct r6_state {
  * Stripe state
  */
 enum {
+	STRIPE_ACTIVE,
 	STRIPE_HANDLE,
 	STRIPE_SYNC_REQUESTED,
 	STRIPE_SYNCING,
@@ -339,7 +336,7 @@ enum {
  * PREREAD_ACTIVE.
  * In stripe_handle, if we find pre-reading is necessary, we do it if
  * PREREAD_ACTIVE is set, else we set DELAYED which will send it to the delayed queue.
- * HANDLE gets cleared if stripe_handle leave nothing locked.
+ * HANDLE gets cleared if stripe_handle leaves nothing locked.
  */
 
 

commit 83206d66b65118d995c38746f21edc2bb8564b49
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jul 26 11:19:49 2011 +1000

    md/raid5: Remove use of sh->lock in sync_request
    
    This is the start of a series of patches to remove sh->lock.
    
    sync_request takes sh->lock before setting STRIPE_SYNCING to ensure
    there is no race with testing it in handle_stripe[56].
    
    Instead, use a new flag STRIPE_SYNC_REQUESTED and test it early
    in handle_stripe[56] (after getting the same lock) and perform the
    same set/clear operations if it was set.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Reviewed-by: Namhyung Kim <namhyung@gmail.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 3ca77a2613ba..a33001137bf8 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -289,21 +289,24 @@ struct r6_state {
 /*
  * Stripe state
  */
-#define STRIPE_HANDLE		2
-#define	STRIPE_SYNCING		3
-#define	STRIPE_INSYNC		4
-#define	STRIPE_PREREAD_ACTIVE	5
-#define	STRIPE_DELAYED		6
-#define	STRIPE_DEGRADED		7
-#define	STRIPE_BIT_DELAY	8
-#define	STRIPE_EXPANDING	9
-#define	STRIPE_EXPAND_SOURCE	10
-#define	STRIPE_EXPAND_READY	11
-#define	STRIPE_IO_STARTED	12 /* do not count towards 'bypass_count' */
-#define	STRIPE_FULL_WRITE	13 /* all blocks are set to be overwritten */
-#define	STRIPE_BIOFILL_RUN	14
-#define	STRIPE_COMPUTE_RUN	15
-#define	STRIPE_OPS_REQ_PENDING	16
+enum {
+	STRIPE_HANDLE,
+	STRIPE_SYNC_REQUESTED,
+	STRIPE_SYNCING,
+	STRIPE_INSYNC,
+	STRIPE_PREREAD_ACTIVE,
+	STRIPE_DELAYED,
+	STRIPE_DEGRADED,
+	STRIPE_BIT_DELAY,
+	STRIPE_EXPANDING,
+	STRIPE_EXPAND_SOURCE,
+	STRIPE_EXPAND_READY,
+	STRIPE_IO_STARTED,	/* do not count towards 'bypass_count' */
+	STRIPE_FULL_WRITE,	/* all blocks are set to be overwritten */
+	STRIPE_BIOFILL_RUN,
+	STRIPE_COMPUTE_RUN,
+	STRIPE_OPS_REQ_PENDING,
+};
 
 /*
  * Operation request flags

commit 482c083492ddaa32ef5864bae3d143dc8bcdf7d1
Author: NeilBrown <neilb@suse.de>
Date:   Mon Apr 18 18:25:42 2011 +1000

    md - remove old plugging code.
    
    md has some plugging infrastructure for RAID5 to use because the
    normal plugging infrastructure required a 'request_queue', and when
    called from dm, RAID5 doesn't have one of those available.
    
    This relied on the ->unplug_fn callback which doesn't exist any more.
    
    So remove all of that code, both in md and raid5.  Subsequent patches
    with restore the plugging functionality.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 8d563a4f022a..3ca77a2613ba 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -400,8 +400,6 @@ struct raid5_private_data {
 					    * Cleared when a sync completes.
 					    */
 
-	struct plug_handle	plug;
-
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */

commit 7eaceaccab5f40bbfda044629a6298616aeaed50
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Thu Mar 10 08:52:07 2011 +0100

    block: remove per-queue plugging
    
    Code has been converted over to the new explicit on-stack plugging,
    and delay users have been converted to use the new API for that.
    So lets kill off the old plugging along with aops->sync_page().
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2ace0582b409..8d563a4f022a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -503,6 +503,6 @@ static inline int algorithm_is_DDF(int layout)
 }
 
 extern int md_raid5_congested(mddev_t *mddev, int bits);
-extern void md_raid5_unplug_device(raid5_conf_t *conf);
+extern void md_raid5_kick_device(raid5_conf_t *conf);
 extern int raid5_set_cache_size(mddev_t *mddev, int size);
 #endif

commit e9c7469bb4f502dafc092166201bea1ad5fc0fbf
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 11:56:18 2010 +0200

    md: implment REQ_FLUSH/FUA support
    
    This patch converts md to support REQ_FLUSH/FUA instead of now
    deprecated REQ_HARDBARRIER.  In the core part (md.c), the following
    changes are notable.
    
    * Unlike REQ_HARDBARRIER, REQ_FLUSH/FUA don't interfere with
      processing of other requests and thus there is no reason to mark the
      queue congested while FLUSH/FUA is in progress.
    
    * REQ_FLUSH/FUA failures are final and its users don't need retry
      logic.  Retry logic is removed.
    
    * Preflush needs to be issued to all member devices but FUA writes can
      be handled the same way as other writes - their processing can be
      deferred to request_queue of member devices.  md_barrier_request()
      is renamed to md_flush_request() and simplified accordingly.
    
    For linear, raid0 and multipath, the core changes are enough.  raid1,
    5 and 10 need the following conversions.
    
    * raid1: Handling of FLUSH/FUA bio's can simply be deferred to
      request_queues of member devices.  Barrier related logic removed.
    
    * raid5: Queue draining logic dropped.  FUA bit is propagated through
      biodrain and stripe resconstruction such that all the updated parts
      of the stripe are written out with FUA writes if any of the dirtying
      writes was FUA.  preread_active_stripes handling in make_request()
      is updated as suggested by Neil Brown.
    
    * raid10: FUA bit needs to be propagated to write clones.
    
    linear, raid0, 1, 5 and 10 tested.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 36eaed5dfd6e..2ace0582b409 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -275,6 +275,7 @@ struct r6_state {
 				    * filling
 				    */
 #define R5_Wantdrain	13 /* dev->towrite needs to be drained */
+#define R5_WantFUA	14	/* Write should be FUA */
 /*
  * Write method
  */

commit 9f7c2220017771253d7d10b3cc017cb79eeac0fb
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jul 26 12:04:13 2010 +1000

    md/raid5: export raid5 unplugging interface.
    
    Also remove remaining accesses to ->queue and ->gendisk when ->queue
    is NULL (As it is in a DM target).
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 6acd458f239d..36eaed5dfd6e 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -502,5 +502,6 @@ static inline int algorithm_is_DDF(int layout)
 }
 
 extern int md_raid5_congested(mddev_t *mddev, int bits);
+extern void md_raid5_unplug_device(raid5_conf_t *conf);
 extern int raid5_set_cache_size(mddev_t *mddev, int size);
 #endif

commit 2ac8740151b082f045e58010eb92560c3a23a0e9
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jun 1 19:37:29 2010 +1000

    md/raid5: add simple plugging infrastructure.
    
    md/raid5 uses the plugging infrastructure provided by the block layer
    and 'struct request_queue'.  However when we plug raid5 under dm there
    is no request queue so we cannot use that.
    
    So create a similar infrastructure that is much lighter weight and use
    it for raid5.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index d6470dec667a..6acd458f239d 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -398,6 +398,9 @@ struct raid5_private_data {
 					    * (fresh device added).
 					    * Cleared when a sync completes.
 					    */
+
+	struct plug_handle	plug;
+
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */

commit 11d8a6e3719519fbc0e2c9d61b6fa931b84bf813
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jul 26 11:57:07 2010 +1000

    md/raid5: export is_congested test
    
    the dm module will need this for dm-raid45.
    
    Also only access ->queue->backing_dev_info->congested_fn
    if ->queue actually exists.  It won't in a dm target.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 61b6b25dc5e7..d6470dec667a 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -497,5 +497,7 @@ static inline int algorithm_is_DDF(int layout)
 {
 	return layout >= 8 && layout <= 10;
 }
+
+extern int md_raid5_congested(mddev_t *mddev, int bits);
 extern int raid5_set_cache_size(mddev_t *mddev, int size);
 #endif

commit f4be6b43f1ac60dff00ef0923ee43b0e08872947
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jun 1 19:37:25 2010 +1000

    md/raid5: ensure we create a unique name for kmem_cache when mddev has no gendisk
    
    We will shortly allow md devices with no gendisk (they are attached to
    a dm-target instead).  That will cause mdname() to return 'mdX'.
    There is one place where mdname really needs to be unique: when
    creating the name for a slab cache.
    So in that case, if there is no gendisk, you the address of the mddev
    formatted in HEX to provide a unique name.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cbdbc77695b3..61b6b25dc5e7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -388,7 +388,7 @@ struct raid5_private_data {
 	 * two caches.
 	 */
 	int			active_name;
-	char			cache_name[2][20];
+	char			cache_name[2][32];
 	struct kmem_cache		*slab_cache; /* for allocating stripes */
 
 	int			seq_flush, seq_write;

commit c41d4ac40df0d01bf9c383ff28f194d1df2d4fd9
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jun 1 19:37:24 2010 +1000

    md/raid5: factor out code for changing size of stripe cache.
    
    Separate the actual 'change' code from the sysfs interface
    so that it can eventually be called internally.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 0f86f5e36724..cbdbc77695b3 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -497,4 +497,5 @@ static inline int algorithm_is_DDF(int layout)
 {
 	return layout >= 8 && layout <= 10;
 }
+extern int raid5_set_cache_size(mddev_t *mddev, int size);
 #endif

commit a29d8b8e2d811a24bbe49215a0f0c536b72ebc18
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 2 14:39:15 2010 +0900

    percpu: add __percpu sparse annotations to what's left
    
    Add __percpu sparse annotations to places which didn't make it in one
    of the previous patches.  All converions are trivial.
    
    These annotations are to make sparse consider percpu variables to be
    in a different address space and warn if accessed without going
    through percpu accessors.  This patch doesn't affect normal builds.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Neil Brown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index dd708359b451..0f86f5e36724 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -405,7 +405,7 @@ struct raid5_private_data {
 					      * lists and performing address
 					      * conversions
 					      */
-	} *percpu;
+	} __percpu *percpu;
 	size_t			scribble_len; /* size of scribble region must be
 					       * associated with conf to handle
 					       * cpu hotplug while reshaping

commit e4424fee1815f996dccd36be44d68ca160ec3e1b
Author: NeilBrown <neilb@suse.de>
Date:   Fri Oct 16 16:27:34 2009 +1100

    md: fix problems with RAID6 calculations for DDF.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index dcefdc9629ee..dd708359b451 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -488,7 +488,7 @@ static inline int algorithm_valid_raid6(int layout)
 {
 	return (layout >= 0 && layout <= 5)
 		||
-		(layout == 8 || layout == 10)
+		(layout >= 8 && layout <= 10)
 		||
 		(layout >= 16 && layout <= 20);
 }

commit 417b8d4ac868cf58d6c68f52d72f7648413e0edc
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Oct 16 16:25:22 2009 +1100

    md/raid456: downlevel multicore operations to raid_run_ops
    
    The percpu conversion allowed a straightforward handoff of stripe
    processing to the async subsytem that initially showed some modest gains
    (+4%).  However, this model is too simplistic and leads to stripes
    bouncing between raid5d and the async thread pool for every invocation
    of handle_stripe().  As reported by Holger this can fall into a
    pathological situation severely impacting throughput (6x performance
    loss).
    
    By downleveling the parallelism to raid_run_ops the pathological
    stripe_head bouncing is eliminated.  This version still exhibits an
    average 11% throughput loss for:
    
            mdadm --create /dev/md0 /dev/sd[b-q] -n 16 -l 6
            echo 1024 > /sys/block/md0/md/stripe_cache_size
            dd if=/dev/zero of=/dev/md0 bs=1024k count=2048
    
    ...but the results are at least stable and can be used as a base for
    further multicore experimentation.
    
    Reported-by: Holger Kiehl <Holger.Kiehl@dwd.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2390e0e83daf..dcefdc9629ee 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -214,12 +214,20 @@ struct stripe_head {
 	int			disks;		/* disks in stripe */
 	enum check_states	check_state;
 	enum reconstruct_states reconstruct_state;
-	/* stripe_operations
+	/**
+	 * struct stripe_operations
 	 * @target - STRIPE_OP_COMPUTE_BLK target
+	 * @target2 - 2nd compute target in the raid6 case
+	 * @zero_sum_result - P and Q verification flags
+	 * @request - async service request flags for raid_run_ops
 	 */
 	struct stripe_operations {
 		int 		     target, target2;
 		enum sum_check_flags zero_sum_result;
+		#ifdef CONFIG_MULTICORE_RAID456
+		unsigned long	     request;
+		wait_queue_head_t    wait_for_ops;
+		#endif
 	} ops;
 	struct r5dev {
 		struct bio	req;
@@ -294,6 +302,8 @@ struct r6_state {
 #define	STRIPE_FULL_WRITE	13 /* all blocks are set to be overwritten */
 #define	STRIPE_BIOFILL_RUN	14
 #define	STRIPE_COMPUTE_RUN	15
+#define	STRIPE_OPS_REQ_PENDING	16
+
 /*
  * Operation request flags
  */

commit bbb20089a3275a19e475dbc21320c3742e3ca423
Merge: 3e48e656903e 657a77fa7284
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Sep 8 17:55:21 2009 -0700

    Merge branch 'dmaengine' into async-tx-next
    
    Conflicts:
            crypto/async_tx/async_xor.c
            drivers/dma/ioat/dma_v2.h
            drivers/dma/ioat/pci.c
            drivers/md/raid5.c

commit ac6b53b6e6acab27e4f3e2383f9ac1f0d7c6200b
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 13:40:19 2009 -0700

    md/raid6: asynchronous raid6 operations
    
    [ Based on an original patch by Yuri Tikhonov ]
    
    The raid_run_ops routine uses the asynchronous offload api and
    the stripe_operations member of a stripe_head to carry out xor+pq+copy
    operations asynchronously, outside the lock.
    
    The operations performed by RAID-6 are the same as in the RAID-5 case
    except for no support of STRIPE_OP_PREXOR operations. All the others
    are supported:
    STRIPE_OP_BIOFILL
     - copy data into request buffers to satisfy a read request
    STRIPE_OP_COMPUTE_BLK
     - generate missing blocks (1 or 2) in the cache from the other blocks
    STRIPE_OP_BIODRAIN
     - copy data out of request buffers to satisfy a write request
    STRIPE_OP_RECONSTRUCT
     - recalculate parity for new data that has entered the cache
    STRIPE_OP_CHECK
     - verify that the parity is correct
    
    The flow is the same as in the RAID-5 case, and reuses some routines, namely:
    1/ ops_complete_postxor (renamed to ops_complete_reconstruct)
    2/ ops_complete_compute (updated to set up to 2 targets uptodate)
    3/ ops_run_check (renamed to ops_run_check_p for xor parity checks)
    
    [neilb@suse.de: fixes to get it to pass mdadm regression suite]
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 75f2c6c4cf90..116d0b44b2a9 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -176,7 +176,9 @@
  */
 enum check_states {
 	check_state_idle = 0,
-	check_state_run, /* parity check */
+	check_state_run, /* xor parity check */
+	check_state_run_q, /* q-parity check */
+	check_state_run_pq, /* pq dual parity check */
 	check_state_check_result,
 	check_state_compute_run, /* parity repair */
 	check_state_compute_result,
@@ -216,7 +218,7 @@ struct stripe_head {
 	 * @target - STRIPE_OP_COMPUTE_BLK target
 	 */
 	struct stripe_operations {
-		int		     target;
+		int 		     target, target2;
 		enum sum_check_flags zero_sum_result;
 	} ops;
 	struct r5dev {
@@ -299,7 +301,7 @@ struct r6_state {
 #define STRIPE_OP_COMPUTE_BLK	1
 #define STRIPE_OP_PREXOR	2
 #define STRIPE_OP_BIODRAIN	3
-#define STRIPE_OP_POSTXOR	4
+#define STRIPE_OP_RECONSTRUCT	4
 #define STRIPE_OP_CHECK	5
 
 /*

commit ad283ea4a3ce82cda2efe33163748a397b31b1eb
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sat Aug 29 19:09:26 2009 -0700

    async_tx: add sum check flags
    
    Replace the flat zero_sum_result with a collection of flags to contain
    the P (xor) zero-sum result, and the soon to be utilized Q (raid6 reed
    solomon syndrome) zero-sum result.  Use the SUM_CHECK_ namespace instead
    of DMA_ since these flags will be used on non-dma-zero-sum enabled
    platforms.
    
    Reviewed-by: Andre Noll <maan@systemlinux.org>
    Acked-by: Maciej Sosnowski <maciej.sosnowski@intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index e7baabffee86..75f2c6c4cf90 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -2,6 +2,7 @@
 #define _RAID5_H
 
 #include <linux/raid/xor.h>
+#include <linux/dmaengine.h>
 
 /*
  *
@@ -215,8 +216,8 @@ struct stripe_head {
 	 * @target - STRIPE_OP_COMPUTE_BLK target
 	 */
 	struct stripe_operations {
-		int		   target;
-		u32		   zero_sum_result;
+		int		     target;
+		enum sum_check_flags zero_sum_result;
 	} ops;
 	struct r5dev {
 		struct bio	req;

commit d6f38f31f3ad4b0dd33fe970988f14e7c65ef702
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 11:50:52 2009 -0700

    md/raid5,6: add percpu scribble region for buffer lists
    
    Use percpu memory rather than stack for storing the buffer lists used in
    parity calculations.  Include space for dma address conversions and pass
    that to async_tx via the async_submit_ctl.scribble pointer.
    
    [ Impact: move memory pressure from stack to heap ]
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 07a7a4102f05..e7baabffee86 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -386,7 +386,15 @@ struct raid5_private_data {
 	/* per cpu variables */
 	struct raid5_percpu {
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
+		void		*scribble;   /* space for constructing buffer
+					      * lists and performing address
+					      * conversions
+					      */
 	} *percpu;
+	size_t			scribble_len; /* size of scribble region must be
+					       * associated with conf to handle
+					       * cpu hotplug while reshaping
+					       */
 #ifdef CONFIG_HOTPLUG_CPU
 	struct notifier_block	cpu_notify;
 #endif

commit 36d1c6476be51101778882897b315bd928c8c7b5
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Jul 14 11:48:22 2009 -0700

    md/raid6: move the spare page to a percpu allocation
    
    In preparation for asynchronous handling of raid6 operations move the
    spare page to a percpu allocation to allow multiple simultaneous
    synchronous raid6 recovery operations.
    
    Make this allocation cpu hotplug aware to maximize allocation
    efficiency.
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 52ba99954dec..07a7a4102f05 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -383,8 +383,13 @@ struct raid5_private_data {
 					    * (fresh device added).
 					    * Cleared when a sync completes.
 					    */
-
-	struct page 		*spare_page; /* Used when checking P/Q in raid6 */
+	/* per cpu variables */
+	struct raid5_percpu {
+		struct page	*spare_page; /* Used when checking P/Q in raid6 */
+	} *percpu;
+#ifdef CONFIG_HOTPLUG_CPU
+	struct notifier_block	cpu_notify;
+#endif
 
 	/*
 	 * Free stripes pool

commit 09c9e5fa1b93ad5b81c9dcf8ce3a5b9ae2ac31e4
Author: Andre Noll <maan@systemlinux.org>
Date:   Thu Jun 18 08:45:55 2009 +1000

    md: convert conf->chunk_size and conf->prev_chunk to sectors.
    
    This kills some more shifts.
    
    Signed-off-by: Andre Noll <maan@systemlinux.org>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 1a25c9e252b4..9459689c4ea0 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -334,7 +334,8 @@ struct raid5_private_data {
 	struct hlist_head	*stripe_hashtbl;
 	mddev_t			*mddev;
 	struct disk_info	*spare;
-	int			chunk_size, level, algorithm;
+	int			chunk_sectors;
+	int			level, algorithm;
 	int			max_degraded;
 	int			raid_disks;
 	int			max_nr_stripes;
@@ -350,7 +351,8 @@ struct raid5_private_data {
 	 */
 	sector_t		reshape_safe;
 	int			previous_raid_disks;
-	int			prev_chunk, prev_algo;
+	int			prev_chunk_sectors;
+	int			prev_algo;
 	short			generation; /* increments with every reshape */
 	unsigned long		reshape_checkpoint; /* Time we last updated
 						     * metadata */

commit 070ec55d07157a3041f92654135c3c6e2eaaf901
Author: NeilBrown <neilb@suse.de>
Date:   Tue Jun 16 16:54:21 2009 +1000

    md: remove mddev_to_conf "helper" macro
    
    Having a macro just to cast a void* isn't really helpful.
    I would must rather see that we are simply de-referencing ->private,
    than have to know what the macro does.
    
    So open code the macro everywhere and remove the pointless cast.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 52ba99954dec..1a25c9e252b4 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -408,8 +408,6 @@ struct raid5_private_data {
 
 typedef struct raid5_private_data raid5_conf_t;
 
-#define mddev_to_conf(mddev) ((raid5_conf_t *) mddev->private)
-
 /*
  * Our supported algorithms
  */

commit c8f517c444e4f9f55b5b5ca202b8404691a35805
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:28:40 2009 +1100

    md/raid5 revise rules for when to update metadata during reshape
    
    We currently update the metadata :
     1/ every 3Megabytes
     2/ When the place we will write new-layout data to is recorded in
        the metadata as still containing old-layout data.
    
    Rule one exists to avoid having to re-do too much reshaping in the
    face of a crash/restart.  So it should really be time based rather
    than size based.  So change it to "every 10 seconds".
    
    Rule two turns out to be too harsh when restriping an array
    'in-place', as in that case the metadata much be updates for every
    stripe.
    For the in-place update, it can only possibly be safe from a crash if
    some user-space program data a backup of every e.g. few hundred
    stripes before allowing them to be reshaped.  In that case, the
    constant metadata update is pointless.
    So only update the metadata if the new metadata will report that the
    end of the 'old-layout' data is beyond where we are currently
    writing 'new-layout' data.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index cdd045681720..52ba99954dec 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -352,6 +352,8 @@ struct raid5_private_data {
 	int			previous_raid_disks;
 	int			prev_chunk, prev_algo;
 	short			generation; /* increments with every reshape */
+	unsigned long		reshape_checkpoint; /* Time we last updated
+						     * metadata */
 
 	struct list_head	handle_list; /* stripes needing handling */
 	struct list_head	hold_list; /* preread ready stripes */

commit e183eaedd53807e33f02ee80573e2833890e1f21
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:20:22 2009 +1100

    md/raid5: prepare for allowing reshape to change layout
    
    Add prev_algo to raid5_conf_t along the same lines as prev_chunk
    and previous_raid_disks.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index b9c93280fc1d..cdd045681720 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -350,7 +350,7 @@ struct raid5_private_data {
 	 */
 	sector_t		reshape_safe;
 	int			previous_raid_disks;
-	int			prev_chunk;
+	int			prev_chunk, prev_algo;
 	short			generation; /* increments with every reshape */
 
 	struct list_head	handle_list; /* stripes needing handling */

commit 784052ecc6ade6b6acf4f67e4ada8e5f2e6df446
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:19:07 2009 +1100

    md/raid5: prepare for allowing reshape to change chunksize.
    
    Add "prev_chunk" to raid5_conf_t, similar to "previous_raid_disks", to
    remember what the chunk size was before the reshape that is currently
    underway.
    
    This seems like duplication with "chunk_size" and "new_chunk" in
    mddev_t, and to some extent it is, but there are differences.
    The values in mddev_t are always defined and often the same.
    The prev* values are only defined if a reshape is underway.
    
    Also (and more significantly) the raid5_conf_t values will be changed
    at the same time (inside an appropriate lock) that the reshape is
    started by setting reshape_position.  In contrast, the new_chunk value
    is set when the sysfs file is written which could be well before the
    reshape starts.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index a081fb40a5a2..b9c93280fc1d 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -350,6 +350,7 @@ struct raid5_private_data {
 	 */
 	sector_t		reshape_safe;
 	int			previous_raid_disks;
+	int			prev_chunk;
 	short			generation; /* increments with every reshape */
 
 	struct list_head	handle_list; /* stripes needing handling */

commit 86b42c713be3e5f6807aa14b4cbdb005d35c64d5
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:19:03 2009 +1100

    md/raid5: clearly differentiate 'before' and 'after' stripes during reshape.
    
    During a raid5 reshape, we have some stripes in the cache that are
    'before' the reshape (and are still to be processed) and some that are
    'after'.  They are currently differentiated by having different
    ->disks values as the only reshape current supported involves changing
    the number of disks.
    
    However we will soon support reshapes that do not change the number
    of disks (chunk parity or chunk size).  So make the difference more
    explicit with a 'generation' number.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index b2edcc434e41..a081fb40a5a2 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -198,6 +198,8 @@ struct stripe_head {
 	struct hlist_node	hash;
 	struct list_head	lru;	      /* inactive_list or handle_list */
 	struct raid5_private_data *raid_conf;
+	short			generation;	/* increments with every
+						 * reshape */
 	sector_t		sector;		/* sector of this row */
 	short			pd_idx;		/* parity disk index */
 	short			qd_idx;		/* 'Q' disk index for raid6 */
@@ -348,6 +350,7 @@ struct raid5_private_data {
 	 */
 	sector_t		reshape_safe;
 	int			previous_raid_disks;
+	short			generation; /* increments with every reshape */
 
 	struct list_head	handle_list; /* stripes needing handling */
 	struct list_head	hold_list; /* preread ready stripes */

commit fef9c61fdfabf97a307c2cf3621a6949f0a4b995
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:16:46 2009 +1100

    md/raid5: change reshape-progress measurement to cope with reshaping backwards.
    
    When reducing the number of devices in a raid4/5/6, the reshape
    process has to start at the end of the array and work down to the
    beginning.  So we need to handle expand_progress and expand_lo
    differently.
    
    This patch renames "expand_progress" and "expand_lo" to avoid the
    implication that anything is getting bigger (expand->reshape) and
    every place they are used, we make sure that they are used the right
    way depending on whether delta_disks is positive or negative.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c2f37f25ef44..b2edcc434e41 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -337,11 +337,16 @@ struct raid5_private_data {
 	int			raid_disks;
 	int			max_nr_stripes;
 
-	/* used during an expand */
-	sector_t		expand_progress;	/* MaxSector when no expand happening */
-	sector_t		expand_lo; /* from here up to expand_progress it out-of-bounds
-					    * as we haven't flushed the metadata yet
-					    */
+	/* reshape_progress is the leading edge of a 'reshape'
+	 * It has value MaxSector when no reshape is happening
+	 * If delta_disks < 0, it is the last sector we started work on,
+	 * else is it the next sector to work on.
+	 */
+	sector_t		reshape_progress;
+	/* reshape_safe is the trailing edge of a reshape.  We know that
+	 * before (or after) this address, all reshape has completed.
+	 */
+	sector_t		reshape_safe;
 	int			previous_raid_disks;
 
 	struct list_head	handle_list; /* stripes needing handling */

commit 34e04e87fb8b2c62c9e8868f41c8179d0e15f51a
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 15:10:16 2009 +1100

    md/raid5: drop qd_idx from r6_state
    
    We now have this value in stripe_head so we don't need to duplicate
    it.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 2934ee0a39c6..c2f37f25ef44 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -239,7 +239,7 @@ struct stripe_head_state {
 
 /* r6_state - extra state data only relevant to r6 */
 struct r6_state {
-	int p_failed, q_failed, qd_idx, failed_num[2];
+	int p_failed, q_failed, failed_num[2];
 };
 
 /* Flags */

commit f701d589aa34d7531183c9ac6f7713ba14212b02
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Mar 31 15:09:39 2009 +1100

    md/raid6: move raid6 data processing to raid6_pq.ko
    
    Move the raid6 data processing routines into a standalone module
    (raid6_pq) to prepare them to be called from async_tx wrappers and other
    non-md drivers/modules.  This precludes a circular dependency of raid456
    needing the async modules for data processing while those modules in
    turn depend on raid456 for the base level synchronous raid6 routines.
    
    To support this move:
    1/ The exportable definitions in raid6.h move to include/linux/raid/pq.h
    2/ The raid6_call, recovery calls, and table symbols are exported
    3/ Extra #ifdef __KERNEL__ statements to enable the userspace raid6test to
       compile
    
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index c172371481c7..2934ee0a39c6 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -269,6 +269,8 @@ struct r6_state {
 #define READ_MODIFY_WRITE	2
 /* not a write method, but a compute_parity mode */
 #define	CHECK_PARITY		3
+/* Additional compute_parity mode -- updates the parity w/o LOCKING */
+#define UPDATE_PARITY		4
 
 /*
  * Stripe state

commit 91adb56473febeeb3ef657bb5147ddd355465700
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 14:39:39 2009 +1100

    md/raid5: refactor raid5 "run"
    
    .. so that the code to create the private data structures is separate.
    This will help with future code to change the level of an active
    array.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 84456b1af204..c172371481c7 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -386,6 +386,11 @@ struct raid5_private_data {
 	int			pool_size; /* number of disks in stripeheads in pool */
 	spinlock_t		device_lock;
 	struct disk_info	*disks;
+
+	/* When taking over an array from a different personality, we store
+	 * the new thread here until we fully activate the array.
+	 */
+	struct mdk_thread_s	*thread;
 };
 
 typedef struct raid5_private_data raid5_conf_t;

commit 67cc2b8165857ba019920d1f00d64bcc4140075d
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 14:39:38 2009 +1100

    md/raid5: finish support for DDF/raid6
    
    DDF requires RAID6 calculations over different devices in a different
    order.
    For md/raid6, we calculate over just the data devices, starting
    immediately after the 'Q' block.
    For ddf/raid6 we calculate over all devices, using zeros in place of
    the P and Q blocks.
    
    This requires unfortunately complex loops...
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 633d79289616..84456b1af204 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -201,6 +201,7 @@ struct stripe_head {
 	sector_t		sector;		/* sector of this row */
 	short			pd_idx;		/* parity disk index */
 	short			qd_idx;		/* 'Q' disk index for raid6 */
+	short			ddf_layout;/* use DDF ordering to calculate Q */
 	unsigned long		state;		/* state flags */
 	atomic_t		count;	      /* nr of active thread/requests */
 	spinlock_t		lock;

commit 99c0fb5f92828ae96909d390f2df137b89093b37
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 14:39:38 2009 +1100

    md/raid5: Add support for new layouts for raid5 and raid6.
    
    DDF uses different layouts for P and Q blocks than current md/raid6
    so add those that are missing.
    Also add support for RAID6 layouts that are identical to various
    raid5 layouts with the simple addition of one device to hold all of
    the 'Q' blocks.
    Finally add 'raid5' layouts to match raid4.
    These last to will allow online level conversion.
    
    Note that this does not provide correct support for DDF/raid6 yet
    as the order in which data blocks are summed to produce the Q block
    is significant and different between current md code and DDF
    requirements.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 0c7375ad12bd..633d79289616 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -394,9 +394,62 @@ typedef struct raid5_private_data raid5_conf_t;
 /*
  * Our supported algorithms
  */
-#define ALGORITHM_LEFT_ASYMMETRIC	0
-#define ALGORITHM_RIGHT_ASYMMETRIC	1
-#define ALGORITHM_LEFT_SYMMETRIC	2
-#define ALGORITHM_RIGHT_SYMMETRIC	3
+#define ALGORITHM_LEFT_ASYMMETRIC	0 /* Rotating Parity N with Data Restart */
+#define ALGORITHM_RIGHT_ASYMMETRIC	1 /* Rotating Parity 0 with Data Restart */
+#define ALGORITHM_LEFT_SYMMETRIC	2 /* Rotating Parity N with Data Continuation */
+#define ALGORITHM_RIGHT_SYMMETRIC	3 /* Rotating Parity 0 with Data Continuation */
 
+/* Define non-rotating (raid4) algorithms.  These allow
+ * conversion of raid4 to raid5.
+ */
+#define ALGORITHM_PARITY_0		4 /* P or P,Q are initial devices */
+#define ALGORITHM_PARITY_N		5 /* P or P,Q are final devices. */
+
+/* DDF RAID6 layouts differ from md/raid6 layouts in two ways.
+ * Firstly, the exact positioning of the parity block is slightly
+ * different between the 'LEFT_*' modes of md and the "_N_*" modes
+ * of DDF.
+ * Secondly, or order of datablocks over which the Q syndrome is computed
+ * is different.
+ * Consequently we have different layouts for DDF/raid6 than md/raid6.
+ * These layouts are from the DDFv1.2 spec.
+ * Interestingly DDFv1.2-Errata-A does not specify N_CONTINUE but
+ * leaves RLQ=3 as 'Vendor Specific'
+ */
+
+#define ALGORITHM_ROTATING_ZERO_RESTART	8 /* DDF PRL=6 RLQ=1 */
+#define ALGORITHM_ROTATING_N_RESTART	9 /* DDF PRL=6 RLQ=2 */
+#define ALGORITHM_ROTATING_N_CONTINUE	10 /*DDF PRL=6 RLQ=3 */
+
+
+/* For every RAID5 algorithm we define a RAID6 algorithm
+ * with exactly the same layout for data and parity, and
+ * with the Q block always on the last device (N-1).
+ * This allows trivial conversion from RAID5 to RAID6
+ */
+#define ALGORITHM_LEFT_ASYMMETRIC_6	16
+#define ALGORITHM_RIGHT_ASYMMETRIC_6	17
+#define ALGORITHM_LEFT_SYMMETRIC_6	18
+#define ALGORITHM_RIGHT_SYMMETRIC_6	19
+#define ALGORITHM_PARITY_0_6		20
+#define ALGORITHM_PARITY_N_6		ALGORITHM_PARITY_N
+
+static inline int algorithm_valid_raid5(int layout)
+{
+	return (layout >= 0) &&
+		(layout <= 5);
+}
+static inline int algorithm_valid_raid6(int layout)
+{
+	return (layout >= 0 && layout <= 5)
+		||
+		(layout == 8 || layout == 10)
+		||
+		(layout >= 16 && layout <= 20);
+}
+
+static inline int algorithm_is_DDF(int layout)
+{
+	return layout >= 8 && layout <= 10;
+}
 #endif

commit d0dabf7e577411c2bf6b616c751544dc241213d4
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 14:39:38 2009 +1100

    md/raid6: remove expectation that Q device is immediately after P device.
    
    
    Code currently assumes that the devices in a raid6 stripe are
      0 1 ... N-1 P Q
    in some rotated order.  We will shortly add new layouts in which
    this strict pattern is broken.
    So remove this expectation.  We still assume that the data disks
    are roughly in-order.  However P and Q can be inserted anywhere within
    that order.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 0ed22dff56e0..0c7375ad12bd 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -196,15 +196,16 @@ enum reconstruct_states {
 
 struct stripe_head {
 	struct hlist_node	hash;
-	struct list_head	lru;			/* inactive_list or handle_list */
-	struct raid5_private_data	*raid_conf;
-	sector_t		sector;			/* sector of this row */
-	int			pd_idx;			/* parity disk index */
-	unsigned long		state;			/* state flags */
-	atomic_t		count;			/* nr of active thread/requests */
+	struct list_head	lru;	      /* inactive_list or handle_list */
+	struct raid5_private_data *raid_conf;
+	sector_t		sector;		/* sector of this row */
+	short			pd_idx;		/* parity disk index */
+	short			qd_idx;		/* 'Q' disk index for raid6 */
+	unsigned long		state;		/* state flags */
+	atomic_t		count;	      /* nr of active thread/requests */
 	spinlock_t		lock;
 	int			bm_seq;	/* sequence number for bitmap flushes */
-	int			disks;			/* disks in stripe */
+	int			disks;		/* disks in stripe */
 	enum check_states	check_state;
 	enum reconstruct_states reconstruct_state;
 	/* stripe_operations

commit bff61975b3d6c18ee31457cc5b4d73042f44915f
Author: NeilBrown <neilb@suse.de>
Date:   Tue Mar 31 14:33:13 2009 +1100

    md: move lots of #include lines out of .h files and into .c
    
    This makes the includes more explicit, and is preparation for moving
    md_k.h to drivers/md/md.h
    
    Remove include/raid/md.h as its only remaining use was to #include
    other files.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
index 40f1d0335c74..0ed22dff56e0 100644
--- a/drivers/md/raid5.h
+++ b/drivers/md/raid5.h
@@ -1,7 +1,6 @@
 #ifndef _RAID5_H
 #define _RAID5_H
 
-#include <linux/raid/md.h>
 #include <linux/raid/xor.h>
 
 /*

commit ef740c372dfd80e706dbf955d4e4aedda6c0c148
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Mar 31 14:27:03 2009 +1100

    md: move headers out of include/linux/raid/
    
    Move the headers with the local structures for the disciplines and
    bitmap.h into drivers/md/ so that they are more easily grepable for
    hacking and not far away.  md.h is left where it is for now as there
    are some uses from the outside.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: NeilBrown <neilb@suse.de>

diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
new file mode 100644
index 000000000000..40f1d0335c74
--- /dev/null
+++ b/drivers/md/raid5.h
@@ -0,0 +1,402 @@
+#ifndef _RAID5_H
+#define _RAID5_H
+
+#include <linux/raid/md.h>
+#include <linux/raid/xor.h>
+
+/*
+ *
+ * Each stripe contains one buffer per disc.  Each buffer can be in
+ * one of a number of states stored in "flags".  Changes between
+ * these states happen *almost* exclusively under a per-stripe
+ * spinlock.  Some very specific changes can happen in bi_end_io, and
+ * these are not protected by the spin lock.
+ *
+ * The flag bits that are used to represent these states are:
+ *   R5_UPTODATE and R5_LOCKED
+ *
+ * State Empty == !UPTODATE, !LOCK
+ *        We have no data, and there is no active request
+ * State Want == !UPTODATE, LOCK
+ *        A read request is being submitted for this block
+ * State Dirty == UPTODATE, LOCK
+ *        Some new data is in this buffer, and it is being written out
+ * State Clean == UPTODATE, !LOCK
+ *        We have valid data which is the same as on disc
+ *
+ * The possible state transitions are:
+ *
+ *  Empty -> Want   - on read or write to get old data for  parity calc
+ *  Empty -> Dirty  - on compute_parity to satisfy write/sync request.(RECONSTRUCT_WRITE)
+ *  Empty -> Clean  - on compute_block when computing a block for failed drive
+ *  Want  -> Empty  - on failed read
+ *  Want  -> Clean  - on successful completion of read request
+ *  Dirty -> Clean  - on successful completion of write request
+ *  Dirty -> Clean  - on failed write
+ *  Clean -> Dirty  - on compute_parity to satisfy write/sync (RECONSTRUCT or RMW)
+ *
+ * The Want->Empty, Want->Clean, Dirty->Clean, transitions
+ * all happen in b_end_io at interrupt time.
+ * Each sets the Uptodate bit before releasing the Lock bit.
+ * This leaves one multi-stage transition:
+ *    Want->Dirty->Clean
+ * This is safe because thinking that a Clean buffer is actually dirty
+ * will at worst delay some action, and the stripe will be scheduled
+ * for attention after the transition is complete.
+ *
+ * There is one possibility that is not covered by these states.  That
+ * is if one drive has failed and there is a spare being rebuilt.  We
+ * can't distinguish between a clean block that has been generated
+ * from parity calculations, and a clean block that has been
+ * successfully written to the spare ( or to parity when resyncing).
+ * To distingush these states we have a stripe bit STRIPE_INSYNC that
+ * is set whenever a write is scheduled to the spare, or to the parity
+ * disc if there is no spare.  A sync request clears this bit, and
+ * when we find it set with no buffers locked, we know the sync is
+ * complete.
+ *
+ * Buffers for the md device that arrive via make_request are attached
+ * to the appropriate stripe in one of two lists linked on b_reqnext.
+ * One list (bh_read) for read requests, one (bh_write) for write.
+ * There should never be more than one buffer on the two lists
+ * together, but we are not guaranteed of that so we allow for more.
+ *
+ * If a buffer is on the read list when the associated cache buffer is
+ * Uptodate, the data is copied into the read buffer and it's b_end_io
+ * routine is called.  This may happen in the end_request routine only
+ * if the buffer has just successfully been read.  end_request should
+ * remove the buffers from the list and then set the Uptodate bit on
+ * the buffer.  Other threads may do this only if they first check
+ * that the Uptodate bit is set.  Once they have checked that they may
+ * take buffers off the read queue.
+ *
+ * When a buffer on the write list is committed for write it is copied
+ * into the cache buffer, which is then marked dirty, and moved onto a
+ * third list, the written list (bh_written).  Once both the parity
+ * block and the cached buffer are successfully written, any buffer on
+ * a written list can be returned with b_end_io.
+ *
+ * The write list and read list both act as fifos.  The read list is
+ * protected by the device_lock.  The write and written lists are
+ * protected by the stripe lock.  The device_lock, which can be
+ * claimed while the stipe lock is held, is only for list
+ * manipulations and will only be held for a very short time.  It can
+ * be claimed from interrupts.
+ *
+ *
+ * Stripes in the stripe cache can be on one of two lists (or on
+ * neither).  The "inactive_list" contains stripes which are not
+ * currently being used for any request.  They can freely be reused
+ * for another stripe.  The "handle_list" contains stripes that need
+ * to be handled in some way.  Both of these are fifo queues.  Each
+ * stripe is also (potentially) linked to a hash bucket in the hash
+ * table so that it can be found by sector number.  Stripes that are
+ * not hashed must be on the inactive_list, and will normally be at
+ * the front.  All stripes start life this way.
+ *
+ * The inactive_list, handle_list and hash bucket lists are all protected by the
+ * device_lock.
+ *  - stripes on the inactive_list never have their stripe_lock held.
+ *  - stripes have a reference counter. If count==0, they are on a list.
+ *  - If a stripe might need handling, STRIPE_HANDLE is set.
+ *  - When refcount reaches zero, then if STRIPE_HANDLE it is put on
+ *    handle_list else inactive_list
+ *
+ * This, combined with the fact that STRIPE_HANDLE is only ever
+ * cleared while a stripe has a non-zero count means that if the
+ * refcount is 0 and STRIPE_HANDLE is set, then it is on the
+ * handle_list and if recount is 0 and STRIPE_HANDLE is not set, then
+ * the stripe is on inactive_list.
+ *
+ * The possible transitions are:
+ *  activate an unhashed/inactive stripe (get_active_stripe())
+ *     lockdev check-hash unlink-stripe cnt++ clean-stripe hash-stripe unlockdev
+ *  activate a hashed, possibly active stripe (get_active_stripe())
+ *     lockdev check-hash if(!cnt++)unlink-stripe unlockdev
+ *  attach a request to an active stripe (add_stripe_bh())
+ *     lockdev attach-buffer unlockdev
+ *  handle a stripe (handle_stripe())
+ *     lockstripe clrSTRIPE_HANDLE ...
+ *		(lockdev check-buffers unlockdev) ..
+ *		change-state ..
+ *		record io/ops needed unlockstripe schedule io/ops
+ *  release an active stripe (release_stripe())
+ *     lockdev if (!--cnt) { if  STRIPE_HANDLE, add to handle_list else add to inactive-list } unlockdev
+ *
+ * The refcount counts each thread that have activated the stripe,
+ * plus raid5d if it is handling it, plus one for each active request
+ * on a cached buffer, and plus one if the stripe is undergoing stripe
+ * operations.
+ *
+ * Stripe operations are performed outside the stripe lock,
+ * the stripe operations are:
+ * -copying data between the stripe cache and user application buffers
+ * -computing blocks to save a disk access, or to recover a missing block
+ * -updating the parity on a write operation (reconstruct write and
+ *  read-modify-write)
+ * -checking parity correctness
+ * -running i/o to disk
+ * These operations are carried out by raid5_run_ops which uses the async_tx
+ * api to (optionally) offload operations to dedicated hardware engines.
+ * When requesting an operation handle_stripe sets the pending bit for the
+ * operation and increments the count.  raid5_run_ops is then run whenever
+ * the count is non-zero.
+ * There are some critical dependencies between the operations that prevent some
+ * from being requested while another is in flight.
+ * 1/ Parity check operations destroy the in cache version of the parity block,
+ *    so we prevent parity dependent operations like writes and compute_blocks
+ *    from starting while a check is in progress.  Some dma engines can perform
+ *    the check without damaging the parity block, in these cases the parity
+ *    block is re-marked up to date (assuming the check was successful) and is
+ *    not re-read from disk.
+ * 2/ When a write operation is requested we immediately lock the affected
+ *    blocks, and mark them as not up to date.  This causes new read requests
+ *    to be held off, as well as parity checks and compute block operations.
+ * 3/ Once a compute block operation has been requested handle_stripe treats
+ *    that block as if it is up to date.  raid5_run_ops guaruntees that any
+ *    operation that is dependent on the compute block result is initiated after
+ *    the compute block completes.
+ */
+
+/*
+ * Operations state - intermediate states that are visible outside of sh->lock
+ * In general _idle indicates nothing is running, _run indicates a data
+ * processing operation is active, and _result means the data processing result
+ * is stable and can be acted upon.  For simple operations like biofill and
+ * compute that only have an _idle and _run state they are indicated with
+ * sh->state flags (STRIPE_BIOFILL_RUN and STRIPE_COMPUTE_RUN)
+ */
+/**
+ * enum check_states - handles syncing / repairing a stripe
+ * @check_state_idle - check operations are quiesced
+ * @check_state_run - check operation is running
+ * @check_state_result - set outside lock when check result is valid
+ * @check_state_compute_run - check failed and we are repairing
+ * @check_state_compute_result - set outside lock when compute result is valid
+ */
+enum check_states {
+	check_state_idle = 0,
+	check_state_run, /* parity check */
+	check_state_check_result,
+	check_state_compute_run, /* parity repair */
+	check_state_compute_result,
+};
+
+/**
+ * enum reconstruct_states - handles writing or expanding a stripe
+ */
+enum reconstruct_states {
+	reconstruct_state_idle = 0,
+	reconstruct_state_prexor_drain_run,	/* prexor-write */
+	reconstruct_state_drain_run,		/* write */
+	reconstruct_state_run,			/* expand */
+	reconstruct_state_prexor_drain_result,
+	reconstruct_state_drain_result,
+	reconstruct_state_result,
+};
+
+struct stripe_head {
+	struct hlist_node	hash;
+	struct list_head	lru;			/* inactive_list or handle_list */
+	struct raid5_private_data	*raid_conf;
+	sector_t		sector;			/* sector of this row */
+	int			pd_idx;			/* parity disk index */
+	unsigned long		state;			/* state flags */
+	atomic_t		count;			/* nr of active thread/requests */
+	spinlock_t		lock;
+	int			bm_seq;	/* sequence number for bitmap flushes */
+	int			disks;			/* disks in stripe */
+	enum check_states	check_state;
+	enum reconstruct_states reconstruct_state;
+	/* stripe_operations
+	 * @target - STRIPE_OP_COMPUTE_BLK target
+	 */
+	struct stripe_operations {
+		int		   target;
+		u32		   zero_sum_result;
+	} ops;
+	struct r5dev {
+		struct bio	req;
+		struct bio_vec	vec;
+		struct page	*page;
+		struct bio	*toread, *read, *towrite, *written;
+		sector_t	sector;			/* sector of this page */
+		unsigned long	flags;
+	} dev[1]; /* allocated with extra space depending of RAID geometry */
+};
+
+/* stripe_head_state - collects and tracks the dynamic state of a stripe_head
+ *     for handle_stripe.  It is only valid under spin_lock(sh->lock);
+ */
+struct stripe_head_state {
+	int syncing, expanding, expanded;
+	int locked, uptodate, to_read, to_write, failed, written;
+	int to_fill, compute, req_compute, non_overwrite;
+	int failed_num;
+	unsigned long ops_request;
+};
+
+/* r6_state - extra state data only relevant to r6 */
+struct r6_state {
+	int p_failed, q_failed, qd_idx, failed_num[2];
+};
+
+/* Flags */
+#define	R5_UPTODATE	0	/* page contains current data */
+#define	R5_LOCKED	1	/* IO has been submitted on "req" */
+#define	R5_OVERWRITE	2	/* towrite covers whole page */
+/* and some that are internal to handle_stripe */
+#define	R5_Insync	3	/* rdev && rdev->in_sync at start */
+#define	R5_Wantread	4	/* want to schedule a read */
+#define	R5_Wantwrite	5
+#define	R5_Overlap	7	/* There is a pending overlapping request on this block */
+#define	R5_ReadError	8	/* seen a read error here recently */
+#define	R5_ReWrite	9	/* have tried to over-write the readerror */
+
+#define	R5_Expanded	10	/* This block now has post-expand data */
+#define	R5_Wantcompute	11 /* compute_block in progress treat as
+				    * uptodate
+				    */
+#define	R5_Wantfill	12 /* dev->toread contains a bio that needs
+				    * filling
+				    */
+#define R5_Wantdrain	13 /* dev->towrite needs to be drained */
+/*
+ * Write method
+ */
+#define RECONSTRUCT_WRITE	1
+#define READ_MODIFY_WRITE	2
+/* not a write method, but a compute_parity mode */
+#define	CHECK_PARITY		3
+
+/*
+ * Stripe state
+ */
+#define STRIPE_HANDLE		2
+#define	STRIPE_SYNCING		3
+#define	STRIPE_INSYNC		4
+#define	STRIPE_PREREAD_ACTIVE	5
+#define	STRIPE_DELAYED		6
+#define	STRIPE_DEGRADED		7
+#define	STRIPE_BIT_DELAY	8
+#define	STRIPE_EXPANDING	9
+#define	STRIPE_EXPAND_SOURCE	10
+#define	STRIPE_EXPAND_READY	11
+#define	STRIPE_IO_STARTED	12 /* do not count towards 'bypass_count' */
+#define	STRIPE_FULL_WRITE	13 /* all blocks are set to be overwritten */
+#define	STRIPE_BIOFILL_RUN	14
+#define	STRIPE_COMPUTE_RUN	15
+/*
+ * Operation request flags
+ */
+#define STRIPE_OP_BIOFILL	0
+#define STRIPE_OP_COMPUTE_BLK	1
+#define STRIPE_OP_PREXOR	2
+#define STRIPE_OP_BIODRAIN	3
+#define STRIPE_OP_POSTXOR	4
+#define STRIPE_OP_CHECK	5
+
+/*
+ * Plugging:
+ *
+ * To improve write throughput, we need to delay the handling of some
+ * stripes until there has been a chance that several write requests
+ * for the one stripe have all been collected.
+ * In particular, any write request that would require pre-reading
+ * is put on a "delayed" queue until there are no stripes currently
+ * in a pre-read phase.  Further, if the "delayed" queue is empty when
+ * a stripe is put on it then we "plug" the queue and do not process it
+ * until an unplug call is made. (the unplug_io_fn() is called).
+ *
+ * When preread is initiated on a stripe, we set PREREAD_ACTIVE and add
+ * it to the count of prereading stripes.
+ * When write is initiated, or the stripe refcnt == 0 (just in case) we
+ * clear the PREREAD_ACTIVE flag and decrement the count
+ * Whenever the 'handle' queue is empty and the device is not plugged, we
+ * move any strips from delayed to handle and clear the DELAYED flag and set
+ * PREREAD_ACTIVE.
+ * In stripe_handle, if we find pre-reading is necessary, we do it if
+ * PREREAD_ACTIVE is set, else we set DELAYED which will send it to the delayed queue.
+ * HANDLE gets cleared if stripe_handle leave nothing locked.
+ */
+
+
+struct disk_info {
+	mdk_rdev_t	*rdev;
+};
+
+struct raid5_private_data {
+	struct hlist_head	*stripe_hashtbl;
+	mddev_t			*mddev;
+	struct disk_info	*spare;
+	int			chunk_size, level, algorithm;
+	int			max_degraded;
+	int			raid_disks;
+	int			max_nr_stripes;
+
+	/* used during an expand */
+	sector_t		expand_progress;	/* MaxSector when no expand happening */
+	sector_t		expand_lo; /* from here up to expand_progress it out-of-bounds
+					    * as we haven't flushed the metadata yet
+					    */
+	int			previous_raid_disks;
+
+	struct list_head	handle_list; /* stripes needing handling */
+	struct list_head	hold_list; /* preread ready stripes */
+	struct list_head	delayed_list; /* stripes that have plugged requests */
+	struct list_head	bitmap_list; /* stripes delaying awaiting bitmap update */
+	struct bio		*retry_read_aligned; /* currently retrying aligned bios   */
+	struct bio		*retry_read_aligned_list; /* aligned bios retry list  */
+	atomic_t		preread_active_stripes; /* stripes with scheduled io */
+	atomic_t		active_aligned_reads;
+	atomic_t		pending_full_writes; /* full write backlog */
+	int			bypass_count; /* bypassed prereads */
+	int			bypass_threshold; /* preread nice */
+	struct list_head	*last_hold; /* detect hold_list promotions */
+
+	atomic_t		reshape_stripes; /* stripes with pending writes for reshape */
+	/* unfortunately we need two cache names as we temporarily have
+	 * two caches.
+	 */
+	int			active_name;
+	char			cache_name[2][20];
+	struct kmem_cache		*slab_cache; /* for allocating stripes */
+
+	int			seq_flush, seq_write;
+	int			quiesce;
+
+	int			fullsync;  /* set to 1 if a full sync is needed,
+					    * (fresh device added).
+					    * Cleared when a sync completes.
+					    */
+
+	struct page 		*spare_page; /* Used when checking P/Q in raid6 */
+
+	/*
+	 * Free stripes pool
+	 */
+	atomic_t		active_stripes;
+	struct list_head	inactive_list;
+	wait_queue_head_t	wait_for_stripe;
+	wait_queue_head_t	wait_for_overlap;
+	int			inactive_blocked;	/* release of inactive stripes blocked,
+							 * waiting for 25% to be free
+							 */
+	int			pool_size; /* number of disks in stripeheads in pool */
+	spinlock_t		device_lock;
+	struct disk_info	*disks;
+};
+
+typedef struct raid5_private_data raid5_conf_t;
+
+#define mddev_to_conf(mddev) ((raid5_conf_t *) mddev->private)
+
+/*
+ * Our supported algorithms
+ */
+#define ALGORITHM_LEFT_ASYMMETRIC	0
+#define ALGORITHM_RIGHT_ASYMMETRIC	1
+#define ALGORITHM_LEFT_SYMMETRIC	2
+#define ALGORITHM_RIGHT_SYMMETRIC	3
+
+#endif
