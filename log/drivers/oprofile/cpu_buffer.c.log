commit 13292494379f92f532de71b31a54018336adc589
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:58:57 2019 -0500

    tracing: Make struct ring_buffer less ambiguous
    
    As there's two struct ring_buffers in the kernel, it causes some confusion.
    The other one being the perf ring buffer. It was agreed upon that as neither
    of the ring buffers are generic enough to be used globally, they should be
    renamed as:
    
       perf's ring_buffer -> perf_buffer
       ftrace's ring_buffer -> trace_buffer
    
    This implements the changes to the ring buffer that ftrace uses.
    
    Link: https://lore.kernel.org/r/20191213140531.116b3200@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index eda2633a393d..9210a95cb4e6 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -32,7 +32,7 @@
 
 #define OP_BUFFER_FLAGS	0
 
-static struct ring_buffer *op_ring_buffer;
+static struct trace_buffer *op_ring_buffer;
 DEFINE_PER_CPU(struct oprofile_cpu_buffer, op_cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);

commit 4cf421e55d69016989548e0fb8585e69f54bd283
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:03:42 2017 +0100

    sched/headers: Prepare for the removal of <asm/ptrace.h> from <linux/sched.h>
    
    Fix up missing #includes in other places that rely on sched.h doing that for them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 0581461c3a67..eda2633a393d 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -23,6 +23,8 @@
 #include <linux/oprofile.h>
 #include <linux/errno.h>
 
+#include <asm/ptrace.h>
+
 #include "event_buffer.h"
 #include "cpu_buffer.h"
 #include "buffer_sync.h"

commit 879d92745a1a5a6573dee83cfa2953413fed23fc
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:31 2014 -0500

    drivers/oprofile: Replace __get_cpu_var uses for address calculation
    
    Replace the uses of __get_cpu_var for address calculation with this_cpu_ptr.
    
    Cc: Robert Richter <rric@kernel.org>
    Cc: oprofile-list@lists.sf.net
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 8aa73fac6ad4..0581461c3a67 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -45,7 +45,7 @@ unsigned long oprofile_get_cpu_buffer_size(void)
 
 void oprofile_cpu_buffer_inc_smpl_lost(void)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = this_cpu_ptr(&op_cpu_buffer);
 
 	cpu_buf->sample_lost_overflow++;
 }
@@ -297,7 +297,7 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 			  unsigned long event, int is_kernel,
 			  struct task_struct *task)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = this_cpu_ptr(&op_cpu_buffer);
 	unsigned long backtrace = oprofile_backtrace_depth;
 
 	/*
@@ -357,7 +357,7 @@ oprofile_write_reserve(struct op_entry *entry, struct pt_regs * const regs,
 {
 	struct op_sample *sample;
 	int is_kernel = !user_mode(regs);
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = this_cpu_ptr(&op_cpu_buffer);
 
 	cpu_buf->sample_received++;
 
@@ -412,13 +412,13 @@ int oprofile_write_commit(struct op_entry *entry)
 
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = this_cpu_ptr(&op_cpu_buffer);
 	log_sample(cpu_buf, pc, 0, is_kernel, event, NULL);
 }
 
 void oprofile_add_trace(unsigned long pc)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = this_cpu_ptr(&op_cpu_buffer);
 
 	if (!cpu_buf->tracing)
 		return;

commit 61bccf191fe2d55b8d003b4ea3f94913745aaefa
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Aug 22 09:23:51 2012 +0200

    oprofile: Remove 'WQ on CPUx, prefer CPUy' warning
    
    Under certain workloads we see the following warnings:
    
     WQ on CPU0, prefer CPU1
     WQ on CPU0, prefer CPU2
     WQ on CPU0, prefer CPU3
    
    It warns the user that the wq to access a per-cpu buffers runs not on
    the same cpu. This happens if the wq is rescheduled on a different cpu
    than where the buffer is located. This was probably implemented to
    detect performance issues. Not sure if there actually is one as the
    buffers are copied to a single buffer anyway which should be the
    actual bottleneck.
    
    We wont change WQ implementation. Since a user can do nothing the
    warning is pointless. Removing it.
    
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index b8ef8ddcc292..8aa73fac6ad4 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -451,14 +451,9 @@ static void wq_sync_buffer(struct work_struct *work)
 {
 	struct oprofile_cpu_buffer *b =
 		container_of(work, struct oprofile_cpu_buffer, work.work);
-	if (b->cpu != smp_processor_id()) {
-		printk(KERN_DEBUG "WQ on CPU%d, prefer CPU%d\n",
-		       smp_processor_id(), b->cpu);
-
-		if (!cpu_online(b->cpu)) {
-			cancel_delayed_work(&b->work);
-			return;
-		}
+	if (b->cpu != smp_processor_id() && !cpu_online(b->cpu)) {
+		cancel_delayed_work(&b->work);
+		return;
 	}
 	sync_buffer(b->cpu);
 

commit 54ebbe7ba51d97a28a9a406203d171d61858e4b9
Author: Heinz Graalfs <graalfs@linux.vnet.ibm.com>
Date:   Fri Jan 21 10:06:54 2011 +0000

    oprofile: Introduce new oprofile sample add function (oprofile_add_ext_hw_sample)
    
    This patch introduces a new oprofile sample add function
    (oprofile_add_ext_hw_sample) that can also take task_struct as an
    argument, which is used by the hwsampler kernel module when copying
    hardware samples to OProfile buffers.
    
    Applied with following changes:
    * removed #include <linux/module.h>
    * whitespace changes
    * removed conditional compilation (CONFIG_HAVE_HWSAMPLER)
    * modified order of functions
    * fix missing function definition in header file
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Maran Pakkirisamy <maranp@linux.vnet.ibm.com>
    Signed-off-by: Heinz Graalfs <graalfs@linux.vnet.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 59f55441e075..b8ef8ddcc292 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -258,8 +258,10 @@ op_add_sample(struct oprofile_cpu_buffer *cpu_buf,
  */
 static int
 log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
-	   unsigned long backtrace, int is_kernel, unsigned long event)
+	   unsigned long backtrace, int is_kernel, unsigned long event,
+	   struct task_struct *task)
 {
+	struct task_struct *tsk = task ? task : current;
 	cpu_buf->sample_received++;
 
 	if (pc == ESCAPE_CODE) {
@@ -267,7 +269,7 @@ log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 		return 0;
 	}
 
-	if (op_add_code(cpu_buf, backtrace, is_kernel, current))
+	if (op_add_code(cpu_buf, backtrace, is_kernel, tsk))
 		goto fail;
 
 	if (op_add_sample(cpu_buf, pc, event))
@@ -292,7 +294,8 @@ static inline void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)
 
 static inline void
 __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
-			  unsigned long event, int is_kernel)
+			  unsigned long event, int is_kernel,
+			  struct task_struct *task)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 	unsigned long backtrace = oprofile_backtrace_depth;
@@ -301,7 +304,7 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	 * if log_sample() fail we can't backtrace since we lost the
 	 * source of this event
 	 */
-	if (!log_sample(cpu_buf, pc, backtrace, is_kernel, event))
+	if (!log_sample(cpu_buf, pc, backtrace, is_kernel, event, task))
 		/* failed */
 		return;
 
@@ -313,10 +316,17 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	oprofile_end_trace(cpu_buf);
 }
 
+void oprofile_add_ext_hw_sample(unsigned long pc, struct pt_regs * const regs,
+				unsigned long event, int is_kernel,
+				struct task_struct *task)
+{
+	__oprofile_add_ext_sample(pc, regs, event, is_kernel, task);
+}
+
 void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 			     unsigned long event, int is_kernel)
 {
-	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
+	__oprofile_add_ext_sample(pc, regs, event, is_kernel, NULL);
 }
 
 void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
@@ -332,7 +342,7 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 		pc = ESCAPE_CODE; /* as this causes an early return. */
 	}
 
-	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
+	__oprofile_add_ext_sample(pc, regs, event, is_kernel, NULL);
 }
 
 /*
@@ -403,7 +413,7 @@ int oprofile_write_commit(struct op_entry *entry)
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
-	log_sample(cpu_buf, pc, 0, is_kernel, event);
+	log_sample(cpu_buf, pc, 0, is_kernel, event, NULL);
 }
 
 void oprofile_add_trace(unsigned long pc)

commit 3d7851b3cdd43a734e5cc4c643fd886ab28ad4d5
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Oct 15 09:51:08 2010 -0400

    oprofile: Remove deprecated use of flush_scheduled_work()
    
    flush_scheduled_work() is deprecated and scheduled to be removed.
    sync_stop() currently cancels cpu_buffer works inside buffer_mutex and
    flushes the system workqueue outside.  Instead, split end_cpu_work()
    into two parts - stopping further work enqueues and flushing works -
    and do the former inside buffer_mutex and latter outside.
    
    For stable kernels v2.6.35.y and v2.6.36.y.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index f179ac2ea801..59f55441e075 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -111,14 +111,18 @@ void start_cpu_work(void)
 
 void end_cpu_work(void)
 {
-	int i;
-
 	work_enabled = 0;
+}
+
+void flush_cpu_work(void)
+{
+	int i;
 
 	for_each_online_cpu(i) {
 		struct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);
 
-		cancel_delayed_work(&b->work);
+		/* these works are per-cpu, no need for flush_sync */
+		flush_delayed_work(&b->work);
 	}
 }
 

commit 750d857c682f4db60d14722d430c7ccc35070962
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Aug 13 16:29:04 2010 +0200

    oprofile: fix crash when accessing freed task structs
    
    This patch fixes a crash during shutdown reported below. The crash is
    caused by accessing already freed task structs. The fix changes the
    order for registering and unregistering notifier callbacks.
    
    All notifiers must be initialized before buffers start working. To
    stop buffer synchronization we cancel all workqueues, unregister the
    notifier callback and then flush all buffers. After all of this we
    finally can free all tasks listed.
    
    This should avoid accessing freed tasks.
    
    On 22.07.10 01:14:40, Benjamin Herrenschmidt wrote:
    
    > So the initial observation is a spinlock bad magic followed by a crash
    > in the spinlock debug code:
    >
    > [ 1541.586531] BUG: spinlock bad magic on CPU#5, events/5/136
    > [ 1541.597564] Unable to handle kernel paging request for data at address 0x6b6b6b6b6b6b6d03
    >
    > Backtrace looks like:
    >
    >       spin_bug+0x74/0xd4
    >       ._raw_spin_lock+0x48/0x184
    >       ._spin_lock+0x10/0x24
    >       .get_task_mm+0x28/0x8c
    >       .sync_buffer+0x1b4/0x598
    >       .wq_sync_buffer+0xa0/0xdc
    >       .worker_thread+0x1d8/0x2a8
    >       .kthread+0xa8/0xb4
    >       .kernel_thread+0x54/0x70
    >
    > So we are accessing a freed task struct in the work queue when
    > processing the samples.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: stable@kernel.org
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 219f79e2210a..f179ac2ea801 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -120,8 +120,6 @@ void end_cpu_work(void)
 
 		cancel_delayed_work(&b->work);
 	}
-
-	flush_scheduled_work();
 }
 
 /*

commit 9414e99672271adcc661f3c160a30b374179b92f
Author: Phil Carmody <ext-phil.2.carmody@nokia.com>
Date:   Wed Apr 28 12:09:16 2010 -0500

    oprofile: protect from not being in an IRQ context
    
    http://lkml.org/lkml/2010/4/27/285
    
    Protect against dereferencing regs when it's NULL, and
    force a magic number into pc to prevent too deep processing.
    This approach permits the dropped samples to be tallied as
    invalid Instruction Pointer events.
    
    e.g. output from about 15mins at 10kHz sample rate:
    Nr. samples received: 2565380
    Nr. samples lost invalid pc: 4
    
    Signed-off-by: Phil Carmody <ext-phil.2.carmody@nokia.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 0ac8b065ee02..219f79e2210a 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -319,8 +319,16 @@ void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 
 void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 {
-	int is_kernel = !user_mode(regs);
-	unsigned long pc = profile_pc(regs);
+	int is_kernel;
+	unsigned long pc;
+
+	if (likely(regs)) {
+		is_kernel = !user_mode(regs);
+		pc = profile_pc(regs);
+	} else {
+		is_kernel = 0;    /* This value will not be used */
+		pc = ESCAPE_CODE; /* as this causes an early return. */
+	}
 
 	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
 }

commit b971f06187d83b5c03d2b597cccdfef421c0ca91
Merge: cb6e943ccf19 c1ab9cab7509
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Apr 23 16:47:51 2010 +0200

    Merge commit 'tip/tracing/core' into oprofile/core
    
    Conflicts:
            drivers/oprofile/cpu_buffer.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

commit cb6e943ccf19ab6d3189147e9d625a992e016084
Author: Andi Kleen <andi@firstfloor.org>
Date:   Thu Apr 1 03:17:25 2010 +0200

    oprofile: remove double ring buffering
    
    oprofile used a double buffer scheme for its cpu event buffer
    to avoid races on reading with the old locked ring buffer.
    
    But that is obsolete now with the new ring buffer, so simply
    use a single buffer. This greatly simplifies the code and avoids
    a lot of sample drops on large runs, especially with call graph.
    
    Based on suggestions from Steven Rostedt
    
    For stable kernels from v2.6.32, but not earlier.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: stable <stable@kernel.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 166b67ea622f..de82183bb9b3 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -30,23 +30,7 @@
 
 #define OP_BUFFER_FLAGS	0
 
-/*
- * Read and write access is using spin locking. Thus, writing to the
- * buffer by NMI handler (x86) could occur also during critical
- * sections when reading the buffer. To avoid this, there are 2
- * buffers for independent read and write access. Read access is in
- * process context only, write access only in the NMI handler. If the
- * read buffer runs empty, both buffers are swapped atomically. There
- * is potentially a small window during swapping where the buffers are
- * disabled and samples could be lost.
- *
- * Using 2 buffers is a little bit overhead, but the solution is clear
- * and does not require changes in the ring buffer implementation. It
- * can be changed to a single buffer solution when the ring buffer
- * access is implemented as non-locking atomic code.
- */
-static struct ring_buffer *op_ring_buffer_read;
-static struct ring_buffer *op_ring_buffer_write;
+static struct ring_buffer *op_ring_buffer;
 DEFINE_PER_CPU(struct oprofile_cpu_buffer, op_cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
@@ -68,12 +52,9 @@ void oprofile_cpu_buffer_inc_smpl_lost(void)
 
 void free_cpu_buffers(void)
 {
-	if (op_ring_buffer_read)
-		ring_buffer_free(op_ring_buffer_read);
-	op_ring_buffer_read = NULL;
-	if (op_ring_buffer_write)
-		ring_buffer_free(op_ring_buffer_write);
-	op_ring_buffer_write = NULL;
+	if (op_ring_buffer)
+		ring_buffer_free(op_ring_buffer);
+	op_ring_buffer = NULL;
 }
 
 #define RB_EVENT_HDR_SIZE 4
@@ -86,11 +67,8 @@ int alloc_cpu_buffers(void)
 	unsigned long byte_size = buffer_size * (sizeof(struct op_sample) +
 						 RB_EVENT_HDR_SIZE);
 
-	op_ring_buffer_read = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);
-	if (!op_ring_buffer_read)
-		goto fail;
-	op_ring_buffer_write = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);
-	if (!op_ring_buffer_write)
+	op_ring_buffer = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);
+	if (!op_ring_buffer)
 		goto fail;
 
 	for_each_possible_cpu(i) {
@@ -162,16 +140,11 @@ struct op_sample
 *op_cpu_buffer_write_reserve(struct op_entry *entry, unsigned long size)
 {
 	entry->event = ring_buffer_lock_reserve
-		(op_ring_buffer_write, sizeof(struct op_sample) +
+		(op_ring_buffer, sizeof(struct op_sample) +
 		 size * sizeof(entry->sample->data[0]));
-	if (entry->event)
-		entry->sample = ring_buffer_event_data(entry->event);
-	else
-		entry->sample = NULL;
-
-	if (!entry->sample)
+	if (!entry->event)
 		return NULL;
-
+	entry->sample = ring_buffer_event_data(entry->event);
 	entry->size = size;
 	entry->data = entry->sample->data;
 
@@ -180,25 +153,16 @@ struct op_sample
 
 int op_cpu_buffer_write_commit(struct op_entry *entry)
 {
-	return ring_buffer_unlock_commit(op_ring_buffer_write, entry->event);
+	return ring_buffer_unlock_commit(op_ring_buffer, entry->event);
 }
 
 struct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)
 {
 	struct ring_buffer_event *e;
-	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
-	if (e)
-		goto event;
-	if (ring_buffer_swap_cpu(op_ring_buffer_read,
-				 op_ring_buffer_write,
-				 cpu))
+	e = ring_buffer_consume(op_ring_buffer, cpu, NULL);
+	if (!e)
 		return NULL;
-	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
-	if (e)
-		goto event;
-	return NULL;
 
-event:
 	entry->event = e;
 	entry->sample = ring_buffer_event_data(e);
 	entry->size = (ring_buffer_event_length(e) - sizeof(struct op_sample))
@@ -209,8 +173,7 @@ struct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)
 
 unsigned long op_cpu_buffer_entries(int cpu)
 {
-	return ring_buffer_entries_cpu(op_ring_buffer_read, cpu)
-		+ ring_buffer_entries_cpu(op_ring_buffer_write, cpu);
+	return ring_buffer_entries_cpu(op_ring_buffer, cpu);
 }
 
 static int

commit 66a8cb95ed04025664d1db4e952155ee1dccd048
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 31 13:21:56 2010 -0400

    ring-buffer: Add place holder recording of dropped events
    
    Currently, when the ring buffer drops events, it does not record
    the fact that it did so. It does inform the writer that the event
    was dropped by returning a NULL event, but it does not put in any
    place holder where the event was dropped.
    
    This is not a trivial thing to add because the ring buffer mostly
    runs in overwrite (flight recorder) mode. That is, when the ring
    buffer is full, new data will overwrite old data.
    
    In a produce/consumer mode, where new data is simply dropped when
    the ring buffer is full, it is trivial to add the placeholder
    for dropped events. When there's more room to write new data, then
    a special event can be added to notify the reader about the dropped
    events.
    
    But in overwrite mode, any new write can overwrite events. A place
    holder can not be inserted into the ring buffer since there never
    may be room. A reader could also come in at anytime and miss the
    placeholder.
    
    Luckily, the way the ring buffer works, the read side can find out
    if events were lost or not, and how many events. Everytime a write
    takes place, if it overwrites the header page (the next read) it
    updates a "overrun" variable that keeps track of the number of
    lost events. When a reader swaps out a page from the ring buffer,
    it can record this number, perfom the swap, and then check to
    see if the number changed, and take the diff if it has, which would be
    the number of events dropped. This can be stored by the reader
    and returned to callers of the reader.
    
    Since the reader page swap will fail if the writer moved the head
    page since the time the reader page set up the swap, this gives room
    to record the overruns without worrying about races. If the reader
    sets up the pages, records the overrun, than performs the swap,
    if the swap succeeds, then the overrun variable has not been
    updated since the setup before the swap.
    
    For binary readers of the ring buffer, a flag is set in the header
    of each sub page (sub buffer) of the ring buffer. This flag is embedded
    in the size field of the data on the sub buffer, in the 31st bit (the size
    can be 32 or 64 bits depending on the architecture), but only 27
    bits needs to be used for the actual size (less actually).
    
    We could add a new field in the sub buffer header to also record the
    number of events dropped since the last read, but this will change the
    format of the binary ring buffer a bit too much. Perhaps this change can
    be made if the information on the number of events dropped is considered
    important enough.
    
    Note, the notification of dropped events is only used by consuming reads
    or peeking at the ring buffer. Iterating over the ring buffer does not
    keep this information because the necessary data is only available when
    a page swap is made, and the iterator does not swap out pages.
    
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "Luis Claudio R. Goncalves" <lclaudio@uudg.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 166b67ea622f..7581dbe456da 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -186,14 +186,14 @@ int op_cpu_buffer_write_commit(struct op_entry *entry)
 struct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)
 {
 	struct ring_buffer_event *e;
-	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
+	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL, NULL);
 	if (e)
 		goto event;
 	if (ring_buffer_swap_cpu(op_ring_buffer_read,
 				 op_ring_buffer_write,
 				 cpu))
 		return NULL;
-	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
+	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL, NULL);
 	if (e)
 		goto event;
 	return NULL;

commit b3e9f672b6cd0f4c2982c1bcc0b3c3fb39d3b0fe
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 29 22:34:13 2009 +0900

    percpu: make percpu symbols in oprofile unique
    
    This patch updates percpu related symbols in oprofile such that percpu
    symbols are unique and don't clash with local symbols.  This serves
    two purposes of decreasing the possibility of global percpu symbol
    collision and allowing dropping per_cpu__ prefix from percpu symbols.
    
    * drivers/oprofile/cpu_buffer.c: s/cpu_buffer/op_cpu_buffer/
    
    Partly based on Rusty Russell's "alloc_percpu: rename percpu vars
    which cause name clashes" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Robert Richter <robert.richter@amd.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index a7aae24f2889..166b67ea622f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -47,7 +47,7 @@
  */
 static struct ring_buffer *op_ring_buffer_read;
 static struct ring_buffer *op_ring_buffer_write;
-DEFINE_PER_CPU(struct oprofile_cpu_buffer, cpu_buffer);
+DEFINE_PER_CPU(struct oprofile_cpu_buffer, op_cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
 
@@ -61,8 +61,7 @@ unsigned long oprofile_get_cpu_buffer_size(void)
 
 void oprofile_cpu_buffer_inc_smpl_lost(void)
 {
-	struct oprofile_cpu_buffer *cpu_buf
-		= &__get_cpu_var(cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 
 	cpu_buf->sample_lost_overflow++;
 }
@@ -95,7 +94,7 @@ int alloc_cpu_buffers(void)
 		goto fail;
 
 	for_each_possible_cpu(i) {
-		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
+		struct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);
 
 		b->last_task = NULL;
 		b->last_is_kernel = -1;
@@ -122,7 +121,7 @@ void start_cpu_work(void)
 	work_enabled = 1;
 
 	for_each_online_cpu(i) {
-		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
+		struct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);
 
 		/*
 		 * Spread the work by 1 jiffy per cpu so they dont all
@@ -139,7 +138,7 @@ void end_cpu_work(void)
 	work_enabled = 0;
 
 	for_each_online_cpu(i) {
-		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
+		struct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);
 
 		cancel_delayed_work(&b->work);
 	}
@@ -330,7 +329,7 @@ static inline void
 __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 			  unsigned long event, int is_kernel)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 	unsigned long backtrace = oprofile_backtrace_depth;
 
 	/*
@@ -375,7 +374,7 @@ oprofile_write_reserve(struct op_entry *entry, struct pt_regs * const regs,
 {
 	struct op_sample *sample;
 	int is_kernel = !user_mode(regs);
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 
 	cpu_buf->sample_received++;
 
@@ -430,13 +429,13 @@ int oprofile_write_commit(struct op_entry *entry)
 
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 	log_sample(cpu_buf, pc, 0, is_kernel, event);
 }
 
 void oprofile_add_trace(unsigned long pc)
 {
-	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);
 
 	if (!cpu_buf->tracing)
 		return;

commit 51563a0e5650d0d76539625388d72d62b34c726e
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jun 3 20:54:56 2009 +0200

    x86/oprofile: introduce oprofile_add_data64()
    
    The IBS implemention writes 64 bit register values to the cpu buffer
    by writing two 32 values using oprofile_add_data(). This patch
    introduces oprofile_add_data64() to write a single 64 bit value to the
    buffer.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 50640cc5eef2..a7aae24f2889 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -406,6 +406,21 @@ int oprofile_add_data(struct op_entry *entry, unsigned long val)
 	return op_cpu_buffer_add_data(entry, val);
 }
 
+int oprofile_add_data64(struct op_entry *entry, u64 val)
+{
+	if (!entry->event)
+		return 0;
+	if (op_cpu_buffer_get_size(entry) < 2)
+		/*
+		 * the function returns 0 to indicate a too small
+		 * buffer, even if there is some space left
+		 */
+		return 0;
+	if (!op_cpu_buffer_add_data(entry, (u32)val))
+		return 0;
+	return op_cpu_buffer_add_data(entry, (u32)(val >> 32));
+}
+
 int oprofile_write_commit(struct op_entry *entry)
 {
 	if (!entry->event)

commit fecfe6320ba71eed6d16849683298f0894aa60de
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu May 7 16:07:41 2009 +0200

    oprofile: remove obselete include headers
    
    This became obsolete with this commit:
    
     6dad828 oprofile: port to the new ring_buffer
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 242257b19441..50640cc5eef2 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -21,7 +21,6 @@
 
 #include <linux/sched.h>
 #include <linux/oprofile.h>
-#include <linux/vmalloc.h>
 #include <linux/errno.h>
 
 #include "event_buffer.h"

commit 54f2c841fa0007e5fee3b7d01a911c774f0a6cda
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu May 7 17:28:59 2009 +0200

    oprofile: fix cpu buffer size
    
    The unit of oprofile_cpu_buffer_size is in samples, but was allocated
    in bytes. This led to the allocation of too small cpu buffers. This
    patch recalculates the buffer size in bytes taking also the
    ring_buffer_event header size into account.
    
    Reported-by: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index f0e99d4c066b..242257b19441 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -78,16 +78,20 @@ void free_cpu_buffers(void)
 	op_ring_buffer_write = NULL;
 }
 
+#define RB_EVENT_HDR_SIZE 4
+
 int alloc_cpu_buffers(void)
 {
 	int i;
 
 	unsigned long buffer_size = oprofile_cpu_buffer_size;
+	unsigned long byte_size = buffer_size * (sizeof(struct op_sample) +
+						 RB_EVENT_HDR_SIZE);
 
-	op_ring_buffer_read = ring_buffer_alloc(buffer_size, OP_BUFFER_FLAGS);
+	op_ring_buffer_read = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);
 	if (!op_ring_buffer_read)
 		goto fail;
-	op_ring_buffer_write = ring_buffer_alloc(buffer_size, OP_BUFFER_FLAGS);
+	op_ring_buffer_write = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);
 	if (!op_ring_buffer_write)
 		goto fail;
 

commit 304cc6ae1bf7a8e6d00053fbe0b7e2b26cdddda2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 6 01:12:02 2009 +0100

    ring_buffer: remove unused flags parameter, fix
    
    Oprofile's ring-buffer use was not considered.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e76d715e4342..f0e99d4c066b 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -161,7 +161,7 @@ struct op_sample
 {
 	entry->event = ring_buffer_lock_reserve
 		(op_ring_buffer_write, sizeof(struct op_sample) +
-		 size * sizeof(entry->sample->data[0]), &entry->irq_flags);
+		 size * sizeof(entry->sample->data[0]));
 	if (entry->event)
 		entry->sample = ring_buffer_event_data(entry->event);
 	else
@@ -178,8 +178,7 @@ struct op_sample
 
 int op_cpu_buffer_write_commit(struct op_entry *entry)
 {
-	return ring_buffer_unlock_commit(op_ring_buffer_write, entry->event,
-					 entry->irq_flags);
+	return ring_buffer_unlock_commit(op_ring_buffer_write, entry->event);
 }
 
 struct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)

commit fdb6a8f4db813b4e50f4e975efe6be12ba5bf460
Author: Robert Richter <robert.richter@amd.com>
Date:   Sat Jan 17 17:13:27 2009 +0100

    oprofile: fix uninitialized use of struct op_entry
    
    Impact: fix crash
    
    In case of losing samples struct op_entry could have been used
    uninitialized causing e.g. a wrong preemption count or NULL pointer
    access. This patch fixes this.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 2e03b6d796d3..e76d715e4342 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -393,16 +393,21 @@ oprofile_write_reserve(struct op_entry *entry, struct pt_regs * const regs,
 	return;
 
 fail:
+	entry->event = NULL;
 	cpu_buf->sample_lost_overflow++;
 }
 
 int oprofile_add_data(struct op_entry *entry, unsigned long val)
 {
+	if (!entry->event)
+		return 0;
 	return op_cpu_buffer_add_data(entry, val);
 }
 
 int oprofile_write_commit(struct op_entry *entry)
 {
+	if (!entry->event)
+		return -EINVAL;
 	return op_cpu_buffer_write_commit(entry);
 }
 

commit 14f0ca8eaea42a5b5a69cfcb699665dd2618db5f
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jan 7 21:50:22 2009 +0100

    oprofile: make new cpu buffer functions part of the api
    
    This patch creates the new functions
    
     oprofile_write_reserve()
     oprofile_add_data()
     oprofile_write_commit()
    
    and makes them part of the oprofile api.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index b846af632c81..2e03b6d796d3 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -364,10 +364,11 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 /*
  * Add samples with data to the ring buffer.
  *
- * Use op_cpu_buffer_add_data(&entry, val) to add data and
- * op_cpu_buffer_write_commit(&entry) to commit the sample.
+ * Use oprofile_add_data(&entry, val) to add data and
+ * oprofile_write_commit(&entry) to commit the sample.
  */
-void oprofile_add_data(struct op_entry *entry, struct pt_regs * const regs,
+void
+oprofile_write_reserve(struct op_entry *entry, struct pt_regs * const regs,
 		       unsigned long pc, int code, int size)
 {
 	struct op_sample *sample;
@@ -395,6 +396,16 @@ void oprofile_add_data(struct op_entry *entry, struct pt_regs * const regs,
 	cpu_buf->sample_lost_overflow++;
 }
 
+int oprofile_add_data(struct op_entry *entry, unsigned long val)
+{
+	return op_cpu_buffer_add_data(entry, val);
+}
+
+int oprofile_write_commit(struct op_entry *entry)
+{
+	return op_cpu_buffer_write_commit(entry);
+}
+
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);

commit ebf8d974e298018f0b4ee02b1b097bf5500d3d27
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Jan 7 00:20:57 2009 +0100

    oprofile: remove #ifdef CONFIG_OPROFILE_IBS in non-ibs code
    
    The ifdefs can be removed since the code is no longer ibs specific and
    can be used for other purposes as well. IBS specific code is only in
    op_model_amd.c.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index ddba9d01f09b..b846af632c81 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -361,8 +361,6 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
 }
 
-#ifdef CONFIG_OPROFILE_IBS
-
 /*
  * Add samples with data to the ring buffer.
  *
@@ -397,8 +395,6 @@ void oprofile_add_data(struct op_entry *entry, struct pt_regs * const regs,
 	cpu_buf->sample_lost_overflow++;
 }
 
-#endif
-
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);

commit 1acda878e20ea0cd3708ba66dca67d52eaafdd2b
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Jan 5 10:35:31 2009 +0100

    oprofile: use new data sample format for ibs
    
    The new ring buffer implementation allows the storage of samples with
    different size. This patch implements the usage of the new sample
    format to store ibs samples in the cpu buffer. Until now, writing to
    the cpu buffer could lead to incomplete sampling sequences since IBS
    samples were transfered in multiple samples. Due to a full buffer,
    data could be lost at any time. This can't happen any more since the
    complete data is reserved in advance and then stored in a single
    sample.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 1b6590746be4..ddba9d01f09b 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -363,31 +363,38 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 
 #ifdef CONFIG_OPROFILE_IBS
 
-void oprofile_add_ibs_sample(struct pt_regs * const regs,
-			     unsigned int * const ibs_sample, int ibs_code)
+/*
+ * Add samples with data to the ring buffer.
+ *
+ * Use op_cpu_buffer_add_data(&entry, val) to add data and
+ * op_cpu_buffer_write_commit(&entry) to commit the sample.
+ */
+void oprofile_add_data(struct op_entry *entry, struct pt_regs * const regs,
+		       unsigned long pc, int code, int size)
 {
+	struct op_sample *sample;
 	int is_kernel = !user_mode(regs);
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
-	int fail = 0;
 
 	cpu_buf->sample_received++;
 
-	/* backtraces disabled for ibs */
-	fail = fail || op_add_code(cpu_buf, 0, is_kernel, current);
+	/* no backtraces for samples with data */
+	if (op_add_code(cpu_buf, 0, is_kernel, current))
+		goto fail;
 
-	fail = fail || op_add_sample(cpu_buf, ESCAPE_CODE,   ibs_code);
-	fail = fail || op_add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
-	fail = fail || op_add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
-	fail = fail || op_add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
+	sample = op_cpu_buffer_write_reserve(entry, size + 2);
+	if (!sample)
+		goto fail;
+	sample->eip = ESCAPE_CODE;
+	sample->event = 0;		/* no flags */
 
-	if (ibs_code == IBS_OP_BEGIN) {
-		fail = fail || op_add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
-		fail = fail || op_add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
-		fail = fail || op_add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
-	}
+	op_cpu_buffer_add_data(entry, code);
+	op_cpu_buffer_add_data(entry, pc);
+
+	return;
 
-	if (fail)
-		cpu_buf->sample_lost_overflow++;
+fail:
+	cpu_buf->sample_lost_overflow++;
 }
 
 #endif

commit d9928c25a6960cf128c2078a89fe6f8e0180ff60
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 25 17:26:07 2008 +0100

    oprofile: add op_cpu_buffer_add_data()
    
    This function can be used to attach data to a sample. It returns the
    remaining free buffer size that has been reserved with
    op_cpu_buffer_write_reserve().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e859d23cfc57..1b6590746be4 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -258,7 +258,7 @@ op_add_code(struct oprofile_cpu_buffer *cpu_buf, unsigned long backtrace,
 	sample->event = flags;
 
 	if (size)
-		sample->data[0] = (unsigned long)task;
+		op_cpu_buffer_add_data(&entry, (unsigned long)task);
 
 	op_cpu_buffer_write_commit(&entry);
 

commit ae735e9964b4584923f2997d98a8d80ae9c1a75c
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 25 17:26:07 2008 +0100

    oprofile: rework implementation of cpu buffer events
    
    Special events such as task or context switches are marked with an
    escape code in the cpu buffer followed by an event code or a task
    identifier. There is one escape code per event. To make escape
    sequences also available for data samples the internal cpu buffer
    format must be changed. The current implementation does not allow the
    extension of event codes since this would lead to collisions with the
    task identifiers. To avoid this, this patch introduces an event mask
    that allows the storage of multiple events with one escape code. Now,
    task identifiers are stored in the data section of the sample. The
    implementation also allows the usage of custom data in a sample. As a
    side effect the new code is much more readable and easier to
    understand.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 400f7fcffdbe..e859d23cfc57 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -212,6 +212,59 @@ unsigned long op_cpu_buffer_entries(int cpu)
 		+ ring_buffer_entries_cpu(op_ring_buffer_write, cpu);
 }
 
+static int
+op_add_code(struct oprofile_cpu_buffer *cpu_buf, unsigned long backtrace,
+	    int is_kernel, struct task_struct *task)
+{
+	struct op_entry entry;
+	struct op_sample *sample;
+	unsigned long flags;
+	int size;
+
+	flags = 0;
+
+	if (backtrace)
+		flags |= TRACE_BEGIN;
+
+	/* notice a switch from user->kernel or vice versa */
+	is_kernel = !!is_kernel;
+	if (cpu_buf->last_is_kernel != is_kernel) {
+		cpu_buf->last_is_kernel = is_kernel;
+		flags |= KERNEL_CTX_SWITCH;
+		if (is_kernel)
+			flags |= IS_KERNEL;
+	}
+
+	/* notice a task switch */
+	if (cpu_buf->last_task != task) {
+		cpu_buf->last_task = task;
+		flags |= USER_CTX_SWITCH;
+	}
+
+	if (!flags)
+		/* nothing to do */
+		return 0;
+
+	if (flags & USER_CTX_SWITCH)
+		size = 1;
+	else
+		size = 0;
+
+	sample = op_cpu_buffer_write_reserve(&entry, size);
+	if (!sample)
+		return -ENOMEM;
+
+	sample->eip = ESCAPE_CODE;
+	sample->event = flags;
+
+	if (size)
+		sample->data[0] = (unsigned long)task;
+
+	op_cpu_buffer_write_commit(&entry);
+
+	return 0;
+}
+
 static inline int
 op_add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	      unsigned long pc, unsigned long event)
@@ -229,26 +282,18 @@ op_add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	return op_cpu_buffer_write_commit(&entry);
 }
 
-static inline int
-add_code(struct oprofile_cpu_buffer *buffer, unsigned long value)
-{
-	return op_add_sample(buffer, ESCAPE_CODE, value);
-}
-
-/* This must be safe from any context. It's safe writing here
- * because of the head/tail separation of the writer and reader
- * of the CPU buffer.
+/*
+ * This must be safe from any context.
  *
  * is_kernel is needed because on some architectures you cannot
  * tell if you are in kernel or user space simply by looking at
  * pc. We tag this in the buffer by generating kernel enter/exit
  * events whenever is_kernel changes
  */
-static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
-		      int is_kernel, unsigned long event)
+static int
+log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
+	   unsigned long backtrace, int is_kernel, unsigned long event)
 {
-	struct task_struct *task;
-
 	cpu_buf->sample_received++;
 
 	if (pc == ESCAPE_CODE) {
@@ -256,23 +301,8 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 		return 0;
 	}
 
-	is_kernel = !!is_kernel;
-
-	task = current;
-
-	/* notice a switch from user->kernel or vice versa */
-	if (cpu_buf->last_is_kernel != is_kernel) {
-		cpu_buf->last_is_kernel = is_kernel;
-		if (add_code(cpu_buf, is_kernel))
-			goto fail;
-	}
-
-	/* notice a task switch */
-	if (cpu_buf->last_task != task) {
-		cpu_buf->last_task = task;
-		if (add_code(cpu_buf, (unsigned long)task))
-			goto fail;
-	}
+	if (op_add_code(cpu_buf, backtrace, is_kernel, current))
+		goto fail;
 
 	if (op_add_sample(cpu_buf, pc, event))
 		goto fail;
@@ -286,7 +316,6 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 
 static inline void oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
-	add_code(cpu_buf, CPU_TRACE_BEGIN);
 	cpu_buf->tracing = 1;
 }
 
@@ -300,21 +329,21 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 			  unsigned long event, int is_kernel)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
-
-	if (!oprofile_backtrace_depth) {
-		log_sample(cpu_buf, pc, is_kernel, event);
-		return;
-	}
-
-	oprofile_begin_trace(cpu_buf);
+	unsigned long backtrace = oprofile_backtrace_depth;
 
 	/*
 	 * if log_sample() fail we can't backtrace since we lost the
 	 * source of this event
 	 */
-	if (log_sample(cpu_buf, pc, is_kernel, event))
-		oprofile_ops.backtrace(regs, oprofile_backtrace_depth);
+	if (!log_sample(cpu_buf, pc, backtrace, is_kernel, event))
+		/* failed */
+		return;
 
+	if (!backtrace)
+		return;
+
+	oprofile_begin_trace(cpu_buf);
+	oprofile_ops.backtrace(regs, backtrace);
 	oprofile_end_trace(cpu_buf);
 }
 
@@ -339,29 +368,14 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 {
 	int is_kernel = !user_mode(regs);
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
-	struct task_struct *task;
 	int fail = 0;
 
 	cpu_buf->sample_received++;
 
-	/* notice a switch from user->kernel or vice versa */
-	if (cpu_buf->last_is_kernel != is_kernel) {
-		if (add_code(cpu_buf, is_kernel))
-			goto fail;
-		cpu_buf->last_is_kernel = is_kernel;
-	}
+	/* backtraces disabled for ibs */
+	fail = fail || op_add_code(cpu_buf, 0, is_kernel, current);
 
-	/* notice a task switch */
-	if (!is_kernel) {
-		task = current;
-		if (cpu_buf->last_task != task) {
-			if (add_code(cpu_buf, (unsigned long)task))
-				goto fail;
-			cpu_buf->last_task = task;
-		}
-	}
-
-	fail = fail || add_code(cpu_buf, ibs_code);
+	fail = fail || op_add_sample(cpu_buf, ESCAPE_CODE,   ibs_code);
 	fail = fail || op_add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
 	fail = fail || op_add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
 	fail = fail || op_add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
@@ -372,11 +386,8 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 		fail = fail || op_add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
 	}
 
-	if (!fail)
-		return;
-
-fail:
-	cpu_buf->sample_lost_overflow++;
+	if (fail)
+		cpu_buf->sample_lost_overflow++;
 }
 
 #endif
@@ -384,7 +395,7 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
-	log_sample(cpu_buf, pc, is_kernel, event);
+	log_sample(cpu_buf, pc, 0, is_kernel, event);
 }
 
 void oprofile_add_trace(unsigned long pc)

commit 2d87b14cf8d0b07720de26d90789d02124141616
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 30 04:10:46 2008 +0100

    oprofile: modify op_cpu_buffer_read_entry()
    
    This implements the support of samples with attached data.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 934ff159e70e..400f7fcffdbe 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -182,20 +182,28 @@ int op_cpu_buffer_write_commit(struct op_entry *entry)
 					 entry->irq_flags);
 }
 
-struct op_sample *op_cpu_buffer_read_entry(int cpu)
+struct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)
 {
 	struct ring_buffer_event *e;
 	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
 	if (e)
-		return ring_buffer_event_data(e);
+		goto event;
 	if (ring_buffer_swap_cpu(op_ring_buffer_read,
 				 op_ring_buffer_write,
 				 cpu))
 		return NULL;
 	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
 	if (e)
-		return ring_buffer_event_data(e);
+		goto event;
 	return NULL;
+
+event:
+	entry->event = e;
+	entry->sample = ring_buffer_event_data(e);
+	entry->size = (ring_buffer_event_length(e) - sizeof(struct op_sample))
+		/ sizeof(entry->sample->data[0]);
+	entry->data = entry->sample->data;
+	return entry->sample;
 }
 
 unsigned long op_cpu_buffer_entries(int cpu)

commit 2cc28b9f261dd28d69767a34682ce55a27d928ed
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 25 17:26:07 2008 +0100

    oprofile: add op_cpu_buffer_write_reserve()
    
    This function prepares the cpu buffer to write a sample.
    
    Struct op_entry is used during operations on the ring buffer while
    struct op_sample contains the data that is stored in the ring
    buffer. Struct entry can be uninitialized. The function reserves a
    data array that is specified by size. Use op_cpu_buffer_write_commit()
    after preparing the sample. In case of errors a null pointer is
    returned, otherwise the pointer to the sample.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index ac79f6676033..934ff159e70e 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -1,11 +1,12 @@
 /**
  * @file cpu_buffer.c
  *
- * @remark Copyright 2002 OProfile authors
+ * @remark Copyright 2002-2009 OProfile authors
  * @remark Read the file COPYING
  *
  * @author John Levon <levon@movementarian.org>
  * @author Barry Kasindorf <barry.kasindorf@amd.com>
+ * @author Robert Richter <robert.richter@amd.com>
  *
  * Each CPU has a local buffer that stores PC value/event
  * pairs. We also log context switches when we notice them.
@@ -143,20 +144,36 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
-int op_cpu_buffer_write_entry(struct op_entry *entry)
+/*
+ * This function prepares the cpu buffer to write a sample.
+ *
+ * Struct op_entry is used during operations on the ring buffer while
+ * struct op_sample contains the data that is stored in the ring
+ * buffer. Struct entry can be uninitialized. The function reserves a
+ * data array that is specified by size. Use
+ * op_cpu_buffer_write_commit() after preparing the sample. In case of
+ * errors a null pointer is returned, otherwise the pointer to the
+ * sample.
+ *
+ */
+struct op_sample
+*op_cpu_buffer_write_reserve(struct op_entry *entry, unsigned long size)
 {
-	entry->event = ring_buffer_lock_reserve(op_ring_buffer_write,
-						sizeof(struct op_sample),
-						&entry->irq_flags);
+	entry->event = ring_buffer_lock_reserve
+		(op_ring_buffer_write, sizeof(struct op_sample) +
+		 size * sizeof(entry->sample->data[0]), &entry->irq_flags);
 	if (entry->event)
 		entry->sample = ring_buffer_event_data(entry->event);
 	else
 		entry->sample = NULL;
 
 	if (!entry->sample)
-		return -ENOMEM;
+		return NULL;
 
-	return 0;
+	entry->size = size;
+	entry->data = entry->sample->data;
+
+	return entry->sample;
 }
 
 int op_cpu_buffer_write_commit(struct op_entry *entry)
@@ -192,14 +209,14 @@ op_add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	      unsigned long pc, unsigned long event)
 {
 	struct op_entry entry;
-	int ret;
+	struct op_sample *sample;
 
-	ret = op_cpu_buffer_write_entry(&entry);
-	if (ret)
-		return ret;
+	sample = op_cpu_buffer_write_reserve(&entry, 0);
+	if (!sample)
+		return -ENOMEM;
 
-	entry.sample->eip = pc;
-	entry.sample->event = event;
+	sample->eip = pc;
+	sample->event = event;
 
 	return op_cpu_buffer_write_commit(&entry);
 }

commit d0e233846dcef56ae78f6d8fd0e0cba85a2a1489
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 23 04:03:05 2008 +0100

    oprofile: rename add_sample() in cpu_buffer.c
    
    Rename the fucntion to op_add_sample() since there is a collision with
    another one with the same name in buffer_sync.c.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 92bf8c0d86fe..ac79f6676033 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -188,8 +188,8 @@ unsigned long op_cpu_buffer_entries(int cpu)
 }
 
 static inline int
-add_sample(struct oprofile_cpu_buffer *cpu_buf,
-	   unsigned long pc, unsigned long event)
+op_add_sample(struct oprofile_cpu_buffer *cpu_buf,
+	      unsigned long pc, unsigned long event)
 {
 	struct op_entry entry;
 	int ret;
@@ -207,7 +207,7 @@ add_sample(struct oprofile_cpu_buffer *cpu_buf,
 static inline int
 add_code(struct oprofile_cpu_buffer *buffer, unsigned long value)
 {
-	return add_sample(buffer, ESCAPE_CODE, value);
+	return op_add_sample(buffer, ESCAPE_CODE, value);
 }
 
 /* This must be safe from any context. It's safe writing here
@@ -249,7 +249,7 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 			goto fail;
 	}
 
-	if (add_sample(cpu_buf, pc, event))
+	if (op_add_sample(cpu_buf, pc, event))
 		goto fail;
 
 	return 1;
@@ -337,14 +337,14 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 	}
 
 	fail = fail || add_code(cpu_buf, ibs_code);
-	fail = fail || add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
-	fail = fail || add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
-	fail = fail || add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
+	fail = fail || op_add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
+	fail = fail || op_add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
+	fail = fail || op_add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
 
 	if (ibs_code == IBS_OP_BEGIN) {
-		fail = fail || add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
-		fail = fail || add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
-		fail = fail || add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
+		fail = fail || op_add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
+		fail = fail || op_add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
+		fail = fail || op_add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
 	}
 
 	if (!fail)
@@ -376,7 +376,7 @@ void oprofile_add_trace(unsigned long pc)
 	if (pc == ESCAPE_CODE)
 		goto fail;
 
-	if (add_sample(cpu_buf, pc, 0))
+	if (op_add_sample(cpu_buf, pc, 0))
 		goto fail;
 
 	return;

commit 8350c78734e67ac1f8bfd4eb14b70ff4d01a9a12
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Dec 19 12:59:28 2008 +0100

    oprofile: remove backtrace code for ibs
    
    This code is broken since a TRACE_BEGIN_CODE is never sent to the
    daemon. The data becomes corrupt since the backtrace is interpreted as
    ibs sample.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 8ae37c9d0ec4..92bf8c0d86fe 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -347,17 +347,11 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 		fail = fail || add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
 	}
 
-	if (fail)
-		goto fail;
-
-	if (oprofile_backtrace_depth)
-		oprofile_ops.backtrace(regs, oprofile_backtrace_depth);
-
-	return;
+	if (!fail)
+		return;
 
 fail:
 	cpu_buf->sample_lost_overflow++;
-	return;
 }
 
 #endif

commit f4ff2364417f0092e49f6a3aa474549a56697f2d
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Jan 5 11:27:52 2009 +0100

    oprofile: remove unused ibs macro
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index b426ae8ad2e2..8ae37c9d0ec4 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -309,8 +309,6 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 
 #ifdef CONFIG_OPROFILE_IBS
 
-#define MAX_IBS_SAMPLE_SIZE 14
-
 void oprofile_add_ibs_sample(struct pt_regs * const regs,
 			     unsigned int * const ibs_sample, int ibs_code)
 {

commit 8d15df84a42b140a8262a325b987a283ef9f5f63
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Dec 24 15:42:58 2008 +0100

    oprofile: remove unused components in struct oprofile_cpu_buffer
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index d92f0020502e..b426ae8ad2e2 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -97,8 +97,6 @@ int alloc_cpu_buffers(void)
 		b->last_is_kernel = -1;
 		b->tracing = 0;
 		b->buffer_size = buffer_size;
-		b->tail_pos = 0;
-		b->head_pos = 0;
 		b->sample_received = 0;
 		b->sample_lost_overflow = 0;
 		b->backtrace_aborted = 0;

commit 3967e93e063d7ee608f465cbccb65abb518e9d33
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 30 05:10:58 2008 +0100

    oprofile: simplify add_sample() in cpu_buffer.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 435bd6e08d5b..d92f0020502e 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -203,11 +203,7 @@ add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	entry.sample->eip = pc;
 	entry.sample->event = event;
 
-	ret = op_cpu_buffer_write_commit(&entry);
-	if (ret)
-		return ret;
-
-	return 0;
+	return op_cpu_buffer_write_commit(&entry);
 }
 
 static inline int

commit 6352d92dec0c4b833c12a169e86762c05d0396f3
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 18 22:09:13 2008 +0100

    oprofile: simplify oprofile_begin_trace()
    
    This patch removes the unused return parameter in
    oprofile_begin_trace(). Also, oprofile_begin_trace() and
    oprofile_end_trace() are inline now.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 9e66c384e016..435bd6e08d5b 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -265,14 +265,13 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 	return 0;
 }
 
-static int oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
+static inline void oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
 	add_code(cpu_buf, CPU_TRACE_BEGIN);
 	cpu_buf->tracing = 1;
-	return 1;
 }
 
-static void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)
+static inline void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
 	cpu_buf->tracing = 0;
 }
@@ -288,8 +287,7 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 		return;
 	}
 
-	if (!oprofile_begin_trace(cpu_buf))
-		return;
+	oprofile_begin_trace(cpu_buf);
 
 	/*
 	 * if log_sample() fail we can't backtrace since we lost the
@@ -297,6 +295,7 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	 */
 	if (log_sample(cpu_buf, pc, is_kernel, event))
 		oprofile_ops.backtrace(regs, oprofile_backtrace_depth);
+
 	oprofile_end_trace(cpu_buf);
 }
 

commit d45d23bed4bf7b25b7dcc336552a251db1aa1279
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 16 12:00:10 2008 +0100

    oprofile: add inline function __oprofile_add_ext_sample()
    
    This patch adds the inline function __oprofile_add_ext_sample() to
    cpu_buffer.c and thus reduces overhead when calling
    oprofile_add_sample().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index b353b19bd786..9e66c384e016 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -277,8 +277,9 @@ static void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)
 	cpu_buf->tracing = 0;
 }
 
-void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
-				unsigned long event, int is_kernel)
+static inline void
+__oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
+			  unsigned long event, int is_kernel)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 
@@ -299,12 +300,18 @@ void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	oprofile_end_trace(cpu_buf);
 }
 
+void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
+			     unsigned long event, int is_kernel)
+{
+	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
+}
+
 void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 {
 	int is_kernel = !user_mode(regs);
 	unsigned long pc = profile_pc(regs);
 
-	oprofile_add_ext_sample(pc, regs, event, is_kernel);
+	__oprofile_add_ext_sample(pc, regs, event, is_kernel);
 }
 
 #ifdef CONFIG_OPROFILE_IBS

commit 300157768f050dabc73a99d958b504282088a132
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 23 01:35:12 2008 +0100

    oprofile: reordering some code in cpu_buffer.c
    
    Reordering code to keep alloc/free functions together.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index cd67d4dd30b7..b353b19bd786 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -54,16 +54,6 @@ static void wq_sync_buffer(struct work_struct *work);
 #define DEFAULT_TIMER_EXPIRE (HZ / 10)
 static int work_enabled;
 
-void free_cpu_buffers(void)
-{
-	if (op_ring_buffer_read)
-		ring_buffer_free(op_ring_buffer_read);
-	op_ring_buffer_read = NULL;
-	if (op_ring_buffer_write)
-		ring_buffer_free(op_ring_buffer_write);
-	op_ring_buffer_write = NULL;
-}
-
 unsigned long oprofile_get_cpu_buffer_size(void)
 {
 	return oprofile_cpu_buffer_size;
@@ -77,6 +67,16 @@ void oprofile_cpu_buffer_inc_smpl_lost(void)
 	cpu_buf->sample_lost_overflow++;
 }
 
+void free_cpu_buffers(void)
+{
+	if (op_ring_buffer_read)
+		ring_buffer_free(op_ring_buffer_read);
+	op_ring_buffer_read = NULL;
+	if (op_ring_buffer_write)
+		ring_buffer_free(op_ring_buffer_write);
+	op_ring_buffer_write = NULL;
+}
+
 int alloc_cpu_buffers(void)
 {
 	int i;

commit 9966718daee592fbdc523703b2d8200009642506
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 16 16:19:54 2008 +0100

    oprofile: remove ring buffer inline functions in cpu_buffer.h
    
    This patch moves ring buffer inline functions to cpu_buffer.c.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e52c085cd186..cd67d4dd30b7 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -45,8 +45,8 @@
  * can be changed to a single buffer solution when the ring buffer
  * access is implemented as non-locking atomic code.
  */
-struct ring_buffer *op_ring_buffer_read;
-struct ring_buffer *op_ring_buffer_write;
+static struct ring_buffer *op_ring_buffer_read;
+static struct ring_buffer *op_ring_buffer_write;
 DEFINE_PER_CPU(struct oprofile_cpu_buffer, cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
@@ -145,6 +145,50 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
+int op_cpu_buffer_write_entry(struct op_entry *entry)
+{
+	entry->event = ring_buffer_lock_reserve(op_ring_buffer_write,
+						sizeof(struct op_sample),
+						&entry->irq_flags);
+	if (entry->event)
+		entry->sample = ring_buffer_event_data(entry->event);
+	else
+		entry->sample = NULL;
+
+	if (!entry->sample)
+		return -ENOMEM;
+
+	return 0;
+}
+
+int op_cpu_buffer_write_commit(struct op_entry *entry)
+{
+	return ring_buffer_unlock_commit(op_ring_buffer_write, entry->event,
+					 entry->irq_flags);
+}
+
+struct op_sample *op_cpu_buffer_read_entry(int cpu)
+{
+	struct ring_buffer_event *e;
+	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
+	if (e)
+		return ring_buffer_event_data(e);
+	if (ring_buffer_swap_cpu(op_ring_buffer_read,
+				 op_ring_buffer_write,
+				 cpu))
+		return NULL;
+	e = ring_buffer_consume(op_ring_buffer_read, cpu, NULL);
+	if (e)
+		return ring_buffer_event_data(e);
+	return NULL;
+}
+
+unsigned long op_cpu_buffer_entries(int cpu)
+{
+	return ring_buffer_entries_cpu(op_ring_buffer_read, cpu)
+		+ ring_buffer_entries_cpu(op_ring_buffer_write, cpu);
+}
+
 static inline int
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)

commit 6d2c53f3cd81e33eec17aa99845d43e599986982
Author: Robert Richter <robert.richter@amd.com>
Date:   Wed Dec 24 16:53:53 2008 +0100

    oprofile: rename cpu buffer functions
    
    This patch renames cpu buffer functions to something more oprofile
    specific names. Functions will be moved to the global name space.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index fcf96f608d86..e52c085cd186 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -152,14 +152,14 @@ add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	struct op_entry entry;
 	int ret;
 
-	ret = cpu_buffer_write_entry(&entry);
+	ret = op_cpu_buffer_write_entry(&entry);
 	if (ret)
 		return ret;
 
 	entry.sample->eip = pc;
 	entry.sample->event = event;
 
-	ret = cpu_buffer_write_commit(&entry);
+	ret = op_cpu_buffer_write_commit(&entry);
 	if (ret)
 		return ret;
 

commit bd2172f58094b3f8afa017e68f3f0b57577824e1
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 16 16:19:54 2008 +0100

    oprofile: rename kernel-wide identifiers
    
    This patch renames kernel-wide identifiers to something more oprofile
    specific names.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 61090969158f..fcf96f608d86 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -66,7 +66,7 @@ void free_cpu_buffers(void)
 
 unsigned long oprofile_get_cpu_buffer_size(void)
 {
-	return fs_cpu_buffer_size;
+	return oprofile_cpu_buffer_size;
 }
 
 void oprofile_cpu_buffer_inc_smpl_lost(void)
@@ -81,7 +81,7 @@ int alloc_cpu_buffers(void)
 {
 	int i;
 
-	unsigned long buffer_size = fs_cpu_buffer_size;
+	unsigned long buffer_size = oprofile_cpu_buffer_size;
 
 	op_ring_buffer_read = ring_buffer_alloc(buffer_size, OP_BUFFER_FLAGS);
 	if (!op_ring_buffer_read)
@@ -238,7 +238,7 @@ void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 
-	if (!backtrace_depth) {
+	if (!oprofile_backtrace_depth) {
 		log_sample(cpu_buf, pc, is_kernel, event);
 		return;
 	}
@@ -251,7 +251,7 @@ void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	 * source of this event
 	 */
 	if (log_sample(cpu_buf, pc, is_kernel, event))
-		oprofile_ops.backtrace(regs, backtrace_depth);
+		oprofile_ops.backtrace(regs, oprofile_backtrace_depth);
 	oprofile_end_trace(cpu_buf);
 }
 
@@ -308,8 +308,8 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 	if (fail)
 		goto fail;
 
-	if (backtrace_depth)
-		oprofile_ops.backtrace(regs, backtrace_depth);
+	if (oprofile_backtrace_depth)
+		oprofile_ops.backtrace(regs, oprofile_backtrace_depth);
 
 	return;
 

commit 211117ff09b7d81d91b7857651587128ed8b13d9
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 9 02:13:25 2008 +0100

    oprofile: fix lost sample counter
    
    The number of lost samples could be greater than the number of
    received samples. This patches fixes this. The implementation
    introduces return values for add_sample() and add_code().
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 7f7fc958297a..61090969158f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -145,32 +145,31 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
-static inline void
+static inline int
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)
 {
 	struct op_entry entry;
+	int ret;
 
-	if (cpu_buffer_write_entry(&entry))
-		goto Error;
+	ret = cpu_buffer_write_entry(&entry);
+	if (ret)
+		return ret;
 
 	entry.sample->eip = pc;
 	entry.sample->event = event;
 
-	if (cpu_buffer_write_commit(&entry))
-		goto Error;
+	ret = cpu_buffer_write_commit(&entry);
+	if (ret)
+		return ret;
 
-	return;
-
-Error:
-	cpu_buf->sample_lost_overflow++;
-	return;
+	return 0;
 }
 
-static inline void
+static inline int
 add_code(struct oprofile_cpu_buffer *buffer, unsigned long value)
 {
-	add_sample(buffer, ESCAPE_CODE, value);
+	return add_sample(buffer, ESCAPE_CODE, value);
 }
 
 /* This must be safe from any context. It's safe writing here
@@ -201,17 +200,25 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 	/* notice a switch from user->kernel or vice versa */
 	if (cpu_buf->last_is_kernel != is_kernel) {
 		cpu_buf->last_is_kernel = is_kernel;
-		add_code(cpu_buf, is_kernel);
+		if (add_code(cpu_buf, is_kernel))
+			goto fail;
 	}
 
 	/* notice a task switch */
 	if (cpu_buf->last_task != task) {
 		cpu_buf->last_task = task;
-		add_code(cpu_buf, (unsigned long)task);
+		if (add_code(cpu_buf, (unsigned long)task))
+			goto fail;
 	}
 
-	add_sample(cpu_buf, pc, event);
+	if (add_sample(cpu_buf, pc, event))
+		goto fail;
+
 	return 1;
+
+fail:
+	cpu_buf->sample_lost_overflow++;
+	return 0;
 }
 
 static int oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
@@ -266,37 +273,49 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 	int is_kernel = !user_mode(regs);
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 	struct task_struct *task;
+	int fail = 0;
 
 	cpu_buf->sample_received++;
 
 	/* notice a switch from user->kernel or vice versa */
 	if (cpu_buf->last_is_kernel != is_kernel) {
+		if (add_code(cpu_buf, is_kernel))
+			goto fail;
 		cpu_buf->last_is_kernel = is_kernel;
-		add_code(cpu_buf, is_kernel);
 	}
 
 	/* notice a task switch */
 	if (!is_kernel) {
 		task = current;
 		if (cpu_buf->last_task != task) {
+			if (add_code(cpu_buf, (unsigned long)task))
+				goto fail;
 			cpu_buf->last_task = task;
-			add_code(cpu_buf, (unsigned long)task);
 		}
 	}
 
-	add_code(cpu_buf, ibs_code);
-	add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
-	add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
-	add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
+	fail = fail || add_code(cpu_buf, ibs_code);
+	fail = fail || add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
+	fail = fail || add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
+	fail = fail || add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
 
 	if (ibs_code == IBS_OP_BEGIN) {
-		add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
-		add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
-		add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
+		fail = fail || add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
+		fail = fail || add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
+		fail = fail || add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
 	}
 
+	if (fail)
+		goto fail;
+
 	if (backtrace_depth)
 		oprofile_ops.backtrace(regs, backtrace_depth);
+
+	return;
+
+fail:
+	cpu_buf->sample_lost_overflow++;
+	return;
 }
 
 #endif
@@ -318,13 +337,17 @@ void oprofile_add_trace(unsigned long pc)
 	 * broken frame can give an eip with the same value as an
 	 * escape code, abort the trace if we get it
 	 */
-	if (pc == ESCAPE_CODE) {
-		cpu_buf->tracing = 0;
-		cpu_buf->backtrace_aborted++;
-		return;
-	}
+	if (pc == ESCAPE_CODE)
+		goto fail;
+
+	if (add_sample(cpu_buf, pc, 0))
+		goto fail;
 
-	add_sample(cpu_buf, pc, 0);
+	return;
+fail:
+	cpu_buf->tracing = 0;
+	cpu_buf->backtrace_aborted++;
+	return;
 }
 
 /*

commit 1d7503b5dccf2b95babca050e4960e10d2633f2b
Author: Robert Richter <robert.richter@amd.com>
Date:   Mon Dec 8 11:59:52 2008 +0100

    oprofile: remove nr_available_slots()
    
    This function is no longer available after the port to the new ring
    buffer. Its removal can lead to incomplete sampling sequences since
    IBS samples and backtraces are transfered in multiple samples. Due to
    a full buffer, samples could be lost any time. The userspace daemon
    has to live with such incomplete sampling sequences as long as the
    data within one sample is consistent.
    
    This will be fixed by changing the internal buffer data there all data
    of one IBS sample or a backtrace is packed in a single ring buffer
    entry. This is possible since the new ring buffer supports variable
    data size.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index eb280ec96e24..7f7fc958297a 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -145,18 +145,6 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
-/* compute number of available slots in cpu_buffer queue */
-static unsigned long nr_available_slots(struct oprofile_cpu_buffer const *b)
-{
-	unsigned long head = b->head_pos;
-	unsigned long tail = b->tail_pos;
-
-	if (tail > head)
-		return (tail - head) - 1;
-
-	return tail + (b->buffer_size - head) - 1;
-}
-
 static inline void
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)
@@ -206,11 +194,6 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 		return 0;
 	}
 
-	if (nr_available_slots(cpu_buf) < 3) {
-		cpu_buf->sample_lost_overflow++;
-		return 0;
-	}
-
 	is_kernel = !!is_kernel;
 
 	task = current;
@@ -233,11 +216,6 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 
 static int oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
-	if (nr_available_slots(cpu_buf) < 4) {
-		cpu_buf->sample_lost_overflow++;
-		return 0;
-	}
-
 	add_code(cpu_buf, CPU_TRACE_BEGIN);
 	cpu_buf->tracing = 1;
 	return 1;
@@ -291,12 +269,6 @@ void oprofile_add_ibs_sample(struct pt_regs * const regs,
 
 	cpu_buf->sample_received++;
 
-	if (nr_available_slots(cpu_buf) < MAX_IBS_SAMPLE_SIZE) {
-		/* we can't backtrace since we lost the source of this event */
-		cpu_buf->sample_lost_overflow++;
-		return;
-	}
-
 	/* notice a switch from user->kernel or vice versa */
 	if (cpu_buf->last_is_kernel != is_kernel) {
 		cpu_buf->last_is_kernel = is_kernel;
@@ -342,12 +314,6 @@ void oprofile_add_trace(unsigned long pc)
 	if (!cpu_buf->tracing)
 		return;
 
-	if (nr_available_slots(cpu_buf) < 1) {
-		cpu_buf->tracing = 0;
-		cpu_buf->sample_lost_overflow++;
-		return;
-	}
-
 	/*
 	 * broken frame can give an eip with the same value as an
 	 * escape code, abort the trace if we get it

commit 6dad828b76c7224a22ddc9ce7aa495d994f03b31
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Dec 9 01:21:32 2008 +0100

    oprofile: port to the new ring_buffer
    
    This patch replaces the current oprofile cpu buffer implementation
    with the ring buffer provided by the tracing framework. The motivation
    here is to leave the pain of implementing ring buffers to others. Oh,
    no, there are more advantages. Main reason is the support of different
    sample sizes that could be stored in the buffer. Use cases for this
    are IBS and Cell spu profiling. Using the new ring buffer ensures
    valid and complete samples and allows copying the cpu buffer stateless
    without knowing its content. Second it will use generic kernel API and
    also reduce code size. And hopefully, there are less bugs.
    
    Since the new tracing ring buffer implementation uses spin locks to
    protect the buffer during read/write access, it is difficult to use
    the buffer in an NMI handler. In this case, writing to the buffer by
    the NMI handler (x86) could occur also during critical sections when
    reading the buffer. To avoid this, there are 2 buffers for independent
    read and write access. Read access is in process context only, write
    access only in the NMI handler. If the read buffer runs empty, both
    buffers are swapped atomically. There is potentially a small window
    during swapping where the buffers are disabled and samples could be
    lost.
    
    Using 2 buffers is a little bit overhead, but the solution is clear
    and does not require changes in the ring buffer implementation. It can
    be changed to a single buffer solution when the ring buffer access is
    implemented as non-locking atomic code.
    
    The new buffer requires more size to store the same amount of samples
    because each sample includes an u32 header. Also, there is more code
    to execute for buffer access. Nonetheless, the buffer implementation
    is proven in the ftrace environment and worth to use also in oprofile.
    
    Patches that changes the internal IBS buffer usage will follow.
    
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 5cf7efe38e67..eb280ec96e24 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -28,6 +28,25 @@
 #include "buffer_sync.h"
 #include "oprof.h"
 
+#define OP_BUFFER_FLAGS	0
+
+/*
+ * Read and write access is using spin locking. Thus, writing to the
+ * buffer by NMI handler (x86) could occur also during critical
+ * sections when reading the buffer. To avoid this, there are 2
+ * buffers for independent read and write access. Read access is in
+ * process context only, write access only in the NMI handler. If the
+ * read buffer runs empty, both buffers are swapped atomically. There
+ * is potentially a small window during swapping where the buffers are
+ * disabled and samples could be lost.
+ *
+ * Using 2 buffers is a little bit overhead, but the solution is clear
+ * and does not require changes in the ring buffer implementation. It
+ * can be changed to a single buffer solution when the ring buffer
+ * access is implemented as non-locking atomic code.
+ */
+struct ring_buffer *op_ring_buffer_read;
+struct ring_buffer *op_ring_buffer_write;
 DEFINE_PER_CPU(struct oprofile_cpu_buffer, cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
@@ -37,12 +56,12 @@ static int work_enabled;
 
 void free_cpu_buffers(void)
 {
-	int i;
-
-	for_each_possible_cpu(i) {
-		vfree(per_cpu(cpu_buffer, i).buffer);
-		per_cpu(cpu_buffer, i).buffer = NULL;
-	}
+	if (op_ring_buffer_read)
+		ring_buffer_free(op_ring_buffer_read);
+	op_ring_buffer_read = NULL;
+	if (op_ring_buffer_write)
+		ring_buffer_free(op_ring_buffer_write);
+	op_ring_buffer_write = NULL;
 }
 
 unsigned long oprofile_get_cpu_buffer_size(void)
@@ -64,14 +83,16 @@ int alloc_cpu_buffers(void)
 
 	unsigned long buffer_size = fs_cpu_buffer_size;
 
+	op_ring_buffer_read = ring_buffer_alloc(buffer_size, OP_BUFFER_FLAGS);
+	if (!op_ring_buffer_read)
+		goto fail;
+	op_ring_buffer_write = ring_buffer_alloc(buffer_size, OP_BUFFER_FLAGS);
+	if (!op_ring_buffer_write)
+		goto fail;
+
 	for_each_possible_cpu(i) {
 		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
 
-		b->buffer = vmalloc_node(sizeof(struct op_sample) * buffer_size,
-			cpu_to_node(i));
-		if (!b->buffer)
-			goto fail;
-
 		b->last_task = NULL;
 		b->last_is_kernel = -1;
 		b->tracing = 0;
@@ -140,10 +161,22 @@ static inline void
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)
 {
-	struct op_sample *entry = cpu_buffer_write_entry(cpu_buf);
-	entry->eip = pc;
-	entry->event = event;
-	cpu_buffer_write_commit(cpu_buf);
+	struct op_entry entry;
+
+	if (cpu_buffer_write_entry(&entry))
+		goto Error;
+
+	entry.sample->eip = pc;
+	entry.sample->event = event;
+
+	if (cpu_buffer_write_commit(&entry))
+		goto Error;
+
+	return;
+
+Error:
+	cpu_buf->sample_lost_overflow++;
+	return;
 }
 
 static inline void

commit fbc9bf9f0ed4f0fbc47dcb5b1c26c28c93b60e33
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Dec 4 16:27:00 2008 +0100

    oprofile: moving cpu_buffer_reset() to cpu_buffer.h
    
    This is in preparation for changes in the cpu buffer implementation.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index d6f5de686363..5cf7efe38e67 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -124,18 +124,6 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
-/* Resets the cpu buffer to a sane state. */
-void cpu_buffer_reset(struct oprofile_cpu_buffer *cpu_buf)
-{
-	/*
-	 * reset these to invalid values; the next sample collected
-	 * will populate the buffer with proper values to initialize
-	 * the buffer
-	 */
-	cpu_buf->last_is_kernel = -1;
-	cpu_buf->last_task = NULL;
-}
-
 /* compute number of available slots in cpu_buffer queue */
 static unsigned long nr_available_slots(struct oprofile_cpu_buffer const *b)
 {

commit 229234ae4a5ed9376b2e0524da04b0e5edadbf76
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Nov 27 18:36:08 2008 +0100

    oprofile: adding cpu_buffer_write_commit()
    
    This is in preparation for changes in the cpu buffer implementation.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 7e5e650e409f..d6f5de686363 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -148,22 +148,6 @@ static unsigned long nr_available_slots(struct oprofile_cpu_buffer const *b)
 	return tail + (b->buffer_size - head) - 1;
 }
 
-static void increment_head(struct oprofile_cpu_buffer *b)
-{
-	unsigned long new_head = b->head_pos + 1;
-
-	/*
-	 * Ensure anything written to the slot before we increment is
-	 * visible
-	 */
-	wmb();
-
-	if (new_head < b->buffer_size)
-		b->head_pos = new_head;
-	else
-		b->head_pos = 0;
-}
-
 static inline void
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)
@@ -171,7 +155,7 @@ add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	struct op_sample *entry = cpu_buffer_write_entry(cpu_buf);
 	entry->eip = pc;
 	entry->event = event;
-	increment_head(cpu_buf);
+	cpu_buffer_write_commit(cpu_buf);
 }
 
 static inline void

commit 7d468abee0f1a7e918b5e2f23120436a54ba9f33
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Nov 27 10:57:09 2008 +0100

    oprofile: adding cpu buffer r/w access functions
    
    This is in preparation for changes in the cpu buffer implementation.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 2c4d54187b90..7e5e650e409f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -168,7 +168,7 @@ static inline void
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
 	   unsigned long pc, unsigned long event)
 {
-	struct op_sample *entry = &cpu_buf->buffer[cpu_buf->head_pos];
+	struct op_sample *entry = cpu_buffer_write_entry(cpu_buf);
 	entry->eip = pc;
 	entry->event = event;
 	increment_head(cpu_buf);

commit cdc1834d1aa2e5b574a25e66f82625b44cdd0d8f
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Sep 26 22:18:44 2008 -0400

    oprofile: whitspace changes only
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 3958107723fb..2c4d54187b90 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -277,8 +277,8 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 
 #define MAX_IBS_SAMPLE_SIZE 14
 
-void oprofile_add_ibs_sample(struct pt_regs *const regs,
-			     unsigned int *const ibs_sample, int ibs_code)
+void oprofile_add_ibs_sample(struct pt_regs * const regs,
+			     unsigned int * const ibs_sample, int ibs_code)
 {
 	int is_kernel = !user_mode(regs);
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);

commit fd13f6c85144bb2026c534a35be1d7cb7628a64a
Author: Robert Richter <robert.richter@amd.com>
Date:   Sun Oct 19 21:00:09 2008 +0200

    oprofile: comment cleanup
    
    This fixes the coding style of some comments.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 01d38e78cde1..3958107723fb 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -127,9 +127,10 @@ void end_cpu_work(void)
 /* Resets the cpu buffer to a sane state. */
 void cpu_buffer_reset(struct oprofile_cpu_buffer *cpu_buf)
 {
-	/* reset these to invalid values; the next sample
-	 * collected will populate the buffer with proper
-	 * values to initialize the buffer
+	/*
+	 * reset these to invalid values; the next sample collected
+	 * will populate the buffer with proper values to initialize
+	 * the buffer
 	 */
 	cpu_buf->last_is_kernel = -1;
 	cpu_buf->last_task = NULL;
@@ -151,8 +152,10 @@ static void increment_head(struct oprofile_cpu_buffer *b)
 {
 	unsigned long new_head = b->head_pos + 1;
 
-	/* Ensure anything written to the slot before we
-	 * increment is visible */
+	/*
+	 * Ensure anything written to the slot before we increment is
+	 * visible
+	 */
 	wmb();
 
 	if (new_head < b->buffer_size)
@@ -253,8 +256,10 @@ void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	if (!oprofile_begin_trace(cpu_buf))
 		return;
 
-	/* if log_sample() fail we can't backtrace since we lost the source
-	 * of this event */
+	/*
+	 * if log_sample() fail we can't backtrace since we lost the
+	 * source of this event
+	 */
 	if (log_sample(cpu_buf, pc, is_kernel, event))
 		oprofile_ops.backtrace(regs, backtrace_depth);
 	oprofile_end_trace(cpu_buf);
@@ -338,8 +343,10 @@ void oprofile_add_trace(unsigned long pc)
 		return;
 	}
 
-	/* broken frame can give an eip with the same value as an escape code,
-	 * abort the trace if we get it */
+	/*
+	 * broken frame can give an eip with the same value as an
+	 * escape code, abort the trace if we get it
+	 */
 	if (pc == ESCAPE_CODE) {
 		cpu_buf->tracing = 0;
 		cpu_buf->backtrace_aborted++;

commit 92fb83afd6664a6f8a05f990d264c998f9b99f69
Merge: a5344876065e df13b31c286b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 23 10:05:40 2008 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rric/oprofile
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rric/oprofile: (21 commits)
      OProfile: Fix buffer synchronization for IBS
      oprofile: hotplug cpu fix
      oprofile: fixing whitespaces in arch/x86/oprofile/*
      oprofile: fixing whitespaces in arch/x86/oprofile/*
      oprofile: fixing whitespaces in drivers/oprofile/*
      x86/oprofile: add the logic for enabling additional IBS bits
      x86/oprofile: reordering functions in nmi_int.c
      x86/oprofile: removing unused function parameter in add_ibs_begin()
      oprofile: more whitespace fixes
      oprofile: whitespace fixes
      OProfile: Rename IBS sysfs dir into "ibs_op"
      OProfile: Rework string handling in setup_ibs_files()
      OProfile: Rework oprofile_add_ibs_sample() function
      oprofile: discover counters for op ppro too
      oprofile: Implement Intel architectural perfmon support
      oprofile: Don't report Nehalem as core_2
      oprofile: drop const in num counters field
      Revert "Oprofile Multiplexing Patch"
      x86, oprofile: BUG: using smp_processor_id() in preemptible code
      x86/oprofile: fix on_each_cpu build error
      ...
    
    Manually fixed trivial conflicts in
            drivers/oprofile/{cpu_buffer.c,event_buffer.h}

commit a5598ca0d49821912a5053c05f07fd650671eb6d
Author: Carl Love <cel@us.ibm.com>
Date:   Tue Oct 14 23:37:01 2008 +0000

    powerpc/oprofile: Fix mutex locking for cell spu-oprofile
    
    The issue is the SPU code is not holding the kernel mutex lock while
    adding samples to the kernel buffer.
    
    This patch creates per SPU buffers to hold the data.  Data
    is added to the buffers from in interrupt context.  The data
    is periodically pushed to the kernel buffer via a new Oprofile
    function oprofile_put_buff(). The oprofile_put_buff() function
    is called via a work queue enabling the funtion to acquire the
    mutex lock.
    
    The existing user controls for adjusting the per CPU buffer
    size is used to control the size of the per SPU buffers.
    Similarly, overflows of the SPU buffers are reported by
    incrementing the per CPU buffer stats.  This eliminates the
    need to have architecture specific controls for the per SPU
    buffers which is not acceptable to the OProfile user tool
    maintainer.
    
    The export of the oprofile add_event_entry() is removed as it
    is no longer needed given this patch.
    
    Note, this patch has not addressed the issue of indexing arrays
    by the spu number.  This still needs to be fixed as the spu
    numbering is not guarenteed to be 0 to max_num_spus-1.
    
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Maynard Johnson <maynardj@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Acked-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e1bd5a937f6c..7ba39fe20a8a 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -38,13 +38,26 @@ static int work_enabled;
 void free_cpu_buffers(void)
 {
 	int i;
- 
+
 	for_each_online_cpu(i) {
 		vfree(per_cpu(cpu_buffer, i).buffer);
 		per_cpu(cpu_buffer, i).buffer = NULL;
 	}
 }
 
+unsigned long oprofile_get_cpu_buffer_size(void)
+{
+	return fs_cpu_buffer_size;
+}
+
+void oprofile_cpu_buffer_inc_smpl_lost(void)
+{
+	struct oprofile_cpu_buffer *cpu_buf
+		= &__get_cpu_var(cpu_buffer);
+
+	cpu_buf->sample_lost_overflow++;
+}
+
 int alloc_cpu_buffers(void)
 {
 	int i;

commit 4bd9b9dc97e344670e9e5762399a07dcd5f15311
Author: Chris J Arges <arges@linux.vnet.ibm.com>
Date:   Wed Oct 15 11:03:39 2008 -0500

    oprofile: hotplug cpu fix
    
    This patch addresses problems when hotplugging cpus while
    profiling. Instead of allocating only online cpus, all possible cpu
    buffers are allocated, which allows cpus to be onlined during
    operation. If a cpu is offlined before profiling is shutdown
    wq_sync_buffer checks for this condition then cancels this work and
    does not sync this buffer.
    
    Signed-off-by: Chris J Arges <arges@linux.vnet.ibm.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 5a178065cfa0..67bcc1c95e60 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -39,7 +39,7 @@ void free_cpu_buffers(void)
 {
 	int i;
 
-	for_each_online_cpu(i) {
+	for_each_possible_cpu(i) {
 		vfree(per_cpu(cpu_buffer, i).buffer);
 		per_cpu(cpu_buffer, i).buffer = NULL;
 	}
@@ -51,7 +51,7 @@ int alloc_cpu_buffers(void)
 
 	unsigned long buffer_size = fs_cpu_buffer_size;
 
-	for_each_online_cpu(i) {
+	for_each_possible_cpu(i) {
 		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
 
 		b->buffer = vmalloc_node(sizeof(struct op_sample) * buffer_size,
@@ -350,6 +350,11 @@ static void wq_sync_buffer(struct work_struct *work)
 	if (b->cpu != smp_processor_id()) {
 		printk(KERN_DEBUG "WQ on CPU%d, prefer CPU%d\n",
 		       smp_processor_id(), b->cpu);
+
+		if (!cpu_online(b->cpu)) {
+			cancel_delayed_work(&b->work);
+			return;
+		}
 	}
 	sync_buffer(b->cpu);
 

commit 6a18037d4165f691063b43816be3152e9006eb06
Author: Robert Richter <robert.richter@amd.com>
Date:   Thu Oct 16 15:01:40 2008 +0200

    oprofile: fixing whitespaces in drivers/oprofile/*
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index b47ce038490f..5a178065cfa0 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -22,7 +22,7 @@
 #include <linux/oprofile.h>
 #include <linux/vmalloc.h>
 #include <linux/errno.h>
- 
+
 #include "event_buffer.h"
 #include "cpu_buffer.h"
 #include "buffer_sync.h"
@@ -38,7 +38,7 @@ static int work_enabled;
 void free_cpu_buffers(void)
 {
 	int i;
- 
+
 	for_each_online_cpu(i) {
 		vfree(per_cpu(cpu_buffer, i).buffer);
 		per_cpu(cpu_buffer, i).buffer = NULL;
@@ -48,17 +48,17 @@ void free_cpu_buffers(void)
 int alloc_cpu_buffers(void)
 {
 	int i;
- 
+
 	unsigned long buffer_size = fs_cpu_buffer_size;
- 
+
 	for_each_online_cpu(i) {
 		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
- 
+
 		b->buffer = vmalloc_node(sizeof(struct op_sample) * buffer_size,
 			cpu_to_node(i));
 		if (!b->buffer)
 			goto fail;
- 
+
 		b->last_task = NULL;
 		b->last_is_kernel = -1;
 		b->tracing = 0;
@@ -150,7 +150,7 @@ static void increment_head(struct oprofile_cpu_buffer *b)
 
 static inline void
 add_sample(struct oprofile_cpu_buffer *cpu_buf,
-           unsigned long pc, unsigned long event)
+	   unsigned long pc, unsigned long event)
 {
 	struct op_sample *entry = &cpu_buf->buffer[cpu_buf->head_pos];
 	entry->eip = pc;
@@ -205,7 +205,7 @@ static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 		cpu_buf->last_task = task;
 		add_code(cpu_buf, (unsigned long)task);
 	}
- 
+
 	add_sample(cpu_buf, pc, event);
 	return 1;
 }

commit 25ad2913cae9c9e3ed28075caeb2eefccd636f4f
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Sep 5 17:12:36 2008 +0200

    oprofile: more whitespace fixes
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index d6c68270b3da..b47ce038490f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -112,7 +112,7 @@ void end_cpu_work(void)
 }
 
 /* Resets the cpu buffer to a sane state. */
-void cpu_buffer_reset(struct oprofile_cpu_buffer * cpu_buf)
+void cpu_buffer_reset(struct oprofile_cpu_buffer *cpu_buf)
 {
 	/* reset these to invalid values; the next sample
 	 * collected will populate the buffer with proper
@@ -123,7 +123,7 @@ void cpu_buffer_reset(struct oprofile_cpu_buffer * cpu_buf)
 }
 
 /* compute number of available slots in cpu_buffer queue */
-static unsigned long nr_available_slots(struct oprofile_cpu_buffer const * b)
+static unsigned long nr_available_slots(struct oprofile_cpu_buffer const *b)
 {
 	unsigned long head = b->head_pos;
 	unsigned long tail = b->tail_pos;
@@ -134,7 +134,7 @@ static unsigned long nr_available_slots(struct oprofile_cpu_buffer const * b)
 	return tail + (b->buffer_size - head) - 1;
 }
 
-static void increment_head(struct oprofile_cpu_buffer * b)
+static void increment_head(struct oprofile_cpu_buffer *b)
 {
 	unsigned long new_head = b->head_pos + 1;
 
@@ -149,17 +149,17 @@ static void increment_head(struct oprofile_cpu_buffer * b)
 }
 
 static inline void
-add_sample(struct oprofile_cpu_buffer * cpu_buf,
+add_sample(struct oprofile_cpu_buffer *cpu_buf,
            unsigned long pc, unsigned long event)
 {
-	struct op_sample * entry = &cpu_buf->buffer[cpu_buf->head_pos];
+	struct op_sample *entry = &cpu_buf->buffer[cpu_buf->head_pos];
 	entry->eip = pc;
 	entry->event = event;
 	increment_head(cpu_buf);
 }
 
 static inline void
-add_code(struct oprofile_cpu_buffer * buffer, unsigned long value)
+add_code(struct oprofile_cpu_buffer *buffer, unsigned long value)
 {
 	add_sample(buffer, ESCAPE_CODE, value);
 }
@@ -173,10 +173,10 @@ add_code(struct oprofile_cpu_buffer * buffer, unsigned long value)
  * pc. We tag this in the buffer by generating kernel enter/exit
  * events whenever is_kernel changes
  */
-static int log_sample(struct oprofile_cpu_buffer * cpu_buf, unsigned long pc,
+static int log_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,
 		      int is_kernel, unsigned long event)
 {
-	struct task_struct * task;
+	struct task_struct *task;
 
 	cpu_buf->sample_received++;
 
@@ -222,7 +222,7 @@ static int oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
 	return 1;
 }
 
-static void oprofile_end_trace(struct oprofile_cpu_buffer * cpu_buf)
+static void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
 	cpu_buf->tracing = 0;
 }
@@ -260,7 +260,7 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 #define MAX_IBS_SAMPLE_SIZE 14
 
 void oprofile_add_ibs_sample(struct pt_regs *const regs,
-			     unsigned int * const ibs_sample, int ibs_code)
+			     unsigned int *const ibs_sample, int ibs_code)
 {
 	int is_kernel = !user_mode(regs);
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
@@ -345,7 +345,7 @@ void oprofile_add_trace(unsigned long pc)
  */
 static void wq_sync_buffer(struct work_struct *work)
 {
-	struct oprofile_cpu_buffer * b =
+	struct oprofile_cpu_buffer *b =
 		container_of(work, struct oprofile_cpu_buffer, work.work);
 	if (b->cpu != smp_processor_id()) {
 		printk(KERN_DEBUG "WQ on CPU%d, prefer CPU%d\n",

commit e2fee2761ad1df2d29b9d502a3cefc87a17b32ca
Author: Robert Richter <robert.richter@amd.com>
Date:   Fri Jul 18 17:36:20 2008 +0200

    OProfile: Rework oprofile_add_ibs_sample() function
    
    Code looks much more cleaner now.
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e1bd5a937f6c..d6c68270b3da 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -257,21 +257,23 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 
 #ifdef CONFIG_OPROFILE_IBS
 
-#define MAX_IBS_SAMPLE_SIZE	14
-static int log_ibs_sample(struct oprofile_cpu_buffer *cpu_buf,
-	unsigned long pc, int is_kernel, unsigned  int *ibs, int ibs_code)
+#define MAX_IBS_SAMPLE_SIZE 14
+
+void oprofile_add_ibs_sample(struct pt_regs *const regs,
+			     unsigned int * const ibs_sample, int ibs_code)
 {
+	int is_kernel = !user_mode(regs);
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 	struct task_struct *task;
 
 	cpu_buf->sample_received++;
 
 	if (nr_available_slots(cpu_buf) < MAX_IBS_SAMPLE_SIZE) {
+		/* we can't backtrace since we lost the source of this event */
 		cpu_buf->sample_lost_overflow++;
-		return 0;
+		return;
 	}
 
-	is_kernel = !!is_kernel;
-
 	/* notice a switch from user->kernel or vice versa */
 	if (cpu_buf->last_is_kernel != is_kernel) {
 		cpu_buf->last_is_kernel = is_kernel;
@@ -281,7 +283,6 @@ static int log_ibs_sample(struct oprofile_cpu_buffer *cpu_buf,
 	/* notice a task switch */
 	if (!is_kernel) {
 		task = current;
-
 		if (cpu_buf->last_task != task) {
 			cpu_buf->last_task = task;
 			add_code(cpu_buf, (unsigned long)task);
@@ -289,36 +290,17 @@ static int log_ibs_sample(struct oprofile_cpu_buffer *cpu_buf,
 	}
 
 	add_code(cpu_buf, ibs_code);
-	add_sample(cpu_buf, ibs[0], ibs[1]);
-	add_sample(cpu_buf, ibs[2], ibs[3]);
-	add_sample(cpu_buf, ibs[4], ibs[5]);
+	add_sample(cpu_buf, ibs_sample[0], ibs_sample[1]);
+	add_sample(cpu_buf, ibs_sample[2], ibs_sample[3]);
+	add_sample(cpu_buf, ibs_sample[4], ibs_sample[5]);
 
 	if (ibs_code == IBS_OP_BEGIN) {
-	add_sample(cpu_buf, ibs[6], ibs[7]);
-	add_sample(cpu_buf, ibs[8], ibs[9]);
-	add_sample(cpu_buf, ibs[10], ibs[11]);
-	}
-
-	return 1;
-}
-
-void oprofile_add_ibs_sample(struct pt_regs *const regs,
-				unsigned int * const ibs_sample, u8 code)
-{
-	int is_kernel = !user_mode(regs);
-	unsigned long pc = profile_pc(regs);
-
-	struct oprofile_cpu_buffer *cpu_buf =
-			 &per_cpu(cpu_buffer, smp_processor_id());
-
-	if (!backtrace_depth) {
-		log_ibs_sample(cpu_buf, pc, is_kernel, ibs_sample, code);
-		return;
+		add_sample(cpu_buf, ibs_sample[6], ibs_sample[7]);
+		add_sample(cpu_buf, ibs_sample[8], ibs_sample[9]);
+		add_sample(cpu_buf, ibs_sample[10], ibs_sample[11]);
 	}
 
-	/* if log_sample() fails we can't backtrace since we lost the source
-	* of this event */
-	if (log_ibs_sample(cpu_buf, pc, is_kernel, ibs_sample, code))
+	if (backtrace_depth)
 		oprofile_ops.backtrace(regs, backtrace_depth);
 }
 

commit accba5f3965d6a9d1bf7c1e1a7995d17e9d521b6
Merge: 6852fd9b86d0 4480f15b3306
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 13 11:05:51 2008 +0200

    Merge branch 'linus' into oprofile-v2
    
    Conflicts:
            arch/x86/kernel/apic_32.c
            arch/x86/oprofile/nmi_int.c
            include/linux/pci_ids.h

commit 59293c8ad54726150cf6178164311b004d615ce4
Merge: 45f197ade73b 94aca1dac6f6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 30 12:16:26 2008 +0200

    Merge commit 'v2.6.27-rc8' into oprofile
    
    Conflicts:
            arch/x86/oprofile/nmi_int.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f4156d1cd1dc260cca767a6c0636625f05446799
Author: Carl Love <cel@us.ibm.com>
Date:   Mon Aug 11 17:25:43 2008 +1000

    powerpc/cell/oprofile: Avoid double vfree of profile buffer
    
    If an error occurs on opcontrol start, the event and per cpu buffers
    are released.  If later opcontrol shutdown is called then the free
    function will be called again to free buffers that no longer
    exist.  This results in a kernel oops.  The following changes
    prevent the call to delete buffers that don't exist.
    
    Signed-off-by: Carl Love <carll@us.ibm.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Robert Richter <robert.richter@amd.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 2450b3a393ff..7ba78e6d210e 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -38,8 +38,10 @@ void free_cpu_buffers(void)
 {
 	int i;
  
-	for_each_online_cpu(i)
+	for_each_online_cpu(i) {
 		vfree(per_cpu(cpu_buffer, i).buffer);
+		per_cpu(cpu_buffer, i).buffer = NULL;
+	}
 }
 
 int alloc_cpu_buffers(void)

commit bd17b625c09d1ed14c4d98604186b0bbb314f6b2
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 22 21:09:07 2008 +0200

    oprofile: fix printk in cpu_buffer.c
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index aba905b3afb8..4decab624e76 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -364,7 +364,7 @@ static void wq_sync_buffer(struct work_struct *work)
 	struct oprofile_cpu_buffer * b =
 		container_of(work, struct oprofile_cpu_buffer, work.work);
 	if (b->cpu != smp_processor_id()) {
-		printk("WQ on CPU%d, prefer CPU%d\n",
+		printk(KERN_DEBUG "WQ on CPU%d, prefer CPU%d\n",
 		       smp_processor_id(), b->cpu);
 	}
 	sync_buffer(b->cpu);

commit 852402cc27bfa1200164e9e8dc7f6e5f0a4fbd46
Author: Robert Richter <robert.richter@amd.com>
Date:   Tue Jul 22 21:09:06 2008 +0200

    x86/oprofile: add CONFIG_OPROFILE_IBS option
    
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index c9ac4e156918..aba905b3afb8 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -253,6 +253,8 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 	oprofile_add_ext_sample(pc, regs, event, is_kernel);
 }
 
+#ifdef CONFIG_OPROFILE_IBS
+
 #define MAX_IBS_SAMPLE_SIZE	14
 static int log_ibs_sample(struct oprofile_cpu_buffer *cpu_buf,
 	unsigned long pc, int is_kernel, unsigned  int *ibs, int ibs_code)
@@ -318,6 +320,8 @@ void oprofile_add_ibs_sample(struct pt_regs *const regs,
 		oprofile_ops.backtrace(regs, backtrace_depth);
 }
 
+#endif
+
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);

commit 345c25730d085c45622ac779da4dbd97dc3a10fe
Author: Barry Kasindorf <barry.kasindorf@amd.com>
Date:   Tue Jul 22 21:08:54 2008 +0200

    x86/oprofile: add IBS support for AMD CPUs, IBS buffer handling routines
    
    This patchset supports the new profiling hardware available in the
    latest AMD CPUs in the oProfile driver.
    
    Signed-off-by: Barry Kasindorf <barry.kasindorf@amd.com>
    Signed-off-by: Robert Richter <robert.richter@amd.com>
    Cc: oprofile-list <oprofile-list@lists.sourceforge.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 2450b3a393ff..c9ac4e156918 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -5,6 +5,7 @@
  * @remark Read the file COPYING
  *
  * @author John Levon <levon@movementarian.org>
+ * @author Barry Kasindorf <barry.kasindorf@amd.com>
  *
  * Each CPU has a local buffer that stores PC value/event
  * pairs. We also log context switches when we notice them.
@@ -207,7 +208,7 @@ static int log_sample(struct oprofile_cpu_buffer * cpu_buf, unsigned long pc,
 	return 1;
 }
 
-static int oprofile_begin_trace(struct oprofile_cpu_buffer * cpu_buf)
+static int oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)
 {
 	if (nr_available_slots(cpu_buf) < 4) {
 		cpu_buf->sample_lost_overflow++;
@@ -252,6 +253,71 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 	oprofile_add_ext_sample(pc, regs, event, is_kernel);
 }
 
+#define MAX_IBS_SAMPLE_SIZE	14
+static int log_ibs_sample(struct oprofile_cpu_buffer *cpu_buf,
+	unsigned long pc, int is_kernel, unsigned  int *ibs, int ibs_code)
+{
+	struct task_struct *task;
+
+	cpu_buf->sample_received++;
+
+	if (nr_available_slots(cpu_buf) < MAX_IBS_SAMPLE_SIZE) {
+		cpu_buf->sample_lost_overflow++;
+		return 0;
+	}
+
+	is_kernel = !!is_kernel;
+
+	/* notice a switch from user->kernel or vice versa */
+	if (cpu_buf->last_is_kernel != is_kernel) {
+		cpu_buf->last_is_kernel = is_kernel;
+		add_code(cpu_buf, is_kernel);
+	}
+
+	/* notice a task switch */
+	if (!is_kernel) {
+		task = current;
+
+		if (cpu_buf->last_task != task) {
+			cpu_buf->last_task = task;
+			add_code(cpu_buf, (unsigned long)task);
+		}
+	}
+
+	add_code(cpu_buf, ibs_code);
+	add_sample(cpu_buf, ibs[0], ibs[1]);
+	add_sample(cpu_buf, ibs[2], ibs[3]);
+	add_sample(cpu_buf, ibs[4], ibs[5]);
+
+	if (ibs_code == IBS_OP_BEGIN) {
+	add_sample(cpu_buf, ibs[6], ibs[7]);
+	add_sample(cpu_buf, ibs[8], ibs[9]);
+	add_sample(cpu_buf, ibs[10], ibs[11]);
+	}
+
+	return 1;
+}
+
+void oprofile_add_ibs_sample(struct pt_regs *const regs,
+				unsigned int * const ibs_sample, u8 code)
+{
+	int is_kernel = !user_mode(regs);
+	unsigned long pc = profile_pc(regs);
+
+	struct oprofile_cpu_buffer *cpu_buf =
+			 &per_cpu(cpu_buffer, smp_processor_id());
+
+	if (!backtrace_depth) {
+		log_ibs_sample(cpu_buf, pc, is_kernel, ibs_sample, code);
+		return;
+	}
+
+	/* if log_sample() fails we can't backtrace since we lost the source
+	* of this event */
+	if (log_ibs_sample(cpu_buf, pc, is_kernel, ibs_sample, code))
+		oprofile_ops.backtrace(regs, backtrace_depth);
+}
+
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);

commit 8b8b498836942c0c855333d357d121c0adeefbd9
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed May 14 16:05:31 2008 -0700

    oprofile: don't request cache line alignment for cpu_buffer
    
    Alignment was previously requested because cpu_buffer was an [NR_CPUS]
    array, to avoid cache line sharing between CPUS.
    
    After commit 608dfddd845da5ab6accef70154c8910529699f7 (oprofile: change
    cpu_buffer from array to per_cpu variable ), we dont need to force an
    alignement anymore since cpu_buffer sits in per_cpu zone.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Mike Travis <travis@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index efcbf4b4579f..2450b3a393ff 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -27,7 +27,7 @@
 #include "buffer_sync.h"
 #include "oprof.h"
 
-DEFINE_PER_CPU_SHARED_ALIGNED(struct oprofile_cpu_buffer, cpu_buffer);
+DEFINE_PER_CPU(struct oprofile_cpu_buffer, cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
 

commit 608dfddd845da5ab6accef70154c8910529699f7
Author: Mike Travis <travis@sgi.com>
Date:   Mon Apr 28 02:14:15 2008 -0700

    oprofile: change cpu_buffer from array to per_cpu variable
    
    Change cpu_buffer from array to per_cpu variable in oprofile functions.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index c93d3d2640ab..efcbf4b4579f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -27,7 +27,7 @@
 #include "buffer_sync.h"
 #include "oprof.h"
 
-struct oprofile_cpu_buffer cpu_buffer[NR_CPUS] __cacheline_aligned;
+DEFINE_PER_CPU_SHARED_ALIGNED(struct oprofile_cpu_buffer, cpu_buffer);
 
 static void wq_sync_buffer(struct work_struct *work);
 
@@ -39,7 +39,7 @@ void free_cpu_buffers(void)
 	int i;
  
 	for_each_online_cpu(i)
-		vfree(cpu_buffer[i].buffer);
+		vfree(per_cpu(cpu_buffer, i).buffer);
 }
 
 int alloc_cpu_buffers(void)
@@ -49,7 +49,7 @@ int alloc_cpu_buffers(void)
 	unsigned long buffer_size = fs_cpu_buffer_size;
  
 	for_each_online_cpu(i) {
-		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
  
 		b->buffer = vmalloc_node(sizeof(struct op_sample) * buffer_size,
 			cpu_to_node(i));
@@ -83,7 +83,7 @@ void start_cpu_work(void)
 	work_enabled = 1;
 
 	for_each_online_cpu(i) {
-		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
 
 		/*
 		 * Spread the work by 1 jiffy per cpu so they dont all
@@ -100,7 +100,7 @@ void end_cpu_work(void)
 	work_enabled = 0;
 
 	for_each_online_cpu(i) {
-		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+		struct oprofile_cpu_buffer *b = &per_cpu(cpu_buffer, i);
 
 		cancel_delayed_work(&b->work);
 	}
@@ -227,7 +227,7 @@ static void oprofile_end_trace(struct oprofile_cpu_buffer * cpu_buf)
 void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 				unsigned long event, int is_kernel)
 {
-	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 
 	if (!backtrace_depth) {
 		log_sample(cpu_buf, pc, is_kernel, event);
@@ -254,13 +254,13 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
-	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 	log_sample(cpu_buf, pc, is_kernel, event);
 }
 
 void oprofile_add_trace(unsigned long pc)
 {
-	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+	struct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(cpu_buffer);
 
 	if (!cpu_buf->tracing)
 		return;

commit df9d177aa28d50e64bae6fbd6b263833079e3571
Author: Philippe Elie <phil.el@wanadoo.fr>
Date:   Wed Nov 14 16:58:48 2007 -0800

    oProfile: oops when profile_pc() returns ~0LU
    
    Instruction pointer returned by profile_pc() can be a random value.  This
    break the assumption than we can safely set struct op_sample.eip field to a
    magic value to signal to the per-cpu buffer reader side special event like
    task switch ending up in a segfault in get_task_mm() when profile_pc()
    return ~0UL.  Fixed by sanitizing the sampled eip and reject/log invalid
    eip.
    
    Problem reported by Sami Farin, patch tested by him.
    
    Signed-off-by: Philippe Elie <phil.el@wanadoo.fr>
    Tested-by: Sami Farin <safari-kernel@safari.iki.fi>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index a83c3db7d18f..c93d3d2640ab 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -64,6 +64,8 @@ int alloc_cpu_buffers(void)
 		b->head_pos = 0;
 		b->sample_received = 0;
 		b->sample_lost_overflow = 0;
+		b->backtrace_aborted = 0;
+		b->sample_invalid_eip = 0;
 		b->cpu = i;
 		INIT_DELAYED_WORK(&b->work, wq_sync_buffer);
 	}
@@ -175,6 +177,11 @@ static int log_sample(struct oprofile_cpu_buffer * cpu_buf, unsigned long pc,
 
 	cpu_buf->sample_received++;
 
+	if (pc == ESCAPE_CODE) {
+		cpu_buf->sample_invalid_eip++;
+		return 0;
+	}
+
 	if (nr_available_slots(cpu_buf) < 3) {
 		cpu_buf->sample_lost_overflow++;
 		return 0;

commit c4028958b6ecad064b1a6303a6a5906d4fe48d73
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:57:56 2006 +0000

    WorkStruct: make allyesconfig
    
    Fix up for make allyesconfig.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index fc4bc9b94c74..a83c3db7d18f 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -29,7 +29,7 @@
 
 struct oprofile_cpu_buffer cpu_buffer[NR_CPUS] __cacheline_aligned;
 
-static void wq_sync_buffer(void *);
+static void wq_sync_buffer(struct work_struct *work);
 
 #define DEFAULT_TIMER_EXPIRE (HZ / 10)
 static int work_enabled;
@@ -65,7 +65,7 @@ int alloc_cpu_buffers(void)
 		b->sample_received = 0;
 		b->sample_lost_overflow = 0;
 		b->cpu = i;
-		INIT_WORK(&b->work, wq_sync_buffer, b);
+		INIT_DELAYED_WORK(&b->work, wq_sync_buffer);
 	}
 	return 0;
 
@@ -282,9 +282,10 @@ void oprofile_add_trace(unsigned long pc)
  * By using schedule_delayed_work_on and then schedule_delayed_work
  * we guarantee this will stay on the correct cpu
  */
-static void wq_sync_buffer(void * data)
+static void wq_sync_buffer(struct work_struct *work)
 {
-	struct oprofile_cpu_buffer * b = data;
+	struct oprofile_cpu_buffer * b =
+		container_of(work, struct oprofile_cpu_buffer, work.work);
 	if (b->cpu != smp_processor_id()) {
 		printk("WQ on CPU%d, prefer CPU%d\n",
 		       smp_processor_id(), b->cpu);

commit 273577165cd206d2d6689ee4b18aa13de1ec4bde
Author: Brian Rogan <bcr6@cornell.edu>
Date:   Tue Mar 28 01:56:20 2006 -0800

    [PATCH] Add oprofile_add_ext_sample
    
    On ppc64 we look at a profiling register to work out the sample address and
    if it was in userspace or kernel.
    
    The backtrace interface oprofile_add_sample does not allow this.  Create
    oprofile_add_ext_sample and make oprofile_add_sample use it too.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Cc: John Levon <levon@movementarian.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 330d3869b41e..fc4bc9b94c74 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -217,11 +217,10 @@ static void oprofile_end_trace(struct oprofile_cpu_buffer * cpu_buf)
 	cpu_buf->tracing = 0;
 }
 
-void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
+void oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
+				unsigned long event, int is_kernel)
 {
 	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
-	unsigned long pc = profile_pc(regs);
-	int is_kernel = !user_mode(regs);
 
 	if (!backtrace_depth) {
 		log_sample(cpu_buf, pc, is_kernel, event);
@@ -238,6 +237,14 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 	oprofile_end_trace(cpu_buf);
 }
 
+void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
+{
+	int is_kernel = !user_mode(regs);
+	unsigned long pc = profile_pc(regs);
+
+	oprofile_add_ext_sample(pc, regs, event, is_kernel);
+}
+
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];

commit 394e3902c55e667945f6f1c2bdbc59842cce70f7
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Mar 23 03:01:05 2006 -0800

    [PATCH] more for_each_cpu() conversions
    
    When we stop allocating percpu memory for not-possible CPUs we must not touch
    the percpu data for not-possible CPUs at all.  The correct way of doing this
    is to test cpu_possible() or to use for_each_cpu().
    
    This patch is a kernel-wide sweep of all instances of NR_CPUS.  I found very
    few instances of this bug, if any.  But the patch converts lots of open-coded
    test to use the preferred helper macros.
    
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Acked-by: Kyle McMartin <kyle@parisc-linux.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Christian Zankel <chris@zankel.net>
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Cc: Nathan Scott <nathans@sgi.com>
    Cc: Jens Axboe <axboe@suse.de>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 78193e4bbdb5..330d3869b41e 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -38,9 +38,8 @@ void free_cpu_buffers(void)
 {
 	int i;
  
-	for_each_online_cpu(i) {
+	for_each_online_cpu(i)
 		vfree(cpu_buffer[i].buffer);
-	}
 }
 
 int alloc_cpu_buffers(void)

commit 25ab7cd84eebdc1869d236029ada3a7b403de8ba
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Sun Jan 8 01:03:21 2006 -0800

    [PATCH] oprofile: Use vmalloc_node() in alloc_cpu_buffers()
    
    Make oprofile alloc_cpu_buffers() function NUMA aware, allocating each CPU
    local buffer in its memory node if possible.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Cc: John Levon <levon@movementarian.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 026f671ea558..78193e4bbdb5 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -52,7 +52,8 @@ int alloc_cpu_buffers(void)
 	for_each_online_cpu(i) {
 		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
  
-		b->buffer = vmalloc(sizeof(struct op_sample) * buffer_size);
+		b->buffer = vmalloc_node(sizeof(struct op_sample) * buffer_size,
+			cpu_to_node(i));
 		if (!b->buffer)
 			goto fail;
  

commit 77933d7276ee8fa0e2947641941a6f7a100a327b
Author: Jesper Juhl <juhl@dif.dk>
Date:   Wed Jul 27 11:46:09 2005 -0700

    [PATCH] clean up inline static vs static inline
    
    `gcc -W' likes to complain if the static keyword is not at the beginning of
    the declaration.  This patch fixes all remaining occurrences of "inline
    static" up with "static inline" in the entire kernel tree (140 occurrences in
    47 files).
    
    While making this change I came across a few lines with trailing whitespace
    that I also fixed up, I have also added or removed a blank line or two here
    and there, but there are no functional changes in the patch.
    
    Signed-off-by: Jesper Juhl <juhl-lkml@dif.dk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index e9b1772a3a28..026f671ea558 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -42,8 +42,7 @@ void free_cpu_buffers(void)
 		vfree(cpu_buffer[i].buffer);
 	}
 }
- 
- 
+
 int alloc_cpu_buffers(void)
 {
 	int i;
@@ -74,7 +73,6 @@ int alloc_cpu_buffers(void)
 	free_cpu_buffers();
 	return -ENOMEM;
 }
- 
 
 void start_cpu_work(void)
 {
@@ -93,7 +91,6 @@ void start_cpu_work(void)
 	}
 }
 
-
 void end_cpu_work(void)
 {
 	int i;
@@ -109,7 +106,6 @@ void end_cpu_work(void)
 	flush_scheduled_work();
 }
 
-
 /* Resets the cpu buffer to a sane state. */
 void cpu_buffer_reset(struct oprofile_cpu_buffer * cpu_buf)
 {
@@ -121,7 +117,6 @@ void cpu_buffer_reset(struct oprofile_cpu_buffer * cpu_buf)
 	cpu_buf->last_task = NULL;
 }
 
-
 /* compute number of available slots in cpu_buffer queue */
 static unsigned long nr_available_slots(struct oprofile_cpu_buffer const * b)
 {
@@ -134,7 +129,6 @@ static unsigned long nr_available_slots(struct oprofile_cpu_buffer const * b)
 	return tail + (b->buffer_size - head) - 1;
 }
 
-
 static void increment_head(struct oprofile_cpu_buffer * b)
 {
 	unsigned long new_head = b->head_pos + 1;
@@ -149,10 +143,7 @@ static void increment_head(struct oprofile_cpu_buffer * b)
 		b->head_pos = 0;
 }
 
-
-
-
-inline static void
+static inline void
 add_sample(struct oprofile_cpu_buffer * cpu_buf,
            unsigned long pc, unsigned long event)
 {
@@ -162,14 +153,12 @@ add_sample(struct oprofile_cpu_buffer * cpu_buf,
 	increment_head(cpu_buf);
 }
 
-
-inline static void
+static inline void
 add_code(struct oprofile_cpu_buffer * buffer, unsigned long value)
 {
 	add_sample(buffer, ESCAPE_CODE, value);
 }
 
-
 /* This must be safe from any context. It's safe writing here
  * because of the head/tail separation of the writer and reader
  * of the CPU buffer.
@@ -223,13 +212,11 @@ static int oprofile_begin_trace(struct oprofile_cpu_buffer * cpu_buf)
 	return 1;
 }
 
-
 static void oprofile_end_trace(struct oprofile_cpu_buffer * cpu_buf)
 {
 	cpu_buf->tracing = 0;
 }
 
-
 void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 {
 	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
@@ -251,14 +238,12 @@ void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
 	oprofile_end_trace(cpu_buf);
 }
 
-
 void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
 {
 	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
 	log_sample(cpu_buf, pc, is_kernel, event);
 }
 
-
 void oprofile_add_trace(unsigned long pc)
 {
 	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
@@ -283,8 +268,6 @@ void oprofile_add_trace(unsigned long pc)
 	add_sample(cpu_buf, pc, 0);
 }
 
-
-
 /*
  * This serves to avoid cpu buffer overflow, and makes sure
  * the task mortuary progresses

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
new file mode 100644
index 000000000000..e9b1772a3a28
--- /dev/null
+++ b/drivers/oprofile/cpu_buffer.c
@@ -0,0 +1,307 @@
+/**
+ * @file cpu_buffer.c
+ *
+ * @remark Copyright 2002 OProfile authors
+ * @remark Read the file COPYING
+ *
+ * @author John Levon <levon@movementarian.org>
+ *
+ * Each CPU has a local buffer that stores PC value/event
+ * pairs. We also log context switches when we notice them.
+ * Eventually each CPU's buffer is processed into the global
+ * event buffer by sync_buffer().
+ *
+ * We use a local buffer for two reasons: an NMI or similar
+ * interrupt cannot synchronise, and high sampling rates
+ * would lead to catastrophic global synchronisation if
+ * a global buffer was used.
+ */
+
+#include <linux/sched.h>
+#include <linux/oprofile.h>
+#include <linux/vmalloc.h>
+#include <linux/errno.h>
+ 
+#include "event_buffer.h"
+#include "cpu_buffer.h"
+#include "buffer_sync.h"
+#include "oprof.h"
+
+struct oprofile_cpu_buffer cpu_buffer[NR_CPUS] __cacheline_aligned;
+
+static void wq_sync_buffer(void *);
+
+#define DEFAULT_TIMER_EXPIRE (HZ / 10)
+static int work_enabled;
+
+void free_cpu_buffers(void)
+{
+	int i;
+ 
+	for_each_online_cpu(i) {
+		vfree(cpu_buffer[i].buffer);
+	}
+}
+ 
+ 
+int alloc_cpu_buffers(void)
+{
+	int i;
+ 
+	unsigned long buffer_size = fs_cpu_buffer_size;
+ 
+	for_each_online_cpu(i) {
+		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+ 
+		b->buffer = vmalloc(sizeof(struct op_sample) * buffer_size);
+		if (!b->buffer)
+			goto fail;
+ 
+		b->last_task = NULL;
+		b->last_is_kernel = -1;
+		b->tracing = 0;
+		b->buffer_size = buffer_size;
+		b->tail_pos = 0;
+		b->head_pos = 0;
+		b->sample_received = 0;
+		b->sample_lost_overflow = 0;
+		b->cpu = i;
+		INIT_WORK(&b->work, wq_sync_buffer, b);
+	}
+	return 0;
+
+fail:
+	free_cpu_buffers();
+	return -ENOMEM;
+}
+ 
+
+void start_cpu_work(void)
+{
+	int i;
+
+	work_enabled = 1;
+
+	for_each_online_cpu(i) {
+		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+
+		/*
+		 * Spread the work by 1 jiffy per cpu so they dont all
+		 * fire at once.
+		 */
+		schedule_delayed_work_on(i, &b->work, DEFAULT_TIMER_EXPIRE + i);
+	}
+}
+
+
+void end_cpu_work(void)
+{
+	int i;
+
+	work_enabled = 0;
+
+	for_each_online_cpu(i) {
+		struct oprofile_cpu_buffer * b = &cpu_buffer[i];
+
+		cancel_delayed_work(&b->work);
+	}
+
+	flush_scheduled_work();
+}
+
+
+/* Resets the cpu buffer to a sane state. */
+void cpu_buffer_reset(struct oprofile_cpu_buffer * cpu_buf)
+{
+	/* reset these to invalid values; the next sample
+	 * collected will populate the buffer with proper
+	 * values to initialize the buffer
+	 */
+	cpu_buf->last_is_kernel = -1;
+	cpu_buf->last_task = NULL;
+}
+
+
+/* compute number of available slots in cpu_buffer queue */
+static unsigned long nr_available_slots(struct oprofile_cpu_buffer const * b)
+{
+	unsigned long head = b->head_pos;
+	unsigned long tail = b->tail_pos;
+
+	if (tail > head)
+		return (tail - head) - 1;
+
+	return tail + (b->buffer_size - head) - 1;
+}
+
+
+static void increment_head(struct oprofile_cpu_buffer * b)
+{
+	unsigned long new_head = b->head_pos + 1;
+
+	/* Ensure anything written to the slot before we
+	 * increment is visible */
+	wmb();
+
+	if (new_head < b->buffer_size)
+		b->head_pos = new_head;
+	else
+		b->head_pos = 0;
+}
+
+
+
+
+inline static void
+add_sample(struct oprofile_cpu_buffer * cpu_buf,
+           unsigned long pc, unsigned long event)
+{
+	struct op_sample * entry = &cpu_buf->buffer[cpu_buf->head_pos];
+	entry->eip = pc;
+	entry->event = event;
+	increment_head(cpu_buf);
+}
+
+
+inline static void
+add_code(struct oprofile_cpu_buffer * buffer, unsigned long value)
+{
+	add_sample(buffer, ESCAPE_CODE, value);
+}
+
+
+/* This must be safe from any context. It's safe writing here
+ * because of the head/tail separation of the writer and reader
+ * of the CPU buffer.
+ *
+ * is_kernel is needed because on some architectures you cannot
+ * tell if you are in kernel or user space simply by looking at
+ * pc. We tag this in the buffer by generating kernel enter/exit
+ * events whenever is_kernel changes
+ */
+static int log_sample(struct oprofile_cpu_buffer * cpu_buf, unsigned long pc,
+		      int is_kernel, unsigned long event)
+{
+	struct task_struct * task;
+
+	cpu_buf->sample_received++;
+
+	if (nr_available_slots(cpu_buf) < 3) {
+		cpu_buf->sample_lost_overflow++;
+		return 0;
+	}
+
+	is_kernel = !!is_kernel;
+
+	task = current;
+
+	/* notice a switch from user->kernel or vice versa */
+	if (cpu_buf->last_is_kernel != is_kernel) {
+		cpu_buf->last_is_kernel = is_kernel;
+		add_code(cpu_buf, is_kernel);
+	}
+
+	/* notice a task switch */
+	if (cpu_buf->last_task != task) {
+		cpu_buf->last_task = task;
+		add_code(cpu_buf, (unsigned long)task);
+	}
+ 
+	add_sample(cpu_buf, pc, event);
+	return 1;
+}
+
+static int oprofile_begin_trace(struct oprofile_cpu_buffer * cpu_buf)
+{
+	if (nr_available_slots(cpu_buf) < 4) {
+		cpu_buf->sample_lost_overflow++;
+		return 0;
+	}
+
+	add_code(cpu_buf, CPU_TRACE_BEGIN);
+	cpu_buf->tracing = 1;
+	return 1;
+}
+
+
+static void oprofile_end_trace(struct oprofile_cpu_buffer * cpu_buf)
+{
+	cpu_buf->tracing = 0;
+}
+
+
+void oprofile_add_sample(struct pt_regs * const regs, unsigned long event)
+{
+	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+	unsigned long pc = profile_pc(regs);
+	int is_kernel = !user_mode(regs);
+
+	if (!backtrace_depth) {
+		log_sample(cpu_buf, pc, is_kernel, event);
+		return;
+	}
+
+	if (!oprofile_begin_trace(cpu_buf))
+		return;
+
+	/* if log_sample() fail we can't backtrace since we lost the source
+	 * of this event */
+	if (log_sample(cpu_buf, pc, is_kernel, event))
+		oprofile_ops.backtrace(regs, backtrace_depth);
+	oprofile_end_trace(cpu_buf);
+}
+
+
+void oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)
+{
+	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+	log_sample(cpu_buf, pc, is_kernel, event);
+}
+
+
+void oprofile_add_trace(unsigned long pc)
+{
+	struct oprofile_cpu_buffer * cpu_buf = &cpu_buffer[smp_processor_id()];
+
+	if (!cpu_buf->tracing)
+		return;
+
+	if (nr_available_slots(cpu_buf) < 1) {
+		cpu_buf->tracing = 0;
+		cpu_buf->sample_lost_overflow++;
+		return;
+	}
+
+	/* broken frame can give an eip with the same value as an escape code,
+	 * abort the trace if we get it */
+	if (pc == ESCAPE_CODE) {
+		cpu_buf->tracing = 0;
+		cpu_buf->backtrace_aborted++;
+		return;
+	}
+
+	add_sample(cpu_buf, pc, 0);
+}
+
+
+
+/*
+ * This serves to avoid cpu buffer overflow, and makes sure
+ * the task mortuary progresses
+ *
+ * By using schedule_delayed_work_on and then schedule_delayed_work
+ * we guarantee this will stay on the correct cpu
+ */
+static void wq_sync_buffer(void * data)
+{
+	struct oprofile_cpu_buffer * b = data;
+	if (b->cpu != smp_processor_id()) {
+		printk("WQ on CPU%d, prefer CPU%d\n",
+		       smp_processor_id(), b->cpu);
+	}
+	sync_buffer(b->cpu);
+
+	/* don't re-add the work if we're shutting down */
+	if (work_enabled)
+		schedule_delayed_work(&b->work, DEFAULT_TIMER_EXPIRE);
+}
