commit c1d51f684c72b5eb2aecbbd47be3a2977a2dc903
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Nov 7 15:25:12 2019 +0100

    cpuidle: Use nanoseconds as the unit of time
    
    Currently, the cpuidle subsystem uses microseconds as the unit of
    time which (among other things) causes the idle loop to incur some
    integer division overhead for no clear benefit.
    
    In order to allow cpuidle to measure time in nanoseconds, add two
    new fields, exit_latency_ns and target_residency_ns, to represent the
    exit latency and target residency of an idle state in nanoseconds,
    respectively, to struct cpuidle_state and initialize them with the
    help of the corresponding values in microseconds provided by drivers.
    Additionally, change cpuidle_governor_latency_req() to return the
    idle state exit latency constraint in nanoseconds.
    
    Also meeasure idle state residency (last_residency_ns in struct
    cpuidle_device and time_ns in struct cpuidle_driver) in nanoseconds
    and update the cpuidle core and governors accordingly.
    
    However, the menu governor still computes typical intervals in
    microseconds to avoid integer overflows.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Doug Smythies <dsmythies@telus.net>
    Tested-by: Doug Smythies <dsmythies@telus.net>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 38b2b72102a8..b0a7ad566081 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -19,22 +19,12 @@
 #include <linux/sched/stat.h>
 #include <linux/math64.h>
 
-/*
- * Please note when changing the tuning values:
- * If (MAX_INTERESTING-1) * RESOLUTION > UINT_MAX, the result of
- * a scaling operation multiplication may overflow on 32 bit platforms.
- * In that case, #define RESOLUTION as ULL to get 64 bit result:
- * #define RESOLUTION 1024ULL
- *
- * The default values do not overflow.
- */
 #define BUCKETS 12
 #define INTERVAL_SHIFT 3
 #define INTERVALS (1UL << INTERVAL_SHIFT)
 #define RESOLUTION 1024
 #define DECAY 8
-#define MAX_INTERESTING 50000
-
+#define MAX_INTERESTING (50000 * NSEC_PER_USEC)
 
 /*
  * Concepts and ideas behind the menu governor
@@ -120,14 +110,14 @@ struct menu_device {
 	int             needs_update;
 	int             tick_wakeup;
 
-	unsigned int	next_timer_us;
+	u64		next_timer_ns;
 	unsigned int	bucket;
 	unsigned int	correction_factor[BUCKETS];
 	unsigned int	intervals[INTERVALS];
 	int		interval_ptr;
 };
 
-static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters)
+static inline int which_bucket(u64 duration_ns, unsigned long nr_iowaiters)
 {
 	int bucket = 0;
 
@@ -140,15 +130,15 @@ static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters
 	if (nr_iowaiters)
 		bucket = BUCKETS/2;
 
-	if (duration < 10)
+	if (duration_ns < 10ULL * NSEC_PER_USEC)
 		return bucket;
-	if (duration < 100)
+	if (duration_ns < 100ULL * NSEC_PER_USEC)
 		return bucket + 1;
-	if (duration < 1000)
+	if (duration_ns < 1000ULL * NSEC_PER_USEC)
 		return bucket + 2;
-	if (duration < 10000)
+	if (duration_ns < 10000ULL * NSEC_PER_USEC)
 		return bucket + 3;
-	if (duration < 100000)
+	if (duration_ns < 100000ULL * NSEC_PER_USEC)
 		return bucket + 4;
 	return bucket + 5;
 }
@@ -276,13 +266,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		       bool *stop_tick)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
-	int latency_req = cpuidle_governor_latency_req(dev->cpu);
-	int i;
-	int idx;
-	unsigned int interactivity_req;
+	s64 latency_req = cpuidle_governor_latency_req(dev->cpu);
 	unsigned int predicted_us;
+	u64 predicted_ns;
+	u64 interactivity_req;
 	unsigned long nr_iowaiters;
 	ktime_t delta_next;
+	int i, idx;
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -290,14 +280,14 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	}
 
 	/* determine the expected residency time, round up */
-	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length(&delta_next));
+	data->next_timer_ns = tick_nohz_get_sleep_length(&delta_next);
 
 	nr_iowaiters = nr_iowait_cpu(dev->cpu);
-	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
+	data->bucket = which_bucket(data->next_timer_ns, nr_iowaiters);
 
 	if (unlikely(drv->state_count <= 1 || latency_req == 0) ||
-	    ((data->next_timer_us < drv->states[1].target_residency ||
-	      latency_req < drv->states[1].exit_latency) &&
+	    ((data->next_timer_ns < drv->states[1].target_residency_ns ||
+	      latency_req < drv->states[1].exit_latency_ns) &&
 	     !dev->states_usage[0].disable)) {
 		/*
 		 * In this case state[0] will be used no matter what, so return
@@ -308,18 +298,15 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		return 0;
 	}
 
-	/*
-	 * Force the result of multiplication to be 64 bits even if both
-	 * operands are 32 bits.
-	 * Make sure to round up for half microseconds.
-	 */
-	predicted_us = DIV_ROUND_CLOSEST_ULL((uint64_t)data->next_timer_us *
-					 data->correction_factor[data->bucket],
-					 RESOLUTION * DECAY);
-	/*
-	 * Use the lowest expected idle interval to pick the idle state.
-	 */
-	predicted_us = min(predicted_us, get_typical_interval(data, predicted_us));
+	/* Round up the result for half microseconds. */
+	predicted_us = div_u64(data->next_timer_ns *
+			       data->correction_factor[data->bucket] +
+			       (RESOLUTION * DECAY * NSEC_PER_USEC) / 2,
+			       RESOLUTION * DECAY * NSEC_PER_USEC);
+	/* Use the lowest expected idle interval to pick the idle state. */
+	predicted_ns = (u64)min(predicted_us,
+				get_typical_interval(data, predicted_us)) *
+				NSEC_PER_USEC;
 
 	if (tick_nohz_tick_stopped()) {
 		/*
@@ -330,14 +317,15 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		 * the known time till the closest timer event for the idle
 		 * state selection.
 		 */
-		if (predicted_us < TICK_USEC)
-			predicted_us = ktime_to_us(delta_next);
+		if (predicted_ns < TICK_NSEC)
+			predicted_ns = delta_next;
 	} else {
 		/*
 		 * Use the performance multiplier and the user-configurable
 		 * latency_req to determine the maximum exit latency.
 		 */
-		interactivity_req = predicted_us / performance_multiplier(nr_iowaiters);
+		interactivity_req = div64_u64(predicted_ns,
+					      performance_multiplier(nr_iowaiters));
 		if (latency_req > interactivity_req)
 			latency_req = interactivity_req;
 	}
@@ -356,19 +344,19 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		if (idx == -1)
 			idx = i; /* first enabled state */
 
-		if (s->target_residency > predicted_us) {
+		if (s->target_residency_ns > predicted_ns) {
 			/*
 			 * Use a physical idle state, not busy polling, unless
 			 * a timer is going to trigger soon enough.
 			 */
 			if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) &&
-			    s->exit_latency <= latency_req &&
-			    s->target_residency <= data->next_timer_us) {
-				predicted_us = s->target_residency;
+			    s->exit_latency_ns <= latency_req &&
+			    s->target_residency_ns <= data->next_timer_ns) {
+				predicted_ns = s->target_residency_ns;
 				idx = i;
 				break;
 			}
-			if (predicted_us < TICK_USEC)
+			if (predicted_ns < TICK_NSEC)
 				break;
 
 			if (!tick_nohz_tick_stopped()) {
@@ -378,7 +366,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 				 * tick in that case and let the governor run
 				 * again in the next iteration of the loop.
 				 */
-				predicted_us = drv->states[idx].target_residency;
+				predicted_ns = drv->states[idx].target_residency_ns;
 				break;
 			}
 
@@ -388,13 +376,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			 * closest timer event, select this one to avoid getting
 			 * stuck in the shallow one for too long.
 			 */
-			if (drv->states[idx].target_residency < TICK_USEC &&
-			    s->target_residency <= ktime_to_us(delta_next))
+			if (drv->states[idx].target_residency_ns < TICK_NSEC &&
+			    s->target_residency_ns <= delta_next)
 				idx = i;
 
 			return idx;
 		}
-		if (s->exit_latency > latency_req)
+		if (s->exit_latency_ns > latency_req)
 			break;
 
 		idx = i;
@@ -408,12 +396,10 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * expected idle duration is shorter than the tick period length.
 	 */
 	if (((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
-	     predicted_us < TICK_USEC) && !tick_nohz_tick_stopped()) {
-		unsigned int delta_next_us = ktime_to_us(delta_next);
-
+	     predicted_ns < TICK_NSEC) && !tick_nohz_tick_stopped()) {
 		*stop_tick = false;
 
-		if (idx > 0 && drv->states[idx].target_residency > delta_next_us) {
+		if (idx > 0 && drv->states[idx].target_residency_ns > delta_next) {
 			/*
 			 * The tick is not going to be stopped and the target
 			 * residency of the state to be returned is not within
@@ -425,7 +411,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 					continue;
 
 				idx = i;
-				if (drv->states[i].target_residency <= delta_next_us)
+				if (drv->states[i].target_residency_ns <= delta_next)
 					break;
 			}
 		}
@@ -461,7 +447,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	int last_idx = dev->last_state_idx;
 	struct cpuidle_state *target = &drv->states[last_idx];
-	unsigned int measured_us;
+	u64 measured_ns;
 	unsigned int new_factor;
 
 	/*
@@ -479,7 +465,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * assume the state was never reached and the exit latency is 0.
 	 */
 
-	if (data->tick_wakeup && data->next_timer_us > TICK_USEC) {
+	if (data->tick_wakeup && data->next_timer_ns > TICK_NSEC) {
 		/*
 		 * The nohz code said that there wouldn't be any events within
 		 * the tick boundary (if the tick was stopped), but the idle
@@ -489,7 +475,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		 * have been idle long (but not forever) to help the idle
 		 * duration predictor do a better job next time.
 		 */
-		measured_us = 9 * MAX_INTERESTING / 10;
+		measured_ns = 9 * MAX_INTERESTING / 10;
 	} else if ((drv->states[last_idx].flags & CPUIDLE_FLAG_POLLING) &&
 		   dev->poll_time_limit) {
 		/*
@@ -499,28 +485,29 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		 * the CPU might have been woken up from idle by the next timer.
 		 * Assume that to be the case.
 		 */
-		measured_us = data->next_timer_us;
+		measured_ns = data->next_timer_ns;
 	} else {
 		/* measured value */
-		measured_us = dev->last_residency;
+		measured_ns = dev->last_residency_ns;
 
 		/* Deduct exit latency */
-		if (measured_us > 2 * target->exit_latency)
-			measured_us -= target->exit_latency;
+		if (measured_ns > 2 * target->exit_latency_ns)
+			measured_ns -= target->exit_latency_ns;
 		else
-			measured_us /= 2;
+			measured_ns /= 2;
 	}
 
 	/* Make sure our coefficients do not exceed unity */
-	if (measured_us > data->next_timer_us)
-		measured_us = data->next_timer_us;
+	if (measured_ns > data->next_timer_ns)
+		measured_ns = data->next_timer_ns;
 
 	/* Update our correction ratio */
 	new_factor = data->correction_factor[data->bucket];
 	new_factor -= new_factor / DECAY;
 
-	if (data->next_timer_us > 0 && measured_us < MAX_INTERESTING)
-		new_factor += RESOLUTION * measured_us / data->next_timer_us;
+	if (data->next_timer_ns > 0 && measured_ns < MAX_INTERESTING)
+		new_factor += div64_u64(RESOLUTION * measured_ns,
+					data->next_timer_ns);
 	else
 		/*
 		 * we were idle so long that we count it as a perfect
@@ -540,7 +527,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	data->correction_factor[data->bucket] = new_factor;
 
 	/* update the repeating-pattern data */
-	data->intervals[data->interval_ptr++] = measured_us;
+	data->intervals[data->interval_ptr++] = ktime_to_us(measured_ns);
 	if (data->interval_ptr >= INTERVALS)
 		data->interval_ptr = 0;
 }

commit 99e98d3fb1008ef7416e16a1fd355cb73a253502
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Nov 4 12:16:17 2019 +0100

    cpuidle: Consolidate disabled state checks
    
    There are two reasons why CPU idle states may be disabled: either
    because the driver has disabled them or because they have been
    disabled by user space via sysfs.
    
    In the former case, the state's "disabled" flag is set once during
    the initialization of the driver and it is never cleared later (it
    is read-only effectively).  In the latter case, the "disable" field
    of the given state's cpuidle_state_usage struct is set and it may be
    changed via sysfs.  Thus checking whether or not an idle state has
    been disabled involves reading these two flags every time.
    
    In order to avoid the additional check of the state's "disabled" flag
    (which is effectively read-only anyway), use the value of it at the
    init time to set a (new) flag in the "disable" field of that state's
    cpuidle_state_usage structure and use the sysfs interface to
    manipulate another (new) flag in it.  This way the state is disabled
    whenever the "disable" field of its cpuidle_state_usage structure is
    nonzero, whatever the reason, and it is the only place to look into
    to check whether or not the state has been disabled.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index e5a5d0c8d66b..38b2b72102a8 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -298,7 +298,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	if (unlikely(drv->state_count <= 1 || latency_req == 0) ||
 	    ((data->next_timer_us < drv->states[1].target_residency ||
 	      latency_req < drv->states[1].exit_latency) &&
-	     !drv->states[0].disabled && !dev->states_usage[0].disable)) {
+	     !dev->states_usage[0].disable)) {
 		/*
 		 * In this case state[0] will be used no matter what, so return
 		 * it right away and keep the tick running if state[0] is a
@@ -349,9 +349,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	idx = -1;
 	for (i = 0; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
-		struct cpuidle_state_usage *su = &dev->states_usage[i];
 
-		if (s->disabled || su->disable)
+		if (dev->states_usage[i].disable)
 			continue;
 
 		if (idx == -1)
@@ -422,8 +421,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			 * tick, so try to correct that.
 			 */
 			for (i = idx - 1; i >= 0; i--) {
-				if (drv->states[i].disabled ||
-				    dev->states_usage[i].disable)
+				if (dev->states_usage[i].disable)
 					continue;
 
 				idx = i;

commit 32b91ca15353b2803d27cfc747156e72dd2cd5d8
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jul 18 10:53:21 2019 +0200

    cpuidle: menu: Allow tick to be stopped if PM QoS is used
    
    After commit 554c8aa8ecad ("sched: idle: Select idle state before
    stopping the tick") the menu governor prevents the scheduler tick from
    being stopped (unless stopped already) if there is a PM QoS latency
    constraint for the given CPU and the target residency of the deepest
    idle state matching that constraint is below the tick boundary.
    
    However, that is problematic if CPUs with PM QoS latency constraints
    are idle for long times, because it effectively causes the tick to
    run on them all the time which is wasteful.  [It is also confusing
    and questionable if they are full dynticks CPUs.]
    
    To address that issue, make the menu governor allow the tick to be
    stopped only if the idle duration predicted by it is beyond the tick
    boundary, except when the shallowest idle state is selected upfront
    and it is not a "polling" one.
    
    Fixes: 554c8aa8ecad ("sched: idle: Select idle state before stopping the tick")
    Link: https://lore.kernel.org/lkml/79b247b3-e056-610e-9a07-e685dfdaa6c9@gmail.com/
    Reported-by: Thomas Lindroth <thomas.lindroth@gmail.com>
    Tested-by: Thomas Lindroth <thomas.lindroth@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index dace4c7f830c..e5a5d0c8d66b 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -301,9 +301,10 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	     !drv->states[0].disabled && !dev->states_usage[0].disable)) {
 		/*
 		 * In this case state[0] will be used no matter what, so return
-		 * it right away and keep the tick running.
+		 * it right away and keep the tick running if state[0] is a
+		 * polling one.
 		 */
-		*stop_tick = false;
+		*stop_tick = !(drv->states[0].flags & CPUIDLE_FLAG_POLLING);
 		return 0;
 	}
 
@@ -394,16 +395,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 
 			return idx;
 		}
-		if (s->exit_latency > latency_req) {
-			/*
-			 * If we break out of the loop for latency reasons, use
-			 * the target residency of the selected state as the
-			 * expected idle duration so that the tick is retained
-			 * as long as that target residency is low enough.
-			 */
-			predicted_us = drv->states[idx].target_residency;
+		if (s->exit_latency > latency_req)
 			break;
-		}
+
 		idx = i;
 	}
 

commit 7d4daeedd575bbc3c40c87fc6708a8b88c50fe7e
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jul 3 20:51:27 2019 -0300

    governors: unify last_state_idx
    
    Since this field is shared by all governors, move it to
    cpuidle device structure.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index e9a28c7846d6..dace4c7f830c 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -117,7 +117,6 @@
  */
 
 struct menu_device {
-	int		last_state_idx;
 	int             needs_update;
 	int             tick_wakeup;
 
@@ -455,7 +454,7 @@ static void menu_reflect(struct cpuidle_device *dev, int index)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
 
-	data->last_state_idx = index;
+	dev->last_state_idx = index;
 	data->needs_update = 1;
 	data->tick_wakeup = tick_nohz_idle_got_tick();
 }
@@ -468,7 +467,7 @@ static void menu_reflect(struct cpuidle_device *dev, int index)
 static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
-	int last_idx = data->last_state_idx;
+	int last_idx = dev->last_state_idx;
 	struct cpuidle_state *target = &drv->states[last_idx];
 	unsigned int measured_us;
 	unsigned int new_factor;

commit 7925f8f78f0190e96d9bfaf767b52e4f1176334c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:18 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 215
    
    Based on 1 normalized pattern(s):
    
      this code is licenced under the gpl version 2 as described in the
      copying file that acompanies the linux kernel
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 1 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171439.466585205@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5951604e7d5c..e9a28c7846d6 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * menu.c - the menu idle governor
  *
@@ -5,9 +6,6 @@
  * Copyright (C) 2009 Intel Corporation
  * Author:
  *        Arjan van de Ven <arjan@linux.intel.com>
- *
- * This code is licenced under the GPL version 2 as described
- * in the COPYING file that acompanies the Linux Kernel.
  */
 
 #include <linux/kernel.h>

commit 814b8797f9863abc2877acf87f6be0f140d00139
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Feb 27 14:35:50 2019 +0100

    cpuidle: menu: Avoid overflows when computing variance
    
    The variance computation in get_typical_interval() may overflow if
    the square of the value of diff exceeds the maximum for the int64_t
    data type value which basically is the case when it is of the order
    of UINT_MAX.
    
    However, data points so far in the future don't matter for idle
    state selection anyway, so change the initial threshold value in
    get_typical_interval() to INT_MAX which will cause more "outlying"
    data points to be discarded without affecting the selection result.
    
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 61316fc51548..5951604e7d5c 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -186,7 +186,7 @@ static unsigned int get_typical_interval(struct menu_device *data,
 	unsigned int min, max, thresh, avg;
 	uint64_t sum, variance;
 
-	thresh = UINT_MAX; /* Discard outliers above this value */
+	thresh = INT_MAX; /* Discard outliers above this value */
 
 again:
 

commit 6ef746769ef5cfef84cdfdf61ecbab5a6aa4651a
Merge: 85b5d4bcab8b c4ac6889930d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 30 09:08:07 2018 -0700

    Merge tag 'pm-4.20-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more power management updates from Rafael Wysocki:
     "These remove a questionable heuristic from the menu cpuidle governor,
      fix a recent build regression in the intel_pstate driver, clean up ARM
      big-Little support in cpufreq and fix up hung task watchdog's
      interaction with system-wide power management transitions.
    
      Specifics:
    
       - Fix build regression in the intel_pstate driver that doesn't build
         without CONFIG_ACPI after recent changes (Dominik Brodowski).
    
       - One of the heuristics in the menu cpuidle governor is based on a
         function returning 0 most of the time, so drop it and clean up the
         scheduler code related to it (Daniel Lezcano).
    
       - Prevent the arm_big_little cpufreq driver from being used on ARM64
         which is not suitable for it and drop the arm_big_little_dt driver
         that is not used any more (Sudeep Holla).
    
       - Prevent the hung task watchdog from triggering during resume from
         system-wide sleep states by disabling it before freezing tasks and
         enabling it again after they have been thawed (Vitaly Kuznetsov)"
    
    * tag 'pm-4.20-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      kernel: hung_task.c: disable on suspend
      cpufreq: remove unused arm_big_little_dt driver
      cpufreq: drop ARM_BIG_LITTLE_CPUFREQ support for ARM64
      cpufreq: intel_pstate: Fix compilation for !CONFIG_ACPI
      cpuidle: menu: Remove get_loadavg() from the performance multiplier
      sched: Factor out nr_iowait and nr_iowait_cpu

commit 8508cf3ffad4defa202b303e5b6379efc4cd9054
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:11 2018 -0700

    sched: loadavg: consolidate LOAD_INT, LOAD_FRAC, CALC_LOAD
    
    There are several definitions of those functions/macros in places that
    mess with fixed-point load averages.  Provide an official version.
    
    [akpm@linux-foundation.org: fix missed conversion in block/blk-iolatency.c]
    Link: http://lkml.kernel.org/r/20180828172258.3185-5-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 575a68f31761..71979605246e 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -130,10 +130,6 @@ struct menu_device {
 	int		interval_ptr;
 };
 
-
-#define LOAD_INT(x) ((x) >> FSHIFT)
-#define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
-
 static inline int get_loadavg(unsigned long load)
 {
 	return LOAD_INT(load) * 10 + LOAD_FRAC(load) / 10;

commit a7fe5190c03f8137ef08db84a58dd4daf2c4785d
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Thu Oct 4 14:04:03 2018 +0200

    cpuidle: menu: Remove get_loadavg() from the performance multiplier
    
    The function get_loadavg() returns almost always zero. To be more
    precise, statistically speaking for a total of 1023379 times passing
    in the function, the load is equal to zero 1020728 times, greater than
    100, 610 times, the remaining is between 0 and 5.
    
    In 2011, the get_loadavg() was removed from the Android tree because
    of the above [1]. At this time, the load was:
    
    unsigned long this_cpu_load(void)
    {
            struct rq *this = this_rq();
            return this->cpu_load[0];
    }
    
    In 2014, the code was changed by commit 372ba8cb46b2 (cpuidle: menu: Lookup CPU
    runqueues less) and the load is:
    
    void get_iowait_load(unsigned long *nr_waiters, unsigned long *load)
    {
            struct rq *rq = this_rq();
            *nr_waiters = atomic_read(&rq->nr_iowait);
            *load = rq->load.weight;
    }
    
    with the same result.
    
    Both measurements show using the load in this code path does no matter
    anymore. Removing it.
    
    [1] https://android.googlesource.com/kernel/common/+/4dedd9f124703207895777ac6e91dacde0f7cc17
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 575a68f31761..76df4f947f07 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -134,11 +134,6 @@ struct menu_device {
 #define LOAD_INT(x) ((x) >> FSHIFT)
 #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
 
-static inline int get_loadavg(unsigned long load)
-{
-	return LOAD_INT(load) * 10 + LOAD_FRAC(load) / 10;
-}
-
 static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters)
 {
 	int bucket = 0;
@@ -172,18 +167,10 @@ static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters
  * to be, the higher this multiplier, and thus the higher
  * the barrier to go to an expensive C state.
  */
-static inline int performance_multiplier(unsigned long nr_iowaiters, unsigned long load)
+static inline int performance_multiplier(unsigned long nr_iowaiters)
 {
-	int mult = 1;
-
-	/* for higher loadavg, we are more reluctant */
-
-	mult += 2 * get_loadavg(load);
-
-	/* for IO wait tasks (per cpu!) we add 5x each */
-	mult += 10 * nr_iowaiters;
-
-	return mult;
+	/* for IO wait tasks (per cpu!) we add 10x each */
+	return 1 + 10 * nr_iowaiters;
 }
 
 static DEFINE_PER_CPU(struct menu_device, menu_devices);
@@ -301,7 +288,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	int idx;
 	unsigned int interactivity_req;
 	unsigned int predicted_us;
-	unsigned long nr_iowaiters, cpu_load;
+	unsigned long nr_iowaiters;
 	ktime_t delta_next;
 
 	if (data->needs_update) {
@@ -312,7 +299,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	/* determine the expected residency time, round up */
 	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length(&delta_next));
 
-	get_iowait_load(&nr_iowaiters, &cpu_load);
+	nr_iowaiters = nr_iowait_cpu(dev->cpu);
 	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
 
 	if (unlikely(drv->state_count <= 1 || latency_req == 0) ||
@@ -356,7 +343,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		 * Use the performance multiplier and the user-configurable
 		 * latency_req to determine the maximum exit latency.
 		 */
-		interactivity_req = predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
+		interactivity_req = predicted_us / performance_multiplier(nr_iowaiters);
 		if (latency_req > interactivity_req)
 			latency_req = interactivity_req;
 	}

commit f1c8e410cdac5df42a7806e49efde2859a10fecd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Oct 15 13:53:25 2018 +0200

    cpuidle: menu: Avoid computations when result will be discarded
    
    If the minimum interval taken into account in the average computation
    loop in get_typical_interval() is less than the expected idle
    duration determined so far, the resultant average cannot be greater
    than that value as well and the entire return result of the function
    is going to be discarded anyway going forward.
    
    In that case, it is a waste of time to carry out the remaining
    computations in get_typical_interval(), so avoid that by returning
    early if the minimum interval is not below the expected idle duration.
    
    No intentional changes of behavior.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 3d283211ad77..575a68f31761 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -196,10 +196,11 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev);
  * of points is below a threshold. If it is... then use the
  * average of these 8 points as the estimated value.
  */
-static unsigned int get_typical_interval(struct menu_device *data)
+static unsigned int get_typical_interval(struct menu_device *data,
+					 unsigned int predicted_us)
 {
 	int i, divisor;
-	unsigned int max, thresh, avg;
+	unsigned int min, max, thresh, avg;
 	uint64_t sum, variance;
 
 	thresh = UINT_MAX; /* Discard outliers above this value */
@@ -207,6 +208,7 @@ static unsigned int get_typical_interval(struct menu_device *data)
 again:
 
 	/* First calculate the average of past intervals */
+	min = UINT_MAX;
 	max = 0;
 	sum = 0;
 	divisor = 0;
@@ -217,8 +219,19 @@ static unsigned int get_typical_interval(struct menu_device *data)
 			divisor++;
 			if (value > max)
 				max = value;
+
+			if (value < min)
+				min = value;
 		}
 	}
+
+	/*
+	 * If the result of the computation is going to be discarded anyway,
+	 * avoid the computation altogether.
+	 */
+	if (min >= predicted_us)
+		return UINT_MAX;
+
 	if (divisor == INTERVALS)
 		avg = sum >> INTERVAL_SHIFT;
 	else
@@ -325,7 +338,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	/*
 	 * Use the lowest expected idle interval to pick the idle state.
 	 */
-	predicted_us = min(predicted_us, get_typical_interval(data));
+	predicted_us = min(predicted_us, get_typical_interval(data, predicted_us));
 
 	if (tick_nohz_tick_stopped()) {
 		/*

commit 12b65eadf0bd41cf0dbf460f13bcd310a81afe2b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Oct 15 13:51:45 2018 +0200

    cpuidle: menu: Drop redundant comparison
    
    Since the correction factor cannot be greater than RESOLUTION * DECAY,
    the result of the predicted_us computation in menu_select() cannot be
    greater than data->next_timer_us, so it is not necessary to compare
    the "typical interval" value coming from get_typical_interval() with
    data->next_timer_us separately.
    
    It is sufficient to copmare predicted_us with the return value of
    get_typical_interval() directly, so do that and drop the now
    redundant expected_interval variable.
    
    No intentional changes of behavior.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 4ed8004643f8..3d283211ad77 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -287,7 +287,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	int i;
 	int idx;
 	unsigned int interactivity_req;
-	unsigned int expected_interval;
 	unsigned int predicted_us;
 	unsigned long nr_iowaiters, cpu_load;
 	ktime_t delta_next;
@@ -323,14 +322,10 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	predicted_us = DIV_ROUND_CLOSEST_ULL((uint64_t)data->next_timer_us *
 					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
-
-	expected_interval = get_typical_interval(data);
-	expected_interval = min(expected_interval, data->next_timer_us);
-
 	/*
 	 * Use the lowest expected idle interval to pick the idle state.
 	 */
-	predicted_us = min(predicted_us, expected_interval);
+	predicted_us = min(predicted_us, get_typical_interval(data));
 
 	if (tick_nohz_tick_stopped()) {
 		/*

commit bde091ece2ad3f78d0896870f041bc52761ea3c1
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Oct 10 14:16:38 2018 +0200

    cpuidle: menu: Simplify checks related to the polling state
    
    After some recent menu governor changes, the promotion of the
    "polling" state to a physical one is mostly controlled by the
    latency limit (resulting from the "interactivity" factor) and
    not by the time to the closest timer event, so it should be
    sufficient to check the exit latency of that state for this
    purpose (of course, its target residency still needs to be
    within the next timer event range for energy-efficiency).
    
    Also, the physical state the "polling" one is promoted to need not
    be the next one in principle (in case the next state is disabled,
    for example).
    
    For these reasons, simplify the checks made to decide whether or
    not to promote the "polling" state to a physical one and update
    the target idle duration when it is promoted in case the residency
    of the new state turns out to be above the tick boundary (in which
    case there is no reason to stop the tick).
    
    Tested-by: Doug Smythies <dsmythies@telus.net>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b754296eb0c5..4ed8004643f8 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -371,12 +371,12 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		if (s->target_residency > predicted_us) {
 			/*
 			 * Use a physical idle state, not busy polling, unless
-			 * a timer is going to trigger really really soon.
+			 * a timer is going to trigger soon enough.
 			 */
 			if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) &&
-			    i == idx + 1 && latency_req > s->exit_latency &&
-			    data->next_timer_us > max_t(unsigned int, 20,
-							s->target_residency)) {
+			    s->exit_latency <= latency_req &&
+			    s->target_residency <= data->next_timer_us) {
+				predicted_us = s->target_residency;
 				idx = i;
 				break;
 			}

commit 53812cdc9100e19f2e782851964355f2db5583de
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:47:43 2018 +0200

    cpuidle: menu: Move the latency_req == 0 special case check
    
    It is better to always update data->bucket before returning from
    menu_select() to avoid updating the correction factor for a stale
    bucket, so combine the latency_req == 0 special check with the more
    general check below.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 667606c6e284..b754296eb0c5 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -297,19 +297,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		data->needs_update = 0;
 	}
 
-	/* Special case when user has set very strict latency requirement */
-	if (unlikely(latency_req == 0)) {
-		*stop_tick = false;
-		return 0;
-	}
-
 	/* determine the expected residency time, round up */
 	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length(&delta_next));
 
 	get_iowait_load(&nr_iowaiters, &cpu_load);
 	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
 
-	if (unlikely(drv->state_count <= 1) ||
+	if (unlikely(drv->state_count <= 1 || latency_req == 0) ||
 	    ((data->next_timer_us < drv->states[1].target_residency ||
 	      latency_req < drv->states[1].exit_latency) &&
 	     !drv->states[0].disabled && !dev->states_usage[0].disable)) {

commit 8b007ebec9a5d120d25c00d9b771aadf45ac5177
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:46:28 2018 +0200

    cpuidle: menu: Avoid computations for very close timers
    
    If the next timer event (not including the tick) is closer than the
    target residency of the second state or the PM QoS latency constraint
    is below its exit latency, state[0] will be used regardless of any
    other factors, so skip the computations in menu_select() then and
    return 0 straight away from it.
    
    Still, do that after the bucket has been determined to avoid
    updating the correction factor for a stale bucket.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 968379d4f207..667606c6e284 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -309,6 +309,18 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	get_iowait_load(&nr_iowaiters, &cpu_load);
 	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
 
+	if (unlikely(drv->state_count <= 1) ||
+	    ((data->next_timer_us < drv->states[1].target_residency ||
+	      latency_req < drv->states[1].exit_latency) &&
+	     !drv->states[0].disabled && !dev->states_usage[0].disable)) {
+		/*
+		 * In this case state[0] will be used no matter what, so return
+		 * it right away and keep the tick running.
+		 */
+		*stop_tick = false;
+		return 0;
+	}
+
 	/*
 	 * Force the result of multiplication to be 64 bits even if both
 	 * operands are 32 bits.

commit eb40a380bff28f84b6583bba6786b46ef26ef548
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:45:07 2018 +0200

    cpuidle: menu: Do not update last_state_idx in menu_select()
    
    It is not necessary to update data->last_state_idx in menu_select()
    as it only is used in menu_update() which only runs when
    data->needs_update is set and that is set only when updating
    data->last_state_idx in menu_reflect().
    
    Accordingly, drop the update of data->last_state_idx from
    menu_select() and get rid of the (now redundant) "out" label
    from it.
    
    No intentional behavior changes.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5c7fafcb1135..968379d4f207 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -398,7 +398,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			    s->target_residency <= ktime_to_us(delta_next))
 				idx = i;
 
-			goto out;
+			return idx;
 		}
 		if (s->exit_latency > latency_req) {
 			/*
@@ -445,10 +445,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		}
 	}
 
-out:
-	data->last_state_idx = idx;
-
-	return data->last_state_idx;
+	return idx;
 }
 
 /**

commit 96c3d11df15323e7f6e55242f7bda610c4bef402
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:44:06 2018 +0200

    cpuidle: menu: Get rid of first_idx from menu_select()
    
    Rearrange the code in menu_select() so that the loop over idle states
    always starts from 0 and get rid of the first_idx variable.
    
    While at it, add two empty lines to separate conditional statements
    from one another.
    
    No intentional behavior changes.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 98a39c57acec..5c7fafcb1135 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -285,7 +285,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	int latency_req = cpuidle_governor_latency_req(dev->cpu);
 	int i;
-	int first_idx;
 	int idx;
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
@@ -348,36 +347,33 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			latency_req = interactivity_req;
 	}
 
-	first_idx = 0;
-	if (drv->states[0].flags & CPUIDLE_FLAG_POLLING) {
-		struct cpuidle_state *s = &drv->states[1];
-		unsigned int polling_threshold;
-
-		/*
-		 * Default to a physical idle state, not to busy polling, unless
-		 * a timer is going to trigger really really soon.
-		 */
-		polling_threshold = max_t(unsigned int, 20, s->target_residency);
-		if (data->next_timer_us > polling_threshold &&
-		    latency_req > s->exit_latency && !s->disabled &&
-		    !dev->states_usage[1].disable)
-			first_idx = 1;
-	}
-
 	/*
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
 	 */
 	idx = -1;
-	for (i = first_idx; i < drv->state_count; i++) {
+	for (i = 0; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
 		struct cpuidle_state_usage *su = &dev->states_usage[i];
 
 		if (s->disabled || su->disable)
 			continue;
+
 		if (idx == -1)
 			idx = i; /* first enabled state */
+
 		if (s->target_residency > predicted_us) {
+			/*
+			 * Use a physical idle state, not busy polling, unless
+			 * a timer is going to trigger really really soon.
+			 */
+			if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) &&
+			    i == idx + 1 && latency_req > s->exit_latency &&
+			    data->next_timer_us > max_t(unsigned int, 20,
+							s->target_residency)) {
+				idx = i;
+				break;
+			}
 			if (predicted_us < TICK_USEC)
 				break;
 

commit 23e8ceb9ce766c81d62434053aef6e7efea6fcc3
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:42:56 2018 +0200

    cpuidle: menu: Compute first_idx when latency_req is known
    
    Since menu_select() can only set first_idx to 1 if the exit latency
    of the second state is not greater than the latency limit, it should
    first determine that limit.  Thus first_idx should be computed after
    the "interactivity" factor has been taken into account.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewedy-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 8b3f9c7bf221..98a39c57acec 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -322,22 +322,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	expected_interval = get_typical_interval(data);
 	expected_interval = min(expected_interval, data->next_timer_us);
 
-	first_idx = 0;
-	if (drv->states[0].flags & CPUIDLE_FLAG_POLLING) {
-		struct cpuidle_state *s = &drv->states[1];
-		unsigned int polling_threshold;
-
-		/*
-		 * Default to a physical idle state, not to busy polling, unless
-		 * a timer is going to trigger really really soon.
-		 */
-		polling_threshold = max_t(unsigned int, 20, s->target_residency);
-		if (data->next_timer_us > polling_threshold &&
-		    latency_req > s->exit_latency && !s->disabled &&
-		    !dev->states_usage[1].disable)
-			first_idx = 1;
-	}
-
 	/*
 	 * Use the lowest expected idle interval to pick the idle state.
 	 */
@@ -364,6 +348,22 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			latency_req = interactivity_req;
 	}
 
+	first_idx = 0;
+	if (drv->states[0].flags & CPUIDLE_FLAG_POLLING) {
+		struct cpuidle_state *s = &drv->states[1];
+		unsigned int polling_threshold;
+
+		/*
+		 * Default to a physical idle state, not to busy polling, unless
+		 * a timer is going to trigger really really soon.
+		 */
+		polling_threshold = max_t(unsigned int, 20, s->target_residency);
+		if (data->next_timer_us > polling_threshold &&
+		    latency_req > s->exit_latency && !s->disabled &&
+		    !dev->states_usage[1].disable)
+			first_idx = 1;
+	}
+
 	/*
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.

commit 5f26bdceb9c0a5e6c696aa2899d077cd3ae93413
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Oct 2 23:42:02 2018 +0200

    cpuidle: menu: Fix wakeup statistics updates for polling state
    
    If the CPU exits the "polling" state due to the time limit in the
    loop in poll_idle(), this is not a real wakeup and it just means
    that the "polling" state selection was not adequate.  The governor
    mispredicted short idle duration, but had a more suitable state been
    selected, the CPU might have spent more time in it.  In fact, there
    is no reason to expect that there would have been a wakeup event
    earlier than the next timer in that case.
    
    Handling such cases as regular wakeups in menu_update() may cause the
    menu governor to make suboptimal decisions going forward, but ignoring
    them altogether would not be correct either, because every time
    menu_select() is invoked, it makes a separate new attempt to predict
    the idle duration taking distinct time to the closest timer event as
    input and the outcomes of all those attempts should be recorded.
    
    For this reason, make menu_update() always assume that if the
    "polling" state was exited due to the time limit, the next proper
    wakeup event for the CPU would be the next timer event (not
    including the tick).
    
    Fixes: a37b969a61c1 "cpuidle: poll_state: Add time limit to poll_idle()"
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b6684fd89085..8b3f9c7bf221 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -511,6 +511,16 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		 * duration predictor do a better job next time.
 		 */
 		measured_us = 9 * MAX_INTERESTING / 10;
+	} else if ((drv->states[last_idx].flags & CPUIDLE_FLAG_POLLING) &&
+		   dev->poll_time_limit) {
+		/*
+		 * The CPU exited the "polling" state due to a time limit, so
+		 * the idle duration prediction leading to the selection of that
+		 * state was inaccurate.  If a better prediction had been made,
+		 * the CPU might have been woken up from idle by the next timer.
+		 * Assume that to be the case.
+		 */
+		measured_us = data->next_timer_us;
 	} else {
 		/* measured value */
 		measured_us = dev->last_residency;

commit 03dba278043358bbbf4f029be169e1e73d2fbe2b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Oct 1 11:56:21 2018 +0200

    cpuidle: menu: Replace data->predicted_us with local variable
    
    The predicted_us field in struct menu_device is only accessed in
    menu_select(), so replace it with a local variable in that function.
    
    With that, stop using expected_interval instead of predicted_us to
    store the new predicted idle duration value if it is set to the
    selected state's target residency which is quite confusing.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 021b08dd139c..b6684fd89085 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -124,7 +124,6 @@ struct menu_device {
 	int             tick_wakeup;
 
 	unsigned int	next_timer_us;
-	unsigned int	predicted_us;
 	unsigned int	bucket;
 	unsigned int	correction_factor[BUCKETS];
 	unsigned int	intervals[INTERVALS];
@@ -290,6 +289,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	int idx;
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
+	unsigned int predicted_us;
 	unsigned long nr_iowaiters, cpu_load;
 	ktime_t delta_next;
 
@@ -315,7 +315,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * operands are 32 bits.
 	 * Make sure to round up for half microseconds.
 	 */
-	data->predicted_us = DIV_ROUND_CLOSEST_ULL((uint64_t)data->next_timer_us *
+	predicted_us = DIV_ROUND_CLOSEST_ULL((uint64_t)data->next_timer_us *
 					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
@@ -341,7 +341,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	/*
 	 * Use the lowest expected idle interval to pick the idle state.
 	 */
-	data->predicted_us = min(data->predicted_us, expected_interval);
+	predicted_us = min(predicted_us, expected_interval);
 
 	if (tick_nohz_tick_stopped()) {
 		/*
@@ -352,19 +352,18 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		 * the known time till the closest timer event for the idle
 		 * state selection.
 		 */
-		if (data->predicted_us < TICK_USEC)
-			data->predicted_us = ktime_to_us(delta_next);
+		if (predicted_us < TICK_USEC)
+			predicted_us = ktime_to_us(delta_next);
 	} else {
 		/*
 		 * Use the performance multiplier and the user-configurable
 		 * latency_req to determine the maximum exit latency.
 		 */
-		interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
+		interactivity_req = predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
 		if (latency_req > interactivity_req)
 			latency_req = interactivity_req;
 	}
 
-	expected_interval = data->predicted_us;
 	/*
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
@@ -378,8 +377,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			continue;
 		if (idx == -1)
 			idx = i; /* first enabled state */
-		if (s->target_residency > data->predicted_us) {
-			if (data->predicted_us < TICK_USEC)
+		if (s->target_residency > predicted_us) {
+			if (predicted_us < TICK_USEC)
 				break;
 
 			if (!tick_nohz_tick_stopped()) {
@@ -389,7 +388,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 				 * tick in that case and let the governor run
 				 * again in the next iteration of the loop.
 				 */
-				expected_interval = drv->states[idx].target_residency;
+				predicted_us = drv->states[idx].target_residency;
 				break;
 			}
 
@@ -412,7 +411,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			 * expected idle duration so that the tick is retained
 			 * as long as that target residency is low enough.
 			 */
-			expected_interval = drv->states[idx].target_residency;
+			predicted_us = drv->states[idx].target_residency;
 			break;
 		}
 		idx = i;
@@ -426,7 +425,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * expected idle duration is shorter than the tick period length.
 	 */
 	if (((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
-	     expected_interval < TICK_USEC) && !tick_nohz_tick_stopped()) {
+	     predicted_us < TICK_USEC) && !tick_nohz_tick_stopped()) {
 		unsigned int delta_next_us = ktime_to_us(delta_next);
 
 		*stop_tick = false;

commit 6a5f95b5a4f4ff29e4071bc5b95f8f3a2aef046b
Author: Fieah Lim <kw@fieahl.im>
Date:   Tue Sep 11 03:59:01 2018 +0800

    cpuidle: Remove unnecessary wrapper cpuidle_get_last_residency()
    
    cpuidle_get_last_residency() is just a wrapper for retrieving
    the last_residency member of struct cpuidle_device.  It's also
    weirdly the only wrapper function for accessing cpuidle_* struct
    member (by my best guess is it could be a leftover from v2.x).
    
    Anyhow, since the only two users (the ladder and menu governors)
    can access dev->last_residency directly, and it's more intuitive to
    do it that way, let's just get rid of the wrapper.
    
    This patch tidies up CPU idle code a bit without functional changes.
    
    Signed-off-by: Fieah Lim <kw@fieahl.im>
    [ rjw: Changelog cleanup ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index e26a40971b26..021b08dd139c 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -514,7 +514,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		measured_us = 9 * MAX_INTERESTING / 10;
 	} else {
 		/* measured value */
-		measured_us = cpuidle_get_last_residency(dev);
+		measured_us = dev->last_residency;
 
 		/* Deduct exit latency */
 		if (measured_us > 2 * target->exit_latency)

commit 757ab15c3f4968b5a29caf3fe8b67660ce84c3cd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 21 10:44:10 2018 +0200

    cpuidle: menu: Retain tick when shallow state is selected
    
    The case addressed by commit 5ef499cd571c (cpuidle: menu: Handle
    stopped tick more aggressively) in the stopped tick case is present
    when the tick has not been stopped yet too.  Namely, if only two CPU
    idle states, shallow state A with target residency significantly
    below the tick boundary and deep state B with target residency
    significantly above it, are available and the predicted idle
    duration is above the tick boundary, but below the target residency
    of state B, state A will be selected and the CPU may spend indefinite
    amount of time in it, which is not quite energy-efficient.
    
    However, if the tick has not been stopped yet and the governor is
    about to select a shallow idle state for the CPU even though the idle
    duration predicted by it is above the tick boundary, it should be
    fine to wake up the CPU early, so the tick can be retained then and
    the governor will have a chance to select a deeper state when it runs
    next time.
    
    [Note that when this really happens, it will make the idle duration
     predictor believe that the CPU might be idle longer than predicted,
     which will make it more likely to predict longer idle durations going
     forward, but that will also cause deeper idle states to be selected
     going forward, on average, which is what's needed here.]
    
    Fixes: 87c9fe6ee495 (cpuidle: menu: Avoid selecting shallow states with stopped tick)
    Reported-by: Leo Yan <leo.yan@linaro.org>
    Cc: 4.17+ <stable@vger.kernel.org> # 4.17+: 5ef499cd571c (cpuidle: menu: Handle ...)
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 110483f0e3fb..e26a40971b26 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -379,9 +379,20 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		if (idx == -1)
 			idx = i; /* first enabled state */
 		if (s->target_residency > data->predicted_us) {
-			if (!tick_nohz_tick_stopped())
+			if (data->predicted_us < TICK_USEC)
 				break;
 
+			if (!tick_nohz_tick_stopped()) {
+				/*
+				 * If the state selected so far is shallow,
+				 * waking up early won't hurt, so retain the
+				 * tick in that case and let the governor run
+				 * again in the next iteration of the loop.
+				 */
+				expected_interval = drv->states[idx].target_residency;
+				break;
+			}
+
 			/*
 			 * If the state selected so far is shallow and this
 			 * state's target residency matches the time till the

commit 5ef499cd571c293b74a30d77e7ef512edb6ded6b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 14 12:34:40 2018 +0200

    cpuidle: menu: Handle stopped tick more aggressively
    
    Commit 87c9fe6ee495 (cpuidle: menu: Avoid selecting shallow states
    with stopped tick) missed the case when the target residencies of
    deep idle states of CPUs are above the tick boundary which may cause
    the CPU to get stuck in a shallow idle state for a long time.
    
    Say there are two CPU idle states available: one shallow, with the
    target residency much below the tick boundary and one deep, with
    the target residency significantly above the tick boundary.  In
    that case, if the tick has been stopped already and the expected
    next timer event is relatively far in the future, the governor will
    assume the idle duration to be equal to TICK_USEC and it will select
    the idle state for the CPU accordingly.  However, that will cause the
    shallow state to be selected even though it would have been more
    energy-efficient to select the deep one.
    
    To address this issue, modify the governor to always use the time
    till the closest timer event instead of the predicted idle duration
    if the latter is less than the tick period length and the tick has
    been stopped already.  Also make it extend the search for a matching
    idle state if the tick is stopped to avoid settling on a shallow
    state if deep states with target residencies above the tick period
    length are available.
    
    In addition, make it always indicate that the tick should be stopped
    if it has been stopped already for consistency.
    
    Fixes: 87c9fe6ee495 (cpuidle: menu: Avoid selecting shallow states with stopped tick)
    Reported-by: Leo Yan <leo.yan@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: 4.17+ <stable@vger.kernel.org> # 4.17+
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 1560a9e76f06..110483f0e3fb 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -348,14 +348,12 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		 * If the tick is already stopped, the cost of possible short
 		 * idle duration misprediction is much higher, because the CPU
 		 * may be stuck in a shallow idle state for a long time as a
-		 * result of it.  In that case say we might mispredict and try
-		 * to force the CPU into a state for which we would have stopped
-		 * the tick, unless a timer is going to expire really soon
-		 * anyway.
+		 * result of it.  In that case say we might mispredict and use
+		 * the known time till the closest timer event for the idle
+		 * state selection.
 		 */
 		if (data->predicted_us < TICK_USEC)
-			data->predicted_us = min_t(unsigned int, TICK_USEC,
-						   ktime_to_us(delta_next));
+			data->predicted_us = ktime_to_us(delta_next);
 	} else {
 		/*
 		 * Use the performance multiplier and the user-configurable
@@ -380,8 +378,22 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			continue;
 		if (idx == -1)
 			idx = i; /* first enabled state */
-		if (s->target_residency > data->predicted_us)
-			break;
+		if (s->target_residency > data->predicted_us) {
+			if (!tick_nohz_tick_stopped())
+				break;
+
+			/*
+			 * If the state selected so far is shallow and this
+			 * state's target residency matches the time till the
+			 * closest timer event, select this one to avoid getting
+			 * stuck in the shallow one for too long.
+			 */
+			if (drv->states[idx].target_residency < TICK_USEC &&
+			    s->target_residency <= ktime_to_us(delta_next))
+				idx = i;
+
+			goto out;
+		}
 		if (s->exit_latency > latency_req) {
 			/*
 			 * If we break out of the loop for latency reasons, use
@@ -402,14 +414,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * Don't stop the tick if the selected state is a polling one or if the
 	 * expected idle duration is shorter than the tick period length.
 	 */
-	if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
-	    expected_interval < TICK_USEC) {
+	if (((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
+	     expected_interval < TICK_USEC) && !tick_nohz_tick_stopped()) {
 		unsigned int delta_next_us = ktime_to_us(delta_next);
 
 		*stop_tick = false;
 
-		if (!tick_nohz_tick_stopped() && idx > 0 &&
-		    drv->states[idx].target_residency > delta_next_us) {
+		if (idx > 0 && drv->states[idx].target_residency > delta_next_us) {
 			/*
 			 * The tick is not going to be stopped and the target
 			 * residency of the state to be returned is not within
@@ -428,6 +439,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		}
 	}
 
+out:
 	data->last_state_idx = idx;
 
 	return data->last_state_idx;

commit 50f7ccc64750610f57983720613713b2c14f0e9c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Aug 16 12:55:01 2018 +0200

    cpuidle: menu: Update stale polling override comment
    
    The comment to explain why the menu governor uses idle state 1
    instead of idle state 0 as the first one sometimes is stale (among
    other things it mentions a user setting not present any more),
    so update it.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5700464baad5..1560a9e76f06 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -328,9 +328,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		unsigned int polling_threshold;
 
 		/*
-		 * We want to default to C1 (hlt), not to busy polling
-		 * unless the timer is happening really really soon, or
-		 * C1's exit latency exceeds the user configured limit.
+		 * Default to a physical idle state, not to busy polling, unless
+		 * a timer is going to trigger really really soon.
 		 */
 		polling_threshold = max_t(unsigned int, 20, s->target_residency);
 		if (data->next_timer_us > polling_threshold &&

commit f390c5eb2858c9fceccba7c4118003b6baf9176a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 14 12:39:02 2018 +0200

    cpuidle: menu: Fix white space
    
    Fix some damaged white space in menu_select().
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 1aef60d160eb..5700464baad5 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -418,8 +418,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 			 * tick, so try to correct that.
 			 */
 			for (i = idx - 1; i >= 0; i--) {
-			    if (drv->states[i].disabled ||
-			        dev->states_usage[i].disable)
+				if (drv->states[i].disabled ||
+				    dev->states_usage[i].disable)
 					continue;
 
 				idx = i;

commit 0fc784fb09f6db8d6650aac137daa779da25c73b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed May 30 13:43:01 2018 +0200

    cpuidle: governors: Consolidate PM QoS handling
    
    There is some code duplication related to the PM QoS handling between
    the existing cpuidle governors, so move that code to a common helper
    function and call that from the governors.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5d15bc0ba2e0..1aef60d160eb 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -12,7 +12,6 @@
 
 #include <linux/kernel.h>
 #include <linux/cpuidle.h>
-#include <linux/pm_qos.h>
 #include <linux/time.h>
 #include <linux/ktime.h>
 #include <linux/hrtimer.h>
@@ -21,7 +20,6 @@
 #include <linux/sched/loadavg.h>
 #include <linux/sched/stat.h>
 #include <linux/math64.h>
-#include <linux/cpu.h>
 
 /*
  * Please note when changing the tuning values:
@@ -286,15 +284,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		       bool *stop_tick)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
-	struct device *device = get_cpu_device(dev->cpu);
-	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
+	int latency_req = cpuidle_governor_latency_req(dev->cpu);
 	int i;
 	int first_idx;
 	int idx;
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
-	int resume_latency = dev_pm_qos_raw_read_value(device);
 	ktime_t delta_next;
 
 	if (data->needs_update) {
@@ -302,9 +298,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		data->needs_update = 0;
 	}
 
-	if (resume_latency < latency_req)
-		latency_req = resume_latency;
-
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0)) {
 		*stop_tick = false;

commit cf7eeea947efaf2348dd87542508b1d426b948cd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed May 30 13:41:09 2018 +0200

    cpuidle: governors: Drop redundant checks related to PM QoS
    
    PM_QOS_RESUME_LATENCY_NO_CONSTRAINT is defined as the 32-bit integer
    maximum, so it is not necessary to test the return value of
    dev_pm_qos_raw_read_value() against it directly in the menu and
    ladder cpuidle governors.
    
    Drop these redundant checks.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 1bfe03ceb236..5d15bc0ba2e0 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -302,8 +302,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		data->needs_update = 0;
 	}
 
-	if (resume_latency < latency_req &&
-	    resume_latency != PM_QOS_RESUME_LATENCY_NO_CONSTRAINT)
+	if (resume_latency < latency_req)
 		latency_req = resume_latency;
 
 	/* Special case when user has set very strict latency requirement */

commit 87c9fe6ee495f78f36d39cb37f6a714444a093ee
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 5 19:12:43 2018 +0200

    cpuidle: menu: Avoid selecting shallow states with stopped tick
    
    If the scheduler tick has been stopped already and the governor
    selects a shallow idle state, the CPU can spend a long time in that
    state if the selection is based on an inaccurate prediction of idle
    time.  That effect turns out to be relevant, so it needs to be
    mitigated.
    
    To that end, modify the menu governor to discard the result of the
    idle time prediction if the tick is stopped and the predicted idle
    time is less than the tick period length, unless the tick timer is
    going to expire soon.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 267982e471e0..1bfe03ceb236 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -352,13 +352,28 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 */
 	data->predicted_us = min(data->predicted_us, expected_interval);
 
-	/*
-	 * Use the performance multiplier and the user-configurable
-	 * latency_req to determine the maximum exit latency.
-	 */
-	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
-	if (latency_req > interactivity_req)
-		latency_req = interactivity_req;
+	if (tick_nohz_tick_stopped()) {
+		/*
+		 * If the tick is already stopped, the cost of possible short
+		 * idle duration misprediction is much higher, because the CPU
+		 * may be stuck in a shallow idle state for a long time as a
+		 * result of it.  In that case say we might mispredict and try
+		 * to force the CPU into a state for which we would have stopped
+		 * the tick, unless a timer is going to expire really soon
+		 * anyway.
+		 */
+		if (data->predicted_us < TICK_USEC)
+			data->predicted_us = min_t(unsigned int, TICK_USEC,
+						   ktime_to_us(delta_next));
+	} else {
+		/*
+		 * Use the performance multiplier and the user-configurable
+		 * latency_req to determine the maximum exit latency.
+		 */
+		interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
+		if (latency_req > interactivity_req)
+			latency_req = interactivity_req;
+	}
 
 	expected_interval = data->predicted_us;
 	/*

commit 296bb1e51a4838a6488ec5ce676607093482ecbc
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 5 19:12:34 2018 +0200

    cpuidle: menu: Refine idle state selection for running tick
    
    If the tick isn't stopped, the target residency of the state selected
    by the menu governor may be greater than the actual time to the next
    tick and that means lost energy.
    
    To avoid that, make tick_nohz_get_sleep_length() return the current
    time to the next event (before stopping the tick) in addition to the
    estimated one via an extra pointer argument and make menu_select()
    use that value to refine the state selection when necessary.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f53a929bd2bd..267982e471e0 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -295,6 +295,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
 	int resume_latency = dev_pm_qos_raw_read_value(device);
+	ktime_t delta_next;
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -312,7 +313,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	}
 
 	/* determine the expected residency time, round up */
-	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length());
+	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length(&delta_next));
 
 	get_iowait_load(&nr_iowaiters, &cpu_load);
 	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
@@ -396,9 +397,31 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * expected idle duration is shorter than the tick period length.
 	 */
 	if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
-	    expected_interval < TICK_USEC)
+	    expected_interval < TICK_USEC) {
+		unsigned int delta_next_us = ktime_to_us(delta_next);
+
 		*stop_tick = false;
 
+		if (!tick_nohz_tick_stopped() && idx > 0 &&
+		    drv->states[idx].target_residency > delta_next_us) {
+			/*
+			 * The tick is not going to be stopped and the target
+			 * residency of the state to be returned is not within
+			 * the time until the next timer event including the
+			 * tick, so try to correct that.
+			 */
+			for (i = idx - 1; i >= 0; i--) {
+			    if (drv->states[i].disabled ||
+			        dev->states_usage[i].disable)
+					continue;
+
+				idx = i;
+				if (drv->states[i].target_residency <= delta_next_us)
+					break;
+			}
+		}
+	}
+
 	data->last_state_idx = idx;
 
 	return data->last_state_idx;

commit 45f1ff59e27ca59d33cc1a317e669d90022ccf7d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 22 17:50:49 2018 +0100

    cpuidle: Return nohz hint from cpuidle_select()
    
    Add a new pointer argument to cpuidle_select() and to the ->select
    cpuidle governor callback to allow a boolean value indicating
    whether or not the tick should be stopped before entering the
    selected state to be returned from there.
    
    Make the ladder governor ignore that pointer (to preserve its
    current behavior) and make the menu governor return 'false" through
    it if:
     (1) the idle exit latency is constrained at 0, or
     (2) the selected state is a polling one, or
     (3) the expected idle period duration is within the tick period
         range.
    
    In addition to that, the correction factor computations in the menu
    governor need to take the possibility that the tick may not be
    stopped into account to avoid artificially small correction factor
    values.  To that end, add a mechanism to record tick wakeups, as
    suggested by Peter Zijlstra, and use it to modify the menu_update()
    behavior when tick wakeup occurs.  Namely, if the CPU is woken up by
    the tick and the return value of tick_nohz_get_sleep_length() is not
    within the tick boundary, the predicted idle duration is likely too
    short, so make menu_update() try to compensate for that by updating
    the governor statistics as though the CPU was idle for a long time.
    
    Since the value returned through the new argument pointer of
    cpuidle_select() is not used by its caller yet, this change by
    itself is not expected to alter the functionality of the code.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index aa390404e85f..f53a929bd2bd 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -123,6 +123,7 @@
 struct menu_device {
 	int		last_state_idx;
 	int             needs_update;
+	int             tick_wakeup;
 
 	unsigned int	next_timer_us;
 	unsigned int	predicted_us;
@@ -279,8 +280,10 @@ static unsigned int get_typical_interval(struct menu_device *data)
  * menu_select - selects the next idle state to enter
  * @drv: cpuidle driver containing state data
  * @dev: the CPU
+ * @stop_tick: indication on whether or not to stop the tick
  */
-static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
+static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
+		       bool *stop_tick)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	struct device *device = get_cpu_device(dev->cpu);
@@ -303,8 +306,10 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		latency_req = resume_latency;
 
 	/* Special case when user has set very strict latency requirement */
-	if (unlikely(latency_req == 0))
+	if (unlikely(latency_req == 0)) {
+		*stop_tick = false;
 		return 0;
+	}
 
 	/* determine the expected residency time, round up */
 	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length());
@@ -354,6 +359,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	if (latency_req > interactivity_req)
 		latency_req = interactivity_req;
 
+	expected_interval = data->predicted_us;
 	/*
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
@@ -369,15 +375,30 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 			idx = i; /* first enabled state */
 		if (s->target_residency > data->predicted_us)
 			break;
-		if (s->exit_latency > latency_req)
+		if (s->exit_latency > latency_req) {
+			/*
+			 * If we break out of the loop for latency reasons, use
+			 * the target residency of the selected state as the
+			 * expected idle duration so that the tick is retained
+			 * as long as that target residency is low enough.
+			 */
+			expected_interval = drv->states[idx].target_residency;
 			break;
-
+		}
 		idx = i;
 	}
 
 	if (idx == -1)
 		idx = 0; /* No states enabled. Must use 0. */
 
+	/*
+	 * Don't stop the tick if the selected state is a polling one or if the
+	 * expected idle duration is shorter than the tick period length.
+	 */
+	if ((drv->states[idx].flags & CPUIDLE_FLAG_POLLING) ||
+	    expected_interval < TICK_USEC)
+		*stop_tick = false;
+
 	data->last_state_idx = idx;
 
 	return data->last_state_idx;
@@ -397,6 +418,7 @@ static void menu_reflect(struct cpuidle_device *dev, int index)
 
 	data->last_state_idx = index;
 	data->needs_update = 1;
+	data->tick_wakeup = tick_nohz_idle_got_tick();
 }
 
 /**
@@ -427,14 +449,27 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * assume the state was never reached and the exit latency is 0.
 	 */
 
-	/* measured value */
-	measured_us = cpuidle_get_last_residency(dev);
-
-	/* Deduct exit latency */
-	if (measured_us > 2 * target->exit_latency)
-		measured_us -= target->exit_latency;
-	else
-		measured_us /= 2;
+	if (data->tick_wakeup && data->next_timer_us > TICK_USEC) {
+		/*
+		 * The nohz code said that there wouldn't be any events within
+		 * the tick boundary (if the tick was stopped), but the idle
+		 * duration predictor had a differing opinion.  Since the CPU
+		 * was woken up by a tick (that wasn't stopped after all), the
+		 * predictor was not quite right, so assume that the CPU could
+		 * have been idle long (but not forever) to help the idle
+		 * duration predictor do a better job next time.
+		 */
+		measured_us = 9 * MAX_INTERESTING / 10;
+	} else {
+		/* measured value */
+		measured_us = cpuidle_get_last_residency(dev);
+
+		/* Deduct exit latency */
+		if (measured_us > 2 * target->exit_latency)
+			measured_us -= target->exit_latency;
+		else
+			measured_us /= 2;
+	}
 
 	/* Make sure our coefficients do not exceed unity */
 	if (measured_us > data->next_timer_us)

commit 0759e80b84e34a84e7e46e2b1adb528c83d84a47
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Nov 7 11:33:49 2017 +0100

    PM / QoS: Fix device resume latency framework
    
    The special value of 0 for device resume latency PM QoS means
    "no restriction", but there are two problems with that.
    
    First, device resume latency PM QoS requests with 0 as the
    value are always put in front of requests with positive
    values in the priority lists used internally by the PM QoS
    framework, causing 0 to be chosen as an effective constraint
    value.  However, that 0 is then interpreted as "no restriction"
    effectively overriding the other requests with specific
    restrictions which is incorrect.
    
    Second, the users of device resume latency PM QoS have no
    way to specify that *any* resume latency at all should be
    avoided, which is an artificial limitation in general.
    
    To address these issues, modify device resume latency PM QoS to
    use S32_MAX as the "no constraint" value and 0 as the "no
    latency at all" one and rework its users (the cpuidle menu
    governor, the genpd QoS governor and the runtime PM framework)
    to follow these changes.
    
    Also add a special "n/a" value to the corresponding user space I/F
    to allow user space to indicate that it cannot accept any resume
    latencies at all for the given device.
    
    Fixes: 85dc0b8a4019 (PM / QoS: Make it possible to expose PM QoS latency constraints)
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=197323
    Reported-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Tested-by: Reinette Chatre <reinette.chatre@intel.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Tero Kristo <t-kristo@ti.com>
    Reviewed-by: Ramesh Thomas <ramesh.thomas@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 48eaf2879228..aa390404e85f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -298,8 +298,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		data->needs_update = 0;
 	}
 
-	/* resume_latency is 0 means no restriction */
-	if (resume_latency && resume_latency < latency_req)
+	if (resume_latency < latency_req &&
+	    resume_latency != PM_QOS_RESUME_LATENCY_NO_CONSTRAINT)
 		latency_req = resume_latency;
 
 	/* Special case when user has set very strict latency requirement */

commit dc2251bf98c66db3f4e055b751968f0871037ae4
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Aug 23 23:19:57 2017 +0200

    cpuidle: Eliminate the CPUIDLE_DRIVER_STATE_START symbol
    
    On some architectures the first (index 0) idle state is a polling
    one and it doesn't really save energy, so there is the
    CPUIDLE_DRIVER_STATE_START symbol allowing some pieces of
    cpuidle code to avoid using that state.
    
    However, this makes the code rather hard to follow.  It is better
    to explicitly avoid the polling state, so add a new cpuidle state
    flag CPUIDLE_FLAG_POLLING to mark it and make the relevant code
    check that flag for the first state instead of using the
    CPUIDLE_DRIVER_STATE_START symbol.
    
    In the ACPI processor driver that cannot always rely on the state
    flags (like before the states table has been set up) define
    a new internal symbol ACPI_IDLE_STATE_START equivalent to the
    CPUIDLE_DRIVER_STATE_START one and drop the latter.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 61b64c2b2cb8..48eaf2879228 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -324,8 +324,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	expected_interval = get_typical_interval(data);
 	expected_interval = min(expected_interval, data->next_timer_us);
 
-	if (CPUIDLE_DRIVER_STATE_START > 0) {
-		struct cpuidle_state *s = &drv->states[CPUIDLE_DRIVER_STATE_START];
+	first_idx = 0;
+	if (drv->states[0].flags & CPUIDLE_FLAG_POLLING) {
+		struct cpuidle_state *s = &drv->states[1];
 		unsigned int polling_threshold;
 
 		/*
@@ -336,12 +337,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		polling_threshold = max_t(unsigned int, 20, s->target_residency);
 		if (data->next_timer_us > polling_threshold &&
 		    latency_req > s->exit_latency && !s->disabled &&
-		    !dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable)
-			first_idx = CPUIDLE_DRIVER_STATE_START;
-		else
-			first_idx = CPUIDLE_DRIVER_STATE_START - 1;
-	} else {
-		first_idx = 0;
+		    !dev->states_usage[1].disable)
+			first_idx = 1;
 	}
 
 	/*

commit 3ed09c94580de9d5b18cc35d1f97e9f24cd9233b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Jun 26 15:38:15 2017 +1000

    cpuidle: menu: allow state 0 to be disabled
    
    The menu driver does not allow state0 to be disabled completely.
    If it is disabled but other enabled states don't meet latency
    requirements, it is still used.
    
    Fix this by starting with the first enabled idle state. Fall back
    to state 0 if no idle states are enabled (arguably this should be
    -EINVAL if it is attempted, but this is the minimal fix).
    
    Acked-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b2330fd69e34..61b64c2b2cb8 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -286,6 +286,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	struct device *device = get_cpu_device(dev->cpu);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
+	int first_idx;
+	int idx;
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
@@ -335,11 +337,11 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		if (data->next_timer_us > polling_threshold &&
 		    latency_req > s->exit_latency && !s->disabled &&
 		    !dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable)
-			data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
+			first_idx = CPUIDLE_DRIVER_STATE_START;
 		else
-			data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
+			first_idx = CPUIDLE_DRIVER_STATE_START - 1;
 	} else {
-		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
+		first_idx = 0;
 	}
 
 	/*
@@ -359,20 +361,28 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
 	 */
-	for (i = data->last_state_idx + 1; i < drv->state_count; i++) {
+	idx = -1;
+	for (i = first_idx; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
 		struct cpuidle_state_usage *su = &dev->states_usage[i];
 
 		if (s->disabled || su->disable)
 			continue;
+		if (idx == -1)
+			idx = i; /* first enabled state */
 		if (s->target_residency > data->predicted_us)
 			break;
 		if (s->exit_latency > latency_req)
 			break;
 
-		data->last_state_idx = i;
+		idx = i;
 	}
 
+	if (idx == -1)
+		idx = 0; /* No states enabled. Must use 0. */
+
+	data->last_state_idx = idx;
+
 	return data->last_state_idx;
 }
 

commit 1827adb11ad26b2290dc9fe2aaf54976b2439865
Merge: 78769912f680 5eca1c10cbaa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 3 10:16:38 2017 -0800

    Merge branch 'WIP.sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull sched.h split-up from Ingo Molnar:
     "The point of these changes is to significantly reduce the
      <linux/sched.h> header footprint, to speed up the kernel build and to
      have a cleaner header structure.
    
      After these changes the new <linux/sched.h>'s typical preprocessed
      size goes down from a previous ~0.68 MB (~22K lines) to ~0.45 MB (~15K
      lines), which is around 40% faster to build on typical configs.
    
      Not much changed from the last version (-v2) posted three weeks ago: I
      eliminated quirks, backmerged fixes plus I rebased it to an upstream
      SHA1 from yesterday that includes most changes queued up in -next plus
      all sched.h changes that were pending from Andrew.
    
      I've re-tested the series both on x86 and on cross-arch defconfigs,
      and did a bisectability test at a number of random points.
    
      I tried to test as many build configurations as possible, but some
      build breakage is probably still left - but it should be mostly
      limited to architectures that have no cross-compiler binaries
      available on kernel.org, and non-default configurations"
    
    * 'WIP.sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (146 commits)
      sched/headers: Clean up <linux/sched.h>
      sched/headers: Remove #ifdefs from <linux/sched.h>
      sched/headers: Remove the <linux/topology.h> include from <linux/sched.h>
      sched/headers, hrtimer: Remove the <linux/wait.h> include from <linux/hrtimer.h>
      sched/headers, x86/apic: Remove the <linux/pm.h> header inclusion from <asm/apic.h>
      sched/headers, timers: Remove the <linux/sysctl.h> include from <linux/timer.h>
      sched/headers: Remove <linux/magic.h> from <linux/sched/task_stack.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/init.h>
      sched/core: Remove unused prefetch_stack()
      sched/headers: Remove <linux/rculist.h> from <linux/sched.h>
      sched/headers: Remove the 'init_pid_ns' prototype from <linux/sched.h>
      sched/headers: Remove <linux/signal.h> from <linux/sched.h>
      sched/headers: Remove <linux/rwsem.h> from <linux/sched.h>
      sched/headers: Remove the runqueue_is_locked() prototype
      sched/headers: Remove <linux/sched.h> from <linux/sched/hotplug.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/debug.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/nohz.h>
      sched/headers: Remove <linux/sched.h> from <linux/sched/stat.h>
      sched/headers: Remove the <linux/gfp.h> include from <linux/sched.h>
      sched/headers: Remove <linux/rtmutex.h> from <linux/sched.h>
      ...

commit 03441a3482a31462c93509939a388877e3cd9261
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/stat.h>
    
    We are going to split <linux/sched/stat.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/stat.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index fe86060e48f7..6d8a4026a903 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -19,6 +19,7 @@
 #include <linux/tick.h>
 #include <linux/sched.h>
 #include <linux/sched/loadavg.h>
+#include <linux/sched/stat.h>
 #include <linux/math64.h>
 #include <linux/cpu.h>
 

commit 4f17722c7256af8e17c2c4f29f170247264bdf48
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 08:45:17 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/loadavg.h>
    
    We are going to split <linux/sched/loadavg.h> out of <linux/sched.h>, which
    will have to be picked up from a couple of .c files.
    
    Create a trivial placeholder <linux/sched/topology.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 8d6d25c38c02..fe86060e48f7 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -18,6 +18,7 @@
 #include <linux/hrtimer.h>
 #include <linux/tick.h>
 #include <linux/sched.h>
+#include <linux/sched/loadavg.h>
 #include <linux/math64.h>
 #include <linux/cpu.h>
 

commit 6dbf5cea05a7098a69f294c96b6d76f08562cae5
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 24 13:25:14 2017 +0100

    cpuidle: menu: Avoid taking spinlock for accessing QoS values
    
    After commit 9908859acaa9 (cpuidle/menu: add per CPU PM QoS resume
    latency consideration) the cpuidle menu governor calls
    dev_pm_qos_read_value() on CPU devices to read the current resume
    latency QoS constraint values for them.  That function takes a spinlock
    to prevent the device's power.qos pointer from becoming NULL during
    the access which is a problem for the RT patchset where spinlocks are
    converted into mutexes and the idle loop stops working.
    
    However, it is not even necessary for the menu governor to take
    that spinlock, because the power.qos pointer accessed under it
    cannot be modified during the access anyway.
    
    For this reason, introduce a "raw" routine for accessing device
    QoS resume latency constraints without locking and use it in the
    menu governor.
    
    Fixes: 9908859acaa9 (cpuidle/menu: add per CPU PM QoS resume latency consideration)
    Acked-by: Alex Shi <alex.shi@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 8d6d25c38c02..6d6f46e79d94 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -287,7 +287,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
-	int resume_latency = dev_pm_qos_read_value(device);
+	int resume_latency = dev_pm_qos_raw_read_value(device);
 
 	if (data->needs_update) {
 		menu_update(drv, dev);

commit 9908859acaa95640d4a07991a93f7cd5bfc18e02
Author: Alex Shi <alex.shi@linaro.org>
Date:   Thu Jan 12 21:27:04 2017 +0800

    cpuidle/menu: add per CPU PM QoS resume latency consideration
    
    There may be special requirements on CPU response time, like if a
    interrupt is pinned to a CPU, that CPU should not go into excessively
    deep idle states.  For this reason, add a mechanism for adding
    PM QoS resume latency constraints for individual CPUs and modify the
    menu governor to take them into account.
    
    To that end, extend the device PM QoS pm_qos_resume_latency attribute
    to CPUs, which is possible, because the exit latency for CPUs is
    effectively equivalent to the resume latency for devices.
    
    Signed-off-by: Alex Shi <alex.shi@linaro.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    [ rjw : Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 07e36bb54006..8d6d25c38c02 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -19,6 +19,7 @@
 #include <linux/tick.h>
 #include <linux/sched.h>
 #include <linux/math64.h>
+#include <linux/cpu.h>
 
 /*
  * Please note when changing the tuning values:
@@ -280,17 +281,23 @@ static unsigned int get_typical_interval(struct menu_device *data)
 static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
+	struct device *device = get_cpu_device(dev->cpu);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
 	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
+	int resume_latency = dev_pm_qos_read_value(device);
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
 		data->needs_update = 0;
 	}
 
+	/* resume_latency is 0 means no restriction */
+	if (resume_latency && resume_latency < latency_req)
+		latency_req = resume_latency;
+
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))
 		return 0;

commit 8e37e1a2a3295f5d99e6dbe99eca24eca7a034ef
Author: Alex Shi <alex.shi@linaro.org>
Date:   Thu Jan 12 21:27:02 2017 +0800

    cpuidle/menu: stop seeking deeper idle if current state is deep enough
    
    Obsolete commit 71abbbf856a0 (cpuidle: extend cpuidle and menu governor
    to handle dynamic states) wanted to introduce dynamic C-states, but that
    idea was dropped long ago.  The nonsense deeper C-state checking
    remained, though.
    
    Since both target_residency and exit_latency are longer for deeper
    idle state, there's no need to waste CPU time on useless checks.
    
    Signed-off-by: Alex Shi <alex.shi@linaro.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index d9b5b9398a0f..07e36bb54006 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -357,9 +357,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		if (s->disabled || su->disable)
 			continue;
 		if (s->target_residency > data->predicted_us)
-			continue;
+			break;
 		if (s->exit_latency > latency_req)
-			continue;
+			break;
 
 		data->last_state_idx = i;
 	}

commit e5f1b245870d59be0e6cc3b33edf5406a3b59648
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Oct 5 09:33:12 2016 +0200

    cpuidle: governors: Remove remaining old module code
    
    The governor's code use try_module_get() and put_module() to refcount
    the governor's module. But the governors are not compiled as module.
    
    The refcount does not prevent to switch the governor or unload
    a module as they aren't compiled as modules. The code is pointless,
    so remove it.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 03d38c291de6..d9b5b9398a0f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -19,7 +19,6 @@
 #include <linux/tick.h>
 #include <linux/sched.h>
 #include <linux/math64.h>
-#include <linux/module.h>
 
 /*
  * Please note when changing the tuning values:
@@ -484,7 +483,6 @@ static struct cpuidle_governor menu_governor = {
 	.enable =	menu_enable_device,
 	.select =	menu_select,
 	.reflect =	menu_reflect,
-	.owner =	THIS_MODULE,
 };
 
 /**

commit 0c313cb207326f759a58f486214288411b25d4cf
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun Mar 20 01:33:35 2016 +0100

    cpuidle: menu: Fall back to polling if next timer event is near
    
    Commit a9ceb78bc75c (cpuidle,menu: use interactivity_req to disable
    polling) changed the behavior of the fallback state selection part
    of menu_select() so it looks at interactivity_req instead of
    data->next_timer_us when it makes its decision.  That effectively
    caused polling to be used more often as fallback idle which led to
    significant increases of energy consumption in some cases.
    
    Commit e132b9b3bc7f (cpuidle: menu: use high confidence factors
    only when considering polling) changed that logic again to be more
    predictable, but that didn't help with the increased energy
    consumption problem.
    
    For this reason, go back to making decisions on which state to fall
    back to based on data->next_timer_us which is the time we know for
    sure something will happen rather than a prediction (which may be
    inaccurate and turns out to be so often enough to be problematic).
    However, take the target residency of the first proper idle state
    (C1) into account, so that state is not used as the fallback one
    if its target residency is greater than data->next_timer_us.
    
    Fixes: a9ceb78bc75c (cpuidle,menu: use interactivity_req to disable polling)
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reported-and-tested-by: Doug Smythies <dsmythies@telus.net>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 00c5f891e352..03d38c291de6 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -315,17 +315,21 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	expected_interval = min(expected_interval, data->next_timer_us);
 
 	if (CPUIDLE_DRIVER_STATE_START > 0) {
-		data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
+		struct cpuidle_state *s = &drv->states[CPUIDLE_DRIVER_STATE_START];
+		unsigned int polling_threshold;
+
 		/*
 		 * We want to default to C1 (hlt), not to busy polling
 		 * unless the timer is happening really really soon, or
 		 * C1's exit latency exceeds the user configured limit.
 		 */
-		if (expected_interval > drv->states[CPUIDLE_DRIVER_STATE_START].target_residency &&
-		    latency_req > drv->states[CPUIDLE_DRIVER_STATE_START].exit_latency &&
-		    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
+		polling_threshold = max_t(unsigned int, 20, s->target_residency);
+		if (data->next_timer_us > polling_threshold &&
+		    latency_req > s->exit_latency && !s->disabled &&
 		    !dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable)
 			data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
+		else
+			data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
 	} else {
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 	}

commit e132b9b3bc7f19e9b158e42b323881d5dee5ecf3
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Mar 16 12:14:00 2016 -0400

    cpuidle: menu: use high confidence factors only when considering polling
    
    The menu governor uses five different factors to pick the
    idle state:
     - the user configured latency_req
     - the time until the next timer (next_timer_us)
     - the typical sleep interval, as measured recently
     - an estimate of sleep time by dividing next_timer_us by an observed factor
     - a load corrected version of the above, divided again by load
    
    Only the first three items are known with enough confidence that
    we can use them to consider polling, instead of an actual CPU
    idle state, because the cost of being wrong about polling can be
    excessive power use.
    
    The latter two are used in the menu governor's main selection
    loop, and can result in choosing a shallower idle state when
    the system is expected to be busy again soon.
    
    This pushes a busy system in the "performance" direction of
    the performance<>power tradeoff, when choosing between idle
    states, but stays more strictly on the "power" state when
    deciding between polling and C1.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 27fc733cb5b9..00c5f891e352 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -196,7 +196,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev);
  * of points is below a threshold. If it is... then use the
  * average of these 8 points as the estimated value.
  */
-static void get_typical_interval(struct menu_device *data)
+static unsigned int get_typical_interval(struct menu_device *data)
 {
 	int i, divisor;
 	unsigned int max, thresh, avg;
@@ -253,9 +253,7 @@ static void get_typical_interval(struct menu_device *data)
 	if (likely(variance <= U64_MAX/36)) {
 		if ((((u64)avg*avg > variance*36) && (divisor * 4 >= INTERVALS * 3))
 							|| variance <= 400) {
-			if (data->next_timer_us > avg)
-				data->predicted_us = avg;
-			return;
+			return avg;
 		}
 	}
 
@@ -269,7 +267,7 @@ static void get_typical_interval(struct menu_device *data)
 	 * with sporadic activity with a bunch of short pauses.
 	 */
 	if ((divisor * 4) <= INTERVALS * 3)
-		return;
+		return UINT_MAX;
 
 	thresh = max - 1;
 	goto again;
@@ -286,6 +284,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
+	unsigned int expected_interval;
 	unsigned long nr_iowaiters, cpu_load;
 
 	if (data->needs_update) {
@@ -312,31 +311,38 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
-	get_typical_interval(data);
-
-	/*
-	 * Performance multiplier defines a minimum predicted idle
-	 * duration / latency ratio. Adjust the latency limit if
-	 * necessary.
-	 */
-	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
-	if (latency_req > interactivity_req)
-		latency_req = interactivity_req;
+	expected_interval = get_typical_interval(data);
+	expected_interval = min(expected_interval, data->next_timer_us);
 
 	if (CPUIDLE_DRIVER_STATE_START > 0) {
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
 		/*
 		 * We want to default to C1 (hlt), not to busy polling
-		 * unless the timer is happening really really soon.
+		 * unless the timer is happening really really soon, or
+		 * C1's exit latency exceeds the user configured limit.
 		 */
-		if (interactivity_req > 20 &&
+		if (expected_interval > drv->states[CPUIDLE_DRIVER_STATE_START].target_residency &&
+		    latency_req > drv->states[CPUIDLE_DRIVER_STATE_START].exit_latency &&
 		    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
-			dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
+		    !dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable)
 			data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 	} else {
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 	}
 
+	/*
+	 * Use the lowest expected idle interval to pick the idle state.
+	 */
+	data->predicted_us = min(data->predicted_us, expected_interval);
+
+	/*
+	 * Use the performance multiplier and the user-configurable
+	 * latency_req to determine the maximum exit latency.
+	 */
+	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
+	if (latency_req > interactivity_req)
+		latency_req = interactivity_req;
+
 	/*
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.

commit 3b99669b75db04e411bb298591224a9e8e4f57fb
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Tue Feb 16 20:19:19 2016 +0100

    cpuidle: menu: help gcc generate slightly better code
    
    We know that the avg variable actually ends up holding a 32 bit
    quantity, since it's an average of such numbers. It is only a u64
    because it is temporarily used to hold the sum. Making it an actual
    u32 allows gcc to generate slightly better code, e.g. when computing
    the square, it can do a 32x32->64 multiply.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index beef7ae123ba..27fc733cb5b9 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -199,8 +199,8 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev);
 static void get_typical_interval(struct menu_device *data)
 {
 	int i, divisor;
-	unsigned int max, thresh;
-	uint64_t avg, variance;
+	unsigned int max, thresh, avg;
+	uint64_t sum, variance;
 
 	thresh = UINT_MAX; /* Discard outliers above this value */
 
@@ -208,28 +208,28 @@ static void get_typical_interval(struct menu_device *data)
 
 	/* First calculate the average of past intervals */
 	max = 0;
-	avg = 0;
+	sum = 0;
 	divisor = 0;
 	for (i = 0; i < INTERVALS; i++) {
 		unsigned int value = data->intervals[i];
 		if (value <= thresh) {
-			avg += value;
+			sum += value;
 			divisor++;
 			if (value > max)
 				max = value;
 		}
 	}
 	if (divisor == INTERVALS)
-		avg >>= INTERVAL_SHIFT;
+		avg = sum >> INTERVAL_SHIFT;
 	else
-		do_div(avg, divisor);
+		avg = div_u64(sum, divisor);
 
 	/* Then try to determine variance */
 	variance = 0;
 	for (i = 0; i < INTERVALS; i++) {
 		unsigned int value = data->intervals[i];
 		if (value <= thresh) {
-			int64_t diff = value - avg;
+			int64_t diff = (int64_t)value - avg;
 			variance += diff * diff;
 		}
 	}
@@ -251,7 +251,7 @@ static void get_typical_interval(struct menu_device *data)
 	 * Use this result only if there is no timer to wake us up sooner.
 	 */
 	if (likely(variance <= U64_MAX/36)) {
-		if (((avg*avg > variance*36) && (divisor * 4 >= INTERVALS * 3))
+		if ((((u64)avg*avg > variance*36) && (divisor * 4 >= INTERVALS * 3))
 							|| variance <= 400) {
 			if (data->next_timer_us > avg)
 				data->predicted_us = avg;

commit 7024b18ca461bab45e5fb329f6e3d904d5109401
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Tue Feb 16 20:19:18 2016 +0100

    cpuidle: menu: avoid expensive square root computation
    
    Computing the integer square root is a rather expensive operation, at
    least compared to doing a 64x64 -> 64 multiply (avg*avg) and, on 64
    bit platforms, doing an extra comparison to a constant (variance <=
    U64_MAX/36).
    
    On 64 bit platforms, this does mean that we add a restriction on the
    range of the variance where we end up using the estimate (since
    previously the stddev <= ULONG_MAX was a tautology), but on the other
    hand, we extend the range quite substantially on 32 bit platforms - in
    both cases, we now allow standard deviations up to 715 seconds, which
    is for example guaranteed if all observations are less than 1430
    seconds.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 0742b3296673..beef7ae123ba 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -200,7 +200,7 @@ static void get_typical_interval(struct menu_device *data)
 {
 	int i, divisor;
 	unsigned int max, thresh;
-	uint64_t avg, stddev;
+	uint64_t avg, variance;
 
 	thresh = UINT_MAX; /* Discard outliers above this value */
 
@@ -224,36 +224,35 @@ static void get_typical_interval(struct menu_device *data)
 	else
 		do_div(avg, divisor);
 
-	/* Then try to determine standard deviation */
-	stddev = 0;
+	/* Then try to determine variance */
+	variance = 0;
 	for (i = 0; i < INTERVALS; i++) {
 		unsigned int value = data->intervals[i];
 		if (value <= thresh) {
 			int64_t diff = value - avg;
-			stddev += diff * diff;
+			variance += diff * diff;
 		}
 	}
 	if (divisor == INTERVALS)
-		stddev >>= INTERVAL_SHIFT;
+		variance >>= INTERVAL_SHIFT;
 	else
-		do_div(stddev, divisor);
+		do_div(variance, divisor);
 
 	/*
-	 * The typical interval is obtained when standard deviation is small
-	 * or standard deviation is small compared to the average interval.
-	 *
-	 * int_sqrt() formal parameter type is unsigned long. When the
-	 * greatest difference to an outlier exceeds ~65 ms * sqrt(divisor)
-	 * the resulting squared standard deviation exceeds the input domain
-	 * of int_sqrt on platforms where unsigned long is 32 bits in size.
-	 * In such case reject the candidate average.
+	 * The typical interval is obtained when standard deviation is
+	 * small (stddev <= 20 us, variance <= 400 us^2) or standard
+	 * deviation is small compared to the average interval (avg >
+	 * 6*stddev, avg^2 > 36*variance). The average is smaller than
+	 * UINT_MAX aka U32_MAX, so computing its square does not
+	 * overflow a u64. We simply reject this candidate average if
+	 * the standard deviation is greater than 715 s (which is
+	 * rather unlikely).
 	 *
 	 * Use this result only if there is no timer to wake us up sooner.
 	 */
-	if (likely(stddev <= ULONG_MAX)) {
-		stddev = int_sqrt(stddev);
-		if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
-							|| stddev <= 20) {
+	if (likely(variance <= U64_MAX/36)) {
+		if (((avg*avg > variance*36) && (divisor * 4 >= INTERVALS * 3))
+							|| variance <= 400) {
 			if (data->next_timer_us > avg)
 				data->predicted_us = avg;
 			return;

commit 5bb1729cbdfbe974ad6385be94b14afbac97e19f
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jan 16 00:56:34 2016 +0100

    cpuidle: menu: Avoid pointless checks in menu_select()
    
    If menu_select() cannot find a suitable state to return, it will
    return the state index stored in data->last_state_idx.  This
    means that it is pointless to look at the states whose indices
    are less than or equal to data->last_state_idx in the main loop,
    so don't do that.
    
    Given that those checks are done on every idle state selection, this
    change can save quite a bit of completely unnecessary overhead.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index be0bae0b41e9..0742b3296673 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -342,7 +342,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
 	 */
-	for (i = CPUIDLE_DRIVER_STATE_START; i < drv->state_count; i++) {
+	for (i = data->last_state_idx + 1; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
 		struct cpuidle_state_usage *su = &dev->states_usage[i];
 

commit 9c4b2867ed7c8c8784dd417ffd16e705e81eb145
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jan 14 23:24:22 2016 +0100

    cpuidle: menu: Fix menu_select() for CPUIDLE_DRIVER_STATE_START == 0
    
    Commit a9ceb78bc75c (cpuidle,menu: use interactivity_req to disable
    polling) exposed a bug in menu_select() causing it to return -1
    on systems with CPUIDLE_DRIVER_STATE_START equal to zero, although
    it should have returned 0.  As a result, idle states are not entered
    by CPUs on those systems.
    
    Namely, on the systems in question data->last_state_idx is initially
    equal to -1 and the above commit modified the condition that would
    have caused it to be changed to 0 to be less likely to trigger which
    exposed the problem.  However, setting data->last_state_idx initially
    to -1 doesn't make sense at all and on the affected systems it should
    always be set to CPUIDLE_DRIVER_STATE_START (ie. 0) unconditionally,
    so make that happen.
    
    Fixes: a9ceb78bc75c (cpuidle,menu: use interactivity_req to disable polling)
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 7b0971d97cc3..be0bae0b41e9 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -294,8 +294,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		data->needs_update = 0;
 	}
 
-	data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
-
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))
 		return 0;
@@ -326,14 +324,19 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	if (latency_req > interactivity_req)
 		latency_req = interactivity_req;
 
-	/*
-	 * We want to default to C1 (hlt), not to busy polling
-	 * unless the timer is happening really really soon.
-	 */
-	if (interactivity_req > 20 &&
-	    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
-		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
+	if (CPUIDLE_DRIVER_STATE_START > 0) {
+		data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
+		/*
+		 * We want to default to C1 (hlt), not to busy polling
+		 * unless the timer is happening really really soon.
+		 */
+		if (interactivity_req > 20 &&
+		    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
+			dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
+			data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
+	} else {
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
+	}
 
 	/*
 	 * Find the idle state with the lowest power while satisfying

commit efddfd90fb7f5ba3c7d1bff923a3626a78eee553
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Nov 3 17:34:19 2015 -0500

    cpuidle,menu: smooth out measured_us calculation
    
    The cpuidle state tables contain the maximum exit latency for each
    cpuidle state. On x86, that is the exit latency for when the entire
    package goes into that same idle state.
    
    However, a lot of the time we only go into the core idle state,
    not the package idle state. This means we see a much smaller exit
    latency.
    
    We have no way to detect whether we went into the core or package
    idle state while idle, and that is ok.
    
    However, the current menu_update logic does have the potential to
    trip up the repeating pattern detection in get_typical_interval.
    If the system is experiencing an exit latency near the idle state's
    exit latency, some of the samples will have exit_us subtracted,
    while others will not. This turns a repeating pattern into mush,
    potentially breaking get_typical_interval.
    
    Furthermore, for smaller sleep intervals, we know the chance that
    all the cores in the package went to the same idle state are fairly
    small. Dividing the measured_us by two, instead of subtracting the
    full exit latency when hitting a small measured_us, will reduce the
    error.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b1a55731f921..7b0971d97cc3 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -404,8 +404,10 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	measured_us = cpuidle_get_last_residency(dev);
 
 	/* Deduct exit latency */
-	if (measured_us > target->exit_latency)
+	if (measured_us > 2 * target->exit_latency)
 		measured_us -= target->exit_latency;
+	else
+		measured_us /= 2;
 
 	/* Make sure our coefficients do not exceed unity */
 	if (measured_us > data->next_timer_us)

commit a9ceb78bc75ca47972096372ff3d48648b16317a
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Nov 3 17:34:18 2015 -0500

    cpuidle,menu: use interactivity_req to disable polling
    
    The menu governor carefully figures out how much time we typically
    sleep for an estimated sleep interval, or whether there is a repeating
    pattern going on, and corrects that estimate for the CPU load.
    
    Then it proceeds to ignore that information when determining whether
    or not to consider polling. This is not a big deal on most x86 CPUs,
    which have very low C1 latencies, and the patch should not have any
    effect on those CPUs.
    
    However, certain CPUs (eg. Atom) have much higher C1 latencies, and
    it would be good to not waste performance and power on those CPUs if
    we are expecting a very low wakeup latency.
    
    Disable polling based on the estimated interactivity requirement, not
    on the time to the next timer interrupt.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index ecc242a586c9..b1a55731f921 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -330,7 +330,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
 	 */
-	if (data->next_timer_us > 20 &&
+	if (interactivity_req > 20 &&
 	    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
 		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;

commit 7884084f3bcc98adfbd8b90a2bd6bcf10c4df2cd
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Nov 3 17:34:17 2015 -0500

    cpuidle,x86: increase forced cut-off for polling to 20us
    
    The cpuidle menu governor has a forced cut-off for polling at 5us,
    in order to deal with firmware that gives the OS bad information
    on cpuidle states, leading to the system spending way too much time
    in polling.
    
    However, at least one x86 CPU family (Atom) has chips that have
    a 20us break-even point for C1. Forcing the polling cut-off to
    less than that wastes performance and power.
    
    Increase the polling cut-off to 20us.
    
    Systems with a lower C1 latency will be found in the states table by
    the menu governor, which will pick those states as appropriate.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 22e4463d1787..ecc242a586c9 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -330,7 +330,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
 	 */
-	if (data->next_timer_us > 5 &&
+	if (data->next_timer_us > 20 &&
 	    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
 		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;

commit a802ea96454570f3c526dd9d7ad8c706e570444d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 4 22:53:28 2015 +0200

    cpuidle: Check the sign of index in cpuidle_reflect()
    
    Avoid calling the governor's ->reflect method if the state index
    passed to cpuidle_reflect() is negative.
    
    This allows the analogous check to be dropped from menu_reflect(),
    so do that too, and ensures that arbitrary error codes can be
    passed to cpuidle_reflect() as the index with no adverse
    consequences.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b8a5fa15ca24..22e4463d1787 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -367,9 +367,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 static void menu_reflect(struct cpuidle_device *dev, int index)
 {
 	struct menu_device *data = this_cpu_ptr(&menu_devices);
+
 	data->last_state_idx = index;
-	if (index >= 0)
-		data->needs_update = 1;
+	data->needs_update = 1;
 }
 
 /**

commit ee3c86f356b4dfc130442d0bd68ff2da2bd60538
Author: Javi Merino <javi.merino@arm.com>
Date:   Thu Apr 16 12:43:51 2015 -0700

    cpuidle: menu: use DIV_ROUND_CLOSEST_ULL()
    
    Now that the kernel provides DIV_ROUND_CLOSEST_ULL(), drop the internal
    implementation and use the kernel one.
    
    Signed-off-by: Javi Merino <javi.merino@arm.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 40580794e23d..b8a5fa15ca24 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -190,12 +190,6 @@ static DEFINE_PER_CPU(struct menu_device, menu_devices);
 
 static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev);
 
-/* This implements DIV_ROUND_CLOSEST but avoids 64 bit division */
-static u64 div_round64(u64 dividend, u32 divisor)
-{
-	return div_u64(dividend + (divisor / 2), divisor);
-}
-
 /*
  * Try detecting repeating patterns by keeping track of the last 8
  * intervals, and checking if the standard deviation of that set
@@ -317,7 +311,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * operands are 32 bits.
 	 * Make sure to round up for half microseconds.
 	 */
-	data->predicted_us = div_round64((uint64_t)data->next_timer_us *
+	data->predicted_us = DIV_ROUND_CLOSEST_ULL((uint64_t)data->next_timer_us *
 					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 

commit 4108b3d96273784f697dd6d8e59ef9203a10a02d
Author: Len Brown <len.brown@intel.com>
Date:   Tue Dec 16 01:52:06 2014 -0500

    cpuidle: menu: Better idle duration measurement without using CPUIDLE_FLAG_TIME_INVALID
    
    When menu sees CPUIDLE_FLAG_TIME_INVALID, it ignores its timestamps,
    and assumes that idle lasted as long as the time till next predicted
    timer expiration.
    
    But if an interrupt was seen and serviced before that duration,
    it would actually be more accurate to use the measured time
    rather than rounding up to the next predicted timer expiration.
    
    And if an interrupt is seen and serviced such that the mesured time
    exceeds the time till next predicted timer expiration, then
    truncating to that expiration is the right thing to do --
    since we can never stay idle past that timer expiration.
    
    So the code can do a better job without
    checking for CPUIDLE_FLAG_TIME_INVALID.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reviewed-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 659d7b0c9ebf..40580794e23d 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -396,8 +396,8 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * power state and occurrence of the wakeup event.
 	 *
 	 * If the entered idle state didn't support residency measurements,
-	 * we are basically lost in the dark how much time passed.
-	 * As a compromise, assume we slept for the whole expected time.
+	 * we use them anyway if they are short, and if long,
+	 * truncate to the whole expected time.
 	 *
 	 * Any measured amount of time will include the exit latency.
 	 * Since we are interested in when the wakeup begun, not when it
@@ -405,22 +405,17 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * the measured amount of time is less than the exit latency,
 	 * assume the state was never reached and the exit latency is 0.
 	 */
-	if (unlikely(target->flags & CPUIDLE_FLAG_TIME_INVALID)) {
-		/* Use timer value as is */
-		measured_us = data->next_timer_us;
 
-	} else {
-		/* Use measured value */
-		measured_us = cpuidle_get_last_residency(dev);
+	/* measured value */
+	measured_us = cpuidle_get_last_residency(dev);
 
-		/* Deduct exit latency */
-		if (measured_us > target->exit_latency)
-			measured_us -= target->exit_latency;
+	/* Deduct exit latency */
+	if (measured_us > target->exit_latency)
+		measured_us -= target->exit_latency;
 
-		/* Make sure our coefficients do not exceed unity */
-		if (measured_us > data->next_timer_us)
-			measured_us = data->next_timer_us;
-	}
+	/* Make sure our coefficients do not exceed unity */
+	if (measured_us > data->next_timer_us)
+		measured_us = data->next_timer_us;
 
 	/* Update our correction ratio */
 	new_factor = data->correction_factor[data->bucket];

commit b82b6cca488074da3852e8a54fde1d9f74bf1557
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Nov 12 16:03:50 2014 +0100

    cpuidle: Invert CPUIDLE_FLAG_TIME_VALID logic
    
    The only place where the time is invalid is when the ACPI_CSTATE_FFH entry
    method is not set. Otherwise for all the drivers, the time can be correctly
    measured.
    
    Instead of duplicating the CPUIDLE_FLAG_TIME_VALID flag in all the drivers
    for all the states, just invert the logic by replacing it by the flag
    CPUIDLE_FLAG_TIME_INVALID, hence we can set this flag only for the acpi idle
    driver, remove the former flag from all the drivers and invert the logic with
    this flag in the different governor.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 710a233b9b0d..659d7b0c9ebf 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -405,7 +405,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * the measured amount of time is less than the exit latency,
 	 * assume the state was never reached and the exit latency is 0.
 	 */
-	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID))) {
+	if (unlikely(target->flags & CPUIDLE_FLAG_TIME_INVALID)) {
 		/* Use timer value as is */
 		measured_us = data->next_timer_us;
 

commit 229b6863b2cf9514f08e468fea586bc195ebcf50
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:30 2014 -0500

    drivers/cpuidle: Replace __get_cpu_var uses for address calculation
    
    All of these are for address calculation. Replace with
    this_cpu_ptr().
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: linux-pm@vger.kernel.org
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    [cpufreq changes]
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 34db2fb3ef1e..710a233b9b0d 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -289,7 +289,7 @@ static void get_typical_interval(struct menu_device *data)
  */
 static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
-	struct menu_device *data = &__get_cpu_var(menu_devices);
+	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
@@ -372,7 +372,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
  */
 static void menu_reflect(struct cpuidle_device *dev, int index)
 {
-	struct menu_device *data = &__get_cpu_var(menu_devices);
+	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	data->last_state_idx = index;
 	if (index >= 0)
 		data->needs_update = 1;
@@ -385,7 +385,7 @@ static void menu_reflect(struct cpuidle_device *dev, int index)
  */
 static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
-	struct menu_device *data = &__get_cpu_var(menu_devices);
+	struct menu_device *data = this_cpu_ptr(&menu_devices);
 	int last_idx = data->last_state_idx;
 	struct cpuidle_state *target = &drv->states[last_idx];
 	unsigned int measured_us;

commit c9d26423e56ce1ab4d786f92aebecf859d419293
Merge: a11c5c9ef6dc af5b7e84d022
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 14 18:13:46 2014 -0600

    Merge tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more ACPI and power management updates from Rafael Wysocki:
     "These are a couple of regression fixes, cpuidle menu governor
      optimizations, fixes for ACPI proccessor and battery drivers,
      hibernation fix to avoid problems related to the e820 memory map,
      fixes for a few cpufreq drivers and a new version of the suspend
      profiling tool analyze_suspend.py.
    
      Specifics:
    
       - Fix for an ACPI-based device hotplug regression introduced in 3.14
         that causes a kernel panic to trigger when memory hot-remove is
         attempted with CONFIG_ACPI_HOTPLUG_MEMORY unset from Tang Chen
    
       - Fix for a cpufreq regression introduced in 3.16 that triggers a
         "sleeping function called from invalid context" bug in
         dev_pm_opp_init_cpufreq_table() from Stephen Boyd
    
       - ACPI battery driver fix for a warning message added in 3.16 that
         prints silly stuff sometimes from Mariusz Ceier
    
       - Hibernation fix for safer handling of mismatches in the 820 memory
         map between the configurations during image creation and during the
         subsequent restore from Chun-Yi Lee
    
       - ACPI processor driver fix to handle CPU hotplug notifications
         correctly during system suspend/resume from Lan Tianyu
    
       - Series of four cpuidle menu governor cleanups that also should
         speed it up a bit from Mel Gorman
    
       - Fixes for the speedstep-smi, integrator, cpu0 and arm_big_little
         cpufreq drivers from Hans Wennborg, Himangi Saraogi, Markus
         Pargmann and Uwe Kleine-König
    
       - Version 3.0 of the analyze_suspend.py suspend profiling tool from
         Todd E Brandt"
    
    * tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      ACPI / battery: Fix warning message in acpi_battery_get_state()
      PM / tools: analyze_suspend.py: update to v3.0
      cpufreq: arm_big_little: fix module license spec
      cpufreq: speedstep-smi: fix decimal printf specifiers
      ACPI / hotplug: Check scan handlers in acpi_scan_hot_remove()
      cpufreq: OPP: Avoid sleeping while atomic
      cpufreq: cpu0: Do not print error message when deferring
      cpufreq: integrator: Use set_cpus_allowed_ptr
      PM / hibernate: avoid unsafe pages in e820 reserved regions
      ACPI / processor: Make acpi_cpu_soft_notify() process CPU FROZEN events
      cpuidle: menu: Lookup CPU runqueues less
      cpuidle: menu: Call nr_iowait_cpu less times
      cpuidle: menu: Use ktime_to_us instead of reinventing the wheel
      cpuidle: menu: Use shifts when calculating averages where possible

commit 158c12948f3012fbe15f066f308db23502d3db0a
Merge: 172bfe09dc52 51a7097426f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 6 21:03:53 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree changes from Jiri Kosina:
     "Summer edition of trivial tree updates"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (23 commits)
      doc: fix two typos in watchdog-api.txt
      irq-gic: remove file name from heading comment
      MAINTAINERS: Add miscdevice.h to file list for char/misc drivers.
      scsi: mvsas: mv_sas.c: Fix for possible null pointer dereference
      doc: replace "practise" with "practice" in Documentation
      befs: remove check for CONFIG_BEFS_RW
      scsi: doc: fix 'SCSI_NCR_SETUP_MASTER_PARITY'
      drivers/usb/phy/phy.c: remove a leading space
      mfd: fix comment
      cpuidle: fix comment
      doc: hpfall.c: fix missing null-terminate after strncpy call
      usb: doc: hotplug.txt code typos
      kbuild: fix comment in Makefile.modinst
      SH: add proper prompt to SH_MAGIC_PANEL_R2_VERSION
      ARM: msm: Remove MSM_SCM
      crypto: Remove MPILIB_EXTRA
      doc: CN: remove dead link, kerneltrap.org no longer works
      media: update reference, kerneltrap.org no longer works
      hexagon: update reference, kerneltrap.org no longer works
      doc: LSM: update reference, kerneltrap.org no longer works
      ...

commit 372ba8cb46b271a7662b92cbefedee56725f6bd0
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 14:19:21 2014 +0100

    cpuidle: menu: Lookup CPU runqueues less
    
    The menu governer makes separate lookups of the CPU runqueue to get
    load and number of IO waiters but it can be done with a single lookup.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f55d8260ec43..27702742b319 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -134,12 +134,9 @@ struct menu_device {
 #define LOAD_INT(x) ((x) >> FSHIFT)
 #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
 
-static int get_loadavg(void)
+static inline int get_loadavg(unsigned long load)
 {
-	unsigned long this = this_cpu_load();
-
-
-	return LOAD_INT(this) * 10 + LOAD_FRAC(this) / 10;
+	return LOAD_INT(load) * 10 + LOAD_FRAC(load) / 10;
 }
 
 static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters)
@@ -175,13 +172,13 @@ static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters
  * to be, the higher this multiplier, and thus the higher
  * the barrier to go to an expensive C state.
  */
-static inline int performance_multiplier(unsigned long nr_iowaiters)
+static inline int performance_multiplier(unsigned long nr_iowaiters, unsigned long load)
 {
 	int mult = 1;
 
 	/* for higher loadavg, we are more reluctant */
 
-	mult += 2 * get_loadavg();
+	mult += 2 * get_loadavg(load);
 
 	/* for IO wait tasks (per cpu!) we add 5x each */
 	mult += 10 * nr_iowaiters;
@@ -296,7 +293,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
-	unsigned long nr_iowaiters;
+	unsigned long nr_iowaiters, cpu_load;
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -312,7 +309,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	/* determine the expected residency time, round up */
 	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length());
 
-	nr_iowaiters = nr_iowait_cpu(smp_processor_id());
+	get_iowait_load(&nr_iowaiters, &cpu_load);
 	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
 
 	/*
@@ -331,7 +328,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * duration / latency ratio. Adjust the latency limit if
 	 * necessary.
 	 */
-	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters);
+	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters, cpu_load);
 	if (latency_req > interactivity_req)
 		latency_req = interactivity_req;
 

commit 64b4ca5cb6e1a9f577588db5765dc996ddf595e1
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 14:19:20 2014 +0100

    cpuidle: menu: Call nr_iowait_cpu less times
    
    menu_select() via inline functions calls nr_iowait_cpu() twice as much
    as necessary.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index ba6df6044fff..f55d8260ec43 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -142,7 +142,7 @@ static int get_loadavg(void)
 	return LOAD_INT(this) * 10 + LOAD_FRAC(this) / 10;
 }
 
-static inline int which_bucket(unsigned int duration)
+static inline int which_bucket(unsigned int duration, unsigned long nr_iowaiters)
 {
 	int bucket = 0;
 
@@ -152,7 +152,7 @@ static inline int which_bucket(unsigned int duration)
 	 * This allows us to calculate
 	 * E(duration)|iowait
 	 */
-	if (nr_iowait_cpu(smp_processor_id()))
+	if (nr_iowaiters)
 		bucket = BUCKETS/2;
 
 	if (duration < 10)
@@ -175,7 +175,7 @@ static inline int which_bucket(unsigned int duration)
  * to be, the higher this multiplier, and thus the higher
  * the barrier to go to an expensive C state.
  */
-static inline int performance_multiplier(void)
+static inline int performance_multiplier(unsigned long nr_iowaiters)
 {
 	int mult = 1;
 
@@ -184,7 +184,7 @@ static inline int performance_multiplier(void)
 	mult += 2 * get_loadavg();
 
 	/* for IO wait tasks (per cpu!) we add 5x each */
-	mult += 10 * nr_iowait_cpu(smp_processor_id());
+	mult += 10 * nr_iowaiters;
 
 	return mult;
 }
@@ -296,6 +296,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
+	unsigned long nr_iowaiters;
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -311,8 +312,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	/* determine the expected residency time, round up */
 	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length());
 
-
-	data->bucket = which_bucket(data->next_timer_us);
+	nr_iowaiters = nr_iowait_cpu(smp_processor_id());
+	data->bucket = which_bucket(data->next_timer_us, nr_iowaiters);
 
 	/*
 	 * Force the result of multiplication to be 64 bits even if both
@@ -330,7 +331,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * duration / latency ratio. Adjust the latency limit if
 	 * necessary.
 	 */
-	interactivity_req = data->predicted_us / performance_multiplier();
+	interactivity_req = data->predicted_us / performance_multiplier(nr_iowaiters);
 	if (latency_req > interactivity_req)
 		latency_req = interactivity_req;
 

commit 107d4f4601a1408d04a5b54ffba507c92c235f58
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 14:19:19 2014 +0100

    cpuidle: menu: Use ktime_to_us instead of reinventing the wheel
    
    The ktime_to_us implementation is slightly better than the one implemented
    in menu.c. Use it
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c36e1ea7ef08..ba6df6044fff 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -296,7 +296,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	unsigned int interactivity_req;
-	struct timespec t;
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -310,9 +309,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		return 0;
 
 	/* determine the expected residency time, round up */
-	t = ktime_to_timespec(tick_nohz_get_sleep_length());
-	data->next_timer_us =
-		t.tv_sec * USEC_PER_SEC + t.tv_nsec / NSEC_PER_USEC;
+	data->next_timer_us = ktime_to_us(tick_nohz_get_sleep_length());
 
 
 	data->bucket = which_bucket(data->next_timer_us);

commit ae77930060338226a4377d3b93580c43b5ec82ae
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Aug 6 14:19:18 2014 +0100

    cpuidle: menu: Use shifts when calculating averages where possible
    
    We use do_div even though the divisor will usually be a power-of-two
    unless there are unusual outliers. Use shifts where possible
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c3732fa74f82..c36e1ea7ef08 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -31,7 +31,8 @@
  * The default values do not overflow.
  */
 #define BUCKETS 12
-#define INTERVALS 8
+#define INTERVAL_SHIFT 3
+#define INTERVALS (1UL << INTERVAL_SHIFT)
 #define RESOLUTION 1024
 #define DECAY 8
 #define MAX_INTERESTING 50000
@@ -227,7 +228,10 @@ static void get_typical_interval(struct menu_device *data)
 				max = value;
 		}
 	}
-	do_div(avg, divisor);
+	if (divisor == INTERVALS)
+		avg >>= INTERVAL_SHIFT;
+	else
+		do_div(avg, divisor);
 
 	/* Then try to determine standard deviation */
 	stddev = 0;
@@ -238,7 +242,11 @@ static void get_typical_interval(struct menu_device *data)
 			stddev += diff * diff;
 		}
 	}
-	do_div(stddev, divisor);
+	if (divisor == INTERVALS)
+		stddev >>= INTERVAL_SHIFT;
+	else
+		do_div(stddev, divisor);
+
 	/*
 	 * The typical interval is obtained when standard deviation is small
 	 * or standard deviation is small compared to the average interval.

commit 8804ed155a5276cfbb7115493570b6874c89a12b
Author: Mohammad Merajul Islam Molla <meraj.enigma@gmail.com>
Date:   Thu Jul 24 21:02:01 2014 +0600

    cpuidle: menu governor - remove unused macro STDDEV_THRESH
    
    STDDEV_THRESH was once defined and used in menu governor. But now its no longer
    used anywhere. So removing the define.
    
    Signed-off-by: Mohammad Merajul Islam Molla <meraj.enigma@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c4f80c15a48d..c3732fa74f82 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -35,7 +35,6 @@
 #define RESOLUTION 1024
 #define DECAY 8
 #define MAX_INTERESTING 50000
-#define STDDEV_THRESH 400
 
 
 /*

commit 2fba5376ed7d8cdebd873ee124b8e5dd3c936f92
Author: Antonio Ospite <ao2@ao2.it>
Date:   Wed Jun 4 14:03:45 2014 +0200

    cpuidle: fix comment
    
    Signed-off-by: Antonio Ospite <ao2@ao2.it>
    Cc: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
    Cc: "tuukka.tikkanen@linaro.org" <tuukka.tikkanen@linaro.org>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 71b523293354..daf850250b6a 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -406,7 +406,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 *
 	 * Any measured amount of time will include the exit latency.
 	 * Since we are interested in when the wakeup begun, not when it
-	 * was completed, we must substract the exit latency. However, if
+	 * was completed, we must subtract the exit latency. However, if
 	 * the measured amount of time is less than the exit latency,
 	 * assume the state was never reached and the exit latency is 0.
 	 */

commit bed4d597a0f99b380d24ab3a9da47b62cbf1ad0e
Author: Chander Kashyap <chander.kashyap@linaro.org>
Date:   Tue Apr 22 18:08:04 2014 +0530

    cpuidle / menu: move repeated correction factor check to init
    
    In menu_select function we check for correction factor every time.
    If it is zero we are initializing to unity. Hence move it to init function
    and initialise by unity, hence avoid repeated comparisons.
    
    Signed-off-by: Chander Kashyap <chander.kashyap@linaro.org>
    Reviewed-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 3ca15a8cbaa8..c4f80c15a48d 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -310,13 +310,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 	data->bucket = which_bucket(data->next_timer_us);
 
-	/*
-	 * if the correction factor is 0 (eg first time init or cpu hotplug
-	 * etc), we actually want to start out with a unity factor.
-	 */
-	if (data->correction_factor[data->bucket] == 0)
-		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
-
 	/*
 	 * Force the result of multiplication to be 64 bits even if both
 	 * operands are 32 bits.
@@ -466,9 +459,17 @@ static int menu_enable_device(struct cpuidle_driver *drv,
 				struct cpuidle_device *dev)
 {
 	struct menu_device *data = &per_cpu(menu_devices, dev->cpu);
+	int i;
 
 	memset(data, 0, sizeof(struct menu_device));
 
+	/*
+	 * if the correction factor is 0 (eg first time init or cpu hotplug
+	 * etc), we actually want to start out with a unity factor.
+	 */
+	for(i = 0; i < BUCKETS; i++)
+		data->correction_factor[i] = RESOLUTION * DECAY;
+
 	return 0;
 }
 

commit 3836785a1bdcd6706c68ad46bf53adc0b057b310
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu May 1 00:14:04 2014 +0200

    cpuidle / menu: Return (-1) if there are no suitable states
    
    If there is a PM QoS latency limit and all of the sufficiently shallow
    C-states are disabled, the cpuidle menu governor returns 0 which on
    some systems is CPUIDLE_DRIVER_STATE_START and shouldn't be returned
    if that C-state has been disabled.
    
    Fix the issue by modifying the menu governor to return (-1) in such
    situations.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 71b523293354..3ca15a8cbaa8 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -296,7 +296,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		data->needs_update = 0;
 	}
 
-	data->last_state_idx = 0;
+	data->last_state_idx = CPUIDLE_DRIVER_STATE_START - 1;
 
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))

commit 96e95182e95fd4e0069ff4d6ee1888fe9031d154
Author: tuukka.tikkanen@linaro.org <tuukka.tikkanen@linaro.org>
Date:   Mon Feb 24 08:29:35 2014 +0200

    cpuidle: Move perf multiplier calculation out of the selection loop
    
    The menu governor performance multiplier defines a minimum predicted
    idle duration to latency ratio. Instead of checking this separately
    in every iteration of the state selection loop, adjust the overall
    latency restriction for the whole loop if this restriction is tighter
    than what is set by the QoS subsystem.
    
    The original test
    s->exit_latency * multiplier > data->predicted_us
    becomes
    s->exit_latency > data->predicted_us / multiplier
    by dividing both sides of the comparison by "multiplier".
    
    While division is likely to be several times slower than multiplication,
    the minor performance hit allows making a generic sleep state selection
    function based on (sleep duration, maximum latency) tuple.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b347c101c1f7..71b523293354 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -288,7 +288,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
-	int multiplier;
+	unsigned int interactivity_req;
 	struct timespec t;
 
 	if (data->needs_update) {
@@ -310,8 +310,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 	data->bucket = which_bucket(data->next_timer_us);
 
-	multiplier = performance_multiplier();
-
 	/*
 	 * if the correction factor is 0 (eg first time init or cpu hotplug
 	 * etc), we actually want to start out with a unity factor.
@@ -330,6 +328,15 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 	get_typical_interval(data);
 
+	/*
+	 * Performance multiplier defines a minimum predicted idle
+	 * duration / latency ratio. Adjust the latency limit if
+	 * necessary.
+	 */
+	interactivity_req = data->predicted_us / performance_multiplier();
+	if (latency_req > interactivity_req)
+		latency_req = interactivity_req;
+
 	/*
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
@@ -353,8 +360,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 			continue;
 		if (s->exit_latency > latency_req)
 			continue;
-		if (s->exit_latency * multiplier > data->predicted_us)
-			continue;
 
 		data->last_state_idx = i;
 	}

commit 61c66d6efa23759f1061d80ced668977fd28337d
Author: tuukka.tikkanen@linaro.org <tuukka.tikkanen@linaro.org>
Date:   Mon Feb 24 08:29:34 2014 +0200

    cpuidle: Do not substract exit latency from assumed sleep length
    
    The menu governor statistics update function tries to determine the
    amount of time between entry to low power state and the occurrence
    of the wakeup event. However, the time measured by the framework
    includes exit latency on top of the desired value. This exit latency
    is substracted from the measured value to obtain the desired value.
    
    When measured value is not available, the menu governor assumes
    the wakeup was caused by the timer and the time is equal to remaining
    timer length. No exit latency should be substracted from this value.
    
    This patch prevents the erroneous substraction and clarifies the
    associated comment. It also removes one intermediate variable that
    serves no purpose.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f0995dd2469f..b347c101c1f7 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -387,32 +387,40 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int last_idx = data->last_state_idx;
-	unsigned int last_idle_us = cpuidle_get_last_residency(dev);
 	struct cpuidle_state *target = &drv->states[last_idx];
 	unsigned int measured_us;
 	unsigned int new_factor;
 
 	/*
-	 * Ugh, this idle state doesn't support residency measurements, so we
-	 * are basically lost in the dark.  As a compromise, assume we slept
-	 * for the whole expected time.
+	 * Try to figure out how much time passed between entry to low
+	 * power state and occurrence of the wakeup event.
+	 *
+	 * If the entered idle state didn't support residency measurements,
+	 * we are basically lost in the dark how much time passed.
+	 * As a compromise, assume we slept for the whole expected time.
+	 *
+	 * Any measured amount of time will include the exit latency.
+	 * Since we are interested in when the wakeup begun, not when it
+	 * was completed, we must substract the exit latency. However, if
+	 * the measured amount of time is less than the exit latency,
+	 * assume the state was never reached and the exit latency is 0.
 	 */
-	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID)))
-		last_idle_us = data->next_timer_us;
-
+	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID))) {
+		/* Use timer value as is */
+		measured_us = data->next_timer_us;
 
-	measured_us = last_idle_us;
+	} else {
+		/* Use measured value */
+		measured_us = cpuidle_get_last_residency(dev);
 
-	/*
-	 * We correct for the exit latency; we are assuming here that the
-	 * exit latency happens after the event that we're interested in.
-	 */
-	if (measured_us > target->exit_latency)
-		measured_us -= target->exit_latency;
+		/* Deduct exit latency */
+		if (measured_us > target->exit_latency)
+			measured_us -= target->exit_latency;
 
-	/* Make sure our coefficients do not exceed unity */
-	if (measured_us > data->next_timer_us)
-		measured_us = data->next_timer_us;
+		/* Make sure our coefficients do not exceed unity */
+		if (measured_us > data->next_timer_us)
+			measured_us = data->next_timer_us;
+	}
 
 	/* Update our correction ratio */
 	new_factor = data->correction_factor[data->bucket];
@@ -439,7 +447,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	data->correction_factor[data->bucket] = new_factor;
 
 	/* update the repeating-pattern data */
-	data->intervals[data->interval_ptr++] = last_idle_us;
+	data->intervals[data->interval_ptr++] = measured_us;
 	if (data->interval_ptr >= INTERVALS)
 		data->interval_ptr = 0;
 }

commit 7ac26436677024ab5e57e25c2c83d4c3fc232106
Author: tuukka.tikkanen@linaro.org <tuukka.tikkanen@linaro.org>
Date:   Mon Feb 24 08:29:33 2014 +0200

    cpuidle: Ensure menu coefficients stay within domain
    
    The menu governor uses coefficients as one method of actual idle
    period length estimation. The coefficients are, as detailed below,
    multipliers giving expected idle period length from time until next
    timer expiry. The multipliers are supposed to have domain of (0..1].
    
    The coefficients are fractions where only the numerators are stored
    and denominators are a shared constant RESOLUTION*DECAY. Since the
    value of the coefficient should always be greater than 0 and less
    than or equal to 1, the numerator must have a value greater than
    0 and less than or equal to RESOLUTION*DECAY.
    
    If the coefficients are updated with measured idle durations exceeding
    timer length, the multiplier may reach values exceeding unity (i.e.
    the stored numerator exceeds RESOLUTION*DECAY). This patch ensures that
    the multipliers are updated with durations capped to timer length.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 115386a46e9e..f0995dd2469f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -410,6 +410,9 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	if (measured_us > target->exit_latency)
 		measured_us -= target->exit_latency;
 
+	/* Make sure our coefficients do not exceed unity */
+	if (measured_us > data->next_timer_us)
+		measured_us = data->next_timer_us;
 
 	/* Update our correction ratio */
 	new_factor = data->correction_factor[data->bucket];

commit 22695ab6314083d9d045ce9276e9ae79eb889531
Author: tuukka.tikkanen@linaro.org <tuukka.tikkanen@linaro.org>
Date:   Mon Feb 24 08:29:32 2014 +0200

    cpuidle: Use actual state latency in menu governor
    
    Currently menu governor records the exit latency of the state it has
    chosen for the idle period. The stored latency value is then later
    used to calculate the actual length of the idle period. This value
    may however be incorrect, as the entered state may not be the one
    chosen by the governor. The entered state information is available,
    so we can use that to obtain the real exit latency.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index e9a2a27134c1..115386a46e9e 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -124,7 +124,6 @@ struct menu_device {
 
 	unsigned int	next_timer_us;
 	unsigned int	predicted_us;
-	unsigned int	exit_us;
 	unsigned int	bucket;
 	unsigned int	correction_factor[BUCKETS];
 	unsigned int	intervals[INTERVALS];
@@ -298,7 +297,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	}
 
 	data->last_state_idx = 0;
-	data->exit_us = 0;
 
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))
@@ -359,7 +357,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 			continue;
 
 		data->last_state_idx = i;
-		data->exit_us = s->exit_latency;
 	}
 
 	return data->last_state_idx;
@@ -410,8 +407,8 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * We correct for the exit latency; we are assuming here that the
 	 * exit latency happens after the event that we're interested in.
 	 */
-	if (measured_us > data->exit_us)
-		measured_us -= data->exit_us;
+	if (measured_us > target->exit_latency)
+		measured_us -= target->exit_latency;
 
 
 	/* Update our correction ratio */

commit 5dc2f5a3030552f2501cb34ac6d979455630e486
Author: tuukka.tikkanen@linaro.org <tuukka.tikkanen@linaro.org>
Date:   Mon Feb 24 08:29:31 2014 +0200

    cpuidle: rename expected_us to next_timer_us in menu governor
    
    The field expected_us is used to store the time remaining until next
    timer expiry. The name is inaccurate, as we really do not expect all
    wakeups to be caused by timers. In addition, another field with a very
    similar name (predicted_us) is used to store the predicted time
    remaining until any wakeup source being active.
    
    This patch renames expected_us to next_timer_us in order to better
    reflect the contained information.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index cf7f2f0e4ef5..e9a2a27134c1 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -122,7 +122,7 @@ struct menu_device {
 	int		last_state_idx;
 	int             needs_update;
 
-	unsigned int	expected_us;
+	unsigned int	next_timer_us;
 	unsigned int	predicted_us;
 	unsigned int	exit_us;
 	unsigned int	bucket;
@@ -257,7 +257,7 @@ static void get_typical_interval(struct menu_device *data)
 		stddev = int_sqrt(stddev);
 		if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
 							|| stddev <= 20) {
-			if (data->expected_us > avg)
+			if (data->next_timer_us > avg)
 				data->predicted_us = avg;
 			return;
 		}
@@ -306,11 +306,11 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 	/* determine the expected residency time, round up */
 	t = ktime_to_timespec(tick_nohz_get_sleep_length());
-	data->expected_us =
+	data->next_timer_us =
 		t.tv_sec * USEC_PER_SEC + t.tv_nsec / NSEC_PER_USEC;
 
 
-	data->bucket = which_bucket(data->expected_us);
+	data->bucket = which_bucket(data->next_timer_us);
 
 	multiplier = performance_multiplier();
 
@@ -326,7 +326,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * operands are 32 bits.
 	 * Make sure to round up for half microseconds.
 	 */
-	data->predicted_us = div_round64((uint64_t)data->expected_us *
+	data->predicted_us = div_round64((uint64_t)data->next_timer_us *
 					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
@@ -336,7 +336,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
 	 */
-	if (data->expected_us > 5 &&
+	if (data->next_timer_us > 5 &&
 	    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
 		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
@@ -401,7 +401,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * for the whole expected time.
 	 */
 	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID)))
-		last_idle_us = data->expected_us;
+		last_idle_us = data->next_timer_us;
 
 
 	measured_us = last_idle_us;
@@ -418,8 +418,8 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	new_factor = data->correction_factor[data->bucket];
 	new_factor -= new_factor / DECAY;
 
-	if (data->expected_us > 0 && measured_us < MAX_INTERESTING)
-		new_factor += RESOLUTION * measured_us / data->expected_us;
+	if (data->next_timer_us > 0 && measured_us < MAX_INTERESTING)
+		new_factor += RESOLUTION * measured_us / data->next_timer_us;
 	else
 		/*
 		 * we were idle so long that we count it as a perfect

commit 51f245b895e3fe4cbac0b2633e54a1830864a83f
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:41 2013 +0300

    cpuidle: Change struct menu_device field types
    
    Field predicted_us value can never exceed expected_us value, but it has
    a potentially larger type. As there is no need for additional 32 bits of
    zeroes on 32 bit plaforms, change the type of predicted_us to match the
    type of expected_us.
    
    Field correction_factor is used to store a value that cannot exceed the
    product of RESOLUTION and DECAY (default 1024*8 = 8192). The constants
    cannot in practice be incremented to such values, that they'd overflow
    unsigned int even on 32 bit systems, so the type is changed to avoid
    unnecessary 64 bit arithmetic on 32 bit systems.
    
    One multiplication of (now) 32 bit values needs an added cast to avoid
    truncation of the result and has been added.
    
    In order to avoid another multiplication from 32 bit domain to 64 bit
    domain, the new correction_factor calculation has been changed from
    new = old * (DECAY-1) / DECAY
    to
    new = old - old / DECAY,
    which with infinite precision would yeild exactly the same result, but
    now changes the direction of rounding. The impact is not significant as
    the maximum accumulated difference cannot exceed the value of DECAY,
    which is relatively small compared to product of RESOLUTION and DECAY
    (8 / 8192).
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index a8b31b0ca57f..cf7f2f0e4ef5 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -123,10 +123,10 @@ struct menu_device {
 	int             needs_update;
 
 	unsigned int	expected_us;
-	u64		predicted_us;
+	unsigned int	predicted_us;
 	unsigned int	exit_us;
 	unsigned int	bucket;
-	u64		correction_factor[BUCKETS];
+	unsigned int	correction_factor[BUCKETS];
 	unsigned int	intervals[INTERVALS];
 	int		interval_ptr;
 };
@@ -321,8 +321,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	if (data->correction_factor[data->bucket] == 0)
 		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
 
-	/* Make sure to round up for half microseconds */
-	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
+	/*
+	 * Force the result of multiplication to be 64 bits even if both
+	 * operands are 32 bits.
+	 * Make sure to round up for half microseconds.
+	 */
+	data->predicted_us = div_round64((uint64_t)data->expected_us *
+					 data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
 	get_typical_interval(data);
@@ -388,7 +393,7 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	unsigned int last_idle_us = cpuidle_get_last_residency(dev);
 	struct cpuidle_state *target = &drv->states[last_idx];
 	unsigned int measured_us;
-	u64 new_factor;
+	unsigned int new_factor;
 
 	/*
 	 * Ugh, this idle state doesn't support residency measurements, so we
@@ -409,10 +414,9 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		measured_us -= data->exit_us;
 
 
-	/* update our correction ratio */
-
-	new_factor = data->correction_factor[data->bucket]
-			* (DECAY - 1) / DECAY;
+	/* Update our correction ratio */
+	new_factor = data->correction_factor[data->bucket];
+	new_factor -= new_factor / DECAY;
 
 	if (data->expected_us > 0 && measured_us < MAX_INTERESTING)
 		new_factor += RESOLUTION * measured_us / data->expected_us;
@@ -425,9 +429,11 @@ static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 	/*
 	 * We don't want 0 as factor; we always want at least
-	 * a tiny bit of estimated time.
+	 * a tiny bit of estimated time. Fortunately, due to rounding,
+	 * new_factor will stay nonzero regardless of measured_us values
+	 * and the compiler can eliminate this test as long as DECAY > 1.
 	 */
-	if (new_factor == 0)
+	if (DECAY == 1 && unlikely(new_factor == 0))
 		new_factor = 1;
 
 	data->correction_factor[data->bucket] = new_factor;

commit decd51bbcd7fd949840da4cc634f6b70baa1b512
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:40 2013 +0300

    cpuidle: Add a comment warning about possible overflow
    
    The menu governor has a number of tunable constants that may be changed
    in the source. If certain combination of values are chosen, an overflow
    is possible when the correction_factor is being recalculated.
    
    This patch adds a warning regarding this possibility and describes the
    change needed for fixing the issue. The change should not be permanently
    enabled, as it will hurt performance when it is not needed.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index a56081ce170f..a8b31b0ca57f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -21,6 +21,15 @@
 #include <linux/math64.h>
 #include <linux/module.h>
 
+/*
+ * Please note when changing the tuning values:
+ * If (MAX_INTERESTING-1) * RESOLUTION > UINT_MAX, the result of
+ * a scaling operation multiplication may overflow on 32 bit platforms.
+ * In that case, #define RESOLUTION as ULL to get 64 bit result:
+ * #define RESOLUTION 1024ULL
+ *
+ * The default values do not overflow.
+ */
 #define BUCKETS 12
 #define INTERVALS 8
 #define RESOLUTION 1024

commit 0e96d5adcfef22f86e4463909728d63f88944749
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:39 2013 +0300

    cpuidle: Fix variable domains in get_typical_interval()
    
    The menu governor uses a static function get_typical_interval() to
    try to detect a repeating pattern of wakeups. The previous interval
    durations are stored as an array of unsigned ints, but the arithmetic
    in the function is performed exclusively as 64 bit values, even when
    the value stored in a variable is known not to exceed unsigned int,
    which may be smaller and more efficient on some platforms.
    
    This patch changes the types of varibles used to store some
    intermediates, the maximum and and the cutoff threshold to unsigned
    ints. Average and standard deviation are still treated as 64 bit values,
    even when the values are known to be within the domain of unsigned int,
    to avoid casts to ensure correct integer promotion for arithmetic
    operations.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index ee88838d7f08..a56081ce170f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -200,18 +200,19 @@ static u64 div_round64(u64 dividend, u32 divisor)
 static void get_typical_interval(struct menu_device *data)
 {
 	int i, divisor;
-	uint64_t max, avg, stddev;
-	int64_t thresh = LLONG_MAX; /* Discard outliers above this value. */
+	unsigned int max, thresh;
+	uint64_t avg, stddev;
+
+	thresh = UINT_MAX; /* Discard outliers above this value */
 
 again:
 
-	/* first calculate average and standard deviation of the past */
+	/* First calculate the average of past intervals */
 	max = 0;
 	avg = 0;
 	divisor = 0;
-	stddev = 0;
 	for (i = 0; i < INTERVALS; i++) {
-		int64_t value = data->intervals[i];
+		unsigned int value = data->intervals[i];
 		if (value <= thresh) {
 			avg += value;
 			divisor++;
@@ -221,8 +222,10 @@ static void get_typical_interval(struct menu_device *data)
 	}
 	do_div(avg, divisor);
 
+	/* Then try to determine standard deviation */
+	stddev = 0;
 	for (i = 0; i < INTERVALS; i++) {
-		int64_t value = data->intervals[i];
+		unsigned int value = data->intervals[i];
 		if (value <= thresh) {
 			int64_t diff = value - avg;
 			stddev += diff * diff;

commit 939e33b7fcd4980f21ff4c9558eb27fe81d16cdb
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:38 2013 +0300

    cpuidle: Fix menu_device->intervals type
    
    Struct menu_device member intervals is declared as u32, but the value
    stored is (unsigned) int. The type is changed to match the value being
    stored.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 94a303739ba8..ee88838d7f08 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -118,7 +118,7 @@ struct menu_device {
 	unsigned int	exit_us;
 	unsigned int	bucket;
 	u64		correction_factor[BUCKETS];
-	u32		intervals[INTERVALS];
+	unsigned int	intervals[INTERVALS];
 	int		interval_ptr;
 };
 

commit 4cd46bca8ca8ad20cadf74fd9b76364c46ed0f31
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:37 2013 +0300

    cpuidle: CodingStyle: Break up multiple assignments on single line
    
    The function get_typical_interval() initializes a number of variables
    that are immediately after declarations assigned constant values.
    In addition, there are multiple assignments on a single line, which
    is explicitly forbidden by Documentation/CodingStyle.
    
    This patch removes redundant initial values for the variables and
    breaks up the multiple assignment line.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index af7b3f6b260d..94a303739ba8 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -199,14 +199,17 @@ static u64 div_round64(u64 dividend, u32 divisor)
  */
 static void get_typical_interval(struct menu_device *data)
 {
-	int i = 0, divisor = 0;
-	uint64_t max = 0, avg = 0, stddev = 0;
+	int i, divisor;
+	uint64_t max, avg, stddev;
 	int64_t thresh = LLONG_MAX; /* Discard outliers above this value. */
 
 again:
 
 	/* first calculate average and standard deviation of the past */
-	max = avg = divisor = stddev = 0;
+	max = 0;
+	avg = 0;
+	divisor = 0;
+	stddev = 0;
 	for (i = 0; i < INTERVALS; i++) {
 		int64_t value = data->intervals[i];
 		if (value <= thresh) {

commit 0d6a7ffa4cc67cc70bf1f4e24fbb0747632845a2
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:36 2013 +0300

    cpuidle: Check called function parameter in get_typical_interval()
    
    get_typical_interval() uses int_sqrt() in calculation of standard
    deviation. The formal parameter of int_sqrt() is unsigned long, which
    may on some platforms be smaller than the 64 bit unsigned integer used
    as the actual parameter. The overflow can occur frequently when actual
    idle period lengths are in hundreds of milliseconds.
    
    This patch adds a check for such overflow and rejects the candidate
    average when an overflow would occur.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f1fadbecfa1b..af7b3f6b260d 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -226,18 +226,26 @@ static void get_typical_interval(struct menu_device *data)
 		}
 	}
 	do_div(stddev, divisor);
-	stddev = int_sqrt(stddev);
 	/*
 	 * The typical interval is obtained when standard deviation is small
 	 * or standard deviation is small compared to the average interval.
 	 *
+	 * int_sqrt() formal parameter type is unsigned long. When the
+	 * greatest difference to an outlier exceeds ~65 ms * sqrt(divisor)
+	 * the resulting squared standard deviation exceeds the input domain
+	 * of int_sqrt on platforms where unsigned long is 32 bits in size.
+	 * In such case reject the candidate average.
+	 *
 	 * Use this result only if there is no timer to wake us up sooner.
 	 */
-	if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
+	if (likely(stddev <= ULONG_MAX)) {
+		stddev = int_sqrt(stddev);
+		if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
 							|| stddev <= 20) {
-		if (data->expected_us > avg)
-			data->predicted_us = avg;
-		return;
+			if (data->expected_us > avg)
+				data->predicted_us = avg;
+			return;
+		}
 	}
 
 	/*

commit 017099e25fb7e482a249d36a654556d32f601f71
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:35 2013 +0300

    cpuidle: Rearrange code and comments in get_typical_interval()
    
    This patch rearranges a if-return-elsif-goto-fi-return sequence into
    if-return-fi-if-return-fi-goto sequence. The functionality remains the
    same. Also, a lengthy comment that did not describe the functionality
    in the order it occurs is split into half and top half is moved closer
    to actual implementation it describes.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 351697305fe7..f1fadbecfa1b 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -228,14 +228,6 @@ static void get_typical_interval(struct menu_device *data)
 	do_div(stddev, divisor);
 	stddev = int_sqrt(stddev);
 	/*
-	 * If we have outliers to the upside in our distribution, discard
-	 * those by setting the threshold to exclude these outliers, then
-	 * calculate the average and standard deviation again. Once we get
-	 * down to the bottom 3/4 of our samples, stop excluding samples.
-	 *
-	 * This can deal with workloads that have long pauses interspersed
-	 * with sporadic activity with a bunch of short pauses.
-	 *
 	 * The typical interval is obtained when standard deviation is small
 	 * or standard deviation is small compared to the average interval.
 	 *
@@ -246,12 +238,22 @@ static void get_typical_interval(struct menu_device *data)
 		if (data->expected_us > avg)
 			data->predicted_us = avg;
 		return;
-
-	} else if ((divisor * 4) > INTERVALS * 3) {
-		/* Exclude the max interval */
-		thresh = max - 1;
-		goto again;
 	}
+
+	/*
+	 * If we have outliers to the upside in our distribution, discard
+	 * those by setting the threshold to exclude these outliers, then
+	 * calculate the average and standard deviation again. Once we get
+	 * down to the bottom 3/4 of our samples, stop excluding samples.
+	 *
+	 * This can deal with workloads that have long pauses interspersed
+	 * with sporadic activity with a bunch of short pauses.
+	 */
+	if ((divisor * 4) <= INTERVALS * 3)
+		return;
+
+	thresh = max - 1;
+	goto again;
 }
 
 /**

commit 330647a9501fe8f93a8ae9361417e51ee0bebd7e
Author: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
Date:   Wed Aug 14 19:02:34 2013 +0300

    cpuidle: Ignore interval prediction result when timer is shorter
    
    This patch prevents cpuidle menu governor from using repeating interval
    prediction result if the idle period predicted is longer than the one
    allowed by shortest running timer.
    
    Signed-off-by: Tuukka Tikkanen <tuukka.tikkanen@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index cbbb73b37a6d..351697305fe7 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -238,10 +238,13 @@ static void get_typical_interval(struct menu_device *data)
 	 *
 	 * The typical interval is obtained when standard deviation is small
 	 * or standard deviation is small compared to the average interval.
+	 *
+	 * Use this result only if there is no timer to wake us up sooner.
 	 */
 	if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
 							|| stddev <= 20) {
-		data->predicted_us = avg;
+		if (data->expected_us > avg)
+			data->predicted_us = avg;
 		return;
 
 	} else if ((divisor * 4) > INTERVALS * 3) {

commit ee42f75dba3c66e559a13ac86ad1889d2a396378
Merge: d4e4ab86bcba 9aadfa8fd9f8
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Aug 14 22:18:04 2013 +0200

    Merge back earlier 'pm-cpuidle' material.

commit 148519120c6d1f19ad53349683aeae9f228b0b8d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jul 27 01:41:34 2013 +0200

    Revert "cpuidle: Quickly notice prediction failure for repeat mode"
    
    Revert commit 69a37bea (cpuidle: Quickly notice prediction failure for
    repeat mode), because it has been identified as the source of a
    significant performance regression in v3.8 and later as explained by
    Jeremy Eder:
    
      We believe we've identified a particular commit to the cpuidle code
      that seems to be impacting performance of variety of workloads.
      The simplest way to reproduce is using netperf TCP_RR test, so
      we're using that, on a pair of Sandy Bridge based servers.  We also
      have data from a large database setup where performance is also
      measurably/positively impacted, though that test data isn't easily
      share-able.
    
      Included below are test results from 3 test kernels:
    
      kernel       reverts
      -----------------------------------------------------------
      1) vanilla   upstream (no reverts)
    
      2) perfteam2 reverts e11538d1f03914eb92af5a1a378375c05ae8520c
    
      3) test      reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
                           e11538d1f03914eb92af5a1a378375c05ae8520c
    
      In summary, netperf TCP_RR numbers improve by approximately 4%
      after reverting 69a37beabf1f0a6705c08e879bdd5d82ff6486c4.  When
      69a37beabf1f0a6705c08e879bdd5d82ff6486c4 is included, C0 residency
      never seems to get above 40%.  Taking that patch out gets C0 near
      100% quite often, and performance increases.
    
      The below data are histograms representing the %c0 residency @
      1-second sample rates (using turbostat), while under netperf test.
    
      - If you look at the first 4 histograms, you can see %c0 residency
        almost entirely in the 30,40% bin.
      - The last pair, which reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4,
        shows %c0 in the 80,90,100% bins.
    
      Below each kernel name are netperf TCP_RR trans/s numbers for the
      particular kernel that can be disclosed publicly, comparing the 3
      test kernels.  We ran a 4th test with the vanilla kernel where
      we've also set /dev/cpu_dma_latency=0 to show overall impact
      boosting single-threaded TCP_RR performance over 11% above
      baseline.
    
      3.10-rc2 vanilla RX + c0 lock (/dev/cpu_dma_latency=0):
      TCP_RR trans/s 54323.78
    
      -----------------------------------------------------------
      3.10-rc2 vanilla RX (no reverts)
      TCP_RR trans/s 48192.47
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    59]:
      ***********************************************************
         40.0000 -    50.0000 [     1]: *
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      Sender %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    11]: ***********
         40.0000 -    50.0000 [    49]:
      *************************************************
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      -----------------------------------------------------------
      3.10-rc2 perfteam2 RX (reverts commit
      e11538d1f03914eb92af5a1a378375c05ae8520c)
      TCP_RR trans/s 49698.69
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     1]: *
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    59]:
      ***********************************************************
         40.0000 -    50.0000 [     0]:
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      Sender %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [     2]: **
         40.0000 -    50.0000 [    58]:
      **********************************************************
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [     0]:
    
      -----------------------------------------------------------
      3.10-rc2 test RX (reverts 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
      and e11538d1f03914eb92af5a1a378375c05ae8520c)
      TCP_RR trans/s 47766.95
    
      Receiver %c0
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     1]: *
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    27]: ***************************
         40.0000 -    50.0000 [     2]: **
         50.0000 -    60.0000 [     0]:
         60.0000 -    70.0000 [     2]: **
         70.0000 -    80.0000 [     0]:
         80.0000 -    90.0000 [     0]:
         90.0000 -   100.0000 [    28]: ****************************
    
      Sender:
          0.0000 -    10.0000 [     1]: *
         10.0000 -    20.0000 [     0]:
         20.0000 -    30.0000 [     0]:
         30.0000 -    40.0000 [    11]: ***********
         40.0000 -    50.0000 [     0]:
         50.0000 -    60.0000 [     1]: *
         60.0000 -    70.0000 [     0]:
         70.0000 -    80.0000 [     3]: ***
         80.0000 -    90.0000 [     7]: *******
         90.0000 -   100.0000 [    38]: **************************************
    
      These results demonstrate gaining back the tendency of the CPU to
      stay in more responsive, performant C-states (and thus yield
      measurably better performance), by reverting commit
      69a37beabf1f0a6705c08e879bdd5d82ff6486c4.
    
    Requested-by: Jeremy Eder <jeder@redhat.com>
    Tested-by: Len Brown <len.brown@intel.com>
    Cc: 3.8+ <stable@vger.kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b69a87e22155..bc580b67a652 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -28,13 +28,6 @@
 #define MAX_INTERESTING 50000
 #define STDDEV_THRESH 400
 
-/* 60 * 60 > STDDEV_THRESH * INTERVALS = 400 * 8 */
-#define MAX_DEVIATION 60
-
-static DEFINE_PER_CPU(struct hrtimer, menu_hrtimer);
-static DEFINE_PER_CPU(int, hrtimer_status);
-/* menu hrtimer mode */
-enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT};
 
 /*
  * Concepts and ideas behind the menu governor
@@ -198,42 +191,17 @@ static u64 div_round64(u64 dividend, u32 divisor)
 	return div_u64(dividend + (divisor / 2), divisor);
 }
 
-/* Cancel the hrtimer if it is not triggered yet */
-void menu_hrtimer_cancel(void)
-{
-	int cpu = smp_processor_id();
-	struct hrtimer *hrtmr = &per_cpu(menu_hrtimer, cpu);
-
-	/* The timer is still not time out*/
-	if (per_cpu(hrtimer_status, cpu)) {
-		hrtimer_cancel(hrtmr);
-		per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
-	}
-}
-EXPORT_SYMBOL_GPL(menu_hrtimer_cancel);
-
-/* Call back for hrtimer is triggered */
-static enum hrtimer_restart menu_hrtimer_notify(struct hrtimer *hrtimer)
-{
-	int cpu = smp_processor_id();
-
-	per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
-
-	return HRTIMER_NORESTART;
-}
-
 /*
  * Try detecting repeating patterns by keeping track of the last 8
  * intervals, and checking if the standard deviation of that set
  * of points is below a threshold. If it is... then use the
  * average of these 8 points as the estimated value.
  */
-static u32 get_typical_interval(struct menu_device *data)
+static void get_typical_interval(struct menu_device *data)
 {
 	int i = 0, divisor = 0;
 	uint64_t max = 0, avg = 0, stddev = 0;
 	int64_t thresh = LLONG_MAX; /* Discard outliers above this value. */
-	unsigned int ret = 0;
 
 again:
 
@@ -274,16 +242,13 @@ static u32 get_typical_interval(struct menu_device *data)
 	if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
 							|| stddev <= 20) {
 		data->predicted_us = avg;
-		ret = 1;
-		return ret;
+		return;
 
 	} else if ((divisor * 4) > INTERVALS * 3) {
 		/* Exclude the max interval */
 		thresh = max - 1;
 		goto again;
 	}
-
-	return ret;
 }
 
 /**
@@ -298,9 +263,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int i;
 	int multiplier;
 	struct timespec t;
-	int repeat = 0, low_predicted = 0;
-	int cpu = smp_processor_id();
-	struct hrtimer *hrtmr = &per_cpu(menu_hrtimer, cpu);
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -335,7 +297,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
-	repeat = get_typical_interval(data);
+	get_typical_interval(data);
 
 	/*
 	 * We want to default to C1 (hlt), not to busy polling
@@ -356,10 +318,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 		if (s->disabled || su->disable)
 			continue;
-		if (s->target_residency > data->predicted_us) {
-			low_predicted = 1;
+		if (s->target_residency > data->predicted_us)
 			continue;
-		}
 		if (s->exit_latency > latency_req)
 			continue;
 		if (s->exit_latency * multiplier > data->predicted_us)
@@ -369,28 +329,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		data->exit_us = s->exit_latency;
 	}
 
-	/* not deepest C-state chosen for low predicted residency */
-	if (low_predicted) {
-		unsigned int timer_us = 0;
-
-		/*
-		 * Set a timer to detect whether this sleep is much
-		 * longer than repeat mode predicted.  If the timer
-		 * triggers, the code will evaluate whether to put
-		 * the CPU into a deeper C-state.
-		 * The timer is cancelled on CPU wakeup.
-		 */
-		timer_us = 2 * (data->predicted_us + MAX_DEVIATION);
-
-		if (repeat && (4 * timer_us < data->expected_us)) {
-			RCU_NONIDLE(hrtimer_start(hrtmr,
-				ns_to_ktime(1000 * timer_us),
-				HRTIMER_MODE_REL_PINNED));
-			/* In repeat case, menu hrtimer is started */
-			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_REPEAT;
-		}
-	}
-
 	return data->last_state_idx;
 }
 
@@ -481,9 +419,6 @@ static int menu_enable_device(struct cpuidle_driver *drv,
 				struct cpuidle_device *dev)
 {
 	struct menu_device *data = &per_cpu(menu_devices, dev->cpu);
-	struct hrtimer *t = &per_cpu(menu_hrtimer, dev->cpu);
-	hrtimer_init(t, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-	t->function = menu_hrtimer_notify;
 
 	memset(data, 0, sizeof(struct menu_device));
 

commit 228b30234f258a193317874854eee1ca7807186e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jul 27 01:13:26 2013 +0200

    Revert "cpuidle: Quickly notice prediction failure in general case"
    
    Revert commit e11538d1 (cpuidle: Quickly notice prediction failure in
    general case), since it depends on commit 69a37be (cpuidle: Quickly
    notice prediction failure for repeat mode) that has been identified
    as the source of a significant performance regression in v3.8 and
    later.
    
    Requested-by: Jeremy Eder <jeder@redhat.com>
    Tested-by: Len Brown <len.brown@intel.com>
    Cc: 3.8+ <stable@vger.kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index fe343a06b7da..b69a87e22155 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -34,7 +34,7 @@
 static DEFINE_PER_CPU(struct hrtimer, menu_hrtimer);
 static DEFINE_PER_CPU(int, hrtimer_status);
 /* menu hrtimer mode */
-enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT, MENU_HRTIMER_GENERAL};
+enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT};
 
 /*
  * Concepts and ideas behind the menu governor
@@ -116,13 +116,6 @@ enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT, MENU_HRTIMER_GENERAL};
  *
  */
 
-/*
- * The C-state residency is so long that is is worthwhile to exit
- * from the shallow C-state and re-enter into a deeper C-state.
- */
-static unsigned int perfect_cstate_ms __read_mostly = 30;
-module_param(perfect_cstate_ms, uint, 0000);
-
 struct menu_device {
 	int		last_state_idx;
 	int             needs_update;
@@ -223,16 +216,6 @@ EXPORT_SYMBOL_GPL(menu_hrtimer_cancel);
 static enum hrtimer_restart menu_hrtimer_notify(struct hrtimer *hrtimer)
 {
 	int cpu = smp_processor_id();
-	struct menu_device *data = &per_cpu(menu_devices, cpu);
-
-	/* In general case, the expected residency is much larger than
-	 *  deepest C-state target residency, but prediction logic still
-	 *  predicts a small predicted residency, so the prediction
-	 *  history is totally broken if the timer is triggered.
-	 *  So reset the correction factor.
-	 */
-	if (per_cpu(hrtimer_status, cpu) == MENU_HRTIMER_GENERAL)
-		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
 
 	per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
 
@@ -389,7 +372,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	/* not deepest C-state chosen for low predicted residency */
 	if (low_predicted) {
 		unsigned int timer_us = 0;
-		unsigned int perfect_us = 0;
 
 		/*
 		 * Set a timer to detect whether this sleep is much
@@ -400,28 +382,13 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		 */
 		timer_us = 2 * (data->predicted_us + MAX_DEVIATION);
 
-		perfect_us = perfect_cstate_ms * 1000;
-
 		if (repeat && (4 * timer_us < data->expected_us)) {
 			RCU_NONIDLE(hrtimer_start(hrtmr,
 				ns_to_ktime(1000 * timer_us),
 				HRTIMER_MODE_REL_PINNED));
 			/* In repeat case, menu hrtimer is started */
 			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_REPEAT;
-		} else if (perfect_us < data->expected_us) {
-			/*
-			 * The next timer is long. This could be because
-			 * we did not make a useful prediction.
-			 * In that case, it makes sense to re-enter
-			 * into a deeper C-state after some time.
-			 */
-			RCU_NONIDLE(hrtimer_start(hrtmr,
-				ns_to_ktime(1000 * timer_us),
-				HRTIMER_MODE_REL_PINNED));
-			/* In general case, menu hrtimer is started */
-			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_GENERAL;
 		}
-
 	}
 
 	return data->last_state_idx;

commit 137b944e100278d696826cf25c83014ac17473fe
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Jun 12 15:08:48 2013 +0200

    cpuidle: Make it clear that governors cannot be modules
    
    cpufreq governors are defined as modules in the code, but the Kconfig
    options do not allow them to be built as modules.  This is not really
    a problem, but the cpuidle init ordering is: the cpuidle init
    functions (framework and driver) and then the governors.  That leads
    to some weirdness in the cpuidle framework.
    
    Namely,  cpuidle_register_device() calls cpuidle_enable_device() which
    fails at the first attempt, because governors have not been registered
    yet.  When a governor is registered, the framework calls
    cpuidle_enable_device() again which runs __cpuidle_register_device()
    only then.  Of course, for that to work, the cpuidle_enable_device()
    return value has to be ignored by cpuidle_register_device().
    
    Instead of having this cyclic call graph and relying on a positive
    side effects of the hackish back and forth cpuidle_enable_device()
    calls it is better to fix the cpuidle init ordering.
    
    To that end, replace the module init code with postcore_initcall()
    so we have:
    
     * cpuidle framework : core_initcall
     * cpuidle governors : postcore_initcall
     * cpuidle drivers   : device_initcall
    
    and remove the corresponding module exit code as it is dead anyway
    (governors can't be built as modules).
    
    [rjw: Changelog]
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index fe343a06b7da..743138c309a1 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -540,14 +540,4 @@ static int __init init_menu(void)
 	return cpuidle_register_governor(&menu_governor);
 }
 
-/**
- * exit_menu - exits the governor
- */
-static void __exit exit_menu(void)
-{
-	cpuidle_unregister_governor(&menu_governor);
-}
-
-MODULE_LICENSE("GPL");
-module_init(init_menu);
-module_exit(exit_menu);
+postcore_initcall(init_menu);

commit 8aef33a7cf40ca9da188e8578b2abe7267a38c52
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Tue Jan 15 14:18:04 2013 +0100

    cpuidle: remove the power_specified field in the driver
    
    We realized that the power usage field is never filled and when it
    is filled for tegra, the power_specified flag is not set causing all
    of these values to be reset when the driver is initialized with
    set_power_state().
    
    However, the power_specified flag can be simply removed under the
    assumption that the states are always backward sorted, which is the
    case with the current code.
    
    This change allows the menu governor select function and the
    cpuidle_play_dead() to be simplified.  Moreover, the
    set_power_states() function can removed as it does not make sense
    any more.
    
    Drop the power_specified flag from struct cpuidle_driver and make
    the related changes as described above.
    
    As a consequence, this also fixes the bug where on the dynamic
    C-states system, the power fields are not initialized.
    
    [rjw: Changelog]
    References: https://bugzilla.kernel.org/show_bug.cgi?id=42870
    References: https://bugzilla.kernel.org/show_bug.cgi?id=43349
    References: https://lkml.org/lkml/2012/10/16/518
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 20ea33afdda1..fe343a06b7da 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -312,7 +312,6 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
-	int power_usage = INT_MAX;
 	int i;
 	int multiplier;
 	struct timespec t;
@@ -383,11 +382,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		if (s->exit_latency * multiplier > data->predicted_us)
 			continue;
 
-		if (s->power_usage < power_usage) {
-			power_usage = s->power_usage;
-			data->last_state_idx = i;
-			data->exit_us = s->exit_latency;
-		}
+		data->last_state_idx = i;
+		data->exit_us = s->exit_latency;
 	}
 
 	/* not deepest C-state chosen for low predicted residency */

commit 0e5537b30d3029d784226ab51c2b923d1155b553
Author: Sivaram Nair <sivaramn@nvidia.com>
Date:   Tue Dec 18 13:52:50 2012 +0100

    cpuidle: Fix finding state with min power_usage
    
    Since cpuidle_state.power_usage is a signed value, use INT_MAX (instead
    of -1) to init the local copies so that functions that tries to find
    cpuidle states with minimum power usage works correctly even if they use
    non-negative values.
    
    Signed-off-by: Sivaram Nair <sivaramn@nvidia.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index bd40b943b6db..20ea33afdda1 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -312,7 +312,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
-	int power_usage = -1;
+	int power_usage = INT_MAX;
 	int i;
 	int multiplier;
 	struct timespec t;

commit a093b93ee0e08cd73a07848752bc09ecea68cb13
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Fri Nov 23 00:05:03 2012 +0100

    cpuidle: fix a suspicious RCU usage in menu governor
    
    I saw this suspicious RCU usage on the next tree of 11/15
    
    [   67.123404] ===============================
    [   67.123413] [ INFO: suspicious RCU usage. ]
    [   67.123423] 3.7.0-rc5-next-20121115-dirty #1 Not tainted
    [   67.123434] -------------------------------
    [   67.123444] include/trace/events/timer.h:186 suspicious rcu_dereference_check() usage!
    [   67.123458]
    [   67.123458] other info that might help us debug this:
    [   67.123458]
    [   67.123474]
    [   67.123474] RCU used illegally from idle CPU!
    [   67.123474] rcu_scheduler_active = 1, debug_locks = 0
    [   67.123493] RCU used illegally from extended quiescent state!
    [   67.123507] 1 lock held by swapper/1/0:
    [   67.123516]  #0:  (&cpu_base->lock){-.-...}, at: [<c0000000000979b0>] .__hrtimer_start_range_ns+0x28c/0x524
    [   67.123555]
    [   67.123555] stack backtrace:
    [   67.123566] Call Trace:
    [   67.123576] [c0000001e2ccb920] [c00000000001275c] .show_stack+0x78/0x184 (unreliable)
    [   67.123599] [c0000001e2ccb9d0] [c0000000000c15a0] .lockdep_rcu_suspicious+0x120/0x148
    [   67.123619] [c0000001e2ccba70] [c00000000009601c] .enqueue_hrtimer+0x1c0/0x1c8
    [   67.123639] [c0000001e2ccbb00] [c000000000097aa0] .__hrtimer_start_range_ns+0x37c/0x524
    [   67.123660] [c0000001e2ccbc20] [c0000000005c9698] .menu_select+0x508/0x5bc
    [   67.123678] [c0000001e2ccbd20] [c0000000005c740c] .cpuidle_idle_call+0xa8/0x6e4
    [   67.123699] [c0000001e2ccbdd0] [c0000000000459a0] .pSeries_idle+0x10/0x34
    [   67.123717] [c0000001e2ccbe40] [c000000000014dc8] .cpu_idle+0x130/0x280
    [   67.123738] [c0000001e2ccbee0] [c0000000006ffa8c] .start_secondary+0x378/0x384
    [   67.123758] [c0000001e2ccbf90] [c00000000000936c] .start_secondary_prolog+0x10/0x14
    
    hrtimer_start was added in 198fd638 and ae515197. The patch below tries
    to use RCU_NONIDLE around it to avoid the above report.
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 2efee27714a0..bd40b943b6db 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -407,8 +407,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		perfect_us = perfect_cstate_ms * 1000;
 
 		if (repeat && (4 * timer_us < data->expected_us)) {
-			hrtimer_start(hrtmr, ns_to_ktime(1000 * timer_us),
-				HRTIMER_MODE_REL_PINNED);
+			RCU_NONIDLE(hrtimer_start(hrtmr,
+				ns_to_ktime(1000 * timer_us),
+				HRTIMER_MODE_REL_PINNED));
 			/* In repeat case, menu hrtimer is started */
 			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_REPEAT;
 		} else if (perfect_us < data->expected_us) {
@@ -418,8 +419,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 			 * In that case, it makes sense to re-enter
 			 * into a deeper C-state after some time.
 			 */
-			hrtimer_start(hrtmr, ns_to_ktime(1000 * timer_us),
-				HRTIMER_MODE_REL_PINNED);
+			RCU_NONIDLE(hrtimer_start(hrtmr,
+				ns_to_ktime(1000 * timer_us),
+				HRTIMER_MODE_REL_PINNED));
 			/* In general case, menu hrtimer is started */
 			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_GENERAL;
 		}

commit c96ca4fb76b711279be063da083f09b8d65af5c5
Author: Youquan Song <youquan.song@intel.com>
Date:   Fri Oct 26 12:27:07 2012 +0200

    cpuidle: Get typical recent sleep interval
    
    The function detect_repeating_patterns was not very useful for
    workloads with alternating long and short pauses, for example
    virtual machines handling network requests for each other (say
    a web and database server).
    
    Instead, try to find a recent sleep interval that is somewhere
    between the median and the mode sleep time, by discarding outliers
    to the up side and recalculating the average and standard deviation
    until that is no longer required.
    
    This should do something sane with a sleep interval series like:
    
            200 180 210 10000 30 1000 170 200
    
    The current code would simply discard such a series, while the
    new code will guess a typical sleep interval just shy of 200.
    
    The original patch come from Rik van Riel <riel@redhat.com>.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Youquan Song <youquan.song@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 43a54fd6bfa2..2efee27714a0 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -245,36 +245,59 @@ static enum hrtimer_restart menu_hrtimer_notify(struct hrtimer *hrtimer)
  * of points is below a threshold. If it is... then use the
  * average of these 8 points as the estimated value.
  */
-static int detect_repeating_patterns(struct menu_device *data)
+static u32 get_typical_interval(struct menu_device *data)
 {
-	int i;
-	uint64_t avg = 0;
-	uint64_t stddev = 0; /* contains the square of the std deviation */
-	int ret = 0;
-
-	/* first calculate average and standard deviation of the past */
-	for (i = 0; i < INTERVALS; i++)
-		avg += data->intervals[i];
-	avg = avg / INTERVALS;
+	int i = 0, divisor = 0;
+	uint64_t max = 0, avg = 0, stddev = 0;
+	int64_t thresh = LLONG_MAX; /* Discard outliers above this value. */
+	unsigned int ret = 0;
 
-	/* if the avg is beyond the known next tick, it's worthless */
-	if (avg > data->expected_us)
-		return 0;
-
-	for (i = 0; i < INTERVALS; i++)
-		stddev += (data->intervals[i] - avg) *
-			  (data->intervals[i] - avg);
+again:
 
-	stddev = stddev / INTERVALS;
+	/* first calculate average and standard deviation of the past */
+	max = avg = divisor = stddev = 0;
+	for (i = 0; i < INTERVALS; i++) {
+		int64_t value = data->intervals[i];
+		if (value <= thresh) {
+			avg += value;
+			divisor++;
+			if (value > max)
+				max = value;
+		}
+	}
+	do_div(avg, divisor);
 
+	for (i = 0; i < INTERVALS; i++) {
+		int64_t value = data->intervals[i];
+		if (value <= thresh) {
+			int64_t diff = value - avg;
+			stddev += diff * diff;
+		}
+	}
+	do_div(stddev, divisor);
+	stddev = int_sqrt(stddev);
 	/*
-	 * now.. if stddev is small.. then assume we have a
-	 * repeating pattern and predict we keep doing this.
+	 * If we have outliers to the upside in our distribution, discard
+	 * those by setting the threshold to exclude these outliers, then
+	 * calculate the average and standard deviation again. Once we get
+	 * down to the bottom 3/4 of our samples, stop excluding samples.
+	 *
+	 * This can deal with workloads that have long pauses interspersed
+	 * with sporadic activity with a bunch of short pauses.
+	 *
+	 * The typical interval is obtained when standard deviation is small
+	 * or standard deviation is small compared to the average interval.
 	 */
-
-	if (avg && stddev < STDDEV_THRESH) {
+	if (((avg > stddev * 6) && (divisor * 4 >= INTERVALS * 3))
+							|| stddev <= 20) {
 		data->predicted_us = avg;
 		ret = 1;
+		return ret;
+
+	} else if ((divisor * 4) > INTERVALS * 3) {
+		/* Exclude the max interval */
+		thresh = max - 1;
+		goto again;
 	}
 
 	return ret;
@@ -330,7 +353,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
-	repeat = detect_repeating_patterns(data);
+	repeat = get_typical_interval(data);
 
 	/*
 	 * We want to default to C1 (hlt), not to busy polling

commit e11538d1f03914eb92af5a1a378375c05ae8520c
Author: Youquan Song <youquan.song@intel.com>
Date:   Fri Oct 26 12:26:50 2012 +0200

    cpuidle: Quickly notice prediction failure in general case
    
    The prediction for future is difficult and when the cpuidle governor prediction
    fails and govenor possibly choose the shallower C-state than it should. How to
    quickly notice and find the failure becomes important for power saving.
    
    The patch extends to general case that prediction logic get a small predicted
    residency, so it choose a shallow C-state though the expected residency is large
    . Once the prediction will be fail, the CPU will keep staying at shallow C-state
    for a long time. Acutally, the CPU has change enter into deep C-state.
    So when the expected residency is long enough but governor choose a shallow
    C-state, an timer will be added in order to monitor if the prediction failure.
    
    When C-state is waken up prior to the adding timer, the timer will be cancelled
    initiatively. When the timer is triggered and menu governor will quickly notice
    prediction failure and re-evaluates deeper C-states possibility.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Youquan Song <youquan.song@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 37c0ff6c805c..43a54fd6bfa2 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -34,7 +34,7 @@
 static DEFINE_PER_CPU(struct hrtimer, menu_hrtimer);
 static DEFINE_PER_CPU(int, hrtimer_status);
 /* menu hrtimer mode */
-enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT};
+enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT, MENU_HRTIMER_GENERAL};
 
 /*
  * Concepts and ideas behind the menu governor
@@ -116,6 +116,13 @@ enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT};
  *
  */
 
+/*
+ * The C-state residency is so long that is is worthwhile to exit
+ * from the shallow C-state and re-enter into a deeper C-state.
+ */
+static unsigned int perfect_cstate_ms __read_mostly = 30;
+module_param(perfect_cstate_ms, uint, 0000);
+
 struct menu_device {
 	int		last_state_idx;
 	int             needs_update;
@@ -216,6 +223,16 @@ EXPORT_SYMBOL_GPL(menu_hrtimer_cancel);
 static enum hrtimer_restart menu_hrtimer_notify(struct hrtimer *hrtimer)
 {
 	int cpu = smp_processor_id();
+	struct menu_device *data = &per_cpu(menu_devices, cpu);
+
+	/* In general case, the expected residency is much larger than
+	 *  deepest C-state target residency, but prediction logic still
+	 *  predicts a small predicted residency, so the prediction
+	 *  history is totally broken if the timer is triggered.
+	 *  So reset the correction factor.
+	 */
+	if (per_cpu(hrtimer_status, cpu) == MENU_HRTIMER_GENERAL)
+		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
 
 	per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
 
@@ -353,6 +370,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	/* not deepest C-state chosen for low predicted residency */
 	if (low_predicted) {
 		unsigned int timer_us = 0;
+		unsigned int perfect_us = 0;
 
 		/*
 		 * Set a timer to detect whether this sleep is much
@@ -363,12 +381,26 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		 */
 		timer_us = 2 * (data->predicted_us + MAX_DEVIATION);
 
+		perfect_us = perfect_cstate_ms * 1000;
+
 		if (repeat && (4 * timer_us < data->expected_us)) {
 			hrtimer_start(hrtmr, ns_to_ktime(1000 * timer_us),
 				HRTIMER_MODE_REL_PINNED);
 			/* In repeat case, menu hrtimer is started */
 			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_REPEAT;
+		} else if (perfect_us < data->expected_us) {
+			/*
+			 * The next timer is long. This could be because
+			 * we did not make a useful prediction.
+			 * In that case, it makes sense to re-enter
+			 * into a deeper C-state after some time.
+			 */
+			hrtimer_start(hrtmr, ns_to_ktime(1000 * timer_us),
+				HRTIMER_MODE_REL_PINNED);
+			/* In general case, menu hrtimer is started */
+			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_GENERAL;
 		}
+
 	}
 
 	return data->last_state_idx;

commit 69a37beabf1f0a6705c08e879bdd5d82ff6486c4
Author: Youquan Song <youquan.song@intel.com>
Date:   Fri Oct 26 12:26:41 2012 +0200

    cpuidle: Quickly notice prediction failure for repeat mode
    
    The prediction for future is difficult and when the cpuidle governor prediction
    fails and govenor possibly choose the shallower C-state than it should. How to
    quickly notice and find the failure becomes important for power saving.
    
    cpuidle menu governor has a method to predict the repeat pattern if there are 8
    C-states residency which are continuous and the same or very close, so it will
    predict the next C-states residency will keep same residency time.
    
    There is a real case that turbostat utility (tools/power/x86/turbostat)
    at kernel 3.3 or early. turbostat utility will read 10 registers one by one at
    Sandybridge, so it will generate 10 IPIs to wake up idle CPUs. So cpuidle menu
     governor will predict it is repeat mode and there is another IPI wake up idle
     CPU soon, so it keeps idle CPU stay at C1 state even though CPU is totally
    idle. However, in the turbostat, following 10 registers reading is sleep 5
    seconds by default, so the idle CPU will keep at C1 for a long time though it is
     idle until break event occurs.
    In a idle Sandybridge system, run "./turbostat -v", we will notice that deep
    C-state dangles between "70% ~ 99%". After patched the kernel, we will notice
    deep C-state stays at >99.98%.
    
    In the patch, a timer is added when menu governor detects a repeat mode and
    choose a shallow C-state. The timer is set to a time out value that greater
    than predicted time, and we conclude repeat mode prediction failure if timer is
    triggered. When repeat mode happens as expected, the timer is not triggered
    and CPU waken up from C-states and it will cancel the timer initiatively.
    When repeat mode does not happen, the timer will be time out and menu governor
    will quickly notice that the repeat mode prediction fails and then re-evaluates
    deeper C-states possibility.
    
    Below is another case which will clearly show the patch much benefit:
    
    #include <stdlib.h>
    #include <stdio.h>
    #include <unistd.h>
    #include <signal.h>
    #include <sys/time.h>
    #include <time.h>
    #include <pthread.h>
    
    volatile int * shutdown;
    volatile long * count;
    int delay = 20;
    int loop = 8;
    
    void usage(void)
    {
            fprintf(stderr,
                    "Usage: idle_predict [options]\n"
                    "  --help       -h  Print this help\n"
                    "  --thread     -n  Thread number\n"
                    "  --loop       -l  Loop times in shallow Cstate\n"
                    "  --delay      -t  Sleep time (uS)in shallow Cstate\n");
    }
    
    void *simple_loop() {
            int idle_num = 1;
            while (!(*shutdown)) {
                    *count = *count + 1;
    
                    if (idle_num % loop)
                            usleep(delay);
                    else {
                            /* sleep 1 second */
                            usleep(1000000);
                            idle_num = 0;
                    }
                    idle_num++;
            }
    
    }
    
    static void sighand(int sig)
    {
            *shutdown = 1;
    }
    
    int main(int argc, char *argv[])
    {
            sigset_t sigset;
            int signum = SIGALRM;
            int i, c, er = 0, thread_num = 8;
            pthread_t pt[1024];
    
            static char optstr[] = "n:l:t:h:";
    
            while ((c = getopt(argc, argv, optstr)) != EOF)
                    switch (c) {
                            case 'n':
                                    thread_num = atoi(optarg);
                                    break;
                            case 'l':
                                    loop = atoi(optarg);
                                    break;
                            case 't':
                                    delay = atoi(optarg);
                                    break;
                            case 'h':
                            default:
                                    usage();
                                    exit(1);
                    }
    
            printf("thread=%d,loop=%d,delay=%d\n",thread_num,loop,delay);
            count = malloc(sizeof(long));
            shutdown = malloc(sizeof(int));
            *count = 0;
            *shutdown = 0;
    
            sigemptyset(&sigset);
            sigaddset(&sigset, signum);
            sigprocmask (SIG_BLOCK, &sigset, NULL);
            signal(SIGINT, sighand);
            signal(SIGTERM, sighand);
    
            for(i = 0; i < thread_num ; i++)
                    pthread_create(&pt[i], NULL, simple_loop, NULL);
    
            for (i = 0; i < thread_num; i++)
                    pthread_join(pt[i], NULL);
    
            exit(0);
    }
    
    Get powertop V2 from git://github.com/fenrus75/powertop, build powertop.
    After build the above test application, then run it.
    Test plaform can be Intel Sandybridge or other recent platforms.
    #./idle_predict -l 10 &
    #./powertop
    
    We will find that deep C-state will dangle between 40%~100% and much time spent
    on C1 state. It is because menu governor wrongly predict that repeat mode
    is kept, so it will choose the C1 shallow C-state even though it has chance to
    sleep 1 second in deep C-state.
    
    While after patched the kernel, we find that deep C-state will keep >99.6%.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Youquan Song <youquan.song@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5b1f2c372c1f..37c0ff6c805c 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -28,6 +28,13 @@
 #define MAX_INTERESTING 50000
 #define STDDEV_THRESH 400
 
+/* 60 * 60 > STDDEV_THRESH * INTERVALS = 400 * 8 */
+#define MAX_DEVIATION 60
+
+static DEFINE_PER_CPU(struct hrtimer, menu_hrtimer);
+static DEFINE_PER_CPU(int, hrtimer_status);
+/* menu hrtimer mode */
+enum {MENU_HRTIMER_STOP, MENU_HRTIMER_REPEAT};
 
 /*
  * Concepts and ideas behind the menu governor
@@ -191,17 +198,42 @@ static u64 div_round64(u64 dividend, u32 divisor)
 	return div_u64(dividend + (divisor / 2), divisor);
 }
 
+/* Cancel the hrtimer if it is not triggered yet */
+void menu_hrtimer_cancel(void)
+{
+	int cpu = smp_processor_id();
+	struct hrtimer *hrtmr = &per_cpu(menu_hrtimer, cpu);
+
+	/* The timer is still not time out*/
+	if (per_cpu(hrtimer_status, cpu)) {
+		hrtimer_cancel(hrtmr);
+		per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
+	}
+}
+EXPORT_SYMBOL_GPL(menu_hrtimer_cancel);
+
+/* Call back for hrtimer is triggered */
+static enum hrtimer_restart menu_hrtimer_notify(struct hrtimer *hrtimer)
+{
+	int cpu = smp_processor_id();
+
+	per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_STOP;
+
+	return HRTIMER_NORESTART;
+}
+
 /*
  * Try detecting repeating patterns by keeping track of the last 8
  * intervals, and checking if the standard deviation of that set
  * of points is below a threshold. If it is... then use the
  * average of these 8 points as the estimated value.
  */
-static void detect_repeating_patterns(struct menu_device *data)
+static int detect_repeating_patterns(struct menu_device *data)
 {
 	int i;
 	uint64_t avg = 0;
 	uint64_t stddev = 0; /* contains the square of the std deviation */
+	int ret = 0;
 
 	/* first calculate average and standard deviation of the past */
 	for (i = 0; i < INTERVALS; i++)
@@ -210,7 +242,7 @@ static void detect_repeating_patterns(struct menu_device *data)
 
 	/* if the avg is beyond the known next tick, it's worthless */
 	if (avg > data->expected_us)
-		return;
+		return 0;
 
 	for (i = 0; i < INTERVALS; i++)
 		stddev += (data->intervals[i] - avg) *
@@ -223,8 +255,12 @@ static void detect_repeating_patterns(struct menu_device *data)
 	 * repeating pattern and predict we keep doing this.
 	 */
 
-	if (avg && stddev < STDDEV_THRESH)
+	if (avg && stddev < STDDEV_THRESH) {
 		data->predicted_us = avg;
+		ret = 1;
+	}
+
+	return ret;
 }
 
 /**
@@ -240,6 +276,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	int i;
 	int multiplier;
 	struct timespec t;
+	int repeat = 0, low_predicted = 0;
+	int cpu = smp_processor_id();
+	struct hrtimer *hrtmr = &per_cpu(menu_hrtimer, cpu);
 
 	if (data->needs_update) {
 		menu_update(drv, dev);
@@ -274,7 +313,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
-	detect_repeating_patterns(data);
+	repeat = detect_repeating_patterns(data);
 
 	/*
 	 * We want to default to C1 (hlt), not to busy polling
@@ -295,8 +334,10 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 
 		if (s->disabled || su->disable)
 			continue;
-		if (s->target_residency > data->predicted_us)
+		if (s->target_residency > data->predicted_us) {
+			low_predicted = 1;
 			continue;
+		}
 		if (s->exit_latency > latency_req)
 			continue;
 		if (s->exit_latency * multiplier > data->predicted_us)
@@ -309,6 +350,27 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		}
 	}
 
+	/* not deepest C-state chosen for low predicted residency */
+	if (low_predicted) {
+		unsigned int timer_us = 0;
+
+		/*
+		 * Set a timer to detect whether this sleep is much
+		 * longer than repeat mode predicted.  If the timer
+		 * triggers, the code will evaluate whether to put
+		 * the CPU into a deeper C-state.
+		 * The timer is cancelled on CPU wakeup.
+		 */
+		timer_us = 2 * (data->predicted_us + MAX_DEVIATION);
+
+		if (repeat && (4 * timer_us < data->expected_us)) {
+			hrtimer_start(hrtmr, ns_to_ktime(1000 * timer_us),
+				HRTIMER_MODE_REL_PINNED);
+			/* In repeat case, menu hrtimer is started */
+			per_cpu(hrtimer_status, cpu) = MENU_HRTIMER_REPEAT;
+		}
+	}
+
 	return data->last_state_idx;
 }
 
@@ -399,6 +461,9 @@ static int menu_enable_device(struct cpuidle_driver *drv,
 				struct cpuidle_device *dev)
 {
 	struct menu_device *data = &per_cpu(menu_devices, dev->cpu);
+	struct hrtimer *t = &per_cpu(menu_hrtimer, dev->cpu);
+	hrtimer_init(t, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	t->function = menu_hrtimer_notify;
 
 	memset(data, 0, sizeof(struct menu_device));
 

commit cbc9ef0287ab764d3da0129efa673808df641fe3
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Jul 3 19:07:42 2012 +0200

    PM / Domains: Add preliminary support for cpuidle, v2
    
    On some systems there are CPU cores located in the same power
    domains as I/O devices.  Then, power can only be removed from the
    domain if all I/O devices in it are not in use and the CPU core
    is idle.  Add preliminary support for that to the generic PM domains
    framework.
    
    First, the platform is expected to provide a cpuidle driver with one
    extra state designated for use with the generic PM domains code.
    This state should be initially disabled and its exit_latency value
    should be set to whatever time is needed to bring up the CPU core
    itself after restoring power to it, not including the domain's
    power on latency.  Its .enter() callback should point to a procedure
    that will remove power from the domain containing the CPU core at
    the end of the CPU power transition.
    
    The remaining characteristics of the extra cpuidle state, referred to
    as the "domain" cpuidle state below, (e.g. power usage, target
    residency) should be populated in accordance with the properties of
    the hardware.
    
    Next, the platform should execute genpd_attach_cpuidle() on the PM
    domain containing the CPU core.  That will cause the generic PM
    domains framework to treat that domain in a special way such that:
    
     * When all devices in the domain have been suspended and it is about
       to be turned off, the states of the devices will be saved, but
       power will not be removed from the domain.  Instead, the "domain"
       cpuidle state will be enabled so that power can be removed from
       the domain when the CPU core is idle and the state has been chosen
       as the target by the cpuidle governor.
    
     * When the first I/O device in the domain is resumed and
       __pm_genpd_poweron(() is called for the first time after
       power has been removed from the domain, the "domain" cpuidle
       state will be disabled to avoid subsequent surprise power removals
       via cpuidle.
    
    The effective exit_latency value of the "domain" cpuidle state
    depends on the time needed to bring up the CPU core itself after
    restoring power to it as well as on the power on latency of the
    domain containing the CPU core.  Thus the "domain" cpuidle state's
    exit_latency has to be recomputed every time the domain's power on
    latency is updated, which may happen every time power is restored
    to the domain, if the measured power on latency is greater than
    the latency stored in the corresponding generic_pm_domain structure.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Reviewed-by: Kevin Hilman <khilman@ti.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 8391d93f57d5..5b1f2c372c1f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -281,6 +281,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * unless the timer is happening really really soon.
 	 */
 	if (data->expected_us > 5 &&
+	    !drv->states[CPUIDLE_DRIVER_STATE_START].disabled &&
 		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 
@@ -292,7 +293,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 		struct cpuidle_state *s = &drv->states[i];
 		struct cpuidle_state_usage *su = &dev->states_usage[i];
 
-		if (su->disable)
+		if (s->disabled || su->disable)
 			continue;
 		if (s->target_residency > data->predicted_us)
 			continue;

commit dc7fd275ae60ef8edf952aff2a62462f5d892fd4
Author: ShuoX Liu <shuox.liu@intel.com>
Date:   Tue Jul 3 19:05:31 2012 +0200

    cpuidle: move field disable from per-driver to per-cpu
    
    Andrew J.Schorr raises a question.  When he changes the disable setting on
    a single CPU, it affects all the other CPUs.  Basically, currently, the
    disable field is per-driver instead of per-cpu.  All the C states of the
    same driver are shared by all CPU in the same machine.
    
    The patch changes the `disable' field to per-cpu, so we could set this
    separately for each cpu.
    
    Signed-off-by: ShuoX Liu <shuox.liu@intel.com>
    Reported-by: Andrew J.Schorr <aschorr@telemetry-investments.com>
    Reviewed-by: Yanmin Zhang <yanmin_zhang@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 06335756ea14..8391d93f57d5 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -281,7 +281,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * unless the timer is happening really really soon.
 	 */
 	if (data->expected_us > 5 &&
-		drv->states[CPUIDLE_DRIVER_STATE_START].disable == 0)
+		dev->states_usage[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 
 	/*
@@ -290,8 +290,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 */
 	for (i = CPUIDLE_DRIVER_STATE_START; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
+		struct cpuidle_state_usage *su = &dev->states_usage[i];
 
-		if (s->disable)
+		if (su->disable)
 			continue;
 		if (s->target_residency > data->predicted_us)
 			continue;

commit 02401c06b7f6bec65f314e3cec7894502c973501
Author: Boris Ostrovsky <boris.ostrovsky@amd.com>
Date:   Tue Mar 13 19:55:10 2012 +0100

    cpuidle: power_usage should be declared signed integer
    
    power_usage is always assigned a negative value and should be declared
    a signed integer
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@amd.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 5c17ca112fc2..06335756ea14 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -236,7 +236,7 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
-	unsigned int power_usage = -1;
+	int power_usage = -1;
 	int i;
 	int multiplier;
 	struct timespec t;

commit 3a53396b0381ec9d5180fd8fe7a681c8ce95fd9a
Author: ShuoX Liu <shuox.liu@intel.com>
Date:   Wed Mar 28 15:19:11 2012 -0700

    cpuidle: add a sysfs entry to disable specific C state for debug purpose.
    
    Some C states of new CPU might be not good.  One reason is BIOS might
    configure them incorrectly.  To help developers root cause it quickly, the
    patch adds a new sysfs entry, so developers could disable specific C state
    manually.
    
    In addition, C state might have much impact on performance tuning, as it
    takes much time to enter/exit C states, which might delay interrupt
    processing.  With the new debug option, developers could check if a deep C
    state could impact performance and how much impact it could cause.
    
    Also add this option in Documentation/cpuidle/sysfs.txt.
    
    [akpm@linux-foundation.org: check kstrtol return value]
    Signed-off-by: ShuoX Liu <shuox.liu@intel.com>
    Reviewed-by: Yanmin Zhang <yanmin_zhang@intel.com>
    Reviewed-and-Tested-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index ad0952601ae2..5c17ca112fc2 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -280,7 +280,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
 	 */
-	if (data->expected_us > 5)
+	if (data->expected_us > 5 &&
+		drv->states[CPUIDLE_DRIVER_STATE_START].disable == 0)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 
 	/*
@@ -290,6 +291,8 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 	for (i = CPUIDLE_DRIVER_STATE_START; i < drv->state_count; i++) {
 		struct cpuidle_state *s = &drv->states[i];
 
+		if (s->disable)
+			continue;
 		if (s->target_residency > data->predicted_us)
 			continue;
 		if (s->exit_latency > latency_req)

commit 3c00303206c3a1ccd86579efdc90bc35f140962e
Merge: 83dbb15e9cd7 efb90582c575
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 7 10:13:52 2011 -0800

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux:
      cpuidle: Single/Global registration of idle states
      cpuidle: Split cpuidle_state structure and move per-cpu statistics fields
      cpuidle: Remove CPUIDLE_FLAG_IGNORE and dev->prepare()
      cpuidle: Move dev->last_residency update to driver enter routine; remove dev->last_state
      ACPI: Fix CONFIG_ACPI_DOCK=n compiler warning
      ACPI: Export FADT pm_profile integer value to userspace
      thermal: Prevent polling from happening during system suspend
      ACPI: Drop ACPI_NO_HARDWARE_INIT
      ACPI atomicio: Convert width in bits to bytes in __acpi_ioremap_fast()
      PNPACPI: Simplify disabled resource registration
      ACPI: Fix possible recursive locking in hwregs.c
      ACPI: use kstrdup()
      mrst pmu: update comment
      tools/power turbostat: less verbose debugging

commit 46bcfad7a819bd17ac4e831b04405152d59784ab
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Fri Oct 28 16:20:42 2011 +0530

    cpuidle: Single/Global registration of idle states
    
    This patch makes the cpuidle_states structure global (single copy)
    instead of per-cpu. The statistics needed on per-cpu basis
    by the governor are kept per-cpu. This simplifies the cpuidle
    subsystem as state registration is done by single cpu only.
    Having single copy of cpuidle_states saves memory. Rare case
    of asymmetric C-states can be handled within the cpuidle driver
    and architectures such as POWER do not have asymmetric C-states.
    
    Having single/global registration of all the idle states,
    dynamic C-state transitions on x86 are handled by
    the boot cpu. Here, the boot cpu  would disable all the devices,
    re-populate the states and later enable all the devices,
    irrespective of the cpu that would receive the notification first.
    
    Reference:
    https://lkml.org/lkml/2011/4/25/83
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Trinabh Gupta <g.trinabh@gmail.com>
    Tested-by: Jean Pihet <j-pihet@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index af724e823c8e..bcbe88142135 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -182,7 +182,7 @@ static inline int performance_multiplier(void)
 
 static DEFINE_PER_CPU(struct menu_device, menu_devices);
 
-static void menu_update(struct cpuidle_device *dev);
+static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev);
 
 /* This implements DIV_ROUND_CLOSEST but avoids 64 bit division */
 static u64 div_round64(u64 dividend, u32 divisor)
@@ -228,9 +228,10 @@ static void detect_repeating_patterns(struct menu_device *data)
 
 /**
  * menu_select - selects the next idle state to enter
+ * @drv: cpuidle driver containing state data
  * @dev: the CPU
  */
-static int menu_select(struct cpuidle_device *dev)
+static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
@@ -240,7 +241,7 @@ static int menu_select(struct cpuidle_device *dev)
 	struct timespec t;
 
 	if (data->needs_update) {
-		menu_update(dev);
+		menu_update(drv, dev);
 		data->needs_update = 0;
 	}
 
@@ -285,8 +286,8 @@ static int menu_select(struct cpuidle_device *dev)
 	 * Find the idle state with the lowest power while satisfying
 	 * our constraints.
 	 */
-	for (i = CPUIDLE_DRIVER_STATE_START; i < dev->state_count; i++) {
-		struct cpuidle_state *s = &dev->states[i];
+	for (i = CPUIDLE_DRIVER_STATE_START; i < drv->state_count; i++) {
+		struct cpuidle_state *s = &drv->states[i];
 
 		if (s->target_residency > data->predicted_us)
 			continue;
@@ -323,14 +324,15 @@ static void menu_reflect(struct cpuidle_device *dev, int index)
 
 /**
  * menu_update - attempts to guess what happened after entry
+ * @drv: cpuidle driver containing state data
  * @dev: the CPU
  */
-static void menu_update(struct cpuidle_device *dev)
+static void menu_update(struct cpuidle_driver *drv, struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int last_idx = data->last_state_idx;
 	unsigned int last_idle_us = cpuidle_get_last_residency(dev);
-	struct cpuidle_state *target = &dev->states[last_idx];
+	struct cpuidle_state *target = &drv->states[last_idx];
 	unsigned int measured_us;
 	u64 new_factor;
 
@@ -384,9 +386,11 @@ static void menu_update(struct cpuidle_device *dev)
 
 /**
  * menu_enable_device - scans a CPU's states and does setup
+ * @drv: cpuidle driver
  * @dev: the CPU
  */
-static int menu_enable_device(struct cpuidle_device *dev)
+static int menu_enable_device(struct cpuidle_driver *drv,
+				struct cpuidle_device *dev)
 {
 	struct menu_device *data = &per_cpu(menu_devices, dev->cpu);
 

commit b25edc42bfb9602f0503474b2c94701d5536ce60
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Fri Oct 28 16:20:24 2011 +0530

    cpuidle: Remove CPUIDLE_FLAG_IGNORE and dev->prepare()
    
    The cpuidle_device->prepare() mechanism causes updates to the
    cpuidle_state[].flags, setting and clearing CPUIDLE_FLAG_IGNORE
    to tell the governor not to chose a state on a per-cpu basis at
    run-time. State demotion is now handled by the driver and it returns
    the actual state entered. Hence, this mechanism is not required.
    Also this removes per-cpu flags from cpuidle_state enabling
    it to be made global.
    
    Reference:
    https://lkml.org/lkml/2011/3/25/52
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm>
    Signed-off-by: Trinabh Gupta <g.trinabh@gmail.com>
    Tested-by: Jean Pihet <j-pihet@ti.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index e4b200c5b441..af724e823c8e 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -288,8 +288,6 @@ static int menu_select(struct cpuidle_device *dev)
 	for (i = CPUIDLE_DRIVER_STATE_START; i < dev->state_count; i++) {
 		struct cpuidle_state *s = &dev->states[i];
 
-		if (s->flags & CPUIDLE_FLAG_IGNORE)
-			continue;
 		if (s->target_residency > data->predicted_us)
 			continue;
 		if (s->exit_latency > latency_req)

commit e978aa7d7d57d04eb5f88a7507c4fb98577def77
Author: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
Date:   Fri Oct 28 16:20:09 2011 +0530

    cpuidle: Move dev->last_residency update to driver enter routine; remove dev->last_state
    
    Cpuidle governor only suggests the state to enter using the
    governor->select() interface, but allows the low level driver to
    override the recommended state. The actual entered state
    may be different because of software or hardware demotion. Software
    demotion is done by the back-end cpuidle driver and can be accounted
    correctly. Current cpuidle code uses last_state field to capture the
    actual state entered and based on that updates the statistics for the
    state entered.
    
    Ideally the driver enter routine should update the counters,
    and it should return the state actually entered rather than the time
    spent there. The generic cpuidle code should simply handle where
    the counters live in the sysfs namespace, not updating the counters.
    
    Reference:
    https://lkml.org/lkml/2011/3/25/52
    
    Signed-off-by: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Signed-off-by: Trinabh Gupta <g.trinabh@gmail.com>
    Tested-by: Jean Pihet <j-pihet@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c47f3d09c1ee..e4b200c5b441 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -310,14 +310,17 @@ static int menu_select(struct cpuidle_device *dev)
 /**
  * menu_reflect - records that data structures need update
  * @dev: the CPU
+ * @index: the index of actual entered state
  *
  * NOTE: it's important to be fast here because this operation will add to
  *       the overall exit latency.
  */
-static void menu_reflect(struct cpuidle_device *dev)
+static void menu_reflect(struct cpuidle_device *dev, int index)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
-	data->needs_update = 1;
+	data->last_state_idx = index;
+	if (index >= 0)
+		data->needs_update = 1;
 }
 
 /**

commit 884b17e109d61e95ee4c652cf6873341bf1dca63
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Aug 29 17:52:39 2011 -0400

    cpuidle: Add module.h to drivers/cpuidle files as required.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 3600f1955e48..00275244ce2f 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -19,6 +19,7 @@
 #include <linux/tick.h>
 #include <linux/sched.h>
 #include <linux/math64.h>
+#include <linux/module.h>
 
 #define BUCKETS 12
 #define INTERVALS 8

commit e8db0be1245de16a6cc6365506abc392c3c212d4
Author: Jean Pihet <j-pihet@ti.com>
Date:   Thu Aug 25 15:35:03 2011 +0200

    PM QoS: Move and rename the implementation files
    
    The PM QoS implementation files are better named
    kernel/power/qos.c and include/linux/pm_qos.h.
    
    The PM QoS support is compiled under the CONFIG_PM option.
    
    Signed-off-by: Jean Pihet <j-pihet@ti.com>
    Acked-by: markgross <markgross@thegnar.org>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c47f3d09c1ee..3600f1955e48 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -12,7 +12,7 @@
 
 #include <linux/kernel.h>
 #include <linux/cpuidle.h>
-#include <linux/pm_qos_params.h>
+#include <linux/pm_qos.h>
 #include <linux/time.h>
 #include <linux/ktime.h>
 #include <linux/hrtimer.h>

commit 7467571f4480b273007517b26297c07154c73924
Author: Tero Kristo <tero.kristo@nokia.com>
Date:   Thu Feb 24 17:19:23 2011 +0200

    cpuidle: menu: fixed wrapping timers at 4.294 seconds
    
    Cpuidle menu governor is using u32 as a temporary datatype for storing
    nanosecond values which wrap around at 4.294 seconds. This causes errors
    in predicted sleep times resulting in higher than should be C state
    selection and increased power consumption. This also breaks cpuidle
    state residency statistics.
    
    cc: stable@kernel.org # .32.x through .39.x
    Signed-off-by: Tero Kristo <tero.kristo@nokia.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f508690eb958..c47f3d09c1ee 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -237,6 +237,7 @@ static int menu_select(struct cpuidle_device *dev)
 	unsigned int power_usage = -1;
 	int i;
 	int multiplier;
+	struct timespec t;
 
 	if (data->needs_update) {
 		menu_update(dev);
@@ -251,8 +252,9 @@ static int menu_select(struct cpuidle_device *dev)
 		return 0;
 
 	/* determine the expected residency time, round up */
+	t = ktime_to_timespec(tick_nohz_get_sleep_length());
 	data->expected_us =
-	    DIV_ROUND_UP((u32)ktime_to_ns(tick_nohz_get_sleep_length()), 1000);
+		t.tv_sec * USEC_PER_SEC + t.tv_nsec / NSEC_PER_USEC;
 
 
 	data->bucket = which_bucket(data->expected_us);

commit 20e3341bb138bc9860adea4d76707470357b76ab
Author: Lucas De Marchi <lucas.de.marchi@gmail.com>
Date:   Tue Sep 7 12:53:49 2010 -0400

    cpuidle: Fix typos
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index c2408bbe9c2e..f508690eb958 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -80,7 +80,7 @@
  * Limiting Performance Impact
  * ---------------------------
  * C states, especially those with large exit latencies, can have a real
- * noticable impact on workloads, which is not acceptable for most sysadmins,
+ * noticeable impact on workloads, which is not acceptable for most sysadmins,
  * and in addition, less performance has a power price of its own.
  *
  * As a general rule of thumb, menu assumes that the following heuristic

commit 71abbbf856a0e70ca478782505c800891260ba84
Author: Ai Li <aili@codeaurora.org>
Date:   Mon Aug 9 17:20:13 2010 -0700

    cpuidle: extend cpuidle and menu governor to handle dynamic states
    
    On some SoC chips, HW resources may be in use during any particular idle
    period.  As a consequence, the cpuidle states that the SoC is safe to
    enter can change from idle period to idle period.  In addition, the
    latency and threshold of each cpuidle state can vary, depending on the
    operating condition when the CPU becomes idle, e.g.  the current cpu
    frequency, the current state of the HW blocks, etc.
    
    cpuidle core and the menu governor, in the current form, are geared
    towards cpuidle states that are static, i.e.  the availabiltiy of the
    states, their latencies, their thresholds are non-changing during run
    time.  cpuidle does not provide any hook that cpuidle drivers can use to
    adjust those values on the fly for the current idle period before the menu
    governor selects the target cpuidle state.
    
    This patch extends cpuidle core and the menu governor to handle states
    that are dynamic.  There are three additions in the patch and the patch
    maintains backwards-compatibility with existing cpuidle drivers.
    
    1) add prepare() to struct cpuidle_device.  A cpuidle driver can hook
       into the callback and cpuidle will call prepare() before calling the
       governor's select function.  The callback gives the cpuidle driver a
       chance to update the dynamic information of the cpuidle states for the
       current idle period, e.g.  state availability, latencies, thresholds,
       power values, etc.
    
    2) add CPUIDLE_FLAG_IGNORE as one of the state flags.  In the prepare()
       function, a cpuidle driver can set/clear the flag to indicate to the
       menu governor whether a cpuidle state should be ignored, i.e.  not
       available, during the current idle period.
    
    3) add power_specified bit to struct cpuidle_device.  The menu governor
       currently assumes that the cpuidle states are arranged in the order of
       increasing latency, threshold, and power savings.  This is true or can
       be made true for static states.  Once the state parameters are dynamic,
       the latencies, thresholds, and power savings for the cpuidle states can
       increase or decrease by different amounts from idle period to idle
       period.  So the assumption of increasing latency, threshold, and power
       savings from Cn to C(n+1) can no longer be guaranteed.
    
    It can be straightforward to calculate the power consumption of each
    available state and to specify it in power_usage for the idle period.
    Using the power_usage fields, the menu governor then selects the state
    that has the lowest power consumption and that still satisfies all other
    critieria.  The power_specified bit defaults to 0.  For existing cpuidle
    drivers, cpuidle detects that power_specified is 0 and fills in a dummy
    set of power_usage values.
    
    Signed-off-by: Ai Li <aili@codeaurora.org>
    Cc: Len Brown <len.brown@intel.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 1b128702d300..c2408bbe9c2e 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -234,6 +234,7 @@ static int menu_select(struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
+	unsigned int power_usage = -1;
 	int i;
 	int multiplier;
 
@@ -278,19 +279,27 @@ static int menu_select(struct cpuidle_device *dev)
 	if (data->expected_us > 5)
 		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 
-
-	/* find the deepest idle state that satisfies our constraints */
+	/*
+	 * Find the idle state with the lowest power while satisfying
+	 * our constraints.
+	 */
 	for (i = CPUIDLE_DRIVER_STATE_START; i < dev->state_count; i++) {
 		struct cpuidle_state *s = &dev->states[i];
 
+		if (s->flags & CPUIDLE_FLAG_IGNORE)
+			continue;
 		if (s->target_residency > data->predicted_us)
-			break;
+			continue;
 		if (s->exit_latency > latency_req)
-			break;
+			continue;
 		if (s->exit_latency * multiplier > data->predicted_us)
-			break;
-		data->exit_us = s->exit_latency;
-		data->last_state_idx = i;
+			continue;
+
+		if (s->power_usage < power_usage) {
+			power_usage = s->power_usage;
+			data->last_state_idx = i;
+			data->exit_us = s->exit_latency;
+		}
 	}
 
 	return data->last_state_idx;

commit 8c215bd3890c347dfb6a2db4779755f8b9c298a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 1 09:07:17 2010 +0200

    sched: Cure nr_iowait_cpu() users
    
    Commit 0224cf4c5e (sched: Intoduce get_cpu_iowait_time_us())
    broke things by not making sure preemption was indeed disabled
    by the callers of nr_iowait_cpu() which took the iowait value of
    the current cpu.
    
    This resulted in a heap of preempt warnings. Cure this by making
    nr_iowait_cpu() take a cpu number and fix up the callers to pass
    in the right number.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: linux-pm@lists.linux-foundation.org
    LKML-Reference: <1277968037.1868.120.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 52ff8aa63f84..1b128702d300 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -143,7 +143,7 @@ static inline int which_bucket(unsigned int duration)
 	 * This allows us to calculate
 	 * E(duration)|iowait
 	 */
-	if (nr_iowait_cpu())
+	if (nr_iowait_cpu(smp_processor_id()))
 		bucket = BUCKETS/2;
 
 	if (duration < 10)
@@ -175,7 +175,7 @@ static inline int performance_multiplier(void)
 	mult += 2 * get_loadavg();
 
 	/* for IO wait tasks (per cpu!) we add 5x each */
-	mult += 10 * nr_iowait_cpu();
+	mult += 10 * nr_iowait_cpu(smp_processor_id());
 
 	return mult;
 }

commit 1f85f87d4f81d1e5a2d502d48316a1bdc5acac0b
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon May 24 14:32:59 2010 -0700

    cpuidle: add a repeating pattern detector to the menu governor
    
    Currently, the menu governor uses the (corrected) next timer as key item
    for predicting the idle duration.
    
    It turns out that there are specific cases where this breaks down: There
    are cases where we have a very repetitive pattern of idle durations, where
    the idle period is pretty much the same, for reasons completely unrelated
    to the next timer event.  Examples of such repeating patterns are network
    loads with irq mitigation, the mouse moving but in theory also the wifi
    beacons.
    
    This patch adds a relatively simple detector for such repeating patterns,
    where the standard deviation of the last 8 idle periods is compared to a
    threshold.
    
    With this extra predictor in place, measurements show that the DECAY
    factor can now be increased (the decaying average will now decay slower)
    to get an even more stable result.
    
    [arjan@infradead.org: fix bug identified by Frank]
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Corrado Zoccolo <czoccolo@gmail.com>
    Cc: Frank Rowand <frank.rowand@am.sony.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b81ad9c731ae..52ff8aa63f84 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -21,9 +21,12 @@
 #include <linux/math64.h>
 
 #define BUCKETS 12
+#define INTERVALS 8
 #define RESOLUTION 1024
-#define DECAY 4
+#define DECAY 8
 #define MAX_INTERESTING 50000
+#define STDDEV_THRESH 400
+
 
 /*
  * Concepts and ideas behind the menu governor
@@ -64,6 +67,16 @@
  * indexed based on the magnitude of the expected duration as well as the
  * "is IO outstanding" property.
  *
+ * Repeatable-interval-detector
+ * ----------------------------
+ * There are some cases where "next timer" is a completely unusable predictor:
+ * Those cases where the interval is fixed, for example due to hardware
+ * interrupt mitigation, but also due to fixed transfer rate devices such as
+ * mice.
+ * For this, we use a different predictor: We track the duration of the last 8
+ * intervals and if the stand deviation of these 8 intervals is below a
+ * threshold value, we use the average of these intervals as prediction.
+ *
  * Limiting Performance Impact
  * ---------------------------
  * C states, especially those with large exit latencies, can have a real
@@ -104,6 +117,8 @@ struct menu_device {
 	unsigned int	exit_us;
 	unsigned int	bucket;
 	u64		correction_factor[BUCKETS];
+	u32		intervals[INTERVALS];
+	int		interval_ptr;
 };
 
 
@@ -175,6 +190,42 @@ static u64 div_round64(u64 dividend, u32 divisor)
 	return div_u64(dividend + (divisor / 2), divisor);
 }
 
+/*
+ * Try detecting repeating patterns by keeping track of the last 8
+ * intervals, and checking if the standard deviation of that set
+ * of points is below a threshold. If it is... then use the
+ * average of these 8 points as the estimated value.
+ */
+static void detect_repeating_patterns(struct menu_device *data)
+{
+	int i;
+	uint64_t avg = 0;
+	uint64_t stddev = 0; /* contains the square of the std deviation */
+
+	/* first calculate average and standard deviation of the past */
+	for (i = 0; i < INTERVALS; i++)
+		avg += data->intervals[i];
+	avg = avg / INTERVALS;
+
+	/* if the avg is beyond the known next tick, it's worthless */
+	if (avg > data->expected_us)
+		return;
+
+	for (i = 0; i < INTERVALS; i++)
+		stddev += (data->intervals[i] - avg) *
+			  (data->intervals[i] - avg);
+
+	stddev = stddev / INTERVALS;
+
+	/*
+	 * now.. if stddev is small.. then assume we have a
+	 * repeating pattern and predict we keep doing this.
+	 */
+
+	if (avg && stddev < STDDEV_THRESH)
+		data->predicted_us = avg;
+}
+
 /**
  * menu_select - selects the next idle state to enter
  * @dev: the CPU
@@ -218,6 +269,8 @@ static int menu_select(struct cpuidle_device *dev)
 	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
 					 RESOLUTION * DECAY);
 
+	detect_repeating_patterns(data);
+
 	/*
 	 * We want to default to C1 (hlt), not to busy polling
 	 * unless the timer is happening really really soon.
@@ -310,6 +363,11 @@ static void menu_update(struct cpuidle_device *dev)
 		new_factor = 1;
 
 	data->correction_factor[data->bucket] = new_factor;
+
+	/* update the repeating-pattern data */
+	data->intervals[data->interval_ptr++] = last_idle_us;
+	if (data->interval_ptr >= INTERVALS)
+		data->interval_ptr = 0;
 }
 
 /**

commit ed77134bfccf5e75b6cbadab268e559dbe6a4ebb
Author: Mark Gross <mgross@linux.intel.com>
Date:   Thu May 6 01:59:26 2010 +0200

    PM QOS update
    
    This patch changes the string based list management to a handle base
    implementation to help with the hot path use of pm-qos, it also renames
    much of the API to use "request" as opposed to "requirement" that was
    used in the initial implementation.  I did this because request more
    accurately represents what it actually does.
    
    Also, I added a string based ABI for users wanting to use a string
    interface.  So if the user writes 0xDDDDDDDD formatted hex it will be
    accepted by the interface.  (someone asked me for it and I don't think
    it hurts anything.)
    
    This patch updates some documentation input I got from Randy.
    
    Signed-off-by: markgross <mgross@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f8e57c6303f2..b81ad9c731ae 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -182,7 +182,7 @@ static u64 div_round64(u64 dividend, u32 divisor)
 static int menu_select(struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
-	int latency_req = pm_qos_requirement(PM_QOS_CPU_DMA_LATENCY);
+	int latency_req = pm_qos_request(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 	int multiplier;
 

commit 1c6fe0364fa7bf28248488753ee0afb6b759cd04
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Sat May 8 15:47:37 2010 -0700

    cpuidle: Fix incorrect optimization
    
    commit 672917dcc78 ("cpuidle: menu governor: reduce latency on exit")
    added an optimization, where the analysis on the past idle period moved
    from the end of idle, to the beginning of the new idle.
    
    Unfortunately, this optimization had a bug where it zeroed one key
    variable for new use, that is needed for the analysis.  The fix is
    simple, zero the variable after doing the work from the previous idle.
    
    During the audit of the code that found this issue, another issue was
    also found; the ->measured_us data structure member is never set, a
    local variable is always used instead.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Corrado Zoccolo <czoccolo@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 1aea7157d8ff..f8e57c6303f2 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -100,7 +100,6 @@ struct menu_device {
 	int             needs_update;
 
 	unsigned int	expected_us;
-	unsigned int	measured_us;
 	u64		predicted_us;
 	unsigned int	exit_us;
 	unsigned int	bucket;
@@ -187,14 +186,14 @@ static int menu_select(struct cpuidle_device *dev)
 	int i;
 	int multiplier;
 
-	data->last_state_idx = 0;
-	data->exit_us = 0;
-
 	if (data->needs_update) {
 		menu_update(dev);
 		data->needs_update = 0;
 	}
 
+	data->last_state_idx = 0;
+	data->exit_us = 0;
+
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))
 		return 0;
@@ -294,7 +293,7 @@ static void menu_update(struct cpuidle_device *dev)
 	new_factor = data->correction_factor[data->bucket]
 			* (DECAY - 1) / DECAY;
 
-	if (data->expected_us > 0 && data->measured_us < MAX_INTERESTING)
+	if (data->expected_us > 0 && measured_us < MAX_INTERESTING)
 		new_factor += RESOLUTION * measured_us / data->expected_us;
 	else
 		/*

commit 56e6943b41468826c26155139629c9a5379550ab
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Fri Mar 5 13:42:30 2010 -0800

    cpuidle menu: remove 8 bytes of padding on 64 bit builds
    
    Reorder struct menu_device to remove 8 bytes of padding on 64 bit builds.
    Size drops from 136 to 128 bytes, so possibly needing one fewer cache
    lines.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 73655aeb3a60..1aea7157d8ff 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -100,8 +100,8 @@ struct menu_device {
 	int             needs_update;
 
 	unsigned int	expected_us;
-	u64		predicted_us;
 	unsigned int	measured_us;
+	u64		predicted_us;
 	unsigned int	exit_us;
 	unsigned int	bucket;
 	u64		correction_factor[BUCKETS];

commit 5787536edf18e33d06e2bf038bfd0910f4def213
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Fri Jan 8 14:43:08 2010 -0800

    drivers/cpuidle/governors/menu.c: fix undefined reference to `__udivdi3'
    
    menu: use proper 64 bit math
    
    The new menu governor is incorrectly doing a 64 bit divide.  Compile
    tested only
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 68104434ebb5..73655aeb3a60 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -18,6 +18,7 @@
 #include <linux/hrtimer.h>
 #include <linux/tick.h>
 #include <linux/sched.h>
+#include <linux/math64.h>
 
 #define BUCKETS 12
 #define RESOLUTION 1024
@@ -169,6 +170,12 @@ static DEFINE_PER_CPU(struct menu_device, menu_devices);
 
 static void menu_update(struct cpuidle_device *dev);
 
+/* This implements DIV_ROUND_CLOSEST but avoids 64 bit division */
+static u64 div_round64(u64 dividend, u32 divisor)
+{
+	return div_u64(dividend + (divisor / 2), divisor);
+}
+
 /**
  * menu_select - selects the next idle state to enter
  * @dev: the CPU
@@ -209,9 +216,8 @@ static int menu_select(struct cpuidle_device *dev)
 		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
 
 	/* Make sure to round up for half microseconds */
-	data->predicted_us = DIV_ROUND_CLOSEST(
-		data->expected_us * data->correction_factor[data->bucket],
-		RESOLUTION * DECAY);
+	data->predicted_us = div_round64(data->expected_us * data->correction_factor[data->bucket],
+					 RESOLUTION * DECAY);
 
 	/*
 	 * We want to default to C1 (hlt), not to busy polling

commit 672917dcc781ead7652a8b11b1fba14e38ac15b8
Author: Corrado Zoccolo <czoccolo@gmail.com>
Date:   Mon Sep 21 17:04:09 2009 -0700

    cpuidle: menu governor: reduce latency on exit
    
    Move the state residency accounting and statistics computation off the hot
    exit path.
    
    On exit, the need to recompute statistics is recorded, and new statistics
    will be computed when menu_select is called again.
    
    The expected effect is to reduce processor wakeup latency from sleep
    (C-states).  We are speaking of few hundreds of cycles reduction out of a
    several microseconds latency (determined by the hardware transition), so
    it is difficult to measure.
    
    Signed-off-by: Corrado Zoccolo <czoccolo@gmail.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Adam Belay <abelay@novell.com
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 9f3d77532ab9..68104434ebb5 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -96,6 +96,7 @@
 
 struct menu_device {
 	int		last_state_idx;
+	int             needs_update;
 
 	unsigned int	expected_us;
 	u64		predicted_us;
@@ -166,6 +167,8 @@ static inline int performance_multiplier(void)
 
 static DEFINE_PER_CPU(struct menu_device, menu_devices);
 
+static void menu_update(struct cpuidle_device *dev);
+
 /**
  * menu_select - selects the next idle state to enter
  * @dev: the CPU
@@ -180,6 +183,11 @@ static int menu_select(struct cpuidle_device *dev)
 	data->last_state_idx = 0;
 	data->exit_us = 0;
 
+	if (data->needs_update) {
+		menu_update(dev);
+		data->needs_update = 0;
+	}
+
 	/* Special case when user has set very strict latency requirement */
 	if (unlikely(latency_req == 0))
 		return 0;
@@ -231,13 +239,23 @@ static int menu_select(struct cpuidle_device *dev)
 }
 
 /**
- * menu_reflect - attempts to guess what happened after entry
+ * menu_reflect - records that data structures need update
  * @dev: the CPU
  *
  * NOTE: it's important to be fast here because this operation will add to
  *       the overall exit latency.
  */
 static void menu_reflect(struct cpuidle_device *dev)
+{
+	struct menu_device *data = &__get_cpu_var(menu_devices);
+	data->needs_update = 1;
+}
+
+/**
+ * menu_update - attempts to guess what happened after entry
+ * @dev: the CPU
+ */
+static void menu_update(struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int last_idx = data->last_state_idx;

commit 69d25870f20c4b2563304f2b79c5300dd60a067e
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Mon Sep 21 17:04:08 2009 -0700

    cpuidle: fix the menu governor to boost IO performance
    
    Fix the menu idle governor which balances power savings, energy efficiency
    and performance impact.
    
    The reason for a reworked governor is that there have been serious
    performance issues reported with the existing code on Nehalem server
    systems.
    
    To show this I'm sure Andrew wants to see benchmark results:
    (benchmark is "fio", "no cstates" is using "idle=poll")
    
                    no cstates      current linux   new algorithm
    1 disk          107 Mb/s        85 Mb/s         105 Mb/s
    2 disks         215 Mb/s        123 Mb/s        209 Mb/s
    12 disks        590 Mb/s        320 Mb/s        585 Mb/s
    
    In various power benchmark measurements, no degredation was found by our
    measurement&diagnostics team.  Obviously a small percentage more power was
    used in the "fio" benchmark, due to the much higher performance.
    
    While it would be a novel idea to describe the new algorithm in this
    commit message, I cheaped out and described it in comments in the code
    instead.
    
    [changes since first post: spelling fixes from akpm, review feedback,
    folded menu-tng into menu.c]
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Yanmin Zhang <yanmin_zhang@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index f1df59f59a37..9f3d77532ab9 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -2,8 +2,12 @@
  * menu.c - the menu idle governor
  *
  * Copyright (C) 2006-2007 Adam Belay <abelay@novell.com>
+ * Copyright (C) 2009 Intel Corporation
+ * Author:
+ *        Arjan van de Ven <arjan@linux.intel.com>
  *
- * This code is licenced under the GPL.
+ * This code is licenced under the GPL version 2 as described
+ * in the COPYING file that acompanies the Linux Kernel.
  */
 
 #include <linux/kernel.h>
@@ -13,20 +17,153 @@
 #include <linux/ktime.h>
 #include <linux/hrtimer.h>
 #include <linux/tick.h>
+#include <linux/sched.h>
 
-#define BREAK_FUZZ	4	/* 4 us */
-#define PRED_HISTORY_PCT	50
+#define BUCKETS 12
+#define RESOLUTION 1024
+#define DECAY 4
+#define MAX_INTERESTING 50000
+
+/*
+ * Concepts and ideas behind the menu governor
+ *
+ * For the menu governor, there are 3 decision factors for picking a C
+ * state:
+ * 1) Energy break even point
+ * 2) Performance impact
+ * 3) Latency tolerance (from pmqos infrastructure)
+ * These these three factors are treated independently.
+ *
+ * Energy break even point
+ * -----------------------
+ * C state entry and exit have an energy cost, and a certain amount of time in
+ * the  C state is required to actually break even on this cost. CPUIDLE
+ * provides us this duration in the "target_residency" field. So all that we
+ * need is a good prediction of how long we'll be idle. Like the traditional
+ * menu governor, we start with the actual known "next timer event" time.
+ *
+ * Since there are other source of wakeups (interrupts for example) than
+ * the next timer event, this estimation is rather optimistic. To get a
+ * more realistic estimate, a correction factor is applied to the estimate,
+ * that is based on historic behavior. For example, if in the past the actual
+ * duration always was 50% of the next timer tick, the correction factor will
+ * be 0.5.
+ *
+ * menu uses a running average for this correction factor, however it uses a
+ * set of factors, not just a single factor. This stems from the realization
+ * that the ratio is dependent on the order of magnitude of the expected
+ * duration; if we expect 500 milliseconds of idle time the likelihood of
+ * getting an interrupt very early is much higher than if we expect 50 micro
+ * seconds of idle time. A second independent factor that has big impact on
+ * the actual factor is if there is (disk) IO outstanding or not.
+ * (as a special twist, we consider every sleep longer than 50 milliseconds
+ * as perfect; there are no power gains for sleeping longer than this)
+ *
+ * For these two reasons we keep an array of 12 independent factors, that gets
+ * indexed based on the magnitude of the expected duration as well as the
+ * "is IO outstanding" property.
+ *
+ * Limiting Performance Impact
+ * ---------------------------
+ * C states, especially those with large exit latencies, can have a real
+ * noticable impact on workloads, which is not acceptable for most sysadmins,
+ * and in addition, less performance has a power price of its own.
+ *
+ * As a general rule of thumb, menu assumes that the following heuristic
+ * holds:
+ *     The busier the system, the less impact of C states is acceptable
+ *
+ * This rule-of-thumb is implemented using a performance-multiplier:
+ * If the exit latency times the performance multiplier is longer than
+ * the predicted duration, the C state is not considered a candidate
+ * for selection due to a too high performance impact. So the higher
+ * this multiplier is, the longer we need to be idle to pick a deep C
+ * state, and thus the less likely a busy CPU will hit such a deep
+ * C state.
+ *
+ * Two factors are used in determing this multiplier:
+ * a value of 10 is added for each point of "per cpu load average" we have.
+ * a value of 5 points is added for each process that is waiting for
+ * IO on this CPU.
+ * (these values are experimentally determined)
+ *
+ * The load average factor gives a longer term (few seconds) input to the
+ * decision, while the iowait value gives a cpu local instantanious input.
+ * The iowait factor may look low, but realize that this is also already
+ * represented in the system load average.
+ *
+ */
 
 struct menu_device {
 	int		last_state_idx;
 
 	unsigned int	expected_us;
-	unsigned int	predicted_us;
-	unsigned int    current_predicted_us;
-	unsigned int	last_measured_us;
-	unsigned int	elapsed_us;
+	u64		predicted_us;
+	unsigned int	measured_us;
+	unsigned int	exit_us;
+	unsigned int	bucket;
+	u64		correction_factor[BUCKETS];
 };
 
+
+#define LOAD_INT(x) ((x) >> FSHIFT)
+#define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
+
+static int get_loadavg(void)
+{
+	unsigned long this = this_cpu_load();
+
+
+	return LOAD_INT(this) * 10 + LOAD_FRAC(this) / 10;
+}
+
+static inline int which_bucket(unsigned int duration)
+{
+	int bucket = 0;
+
+	/*
+	 * We keep two groups of stats; one with no
+	 * IO pending, one without.
+	 * This allows us to calculate
+	 * E(duration)|iowait
+	 */
+	if (nr_iowait_cpu())
+		bucket = BUCKETS/2;
+
+	if (duration < 10)
+		return bucket;
+	if (duration < 100)
+		return bucket + 1;
+	if (duration < 1000)
+		return bucket + 2;
+	if (duration < 10000)
+		return bucket + 3;
+	if (duration < 100000)
+		return bucket + 4;
+	return bucket + 5;
+}
+
+/*
+ * Return a multiplier for the exit latency that is intended
+ * to take performance requirements into account.
+ * The more performance critical we estimate the system
+ * to be, the higher this multiplier, and thus the higher
+ * the barrier to go to an expensive C state.
+ */
+static inline int performance_multiplier(void)
+{
+	int mult = 1;
+
+	/* for higher loadavg, we are more reluctant */
+
+	mult += 2 * get_loadavg();
+
+	/* for IO wait tasks (per cpu!) we add 5x each */
+	mult += 10 * nr_iowait_cpu();
+
+	return mult;
+}
+
 static DEFINE_PER_CPU(struct menu_device, menu_devices);
 
 /**
@@ -38,37 +175,59 @@ static int menu_select(struct cpuidle_device *dev)
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int latency_req = pm_qos_requirement(PM_QOS_CPU_DMA_LATENCY);
 	int i;
+	int multiplier;
+
+	data->last_state_idx = 0;
+	data->exit_us = 0;
 
 	/* Special case when user has set very strict latency requirement */
-	if (unlikely(latency_req == 0)) {
-		data->last_state_idx = 0;
+	if (unlikely(latency_req == 0))
 		return 0;
-	}
 
-	/* determine the expected residency time */
+	/* determine the expected residency time, round up */
 	data->expected_us =
-		(u32) ktime_to_ns(tick_nohz_get_sleep_length()) / 1000;
+	    DIV_ROUND_UP((u32)ktime_to_ns(tick_nohz_get_sleep_length()), 1000);
+
+
+	data->bucket = which_bucket(data->expected_us);
+
+	multiplier = performance_multiplier();
+
+	/*
+	 * if the correction factor is 0 (eg first time init or cpu hotplug
+	 * etc), we actually want to start out with a unity factor.
+	 */
+	if (data->correction_factor[data->bucket] == 0)
+		data->correction_factor[data->bucket] = RESOLUTION * DECAY;
+
+	/* Make sure to round up for half microseconds */
+	data->predicted_us = DIV_ROUND_CLOSEST(
+		data->expected_us * data->correction_factor[data->bucket],
+		RESOLUTION * DECAY);
+
+	/*
+	 * We want to default to C1 (hlt), not to busy polling
+	 * unless the timer is happening really really soon.
+	 */
+	if (data->expected_us > 5)
+		data->last_state_idx = CPUIDLE_DRIVER_STATE_START;
 
-	/* Recalculate predicted_us based on prediction_history_pct */
-	data->predicted_us *= PRED_HISTORY_PCT;
-	data->predicted_us += (100 - PRED_HISTORY_PCT) *
-				data->current_predicted_us;
-	data->predicted_us /= 100;
 
 	/* find the deepest idle state that satisfies our constraints */
-	for (i = CPUIDLE_DRIVER_STATE_START + 1; i < dev->state_count; i++) {
+	for (i = CPUIDLE_DRIVER_STATE_START; i < dev->state_count; i++) {
 		struct cpuidle_state *s = &dev->states[i];
 
-		if (s->target_residency > data->expected_us)
-			break;
 		if (s->target_residency > data->predicted_us)
 			break;
 		if (s->exit_latency > latency_req)
 			break;
+		if (s->exit_latency * multiplier > data->predicted_us)
+			break;
+		data->exit_us = s->exit_latency;
+		data->last_state_idx = i;
 	}
 
-	data->last_state_idx = i - 1;
-	return i - 1;
+	return data->last_state_idx;
 }
 
 /**
@@ -85,35 +244,49 @@ static void menu_reflect(struct cpuidle_device *dev)
 	unsigned int last_idle_us = cpuidle_get_last_residency(dev);
 	struct cpuidle_state *target = &dev->states[last_idx];
 	unsigned int measured_us;
+	u64 new_factor;
 
 	/*
 	 * Ugh, this idle state doesn't support residency measurements, so we
 	 * are basically lost in the dark.  As a compromise, assume we slept
-	 * for one full standard timer tick.  However, be aware that this
-	 * could potentially result in a suboptimal state transition.
+	 * for the whole expected time.
 	 */
 	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID)))
-		last_idle_us = USEC_PER_SEC / HZ;
+		last_idle_us = data->expected_us;
+
+
+	measured_us = last_idle_us;
 
 	/*
-	 * measured_us and elapsed_us are the cumulative idle time, since the
-	 * last time we were woken out of idle by an interrupt.
+	 * We correct for the exit latency; we are assuming here that the
+	 * exit latency happens after the event that we're interested in.
 	 */
-	if (data->elapsed_us <= data->elapsed_us + last_idle_us)
-		measured_us = data->elapsed_us + last_idle_us;
+	if (measured_us > data->exit_us)
+		measured_us -= data->exit_us;
+
+
+	/* update our correction ratio */
+
+	new_factor = data->correction_factor[data->bucket]
+			* (DECAY - 1) / DECAY;
+
+	if (data->expected_us > 0 && data->measured_us < MAX_INTERESTING)
+		new_factor += RESOLUTION * measured_us / data->expected_us;
 	else
-		measured_us = -1;
+		/*
+		 * we were idle so long that we count it as a perfect
+		 * prediction
+		 */
+		new_factor += RESOLUTION;
 
-	/* Predict time until next break event */
-	data->current_predicted_us = max(measured_us, data->last_measured_us);
+	/*
+	 * We don't want 0 as factor; we always want at least
+	 * a tiny bit of estimated time.
+	 */
+	if (new_factor == 0)
+		new_factor = 1;
 
-	if (last_idle_us + BREAK_FUZZ <
-	    data->expected_us - target->exit_latency) {
-		data->last_measured_us = measured_us;
-		data->elapsed_us = 0;
-	} else {
-		data->elapsed_us = measured_us;
-	}
+	data->correction_factor[data->bucket] = new_factor;
 }
 
 /**

commit 816bb611e41be29b476dc16f6297eb551bf4d747
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Tue Dec 30 14:46:02 2008 -0800

    cpuidle: Add decaying history logic to menu idle predictor
    
    Add decaying history of predicted idle time, instead of using the last early
    wakeup. This logic helps menu governor do better job of predicting idle time.
    
    With this change, we also measured noticable (~8%) power savings on
    a DP server system with CPUs supporting deep C states, when system
    was lightly loaded. There was no change to power or perf on other load
    conditions.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 8d7cf3f31450..f1df59f59a37 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -15,12 +15,14 @@
 #include <linux/tick.h>
 
 #define BREAK_FUZZ	4	/* 4 us */
+#define PRED_HISTORY_PCT	50
 
 struct menu_device {
 	int		last_state_idx;
 
 	unsigned int	expected_us;
 	unsigned int	predicted_us;
+	unsigned int    current_predicted_us;
 	unsigned int	last_measured_us;
 	unsigned int	elapsed_us;
 };
@@ -47,6 +49,12 @@ static int menu_select(struct cpuidle_device *dev)
 	data->expected_us =
 		(u32) ktime_to_ns(tick_nohz_get_sleep_length()) / 1000;
 
+	/* Recalculate predicted_us based on prediction_history_pct */
+	data->predicted_us *= PRED_HISTORY_PCT;
+	data->predicted_us += (100 - PRED_HISTORY_PCT) *
+				data->current_predicted_us;
+	data->predicted_us /= 100;
+
 	/* find the deepest idle state that satisfies our constraints */
 	for (i = CPUIDLE_DRIVER_STATE_START + 1; i < dev->state_count; i++) {
 		struct cpuidle_state *s = &dev->states[i];
@@ -97,7 +105,7 @@ static void menu_reflect(struct cpuidle_device *dev)
 		measured_us = -1;
 
 	/* Predict time until next break event */
-	data->predicted_us = max(measured_us, data->last_measured_us);
+	data->current_predicted_us = max(measured_us, data->last_measured_us);
 
 	if (last_idle_us + BREAK_FUZZ <
 	    data->expected_us - target->exit_latency) {

commit 320eee776357db52d6fcfb11cff985b1976a4595
Author: venkatesh.pallipadi@intel.com <venkatesh.pallipadi@intel.com>
Date:   Wed Jul 30 19:21:43 2008 -0700

    cpuidle: Menu governor fix wrong usage of measured_us
    
    There is a bug in menu governor where we have
                    if (data->elapsed_us < data->elapsed_us + measured_us)
    
    with measured_us already having elapsed_us added in tickless case here
            unsigned int measured_us =
                    cpuidle_get_last_residency(dev) + data->elapsed_us;
    
    Also, it should be last_residency, not measured_us, that need to be used to
    do comparing and distinguish between expected & non-expected events.
    
    Refactor menu_reflect() to fix these two problems.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Wei Gang <gang.wei@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index b8f3e21530bd..8d7cf3f31450 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -74,9 +74,9 @@ static void menu_reflect(struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
 	int last_idx = data->last_state_idx;
-	unsigned int measured_us =
-		cpuidle_get_last_residency(dev) + data->elapsed_us;
+	unsigned int last_idle_us = cpuidle_get_last_residency(dev);
 	struct cpuidle_state *target = &dev->states[last_idx];
+	unsigned int measured_us;
 
 	/*
 	 * Ugh, this idle state doesn't support residency measurements, so we
@@ -84,20 +84,27 @@ static void menu_reflect(struct cpuidle_device *dev)
 	 * for one full standard timer tick.  However, be aware that this
 	 * could potentially result in a suboptimal state transition.
 	 */
-	if (!(target->flags & CPUIDLE_FLAG_TIME_VALID))
-		measured_us = USEC_PER_SEC / HZ;
+	if (unlikely(!(target->flags & CPUIDLE_FLAG_TIME_VALID)))
+		last_idle_us = USEC_PER_SEC / HZ;
 
-	/* Predict time remaining until next break event */
-	if (measured_us + BREAK_FUZZ < data->expected_us - target->exit_latency) {
-		data->predicted_us = max(measured_us, data->last_measured_us);
+	/*
+	 * measured_us and elapsed_us are the cumulative idle time, since the
+	 * last time we were woken out of idle by an interrupt.
+	 */
+	if (data->elapsed_us <= data->elapsed_us + last_idle_us)
+		measured_us = data->elapsed_us + last_idle_us;
+	else
+		measured_us = -1;
+
+	/* Predict time until next break event */
+	data->predicted_us = max(measured_us, data->last_measured_us);
+
+	if (last_idle_us + BREAK_FUZZ <
+	    data->expected_us - target->exit_latency) {
 		data->last_measured_us = measured_us;
 		data->elapsed_us = 0;
 	} else {
-		if (data->elapsed_us < data->elapsed_us + measured_us)
-			data->elapsed_us = measured_us;
-		else
-			data->elapsed_us = -1;
-		data->predicted_us = max(measured_us, data->last_measured_us);
+		data->elapsed_us = measured_us;
 	}
 }
 

commit a2bd92023357e47f22a34d4cb1635453546662bc
Author: venkatesh.pallipadi@intel.com <venkatesh.pallipadi@intel.com>
Date:   Wed Jul 30 19:21:42 2008 -0700

    cpuidle: Do not use poll_idle unless user asks for it
    
    poll_idle was added to CPUIDLE, just as a low latency idle handler, to be
    used in cases when user desires CPUs not to enter any idle state at all. It
    was supposed to be a run time idle=poll option to the user. But, it was indeed
    getting used during normal menu and ladder governor default case, with no
    special user setting (Reported by Linus Torvalds).
    
    Change below ensures that poll_idle will not be used unless user explicitly
    asks pm_qos infrastructure for zero latency requirement.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 78d77c5dc35c..b8f3e21530bd 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -34,21 +34,28 @@ static DEFINE_PER_CPU(struct menu_device, menu_devices);
 static int menu_select(struct cpuidle_device *dev)
 {
 	struct menu_device *data = &__get_cpu_var(menu_devices);
+	int latency_req = pm_qos_requirement(PM_QOS_CPU_DMA_LATENCY);
 	int i;
 
+	/* Special case when user has set very strict latency requirement */
+	if (unlikely(latency_req == 0)) {
+		data->last_state_idx = 0;
+		return 0;
+	}
+
 	/* determine the expected residency time */
 	data->expected_us =
 		(u32) ktime_to_ns(tick_nohz_get_sleep_length()) / 1000;
 
 	/* find the deepest idle state that satisfies our constraints */
-	for (i = 1; i < dev->state_count; i++) {
+	for (i = CPUIDLE_DRIVER_STATE_START + 1; i < dev->state_count; i++) {
 		struct cpuidle_state *s = &dev->states[i];
 
 		if (s->target_residency > data->expected_us)
 			break;
 		if (s->target_residency > data->predicted_us)
 			break;
-		if (s->exit_latency > pm_qos_requirement(PM_QOS_CPU_DMA_LATENCY))
+		if (s->exit_latency > latency_req)
 			break;
 	}
 

commit d82b35186eaa816267f044bd70cc0acb3c7971a3
Author: Mark Gross <mgross@linux.intel.com>
Date:   Mon Feb 4 22:30:08 2008 -0800

    pm qos infrastructure and interface
    
    The following patch is a generalization of the latency.c implementation done
    by Arjan last year.  It provides infrastructure for more than one parameter,
    and exposes a user mode interface for processes to register pm_qos
    expectations of processes.
    
    This interface provides a kernel and user mode interface for registering
    performance expectations by drivers, subsystems and user space applications on
    one of the parameters.
    
    Currently we have {cpu_dma_latency, network_latency, network_throughput} as
    the initial set of pm_qos parameters.
    
    The infrastructure exposes multiple misc device nodes one per implemented
    parameter.  The set of parameters implement is defined by pm_qos_power_init()
    and pm_qos_params.h.  This is done because having the available parameters
    being runtime configurable or changeable from a driver was seen as too easy to
    abuse.
    
    For each parameter a list of performance requirements is maintained along with
    an aggregated target value.  The aggregated target value is updated with
    changes to the requirement list or elements of the list.  Typically the
    aggregated target value is simply the max or min of the requirement values
    held in the parameter list elements.
    
    >From kernel mode the use of this interface is simple:
    
    pm_qos_add_requirement(param_id, name, target_value):
    
      Will insert a named element in the list for that identified PM_QOS
      parameter with the target value.  Upon change to this list the new target is
      recomputed and any registered notifiers are called only if the target value
      is now different.
    
    pm_qos_update_requirement(param_id, name, new_target_value):
    
      Will search the list identified by the param_id for the named list element
      and then update its target value, calling the notification tree if the
      aggregated target is changed.  with that name is already registered.
    
    pm_qos_remove_requirement(param_id, name):
    
      Will search the identified list for the named element and remove it, after
      removal it will update the aggregate target and call the notification tree
      if the target was changed as a result of removing the named requirement.
    
    >From user mode:
    
      Only processes can register a pm_qos requirement.  To provide for
      automatic cleanup for process the interface requires the process to register
      its parameter requirements in the following way:
    
      To register the default pm_qos target for the specific parameter, the
      process must open one of /dev/[cpu_dma_latency, network_latency,
      network_throughput]
    
      As long as the device node is held open that process has a registered
      requirement on the parameter.  The name of the requirement is
      "process_<PID>" derived from the current->pid from within the open system
      call.
    
      To change the requested target value the process needs to write a s32
      value to the open device node.  This translates to a
      pm_qos_update_requirement call.
    
      To remove the user mode request for a target value simply close the device
      node.
    
    [akpm@linux-foundation.org: fix warnings]
    [akpm@linux-foundation.org: fix build]
    [akpm@linux-foundation.org: fix build again]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: mark gross <mgross@linux.intel.com>
    Cc: "John W. Linville" <linville@tuxdriver.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Jaroslav Kysela <perex@suse.cz>
    Cc: Takashi Iwai <tiwai@suse.de>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Adam Belay <abelay@novell.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 299d45c3bdd2..78d77c5dc35c 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -8,7 +8,7 @@
 
 #include <linux/kernel.h>
 #include <linux/cpuidle.h>
-#include <linux/latency.h>
+#include <linux/pm_qos_params.h>
 #include <linux/time.h>
 #include <linux/ktime.h>
 #include <linux/hrtimer.h>
@@ -48,7 +48,7 @@ static int menu_select(struct cpuidle_device *dev)
 			break;
 		if (s->target_residency > data->predicted_us)
 			break;
-		if (s->exit_latency > system_latency_constraint())
+		if (s->exit_latency > pm_qos_requirement(PM_QOS_CPU_DMA_LATENCY))
 			break;
 	}
 

commit 4f86d3a8e297205780cca027e974fd5f81064780
Author: Len Brown <len.brown@intel.com>
Date:   Wed Oct 3 18:58:00 2007 -0400

    cpuidle: consolidate 2.6.22 cpuidle branch into one patch
    
    commit e5a16b1f9eec0af7cfa0830304b41c1c0833cf9f
    Author: Len Brown <len.brown@intel.com>
    Date:   Tue Oct 2 23:44:44 2007 -0400
    
        cpuidle: shrink diff
    
        processor_idle.c |  440 +++++++++++++++++++++++++++++++++++++++++--
        1 file changed, 429 insertions(+), 11 deletions(-)
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit dfbb9d5aedfb18848a3e0d6f6e3e4969febb209c
    Author: Len Brown <len.brown@intel.com>
    Date:   Wed Sep 26 02:17:55 2007 -0400
    
        cpuidle: reduce diff size
    
        Reduces the cpuidle processor_idle.c diff vs 2.6.22 from this
         processor_idle.c | 2006 ++++++++++++++++++++++++++-----------------
         1 file changed, 1219 insertions(+), 787 deletions(-)
    
        to this:
         processor_idle.c |  502 +++++++++++++++++++++++++++++++++++++++----
         1 file changed, 458 insertions(+), 44 deletions(-)
    
        ...for the purpose of making the cpuilde patch less invasive
        and easier to review.
    
        no functional changes.  build tested only.
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 889172fc915f5a7fe20f35b133cbd205ce69bf6c
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Sep 13 13:40:05 2007 -0700
    
        cpuidle: Retain old ACPI policy for !CONFIG_CPU_IDLE
    
        Retain the old policy in processor_idle, so that when CPU_IDLE is not
        configured, old C-state policy will still be used. This provides a
        clean gradual migration path from old ACPI policy to new cpuidle
        based policy.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 9544a8181edc7ecc33b3bfd69271571f98ed08bc
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Sep 13 13:39:17 2007 -0700
    
        cpuidle: Configure governors by default
    
        Quoting Len "Do not give an option to users to shoot themselves in the foot".
    
        Remove the configurability of ladder and menu governors as they are
        needed for default policy of cpuidle. That way users will not be able to
        have cpuidle without any policy loosing all C-state power savings.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8975059a2c1e56cfe83d1bcf031bcf4cb39be743
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:27:07 2007 -0400
    
        CPUIDLE: load ACPI properly when CPUIDLE is disabled
    
        Change the registration return codes for when CPUIDLE
        support is not compiled into the kernel.  As a result, the ACPI
        processor driver will load properly even if CPUIDLE is unavailable.
        However, it may be possible to cleanup the ACPI processor driver further
        and eliminate some dead code paths.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit e0322e2b58dd1b12ec669bf84693efe0dc2414a8
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:26:06 2007 -0400
    
        CPUIDLE: remove cpuidle_get_bm_activity()
    
        Remove cpuidle_get_bm_activity() and updates governors
        accordingly.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 18a6e770d5c82ba26653e53d240caa617e09e9ab
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:58 2007 -0400
    
        CPUIDLE: max_cstate fix
    
        Currently max_cstate is limited to 0, resulting in no idle processor
        power management on ACPI platforms.  This patch restores the value to
        the array size.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1fdc0887286179b40ce24bcdbde663172e205ef0
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:40 2007 -0400
    
        CPUIDLE: handle BM detection inside the ACPI Processor driver
    
        Update the ACPI processor driver to detect BM activity and
        limit state entry depth internally, rather than exposing such
        requirements to CPUIDLE.  As a result, CPUIDLE can drop this
        ACPI-specific interface and become more platform independent.  BM
        activity is now handled much more aggressively than it was in the
        original implementation, so some testing coverage may be needed to
        verify that this doesn't introduce any DMA buffer under-run issues.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0ef38840db666f48e3cdd2b769da676c57228dd9
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:25:14 2007 -0400
    
        CPUIDLE: menu governor updates
    
        Tweak the menu governor to more effectively handle non-timer
        break events.  Non-timer break events are detected by comparing the
        actual sleep time to the expected sleep time.  In future revisions, it
        may be more reliable to use the timer data structures directly.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit bb4d74fca63fa96cf3ace644b15ae0f12b7df5a1
    Author: Adam Belay <abelay@novell.com>
    Date:   Tue Aug 21 18:24:40 2007 -0400
    
        CPUIDLE: fix 'current_governor' sysfs entry
    
        Allow the "current_governor" sysfs entry to properly handle
        input terminated with '\n'.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit df3c71559bb69b125f1a48971bf0d17f78bbdf47
    Author: Len Brown <len.brown@intel.com>
    Date:   Sun Aug 12 02:00:45 2007 -0400
    
        cpuidle: fix IA64 build (again)
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit a02064579e3f9530fd31baae16b1fc46b5a7bca8
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:39:27 2007 -0400
    
        cpuidle: Remove support for runtime changing of max_cstate
    
        Remove support for runtime changeability of max_cstate. Drivers can use
        use latency APIs.
    
        max_cstate can still be used as a boot time option and dmi override.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0912a44b13adf22f5e3f607d263aed23b4910d7e
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:39:16 2007 -0400
    
        cpuidle: Remove ACPI cstate_limit calls from ipw2100
    
        ipw2100 already has code to use accetable_latency interfaces to limit the
        C-state. Remove the calls to acpi_set_cstate_limit and acpi_get_cstate_limit
        as they are redundant.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c649a76e76be6bff1fd770d0a775798813a3f6e0
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Sun Aug 12 01:35:39 2007 -0400
    
        cpuidle: compile fix for pause and resume functions
    
        Fix the compilation failure when cpuidle is not compiled in.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Acked-by: Adam Belay <adam.belay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 2305a5920fb8ee6ccec1c62ade05aa8351091d71
    Author: Adam Belay <abelay@novell.com>
    Date:   Thu Jul 19 00:49:00 2007 -0400
    
        cpuidle: re-write
    
        Some portions have been rewritten to make the code cleaner and lighter
        weight.  The following is a list of changes:
    
        1.) the state name is now included in the sysfs interface
        2.) detection, hotplug, and available state modifications are handled by
        CPUIDLE drivers directly
        3.) the CPUIDLE idle handler is only ever installed when at least one
        cpuidle_device is enabled and ready
        4.) the menu governor BM code no longer overflows
        5.) the sysfs attributes are now printed as unsigned integers, avoiding
        negative values
        6.) a variety of other small cleanups
    
        Also, Idle drivers are no longer swappable during runtime through the
        CPUIDLE sysfs inteface.  On i386 and x86_64 most idle handlers (e.g.
        poll, mwait, halt, etc.) don't benefit from an infrastructure that
        supports multiple states, so I think using a more general case idle
        handler selection mechanism would be cleaner.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Acked-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Acked-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit df25b6b56955714e6e24b574d88d1fd11f0c3ee5
    Author: Len Brown <len.brown@intel.com>
    Date:   Tue Jul 24 17:08:21 2007 -0400
    
        cpuidle: fix IA64 buid
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit fd6ada4c14488755ff7068860078c437431fbccd
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Mon Jul 9 11:33:13 2007 -0700
    
        cpuidle: static
    
        make cpuidle_replace_governor() static
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c1d4a2cebcadf2429c0c72e1d29aa2a9684c32e0
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Tue Jul 3 00:54:40 2007 -0400
    
        cpuidle: static
    
        This patch makes the needlessly global struct menu_governor static.
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit dbf8780c6e8d572c2c273da97ed1cca7608fd999
    Author: Andrew Morton <akpm@linux-foundation.org>
    Date:   Tue Jul 3 00:49:14 2007 -0400
    
        export symbol tick_nohz_get_sleep_length
    
        ERROR: "tick_nohz_get_sleep_length" [drivers/cpuidle/governors/menu.ko] undefined!
        ERROR: "tick_nohz_get_idle_jiffies" [drivers/cpuidle/governors/menu.ko] undefined!
    
        And please be sure to get your changes to core kernel suitably reviewed.
    
        Cc: Adam Belay <abelay@novell.com>
        Cc: Venki Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Ingo Molnar <mingo@elte.hu>
        Cc: Thomas Gleixner <tglx@linutronix.de>
        Cc: john stultz <johnstul@us.ibm.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 29f0e248e7017be15f99febf9143a2cef00b2961
    Author: Andrew Morton <akpm@linux-foundation.org>
    Date:   Tue Jul 3 00:43:04 2007 -0400
    
        tick.h needs hrtimer.h
    
        It uses hrtimers.
    
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit e40cede7d63a029e92712a3fe02faee60cc38fb4
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:40:34 2007 -0400
    
        cpuidle: first round of documentation updates
    
        Documentation changes based on Pavel's feedback.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 83b42be2efece386976507555c29e7773a0dfcd1
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:39:25 2007 -0400
    
        cpuidle: add rating to the governors and pick the one with highest rating by default
    
        Introduce a governor rating scheme to pick the right governor by default.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d2a74b8c5e8f22def4709330d4bfc4a29209b71c
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:38:08 2007 -0400
    
        cpuidle: make cpuidle sysfs driver governor switch off by default
    
        Make default cpuidle sysfs to show current_governor and current_driver in
        read-only mode.  More elaborate available_governors and available_drivers with
        writeable current_governor and current_driver interface only appear with
        "cpuidle_sysfs_switch" boot parameter.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1f60a0e80bf83cf6b55c8845bbe5596ed8f6307b
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:37:00 2007 -0400
    
        cpuidle: menu governor: change the early break condition
    
        Change the C-state early break out algorithm in menu governor.
    
        We only look at early breakouts that result in wakeups shorter than idle
        state's target_residency.  If such a breakout is frequent enough, eliminate
        the particular idle state upto a timeout period.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 45a42095cf64b003b4a69be3ce7f434f97d7af51
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:35:38 2007 -0400
    
        cpuidle: fix uninitialized variable in sysfs routine
    
        Fix the uninitialized usage of ret.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 80dca7cdba3e6ee13eae277660873ab9584eb3be
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:34:16 2007 -0400
    
        cpuidle: reenable /proc/acpi//power interface for the time being
    
        Keep /proc/acpi/processor/CPU*/power around for a while as powertop depends
        on it. It will be marked deprecated and removed in future. powertop can use
        cpuidle interfaces instead.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 589c37c2646c5e3813a51255a5ee1159cb4c33fc
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Jul 3 00:32:37 2007 -0400
    
        cpuidle: menu governor and hrtimer compile fix
    
        Compile fix for menu governor.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0ba80bd9ab3ed304cb4f19b722e4cc6740588b5e
    Author: Len Brown <len.brown@intel.com>
    Date:   Thu May 31 22:51:43 2007 -0400
    
        cpuidle: build fix - cpuidle vs ipw2100 module
    
        ERROR: "acpi_set_cstate_limit" [drivers/net/wireless/ipw2100.ko] undefined!
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d7d8fa7f96a7f7682be7c6cc0cc53fa7a18c3b58
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:47:07 2007 -0400
    
        cpuidle: add the 'menu' governor
    
        Here is my first take at implementing an idle PM governor that takes
        full advantage of NO_HZ.  I call it the 'menu' governor because it
        considers the full list of idle states before each entry.
    
        I've kept the implementation fairly simple.  It attempts to guess the
        next residency time and then chooses a state that would meet at least
        the break-even point between power savings and entry cost.  To this end,
        it selects the deepest idle state that satisfies the following
        constraints:
             1. If the idle time elapsed since bus master activity was detected
                is below a threshold (currently 20 ms), then limit the selection
                to C2-type or above.
             2. Do not choose a state with a break-even residency that exceeds
                the expected time remaining until the next timer interrupt.
             3. Do not choose a state with a break-even residency that exceeds
                the elapsed time between the last pair of break events,
                excluding timer interrupts.
    
        This governor has an advantage over "ladder" governor because it
        proactively checks how much time remains until the next timer interrupt
        using the tick infrastructure.  Also, it handles device interrupt
        activity more intelligently by not including timer interrupts in break
        event calculations.  Finally, it doesn't make policy decisions using the
        number of state entries, which can have variable residency times (NO_HZ
        makes these potentially very large), and instead only considers sleep
        time deltas.
    
        The menu governor can be selected during runtime using the cpuidle sysfs
        interface like so:
        "echo "menu" > /sys/devices/system/cpu/cpuidle/current_governor"
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit a4bec7e65aa3b7488b879d971651cc99a6c410fe
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:47:03 2007 -0400
    
        cpuidle: export time until next timer interrupt using NO_HZ
    
        Expose information about the time remaining until the next
        timer interrupt expires by utilizing the dynticks infrastructure.
        Also modify the main idle loop to allow dynticks to handle
        non-interrupt break events (e.g. DMA).  Finally, expose sleep ticks
        information to external code.  Thomas Gleixner is responsible for much
        of the code in this patch.  However, I've made some additional changes,
        so I'm probably responsible if there are any bugs or oversights :)
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 2929d8996fbc77f41a5ff86bb67cdde3ca7d2d72
    Author: Adam Belay <abelay@novell.com>
    Date:   Sat Mar 24 03:46:58 2007 -0400
    
        cpuidle: governor API changes
    
        This patch prepares cpuidle for the menu governor.  It adds an optional
        stage after idle state entry to give the governor an opportunity to
        check why the state was exited.  Also it makes sure the idle loop
        returns after each state entry, allowing the appropriate dynticks code
        to run.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 3a7fd42f9825c3b03e364ca59baa751bb350775f
    Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Apr 26 00:03:59 2007 -0700
    
        cpuidle: hang fix
    
        Prevent hang on x86-64, when ACPI processor driver is added as a module on
        a system that does not support C-states.
    
        x86-64 expects all idle handlers to enable interrupts before returning from
        idle handler.  This is due to enter_idle(), exit_idle() races.  Make
        cpuidle_idle_call() confirm to this when there is no pm_idle_old.
    
        Also, cpuidle look at the return values of attch_driver() and set
        current_driver to NULL if attach fails on all CPUs.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 4893339a142afbd5b7c01ffadfd53d14746e858e
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:09 2007 +0800
    
        cpuidle: add support for max_cstate limit
    
        With CPUIDLE framework, the max_cstate (to limit max cpu c-state)
        parameter is ingored. Some systems require it to ignore C2/C3
        and some drivers like ipw require it too.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 43bbbbe1cb998cbd2df656f55bb3bfe30f30e7d1
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:13 2007 +0800
    
        cpuidle: add cpuidle_fore_redetect_devices API
    
        add cpuidle_force_redetect_devices API,
        which forces all CPU redetect idle states.
        Next patch will use it.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit d1edadd608f24836def5ec483d2edccfb37b1d19
    Author: Shaohua Li <shaohua.li@intel.com>
    Date:   Thu Apr 26 10:40:01 2007 +0800
    
        cpuidle: fix sysfs related issue
    
        Fix the cpuidle sysfs issue.
        a. make kobject dynamicaly allocated
        b. fixed sysfs init issue to avoid suspend/resume issue
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 7169a5cc0d67b263978859672e86c13c23a5570d
    Author: Randy Dunlap <randy.dunlap@oracle.com>
    Date:   Wed Mar 28 22:52:53 2007 -0400
    
        cpuidle: 1-bit field must be unsigned
    
        A 1-bit bitfield has no room for a sign bit.
        drivers/cpuidle/governors/ladder.c:54:16: error: dubious bitfield without explicit `signed' or `unsigned'
    
        Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 4658620158dc2fbd9e4bcb213c5b6fb5d05ba7d4
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Wed Mar 28 22:52:41 2007 -0400
    
        cpuidle: fix boot hang
    
        Patch for cpuidle boot hang reported by Larry Finger here.
        http://www.ussg.iu.edu/hypermail/linux/kernel/0703.2/2025.html
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Larry Finger <larry.finger@lwfinger.net>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit c17e168aa6e5fe3851baaae8df2fbc1cf11443a9
    Author: Len Brown <len.brown@intel.com>
    Date:   Wed Mar 7 04:37:53 2007 -0500
    
        cpuidle: ladder does not depend on ACPI
    
        build fix for CONFIG_ACPI=n
    
        In file included from drivers/cpuidle/governors/ladder.c:21:
        include/acpi/processor.h:88: error: expected specifier-qualifier-list before âacpi_integerâ
        include/acpi/processor.h:106: error: expected specifier-qualifier-list before âacpi_integerâ
        include/acpi/processor.h:168: error: expected specifier-qualifier-list before âacpi_handleâ
    
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8c91d958246bde68db0c3f0c57b535962ce861cb
    Author: Adrian Bunk <bunk@stusta.de>
    Date:   Tue Mar 6 02:29:40 2007 -0800
    
        cpuidle: make code static
    
        This patch makes the following needlessly global code static:
        - driver.c: __cpuidle_find_driver()
        - governor.c: __cpuidle_find_governor()
        - ladder.c: struct ladder_governor
    
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Adam Belay <abelay@novell.com>
        Cc: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 0c39dc3187094c72c33ab65a64d2017b21f372d2
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Wed Mar 7 02:38:22 2007 -0500
    
        cpu_idle: fix build break
    
        This patch fixes a build breakage with !CONFIG_HOTPLUG_CPU and
        CONFIG_CPU_IDLE.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adrian Bunk <bunk@stusta.de>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 8112e3b115659b07df340ef170515799c0105f82
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Tue Mar 6 02:29:39 2007 -0800
    
        cpuidle: build fix for !CPU_IDLE
    
        Fix the compile issues when CPU_IDLE is not configured.
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Cc: Adam Belay <abelay@novell.com>
        Cc: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 1eb4431e9599cd25e0d9872f3c2c8986821839dd
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:54:57 2007 -0800
    
        cpuidle take2: Basic documentation for cpuidle
    
        Documentation for cpuidle infrastructure
    
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit ef5f15a8b79123a047285ec2e3899108661df779
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:54:03 2007 -0800
    
        cpuidle take2: Hookup ACPI C-states driver with cpuidle
    
        Hookup ACPI C-states onto generic cpuidle infrastructure.
    
        drivers/acpi/procesor_idle.c is now a ACPI C-states driver that registers as
        a driver in cpuidle infrastructure and the policy part is removed from
        drivers/acpi/processor_idle.c. We use governor in cpuidle instead.
    
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    commit 987196fa82d4db52c407e8c9d5dec884ba602183
    Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Date:   Thu Feb 22 13:52:57 2007 -0800
    
        cpuidle take2: Core cpuidle infrastructure
    
        Announcing 'cpuidle', a new CPU power management infrastructure to manage
        idle CPUs in a clean and efficient manner.
        cpuidle separates out the drivers that can provide support for multiple types
        of idle states and policy governors that decide on what idle state to use
        at run time.
        A cpuidle driver can support multiple idle states based on parameters like
        varying power consumption, wakeup latency, etc (ACPI C-states for example).
        A cpuidle governor can be usage model specific (laptop, server,
        laptop on battery etc).
        Main advantage of the infrastructure being, it allows independent development
        of drivers and governors and allows for better CPU power management.
    
        A huge thanks to Adam Belay and Shaohua Li who were part of this mini-project
        since its beginning and are greatly responsible for this patchset.
    
        This patch:
    
        Core cpuidle infrastructure.
        Introduces a new abstraction layer for cpuidle:
        * which manages drivers that can support multiple idles states. Drivers
          can be generic or particular to specific hardware/platform
        * allows pluging in multiple policy governors that can take idle state policy
          decision
        * The core also has a set of sysfs interfaces with which administrato can know
          about supported drivers and governors and switch them at run time.
    
        Signed-off-by: Adam Belay <abelay@novell.com>
        Signed-off-by: Shaohua Li <shaohua.li@intel.com>
        Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
        Signed-off-by: Len Brown <len.brown@intel.com>
    
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
new file mode 100644
index 000000000000..299d45c3bdd2
--- /dev/null
+++ b/drivers/cpuidle/governors/menu.c
@@ -0,0 +1,137 @@
+/*
+ * menu.c - the menu idle governor
+ *
+ * Copyright (C) 2006-2007 Adam Belay <abelay@novell.com>
+ *
+ * This code is licenced under the GPL.
+ */
+
+#include <linux/kernel.h>
+#include <linux/cpuidle.h>
+#include <linux/latency.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+
+#define BREAK_FUZZ	4	/* 4 us */
+
+struct menu_device {
+	int		last_state_idx;
+
+	unsigned int	expected_us;
+	unsigned int	predicted_us;
+	unsigned int	last_measured_us;
+	unsigned int	elapsed_us;
+};
+
+static DEFINE_PER_CPU(struct menu_device, menu_devices);
+
+/**
+ * menu_select - selects the next idle state to enter
+ * @dev: the CPU
+ */
+static int menu_select(struct cpuidle_device *dev)
+{
+	struct menu_device *data = &__get_cpu_var(menu_devices);
+	int i;
+
+	/* determine the expected residency time */
+	data->expected_us =
+		(u32) ktime_to_ns(tick_nohz_get_sleep_length()) / 1000;
+
+	/* find the deepest idle state that satisfies our constraints */
+	for (i = 1; i < dev->state_count; i++) {
+		struct cpuidle_state *s = &dev->states[i];
+
+		if (s->target_residency > data->expected_us)
+			break;
+		if (s->target_residency > data->predicted_us)
+			break;
+		if (s->exit_latency > system_latency_constraint())
+			break;
+	}
+
+	data->last_state_idx = i - 1;
+	return i - 1;
+}
+
+/**
+ * menu_reflect - attempts to guess what happened after entry
+ * @dev: the CPU
+ *
+ * NOTE: it's important to be fast here because this operation will add to
+ *       the overall exit latency.
+ */
+static void menu_reflect(struct cpuidle_device *dev)
+{
+	struct menu_device *data = &__get_cpu_var(menu_devices);
+	int last_idx = data->last_state_idx;
+	unsigned int measured_us =
+		cpuidle_get_last_residency(dev) + data->elapsed_us;
+	struct cpuidle_state *target = &dev->states[last_idx];
+
+	/*
+	 * Ugh, this idle state doesn't support residency measurements, so we
+	 * are basically lost in the dark.  As a compromise, assume we slept
+	 * for one full standard timer tick.  However, be aware that this
+	 * could potentially result in a suboptimal state transition.
+	 */
+	if (!(target->flags & CPUIDLE_FLAG_TIME_VALID))
+		measured_us = USEC_PER_SEC / HZ;
+
+	/* Predict time remaining until next break event */
+	if (measured_us + BREAK_FUZZ < data->expected_us - target->exit_latency) {
+		data->predicted_us = max(measured_us, data->last_measured_us);
+		data->last_measured_us = measured_us;
+		data->elapsed_us = 0;
+	} else {
+		if (data->elapsed_us < data->elapsed_us + measured_us)
+			data->elapsed_us = measured_us;
+		else
+			data->elapsed_us = -1;
+		data->predicted_us = max(measured_us, data->last_measured_us);
+	}
+}
+
+/**
+ * menu_enable_device - scans a CPU's states and does setup
+ * @dev: the CPU
+ */
+static int menu_enable_device(struct cpuidle_device *dev)
+{
+	struct menu_device *data = &per_cpu(menu_devices, dev->cpu);
+
+	memset(data, 0, sizeof(struct menu_device));
+
+	return 0;
+}
+
+static struct cpuidle_governor menu_governor = {
+	.name =		"menu",
+	.rating =	20,
+	.enable =	menu_enable_device,
+	.select =	menu_select,
+	.reflect =	menu_reflect,
+	.owner =	THIS_MODULE,
+};
+
+/**
+ * init_menu - initializes the governor
+ */
+static int __init init_menu(void)
+{
+	return cpuidle_register_governor(&menu_governor);
+}
+
+/**
+ * exit_menu - exits the governor
+ */
+static void __exit exit_menu(void)
+{
+	cpuidle_unregister_governor(&menu_governor);
+}
+
+MODULE_LICENSE("GPL");
+module_init(init_menu);
+module_exit(exit_menu);
