commit 72d447113bb751ded97b2e2c38f886e4a4139082
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 29 16:30:19 2020 +0200

    nvme: fix a crash in nvme_mpath_add_disk
    
    For private namespaces ns->head_disk is NULL, so add a NULL check
    before updating the BDI capabilities.
    
    Fixes: b2ce4d90690b ("nvme-multipath: set bdi capabilities once")
    Reported-by: Avinash M N <Avinash.M.N@wdc.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Max Gurtovoy <maxg@mellanox.com>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 18d084ed497e..66509472fe06 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -672,10 +672,11 @@ void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 	}
 
 	if (bdi_cap_stable_pages_required(ns->queue->backing_dev_info)) {
-		struct backing_dev_info *info =
-					ns->head->disk->queue->backing_dev_info;
+		struct gendisk *disk = ns->head->disk;
 
-		info->capabilities |= BDI_CAP_STABLE_WRITES;
+		if (disk)
+			disk->queue->backing_dev_info->capabilities |=
+					BDI_CAP_STABLE_WRITES;
 	}
 }
 

commit c31244669f57963b6ce133a5555b118fc50aec95
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Jun 24 01:53:12 2020 -0700

    nvme-multipath: fix bogus request queue reference put
    
    The mpath disk node takes a reference on the request mpath
    request queue when adding live path to the mpath gendisk.
    However if we connected to an inaccessible path device_add_disk
    is not called, so if we disconnect and remove the mpath gendisk
    we endup putting an reference on the request queue that was
    never taken [1].
    
    Fix that to check if we ever added a live path (using
    NVME_NS_HEAD_HAS_DISK flag) and if not, clear the disk->queue
    reference.
    
    [1]:
    ------------[ cut here ]------------
    refcount_t: underflow; use-after-free.
    WARNING: CPU: 1 PID: 1372 at lib/refcount.c:28 refcount_warn_saturate+0xa6/0xf0
    CPU: 1 PID: 1372 Comm: nvme Tainted: G           O      5.7.0-rc2+ #3
    Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.13.0-1ubuntu1 04/01/2014
    RIP: 0010:refcount_warn_saturate+0xa6/0xf0
    RSP: 0018:ffffb29e8053bdc0 EFLAGS: 00010282
    RAX: 0000000000000000 RBX: ffff8b7a2f4fc060 RCX: 0000000000000007
    RDX: 0000000000000007 RSI: 0000000000000092 RDI: ffff8b7a3ec99980
    RBP: ffff8b7a2f4fc000 R08: 00000000000002e1 R09: 0000000000000004
    R10: 0000000000000000 R11: 0000000000000001 R12: 0000000000000000
    R13: fffffffffffffff2 R14: ffffb29e8053bf08 R15: ffff8b7a320e2da0
    FS:  00007f135d4ca800(0000) GS:ffff8b7a3ec80000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00005651178c0c30 CR3: 000000003b650005 CR4: 0000000000360ee0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     disk_release+0xa2/0xc0
     device_release+0x28/0x80
     kobject_put+0xa5/0x1b0
     nvme_put_ns_head+0x26/0x70 [nvme_core]
     nvme_put_ns+0x30/0x60 [nvme_core]
     nvme_remove_namespaces+0x9b/0xe0 [nvme_core]
     nvme_do_delete_ctrl+0x43/0x5c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write+0xc1/0x1a0
     vfs_write+0xb6/0x1a0
     ksys_write+0x5f/0xe0
     do_syscall_64+0x52/0x1a0
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Reported-by: Anton Eidelman <anton@lightbitslabs.com>
    Tested-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 79c8caced81f..18d084ed497e 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -690,6 +690,14 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	kblockd_schedule_work(&head->requeue_work);
 	flush_work(&head->requeue_work);
 	blk_cleanup_queue(head->disk->queue);
+	if (!test_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {
+		/*
+		 * if device_add_disk wasn't called, prevent
+		 * disk release to put a bogus reference on the
+		 * request queue
+		 */
+		head->disk->queue = NULL;
+	}
 	put_disk(head->disk);
 }
 

commit d8a22f85609fadb46ba699e0136cc3ebdeebff79
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Wed Jun 24 01:53:11 2020 -0700

    nvme-multipath: fix deadlock due to head->lock
    
    In the following scenario scan_work and ana_work will deadlock:
    
    When scan_work calls nvme_mpath_add_disk() this holds ana_lock
    and invokes nvme_parse_ana_log(), which may issue IO
    in device_add_disk() and hang waiting for an accessible path.
    
    While nvme_mpath_set_live() only called when nvme_state_is_live(),
    a transition may cause NVME_SC_ANA_TRANSITION and requeue the IO.
    
    Since nvme_mpath_set_live() holds ns->head->lock, an ana_work on
    ANY ctrl will not be able to complete nvme_mpath_set_live()
    on the same ns->head, which is required in order to update
    the new accessible path and remove NVME_NS_ANA_PENDING..
    Therefore IO never completes: deadlock [1].
    
    Fix:
    Move device_add_disk out of the head->lock and protect it with an
    atomic test_and_set for a new NVME_NS_HEAD_HAS_DISK bit.
    
    [1]:
    kernel: INFO: task kworker/u8:2:160 blocked for more than 120 seconds.
    kernel:       Tainted: G           OE     5.3.5-050305-generic #201910071830
    kernel: "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    kernel: kworker/u8:2    D    0   160      2 0x80004000
    kernel: Workqueue: nvme-wq nvme_ana_work [nvme_core]
    kernel: Call Trace:
    kernel:  __schedule+0x2b9/0x6c0
    kernel:  schedule+0x42/0xb0
    kernel:  schedule_preempt_disabled+0xe/0x10
    kernel:  __mutex_lock.isra.0+0x182/0x4f0
    kernel:  __mutex_lock_slowpath+0x13/0x20
    kernel:  mutex_lock+0x2e/0x40
    kernel:  nvme_update_ns_ana_state+0x22/0x60 [nvme_core]
    kernel:  nvme_update_ana_state+0xca/0xe0 [nvme_core]
    kernel:  nvme_parse_ana_log+0xa1/0x180 [nvme_core]
    kernel:  nvme_read_ana_log+0x76/0x100 [nvme_core]
    kernel:  nvme_ana_work+0x15/0x20 [nvme_core]
    kernel:  process_one_work+0x1db/0x380
    kernel:  worker_thread+0x4d/0x400
    kernel:  kthread+0x104/0x140
    kernel:  ret_from_fork+0x35/0x40
    kernel: INFO: task kworker/u8:4:439 blocked for more than 120 seconds.
    kernel:       Tainted: G           OE     5.3.5-050305-generic #201910071830
    kernel: "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    kernel: kworker/u8:4    D    0   439      2 0x80004000
    kernel: Workqueue: nvme-wq nvme_scan_work [nvme_core]
    kernel: Call Trace:
    kernel:  __schedule+0x2b9/0x6c0
    kernel:  schedule+0x42/0xb0
    kernel:  io_schedule+0x16/0x40
    kernel:  do_read_cache_page+0x438/0x830
    kernel:  read_cache_page+0x12/0x20
    kernel:  read_dev_sector+0x27/0xc0
    kernel:  read_lba+0xc1/0x220
    kernel:  efi_partition+0x1e6/0x708
    kernel:  check_partition+0x154/0x244
    kernel:  rescan_partitions+0xae/0x280
    kernel:  __blkdev_get+0x40f/0x560
    kernel:  blkdev_get+0x3d/0x140
    kernel:  __device_add_disk+0x388/0x480
    kernel:  device_add_disk+0x13/0x20
    kernel:  nvme_mpath_set_live+0x119/0x140 [nvme_core]
    kernel:  nvme_update_ns_ana_state+0x5c/0x60 [nvme_core]
    kernel:  nvme_mpath_add_disk+0xbe/0x100 [nvme_core]
    kernel:  nvme_validate_ns+0x396/0x940 [nvme_core]
    kernel:  nvme_scan_work+0x256/0x390 [nvme_core]
    kernel:  process_one_work+0x1db/0x380
    kernel:  worker_thread+0x4d/0x400
    kernel:  kthread+0x104/0x140
    kernel:  ret_from_fork+0x35/0x40
    
    Fixes: 0d0b660f214d ("nvme: add ANA support")
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5c48a38aebf8..79c8caced81f 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -412,11 +412,11 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 	if (!head->disk)
 		return;
 
-	mutex_lock(&head->lock);
-	if (!(head->disk->flags & GENHD_FL_UP))
+	if (!test_and_set_bit(NVME_NSHEAD_DISK_LIVE, &head->flags))
 		device_add_disk(&head->subsys->dev, head->disk,
 				nvme_ns_id_attr_groups);
 
+	mutex_lock(&head->lock);
 	if (nvme_path_is_optimized(ns)) {
 		int node, srcu_idx;
 

commit e164471dcf19308d154adb69e7760d8ba426a77f
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Jun 24 01:53:10 2020 -0700

    nvme: don't protect ns mutation with ns->head->lock
    
    Right now ns->head->lock is protecting namespace mutation
    which is wrong and unneeded. Move it to only protect
    against head mutations. While we're at it, remove unnecessary
    ns->head reference as we already have head pointer.
    
    The problem with this is that the head->lock spans
    mpath disk node I/O that may block under some conditions (if
    for example the controller is disconnecting or the path
    became inaccessible), The locking scheme does not allow any
    other path to enable itself, preventing blocked I/O to complete
    and forward-progress from there.
    
    This is a preparation patch for the fix in a subsequent patch
    where the disk I/O will also be done outside the head->lock.
    
    Fixes: 0d0b660f214d ("nvme: add ANA support")
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 0ef183c1153e..5c48a38aebf8 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -409,11 +409,10 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 {
 	struct nvme_ns_head *head = ns->head;
 
-	lockdep_assert_held(&ns->head->lock);
-
 	if (!head->disk)
 		return;
 
+	mutex_lock(&head->lock);
 	if (!(head->disk->flags & GENHD_FL_UP))
 		device_add_disk(&head->subsys->dev, head->disk,
 				nvme_ns_id_attr_groups);
@@ -426,9 +425,10 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 			__nvme_find_path(head, node);
 		srcu_read_unlock(&head->srcu, srcu_idx);
 	}
+	mutex_unlock(&head->lock);
 
-	synchronize_srcu(&ns->head->srcu);
-	kblockd_schedule_work(&ns->head->requeue_work);
+	synchronize_srcu(&head->srcu);
+	kblockd_schedule_work(&head->requeue_work);
 }
 
 static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
@@ -483,14 +483,12 @@ static inline bool nvme_state_is_live(enum nvme_ana_state state)
 static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
 		struct nvme_ns *ns)
 {
-	mutex_lock(&ns->head->lock);
 	ns->ana_grpid = le32_to_cpu(desc->grpid);
 	ns->ana_state = desc->state;
 	clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
 
 	if (nvme_state_is_live(ns->ana_state))
 		nvme_mpath_set_live(ns);
-	mutex_unlock(&ns->head->lock);
 }
 
 static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
@@ -669,10 +667,8 @@ void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 			nvme_update_ns_ana_state(&desc, ns);
 		}
 	} else {
-		mutex_lock(&ns->head->lock);
 		ns->ana_state = NVME_ANA_OPTIMIZED; 
 		nvme_mpath_set_live(ns);
-		mutex_unlock(&ns->head->lock);
 	}
 
 	if (bdi_cap_stable_pages_required(ns->queue->backing_dev_info)) {

commit 489dd102a2c7c94d783a35f9412eb085b8da1aa4
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Wed Jun 24 01:53:09 2020 -0700

    nvme-multipath: fix deadlock between ana_work and scan_work
    
    When scan_work calls nvme_mpath_add_disk() this holds ana_lock
    and invokes nvme_parse_ana_log(), which may issue IO
    in device_add_disk() and hang waiting for an accessible path.
    While nvme_mpath_set_live() only called when nvme_state_is_live(),
    a transition may cause NVME_SC_ANA_TRANSITION and requeue the IO.
    
    In order to recover and complete the IO ana_work on the same ctrl
    should be able to update the path state and remove NVME_NS_ANA_PENDING.
    
    The deadlock occurs because scan_work keeps holding ana_lock,
    so ana_work hangs [1].
    
    Fix:
    Now nvme_mpath_add_disk() uses nvme_parse_ana_log() to obtain a copy
    of the ANA group desc, and then calls nvme_update_ns_ana_state() without
    holding ana_lock.
    
    [1]:
    kernel: Workqueue: nvme-wq nvme_scan_work [nvme_core]
    kernel: Call Trace:
    kernel:  __schedule+0x2b9/0x6c0
    kernel:  schedule+0x42/0xb0
    kernel:  io_schedule+0x16/0x40
    kernel:  do_read_cache_page+0x438/0x830
    kernel:  read_cache_page+0x12/0x20
    kernel:  read_dev_sector+0x27/0xc0
    kernel:  read_lba+0xc1/0x220
    kernel:  efi_partition+0x1e6/0x708
    kernel:  check_partition+0x154/0x244
    kernel:  rescan_partitions+0xae/0x280
    kernel:  __blkdev_get+0x40f/0x560
    kernel:  blkdev_get+0x3d/0x140
    kernel:  __device_add_disk+0x388/0x480
    kernel:  device_add_disk+0x13/0x20
    kernel:  nvme_mpath_set_live+0x119/0x140 [nvme_core]
    kernel:  nvme_update_ns_ana_state+0x5c/0x60 [nvme_core]
    kernel:  nvme_set_ns_ana_state+0x1e/0x30 [nvme_core]
    kernel:  nvme_parse_ana_log+0xa1/0x180 [nvme_core]
    kernel:  nvme_mpath_add_disk+0x47/0x90 [nvme_core]
    kernel:  nvme_validate_ns+0x396/0x940 [nvme_core]
    kernel:  nvme_scan_work+0x24f/0x380 [nvme_core]
    kernel:  process_one_work+0x1db/0x380
    kernel:  worker_thread+0x249/0x400
    kernel:  kthread+0x104/0x140
    
    kernel: Workqueue: nvme-wq nvme_ana_work [nvme_core]
    kernel: Call Trace:
    kernel:  __schedule+0x2b9/0x6c0
    kernel:  schedule+0x42/0xb0
    kernel:  schedule_preempt_disabled+0xe/0x10
    kernel:  __mutex_lock.isra.0+0x182/0x4f0
    kernel:  ? __switch_to_asm+0x34/0x70
    kernel:  ? select_task_rq_fair+0x1aa/0x5c0
    kernel:  ? kvm_sched_clock_read+0x11/0x20
    kernel:  ? sched_clock+0x9/0x10
    kernel:  __mutex_lock_slowpath+0x13/0x20
    kernel:  mutex_lock+0x2e/0x40
    kernel:  nvme_read_ana_log+0x3a/0x100 [nvme_core]
    kernel:  nvme_ana_work+0x15/0x20 [nvme_core]
    kernel:  process_one_work+0x1db/0x380
    kernel:  worker_thread+0x4d/0x400
    kernel:  kthread+0x104/0x140
    kernel:  ? process_one_work+0x380/0x380
    kernel:  ? kthread_park+0x80/0x80
    kernel:  ret_from_fork+0x35/0x40
    
    Fixes: 0d0b660f214d ("nvme: add ANA support")
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index da78e499947a..0ef183c1153e 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -640,26 +640,34 @@ static ssize_t ana_state_show(struct device *dev, struct device_attribute *attr,
 }
 DEVICE_ATTR_RO(ana_state);
 
-static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
+static int nvme_lookup_ana_group_desc(struct nvme_ctrl *ctrl,
 		struct nvme_ana_group_desc *desc, void *data)
 {
-	struct nvme_ns *ns = data;
+	struct nvme_ana_group_desc *dst = data;
 
-	if (ns->ana_grpid == le32_to_cpu(desc->grpid)) {
-		nvme_update_ns_ana_state(desc, ns);
-		return -ENXIO; /* just break out of the loop */
-	}
+	if (desc->grpid != dst->grpid)
+		return 0;
 
-	return 0;
+	*dst = *desc;
+	return -ENXIO; /* just break out of the loop */
 }
 
 void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 {
 	if (nvme_ctrl_use_ana(ns->ctrl)) {
+		struct nvme_ana_group_desc desc = {
+			.grpid = id->anagrpid,
+			.state = 0,
+		};
+
 		mutex_lock(&ns->ctrl->ana_lock);
 		ns->ana_grpid = le32_to_cpu(id->anagrpid);
-		nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+		nvme_parse_ana_log(ns->ctrl, &desc, nvme_lookup_ana_group_desc);
 		mutex_unlock(&ns->ctrl->ana_lock);
+		if (desc.state) {
+			/* found the group desc: update */
+			nvme_update_ns_ana_state(&desc, ns);
+		}
 	} else {
 		mutex_lock(&ns->head->lock);
 		ns->ana_state = NVME_ANA_OPTIMIZED; 

commit 92decf118f1da4c866515f80387f9cf4d48611d6
Author: Keith Busch <kbusch@kernel.org>
Date:   Fri Apr 3 10:53:46 2020 -0700

    nvme: define constants for identification values
    
    Improve code readability by defining the specification's constants that
    the driver is using when decoding identification payloads.
    
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Reviewed-by: Bart van Assche <bvanassche@acm.org>
    Reviewed-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Acked-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5f5537d0a024..da78e499947a 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -372,7 +372,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	 * We also do this for private namespaces as the namespace sharing data could
 	 * change after a rescan.
 	 */
-	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
+	if (!(ctrl->subsys->cmic & NVME_CTRL_CMIC_MULTI_CTRL) || !multipath)
 		return 0;
 
 	q = blk_alloc_queue(nvme_ns_head_make_request, ctrl->numa_node);
@@ -694,7 +694,8 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	int error;
 
 	/* check if multipath is enabled and we have the capability */
-	if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
+	if (!multipath || !ctrl->subsys ||
+	    !(ctrl->subsys->cmic & NVME_CTRL_CMIC_ANA))
 		return 0;
 
 	ctrl->anacap = id->anacap;

commit 7890b9701b792a4d75b4adef4abe325383ccfca4
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Mar 29 19:41:38 2020 +0200

    nvme-multipath: stop using ->queuedata
    
    nvme-multipath already uses the gendisk private data, not need to
    also set up the request_queue queuedata and use it in one place only.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 9f2844935fdf..5f5537d0a024 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -294,7 +294,7 @@ static bool nvme_available_path(struct nvme_ns_head *head)
 static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 		struct bio *bio)
 {
-	struct nvme_ns_head *head = q->queuedata;
+	struct nvme_ns_head *head = bio->bi_disk->private_data;
 	struct device *dev = disk_to_dev(head->disk);
 	struct nvme_ns *ns;
 	blk_qc_t ret = BLK_QC_T_NONE;
@@ -378,7 +378,6 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	q = blk_alloc_queue(nvme_ns_head_make_request, ctrl->numa_node);
 	if (!q)
 		goto out;
-	q->queuedata = head;
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);

commit b2ce4d90690bd29ce5b554e203cd03682dd59697
Author: Keith Busch <kbusch@kernel.org>
Date:   Thu Apr 9 09:09:04 2020 -0700

    nvme-multipath: set bdi capabilities once
    
    The queues' backing device info capabilities don't change with each
    namespace revalidation. Set it only when each path's request_queue
    is initially added to a multipath queue.
    
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 54603bd3e02d..9f2844935fdf 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -3,6 +3,7 @@
  * Copyright (c) 2017-2018 Christoph Hellwig.
  */
 
+#include <linux/backing-dev.h>
 #include <linux/moduleparam.h>
 #include <trace/events/block.h>
 #include "nvme.h"
@@ -666,6 +667,13 @@ void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
 		nvme_mpath_set_live(ns);
 		mutex_unlock(&ns->head->lock);
 	}
+
+	if (bdi_cap_stable_pages_required(ns->queue->backing_dev_info)) {
+		struct backing_dev_info *info =
+					ns->head->disk->queue->backing_dev_info;
+
+		info->capabilities |= BDI_CAP_STABLE_WRITES;
+	}
 }
 
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)

commit 657f1975e9d9c880fa13030e88ba6cc84964f1db
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Thu Apr 2 09:34:54 2020 -0700

    nvme: fix deadlock caused by ANA update wrong locking
    
    The deadlock combines 4 flows in parallel:
    - ns scanning (triggered from reconnect)
    - request timeout
    - ANA update (triggered from reconnect)
    - I/O coming into the mpath device
    
    (1) ns scanning triggers disk revalidation -> update disk info ->
        freeze queue -> but blocked, due to (2)
    
    (2) timeout handler reference the g_usage_counter - > but blocks in
        the transport .timeout() handler, due to (3)
    
    (3) the transport timeout handler (indirectly) calls nvme_stop_queue() ->
        which takes the (down_read) namespaces_rwsem - > but blocks, due to (4)
    
    (4) ANA update takes the (down_write) namespaces_rwsem -> calls
        nvme_mpath_set_live() -> which synchronize the ns_head srcu
        (see commit 504db087aacc) -> but blocks, due to (5)
    
    (5) I/O came into nvme_mpath_make_request -> took srcu_read_lock ->
        direct_make_request > blk_queue_enter -> but blocked, due to (1)
    
    ==> the request queue is under freeze -> deadlock.
    
    The fix is making ANA update take a read lock as the namespaces list
    is not manipulated, it is just the ns and ns->head that are being
    updated (which is protected with the ns->head lock).
    
    Fixes: 0d0b660f214dc ("nvme: add ANA support")
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Keith Busch <kbusch@kernel.org>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 61bf87592570..54603bd3e02d 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -510,7 +510,7 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 	if (!nr_nsids)
 		return 0;
 
-	down_write(&ctrl->namespaces_rwsem);
+	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
 		unsigned nsid = le32_to_cpu(desc->nsids[n]);
 
@@ -521,7 +521,7 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 		if (++n == nr_nsids)
 			break;
 	}
-	up_write(&ctrl->namespaces_rwsem);
+	up_read(&ctrl->namespaces_rwsem);
 	return 0;
 }
 

commit 1592614838cb52f4313ceff64894e2ca78591498
Merge: 10f36b1e80a9 766c3297d7e1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 11:43:51 2020 -0700

    Merge tag 'for-5.7/drivers-2020-03-29' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
    
     - floppy driver cleanup series from Willy
    
     - NVMe updates and fixes (Various)
    
     - null_blk trace improvements (Chaitanya)
    
     - bcache fixes (Coly)
    
     - md fixes (via Song)
    
     - loop block size change optimizations (Martijn)
    
     - scnprintf() use (Takashi)
    
    * tag 'for-5.7/drivers-2020-03-29' of git://git.kernel.dk/linux-block: (81 commits)
      null_blk: add trace in null_blk_zoned.c
      null_blk: add tracepoint helpers for zoned mode
      block: add a zone condition debug helper
      nvme: cleanup namespace identifier reporting in nvme_init_ns_head
      nvme: rename __nvme_find_ns_head to nvme_find_ns_head
      nvme: refactor nvme_identify_ns_descs error handling
      nvme-tcp: Add warning on state change failure at nvme_tcp_setup_ctrl
      nvme-rdma: Add warning on state change failure at nvme_rdma_setup_ctrl
      nvme: Fix controller creation races with teardown flow
      nvme: Make nvme_uninit_ctrl symmetric to nvme_init_ctrl
      nvme: Fix ctrl use-after-free during sysfs deletion
      nvme-pci: Re-order nvme_pci_free_ctrl
      nvme: Remove unused return code from nvme_delete_ctrl_sync
      nvme: Use nvme_state_terminal helper
      nvme: release ida resources
      nvme: Add compat_ioctl handler for NVME_IOCTL_SUBMIT_IO
      nvmet-tcp: optimize tcp stack TX when data digest is used
      nvme-fabrics: Use scnprintf() for avoiding potential buffer overflow
      nvme-multipath: do not reset on unknown status
      nvmet-rdma: allocate RW ctxs according to mdts
      ...

commit 3d745ea5b095a3985129e162900b7e6c22518a9d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Mar 27 09:30:11 2020 +0100

    block: simplify queue allocation
    
    Current make_request based drivers use either blk_alloc_queue_node or
    blk_alloc_queue to allocate a queue, and then set up the make_request_fn
    function pointer and a few parameters using the blk_queue_make_request
    helper.  Simplify this by passing the make_request pointer to
    blk_alloc_queue, and while at it merge the _node variant into the main
    helper by always passing a node_id, and remove the superfluous gfp_mask
    parameter.  A lower-level __blk_alloc_queue is kept for the blk-mq case.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index a11900cf3a36..a38d7f196aba 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -377,11 +377,10 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
 		return 0;
 
-	q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+	q = blk_alloc_queue(nvme_ns_head_make_request, ctrl->numa_node);
 	if (!q)
 		goto out;
 	q->queuedata = head;
-	blk_queue_make_request(q, nvme_ns_head_make_request);
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);

commit 764e9332098c0e60251386a507fe46ac91276120
Author: John Meneghini <johnm@netapp.com>
Date:   Thu Feb 20 10:05:38 2020 +0900

    nvme-multipath: do not reset on unknown status
    
    The nvme multipath error handling defaults to controller reset if the
    error is unknown. There are, however, no existing nvme status codes that
    indicate a reset should be used, and resetting causes unnecessary
    disruption to the rest of IO.
    
    Change nvme's error handling to first check if failover should happen.
    If not, let the normal error handling take over rather than reset the
    controller.
    
    Based-on-a-patch-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Signed-off-by: John Meneghini <johnm@netapp.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index a11900cf3a36..90dd1d641b7b 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -64,17 +64,12 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 	}
 }
 
-void nvme_failover_req(struct request *req)
+bool nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
 	u16 status = nvme_req(req)->status;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ns->head->requeue_lock, flags);
-	blk_steal_bios(&ns->head->requeue_list, req);
-	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
-	blk_mq_end_request(req, 0);
-
 	switch (status & 0x7ff) {
 	case NVME_SC_ANA_TRANSITION:
 	case NVME_SC_ANA_INACCESSIBLE:
@@ -103,15 +98,17 @@ void nvme_failover_req(struct request *req)
 		nvme_mpath_clear_current_path(ns);
 		break;
 	default:
-		/*
-		 * Reset the controller for any non-ANA error as we don't know
-		 * what caused the error.
-		 */
-		nvme_reset_ctrl(ns->ctrl);
-		break;
+		/* This was a non-ANA error so follow the normal error path. */
+		return false;
 	}
 
+	spin_lock_irqsave(&ns->head->requeue_lock, flags);
+	blk_steal_bios(&ns->head->requeue_list, req);
+	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
+	blk_mq_end_request(req, 0);
+
 	kblockd_schedule_work(&ns->head->requeue_work);
+	return true;
 }
 
 void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)

commit 3b7830904e17202524bad1974505a9bfc718d31f
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Thu Feb 20 13:29:53 2020 -0700

    nvme-multipath: Fix memory leak with ana_log_buf
    
    kmemleak reports a memory leak with the ana_log_buf allocated by
    nvme_mpath_init():
    
    unreferenced object 0xffff888120e94000 (size 8208):
      comm "nvme", pid 6884, jiffies 4295020435 (age 78786.312s)
        hex dump (first 32 bytes):
          00 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00  ................
          01 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00  ................
        backtrace:
          [<00000000e2360188>] kmalloc_order+0x97/0xc0
          [<0000000079b18dd4>] kmalloc_order_trace+0x24/0x100
          [<00000000f50c0406>] __kmalloc+0x24c/0x2d0
          [<00000000f31a10b9>] nvme_mpath_init+0x23c/0x2b0
          [<000000005802589e>] nvme_init_identify+0x75f/0x1600
          [<0000000058ef911b>] nvme_loop_configure_admin_queue+0x26d/0x280
          [<00000000673774b9>] nvme_loop_create_ctrl+0x2a7/0x710
          [<00000000f1c7a233>] nvmf_dev_write+0xc66/0x10b9
          [<000000004199f8d0>] __vfs_write+0x50/0xa0
          [<0000000065466fef>] vfs_write+0xf3/0x280
          [<00000000b0db9a8b>] ksys_write+0xc6/0x160
          [<0000000082156b91>] __x64_sys_write+0x43/0x50
          [<00000000c34fbb6d>] do_syscall_64+0x77/0x2f0
          [<00000000bbc574c9>] entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    nvme_mpath_init() is called by nvme_init_identify() which is called in
    multiple places (nvme_reset_work(), nvme_passthru_end(), etc). This
    means nvme_mpath_init() may be called multiple times before
    nvme_mpath_uninit() (which is only called on nvme_free_ctrl()).
    
    When nvme_mpath_init() is called multiple times, it overwrites the
    ana_log_buf pointer with a new allocation, thus leaking the previous
    allocation.
    
    To fix this, free ana_log_buf before allocating a new one.
    
    Fixes: 0d0b660f214dc490 ("nvme: add ANA support")
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 797c18337d96..a11900cf3a36 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -715,6 +715,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	}
 
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	kfree(ctrl->ana_log_buf);
 	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
 	if (!ctrl->ana_log_buf) {
 		error = -ENOMEM;

commit 2d53943090c336c9d298638bad292be349e1b9c4
Merge: ff6814b078e3 00b89892c869
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 11:15:41 2019 -0800

    Merge tag 'for-5.5/drivers-20191121' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "Here are the main block driver updates for 5.5. Nothing major in here,
      mostly just fixes. This contains:
    
       - a set of bcache changes via Coly
    
       - MD changes from Song
    
       - loop unmap write-zeroes fix (Darrick)
    
       - spelling fixes (Geert)
    
       - zoned additions cleanups to null_blk/dm (Ajay)
    
       - allow null_blk online submit queue changes (Bart)
    
       - NVMe changes via Keith, nothing major here either"
    
    * tag 'for-5.5/drivers-20191121' of git://git.kernel.dk/linux-block: (56 commits)
      Revert "bcache: fix fifo index swapping condition in journal_pin_cmp()"
      drivers/md/raid5-ppl.c: use the new spelling of RWH_WRITE_LIFE_NOT_SET
      drivers/md/raid5.c: use the new spelling of RWH_WRITE_LIFE_NOT_SET
      bcache: don't export symbols
      bcache: remove the extra cflags for request.o
      bcache: at least try to shrink 1 node in bch_mca_scan()
      bcache: add idle_max_writeback_rate sysfs interface
      bcache: add code comments in bch_btree_leaf_dirty()
      bcache: fix deadlock in bcache_allocator
      bcache: add code comment bch_keylist_pop() and bch_keylist_pop_front()
      bcache: deleted code comments for dead code in bch_data_insert_keys()
      bcache: add more accurate error messages in read_super()
      bcache: fix static checker warning in bcache_device_free()
      bcache: fix a lost wake-up problem caused by mca_cannibalize_lock
      bcache: fix fifo index swapping condition in journal_pin_cmp()
      md/raid10: prevent access of uninitialized resync_pages offset
      md: avoid invalid memory access for array sb->dev_roles
      md/raid1: avoid soft lockup under high load
      null_blk: add zone open, close, and finish support
      dm: add zone open, close and finish support
      ...

commit 763303a83a095a88c3a8a0d1abf97165db2e8bf5
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Fri Nov 1 17:27:55 2019 -0700

    nvme-multipath: fix crash in nvme_mpath_clear_ctrl_paths
    
    nvme_mpath_clear_ctrl_paths() iterates through
    the ctrl->namespaces list while holding ctrl->scan_lock.
    This does not seem to be the correct way of protecting
    from concurrent list modification.
    
    Specifically, nvme_scan_work() sorts ctrl->namespaces
    AFTER unlocking scan_lock.
    
    This may result in the following (rare) crash in ctrl disconnect
    during scan_work:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000050
        Oops: 0000 [#1] SMP PTI
        CPU: 0 PID: 3995 Comm: nvme 5.3.5-050305-generic
        RIP: 0010:nvme_mpath_clear_current_path+0xe/0x90 [nvme_core]
        ...
        Call Trace:
         nvme_mpath_clear_ctrl_paths+0x3c/0x70 [nvme_core]
         nvme_remove_namespaces+0x35/0xe0 [nvme_core]
         nvme_do_delete_ctrl+0x47/0x90 [nvme_core]
         nvme_sysfs_delete+0x49/0x60 [nvme_core]
         dev_attr_store+0x17/0x30
         sysfs_kf_write+0x3e/0x50
         kernfs_fop_write+0x11e/0x1a0
         __vfs_write+0x1b/0x40
         vfs_write+0xb9/0x1a0
         ksys_write+0x67/0xe0
         __x64_sys_write+0x1a/0x20
         do_syscall_64+0x5a/0x130
         entry_SYSCALL_64_after_hwframe+0x44/0xa9
        RIP: 0033:0x7f8d02bfb154
    
    Fix:
    After taking scan_lock in nvme_mpath_clear_ctrl_paths()
    down_read(&ctrl->namespaces_rwsem) as well to make list traversal safe.
    This will not cause deadlocks because taking scan_lock never happens
    while holding the namespaces_rwsem.
    Moreover, scan work downs namespaces_rwsem in the same order.
    
    Alternative: sort ctrl->namespaces in nvme_scan_work()
    while still holding the scan_lock.
    This would leave nvme_mpath_clear_ctrl_paths() without correct protection
    against ctrl->namespaces modification by anyone other than scan_work.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index fc99a40c1ec4..e0f064dcbd02 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -158,9 +158,11 @@ void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)
 	struct nvme_ns *ns;
 
 	mutex_lock(&ctrl->scan_lock);
+	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list)
 		if (nvme_mpath_clear_current_path(ns))
 			kblockd_schedule_work(&ns->head->requeue_work);
+	up_read(&ctrl->namespaces_rwsem);
 	mutex_unlock(&ctrl->scan_lock);
 }
 

commit 64fab7290dc3561729bbc1e35895a517eb2e549e
Author: Prabhath Sajeepa <psajeepa@purestorage.com>
Date:   Mon Oct 28 16:56:48 2019 -0600

    nvme: Fix parsing of ANA log page
    
    Check validity of offset into ANA log buffer before accessing
    nvme_ana_group_desc. This check ensures the size of ANA log buffer >=
    offset + sizeof(nvme_ana_group_desc)
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Prabhath Sajeepa <psajeepa@purestorage.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 103c04fa7746..682be6195a95 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -445,8 +445,14 @@ static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
 
 	for (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {
 		struct nvme_ana_group_desc *desc = base + offset;
-		u32 nr_nsids = le32_to_cpu(desc->nnsids);
-		size_t nsid_buf_size = nr_nsids * sizeof(__le32);
+		u32 nr_nsids;
+		size_t nsid_buf_size;
+
+		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))
+			return -EINVAL;
+
+		nr_nsids = le32_to_cpu(desc->nnsids);
+		nsid_buf_size = nr_nsids * sizeof(__le32);
 
 		if (WARN_ON_ONCE(desc->grpid == 0))
 			return -EINVAL;
@@ -466,8 +472,6 @@ static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
 			return error;
 
 		offset += nsid_buf_size;
-		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))
-			return -EINVAL;
 	}
 
 	return 0;

commit 2dc3947b53f573e8a75ea9cbec5588df88ca502e
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Sun Oct 13 19:57:35 2019 +0300

    nvme: introduce "Command Aborted By host" status code
    
    Fix the status code of canceled requests initiated by the host according
    to TP4028 (Status Code 0x371):
    "Command Aborted By host: The command was aborted as a result of host
    action (e.g., the host disconnected the Fabric connection)."
    
    Also in a multipath environment, unless otherwise specified, errors of
    this type (path related) should be retried using a different path, if
    one is available.
    
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 30de7efef003..103c04fa7746 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -95,6 +95,7 @@ void nvme_failover_req(struct request *req)
 		}
 		break;
 	case NVME_SC_HOST_PATH_ERROR:
+	case NVME_SC_HOST_ABORTED_CMD:
 		/*
 		 * Temporary transport disruption in talking to the controller.
 		 * Try to send on a new path.

commit 86cccfbf773fafb88898c5627aa3727b02bc4708
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Fri Oct 18 11:32:51 2019 -0700

    nvme-multipath: remove unused groups_only mode in ana log
    
    groups_only mode in nvme_read_ana_log() is no longer used: remove it.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index d320684d25b2..fc99a40c1ec4 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -522,14 +522,13 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 	return 0;
 }
 
-static int nvme_read_ana_log(struct nvme_ctrl *ctrl, bool groups_only)
+static int nvme_read_ana_log(struct nvme_ctrl *ctrl)
 {
 	u32 nr_change_groups = 0;
 	int error;
 
 	mutex_lock(&ctrl->ana_lock);
-	error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA,
-			groups_only ? NVME_ANA_LOG_RGO : 0,
+	error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA, 0,
 			ctrl->ana_log_buf, ctrl->ana_log_size, 0);
 	if (error) {
 		dev_warn(ctrl->device, "Failed to get ANA log: %d\n", error);
@@ -565,7 +564,7 @@ static void nvme_ana_work(struct work_struct *work)
 {
 	struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
 
-	nvme_read_ana_log(ctrl, false);
+	nvme_read_ana_log(ctrl);
 }
 
 static void nvme_anatt_timeout(struct timer_list *t)
@@ -715,7 +714,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 		goto out;
 	}
 
-	error = nvme_read_ana_log(ctrl, false);
+	error = nvme_read_ana_log(ctrl);
 	if (error)
 		goto out_free_ana_log_buf;
 	return 0;

commit af8fd0424713a2adb812d10d55e86718152cf656
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Fri Oct 18 11:32:50 2019 -0700

    nvme-multipath: fix possible io hang after ctrl reconnect
    
    The following scenario results in an IO hang:
    1) ctrl completes a request with NVME_SC_ANA_TRANSITION.
       NVME_NS_ANA_PENDING bit in ns->flags is set and ana_work is triggered.
    2) ana_work: nvme_read_ana_log() tries to get the ANA log page from the ctrl.
       This fails because ctrl disconnects.
       Therefore nvme_update_ns_ana_state() is not called
       and NVME_NS_ANA_PENDING bit in ns->flags is not cleared.
    3) ctrl reconnects: nvme_mpath_init(ctrl,...) calls
       nvme_read_ana_log(ctrl, groups_only=true).
       However, nvme_update_ana_state() does not update namespaces
       because nr_nsids = 0 (due to groups_only mode).
    4) scan_work calls nvme_validate_ns() finds the ns and re-validates OK.
    
    Result:
    The ctrl is now live but NVME_NS_ANA_PENDING bit in ns->flags is still set.
    Consequently ctrl will never be considered a viable path by __nvme_find_path().
    IO will hang if ctrl is the only or the last path to the namespace.
    
    More generally, while ctrl is reconnecting, its ANA state may change.
    And because nvme_mpath_init() requests ANA log in groups_only mode,
    these changes are not propagated to the existing ctrl namespaces.
    This may result in a mal-function or an IO hang.
    
    Solution:
    nvme_mpath_init() will nvme_read_ana_log() with groups_only set to false.
    This will not harm the new ctrl case (no namespaces present),
    and will make sure the ANA state of namespaces gets updated after reconnect.
    
    Note: Another option would be for nvme_mpath_init() to invoke
    nvme_parse_ana_log(..., nvme_set_ns_ana_state) for each existing namespace.
    
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 30de7efef003..d320684d25b2 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -715,7 +715,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 		goto out;
 	}
 
-	error = nvme_read_ana_log(ctrl, true);
+	error = nvme_read_ana_log(ctrl, false);
 	if (error)
 		goto out_free_ana_log_buf;
 	return 0;

commit 7ad67ca5534ee7c958559c4ad610f05c4578e361
Merge: 5260c2b863ef 9c7eddf1b080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 16:57:47 2019 -0700

    Merge tag 'for-5.4/block-2019-09-16' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
    
     - Two NVMe pull requests:
         - ana log parse fix from Anton
         - nvme quirks support for Apple devices from Ben
         - fix missing bio completion tracing for multipath stack devices
           from Hannes and Mikhail
         - IP TOS settings for nvme rdma and tcp transports from Israel
         - rq_dma_dir cleanups from Israel
         - tracing for Get LBA Status command from Minwoo
         - Some nvme-tcp cleanups from Minwoo, Potnuri and Myself
         - Some consolidation between the fabrics transports for handling
           the CAP register
         - reset race with ns scanning fix for fabrics (move fabrics
           commands to a dedicated request queue with a different lifetime
           from the admin request queue)."
         - controller reset and namespace scan races fixes
         - nvme discovery log change uevent support
         - naming improvements from Keith
         - multiple discovery controllers reject fix from James
         - some regular cleanups from various people
    
     - Series fixing (and re-fixing) null_blk debug printing and nr_devices
       checks (Andr√©)
    
     - A few pull requests from Song, with fixes from Andy, Guoqing,
       Guilherme, Neil, Nigel, and Yufen.
    
     - REQ_OP_ZONE_RESET_ALL support (Chaitanya)
    
     - Bio merge handling unification (Christoph)
    
     - Pick default elevator correctly for devices with special needs
       (Damien)
    
     - Block stats fixes (Hou)
    
     - Timeout and support devices nbd fixes (Mike)
    
     - Series fixing races around elevator switching and device add/remove
       (Ming)
    
     - sed-opal cleanups (Revanth)
    
     - Per device weight support for BFQ (Fam)
    
     - Support for blk-iocost, a new model that can properly account cost of
       IO workloads. (Tejun)
    
     - blk-cgroup writeback fixes (Tejun)
    
     - paride queue init fixes (zhengbin)
    
     - blk_set_runtime_active() cleanup (Stanley)
    
     - Block segment mapping optimizations (Bart)
    
     - lightnvm fixes (Hans/Minwoo/YueHaibing)
    
     - Various little fixes and cleanups
    
    * tag 'for-5.4/block-2019-09-16' of git://git.kernel.dk/linux-block: (186 commits)
      null_blk: format pr_* logs with pr_fmt
      null_blk: match the type of parameter nr_devices
      null_blk: do not fail the module load with zero devices
      block: also check RQF_STATS in blk_mq_need_time_stamp()
      block: make rq sector size accessible for block stats
      bfq: Fix bfq linkage error
      raid5: use bio_end_sector in r5_next_bio
      raid5: remove STRIPE_OPS_REQ_PENDING
      md: add feature flag MD_FEATURE_RAID0_LAYOUT
      md/raid0: avoid RAID0 data corruption due to layout confusion.
      raid5: don't set STRIPE_HANDLE to stripe which is in batch list
      raid5: don't increment read_errors on EILSEQ return
      nvmet: fix a wrong error status returned in error log page
      nvme: send discovery log page change events to userspace
      nvme: add uevent variables for controller devices
      nvme: enable aen regardless of the presence of I/O queues
      nvme-fabrics: allow discovery subsystems accept a kato
      nvmet: Use PTR_ERR_OR_ZERO() in nvmet_init_discovery()
      nvme: Remove redundant assignment of cq vector
      nvme: Assign subsys instance from first ctrl
      ...

commit e01f91dff91c7b16a6e3faf2565017d497a73f83
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Fri Aug 16 13:00:10 2019 -0700

    nvme-multipath: fix ana log nsid lookup when nsid is not found
    
    ANA log parsing invokes nvme_update_ana_state() per ANA group desc.
    This updates the state of namespaces with nsids in desc->nsids[].
    
    Both ctrl->namespaces list and desc->nsids[] array are sorted by nsid.
    Hence nvme_update_ana_state() performs a single walk over ctrl->namespaces:
    - if current namespace matches the current desc->nsids[n],
      this namespace is updated, and n is incremented.
    - the process stops when it encounters the end of either
      ctrl->namespaces end or desc->nsids[]
    
    In case desc->nsids[n] does not match any of ctrl->namespaces,
    the remaining nsids following desc->nsids[n] will not be updated.
    Such situation was considered abnormal and generated WARN_ON_ONCE.
    
    However ANA log MAY contain nsids not (yet) found in ctrl->namespaces.
    For example, lets consider the following scenario:
    - nvme0 exposes namespaces with nsids = [2, 3] to the host
    - a new namespace nsid = 1 is added dynamically
    - also, a ANA topology change is triggered
    - NS_CHANGED aen is generated and triggers scan_work
    - before scan_work discovers nsid=1 and creates a namespace, a NOTICE_ANA
      aen was issues and ana_work receives ANA log with nsids=[1, 2, 3]
    
    Result: ana_work fails to update ANA state on existing namespaces [2, 3]
    
    Solution:
    Change the way nvme_update_ana_state() namespace list walk
    checks the current namespace against desc->nsids[n] as follows:
    a) ns->head->ns_id < desc->nsids[n]: keep walking ctrl->namespaces.
    b) ns->head->ns_id == desc->nsids[n]: match, update the namespace
    c) ns->head->ns_id >= desc->nsids[n]: skip to desc->nsids[n+1]
    
    This enables correct operation in the scenario described above.
    This also allows ANA log to contain nsids currently invisible
    to the host, i.e. inactive nsids.
    
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Reviewed-by:   James Smart <james.smart@broadcom.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 4f0d0d12744e..961011abd2ab 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -444,14 +444,16 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 
 	down_write(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
-		if (ns->head->ns_id != le32_to_cpu(desc->nsids[n]))
+		unsigned nsid = le32_to_cpu(desc->nsids[n]);
+
+		if (ns->head->ns_id < nsid)
 			continue;
-		nvme_update_ns_ana_state(desc, ns);
+		if (ns->head->ns_id == nsid)
+			nvme_update_ns_ana_state(desc, ns);
 		if (++n == nr_nsids)
 			break;
 	}
 	up_write(&ctrl->namespaces_rwsem);
-	WARN_ON_ONCE(n < nr_nsids);
 	return 0;
 }
 

commit 504db087aaccdb32af61539916409f7dca31ceb5
Author: Anton Eidelman <anton@lightbitslabs.com>
Date:   Mon Aug 12 23:00:36 2019 +0300

    nvme-multipath: fix possible I/O hang when paths are updated
    
    nvme_state_set_live() making a path available triggers requeue_work
    in order to resubmit requests that ended up on requeue_list when no
    paths were available.
    
    This requeue_work may race with concurrent nvme_ns_head_make_request()
    that do not observe the live path yet.
    Such concurrent requests may by made by either:
    - New IO submission.
    - Requeue_work triggered by nvme_failover_req() or another ana_work.
    
    A race may cause requeue_work capture the state of requeue_list before
    more requests get onto the list. These requests will stay on the list
    forever unless requeue_work is triggered again.
    
    In order to prevent such race, nvme_state_set_live() should
    synchronize_srcu(&head->srcu) before triggering the requeue_work and
    prevent nvme_ns_head_make_request referencing an old snapshot of the
    path list.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Anton Eidelman <anton@lightbitslabs.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 888d4543894e..af831d3d15d0 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -428,6 +428,7 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 		srcu_read_unlock(&head->srcu, srcu_idx);
 	}
 
+	synchronize_srcu(&ns->head->srcu);
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 

commit 0157ec8dad3c8fc9bc9790f76e0831ffdaf2e7f0
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Thu Jul 25 11:56:57 2019 -0700

    nvme: fix controller removal race with scan work
    
    With multipath enabled, nvme_scan_work() can read from the device
    (through nvme_mpath_add_disk()) and hang [1]. However, with fabrics,
    once ctrl->state is set to NVME_CTRL_DELETING, the reads will hang
    (see nvmf_check_ready()) and the mpath stack device make_request
    will block if head->list is not empty. However, when the head->list
    consistst of only DELETING/DEAD controllers, we should actually not
    block, but rather fail immediately.
    
    In addition, before we go ahead and remove the namespaces, make sure
    to clear the current path and kick the requeue list so that the
    request will fast fail upon requeuing.
    
    [1]:
    --
      INFO: task kworker/u4:3:166 blocked for more than 120 seconds.
            Not tainted 5.2.0-rc6-vmlocalyes-00005-g808c8c2dc0cf #316
      "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      kworker/u4:3    D    0   166      2 0x80004000
      Workqueue: nvme-wq nvme_scan_work
      Call Trace:
       __schedule+0x851/0x1400
       schedule+0x99/0x210
       io_schedule+0x21/0x70
       do_read_cache_page+0xa57/0x1330
       read_cache_page+0x4a/0x70
       read_dev_sector+0xbf/0x380
       amiga_partition+0xc4/0x1230
       check_partition+0x30f/0x630
       rescan_partitions+0x19a/0x980
       __blkdev_get+0x85a/0x12f0
       blkdev_get+0x2a5/0x790
       __device_add_disk+0xe25/0x1250
       device_add_disk+0x13/0x20
       nvme_mpath_set_live+0x172/0x2b0
       nvme_update_ns_ana_state+0x130/0x180
       nvme_set_ns_ana_state+0x9a/0xb0
       nvme_parse_ana_log+0x1c3/0x4a0
       nvme_mpath_add_disk+0x157/0x290
       nvme_validate_ns+0x1017/0x1bd0
       nvme_scan_work+0x44d/0x6a0
       process_one_work+0x7d7/0x1240
       worker_thread+0x8e/0xff0
       kthread+0x2c3/0x3b0
       ret_from_fork+0x35/0x40
    
       INFO: task kworker/u4:1:1034 blocked for more than 120 seconds.
            Not tainted 5.2.0-rc6-vmlocalyes-00005-g808c8c2dc0cf #316
      "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      kworker/u4:1    D    0  1034      2 0x80004000
      Workqueue: nvme-delete-wq nvme_delete_ctrl_work
      Call Trace:
       __schedule+0x851/0x1400
       schedule+0x99/0x210
       schedule_timeout+0x390/0x830
       wait_for_completion+0x1a7/0x310
       __flush_work+0x241/0x5d0
       flush_work+0x10/0x20
       nvme_remove_namespaces+0x85/0x3d0
       nvme_do_delete_ctrl+0xb4/0x1e0
       nvme_delete_ctrl_work+0x15/0x20
       process_one_work+0x7d7/0x1240
       worker_thread+0x8e/0xff0
       kthread+0x2c3/0x3b0
       ret_from_fork+0x35/0x40
    --
    
    Reported-by: Logan Gunthorpe <logang@deltatee.com>
    Tested-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Logan Gunthorpe <logang@deltatee.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index b34dcb2288e7..888d4543894e 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -134,18 +134,34 @@ static const char *nvme_ana_state_names[] = {
 	[NVME_ANA_CHANGE]		= "change",
 };
 
-void nvme_mpath_clear_current_path(struct nvme_ns *ns)
+bool nvme_mpath_clear_current_path(struct nvme_ns *ns)
 {
 	struct nvme_ns_head *head = ns->head;
+	bool changed = false;
 	int node;
 
 	if (!head)
-		return;
+		goto out;
 
 	for_each_node(node) {
-		if (ns == rcu_access_pointer(head->current_path[node]))
+		if (ns == rcu_access_pointer(head->current_path[node])) {
 			rcu_assign_pointer(head->current_path[node], NULL);
+			changed = true;
+		}
 	}
+out:
+	return changed;
+}
+
+void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	mutex_lock(&ctrl->scan_lock);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		if (nvme_mpath_clear_current_path(ns))
+			kblockd_schedule_work(&ns->head->requeue_work);
+	mutex_unlock(&ctrl->scan_lock);
 }
 
 static bool nvme_path_is_disabled(struct nvme_ns *ns)
@@ -256,6 +272,24 @@ inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 	return ns;
 }
 
+static bool nvme_available_path(struct nvme_ns_head *head)
+{
+	struct nvme_ns *ns;
+
+	list_for_each_entry_rcu(ns, &head->list, siblings) {
+		switch (ns->ctrl->state) {
+		case NVME_CTRL_LIVE:
+		case NVME_CTRL_RESETTING:
+		case NVME_CTRL_CONNECTING:
+			/* fallthru */
+			return true;
+		default:
+			break;
+		}
+	}
+	return false;
+}
+
 static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 		struct bio *bio)
 {
@@ -282,14 +316,14 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 				      disk_devt(ns->head->disk),
 				      bio->bi_iter.bi_sector);
 		ret = direct_make_request(bio);
-	} else if (!list_empty_careful(&head->list)) {
-		dev_warn_ratelimited(dev, "no path available - requeuing I/O\n");
+	} else if (nvme_available_path(head)) {
+		dev_warn_ratelimited(dev, "no usable path - requeuing I/O\n");
 
 		spin_lock_irq(&head->requeue_lock);
 		bio_list_add(&head->requeue_list, bio);
 		spin_unlock_irq(&head->requeue_lock);
 	} else {
-		dev_warn_ratelimited(dev, "no path - failing I/O\n");
+		dev_warn_ratelimited(dev, "no available path - failing I/O\n");
 
 		bio->bi_status = BLK_STS_IOERR;
 		bio_endio(bio);

commit b9156daeb1601d69007b7e50efcf89d69d72ec1d
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Wed Jul 31 11:00:26 2019 -0700

    nvme: fix a possible deadlock when passthru commands sent to a multipath device
    
    When the user issues a command with side effects, we will end up freezing
    the namespace request queue when updating disk info (and the same for
    the corresponding mpath disk node).
    
    However, we are not freezing the mpath node request queue,
    which means that mpath I/O can still come in and block on blk_queue_enter
    (called from nvme_ns_head_make_request -> direct_make_request).
    
    This is a deadlock, because blk_queue_enter will block until the inner
    namespace request queue is unfroze, but that process is blocked because
    the namespace revalidation is trying to update the mpath disk info
    and freeze its request queue (which will never complete because
    of the I/O that is blocked on blk_queue_enter).
    
    Fix this by freezing all the subsystem nsheads request queues before
    executing the passthru command. Given that these commands are infrequent
    we should not worry about this temporary I/O freeze to keep things sane.
    
    Here is the matching hang traces:
    --
    [ 374.465002] INFO: task systemd-udevd:17994 blocked for more than 122 seconds.
    [ 374.472975] Not tainted 5.2.0-rc3-mpdebug+ #42
    [ 374.478522] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [ 374.487274] systemd-udevd D 0 17994 1 0x00000000
    [ 374.493407] Call Trace:
    [ 374.496145] __schedule+0x2ef/0x620
    [ 374.500047] schedule+0x38/0xa0
    [ 374.503569] blk_queue_enter+0x139/0x220
    [ 374.507959] ? remove_wait_queue+0x60/0x60
    [ 374.512540] direct_make_request+0x60/0x130
    [ 374.517219] nvme_ns_head_make_request+0x11d/0x420 [nvme_core]
    [ 374.523740] ? generic_make_request_checks+0x307/0x6f0
    [ 374.529484] generic_make_request+0x10d/0x2e0
    [ 374.534356] submit_bio+0x75/0x140
    [ 374.538163] ? guard_bio_eod+0x32/0xe0
    [ 374.542361] submit_bh_wbc+0x171/0x1b0
    [ 374.546553] block_read_full_page+0x1ed/0x330
    [ 374.551426] ? check_disk_change+0x70/0x70
    [ 374.556008] ? scan_shadow_nodes+0x30/0x30
    [ 374.560588] blkdev_readpage+0x18/0x20
    [ 374.564783] do_read_cache_page+0x301/0x860
    [ 374.569463] ? blkdev_writepages+0x10/0x10
    [ 374.574037] ? prep_new_page+0x88/0x130
    [ 374.578329] ? get_page_from_freelist+0xa2f/0x1280
    [ 374.583688] ? __alloc_pages_nodemask+0x179/0x320
    [ 374.588947] read_cache_page+0x12/0x20
    [ 374.593142] read_dev_sector+0x2d/0xd0
    [ 374.597337] read_lba+0x104/0x1f0
    [ 374.601046] find_valid_gpt+0xfa/0x720
    [ 374.605243] ? string_nocheck+0x58/0x70
    [ 374.609534] ? find_valid_gpt+0x720/0x720
    [ 374.614016] efi_partition+0x89/0x430
    [ 374.618113] ? string+0x48/0x60
    [ 374.621632] ? snprintf+0x49/0x70
    [ 374.625339] ? find_valid_gpt+0x720/0x720
    [ 374.629828] check_partition+0x116/0x210
    [ 374.634214] rescan_partitions+0xb6/0x360
    [ 374.638699] __blkdev_reread_part+0x64/0x70
    [ 374.643377] blkdev_reread_part+0x23/0x40
    [ 374.647860] blkdev_ioctl+0x48c/0x990
    [ 374.651956] block_ioctl+0x41/0x50
    [ 374.655766] do_vfs_ioctl+0xa7/0x600
    [ 374.659766] ? locks_lock_inode_wait+0xb1/0x150
    [ 374.664832] ksys_ioctl+0x67/0x90
    [ 374.668539] __x64_sys_ioctl+0x1a/0x20
    [ 374.672732] do_syscall_64+0x5a/0x1c0
    [ 374.676828] entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    [ 374.738474] INFO: task nvmeadm:49141 blocked for more than 123 seconds.
    [ 374.745871] Not tainted 5.2.0-rc3-mpdebug+ #42
    [ 374.751419] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [ 374.760170] nvmeadm D 0 49141 36333 0x00004080
    [ 374.766301] Call Trace:
    [ 374.769038] __schedule+0x2ef/0x620
    [ 374.772939] schedule+0x38/0xa0
    [ 374.776452] blk_mq_freeze_queue_wait+0x59/0x100
    [ 374.781614] ? remove_wait_queue+0x60/0x60
    [ 374.786192] blk_mq_freeze_queue+0x1a/0x20
    [ 374.790773] nvme_update_disk_info.isra.57+0x5f/0x350 [nvme_core]
    [ 374.797582] ? nvme_identify_ns.isra.50+0x71/0xc0 [nvme_core]
    [ 374.804006] __nvme_revalidate_disk+0xe5/0x110 [nvme_core]
    [ 374.810139] nvme_revalidate_disk+0xa6/0x120 [nvme_core]
    [ 374.816078] ? nvme_submit_user_cmd+0x11e/0x320 [nvme_core]
    [ 374.822299] nvme_user_cmd+0x264/0x370 [nvme_core]
    [ 374.827661] nvme_dev_ioctl+0x112/0x1d0 [nvme_core]
    [ 374.833114] do_vfs_ioctl+0xa7/0x600
    [ 374.837117] ? __audit_syscall_entry+0xdd/0x130
    [ 374.842184] ksys_ioctl+0x67/0x90
    [ 374.845891] __x64_sys_ioctl+0x1a/0x20
    [ 374.850082] do_syscall_64+0x5a/0x1c0
    [ 374.854178] entry_SYSCALL_64_after_hwframe+0x44/0xa9
    --
    
    Reported-by: James Puthukattukaran <james.puthukattukaran@oracle.com>
    Tested-by: James Puthukattukaran <james.puthukattukaran@oracle.com>
    Reviewed-by: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 4f0d0d12744e..b34dcb2288e7 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -12,6 +12,36 @@ module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 
+void nvme_mpath_unfreeze(struct nvme_subsystem *subsys)
+{
+	struct nvme_ns_head *h;
+
+	lockdep_assert_held(&subsys->lock);
+	list_for_each_entry(h, &subsys->nsheads, entry)
+		if (h->disk)
+			blk_mq_unfreeze_queue(h->disk->queue);
+}
+
+void nvme_mpath_wait_freeze(struct nvme_subsystem *subsys)
+{
+	struct nvme_ns_head *h;
+
+	lockdep_assert_held(&subsys->lock);
+	list_for_each_entry(h, &subsys->nsheads, entry)
+		if (h->disk)
+			blk_mq_freeze_queue_wait(h->disk->queue);
+}
+
+void nvme_mpath_start_freeze(struct nvme_subsystem *subsys)
+{
+	struct nvme_ns_head *h;
+
+	lockdep_assert_held(&subsys->lock);
+	list_for_each_entry(h, &subsys->nsheads, entry)
+		if (h->disk)
+			blk_freeze_queue_start(h->disk->queue);
+}
+
 /*
  * If multipathing is enabled we need to always use the subsystem instance
  * number for numbering our devices to avoid conflicts between subsystems that

commit 66b20ac0a1a10769d059d6903202f53494e3d902
Author: Marta Rybczynska <mrybczyn@kalray.eu>
Date:   Tue Jul 23 07:41:20 2019 +0200

    nvme: fix multipath crash when ANA is deactivated
    
    Fix a crash with multipath activated. It happends when ANA log
    page is larger than MDTS and because of that ANA is disabled.
    The driver then tries to access unallocated buffer when connecting
    to a nvme target. The signature is as follows:
    
    [  300.433586] nvme nvme0: ANA log page size (8208) larger than MDTS (8192).
    [  300.435387] nvme nvme0: disabling ANA support.
    [  300.437835] nvme nvme0: creating 4 I/O queues.
    [  300.459132] nvme nvme0: new ctrl: NQN "nqn.0.0.0", addr 10.91.0.1:8009
    [  300.464609] BUG: unable to handle kernel NULL pointer dereference at 0000000000000008
    [  300.466342] #PF error: [normal kernel read fault]
    [  300.467385] PGD 0 P4D 0
    [  300.467987] Oops: 0000 [#1] SMP PTI
    [  300.468787] CPU: 3 PID: 50 Comm: kworker/u8:1 Not tainted 5.0.20kalray+ #4
    [  300.470264] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011
    [  300.471532] Workqueue: nvme-wq nvme_scan_work [nvme_core]
    [  300.472724] RIP: 0010:nvme_parse_ana_log+0x21/0x140 [nvme_core]
    [  300.474038] Code: 45 01 d2 d8 48 98 c3 66 90 0f 1f 44 00 00 41 57 41 56 41 55 41 54 55 53 48 89 fb 48 83 ec 08 48 8b af 20 0a 00 00 48 89 34 24 <66> 83 7d 08 00 0f 84 c6 00 00 00 44 8b 7d 14 49 89 d5 8b 55 10 48
    [  300.477374] RSP: 0018:ffffa50e80fd7cb8 EFLAGS: 00010296
    [  300.478334] RAX: 0000000000000001 RBX: ffff9130f1872258 RCX: 0000000000000000
    [  300.479784] RDX: ffffffffc06c4c30 RSI: ffff9130edad4280 RDI: ffff9130f1872258
    [  300.481488] RBP: 0000000000000000 R08: 0000000000000001 R09: 0000000000000044
    [  300.483203] R10: 0000000000000220 R11: 0000000000000040 R12: ffff9130f18722c0
    [  300.484928] R13: ffff9130f18722d0 R14: ffff9130edad4280 R15: ffff9130f18722c0
    [  300.486626] FS:  0000000000000000(0000) GS:ffff9130f7b80000(0000) knlGS:0000000000000000
    [  300.488538] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  300.489907] CR2: 0000000000000008 CR3: 00000002365e6000 CR4: 00000000000006e0
    [  300.491612] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [  300.493303] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [  300.494991] Call Trace:
    [  300.495645]  nvme_mpath_add_disk+0x5c/0xb0 [nvme_core]
    [  300.496880]  nvme_validate_ns+0x2ef/0x550 [nvme_core]
    [  300.498105]  ? nvme_identify_ctrl.isra.45+0x6a/0xb0 [nvme_core]
    [  300.499539]  nvme_scan_work+0x2b4/0x370 [nvme_core]
    [  300.500717]  ? __switch_to_asm+0x35/0x70
    [  300.501663]  process_one_work+0x171/0x380
    [  300.502340]  worker_thread+0x49/0x3f0
    [  300.503079]  kthread+0xf8/0x130
    [  300.503795]  ? max_active_store+0x80/0x80
    [  300.504690]  ? kthread_bind+0x10/0x10
    [  300.505502]  ret_from_fork+0x35/0x40
    [  300.506280] Modules linked in: nvme_tcp nvme_rdma rdma_cm iw_cm ib_cm ib_core nvme_fabrics nvme_core xt_physdev ip6table_raw ip6table_mangle ip6table_filter ip6_tables xt_comment iptable_nat nf_nat_ipv4 nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_CHECKSUM iptable_mangle iptable_filter veth ebtable_filter ebtable_nat ebtables iptable_raw vxlan ip6_udp_tunnel udp_tunnel sunrpc joydev pcspkr virtio_balloon br_netfilter bridge stp llc ip_tables xfs libcrc32c ata_generic pata_acpi virtio_net virtio_console net_failover virtio_blk failover ata_piix serio_raw libata virtio_pci virtio_ring virtio
    [  300.514984] CR2: 0000000000000008
    [  300.515569] ---[ end trace faa2eefad7e7f218 ]---
    [  300.516354] RIP: 0010:nvme_parse_ana_log+0x21/0x140 [nvme_core]
    [  300.517330] Code: 45 01 d2 d8 48 98 c3 66 90 0f 1f 44 00 00 41 57 41 56 41 55 41 54 55 53 48 89 fb 48 83 ec 08 48 8b af 20 0a 00 00 48 89 34 24 <66> 83 7d 08 00 0f 84 c6 00 00 00 44 8b 7d 14 49 89 d5 8b 55 10 48
    [  300.520353] RSP: 0018:ffffa50e80fd7cb8 EFLAGS: 00010296
    [  300.521229] RAX: 0000000000000001 RBX: ffff9130f1872258 RCX: 0000000000000000
    [  300.522399] RDX: ffffffffc06c4c30 RSI: ffff9130edad4280 RDI: ffff9130f1872258
    [  300.523560] RBP: 0000000000000000 R08: 0000000000000001 R09: 0000000000000044
    [  300.524734] R10: 0000000000000220 R11: 0000000000000040 R12: ffff9130f18722c0
    [  300.525915] R13: ffff9130f18722d0 R14: ffff9130edad4280 R15: ffff9130f18722c0
    [  300.527084] FS:  0000000000000000(0000) GS:ffff9130f7b80000(0000) knlGS:0000000000000000
    [  300.528396] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  300.529440] CR2: 0000000000000008 CR3: 00000002365e6000 CR4: 00000000000006e0
    [  300.530739] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [  300.531989] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [  300.533264] Kernel panic - not syncing: Fatal exception
    [  300.534338] Kernel Offset: 0x17c00000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)
    [  300.536227] ---[ end Kernel panic - not syncing: Fatal exception ]---
    
    Condition check refactoring from Christoph Hellwig.
    
    Signed-off-by: Marta Rybczynska <marta.rybczynska@kalray.eu>
    Tested-by: Jean-Baptiste Riaux <jbriaux@kalray.eu>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index a9a927677970..4f0d0d12744e 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -12,11 +12,6 @@ module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 
-inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
-{
-	return multipath && ctrl->subsys && (ctrl->subsys->cmic & (1 << 3));
-}
-
 /*
  * If multipathing is enabled we need to always use the subsystem instance
  * number for numbering our devices to avoid conflicts between subsystems that
@@ -622,7 +617,8 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 {
 	int error;
 
-	if (!nvme_ctrl_use_ana(ctrl))
+	/* check if multipath is enabled and we have the capability */
+	if (!multipath || !ctrl->subsys || !(ctrl->subsys->cmic & (1 << 3)))
 		return 0;
 
 	ctrl->anacap = id->anacap;

commit 04e70bd4a0264a3d488a9eff6e116d7dc9a77967
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Jul 4 08:10:47 2019 +0200

    nvme-multipath: do not select namespaces which are about to be removed
    
    nvme_ns_remove() will first set the NVME_NS_REMOVING flag before removing
    it from the list at the very last step.
    So to avoid selecting a namespace in nvme_find_path() which is about to be
    removed check the NVME_NS_REMOVING flag, too, when selecting a new path.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 9b6dc11fa559..a9a927677970 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -126,7 +126,8 @@ void nvme_mpath_clear_current_path(struct nvme_ns *ns)
 static bool nvme_path_is_disabled(struct nvme_ns *ns)
 {
 	return ns->ctrl->state != NVME_CTRL_LIVE ||
-		test_bit(NVME_NS_ANA_PENDING, &ns->flags);
+		test_bit(NVME_NS_ANA_PENDING, &ns->flags) ||
+		test_bit(NVME_NS_REMOVING, &ns->flags);
 }
 
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)

commit 2032d074716a811440aa9cd2e971a0716646d6af
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Jul 4 08:10:46 2019 +0200

    nvme-multipath: also check for a disabled path if there is a single sibling
    
    When we have a singular list in nvme_round_robin_path() we still
    need to check its validity.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5a6dbb422a9c..9b6dc11fa559 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -183,8 +183,11 @@ static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
 {
 	struct nvme_ns *ns, *found, *fallback = NULL;
 
-	if (list_is_singular(&head->list))
+	if (list_is_singular(&head->list)) {
+		if (nvme_path_is_disabled(old))
+			return NULL;
 		return old;
+	}
 
 	for (ns = nvme_next_ns(head, old);
 	     ns != old;

commit ca7ae5c966bd4c00626d6ba05d68219f3c1fba36
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Jul 4 08:10:46 2019 +0200

    nvme-multipath: factor out a nvme_path_is_disabled helper
    
    Factor our a common helper to check if a path has been disabled
    by something other than the per-namespace ANA state.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    [hch: split from a bigger patch]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 499acf07d61a..5a6dbb422a9c 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -123,14 +123,19 @@ void nvme_mpath_clear_current_path(struct nvme_ns *ns)
 	}
 }
 
+static bool nvme_path_is_disabled(struct nvme_ns *ns)
+{
+	return ns->ctrl->state != NVME_CTRL_LIVE ||
+		test_bit(NVME_NS_ANA_PENDING, &ns->flags);
+}
+
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 {
 	int found_distance = INT_MAX, fallback_distance = INT_MAX, distance;
 	struct nvme_ns *found = NULL, *fallback = NULL, *ns;
 
 	list_for_each_entry_rcu(ns, &head->list, siblings) {
-		if (ns->ctrl->state != NVME_CTRL_LIVE ||
-		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+		if (nvme_path_is_disabled(ns))
 			continue;
 
 		if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_NUMA)
@@ -184,8 +189,7 @@ static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
 	for (ns = nvme_next_ns(head, old);
 	     ns != old;
 	     ns = nvme_next_ns(head, ns)) {
-		if (ns->ctrl->state != NVME_CTRL_LIVE ||
-		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+		if (nvme_path_is_disabled(ns))
 			continue;
 
 		if (ns->ana_state == NVME_ANA_OPTIMIZED) {

commit 8a03b27ea61c2ab9de16a8a195822ef05e799748
Author: Hannes Reinecke <hare@suse.de>
Date:   Fri May 3 15:37:35 2019 +0200

    nvme-multipath: avoid crash on invalid subsystem cntlid enumeration
    
    A process holding an open reference to a removed disk prevents it
    from completing deletion, so its name continues to exist. A subsequent
    gendisk creation may have the same cntlid which risks collision when
    using that for the name. Use the unique ctrl->instance instead.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5c9429d41120..499acf07d61a 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -31,7 +31,7 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 		sprintf(disk_name, "nvme%dn%d", ctrl->instance, ns->head->instance);
 	} else if (ns->head->disk) {
 		sprintf(disk_name, "nvme%dc%dn%d", ctrl->subsys->instance,
-				ctrl->cntlid, ns->head->instance);
+				ctrl->instance, ns->head->instance);
 		*flags = GENHD_FL_HIDDEN;
 	} else {
 		sprintf(disk_name, "nvme%dn%d", ctrl->subsys->instance,

commit 592b6e7b0226b198c12439065f725be00c92d559
Author: Hannes Reinecke <hare@suse.com>
Date:   Sun Apr 28 20:24:42 2019 -0700

    nvme-multipath: don't print ANA group state by default
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Chaitanya Kulkarni <chaitanya.kulkarni@wdc.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index e6ddc83223df..5c9429d41120 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -429,7 +429,7 @@ static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
 	unsigned *nr_change_groups = data;
 	struct nvme_ns *ns;
 
-	dev_info(ctrl->device, "ANA group %d: %s.\n",
+	dev_dbg(ctrl->device, "ANA group %d: %s.\n",
 			le32_to_cpu(desc->grpid),
 			nvme_ana_state_names[desc->state]);
 

commit 525aa5a705d86e193726ee465d1a975265fabf19
Author: Hannes Reinecke <hare@suse.de>
Date:   Tue Apr 30 18:57:09 2019 +0200

    nvme-multipath: split bios with the ns_head bio_set before submitting
    
    If the bio is moved to a different queue via blk_steal_bios() and
    the original queue is destroyed in nvme_remove_ns() we'll be ending
    with a crash in bio_endio() as the mempool for the split bio bvecs
    had already been destroyed.
    So split the bio using the original queue (which will remain during the
    lifetime of the bio) before sending it down to the underlying device.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index f0716f6ce41f..e6ddc83223df 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -232,6 +232,14 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	blk_qc_t ret = BLK_QC_T_NONE;
 	int srcu_idx;
 
+	/*
+	 * The namespace might be going away and the bio might
+	 * be moved to a different queue via blk_steal_bios(),
+	 * so we need to use the bio_split pool from the original
+	 * queue to allocate the bvecs from.
+	 */
+	blk_queue_split(q, &bio);
+
 	srcu_idx = srcu_read_lock(&head->srcu);
 	ns = nvme_find_path(head);
 	if (likely(ns)) {

commit cc2278c413c3a06a93c23ee8722e4dd3d621de12
Author: Martin George <marting@netapp.com>
Date:   Wed Mar 27 09:52:56 2019 +0100

    nvme-multipath: relax ANA state check
    
    When undergoing state transitions I/O might be requeued, hence
    we should always call nvme_mpath_set_live() to schedule requeue_work
    whenever the nvme device is live, independent on whether the
    old state was live or not.
    
    Signed-off-by: Martin George <marting@netapp.com>
    Signed-off-by: Gargi Srinivas <sring@netapp.com>
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 2839bb70badf..f0716f6ce41f 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -404,15 +404,12 @@ static inline bool nvme_state_is_live(enum nvme_ana_state state)
 static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
 		struct nvme_ns *ns)
 {
-	enum nvme_ana_state old;
-
 	mutex_lock(&ns->head->lock);
-	old = ns->ana_state;
 	ns->ana_grpid = le32_to_cpu(desc->grpid);
 	ns->ana_state = desc->state;
 	clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
 
-	if (nvme_state_is_live(ns->ana_state) && !nvme_state_is_live(old))
+	if (nvme_state_is_live(ns->ana_state))
 		nvme_mpath_set_live(ns);
 	mutex_unlock(&ns->head->lock);
 }

commit bc50ad7501dd3629af9aa423ed0d1eae0061bcf1
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Feb 18 09:36:29 2019 +0100

    nvme: convert to SPDX identifiers
    
    Update license to use SPDX-License-Identifier instead of verbose license
    text.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 1f7fe1bd2936..2839bb70badf 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -1,14 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (c) 2017-2018 Christoph Hellwig.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
  */
 
 #include <linux/moduleparam.h>

commit 75c10e73272484bc3a940a9c8e4ec39a7a1b8c21
Author: Hannes Reinecke <hare@suse.de>
Date:   Mon Feb 18 11:43:26 2019 +0100

    nvme-multipath: round-robin I/O policy
    
    Implement a simple round-robin I/O policy for multipathing.  Path
    selection is done in two rounds, first iterating across all optimized
    paths, and if that doesn't return any valid paths, iterate over all
    optimized and non-optimized paths.  If no paths are found, use the
    existing algorithm.  Also add a sysfs attribute 'iopolicy' to switch
    between the current NUMA-aware I/O policy and the 'round-robin' I/O
    policy.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index b9fff3b8ed1b..1f7fe1bd2936 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -141,7 +141,10 @@ static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
 			continue;
 
-		distance = node_distance(node, ns->ctrl->numa_node);
+		if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_NUMA)
+			distance = node_distance(node, ns->ctrl->numa_node);
+		else
+			distance = LOCAL_DISTANCE;
 
 		switch (ns->ana_state) {
 		case NVME_ANA_OPTIMIZED:
@@ -168,6 +171,47 @@ static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 	return found;
 }
 
+static struct nvme_ns *nvme_next_ns(struct nvme_ns_head *head,
+		struct nvme_ns *ns)
+{
+	ns = list_next_or_null_rcu(&head->list, &ns->siblings, struct nvme_ns,
+			siblings);
+	if (ns)
+		return ns;
+	return list_first_or_null_rcu(&head->list, struct nvme_ns, siblings);
+}
+
+static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
+		int node, struct nvme_ns *old)
+{
+	struct nvme_ns *ns, *found, *fallback = NULL;
+
+	if (list_is_singular(&head->list))
+		return old;
+
+	for (ns = nvme_next_ns(head, old);
+	     ns != old;
+	     ns = nvme_next_ns(head, ns)) {
+		if (ns->ctrl->state != NVME_CTRL_LIVE ||
+		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+			continue;
+
+		if (ns->ana_state == NVME_ANA_OPTIMIZED) {
+			found = ns;
+			goto out;
+		}
+		if (ns->ana_state == NVME_ANA_NONOPTIMIZED)
+			fallback = ns;
+	}
+
+	if (!fallback)
+		return NULL;
+	found = fallback;
+out:
+	rcu_assign_pointer(head->current_path[node], found);
+	return found;
+}
+
 static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
 {
 	return ns->ctrl->state == NVME_CTRL_LIVE &&
@@ -180,6 +224,8 @@ inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 	struct nvme_ns *ns;
 
 	ns = srcu_dereference(head->current_path[node], &head->srcu);
+	if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_RR && ns)
+		ns = nvme_round_robin_path(head, node, ns);
 	if (unlikely(!ns || !nvme_path_is_optimized(ns)))
 		ns = __nvme_find_path(head, node);
 	return ns;
@@ -471,6 +517,44 @@ void nvme_mpath_stop(struct nvme_ctrl *ctrl)
 	cancel_work_sync(&ctrl->ana_work);
 }
 
+#define SUBSYS_ATTR_RW(_name, _mode, _show, _store)  \
+	struct device_attribute subsys_attr_##_name =	\
+		__ATTR(_name, _mode, _show, _store)
+
+static const char *nvme_iopolicy_names[] = {
+	[NVME_IOPOLICY_NUMA]	= "numa",
+	[NVME_IOPOLICY_RR]	= "round-robin",
+};
+
+static ssize_t nvme_subsys_iopolicy_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct nvme_subsystem *subsys =
+		container_of(dev, struct nvme_subsystem, dev);
+
+	return sprintf(buf, "%s\n",
+			nvme_iopolicy_names[READ_ONCE(subsys->iopolicy)]);
+}
+
+static ssize_t nvme_subsys_iopolicy_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct nvme_subsystem *subsys =
+		container_of(dev, struct nvme_subsystem, dev);
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(nvme_iopolicy_names); i++) {
+		if (sysfs_streq(buf, nvme_iopolicy_names[i])) {
+			WRITE_ONCE(subsys->iopolicy, i);
+			return count;
+		}
+	}
+
+	return -EINVAL;
+}
+SUBSYS_ATTR_RW(iopolicy, S_IRUGO | S_IWUSR,
+		      nvme_subsys_iopolicy_show, nvme_subsys_iopolicy_store);
+
 static ssize_t ana_grpid_show(struct device *dev, struct device_attribute *attr,
 		char *buf)
 {

commit 78a61cd42a64f3587862b372a79e1d6aaf131fd7
Author: Hannes Reinecke <hare@suse.de>
Date:   Wed Jan 9 09:45:15 2019 +0100

    nvme-multipath: drop optimization for static ANA group IDs
    
    Bit 6 in the ANACAP field is used to indicate that the ANA group ID
    doesn't change while the namespace is attached to the controller.
    There is an optimisation in the code to only allocate space
    for the ANA group header, as the namespace list won't change and
    hence would not need to be refreshed.
    However, this optimisation was never carried over to the actual
    workflow, which always assumes that the buffer is large enough
    to hold the ANA header _and_ the namespace list.
    So drop this optimisation and always allocate enough space.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index df4b3a6db51b..b9fff3b8ed1b 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -545,8 +545,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
 	ctrl->ana_log_size = sizeof(struct nvme_ana_rsp_hdr) +
 		ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
-	if (!(ctrl->anacap & (1 << 6)))
-		ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+	ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
 
 	if (ctrl->ana_log_size > ctrl->max_hw_sectors << SECTOR_SHIFT) {
 		dev_err(ctrl->device,

commit c7055fd15ff46d92eb0dd1c16a4fe010d58224c8
Author: Hannes Reinecke <hare@suse.de>
Date:   Tue Jan 8 12:46:58 2019 +0100

    nvme-multipath: zero out ANA log buffer
    
    When nvme_init_identify() fails the ANA log buffer is deallocated
    but _not_ set to NULL. This can cause double free oops when this
    controller is deleted without ever being reconnected.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 183ec17ba067..df4b3a6db51b 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -570,6 +570,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 	return 0;
 out_free_ana_log_buf:
 	kfree(ctrl->ana_log_buf);
+	ctrl->ana_log_buf = NULL;
 out:
 	return error;
 }
@@ -577,5 +578,6 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 void nvme_mpath_uninit(struct nvme_ctrl *ctrl)
 {
 	kfree(ctrl->ana_log_buf);
+	ctrl->ana_log_buf = NULL;
 }
 

commit 103e515efa89be33d04e45aae82de136f0c49865
Author: Hannes Reinecke <hare@suse.com>
Date:   Fri Nov 16 09:22:29 2018 +0100

    nvme: add a numa_node field to struct nvme_ctrl
    
    Instead of directly poking into the struct device add a new numa_node
    field to struct nvme_ctrl.  This allows fabrics drivers where ctrl->dev
    is a virtual device to support NUMA affinity as well.
    
    Also expose the field as a sysfs attribute, and populate it for the
    RDMA and FC transports.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index ec310b1b9267..183ec17ba067 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -141,7 +141,7 @@ static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
 		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
 			continue;
 
-		distance = node_distance(node, dev_to_node(ns->ctrl->dev));
+		distance = node_distance(node, ns->ctrl->numa_node);
 
 		switch (ns->ana_state) {
 		case NVME_ANA_OPTIMIZED:
@@ -261,7 +261,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
 		return 0;
 
-	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+	q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
 	if (!q)
 		goto out;
 	q->queuedata = head;

commit 9d6610b76fa374eae3deb93bcbace4a06c2e3b95
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 2 17:46:25 2018 +0100

    nvme-mpath: remove I/O polling support
    
    The ->poll_fn has been stale for a while, as a lot of places check for mq
    ops.  But there is no real point in it anyway, as we don't even use
    the multipath code for subsystems without multiple ports, which is usually
    what we do high performance I/O to.  If it really becomes an issue we
    should rework the nvme code to also skip the multipath code for any
    private namespace, even if that could mean some trouble when rescanning.
    
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index ffebdd0ae34b..ec310b1b9267 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -220,21 +220,6 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	return ret;
 }
 
-static int nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc, bool spin)
-{
-	struct nvme_ns_head *head = q->queuedata;
-	struct nvme_ns *ns;
-	int found = 0;
-	int srcu_idx;
-
-	srcu_idx = srcu_read_lock(&head->srcu);
-	ns = srcu_dereference(head->current_path[numa_node_id()], &head->srcu);
-	if (likely(ns && nvme_path_is_optimized(ns)))
-		found = ns->queue->poll_fn(q, qc, spin);
-	srcu_read_unlock(&head->srcu, srcu_idx);
-	return found;
-}
-
 static void nvme_requeue_work(struct work_struct *work)
 {
 	struct nvme_ns_head *head =
@@ -281,7 +266,6 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 		goto out;
 	q->queuedata = head;
 	blk_queue_make_request(q, nvme_ns_head_make_request);
-	q->poll_fn = nvme_ns_head_poll;
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);

commit 0a1b8b87d064a47fad9ec475316002da28559207
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Nov 26 08:24:43 2018 -0700

    block: make blk_poll() take a parameter on whether to spin or not
    
    blk_poll() has always kept spinning until it found an IO. This is
    fine for SYNC polling, since we need to find one request we have
    pending, but in preparation for ASYNC polling it can be beneficial
    to just check if we have any entries available or not.
    
    Existing callers are converted to pass in 'spin == true', to retain
    the old behavior.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index f9eeb3b58632..ffebdd0ae34b 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -220,7 +220,7 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	return ret;
 }
 
-static int nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
+static int nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc, bool spin)
 {
 	struct nvme_ns_head *head = q->queuedata;
 	struct nvme_ns *ns;
@@ -230,7 +230,7 @@ static int nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
 	srcu_idx = srcu_read_lock(&head->srcu);
 	ns = srcu_dereference(head->current_path[numa_node_id()], &head->srcu);
 	if (likely(ns && nvme_path_is_optimized(ns)))
-		found = ns->queue->poll_fn(q, qc);
+		found = ns->queue->poll_fn(q, qc, spin);
 	srcu_read_unlock(&head->srcu, srcu_idx);
 	return found;
 }

commit 85f4d4b65fdd67f1d6dc9eeb1d91923cef07eb6a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 6 13:30:55 2018 -0700

    block: have ->poll_fn() return number of entries polled
    
    We currently only really support sync poll, ie poll with 1 IO in flight.
    This prepares us for supporting async poll.
    
    Note that the returned value isn't necessarily 100% accurate. If poll
    races with IRQ completion, we assume that the fact that the task is now
    runnable means we found at least one entry. In reality it could be more
    than 1, or not even 1. This is fine, the caller will just need to take
    this into account.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 8b841f39734c..f9eeb3b58632 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -220,11 +220,11 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	return ret;
 }
 
-static bool nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
+static int nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
 {
 	struct nvme_ns_head *head = q->queuedata;
 	struct nvme_ns *ns;
-	bool found = false;
+	int found = 0;
 	int srcu_idx;
 
 	srcu_idx = srcu_read_lock(&head->srcu);

commit a78b03bc7300e4f17b1e510884bea1095d92b17b
Merge: fce15a609f8f 9ff01193a20d
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Nov 18 15:46:03 2018 -0700

    Merge tag 'v4.20-rc3' into for-4.21/block
    
    Merge in -rc3 to resolve a few conflicts, but also to get a few
    important fixes that have gone into mainline since the block
    4.21 branch was forked off (most notably the SCSI queue issue,
    which is both a conflict AND needed fix).
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 6d46964230d182c4b6097379738849a809d791dc
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 14 17:02:18 2018 +0100

    block: remove the lock argument to blk_alloc_queue_node
    
    With the legacy request path gone there is no real need to override the
    queue_lock.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5e3cc8c59a39..b82b0d3ca39a 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -276,7 +276,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
 		return 0;
 
-	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE, NULL);
+	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
 	if (!q)
 		goto out;
 	q->queuedata = head;

commit 8f676b8508c250bbe255096522fdefb73f1ea0b9
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Fri Nov 2 11:22:13 2018 -0700

    nvme: make sure ns head inherits underlying device limits
    
    Whenever we update ns_head info, we need to make sure it is still
    compatible with all underlying backing devices because although nvme
    multipath doesn't have any explicit use of these limits, other devices
    can still be stacked on top of it which may rely on the underlying limits.
    Start with unlimited stacking limits, and every info update iterate over
    siblings and adjust queue limits.
    
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5e3cc8c59a39..9901afd804ce 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -285,6 +285,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);
+	blk_set_stacking_limits(&q->limits);
 
 	/* we need to propagate up the VMC settings */
 	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)

commit 886fabf693263e8651c0c4ab84fc626ad6d3a6e7
Author: Keith Busch <keith.busch@intel.com>
Date:   Fri Oct 5 09:49:37 2018 -0600

    nvme: update node paths after adding new path
    
    The nvme namespace paths were being updated only when the current path
    was not set or nonoptimized. If a new path comes online that is a better
    path for its NUMA node, the multipath selector may continue using the
    previously set path on a potentially further node.
    
    This patch re-runs the path assignment after successfully adding a new
    optimized path.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 52987052b7fc..5e3cc8c59a39 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -321,6 +321,15 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 		device_add_disk(&head->subsys->dev, head->disk,
 				nvme_ns_id_attr_groups);
 
+	if (nvme_path_is_optimized(ns)) {
+		int node, srcu_idx;
+
+		srcu_idx = srcu_read_lock(&head->srcu);
+		for_each_node(node)
+			__nvme_find_path(head, node);
+		srcu_read_unlock(&head->srcu, srcu_idx);
+	}
+
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 

commit f333444708f82c4a4d3ccac004da0bfd9cfdfa42
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Sep 11 09:51:29 2018 +0200

    nvme: take node locality into account when selecting a path
    
    Make current_path an array with an entry for every possible node, and
    cache the best path on a per-node basis.  Take the node distance into
    account when selecting it.  This is primarily useful for dual-ported PCIe
    devices which are connected to PCIe root ports on different sockets.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index ac16093a7928..52987052b7fc 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -117,29 +117,55 @@ static const char *nvme_ana_state_names[] = {
 	[NVME_ANA_CHANGE]		= "change",
 };
 
-static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head)
+void nvme_mpath_clear_current_path(struct nvme_ns *ns)
 {
-	struct nvme_ns *ns, *fallback = NULL;
+	struct nvme_ns_head *head = ns->head;
+	int node;
+
+	if (!head)
+		return;
+
+	for_each_node(node) {
+		if (ns == rcu_access_pointer(head->current_path[node]))
+			rcu_assign_pointer(head->current_path[node], NULL);
+	}
+}
+
+static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
+{
+	int found_distance = INT_MAX, fallback_distance = INT_MAX, distance;
+	struct nvme_ns *found = NULL, *fallback = NULL, *ns;
 
 	list_for_each_entry_rcu(ns, &head->list, siblings) {
 		if (ns->ctrl->state != NVME_CTRL_LIVE ||
 		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
 			continue;
+
+		distance = node_distance(node, dev_to_node(ns->ctrl->dev));
+
 		switch (ns->ana_state) {
 		case NVME_ANA_OPTIMIZED:
-			rcu_assign_pointer(head->current_path, ns);
-			return ns;
+			if (distance < found_distance) {
+				found_distance = distance;
+				found = ns;
+			}
+			break;
 		case NVME_ANA_NONOPTIMIZED:
-			fallback = ns;
+			if (distance < fallback_distance) {
+				fallback_distance = distance;
+				fallback = ns;
+			}
 			break;
 		default:
 			break;
 		}
 	}
 
-	if (fallback)
-		rcu_assign_pointer(head->current_path, fallback);
-	return fallback;
+	if (!found)
+		found = fallback;
+	if (found)
+		rcu_assign_pointer(head->current_path[node], found);
+	return found;
 }
 
 static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
@@ -150,10 +176,12 @@ static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
 
 inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 {
-	struct nvme_ns *ns = srcu_dereference(head->current_path, &head->srcu);
+	int node = numa_node_id();
+	struct nvme_ns *ns;
 
+	ns = srcu_dereference(head->current_path[node], &head->srcu);
 	if (unlikely(!ns || !nvme_path_is_optimized(ns)))
-		ns = __nvme_find_path(head);
+		ns = __nvme_find_path(head, node);
 	return ns;
 }
 
@@ -200,7 +228,7 @@ static bool nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
 	int srcu_idx;
 
 	srcu_idx = srcu_read_lock(&head->srcu);
-	ns = srcu_dereference(head->current_path, &head->srcu);
+	ns = srcu_dereference(head->current_path[numa_node_id()], &head->srcu);
 	if (likely(ns && nvme_path_is_optimized(ns)))
 		found = ns->queue->poll_fn(q, qc);
 	srcu_read_unlock(&head->srcu, srcu_idx);

commit 783f4a4408e1251d17f333ad56abac24dde988b9
Author: James Smart <jsmart2021@gmail.com>
Date:   Thu Sep 27 16:58:54 2018 -0700

    nvme: call nvme_complete_rq when nvmf_check_ready fails for mpath I/O
    
    When an io is rejected by nvmf_check_ready() due to validation of the
    controller state, the nvmf_fail_nonready_command() will normally return
    BLK_STS_RESOURCE to requeue and retry.  However, if the controller is
    dying or the I/O is marked for NVMe multipath, the I/O is failed so that
    the controller can terminate or so that the io can be issued on a
    different path.  Unfortunately, as this reject point is before the
    transport has accepted the command, blk-mq ends up completing the I/O
    and never calls nvme_complete_rq(), which is where multipath may preserve
    or re-route the I/O. The end result is, the device user ends up seeing an
    EIO error.
    
    Example: single path connectivity, controller is under load, and a reset
    is induced.  An I/O is received:
    
      a) while the reset state has been set but the queues have yet to be
         stopped; or
      b) after queues are started (at end of reset) but before the reconnect
         has completed.
    
    The I/O finishes with an EIO status.
    
    This patch makes the following changes:
    
      - Adds the HOST_PATH_ERROR pathing status from TP4028
      - Modifies the reject point such that it appears to queue successfully,
        but actually completes the io with the new pathing status and calls
        nvme_complete_rq().
      - nvme_complete_rq() recognizes the new status, avoids resetting the
        controller (likely was already done in order to get this new status),
        and calls the multipather to clear the current path that errored.
        This allows the next command (retry or new command) to select a new
        path if there is one.
    
    Signed-off-by: James Smart <jsmart2021@gmail.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index bfbc6d5b1d93..ac16093a7928 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -77,6 +77,13 @@ void nvme_failover_req(struct request *req)
 			queue_work(nvme_wq, &ns->ctrl->ana_work);
 		}
 		break;
+	case NVME_SC_HOST_PATH_ERROR:
+		/*
+		 * Temporary transport disruption in talking to the controller.
+		 * Try to send on a new path.
+		 */
+		nvme_mpath_clear_current_path(ns);
+		break;
 	default:
 		/*
 		 * Reset the controller for any non-ANA error as we don't know

commit c0aac682fa6590cb660cb083dbc09f55e799d2d2
Merge: 451bb7c33197 17b57b1883c1
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon Oct 1 08:58:57 2018 -0600

    Merge tag 'v4.19-rc6' into for-4.20/block
    
    Merge -rc6 in, for two reasons:
    
    1) Resolve a trivial conflict in the blk-mq-tag.c documentation
    2) A few important regression fixes went into upstream directly, so
       they aren't in the 4.20 branch.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    
    * tag 'v4.19-rc6': (780 commits)
      Linux 4.19-rc6
      MAINTAINERS: fix reference to moved drivers/{misc => auxdisplay}/panel.c
      cpufreq: qcom-kryo: Fix section annotations
      perf/core: Add sanity check to deal with pinned event failure
      xen/blkfront: correct purging of persistent grants
      Revert "xen/blkfront: When purging persistent grants, keep them in the buffer"
      selftests/powerpc: Fix Makefiles for headers_install change
      blk-mq: I/O and timer unplugs are inverted in blktrace
      dax: Fix deadlock in dax_lock_mapping_entry()
      x86/boot: Fix kexec booting failure in the SEV bit detection code
      bcache: add separate workqueue for journal_write to avoid deadlock
      drm/amd/display: Fix Edid emulation for linux
      drm/amd/display: Fix Vega10 lightup on S3 resume
      drm/amdgpu: Fix vce work queue was not cancelled when suspend
      Revert "drm/panel: Add device_link from panel device to DRM device"
      xen/blkfront: When purging persistent grants, keep them in the buffer
      clocksource/drivers/timer-atmel-pit: Properly handle error cases
      block: fix deadline elevator drain for zoned block devices
      ACPI / hotplug / PCI: Don't scan for non-hotplug bridges if slot is not bridge
      drm/syncobj: Don't leak fences when WAIT_FOR_SUBMIT is set
      ...
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 33b14f67a4e1eabd219fd6543da8f15ed86b641c
Author: Hannes Reinecke <hare@suse.de>
Date:   Fri Sep 28 08:17:20 2018 +0200

    nvme: register ns_id attributes as default sysfs groups
    
    We should be registering the ns_id attribute as default sysfs
    attribute groups, otherwise we have a race condition between
    the uevent and the attributes appearing in sysfs.
    
    Suggested-by: Bart van Assche <bvanassche@acm.org>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 477af51d01e8..8e846095c42d 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -282,13 +282,9 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 	if (!head->disk)
 		return;
 
-	if (!(head->disk->flags & GENHD_FL_UP)) {
-		device_add_disk(&head->subsys->dev, head->disk, NULL);
-		if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
-				&nvme_ns_id_attr_group))
-			dev_warn(&head->subsys->dev,
-				 "failed to create id group.\n");
-	}
+	if (!(head->disk->flags & GENHD_FL_UP))
+		device_add_disk(&head->subsys->dev, head->disk,
+				nvme_ns_id_attr_groups);
 
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
@@ -494,11 +490,8 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
 		return;
-	if (head->disk->flags & GENHD_FL_UP) {
-		sysfs_remove_group(&disk_to_dev(head->disk)->kobj,
-				   &nvme_ns_id_attr_group);
+	if (head->disk->flags & GENHD_FL_UP)
 		del_gendisk(head->disk);
-	}
 	blk_set_queue_dying(head->disk->queue);
 	/* make sure all pending bios are cleaned up */
 	kblockd_schedule_work(&head->requeue_work);

commit fef912bf860e8e7e48a2bfb978a356bba743a8b7
Author: Hannes Reinecke <hare@suse.de>
Date:   Fri Sep 28 08:17:19 2018 +0200

    block: genhd: add 'groups' argument to device_add_disk
    
    Update device_add_disk() to take an 'groups' argument so that
    individual drivers can register a device with additional sysfs
    attributes.
    This avoids race condition the driver would otherwise have if these
    groups were to be created with sysfs_add_groups().
    
    Signed-off-by: Martin Wilck <martin.wilck@suse.com>
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5a9562881d4e..477af51d01e8 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -283,7 +283,7 @@ static void nvme_mpath_set_live(struct nvme_ns *ns)
 		return;
 
 	if (!(head->disk->flags & GENHD_FL_UP)) {
-		device_add_disk(&head->subsys->dev, head->disk);
+		device_add_disk(&head->subsys->dev, head->disk, NULL);
 		if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
 				&nvme_ns_id_attr_group))
 			dev_warn(&head->subsys->dev,

commit bb830add192e9d8338082c0fc2c209e23b43d865
Author: Susobhan Dey <susobhan.dey@gmail.com>
Date:   Tue Sep 25 12:29:15 2018 -0700

    nvme: properly propagate errors in nvme_mpath_init
    
    Signed-off-by: Susobhan Dey <susobhan.dey@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 5a9562881d4e..9fe3fff818b8 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -537,8 +537,10 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 
 	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
 	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
-	if (!ctrl->ana_log_buf)
+	if (!ctrl->ana_log_buf) {
+		error = -ENOMEM;
 		goto out;
+	}
 
 	error = nvme_read_ana_log(ctrl, true);
 	if (error)
@@ -547,7 +549,7 @@ int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
 out_free_ana_log_buf:
 	kfree(ctrl->ana_log_buf);
 out:
-	return -ENOMEM;
+	return error;
 }
 
 void nvme_mpath_uninit(struct nvme_ctrl *ctrl)

commit 8f220c418d070a097f7d292cf6b37f88d67845ad
Author: Hannes Reinecke <hare@suse.de>
Date:   Tue Aug 7 12:43:42 2018 +0200

    nvme: fixup crash on failed discovery
    
    When the initial discovery fails the subsystem hasn't been setup yet
    in nvme_mpath_stop, and we can't dereference ctrl->subsys.
    
    Fixes: 0d0b660f ("nvme: add ANA support")
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index c643872f8dac..5a9562881d4e 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -22,7 +22,7 @@ MODULE_PARM_DESC(multipath,
 
 inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
 {
-	return multipath && (ctrl->subsys->cmic & (1 << 3));
+	return multipath && ctrl->subsys && (ctrl->subsys->cmic & (1 << 3));
 }
 
 /*

commit 0d0b660f214dc4905db7b6bc998bad0c16dfb1ba
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 14 08:48:54 2018 +0200

    nvme: add ANA support
    
    Add support for Asynchronous Namespace Access as specified in NVMe 1.3
    TP 4004.  With ANA each namespace attached to a controller belongs to an
    ANA group that describes the characteristics of accessing the namespaces
    through this controller.  In the optimized and non-optimized states
    namespaces can be accessed regularly, although in a multi-pathing
    environment we should always prefer to access a namespace through a
    controller where an optimized relationship exists.  Namespaces in
    Inaccessible, Permanent-Loss or Change state for a given controller
    should not be accessed.
    
    The states are updated through reading the ANA log page, which is read
    once during controller initialization, whenever the ANA change notice
    AEN is received, or when one of the ANA specific status codes that
    signal a state change is received on a command.
    
    The ANA state is kept in the nvme_ns structure, which makes the checks in
    the fast path very simple.  Updating the ANA state when reading the log
    page is also very simple, the only downside is that finding the initial
    ANA state when scanning for namespaces is a bit cumbersome.
    
    The gendisk for a ns_head is only registered once a live path for it
    exists.  Without that the kernel would hang during partition scanning.
    
    Includes fixes and improvements from Hannes Reinecke.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 348aa405b641..c643872f8dac 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2017 Christoph Hellwig.
+ * Copyright (c) 2017-2018 Christoph Hellwig.
  *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms and conditions of the GNU General Public License,
@@ -20,6 +20,11 @@ module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 
+inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
+{
+	return multipath && (ctrl->subsys->cmic & (1 << 3));
+}
+
 /*
  * If multipathing is enabled we need to always use the subsystem instance
  * number for numbering our devices to avoid conflicts between subsystems that
@@ -45,6 +50,7 @@ void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
 void nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;
+	u16 status = nvme_req(req)->status;
 	unsigned long flags;
 
 	spin_lock_irqsave(&ns->head->requeue_lock, flags);
@@ -52,7 +58,34 @@ void nvme_failover_req(struct request *req)
 	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
 	blk_mq_end_request(req, 0);
 
-	nvme_reset_ctrl(ns->ctrl);
+	switch (status & 0x7ff) {
+	case NVME_SC_ANA_TRANSITION:
+	case NVME_SC_ANA_INACCESSIBLE:
+	case NVME_SC_ANA_PERSISTENT_LOSS:
+		/*
+		 * If we got back an ANA error we know the controller is alive,
+		 * but not ready to serve this namespaces.  The spec suggests
+		 * we should update our general state here, but due to the fact
+		 * that the admin and I/O queues are not serialized that is
+		 * fundamentally racy.  So instead just clear the current path,
+		 * mark the the path as pending and kick of a re-read of the ANA
+		 * log page ASAP.
+		 */
+		nvme_mpath_clear_current_path(ns);
+		if (ns->ctrl->ana_log_buf) {
+			set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+			queue_work(nvme_wq, &ns->ctrl->ana_work);
+		}
+		break;
+	default:
+		/*
+		 * Reset the controller for any non-ANA error as we don't know
+		 * what caused the error.
+		 */
+		nvme_reset_ctrl(ns->ctrl);
+		break;
+	}
+
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
@@ -68,25 +101,51 @@ void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
 	up_read(&ctrl->namespaces_rwsem);
 }
 
+static const char *nvme_ana_state_names[] = {
+	[0]				= "invalid state",
+	[NVME_ANA_OPTIMIZED]		= "optimized",
+	[NVME_ANA_NONOPTIMIZED]		= "non-optimized",
+	[NVME_ANA_INACCESSIBLE]		= "inaccessible",
+	[NVME_ANA_PERSISTENT_LOSS]	= "persistent-loss",
+	[NVME_ANA_CHANGE]		= "change",
+};
+
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head)
 {
-	struct nvme_ns *ns;
+	struct nvme_ns *ns, *fallback = NULL;
 
 	list_for_each_entry_rcu(ns, &head->list, siblings) {
-		if (ns->ctrl->state == NVME_CTRL_LIVE) {
+		if (ns->ctrl->state != NVME_CTRL_LIVE ||
+		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+			continue;
+		switch (ns->ana_state) {
+		case NVME_ANA_OPTIMIZED:
 			rcu_assign_pointer(head->current_path, ns);
 			return ns;
+		case NVME_ANA_NONOPTIMIZED:
+			fallback = ns;
+			break;
+		default:
+			break;
 		}
 	}
 
-	return NULL;
+	if (fallback)
+		rcu_assign_pointer(head->current_path, fallback);
+	return fallback;
+}
+
+static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
+{
+	return ns->ctrl->state == NVME_CTRL_LIVE &&
+		ns->ana_state == NVME_ANA_OPTIMIZED;
 }
 
 inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
 {
 	struct nvme_ns *ns = srcu_dereference(head->current_path, &head->srcu);
 
-	if (unlikely(!ns || ns->ctrl->state != NVME_CTRL_LIVE))
+	if (unlikely(!ns || !nvme_path_is_optimized(ns)))
 		ns = __nvme_find_path(head);
 	return ns;
 }
@@ -135,7 +194,7 @@ static bool nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
 
 	srcu_idx = srcu_read_lock(&head->srcu);
 	ns = srcu_dereference(head->current_path, &head->srcu);
-	if (likely(ns && ns->ctrl->state == NVME_CTRL_LIVE))
+	if (likely(ns && nvme_path_is_optimized(ns)))
 		found = ns->queue->poll_fn(q, qc);
 	srcu_read_unlock(&head->srcu, srcu_idx);
 	return found;
@@ -169,6 +228,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	struct request_queue *q;
 	bool vwc = false;
 
+	mutex_init(&head->lock);
 	bio_list_init(&head->requeue_list);
 	spin_lock_init(&head->requeue_lock);
 	INIT_WORK(&head->requeue_work, nvme_requeue_work);
@@ -213,29 +273,232 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	return -ENOMEM;
 }
 
-void nvme_mpath_add_disk(struct nvme_ns_head *head)
+static void nvme_mpath_set_live(struct nvme_ns *ns)
 {
+	struct nvme_ns_head *head = ns->head;
+
+	lockdep_assert_held(&ns->head->lock);
+
 	if (!head->disk)
 		return;
 
-	mutex_lock(&head->subsys->lock);
 	if (!(head->disk->flags & GENHD_FL_UP)) {
 		device_add_disk(&head->subsys->dev, head->disk);
 		if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
 				&nvme_ns_id_attr_group))
-			pr_warn("%s: failed to create sysfs group for identification\n",
-				head->disk->disk_name);
+			dev_warn(&head->subsys->dev,
+				 "failed to create id group.\n");
+	}
+
+	kblockd_schedule_work(&ns->head->requeue_work);
+}
+
+static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
+		int (*cb)(struct nvme_ctrl *ctrl, struct nvme_ana_group_desc *,
+			void *))
+{
+	void *base = ctrl->ana_log_buf;
+	size_t offset = sizeof(struct nvme_ana_rsp_hdr);
+	int error, i;
+
+	lockdep_assert_held(&ctrl->ana_lock);
+
+	for (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {
+		struct nvme_ana_group_desc *desc = base + offset;
+		u32 nr_nsids = le32_to_cpu(desc->nnsids);
+		size_t nsid_buf_size = nr_nsids * sizeof(__le32);
+
+		if (WARN_ON_ONCE(desc->grpid == 0))
+			return -EINVAL;
+		if (WARN_ON_ONCE(le32_to_cpu(desc->grpid) > ctrl->anagrpmax))
+			return -EINVAL;
+		if (WARN_ON_ONCE(desc->state == 0))
+			return -EINVAL;
+		if (WARN_ON_ONCE(desc->state > NVME_ANA_CHANGE))
+			return -EINVAL;
+
+		offset += sizeof(*desc);
+		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - nsid_buf_size))
+			return -EINVAL;
+
+		error = cb(ctrl, desc, data);
+		if (error)
+			return error;
+
+		offset += nsid_buf_size;
+		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static inline bool nvme_state_is_live(enum nvme_ana_state state)
+{
+	return state == NVME_ANA_OPTIMIZED || state == NVME_ANA_NONOPTIMIZED;
+}
+
+static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
+		struct nvme_ns *ns)
+{
+	enum nvme_ana_state old;
+
+	mutex_lock(&ns->head->lock);
+	old = ns->ana_state;
+	ns->ana_grpid = le32_to_cpu(desc->grpid);
+	ns->ana_state = desc->state;
+	clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
+
+	if (nvme_state_is_live(ns->ana_state) && !nvme_state_is_live(old))
+		nvme_mpath_set_live(ns);
+	mutex_unlock(&ns->head->lock);
+}
+
+static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
+		struct nvme_ana_group_desc *desc, void *data)
+{
+	u32 nr_nsids = le32_to_cpu(desc->nnsids), n = 0;
+	unsigned *nr_change_groups = data;
+	struct nvme_ns *ns;
+
+	dev_info(ctrl->device, "ANA group %d: %s.\n",
+			le32_to_cpu(desc->grpid),
+			nvme_ana_state_names[desc->state]);
+
+	if (desc->state == NVME_ANA_CHANGE)
+		(*nr_change_groups)++;
+
+	if (!nr_nsids)
+		return 0;
+
+	down_write(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		if (ns->head->ns_id != le32_to_cpu(desc->nsids[n]))
+			continue;
+		nvme_update_ns_ana_state(desc, ns);
+		if (++n == nr_nsids)
+			break;
+	}
+	up_write(&ctrl->namespaces_rwsem);
+	WARN_ON_ONCE(n < nr_nsids);
+	return 0;
+}
+
+static int nvme_read_ana_log(struct nvme_ctrl *ctrl, bool groups_only)
+{
+	u32 nr_change_groups = 0;
+	int error;
+
+	mutex_lock(&ctrl->ana_lock);
+	error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA,
+			groups_only ? NVME_ANA_LOG_RGO : 0,
+			ctrl->ana_log_buf, ctrl->ana_log_size, 0);
+	if (error) {
+		dev_warn(ctrl->device, "Failed to get ANA log: %d\n", error);
+		goto out_unlock;
+	}
+
+	error = nvme_parse_ana_log(ctrl, &nr_change_groups,
+			nvme_update_ana_state);
+	if (error)
+		goto out_unlock;
+
+	/*
+	 * In theory we should have an ANATT timer per group as they might enter
+	 * the change state at different times.  But that is a lot of overhead
+	 * just to protect against a target that keeps entering new changes
+	 * states while never finishing previous ones.  But we'll still
+	 * eventually time out once all groups are in change state, so this
+	 * isn't a big deal.
+	 *
+	 * We also double the ANATT value to provide some slack for transports
+	 * or AEN processing overhead.
+	 */
+	if (nr_change_groups)
+		mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+	else
+		del_timer_sync(&ctrl->anatt_timer);
+out_unlock:
+	mutex_unlock(&ctrl->ana_lock);
+	return error;
+}
+
+static void nvme_ana_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+
+	nvme_read_ana_log(ctrl, false);
+}
+
+static void nvme_anatt_timeout(struct timer_list *t)
+{
+	struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+
+	dev_info(ctrl->device, "ANATT timeout, resetting controller.\n");
+	nvme_reset_ctrl(ctrl);
+}
+
+void nvme_mpath_stop(struct nvme_ctrl *ctrl)
+{
+	if (!nvme_ctrl_use_ana(ctrl))
+		return;
+	del_timer_sync(&ctrl->anatt_timer);
+	cancel_work_sync(&ctrl->ana_work);
+}
+
+static ssize_t ana_grpid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", nvme_get_ns_from_dev(dev)->ana_grpid);
+}
+DEVICE_ATTR_RO(ana_grpid);
+
+static ssize_t ana_state_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
+
+	return sprintf(buf, "%s\n", nvme_ana_state_names[ns->ana_state]);
+}
+DEVICE_ATTR_RO(ana_state);
+
+static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
+		struct nvme_ana_group_desc *desc, void *data)
+{
+	struct nvme_ns *ns = data;
+
+	if (ns->ana_grpid == le32_to_cpu(desc->grpid)) {
+		nvme_update_ns_ana_state(desc, ns);
+		return -ENXIO; /* just break out of the loop */
+	}
+
+	return 0;
+}
+
+void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
+{
+	if (nvme_ctrl_use_ana(ns->ctrl)) {
+		mutex_lock(&ns->ctrl->ana_lock);
+		ns->ana_grpid = le32_to_cpu(id->anagrpid);
+		nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+		mutex_unlock(&ns->ctrl->ana_lock);
+	} else {
+		mutex_lock(&ns->head->lock);
+		ns->ana_state = NVME_ANA_OPTIMIZED; 
+		nvme_mpath_set_live(ns);
+		mutex_unlock(&ns->head->lock);
 	}
-	mutex_unlock(&head->subsys->lock);
 }
 
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
 		return;
-	sysfs_remove_group(&disk_to_dev(head->disk)->kobj,
-			   &nvme_ns_id_attr_group);
-	del_gendisk(head->disk);
+	if (head->disk->flags & GENHD_FL_UP) {
+		sysfs_remove_group(&disk_to_dev(head->disk)->kobj,
+				   &nvme_ns_id_attr_group);
+		del_gendisk(head->disk);
+	}
 	blk_set_queue_dying(head->disk->queue);
 	/* make sure all pending bios are cleaned up */
 	kblockd_schedule_work(&head->requeue_work);
@@ -243,3 +506,52 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	blk_cleanup_queue(head->disk->queue);
 	put_disk(head->disk);
 }
+
+int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
+{
+	int error;
+
+	if (!nvme_ctrl_use_ana(ctrl))
+		return 0;
+
+	ctrl->anacap = id->anacap;
+	ctrl->anatt = id->anatt;
+	ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
+	ctrl->anagrpmax = le32_to_cpu(id->anagrpmax);
+
+	mutex_init(&ctrl->ana_lock);
+	timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+	ctrl->ana_log_size = sizeof(struct nvme_ana_rsp_hdr) +
+		ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
+	if (!(ctrl->anacap & (1 << 6)))
+		ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+
+	if (ctrl->ana_log_size > ctrl->max_hw_sectors << SECTOR_SHIFT) {
+		dev_err(ctrl->device,
+			"ANA log page size (%zd) larger than MDTS (%d).\n",
+			ctrl->ana_log_size,
+			ctrl->max_hw_sectors << SECTOR_SHIFT);
+		dev_err(ctrl->device, "disabling ANA support.\n");
+		return 0;
+	}
+
+	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
+	if (!ctrl->ana_log_buf)
+		goto out;
+
+	error = nvme_read_ana_log(ctrl, true);
+	if (error)
+		goto out_free_ana_log_buf;
+	return 0;
+out_free_ana_log_buf:
+	kfree(ctrl->ana_log_buf);
+out:
+	return -ENOMEM;
+}
+
+void nvme_mpath_uninit(struct nvme_ctrl *ctrl)
+{
+	kfree(ctrl->ana_log_buf);
+}
+

commit 8decf5d5b9f3f72b802a017b0b035f7db0592acf
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 4 08:43:00 2018 +0200

    nvme: remove nvme_req_needs_failover
    
    Now that we just call out to blk_path_error there isn't really any good
    reason to not merge it into the only caller.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 1ffd3e8b13a1..348aa405b641 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -56,13 +56,6 @@ void nvme_failover_req(struct request *req)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
-bool nvme_req_needs_failover(struct request *req, blk_status_t error)
-{
-	if (!(req->cmd_flags & REQ_NVME_MPATH))
-		return false;
-	return blk_path_error(error);
-}
-
 void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;

commit 2796b569591c6f0681303f148a55e50c2cc8304a
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Jun 7 10:38:47 2018 +0200

    nvme: add bio remapping tracepoint
    
    Adding a tracepoint to trace bio remapping for native nvme multipath.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index d7b664ae5923..1ffd3e8b13a1 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -12,6 +12,7 @@
  */
 
 #include <linux/moduleparam.h>
+#include <trace/events/block.h>
 #include "nvme.h"
 
 static bool multipath = true;
@@ -111,6 +112,9 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 	if (likely(ns)) {
 		bio->bi_disk = ns->disk;
 		bio->bi_opf |= REQ_NVME_MPATH;
+		trace_block_bio_remap(bio->bi_disk->queue, bio,
+				      disk_devt(ns->head->disk),
+				      bio->bi_iter.bi_sector);
 		ret = direct_make_request(bio);
 	} else if (!list_empty_careful(&head->list)) {
 		dev_warn_ratelimited(dev, "no path available - requeuing I/O\n");

commit a785dbccd95c37606c720580714f5a7a8b3255f1
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Apr 26 14:22:41 2018 -0600

    nvme/multipath: Fix multipath disabled naming collisions
    
    When CONFIG_NVME_MULTIPATH is set, but we're not using nvme to multipath,
    namespaces with multiple paths were not creating unique names due to
    reusing the same instance number from the namespace's head.
    
    This patch fixes this by falling back to the non-multipath naming method
    when the parameter disabled using multipath.
    
    Reported-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 3ded009e7631..d7b664ae5923 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -19,6 +19,28 @@ module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 
+/*
+ * If multipathing is enabled we need to always use the subsystem instance
+ * number for numbering our devices to avoid conflicts between subsystems that
+ * have multiple controllers and thus use the multipath-aware subsystem node
+ * and those that have a single controller and use the controller node
+ * directly.
+ */
+void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
+			struct nvme_ctrl *ctrl, int *flags)
+{
+	if (!multipath) {
+		sprintf(disk_name, "nvme%dn%d", ctrl->instance, ns->head->instance);
+	} else if (ns->head->disk) {
+		sprintf(disk_name, "nvme%dc%dn%d", ctrl->subsys->instance,
+				ctrl->cntlid, ns->head->instance);
+		*flags = GENHD_FL_HIDDEN;
+	} else {
+		sprintf(disk_name, "nvme%dn%d", ctrl->subsys->instance,
+				ns->head->instance);
+	}
+}
+
 void nvme_failover_req(struct request *req)
 {
 	struct nvme_ns *ns = req->q->queuedata;

commit 5cadde8019a6a80550fdde92d5a3327565974eab
Author: Keith Busch <keith.busch@intel.com>
Date:   Thu Apr 26 14:24:29 2018 -0600

    nvme/multipath: Disable runtime writable enabling parameter
    
    We can't allow the user to change multipath settings at runtime, as this
    will create naming conflicts due to the different naming schemes used
    for each mode.
    
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 956e0b8e9c4d..3ded009e7631 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -15,7 +15,7 @@
 #include "nvme.h"
 
 static bool multipath = true;
-module_param(multipath, bool, 0644);
+module_param(multipath, bool, 0444);
 MODULE_PARM_DESC(multipath,
 	"turn on native support for multiple controllers per subsystem");
 

commit 3526dd0c7832f1011a0477cc6d903662bae05ea8
Merge: dd972f924df6 bc6d65e6dc89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 5 14:27:02 2018 -0700

    Merge tag 'for-4.17/block-20180402' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
     "It's a pretty quiet round this time, which is nice. This contains:
    
       - series from Bart, cleaning up the way we set/test/clear atomic
         queue flags.
    
       - series from Bart, fixing races between gendisk and queue
         registration and removal.
    
       - set of bcache fixes and improvements from various folks, by way of
         Michael Lyle.
    
       - set of lightnvm updates from Matias, most of it being the 1.2 to
         2.0 transition.
    
       - removal of unused DIO flags from Nikolay.
    
       - blk-mq/sbitmap memory ordering fixes from Omar.
    
       - divide-by-zero fix for BFQ from Paolo.
    
       - minor documentation patches from Randy.
    
       - timeout fix from Tejun.
    
       - Alpha "can't write a char atomically" fix from Mikulas.
    
       - set of NVMe fixes by way of Keith.
    
       - bsg and bsg-lib improvements from Christoph.
    
       - a few sed-opal fixes from Jonas.
    
       - cdrom check-disk-change deadlock fix from Maurizio.
    
       - various little fixes, comment fixes, etc from various folks"
    
    * tag 'for-4.17/block-20180402' of git://git.kernel.dk/linux-block: (139 commits)
      blk-mq: Directly schedule q->timeout_work when aborting a request
      blktrace: fix comment in blktrace_api.h
      lightnvm: remove function name in strings
      lightnvm: pblk: remove some unnecessary NULL checks
      lightnvm: pblk: don't recover unwritten lines
      lightnvm: pblk: implement 2.0 support
      lightnvm: pblk: implement get log report chunk
      lightnvm: pblk: rename ppaf* to addrf*
      lightnvm: pblk: check for supported version
      lightnvm: implement get log report chunk helpers
      lightnvm: make address conversions depend on generic device
      lightnvm: add support for 2.0 address format
      lightnvm: normalize geometry nomenclature
      lightnvm: complete geo structure with maxoc*
      lightnvm: add shorten OCSSD version in geo
      lightnvm: add minor version to generic geometry
      lightnvm: simplify geometry structure
      lightnvm: pblk: refactor init/exit sequences
      lightnvm: Avoid validation of default op value
      lightnvm: centralize permission check for lightnvm ioctl
      ...

commit 765cc031cddde40bdc279e8e2697571c7956c54e
Author: Jianchao Wang <jianchao.w.wang@oracle.com>
Date:   Mon Feb 12 20:54:46 2018 +0800

    nvme: change namespaces_mutext to namespaces_rwsem
    
    namespaces_mutext is used to synchronize the operations on ctrl
    namespaces list. Most of the time, it is a read operation.
    
    On the other hand, there are many interfaces in nvme core that
    need this lock, such as nvme_wait_freeze, and even more interfaces
    will be added. If we use mutex here, circular dependency could be
    introduced easily. For example:
    context A                  context B
    nvme_xxx                   nvme_xxx
    hold namespaces_mutext     require namespaces_mutext
    sync context B
    
    So it is better to change it from mutex to rwsem.
    
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 7283d7149baf..affd67021b6f 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -44,12 +44,12 @@ void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
 
-	mutex_lock(&ctrl->namespaces_mutex);
+	down_read(&ctrl->namespaces_rwsem);
 	list_for_each_entry(ns, &ctrl->namespaces, list) {
 		if (ns->head->disk)
 			kblockd_schedule_work(&ns->head->requeue_work);
 	}
-	mutex_unlock(&ctrl->namespaces_mutex);
+	up_read(&ctrl->namespaces_rwsem);
 }
 
 static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head)

commit 8b904b5b6b58b9a29dcf3f82d936d9e7fd69fda6
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Mar 7 17:10:10 2018 -0800

    block: Use blk_queue_flag_*() in drivers instead of queue_flag_*()
    
    This patch has been generated as follows:
    
    for verb in set_unlocked clear_unlocked set clear; do
      replace-in-files queue_flag_${verb} blk_queue_flag_${verb%_unlocked} \
        $(git grep -lw queue_flag_${verb} drivers block/bsg*)
    done
    
    Except for protecting all queue flag changes with the queue lock
    this patch does not change any functionality.
    
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Shaohua Li <shli@fb.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Acked-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 88440562a197..7283d7149baf 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -168,7 +168,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	q->queuedata = head;
 	blk_queue_make_request(q, nvme_ns_head_make_request);
 	q->poll_fn = nvme_ns_head_poll;
-	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
 	/* set to a default value for 512 until disk is validated */
 	blk_queue_logical_block_size(q, 512);
 

commit 8a30ecc6e0ecbb9ae95daf499b2680b885ed0349
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Mar 7 14:13:58 2018 +0100

    Revert "nvme: create 'slaves' and 'holders' entries for hidden controllers"
    
    This reverts commit e9a48034d7d1318ece7d4a235838a86c94db9d68.
    
    The slaves and holders link for the hidden gendisks confuse lsblk so that
    it errors out on, or doesn't report the nvme multipath devices.  Given
    that we don't need holder relationships for something that can't even be
    directly accessed we should just stop creating those links.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reported-by: Potnuri Bharat Teja <bharat@chelsio.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Keith Busch <keith.busch@intel.com>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index b7e5c6db4d92..060f69e03427 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -210,25 +210,6 @@ void nvme_mpath_add_disk(struct nvme_ns_head *head)
 	mutex_unlock(&head->subsys->lock);
 }
 
-void nvme_mpath_add_disk_links(struct nvme_ns *ns)
-{
-	struct kobject *slave_disk_kobj, *holder_disk_kobj;
-
-	if (!ns->head->disk)
-		return;
-
-	slave_disk_kobj = &disk_to_dev(ns->disk)->kobj;
-	if (sysfs_create_link(ns->head->disk->slave_dir, slave_disk_kobj,
-			kobject_name(slave_disk_kobj)))
-		return;
-
-	holder_disk_kobj = &disk_to_dev(ns->head->disk)->kobj;
-	if (sysfs_create_link(ns->disk->part0.holder_dir, holder_disk_kobj,
-			kobject_name(holder_disk_kobj)))
-		sysfs_remove_link(ns->head->disk->slave_dir,
-			kobject_name(slave_disk_kobj));
-}
-
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
@@ -243,14 +224,3 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	blk_cleanup_queue(head->disk->queue);
 	put_disk(head->disk);
 }
-
-void nvme_mpath_remove_disk_links(struct nvme_ns *ns)
-{
-	if (!ns->head->disk)
-		return;
-
-	sysfs_remove_link(ns->disk->part0.holder_dir,
-			kobject_name(&disk_to_dev(ns->head->disk)->kobj));
-	sysfs_remove_link(ns->head->disk->slave_dir,
-			kobject_name(&disk_to_dev(ns->disk)->kobj));
-}

commit 5ee0524ba137fe928a88b440d014e3c8451fb32c
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Wed Feb 28 10:15:31 2018 -0800

    block: Add 'lock' as third argument to blk_alloc_queue_node()
    
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Philipp Reisner <philipp.reisner@linbit.com>
    Cc: Ulf Hansson <ulf.hansson@linaro.org>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index b7e5c6db4d92..88440562a197 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -162,7 +162,7 @@ int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
 	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
 		return 0;
 
-	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE, NULL);
 	if (!q)
 		goto out;
 	q->queuedata = head;

commit 9bd82b1a4418d9b7db000bf557ed608f2872b7c9
Author: Baegjae Sung <baegjae@gmail.com>
Date:   Wed Feb 28 16:06:04 2018 +0900

    nvme-multipath: fix sysfs dangerously created links
    
    If multipathing is enabled, each NVMe subsystem creates a head
    namespace (e.g., nvme0n1) and multiple private namespaces
    (e.g., nvme0c0n1 and nvme0c1n1) in sysfs. When creating links for
    private namespaces, links of head namespace are used, so the
    namespace creation order must be followed (e.g., nvme0n1 ->
    nvme0c1n1). If the order is not followed, links of sysfs will be
    incomplete or kernel panic will occur.
    
    The kernel panic was:
      kernel BUG at fs/sysfs/symlink.c:27!
      Call Trace:
        nvme_mpath_add_disk_links+0x5d/0x80 [nvme_core]
        nvme_validate_ns+0x5c2/0x850 [nvme_core]
        nvme_scan_work+0x1af/0x2d0 [nvme_core]
    
    Correct order
    Context A     Context B
    nvme0n1
    nvme0c0n1     nvme0c1n1
    
    Incorrect order
    Context A     Context B
                  nvme0c1n1
    nvme0n1
    nvme0c0n1
    
    The nvme_mpath_add_disk (for creating head namespace) is called
    just before the nvme_mpath_add_disk_links (for creating private
    namespaces). In nvme_mpath_add_disk, the first context acquires
    the lock of subsystem and creates a head namespace, and other
    contexts do nothing by checking GENHD_FL_UP of a head namespace
    after waiting to acquire the lock. We verified the code with or
    without multipathing using three vendors of dual-port NVMe SSDs.
    
    Signed-off-by: Baegjae Sung <baegjae@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Keith Busch <keith.busch@intel.com>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 3b211d9e58b8..b7e5c6db4d92 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -198,11 +198,16 @@ void nvme_mpath_add_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
 		return;
-	device_add_disk(&head->subsys->dev, head->disk);
-	if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
-			&nvme_ns_id_attr_group))
-		pr_warn("%s: failed to create sysfs group for identification\n",
-			head->disk->disk_name);
+
+	mutex_lock(&head->subsys->lock);
+	if (!(head->disk->flags & GENHD_FL_UP)) {
+		device_add_disk(&head->subsys->dev, head->disk);
+		if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
+				&nvme_ns_id_attr_group))
+			pr_warn("%s: failed to create sysfs group for identification\n",
+				head->disk->disk_name);
+	}
+	mutex_unlock(&head->subsys->lock);
 }
 
 void nvme_mpath_add_disk_links(struct nvme_ns *ns)

commit e1f425e770d21a34f51d7284e55f3fa984f8e275
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue Jan 9 12:04:17 2018 -0700

    nvme/multipath: Use blk_path_error
    
    Uses common code for determining if an error should be retried on
    alternate path.
    
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index ae9abb600c0f..3b211d9e58b8 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -37,19 +37,7 @@ bool nvme_req_needs_failover(struct request *req, blk_status_t error)
 {
 	if (!(req->cmd_flags & REQ_NVME_MPATH))
 		return false;
-
-	switch (error) {
-	case BLK_STS_NOTSUPP:
-	case BLK_STS_NOSPC:
-	case BLK_STS_TARGET:
-	case BLK_STS_NEXUS:
-	case BLK_STS_MEDIUM:
-	case BLK_STS_PROTECTION:
-		return false;
-	}
-
-	/* Everything else could be a path failure, so should be retried */
-	return true;
+	return blk_path_error(error);
 }
 
 void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)

commit 908e45643d6450551bfbdbad3f088d4bd1f1c1fb
Author: Keith Busch <keith.busch@intel.com>
Date:   Tue Jan 9 12:04:15 2018 -0700

    nvme/multipath: Consult blk_status_t for failover
    
    This removes nvme multipath's specific status decoding to see if failover
    is needed, using the generic blk_status_t that was decoded earlier. This
    abstraction from the raw NVMe status means all status decoding exists
    in one place.
    
    Acked-by: Mike Snitzer <snitzer@redhat.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 1218a9fca846..ae9abb600c0f 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -33,46 +33,18 @@ void nvme_failover_req(struct request *req)
 	kblockd_schedule_work(&ns->head->requeue_work);
 }
 
-bool nvme_req_needs_failover(struct request *req)
+bool nvme_req_needs_failover(struct request *req, blk_status_t error)
 {
 	if (!(req->cmd_flags & REQ_NVME_MPATH))
 		return false;
 
-	switch (nvme_req(req)->status & 0x7ff) {
-	/*
-	 * Generic command status:
-	 */
-	case NVME_SC_INVALID_OPCODE:
-	case NVME_SC_INVALID_FIELD:
-	case NVME_SC_INVALID_NS:
-	case NVME_SC_LBA_RANGE:
-	case NVME_SC_CAP_EXCEEDED:
-	case NVME_SC_RESERVATION_CONFLICT:
-		return false;
-
-	/*
-	 * I/O command set specific error.  Unfortunately these values are
-	 * reused for fabrics commands, but those should never get here.
-	 */
-	case NVME_SC_BAD_ATTRIBUTES:
-	case NVME_SC_INVALID_PI:
-	case NVME_SC_READ_ONLY:
-	case NVME_SC_ONCS_NOT_SUPPORTED:
-		WARN_ON_ONCE(nvme_req(req)->cmd->common.opcode ==
-			nvme_fabrics_command);
-		return false;
-
-	/*
-	 * Media and Data Integrity Errors:
-	 */
-	case NVME_SC_WRITE_FAULT:
-	case NVME_SC_READ_ERROR:
-	case NVME_SC_GUARD_CHECK:
-	case NVME_SC_APPTAG_CHECK:
-	case NVME_SC_REFTAG_CHECK:
-	case NVME_SC_COMPARE_FAILED:
-	case NVME_SC_ACCESS_DENIED:
-	case NVME_SC_UNWRITTEN_BLOCK:
+	switch (error) {
+	case BLK_STS_NOTSUPP:
+	case BLK_STS_NOSPC:
+	case BLK_STS_TARGET:
+	case BLK_STS_NEXUS:
+	case BLK_STS_MEDIUM:
+	case BLK_STS_PROTECTION:
 		return false;
 	}
 

commit 89c4aff6d4f71726f22e567f046dc1dd73c35de1
Author: Colin Ian King <colin.king@canonical.com>
Date:   Tue Nov 14 14:26:27 2017 +0000

    nvme: fix spelling mistake: "requeing" -> "requeuing"
    
    Trivial fix to spelling mistake in dev_warn_ratelimited message text
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 78d92151a904..1218a9fca846 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -131,7 +131,7 @@ static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
 		bio->bi_opf |= REQ_NVME_MPATH;
 		ret = direct_make_request(bio);
 	} else if (!list_empty_careful(&head->list)) {
-		dev_warn_ratelimited(dev, "no path available - requeing I/O\n");
+		dev_warn_ratelimited(dev, "no path available - requeuing I/O\n");
 
 		spin_lock_irq(&head->requeue_lock);
 		bio_list_add(&head->requeue_list, bio);

commit e9a48034d7d1318ece7d4a235838a86c94db9d68
Author: Hannes Reinecke <hare@suse.de>
Date:   Thu Nov 9 17:57:06 2017 +0100

    nvme: create 'slaves' and 'holders' entries for hidden controllers
    
    When creating nvme multipath devices we should populate the 'slaves' and
    'holders' directorys properly to aid userspace topology detection.
    
    Signed-off-by: Hannes Reinecke <hare@suse.com>
    [hch: split from a larger patch, compile fix for CONFIG_NVME_MULTIPATH=n]
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 850275896e49..78d92151a904 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -245,6 +245,25 @@ void nvme_mpath_add_disk(struct nvme_ns_head *head)
 			head->disk->disk_name);
 }
 
+void nvme_mpath_add_disk_links(struct nvme_ns *ns)
+{
+	struct kobject *slave_disk_kobj, *holder_disk_kobj;
+
+	if (!ns->head->disk)
+		return;
+
+	slave_disk_kobj = &disk_to_dev(ns->disk)->kobj;
+	if (sysfs_create_link(ns->head->disk->slave_dir, slave_disk_kobj,
+			kobject_name(slave_disk_kobj)))
+		return;
+
+	holder_disk_kobj = &disk_to_dev(ns->head->disk)->kobj;
+	if (sysfs_create_link(ns->disk->part0.holder_dir, holder_disk_kobj,
+			kobject_name(holder_disk_kobj)))
+		sysfs_remove_link(ns->head->disk->slave_dir,
+			kobject_name(slave_disk_kobj));
+}
+
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
@@ -259,3 +278,14 @@ void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 	blk_cleanup_queue(head->disk->queue);
 	put_disk(head->disk);
 }
+
+void nvme_mpath_remove_disk_links(struct nvme_ns *ns)
+{
+	if (!ns->head->disk)
+		return;
+
+	sysfs_remove_link(ns->disk->part0.holder_dir,
+			kobject_name(&disk_to_dev(ns->head->disk)->kobj));
+	sysfs_remove_link(ns->head->disk->slave_dir,
+			kobject_name(&disk_to_dev(ns->disk)->kobj));
+}

commit 5b85b826b8925b3f1910b6613eb82c4df240a5b5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 9 13:51:03 2017 +0100

    nvme: also expose the namespace identification sysfs files for mpath nodes
    
    We do this by adding a helper that returns the ns_head for a device that
    can belong to either the per-controller or per-subsystem block device
    nodes, and otherwise reuse all the existing code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
index 062754ebebfd..850275896e49 100644
--- a/drivers/nvme/host/multipath.c
+++ b/drivers/nvme/host/multipath.c
@@ -239,12 +239,18 @@ void nvme_mpath_add_disk(struct nvme_ns_head *head)
 	if (!head->disk)
 		return;
 	device_add_disk(&head->subsys->dev, head->disk);
+	if (sysfs_create_group(&disk_to_dev(head->disk)->kobj,
+			&nvme_ns_id_attr_group))
+		pr_warn("%s: failed to create sysfs group for identification\n",
+			head->disk->disk_name);
 }
 
 void nvme_mpath_remove_disk(struct nvme_ns_head *head)
 {
 	if (!head->disk)
 		return;
+	sysfs_remove_group(&disk_to_dev(head->disk)->kobj,
+			   &nvme_ns_id_attr_group);
 	del_gendisk(head->disk);
 	blk_set_queue_dying(head->disk->queue);
 	/* make sure all pending bios are cleaned up */

commit 32acab3181c7053c775ca128c3a5c6ce50197d7f
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 2 12:59:30 2017 +0100

    nvme: implement multipath access to nvme subsystems
    
    This patch adds native multipath support to the nvme driver.  For each
    namespace we create only single block device node, which can be used
    to access that namespace through any of the controllers that refer to it.
    The gendisk for each controllers path to the name space still exists
    inside the kernel, but is hidden from userspace.  The character device
    nodes are still available on a per-controller basis.  A new link from
    the sysfs directory for the subsystem allows to find all controllers
    for a given subsystem.
    
    Currently we will always send I/O to the first available path, this will
    be changed once the NVMe Asynchronous Namespace Access (ANA) TP is
    ratified and implemented, at which point we will look at the ANA state
    for each namespace.  Another possibility that was prototyped is to
    use the path that is closes to the submitting NUMA code, which will be
    mostly interesting for PCI, but might also be useful for RDMA or FC
    transports in the future.  There is not plan to implement round robin
    or I/O service time path selectors, as those are not scalable with
    the performance rates provided by NVMe.
    
    The multipath device will go away once all paths to it disappear,
    any delay to keep it alive needs to be implemented at the controller
    level.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/nvme/host/multipath.c b/drivers/nvme/host/multipath.c
new file mode 100644
index 000000000000..062754ebebfd
--- /dev/null
+++ b/drivers/nvme/host/multipath.c
@@ -0,0 +1,255 @@
+/*
+ * Copyright (c) 2017 Christoph Hellwig.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/moduleparam.h>
+#include "nvme.h"
+
+static bool multipath = true;
+module_param(multipath, bool, 0644);
+MODULE_PARM_DESC(multipath,
+	"turn on native support for multiple controllers per subsystem");
+
+void nvme_failover_req(struct request *req)
+{
+	struct nvme_ns *ns = req->q->queuedata;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ns->head->requeue_lock, flags);
+	blk_steal_bios(&ns->head->requeue_list, req);
+	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
+	blk_mq_end_request(req, 0);
+
+	nvme_reset_ctrl(ns->ctrl);
+	kblockd_schedule_work(&ns->head->requeue_work);
+}
+
+bool nvme_req_needs_failover(struct request *req)
+{
+	if (!(req->cmd_flags & REQ_NVME_MPATH))
+		return false;
+
+	switch (nvme_req(req)->status & 0x7ff) {
+	/*
+	 * Generic command status:
+	 */
+	case NVME_SC_INVALID_OPCODE:
+	case NVME_SC_INVALID_FIELD:
+	case NVME_SC_INVALID_NS:
+	case NVME_SC_LBA_RANGE:
+	case NVME_SC_CAP_EXCEEDED:
+	case NVME_SC_RESERVATION_CONFLICT:
+		return false;
+
+	/*
+	 * I/O command set specific error.  Unfortunately these values are
+	 * reused for fabrics commands, but those should never get here.
+	 */
+	case NVME_SC_BAD_ATTRIBUTES:
+	case NVME_SC_INVALID_PI:
+	case NVME_SC_READ_ONLY:
+	case NVME_SC_ONCS_NOT_SUPPORTED:
+		WARN_ON_ONCE(nvme_req(req)->cmd->common.opcode ==
+			nvme_fabrics_command);
+		return false;
+
+	/*
+	 * Media and Data Integrity Errors:
+	 */
+	case NVME_SC_WRITE_FAULT:
+	case NVME_SC_READ_ERROR:
+	case NVME_SC_GUARD_CHECK:
+	case NVME_SC_APPTAG_CHECK:
+	case NVME_SC_REFTAG_CHECK:
+	case NVME_SC_COMPARE_FAILED:
+	case NVME_SC_ACCESS_DENIED:
+	case NVME_SC_UNWRITTEN_BLOCK:
+		return false;
+	}
+
+	/* Everything else could be a path failure, so should be retried */
+	return true;
+}
+
+void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	mutex_lock(&ctrl->namespaces_mutex);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		if (ns->head->disk)
+			kblockd_schedule_work(&ns->head->requeue_work);
+	}
+	mutex_unlock(&ctrl->namespaces_mutex);
+}
+
+static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head)
+{
+	struct nvme_ns *ns;
+
+	list_for_each_entry_rcu(ns, &head->list, siblings) {
+		if (ns->ctrl->state == NVME_CTRL_LIVE) {
+			rcu_assign_pointer(head->current_path, ns);
+			return ns;
+		}
+	}
+
+	return NULL;
+}
+
+inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
+{
+	struct nvme_ns *ns = srcu_dereference(head->current_path, &head->srcu);
+
+	if (unlikely(!ns || ns->ctrl->state != NVME_CTRL_LIVE))
+		ns = __nvme_find_path(head);
+	return ns;
+}
+
+static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
+		struct bio *bio)
+{
+	struct nvme_ns_head *head = q->queuedata;
+	struct device *dev = disk_to_dev(head->disk);
+	struct nvme_ns *ns;
+	blk_qc_t ret = BLK_QC_T_NONE;
+	int srcu_idx;
+
+	srcu_idx = srcu_read_lock(&head->srcu);
+	ns = nvme_find_path(head);
+	if (likely(ns)) {
+		bio->bi_disk = ns->disk;
+		bio->bi_opf |= REQ_NVME_MPATH;
+		ret = direct_make_request(bio);
+	} else if (!list_empty_careful(&head->list)) {
+		dev_warn_ratelimited(dev, "no path available - requeing I/O\n");
+
+		spin_lock_irq(&head->requeue_lock);
+		bio_list_add(&head->requeue_list, bio);
+		spin_unlock_irq(&head->requeue_lock);
+	} else {
+		dev_warn_ratelimited(dev, "no path - failing I/O\n");
+
+		bio->bi_status = BLK_STS_IOERR;
+		bio_endio(bio);
+	}
+
+	srcu_read_unlock(&head->srcu, srcu_idx);
+	return ret;
+}
+
+static bool nvme_ns_head_poll(struct request_queue *q, blk_qc_t qc)
+{
+	struct nvme_ns_head *head = q->queuedata;
+	struct nvme_ns *ns;
+	bool found = false;
+	int srcu_idx;
+
+	srcu_idx = srcu_read_lock(&head->srcu);
+	ns = srcu_dereference(head->current_path, &head->srcu);
+	if (likely(ns && ns->ctrl->state == NVME_CTRL_LIVE))
+		found = ns->queue->poll_fn(q, qc);
+	srcu_read_unlock(&head->srcu, srcu_idx);
+	return found;
+}
+
+static void nvme_requeue_work(struct work_struct *work)
+{
+	struct nvme_ns_head *head =
+		container_of(work, struct nvme_ns_head, requeue_work);
+	struct bio *bio, *next;
+
+	spin_lock_irq(&head->requeue_lock);
+	next = bio_list_get(&head->requeue_list);
+	spin_unlock_irq(&head->requeue_lock);
+
+	while ((bio = next) != NULL) {
+		next = bio->bi_next;
+		bio->bi_next = NULL;
+
+		/*
+		 * Reset disk to the mpath node and resubmit to select a new
+		 * path.
+		 */
+		bio->bi_disk = head->disk;
+		generic_make_request(bio);
+	}
+}
+
+int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
+{
+	struct request_queue *q;
+	bool vwc = false;
+
+	bio_list_init(&head->requeue_list);
+	spin_lock_init(&head->requeue_lock);
+	INIT_WORK(&head->requeue_work, nvme_requeue_work);
+
+	/*
+	 * Add a multipath node if the subsystems supports multiple controllers.
+	 * We also do this for private namespaces as the namespace sharing data could
+	 * change after a rescan.
+	 */
+	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
+		return 0;
+
+	q = blk_alloc_queue_node(GFP_KERNEL, NUMA_NO_NODE);
+	if (!q)
+		goto out;
+	q->queuedata = head;
+	blk_queue_make_request(q, nvme_ns_head_make_request);
+	q->poll_fn = nvme_ns_head_poll;
+	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);
+	/* set to a default value for 512 until disk is validated */
+	blk_queue_logical_block_size(q, 512);
+
+	/* we need to propagate up the VMC settings */
+	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
+		vwc = true;
+	blk_queue_write_cache(q, vwc, vwc);
+
+	head->disk = alloc_disk(0);
+	if (!head->disk)
+		goto out_cleanup_queue;
+	head->disk->fops = &nvme_ns_head_ops;
+	head->disk->private_data = head;
+	head->disk->queue = q;
+	head->disk->flags = GENHD_FL_EXT_DEVT;
+	sprintf(head->disk->disk_name, "nvme%dn%d",
+			ctrl->subsys->instance, head->instance);
+	return 0;
+
+out_cleanup_queue:
+	blk_cleanup_queue(q);
+out:
+	return -ENOMEM;
+}
+
+void nvme_mpath_add_disk(struct nvme_ns_head *head)
+{
+	if (!head->disk)
+		return;
+	device_add_disk(&head->subsys->dev, head->disk);
+}
+
+void nvme_mpath_remove_disk(struct nvme_ns_head *head)
+{
+	if (!head->disk)
+		return;
+	del_gendisk(head->disk);
+	blk_set_queue_dying(head->disk->queue);
+	/* make sure all pending bios are cleaned up */
+	kblockd_schedule_work(&head->requeue_work);
+	flush_work(&head->requeue_work);
+	blk_cleanup_queue(head->disk->queue);
+	put_disk(head->disk);
+}
