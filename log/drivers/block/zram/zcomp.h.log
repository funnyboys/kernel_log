commit 19f545b6e07f753c4dc639c2f0ab52345733b6a8
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Wed May 27 22:11:19 2020 +0200

    zram: Use local lock to protect per-CPU data
    
    The zcomp driver uses per-CPU compression. The per-CPU data pointer is
    acquired with get_cpu_ptr() which implicitly disables preemption.
    It allocates memory inside the preempt disabled region which conflicts
    with the PREEMPT_RT semantics.
    
    Replace the implicit preemption control with an explicit local lock.
    This allows RT kernels to substitute it with a real per CPU lock, which
    serializes the access but keeps the code section preemptible. On non RT
    kernels this maps to preempt_disable() as before, i.e. no functional
    change.
    
    [bigeasy: Use local_lock(), description, drop reordering]
    
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200527201119.1692513-8-bigeasy@linutronix.de

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 72c2ee4d843e..40f6420f4b2e 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -5,8 +5,11 @@
 
 #ifndef _ZCOMP_H_
 #define _ZCOMP_H_
+#include <linux/local_lock.h>
 
 struct zcomp_strm {
+	/* The members ->buffer and ->tfm are protected by ->lock. */
+	local_lock_t lock;
 	/* compression/decompression buffer */
 	void *buffer;
 	struct crypto_comp *tfm;

commit ed19f19256be2949af1ab5634e62178d30a355c2
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed May 27 22:11:18 2020 +0200

    zram: Allocate struct zcomp_strm as per-CPU memory
    
    zcomp::stream is a per-CPU pointer, pointing to struct zcomp_strm
    which contains two pointers. Having struct zcomp_strm allocated
    directly as per-CPU memory would avoid one additional memory
    allocation and a pointer dereference. This also simplifies the
    addition of a local_lock to struct zcomp_strm.
    
    Allocate zcomp::stream directly as per-CPU memory.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200527201119.1692513-7-bigeasy@linutronix.de

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 1806475b919d..72c2ee4d843e 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -14,7 +14,7 @@ struct zcomp_strm {
 
 /* dynamic per-device compression frontend */
 struct zcomp {
-	struct zcomp_strm * __percpu *stream;
+	struct zcomp_strm __percpu *stream;
 	const char *name;
 	struct hlist_node node;
 };

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 41c1002a7d7d..1806475b919d 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -1,10 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Copyright (C) 2014 Sergey Senozhatsky.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #ifndef _ZCOMP_H_

commit 1dd6c834fa4a75a86fecefb6d1f1525f1cb755c7
Author: Anna-Maria Gleixner <anna-maria@linutronix.de>
Date:   Sun Nov 27 00:13:46 2016 +0100

    zram: Convert to hotplug state machine
    
    Install the callbacks via the state machine with multi instance support and let
    the core invoke the callbacks on the already online CPUs.
    
    [bigeasy: wire up the multi instance stuff]
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: rt@linutronix.de
    Cc: Nitin Gupta <ngupta@vflare.org>
    Link: http://lkml.kernel.org/r/20161126231350.10321-19-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 478cac2ed465..41c1002a7d7d 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -19,11 +19,12 @@ struct zcomp_strm {
 /* dynamic per-device compression frontend */
 struct zcomp {
 	struct zcomp_strm * __percpu *stream;
-	struct notifier_block notifier;
-
 	const char *name;
+	struct hlist_node node;
 };
 
+int zcomp_cpu_up_prepare(unsigned int cpu, struct hlist_node *node);
+int zcomp_cpu_dead(unsigned int cpu, struct hlist_node *node);
 ssize_t zcomp_available_show(const char *comp, char *buf);
 bool zcomp_available_algorithm(const char *comp);
 

commit ce1ed9f98e888aa220fb09da2e2bcfcfba218a27
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Jul 26 15:22:54 2016 -0700

    zram: delete custom lzo/lz4
    
    Remove lzo/lz4 backends, we use crypto API now.
    
    [sergey.senozhatsky@gmail.com: zram-delete-custom-lzo-lz4-v3]
      Link: http://lkml.kernel.org/r/20160604024902.11778-6-sergey.senozhatsky@gmail.com
    Link: http://lkml.kernel.org/r/20160531122017.2878-7-sergey.senozhatsky@gmail.com
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index c914ab7972ef..478cac2ed465 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -16,24 +16,9 @@ struct zcomp_strm {
 	struct crypto_comp *tfm;
 };
 
-/* static compression backend */
-struct zcomp_backend {
-	int (*compress)(const unsigned char *src, unsigned char *dst,
-			size_t *dst_len, void *private);
-
-	int (*decompress)(const unsigned char *src, size_t src_len,
-			unsigned char *dst);
-
-	void *(*create)(gfp_t flags);
-	void (*destroy)(void *private);
-
-	const char *name;
-};
-
 /* dynamic per-device compression frontend */
 struct zcomp {
 	struct zcomp_strm * __percpu *stream;
-	struct zcomp_backend *backend;
 	struct notifier_block notifier;
 
 	const char *name;

commit ebaf9ab56d9d5f350969bd1ea8f47234623c9684
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Jul 26 15:22:45 2016 -0700

    zram: switch to crypto compress API
    
    We don't have an idle zstreams list anymore and our write path now works
    absolutely differently, preventing preemption during compression.  This
    removes possibilities of read paths preempting writes at wrong places
    (which could badly affect the performance of both paths) and at the same
    time opens the door for a move from custom LZO/LZ4 compression backends
    implementation to a more generic one, using crypto compress API.
    
    Joonsoo Kim [1] attempted to do this a while ago, but faced with the
    need of introducing a new crypto API interface.  The root cause was the
    fact that crypto API compression algorithms require a compression stream
    structure (in zram terminology) for both compression and decompression
    ops, while in reality only several of compression algorithms really need
    it.  This resulted in a concept of context-less crypto API compression
    backends [2].  Both write and read paths, though, would have been
    executed with the preemption enabled, which in the worst case could have
    resulted in a decreased worst-case performance, e.g.  consider the
    following case:
    
            CPU0
    
            zram_write()
              spin_lock()
                take the last idle stream
              spin_unlock()
    
            << preempted >>
    
                    zram_read()
                      spin_lock()
                       no idle streams
                              spin_unlock()
                              schedule()
    
            resuming zram_write compression()
    
    but it took me some time to realize that, and it took even longer to
    evolve zram and to make it ready for crypto API.  The key turned out to be
    -- drop the idle streams list entirely.  Without the idle streams list we
    are free to use compression algorithms that require compression stream for
    decompression (read), because streams are now placed in per-cpu data and
    each write path has to disable preemption for compression op, almost
    completely eliminating the aforementioned case (technically, we still have
    a small chance, because write path has a fast and a slow paths and the
    slow path is executed with the preemption enabled; but the frequency of
    failed fast path is too low).
    
    TEST
    ====
    
    - 4 CPUs, x86_64 system
    - 3G zram, lzo
    - fio tests: read, randread, write, randwrite, rw, randrw
    
    test script [3] command:
     ZRAM_SIZE=3G LOG_SUFFIX=XXXX FIO_LOOPS=5 ./zram-fio-test.sh
    
                       BASE           PATCHED
    jobs1
    READ:           2527.2MB/s       2482.7MB/s
    READ:           2102.7MB/s       2045.0MB/s
    WRITE:          1284.3MB/s       1324.3MB/s
    WRITE:          1080.7MB/s       1101.9MB/s
    READ:           430125KB/s       437498KB/s
    WRITE:          430538KB/s       437919KB/s
    READ:           399593KB/s       403987KB/s
    WRITE:          399910KB/s       404308KB/s
    jobs2
    READ:           8133.5MB/s       7854.8MB/s
    READ:           7086.6MB/s       6912.8MB/s
    WRITE:          3177.2MB/s       3298.3MB/s
    WRITE:          2810.2MB/s       2871.4MB/s
    READ:           1017.6MB/s       1023.4MB/s
    WRITE:          1018.2MB/s       1023.1MB/s
    READ:           977836KB/s       984205KB/s
    WRITE:          979435KB/s       985814KB/s
    jobs3
    READ:           13557MB/s        13391MB/s
    READ:           11876MB/s        11752MB/s
    WRITE:          4641.5MB/s       4682.1MB/s
    WRITE:          4164.9MB/s       4179.3MB/s
    READ:           1453.8MB/s       1455.1MB/s
    WRITE:          1455.1MB/s       1458.2MB/s
    READ:           1387.7MB/s       1395.7MB/s
    WRITE:          1386.1MB/s       1394.9MB/s
    jobs4
    READ:           20271MB/s        20078MB/s
    READ:           18033MB/s        17928MB/s
    WRITE:          6176.8MB/s       6180.5MB/s
    WRITE:          5686.3MB/s       5705.3MB/s
    READ:           2009.4MB/s       2006.7MB/s
    WRITE:          2007.5MB/s       2004.9MB/s
    READ:           1929.7MB/s       1935.6MB/s
    WRITE:          1926.8MB/s       1932.6MB/s
    jobs5
    READ:           18823MB/s        19024MB/s
    READ:           18968MB/s        19071MB/s
    WRITE:          6191.6MB/s       6372.1MB/s
    WRITE:          5818.7MB/s       5787.1MB/s
    READ:           2011.7MB/s       1981.3MB/s
    WRITE:          2011.4MB/s       1980.1MB/s
    READ:           1949.3MB/s       1935.7MB/s
    WRITE:          1940.4MB/s       1926.1MB/s
    jobs6
    READ:           21870MB/s        21715MB/s
    READ:           19957MB/s        19879MB/s
    WRITE:          6528.4MB/s       6537.6MB/s
    WRITE:          6098.9MB/s       6073.6MB/s
    READ:           2048.6MB/s       2049.9MB/s
    WRITE:          2041.7MB/s       2042.9MB/s
    READ:           2013.4MB/s       1990.4MB/s
    WRITE:          2009.4MB/s       1986.5MB/s
    jobs7
    READ:           21359MB/s        21124MB/s
    READ:           19746MB/s        19293MB/s
    WRITE:          6660.4MB/s       6518.8MB/s
    WRITE:          6211.6MB/s       6193.1MB/s
    READ:           2089.7MB/s       2080.6MB/s
    WRITE:          2085.8MB/s       2076.5MB/s
    READ:           2041.2MB/s       2052.5MB/s
    WRITE:          2037.5MB/s       2048.8MB/s
    jobs8
    READ:           20477MB/s        19974MB/s
    READ:           18922MB/s        18576MB/s
    WRITE:          6851.9MB/s       6788.3MB/s
    WRITE:          6407.7MB/s       6347.5MB/s
    READ:           2134.8MB/s       2136.1MB/s
    WRITE:          2132.8MB/s       2134.4MB/s
    READ:           2074.2MB/s       2069.6MB/s
    WRITE:          2087.3MB/s       2082.4MB/s
    jobs9
    READ:           19797MB/s        19994MB/s
    READ:           18806MB/s        18581MB/s
    WRITE:          6878.7MB/s       6822.7MB/s
    WRITE:          6456.8MB/s       6447.2MB/s
    READ:           2141.1MB/s       2154.7MB/s
    WRITE:          2144.4MB/s       2157.3MB/s
    READ:           2084.1MB/s       2085.1MB/s
    WRITE:          2091.5MB/s       2092.5MB/s
    jobs10
    READ:           19794MB/s        19784MB/s
    READ:           18794MB/s        18745MB/s
    WRITE:          6984.4MB/s       6676.3MB/s
    WRITE:          6532.3MB/s       6342.7MB/s
    READ:           2150.6MB/s       2155.4MB/s
    WRITE:          2156.8MB/s       2161.5MB/s
    READ:           2106.4MB/s       2095.6MB/s
    WRITE:          2109.7MB/s       2098.4MB/s
    
                                        BASE                       PATCHED
    jobs1                              perfstat
    stalled-cycles-frontend     102,480,595,419 (  41.53%)    114,508,864,804 (  46.92%)
    stalled-cycles-backend       51,941,417,832 (  21.05%)     46,836,112,388 (  19.19%)
    instructions                283,612,054,215 (    1.15)    283,918,134,959 (    1.16)
    branches                     56,372,560,385 ( 724.923)     56,449,814,753 ( 733.766)
    branch-misses                   374,826,000 (   0.66%)        326,935,859 (   0.58%)
    jobs2                              perfstat
    stalled-cycles-frontend     155,142,745,777 (  40.99%)    164,170,979,198 (  43.82%)
    stalled-cycles-backend       70,813,866,387 (  18.71%)     66,456,858,165 (  17.74%)
    instructions                463,436,648,173 (    1.22)    464,221,890,191 (    1.24)
    branches                     91,088,733,902 ( 760.088)     91,278,144,546 ( 769.133)
    branch-misses                   504,460,363 (   0.55%)        394,033,842 (   0.43%)
    jobs3                              perfstat
    stalled-cycles-frontend     201,300,397,212 (  39.84%)    223,969,902,257 (  44.44%)
    stalled-cycles-backend       87,712,593,974 (  17.36%)     81,618,888,712 (  16.19%)
    instructions                642,869,545,023 (    1.27)    644,677,354,132 (    1.28)
    branches                    125,724,560,594 ( 690.682)    126,133,159,521 ( 694.542)
    branch-misses                   527,941,798 (   0.42%)        444,782,220 (   0.35%)
    jobs4                              perfstat
    stalled-cycles-frontend     246,701,197,429 (  38.12%)    280,076,030,886 (  43.29%)
    stalled-cycles-backend      119,050,341,112 (  18.40%)    110,955,641,671 (  17.15%)
    instructions                822,716,962,127 (    1.27)    825,536,969,320 (    1.28)
    branches                    160,590,028,545 ( 688.614)    161,152,996,915 ( 691.068)
    branch-misses                   650,295,287 (   0.40%)        550,229,113 (   0.34%)
    jobs5                              perfstat
    stalled-cycles-frontend     298,958,462,516 (  38.30%)    344,852,200,358 (  44.16%)
    stalled-cycles-backend      137,558,742,122 (  17.62%)    129,465,067,102 (  16.58%)
    instructions              1,005,714,688,752 (    1.29)  1,007,657,999,432 (    1.29)
    branches                    195,988,773,962 ( 697.730)    196,446,873,984 ( 700.319)
    branch-misses                   695,818,940 (   0.36%)        624,823,263 (   0.32%)
    jobs6                              perfstat
    stalled-cycles-frontend     334,497,602,856 (  36.71%)    387,590,419,779 (  42.38%)
    stalled-cycles-backend      163,539,365,335 (  17.95%)    152,640,193,639 (  16.69%)
    instructions              1,184,738,177,851 (    1.30)  1,187,396,281,677 (    1.30)
    branches                    230,592,915,640 ( 702.902)    231,253,802,882 ( 702.356)
    branch-misses                   747,934,786 (   0.32%)        643,902,424 (   0.28%)
    jobs7                              perfstat
    stalled-cycles-frontend     396,724,684,187 (  37.71%)    460,705,858,952 (  43.84%)
    stalled-cycles-backend      188,096,616,496 (  17.88%)    175,785,787,036 (  16.73%)
    instructions              1,364,041,136,608 (    1.30)  1,366,689,075,112 (    1.30)
    branches                    265,253,096,936 ( 700.078)    265,890,524,883 ( 702.839)
    branch-misses                   784,991,589 (   0.30%)        729,196,689 (   0.27%)
    jobs8                              perfstat
    stalled-cycles-frontend     440,248,299,870 (  36.92%)    509,554,793,816 (  42.46%)
    stalled-cycles-backend      222,575,930,616 (  18.67%)    213,401,248,432 (  17.78%)
    instructions              1,542,262,045,114 (    1.29)  1,545,233,932,257 (    1.29)
    branches                    299,775,178,439 ( 697.666)    300,528,458,505 ( 694.769)
    branch-misses                   847,496,084 (   0.28%)        748,794,308 (   0.25%)
    jobs9                              perfstat
    stalled-cycles-frontend     506,269,882,480 (  37.86%)    592,798,032,820 (  44.43%)
    stalled-cycles-backend      253,192,498,861 (  18.93%)    233,727,666,185 (  17.52%)
    instructions              1,721,985,080,913 (    1.29)  1,724,666,236,005 (    1.29)
    branches                    334,517,360,255 ( 694.134)    335,199,758,164 ( 697.131)
    branch-misses                   873,496,730 (   0.26%)        815,379,236 (   0.24%)
    jobs10                             perfstat
    stalled-cycles-frontend     549,063,363,749 (  37.18%)    651,302,376,662 (  43.61%)
    stalled-cycles-backend      281,680,986,810 (  19.07%)    277,005,235,582 (  18.55%)
    instructions              1,901,859,271,180 (    1.29)  1,906,311,064,230 (    1.28)
    branches                    369,398,536,153 ( 694.004)    370,527,696,358 ( 688.409)
    branch-misses                   967,929,335 (   0.26%)        890,125,056 (   0.24%)
    
                                BASE           PATCHED
    seconds elapsed        79.421641008     78.735285546
    seconds elapsed        61.471246133     60.869085949
    seconds elapsed        62.317058173     62.224188495
    seconds elapsed        60.030739363     60.081102518
    seconds elapsed        74.070398362     74.317582865
    seconds elapsed        84.985953007     85.414364176
    seconds elapsed        97.724553255     98.173311344
    seconds elapsed        109.488066758    110.268399318
    seconds elapsed        122.768189405    122.967164498
    seconds elapsed        135.130035105    136.934770801
    
    On my other system (8 x86_64 CPUs, short version of test results):
    
                                BASE           PATCHED
    seconds elapsed        19.518065994     19.806320662
    seconds elapsed        15.172772749     15.594718291
    seconds elapsed        13.820925970     13.821708564
    seconds elapsed        13.293097816     14.585206405
    seconds elapsed        16.207284118     16.064431606
    seconds elapsed        17.958376158     17.771825767
    seconds elapsed        19.478009164     19.602961508
    seconds elapsed        21.347152811     21.352318709
    seconds elapsed        24.478121126     24.171088735
    seconds elapsed        26.865057442     26.767327618
    
    So performance-wise the numbers are quite similar.
    
    Also update zcomp interface to be more aligned with the crypto API.
    
    [1] http://marc.info/?l=linux-kernel&m=144480832108927&w=2
    [2] http://marc.info/?l=linux-kernel&m=145379613507518&w=2
    [3] https://github.com/sergey-senozhatsky/zram-perf-test
    
    Link: http://lkml.kernel.org/r/20160531122017.2878-3-sergey.senozhatsky@gmail.com
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Suggested-by: Minchan Kim <minchan@kernel.org>
    Suggested-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 944b8e60dd82..c914ab7972ef 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -13,12 +13,7 @@
 struct zcomp_strm {
 	/* compression/decompression buffer */
 	void *buffer;
-	/*
-	 * The private data of the compression stream, only compression
-	 * stream backend can touch this (e.g. compression algorithm
-	 * working memory)
-	 */
-	void *private;
+	struct crypto_comp *tfm;
 };
 
 /* static compression backend */
@@ -40,6 +35,8 @@ struct zcomp {
 	struct zcomp_strm * __percpu *stream;
 	struct zcomp_backend *backend;
 	struct notifier_block notifier;
+
+	const char *name;
 };
 
 ssize_t zcomp_available_show(const char *comp, char *buf);
@@ -51,11 +48,11 @@ void zcomp_destroy(struct zcomp *comp);
 struct zcomp_strm *zcomp_stream_get(struct zcomp *comp);
 void zcomp_stream_put(struct zcomp *comp);
 
-int zcomp_compress(struct zcomp *comp, struct zcomp_strm *zstrm,
-		const unsigned char *src, size_t *dst_len);
+int zcomp_compress(struct zcomp_strm *zstrm,
+		const void *src, unsigned int *dst_len);
 
-int zcomp_decompress(struct zcomp *comp, const unsigned char *src,
-		size_t src_len, unsigned char *dst);
+int zcomp_decompress(struct zcomp_strm *zstrm,
+		const void *src, unsigned int src_len, void *dst);
 
 bool zcomp_set_max_streams(struct zcomp *comp, int num_strm);
 #endif /* _ZCOMP_H_ */

commit 2aea8493d326bdf15446768333e1d2c91b040b5c
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Tue Jul 26 15:22:42 2016 -0700

    zram: rename zstrm find-release functions
    
    This has started as a 'add zlib support' work, but after some thinking I
    saw no blockers for a bigger change -- a switch to crypto API.
    
    We don't have an idle zstreams list anymore and our write path now works
    absolutely differently, preventing preemption during compression.  This
    removes possibilities of read paths preempting writes at wrong places
    and opens the door for a move from custom LZO/LZ4 compression backends
    implementation to a more generic one, using crypto compress API.
    
    This patch set also eliminates the need of a new context-less crypto API
    interface, which was quite hard to sell, so we can move along faster.
    
    benchmarks:
    
    (x86_64, 4GB, zram-perf script)
    
    perf reported run-time fio (max jobs=3).  I performed fio test with the
    increasing number of parallel jobs (max to 3) on a 3G zram device, using
    `static' data and the following crypto comp algorithms:
    
            842, deflate, lz4, lz4hc, lzo
    
    the output was:
    
     - test running time (which can tell us what algorithms performs faster)
    
    and
    
     - zram mm_stat (which tells the compressed memory size, max used memory, etc).
    
    It's just for information.  for example, LZ4HC has twice the running
    time of LZO, but the compressed memory size is: 23592960 vs 34603008
    bytes.
    
      test-fio-zram-842
         197.907655282 seconds time elapsed
         201.623142884 seconds time elapsed
         226.854291345 seconds time elapsed
      test-fio-zram-DEFLATE
         253.259516155 seconds time elapsed
         258.148563401 seconds time elapsed
         290.251909365 seconds time elapsed
      test-fio-zram-LZ4
          27.022598717 seconds time elapsed
          29.580522717 seconds time elapsed
          33.293463430 seconds time elapsed
      test-fio-zram-LZ4HC
          56.393954615 seconds time elapsed
          74.904659747 seconds time elapsed
         101.940998564 seconds time elapsed
      test-fio-zram-LZO
          28.155948075 seconds time elapsed
          30.390036330 seconds time elapsed
          34.455773159 seconds time elapsed
    
    zram mm_stat-s (max fio jobs=3)
    
      test-fio-zram-842
      mm_stat (jobs1): 3221225472 673185792 690266112        0 690266112        0        0
      mm_stat (jobs2): 3221225472 673185792 690266112        0 690266112        0        0
      mm_stat (jobs3): 3221225472 673185792 690266112        0 690266112        0        0
      test-fio-zram-DEFLATE
      mm_stat (jobs1): 3221225472  24379392  37761024        0  37761024        0        0
      mm_stat (jobs2): 3221225472  24379392  37761024        0  37761024        0        0
      mm_stat (jobs3): 3221225472  24379392  37761024        0  37761024        0        0
      test-fio-zram-LZ4
      mm_stat (jobs1): 3221225472  23592960  37761024        0  37761024        0        0
      mm_stat (jobs2): 3221225472  23592960  37761024        0  37761024        0        0
      mm_stat (jobs3): 3221225472  23592960  37761024        0  37761024        0        0
      test-fio-zram-LZ4HC
      mm_stat (jobs1): 3221225472  23592960  37761024        0  37761024        0        0
      mm_stat (jobs2): 3221225472  23592960  37761024        0  37761024        0        0
      mm_stat (jobs3): 3221225472  23592960  37761024        0  37761024        0        0
      test-fio-zram-LZO
      mm_stat (jobs1): 3221225472  34603008  50335744        0  50335744        0        0
      mm_stat (jobs2): 3221225472  34603008  50335744        0  50335744        0        0
      mm_stat (jobs3): 3221225472  34603008  50335744        0  50339840        0        0
    
    This patch (of 8):
    
    We don't perform any zstream idle list lookup anymore, so
    zcomp_strm_find()/zcomp_strm_release() names are not representative.
    
    Rename to zcomp_stream_get()/zcomp_stream_put().
    
    Link: http://lkml.kernel.org/r/20160531122017.2878-2-sergey.senozhatsky@gmail.com
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index ffd88cb747fe..944b8e60dd82 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -48,8 +48,8 @@ bool zcomp_available_algorithm(const char *comp);
 struct zcomp *zcomp_create(const char *comp);
 void zcomp_destroy(struct zcomp *comp);
 
-struct zcomp_strm *zcomp_strm_find(struct zcomp *comp);
-void zcomp_strm_release(struct zcomp *comp, struct zcomp_strm *zstrm);
+struct zcomp_strm *zcomp_stream_get(struct zcomp *comp);
+void zcomp_stream_put(struct zcomp *comp);
 
 int zcomp_compress(struct zcomp *comp, struct zcomp_strm *zstrm,
 		const unsigned char *src, size_t *dst_len);

commit da9556a2367cf2261ab4d3e100693c82fb1ddb26
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Fri May 20 16:59:51 2016 -0700

    zram: user per-cpu compression streams
    
    Remove idle streams list and keep compression streams in per-cpu data.
    This removes two contented spin_lock()/spin_unlock() calls from write
    path and also prevent write OP from being preempted while holding the
    compression stream, which can cause slow downs.
    
    For instance, let's assume that we have N cpus and N-2
    max_comp_streams.TASK1 owns the last idle stream, TASK2-TASK3 come in
    with the write requests:
    
      TASK1            TASK2              TASK3
     zram_bvec_write()
      spin_lock
      find stream
      spin_unlock
    
      compress
    
      <<preempted>>   zram_bvec_write()
                       spin_lock
                       find stream
                       spin_unlock
                         no_stream
                           schedule
                                         zram_bvec_write()
                                          spin_lock
                                          find_stream
                                          spin_unlock
                                            no_stream
                                              schedule
       spin_lock
       release stream
       spin_unlock
         wake up TASK2
    
    not only TASK2 and TASK3 will not get the stream, TASK1 will be
    preempted in the middle of its operation; while we would prefer it to
    finish compression and release the stream.
    
    Test environment: x86_64, 4 CPU box, 3G zram, lzo
    
    The following fio tests were executed:
          read, randread, write, randwrite, rw, randrw
    with the increasing number of jobs from 1 to 10.
    
                      4 streams        8 streams       per-cpu
      ===========================================================
      jobs1
      READ:           2520.1MB/s       2566.5MB/s      2491.5MB/s
      READ:           2102.7MB/s       2104.2MB/s      2091.3MB/s
      WRITE:          1355.1MB/s       1320.2MB/s      1378.9MB/s
      WRITE:          1103.5MB/s       1097.2MB/s      1122.5MB/s
      READ:           434013KB/s       435153KB/s      439961KB/s
      WRITE:          433969KB/s       435109KB/s      439917KB/s
      READ:           403166KB/s       405139KB/s      403373KB/s
      WRITE:          403223KB/s       405197KB/s      403430KB/s
      jobs2
      READ:           7958.6MB/s       8105.6MB/s      8073.7MB/s
      READ:           6864.9MB/s       6989.8MB/s      7021.8MB/s
      WRITE:          2438.1MB/s       2346.9MB/s      3400.2MB/s
      WRITE:          1994.2MB/s       1990.3MB/s      2941.2MB/s
      READ:           981504KB/s       973906KB/s      1018.8MB/s
      WRITE:          981659KB/s       974060KB/s      1018.1MB/s
      READ:           937021KB/s       938976KB/s      987250KB/s
      WRITE:          934878KB/s       936830KB/s      984993KB/s
      jobs3
      READ:           13280MB/s        13553MB/s       13553MB/s
      READ:           11534MB/s        11785MB/s       11755MB/s
      WRITE:          3456.9MB/s       3469.9MB/s      4810.3MB/s
      WRITE:          3029.6MB/s       3031.6MB/s      4264.8MB/s
      READ:           1363.8MB/s       1362.6MB/s      1448.9MB/s
      WRITE:          1361.9MB/s       1360.7MB/s      1446.9MB/s
      READ:           1309.4MB/s       1310.6MB/s      1397.5MB/s
      WRITE:          1307.4MB/s       1308.5MB/s      1395.3MB/s
      jobs4
      READ:           20244MB/s        20177MB/s       20344MB/s
      READ:           17886MB/s        17913MB/s       17835MB/s
      WRITE:          4071.6MB/s       4046.1MB/s      6370.2MB/s
      WRITE:          3608.9MB/s       3576.3MB/s      5785.4MB/s
      READ:           1824.3MB/s       1821.6MB/s      1997.5MB/s
      WRITE:          1819.8MB/s       1817.4MB/s      1992.5MB/s
      READ:           1765.7MB/s       1768.3MB/s      1937.3MB/s
      WRITE:          1767.5MB/s       1769.1MB/s      1939.2MB/s
      jobs5
      READ:           18663MB/s        18986MB/s       18823MB/s
      READ:           16659MB/s        16605MB/s       16954MB/s
      WRITE:          3912.4MB/s       3888.7MB/s      6126.9MB/s
      WRITE:          3506.4MB/s       3442.5MB/s      5519.3MB/s
      READ:           1798.2MB/s       1746.5MB/s      1935.8MB/s
      WRITE:          1792.7MB/s       1740.7MB/s      1929.1MB/s
      READ:           1727.6MB/s       1658.2MB/s      1917.3MB/s
      WRITE:          1726.5MB/s       1657.2MB/s      1916.6MB/s
      jobs6
      READ:           21017MB/s        20922MB/s       21162MB/s
      READ:           19022MB/s        19140MB/s       18770MB/s
      WRITE:          3968.2MB/s       4037.7MB/s      6620.8MB/s
      WRITE:          3643.5MB/s       3590.2MB/s      6027.5MB/s
      READ:           1871.8MB/s       1880.5MB/s      2049.9MB/s
      WRITE:          1867.8MB/s       1877.2MB/s      2046.2MB/s
      READ:           1755.8MB/s       1710.3MB/s      1964.7MB/s
      WRITE:          1750.5MB/s       1705.9MB/s      1958.8MB/s
      jobs7
      READ:           21103MB/s        20677MB/s       21482MB/s
      READ:           18522MB/s        18379MB/s       19443MB/s
      WRITE:          4022.5MB/s       4067.4MB/s      6755.9MB/s
      WRITE:          3691.7MB/s       3695.5MB/s      5925.6MB/s
      READ:           1841.5MB/s       1933.9MB/s      2090.5MB/s
      WRITE:          1842.7MB/s       1935.3MB/s      2091.9MB/s
      READ:           1832.4MB/s       1856.4MB/s      1971.5MB/s
      WRITE:          1822.3MB/s       1846.2MB/s      1960.6MB/s
      jobs8
      READ:           20463MB/s        20194MB/s       20862MB/s
      READ:           18178MB/s        17978MB/s       18299MB/s
      WRITE:          4085.9MB/s       4060.2MB/s      7023.8MB/s
      WRITE:          3776.3MB/s       3737.9MB/s      6278.2MB/s
      READ:           1957.6MB/s       1944.4MB/s      2109.5MB/s
      WRITE:          1959.2MB/s       1946.2MB/s      2111.4MB/s
      READ:           1900.6MB/s       1885.7MB/s      2082.1MB/s
      WRITE:          1896.2MB/s       1881.4MB/s      2078.3MB/s
      jobs9
      READ:           19692MB/s        19734MB/s       19334MB/s
      READ:           17678MB/s        18249MB/s       17666MB/s
      WRITE:          4004.7MB/s       4064.8MB/s      6990.7MB/s
      WRITE:          3724.7MB/s       3772.1MB/s      6193.6MB/s
      READ:           1953.7MB/s       1967.3MB/s      2105.6MB/s
      WRITE:          1953.4MB/s       1966.7MB/s      2104.1MB/s
      READ:           1860.4MB/s       1897.4MB/s      2068.5MB/s
      WRITE:          1858.9MB/s       1895.9MB/s      2066.8MB/s
      jobs10
      READ:           19730MB/s        19579MB/s       19492MB/s
      READ:           18028MB/s        18018MB/s       18221MB/s
      WRITE:          4027.3MB/s       4090.6MB/s      7020.1MB/s
      WRITE:          3810.5MB/s       3846.8MB/s      6426.8MB/s
      READ:           1956.1MB/s       1994.6MB/s      2145.2MB/s
      WRITE:          1955.9MB/s       1993.5MB/s      2144.8MB/s
      READ:           1852.8MB/s       1911.6MB/s      2075.8MB/s
      WRITE:          1855.7MB/s       1914.6MB/s      2078.1MB/s
    
    perf stat
    
                                      4 streams                       8 streams                       per-cpu
      ====================================================================================================================
      jobs1
      stalled-cycles-frontend      23,174,811,209 (  38.21%)     23,220,254,188 (  38.25%)       23,061,406,918 (  38.34%)
      stalled-cycles-backend       11,514,174,638 (  18.98%)     11,696,722,657 (  19.27%)       11,370,852,810 (  18.90%)
      instructions                 73,925,005,782 (    1.22)     73,903,177,632 (    1.22)       73,507,201,037 (    1.22)
      branches                     14,455,124,835 ( 756.063)     14,455,184,779 ( 755.281)       14,378,599,509 ( 758.546)
      branch-misses                    69,801,336 (   0.48%)         80,225,529 (   0.55%)           72,044,726 (   0.50%)
      jobs2
      stalled-cycles-frontend      49,912,741,782 (  46.11%)     50,101,189,290 (  45.95%)       32,874,195,633 (  35.11%)
      stalled-cycles-backend       27,080,366,230 (  25.02%)     27,949,970,232 (  25.63%)       16,461,222,706 (  17.58%)
      instructions                122,831,629,690 (    1.13)    122,919,846,419 (    1.13)      121,924,786,775 (    1.30)
      branches                     23,725,889,239 ( 692.663)     23,733,547,140 ( 688.062)       23,553,950,311 ( 794.794)
      branch-misses                    90,733,041 (   0.38%)         96,320,895 (   0.41%)           84,561,092 (   0.36%)
      jobs3
      stalled-cycles-frontend      66,437,834,608 (  45.58%)     63,534,923,344 (  43.69%)       42,101,478,505 (  33.19%)
      stalled-cycles-backend       34,940,799,661 (  23.97%)     34,774,043,148 (  23.91%)       21,163,324,388 (  16.68%)
      instructions                171,692,121,862 (    1.18)    171,775,373,044 (    1.18)      170,353,542,261 (    1.34)
      branches                     32,968,962,622 ( 628.723)     32,987,739,894 ( 630.512)       32,729,463,918 ( 717.027)
      branch-misses                   111,522,732 (   0.34%)        110,472,894 (   0.33%)           99,791,291 (   0.30%)
      jobs4
      stalled-cycles-frontend      98,741,701,675 (  49.72%)     94,797,349,965 (  47.59%)       54,535,655,381 (  33.53%)
      stalled-cycles-backend       54,642,609,615 (  27.51%)     55,233,554,408 (  27.73%)       27,882,323,541 (  17.14%)
      instructions                220,884,807,851 (    1.11)    220,930,887,273 (    1.11)      218,926,845,851 (    1.35)
      branches                     42,354,518,180 ( 592.105)     42,362,770,587 ( 590.452)       41,955,552,870 ( 716.154)
      branch-misses                   138,093,449 (   0.33%)        131,295,286 (   0.31%)          121,794,771 (   0.29%)
      jobs5
      stalled-cycles-frontend     116,219,747,212 (  48.14%)    110,310,397,012 (  46.29%)       66,373,082,723 (  33.70%)
      stalled-cycles-backend       66,325,434,776 (  27.48%)     64,157,087,914 (  26.92%)       32,999,097,299 (  16.76%)
      instructions                270,615,008,466 (    1.12)    270,546,409,525 (    1.14)      268,439,910,948 (    1.36)
      branches                     51,834,046,557 ( 599.108)     51,811,867,722 ( 608.883)       51,412,576,077 ( 729.213)
      branch-misses                   158,197,086 (   0.31%)        142,639,805 (   0.28%)          133,425,455 (   0.26%)
      jobs6
      stalled-cycles-frontend     138,009,414,492 (  48.23%)    139,063,571,254 (  48.80%)       75,278,568,278 (  32.80%)
      stalled-cycles-backend       79,211,949,650 (  27.68%)     79,077,241,028 (  27.75%)       37,735,797,899 (  16.44%)
      instructions                319,763,993,731 (    1.12)    319,937,782,834 (    1.12)      316,663,600,784 (    1.38)
      branches                     61,219,433,294 ( 595.056)     61,250,355,540 ( 598.215)       60,523,446,617 ( 733.706)
      branch-misses                   169,257,123 (   0.28%)        154,898,028 (   0.25%)          141,180,587 (   0.23%)
      jobs7
      stalled-cycles-frontend     162,974,812,119 (  49.20%)    159,290,061,987 (  48.43%)       88,046,641,169 (  33.21%)
      stalled-cycles-backend       92,223,151,661 (  27.84%)     91,667,904,406 (  27.87%)       44,068,454,971 (  16.62%)
      instructions                369,516,432,430 (    1.12)    369,361,799,063 (    1.12)      365,290,380,661 (    1.38)
      branches                     70,795,673,950 ( 594.220)     70,743,136,124 ( 597.876)       69,803,996,038 ( 732.822)
      branch-misses                   181,708,327 (   0.26%)        165,767,821 (   0.23%)          150,109,797 (   0.22%)
      jobs8
      stalled-cycles-frontend     185,000,017,027 (  49.30%)    182,334,345,473 (  48.37%)       99,980,147,041 (  33.26%)
      stalled-cycles-backend      105,753,516,186 (  28.18%)    107,937,830,322 (  28.63%)       51,404,177,181 (  17.10%)
      instructions                418,153,161,055 (    1.11)    418,308,565,828 (    1.11)      413,653,475,581 (    1.38)
      branches                     80,035,882,398 ( 592.296)     80,063,204,510 ( 589.843)       79,024,105,589 ( 730.530)
      branch-misses                   199,764,528 (   0.25%)        177,936,926 (   0.22%)          160,525,449 (   0.20%)
      jobs9
      stalled-cycles-frontend     210,941,799,094 (  49.63%)    204,714,679,254 (  48.55%)      114,251,113,756 (  33.96%)
      stalled-cycles-backend      122,640,849,067 (  28.85%)    122,188,553,256 (  28.98%)       58,360,041,127 (  17.35%)
      instructions                468,151,025,415 (    1.10)    467,354,869,323 (    1.11)      462,665,165,216 (    1.38)
      branches                     89,657,067,510 ( 585.628)     89,411,550,407 ( 588.990)       88,360,523,943 ( 730.151)
      branch-misses                   218,292,301 (   0.24%)        191,701,247 (   0.21%)          178,535,678 (   0.20%)
      jobs10
      stalled-cycles-frontend     233,595,958,008 (  49.81%)    227,540,615,689 (  49.11%)      160,341,979,938 (  43.07%)
      stalled-cycles-backend      136,153,676,021 (  29.03%)    133,635,240,742 (  28.84%)       65,909,135,465 (  17.70%)
      instructions                517,001,168,497 (    1.10)    516,210,976,158 (    1.11)      511,374,038,613 (    1.37)
      branches                     98,911,641,329 ( 585.796)     98,700,069,712 ( 591.583)       97,646,761,028 ( 728.712)
      branch-misses                   232,341,823 (   0.23%)        199,256,308 (   0.20%)          183,135,268 (   0.19%)
    
    per-cpu streams tend to cause significantly less stalled cycles; execute
    less branches and hit less branch-misses.
    
    perf stat reported execution time
    
                              4 streams        8 streams       per-cpu
      ====================================================================
      jobs1
      seconds elapsed        20.909073870     20.875670495    20.817838540
      jobs2
      seconds elapsed        18.529488399     18.720566469    16.356103108
      jobs3
      seconds elapsed        18.991159531     18.991340812    16.766216066
      jobs4
      seconds elapsed        19.560643828     19.551323547    16.246621715
      jobs5
      seconds elapsed        24.746498464     25.221646740    20.696112444
      jobs6
      seconds elapsed        28.258181828     28.289765505    22.885688857
      jobs7
      seconds elapsed        32.632490241     31.909125381    26.272753738
      jobs8
      seconds elapsed        35.651403851     36.027596308    29.108024711
      jobs9
      seconds elapsed        40.569362365     40.024227989    32.898204012
      jobs10
      seconds elapsed        44.673112304     43.874898137    35.632952191
    
    Please see
      Link: http://marc.info/?l=linux-kernel&m=146166970727530
      Link: http://marc.info/?l=linux-kernel&m=146174716719650
    for more test results (under low memory conditions).
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Suggested-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index b7d2a4bcae54..ffd88cb747fe 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -10,8 +10,6 @@
 #ifndef _ZCOMP_H_
 #define _ZCOMP_H_
 
-#include <linux/mutex.h>
-
 struct zcomp_strm {
 	/* compression/decompression buffer */
 	void *buffer;
@@ -21,8 +19,6 @@ struct zcomp_strm {
 	 * working memory)
 	 */
 	void *private;
-	/* used in multi stream backend, protected by backend strm_lock */
-	struct list_head list;
 };
 
 /* static compression backend */
@@ -41,19 +37,15 @@ struct zcomp_backend {
 
 /* dynamic per-device compression frontend */
 struct zcomp {
-	void *stream;
+	struct zcomp_strm * __percpu *stream;
 	struct zcomp_backend *backend;
-
-	struct zcomp_strm *(*strm_find)(struct zcomp *comp);
-	void (*strm_release)(struct zcomp *comp, struct zcomp_strm *zstrm);
-	bool (*set_max_streams)(struct zcomp *comp, int num_strm);
-	void (*destroy)(struct zcomp *comp);
+	struct notifier_block notifier;
 };
 
 ssize_t zcomp_available_show(const char *comp, char *buf);
 bool zcomp_available_algorithm(const char *comp);
 
-struct zcomp *zcomp_create(const char *comp, int max_strm);
+struct zcomp *zcomp_create(const char *comp);
 void zcomp_destroy(struct zcomp *comp);
 
 struct zcomp_strm *zcomp_strm_find(struct zcomp *comp);

commit 75d8947a36d0c9aedd69118d1f14bf424005c7c2
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Jan 14 15:22:32 2016 -0800

    zram: pass gfp from zcomp frontend to backend
    
    Each zcomp backend uses own gfp flag but it's pointless because the
    context they could be called is driven by upper layer(ie, zcomp
    frontend).  As well, zcomp frondend could call them in different
    context.  One context(ie, zram init part) is it should be better to make
    sure successful allocation other context(ie, further stream allocation
    part for accelarating I/O speed) is just optional so let's pass gfp down
    from driver (ie, zcomp frontend) like normal MM convention.
    
    [sergey.senozhatsky@gmail.com: add missing __vmalloc zero and highmem gfps]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 46e2b9f8f1f0..b7d2a4bcae54 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -33,7 +33,7 @@ struct zcomp_backend {
 	int (*decompress)(const unsigned char *src, size_t src_len,
 			unsigned char *dst);
 
-	void *(*create)(void);
+	void *(*create)(gfp_t flags);
 	void (*destroy)(void *private);
 
 	const char *name;

commit d93435c3fba4a47b773693b0c8992470d38510d5
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Thu Jun 25 15:00:32 2015 -0700

    zram: check comp algorithm availability earlier
    
    Improvement idea by Marcin Jabrzyk.
    
    comp_algorithm_store() silently accepts any supplied algorithm name,
    because zram performs algorithm availability check later, during the
    device configuration phase in disksize_store() and emits the following
    error:
    
      "zram: Cannot initialise %s compressing backend"
    
    this error line is somewhat generic and, besides, can indicate a failed
    attempt to allocate compression backend's working buffers.
    
    add algorithm availability check to comp_algorithm_store():
    
      echo lzz > /sys/block/zram0/comp_algorithm
      -bash: echo: write error: Invalid argument
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reported-by: Marcin Jabrzyk <m.jabrzyk@samsung.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index c59d1fca72c0..46e2b9f8f1f0 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -51,6 +51,7 @@ struct zcomp {
 };
 
 ssize_t zcomp_available_show(const char *comp, char *buf);
+bool zcomp_available_algorithm(const char *comp);
 
 struct zcomp *zcomp_create(const char *comp, int max_strm);
 void zcomp_destroy(struct zcomp *comp);

commit 60a726e33375a1096e85399cfa1327081b4c38be
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Apr 7 15:38:21 2014 -0700

    zram: propagate error to user
    
    When we initialized zcomp with single, we couldn't change
    max_comp_streams without zram reset but current interface doesn't show
    any error to user and even it changes max_comp_streams's value without
    any effect so it would make user very confusing.
    
    This patch prevents max_comp_streams's change when zcomp was initialized
    as single zcomp and emit the error to user(ex, echo).
    
    [akpm@linux-foundation.org: don't return with the lock held, per Sergey]
    [fengguang.wu@intel.com: fix coccinelle warnings]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Acked-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 8b8997f8613b..c59d1fca72c0 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -46,7 +46,7 @@ struct zcomp {
 
 	struct zcomp_strm *(*strm_find)(struct zcomp *comp);
 	void (*strm_release)(struct zcomp *comp, struct zcomp_strm *zstrm);
-	int (*set_max_streams)(struct zcomp *comp, int num_strm);
+	bool (*set_max_streams)(struct zcomp *comp, int num_strm);
 	void (*destroy)(struct zcomp *comp);
 };
 
@@ -64,5 +64,5 @@ int zcomp_compress(struct zcomp *comp, struct zcomp_strm *zstrm,
 int zcomp_decompress(struct zcomp *comp, const unsigned char *src,
 		size_t src_len, unsigned char *dst);
 
-int zcomp_set_max_streams(struct zcomp *comp, int num_strm);
+bool zcomp_set_max_streams(struct zcomp *comp, int num_strm);
 #endif /* _ZCOMP_H_ */

commit e46b8a030d76d3c94156c545c3f4c3676d813435
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Mon Apr 7 15:38:17 2014 -0700

    zram: make compression algorithm selection possible
    
    Add and document `comp_algorithm' device attribute.  This attribute allows
    to show supported compression and currently selected compression
    algorithms:
    
            cat /sys/block/zram0/comp_algorithm
            [lzo] lz4
    
    and change selected compression algorithm:
            echo lzo > /sys/block/zram0/comp_algorithm
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index bd11d59c5dd1..8b8997f8613b 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -50,6 +50,8 @@ struct zcomp {
 	void (*destroy)(struct zcomp *comp);
 };
 
+ssize_t zcomp_available_show(const char *comp, char *buf);
+
 struct zcomp *zcomp_create(const char *comp, int max_strm);
 void zcomp_destroy(struct zcomp *comp);
 

commit fe8eb122c82b2049c460fc6df6e8583a2f935cff
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Mon Apr 7 15:38:15 2014 -0700

    zram: add set_max_streams knob
    
    This patch allows to change max_comp_streams on initialised zcomp.
    
    Introduce zcomp set_max_streams() knob, zcomp_strm_multi_set_max_streams()
    and zcomp_strm_single_set_max_streams() callbacks to change streams limit
    for zcomp_strm_multi and zcomp_strm_single, accordingly.  set_max_streams
    for single steam zcomp does nothing.
    
    If user has lowered the limit, then zcomp_strm_multi_set_max_streams()
    attempts to immediately free extra streams (as much as it can, depending
    on idle streams availability).
    
    Note, this patch does not allow to change stream 'policy' from single to
    multi stream (or vice versa) on already initialised compression backend.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index 2a3684446160..bd11d59c5dd1 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -46,6 +46,7 @@ struct zcomp {
 
 	struct zcomp_strm *(*strm_find)(struct zcomp *comp);
 	void (*strm_release)(struct zcomp *comp, struct zcomp_strm *zstrm);
+	int (*set_max_streams)(struct zcomp *comp, int num_strm);
 	void (*destroy)(struct zcomp *comp);
 };
 
@@ -60,4 +61,6 @@ int zcomp_compress(struct zcomp *comp, struct zcomp_strm *zstrm,
 
 int zcomp_decompress(struct zcomp *comp, const unsigned char *src,
 		size_t src_len, unsigned char *dst);
+
+int zcomp_set_max_streams(struct zcomp *comp, int num_strm);
 #endif /* _ZCOMP_H_ */

commit beca3ec71fe5490ee9237dc42400f50402baf83e
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Mon Apr 7 15:38:14 2014 -0700

    zram: add multi stream functionality
    
    Existing zram (zcomp) implementation has only one compression stream
    (buffer and algorithm private part), so in order to prevent data
    corruption only one write (compress operation) can use this compression
    stream, forcing all concurrent write operations to wait for stream lock
    to be released.  This patch changes zcomp to keep a compression streams
    list of user-defined size (via sysfs device attr).  Each write operation
    still exclusively holds compression stream, the difference is that we
    can have N write operations (depending on size of streams list)
    executing in parallel.  See TEST section later in commit message for
    performance data.
    
    Introduce struct zcomp_strm_multi and a set of functions to manage
    zcomp_strm stream access.  zcomp_strm_multi has a list of idle
    zcomp_strm structs, spinlock to protect idle list and wait queue, making
    it possible to perform parallel compressions.
    
    The following set of functions added:
    - zcomp_strm_multi_find()/zcomp_strm_multi_release()
      find and release a compression stream, implement required locking
    - zcomp_strm_multi_create()/zcomp_strm_multi_destroy()
      create and destroy zcomp_strm_multi
    
    zcomp ->strm_find() and ->strm_release() callbacks are set during
    initialisation to zcomp_strm_multi_find()/zcomp_strm_multi_release()
    correspondingly.
    
    Each time zcomp issues a zcomp_strm_multi_find() call, the following set
    of operations performed:
    
    - spin lock strm_lock
    - if idle list is not empty, remove zcomp_strm from idle list, spin
      unlock and return zcomp stream pointer to caller
    - if idle list is empty, current adds itself to wait queue. it will be
      awaken by zcomp_strm_multi_release() caller.
    
    zcomp_strm_multi_release():
    - spin lock strm_lock
    - add zcomp stream to idle list
    - spin unlock, wake up sleeper
    
    Minchan Kim reported that spinlock-based locking scheme has demonstrated
    a severe perfomance regression for single compression stream case,
    comparing to mutex-based (see https://lkml.org/lkml/2014/2/18/16)
    
    base                      spinlock                    mutex
    
    ==Initial write           ==Initial write             ==Initial  write
    records:  5               records:  5                 records:   5
    avg:      1642424.35      avg:      699610.40         avg:       1655583.71
    std:      39890.95(2.43%) std:      232014.19(33.16%) std:       52293.96
    max:      1690170.94      max:      1163473.45        max:       1697164.75
    min:      1568669.52      min:      573429.88         min:       1553410.23
    ==Rewrite                 ==Rewrite                   ==Rewrite
    records:  5               records:  5                 records:   5
    avg:      1611775.39      avg:      501406.64         avg:       1684419.11
    std:      17144.58(1.06%) std:      15354.41(3.06%)   std:       18367.42
    max:      1641800.95      max:      531356.78         max:       1706445.84
    min:      1593515.27      min:      488817.78         min:       1655335.73
    
    When only one compression stream available, mutex with spin on owner
    tends to perform much better than frequent wait_event()/wake_up().  This
    is why single stream implemented as a special case with mutex locking.
    
    Introduce and document zram device attribute max_comp_streams.  This
    attr shows and stores current zcomp's max number of zcomp streams
    (max_strm).  Extend zcomp's zcomp_create() with `max_strm' parameter.
    `max_strm' limits the number of zcomp_strm structs in compression
    backend's idle list (max_comp_streams).
    
    max_comp_streams used during initialisation as follows:
    -- passing to zcomp_create() max_strm equals to 1 will initialise zcomp
    using single compression stream zcomp_strm_single (mutex-based locking).
    -- passing to zcomp_create() max_strm greater than 1 will initialise zcomp
    using multi compression stream zcomp_strm_multi (spinlock-based locking).
    
    default max_comp_streams value is 1, meaning that zram with single stream
    will be initialised.
    
    Later patch will introduce configuration knob to change max_comp_streams
    on already initialised and used zcomp.
    
    TEST
    iozone -t 3 -R -r 16K -s 60M -I +Z
    
           test           base       1 strm (mutex)     3 strm (spinlock)
    -----------------------------------------------------------------------
     Initial write      589286.78       583518.39          718011.05
           Rewrite      604837.97       596776.38         1515125.72
      Random write      584120.11       595714.58         1388850.25
            Pwrite      535731.17       541117.38          739295.27
            Fwrite     1418083.88      1478612.72         1484927.06
    
    Usage example:
    set max_comp_streams to 4
            echo 4 > /sys/block/zram0/max_comp_streams
    
    show current max_comp_streams (default value is 1).
            cat /sys/block/zram0/max_comp_streams
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index dc3500d842a3..2a3684446160 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -21,6 +21,8 @@ struct zcomp_strm {
 	 * working memory)
 	 */
 	void *private;
+	/* used in multi stream backend, protected by backend strm_lock */
+	struct list_head list;
 };
 
 /* static compression backend */
@@ -47,7 +49,7 @@ struct zcomp {
 	void (*destroy)(struct zcomp *comp);
 };
 
-struct zcomp *zcomp_create(const char *comp);
+struct zcomp *zcomp_create(const char *comp, int max_strm);
 void zcomp_destroy(struct zcomp *comp);
 
 struct zcomp_strm *zcomp_strm_find(struct zcomp *comp);

commit 9cc97529a180b369fcb7e5265771b6ba7e01f05b
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Mon Apr 7 15:38:13 2014 -0700

    zram: factor out single stream compression
    
    This is preparation patch to add multi stream support to zcomp.
    
    Introduce struct zcomp_strm_single and a set of functions to manage
    zcomp_strm stream access.  zcomp_strm_single implements single compession
    stream, same way as current zcomp implementation.  This moves zcomp_strm
    stream control and locking from zcomp, so compressing backend zcomp is not
    aware of required locking.
    
    Single and multi streams require different locking schemes.  Minchan Kim
    reported that spinlock-based locking scheme (which is used in multi stream
    implementation) has demonstrated a severe perfomance regression for single
    compression stream case, comparing to mutex-based.  see
    https://lkml.org/lkml/2014/2/18/16
    
    The following set of functions added:
    - zcomp_strm_single_find()/zcomp_strm_single_release()
      find and release a compression stream, implement required locking
    - zcomp_strm_single_create()/zcomp_strm_single_destroy()
      create and destroy zcomp_strm_single
    
    New ->strm_find() and ->strm_release() callbacks added to zcomp, which are
    set to zcomp_strm_single_find() and zcomp_strm_single_release() during
    initialisation.  Instead of direct locking and zcomp_strm access from
    zcomp_strm_find() and zcomp_strm_release(), zcomp now calls ->strm_find()
    and ->strm_release() correspondingly.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
index c9a98e1317fe..dc3500d842a3 100644
--- a/drivers/block/zram/zcomp.h
+++ b/drivers/block/zram/zcomp.h
@@ -39,9 +39,12 @@ struct zcomp_backend {
 
 /* dynamic per-device compression frontend */
 struct zcomp {
-	struct mutex strm_lock;
-	struct zcomp_strm *zstrm;
+	void *stream;
 	struct zcomp_backend *backend;
+
+	struct zcomp_strm *(*strm_find)(struct zcomp *comp);
+	void (*strm_release)(struct zcomp *comp, struct zcomp_strm *zstrm);
+	void (*destroy)(struct zcomp *comp);
 };
 
 struct zcomp *zcomp_create(const char *comp);

commit e7e1ef439d18f9a21521116ea9f2b976d7230e54
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Mon Apr 7 15:38:11 2014 -0700

    zram: introduce compressing backend abstraction
    
    ZRAM performs direct LZO compression algorithm calls, making it the one
    and only option.  While LZO is generally performs well, LZ4 algorithm
    tends to have a faster decompression (see http://code.google.com/p/lz4/
    for full report)
    
            Name            Ratio  C.speed D.speed
                                    MB/s    MB/s
            LZ4 (r101)      2.084    422    1820
            LZO 2.06        2.106    414     600
    
    Thus, users who have mostly read (decompress) usage scenarious or mixed
    workflow (writes with relatively high read ops number) will benefit from
    using LZ4 compression backend.
    
    Introduce compressing backend abstraction zcomp in order to support
    multiple compression algorithms with the following set of operations:
    
            .create
            .destroy
            .compress
            .decompress
    
    Schematically zram write() usually contains the following steps:
    0) preparation (decompression of partioal IO, etc.)
    1) lock buffer_lock mutex (protects meta compress buffers)
    2) compress (using meta compress buffers)
    3) alloc and map zs_pool object
    4) copy compressed data (from meta compress buffers) to object allocated by 3)
    5) free previous pool page, assign a new one
    6) unlock buffer_lock mutex
    
    As we can see, compressing buffers must remain untouched from 1) to 4),
    because, otherwise, concurrent write() can overwrite data.  At the same
    time, zram_meta must be aware of a) specific compression algorithm memory
    requirements and b) necessary locking to protect compression buffers.  To
    remove requirement a) new struct zcomp_strm introduced, which contains a
    compress/decompress `buffer' and compression algorithm `private' part.
    While struct zcomp implements zcomp_strm stream handling and locking and
    removes requirement b) from zram meta.  zcomp ->create() and ->destroy(),
    respectively, allocate and deallocate algorithm specific zcomp_strm
    `private' part.
    
    Every zcomp has zcomp stream and mutex to protect its compression stream.
    Stream usage semantics remains the same -- only one write can hold stream
    lock and use its buffers.  zcomp_strm_find() turns caller into exclusive
    user of a stream (holding stream mutex until zram release stream), and
    zcomp_strm_release() makes zcomp stream available (unlock the stream
    mutex).  Hence no concurrent write (compression) operations possible at
    the moment.
    
    iozone -t 3 -R -r 16K -s 60M -I +Z
    
           test            base           patched
    --------------------------------------------------
      Initial write      597992.91       591660.58
            Rewrite      609674.34       616054.97
               Read     2404771.75      2452909.12
            Re-read     2459216.81      2470074.44
       Reverse Read     1652769.66      1589128.66
        Stride read     2202441.81      2202173.31
        Random read     2236311.47      2276565.31
     Mixed workload     1423760.41      1709760.06
       Random write      579584.08       615933.86
             Pwrite      597550.02       594933.70
              Pread     1703672.53      1718126.72
             Fwrite     1330497.06      1461054.00
              Fread     3922851.00      3957242.62
    
    Usage examples:
    
            comp = zcomp_create(NAME) /* NAME e.g. "lzo" */
    
    which initialises compressing backend if requested algorithm is supported.
    
    Compress:
            zstrm = zcomp_strm_find(comp)
            zcomp_compress(comp, zstrm, src, &dst_len)
            [..] /* copy compressed data */
            zcomp_strm_release(comp, zstrm)
    
    Decompress:
            zcomp_decompress(comp, src, src_len, dst);
    
    Free compessing backend and its zcomp stream:
            zcomp_destroy(comp)
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/block/zram/zcomp.h b/drivers/block/zram/zcomp.h
new file mode 100644
index 000000000000..c9a98e1317fe
--- /dev/null
+++ b/drivers/block/zram/zcomp.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2014 Sergey Senozhatsky.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _ZCOMP_H_
+#define _ZCOMP_H_
+
+#include <linux/mutex.h>
+
+struct zcomp_strm {
+	/* compression/decompression buffer */
+	void *buffer;
+	/*
+	 * The private data of the compression stream, only compression
+	 * stream backend can touch this (e.g. compression algorithm
+	 * working memory)
+	 */
+	void *private;
+};
+
+/* static compression backend */
+struct zcomp_backend {
+	int (*compress)(const unsigned char *src, unsigned char *dst,
+			size_t *dst_len, void *private);
+
+	int (*decompress)(const unsigned char *src, size_t src_len,
+			unsigned char *dst);
+
+	void *(*create)(void);
+	void (*destroy)(void *private);
+
+	const char *name;
+};
+
+/* dynamic per-device compression frontend */
+struct zcomp {
+	struct mutex strm_lock;
+	struct zcomp_strm *zstrm;
+	struct zcomp_backend *backend;
+};
+
+struct zcomp *zcomp_create(const char *comp);
+void zcomp_destroy(struct zcomp *comp);
+
+struct zcomp_strm *zcomp_strm_find(struct zcomp *comp);
+void zcomp_strm_release(struct zcomp *comp, struct zcomp_strm *zstrm);
+
+int zcomp_compress(struct zcomp *comp, struct zcomp_strm *zstrm,
+		const unsigned char *src, size_t *dst_len);
+
+int zcomp_decompress(struct zcomp *comp, const unsigned char *src,
+		size_t src_len, unsigned char *dst);
+#endif /* _ZCOMP_H_ */
